---
ver: rpa2
title: Visual Question Generation in Bengali
arxiv_id: '2310.08187'
source_url: https://arxiv.org/abs/2310.08187
tags:
- image
- question
- questions
- category
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first Visual Question Generation (VQG)
  system for the Bengali language. The authors propose a transformer-based encoder-decoder
  architecture that generates questions in Bengali from images, with optional additional
  information like answer categories.
---

# Visual Question Generation in Bengali

## Quick Facts
- **arXiv ID**: 2310.08187
- **Source URL**: https://arxiv.org/abs/2310.08187
- **Reference count**: 16
- **Primary result**: First VQG system for Bengali achieves BLEU-1 of 33.12 and BLEU-3 of 7.56 with image-category model

## Executive Summary
This paper introduces the first Visual Question Generation (VQG) system for the Bengali language, addressing the lack of research in this area despite Bengali being one of the world's most spoken languages. The authors develop a transformer-based encoder-decoder architecture that generates questions in Bengali from images, with optional additional information like answer categories. The system is trained and evaluated on a translated VQAv2.0 dataset, achieving competitive performance metrics and demonstrating strong human evaluation scores for relevance and quality.

## Method Summary
The proposed method employs a transformer-based encoder-decoder architecture with three variants: image-only, image-category, and image-answer-category. The image encoder uses ResNet-18 to extract visual features, while the text encoder processes Bengali text using GloVe embeddings. The decoder generates questions conditioned on either image features alone or in combination with categorical information. The models are trained on a translated VQAv2.0 dataset using cross-entropy loss for question generation and l2 loss for optional image reconstruction. The image-category model, which uses answer categories as conditioning information, achieves the best performance with a BLEU-1 score of 33.12 and BLEU-3 score of 7.56.

## Key Results
- Image-category model achieves highest performance: BLEU-1 of 33.12, BLEU-3 of 7.56
- Human evaluation confirms ability to generate relevant, goal-driven, and attribute-specific questions
- Model successfully conditions questions based on given answer categories
- First VQG system developed for Bengali language

## Why This Works (Mechanism)

### Mechanism 1
Using answer categories as additional input conditions the model to generate more goal-driven and attribute-specific questions rather than generic ones. The categorical conditioning acts as a semantic constraint that narrows the question generation space toward the expected answer type, improving relevance.

### Mechanism 2
The combination of image features with textual modality through concatenation provides richer representation for the decoder. The encoder fuses vision and text by concatenating image features with encoded textual features, creating a multimodal context that the decoder uses to generate questions.

### Mechanism 3
Image reconstruction loss encourages the encoder to retain more visual information, improving question relevance. By minimizing l2 loss between reconstructed and original image features, the model maximizes mutual information between the input image and encoder outputs.

## Foundational Learning

- **Transformer-based encoder-decoder architecture**: Enables end-to-end learning of multimodal representation fusion and sequence generation for VQG. Quick check: What are the three main components of a transformer encoder layer?
- **GloVe embeddings for Bengali text**: Provides pre-trained word vector representations adapted to Bengali language structure. Quick check: How does GloVe differ from word2vec in terms of training objective?
- **BLEU score for evaluating text generation**: Standard metric to measure n-gram overlap between generated and reference questions. Quick check: What is the difference between BLEU-1 and BLEU-4?

## Architecture Onboarding

- **Component map**: Image → ResNet-18 → Flatten → FC → BN → Image features → Concatenate with Text features → Decoder → Generated question
- **Critical path**: Image features flow through ResNet-18 to FC layer, then batch normalization, before concatenation with text features for decoder input
- **Design tradeoffs**: Using answer categories vs. answers (guidance without answer leakage), concatenation vs. attention fusion (simplicity vs. fine-grained interactions), image reconstruction loss (visual preservation vs. training complexity)
- **Failure signatures**: Grammatically correct but semantically irrelevant questions, overfitting to training categories, high BLEU but low human evaluation scores
- **First 3 experiments**: 1) Train image-only variant and evaluate BLEU scores, 2) Add categorical conditioning and compare BLEU scores, 3) Add image reconstruction loss and measure impact on both BLEU and human evaluation

## Open Questions the Paper Calls Out
- How does Bengali VQG performance compare to other low-resource languages under similar conditions?
- Would more modern CNN architectures like EfficientNet or Vision Transformers improve Bengali VQG performance compared to ResNet-18?
- How does the quality of automatically translated VQA datasets compare to manually annotated Bengali VQG datasets?

## Limitations
- Translation quality of VQA v2.0 to Bengali is not validated, potentially introducing noise
- Human evaluation methodology and sample size are not specified
- Bengali GloVe embeddings source and training details are undisclosed

## Confidence
- **High**: Architecture design and implementation details are well-specified and reproducible
- **Medium**: Reported BLEU scores and their interpretation for Bengali VQG
- **Low**: Generalization of findings to other Bengali datasets or real-world applications

## Next Checks
1. Replicate with different translation methods: Compare results using Google Translate vs. professional translation services to assess translation quality impact
2. Cross-lingual transfer validation: Test the best-performing model on an English VQA dataset to evaluate whether improvements are language-specific or architecture-driven
3. Ablation study on categorical conditioning: Systematically remove answer categories from the image-cat model to quantify their exact contribution to performance gains