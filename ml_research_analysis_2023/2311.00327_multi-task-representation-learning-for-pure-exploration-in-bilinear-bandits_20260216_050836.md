---
ver: rpa2
title: Multi-task Representation Learning for Pure Exploration in Bilinear Bandits
arxiv_id: '2311.00327'
source_url: https://arxiv.org/abs/2311.00327
tags:
- log2
- follows
- lemma
- where
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new approach for pure exploration in multi-task
  bilinear bandits with shared representation. The problem involves finding optimal
  action pairs for multiple tasks where each task has a bilinear reward function with
  a low-rank hidden parameter matrix.
---

# Multi-task Representation Learning for Pure Exploration in Bilinear Bandits

## Quick Facts
- arXiv ID: 2311.00327
- Source URL: https://arxiv.org/abs/2311.00327
- Reference count: 40
- Primary result: Achieves improved sample complexity for pure exploration in multi-task bilinear bandits with shared representation

## Executive Summary
This paper introduces GOBLIN, a novel algorithm for pure exploration in multi-task bilinear bandits where tasks share a low-dimensional linear representation. The key innovation is leveraging this shared structure to significantly reduce sample complexity compared to existing methods. GOBLIN alternates between estimating the shared representation and identifying optimal action pairs for each task, achieving theoretical sample complexity of O(M(k₁+k₂)r/∆²) for multi-task and O((d₁+d₂)r/∆²) for single-task settings. Experiments on synthetic data validate the theoretical findings, demonstrating GOBLIN's effectiveness in reducing sample complexity compared to state-of-the-art baselines.

## Method Summary
GOBLIN is a phase-based algorithm that tackles pure exploration in multi-task bilinear bandits with shared low-dimensional representations. The algorithm alternates between three key stages: (1) estimating shared feature extractors using E-optimal experimental design, (2) estimating task-specific parameters with nuclear norm regularization, and (3) performing pure exploration using G-optimal design in the rotated arm space. This approach reduces the effective dimension from d₁d₂ to (k₁+k₂)r, enabling significant sample complexity improvements. The algorithm's theoretical guarantees rely on matrix concentration inequalities and SVD perturbation theory, with performance scaling as O(M(k₁+k₂)r/∆²) for the multi-task setting.

## Key Results
- Achieves improved sample complexity of O((d₁+d₂)r/∆²) for single-task bilinear bandit pure exploration
- Extends to multi-task setting with sample complexity O(M(k₁+k₂)r/∆²), where M is the number of tasks
- Validated through synthetic experiments showing significant reduction in sample complexity compared to RAGE and DouExpDes baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GOBLIN achieves improved sample complexity by learning shared representations across tasks
- Mechanism: The algorithm alternates between estimating shared feature extractors (B₁, B₂) and task-specific parameters (Sₘ,*), reducing the effective dimension from d₁d₂ to (k₁+k₂)r
- Core assumption: Tasks share common low-dimensional feature extractors and have diverse task-specific parameters
- Evidence anchors:
  - [abstract] "leverage this characteristic to expedite the process of identifying the best pair of arms for all tasks"
  - [section] "GOBLIN uses an experimental design approach to optimize sample allocations for learning the global representation"
- Break condition: If tasks don't share feature extractors or if diversity assumption is violated, the dimensionality reduction fails

### Mechanism 2
- Claim: GOBLIN reduces single-task sample complexity by rotating arms to effective dimension
- Mechanism: After estimating Θ*, GOBLIN projects arms onto row and column subspaces, converting the problem to a (d₁+d₂)r dimensional linear bandit
- Core assumption: The low-rank structure of Θ* allows effective dimensionality reduction
- Evidence anchors:
  - [section] "GOBLIN uses the information about the learned sub-space of Θ* to reduce the problem from ambient dimension d₁d₂ to effective dimension (d₁+d₂)r"
  - [section] "This reduction is done as follows: Let bΘℓ = bUℓbDℓbV⊤ℓ be the SVD of bΘℓ"
- Break condition: If rank estimation is inaccurate or if projection doesn't preserve optimal arms, performance degrades

### Mechanism 3
- Claim: GOBLIN uses optimal experimental design to minimize samples while maintaining accuracy
- Mechanism: G-optimal design selects arms that maximize information gain while maintaining statistical confidence bounds
- Core assumption: G-optimal design provides near-optimal sample complexity for identification problems
- Evidence anchors:
  - [section] "GOBLIN implements G-optimal design (Pukelsheim, 2006; Fiez et al., 2019) in the rotated arm set"
  - [section] "sampling according tobbGℓ leads to the optimal sample complexity"
- Break condition: If optimal design solution has large support or if confidence bounds are loose, sample complexity increases

## Foundational Learning

- Concept: Bilinear bandits with low-rank structure
  - Why needed here: The entire algorithm relies on the assumption that Θ* has low rank, enabling dimensionality reduction
  - Quick check question: Why does low-rank structure allow us to reduce from d₁d₂ to (d₁+d₂)r dimensions?

- Concept: Experimental design theory (G-optimal and E-optimal designs)
  - Why needed here: GOBLIN uses these designs to allocate samples efficiently for parameter estimation
  - Quick check question: What's the difference between G-optimal and E-optimal designs, and why does GOBLIN use both?

- Concept: Matrix concentration inequalities and SVD perturbation theory
  - Why needed here: The algorithm's theoretical guarantees rely on concentration of estimated matrices and SVD stability
  - Quick check question: How does Davis-Kahan theorem ensure that estimated singular vectors are close to true singular vectors?

## Architecture Onboarding

- Component map: Estimate shared feature extractors (B₁, B₂) → Estimate task-specific parameters (Sₘ,*) → Rotate arms to effective dimension → Apply G-optimal design for elimination

- Critical path: Arm elimination loop → Parameter estimation → Arm rotation → Optimal design sampling → Confidence checking

- Design tradeoffs:
  - Single vs multi-task: Multi-task trades off complexity for shared representation benefits
  - E-optimal vs G-optimal: E-optimal for exploration, G-optimal for exploitation
  - Phase length: Must balance between too short (inefficient) and too long (slow adaptation)

- Failure signatures:
  - High variance in parameter estimates → Check concentration bounds and sample sizes
  - Slow arm elimination → Verify rotation preserves optimal arms and check gap estimates
  - Poor multi-task performance → Validate shared representation assumption and task diversity

- First 3 experiments:
  1. Single-task bilinear bandit with known rank: Verify dimensionality reduction and sample complexity improvement over RAGE
  2. Multi-task bilinear bandit with varying task similarity: Test performance across different levels of task diversity
  3. Ablation study on design choices: Compare E-optimal vs random sampling, G-optimal vs uniform sampling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the multi-task bilinear bandit framework be extended to handle non-stationary tasks where the hidden parameters Θₘ,* change over time?
- Basis in paper: [inferred] The paper focuses on pure exploration with static hidden parameters across tasks, but real-world applications often involve evolving parameters.
- Why unresolved: The theoretical analysis assumes fixed low-rank structures and does not account for temporal dynamics or concept drift in the multi-task setting.
- What evidence would resolve it: Empirical evaluation showing performance degradation when parameters drift, and theoretical bounds that incorporate adaptation costs for non-stationary environments.

### Open Question 2
- Question: How does the performance of GOBLIN scale when the shared representation dimension (k₁, k₂) approaches the ambient dimension (d₁, d₂)?
- Basis in paper: [explicit] The paper assumes k₁, k₂ ≪ d₁, d₂ for computational efficiency, but doesn't explore the regime where this assumption breaks down.
- Why unresolved: The analysis relies heavily on the low-dimensional representation assumption, and the effective dimension scaling would need re-evaluation.
- What evidence would resolve it: Comparative experiments showing sample complexity growth as k₁, k₂ increase, and theoretical analysis of the transition point where multi-task benefits diminish.

### Open Question 3
- Question: Can the optimal design approach be adapted for contextual bilinear bandits where context features modify the reward structure?
- Basis in paper: [inferred] The current framework assumes static feature vectors for arms, but many applications involve context-dependent rewards.
- Why unresolved: The theoretical guarantees depend on the bilinear structure without contextual modifiers, and extending the experimental design to handle context-arm interactions would require new mathematical tools.
- What evidence would resolve it: A modified algorithm incorporating context features, with theoretical analysis showing how sample complexity scales with context dimension.

### Open Question 4
- Question: What is the impact of noise distribution on the sample complexity bounds, particularly for heavy-tailed noise?
- Basis in paper: [explicit] The paper assumes 1-subGaussian noise, but doesn't explore robustness to other noise distributions.
- Why unresolved: The concentration inequalities and estimation procedures rely on sub-Gaussian assumptions, which may not hold in practice.
- What evidence would resolve it: Experiments with heavy-tailed noise showing performance degradation, and theoretical bounds that incorporate robust estimation techniques.

### Open Question 5
- Question: How does the algorithm perform when the rank r is unknown and must be estimated during execution?
- Basis in paper: [explicit] The paper assumes known rank r for all hidden parameters, which is often unrealistic in practice.
- Why unresolved: The experimental design and elimination procedures are designed around a fixed rank, and rank estimation would introduce additional uncertainty.
- What evidence would resolve it: Empirical evaluation with varying true ranks and proposed rank estimation procedures, along with theoretical analysis of the trade-offs between estimation accuracy and sample complexity.

## Limitations
- Assumes known feature vectors and shared low-dimensional representations, which may not hold in real-world applications
- Performance guarantees rely on accurate estimation of low-rank structure and sufficient task diversity
- Limited experimental validation, primarily based on synthetic data rather than real-world benchmarks

## Confidence
- Sample complexity bounds: High
- Shared representation mechanism: Medium-High
- Multi-task performance gains: Medium
- Experimental validation: Medium

## Next Checks
1. **Empirical scalability test**: Evaluate GOBLIN on larger-scale synthetic datasets with varying numbers of tasks and feature dimensions to verify the claimed O(M(k₁+k₂)r/∆²) sample complexity scaling holds in practice.

2. **Robustness to representation mismatch**: Test GOBLIN's performance when the shared representation assumption is partially violated (e.g., with noisy or partially shared feature extractors) to assess real-world applicability.

3. **Comparison with hybrid approaches**: Implement and benchmark against methods that combine GOBLIN's representation learning with alternative pure exploration strategies to isolate the contribution of the shared representation mechanism.