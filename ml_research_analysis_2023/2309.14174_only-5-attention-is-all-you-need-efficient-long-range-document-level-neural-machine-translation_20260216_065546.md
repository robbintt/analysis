---
ver: rpa2
title: 'Only 5\% Attention Is All You Need: Efficient Long-range Document-level Neural
  Machine Translation'
arxiv_id: '2309.14174'
source_url: https://arxiv.org/abs/2309.14174
tags:
- attention
- translation
- tokens
- cost
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Lasformer to address the quadratic computational
  cost of attention in document-level neural machine translation. The core method
  idea is to use lightweight attention to select important tokens and only attend
  to them, while ensuring performance through attention supervision.
---

# Only 5\% Attention Is All You Need: Efficient Long-range Document-level Neural Machine Translation

## Quick Facts
- arXiv ID: 2309.14174
- Source URL: https://arxiv.org/abs/2309.14174
- Reference count: 17
- Primary result: Achieves 95% sparsity (5% tokens attended) and 93% attention computation savings while maintaining translation quality

## Executive Summary
This paper introduces Lasformer, a method for efficient document-level neural machine translation that addresses the quadratic computational cost of attention in long sequences. Lasformer uses lightweight attention to select a small portion of tokens to attend to, while maintaining performance through attention supervision and adaptive sparsity. The method achieves significant computational savings while preserving translation quality on English-German and Chinese-English document-level translation tasks.

## Method Summary
Lasformer modifies the Transformer architecture by inserting a lightweight attention-based selection module before each attention layer. This selection module projects hidden states from dimension d to a smaller dimension ds (e.g., 512→64), computes rough attention scores, and selects the top-k tokens for the main attention layer. The selection is guided by KL divergence supervision from the original attention distribution and uses adaptive sparsity controlled by a threshold. Layer sharing is employed to reduce selection computation by grouping multiple attention layers to share the same selection module.

## Key Results
- Achieves 95% sparsity (only 5% tokens attended)
- Saves 93% computation cost on the attention module
- Maintains BLEU scores close to the original Transformer on document-level translation tasks
- Adaptive sparsity and attention supervision are critical for maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
The lightweight attention layer selects tokens that contribute most to the current query by using a low-dimensional projection (d → ds) to reduce computation while maintaining effectiveness. A reduced-dimension attention computes rough scores, then top-k tokens are selected for the main attention layer. The core assumption is that tokens with high attention scores in the lightweight layer are likely to be important in the full attention layer. Evidence shows that this selection works empirically, though the theoretical foundation for why this approximation succeeds is not established.

### Mechanism 2
Attention supervision via KL divergence aligns the lightweight attention distribution with the original attention, ensuring selected tokens remain relevant. A KL loss pulls the lightweight attention distribution toward the original attention during training. The core assumption is that consistency between lightweight and original attention distributions preserves translation quality. Ablation studies show removing attention supervision drops BLEU from 28.04 to 12.94, confirming its necessity.

### Mechanism 3
Adaptive sparsity adjusts the number of tokens attended (k) based on attention concentration, balancing efficiency and quality. A threshold t monitors cumulative attention; if sum(topk) < t, k is increased slightly, otherwise decreased. The core assumption is that the top tokens' cumulative attention can indicate whether enough relevant information is retained. Experiments show t = 0.95 achieves 7% sparsity with best BLEU, supporting adaptive control.

## Foundational Learning

- Concept: Attention mechanism in Transformers
  - Why needed here: Lasformer modifies attention by selecting tokens; understanding attention scores, query/key/value projections, and softmax is essential.
  - Quick check question: How does the attention score As = softmax(QsKT_s / √ds) differ from the standard attention A = softmax(QKT / √d)?

- Concept: Sparsity and its effect on computation
  - Why needed here: The paper reduces attention cost by attending only k% of tokens; knowing how sparsity reduces O(N²) to O(kN²) is critical.
  - Quick check question: If N=1000 and k=0.05, how much faster is the attention computation compared to full attention?

- Concept: KL divergence for distribution alignment
  - Why needed here: Supervision aligns lightweight and original attention; understanding KL divergence's role in pulling distributions together is necessary.
  - Quick check question: What does a high KL loss between As and A indicate about the lightweight attention distribution?

## Architecture Onboarding

- Component map: Input → Encoder/Decoder layers with standard multi-head attention → Selection module (lightweight attention) inserted before each attention layer → Dynamic top-k selection with adaptive k and attention supervision → Layer sharing groups reduce selection computation

- Critical path: Lightweight attention → Top-k mask → Masked main attention → Output. Each main attention layer has an associated selection layer; masks are shared within layer groups.

- Design tradeoffs:
  - Lower ds (e.g., 16) → less selection cost but worse BLEU
  - Higher k (more tokens) → better BLEU but less efficiency
  - More layer groups (smaller r) → better performance but higher selection cost
  - Without attention supervision → much lower BLEU

- Failure signatures:
  - BLEU drops sharply if k is too low or supervision is removed
  - If ds is too small (e.g., 16), translation quality suffers
  - Without adaptive k, fixed sparsity may not suit all layers

- First 3 experiments:
  1. Baseline: Run original Transformer on document-level dataset (e.g., PDC) and record BLEU and attention cost.
  2. Add lightweight selection only: Insert ds=64 selection before attention, disable supervision, measure impact on efficiency and BLEU.
  3. Full Lasformer: Enable adaptive k with t=0.95, supervision (α=0.01), and layer sharing (r=3), compare against baseline.

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal sparsity threshold (t) that balances translation quality and computational efficiency across different document lengths and domains? The paper only tests a few values of t (0.90, 0.95, 0.99) and suggests 0.95 as a tradeoff, but does not systematically explore the relationship between t and document characteristics. Comprehensive experiments varying t across different document lengths, domains, and translation tasks would identify the optimal threshold for each scenario.

### Open Question 2
How does the learned sparsity pattern generalize to languages with different syntactic structures and morphological complexities? The paper demonstrates effectiveness on English-German and Chinese-English translation but does not address cross-linguistic applicability. Testing Lasformer on diverse language pairs (e.g., morphologically rich languages, languages with different word orders) and comparing the learned sparsity patterns and translation quality would resolve this question.

### Open Question 3
Can the layer sharing mechanism be optimized to share more layers without sacrificing translation quality? The paper mentions that sharing layers can reduce computation but also notes that sharing too many layers might limit model capacity. The paper uses a fixed group size of 3 layers for sharing but does not explore whether different sharing strategies or adaptive sharing could improve efficiency without harming performance. Experiments varying the number of layers in each sharing group would provide evidence.

## Limitations
- Theoretical uncertainty about the selection mechanism's reliability - the lightweight attention-based token selection depends on the assumption that low-dimensional attention scores reliably identify important tokens
- Sensitivity to hyperparameter choices - performance appears highly sensitive to specific parameter choices including ds, k, t, α, and r
- GPU optimization assumptions - claimed 93% computation savings assumes efficient sparse implementation support by hardware

## Confidence

**High confidence (8/10):** The core empirical claims about achieving 95% sparsity and 93% attention computation reduction while maintaining BLEU scores are well-supported by the experimental results presented.

**Medium confidence (6/10):** The claim that Lasformer "maintains translation performance" is substantiated for the specific datasets and settings tested, but generalization to other language pairs, document domains, or longer sequences remains uncertain.

**Low confidence (4/10):** The theoretical claims about why lightweight attention selection works are weakly supported - the paper provides empirical evidence but lacks rigorous analysis of when and why the approximation error remains acceptable.

## Next Checks

1. **Cross-domain robustness test:** Evaluate Lasformer on multiple document-level translation datasets with varying characteristics (e.g., Wikipedia articles, legal documents, biomedical texts) to assess whether the 95% sparsity and 93% computation savings generalize beyond the TED and PDC datasets used in the paper.

2. **Scaling analysis:** Test Lasformer on sequences longer than 4096 tokens to determine whether the lightweight selection mechanism maintains its effectiveness as document length increases, and measure whether the claimed computational savings scale proportionally.

3. **Implementation benchmark:** Create a comprehensive benchmark comparing Lasformer's actual wall-clock time and memory usage against the original Transformer on identical hardware, including measurements for different batch sizes, sequence lengths, and GPU architectures to verify the practical efficiency gains claimed in the paper.