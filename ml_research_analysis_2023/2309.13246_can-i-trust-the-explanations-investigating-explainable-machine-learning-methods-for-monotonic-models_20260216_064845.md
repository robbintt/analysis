---
ver: rpa2
title: Can I Trust the Explanations? Investigating Explainable Machine Learning Methods
  for Monotonic Models
arxiv_id: '2309.13246'
source_url: https://arxiv.org/abs/2309.13246
tags:
- u1d465
- u1d446
- u1d453
- monotonicity
- u1d6fd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether explainable machine learning methods
  can provide consistent scientific explanations when applied to science-informed
  machine learning models with monotonicity constraints. The authors propose three
  new axioms for monotonicity-preserving attribution methods and analyze two popular
  approaches - Integrated Gradients (IG) and Baseline Shapley Value (BShap).
---

# Can I Trust the Explanations? Investigating Explainable Machine Learning Methods for Monotonic Models

## Quick Facts
- arXiv ID: 2309.13246
- Source URL: https://arxiv.org/abs/2309.13246
- Reference count: 25
- Key outcome: BShap works well for individual monotonicity, IG performs better for strong pairwise monotonicity

## Executive Summary
This paper investigates whether popular explainable machine learning methods can provide consistent scientific explanations when applied to science-informed machine learning models with monotonicity constraints. The authors propose three new axioms for monotonicity-preserving attribution methods and analyze two popular approaches - Integrated Gradients (IG) and Baseline Shapley Value (BShap). Through theoretical analysis and empirical examples on credit scoring data, they demonstrate that IG satisfies all monotonicity axioms on average but BShap fails for strong pairwise monotonicity cases. The study concludes that domain knowledge like monotonicity is crucial for model interpretation, especially in high-stakes applications like finance.

## Method Summary
The study implements two model architectures - a fully-connected neural network (FCNN) with one hidden layer and a monotonic groves of neural additive model (MGNAM) - both trained on Kaggle credit score data to predict 90+ day delinquency. The models incorporate monotonicity constraints on past due features. Attribution methods are applied using zero baseline, with IG computing path integrals and BShap averaging marginal contributions. The authors theoretically analyze whether these methods preserve three monotonicity axioms (individual, weak pairwise, and strong pairwise) and empirically validate their findings.

## Key Results
- BShap preserves individual monotonicity and weak pairwise monotonicity but fails strong pairwise monotonicity
- IG satisfies all monotonicity axioms on average but does not preserve individual monotonicity
- On credit scoring data, IG provides better explanations when strong pairwise monotonicity is required
- Domain knowledge through monotonicity constraints improves both model generalization and interpretation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrated Gradients preserves average monotonicity axioms but fails individual monotonicity preservation
- Mechanism: IG computes attributions by integrating gradients along a path from baseline to input, which on average captures monotonic behavior even if individual feature paths violate monotonicity
- Core assumption: The monotonic relationship holds when averaged across all possible paths from baseline to input
- Evidence anchors:
  - [abstract] "IG satisfies all monotonicity axioms on average but BShap fails for strong pairwise monotonicity cases"
  - [section] "Theorem 3.10. IG preserves AIM. Suppose /u1D453is individually monotonic with respect to /u1D465/u1D6FC, then IG/u1D6FC = (/u1D465/u1D6FC - /u1D465′/u1D6FC) ∫₀¹ ∂/∂x_/u1D6FC /u1D453(x′ + τ(x-x′))dτ ≥ 0, since /u1D465/u1D6FC ≥ /u1D465′/u1D6FC and ∂/∂x_/u1D6FC /u1D453 ≥ 0"
  - [corpus] Weak or missing - corpus neighbors don't address monotonicity preservation specifically
- Break condition: If the monotonic relationship only holds for specific paths but not when averaged, IG will fail to preserve monotonicity

### Mechanism 2
- Claim: BShap preserves individual monotonicity and weak pairwise monotonicity but fails strong pairwise monotonicity
- Mechanism: BShap computes attributions by averaging marginal contributions across all possible feature orderings, which preserves monotonic relationships when features are individually monotonic or when weak pairwise monotonicity holds
- Core assumption: Feature interactions in monotonic models are captured by the averaging process in Shapley value computation
- Evidence anchors:
  - [abstract] "BShap works well when only individual monotonicity is involved, while IG provides better explanations on average when strong pairwise monotonicity is required"
  - [section] "Theorem 3.3. BShap preserves DIM. Without loss of generality, suppose /u1D453is individually monotonic with respect to /u1D4651. Suppose x = (/u1D4651, x¬) and x∗ = (/u1D4651 + /u1D450,x¬) for /u1D450> 0. Then BS1(x∗, x′, /u1D453) - BS1(x, x′, /u1D453) = Σ/|S|!(| /u1D440| - |S| - 1)! /u1D440! Δ /u1D453/S, where Δ /u1D453/S = /u1D453(/u1D4651 + /u1D450; x/S, x′/u1D440\(S∪1)) - /u1D453(/u1D4651; x/S, x′/u1D440\(S∪1)) ≥ 0 because of individual monotonicity"
  - [corpus] Weak or missing - corpus neighbors don't address Shapley value monotonicity properties specifically
- Break condition: When strong pairwise monotonicity is required and feature interactions create diminishing returns, BShap fails to capture the correct attribution ordering

### Mechanism 3
- Claim: Domain knowledge through monotonicity constraints improves model generalization and interpretation
- Mechanism: By enforcing monotonicity constraints during model training, the model learns relationships that align with domain knowledge, making attributions more meaningful and consistent with scientific understanding
- Core assumption: The true underlying relationship between features and target follows monotonic patterns that can be captured through constraints
- Evidence anchors:
  - [abstract] "By incorporating domain knowledge, science-informed machine learning models have demonstrated better generalization and interpretation"
  - [section] "Modeling and ensuring conceptual soundness require domain knowledge: the true model should be consistent with the underlying theories"
  - [corpus] Weak or missing - corpus neighbors don't address domain knowledge incorporation specifically
- Break condition: If the true relationship is non-monotonic or the monotonicity constraints are incorrectly specified, model performance and interpretation will degrade

## Foundational Learning

- Concept: Monotonicity in machine learning
  - Why needed here: The paper's core contribution relies on understanding different types of monotonicity (individual, weak pairwise, strong pairwise) and how they affect attribution methods
  - Quick check question: What is the difference between individual monotonicity and strong pairwise monotonicity, and why does this distinction matter for attribution methods?

- Concept: Shapley values and Integrated Gradients
  - Why needed here: The paper compares two specific attribution methods (BShap and IG) and analyzes their properties under different monotonicity conditions
  - Quick check question: How do Shapley values compute feature attributions differently from Integrated Gradients, and what are the implications for monotonicity preservation?

- Concept: Domain-informed model constraints
  - Why needed here: The paper argues that incorporating domain knowledge like monotonicity improves both model performance and interpretability
  - Quick check question: Why might enforcing monotonicity constraints during model training lead to better generalization and more interpretable attributions?

## Architecture Onboarding

- Component map: Monotonic model architecture -> Attribution method (BShap/IG) -> Axiom verification -> Model interpretation
- Critical path: Monotonic model → Attribution computation → Axiom verification → Model interpretation
- Design tradeoffs: BShap preserves individual monotonicity but fails strong pairwise monotonicity; IG preserves average monotonicity but not individual monotonicity
- Failure signatures: Attribution methods that violate monotonicity axioms indicate either model misspecification or inappropriate attribution method selection
- First 3 experiments:
  1. Implement BShap and IG on a simple monotonic function (e.g., linear or min/max) to verify basic functionality
  2. Test both methods on a function with individual monotonicity only to confirm BShap works while IG fails DIM
  3. Test both methods on a function with strong pairwise monotonicity to confirm IG works while BShap fails ASPM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we develop a modified Shapley value method that preserves strong pairwise monotonicity while maintaining other desirable properties?
- Basis in paper: [explicit] The paper concludes by stating "it is desirable to have a modified Shapley value method that is able to preserve strong pairwise monotonicity."
- Why unresolved: While the paper demonstrates that BShap fails to preserve strong pairwise monotonicity, it does not provide a concrete alternative or modification that would achieve this property.
- What evidence would resolve it: A formal mathematical proof that a modified Shapley value method preserves strong pairwise monotonicity, along with empirical validation on multiple datasets.

### Open Question 2
- Question: How do attribution methods perform when monotonicity constraints interact with other domain knowledge like fairness constraints or causal relationships?
- Basis in paper: [inferred] The paper focuses specifically on monotonicity but mentions that "it is necessary to investigate more domain knowledge" and notes the connection between monotonicity and fairness.
- Why unresolved: The paper only analyzes monotonicity preservation in isolation, without considering how attribution methods behave when multiple types of domain knowledge must be simultaneously satisfied.
- What evidence would resolve it: Empirical studies comparing attribution methods on models with multiple overlapping constraints (monotonicity + fairness + causal relationships) and theoretical analysis of their interactions.

### Open Question 3
- Question: What are the implications of using differentiable attribution methods like IG on inherently discrete features in real-world applications?
- Basis in paper: [explicit] The paper notes that "feature such as the number of defaults are naturally discrete" and that "IG can still be applied in this case using neural networks, it uses function values at points that are not useful in practice."
- Why unresolved: The paper identifies this as a limitation but does not provide guidance on how to handle this discrepancy or quantify the potential errors introduced.
- What evidence would resolve it: Comparative studies measuring attribution accuracy when applying continuous methods to discrete features versus using discrete-specific attribution methods, along with error bounds or guidelines for practitioners.

## Limitations

- The empirical validation is limited to one specific credit scoring dataset, which may not generalize to other domains or data distributions
- The paper does not address computational complexity differences between IG and BShap in large-scale applications
- Theoretical analysis relies on idealized conditions that may not hold in practice when models are trained with limited data or approximate training procedures

## Confidence

- **High Confidence**: The theoretical proofs of monotonicity axioms for IG and BShap under specific conditions
- **Medium Confidence**: The empirical validation showing IG's superiority for strong pairwise monotonicity cases, given the single dataset used
- **Low Confidence**: The generalizability of conclusions to non-credit scoring domains or different monotonic function types not tested

## Next Checks

1. Cross-dataset validation: Test the IG vs BShap comparison on at least two additional datasets from different domains (e.g., healthcare, environmental science) to verify the generalizability of findings
2. Robustness to noise: Introduce varying levels of noise to the monotonic features and evaluate how each attribution method's monotonicity preservation degrades
3. Computational efficiency analysis: Benchmark the runtime and memory requirements of IG versus BShap across models of increasing size to understand practical deployment constraints