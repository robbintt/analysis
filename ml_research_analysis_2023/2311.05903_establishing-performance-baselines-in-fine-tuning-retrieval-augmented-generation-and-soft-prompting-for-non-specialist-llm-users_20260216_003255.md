---
ver: rpa2
title: Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation
  and Soft-Prompting for Non-Specialist LLM Users
arxiv_id: '2311.05903'
source_url: https://arxiv.org/abs/2311.05903
tags:
- data
- fine-tuning
- arxiv
- block
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper establishes baseline performance metrics for three
  approaches to improving large language model outputs: fine-tuning, retrieval-augmented
  generation (RAG), and soft-prompting. The authors tested these methods on 100 questions
  about cryptocurrency events post-2021 using GPT 3.5 Turbo as the base model, with
  default settings and no iteration to simulate non-specialist usage.'
---

# Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users

## Quick Facts
- arXiv ID: 2311.05903
- Source URL: https://arxiv.org/abs/2311.05903
- Reference count: 24
- Primary result: RAG achieved 77-81% accuracy, significantly outperforming fine-tuned (38%) and unmodified models (24-37%)

## Executive Summary
This paper establishes baseline performance metrics for three approaches to improving large language model outputs: fine-tuning, retrieval-augmented generation (RAG), and soft-prompting. The authors tested these methods on 100 questions about cryptocurrency events post-2021 using GPT 3.5 Turbo as the base model, with default settings and no iteration to simulate non-specialist usage. Results showed RAG achieved 77-81% accuracy, significantly outperforming fine-tuned (38%) and unmodified models (24-37%). Soft prompting improved all approaches, with RAG gaining the most. The study found that while fine-tuning can alter model behavior, it may increase hallucinations compared to the base model, whereas RAG provides more factual responses. The authors recommend RAG as the preferred approach for non-expert users seeking improved LLM performance.

## Method Summary
The study compared three approaches to improving LLM performance using GPT 3.5 Turbo on 100 questions about LayerZero cryptocurrency project events post-2021. The researchers used default settings on commercial platforms (OpenAI fine-tuning API, KIPLEY.AI RAG platform) without iteration to simulate non-specialist usage. They tested each approach with and without soft prompts, evaluating accuracy (error-free responses), false positives (hallucinations), and false negatives (missed correct answers). The knowledge base consisted of LayerZero information vectorized for semantic search, while fine-tuning data was formatted in OpenAI's JSONL format with questions and answers.

## Key Results
- RAG achieved 77-81% accuracy, significantly outperforming fine-tuned (38%) and unmodified models (24-37%)
- Soft prompting improved the performance of all approaches, with RAG showing the greatest gains
- Fine-tuned models showed increased hallucinations (false positives) compared to both base and RAG-augmented models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG significantly outperforms fine-tuning for non-specialist users because it bypasses the need to alter model weights while still injecting task-specific knowledge.
- Mechanism: RAG combines external knowledge retrieval with LLM reasoning, allowing the model to answer questions using retrieved documents without retraining.
- Core assumption: The external knowledge source contains accurate, relevant information for the task.
- Evidence anchors:
  - [abstract] "RAG achieved 77-81% accuracy, significantly outperforming fine-tuned (38%) and unmodified models (24-37%)."
  - [section] "RAG, by contrast, does not involve altering the model weights, but rather passing in information relevant to user prompts via a multi-stage information retrieval system."
- Break condition: If the knowledge source is sparse, outdated, or noisy, RAG performance degrades sharply.

### Mechanism 2
- Claim: Soft prompting improves all approaches by steering the LLM into the correct latent vector space for the task.
- Mechanism: Adding a meta-prompt like "You are an expert financial analyst..." activates relevant knowledge and influences the model's reasoning path without retraining.
- Core assumption: The LLM's pretraining includes relevant domain knowledge that can be triggered by prompt context.
- Evidence anchors:
  - [abstract] "The application of a soft prompt significantly improved the performance of each approach."
  - [section] "prompt construction can greatly improve LLMs' ability to understand long contexts, and leverage this context to generate complex responses."
- Break condition: If the prompt is too generic or misaligned with the task, it may not improve or could even degrade performance.

### Mechanism 3
- Claim: Fine-tuning can degrade model reliability by overriding RLHF safeguards, leading to increased hallucinations.
- Mechanism: Fine-tuning updates weights to fit the training data, which may weaken the model's learned reluctance to answer uncertain questions.
- Core assumption: The fine-tuning process prioritizes fitting training examples over maintaining original safety or accuracy constraints.
- Evidence anchors:
  - [abstract] "fine-tuning can alter model behavior, it may increase hallucinations compared to the base model"
  - [section] "GPT has conducted extensive reinforcement learning via human feedback to dissuade the model from attempting to answer questions for which it has no useful information, and the fine-tuning process undermines this and alters the model's priorities."
- Break condition: If fine-tuning includes balanced, high-quality examples with explicit uncertainty handling, hallucination risk may be mitigated.

## Foundational Learning

- Concept: Vector embeddings and semantic search
  - Why needed here: RAG relies on converting documents to dense numerical representations and retrieving the most relevant ones based on semantic similarity.
  - Quick check question: How does a vector database rank documents for relevance to a user query?

- Concept: Prompt engineering and instruction tuning
  - Why needed here: Soft prompts and instructional fine-tuning are key to shaping LLM outputs without retraining.
  - Quick check question: What is the difference between a system prompt and a soft prompt?

- Concept: Model evaluation metrics (accuracy, false positives, false negatives)
  - Why needed here: The paper compares approaches using specific error types, which is essential for interpreting results.
  - Quick check question: Why might a system with fewer false positives still have lower overall accuracy?

## Architecture Onboarding

- Component map: Knowledge Base Creator Module (data ingestion, cleaning, vectorization) -> LLM Deployer Module (multi-agent retrieval, instructional prompt tuning) -> GUI-based Application Maker Module (user interface for non-experts)
- Critical path: Data → Vectorization → Retrieval → Prompt + LLM → Response
- Design tradeoffs:
  - RAG: Lower setup complexity but requires maintaining a knowledge base; fine-tuning: Higher control but more prone to overfitting and hallucinations.
  - Soft prompts: Easy to adjust but may not be sufficient for highly specialized domains.
- Failure signatures:
  - RAG: Low retrieval recall (answers miss relevant context); Fine-tuning: High hallucination rates; Soft prompts: Irrelevant or overly cautious outputs.
- First 3 experiments:
  1. Compare vanilla GPT-3.5 Turbo vs. same model with a simple soft prompt on a fixed Q&A set.
  2. Add a basic RAG pipeline (vectorized FAQ) to the same model and measure accuracy gain.
  3. Fine-tune on a small, high-quality dataset and compare hallucination rates vs. RAG.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific fine-tuning parameters and iterations would be required to achieve comparable performance to RAG in this baseline study?
- Basis in paper: [explicit] The authors note that while RAG significantly outperforms fine-tuning in their baseline study, they also acknowledge that extensive trial and error-based fine-tuning processes could potentially improve results significantly. However, they deliberately avoided iteration to maintain a non-expert user perspective.
- Why unresolved: The study deliberately used default fine-tuning settings without iteration to establish a baseline for non-expert users. This means the optimal fine-tuning parameters for this specific task remain unknown.
- What evidence would resolve it: A follow-up study testing various fine-tuning parameters (epochs, learning rates, batch sizes, etc.) through systematic iteration could determine the optimal settings for this use case.

### Open Question 2
- Question: How does the hallucination tendency in fine-tuned models compare to RAG-augmented models across different domains and types of queries?
- Basis in paper: [explicit] The study found that fine-tuned models showed significantly more hallucinations (false positives) than both the base model and RAG-augmented model. The authors suggest this may be due to fine-tuning undermining the base model's RLHF training that discourages answering questions without useful information.
- Why unresolved: The study only tested this on cryptocurrency-related questions. The hallucination patterns may differ across other domains or query types.
- What evidence would resolve it: Testing both fine-tuned and RAG-augmented models across multiple domains (medicine, law, general knowledge) and query types (factual, opinion-based, hypothetical) would reveal whether this pattern holds more broadly.

### Open Question 3
- Question: How do different soft prompt formulations impact the relative performance of fine-tuned versus RAG-augmented models?
- Basis in paper: [inferred] While the study tested one soft prompt ("You are an expert financial analyst...") and found it improved all approaches, the authors note that prompt construction can greatly influence model performance and that skillful auto-prompting could improve results.
- Why unresolved: The study used only one soft prompt formulation. Different prompts might have varying effects on fine-tuned versus RAG-augmented models.
- What evidence would resolve it: Testing multiple soft prompt formulations (varying in specificity, tone, and instruction style) across both fine-tuned and RAG-augmented models would reveal whether certain prompts favor one approach over the other.

## Limitations

- The study's narrow focus on a single cryptocurrency project (LayerZero) and time period (post-2021 events) limits generalizability to other domains.
- The evaluation relied on human assessment without inter-rater reliability measures, introducing potential subjectivity in accuracy scoring.
- Using default settings without optimization means results represent a "non-specialist" baseline rather than best-case performance for any approach.

## Confidence

**High confidence** in the comparative ranking of approaches (RAG > soft prompting > fine-tuning for accuracy) given the controlled experimental conditions and clear quantitative differences. The mechanism by which RAG achieves higher accuracy is well-established in the literature and consistently demonstrated across studies.

**Medium confidence** in the hallucination claims about fine-tuning. While the study observed increased false positives with fine-tuned models, the causal relationship between fine-tuning and hallucination rates requires more systematic investigation across different datasets and fine-tuning methodologies.

**Low confidence** in the practical significance of soft prompting improvements. The study shows soft prompting helps across all approaches, but the magnitude of improvement and its consistency across different domains or query types remains unclear without additional validation.

## Next Checks

1. **Cross-domain replication**: Test the same three approaches (RAG, fine-tuning, soft prompting) on at least two additional domains (e.g., medical information and legal compliance) to assess whether the observed performance hierarchy holds across different knowledge areas.

2. **Optimized parameter tuning**: Re-run the experiment with optimized hyperparameters for each approach rather than default settings to establish performance ceilings and better understand the tradeoffs between ease of use and maximum achievable accuracy.

3. **Hallucination attribution study**: Conduct a controlled experiment comparing fine-tuned models with different fine-tuning strategies (e.g., instruction tuning vs. continued pretraining) to isolate whether increased hallucinations are inherent to fine-tuning or specific to the methodology used.