---
ver: rpa2
title: How to Turn Your Knowledge Graph Embeddings into Generative Models
arxiv_id: '2305.15944'
source_url: https://arxiv.org/abs/2305.15944
tags:
- compl
- gekcs
- triples
- circuit
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper reinterprets the score functions of popular knowledge\
  \ graph embedding (KGE) models\u2014CP, RESCAL, TuckER, and ComplEx\u2014as computational\
  \ circuits. Two methods are proposed to convert these into generative models: constraining\
  \ parameters to be non-negative or squaring the output, yielding valid probabilistic\
  \ circuits (GeKCs)."
---

# How to Turn Your Knowledge Graph Embeddings into Generative Models

## Quick Facts
- arXiv ID: 2305.15944
- Source URL: https://arxiv.org/abs/2305.15944
- Reference count: 40
- Primary result: Reinterprets KGE models as circuits and proposes two methods to convert them into generative models (GeKCs) with exact MLE training, efficient sampling, and constraint integration.

## Executive Summary
This paper presents a novel approach to convert knowledge graph embedding (KGE) models into generative models by reinterpreting their score functions as computational circuits. The authors propose two transformation methods—non-negative parameter restrictions and squaring outputs—to create valid probabilistic circuits (GeKCs) that enable exact maximum likelihood training and efficient sampling. The framework also integrates logical constraints by design, ensuring domain-consistent predictions. Experiments demonstrate that GeKCs achieve competitive link prediction accuracy while scaling more gracefully than traditional KGEs on large graphs with millions of entities.

## Method Summary
The method involves reinterpreting popular KGE models (CP, RESCAL, TuckER, ComplEx) as structured computational graphs or circuits. Two transformation approaches are then applied: (1) constraining parameters to be non-negative, or (2) squaring the output to ensure valid probability distributions. These transformations create smooth and decomposable probabilistic circuits that enable exact maximum likelihood estimation through efficient partition function computation. The resulting GeKCs can integrate logical constraints via constraint circuits, guaranteeing predictions satisfy domain rules without additional hyperparameters.

## Key Results
- GeKCs achieve competitive link prediction performance (MRR, Hits@k) compared to original KGE models
- The non-negative variant produces more calibrated probability estimates
- GeKCs demonstrate better scalability on large knowledge graphs with millions of entities
- Exact maximum likelihood training is achieved through efficient marginalisation in polynomial time

## Why This Works (Mechanism)

### Mechanism 1
The proposed GeKCs achieve exact maximum likelihood training by converting score functions into smooth and decomposable probabilistic circuits. By reinterpreting the score functions of CP, RESCAL, TUCKER, and ComplEx as computational circuits, and then applying either non-negative parameter restrictions or squaring, the model ensures that the partition function Z can be computed in polynomial time O((|E| + |R|) · |ϕ|) instead of exponential time over all possible triples.

### Mechanism 2
GeKCs guarantee domain-consistent predictions by design through integration of logical constraints. By compiling domain constraints into constraint circuits and multiplying them with the GeKC, the resulting model assigns zero probability to any triple violating the constraints, ensuring predictions are always schema-compliant.

### Mechanism 3
GeKCs provide better scalability for large knowledge graphs compared to traditional KGE models. The memory requirements for computing the PLL objective are significantly reduced because GeKCs don't require storing large matrices for GPU parallelism, allowing larger batch sizes and better scaling with the number of entities.

## Foundational Learning

- Concept: Knowledge Graph Embeddings and Score Functions
  - Why needed here: Understanding how KGE models like CP, RESCAL, TUCKER, and ComplEx work is fundamental to grasping how they can be reinterpreted as circuits.
  - Quick check question: What is the main difference between the scoring approach of ComplEx and the other models mentioned?

- Concept: Probabilistic Circuits and Their Properties
  - Why needed here: The efficiency of GeKCs relies on the properties of smooth and decomposable circuits, particularly their ability to compute marginal probabilities efficiently.
  - Quick check question: Why is it important for a circuit to be both smooth and decomposable for efficient marginalisation?

- Concept: Logical Constraints and Circuit Compilation
  - Why needed here: The ability to integrate domain constraints into GeKCs requires understanding how logical formulas can be compiled into circuits.
  - Quick check question: What is the relationship between a constraint circuit and the logical formula it represents?

## Architecture Onboarding

- Component map: KGE score function circuit -> Transformation mechanism (non-negative or squaring) -> Constraint circuit integration module
- Critical path: (1) Computing PLL objective using efficient marginalisation, (2) Backpropagating through circuit structure, (3) Updating parameters while maintaining circuit properties
- Design tradeoffs: Non-negative restriction is simpler but potentially less expressive than squaring; integrating constraints guarantees schema compliance but adds compilation overhead
- Failure signatures: Probabilities outside [0,1], intractable marginalisation, poor link prediction compared to original KGEs, constraint violations in predictions
- First 3 experiments:
  1. Implement CP as a circuit and verify it computes the same scores as the original model for a small knowledge graph
  2. Convert the CP circuit to a GeKC via non-negative restriction and test that it can compute the partition function efficiently on a small graph
  3. Add a simple domain constraint and verify the model assigns zero probability to violating triples while maintaining reasonable link prediction performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implications arise from the work. The effectiveness of GeKCs on truly massive knowledge graphs with billions of triples remains unexplored. The trade-off between expressiveness and tractability when applying different transformation methods (non-negative vs. squaring) needs further investigation. Additionally, the framework's applicability to more complex logical constraints beyond simple domain rules is an open area for research.

## Limitations

- Limited empirical validation on extremely large knowledge graphs (billions of triples)
- The non-negative restriction may significantly limit model expressiveness compared to original KGEs
- Focus on link prediction limits understanding of GeKCs' effectiveness on other knowledge graph tasks

## Confidence

- **High confidence**: The circuit representation of CP, RESCAL, TUCKER, and ComplEx as computational graphs
- **Medium confidence**: Exact MLE training claims, given dependency on efficient marginalisation through circuit properties
- **Medium confidence**: Scalability improvements based on theoretical complexity analysis but limited empirical validation on truly massive KGs

## Next Checks

1. Benchmark GeKCs against state-of-the-art KGE models on larger, more diverse knowledge graphs (e.g., YAGO, DBpedia) to verify scalability claims
2. Conduct ablation studies comparing the non-negative and squaring transformations to quantify expressiveness trade-offs
3. Test the constraint integration mechanism on a KG with well-defined domain constraints to measure schema compliance improvements in practice