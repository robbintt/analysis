---
ver: rpa2
title: 'Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis'
arxiv_id: '2306.09417'
source_url: https://arxiv.org/abs/2306.09417
tags:
- speech
- synthesis
- motion
- gesture
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the first diffusion-based probabilistic model,
  Diff-TTSG, for joint synthesis of speech audio and 3D gesture motion from text.
  Unlike prior work, it can be trained from scratch on small datasets and uses diffusion
  probabilistic models to capture the variability of human speech and motion.
---

# Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis

## Quick Facts
- arXiv ID: 2306.09417
- Source URL: https://arxiv.org/abs/2306.09417
- Reference count: 0
- This paper proposes the first diffusion-based probabilistic model for joint synthesis of speech audio and 3D gesture motion from text, trained from scratch on small datasets.

## Executive Summary
This paper introduces Diff-TTSG, a novel diffusion-based probabilistic model for joint synthesis of speech audio and 3D gesture motion from text. Unlike prior work, Diff-TTSG can be trained from scratch on small datasets without requiring pre-training on large speech datasets or multi-stage training procedures. The method uses diffusion probabilistic models to capture the variability of spontaneous human speech and motion, achieving significant improvements over the previous state-of-the-art in naturalness ratings for both audio and gesture, as well as appropriateness of speech-gesture pairing.

## Method Summary
Diff-TTSG extends the Grad-TTS architecture with a shared text encoder and separate denoising U-Nets for speech and gesture. The system uses a Conformer pre-net to map upsampled acoustic features to pose mean predictions, then applies two U-Nets - one with 2D convolutions for speech and one with 1D convolutions for gesture - to denoise from Gaussian noise to final outputs. The model is trained jointly on multimodal data using monotonic alignment search to learn text-to-audio and text-to-motion alignments without external aligners. Diffusion steps are set to 50 for speech and 500 for motion to reflect different convergence rates.

## Key Results
- Diff-TTSG achieves higher naturalness ratings for speech and gesture compared to previous state-of-the-art methods
- The model produces more appropriate linking between speech and gesture modalities
- Joint training of speech and gesture components yields better gesture quality than sequential training approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diff-TTSG jointly learns speech and gesture by using a shared text encoder and separate denoising U-Nets, allowing both modalities to be trained from scratch on small datasets.
- Mechanism: The shared text encoder produces acoustic and pose mean predictions (μ₁:ₚ), which are upsampled via a duration predictor to create rough approximations (μ₁:ₜ). Two U-Nets—one with 2D convolutions for speech and one with 1D convolutions for gesture—denoise from N(μ′₁:ₜ, I) to generate final outputs.
- Core assumption: Acoustic and pose feature spaces can be modelled with different U-Net architectures while sharing the same text conditioning.
- Evidence anchors: [abstract] "first diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to synthesise speech and gestures together"; [section 3.2] "we made a number of changes to the Grad-TTS architecture, resulting in the architecture illustrated in Fig. 1"

### Mechanism 2
- Claim: Using diffusion probabilistic models instead of deterministic models captures the variability of spontaneous human speech and motion, avoiding oversmoothing artefacts.
- Mechanism: Diffusion models learn the full probability distribution of data by gradually denoising from Gaussian noise, enabling stochastic sampling that reflects natural variability.
- Core assumption: The probability distribution of speech and gesture in spontaneous communication is complex enough to benefit from a full generative model rather than a point estimate.
- Evidence anchors: [abstract] "diffusion probabilistic models to capture the variability of human speech and motion"; [section 1] "approaches that learn to solve both problems simultaneously are still in their infancy"

### Mechanism 3
- Claim: Replacing 2D convolutions with 1D convolutions for gesture synthesis avoids producing unrealistic "T-pose" artefacts that occur when treating pose vectors as images.
- Mechanism: 1D convolutions respect the sequential nature of pose features without imposing spatial relationships that don't exist between pose vector dimensions.
- Core assumption: Pose vectors have no inherent spatial structure that would benefit from 2D convolution operations.
- Evidence anchors: [section 3.2] "we replaced all 2D convolutions with 1D convolutions along the time dimension t"; [section 3.2] "Changing to 1D convolutions also means that the dimensionality of gt no longer has to be evenly divisible by four"

## Foundational Learning

- Concept: Diffusion probabilistic models
  - Why needed here: They enable learning the full probability distribution of spontaneous speech and gesture data, capturing variability that deterministic models miss
  - Quick check question: What is the key difference between diffusion models and traditional autoregressive models in terms of what they learn?

- Concept: Monotonic alignment search
  - Why needed here: It learns the alignment between text and both speech and gesture during training without requiring external aligners
  - Quick check question: How does monotonic alignment search differ from standard attention mechanisms in TTS systems?

- Concept: Conformer architecture
  - Why needed here: It maps the upsampled acoustic features to pose mean predictions, enabling the separate gesture U-Net to generate motion conditioned on text
  - Quick check question: What specific architectural feature of Conformer makes it suitable for this pre-net role compared to a standard transformer?

## Architecture Onboarding

- Component map: Text → Encoder → Duration Predictor → Upsampling → [Conformer Pre-net] → Separate U-Nets → [HiFi-GAN] → Audio/Video output
- Critical path: Text → Encoder → Duration Predictor → Upsampling → [Conformer Pre-net] → Separate U-Nets → [HiFi-GAN] → Audio/Video output
- Design tradeoffs:
  - Joint vs. sequential training: Joint training (proposed) produces better gesture quality than freezing TTS weights first
  - 2D vs. 1D convolutions: 1D for gestures avoids T-pose artefacts but may limit temporal modeling
  - Diffusion steps: 50 for speech, 500 for motion reflects different convergence rates
- Failure signatures:
  - Jittery gestures: Likely issues with 1D convolution receptive fields or training instability
  - Unnatural speech: May indicate diffusion step issues or conditioning problems
  - Poor cross-modal appropriateness: Could indicate misalignment between text conditioning and output modalities
- First 3 experiments:
  1. Train only the speech U-Net (remove gesture path) and verify it matches Grad-TTS performance
  2. Train only the gesture U-Net with random noise input to verify it can generate plausible motion
  3. Joint training with reduced dataset to verify the "trained from scratch on small datasets" claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Diff-TTSG's gesture quality compare to models trained on larger datasets?
- Basis in paper: [explicit] The paper notes that Diff-TTSG can be trained on small datasets from scratch, removing the need for pre-training on large speech datasets.
- Why unresolved: The paper does not directly compare Diff-TTSG's gesture quality to models trained on larger datasets, so it's unclear if the smaller dataset approach impacts gesture quality.
- What evidence would resolve it: Direct comparison of gesture quality between Diff-TTSG and models trained on larger datasets using the same evaluation metrics.

### Open Question 2
- Question: What is the impact of using different motion representations (e.g., Euler angles vs. exponential map) on the quality of generated gestures?
- Basis in paper: [explicit] The paper mentions that rotations can be parameterized using different methods like Euler angles or the exponential map, but does not explore the impact of these choices.
- Why unresolved: The paper uses a specific motion representation but does not investigate how alternative representations might affect the quality of generated gestures.
- What evidence would resolve it: Comparative study of gesture quality using different motion representations with the same model architecture and training setup.

### Open Question 3
- Question: How does the quality of Diff-TTSG's synthesized speech and gestures vary with different levels of spontaneous speech in the training data?
- Basis in paper: [inferred] The paper evaluates on spontaneous speech data and notes that spontaneous speech synthesis is more challenging than read-aloud speech synthesis.
- Why unresolved: The paper does not explore how the level of spontaneity in the training data affects the quality of the synthesized speech and gestures.
- What evidence would resolve it: Experiments varying the spontaneity level of the training data and evaluating the impact on synthesis quality using the same metrics.

## Limitations

- The claims about training from scratch on small datasets lack definition of what constitutes "small" and minimum viable dataset size
- Architectural details for the Conformer pre-net and 1D U-Net are underspecified, making exact reproduction difficult
- Evaluation relies entirely on subjective ratings without objective metrics or statistical significance tests

## Confidence

**High Confidence**: The core technical contribution of using diffusion models for joint speech and gesture synthesis is novel and well-justified by the limitations of deterministic approaches in capturing natural variability.

**Medium Confidence**: The architectural claims about 1D convolutions preventing T-pose artefacts and joint training improving gesture quality are plausible but lack detailed ablation studies or quantitative evidence to support the specific design choices.

**Low Confidence**: The claims about eliminating the need for pre-training and achieving state-of-the-art results with minimal data are difficult to verify without access to the exact implementation details and more rigorous statistical validation of the subjective ratings.

## Next Checks

1. **Ablation Study on Training Regimes**: Conduct experiments comparing joint training versus sequential training where TTS weights are frozen after pre-training, measuring both gesture quality and cross-modal appropriateness to validate the claim that joint training is superior.

2. **Dataset Size Scaling Analysis**: Systematically train Diff-TTSG on subsets of TSGD2 ranging from 1 hour to 6 hours of data, evaluating whether the claimed ability to train from scratch on small datasets holds across different data scales and identifying the minimum viable dataset size.

3. **Objective Metric Implementation**: Implement and report objective metrics such as Frechet Gesture Distance (FGD) for gesture quality, alignment error rates between speech and gesture, and compare these against the subjective ratings to provide statistical validation of the claimed improvements.