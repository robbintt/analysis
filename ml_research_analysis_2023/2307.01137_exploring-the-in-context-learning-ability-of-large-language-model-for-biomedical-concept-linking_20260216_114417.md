---
ver: rpa2
title: Exploring the In-context Learning Ability of Large Language Model for Biomedical
  Concept Linking
arxiv_id: '2307.01137'
source_url: https://arxiv.org/abs/2307.01137
tags:
- biomedical
- concept
- linking
- language
- ontology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a zero-training framework for biomedical
  concept linking that leverages large language models' in-context learning capabilities.
  The method uses a two-stage retrieve-and-rank approach, first retrieving candidate
  concepts via semantic embeddings, then re-ranking them using LLMs with rich contextual
  prompts.
---

# Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking

## Quick Facts
- arXiv ID: 2307.01137
- Source URL: https://arxiv.org/abs/2307.01137
- Reference count: 15
- Primary result: Zero-training biomedical concept linking achieves 90.1% disease and 94.7% chemical entity normalization accuracy using in-context learning

## Executive Summary
This paper introduces a zero-training framework for biomedical concept linking that leverages large language models' in-context learning capabilities. The method uses a two-stage retrieve-and-rank approach, first retrieving candidate concepts via semantic embeddings, then re-ranking them using LLMs with rich contextual prompts. Experiments on BC5CDR entity normalization achieve 90.1% (disease) and 94.7% (chemical) accuracy, while ontology matching tasks show over 20 F1 score improvement. Extensive qualitative analysis demonstrates LLMs' effectiveness in handling complex biomedical concept relationships and providing interpretable reasoning. The approach offers a flexible, training-free solution adaptable to diverse biomedical linking tasks.

## Method Summary
The framework employs a two-stage retrieve-and-rank approach for biomedical concept linking. First, biomedical concepts are embedded using language models (SapBERT, LLaMa, GPT-3 embeddings) and retrieved based on cosine similarity to generate candidate concepts. Second, the top candidates are re-ranked using large language models (GPT-3.5, GPT-4, LLaMa-65b) through contextual prompts that include candidate descriptions and source entity context. This zero-training approach eliminates the need for labeled data while achieving state-of-the-art performance on BC5CDR entity normalization and ontology matching tasks.

## Key Results
- BC5CDR entity normalization: 90.1% accuracy for disease linking, 94.7% accuracy for chemical linking
- Ontology matching: Over 20 F1 score improvement compared to baseline methods
- Cross-ontology matching: Successfully links concepts between SNOMED-CT and NCIT ontologies
- Interpretability: LLMs provide reasoning paths that can be evaluated for correctness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In-context learning enables zero-training biomedical concept linking by providing structured contextual prompts that guide LLMs to perform accurate matching.
- **Mechanism:** The method uses a two-stage retrieve-and-rank approach where semantic embeddings first retrieve candidate concepts, then LLMs re-rank them using rich contextual prompts containing entity descriptions and relationships.
- **Core assumption:** LLMs can effectively reason about biomedical concepts when provided with sufficient contextual information and candidate options.
- **Evidence anchors:**
  - [abstract] "This approach adopted a two-stage retrieve-and-rank framework. Initially, biomedical concepts are embedded using language models, and then embedding similarity is utilized to retrieve the top candidates. These candidates' contextual information is subsequently incorporated into the prompt and processed by a large language model to re-rank the concepts."
  - [section] "When constructing the prompt, we initially define the task and inform the model that our aim is to identify analogous concepts. We then present the candidate concepts retrieved from long-term memory. These candidates are options within the prompt. Further, we fetch the descriptions of these candidates from the ontology and associated text of the source entity."
- **Break condition:** If contextual information is insufficient or candidates are poorly retrieved, LLM reasoning degrades significantly (as shown in ablation tests where no context dropped F1 from 0.882 to 0.698).

### Mechanism 2
- **Claim:** Embedding quality directly impacts recall performance in the candidate generation stage.
- **Mechanism:** Domain-specific embeddings (SapBERT) that include contextual information significantly improve candidate retrieval compared to generic or context-free embeddings.
- **Core assumption:** Biomedical concepts require domain-specific semantic understanding that general embeddings cannot capture.
- **Evidence anchors:**
  - [abstract] "The first stage embeds biomedical concepts using language models and uses these embedding to retrieve top candidate concepts."
  - [section] "SapBERT employs contrastive learning for its embedding model, and it's plausible that OpenAI employs a similar method for their models."
- **Break condition:** When using non-domain-specific embeddings like generic LLaMa, performance drops significantly (from 0.947 accuracy to 0.728 for chemical entity linking).

### Mechanism 3
- **Claim:** Process correctness is as important as prediction accuracy for trustworthy biomedical AI.
- **Mechanism:** LLMs provide interpretable reasoning paths that can be evaluated for correctness, enabling trust in the linking decisions.
- **Core assumption:** In biomedical applications, understanding why a model made a decision is as important as the decision itself.
- **Evidence anchors:**
  - [abstract] "Extensive qualitative assessments were conducted, and the benefits and potential shortcomings of using large language models within the biomedical domain were discussed."
  - [section] "We noticed from the LLama results that there are instances where the process was incorrect, yet the final answer was right. For example, in case 9 from the appendix, LLama provided the correct prediction, yet the reasoning appeared to be based on shared keywords between disease concept names."
- **Break condition:** When LLM reasoning is based on superficial features (like shared keywords) rather than semantic understanding, the system becomes unreliable for complex biomedical concepts.

## Foundational Learning

- **Concept: In-context learning**
  - Why needed here: Enables zero-training adaptation to new biomedical tasks without requiring labeled data or fine-tuning.
  - Quick check question: How does in-context learning differ from traditional supervised learning in terms of model updates and training requirements?

- **Concept: Semantic embeddings**
  - Why needed here: Provides the initial candidate retrieval mechanism that narrows down potential matches before LLM re-ranking.
  - Quick check question: Why might domain-specific embeddings (like SapBERT) outperform general embeddings for biomedical concept retrieval?

- **Concept: Biomedical ontologies**
  - Why needed here: Provides the structured knowledge base that biomedical concepts are linked to, including relationships and descriptions.
  - Quick check question: What challenges arise when linking concepts across different biomedical ontologies like OMIM and ORDO?

## Architecture Onboarding

- **Component map:** Embedding generator → Vector database (long-term memory) → Candidate retriever → Prompt constructor → LLM ranker → Result selector
- **Critical path:** Query text → Embedding generation → Candidate retrieval (top-k) → Prompt construction with context → LLM re-ranking → Final selection
- **Design tradeoffs:** Speed vs accuracy (faster embeddings sacrifice domain specificity), cost vs performance (GPT-4 vs LLaMa), prompt complexity vs token limits
- **Failure signatures:** Poor candidate retrieval (hits@10 < 70%), LLM choosing "None" option frequently, inconsistent reasoning across similar examples, performance degradation with rare concepts
- **First 3 experiments:**
  1. Compare retrieval quality (hits@10) using different embedding methods (SapBERT vs LLaMa vs GPT-3) on OMIM-ORDO dataset
  2. Test impact of adding context information to embeddings by comparing "name-only" vs "name+context" representations
  3. Evaluate different prompt formats (one-shot vs no example vs full context) on rare disease matching accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does data leakage from pre-training affect the performance of large language models on biomedical concept linking tasks?
- Basis in paper: explicit
- Why unresolved: The paper notes that LLMs are trained on vast amounts of text data including published papers and webpages, making data leakage inevitable for many existing datasets. However, estimating the impact of this leakage on performance is not straightforward.
- What evidence would resolve it: Controlled experiments comparing LLM performance on datasets with and without potential pre-training data overlap, or systematic analysis of model predictions for concepts likely present in pre-training data versus novel concepts.

### Open Question 2
- Question: What is the optimal balance between using context information versus one-shot examples in prompts for biomedical concept linking?
- Basis in paper: explicit
- Why unresolved: The ablation test showed that OMIM context information provided a larger performance increase than one-shot learning without context, but the marginal improvement when combining both suggests a complex relationship that wasn't fully explored.
- What evidence would resolve it: Systematic experiments varying the amount and type of context versus examples in prompts across multiple biomedical datasets to identify the optimal configuration for different concept linking tasks.

### Open Question 3
- Question: How can the slow inference speed and high computational cost of large language models be addressed for practical biomedical concept linking applications?
- Basis in paper: explicit
- Why unresolved: The paper acknowledges that LLM inference is "exceedingly slow" and "expensive," with specific examples of high costs and slow processing times, but doesn't propose concrete solutions beyond mentioning future research directions.
- What evidence would resolve it: Development and evaluation of efficient fine-tuned models specifically designed for biomedical concept linking, or successful implementation of model compression techniques that maintain performance while significantly improving inference speed.

## Limitations
- Performance highly sensitive to LLM choice, with LLaMa showing significant degradation compared to GPT models
- High computational cost and slow inference speed make the approach expensive for production use
- Limited evaluation on extremely rare biomedical concepts and complex subtype relationships

## Confidence
- **High confidence**: The two-stage retrieve-and-rank framework is well-established and the retrieval component using domain-specific embeddings (SapBERT) shows consistent improvements.
- **Medium confidence**: The LLM re-ranking component shows strong performance with GPT models, but the significant performance drop with LLaMa suggests sensitivity to model choice and prompt quality.
- **Low confidence**: Claims about interpretable reasoning and process correctness are supported by qualitative examples but lack systematic evaluation across diverse biomedical scenarios.

## Next Checks
1. **Cross-model consistency test**: Run identical prompts across multiple LLM variants (GPT-3.5, GPT-4, LLaMa) on the same ontology matching dataset and compare both final predictions and intermediate reasoning quality to identify systematic biases.

2. **Rare concept robustness evaluation**: Create a stratified test set focusing on rare diseases, uncommon abbreviations, and complex subtype relationships, then measure whether performance degrades more sharply than on common concepts.

3. **Process correctness audit**: Implement a rubric to evaluate LLM reasoning paths for semantic validity (not just keyword matching) and measure correlation between reasoning quality scores and final prediction accuracy across diverse biomedical linking scenarios.