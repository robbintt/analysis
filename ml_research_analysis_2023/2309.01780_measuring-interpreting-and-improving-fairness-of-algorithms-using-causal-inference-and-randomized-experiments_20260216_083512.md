---
ver: rpa2
title: Measuring, Interpreting, and Improving Fairness of Algorithms using Causal
  Inference and Randomized Experiments
arxiv_id: '2309.01780'
source_url: https://arxiv.org/abs/2309.01780
tags:
- fairness
- treatment
- which
- dataset
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MIIF, a framework to measure, interpret, and
  improve algorithmic fairness using causal inference and randomized experiments.
  It measures treatment and outcome fairness, along with economic value, using randomized
  experiments.
---

# Measuring, Interpreting, and Improving Fairness of Algorithms using Causal Inference and Randomized Experiments

## Quick Facts
- arXiv ID: 2309.01780
- Source URL: https://arxiv.org/abs/2309.01780
- Reference count: 40
- The paper introduces MIIF, a framework to measure, interpret, and improve algorithmic fairness using causal inference and randomized experiments.

## Executive Summary
This paper presents the MIIF framework for addressing algorithmic fairness through a combination of causal inference and randomized experiments. The framework measures treatment and outcome fairness along with economic value, interprets blackbox models using feature interaction detection and generalized additive models (GAMs), and provides two improvement techniques: multiple thresholds and sensitive feature functions. Experiments on four datasets demonstrate the framework's ability to accurately fit data, improve fairness metrics, and provide insights into the tradeoffs between fairness and economic benefit through the introduction of the "no-worse-off" metric.

## Method Summary
The MIIF framework operates in three phases: measurement, interpretation, and improvement. It uses randomized experiments to measure treatment fairness (statistical parity), outcome fairness (predictive parity), and economic value without structural causal assumptions. For interpretation, it leverages Archipelago for feature interaction detection combined with GAMs to create interpretable models that approximate blackbox algorithms. The improvement phase applies either multiple thresholds or sensitive feature functions to enhance fairness. The framework introduces a "no-worse-off" metric to benchmark fairness improvements against a baseline, balancing fairness gains with economic considerations.

## Key Results
- MIIF accurately fits data and improves fairness metrics (TF, OF) while maintaining economic value
- GAM2 with top K feature interactions detected by Archipelago represents 98.5% of DNN prediction variance
- The "no-worse-off" metric enables benchmarking fairness improvements against baseline performance
- Experiments on college admissions, synthetic causal, blood donation, and collage referral datasets validate the framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Randomized experiments allow simultaneous measurement of disparate treatment, disparate impact, and economic value without structural causal assumptions.
- Mechanism: By randomly assigning treatments, the framework can measure causal effects on outcomes for different sensitive groups, isolating algorithmic bias from dataset bias.
- Core assumption: Randomized experiments are feasible and provide unbiased estimates of causal effects.
- Evidence anchors:
  - [abstract]: "We measure the algorithm bias using randomized experiments, which enables the simultaneous measurement of disparate treatment, disparate impact, and economic value."
  - [section 2.3]: "Randomized experiments or clinical trials refer to the scenario where each member of the initial population was assigned a truly random treatment, allowing us to unambiguously identify the effect of the treatment."
  - [corpus]: Weak evidence - corpus contains related work on fairness measurement but lacks direct evidence for simultaneous measurement of all three aspects.
- Break condition: If randomized experiments are not feasible due to ethical or practical constraints, this mechanism would break down.

### Mechanism 2
- Claim: Feature interaction detection combined with generalized additive models (GAM) provides an interpretable model that accurately approximates blackbox models.
- Mechanism: Archipelago detects important feature interactions, which are then incorporated into GAMs to create interpretable models that capture the decision logic of blackbox models.
- Core assumption: The blackbox model's predictions can be accurately approximated by a GAM with detected feature interactions.
- Evidence anchors:
  - [section 3.2]: "We leverage a novel interaction detection procedure [58] to rank all possible feature pairs, including only those which are ranked highest. This allows us to drastically cut down on our distillation data requirements while also obtaining highly flexible modeling capacity."
  - [section 5.2]: "Using our learned interactions, however, the GAM2 can represent 98.5% of the DNN's prediction variance."
  - [corpus]: Weak evidence - corpus contains related work on interpretable machine learning but lacks direct evidence for this specific combination of feature interaction detection and GAMs.
- Break condition: If the blackbox model's decision logic is too complex to be captured by GAMs with detected interactions, this mechanism would break down.

### Mechanism 3
- Claim: The "no-worse-off" metric provides a causal-aware fairness benchmark that balances fairness and economic value.
- Mechanism: By comparing outcomes against a baseline treatment (e.g., no algorithm), the metric allows for a fair comparison that accounts for the natural state of the world.
- Core assumption: The baseline treatment represents a fair comparison point that captures the natural state of the world.
- Evidence anchors:
  - [section 3.4]: "We define No-Worse-Off for Outcome Fairness as follows: ð‘ð‘Š ð‘‚(ð·ðµ, ð·ð´) = min{ ð‘‚ð¹ (ð·ðµ) / ð‘‚ð¹ (ð·ð´), 100% }"
  - [section 2.2]: "Accordingly, comparing any fairness metric against the value achieved in such a counterfactual, baseline world allows for additional flexibility which is not plausible in a purely observational setting."
  - [corpus]: Weak evidence - corpus contains related work on fairness metrics but lacks direct evidence for this specific metric.
- Break condition: If the baseline treatment does not accurately represent the natural state of the world, this mechanism would break down.

## Foundational Learning

- Concept: Causal inference and the Neyman-Rubin causal model
  - Why needed here: Understanding how treatments affect outcomes is crucial for measuring algorithmic fairness.
  - Quick check question: What is the fundamental problem of causal inference, and how does the Neyman-Rubin model address it?

- Concept: Feature interactions and their detection
  - Why needed here: Identifying which features interact is key to building interpretable models that approximate blackbox algorithms.
  - Quick check question: How does the Archipelago method approximate the Hessian of a function to detect feature interactions?

- Concept: Generalized additive models (GAMs) and their extension to include interactions
  - Why needed here: GAMs provide a balance between interpretability and modeling capacity, especially when extended to include feature interactions.
  - Quick check question: How does the GAM2 model extend the original GAM formulation to include feature interactions?

## Architecture Onboarding

- Component map:
  Randomized experiments -> Blackbox algorithm -> Feature interaction detection (Archipelago) -> GAMs -> Fairness metrics (TF, OF, NWO) -> Improvement techniques (multiple thresholds, sensitive feature functions)

- Critical path:
  1. Collect randomized experiment data
  2. Train blackbox algorithm on the data
  3. Use Archipelago to detect feature interactions
  4. Train GAMs with detected interactions to approximate blackbox algorithm
  5. Evaluate fairness using treatment fairness, outcome fairness, and no-worse-off metrics
  6. Improve fairness using multiple thresholds or sensitive feature functions
  7. Repeat from step 2 until desired fairness levels are achieved

- Design tradeoffs:
  - Randomized experiments vs. observational data: Randomized experiments provide unbiased estimates but may be costly or impractical in some settings.
  - GAM complexity vs. interpretability: More complex GAMs (with more interactions) can better approximate blackbox algorithms but may sacrifice interpretability.
  - Fairness vs. economic value: Improving fairness may come at the cost of economic value, and the no-worse-off metric helps balance these tradeoffs.

- Failure signatures:
  - If the blackbox algorithm cannot be accurately approximated by the GAMs, the framework may fail to provide meaningful insights into the algorithm's decision-making process.
  - If the randomized experiments are not properly designed or executed, the causal effects measured may be biased or unreliable.
  - If the no-worse-off metric is not appropriate for the specific application or context, it may not provide a fair comparison of algorithms.

- First 3 experiments:
  1. Implement the feature interaction detection using Archipelago on a small dataset to verify that it correctly identifies known interactions.
  2. Train a simple GAM with detected interactions on a small dataset and compare its performance to a blackbox algorithm to verify that it can accurately approximate the blackbox model.
  3. Implement the no-worse-off metric and apply it to a small dataset to verify that it provides a meaningful comparison of algorithms in terms of both fairness and economic value.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided. The open questions section from the input appears to be additional analysis rather than questions raised by the paper itself.

## Limitations
- Experimental design feasibility: The paper relies heavily on randomized experiments but does not adequately address when such experiments are feasible, particularly for sensitive applications like college admissions or credit lending.
- Synthetic data generation details: The exact data generation process and structural equations for synthetic datasets are underspecified, making it difficult to verify real-world applicability.
- Implementation complexity: The framework requires implementing multiple sophisticated components including Archipelago, GAMs with interactions, and T-learner models, but lacks detailed implementation guidance.

## Confidence
**High confidence**: The theoretical foundation connecting causal inference to algorithmic fairness measurement is well-established.
**Medium confidence**: The effectiveness of feature interaction detection and GAMs for interpreting blackbox models is demonstrated on synthetic data but needs more real-world validation.
**Low confidence**: The "no-worse-off" metric's practical utility in real-world fairness interventions is not well-established and requires more thorough investigation.

## Next Checks
1. Replicate Archipelago feature interaction detection on a well-understood dataset with known interactions to verify the method correctly identifies feature relationships and assess computational efficiency for larger feature spaces.

2. Test framework with observational data where randomization is infeasible to evaluate how well the approach performs when relying on observational causal inference methods versus the ideal randomized experiment scenario.

3. Conduct user study on interpretability to determine whether the GAM models with detected interactions actually help practitioners understand and debug algorithmic decisions, or if the added complexity reduces transparency.