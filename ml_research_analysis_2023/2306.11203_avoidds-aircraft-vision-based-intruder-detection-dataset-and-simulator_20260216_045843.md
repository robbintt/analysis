---
ver: rpa2
title: 'AVOIDDS: Aircraft Vision-based Intruder Detection Dataset and Simulator'
arxiv_id: '2306.11203'
source_url: https://arxiv.org/abs/2306.11203
tags:
- aircraft
- evaluation
- dataset
- detection
- intruder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The AVOIDDS benchmark provides a comprehensive dataset and evaluation
  framework for vision-based aircraft detection systems, addressing the need for robust
  machine learning systems in safety-critical applications. The dataset contains 72,000
  photorealistic images of intruder aircraft with varying environmental conditions,
  including weather, lighting, and geographic locations.
---

# AVOIDDS: Aircraft Vision-based Intruder Detection Dataset and Simulator

## Quick Facts
- arXiv ID: 2306.11203
- Source URL: https://arxiv.org/abs/2306.11203
- Reference count: 40
- Primary result: Benchmark provides 72,000 photorealistic images with metadata for evaluating vision-based aircraft detection systems

## Executive Summary
The AVOIDDS benchmark addresses the need for robust evaluation frameworks for vision-based aircraft detection systems in detect-and-avoid (DAA) applications. The dataset contains 72,000 images generated from X-Plane 11 simulator, featuring varied environmental conditions including 6 weather types, 4 geographic regions, 3 aircraft types, and 4 time-of-day windows. Each image includes metadata annotations describing environmental conditions. The benchmark evaluates models using both traditional test set metrics (precision, recall, mAP) and closed-loop simulation that measures real-world task performance through NMAC and alert frequencies.

## Method Summary
AVOIDDS uses X-Plane 11 flight simulator to generate photorealistic images of ownship aircraft encountering intruder aircraft under varied environmental conditions. The dataset follows YOLO format with additional metadata annotations for environmental factors. A baseline YOLOv8s model is trained for 100 epochs with default hyperparameters. Evaluation consists of two components: standard test set evaluation using precision, recall, and mAP metrics across condition slices, and closed-loop simulation using an encounter model and collision avoidance controller to measure task-specific performance through NMAC frequency and alert frequency metrics.

## Key Results
- Test set evaluation shows high performance across conditions with mAP 0.866
- Closed-loop simulation reveals task-specific challenges not visible in isolated test metrics
- Performance varies across environmental conditions, highlighting distribution shift effects
- Baseline YOLOv8 model demonstrates the benchmark's utility for evaluating vision-based detection systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset's distribution shift coverage enables robust model evaluation across environmental conditions.
- Mechanism: By randomizing images across 6 weather types, 4 geographic regions, 3 aircraft types, and 4 time-of-day windows, the benchmark captures systematic performance degradation patterns that isolated test metrics would miss.
- Core assumption: The randomization covers the true deployment space of aircraft vision systems without leaving significant gaps.
- Evidence anchors:
  - [abstract] "comprehensive dataset and evaluation framework for vision-based aircraft detection systems"
  - [section] "The A VOIDDS dataset is a collection of 72,000 images and labels from the ownship's point of view of encounters with intruder aircraft in the airspace."
- Break condition: If real-world deployment conditions include variations not represented in the synthetic dataset (e.g., novel weather phenomena or aircraft types), the benchmark's coverage assumption fails.

### Mechanism 2
- Claim: Closed-loop simulation reveals task-specific failures invisible to standard detection metrics.
- Mechanism: The simulator evaluates detection models not just on isolated image performance but on their downstream impact on collision avoidance decisions, measuring NMAC frequency and alert frequency.
- Core assumption: The encounter model accurately represents realistic airspace scenarios where vision-based detection errors propagate to safety-critical outcomes.
- Evidence anchors:
  - [abstract] "closed-loop simulation reveals task-specific challenges, emphasizing the importance of comprehensive evaluation"
  - [section] "By simulating a full set of encounters and determining the number of encounters that resulted in a near mid-air collision (NMAC), we can evaluate the performance of the detection model with respect to the full closed-loop system."
- Break condition: If the encounter model oversimplifies real-world dynamics or the controller logic doesn't capture realistic pilot/aircraft behavior, the simulation results may not generalize.

### Mechanism 3
- Claim: Metadata annotations enable targeted performance analysis across semantic concept groups.
- Mechanism: Each image includes metadata about environmental conditions, allowing evaluation of model performance on specific slices (e.g., "clear weather in Palo Alto at midday") to identify where the model fails.
- Core assumption: The metadata accurately captures the semantic factors that influence detection performance.
- Evidence anchors:
  - [abstract] "annotated with metadata that describes the environmental conditions under which each image was captured"
  - [section] "Using this metadata, we can choose to evaluate a model on images from a particular location or time of day or even images with the intruder in specific orientations relative to the ownship."
- Break condition: If metadata annotations miss important factors (e.g., sun angle relative to aircraft orientation) or contain errors, targeted analysis becomes unreliable.

## Foundational Learning

- Concept: Object detection evaluation metrics (precision, recall, mAP)
  - Why needed here: These metrics provide the baseline performance assessment for vision-based detection models before considering closed-loop performance.
  - Quick check question: If a model has precision 0.99 and recall 0.92, what does this tell you about its false positive and false negative rates?

- Concept: Distribution shift and domain generalization
  - Why needed here: Understanding how models perform when test data differs from training data is crucial for evaluating robustness across the environmental variations in AVOIDDS.
  - Quick check question: If a model trained only on clear weather images achieves mAP 0.9 on clear images but only 0.6 on overcast images, what type of distribution shift is occurring?

- Concept: Closed-loop simulation and system integration
  - Why needed here: The benchmark evaluates not just detection accuracy but how detection errors propagate through the collision avoidance system to affect safety outcomes.
  - Quick check question: Why might a model with high mAP still result in high NMAC frequency in the closed-loop simulator?

## Architecture Onboarding

- Component map: Data generation (X-Plane 11 integration) -> Dataset storage (YOLO format with metadata) -> Baseline model training (YOLOv8 architecture) -> Test set evaluation (precision/recall/mAP calculation) -> Closed-loop simulator (encounter generation + perception + controller) -> Evaluation interface (task-specific metrics)

- Critical path: 1. Generate or download dataset with metadata 2. Train baseline detection model 3. Evaluate on test set slices 4. Run closed-loop simulation with encounter model 5. Analyze safety vs efficiency tradeoffs

- Design tradeoffs:
  - Synthetic vs real data: Synthetic data allows controlled variation but may miss real-world edge cases
  - Closed-loop complexity vs evaluation speed: Full simulation is computationally expensive but reveals task-specific failures
  - Metadata granularity vs annotation burden: More detailed metadata enables better analysis but increases generation complexity

- Failure signatures:
  - High mAP but high NMAC frequency: Detection model misses critical cases that matter for safety
  - Poor performance on specific weather conditions: Model overfits to training conditions
  - Performance degradation with distance: Detection range limitations not captured in mAP

- First 3 experiments:
  1. Evaluate baseline model on test set across all condition slices to identify worst-performing categories
  2. Run closed-loop simulation on a subset of encounters (one weather type) to compare test vs task performance
  3. Train alternative model on reduced dataset (clear weather only) and compare both test set and closed-loop results to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of vision-based aircraft detection models degrade when tested on real-world images compared to the photorealistic X-Plane 11 simulated images used in AVOIDDS?
- Basis in paper: [inferred] The paper uses X-Plane 11 for data generation and acknowledges the "sim-to-real gap should be further investigated" but doesn't provide empirical data on real-world performance.
- Why unresolved: The dataset and evaluation framework are based entirely on simulated imagery, leaving the generalization to real-world conditions untested.
- What evidence would resolve it: Comparative testing of models trained on AVOIDDS data against both X-Plane simulated test data and real-world aircraft imagery, measuring performance differences.

### Open Question 2
- Question: What is the impact of incorporating state uncertainty into the controller design on the overall collision avoidance performance?
- Basis in paper: [explicit] The paper mentions "Future work could also explore accounting for state uncertainty in the controller to improve overall performance."
- Why unresolved: The current evaluation uses a deterministic controller that assumes perfect state estimates, which may not reflect real-world uncertainty.
- What evidence would resolve it: Experiments comparing collision avoidance performance using controllers with and without state uncertainty modeling, using metrics like NMAC frequency and alert frequency.

### Open Question 3
- Question: How does the performance of vision-based detection models vary across different weather conditions, and which specific weather factors most significantly impact detection accuracy?
- Basis in paper: [explicit] The dataset includes various weather conditions (clear, high cirrus, scattered, broken, overcast, stratus), and test results show performance differences across these conditions.
- Why unresolved: While the paper shows that weather affects performance, it doesn't analyze which specific weather factors (e.g., cloud cover percentage, precipitation) are most critical.
- What evidence would resolve it: Detailed analysis correlating detection performance with specific weather parameters (cloud density, visibility, precipitation) to identify the most significant factors.

## Limitations
- Synthetic data may not capture all real-world edge cases and distribution gaps
- Closed-loop simulation relies on encounter model that may oversimplify real airspace dynamics
- Baseline model trained with default hyperparameters, optimal training procedures unexplored
- Sim-to-real gap not empirically validated with real-world imagery

## Confidence
- High Confidence: The dataset's technical specifications (72,000 images, YOLO format, metadata structure) and baseline evaluation methodology are clearly specified and reproducible.
- Medium Confidence: The claim that closed-loop simulation reveals task-specific challenges is well-supported by the methodology, though real-world validation would strengthen this claim.
- Low Confidence: Generalization of benchmark results to actual aviation safety systems requires field testing beyond the synthetic environment.

## Next Checks
1. Evaluate a model trained only on clear weather conditions against the full AVOIDDS test set to quantify distribution shift impact on detection performance.
2. Run closed-loop simulation with the same detection model across multiple encounter scenarios (different weather, aircraft types) to assess consistency of safety metrics.
3. Compare baseline model performance against a human-in-the-loop study where pilots evaluate the same encounter scenarios to validate the simulation's relevance to real-world decision-making.