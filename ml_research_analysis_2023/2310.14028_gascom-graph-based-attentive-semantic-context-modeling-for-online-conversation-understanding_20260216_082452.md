---
ver: rpa2
title: 'GASCOM: Graph-based Attentive Semantic Context Modeling for Online Conversation
  Understanding'
arxiv_id: '2310.14028'
source_url: https://arxiv.org/abs/2310.14028
tags:
- context
- walk
- graph
- conversation
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses online conversation understanding, a challenging
  NLP problem with applications like hate speech detection and polarity prediction.
  It tackles the difficulty of understanding individual posts in tree-structured conversations
  where semantic cross-referencing occurs.
---

# GASCOM: Graph-based Attentive Semantic Context Modeling for Online Conversation Understanding

## Quick Facts
- arXiv ID: 2310.14028
- Source URL: https://arxiv.org/abs/2310.14028
- Reference count: 40
- Improves macro-F1 scores by 4.5% for polarity prediction and 5% for hate speech detection

## Executive Summary
This paper addresses the challenge of understanding individual posts in online conversations where posts are structured as discussion trees with semantic cross-referencing. The authors propose GASCOM, a graph-based attentive semantic context modeling framework that overcomes limitations of existing methods by considering both graph structure and semantic information. GASCOM uses novel semantic-aware random walk algorithms for context selection and a token-level multi-head graph attention mechanism for fine-grained context modeling, significantly outperforming state-of-the-art methods while providing interpretability through context weights.

## Method Summary
GASCOM processes online conversations structured as discussion trees by first selecting relevant context nodes using semantic-aware random walks that consider both graph structure and semantic similarity between posts. The framework then employs a token-level multi-head graph attention mechanism to capture fine-grained relationships between tokens from different context utterances. The model is trained using RoBERTa embeddings, cross-entropy loss, and the Adam optimizer on datasets for polarity prediction (Kialo) and hate speech detection (Guest).

## Key Results
- Outperforms state-of-the-art methods by 4.5% macro-F1 for polarity prediction and 5% for hate speech detection
- Semantic-aware random walks outperform structure-only approaches for context