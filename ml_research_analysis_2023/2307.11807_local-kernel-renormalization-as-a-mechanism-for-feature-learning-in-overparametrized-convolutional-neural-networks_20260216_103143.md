---
ver: rpa2
title: Local Kernel Renormalization as a mechanism for feature learning in overparametrized
  Convolutional Neural Networks
arxiv_id: '2307.11807'
source_url: https://arxiv.org/abs/2307.11807
tags:
- kernel
- learning
- networks
- matrix
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates feature learning in overparameterized CNNs
  compared to fully-connected networks. The authors show that CNNs undergo local kernel
  renormalization, allowing them to select data-dependent local components for prediction,
  while FC networks only undergo global renormalization.
---

# Local Kernel Renormalization as a mechanism for feature learning in overparametrized Convolutional Neural Networks

## Quick Facts
- arXiv ID: 2307.11807
- Source URL: https://arxiv.org/abs/2307.11807
- Authors:
- Reference count: 0
- This paper investigates feature learning in overparameterized CNNs compared to fully-connected networks, showing that CNNs undergo local kernel renormalization while FC networks only undergo global renormalization.

## Executive Summary
This paper provides a theoretical framework for understanding feature learning in overparametrized Convolutional Neural Networks (CNNs) versus Fully-Connected Networks (FCNs). The authors show that CNNs undergo local kernel renormalization, enabling data-dependent selection of local components for prediction, while FCNs only undergo global renormalization. They derive an effective action for one-hidden-layer networks and demonstrate that finite-width CNNs can outperform their infinite-width counterparts, while FCNs cannot. Empirical evidence supports these theoretical predictions, revealing that the local connectivity and weight sharing in CNNs enable a more efficient form of feature learning at finite width.

## Method Summary
The authors use a Bayesian framework with effective actions to study feature learning in one-hidden-layer networks. They consider both FC and CNN architectures with Gaussian inputs and linear teacher labels, operating in the proportional limit where the number of filters and training samples approach infinity while maintaining a fixed ratio. The framework derives how the kernel renormalization differs between architectures - CNNs exhibit local renormalization controlled by a spatially-varying feature matrix, while FCNs show only global renormalization controlled by a scalar parameter. The theory is validated through numerical experiments on synthetic data and the CIFAR10 dataset.

## Key Results
- CNNs undergo local kernel renormalization, allowing data-dependent selection of local components for prediction
- FCNs only undergo global renormalization, limiting their feature learning capacity to a single scalar parameter
- Finite-width CNNs can outperform their infinite-width counterparts, while FCNs cannot
- Empirical evidence from synthetic data and CIFAR10 binary classification supports theoretical predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CNNs undergo local kernel renormalization, enabling data-dependent selection of local components for prediction
- Mechanism: Convolutional layers with local connectivity and weight sharing create a local kernel matrix K_ij^μν that captures spatial correlations between patches. During training, the feature matrix Q_ij acts as local weights that renormalize different components of this kernel, allowing the network to emphasize or de-emphasize specific spatial relationships based on the data
- Core assumption: The thermodynamic limit where the number of filters and training samples both approach infinity while maintaining a fixed ratio (α_c = P/N_c) allows the effective action framework to capture the renormalization behavior
- Evidence anchors:
  - [abstract]: "the CNN kernel undergoes a local renormalization, meaning that the network can select the local components that will contribute to the final prediction in a data-dependent way"
  - [section III]: "the renormalized kernel K_CNN^R(¯Q)_μν = 1/(λ_1 ⌊N_0/S⌋) Σ_ij ¯Q_ij K_ij^μν"
  - [corpus]: Weak - no direct matches found in neighboring papers
- Break condition: If the stride S scales super-extensively with input size N_0, breaking the proportional limit assumption

### Mechanism 2
- Claim: Fully-connected networks only undergo global renormalization, limiting their feature learning capacity
- Mechanism: In FC networks, the kernel renormalization is controlled by a single scalar parameter Q̄ that globally scales the entire kernel matrix. This means all input features are weighted uniformly based on global correlations, preventing selective emphasis of local patterns
- Core assumption: The finite-width effective action framework correctly captures the kernel renormalization behavior through the order parameter Q̄
- Evidence anchors:
  - [abstract]: "the kernel of the FC architecture is just globally renormalized by a single scalar parameter"
  - [section II]: "the scalar parameter ¯Q always appears in combination with the corresponding Gaussian prior λ_1 as ¯Q/λ_1"
  - [corpus]: Weak - neighboring papers discuss feature learning but don't directly address global vs local renormalization
- Break condition: If the network operates in the mean-field regime where the last layer normalization scales as 1/N_1 instead of 1/√N_1

### Mechanism 3
- Claim: The difference in kernel renormalization between FC and CNN architectures explains why CNNs outperform FC networks in finite-width regimes
- Mechanism: The local kernel renormalization in CNNs allows them to optimize both bias and variance at finite width, while FC networks are constrained to optimizing only variance with fixed bias. This enables CNNs to achieve better generalization performance in the finite-width regime
- Core assumption: The effective action framework accurately predicts generalization performance through the bias-variance decomposition
- Evidence anchors:
  - [abstract]: "state-of-the-art architectures with convolutional layers achieve optimal performances in the finite-width regime"
  - [section V]: "we observe that: (i) in FCNs, the best possible performance is the infinite-width one. Here the variance is monotonically decreasing with the width N_1, while the bias is a constant"
  - [corpus]: Weak - neighboring papers discuss feature learning but don't directly compare FC vs CNN performance in finite-width regimes
- Break condition: If practical optimization algorithms (like Adam) deviate significantly from the Bayesian posterior sampling assumed in the theoretical framework

## Foundational Learning

- Concept: Kernel methods and kernel renormalization
  - Why needed here: The paper's core insight is that CNNs and FC networks differ in how their kernels renormalize during training, which fundamentally affects their feature learning capacity
  - Quick check question: Can you explain the difference between a global kernel (scaled by a single parameter) and a local kernel (with spatially varying components)?

- Concept: Bayesian learning and effective action formalism
  - Why needed here: The theoretical framework uses Bayesian effective actions to derive how finite-width networks renormalize their kernels, which is essential for understanding the mechanisms described
  - Quick check question: How does the effective action approach relate the training dynamics to changes in the kernel matrix?

- Concept: Convolutional layer mechanics (local connectivity and weight sharing)
  - Why needed here: The local connectivity and weight sharing in CNNs are the architectural features that enable the local kernel renormalization mechanism
  - Quick check question: What are the two key architectural features of CNNs that distinguish them from fully-connected networks, and how do they enable local kernel renormalization?

## Architecture Onboarding

- Component map:
  - Input data → Convolutional filters (local connectivity + weight sharing) → Local kernel matrix K_ij^μν → Feature matrix Q_ij → Renormalized kernel K_CNN^R → Prediction
  - Input data → Fully-connected layer → Global kernel K_μν → Global renormalization parameter Q̄ → Renormalized kernel K_FC^R → Prediction

- Critical path: The path from input data through the convolutional architecture to the renormalized kernel is critical for understanding how CNNs achieve superior feature learning in finite-width regimes

- Design tradeoffs:
  - Filter size M vs. input resolution N_0: Larger filters capture more context but reduce spatial resolution
  - Stride S: Smaller strides provide finer spatial resolution but increase computational cost
  - Number of channels Nc: More channels provide more expressive power but increase the number of parameters

- Failure signatures:
  - If the variance of internal representations doesn't converge to zero in the proportional limit for FC networks (expected behavior)
  - If the variance of internal representations doesn't show a non-trivial dependence on spatial position for CNN networks (unexpected behavior)
  - If the bias in CNNs doesn't show dependence on the width N_c (would contradict local kernel renormalization)

- First 3 experiments:
  1. Train a one-hidden-layer FC network and CNN on a synthetic dataset with Gaussian inputs and linear teacher labels. Compare the variance of the similarity matrix between trained and untrained networks as a function of width
  2. Fix the number of filters Nc and vary the filter size M in a CNN. Plot test loss vs. width to observe whether CNNs outperform their infinite-width limit
  3. Implement the effective action framework numerically to compute the feature matrix Q_ij for both FC and CNN architectures, and visualize the difference in renormalization patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do CNNs with more than one convolutional layer exhibit local kernel renormalization compared to single-layer architectures?
- Basis in paper: [explicit] The paper mentions that extending the framework to L convolutional layers is a possible follow-up, implying that the current analysis is limited to one hidden layer.
- Why unresolved: The current effective action framework is derived for a single convolutional hidden layer, and the authors acknowledge that extending it to multiple layers would be an interesting direction.
- What evidence would resolve it: Deriving an effective action for deep CNNs with L convolutional layers and comparing the local kernel renormalization behavior to the single-layer case.

### Open Question 2
- Question: Does the feature learning mechanism in CNNs change in the mean-field regime compared to the proportional limit studied in this paper?
- Basis in paper: [explicit] The authors state that their current approach cannot cover the mean-field regime and suggest that the formalism in [23] might be more suitable for investigating finite-width effects in this case.
- Why unresolved: The paper focuses on the proportional limit where the size of the training set and hidden layer are taken to infinity while keeping their ratio fixed. The mean-field regime, where the last layer normalization is 1/N1 instead of 1/√N1, is not covered by their analysis.
- What evidence would resolve it: Extending the effective action framework to the mean-field regime and studying the feature learning behavior of CNNs in this setting.

### Open Question 3
- Question: How does the choice of activation function affect the local kernel renormalization in CNNs?
- Basis in paper: [inferred] The paper uses tanh and Erf activation functions in the numerical experiments but does not explore the impact of different activation functions on the local kernel renormalization mechanism.
- Why unresolved: The current analysis assumes odd activation functions and does not investigate whether the local kernel renormalization is specific to certain activation functions or a more general phenomenon.
- What evidence would resolve it: Performing a systematic study of local kernel renormalization in CNNs with various activation functions (e.g., ReLU, Leaky ReLU, Swish) and comparing the results to the tanh and Erf cases.

## Limitations
- The theoretical analysis relies heavily on the proportional limit and Gaussian equivalence assumptions
- Empirical validation is limited to relatively simple datasets and architectures
- The connection between the effective action framework and practical optimization algorithms requires further investigation

## Confidence
- **High confidence**: The fundamental distinction between local and global kernel renormalization in CNNs vs FC networks is well-established theoretically and supported by empirical evidence
- **Medium confidence**: The predictions about finite-width advantages for CNNs versus FC networks are theoretically sound but rely on assumptions about the training dynamics that may not hold in practice
- **Low confidence**: The generalization of these results to deeper networks and more complex datasets remains speculative and requires further investigation

## Next Checks
1. **Numerical verification of effective action predictions**: Implement the exact formulas for the feature matrix Q_ij and renormalized kernels K_CNN^R and K_FC^R, then numerically compute these quantities during training to verify the predicted renormalization patterns
2. **Empirical test on larger architectures**: Extend the experimental validation to deeper networks with multiple convolutional and fully-connected layers to assess whether the local renormalization mechanism persists
3. **Connection to practical optimization**: Investigate how the theoretical predictions change when using practical optimizers (Adam, SGD with momentum) versus the idealized Bayesian posterior sampling assumed in the theoretical framework