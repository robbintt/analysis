---
ver: rpa2
title: Evaluation and Analysis of Hallucination in Large Vision-Language Models
arxiv_id: '2308.15126'
source_url: https://arxiv.org/abs/2308.15126
tags:
- hallucination
- lvlms
- evaluation
- haelm
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Hallucination Evaluation based on Large Language
  Models (HaELM), an LLM-based hallucination evaluation framework for Large Vision-Language
  Models (LVLMs). The authors identify that current object-based hallucination evaluation
  methods are flawed due to LVLMs' susceptibility to prompts, leading to biased responses
  that do not reflect real-world hallucinations.
---

# Evaluation and Analysis of Hallucination in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2308.15126
- Source URL: https://arxiv.org/abs/2308.15126
- Reference count: 6
- Primary result: Proposes HaELM, an LLM-based hallucination evaluation framework for LVLMs, achieving 95% alignment with ChatGPT while offering cost and reproducibility benefits

## Executive Summary
This paper addresses the challenge of hallucination evaluation in Large Vision-Language Models (LVLMs) by introducing Hallucination Evaluation based on Large Language Models (HaELM). The authors identify that current object-based hallucination evaluation methods are flawed due to LVLMs' susceptibility to prompts, leading to biased responses. To overcome this, they fine-tune LLaMA using LoRA on hallucination data to create HaELM, which evaluates LVLM responses by comparing them against reference captions. Experimental results demonstrate that HaELM achieves comparable performance to ChatGPT while providing advantages in cost, reproducibility, privacy preservation, and local deployment. The study also analyzes factors contributing to hallucination, such as generation settings and model architecture.

## Method Summary
The authors collect hallucination data by analyzing LVLM responses to prompts and use this data to fine-tune LLaMA via LoRA, creating HaELM. HaELM evaluates LVLM outputs by comparing them against reference captions from datasets like MS-COCO. The evaluation process involves fine-tuning HaELM on a curated dataset of hallucination and non-hallucination examples, followed by systematic testing of LVLM outputs under varying generation parameters (length, temperature, sampling). The study evaluates hallucination in current LVLMs and analyzes factors contributing to hallucination, offering suggestions for mitigation.

## Key Results
- HaELM achieves approximately 95% performance comparable to ChatGPT in hallucination detection
- LVLM hallucinations are influenced by generation parameters: longer outputs and higher diversity (via temperature) increase hallucination rates
- There is a trade-off between model performance and hallucination susceptibility, with more powerful LVLMs exhibiting higher hallucination rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based hallucination evaluation avoids bias from prompt-dependent object detection queries.
- Mechanism: By using a pre-trained LLM (HaELM) to judge whether LVLM responses align with image captions, the evaluation is decoupled from the LVLM's own response generation patterns and prompt susceptibility.
- Core assumption: HaELM can be fine-tuned on hallucination patterns and will reliably detect mismatches without being misled by the same biases that affect the LVLMs.
- Evidence anchors:
  - [abstract] "HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment."
  - [section 3] "We notice that LLMs demonstrate powerful text-understanding capabilities. Based on this, we propose an innovative framework called Hallucination Evaluation based on Large Language Models (HaELM)."
  - [corpus] Weak; related papers focus on hallucination evaluation but not on the mechanism of decoupling from prompt bias.
- Break condition: If HaELM inherits biases from its training data or fine-tuning process that cause it to misjudge hallucinations similarly to how LVLMs are misled.

### Mechanism 2
- Claim: HaELM provides consistent, reproducible hallucination evaluation without reliance on costly external APIs.
- Mechanism: By fine-tuning LLaMA with LoRA on a curated dataset of hallucination/non-hallucination examples, HaELM becomes a standalone evaluation tool that can be reused without per-evaluation costs.
- Core assumption: The curated dataset adequately represents hallucination patterns, and the fine-tuned model generalizes to unseen LVLMs.
- Evidence anchors:
  - [abstract] "HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment."
  - [section 4.2] "HaELM can be reused multiple times once data collection and training finish, which offers a cost advantage over ChatGPT while ensuring reproducibility."
  - [corpus] Weak; related papers do not discuss cost or reproducibility of hallucination evaluation frameworks.
- Break condition: If the hallucination patterns in the training data do not generalize, leading to false positives or negatives in new evaluation scenarios.

### Mechanism 3
- Claim: Analyzing generation settings (length, sampling, temperature) reveals controllable factors influencing hallucination.
- Mechanism: By systematically varying LVLM generation parameters and measuring hallucination frequency, the study identifies how diversity and length affect hallucination incidence.
- Core assumption: Hallucinations are partly driven by generation artifacts (e.g., longer outputs drift from image content, high diversity increases errors).
- Evidence anchors:
  - [section 5.2] "We observed that as the maximum length increased, the hallucination became stronger... We suggest that obtaining relatively accurate results can be achieved by truncating the responses."
  - [section 5.2] "Random sampling may cause LVLMs to choose tokens that are less aligned with the visual input, resulting in factual errors... There is still a trade-off between diversity and hallucination."
  - [corpus] Weak; related papers do not systematically analyze generation settings' impact on hallucination.
- Break condition: If hallucinations are primarily driven by model architecture or training data rather than generation settings, making mitigation via parameter tuning ineffective.

## Foundational Learning

- Concept: Understanding of hallucination in multimodal models (discrepancy between visual input and generated text).
  - Why needed here: Central to defining the problem and designing the evaluation framework.
  - Quick check question: What distinguishes hallucinations in LVLMs from hallucinations in LLMs?
- Concept: Knowledge of fine-tuning techniques (LoRA, instruction tuning) for adapting pre-trained models.
  - Why needed here: Required to build HaELM by adapting LLaMA to hallucination detection.
  - Quick check question: How does LoRA fine-tuning differ from full fine-tuning in terms of parameter efficiency?
- Concept: Familiarity with generation parameters (temperature, top-K sampling, max length) and their effects on output diversity.
  - Why needed here: Essential for analyzing how these parameters influence hallucination rates.
  - Quick check question: What is the effect of increasing temperature on the randomness of model outputs?

## Architecture Onboarding

- Component map: Reference captions + LVLM-generated responses -> HaELM (fine-tuned LLaMA via LoRA) -> Binary judgment ("yes"/"no") on hallucination presence
- Critical path: Data collection -> Fine-tuning HaELM -> Evaluation of LVLM outputs -> Analysis of results
- Design tradeoffs:
  - Using reference captions instead of raw images trades off evaluation accuracy for feasibility (since LVLMs themselves hallucinate).
  - Fine-tuning on simulated hallucinations may not capture all real hallucination patterns, affecting generalizability.
- Failure signatures:
  - High false positive rate: HaELM incorrectly flags non-hallucinatory responses.
  - High false negative rate: HaELM misses actual hallucinations.
  - Bias toward "yes" or "no" responses due to imbalanced training data.
- First 3 experiments:
  1. Evaluate HaELM on a held-out test set of manually annotated hallucinations/non-hallucinations to measure precision/recall.
  2. Compare HaELM's judgments with ChatGPT's on the same LVLM outputs to quantify alignment.
  3. Systematically vary LVLM generation parameters (length, temperature, top-K) and measure hallucination frequency to validate controllable factors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms in LVLMs lead to hallucinations during the autoregressive generation process?
- Basis in paper: [explicit] The paper discusses analyzing attention patterns during hallucination generation and identifies that tokens like "<sp>" and "1" appear frequently in training data, leading to biased false correlations.
- Why unresolved: While the paper identifies attention patterns during hallucination generation, it does not fully explain the underlying mechanisms that cause these attention patterns to lead to hallucinations.
- What evidence would resolve it: Detailed analysis of attention patterns across different LVLMs and their correlation with hallucination occurrence, potentially using techniques like gradient-based attribution or attention rollout methods.

### Open Question 2
- Question: How can we effectively mitigate hallucinations in LVLMs without compromising their generative capabilities?
- Basis in paper: [explicit] The paper suggests that penalizing attention that deviates from the image could be a potential approach to addressing hallucinations, but this remains to be explored in future work.
- Why unresolved: The paper identifies a potential mitigation strategy but does not implement or evaluate it. Additionally, there may be trade-offs between hallucination mitigation and maintaining the model's generative performance.
- What evidence would resolve it: Implementation and evaluation of attention-based hallucination mitigation techniques, comparing their effectiveness in reducing hallucinations while preserving the model's ability to generate diverse and creative responses.

### Open Question 3
- Question: What is the optimal balance between model performance and hallucination susceptibility in LVLMs?
- Basis in paper: [explicit] The paper notes that there exists a trade-off between model performance and hallucinations, with more powerful LVLMs exhibiting higher hallucination rates.
- Why unresolved: The paper observes the trade-off but does not provide specific guidelines or quantitative measures to determine the optimal balance for different applications or use cases.
- What evidence would resolve it: Systematic evaluation of various LVLMs across different tasks and domains, measuring both their performance and hallucination rates, to identify the point of diminishing returns or the threshold beyond which hallucinations become unacceptable for a given application.

## Limitations

- The hallucination detection framework relies on a curated dataset that may not capture all hallucination patterns, potentially limiting generalizability to new LVLM architectures.
- The use of reference captions rather than raw images for evaluation introduces a tradeoff between feasibility and accuracy, as caption generation itself may introduce artifacts.
- The study does not explore potential bias in the hallucination dataset creation process, which could affect HaELM's reliability.

## Confidence

- **High Confidence**: The experimental results showing HaELM's 95% alignment with ChatGPT performance and the analysis of generation parameters' effects on hallucination.
- **Medium Confidence**: The claim that decoupling from prompt-dependent object detection queries reduces bias, as this relies on assumptions about HaELM's training data quality.
- **Medium Confidence**: The assertion that controllable factors (length, sampling) significantly influence hallucination, as this requires further validation across diverse LVLM architectures.

## Next Checks

1. Test HaELM's performance on a diverse set of held-out LVLMs not included in the original evaluation to assess generalizability.
2. Conduct ablation studies varying the size and diversity of the hallucination training dataset to identify the minimum viable training set.
3. Compare hallucination detection performance using reference captions versus raw image input (where feasible) to quantify the accuracy tradeoff.