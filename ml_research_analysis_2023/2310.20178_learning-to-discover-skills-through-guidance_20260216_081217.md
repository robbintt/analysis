---
ver: rpa2
title: Learning to Discover Skills through Guidance
arxiv_id: '2310.20178'
source_url: https://arxiv.org/abs/2310.20178
tags:
- skill
- skills
- disco-dance
- state
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DISCO-DANCE introduces a guidance-based exploration method for
  unsupervised skill discovery that addresses the limited exploration problem by directly
  guiding skills toward unexplored states. The algorithm selects a guide skill with
  high potential to reach unexplored regions and uses KL-divergence rewards to encourage
  other skills to follow it.
---

# Learning to Discover Skills through Guidance

## Quick Facts
- arXiv ID: 2310.20178
- Source URL: https://arxiv.org/abs/2310.20178
- Reference count: 40
- DISCO-DANCE achieves 86.3% state coverage vs 52.4% for DIAYN on Ant Π-maze

## Executive Summary
DISCO-DANCE introduces a guidance-based exploration method for unsupervised skill discovery that addresses the limited exploration problem by directly guiding skills toward unexplored states. The algorithm selects a guide skill with high potential to reach unexplored regions and uses KL-divergence rewards to encourage other skills to follow it. Experiments show DISCO-DANCE outperforms existing methods on 2D and Ant mazes, achieving state coverages of 86.3% vs 52.4% for DIAYN on Ant Π-maze. It also demonstrates superior performance on Deepmind Control Suite tasks, with an average score of 654.69 across 8 tasks compared to 553.40 for DIAYN.

## Method Summary
DISCO-DANCE is an unsupervised skill discovery algorithm that addresses limited exploration by guiding skills toward unexplored states. The method operates in two phases: pretraining where initial skills are learned using mutual information maximization, and guidance where a guide skill is selected based on proximity to unexplored states via random walks, and apprentice skills are encouraged to follow it using KL-divergence rewards. When most skills converge, new skills are added as apprentices to continue exploration. The approach combines a skill policy network, discriminator network, guide skill selector, and KL divergence reward calculator to enable efficient state space coverage.

## Key Results
- DISCO-DANCE achieves 86.3% state coverage on Ant Π-maze vs 52.4% for DIAYN
- Outperforms baselines on Deepmind Control Suite with average score of 654.69 vs 553.40 for DIAYN
- Successfully explores challenging regions like upper-left corner in bottleneck mazes that other methods fail to reach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The guide skill selection process identifies skills adjacent to unexplored states through random walks.
- **Mechanism:** DISCO-DANCE performs random walks from terminal states of existing skills and selects the skill whose random walk reaches the lowest density region among these states. This identifies skills with highest potential to reach unexplored areas.
- **Core assumption:** Random walks from terminal states can approximate proximity to unexplored regions even in high-dimensional spaces.
- **Evidence anchors:**
  - [abstract]: "DISCO-DANCE introduces a guidance-based exploration method for unsupervised skill discovery that addresses the limited exploration problem by directly guiding skills toward unexplored states."
  - [section 3.1]: "We define the guide skill z∗ as the skill which is most adjacent to the unexplored states... we utilize a simple random walk process... pinpoint the state in the lowest density region among collected random walk arrival states and select the skill which that state originated from as the guide skill z∗."
  - [corpus]: Weak evidence - no direct comparison of random walk vs alternative selection methods found.
- **Break condition:** If random walks fail to reach diverse states in high-dimensional spaces, the guide selection becomes ineffective.

### Mechanism 2
- **Claim:** The KL-divergence reward encourages apprentice skills to follow the guide skill while maintaining diversity.
- **Mechanism:** Apprentice skills receive a reward proportional to -DKL(πθ(a|s, zi)||πθ(a|s, z∗)), weighted by (1 - qϕ(zi|s)), encouraging them to imitate the guide skill's behavior while avoiding convergence to the same states.
- **Core assumption:** Skills with low discriminator accuracy can be effectively guided by minimizing KL divergence to the guide skill's policy.
- **Evidence anchors:**
  - [abstract]: "selects a guide skill with high potential to reach unexplored regions and uses KL-divergence rewards to encourage other skills to follow it."
  - [section 3.2]: "the objective of the guidance... minimize the KL-divergence between the apprentice skill and the guide skill policy... the extent to which the apprentice skills are encouraged to follow z∗ can be represented as the weight of the KL-divergence. We set the weight as 1 - qϕ(zi|s) to make the skills with low discriminator accuracy be guided more."
  - [corpus]: No direct evidence found in corpus papers about KL-divergence guidance for skill discovery.
- **Break condition:** If the guide skill's policy changes too rapidly, the KL divergence reward becomes unstable and ineffective.

### Mechanism 3
- **Claim:** Gradually adding new skills prevents converged skills from occupying all available state space.
- **Mechanism:** When most skills are discriminable, DISCO-DANCE adds new skills as apprentice skills rather than relying on fixed skill count, allowing continuous exploration of new regions.
- **Core assumption:** Adding new skills to unexplored regions is more effective than trying to redirect converged skills.
- **Evidence anchors:**
  - [section 3.1]: "If most of the skills are already well discriminable (i.e., high MI rewards), we simply add new skills and make them apprentice skills. This will leave converged skills intact and send new skills to unexplored states."
  - [section 4.4.1]: "We empirically show that adding new skills during training is generally difficult to apply to previous algorithms because the new skills will face pessimistic exploration problems."
  - [corpus]: Weak evidence - no corpus papers specifically address adding new skills during training in USD.
- **Break condition:** If discriminator accuracy drops too quickly with increasing skills, the system may fail to maintain skill discriminability.

## Foundational Learning

- **Concept:** Mutual Information maximization for skill discriminability
  - Why needed here: Forms the baseline reward structure that DISCO-DANCE builds upon
  - Quick check question: Why does maximizing I(Z,S) lead to skills that avoid exploring novel states?

- **Concept:** KL divergence as a guidance mechanism
  - Why needed here: Enables apprentice skills to follow guide skill while maintaining policy diversity
  - Quick check question: How does weighting KL divergence by (1 - qϕ(zi|s)) prevent over-guidance of already-converged skills?

- **Concept:** Random walk processes for state space exploration
  - Why needed here: Provides a method to identify skills adjacent to unexplored regions
  - Quick check question: What properties must a random walk process have to effectively identify proximity to unexplored states?

## Architecture Onboarding

- **Component map:**
  Skill policy network πθ(a|s, z) -> Discriminator network qϕ(z|s) -> Guide skill selector -> Apprentice skill manager -> Random walk processor -> KL divergence reward calculator

- **Critical path:**
  1. Pretraining phase: Learn initial skills using MI maximization
  2. Guide selection: Identify guide skill via random walks and density estimation
  3. Apprentice identification: Find skills with low discriminator accuracy
  4. Guidance phase: Apply KL divergence rewards to encourage following guide skill
  5. Extension phase: Add new skills when existing skills converge

- **Design tradeoffs:**
  - Random walk step count vs. sample efficiency: More steps provide better density estimates but cost more samples
  - Guide coefficient α vs. exploration vs. exploitation: Higher values encourage stronger guidance but may reduce diversity
  - Apprentice accuracy threshold ϵ vs. skill utilization: Lower thresholds create more apprentices but may waste guidance on nearly converged skills

- **Failure signatures:**
  - Skills converge to same regions: Indicates guide selection or KL guidance is ineffective
  - Discriminator accuracy drops sharply: Suggests too many skills or ineffective skill discriminability
  - Guide skill changes frequently: May indicate unstable random walk density estimates
  - New skills don't explore: Suggests initialization or guidance mechanism issues

- **First 3 experiments:**
  1. Baseline comparison: Run DIAYN vs DISCO-DANCE on 2D Empty maze to verify guidance improves exploration
  2. Guide selection ablation: Compare random walk vs naive farthest-skill selection on bottleneck maze
  3. Apprentice initialization: Test guide skill initialization vs random initialization for new skills

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DISCO-DANCE's performance scale with increasing state space dimensionality beyond what was tested in the paper?
- Basis in paper: [inferred] The paper mentions the random walk process may cause sample inefficiencies in long-horizon environments like DMC (1000 timesteps), suggesting potential scaling issues.
- Why unresolved: The experiments were limited to specific environments (2D mazes, Ant mazes, and DMC) with finite state space sizes. The paper does not provide evidence of DISCO-DANCE's performance on environments with much higher dimensional state spaces.
- What evidence would resolve it: Empirical results demonstrating DISCO-DANCE's performance on environments with significantly higher dimensional state spaces (e.g., pixel-based control tasks with 84x84x3 input) compared to baseline methods.

### Open Question 2
- Question: How does the random walk-based guide skill selection compare to other density estimation methods in high-dimensional state spaces?
- Basis in paper: [explicit] The paper mentions using k-nearest neighbors for density estimation in the random walk process and briefly discusses using kernel density estimation (KDE) as an alternative in the ablation study.
- Why unresolved: While the paper shows that KNN and KDE perform similarly in their experiments, it does not provide a comprehensive comparison of different density estimation methods for guide skill selection in high-dimensional state spaces.
- What evidence would resolve it: A systematic comparison of various density estimation methods (e.g., KNN, KDE, Gaussian mixture models, neural density estimators) for guide skill selection in high-dimensional state spaces, with empirical results showing the impact on DISCO-DANCE's performance.

### Open Question 3
- Question: How does DISCO-DANCE's performance change when combined with other skill discovery methods that refine the skill representation objective (e.g., LSD)?
- Basis in paper: [explicit] The paper mentions that DISCO-DANCE's exploration reward (rguide) can be combined with other skill representation objectives and provides a brief comparison with LSD in the ablation study.
- Why unresolved: The paper does not provide a detailed analysis of how DISCO-DANCE's performance changes when combined with various skill representation methods. It only shows a single example with LSD.
- What evidence would resolve it: Empirical results demonstrating DISCO-DANCE's performance when combined with different skill representation methods (e.g., LSD, CIC, Choreographer) compared to using DISCO-DANCE alone or the skill representation methods alone.

## Limitations
- Random walk-based guide selection may become less effective in very high-dimensional state spaces
- Method relies heavily on discriminator's ability to accurately identify skill states, which may degrade with many skills
- KL-divergence guidance assumes guide skill maintains relatively stable policy

## Confidence

- **High confidence** in the mechanism of KL-divergence guidance for skill following
- **Medium confidence** in random walk effectiveness for guide selection in high-dimensional spaces
- **Medium confidence** in the scalability of adding new skills without discriminator collapse

## Next Checks

1. Test DISCO-DANCE in environments with significantly higher dimensional state spaces (>100 dimensions) to evaluate random walk guide selection effectiveness
2. Implement an ablation study varying the number of skills from 5 to 50 to measure discriminator accuracy degradation
3. Compare DISCO-DANCE with a variant that uses state visitation counts instead of random walks for guide selection to validate the random walk mechanism's contribution