---
ver: rpa2
title: Impact of HPO on AutoML Forecasting Ensembles
arxiv_id: '2311.04034'
source_url: https://arxiv.org/abs/2311.04034
tags:
- hyperband
- forecasting
- iter
- ensemble
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the impact of adding hyperparameter optimization
  (HPO) to deep learning models in an AutoML forecasting ensemble. The ensemble combines
  diverse algorithms including ARIMA, ETS, NPTS, Prophet, MQ-CNN and DeepAR.
---

# Impact of HPO on AutoML Forecasting Ensembles

## Quick Facts
- arXiv ID: 2311.04034
- Source URL: https://arxiv.org/abs/2311.04034
- Reference count: 40
- Adding HPO to deep learning models in AutoML forecasting ensembles improves accuracy by up to 9.9% but increases latency by 65.8%

## Executive Summary
This paper evaluates the impact of hyperparameter optimization (HPO) on AutoML forecasting ensembles, specifically examining how adding HPO to deep learning models affects ensemble accuracy and latency. The study compares Bayesian Optimisation and Hyperband strategies across four public datasets, finding that HPO can improve ensemble accuracy by up to 9.9% while increasing end-to-end latency by 65.8%. Hyperband emerges as the preferred HPO strategy, offering 3.1% lower error and 32.7% lower latency compared to Bayesian Optimisation. The research provides practical guidance for selecting optimal HPO configurations based on desired accuracy-latency trade-offs.

## Method Summary
The study implements an AutoML forecasting ensemble combining ARIMA, ETS, NPTS, Prophet, MQ-CNN, and DeepAR models. HPO is applied specifically to DeepAR and MQ-CNN using Bayesian Optimisation and Hyperband strategies with varying max_training_jobs parameters. Experiments run on four datasets (Covid Death, Solar, Electricity, Kaggle retail) with 3 random seeds each. The ensemble uses a three-dimensional strategy (local, global, combination parameters) optimized via Basin-Hopping. Performance is measured using avg-wQL accuracy metric and end-to-end latency.

## Key Results
- HPO improves ensemble accuracy by up to 9.9% in terms of avg-wQL
- Hyperband achieves 3.1% lower error and 32.7% lower latency than Bayesian Optimisation
- Hyperband with 30 max_training_jobs achieves 3.5% lower error and 16.0% lower latency than Amazon Forecast
- Optimal HPO configuration depends on the desired accuracy-latency trade-off

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding HPO to DeepAR and MQ-CNN improves ensemble accuracy by reducing individual model errors.
- Mechanism: HPO tunes context length and learning rate for DeepAR, and context length for MQ-CNN, optimizing forecast distribution parameters. Better-tuned individual models contribute stronger predictions to the ensemble, lifting overall performance.
- Core assumption: Individual model improvements translate linearly to ensemble accuracy gains.
- Evidence anchors:
  - [abstract] "adding hyperparameter optimization can lead to performance improvements, with the final setup having a 9.9 % percent accuracy improvement"
  - [section] "HPO improves accuracy by 21.2 % for DeepAR and 6.6 % for MQ-CNN"
  - [corpus] Weak correlation; no direct evidence of HPO improving ensemble accuracy in related work.
- Break condition: If ensemble strategy down-weights poorly tuned models or if diversity loss outweighs individual gains.

### Mechanism 2
- Claim: Hyperband yields lower latency than Bayesian Optimisation while maintaining comparable accuracy.
- Mechanism: Hyperband uses successive halving to stop unpromising configurations early, reducing training jobs and wall-clock time. Bayesian Optimisation evaluates all configurations sequentially, incurring higher latency.
- Core assumption: Early stopping in Hyperband does not discard near-optimal configurations.
- Evidence anchors:
  - [abstract] "Hyperband is found to be generally preferable to Bayesian Optimisation, offering 3.1% lower error and 32.7% lower latency"
  - [section] "Hyperband is on average 32.6 % faster than Bayesian Optimisation"
  - [corpus] No direct latency comparison in related work; evidence is paper-specific.
- Break condition: If early stopping threshold is too aggressive, leading to loss of best configurations.

### Mechanism 3
- Claim: Optimal max_training_jobs balances accuracy improvement against latency cost.
- Mechanism: More training jobs improve hyperparameter coverage and model fit (lower error), but increase sequential training time. The trade-off is modeled as a convex combination of normalized error and latency.
- Core assumption: Error reduction scales predictably with number of training jobs up to a point of diminishing returns/overfitting.
- Evidence anchors:
  - [abstract] "optimising the configuration of the Hyperband hyperparameter tuning strategy can lead to further improvements in accuracy up to 9.9%"
  - [section] "Hyperband with 30 maximum training jobs is competitive with state of the art commercial AutoML forecasting solutions"
  - [corpus] Weak evidence; no corpus examples of explicit max_training_jobs tuning for forecasting ensembles.
- Break condition: If hyperparameter space is too small for added jobs to matter, or if overfitting dominates beyond a threshold.

## Foundational Learning

- Concept: Ensemble learning combining local and global strategies
  - Why needed here: The paper uses a three-dimensional ensemble (local, global, combination parameters) to balance item-level specificity with robustness.
  - Quick check question: How does the local strategy differ from the global strategy in selecting models for each item?

- Concept: Successive halving and Hyperband mechanics
  - Why needed here: Hyperband’s inner loop allocates resources to configurations and prunes poor performers; understanding this is key to latency gains.
  - Quick check question: In Hyperband, what determines the number of configurations terminated at each round?

- Concept: Quantile regression for probabilistic forecasting
  - Why needed here: MQ-CNN outputs multi-horizon quantile forecasts; avg-wQL is used as the primary accuracy metric.
  - Quick check question: What does the 0.5-quantile wQL represent in terms of forecast error?

## Architecture Onboarding

- Component map: Dataset → DataPrep → HPO (DeepAR/MQ-CNN) → ModelTrain (all algos) → EnsembleSelect → EnsemblePredict → Metrics
- Critical path: HPO tuning → Best config selection → Ensemble parameter optimization (Basin-Hopping) → Final forecast
- Design tradeoffs: More max_training_jobs → better accuracy, higher latency; parallelization reduces latency but increases resource usage.
- Failure signatures: HPO latency spikes due to resource allocation limits; ensemble accuracy stalls if diversity is lost; overfitting if max_training_jobs too high.
- First 3 experiments:
  1. Run ensemble without HPO (baseline) to confirm default accuracy/latency.
  2. Run ensemble with Hyperband (5 max_training_jobs) to measure latency gain.
  3. Run ensemble with Hyperband (30 max_training_jobs) to measure accuracy gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal number of max_training_jobs vary with dataset size and characteristics?
- Basis in paper: [explicit] The paper mentions that the optimal configuration depends on the trade-off between accuracy and latency, and provides a tool for choosing optimal configurations based on this trade-off. It also shows that different datasets (Covid Death, Solar, Electricity, Kaggle retail) have different latencies and errors for the same configurations.
- Why unresolved: The paper does not provide a detailed analysis of how the optimal number of max_training_jobs changes with dataset size, number of items, or number of time steps. It only shows a general trend that more training jobs lead to better accuracy but higher latency.
- What evidence would resolve it: A comprehensive study comparing the performance of different max_training_jobs on datasets with varying sizes and characteristics, showing a clear relationship between dataset properties and optimal max_training_jobs.

### Open Question 2
- Question: How does the parallelization factor max_parallel_jobs affect the accuracy and latency of the ensemble?
- Basis in paper: [explicit] The paper mentions that max_parallel_jobs was kept constant at 5 in all experiments, but acknowledges that it could be varied to achieve different trade-offs between accuracy and latency.
- Why unresolved: The paper does not explore the impact of different max_parallel_jobs values on the ensemble's performance. It only mentions that parallelization does not change training cost but reduces latency.
- What evidence would resolve it: Experiments comparing the performance of the ensemble with different max_parallel_jobs values, showing how accuracy and latency change with parallelization.

### Open Question 3
- Question: How does the ensemble strategy (local, global, and combination parameters) affect the performance of the AutoML forecast ensemble?
- Basis in paper: [explicit] The paper mentions that the ensemble strategy is determined by three parameters: local, global, and combination, and that the optimal values for these parameters are determined by hyperparameter optimization.
- Why unresolved: The paper does not provide a detailed analysis of how different ensemble strategies affect the performance of the ensemble. It only mentions that the ensemble strategy is used to combine the forecasts of different algorithms.
- What evidence would resolve it: Experiments comparing the performance of the ensemble with different ensemble strategies, showing how accuracy and latency change with different combinations of local, global, and combination parameters.

## Limitations

- Limited generalizability due to experiments on four public datasets with relatively small item counts (54-370 items)
- Incomplete specification of ensemble selection mechanism and hyperparameter bounds makes exact reproduction challenging
- Focus on avg-wQL metric may miss other important forecasting performance dimensions like calibration or sharpness

## Confidence

- **High confidence**: The reported latency differences between Hyperband and Bayesian Optimisation are robust, with clear evidence showing Hyperband's 32.6% average speedup and the specific configuration achieving 16.0% lower latency than Amazon Forecast.
- **Medium confidence**: The accuracy improvements from HPO (up to 9.9% avg-wQL reduction) are well-supported within the experimental framework, though the assumption that individual model improvements translate linearly to ensemble gains requires further validation on larger, more diverse datasets.
- **Low confidence**: The optimal max_training_jobs configuration (30 jobs) achieving 3.5% lower error than Amazon Forecast may be overfit to the specific experimental setup and may not generalize across different problem domains or dataset characteristics.

## Next Checks

1. **Scale-up validation**: Replicate experiments on enterprise-scale datasets with >10,000 items to verify that Hyperband's latency advantages and accuracy improvements persist at production scale.

2. **Ensemble diversity analysis**: Conduct ablation studies to measure how HPO affects ensemble diversity and whether the accuracy gains come from individual model improvements or changes in model complementarity.

3. **Cost-benefit optimization**: Implement a resource-aware HPO framework that explicitly models cloud compute costs against accuracy gains to determine if the 65.8% latency increase justifies the performance improvements in practical deployment scenarios.