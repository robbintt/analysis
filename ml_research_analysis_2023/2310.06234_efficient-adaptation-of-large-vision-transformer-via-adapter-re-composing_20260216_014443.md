---
ver: rpa2
title: Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing
arxiv_id: '2310.06234'
source_url: https://arxiv.org/abs/2310.06234
tags:
- uni00000013
- uni00000011
- adaptation
- uni00000003
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ARC, a parameter-efficient adapter method
  for adapting large vision transformers. ARC reuses bottleneck projections across
  layers and composes layer-adaptive adapters via low-dimensional rescaling coefficients,
  significantly reducing the number of new parameters.
---

# Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing

## Quick Facts
- arXiv ID: 2310.06234
- Source URL: https://arxiv.org/abs/2310.06234
- Reference count: 40
- Parameter-efficient adapter method for vision transformers achieves 8-18× parameter reduction

## Executive Summary
This paper introduces ARC (Adapter Re-Composing), a parameter-efficient method for adapting large vision transformers to downstream tasks. ARC reuses bottleneck projections across layers and composes layer-adaptive adapters through low-dimensional rescaling coefficients, significantly reducing the number of new parameters while maintaining competitive accuracy. The method is evaluated across 24 datasets using ViT and Swin backbones, demonstrating substantial parameter efficiency gains compared to existing adapter methods.

## Method Summary
ARC inserts small adapter modules into vision transformer layers by reusing bottleneck projections across multiple layers and composing layer-specific adaptations through learned diagonal rescaling coefficients. The method uses symmetric down-/up-projections to construct bottleneck operations, where a single learned matrix serves both directions, halving parameter count. Each adapter consists of a shared projection matrix followed by layer-specific diagonal scaling, allowing flexible re-composition while keeping parameters low. The adapters are inserted before both MHA and FFN blocks in each encoder layer.

## Key Results
- Achieves 8-18× parameter reduction compared to existing adapter methods
- Maintains competitive accuracy across 24 diverse datasets
- Effective on both ViT and Swin backbones
- Shows parameter efficiency gains while preserving transfer learning performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapter parameters can be shared across layers when they exhibit low-rank structure
- Mechanism: Adaptation matrices follow power-law singular value distribution, allowing small basis vectors to reconstruct full matrix
- Core assumption: Natural adaptation matrices have extremely low-rank characteristics
- Evidence: Singular values exhibit extreme sparsity and power-law distribution
- Break condition: Performance degrades if adaptation matrices become full-rank

### Mechanism 2
- Claim: Symmetric down-/up-projections reduce parameter count while maintaining expressiveness
- Mechanism: Using Wup = (Wdown)T allows single matrix to serve both directions, halving parameter count
- Core assumption: Bottleneck benefits from equal dimensionality in both directions
- Evidence: Symmetric design enables parameter compression
- Break condition: Asymmetric transformations needed for certain downstream tasks

### Mechanism 3
- Claim: Low-dimensional re-scaling coefficients can compose layer-adaptive adapters from shared basis
- Mechanism: Learning D'×D' diagonal matrices C(l) per layer allows flexible re-composition while keeping parameters low
- Core assumption: Layer-wise variations captured by multiplicative scaling of shared projections
- Evidence: Layer-adaptive re-composition achieved through rescaling
- Break condition: Complex layer-wise variations require more than simple scaling

## Foundational Learning

- Concept: Vision Transformer architecture (patch embedding, MHA, FFN blocks)
  - Why needed: ARC modifies standard ViT architecture by inserting adapters
  - Quick check: What are the two main blocks in each ViT encoder layer?

- Concept: Parameter-efficient fine-tuning (LoRA, Adapter, Prompt tuning)
  - Why needed: ARC builds on existing PEFT methods but introduces parameter sharing
  - Quick check: How does LoRA achieve parameter efficiency compared to full fine-tuning?

- Concept: Low-rank matrix approximation and singular value decomposition
  - Why needed: Core insight relies on low-rank nature of adaptation matrices
  - Quick check: What does power-law singular value distribution mean?

## Architecture Onboarding

- Component map: Input → Patch embedding → [Class token + position embedding] → Encoder layers → Output
  - Each encoder layer: MHA block → ARC adapter → FFN block → ARC adapter
  - ARC adapter: Down projection (shared) → Layer-specific diagonal scaling → Up projection (shared)

- Critical path: Forward pass through ViT with ARC adapters inserted before MHA and FFN blocks

- Design tradeoffs: Parameter sharing vs. layer-specific adaptation, symmetric vs. asymmetric projections

- Failure signatures: Degraded performance on tasks requiring complex layer-wise variations, overfitting with small bottleneck dimensions

- First 3 experiments:
  1. Verify adapter insertion works by checking output shape matches expected dimensions
  2. Test parameter count reduction by comparing ARC to non-shared adapter variants
  3. Validate re-parameterization by ensuring inference speed matches baseline without ARC

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ARC method be extended to handle varying dimensionalities across layers in Vision Transformers?
- Basis: Paper suggests exploring strategies for varying dimensionalities as future research
- Why unresolved: Identifies this as limitation and open area for research
- What evidence would resolve it: Experimental results showing successful adaptation to models with varying layer dimensionalities

### Open Question 2
- Question: What are long-term effects of using ARC on model performance and parameter efficiency across larger Vision Transformer architectures?
- Basis: Shows effectiveness on various model sizes but lacks long-term scalability data
- Why unresolved: Doesn't explore scalability limits or potential diminishing returns as model size increases
- What evidence would resolve it: Comprehensive studies on ARC's performance across wide range of model sizes

### Open Question 3
- Question: How does ARC perform when applied to Vision Transformers trained with different self-supervised learning objectives?
- Basis: Includes experiments with MAE and Moco V3 but suggests further investigation needed
- Why unresolved: Only tests two self-supervised methods, lacks comprehensive comparison
- What evidence would resolve it: Extensive experiments comparing ARC across diverse set of self-supervised pre-trained Vision Transformers

## Limitations
- Symmetric projection design may not suit all downstream applications requiring asymmetric transformations
- Assumes low-rank structure in adaptation matrices which may not hold for all tasks
- Limited exploration of scalability to extremely large Vision Transformer architectures

## Confidence

**High Confidence**: Parameter efficiency improvements (8-18× reduction) and competitive accuracy results supported by extensive empirical evaluation across 24 datasets with multiple backbones.

**Medium Confidence**: Shared bottleneck projections mechanism well-supported by experiments but relies on assumptions about adaptation matrix structure needing broader validation.

**Low Confidence**: Theoretical justification for natural low-rank structure and power-law singular value distributions lacks rigorous proof and may be dataset-specific.

## Next Checks

1. **Singular Value Distribution Validation**: Analyze adaptation matrices from non-shared adapter baselines on diverse tasks to verify whether power-law distribution holds universally or is task-dependent.

2. **Break Condition Testing**: Systematically increase complexity of adaptation tasks to identify when shared projection approach fails and determine threshold where full-rank adaptation becomes necessary.

3. **Layer-wise Variation Analysis**: Conduct ablation studies varying bottleneck dimension and rescaling coefficient expressiveness to quantify minimum requirements for capturing layer-specific variations across different ViT architectures.