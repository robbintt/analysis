---
ver: rpa2
title: 'FactKB: Generalizable Factuality Evaluation using Language Models Enhanced
  with Factual Knowledge'
arxiv_id: '2305.08281'
source_url: https://arxiv.org/abs/2305.08281
tags:
- uni00000011
- uni0000001b
- factuality
- uni00000013
- factkb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present FACTKB, a simple and novel approach to factuality evaluation
  that leverages external knowledge bases (KBs) to improve fact representations of
  language models (LMs). FACTKB employs three complementary factuality pretraining
  strategies based on direct entity facts, facts grounded in auxiliary knowledge about
  entities, and facts constructed compositionally through KB walks.
---

# FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge

## Quick Facts
- **arXiv ID**: 2305.08281
- **Source URL**: https://arxiv.org/abs/2305.08281
- **Reference count**: 40
- **Primary result**: FACTKB achieves state-of-the-art factuality evaluation performance on both in-domain news summarization and out-of-domain scientific literature datasets.

## Executive Summary
FACTKB is a novel approach to factuality evaluation that enhances language models with factual knowledge from external knowledge bases (KBs). The method uses three complementary pretraining strategies—Entity Wiki, Evidence Extraction, and Knowledge Walk—to teach LMs about entity and relation facts through masked language modeling. FACTKB demonstrates significant improvements over existing approaches on both in-domain news summarization benchmarks and three out-of-domain scientific literature datasets, while also showing enhanced ability to detect semantic frame errors in summaries.

## Method Summary
FACTKB pretrains RoBERTa-base using three strategies: Entity Wiki (masking entities in KB triples), Evidence Extraction (masking entities in KB-supported sentences), and Knowledge Walk (masking in compositional sentences from KB random walks). After pretraining, the model is fine-tuned on FactCollect for binary classification of summary factuality. The approach leverages KB facts to improve LM representations of entities and relations, enabling better detection of factual inconsistencies between summaries and source documents.

## Key Results
- Achieves state-of-the-art performance on two in-domain news summarization benchmarks
- Significantly outperforms existing approaches on three out-of-domain scientific literature datasets
- Shows improved ability to detect erroneous entities and relations in summaries
- Demonstrates robustness and generalizability across domains

## Why This Works (Mechanism)

### Mechanism 1
Pretraining LMs with external KB facts improves their entity and relation representations. FACTKB uses three complementary pretraining strategies that mask and predict entities/relations in KB-derived sentences, forcing the LM to learn factual associations. Core assumption: LM pretraining on masked KB facts improves entity and relation representations in a way that transfers to factuality evaluation.

### Mechanism 2
Pretraining with compositional knowledge walks improves multi-hop reasoning about entities and relations. Knowledge walk strategy generates K-hop random walks over KB, producing sentences with chained relations; LM learns to fill masked relations/entities across hops. Core assumption: LM fine-tuned for factuality evaluation benefits from compositional reasoning ability gained during pretraining.

### Mechanism 3
Factuality pretraining improves detection of semantic frame errors (entity/relation errors) in summaries. Pretraining objectives focus LM on entity/relation facts, so fine-tuned FACTKB is better at identifying inconsistencies in entity/relation claims in summaries vs. documents. Core assumption: Semantic frame errors dominate factual errors in summarization (as FRANK analysis suggests).

## Foundational Learning

- **Knowledge Base Completion (KBC)**: Understanding KBC helps reason about how masking KB facts trains entity/relation representations. Quick check: In KBC, what is the typical prediction target when masking a triple (e1, r, e2)? (Answer: predict either e2 or r given e1 and the other element.)

- **Masked Language Modeling (MLM)**: All three FACTKB pretraining strategies use MLM to train the LM to predict masked tokens in KB-derived sentences. Understanding MLM objective helps debug pretraining and predict how masking strategy affects learning. Quick check: In MLM, what proportion of tokens are typically masked during training? (Answer: Usually 15%.)

- **Catastrophic Forgetting**: FACTKB does two-stage training (pretrain then fine-tune). Catastrophic forgetting could cause LM to lose KB-learned facts during fine-tuning on factuality datasets. Awareness helps design retention strategies. Quick check: What is one common technique to mitigate catastrophic forgetting in continual learning? (Answer: Learning rate scheduling, e.g., lower learning rate in later stages.)

## Architecture Onboarding

- **Component map**: Input: Summary + Document pair → Encoder: Fact-enhanced LM (e.g., RoBERTa-base) trained with KB pretraining → Classifier: [CLS] token fed to binary classifier (FACTUAL vs NON-FACTUAL) → Output: Factuality label

- **Critical path**: 
  1. Load fact-enhanced LM checkpoint
  2. Tokenize SUMMARY [SEP] DOCUMENT
  3. Pass through LM to get [CLS] representation
  4. Feed [CLS] to classifier head
  5. Output binary factuality label

- **Design tradeoffs**: 
  - Pretraining corpus size vs. training time vs. performance (larger N improves generalization but increases compute)
  - Choice of LM (RoBERTa vs. BART vs. DeBERTa) vs. factuality metric compatibility
  - Choice of KB (YAGO vs. Wikidata vs. ConceptNet) vs. domain coverage and fact quality

- **Failure signatures**: 
  - High variance in predictions across seeds → pretraining/fine-tuning instability
  - Poor correlation with human judgments on FRANK → pretraining not transferring well to semantic frame errors
  - Performance drop on out-of-domain datasets → KB pretraining not generalizing across domains

- **First 3 experiments**:
  1. Ablate pretraining strategies: Train FACTKB with only entity wiki, only evidence extraction, only knowledge walk; compare on FactCollect
  2. Vary pretraining corpus size N: Train with N=1e4, 1e5, 1e6; evaluate impact on FactCollect and out-of-domain datasets
  3. Cross-domain evaluation: Train FACTKB on FactCollect, evaluate zero-shot on SciFact, HealthVer, CovidFact; analyze per-error-type performance

## Open Questions the Paper Calls Out

### Open Question 1
How do different knowledge bases (KBs) compare in terms of their effectiveness for factuality pretraining in FACTKB? The paper tested six different KBs but did not provide detailed comparison or analysis of why some KBs perform better than others.

### Open Question 2
How does the performance of FACTKB vary across different domains and types of documents? While tested on news media and scientific literature, the paper does not analyze performance across other domains like social media or legal documents.

### Open Question 3
How does the complexity of the knowledge base walks (K) affect the performance of FACTKB? The paper used K=5 but did not explore how different values affect performance or the trade-offs between walk complexity and accuracy.

## Limitations
- Heavy reliance on FRANK's human annotations without analyzing error type distributions across domains
- Knowledge base (YAGO) may not capture domain-specific facts needed for scientific literature
- Pretraining stage is computationally intensive, requiring training on three separate corpora

## Confidence

**High Confidence**: The core mechanism of pretraining LMs with KB facts improves factuality evaluation on in-domain news summarization benchmarks. The three pretraining strategies are well-defined and their individual contributions are supported by ablation results.

**Medium Confidence**: The cross-domain generalization claims are supported by results on three scientific datasets, but limited diversity of out-of-domain domains and lack of error type analysis reduces confidence in generalizability.

**Low Confidence**: The claim that compositional knowledge walks specifically improve multi-hop reasoning is weakly supported, as the evaluation does not include tasks specifically designed to test multi-hop reasoning capabilities.

## Next Checks

1. **Error Type Analysis**: Conduct detailed analysis of error type distributions in FactCollect, CovidFact, HealthVer, and SciFact datasets to determine whether semantic frame errors dominate across domains.

2. **Domain-Specific KB Pretraining**: Replicate the pretraining process using domain-specific knowledge bases (e.g., scientific ontologies for biomedical datasets) to assess whether domain alignment of KB facts improves cross-domain performance.

3. **Pretraining Duration Impact**: Systematically vary pretraining epochs (1, 3, 5, 10) and measure the impact on both in-domain FactCollect performance and out-of-domain generalization to identify optimal pretraining duration.