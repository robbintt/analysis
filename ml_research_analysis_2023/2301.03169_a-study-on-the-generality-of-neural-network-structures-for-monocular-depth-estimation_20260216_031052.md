---
ver: rpa2
title: A Study on the Generality of Neural Network Structures for Monocular Depth
  Estimation
arxiv_id: '2301.03169'
source_url: https://arxiv.org/abs/2301.03169
tags:
- depth
- datasets
- estimation
- attention
- monocular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the generalization performance of various
  backbone networks (CNN and Transformer) for monocular depth estimation (MDE). The
  authors evaluate state-of-the-art MDE models on both in-distribution and out-of-distribution
  datasets, including synthetic texture-shifted datasets.
---

# A Study on the Generality of Neural Network Structures for Monocular Depth Estimation

## Quick Facts
- arXiv ID: 2301.03169
- Source URL: https://arxiv.org/abs/2301.03169
- Authors: [Not specified in source]
- Reference count: 40
- Primary result: Transformers exhibit shape-bias and better generalization for monocular depth estimation compared to texture-biased CNNs

## Executive Summary
This paper investigates how different neural network architectures generalize for monocular depth estimation (MDE) across various environments. Through extensive experiments on in-distribution and out-of-distribution datasets, including synthetic texture-shifted datasets, the authors demonstrate that Transformers exhibit strong shape-bias while CNNs show strong texture-bias. The study reveals that texture-biased models perform worse on out-of-distribution datasets than shape-biased models, providing important insights for designing generalized networks for dense prediction tasks. The findings suggest that architectural inductive biases (locality for CNNs vs. self-attention for Transformers) fundamentally determine whether models learn texture or shape representations.

## Method Summary
The study evaluates state-of-the-art MDE models using an encoder-decoder architecture with ResNet50 backbone feeding into 4-layer Transformer architecture. The model incorporates an Attention Connection Module (ACM) that extracts position and channel attention maps, and a Feature Fusion Decoder (FFD) that fuses encoder features with attention maps to produce depth maps. Training uses self-supervised learning with photometric consistency (L2 + SSIM) and edge-aware smoothness losses, optimized with Adam (learning rates 2×10⁻⁵ for depth, 5×10⁻⁴ for pose) for 50 epochs on 4 Titan RTX GPUs. The evaluation includes KITTI Eigen split, various out-of-distribution datasets (RGBD, SUN3D, MVS, ETH3D, Scenes11), and synthetic/real-world texture-shifted datasets.

## Key Results
- Transformers exhibit shape-bias while CNNs exhibit texture-bias in monocular depth estimation
- Texture-biased models show worse generalization performance on out-of-distribution datasets than shape-biased models
- The intrinsic locality of CNNs induces texture-bias, while the self-attention of Transformers induces shape-bias
- Shape-biased models maintain performance across texture-shifted environments (watercolor, pencil-sketch, style-transfer) while texture-biased models degrade significantly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers exhibit shape-bias while CNNs exhibit texture-bias in monocular depth estimation
- Mechanism: Transformers capture global spatial context via self-attention, leading to shape-based reasoning. CNNs rely on local convolutions that aggregate texture patterns
- Core assumption: Shape information is more generalizable across environments than texture information
- Evidence anchors: Abstract observation of texture vs shape bias; CNN performance on texture-removed images; weak corpus evidence
- Break condition: If local texture features are more discriminative than global shape for a specific task, CNN texture-bias could outperform shape-bias

### Mechanism 2
- Claim: Texture-biased models exhibit worse generalization performance for MDE than shape-biased models
- Mechanism: Texture features are environment-specific (lighting, weather, material), while shape features are consistent across domains, leading to better out-of-distribution performance for shape-biased models
- Core assumption: Real-world depth estimation tasks encounter varying textures across environments
- Evidence anchors: Abstract claim about generalization; experiment demonstrating properties in texture-shifted scenarios; no direct corpus evidence
- Break condition: If texture patterns are more stable than shape across target domains, texture-biased models could generalize better

### Mechanism 3
- Claim: Intrinsic locality of CNNs induces texture-bias, while self-attention of Transformers induces shape-bias
- Mechanism: Local receptive fields in CNNs aggregate fine-grained texture details, while global self-attention aggregates holistic shape representations
- Core assumption: Architectural inductive biases determine the type of visual features learned
- Evidence anchors: Abstract statement about intrinsic properties; CKA similarity experiments showing ConvNeXt's global focus; no corpus discussion of architectural biases
- Break condition: If locality is modified to capture global context (e.g., large kernels, dilated convolutions), CNNs could exhibit shape-bias

## Foundational Learning

- Concept: Convolutional Neural Networks (CNNs)
  - Why needed here: Understanding CNN locality and texture bias is essential to grasp why CNNs perform poorly on texture-shifted datasets
  - Quick check question: How does a CNN's local receptive field affect its ability to generalize across environments with different textures?

- Concept: Vision Transformers
  - Why needed here: Understanding self-attention and global context is essential to grasp why Transformers are more robust to texture changes
  - Quick check question: How does self-attention in Transformers enable them to capture shape information more effectively than CNNs?

- Concept: Texture vs. Shape Bias
  - Why needed here: Understanding the distinction between texture and shape bias is essential to interpret the experimental results and their implications for depth estimation
  - Quick check question: Why might shape information be more generalizable across environments than texture information in depth estimation?

## Architecture Onboarding

- Component map: Image → Encoder → ACM → FFD → Depth map
- Critical path: Image → Encoder → ACM → FFD → Depth map
- Design tradeoffs:
  - Using only CNNs: Faster inference, but poor generalization to texture-shifted environments
  - Using only Transformers: Better generalization, but potentially higher computational cost
  - Hybrid approach: Balances speed and generalization
- Failure signatures:
  - Poor depth estimation on texture-shifted datasets
  - Inconsistent feature extraction from original and texture-shifted images (low CKA similarity)
  - Reliance on texture information rather than shape information
- First 3 experiments:
  1. Evaluate model performance on in-distribution (KITTI) vs. out-of-distribution datasets to assess generalization
  2. Generate synthetic texture-shifted datasets (e.g., watercolor, pencil-sketch, style-transfer) and evaluate model performance
  3. Measure CKA similarity between features extracted from original and texture-shifted images to analyze texture/shape bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the shape bias in Transformers stem from the self-attention mechanism or from architectural features like patch size and layer depth?
- Basis in paper: [inferred] The paper notes that self-attention captures global information and induces shape bias, while locality weakens generality, but doesn't definitively separate these architectural factors
- Why unresolved: The experiments compare different backbone architectures (CNN, Transformer, and modern variants) but don't isolate self-attention from other architectural differences
- What evidence would resolve it: Ablation studies isolating self-attention modules from other architectural features in both CNN and Transformer designs would clarify the source of shape bias

### Open Question 2
- Question: Can combining self-attention and localized receptive fields create a network with optimal generalization performance for depth estimation?
- Basis in paper: [explicit] The authors observe that self-attention induces shape bias while locality induces texture bias, suggesting these mechanisms could be complementary
- Why unresolved: The paper only tests existing architectures rather than designing new hybrid architectures specifically to combine these mechanisms
- What evidence would resolve it: Designing and testing novel architectures that explicitly combine strong self-attention with carefully controlled localized processing would determine if this combination improves generalization

### Open Question 3
- Question: How does the texture-to-shape bias transition affect depth estimation performance on datasets with varying levels of texture complexity?
- Basis in paper: [explicit] The paper shows that texture-biased models perform worse on out-of-distribution datasets, but doesn't systematically vary texture complexity in training data
- Why unresolved: The experiments use fixed training datasets and synthetic texture shifts rather than controlled variations in texture complexity during training
- What evidence would resolve it: Training networks on datasets with systematically varied texture complexity and testing on corresponding test sets would reveal how the texture-to-shape bias transition affects performance

## Limitations
- Core claims primarily supported by synthetic texture-shifted experiments rather than comprehensive real-world validation
- Generalization benefits of shape-biased Transformers in practical scenarios remain less established
- Analysis relies heavily on CKA similarity as a proxy for understanding architectural biases

## Confidence
- High confidence: Transformer architectures consistently outperform CNNs on out-of-distribution datasets in the experiments conducted
- Medium confidence: The causal relationship between self-attention (Transformers) and shape-bias, and local convolutions (CNNs) and texture-bias, is demonstrated but could benefit from ablation studies with modified architectures
- Medium confidence: The claim that shape-bias leads to better generalization is supported by experimental results but needs validation across more diverse real-world scenarios

## Next Checks
1. Test the same models on additional real-world texture-shifted datasets (e.g., different weather conditions, time-of-day variations) to verify that shape-bias consistently improves generalization beyond synthetic modifications

2. Conduct controlled experiments with hybrid architectures that progressively increase receptive field size in CNNs or reduce global context in Transformers to establish clearer causal links between architectural choices and bias emergence

3. Perform ablation studies measuring how different components of the Transformer (self-attention vs. position embeddings vs. patch embedding) contribute to shape-bias, and similarly analyze which aspects of CNN architectures drive texture-bias