---
ver: rpa2
title: 'Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs against Query-Based
  Attacks'
arxiv_id: '2312.10132'
source_url: https://arxiv.org/abs/2312.10132
tags:
- attacks
- iter
- adversarial
- cara
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving the accuracy-robustness
  tradeoff in defending against query-based black-box attacks on deep neural networks.
  The authors propose a test-time defense method that activates existing defenses,
  such as random noise and random image transformations, only on inputs classified
  with low confidence.
---

# Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs against Query-Based Attacks

## Quick Facts
- arXiv ID: 2312.10132
- Source URL: https://arxiv.org/abs/2312.10132
- Reference count: 40
- Primary result: Training-free method that improves robustness against query-based black-box attacks while maintaining high accuracy on clean samples

## Executive Summary
This work addresses the fundamental challenge of improving accuracy-robustness tradeoffs in defending against query-based black-box attacks on deep neural networks. The authors propose a test-time defense that activates existing randomization defenses only on inputs classified with low confidence, leveraging the insight that adversarial samples necessarily explore low-confidence regions. The method is training-free and can be generically combined with any existing test-time defense, demonstrating significant improvements in robust accuracy against state-of-the-art attacks while maintaining high clean accuracy.

## Method Summary
The proposed method implements a confidence-based activation mechanism where randomization defenses (such as random noise, random image transformations, or JPEG compression) are applied only when the model's highest predicted probability falls below a threshold τ. This approach uses temperature scaling to calibrate model confidence, making the threshold effective across different models. The defense function FD,τ(x) applies the chosen defense D only when maxi∈[n] fi(x) < τ, where fi represents the classifier's output probabilities. This selective activation prevents decision-based attacks from exploring low-confidence regions where adversarial samples must lie, while preserving clean accuracy by avoiding unnecessary defense application on high-confidence genuine inputs.

## Key Results
- Achieves up to 8-20% improvement in robust accuracy against PSJA attack
- Demonstrates up to 34% improvement against SurFree attack
- Maintains minimal impact on clean accuracy (at most 2% reduction)
- Outperforms or closely matches existing training-based defenses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Defenses only trigger on low-confidence inputs, blocking adversarial search
- Mechanism: Uses confidence threshold τ to selectively activate randomization defenses only when model confidence is low
- Core assumption: Genuine inputs are typically classified with high confidence while adversarial samples explore low-confidence regions
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: Attack generates high-confidence adversarial samples

### Mechanism 2
- Claim: Binary search procedures in decision-based attacks inherently explore low-confidence regions
- Mechanism: Binary search between source and target images generates intermediate low-confidence samples
- Core assumption: Binary search must cross low-confidence regions to locate decision boundary
- Evidence anchors: [section], [abstract], [corpus]
- Break condition: Attack locates decision boundary without generating low-confidence intermediates

### Mechanism 3
- Claim: Temperature scaling calibrates model confidence for consistent threshold application
- Mechanism: Adjusts model's confidence predictions using temperature scaling parameter T
- Core assumption: Modern neural networks produce overconfident predictions requiring calibration
- Evidence anchors: [section], [abstract], [corpus]
- Break condition: Temperature scaling fails to properly calibrate model

## Foundational Learning

- Concept: Decision-based black-box attacks
  - Why needed here: Essential to understand why blocking low-confidence regions is effective
  - Quick check question: What is the key difference between decision-based and score-based attacks in terms of information access?

- Concept: Confidence calibration in neural networks
  - Why needed here: Temperature scaling ensures consistent confidence scores across models
  - Quick check question: Why do modern neural networks typically produce overconfident predictions that require calibration?

- Concept: Pareto frontiers in multi-objective optimization
  - Why needed here: Used to evaluate accuracy-robustness tradeoff achieved by different parameter combinations
  - Quick check question: What does it mean for a solution to be Pareto optimal in accuracy-robustness tradeoffs?

## Architecture Onboarding

- Component map: Input → Confidence calculation → Threshold comparison → Defense application (if confidence < τ) → Classification output
- Critical path: Input passes through confidence calculation, compares against threshold τ, defense applies if needed, then classification output
- Design tradeoffs: Higher τ provides more protection but reduces clean accuracy; lower τ preserves accuracy but offers less robustness
- Failure signatures: Minimal RA improvement suggests τ too high; significant CA drops suggest τ too low; neither improves suggests defense ineffective
- First 3 experiments:
  1. Run PSJA attack on CIFAR-10 with τ=1.0 to establish baseline RA/CA values
  2. Test τ=0.8 with RND defense at ν=0.05 to observe RA improvement while maintaining CA
  3. Vary τ from 0.3 to 0.99 with fixed ν to find optimal threshold for maximum RA improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed confidence threshold τ affect the defense's performance against targeted attacks versus untargeted attacks?
- Basis in paper: [inferred] Paper focuses on untargeted attacks without explicit analysis of τ impact on targeted attacks
- Why unresolved: No experiments or analysis provided on τ performance against targeted attacks
- What evidence would resolve it: Experiments comparing defense performance against targeted attacks with varying τ values

### Open Question 2
- Question: What is the impact of confidence threshold τ on performance against sophisticated query-based attacks that adaptively estimate gradients or exploit geometric properties?
- Basis in paper: [explicit] Paper evaluates only PSJA and SurFree attacks, not more advanced adaptive attacks
- Why unresolved: Only considers standard decision-based attacks, not adaptive attackers
- What evidence would resolve it: Experiments against wider range of query-based attacks including gradient estimation and geometric exploitation

### Open Question 3
- Question: How does the proposed defense perform when attacker knows the defense mechanism and adapts strategy?
- Basis in paper: [inferred] Assumes limited knowledge attacker without addressing adaptive attackers aware of defense
- Why unresolved: Focuses on standard attack strategies without considering adaptive attackers
- What evidence would resolve it: Experiments simulating adaptive attackers who know defense mechanism

## Limitations

- The effectiveness relies heavily on the assumption that genuine inputs consistently have high confidence while adversarial samples explore low-confidence regions
- Performance against white-box attacks or non-query-based attack methods remains unexplored
- Computational overhead of confidence calculations and dynamic defense application during inference is not quantified

## Confidence

- High confidence: Empirical results showing CA-RA improvements against PSJA and SurFree attacks are well-supported
- Medium confidence: Theoretical justification for blocking low-confidence regions is logically sound but lacks extensive mathematical proof
- Medium confidence: Claim that training-free approach can outperform training-based defenses needs more rigorous comparison

## Next Checks

1. Test defense's effectiveness against non-query-based attacks (white-box, score-based) to establish broader attack surface coverage
2. Evaluate approach on additional model architectures beyond DenseNet-121 and ResNet-50 to verify generalizability
3. Quantify computational overhead introduced by confidence calculations and dynamic defense application during inference