---
ver: rpa2
title: 'FwdLLM: Efficient FedLLM using Forward Gradient'
arxiv_id: '2308.13894'
source_url: https://arxiv.org/abs/2308.13894
tags:
- fwdllm
- training
- learning
- mobile
- devices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of federated fine-tuning large language
  models (LLMs) on mobile devices, which is challenging due to memory constraints,
  lack of NPU support for training, and limited device scalability. The authors propose
  FwdLLM, a novel FL protocol that uses BP-free training via "perturbed inference"
  to significantly improve memory and time efficiency.
---

# FwdLLM: Efficient FedLLM using Forward Gradient

## Quick Facts
- arXiv ID: 2308.13894
- Source URL: https://arxiv.org/abs/2308.13894
- Authors: 
- Reference count: 40
- Primary result: Achieves up to 217.3× faster convergence and 14.6× memory reduction for federated fine-tuning of LLMs on mobile devices

## Executive Summary
This paper tackles the challenge of federated fine-tuning large language models (LLMs) on resource-constrained mobile devices. The authors propose FwdLLM, a novel federated learning protocol that leverages backpropagation-free training through "perturbed inference" to dramatically improve memory and time efficiency. By combining this with parameter-efficient fine-tuning methods and systematic computational load allocation, FwdLLM enables the first successful federated learning of billion-parameter LLMs like LLaMA on commercial mobile devices.

## Method Summary
FwdLLM introduces a backpropagation-free training approach using forward gradients computed via perturbed inference. The method generates random perturbations, performs two forward passes to compute directional derivatives, and aggregates these to approximate true gradients. This is combined with parameter-efficient fine-tuning (PEFT) methods like LoRA and Adapter to reduce the number of trainable parameters. The system implements adaptive perturbation pacing based on gradient variance across clients and uses discriminative perturbation sampling to filter low-value perturbations. These innovations collectively reduce memory requirements by eliminating the need to store intermediate values for backward passes while maintaining convergence quality.

## Key Results
- Achieves up to 217.3× faster convergence compared to conventional federated learning methods
- Reduces memory footprint by up to 14.6× on mobile devices
- Successfully enables federated fine-tuning of LLaMA-7B on commercial mobile devices
- Demonstrates consistent performance improvements across five transformer-based models and three NLP datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forward gradients can approximate true gradients without backpropagation, reducing memory usage.
- Mechanism: Forward gradient computes directional derivatives via two forward passes instead of storing activations for backward pass, eliminating need to store intermediate values.
- Core assumption: The approximation error is acceptable for convergence when combined with sufficient perturbation sampling.
- Evidence anchors:
  - [abstract] "require devices only to execute 'perturbed inferences'"
  - [section] "computing a forward gradient is equivalent to executing inferences twice"
  - [corpus] Weak evidence - corpus lacks papers specifically validating forward gradient convergence on LLMs
- Break condition: If approximation error becomes too large relative to true gradient direction, convergence fails or slows dramatically.

### Mechanism 2
- Claim: Combining BP-free training with parameter-efficient fine-tuning (PEFT) enables scaling to billion-parameter models.
- Mechanism: PEFT reduces trainable parameters, while BP-free methods reduce memory per trainable parameter, creating multiplicative efficiency gains.
- Core assumption: The memory/compute complexity of BP-free training scales with trainable parameter count, not total parameter count.
- Evidence anchors:
  - [section] "BP-free training complexity is primarily related to the trainable parameter size"
  - [section] "For the first time, FwdLLM integrates BP-free training with PEFT methods"
  - [corpus] Moderate evidence - corpus shows PEFT methods are being combined with federated learning but not BP-free methods specifically
- Break condition: If PEFT method introduces too much accuracy degradation or if BP-free approximation error overwhelms benefits.

### Mechanism 3
- Claim: Adaptive perturbation pacing based on gradient variance enables faster convergence than fixed perturbation counts.
- Mechanism: Variance across client gradients increases as model converges, requiring more perturbations for accurate estimation; system automatically increases perturbation count when variance exceeds threshold.
- Core assumption: Gradient variance across clients monotonically increases as model approaches convergence.
- Evidence anchors:
  - [section] "numerical variance across the forward gradients uploaded by clients increases as the model approaches convergence"
  - [section] "FwdLLM proposes a variance-controlled pacing mechanism"
  - [corpus] Weak evidence - corpus lacks papers specifically studying variance-based pacing in federated learning
- Break condition: If variance signal becomes unreliable or if adaptive pacing introduces instability in convergence.

## Foundational Learning

- Concept: Forward gradient computation
  - Why needed here: Understanding how directional derivatives replace backpropagation is essential for grasping FwdLLM's core innovation
  - Quick check question: How many forward passes are required to compute a forward gradient versus a backward gradient?

- Concept: Parameter-efficient fine-tuning methods
  - Why needed here: FwdLLM's ability to scale to billion-parameter models depends on combining PEFT with BP-free training
  - Quick check question: What is the typical percentage of trainable parameters in LoRA adapters versus full fine-tuning?

- Concept: Federated learning participant selection and scaling
  - Why needed here: FwdLLM's scalability advantage depends on effectively utilizing many more devices than traditional FL
  - Quick check question: Why does traditional FL saturate convergence speed with relatively few devices per round?

## Architecture Onboarding

- Component map:
  - Cloud aggregator: manages perturbation seeds, validates gradients, controls pacing via variance monitoring
  - Client devices: generate perturbations, perform perturbed inferences, compute forward gradients, upload results
  - Communication: seeds and model updates flow down, forward gradients flow up
  - Perturbation generator: creates perturbations with similarity-aware sampling to avoid orthogonal directions

- Critical path:
  1. Aggregator sends model + random seeds to clients
  2. Clients generate perturbations and perform perturbed inferences
  3. Clients compute forward gradients and upload to aggregator
  4. Aggregator validates variance, aggregates gradients, updates model
  5. Repeat until convergence

- Design tradeoffs:
  - More perturbations → better gradient approximation but higher compute cost
  - More devices → faster convergence but higher network load
  - Higher variance threshold → fewer rounds but potentially less accurate gradients
  - Sampling ratio → computational efficiency vs gradient quality

- Failure signatures:
  - High variance that never drops below threshold → pacing mechanism stuck, possibly due to poor perturbation quality
  - Oscillating accuracy → gradient estimates too noisy, may need more perturbations
  - Memory usage still too high → PEFT method not aggressive enough or perturbations not properly sized
  - Convergence extremely slow → approximation error too large, may need more sophisticated PEFT

- First 3 experiments:
  1. Implement basic forward gradient computation on a small BERT model and verify it approximates true gradients within acceptable error
  2. Test variance-controlled pacing by simulating client gradients with controlled variance and observing pacing behavior
  3. Implement discriminative perturbation sampling and measure improvement in gradient quality versus random sampling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between the number of perturbations and local data size for maximizing convergence speed in FwdLLM across different models and tasks?
- Basis in paper: [explicit] The paper mentions that "the optimal setting depends on the specific tasks and models" and that "the quantity of perturbations required to calculate a single forward gradient plays a pivotal role in determining convergence performance."
- Why unresolved: While the paper introduces a variance-controlled pacing mechanism, it does not provide a definitive formula or strategy for determining the optimal balance for different scenarios.
- What evidence would resolve it: Systematic experiments varying both the number of perturbations and local data size across a wide range of models and tasks, measuring convergence speed and accuracy, would help establish guidelines for optimal settings.

### Open Question 2
- Question: How does the performance of FwdLLM scale with model size beyond LLaMA-7B, particularly for trillion-parameter models?
- Basis in paper: [explicit] The paper states that "FwdLLM paves the way for federated learning of billion-parameter LLMs such as LLaMA on COTS mobile devices – a feat previously unattained," but does not explore scaling beyond 7B parameters.
- Why unresolved: The paper's experiments are limited to models up to LLaMA-7B, leaving uncertainty about the method's effectiveness for significantly larger models.
- What evidence would resolve it: Experiments applying FwdLLM to trillion-parameter models, measuring memory footprint, convergence speed, and accuracy, would provide insights into scalability limits.

### Open Question 3
- Question: How does the accuracy of FwdLLM's forward gradient estimation compare to backpropagation across different model architectures and training dynamics?
- Basis in paper: [inferred] The paper discusses the challenge that "existing studies primarily validate its utility for diminutive models" and that "with the increase of parameter size, the generated forward gradients deviate significantly from the true gradients."
- Why unresolved: While the paper demonstrates FwdLLM's effectiveness, it does not provide a comprehensive comparison of forward gradient accuracy versus backpropagation across diverse architectures and training scenarios.
- What evidence would resolve it: Systematic studies measuring the cosine similarity between forward and backpropagation gradients across various architectures, model sizes, and training stages would quantify the accuracy trade-offs.

## Limitations

- The variance-controlled pacing mechanism lacks precise threshold definitions and algorithmic specifications, making exact reproduction challenging
- The discriminative perturbation sampling's implementation, particularly cosine similarity computation, requires clarification
- The paper doesn't provide detailed memory breakdown analysis showing contributions from individual components
- Claims about scaling to billion-parameter models are demonstrated only on LLaMA-7B, not on larger models with more severe memory constraints

## Confidence

**High confidence**: The core mechanism of forward gradient computation through perturbed inference is mathematically sound and the memory efficiency claims are supported by the elimination of backward pass storage requirements. The combination of BP-free training with PEFT methods is a logical approach that should yield multiplicative efficiency gains.

**Medium confidence**: The adaptive variance-based pacing mechanism should improve convergence in theory, but the paper doesn't provide sufficient empirical evidence showing consistent superiority over fixed perturbation counts across different model sizes and datasets. The discriminative perturbation sampling's impact on gradient quality is plausible but not rigorously validated.

**Low confidence**: The scalability claims to billion-parameter models on commercial mobile devices, while theoretically supported by the component-level efficiency gains, lack comprehensive validation on larger models or with more diverse device capabilities.

## Next Checks

1. **Implement a minimal forward gradient prototype**: Create a small-scale implementation of forward gradient computation on a BERT-base model and measure approximation error compared to true gradients across different perturbation counts and model fine-tuning scenarios.

2. **Validate variance pacing behavior**: Simulate client gradient distributions with controlled variance profiles and implement the variance-controlled pacing algorithm to verify it responds appropriately to increasing variance and maintains convergence stability.

3. **Conduct ablation studies**: Run experiments isolating each FwdLLM component (BP-free training alone, PEFT alone, variance pacing alone, discriminative sampling alone) to quantify their individual contributions to convergence speed and memory reduction, providing clearer evidence for the claimed synergistic effects.