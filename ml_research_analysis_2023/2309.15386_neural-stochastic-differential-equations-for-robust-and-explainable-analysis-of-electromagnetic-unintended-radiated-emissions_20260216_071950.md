---
ver: rpa2
title: Neural Stochastic Differential Equations for Robust and Explainable Analysis
  of Electromagnetic Unintended Radiated Emissions
arxiv_id: '2309.15386'
source_url: https://arxiv.org/abs/2309.15386
tags:
- neural
- data
- noise
- robust
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates ResNet-like models for Unintended Radiated
  Emission (URE) classification and proposes Neural Stochastic Differential Equations
  (SDEs) to address identified limitations. The authors demonstrate that ResNet models
  are fragile to Gaussian noise perturbations, with F1-score dropping to near insignificance
  at 0.008 with a standard deviation of 0.5.
---

# Neural Stochastic Differential Equations for Robust and Explainable Analysis of Electromagnetic Unintended Radiated Emissions

## Quick Facts
- arXiv ID: 2309.15386
- Source URL: https://arxiv.org/abs/2309.15386
- Reference count: 5
- ResNet models drop to near insignificance at F1-score 0.008 with 0.5 standard deviation Gaussian noise

## Executive Summary
This paper addresses the challenge of classifying Unintended Radiated Emissions (URE) from electronic devices while maintaining robustness to noise and providing interpretable explanations. The authors demonstrate that traditional ResNet-like models are highly sensitive to Gaussian noise perturbations, with performance degrading significantly as noise increases. In contrast, they propose Neural Stochastic Differential Equations (SDEs) as a more robust alternative that maintains high accuracy even under substantial noise. The study shows that Neural SDE models not only perform better under noisy conditions but also generate more meaningful explanations that capture the inherent periodicity in URE data, which is critical for real-world applications where data is inherently noisy and assurance arguments demand interpretable machine learning predictions.

## Method Summary
The study compares ResNet-50 models with stochastic ResNet variants for classifying 18 different devices from the Flaming Moe dataset containing 43,200 short-term Fourier transform images. Both models are trained using Adam optimizer (learning rate 1e-5) for 10 epochs with CrossEntropy loss. The key innovation is replacing deterministic ResNet blocks with stochastic differential equation layers that model network dynamics with both drift and diffusion terms. The models are evaluated under Gaussian noise perturbations at three levels (0.1, 0.25, 0.5 standard deviation), and attributions are generated using Integrated Gradients with noise tunnel smoothing to assess explainability.

## Key Results
- ResNet models experience catastrophic performance degradation under noise, with F1-score dropping to near insignificance (0.008) at 0.5 standard deviation
- Neural SDE models maintain high F1-score (0.93) even under 0.5 standard deviation Gaussian noise
- Neural SDE attributions successfully recover time-invariant or periodic horizontal bands from input data, a feature absent in ResNet explanations
- The stochastic formulation allows the model to treat noise as a regular component of system dynamics rather than adversarial disruption

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural SDE models maintain higher F1-score under Gaussian noise because they inherently incorporate stochastic dynamics.
- Mechanism: By modeling the network as a stochastic differential equation, noise perturbations become part of the expected model dynamics rather than adversarial disruptions.
- Core assumption: The stochastic nature of the model allows it to better generalize to noisy inputs by treating noise as a regular component of the system.
- Evidence anchors:
  - Neural SDE models maintain a high F1-score of 0.93 even when exposed to Gaussian noise with a standard deviation of 0.5
  - dX(t) = G(X(t), W (t)) dt + σ(X(t), t) dB(t) (3) Here, the noise is depicted as a Brownian motion term B(t), scaled by a suitable diffusion coefficient σ(X(t), t).
  - Reconstruction and Prediction of Volterra Integral Equations Driven by Gaussian Noise

### Mechanism 2
- Claim: Neural SDE models provide more interpretable explanations because they recover the inherent periodicity in the data.
- Mechanism: The stochastic formulation allows the model to learn and represent the time-invariant or periodic horizontal bands that characterize stable device emissions.
- Core assumption: The data from stable devices has an inherent inductive bias toward periodicity that can be captured by stochastic models.
- Evidence anchors:
  - Neural SDE models successfully recover the time-invariant or periodic horizontal bands from the input data
  - These bands are surprisingly absent in the explanations provided by the ResNet-like models
  - Integrated Gradients (IG) [STY17] is an attribution method widely used for feature importance analysis in deep neural networks

### Mechanism 3
- Claim: The integrated gradients with noise tunnel method provides more robust attributions by reducing variability.
- Mechanism: Adding noise during attribution computation and averaging over multiple samples smooths the attributions, making them more reliable.
- Core assumption: Attribution methods based on gradients are sensitive to noise, and smoothing reduces this sensitivity.
- Evidence anchors:
  - We further used a Noise Tunnel [STK+17] with Integrated Gradients to generate smoother attributions and reduce variability in the attributions
  - SmoothGrad [STK+17] and Stochastic Differential Equations [JEVJ21] have led to more robust attributions with smaller sensitivity scores
  - Improving the Noise Estimation of Latent Neural Stochastic Differential Equations

## Foundational Learning

- Concept: Stochastic Differential Equations (SDEs)
  - Why needed here: Understanding how SDEs model both deterministic dynamics and stochastic perturbations is crucial for grasping how Neural SDE models work.
  - Quick check question: What are the two main components of a stochastic differential equation?

- Concept: Residual Neural Networks (ResNets)
  - Why needed here: Comparing the deterministic ResNet approach with the stochastic SDE approach highlights the benefits of incorporating stochasticity.
  - Quick check question: How does a ResNet block update its input?

- Concept: Integrated Gradients and Attribution Methods
  - Why needed here: These methods are used to generate explanations for model predictions, which is a key aspect of the study.
  - Quick check question: What are the core axioms that integrated gradients satisfy?

## Architecture Onboarding

- Component map:
  - Short-term Fourier transform preprocessing -> Neural SDE model -> Attribution generation with noise tunnel -> Performance evaluation under noise

- Critical path:
  1. Transform URE time-series data using short-term Fourier transform
  2. Feed transformed data into Neural SDE model
  3. Train model with appropriate loss function
  4. Evaluate robustness under Gaussian noise perturbations
  5. Generate attributions using integrated gradients with noise tunnel

- Design tradeoffs:
  - Deterministic vs. stochastic modeling: Stochastic models are more robust but potentially harder to train
  - Model complexity: Neural SDEs may require more computational resources
  - Attribution clarity: Noise tunnel smoothing can reduce detail in attributions

- Failure signatures:
  - High sensitivity to noise: Indicates need for more robust modeling
  - Lack of periodic patterns in attributions: Suggests model not capturing data inductive bias
  - Poor generalization to unseen noise levels: Model may be overfitting to specific noise characteristics

- First 3 experiments:
  1. Train a ResNet model on URE data and evaluate F1-score under increasing Gaussian noise levels
  2. Implement a basic Neural SDE model and compare robustness to the ResNet model
  3. Generate attributions using integrated gradients with and without noise tunnel for both models

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but identifies several areas for future work, including extending the approach to other types of noise distributions, investigating the impact of model complexity on robustness and explainability, and testing generalization to unseen devices or new classes in the URE domain.

## Limitations
- Evaluation relies on a single dataset (Flaming Moe), limiting generalizability to other URE scenarios
- Noise injection mechanism for the stochastic ResNet variant is not fully specified, making exact replication challenging
- Attribution methods' hyperparameters (particularly noise tunnel settings) could significantly impact interpretability results

## Confidence
- ResNet fragility to noise: High confidence (supported by clear quantitative comparison)
- Neural SDE robustness: Medium confidence (strong results but limited to one dataset)
- Attribution interpretability: Low confidence (qualitative claims with limited quantitative validation)

## Next Checks
1. Replicate the Gaussian noise robustness test on an independent URE dataset to verify generalizability
2. Conduct ablation studies varying noise tunnel parameters to quantify their impact on attribution quality
3. Test model performance with non-Gaussian noise distributions to assess real-world applicability