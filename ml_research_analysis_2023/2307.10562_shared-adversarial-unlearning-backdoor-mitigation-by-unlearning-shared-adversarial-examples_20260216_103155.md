---
ver: rpa2
title: 'Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial
  Examples'
arxiv_id: '2307.10562'
source_url: https://arxiv.org/abs/2307.10562
tags:
- adversarial
- backdoor
- risk
- r-acc
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses backdoor defense in deep learning models by
  introducing a novel method called Shared Adversarial Unlearning (SAU). SAU is based
  on the insight that backdoor risk is closely related to shared adversarial examples
  (SAEs) between the poisoned model and the fine-tuned model.
---

# Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples

## Quick Facts
- arXiv ID: 2307.10562
- Source URL: https://arxiv.org/abs/2307.10562
- Reference count: 40
- Key outcome: SAU achieves state-of-the-art performance for backdoor defense, significantly lowering attack success rates while maintaining high clean accuracy

## Executive Summary
This paper introduces Shared Adversarial Unlearning (SAU), a novel backdoor defense method that leverages shared adversarial examples (SAEs) to mitigate backdoor effects in deep learning models. The key insight is that backdoor risk can be upper bounded by the risk on SAEs between poisoned and purified models. SAU formulates this relationship as a bi-level optimization problem, first generating SAEs and then unlearning them to break the connection between poisoned samples and target labels. Extensive experiments on CIFAR-10, Tiny ImageNet, and GTSRB demonstrate that SAU outperforms existing backdoor defense methods, achieving significantly lower attack success rates while maintaining high accuracy on clean data.

## Method Summary
SAU is based on a bi-level optimization framework that first generates shared adversarial examples between the poisoned model and the purified model, then unlearning these SAEs through adversarial training. The method uses Projected Gradient Descent (PGD) to generate SAEs within an L∞ norm bound of 0.2, and then optimizes the purified model to either correctly classify these SAEs or misclassify them differently than the poisoned model. The approach balances clean accuracy and backdoor risk mitigation through carefully tuned hyper-parameters (λ1, λ2, λ3, λ4) in the optimization objective. SAU is applied for 100 epochs on CIFAR-10/GTSRB or 20 epochs on Tiny ImageNet using a pre-trained poisoned model and 5% of the training data as clean data for fine-tuning.

## Key Results
- SAU achieves significantly lower attack success rates (ASR) compared to existing backdoor defense methods across multiple datasets
- Maintains high clean accuracy (ACC) on benign data while effectively mitigating backdoor effects
- Demonstrates state-of-the-art performance in terms of Defense Effectiveness Rating (DER)
- Effectively recovers predictions on poisoned samples, breaking the backdoor trigger-label association

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backdoor risk can be upper bounded by the shared adversarial risk between the poisoned and purified models.
- Mechanism: Shared adversarial examples (SAEs) are adversarial examples that both the poisoned model and the purified model misclassify to the same wrong class. By unlearning these SAEs, the backdoor effect can be mitigated.
- Core assumption: SAEs are the primary source of backdoor risk, and reducing their impact will reduce backdoor risk.
- Evidence anchors: [abstract]: "By establishing the connection between backdoor risk and adversarial risk, we derive a novel upper bound for backdoor risk, which mainly captures the risk on the shared adversarial examples (SAEs) between the backdoored model and the purified model."
- Break condition: If the trigger norm is larger than the perturbation set used for SAE generation, SAEs may not capture the backdoor behavior effectively.

### Mechanism 2
- Claim: Unlearning SAEs ensures they are either correctly classified by the purified model or differently classified by the two models.
- Mechanism: By optimizing the purified model to either correctly classify SAEs or misclassify them differently than the poisoned model, the connection between poisoned samples and target labels is broken.
- Core assumption: The purified model can learn to distinguish SAEs from poisoned samples effectively.
- Evidence anchors: [abstract]: "SAU first generates SAEs, and then, unlearns the generated SAEs such that they are either correctly classified by the purified model and/or differently classified by the two models, such that the backdoor effect in the backdoored model will be mitigated in the purified model."
- Break condition: If the SAEs are too complex or numerous, the purified model may not be able to unlearn them effectively within the given training constraints.

### Mechanism 3
- Claim: The proposed bi-level optimization problem effectively balances clean accuracy and backdoor risk mitigation.
- Mechanism: The bi-level optimization problem first generates SAEs through an inner maximization step and then unlearns them through an outer minimization step, balancing the trade-off between clean accuracy and backdoor risk.
- Core assumption: The bi-level optimization problem can be solved efficiently and effectively with the proposed SAU algorithm.
- Evidence anchors: [abstract]: "This upper bound further suggests a novel bi-level optimization problem for mitigating backdoor using adversarial training techniques. To solve it, we propose Shared Adversarial Unlearning (SAU)."
- Break condition: If the hyper-parameters (λ1, λ2, λ3, λ4) are not set correctly, the optimization problem may not converge to a solution that effectively balances clean accuracy and backdoor risk mitigation.

## Foundational Learning

- Concept: Adversarial training
  - Why needed here: SAU is based on adversarial training techniques to generate SAEs and unlearn them.
  - Quick check question: What is the main goal of adversarial training in the context of backdoor defense?

- Concept: Backdoor attacks
  - Why needed here: Understanding backdoor attacks is crucial for comprehending the threat model and the need for backdoor defense methods like SAU.
  - Quick check question: How do backdoor attacks manipulate the training process to inject backdoors into models?

- Concept: Optimization problems
  - Why needed here: SAU formulates a bi-level optimization problem to balance clean accuracy and backdoor risk mitigation.
  - Quick check question: What is the difference between a bi-level optimization problem and a standard optimization problem?

## Architecture Onboarding

- Component map: SAE generation -> SAE unlearning -> model fine-tuning
- Critical path: The critical path is SAE generation → SAE unlearning → model fine-tuning, as each step depends on the previous one.
- Design tradeoffs: The main design tradeoff is between clean accuracy and backdoor risk mitigation, which is balanced by the hyper-parameters in the bi-level optimization problem.
- Failure signatures: If SAEs are not generated effectively, the backdoor risk may not be mitigated. If SAEs are not unlearned effectively, the clean accuracy may suffer.
- First 3 experiments:
  1. Test SAU on a simple dataset (e.g., MNIST) with a single backdoor attack to verify the basic functionality.
  2. Test SAU on a more complex dataset (e.g., CIFAR-10) with multiple backdoor attacks to evaluate its effectiveness in a more realistic setting.
  3. Test SAU with different hyper-parameter configurations to find the optimal balance between clean accuracy and backdoor risk mitigation.

## Open Questions the Paper Calls Out

The paper mentions that SAU can be extended to handle backdoor attacks with multiple triggers and multiple targets by modifying the sub-adversarial risk definition, but does not provide experimental results or detailed analysis of this extension's effectiveness.

## Limitations

- SAU's performance degrades when facing backdoor attacks with trigger norms larger than the L∞ norm bound of 0.2 used for SAE generation
- The method requires 5% of clean training data for fine-tuning, which may be impractical in scenarios with limited clean data availability
- The effectiveness against adaptive backdoor attacks specifically designed to evade SAE-based defenses remains unexplored

## Confidence

- High Confidence: The theoretical upper bound for backdoor risk and its relationship to SAEs
- Medium Confidence: Empirical results showing state-of-the-art performance across multiple datasets
- Low Confidence: Claims about effectiveness against adaptive backdoor attacks with large triggers

## Next Checks

1. **Perturbation Analysis**: Test SAU with backdoor triggers exceeding the L∞ norm bound of 0.2 to verify robustness to larger perturbations
2. **Data Efficiency**: Evaluate SAU's performance with varying amounts of clean data (1%, 3%, 10%) to determine minimum requirements
3. **Adaptive Attack Testing**: Implement and test against backdoor attacks specifically designed to evade SAE-based defenses, such as dynamic trigger patterns or distributed triggers