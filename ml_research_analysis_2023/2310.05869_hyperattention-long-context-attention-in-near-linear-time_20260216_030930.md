---
ver: rpa2
title: 'HyperAttention: Long-context Attention in Near-Linear Time'
arxiv_id: '2310.05869'
source_url: https://arxiv.org/abs/2310.05869
tags:
- matrix
- attention
- algorithm
- entries
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces HyperAttention, an efficient approximate
  attention mechanism designed to address computational challenges posed by long contexts
  in large language models (LLMs). The method focuses on two key parameters: the maximum
  column norm in the normalized attention matrix and the ratio of row norms in the
  unnormalized attention matrix after detecting and removing large entries.'
---

# HyperAttention: Long-context Attention in Near-Linear Time

## Quick Facts
- arXiv ID: 2310.05869
- Source URL: https://arxiv.org/abs/2310.05869
- Reference count: 40
- Key outcome: HyperAttention achieves 50% faster inference on ChatGLM2 for 32k context and 5x speedup on single attention layer for 131k context while maintaining perplexity close to original models

## Executive Summary
HyperAttention introduces an efficient approximate attention mechanism for long-context language models that achieves near-linear time complexity. The method uses Locality Sensitive Hashing (LSH) to identify large entries in the attention matrix, then removes them and performs uniform sampling on the remaining small entries. This approach avoids the computational overhead of kernel density estimation while maintaining strong spectral approximation guarantees. The algorithm is particularly effective when two key parameters - the maximum column norm in the normalized attention matrix and the ratio of row norms after removing large entries - are small.

## Method Summary
HyperAttention works by first identifying dominant entries in the attention matrix using Hamming sorted LSH, then removing these entries to create a mask. The algorithm approximates the diagonal scaling matrix using uniform sampling combined with the mask, eliminating the need for importance sampling based on kernel density estimation. For causal masking, it recursively partitions the attention matrix into smaller subproblems. The final output is computed using a combination of the approximated diagonal matrix and a sampling matrix based on squared row norms of the value matrix. This approach achieves spectral approximation guarantees while maintaining near-linear time complexity when the two key parameters are small.

## Key Results
- 50% faster inference time on ChatGLM2 for 32k context length
- 5-fold speedup on a single attention layer for 131k context length with causal masking
- Maintains perplexity close to original models without requiring fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HyperAttention achieves near-linear time complexity for attention approximation even with unbounded attention matrix entries, provided certain column norm and row norm ratio parameters are small.
- Mechanism: The method first identifies large entries in the attention matrix using Locality Sensitive Hashing (LSH), then removes them. The remaining attention matrix has small column norms and row norm ratios, enabling efficient uniform sampling instead of complex kernel density estimation (KDE).
- Core assumption: After removing large entries, the max column norm in the normalized attention matrix and the ratio of row norms in the unnormalized attention matrix are both small enough to enable efficient approximation.
- Evidence anchors:
  - [abstract]: "We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries."
  - [section]: "We assume these procedures are fast, and that after removing the heavy entries, two parameters in the resulting attention matrix are small: (1) the max column ℓ1-norm, and (2) the ratio of row norms in the un-normalized attention matrix."
- Break condition: If the attention matrix has many large entries distributed in a way that prevents both parameters from being small, the algorithm's efficiency guarantees break down.

### Mechanism 2
- Claim: The spectral approximation of the diagonal matrix D can be achieved through uniform sampling of columns combined with LSH-based identification of large entries.
- Mechanism: The algorithm uses LSH to find a mask of large entries, then samples a small subset of columns uniformly. This combination provides a spectral guarantee on the estimated diagonal matrix without needing importance sampling based on KDE.
- Core assumption: Uniform sampling of columns is sufficient to achieve the desired spectral approximation when combined with a mask that captures the large entries.
- Evidence anchors:
  - [section]: "We demonstrate that uniform sampling is sufficient to achieve the desired spectral guarantee, eliminating the need for importance sampling based on kernel densities."
  - [section]: "Our approach involves designing an efficient estimator for the diagonal scaling matrix D in near-linear time."
- Break condition: If the attention matrix has a very uneven column norm distribution that uniform sampling cannot capture effectively, the spectral approximation may fail.

### Mechanism 3
- Claim: Causal masking can be handled efficiently through recursive partitioning of the attention matrix into smaller causal attention subproblems.
- Mechanism: The causal attention matrix is decomposed into three non-zero sections: two smaller causal attention matrices (handled recursively) and one unmasked attention matrix (handled directly). This allows the algorithm to maintain efficiency while supporting causal masking.
- Core assumption: The causal attention matrix can be effectively decomposed into smaller causal attention subproblems plus an unmasked component that can be processed directly.
- Evidence anchors:
  - [section]: "To handle these, we apply a recursive approach and further partition them into smaller blocks, and repeat this procedure."
  - [section]: "The masked attention M C ⊙ A can be decomposed into three non-zero matrices, each of which has half the size of the original attention matrix."
- Break condition: If the recursive decomposition leads to subproblems that still have large entries or poor parameter distributions, the efficiency benefits may be lost.

## Foundational Learning

- Concept: Locality Sensitive Hashing (LSH)
  - Why needed here: LSH is used to identify large entries in the attention matrix efficiently, which is crucial for the first phase of HyperAttention's algorithm.
  - Quick check question: How does LSH help in identifying large entries in the attention matrix, and what property of the hash function makes this possible?

- Concept: Spectral approximation and operator norm
  - Why needed here: The algorithm provides guarantees in terms of spectral approximation (operator norm), which is the standard way to measure the quality of matrix approximations in this context.
  - Quick check question: What does it mean for one matrix to be a spectral approximation of another, and how is this different from element-wise approximation?

- Concept: Stable rank of matrices
  - Why needed here: The sampling complexity for approximating the matrix product depends on the stable rank of the softmax matrix, which measures the "intrinsic dimensionality" of the matrix.
  - Quick check question: How does the stable rank of a matrix relate to its sampling complexity in approximate matrix multiplication, and why is this important for HyperAttention?

## Architecture Onboarding

- Component map:
  Input: Query (Q), Key (K), Value (V) matrices of size n×d
  LSH component: Identifies large entries in attention matrix using Hamming sorted LSH
  Masking component: Creates mask M_H of large entries
  Diagonal approximation: Approximates diagonal matrix D using uniform sampling and mask
  Sampling component: Creates sampling matrix S based on squared row norms of V
  Output: Approximate attention matrix

- Critical path:
  1. Compute attention matrix A = exp(QK⊤)
  2. Use LSH to find large entries and create mask M_H
  3. Approximate diagonal matrix D using uniform sampling and mask
  4. Create sampling matrix S based on squared row norms of V
  5. Compute final output: eD⁻¹AS⊤ · SV

- Design tradeoffs:
  - Parameter choice: The algorithm's efficiency depends on two key parameters (max column norm and row norm ratio) being small. If these are large, the algorithm becomes less efficient.
  - Sampling vs. accuracy: More samples (larger m) improve accuracy but reduce speedup.
  - LSH precision: More precise LSH (larger r) finds more large entries but increases computational cost.

- Failure signatures:
  - Poor speedup: If the two key parameters are not small, the algorithm may not achieve near-linear time.
  - Accuracy degradation: If the uniform sampling misses important columns, perplexity may increase significantly.
  - Memory issues: If the mask M_H becomes too dense (many large entries), the algorithm may approach quadratic complexity.

- First 3 experiments:
  1. Measure the two key parameters (max column norm and row norm ratio) on a pretrained model's attention matrices to verify they are small.
  2. Compare perplexity and speedup on a long-context benchmark (like LongBench) with varying numbers of layers replaced by HyperAttention.
  3. Test causal masking performance by measuring speedup on sequences with and without causal masking, and verify the recursive decomposition works correctly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HyperAttention scale with increasing context length beyond 131k tokens?
- Basis in paper: [inferred] The paper mentions empirical results up to 131k context length but does not explore longer sequences.
- Why unresolved: The paper does not provide theoretical bounds on performance scaling for context lengths beyond what was tested.
- What evidence would resolve it: Additional experiments with even longer context lengths (e.g., 262k, 524k tokens) would demonstrate how the speedup and perplexity degradation scale with sequence length.

### Open Question 2
- Question: Can HyperAttention be effectively combined with other attention approximation techniques like low-rank or sparse methods?
- Basis in paper: [inferred] The paper mentions that HyperAttention can integrate with other fast low-level implementations but does not explore combinations with other approximation methods.
- Why unresolved: The paper focuses on HyperAttention's performance in isolation and does not investigate potential synergies with complementary techniques.
- What evidence would resolve it: Experiments combining HyperAttention with low-rank or sparse attention methods would show if such combinations yield better performance than either approach alone.

### Open Question 3
- Question: How does HyperAttention perform on tasks with very long-range dependencies, such as multi-document question answering with hundreds of pages?
- Basis in paper: [inferred] The paper evaluates on LongBench datasets but does not specifically test scenarios with extremely long-range dependencies across multiple large documents.
- Why unresolved: The existing experiments use trimmed sequences and may not capture the full complexity of tasks requiring information from very distant parts of the input.
- What evidence would resolve it: Testing HyperAttention on tasks involving extremely long documents (e.g., book-length texts) would reveal its effectiveness in handling long-range dependencies.

## Limitations

- The theoretical efficiency guarantees depend on two key parameters being small, but the paper doesn't provide comprehensive empirical validation that these parameters are consistently small across different model architectures and attention heads.
- The recursive partitioning approach for causal masking lacks detailed complexity analysis, raising concerns about whether efficiency gains are maintained throughout the recursion.
- Limited evaluation on tasks with extremely long-range dependencies and very long documents (beyond 131k tokens) leaves uncertainty about performance in real-world long-context scenarios.

## Confidence

**High Confidence**: The empirical speedup results (50% faster inference on ChatGLM2, 5x speedup on single attention layer) are well-supported by the experimental results section. The methodology for measuring runtime is clear and the results are consistent across different benchmarks.

**Medium Confidence**: The theoretical framework for near-linear time complexity is mathematically sound, but the practical applicability depends on the two key parameters being small in real models. While the paper demonstrates good results, the connection between theory and practice needs more empirical validation.

**Low Confidence**: The claims about maintaining perplexity close to original models without fine-tuning are supported by limited experiments. The paper only tests on a few specific models and doesn't explore how performance varies across different model architectures or training regimes.

## Next Checks

1. **Parameter Distribution Analysis**: Measure the two key parameters (max column norm and row norm ratio) across all attention heads in multiple pretrained models (including different architectures like BERT, GPT, and LLaMA) to verify they are consistently small enough for HyperAttention to achieve near-linear time.

2. **Recursive Partitioning Complexity**: Implement instrumentation to track the number of recursive calls and the parameter values at each recursion level when processing causal attention. This will verify whether the decomposition maintains the required bounds throughout the computation.

3. **LSH Parameter Sensitivity**: Conduct a systematic study varying LSH parameters (number of hash functions, bucket size) and measure their impact on both the quality of large entry detection and overall runtime performance. This will help establish guidelines for parameter selection in different scenarios.