---
ver: rpa2
title: A Contrastive Learning Scheme with Transformer Innate Patches
arxiv_id: '2303.14806'
source_url: https://arxiv.org/abs/2303.14806
tags:
- contrastive
- learning
- transformer
- segmentation
- patches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Contrastive Transformer (CT), a patch-based
  contrastive learning scheme that leverages the innate patch architecture of vision
  transformers to improve semantic segmentation performance. CT performs supervised
  patch-level contrastive learning using ground truth masks to select patches for
  hard-negative and hard-positive sampling, eliminating the need for large batch sizes
  since each patch is treated as an image.
---

# A Contrastive Learning Scheme with Transformer Innate Patches

## Quick Facts
- arXiv ID: 2303.14806
- Source URL: https://arxiv.org/abs/2303.14806
- Reference count: 23
- Primary result: Patch-based contrastive learning using transformer innate patches improves semantic segmentation mean IoU across multiple transformer architectures

## Executive Summary
This paper introduces Contrastive Transformer (CT), a patch-based contrastive learning scheme that leverages the innate patch architecture of vision transformers to improve semantic segmentation performance. CT performs supervised patch-level contrastive learning using ground truth masks to select patches for hard-negative and hard-positive sampling, eliminating the need for large batch sizes since each patch is treated as an image. The method is model-agnostic, easy to implement, and introduces minimal memory overhead.

Evaluated on the ISPRS Potsdam aerial image segmentation dataset, CT consistently improves mean IoU across all classes when applied to multiple transformer architectures including DCSwin, UnetFormer, and DCPoolFormer, demonstrating both effectiveness and generalizability. The results show that CT enhances semantic representation even for classes smaller than the patch size by including them in negative samples.

## Method Summary
CT implements supervised patch-level contrastive learning by extracting patches from transformer encoder outputs and using ground truth masks to select positive (homogeneous class distribution) and negative (heterogeneous distribution without target class) patches. The method uses parallel projection heads to generate n-dimensional patch representations for contrastive learning while maintaining standard segmentation heads. Training combines contrastive losses (InfoNCE and CL) with segmentation losses (soft cross-entropy and dice loss) using AdamW optimizer with learning rate 8e-5, batch size 32, and 50 epochs. The approach is model-agnostic and applies to multiple transformer architectures without architectural modifications.

## Key Results
- CT consistently improves mean IoU across all classes on ISPRS Potsdam dataset when applied to DCSwin, UnetFormer, and DCPoolFormer architectures
- CT enhances semantic representation even for classes smaller than patch size by including them in negative samples
- CT removes the need for large batch sizes by treating each patch as an individual image for contrastive learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Patch-based contrastive learning improves semantic representation even for classes smaller than the patch size by including them in negative samples.
- Mechanism: Small class instances (e.g., cars) are not used as positive samples due to their size, but their inclusion in negative samples still contributes to learning better discriminative features for the class.
- Core assumption: Negative sampling can improve representation of classes even if they are not present as positive samples.
- Evidence anchors:
  - [abstract]: "The results show that CT enhances semantic representation even for classes smaller than the patch size by including them in negative samples."
  - [section]: "CT is still able to perform better than the baseline for this class, indicating that it is sufficient for the class to be present in negative samples to create a better semantic representation than the baseline."
  - [corpus]: Weak/no direct match; this appears to be a novel finding specific to the paper.
- Break condition: If negative samples containing the target class do not improve the model's ability to segment that class.

### Mechanism 2
- Claim: Treating each patch as an image removes the need for huge batch sizes in contrastive learning.
- Mechanism: By treating patches as individual images, the contrastive loss can be computed using the existing batch of patches without requiring an external memory bank or large batch sizes to collect sufficient samples.
- Core assumption: The number of patches per image is sufficient to provide diverse positive and negative samples for effective contrastive learning.
- Evidence anchors:
  - [abstract]: "Additionally, the scheme removes the need for huge batch sizes, as each patch is treated as an image."
  - [section]: "Therefore we're not required to use a large batch size to collect a sufficient amount of samples for contrastive learning."
  - [corpus]: Weak/no direct match; the concept is specific to the CT approach.
- Break condition: If the number of patches per image is insufficient to provide meaningful contrastive pairs.

### Mechanism 3
- Claim: Supervised patch-level contrastive learning using ground truth masks selects patches for hard-negative and hard-positive sampling, improving dense prediction tasks.
- Mechanism: Ground truth masks guide the selection of patches with homogenous class distributions for positive samples and heterogeneous distributions (excluding the target class) for negative samples, enabling supervised contrastive learning.
- Core assumption: Ground truth masks provide reliable guidance for selecting informative patches for contrastive learning.
- Evidence anchors:
  - [abstract]: "The scheme performs supervised patch-level contrastive learning, selecting the patches based on the ground truth mask, subsequently used for hard-negative and hard-positive sampling."
  - [section]: "For each class c in G, we sample positive patches Ps and negative patches Ns from Fs. Ps are sampled from Fs where Gs have a homogenous class distribution of class c. Similarly, Ns are sampled from Fs where class c is not in Gs."
  - [corpus]: Weak/no direct match; the specific use of ground truth masks for patch selection is unique to this work.
- Break condition: If ground truth masks are noisy or inaccurate, leading to poor patch selection for contrastive learning.

## Foundational Learning

- Concept: Vision Transformer (ViT) patch embeddings
  - Why needed here: CT relies on the innate patch architecture of vision transformers to perform contrastive learning at the patch level.
  - Quick check question: What is the size of the patches used in the ViT backbone, and how does it relate to the smallest class instances in the dataset?

- Concept: Contrastive learning loss functions (e.g., InfoNCE)
  - Why needed here: CT uses contrastive loss functions to pull positive samples closer and push negative samples apart in the representation space.
  - Quick check question: How does the InfoNCE loss function compute the similarity between positive and negative samples, and what is the role of temperature in the loss?

- Concept: Semantic segmentation evaluation metrics (e.g., mean IoU)
  - Why needed here: CT is evaluated on its ability to improve mean IoU for semantic segmentation tasks.
  - Quick check question: How is mean IoU calculated, and why is it a suitable metric for evaluating the performance of semantic segmentation models?

## Architecture Onboarding

- Component map:
  Transformer backbone (e.g., Swin, PoolFormer) -> Feature extraction -> Multi-stage feature extraction -> Feature patches + ground truth patches -> Patch selection (positive and negative samples) -> Patch representations + contrastive loss -> Contrastive learning feedback -> Decoder + segmentation loss -> Semantic segmentation output

- Critical path:
  1. Input image → Transformer backbone → Multi-stage feature extraction
  2. Feature patches + ground truth patches → Patch selection (positive and negative samples)
  3. Patch representations + contrastive loss → Contrastive learning feedback
  4. Decoder + segmentation loss → Semantic segmentation output
  5. Joint optimization of contrastive and segmentation losses

- Design tradeoffs:
  - Patch size vs. smallest class instance size: Smaller patches allow for more precise positive sampling but increase computational cost.
  - Number of patches vs. batch size: More patches per image reduce the need for large batch sizes but may lead to redundant contrastive pairs.
  - Complexity of patch selection strategy: Simple random sampling is easy to implement but may miss informative hard samples.

- Failure signatures:
  - Degraded segmentation performance: If contrastive learning is not properly balanced with segmentation loss, it may lead to overfitting or underfitting.
  - Unstable training: If the number of positive and negative samples is imbalanced or if the contrastive loss dominates the segmentation loss, it may cause training instability.
  - Poor generalization: If the patch selection strategy is too specific to the training data, it may lead to poor performance on unseen data.

- First 3 experiments:
  1. Implement CT with a simple random patch selection strategy and InfoNCE loss on a small subset of the ISPRS Potsdam dataset. Evaluate the impact on mean IoU for a single class (e.g., buildings).
  2. Compare the performance of CT with different patch sizes (e.g., 8x8, 16x16, 32x32) on the full ISPRS Potsdam dataset. Analyze the tradeoff between patch size and segmentation accuracy.
  3. Investigate the effect of different patch selection strategies (e.g., hard negative mining, class-balanced sampling) on the performance of CT. Evaluate the impact on mean IoU for all classes in the ISPRS Potsdam dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal patch size and number of patches to use in contrastive learning for different dense prediction tasks?
- Basis in paper: [inferred] The paper mentions that future work should investigate the impact of patch size and number of patches on CT's performance, suggesting this relationship is not yet fully understood.
- Why unresolved: The paper does not provide empirical evidence or theoretical analysis of how different patch sizes and quantities affect the contrastive learning outcomes across various tasks and architectures.
- What evidence would resolve it: Systematic experiments varying patch sizes and numbers while measuring performance on multiple dense prediction benchmarks would clarify optimal configurations.

### Open Question 2
- Question: How do different transformer backbone architectures interact with the Contrastive Transformer scheme in terms of performance and representation learning?
- Basis in paper: [explicit] The paper notes that CT pairs better with PoolFormer than Swin Transformer, but states this needs further research to understand why and whether this pattern generalizes.
- Why unresolved: The experiments only compared two architectures, and the paper acknowledges that more research is needed to understand the underlying reasons for performance differences.
- What evidence would resolve it: Extensive experiments testing CT across diverse transformer architectures (ConvNeXt, ViT, etc.) while analyzing learned representations would identify patterns and explanations.

### Open Question 3
- Question: Can heterogeneous class distribution patches (containing both positive and negative samples) be effectively utilized in contrastive learning to improve segmentation performance?
- Basis in paper: [explicit] The paper identifies this as a future research direction, noting that hard samples often exist at class boundaries and could potentially improve results.
- Why unresolved: The current implementation only uses homogeneous patches for positive samples and heterogeneous patches only for negatives, leaving the potential benefits of using boundary patches unexplored.
- What evidence would resolve it: Experiments implementing strategies to incorporate boundary patches into contrastive learning while comparing performance against the current approach would demonstrate whether this improves results.

## Limitations

- Limited evaluation to single aerial imagery dataset (ISPRS Potsdam), raising questions about generalizability to other domains
- Specific patch sampling strategy details are not fully specified, creating uncertainty in implementation
- Performance improvement claims require further validation through ablation studies to isolate contrastive learning contribution

## Confidence

- High confidence: The core mechanism of using transformer innate patches for contrastive learning and the elimination of large batch size requirements are well-supported by the experimental results.
- Medium confidence: The improvement in mean IoU across all classes is demonstrated, but the specific contribution of contrastive learning versus other factors (like model architecture) requires further isolation.
- Low confidence: The claim that negative sampling alone can improve representation for classes smaller than patch size needs more rigorous validation across diverse datasets and object scales.

## Next Checks

1. Implement ablation studies to isolate the contribution of contrastive learning by comparing CT with and without the contrastive component while keeping all other factors constant.

2. Test CT on additional semantic segmentation datasets beyond ISPRS Potsdam, particularly those with diverse object scales and classes smaller than typical patch sizes.

3. Conduct experiments varying the patch selection strategy parameters (number of positive/negative patches per class) to determine optimal sampling configurations and robustness to parameter changes.