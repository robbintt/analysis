---
ver: rpa2
title: Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation
arxiv_id: '2307.13412'
source_url: https://arxiv.org/abs/2307.13412
tags: []
core_contribution: This paper addresses the performance bottlenecks in FPGA-based
  CNN accelerators, specifically memory-bound layers and resource underutilization
  due to suboptimal layer mapping. The authors propose unzipFPGA, a framework that
  introduces on-the-fly weights generation using orthogonal variable spreading factor
  (OVSF) codes.
---

# Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation

## Quick Facts
- arXiv ID: 2307.13412
- Source URL: https://arxiv.org/abs/2307.13412
- Reference count: 40
- Primary result: Achieves 2.57x performance efficiency gain over GPU designs and 3.94x higher performance density compared to state-of-the-art FPGA-based CNN accelerators

## Executive Summary
This paper addresses performance bottlenecks in FPGA-based CNN accelerators by introducing on-the-fly weights generation using orthogonal variable spreading factor (OVSF) codes. The proposed unzipFPGA framework dynamically constructs CNN weights on-chip during runtime, alleviating memory bandwidth constraints that plague memory-bound layers. By combining this weights generation approach with input selective processing elements for load balancing and hardware-aware tuning of OVSF ratios, the framework achieves significant performance improvements over both GPU and existing FPGA implementations while maintaining minimal accuracy degradation.

## Method Summary
The method converts standard CNNs to OVSF-CNNs by training with PyTorch, then deploying on FPGA with a custom CNN engine that includes a weights generator module. This module uses OVSF codes and learned coefficients to construct weight tiles dynamically on-chip, replacing memory transactions with on-chip computation. The framework includes input selective PEs that enable load balancing through inter-PE work-stealing, and employs hardware-aware tuning to optimize per-layer OVSF compression ratios based on bottleneck analysis. The approach is validated on ResNet and SqueezeNet architectures using the ImageNet dataset on Xilinx ZC706 and ZCU104 FPGA platforms.

## Key Results
- Average 2.57x performance efficiency gain over optimized GPU designs
- Up to 3.94x higher performance density compared to state-of-the-art FPGA-based CNN accelerators
- Maintains accuracy with less than 1 percentage point drop across tested models
- Successfully mitigates memory wall effects through on-the-fly weights generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: On-the-fly weights generation alleviates memory bandwidth bottlenecks by constructing CNN weights dynamically on-chip rather than fetching them from external memory.
- Mechanism: The CNN-WGen module generates weight tiles using orthogonal variable spreading factor (OVSF) codes combined with learned coefficients. This replaces memory transactions with on-chip computation, matching the rate of vector units through rate-matching techniques.
- Core assumption: The computational overhead of generating weights on-chip is less than the memory access latency and bandwidth costs of fetching pre-computed weights.
- Evidence anchors:
  - [abstract] "The proposed framework comprises a novel CNN hardware architecture that introduces a weights generator module that enables the on-chip on-the-fly generation of weights, alleviating the negative impact of limited bandwidth on memory-bound layers."
  - [section 4.2.2] "Bringing forth on-the-fly weights generation requires devising two major components: 1) Tiled Weights Generation... 2) Weights Generator Microarchitecture."
- Break condition: If the weights generation computational latency exceeds the memory access time, or if the OVSF code storage and computation consume excessive on-chip resources.

### Mechanism 2
- Claim: Input selective PEs balance load in underutilised layers by enabling work-stealing between adjacent processing elements.
- Mechanism: Underutilised PEs are augmented with lightweight switches that allow them to receive weights from neighbouring PEs when their own weight slots are empty, effectively unrolling the output dimension and distributing work more evenly.
- Core assumption: Suboptimal PE utilisation in certain layers creates opportunities for load balancing without significant hardware overhead.
- Evidence anchors:
  - [abstract] "Finally, we introduce an input selective processing element (PE) design that balances the load between PEs in suboptimally mapped layers."
  - [section 4.3] "To alleviate this, we propose input selective PEs, a design that enables existing PEs to perform load-balancing through inter-PE work-stealing in a resource-efficient manner."
- Break condition: If the interconnection overhead or switch complexity negates the performance gains from improved PE utilisation.

### Mechanism 3
- Claim: Hardware-aware tuning of OVSF ratios optimises the trade-off between weights generation accuracy and throughput by adjusting per-layer compression based on bottleneck analysis.
- Mechanism: The framework analyses whether each layer is compute-bound, memory-bound, or weights-generation-bound, then iteratively increases OVSF ratios for non-bottlenecked layers to improve weight approximation without affecting processing speed.
- Core assumption: Layers that are not bottlenecked by weights generation can afford additional cycles for more accurate weight construction without performance degradation.
- Evidence anchors:
  - [section 6.2] "A critical component of unzipFPGA is the OVSF Ratios Selection module... In this work, we make steps towards automation by introducing a hardware-aware scheme for tuning the per-layer compression ratios of OVSF models."
  - [section 6.2] "Our method exploits the bottleneck characteristics of each layer in order to generate more accurately its weights while sustaining high throughput."
- Break condition: If the bottleneck analysis misidentifies the limiting factor, leading to either unnecessary weight generation overhead or missed accuracy improvements.

## Foundational Learning

- Concept: OVSF codes and their construction using Sylvester's algorithm
  - Why needed here: OVSF codes form the basis for on-the-fly weight generation, enabling compact representation and reconstruction of CNN filters
  - Quick check question: How are OVSF codes constructed recursively using the Kronecker product, and why must their length be a power of two?

- Concept: Single computation engine (SCE) architecture and dataflow patterns
  - Why needed here: Understanding SCE limitations with memory-bound layers and PE underutilisation is crucial for appreciating unzipFPGA's design choices
  - Quick check question: What distinguishes output-stationary dataflow from other dataflows, and how does it minimise memory accesses for partial sums?

- Concept: FPGA resource constraints and DSP/BRAM utilisation
  - Why needed here: The design space exploration must balance weights generator complexity against available FPGA resources
  - Quick check question: How does tile size parameterisation affect DSP and BRAM consumption in CNN accelerators?

## Architecture Onboarding

- Component map: Input buffer -> DMA controller -> CNN engine core (PE array, weights buffer, output buffer) -> CNN-WGen module (OVSF generator, alpha buffer, vector compute datapath) -> Control unit
- Critical path: Input activation transfer -> weights generation (CNN-WGen) -> PE computation -> output activation transfer. The weights generation stage must pipeline efficiently with the PE array to avoid becoming the bottleneck.
- Design tradeoffs: Larger OVSF ratio values improve weight approximation accuracy but increase generation time; larger vector unit size (parameter M) improves throughput but consumes more DSP resources; input selective PEs add load balancing capability but require additional switching logic.
- Failure signatures: PE underutilisation despite input selective PEs indicates incorrect bottleneck analysis; memory bandwidth constraints despite weights generation suggest insufficient OVSF ratio tuning; resource exhaustion during DSE indicates overly aggressive parameter choices.
- First 3 experiments:
  1. Implement a single OVSF generator with fixed M parameter and verify weight generation correctness against reference implementation
  2. Integrate the CNN-WGen module with a basic PE array using output-stationary dataflow and measure throughput improvement over baseline
  3. Implement the input selective PE enhancement on a simple 2x2 PE array and verify load balancing functionality on layers with mismatched dimensions

## Open Questions the Paper Calls Out

The paper mentions that activation compression techniques could be "orthogonally combined" with unzipFPGA's weight generation approach but doesn't evaluate this combination or explore how these techniques would interact in practice.

## Limitations

- The computational overhead of on-the-fly weights generation must consistently remain below memory access costs across diverse CNN architectures, but this assumption requires validation
- The hardware-aware tuning strategy for OVSF ratios depends on accurate bottleneck characterization, but the specific analysis methodology is not detailed
- Resource allocation between the CNN engine and weights generator during design space exploration lacks explicit constraints, potentially leading to suboptimal implementations

## Confidence

- **High Confidence**: The fundamental concept of replacing memory accesses with on-chip computation is well-established. The use of OVSF codes for weight approximation and the basic PE load balancing mechanism have clear theoretical foundations.
- **Medium Confidence**: The claimed performance gains (2.57× over GPU, 3.94× over FPGA) are specific and impressive but depend heavily on the effectiveness of the hardware-aware tuning and the generality of the OVSF approach across different CNN architectures.
- **Low Confidence**: The paper's claim about minimal accuracy degradation with OVSF approximations lacks comprehensive validation across diverse model architectures and datasets beyond the demonstrated cases.

## Next Checks

1. Implement the hardware-aware OVSF ratio tuning strategy and verify its ability to correctly identify compute-bound vs memory-bound vs weights-generation-bound layers across multiple CNN architectures. Compare the automated selection against manual tuning for a subset of layers.

2. Quantify the actual DSP and BRAM consumption of the CNN-WGen module across different OVSF ratio configurations and vector unit sizes. Validate that the computational overhead of weights generation consistently remains below the memory access time it replaces.

3. Apply unzipFPGA to CNN architectures not included in the original evaluation (e.g., MobileNet, EfficientNet) and measure performance efficiency and accuracy retention. This would test whether the proposed mechanisms generalize beyond the ResNet and SqueezeNet family.