---
ver: rpa2
title: Extreme Multi-Label Skill Extraction Training using Large Language Models
arxiv_id: '2307.10778'
source_url: https://arxiv.org/abs/2307.10778
tags:
- skill
- skills
- sentences
- extraction
- esco
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of extracting skills from job
  ads, which is challenging due to the large number of possible skills (over 14,000
  in ESCO) and the lack of labeled training data. The authors propose a method to
  automatically generate a large synthetic dataset of (skill, job ad sentence) pairs
  using an LLM and the ESCO ontology.
---

# Extreme Multi-Label Skill Extraction Training using Large Language Models

## Quick Facts
- arXiv ID: 2307.10778
- Source URL: https://arxiv.org/abs/2307.10778
- Reference count: 34
- Primary result: 15-25 percentage point improvements in R-Precision@5 using LLM-generated synthetic data vs distant supervision

## Executive Summary
This paper tackles the challenging problem of skill extraction from job ads using the ESCO ontology (14k+ skills). The authors propose generating synthetic training data with an LLM to overcome the lack of labeled examples, then training a bi-encoder model using contrastive learning. Their approach achieves significant improvements (15-25 points in R-Precision@5) over distant supervision baselines on three benchmarks. The method is particularly effective because it can cover 99.5% of ESCO skills with high accuracy (94%) synthetic examples.

## Method Summary
The authors use GPT-3.5-turbo-0301 to generate synthetic (skill, job ad sentence) pairs based on ESCO skill descriptions, creating a dataset covering 99.5% of skills. A bi-encoder model is then trained using contrastive learning with multiple negatives ranking loss, where skill names and corresponding sentences are pushed close together in embedding space. An augmentation strategy randomly concatenates additional sentences during training to improve handling of multi-skill sentences. The model is evaluated on three skill extraction benchmarks (TECH, HOUSE, TECHWOLF) using R-Precision@5 and MRR metrics.

## Key Results
- 15-25 percentage point improvements in R-Precision@5 over distant supervision baseline
- Best performance achieved with GPT-generated sentences and augmentation
- 99.5% coverage of ESCO skills in synthetic dataset
- 94% accuracy for the final synthetic data generation prompt

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The contrastive learning setup learns to align skill names and corresponding sentences in the same representation space.
- **Mechanism:** The bi-encoder model is trained using multiple negatives ranking loss, where skill names and their corresponding job ad sentences are encoded close together while being pushed apart from other skills/sentences in the batch. This creates a shared embedding space where semantically related pairs have high cosine similarity.
- **Core assumption:** The synthetic sentences generated by the LLM accurately reflect the skills they are paired with, creating valid positive training pairs.
- **Evidence anchors:**
  - [abstract] "We use an LLM to generate training data for skill extraction, grounded in the ESCO ontology. Based on this synthetic data, we optimize a model using contrastive learning to represent skill names and corresponding sentences in close proximity within the same space."
  - [section 2.3] "We use a bi-encoder architecture, in which pairs of skills and corresponding sentences are encoded by the same encoder-only transformer architecture... We make use of multiple negatives ranking loss with in-batch negatives."
  - [corpus] "Weak - only 0 citations for related papers, making it hard to validate this mechanism through external evidence."

### Mechanism 2
- **Claim:** The augmentation strategy improves the model's ability to handle multi-skill sentences by forcing it to represent multiple concepts simultaneously.
- **Mechanism:** During training, random sentences are concatenated before or after the target sentence, creating pairs where only one sentence relates to the skill. The model must learn to maintain the skill-sentence alignment despite the added noise, making it more robust to real-world multi-skill job ads.
- **Core assumption:** Real job ads often mention multiple skills in a single sentence, and the model needs to be able to handle this complexity.
- **Evidence anchors:**
  - [section 2.3] "We hypothesize that this setup limits the model's capability to reflect multiple skills in its embeddings, which would harm performance for skill extraction. To this end, we introduce an augmentation strategy that randomly adds another sentence in front or behind each sentence during training."
  - [section 3] "The synthetic sentences typically only discuss its linked skill, while real sentences can mention more skills."
  - [corpus] "Weak - no direct citations supporting this specific augmentation approach, though contrastive learning with augmentation is a known technique."

### Mechanism 3
- **Claim:** Using synthetic data generated by an LLM provides higher quality training data than distant supervision through literal matches.
- **Mechanism:** The LLM can generate diverse, contextually rich sentences that demonstrate skills in realistic scenarios, whereas literal matching only captures exact phrase occurrences. This synthetic data covers 99.5% of ESCO skills with high accuracy (94% for the final prompt).
- **Core assumption:** The LLM has sufficient domain knowledge and language understanding to generate accurate skill demonstrations.
- **Evidence anchors:**
  - [abstract] "We describe a cost-effective approach to generate an accurate, fully synthetic labeled dataset for skill extraction... Our results across three skill extraction benchmarks show a consistent increase of between 15 to 25 percentage points in R-Precision@5 compared to previously published results that relied solely on distant supervision through literal matches."
  - [section 2.2] "The accuracy of the generated data was manually assessed on a subset, and determined to be at around 88% and 94%, respectively, for the first and second prompts."
  - [corpus] "Missing - no corpus evidence found to validate the quality of LLM-generated skill sentences versus literal matching."

## Foundational Learning

- **Concept:** Contrastive learning and metric learning
  - Why needed here: The entire training approach relies on learning a representation space where similar items (skills and their corresponding sentences) are close together and dissimilar items are far apart.
  - Quick check question: What loss function is used to train the bi-encoder model in this approach?

- **Concept:** Extreme multi-label classification (XMLC)
  - Why needed here: The task involves mapping job ads to thousands of possible skill labels from the ESCO ontology, making it an extreme multi-label problem.
  - Quick check question: How many skills are in the ESCO ontology used in this paper?

- **Concept:** Bi-encoder architecture
  - Why needed here: The model needs to encode skills and sentences separately but in the same space to compute similarity scores for ranking.
  - Quick check question: What is the key advantage of using a bi-encoder over a cross-encoder for this task?

## Architecture Onboarding

- **Component map:**
  ESCO ontology -> GPT-3.5-turbo-0301 -> Synthetic dataset -> Bi-encoder model -> Contrastive learning -> Augmentation -> Evaluation

- **Critical path:**
  1. Generate synthetic skill-sentence pairs using LLM
  2. Prepare training data with augmentation
  3. Train bi-encoder using contrastive loss
  4. Evaluate on benchmarks using R-Precision@5

- **Design tradeoffs:**
  - Using synthetic data vs. manually annotated data (cost vs. potential quality issues)
  - Bi-encoder vs. cross-encoder (inference speed vs. interaction modeling)
  - Augmentation level (robustness vs. potential noise)

- **Failure signatures:**
  - Low R-Precision@5 scores indicating poor skill extraction
  - High variance across benchmarks suggesting overfitting
  - Synthetic data coverage gaps leaving some skills unrepresented

- **First 3 experiments:**
  1. Train without augmentation to establish baseline performance
  2. Vary the amount of synthetic data per skill to find optimal coverage
  3. Compare ESCO descriptions vs. synthetic sentences as training data source

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Reliance on LLM-generated synthetic data whose quality may vary across skills
- Potential domain mismatch between synthetic sentences and real job ads
- Focus on R-Precision@5 which may not capture all aspects of skill extraction performance

## Confidence
- Methodology and synthetic data generation: High (well-specified with concrete quality assessments)
- Comparative results against distant supervision: Medium (improvement magnitude depends on baseline implementation)
- Real-world applicability claims: Low (not validated on diverse job ad datasets)

## Next Checks
1. Test the model on a completely independent dataset of real job ads not seen during any training phase
2. Conduct ablation studies varying the amount of synthetic data per skill to determine optimal coverage
3. Compare performance across different LLM models (e.g., GPT-4, Claude) to assess sensitivity to the choice of language model