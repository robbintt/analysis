---
ver: rpa2
title: How FaR Are Large Language Models From Agents with Theory-of-Mind?
arxiv_id: '2310.03051'
source_url: https://arxiv.org/abs/2310.03051
tags:
- reasoning
- llms
- answer
- inferences
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new evaluation paradigm, Thinking for Doing
  (T4D), to test whether large language models (LLMs) can connect Theory-of-Mind (ToM)
  reasoning to actions in social scenarios. T4D requires models to choose actions
  based on observations of characters' mental states, rather than just answering questions
  about those mental states.
---

# How FaR Are Large Language Models From Agents with Theory-of-Mind?

## Quick Facts
- arXiv ID: 2310.03051
- Source URL: https://arxiv.org/abs/2310.03051
- Authors: 
- Reference count: 17
- Key outcome: FaR improves GPT-4's performance on T4D from 50% to 71%, outperforming other prompting methods

## Executive Summary
This paper introduces the Thinking for Doing (T4D) task to evaluate whether large language models (LLMs) can connect Theory-of-Mind (ToM) reasoning to actions in social scenarios. While LLMs perform well on traditional ToM tasks, they struggle when asked to choose actions based on mental state observations rather than just answering questions about those mental states. The authors propose a new zero-shot prompting framework called Foresee and Reflect (FaR) that encourages models to anticipate future challenges and reason about potential actions. FaR significantly improves LLM performance on T4D, achieving 71% accuracy on GPT-4 compared to 50% with baseline methods.

## Method Summary
The paper converts the ToMi benchmark stories into T4D format by adding intent statements and changing questions to action-oriented choices. They then evaluate LLMs using various zero-shot prompting methods including Chain-of-Thought, Tree-of-Thought, Self-Ask, and their proposed FaR framework. FaR has two components: Foresee (prompting models to predict future events based on observations) and Reflect (reasoning about which action choice better helps characters with potential challenges). The authors compare model performance on T4D against traditional ToM tasks and test generalization across different story structures and scenarios.

## Key Results
- GPT-4 achieves only 50% accuracy on T4D compared to 90% human agreement
- FaR improves GPT-4's T4D performance from 50% to 71%, outperforming other prompting methods
- LLMs struggle specifically with identifying implicit inferences about mental states without explicit guidance
- FaR generalizes well to diverse out-of-distribution story structures and scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The key bottleneck for LLMs on T4D is their inability to identify the correct implicit inference about mental states without explicit guidance.
- **Mechanism:** LLMs struggle to connect observations to the latent inference variable I that determines the correct action. When given oracle hints about relevant inferences (+QD or +ToM), performance improves dramatically because the model no longer needs to navigate the unconstrained inference space.
- **Core assumption:** The inference space I contains a unique correct inference path that leads to the proper action, and LLMs can follow this path when properly guided.
- **Evidence anchors:**
  - [abstract]: "Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D."
  - [section 4.2]: "When we provide models with specific hints about relevant inferences, their performance significantly improves, approaching human levels."
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism.

### Mechanism 2
- **Claim:** FaR's "Foresee" component helps LLMs by explicitly prompting them to consider future events and challenges, which guides them toward relevant inferences.
- **Mechanism:** By prompting models to predict future events and identify potential challenges, FaR creates a structured reasoning path that leads to the correct inference about mental states. This mimics human reasoning about "Consideration of Future Consequences."
- **Core assumption:** Considering future consequences is a natural and effective way to identify relevant mental state inferences for action selection.
- **Evidence anchors:**
  - [abstract]: "Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions."
  - [section 5.1]: "We design FaR by first prompting models to look into the future by considering potential events that are likely to happen."
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism.

### Mechanism 3
- **Claim:** The "Reflect" component of FaR prunes the generated inferences by evaluating them against available action options, similar to path pruning in A* search.
- **Mechanism:** After generating potential future inferences, the Reflect phase narrows down the options based on which action would best address the identified challenges, effectively pruning the search space.
- **Core assumption:** The action space provides sufficient constraints to effectively prune the inference space and identify the correct mental state reasoning.
- **Evidence anchors:**
  - [abstract]: "FaR has two components: Foresee, where it prompts the models to predict future events based on observations and Reflect, where models reason on which action choice better helps the characters with potential challenges."
  - [section 5.2]: "After foreseeing likely future events, we prompt models to reflect on whether performing actions at the moment could help with the potential challenges identified in the first step."
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism.

## Foundational Learning

- **Concept: Theory of Mind (ToM)**
  - Why needed here: T4D specifically tests whether models can use ToM reasoning to choose actions based on others' mental states.
  - Quick check question: Can you explain the Sally-Anne test and what it demonstrates about mental state reasoning?

- **Concept: False Belief Tests**
  - Why needed here: The T4D task is built on false belief scenarios where characters hold incorrect beliefs about object locations.
  - Quick check question: What is the difference between a true belief and a false belief in the context of ToM testing?

- **Concept: Zero-shot prompting techniques**
  - Why needed here: FaR is a zero-shot prompting framework, and understanding existing techniques (CoT, ToT, Self-Ask) is crucial for appreciating its novelty.
  - Quick check question: How does Chain-of-Thought prompting differ from Tree-of-Thought prompting in its approach to guiding LLM reasoning?

## Architecture Onboarding

- **Component map:** ToMi stories -> T4D conversion -> Zero-shot prompts (base, CoT, ToT, Self-Ask, FaR) -> LLM (GPT-4, PaLM 2) -> Action choice output

- **Critical path:**
  1. Convert ToMi stories to T4D format
  2. Apply zero-shot prompts (base, CoT, ToT, Self-Ask, FaR)
  3. Evaluate model accuracy on T4D
  4. Analyze performance differences

- **Design tradeoffs:**
  - Zero-shot vs few-shot: Zero-shot allows testing true capability but may miss potential improvements from examples
  - Prompt complexity vs performance: More complex prompts (like FaR) may improve performance but could be harder to generalize
  - Noise tolerance: FaR is sensitive to noisy foresight, which could limit its robustness

- **Failure signatures:**
  - Low accuracy on T4D but high accuracy on ToMi (indicates ToM reasoning vs action selection gap)
  - Performance drops significantly with noisy foresight (indicates sensitivity to prompt quality)
  - Inconsistent performance across different story structures (indicates overfitting to specific formats)

- **First 3 experiments:**
  1. Compare base LLM performance on T4D vs ToMi to establish the reasoning gap
  2. Test oracle hints for question decomposition and ToM inferences to identify key bottlenecks
  3. Apply FaR framework and measure improvement over other zero-shot prompting methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Foresee and Reflect (FaR) prompting framework generalize to other types of social reasoning tasks beyond Theory of Mind?
- Basis in paper: Explicit
- Why unresolved: While the paper demonstrates FaR's effectiveness on Theory of Mind tasks and some generalization to story structure changes and Faux Pas scenarios, it doesn't explore its applicability to a broader range of social reasoning tasks.
- What evidence would resolve it: Experiments testing FaR on diverse social reasoning tasks such as moral reasoning, emotion recognition, or sarcasm detection would provide evidence for its generalizability.

### Open Question 2
- Question: What are the limitations of the Foresee and Reflect (FaR) framework in handling complex, multi-step reasoning tasks?
- Basis in paper: Inferred
- Why unresolved: The paper mentions that FaR is sensitive to noisy foresight, but it doesn't explore how the framework performs on more complex reasoning tasks that require multiple steps or involve intricate dependencies between steps.
- What evidence would resolve it: Experiments testing FaR on increasingly complex reasoning tasks, such as multi-hop question answering or logical reasoning puzzles, would reveal its limitations and potential areas for improvement.

### Open Question 3
- Question: How does the performance of the Foresee and Reflect (FaR) framework compare to other advanced prompting techniques or fine-tuning approaches for Theory of Mind tasks?
- Basis in paper: Inferred
- Why unresolved: While the paper compares FaR to some prompting techniques, it doesn't explore its performance against other advanced methods like chain-of-thought with fine-tuning or reinforcement learning from human feedback (RLHF).
- What evidence would resolve it: Experiments comparing FaR to other advanced prompting techniques or fine-tuning approaches on Theory of Mind tasks would provide insights into its relative strengths and weaknesses.

## Limitations

- The core claims about FaR's effectiveness rest on comparing zero-shot performance without establishing baselines for few-shot learning or fine-tuning approaches.
- The mechanism analysis assumes a single "correct" inference path exists for each T4D scenario, but doesn't explore cases where multiple valid inferences might lead to different but reasonable actions.
- The generalization claims to out-of-distribution scenarios are based on limited test cases and don't establish systematic robustness across diverse story structures.

## Confidence

- **High confidence**: The empirical finding that LLMs show significant performance gaps between ToMi (traditional ToM) and T4D (action-based ToM) tasks is well-supported by the experimental data. The improvement from FaR over baseline methods is also robustly demonstrated.
- **Medium confidence**: The mechanism explanations for why FaR works are plausible but rely on assumptions about LLM reasoning processes that aren't directly validated. The claim that LLMs struggle specifically with implicit inferences rather than action selection is supported but could have alternative explanations.
- **Low confidence**: The generalization claims to out-of-distribution scenarios are based on limited test cases and don't establish systematic robustness across diverse story structures.

## Next Checks

1. **Controlled inference space experiment**: Test whether providing multiple valid inference hints (not just the "correct" one) improves performance, to validate whether the bottleneck is identifying the single correct inference or navigating any inference space at all.

2. **Cultural bias analysis**: Evaluate model performance on T4D scenarios with diverse cultural contexts to determine if the observed performance gaps reflect genuine reasoning limitations or cultural bias in the benchmark design.

3. **Alternative prompting comparison**: Implement and test a few-shot learning baseline with explicit examples of the T4D task to establish whether zero-shot prompting limitations explain the performance gaps, or if the task fundamentally requires different learning approaches.