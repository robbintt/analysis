---
ver: rpa2
title: Continual Improvement of Threshold-Based Novelty Detection
arxiv_id: '2309.02551'
source_url: https://arxiv.org/abs/2309.02551
tags:
- threshold
- classes
- data
- accuracy
- total
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of automatic novelty detection
  in continual learning settings, where models must identify and incorporate unseen
  classes without prior notification. The core method introduces a dynamic threshold
  selection mechanism that replaces fixed thresholds with values optimized through
  leave-one-class-out cross-validation on known classes.
---

# Continual Improvement of Threshold-Based Novelty Detection

## Quick Facts
- arXiv ID: 2309.02551
- Source URL: https://arxiv.org/abs/2309.02551
- Reference count: 40
- One-line primary result: Dynamic threshold selection using leave-one-class-out cross-validation outperforms fixed thresholds in continual novelty detection

## Executive Summary
This work addresses the challenge of automatic novelty detection in continual learning settings, where models must identify and incorporate unseen classes without prior notification. The core method introduces a dynamic threshold selection mechanism that replaces fixed thresholds with values optimized through leave-one-class-out cross-validation on known classes. Experiments on MNIST, Fashion MNIST, and CIFAR-10 demonstrate that this dynamic threshold selection consistently outperforms the fixed threshold baseline used in previous approaches, achieving higher total accuracy and improved novelty detection rates while maintaining competitive in-distribution classification performance.

## Method Summary
The method uses a dynamic threshold selection mechanism that leverages leave-one-class-out cross-validation on known classes to estimate optimal thresholds for detecting unknown classes. The approach treats each known class as "unknown" in turn, computes optimal thresholds for each scenario, and averages them to estimate the threshold for true unknown classes. After detecting and incorporating a new class, the threshold is updated by averaging the previous threshold with the newly computed optimal threshold. The search space for threshold values is defined by Z-scores of correctly classified in-distribution points and out-of-distribution points, allowing efficient linear search over threshold values.

## Key Results
- Dynamic threshold selection consistently outperforms fixed thresholds on MNIST, Fashion MNIST, and CIFAR-10
- Higher total accuracy (ID + OOD accuracy / 2) compared to baseline methods
- Improved novelty detection rates while maintaining competitive in-distribution classification performance
- The averaging strategy for threshold updates effectively balances adaptation to new classes with retention of previously learned classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Leave-one-class-out cross-validation approximates the optimal threshold for novelty detection
- Mechanism: The method treats each known class as "unknown" in turn, computes optimal thresholds for each scenario, and averages them to estimate the threshold for true unknown classes
- Core assumption: The distribution of similarity scores for ID classes is representative of the distribution that unknown classes will follow
- Evidence anchors:
  - [abstract]: "utilizing a linear search and leave-one-out cross-validation on the ID classes"
  - [section 3.2]: "we treat each ID class as OOD in turn in a leave-one-class-out cross-validation fashion"
  - [corpus]: No direct corpus evidence found - this appears to be a novel methodological contribution
- Break condition: The method fails when unknown classes have fundamentally different similarity score distributions than known classes

### Mechanism 2
- Claim: Dynamic threshold adjustment improves detection of unknown classes without catastrophic forgetting
- Mechanism: After detecting and incorporating a new class, the threshold is updated by averaging the previous threshold with the newly computed optimal threshold for the current unknown class
- Core assumption: Averaging thresholds provides a reasonable balance between maintaining detection of previously known classes and adapting to new data distributions
- Evidence anchors:
  - [section 3.2]: "we update our previous threshold by averaging it with the new one, to use for the next round of OOD detection"
  - [section 3.3]: "Once an OOD instance is detected, SHELS incorporates the novel concept into its knowledge base"
  - [corpus]: No direct corpus evidence found for this specific averaging approach
- Break condition: The averaging approach may fail when the new unknown class has significantly different feature characteristics than previous classes

### Mechanism 3
- Claim: Z-score based threshold search provides efficient exploration of threshold space
- Mechanism: The search space is defined by Z-scores of correctly classified ID points and OOD points, allowing efficient linear search over threshold values
- Core assumption: The optimal threshold lies within the range of observed Z-scores from both ID and OOD data
- Evidence anchors:
  - [section 3.3]: "Zc(x)′ = µc−sc(x) σc as the negative Z-score" and "The search space for possible values of η consists of the Zc(x)′ scores"
  - [section 3.3]: "We perform our search in terms of numbers of standard deviations from the mean similarity score"
  - [corpus]: No direct corpus evidence found for this specific Z-score approach to threshold search
- Break condition: The method fails when the optimal threshold lies outside the observed Z-score range

## Foundational Learning

- Concept: Leave-one-out cross-validation
  - Why needed here: This is the core technique for estimating optimal thresholds without access to true unknown classes
  - Quick check question: If we have 5 known classes, how many times would we need to train models during the initial threshold selection phase?
  - Answer: 5 times (once for each class treated as unknown)

- Concept: Z-score normalization
  - Why needed here: Z-scores standardize similarity scores across different classes, making threshold search more efficient and interpretable
  - Quick check question: Why is Z-score normalization particularly useful when dealing with similarity scores that may have different scales across classes?
  - Answer: It puts all scores on the same scale, allowing meaningful comparison and threshold setting across classes

- Concept: Catastrophic forgetting
  - Why needed here: The method must detect new classes without losing the ability to recognize previously learned classes
  - Quick check question: What regularization techniques does SHELS use to prevent catastrophic forgetting when incorporating new classes?
  - Answer: Group sparsity penalty and soft freezing of previously used weights

## Architecture Onboarding

- Component map: Feature extractor -> Similarity scorer -> Threshold selector -> Class accommodator -> OOD detector
- Critical path:
  1. Initial training on known classes
  2. Leave-one-out cross-validation for initial threshold
  3. Continuous observation and detection
  4. Threshold update after each new class
  5. Class accommodation with regularization
- Design tradeoffs:
  - Storage vs. Recalculation: Must store all training data to recompute activations for threshold updates
  - Computation vs. Accuracy: Linear search over Z-scores is efficient but may miss optimal thresholds between sampled values
  - Sensitivity vs. Specificity: Threshold choice involves balancing ID accuracy against OOD detection rate
- Failure signatures:
  - Threshold too high: High ID accuracy but poor OOD detection
  - Threshold too low: Good OOD detection but poor ID accuracy
  - Threshold drift: Performance degradation over time as class distributions change
- First 3 experiments:
  1. Implement leave-one-out cross-validation on a small dataset (e.g., MNIST with 3 classes) to verify threshold estimation
  2. Test Z-score based linear search on synthetic data with known optimal threshold
  3. Evaluate threshold averaging strategy by simulating class arrivals in a controlled environment

## Open Questions the Paper Calls Out

- Question: How does the performance of dynamic threshold selection compare when applied to other novelty detection methods beyond SHELS?
- Basis in paper: [explicit] The paper states "Our method could also be used to select other post hoc hyperparameters efficiently in a continual OOD detection and accommodation setting."
- Why unresolved: The paper only evaluates the dynamic threshold selection on the SHELS method, leaving uncertainty about its effectiveness on alternative novelty detection approaches.
- What evidence would resolve it: Experiments applying the dynamic threshold selection method to multiple other novelty detection frameworks (e.g., one-class SVMs, autoencoders, deep anomaly detection methods) with comparison of performance metrics.

- Question: What is the computational overhead of the leave-one-class-out cross-validation stage in practical continual learning scenarios with large numbers of classes?
- Basis in paper: [inferred] The paper mentions "multiple training rounds required during the (initial) cross-validation stage" as a downside but doesn't quantify the computational cost.
- Why unresolved: While the paper acknowledges this limitation, it doesn't provide empirical measurements of the training time or computational resources needed for the cross-validation stage across different dataset sizes.
- What evidence would resolve it: Detailed profiling of computational resources (time, memory) for the cross-validation stage across varying numbers of classes and dataset sizes, including comparison to baseline methods.

- Question: How sensitive is the dynamic threshold selection method to the choice of scoring function sc(x)?
- Basis in paper: [explicit] The paper defines a scoring function sc(x) but only evaluates one specific implementation (the SHELS scoring function).
- Why unresolved: The paper assumes a particular scoring function without exploring how different scoring functions might affect the effectiveness of the dynamic threshold selection.
- What evidence would resolve it: Systematic experiments testing the dynamic threshold selection with multiple different scoring functions (e.g., Euclidean distance, Mahalanobis distance, learned similarity metrics) and comparing their performance.

## Limitations
- Computational overhead: Multiple training rounds required during the initial cross-validation stage
- Sensitivity to scoring function: Performance depends on the choice of similarity scoring function
- Dataset specificity: Evaluation limited to relatively simple datasets (MNIST, Fashion MNIST, CIFAR-10)

## Confidence

**High Confidence Claims**: The dynamic threshold selection mechanism using leave-one-class-out cross-validation is well-supported by the methodology description and experimental results. The improvement over fixed thresholds is clearly demonstrated across multiple datasets.

**Medium Confidence Claims**: The effectiveness of the threshold averaging strategy for continual adaptation is demonstrated but could benefit from additional ablation studies showing how performance degrades with different averaging approaches or when skipping updates.

**Low Confidence Claims**: The generalizability to more complex datasets and real-world scenarios where unknown classes may have vastly different feature distributions than known classes. The paper focuses on relatively simple datasets (MNIST, Fashion MNIST, CIFAR-10) which may not capture the full complexity of open-world learning scenarios.

## Next Checks

1. **Threshold Search Sensitivity**: Conduct experiments varying the granularity of the linear search over Z-scores to determine how search resolution affects final performance and computational efficiency.

2. **Cross-Dataset Robustness**: Test the method on datasets with more significant distribution shifts between known and unknown classes (e.g., CIFAR-100 with CIFAR-10 unknowns) to evaluate threshold estimation reliability under challenging conditions.

3. **Ablation of Averaging Strategy**: Compare the proposed threshold averaging approach against alternative update strategies such as exponential moving averages or adaptive weight updates based on class similarity.