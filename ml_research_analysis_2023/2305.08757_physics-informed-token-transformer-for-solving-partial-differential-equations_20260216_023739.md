---
ver: rpa2
title: Physics Informed Token Transformer for Solving Partial Differential Equations
arxiv_id: '2305.08757'
source_url: https://arxiv.org/abs/2305.08757
tags:
- pitt
- equation
- attention
- equations
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PITT (Physics Informed Token Transformer),
  a novel approach for solving Partial Differential Equations (PDEs) using transformer
  models. The key innovation is incorporating physics knowledge by tokenizing governing
  equations and using them as input to a transformer-based neural operator.
---

# Physics Informed Token Transformer for Solving Partial Differential Equations

## Quick Facts
- arXiv ID: 2305.08757
- Source URL: https://arxiv.org/abs/2305.08757
- Authors: 
- Reference count: 29
- Key outcome: PITT outperforms FNO on 1D/2D PDEs with 10x fewer parameters and lower prediction errors

## Executive Summary
This paper introduces PITT (Physics Informed Token Transformer), a novel approach for solving Partial Differential Equations using transformer models with physics-informed equation tokenization. The key innovation is incorporating governing equations as tokenized inputs to a transformer-based neural operator, making the model aware of the underlying physical processes. PITT achieves significantly lower prediction errors than popular neural operator models like FNO while using an order of magnitude fewer parameters, and demonstrates better error accumulation over long rollout predictions.

## Method Summary
PITT combines Fourier Neural Operators with equation tokenization to create a physics-informed transformer for PDE solving. The method tokenizes governing equations and uses them as input to a transformer-based neural operator, embedding partial derivatives and equation structure into the model. The architecture consists of equation embedding, token attention, and numerical update blocks that work together to produce physics-aware predictions. PITT is trained on 1D and 2D PDEs including heat equation, Burgers' equation, KdV equation, Navier-Stokes equations, and Poisson equation, using a 60-20-20 train-validation-test split with early stopping based on validation loss.

## Key Results
- PITT achieves 0.38±0.02 MAE vs 4.95±0.25 MAE for FNO on 1D heat equation
- Uses approximately 10x fewer parameters than FNO while maintaining superior accuracy
- Demonstrates better error accumulation over long rollout predictions compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1
Tokenizing equations and embedding them into the transformer attention mechanism allows the model to learn physically relevant information from governing equations. The tokenization process converts symbolic equations into numerical tokens that can be processed by transformers. These tokens are then used as queries and keys in attention mechanisms, allowing the model to extract relationships between equation components and physical properties.

### Mechanism 2
The combination of FNO and equation-informed updates creates an analytically-driven correction to the numerical solution. PITT uses FNO to handle the base state propagation while the tokenized equation information provides a correction through the numerical update block. This mimics traditional numerical methods where analytical knowledge informs numerical updates.

### Mechanism 3
Linear attention with instance normalization provides an efficient way to compute the kernel integral interpretation of attention for PDE solutions. The linear attention formulation allows PITT to compute attention efficiently while maintaining the mathematical properties needed for operator learning. Instance normalization helps stabilize the attention computation across different equation instances.

## Foundational Learning

- Concept: Tokenization of mathematical expressions
  - Why needed here: The method relies on converting symbolic equations into numerical tokens that can be processed by neural networks
  - Quick check question: How would you tokenize the equation ∂u/∂t = α∂²u/∂x² for a 1D heat equation?

- Concept: Fourier Neural Operators (FNO)
  - Why needed here: PITT builds upon FNO as its base architecture, using it for state propagation before applying equation-informed corrections
  - Quick check question: What is the key advantage of FNO over traditional numerical methods for solving PDEs?

- Concept: Attention mechanisms in deep learning
  - Why needed here: The method uses multi-head attention to process both the tokenized equations and their interaction with the numerical state
  - Quick check question: How does self-attention differ from cross-attention in transformer architectures?

## Architecture Onboarding

- Component map: Input: Numerical grid data, tokenized equations, time step information -> Token Transformer: Multi-head self-attention on equation tokens -> Linear Attention Update: Cross-attention between token representation and FNO output -> FNO Backbone: Fourier-based state propagation -> Output: Updated state for next time step

- Critical path: Tokenization → Token Transformer → Linear Attention Update → FNO → State Update

- Design tradeoffs: Smaller parameter count vs. accuracy (PITT uses ~10x fewer parameters than FNO), explicit physics encoding vs. pure data-driven learning

- Failure signatures: Unstable rollout predictions, inability to generalize to unseen equation parameters, poor performance on boundary conditions

- First 3 experiments:
  1. Test tokenization quality by visualizing attention weights when equation parameters are modified
  2. Compare FNO-only predictions vs. PITT predictions to isolate the impact of equation information
  3. Evaluate rollout stability by comparing error accumulation over long prediction horizons

## Open Questions the Paper Calls Out

### Open Question 1
How does PITT's performance scale with increasing spatial dimensions beyond 2D for PDEs? The paper demonstrates PITT's effectiveness on 1D and 2D PDEs but does not explore higher-dimensional cases. This is a common limitation in neural operator studies due to computational constraints and the need for more sophisticated architectures to handle higher-dimensional data efficiently.

### Open Question 2
What is the impact of different tokenization strategies on PITT's ability to learn physics-informed representations? The paper describes a specific tokenization method for equations and discusses its effectiveness, but does not explore alternative tokenization strategies or their comparative performance.

### Open Question 3
How does PITT's attention mechanism contribute to its ability to generalize across different PDE parameters and initial conditions? While the paper shows that PITT's attention weights change distinctively when equation parameters are modified, it doesn't provide a detailed analysis of how attention contributes to generalization.

### Open Question 4
What is the theoretical understanding of PITT's convergence properties and error bounds for different types of PDEs? The paper demonstrates empirical performance but doesn't provide theoretical analysis of convergence or error bounds, which is a common gap in neural operator literature.

## Limitations

- Limited validation of tokenization robustness for complex or ambiguous PDE formulations
- Unclear generalization beyond specific 1D and 2D PDEs tested
- Potential computational overhead from tokenization and attention mechanisms not fully analyzed

## Confidence

**High Confidence Claims**:
- PITT outperforms FNO on the specific 1D and 2D PDEs tested
- The tokenization approach effectively encodes equation structure for attention mechanisms
- PITT achieves significantly lower prediction errors while using fewer parameters

**Medium Confidence Claims**:
- PITT demonstrates better rollout stability than FNO over long prediction horizons
- The method's performance advantage generalizes across different types of PDEs
- Equation tokenization is the primary driver of PITT's improved performance

**Low Confidence Claims**:
- PITT will maintain its performance advantage on higher-dimensional PDEs
- The tokenization approach will work effectively for PDEs with complex or implicit formulations
- The computational efficiency gains will scale favorably to larger problems

## Next Checks

1. **Ablation Study on Tokenization**: Remove the equation tokenization component and retrain PITT as a pure FNO variant to isolate the contribution of physics-informed information. Compare performance degradation to quantify how much of the improvement comes specifically from equation tokenization versus the transformer architecture itself.

2. **Generalization to Unseen PDE Families**: Test PITT on PDEs from different mathematical families (e.g., elliptic vs. parabolic) or with different numbers of spatial dimensions than those used in training. This would validate whether the tokenization approach generalizes beyond the specific equations studied.

3. **Attention Mechanism Analysis**: Visualize and analyze the attention weights between equation tokens and state representations to understand what information the model is extracting from the equations. This would provide insight into whether the model is learning physically meaningful relationships or exploiting superficial patterns in the tokenization scheme.