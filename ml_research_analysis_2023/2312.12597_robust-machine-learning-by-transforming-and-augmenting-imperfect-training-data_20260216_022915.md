---
ver: rpa2
title: Robust Machine Learning by Transforming and Augmenting Imperfect Training Data
arxiv_id: '2312.12597'
source_url: https://arxiv.org/abs/2312.12597
tags:
- learning
- data
- training
- which
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis explores several data sensitivities of modern machine
  learning and how to address them. We begin by discussing how to prevent ML from
  codifying prior human discrimination measured in the training data, where we take
  a fair representation learning approach.
---

# Robust Machine Learning by Transforming and Augmenting Imperfect Training Data

## Quick Facts
- arXiv ID: 2312.12597
- Source URL: https://arxiv.org/abs/2312.12597
- Reference count: 0
- This thesis explores several data sensitivities of modern machine learning and how to address them through fair representation learning, invariant learning, and data augmentation techniques.

## Executive Summary
This thesis addresses fundamental challenges in modern machine learning arising from imperfect training data. The work systematically tackles three major data sensitivities: algorithmic discrimination in training data, spurious features that provide unreliable predictions, and insufficient coverage in reinforcement learning settings. Through a combination of causal inference, representation learning, and data augmentation techniques, the thesis proposes novel methods to create more robust machine learning systems that can handle these data imperfections effectively.

## Method Summary
The thesis presents three main methodological contributions. First, it develops fair representation learning approaches that use adversarial learning to remove discriminatory information while maintaining predictive utility. Second, it introduces environment inference for invariant learning (EIIL), which partitions training data based on a reference classifier's reliance on spurious features to find informative environments for invariant learning. Third, it proposes counterfactual data augmentation (CoDA) and model-based counterfactual data augmentation (MoCoDA) for reinforcement learning, which generate new plausible trajectories by swapping independent components or applying locally factored dynamics models to carefully constructed parent distributions.

## Key Results
- Fair representation learning methods (LAFTR and FFVAE) achieve favorable fairness-accuracy tradeoffs on benchmark datasets like UCI Adult and Health
- Environment inference for invariant learning (EIIL) improves out-of-distribution generalization on datasets like CMNIST and Waterbirds compared to standard ERM
- Data augmentation techniques (CoDA and MoCoDA) enhance sample efficiency and performance on continuous control tasks like 2D Navigation and Slide2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning algorithms can leverage their propensity to learn spurious features as a search tool to find data partitions that expose prediction inconsistencies, ultimately promoting robustness to spurious features.
- Mechanism: The algorithm uses a reference classifier (e.g., ERM) to identify which features are most predictive in the training data. It then partitions the data into environments where these features agree/disagree with the label, maximizing violations of the invariance principle. Invariant learning methods applied to these partitions learn representations invariant to the spurious features.
- Core assumption: The reference classifier will focus on spurious features in the presence of label noise or correlation, and these features will vary across environments in a way that can be detected.
- Evidence anchors:
  - [abstract]: "Here we observe that insofar as standard training methods tend to learn such features, this propensity can be leveraged to search for partitions of training data that expose this inconsistency, ultimately promoting learning algorithms invariant to spurious features."
  - [section]: "We propose a novel invariant learning framework that does not require a priori domain/environment knowledge...Our aim is to find environments that maximally violate the invariant learning principle."
  - [corpus]: Weak evidence - only 5 neighbor papers with very low FMR scores, none directly addressing spurious features or invariant learning.
- Break condition: If the reference classifier focuses on causal features rather than spurious ones, or if the spurious features do not vary across environments in a detectable way.

### Mechanism 2
- Claim: Local independence of objects in dynamical systems allows for counterfactual data augmentation by swapping independent components between observed trajectories.
- Mechanism: The algorithm identifies local causal structures where subprocesses (e.g., billiard balls) are independent during time slices between interactions. It then swaps these independent components between trajectories to generate new, plausible counterfactual data that expands the training distribution.
- Core assumption: The dynamical system can be decomposed into locally independent subprocesses during time slices between interactions, and these local structures can be identified or learned.
- Evidence anchors:
  - [abstract]: "To address the coverage issue, we discuss how causal priors can be used to model the single-step dynamics of the setting where data are collected. This enables a new type of data augmentation where observed trajectories are stitched together to produce new but plausible counterfactual trajectories."
  - [section]: "We propose a novel form of data augmentation...whenever an environment transition is within the bounds of some local modelML whose graph GL has the locally independent causal mechanismGi as a disconnected subgraph...we may mix and match the samples ofGi to generate counterfactual data."
  - [corpus]: Weak evidence - no neighbor papers directly addressing counterfactual data augmentation or local independence in dynamical systems.
- Break condition: If the dynamical system cannot be decomposed into locally independent subprocesses, or if the local structures cannot be identified or learned accurately.

### Mechanism 3
- Claim: Model-based counterfactual data augmentation (MoCoDA) can extrapolate the training distribution by applying a locally factored dynamics model to a carefully constructed parent distribution.
- Mechanism: The algorithm trains a locally factored dynamics model that generalizes well to out-of-distribution states/actions. It then constructs a parent distribution (e.g., Mocoda) where the marginals of each parent set match the empirical distribution, applies the dynamics model to generate new transitions, and uses these transitions to train an RL agent.
- Core assumption: A locally factored dynamics model trained on empirical data will generalize well to a parent distribution constructed to match empirical marginals, and this generalization can be leveraged to train an RL agent on out-of-distribution tasks.
- Evidence anchors:
  - [abstract]: "To address the coverage issue, we discuss how causal priors can be used to model the single-step dynamics of the setting where data are collected. This enables a new type of data augmentation where observed trajectories are stitched together to produce new but plausible counterfactual trajectories."
  - [section]: "We propose Model-based Counterfactual Data Augmentation (MoCoDA), which generates an augmented state-action distribution where its locally factored dynamics model is likely to perform well, then applies its dynamics model to generate new transition data."
  - [corpus]: Weak evidence - no neighbor papers directly addressing MoCoDA or model-based counterfactual data augmentation.
- Break condition: If the locally factored dynamics model does not generalize well to the parent distribution, or if the parent distribution is not constructed carefully enough to ensure generalization.

## Foundational Learning

- Concept: Causal inference and structural causal models (SCMs)
  - Why needed here: The thesis heavily relies on causal priors to model dynamics, identify local independence, and perform counterfactual data augmentation. Understanding SCMs is crucial for grasping the theoretical foundations of these methods.
  - Quick check question: What is the difference between a structural equation and a conditional probability in a causal model?

- Concept: Reinforcement learning and Markov Decision Processes (MDPs)
  - Why needed here: The thesis applies data augmentation techniques to improve RL agents, which requires understanding MDPs, policies, value functions, and the coverage assumptions of RL algorithms.
  - Quick check question: What is the Bellman optimality equation, and how does it relate to Q-learning?

- Concept: Representation learning and disentanglement
  - Why needed here: The thesis discusses fair representation learning and how to encourage structure in learned representations conducive to fairness. Understanding representation learning and disentanglement is key to grasping these methods.
  - Quick check question: What is the difference between a disentangled representation and a fair representation?

## Architecture Onboarding

- Component map: Fair representation learning -> Environment inference for invariant learning -> Counterfactual data augmentation for reinforcement learning
- Critical path: Identify the data imperfection → Propose a method to address it using causal priors, representation learning, or data augmentation → Validate the method experimentally on benchmark tasks
- Design tradeoffs: The proposed methods often involve trade-offs between accuracy and robustness/fairness. For example, fair representation learning may sacrifice some accuracy to ensure demographic parity. Similarly, data augmentation techniques may introduce distributional shift that needs to be carefully controlled.
- Failure signatures: If the proposed methods fail, it could be due to incorrect causal assumptions, poor representation learning, or ineffective data augmentation. Careful analysis of the failure modes is needed to diagnose the issue.
- First 3 experiments:
  1. Test the fair representation learning method on a benchmark dataset like Adult or Health to measure fairness-accuracy tradeoffs.
  2. Test the environment inference method on a synthetic dataset like CMNIST to measure robustness to spurious features.
  3. Test the data augmentation method on a continuous control task like 2D Navigation to measure improvements in sample efficiency and out-of-distribution generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the tendency of deep learning models to rely on spurious features be reliably exploited for robust feature learning in diverse real-world datasets?
- Basis in paper: [explicit] The paper discusses how ERM tends to learn spurious features, and proposes EIIL to partition data based on a reference model's reliance on these features to find informative environments for invariant learning.
- Why unresolved: While EIIL shows promise on controlled datasets like CMNIST, its effectiveness on more complex, high-dimensional datasets with multiple spurious features remains unclear. The choice of a suitable reference model for diverse datasets is also an open question.
- What evidence would resolve it: Rigorous empirical studies applying EIIL to various real-world datasets with known spurious features, demonstrating consistent improvements in out-of-distribution generalization compared to ERM and other invariant learning methods.

### Open Question 2
- Question: How can causal priors be effectively sourced in a participatory way to guide the design of robust machine learning algorithms?
- Basis in paper: [inferred] The paper highlights the importance of causal priors in CoDA and MoCoDA for data augmentation, but acknowledges the challenges in specifying these priors, especially in complex social systems.
- Why unresolved: Current methods rely on domain experts to specify causal priors, which may not be feasible or reliable in all settings. There is a need for methods to source causal priors from stakeholders, such as community members or affected populations.
- What evidence would resolve it: Successful implementation of participatory methods for causal inference in ML algorithm design, leading to more inclusive and equitable outcomes in real-world applications.

### Open Question 3
- Question: How can the trade-off between model accuracy and fairness be effectively managed in the context of robust machine learning?
- Basis in paper: [explicit] The paper discusses the potential for fair representation learning to provide a degree of robustness to distribution shift, but also acknowledges the limitations of fairness metrics and the potential for adversarial manipulation.
- Why unresolved: Balancing accuracy and fairness is a complex challenge, and current approaches may not fully address the trade-offs involved. The impact of fairness constraints on model robustness to different types of distribution shifts is not well understood.
- What evidence would resolve it: Comprehensive empirical studies comparing the performance of fair and unfair models on various datasets with different types of distribution shifts, quantifying the trade-offs between accuracy and fairness in terms of robustness.

## Limitations
- Empirical validation relies heavily on synthetic or benchmark datasets with controlled imperfections, raising questions about real-world applicability
- Assumes known causal structures for data augmentation methods, which may not be available in all settings
- Potential for trade-offs between fairness/robustness and accuracy that are not fully characterized across all domains

## Confidence
- Fair representation learning: High - established theoretical foundations and promising empirical results
- Environment inference for invariant learning: Medium - shows promise on controlled datasets but requires validation on more complex real-world data
- Data augmentation for reinforcement learning: Medium-High - demonstrates improvements on benchmark tasks but needs further validation on more complex domains

## Next Checks
1. Conduct experiments on real-world datasets with known biases (e.g., criminal justice recidivism data) to validate the fair representation learning methods under practical conditions.

2. Test the environment inference method on high-dimensional image datasets with multiple spurious correlations to assess its scalability and effectiveness in more complex scenarios.

3. Evaluate the data augmentation methods on continuous control tasks with longer time horizons and more complex dynamics to verify their ability to handle increased state and action space complexity.