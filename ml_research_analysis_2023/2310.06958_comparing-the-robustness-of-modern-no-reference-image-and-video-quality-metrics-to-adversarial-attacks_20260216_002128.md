---
ver: rpa2
title: Comparing the Robustness of Modern No-Reference Image- and Video-Quality Metrics
  to Adversarial Attacks
arxiv_id: '2310.06958'
source_url: https://arxiv.org/abs/2310.06958
tags:
- uni00000024
- metrics
- uni0000002c
- attacks
- uni00000033
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new benchmark for evaluating the robustness
  of no-reference image- and video-quality metrics to adversarial attacks. The authors
  adapt adversarial attacks from computer vision tasks and test their effectiveness
  against 15 popular no-reference quality metrics.
---

# Comparing the Robustness of Modern No-Reference Image- and Video-Quality Metrics to Adversarial Attacks

## Quick Facts
- arXiv ID: 2310.06958
- Source URL: https://arxiv.org/abs/2310.06958
- Reference count: 40
- Primary result: All tested no-reference quality metrics can be attacked, but MANIQA, NIMA, RANK-IQA, META-IQA, and MDTVSFA show highest resistance

## Executive Summary
This paper introduces a benchmark for evaluating the robustness of no-reference image- and video-quality metrics to adversarial attacks. The authors adapt 8 types of adversarial attacks from computer vision and test their effectiveness against 15 popular no-reference quality metrics. The study finds that while all metrics can be attacked, some show significantly higher resistance. The benchmark is made available online at https://videoprocessing.ai/benchmarks/metrics-robustness.html, along with code for generating attacks and a list of open datasets. This work highlights the importance of considering adversarial robustness when evaluating quality metrics, as some widely used metrics like LPIPS and DISTS have been shown to be vulnerable to such attacks.

## Method Summary
The authors create a benchmark to test the robustness of 15 no-reference quality metrics against 8 types of adversarial attacks (FGSM-based, UAP-based, and perceptual attacks). The evaluation uses 3 test datasets and measures robustness through absolute/relative gain, robustness score, energy distance score, and Wasserstein score. A domain transformation using MDTVSFA as reference normalizes scores across metrics. The evaluation pipeline is automated through GitLab CI/CD, with metrics using default parameters and attacks applied with specific epsilon values and iterations.

## Key Results
- All tested metrics can be attacked by at least one adversarial attack type
- MANIQA, NIMA, RANK-IQA, META-IQA, and MDTVSFA show the highest resistance to attacks
- UAP-based attacks are particularly effective against many metrics
- Domain transformation using MDTVSFA as reference enables fair comparison across metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Metrics with strong global spatial pooling (e.g., MANIQA, META-IQA) resist attacks because adversarial perturbations are averaged out across the image.
- Mechanism: Global pooling reduces the influence of localized adversarial noise by aggregating over the entire spatial extent.
- Core assumption: Adversarial attacks are most effective when they can concentrate perturbations in feature regions most sensitive to quality degradation.
- Evidence anchors:
  - [abstract] "Some metrics showed high resistance to adversarial attacks which makes their usage in benchmarks safer than vulnerable metrics."
  - [section] "MANIQA uses ViT and applies attention mechanisms across the channel and spatial dimensions increasing interaction among different regions of images globally and locally."
  - [corpus] No direct evidence found.

### Mechanism 2
- Claim: Meta-learning on diverse distortions (META-IQA) improves robustness by generalizing across attack patterns.
- Mechanism: Prior knowledge from many distortion types allows the model to distinguish between natural degradations and adversarial perturbations.
- Core assumption: The diversity of training distortions covers the space of adversarial perturbations.
- Evidence anchors:
  - [section] "META-IQA, one of the most robust metrics, uses a relatively small backbone network but efficiently leverages prior knowledge of various image distortions obtained during so-called meta-learning."
  - [abstract] "Some metrics showed high resistance to adversarial attacks which makes their usage in benchmarks safer than vulnerable metrics."
  - [corpus] No direct evidence found.

### Mechanism 3
- Claim: Metrics with strong content-awareness (MDTVSFA) are more resistant because they separate content from distortion.
- Mechanism: By disentangling content features from quality-related features, the metric is less likely to be misled by content-preserving adversarial changes.
- Core assumption: Adversarial attacks that do not alter content significantly will have less impact on content-aware metrics.
- Evidence anchors:
  - [section] "MDTVSFA enhances VSFA with explicit mapping between predicted and dataset-specific scores, supported by multi-dataset training."
  - [abstract] "Some metrics showed high resistance to adversarial attacks which makes their usage in benchmarks safer than vulnerable metrics."
  - [corpus] No direct evidence found.

## Foundational Learning

- Concept: Adversarial attacks aim to increase metric scores without improving visual quality.
  - Why needed here: Understanding the goal of attacks is essential to interpret robustness results.
  - Quick check question: Why would an attacker want to increase a quality metric score without improving visual quality?

- Concept: Domain transformation via optimal transport normalizes metrics for fair comparison.
  - Why needed here: Different metrics have different score ranges and distributions; transformation allows direct comparison of robustness.
  - Quick check question: What problem does optimal transport solve when comparing metric robustness?

- Concept: Wasserstein and energy distance measure distributional shifts in metric scores.
  - Why needed here: These metrics quantify how much the score distribution changes after an attack, indicating attack effectiveness.
  - Quick check question: How do Wasserstein and energy distance differ from simple mean differences in assessing attack impact?

## Architecture Onboarding

- Component map: Attack generation (FGSM, I-FGSM, MI-FGSM, AMI-FGSM, UAP variants, MADC, Korhonen) -> Metric evaluation pipeline -> Domain transformation -> Robustness scoring (absolute/relative gain, Rscore, Wscore, Escore) -> Aggregate results
- Critical path: Generate adversarial examples → apply to metrics → transform scores → compute robustness scores → aggregate results
- Design tradeoffs: Training UAP attacks on high-res data vs. computational cost; using domain transformation vs. raw score comparison; evaluating on multiple datasets vs. depth on fewer
- Failure signatures: If robustness scores are inconsistent across datasets, it may indicate overfitting of attacks to specific data; if all metrics show similar vulnerability, attacks may be too strong or metrics too weak
- First 3 experiments:
  1. Run FGSM attack with ε=10/255 on CLIP-IQA and compare absolute gain vs. MDTVSFA.
  2. Apply Optimized-UAP (amplitude 0.4) to META-IQA and measure energy distance score.
  3. Test MADC attack on RANK-IQA and check if relative gain exceeds 0.5.

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but the following are implied:

- How do the robustness results change when combining multiple adversarial attack types?
- What is the relationship between metrics' architectural complexity and their robustness to adversarial attacks?
- How do different image preprocessing techniques affect metrics' resistance to adversarial attacks?
- How does training data diversity affect metrics' robustness to adversarial attacks?
- What is the impact of different attack amplitudes on the relative robustness rankings of metrics?

## Limitations
- Benchmark depends on attack methodologies which may not represent full space of potential adversarial strategies
- Domain transformation using MDTVSFA as reference introduces potential bias
- Evaluation focuses on synthetic attacks rather than real-world adversarial scenarios

## Confidence
**High Confidence**: The methodology for attack generation and robustness measurement is clearly specified and reproducible.

**Medium Confidence**: The relative ranking of metric robustness is reasonably reliable but could shift with different attack configurations or datasets.

**Low Confidence**: The claim about specific mechanisms underlying metric robustness (global pooling, meta-learning, content-awareness) lacks direct empirical validation within the paper.

## Next Checks
1. Test whether attacks specifically designed to exploit global pooling (e.g., focusing perturbations on pooled regions) can degrade the robustness of MANIQA and similar metrics.

2. Evaluate META-IQA on a dataset containing distortion types not present in its training data to verify if meta-learning provides robustness beyond the training distribution.

3. Apply content-preserving attacks that modify perceptually similar features to MDTVSFA to determine if content-awareness truly provides resistance to adversarial perturbations.