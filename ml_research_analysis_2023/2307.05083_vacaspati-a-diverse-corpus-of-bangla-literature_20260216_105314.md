---
ver: rpa2
title: 'Vacaspati: A Diverse Corpus of Bangla Literature'
arxiv_id: '2307.05083'
source_url: https://arxiv.org/abs/2307.05083
tags:
- bangla
- corpus
- language
- tasks
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces V\u0101caspati, a large and diverse corpus\
  \ of Bangla literature built exclusively from classical literary works. The corpus\
  \ contains over 11 million sentences and 115 million words, spanning multiple dimensions\
  \ including time period, geographic region, composition type, and topic."
---

# Vacaspati: A Diverse Corpus of Bangla Literature

## Quick Facts
- arXiv ID: 2307.05083
- Source URL: https://arxiv.org/abs/2307.05083
- Reference count: 24
- Key outcome: Vācaspati corpus and models achieve state-of-the-art performance on Bangla NLP tasks while using significantly fewer parameters than competing models

## Executive Summary
This paper introduces Vācaspati, a large corpus of Bangla literature built exclusively from classical literary works, containing over 11 million sentences and 115 million words. The corpus is uniquely diverse across time periods, geographic regions, composition types, and topics, spanning from the 14th century to the present. Two models are developed using this corpus: Vācaspati-FT (FastText embeddings) and Vācaspati-BERT (Electra-based language model). The corpus addresses the lack of quality Bangla text data by focusing on literary sources rather than newspapers or social media, capturing linguistic features absent in other corpora. The models demonstrate superior or comparable performance to larger models on multiple downstream tasks while requiring significantly fewer parameters and less training time.

## Method Summary
The authors collected and cleaned a large corpus of Bangla literature from various sources, spanning multiple time periods and genres. They developed FastText embeddings and an Electra-based language model using this corpus, then fine-tuned these models on various downstream NLP tasks including word similarity, poem classification, sentiment analysis, spelling error detection, and named entity recognition. The models were evaluated using macro-F1 scores and compared against existing state-of-the-art Bangla language models. The corpus preprocessing involved web scraping, Unicode cleaning, punctuation normalization, and tokenization, while model training used standard approaches for FastText and Electra architectures.

## Key Results
- Vācaspati-FT achieves 64.5% macro-F1 on word similarity compared to 55-57% for competing FastText models
- Vācaspati-BERT demonstrates state-of-the-art performance on multiple downstream tasks while using 7x fewer parameters than BanglaBERT (17M vs 110M+ parameters)
- The corpus contains 11.7 million sentences spanning 14th century to present, including poetry, drama, essays, and specialized topics
- Models built on Vācaspati perform better or comparably to models trained on larger but less diverse corpora

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Literary corpus improves model quality by capturing richer vocabulary and syntactic diversity than newspaper/social media sources
- Mechanism: Literature contains formal and archaic vocabulary, varied syntactic structures, and regional dialects absent in news or social media, enabling models to learn broader language patterns
- Core assumption: Published literature is more linguistically diverse and grammatically correct than informal text sources
- Evidence anchors:
  - [abstract] "We believe that published literature captures the features of a language much better than newspapers, blogs or social media posts which tend to follow only a certain literary pattern and, therefore, miss out on language variety."
  - [section 3] "The sources mostly comprise of newspapers, blogs and social media posts. Newspapers are known to follow a certain style of language which is typically urban and devoid of common words such as 'mahIpati' (king), 'hotRRigaNa' (priest) used in day-to-day lives of native speakers."
  - [corpus] Vācaspati contains 11.7 million sentences spanning 14th century to present, including poetry, drama, essays, and specialized topics like law and religion
- Break condition: If literature corpus contains significant OCR errors or outdated language that modern models cannot effectively learn from

### Mechanism 2
- Claim: Temporal variation in corpus improves model performance across different time periods of language use
- Mechanism: Including works from different centuries captures linguistic evolution, particularly the transition from sAdhu bhAShA to chalita bhAShA, enabling models to handle both historical and modern text
- Core assumption: Language changes over time in systematic ways that can be captured through corpus composition
- Evidence anchors:
  - [abstract] "Our corpus Vācaspati is varied from multiple aspects, including type of composition, topic, author, time, space, etc."
  - [section 3.1] "One of the unique temporal features in Bangla is the transformation of sAdhu bhAShA (or refined language) to chalita bhAShA (or colloquial language). While till the 19th century, all written works were exclusively in sAdhu bhAShA, authors started switching to chalita bhAShA in different decades of the 20th century."
  - [corpus] Temporal breakdown shows works from 14-18th centuries (1.9M words), 19th century (11.8M words), 20th century (80.6M words), and 21st century (20.9M words)
- Break condition: If temporal variation is too extreme, causing models to struggle with context switching between vastly different language forms

### Mechanism 3
- Claim: Smaller Electra-based models with literature corpus can outperform larger BERT models on downstream tasks
- Mechanism: Literature corpus provides high-quality linguistic signal that compensates for smaller model size, while Electra's training objective (replaced token detection) is more efficient than masked language modeling
- Core assumption: Quality of training data can compensate for reduced model capacity
- Evidence anchors:
  - [abstract] "Vācaspati-BERT has far fewer parameters and requires only a fraction of resources compared to other state-of-the-art transformer models and yet performs either better or similar on various downstream tasks."
  - [section 5.3.1] "Our model is 7 times lighter than BanglaBERT (Bhattacharjee et al., 2022), which has 110M parameters and is trained for 2.5M steps in a TPU v3.0 instance."
  - [corpus] Vācaspati-BERT uses 17M parameters vs 110M+ for competing models while achieving better or comparable results
- Break condition: If downstream tasks require extensive world knowledge not captured in literature corpus

## Foundational Learning

- Concept: FastText embeddings and subword information
  - Why needed here: Bangla is morphologically rich with inflections, requiring subword modeling to handle out-of-vocabulary words effectively
  - Quick check question: How does FastText handle words not seen during training compared to standard word2vec?

- Concept: Transformer architectures (BERT, Electra)
  - Why needed here: Modern NLP tasks require contextual understanding that traditional models cannot provide, especially for complex tasks like named entity recognition and spelling correction
  - Quick check question: What is the key difference between BERT's masked language modeling and Electra's replaced token detection?

- Concept: Corpus composition and linguistic diversity
  - Why needed here: Model performance depends heavily on training data quality and variety, particularly for low-resource languages like Bangla
  - Quick check question: Why might literature provide better linguistic coverage than news articles for NLP model training?

## Architecture Onboarding

- Component map:
  - Data collection pipeline: Web scraping → Unicode cleaning → Punctuation normalization → Tokenization
  - Model training: FastText for word embeddings → Electra for transformer model → Fine-tuning for specific tasks
  - Evaluation framework: Word similarity tests → Classification benchmarks → Named entity recognition → Spelling error detection

- Critical path:
  1. Clean and preprocess Vācaspati corpus
  2. Train FastText embeddings on cleaned corpus
  3. Train Electra model using Vācaspati
  4. Fine-tune Electra for specific downstream tasks
  5. Evaluate against benchmark datasets

- Design tradeoffs:
  - Literature vs. news/social media: Better linguistic quality vs. more contemporary relevance
  - Model size vs. performance: Smaller models save resources but may miss some linguistic nuances
  - Temporal coverage: Older texts provide historical context but may be less relevant for modern applications

- Failure signatures:
  - High out-of-vocabulary rate: Indicates corpus cleaning issues or insufficient coverage
  - Poor performance on modern text: Suggests temporal imbalance in corpus
  - Slow training convergence: May indicate data quality issues or inappropriate hyperparameters

- First 3 experiments:
  1. Train FastText on Vācaspati and test word similarity against benchmark dataset
  2. Fine-tune Electra on poem classification task and compare with baseline models
  3. Evaluate spelling error detection on OCR-generated test set to measure context sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Vācaspati models change when tested on literary versus non-literary Bangla text?
- Basis in paper: [explicit] The paper notes that Vācaspati is built exclusively from classical literary works and questions whether this focus might limit its effectiveness on non-literary domains like newspapers, blogs, and social media posts
- Why unresolved: The paper primarily evaluates performance on a mix of literary and non-literary downstream tasks but doesn't systematically compare performance across these domains
- What evidence would resolve it: Testing Vācaspati models on separate datasets of pure literary and pure non-literary Bangla text would show if there's a domain-specific performance gap

### Open Question 2
- Question: What is the minimum corpus size required for Vācaspati to achieve optimal performance on downstream tasks?
- Basis in paper: [inferred] The paper demonstrates that Vācaspati achieves state-of-the-art performance with 115 million words, but doesn't explore whether even smaller literary corpora could suffice
- Why unresolved: The authors used the largest corpus they could compile but didn't experiment with systematically reducing corpus size to find the optimal trade-off between performance and data collection effort
- What evidence would resolve it: Training models on progressively smaller subsets of Vācaspati and measuring performance degradation would reveal the minimum effective corpus size

### Open Question 3
- Question: How do Vācaspati models handle code-mixed Bangla-English text?
- Basis in paper: [inferred] The paper focuses exclusively on pure Bangla literature and mentions that other corpora contain social media posts and blogs, implying code-mixing might be an issue
- Why unresolved: The evaluation tasks don't include code-mixed text scenarios, and the corpus itself doesn't contain any code-mixed content to train on
- What evidence would resolve it: Testing Vācaspati models on code-mixed Bangla-English datasets and comparing their performance to models trained on mixed-language corpora would reveal their adaptability

## Limitations
- Lack of detailed hyperparameter specifications for Electra model training makes exact reproduction challenging
- Temporal distribution heavily favors 20th century texts (80.6M words out of 115.5M total), potentially limiting effectiveness on earlier Bangla forms
- Evaluation relies on benchmark datasets that are not fully described, making it difficult to assess generalizability to real-world applications

## Confidence

**High Confidence:** The claim that literature provides richer linguistic diversity than newspapers and social media is well-supported by linguistic theory and corpus statistics showing coverage of formal, archaic, and regional vocabulary absent in other corpora.

**Medium Confidence:** The assertion that Electra-based models can outperform larger BERT models while using fewer parameters is supported by experimental results, but lack of detailed hyperparameter information and limited comparison to other contemporary models introduces uncertainty about generalizability.

**Low Confidence:** The claim that the corpus captures "linguistic features absent in other corpora" is difficult to verify without access to specific datasets used in other studies for comparison, and without systematic analysis of what linguistic features are actually captured by different corpus types.

## Next Checks

1. **Corpus Composition Analysis:** Conduct systematic comparison of vocabulary coverage between Vācaspati and existing Bangla corpora using statistical measures like type-token ratio and coverage of specific linguistic phenomena to quantify claimed linguistic diversity.

2. **Temporal Generalization Test:** Evaluate Vācaspati-BERT's performance on text samples from different historical periods, particularly focusing on 18th-19th century sAdhu bhAShA texts, to verify that temporal diversity translates to effective handling of historical language forms.

3. **Hyperparameter Sensitivity Analysis:** Systematically vary key Electra training hyperparameters within reasonable ranges to establish stability of reported performance improvements and identify most critical factors for successful model training.