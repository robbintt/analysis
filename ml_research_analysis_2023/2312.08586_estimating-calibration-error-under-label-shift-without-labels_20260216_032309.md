---
ver: rpa2
title: Estimating calibration error under label shift without labels
arxiv_id: '2312.08586'
source_url: https://arxiv.org/abs/2312.08586
tags:
- target
- distribution
- source
- shift
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the first consistent and asymptotically unbiased
  estimator for calibration error under label shift without access to target labels.
  The method leverages importance re-weighting of labeled source data using state-of-the-art
  label shift adaptation techniques to estimate calibration error on unlabeled target
  data.
---

# Estimating calibration error under label shift without labels

## Quick Facts
- arXiv ID: 2312.08586
- Source URL: https://arxiv.org/abs/2312.08586
- Reference count: 40
- Key outcome: Introduces first consistent and asymptotically unbiased estimator for calibration error under label shift without target labels

## Executive Summary
This paper addresses the challenge of estimating calibration error when target labels are unavailable under label shift conditions. The authors propose a novel approach that leverages importance re-weighting of labeled source data using state-of-the-art label shift adaptation techniques to estimate calibration error on unlabeled target data. The method is theoretically grounded with proofs of consistency and asymptotic unbiasedness, and is empirically validated across diverse datasets and model types.

## Method Summary
The method uses importance re-weighting of labeled source distribution to estimate calibration error without target labels. It leverages kernel-based comparisons between target samples and weighted source samples to approximate E[Y|f(X)] under the target distribution. The approach makes use of state-of-the-art label shift adaptation techniques like RLLS, BBSL, ELSA, or EM-BCTS for importance weight estimation, and employs a binning kernel to make the estimator differentiable and integrate with post-hoc calibration methods.

## Key Results
- First consistent and asymptotically unbiased CE estimator under label shift without target labels
- Reliable CE estimates closely track ground truth across diverse datasets, models, and shift intensities
- RLLS importance weight estimator shows particularly strong performance in empirical evaluations
- Provides critical variance estimates for assessing reliability of CE estimates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Importance re-weighting of labeled source data using estimated label shift ratios allows consistent estimation of calibration error without target labels.
- Mechanism: The estimator uses kernel-based comparisons between target samples and weighted source samples to approximate E[Y|f(X)] under the target distribution, bypassing the need for target labels.
- Core assumption: The conditional distribution p(X|Y) remains constant between source and target (label shift assumption).
- Evidence anchors:
  - [abstract] "Our contribution is an approach, which, by leveraging importance re-weighting of the labeled source distribution, provides consistent and asymptotically unbiased CE estimation with respect to the shifted target distribution."
  - [section] "Our approach makes use of importance weights ω = (ω1, ..., ωk), where ωi := pt(Y = i)/ps(Y = i), which would be used to re-weight the source label distribution."
  - [corpus] No direct evidence found in corpus about importance re-weighting mechanism.
- Break condition: If p(X|Y) changes between source and target (violating label shift assumption), the estimator becomes biased.

### Mechanism 2
- Claim: Using a binning kernel makes the estimator differentiable and integrates well with calibration methods.
- Mechanism: The binning kernel k(f(xi), f(xj)) = 1 if predictions fall in same bin, 0 otherwise, simplifies the importance weighting computation while maintaining consistency.
- Core assumption: Adaptive binning with equal number of samples per bin provides sufficient granularity for reliable estimation.
- Evidence anchors:
  - [abstract] "Depending on the choice of kernel, it can also be made differentiable and integrated as part of post-hoc and trainable calibration methods."
  - [section] "Here, in order to facilitate variance estimates in the next section, we propose using a binning kernel defined as: k(f(xi), f(xj)) = {1 if f(xi) and f(xj) fall in the same bin, 0 otherwise}"
  - [corpus] No direct evidence found in corpus about binning kernel differentiability.
- Break condition: If binning creates too few or too many bins, estimation variance increases dramatically.

### Mechanism 3
- Claim: Variance estimation provides critical reliability assessment for calibration error estimates under label shift.
- Mechanism: The variance formula accounts for both within-bin variance and covariance between samples in the same bin, enabling uncertainty quantification without target labels.
- Core assumption: Normal approximation to binomial distribution holds for large enough sample sizes in each bin.
- Evidence anchors:
  - [abstract] "Furthermore, we derive the variance of the CE estimator, both in the case when labels are available (e.g., when estimating CE on source data), and in the label-shifted scenario."
  - [section] "We lay out a procedure for calculating the variance both when labels are available (e.g. on source), and in the label shift scenario when target labels are unavailable."
  - [corpus] No direct evidence found in corpus about variance estimation methodology.
- Break condition: If sample sizes per bin become too small, normal approximation breaks down and variance estimates become unreliable.

## Foundational Learning

- Concept: Importance weighting for domain adaptation
  - Why needed here: The method relies on re-weighting source samples by importance weights to match target distribution
  - Quick check question: How do you compute importance weights ωi = pt(Y=i)/ps(Y=i) when you don't have target labels?

- Concept: Label shift vs covariate shift
  - Why needed here: The method specifically addresses label shift, not other types of distribution shift
  - Quick check question: What's the key difference between label shift and covariate shift that makes this method applicable?

- Concept: Calibration error estimation
  - Why needed here: Understanding how CE is normally computed helps understand why this method is novel
  - Quick check question: Why can't standard CE estimators be used when target labels are unavailable?

## Architecture Onboarding

- Component map: Model f -> Weight estimator -> Binning module -> CE estimator -> Variance calculator
- Critical path: Model predictions → Weight estimation → Binning → CE computation → Variance calculation
- Design tradeoffs:
  - Binning granularity vs estimation variance
  - Choice of weight estimator vs accuracy
  - Sample size vs reliable variance estimates
- Failure signatures:
  - High variance indicates unreliable estimates
  - Abnormal CE values suggest weight estimator issues
  - Divergence between cCEt(ω) and cCEt(ω*) indicates poor weight estimation
- First 3 experiments:
  1. Test CE estimator on balanced source vs balanced target (should match ground truth)
  2. Vary imbalance factor on source while keeping target balanced
  3. Compare different weight estimators on same dataset to identify most reliable one

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed calibration error estimator perform under different types of dataset shift beyond label shift, such as covariate shift or concept drift?
- Basis in paper: [inferred] The paper explicitly states that the estimator is specifically designed for label shift scenarios and acknowledges that other types of dataset shift, particularly covariate shift, are important and beyond the scope of this paper.
- Why unresolved: The paper focuses on label shift and does not provide experimental results or theoretical analysis for other types of dataset shift. Testing the estimator under different shift conditions would require additional datasets and experimental setups.
- What evidence would resolve it: Experimental results comparing the estimator's performance under various types of dataset shift (label shift, covariate shift, concept drift) on multiple datasets would provide evidence of its generalizability.

### Open Question 2
- Question: How sensitive is the proposed estimator to the choice of kernel function in the importance weighting scheme?
- Basis in paper: [explicit] The paper mentions that the estimator is consistent and asymptotically unbiased for any consistent kernel over its domain, but it focuses on a binning kernel due to its common use in calibration error estimation.
- Why unresolved: The paper does not provide empirical results comparing the performance of different kernel functions (e.g., Dirichlet, Triweight) when integrated into the estimator.
- What evidence would resolve it: Experimental results comparing the estimator's performance using different kernel functions (binning, Dirichlet, Triweight) on multiple datasets would indicate the sensitivity to kernel choice.

### Open Question 3
- Question: What is the optimal number of bins for the adaptive binning scheme in different classification tasks and dataset sizes?
- Basis in paper: [explicit] The paper shows that for common choices of 10, 15, and 20 bins, the estimates closely align with ground truth, but observes higher discrepancies with 50 bins. However, it does not provide guidance on optimal bin selection for different scenarios.
- Why unresolved: The paper presents results for a fixed number of bins (15) and only briefly mentions the impact of using 50 bins, without providing a systematic analysis of bin selection across different tasks and dataset sizes.
- What evidence would resolve it: A comprehensive study examining the impact of different numbers of bins on the estimator's performance across various classification tasks, dataset sizes, and imbalance factors would provide guidance on optimal bin selection.

## Limitations

- Reliance on label shift assumption (p(X|Y) constant) may not hold in many real-world scenarios
- Binning kernel approach may introduce approximation errors if bin count is not properly calibrated
- Variance estimation depends on normal approximations that may break down for small sample sizes per bin

## Confidence

**High Confidence**: Theoretical consistency and asymptotic unbiasedness of the CE estimator under proper label shift conditions. Mathematically sound variance estimation methodology.

**Medium Confidence**: Empirical performance across diverse datasets and shift intensities, though results may be sensitive to specific dataset characteristics and hyperparameter choices.

**Low Confidence**: Generalizability to scenarios with significant p(X|Y) changes, as the method explicitly assumes this condition holds. Sensitivity to binning granularity choices not thoroughly explored.

## Next Checks

1. **Robustness to Label Shift Violations**: Systematically test the CE estimator on datasets where p(X|Y) changes between source and target, quantifying how violations of the label shift assumption affect estimation accuracy.

2. **Hyperparameter Sensitivity Analysis**: Conduct ablation studies varying the number of bins, sample sizes per bin, and weight estimator hyperparameters to establish guidelines for reliable performance across different dataset scales.

3. **Real-World Application Testing**: Apply the method to production ML systems with known calibration issues under distribution shift, comparing the estimated CE values with observed performance degradation in deployment.