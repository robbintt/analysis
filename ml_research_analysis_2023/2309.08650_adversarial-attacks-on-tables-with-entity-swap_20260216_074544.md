---
ver: rpa2
title: Adversarial Attacks on Tables with Entity Swap
arxiv_id: '2309.08650'
source_url: https://arxiv.org/abs/2309.08650
tags:
- adversarial
- entities
- column
- attack
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first black-box adversarial attack on
  tabular language models (TaLMs) for the column type annotation (CTA) task. The attack
  uses a similarity-based strategy to swap entities within table columns, exploiting
  entity leakage between training and test sets.
---

# Adversarial Attacks on Tables with Entity Swap

## Quick Facts
- arXiv ID: 2309.08650
- Source URL: https://arxiv.org/abs/2309.08650
- Reference count: 24
- This paper introduces the first black-box adversarial attack on tabular language models for the column type annotation task.

## Executive Summary
This paper presents a novel black-box adversarial attack on tabular language models (TaLMs) for column type annotation (CTA) tasks. The attack exploits entity leakage between training and test sets by replacing entities with semantically similar but unseen ones, causing significant performance degradation. The method uses importance scores to identify which entities most influence model predictions, then substitutes these with adversarial examples based on embedding similarity. The attack demonstrates that TaLMs are vulnerable to entity-level manipulations, raising concerns about their robustness in real-world applications.

## Method Summary
The paper introduces a black-box adversarial attack framework for tabular language models that performs entity substitution within table columns. The attack calculates importance scores for each entity by measuring prediction logit changes when entities are masked, then selects top entities for replacement. Using an embedding model, it finds semantically similar but unseen entities to swap with the original entities. The attack is evaluated on the WikiTables dataset using the TURL model for column type annotation, measuring performance degradation in F1 score, precision, and recall as entities are progressively replaced.

## Key Results
- Up to 70% reduction in F1 score when 100% of entities are replaced
- Performance drops are more pronounced when using importance scores to guide entity selection
- The attack exploits entity leakage between training and test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Swapping semantically similar but unseen entities in table columns causes the model to misclassify column types.
- Mechanism: The model relies on entity-level features to predict semantic types. When seen entities are replaced with unseen but semantically similar ones, the learned correlations break, leading to misclassification.
- Core assumption: The model's performance depends on entity co-occurrence patterns that are not robust to entity substitutions of the same type.
- Evidence anchors:
  - [abstract] "replacing entities with semantically similar but unseen ones, the attack causes a substantial drop in model performance: up to 70% reduction in F1 score"
  - [section 3.3] "As an adversarial example, we take the most dissimilar entity from the original entity"
- Break condition: If the model uses contextual or relational features beyond entity identity, the attack may fail.

### Mechanism 2
- Claim: Importance scores guide the attack to target the most influential entities first.
- Mechanism: By masking each entity and measuring the change in prediction logits, the attack identifies which entities contribute most to correct classification. Swapping these first maximizes performance degradation.
- Core assumption: The importance score calculation accurately reflects the entity's influence on model predictions.
- Evidence anchors:
  - [section 3.2] "we calculate an importance score for every entity in the attacked column"
  - [section 4.1] "we notice that the drop in F1 score is around 3% higher when using the importance scores"
- Break condition: If the model is insensitive to individual entity changes or uses averaging over all entities, importance scores become less useful.

### Mechanism 3
- Claim: Data leakage between training and test sets makes models vulnerable to entity substitution attacks.
- Mechanism: When test entities overlap with training entities, the model memorizes entity-type associations. Swapping these with novel entities exposes this weakness.
- Core assumption: The model's high performance is partially due to entity memorization rather than true generalization.
- Evidence anchors:
  - [abstract] "a closer look into the datasets commonly used for evaluation reveals an entity leakage from the train set into the test set"
  - [section 1] "there is a data leakage from entities from the training set into the test set"
- Break condition: If the model uses robust features or the evaluation data has no entity overlap, the attack effectiveness diminishes.

## Foundational Learning

- Concept: Black-box adversarial attacks
  - Why needed here: The attack only uses model prediction scores, not gradients or internal states, making it practical against real-world APIs.
  - Quick check question: What distinguishes a black-box attack from a white-box attack in terms of information access?

- Concept: Entity linking and semantic type classification
  - Why needed here: Understanding how models map table entities to semantic types is crucial for designing effective attacks.
  - Quick check question: How does a CTA model typically represent and classify column entities?

- Concept: Cosine similarity in embedding space
  - Why needed here: The attack relies on finding semantically similar entities using embedding similarity measures.
  - Quick check question: What does a low cosine similarity between two entity embeddings indicate about their semantic relationship?

## Architecture Onboarding

- Component map:
  Input Table -> Importance Scoring -> Entity Selection -> Similarity Search -> Adversarial Table -> CTA Model Evaluation

- Critical path:
  1. Load table and identify target column
  2. Calculate importance scores for each entity
  3. Select top entities based on importance
  4. Find semantically similar replacement entities
  5. Generate adversarial table
  6. Evaluate model performance drop

- Design tradeoffs:
  - Black-box vs white-box: Black-box is more practical but may be less effective
  - Similarity vs randomness: Similarity-based attacks are more realistic but require an embedding model
  - Percentage of entities swapped: Higher percentages cause larger drops but may be more detectable

- Failure signatures:
  - Minimal performance drop despite high entity substitution
  - Model performance improves after substitution (unlikely but possible)
  - Importance scores fail to identify influential entities

- First 3 experiments:
  1. Replace 20% of entities with random entities of same type and measure F1 drop
  2. Replace 20% of entities with most similar entities and measure F1 drop
  3. Replace 20% of entities with least similar entities and measure F1 drop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TaLMs degrade when using more sophisticated adversarial attacks that target multiple columns simultaneously rather than single columns?
- Basis in paper: [explicit] The paper mentions future work to extend evaluation with more sophisticated attacks targeting other models and tasks.
- Why unresolved: The current study only evaluates single-column attacks using the TURL model.
- What evidence would resolve it: Experimental results showing performance degradation when attacking multiple columns simultaneously using more advanced attack strategies.

### Open Question 2
- Question: How do different embedding models affect the effectiveness of the similarity-based adversarial entity sampling strategy?
- Basis in paper: [explicit] The paper uses an embedding model to generate contextualized representations for entity similarity calculations.
- Why unresolved: The study uses a single embedding model without comparing alternatives or analyzing sensitivity to embedding choice.
- What evidence would resolve it: Comparative experiments using different embedding models (BERT, RoBERTa, etc.) to measure variations in attack success rates.

### Open Question 3
- Question: What is the relationship between entity overlap percentage in training/test sets and vulnerability to adversarial attacks across different table interpretation tasks?
- Basis in paper: [explicit] The paper identifies substantial entity leakage between train and test sets as motivation for the attack.
- Why unresolved: The study only examines one dataset and doesn't systematically analyze how varying degrees of entity overlap affect attack effectiveness.
- What evidence would resolve it: Experiments across multiple datasets with controlled entity overlap percentages showing correlation between overlap and attack success rates.

### Open Question 4
- Question: How do defensive techniques like adversarial training or input sanitization affect TaLM robustness against entity swap attacks?
- Basis in paper: [explicit] The paper concludes that TaLMs are susceptible to adversarial attacks and mentions need for more robust evaluation.
- Why unresolved: The study only demonstrates attack effectiveness without exploring defensive countermeasures.
- What evidence would resolve it: Experimental results comparing model performance with and without various defense mechanisms implemented.

## Limitations
- The attack's effectiveness depends on entity leakage between training and test sets, limiting generalizability to datasets without such overlap
- Results are based on a single CTA model (TURL) and dataset (WikiTables), which may not represent all tabular models and domains
- The practical impact may be overstated as the attack exploits a specific vulnerability related to entity memorization

## Confidence
- Core claim (Attack effectiveness): Medium - substantial performance drops demonstrated but dependent on specific dataset characteristics
- Methodology validity: High - experimental approach is sound and well-documented
- Generalizability: Low - results limited to one model and dataset with entity leakage

## Next Checks
1. Test the attack on a dataset with no entity overlap between training and test sets to determine if the performance degradation persists without memorization effects.
2. Implement the attack against multiple CTA models beyond TURL (e.g., TaBERT, TaPas) to assess the vulnerability's prevalence across different architectures.
3. Evaluate whether incorporating entity anonymization or data augmentation during training can mitigate the attack's effectiveness, providing insights into potential defenses.