---
ver: rpa2
title: 'Understanding the Role of Input Token Characters in Language Models: How Does
  Information Loss Affect Performance?'
arxiv_id: '2310.17271'
source_url: https://arxiv.org/abs/2310.17271
tags:
- token
- characters
- tokens
- pre-training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how information loss in input token characters
  affects the performance of pre-trained language models (PLMs). The authors pre-train
  BERT-like models using small subsets of characters from individual tokens (one,
  two, or three characters) and evaluate their performance on GLUE and SuperGLUE benchmarks.
---

# Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?

## Quick Facts
- **arXiv ID**: 2310.17271
- **Source URL**: https://arxiv.org/abs/2310.17271
- **Reference count**: 40
- **Primary result**: Language models pre-trained on single characters retain ~90% performance on SuperGLUE and ~77% on GLUE benchmarks compared to full-token models

## Executive Summary
This paper investigates how information loss in input token characters affects pre-trained language model performance. The authors systematically pre-train BERT-like models using increasingly smaller character subsets from individual tokens (one, two, or three characters) and evaluate their performance on standard NLU benchmarks. Surprisingly, even when pre-training with only a single character from each token, models retain high performance compared to full-token models, with approximately 90% and 77% retention on SuperGLUE and GLUE tasks respectively. The study reveals that first characters and consonants carry more discriminative information than last characters and vowels, and that specific character positions within tokens significantly affect how PLMs encode linguistic information.

## Method Summary
The authors pre-train BERT-base models using BookCorpus and English Wikipedia datasets, converting text to lowercase and tokenizing by whitespace. They create five different vocabularies based on character subsets: single first characters, single last characters, consonants only, vowels only, and three-character combinations (first+middle+last). Models are pre-trained for 1M steps using masked language modeling with standard BERT hyperparameters. Fine-tuning is performed on GLUE and SuperGLUE benchmarks for up to 20 epochs with early stopping. Probing tasks measure syntactic and semantic information retention across different character subset models.

## Key Results
- Models pre-trained on single characters achieve ~90% SuperGLUE and ~77% GLUE performance compared to full-token models
- First character pre-training outperforms last character pre-training across all tasks
- Consonant-only pre-training achieves the best average performance (67.7%) on GLUE tasks
- Models pre-trained on first+middle+last character combinations show the best performance on probing tasks for syntactic and semantic information encoding

## Why This Works (Mechanism)

### Mechanism 1
Language models can recover token identity from partial character information through learned positional and contextual patterns. The model leverages subword co-occurrence statistics and positional embeddings to reconstruct full token representations from character subsets, even when individual characters appear in multiple words. Core assumption: The pre-training data contains sufficient statistical redundancy that allows the model to infer missing characters based on context and position.

### Mechanism 2
First and last characters provide more discriminative information than middle characters for language model learning. Initial and final characters carry stronger syntactic and semantic signals due to their role in word boundaries, morphological markers, and word frequency distributions. Core assumption: The statistical properties of first and last characters differ systematically from middle characters in natural language.

### Mechanism 3
Consonant retention outperforms vowel retention because consonants carry more lexical information. Consonants provide stronger phonological and morphological cues that help distinguish between words, while vowels often follow predictable patterns. Core assumption: Natural language consonant-vowel patterns create differential information density across character types.

## Foundational Learning

- **Concept**: Subword tokenization and vocabulary construction
  - Why needed here: Understanding how character subsets create different vocabulary sizes and token distributions is crucial for interpreting model performance
  - Quick check question: How does reducing tokens to single characters affect the vocabulary size compared to full tokens, and why does this matter for model capacity?

- **Concept**: Masked Language Modeling objective
  - Why needed here: The MLM objective determines what the model learns from partial character inputs and how it reconstructs missing information
  - Quick check question: What is the difference between predicting masked character subsets versus predicting the original full token, and how might this affect learning from partial inputs?

- **Concept**: Probing task methodology
  - Why needed here: Probing tasks reveal what linguistic information models retain from partial character inputs, beyond just task performance
  - Quick check question: How do syntactic and semantic probing tasks differ in what they measure about model representations, and why is this distinction important for understanding character subset learning?

## Architecture Onboarding

- **Component map**: Character subset tokenizer -> Embedding layer -> Transformer encoder -> MLM prediction head -> Task-specific classification heads
- **Critical path**: Character subset tokenization and embedding lookup -> Position encoding application -> Transformer self-attention computation -> MLM prediction from final layer representations -> Cross-entropy loss calculation
- **Design tradeoffs**: Vocabulary size vs. model capacity (smaller vocabularies reduce parameters but may limit representational power); pre-training task choice (predicting character subsets vs. full tokens affects what information the model prioritizes); character position selection (different positions provide varying amounts of discriminative information)
- **Failure signatures**: Loss plateaus at high values during pre-training; fine-tuning performance collapses to random chance; probing task accuracy drops significantly below full-token models; training instability when character subsets overlap extensively
- **First 3 experiments**: 1) Compare pre-training with single character (first vs. last) to establish baseline information retention; 2) Test two-character combinations (first+last vs. first+middle) to identify optimal character positions; 3) Evaluate consonant-only vs. vowel-only pre-training to measure character type information content

## Open Questions the Paper Calls Out

### Open Question 1
How do language models process partial word information compared to human reading comprehension strategies? While the paper shows that models can process partial tokens effectively, it doesn't directly compare the cognitive mechanisms of humans and models. Comparative studies of human eye-tracking data and model attention patterns when processing partial words would help resolve this question.

### Open Question 2
What specific linguistic information is encoded when language models are pre-trained on single characters versus full tokens? The probing tasks used provide broad categories but don't reveal detailed patterns of information preservation at the character level. Detailed analysis of feature importance across different character positions would help answer this question.

### Open Question 3
How does information loss in token characters affect different types of language models (e.g., generative vs. discriminative)? The paper only tests BERT-like models and mentions that testing different model types is computationally prohibitive, but doesn't explore how other architectures might handle partial tokens. Systematic comparison of various model architectures would help resolve this question.

## Limitations
- Experiments are limited to English text using Wikipedia and BookCorpus, with no cross-linguistic validation
- The paper doesn't distinguish between character-level and subword-level modeling approaches
- Statistical significance testing is not reported for performance comparisons
- No information-theoretic analysis to quantify information retention in character subsets

## Confidence

**High Confidence Claims**:
- Models pre-trained on partial character inputs retain substantial performance compared to full-token models
- First characters provide more discriminative information than last characters for language model learning

**Medium Confidence Claims**:
- Consonant characters carry more lexical information than vowels
- Specific character position combinations (first+middle+last) optimize linguistic information encoding

**Low Confidence Claims**:
- The exact mechanisms by which PLMs reconstruct token identity from character subsets
- Generalization of findings to non-English languages or specialized domains

## Next Checks
1. **Statistical Significance Testing**: Replicate key experiments with 10+ random seeds and report confidence intervals, p-values, and effect sizes for all performance comparisons
2. **Cross-Lingual Transfer Experiments**: Pre-train models on character subsets using morphologically rich languages (e.g., Arabic, Finnish) and logographic systems (e.g., Chinese) to test the universality of observed positional and character-type effects
3. **Information-Theoretic Analysis**: Conduct controlled experiments measuring mutual information between character subsets and full tokens, and between character subsets and linguistic properties (syntax, semantics), to validate the proposed reconstruction mechanisms quantitatively