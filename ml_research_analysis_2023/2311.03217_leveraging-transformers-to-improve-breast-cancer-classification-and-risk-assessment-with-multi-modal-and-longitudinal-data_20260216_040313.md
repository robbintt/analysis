---
ver: rpa2
title: Leveraging Transformers to Improve Breast Cancer Classification and Risk Assessment
  with Multi-modal and Longitudinal Data
arxiv_id: '2311.03217'
source_url: https://arxiv.org/abs/2311.03217
tags:
- cancer
- breast
- risk
- imaging
- multi-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents MMT, a deep learning model that integrates
  mammography and ultrasound to improve breast cancer detection and risk prediction.
  MMT uses a transformer encoder to combine features from multi-modal imaging and
  leverages prior exams to track temporal changes.
---

# Leveraging Transformers to Improve Breast Cancer Classification and Risk Assessment with Multi-modal and Longitudinal Data

## Quick Facts
- arXiv ID: 2311.03217
- Source URL: https://arxiv.org/abs/2311.03217
- Authors: 
- Reference count: 27
- Key outcome: MMT achieves AUROC of 0.943 in cancer detection and 0.826 in 5-year risk prediction using multi-modal imaging and transformer-based feature fusion

## Executive Summary
This study introduces MMT, a transformer-based deep learning model that integrates mammography (FFDM, DBT) and ultrasound imaging to improve breast cancer detection and risk prediction. The model leverages self-attention mechanisms to combine features from different imaging modalities and tracks temporal changes by comparing current exams to prior imaging. Trained on 1.3 million exams, MMT demonstrates superior performance compared to single-modality baselines and prior risk models, with AUROC scores of 0.943 for cancer detection and 0.826 for 5-year risk prediction.

## Method Summary
MMT employs modality-specific object detectors (YOLOX, MogaNet, GMIC, UltraNet) to extract regions of interest from FFDM, DBT, and ultrasound images. These features are projected through modality-specific layers and concatenated with categorical embeddings for non-image variables. A transformer encoder then processes this sequence using self-attention to integrate cross-modal and temporal information. The model outputs cumulative cancer risk using an additive hazard layer with sigmoid nonlinearity. Training occurs in two phases: first training individual detectors, then fine-tuning the complete MMT architecture using ensemble averaging of top-performing models.

## Key Results
- AUROC of 0.943 for detecting existing cancers across multi-modal imaging
- AUROC of 0.826 for predicting 5-year cancer risk
- Outperforms single-modality baselines and prior risk prediction models
- Demonstrates benefits of combining multi-modal and longitudinal imaging data

## Why This Works (Mechanism)

### Mechanism 1
The transformer encoder effectively integrates heterogeneous features from different imaging modalities by learning cross-modal attention patterns. The transformer applies multi-head self-attention across ROI feature vectors from FFDM, DBT, and ultrasound, allowing it to weigh modality-specific information based on relevance to cancer detection and risk prediction. Core assumption: Malignant lesions exhibit complementary visual patterns across modalities that can be captured through attention mechanisms.

### Mechanism 2
Comparing current exams to prior imaging enables detection of temporal tissue changes associated with cancer development. The transformer processes ROI vectors from both current and prior exams, learning to identify suspicious changes in tissue patterns over time through its sequential processing capability. Core assumption: Early-stage cancer development manifests as detectable changes in tissue appearance that can be identified through longitudinal comparison.

### Mechanism 3
The additive hazard layer with sigmoid nonlinearity effectively models cumulative cancer risk over time intervals. The architecture computes cumulative risk as a sum of baseline risk and incremental risks for each time interval, then applies sigmoid to produce calibrated probabilities. Core assumption: Cancer risk accumulates additively across time intervals, allowing decomposition into baseline and incremental components.

## Foundational Learning

- Concept: Transformer self-attention mechanisms
  - Why needed here: To understand how the model integrates information across different imaging modalities and temporal sequences
  - Quick check question: How does multi-head attention allow the transformer to capture different types of relationships between ROI features?

- Concept: Multi-modal feature fusion strategies
  - Why needed here: To evaluate why the transformer approach outperforms simple ensemble averaging of modality-specific predictions
  - Quick check question: What are the advantages and disadvantages of early fusion, late fusion, and attention-based fusion for multi-modal medical imaging?

- Concept: Additive hazard modeling in survival analysis
  - Why needed here: To understand the theoretical foundation for the cumulative risk prediction framework
  - Quick check question: How does the additive hazard model differ from proportional hazards models, and when is each appropriate?

## Architecture Onboarding

- Component map: Detector family (YOLOX/MogaNet/GMIC/UltraNet) → Modality-specific projection → Categorical embedding → Transformer encoder → Additive hazard layer → Risk predictions
- Critical path: Input images → Region of interest extraction → Feature projection → Embedding concatenation → Transformer processing → Risk output
- Design tradeoffs: Modality-specific detectors allow specialized feature extraction but require separate training; transformer enables complex cross-modal interactions but increases computational cost; additive hazard modeling provides interpretable risk decomposition but assumes specific risk accumulation patterns
- Failure signatures: Poor cross-modal attention weights suggest modality redundancy; failure to capture temporal changes indicates inadequate temporal modeling; miscalibrated risk predictions suggest issues with the additive hazard layer
- First 3 experiments:
  1. Ablation study comparing transformer-based fusion vs simple averaging of modality-specific predictions
  2. Temporal ablation comparing performance with different numbers of prior exams (0, 1, 2, 3+)
  3. Risk calibration analysis comparing predicted vs actual cancer incidence across time intervals

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does MMT generalize to external datasets with different patient populations and acquisition protocols?
- Basis in paper: The study only evaluated MMT on a single anonymized institution's dataset, limiting understanding of its performance in different clinical settings.
- Why unresolved: No external validation was performed across diverse healthcare systems and populations.
- What evidence would resolve it: Evaluating MMT on multiple external datasets from different institutions and countries would demonstrate its generalizability and robustness across diverse patient populations and imaging protocols.

### Open Question 2
- Question: How would incorporating non-imaging risk factors (e.g., demographics, family history) impact MMT's performance in risk prediction?
- Basis in paper: The current MMT model focuses solely on imaging data, potentially missing important non-imaging risk factors that contribute to breast cancer risk.
- Why unresolved: The study does not explore the impact of adding clinical and demographic risk factors to the model.
- What evidence would resolve it: Training and evaluating MMT with the inclusion of non-imaging risk factors would demonstrate the impact of these factors on the model's risk prediction performance.

### Open Question 3
- Question: What is the clinical impact of MMT's improved risk stratification on screening strategies and patient outcomes?
- Basis in paper: While the paper demonstrates MMT's superior performance in risk prediction, it does not explore the potential clinical implications of this improved risk stratification.
- Why unresolved: The study focuses on technical performance metrics without evaluating real-world clinical implementation or patient outcomes.
- What evidence would resolve it: Conducting clinical studies to assess the impact of MMT's risk predictions on screening strategies, patient management, and long-term outcomes would provide insights into its clinical utility.

## Limitations
- Unknown generalization potential to clinical settings without diverse multi-modal imaging datasets
- Absence of validation on external populations from different healthcare systems
- Exact preprocessing and ROI extraction pipeline details remain unspecified, creating challenges for faithful reproduction

## Confidence

- **High confidence** in the transformer's ability to integrate multi-modal features through self-attention, as this is a well-established mechanism with demonstrated success in related domains
- **Medium confidence** in the additive hazard modeling approach for cumulative risk prediction, as this requires specific assumptions about cancer risk accumulation that may not hold across all populations
- **Medium confidence** in the model's clinical utility, given the strong performance metrics but limited evaluation of real-world implementation challenges and potential biases

## Next Checks
1. Conduct external validation on independent multi-modal imaging datasets from different healthcare systems to assess generalization and potential demographic biases
2. Perform ablation studies comparing transformer-based fusion with alternative multi-modal integration approaches (e.g., early fusion, late fusion, simple averaging) to quantify the specific contribution of attention mechanisms
3. Execute clinical workflow simulation studies to evaluate practical implementation challenges, including radiologist acceptance, integration with existing reporting systems, and impact on diagnostic efficiency