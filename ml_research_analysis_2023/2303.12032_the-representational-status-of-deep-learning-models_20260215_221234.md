---
ver: rpa2
title: The Representational Status of Deep Learning Models
arxiv_id: '2303.12032'
source_url: https://arxiv.org/abs/2303.12032
tags:
- deep
- learning
- representational
- representation
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that while deep learning models (DLMs) represent
  their targets in a relational sense, they do not encode locally semantically decomposable
  representations of their targets. The author distinguishes between functional and
  relational conceptions of representation, arguing that while DLMs clearly have a
  functional role in transforming inputs to outputs, there is no good reason to believe
  they encode fine-grained, locally interpretable representations of their targets
  in the relational sense.
---

# The Representational Status of Deep Learning Models

## Quick Facts
- arXiv ID: 2303.12032
- Source URL: https://arxiv.org/abs/2303.12032
- Reference count: 20
- Primary result: DLMs represent targets relationally but lack locally interpretable subrepresentations

## Executive Summary
This paper argues that deep learning models (DLMs) have a functional role in transforming inputs to outputs but do not encode fine-grained, locally interpretable representations of their targets in the relational sense. The author distinguishes between functional and relational conceptions of representation, claiming that while DLMs clearly transform data (functional), they lack the stable, interpretable subrepresentations required for relational representation. This has immediate implications for explainable AI, suggesting current interpretability approaches are misguided in assuming DLMs encode decomposable representations that can be locally interpreted.

## Method Summary
The paper employs philosophical analysis to examine the representational status of DLMs, systematically evaluating whether they encode fine-grained representations by analyzing composition and distribution arguments. The approach involves clarifying the distinction between functional and relational conceptions of representation, then demonstrating why opacity prevents direct assessment of DLMs' representational status. The analysis culminates in dismantling common arguments that compositional layers or distributed representations imply interpretable subrepresentations.

## Key Results
- DLMs represent targets relationally but not in a locally decomposable, semantically interpretable way
- Opacity prevents direct assessment of DLMs' representational status
- Composition and distribution arguments fail to establish DLMs as fine-grained representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DLMs represent targets relationally but lack locally interpretable subrepresentations
- Core assumption: Representation can be meaningfully separated into functional (mechanistic) and relational (target-mirroring) conceptions
- Evidence: The author argues DLMs transform inputs to outputs (functional) but lack the fine-grained, interpretable subrepresentations required for relational representation

### Mechanism 2
- Claim: Opacity prevents direct assessment of DLMs' representational status
- Core assumption: Opacity is a fundamental barrier to understanding DLMs' internal representational structure
- Evidence: DLMs lack algorithmic, structural, and runtime transparency, making it impossible to directly evaluate whether individual units/weights represent target aspects

### Mechanism 3
- Claim: Distribution and composition arguments fail to establish DLMs as fine-grained representations
- Core assumption: Arguments from composition and distribution are commonly used but flawed justifications
- Evidence: The author systematically dismantles these arguments, showing they either beg the question or lead to arbitrary interpretations

## Foundational Learning

- Concept: Distinction between functional and relational conceptions of representation
  - Why needed: The paper's central argument hinges on this distinction - functional representations are about mechanisms, relational representations are about target-mirroring
  - Quick check: Can you explain the difference between a DLMs functional transformation of data and a relational representation of its target?

- Concept: Epistemic opacity in complex computational systems
- Concept: Composition vs semantic composition in neural networks
- Concept: Distribution in neural networks vs distribution in cognitive science

## Architecture Onboarding

- Component map: Input -> Hidden layers (functional transformations) -> Output
- Critical path: Functional transformation of data through compositional layers
- Design tradeoffs: Between model complexity and interpretability
- Failure signatures: Assuming compositional structure implies interpretable subrepresentations
- First 3 experiments:
  1. Examine how specific DL architectures transform inputs through layers
  2. Test composition/distribution arguments against known interpretable models
  3. Evaluate whether opacity prevents any representational assessment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the exact boundaries between functional and relational conceptions of representation in deep learning models?
- Basis: The paper argues these conceptions are conflated but doesn't provide precise demarcation criteria
- Why unresolved: The distinction is acknowledged as important but lacks rigorous definition
- What evidence would resolve it: A formal framework specifying necessary and sufficient conditions for each conception

### Open Question 2
- Question: Can deep learning models encode fine-grained representations under any specific conditions or architectures?
- Basis: The paper argues DLMs are highly idealized models but doesn't rule out fine-grained representation in all cases
- Why unresolved: The general claim doesn't examine all possible architectures or training regimes
- What evidence would resolve it: Systematic studies showing specific architectures that enable fine-grained representations

### Open Question 3
- Question: What would constitute evidence that a deep learning model's learned transformations correspond to actual mechanisms in the target system?
- Basis: The paper discusses the difficulty of empirically validating representational claims due to opacity
- Why unresolved: Current interpretability methods are criticized as underdetermined, but no alternative validation criteria are proposed
- What evidence would resolve it: A validated method showing that model interpretations reliably correspond to known mechanisms in target systems

## Limitations

- Operates entirely at conceptual level without empirical grounding
- Doesn't engage with specific DL architectures or tasks
- Opacity claim requires more rigorous treatment of what transparency would entail

## Confidence

- High confidence: The distinction between functional and relational conceptions of representation
- Medium confidence: Arguments that composition and distribution don't establish fine-grained representational status
- Medium confidence: The opacity argument as a barrier to representational assessment

## Next Checks

1. Apply the functional/relational distinction to specific DL architectures (transformers, CNNs) to test whether common interpretability assumptions conflate these concepts
2. Systematically review cases where partial transparency has been achieved in DLMs to evaluate whether opacity is truly binary or admits degrees
3. Examine scientific modeling literature for examples where highly idealized models successfully represented phenomena despite lacking fine-grained structural correspondence