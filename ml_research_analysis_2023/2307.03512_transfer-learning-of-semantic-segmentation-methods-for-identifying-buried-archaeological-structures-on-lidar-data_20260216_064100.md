---
ver: rpa2
title: Transfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological
  Structures on LiDAR Data
arxiv_id: '2307.03512'
source_url: https://arxiv.org/abs/2307.03512
tags:
- archaeological
- datasets
- dataset
- learning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates transfer learning (TL) for semantic segmentation\
  \ in archaeology, where limited labeled data is a challenge. The authors compare\
  \ several TL configurations using U-Net and DeepLabV3+ with backbones ResNet, EfficientNet,\
  \ and SegFormer on two LiDAR datasets: Chact\xFAn (Mexico) and Veluwe (Netherlands)."
---

# Transfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data

## Quick Facts
- arXiv ID: 2307.03512
- Source URL: https://arxiv.org/abs/2307.03512
- Authors: 
- Reference count: 21
- Key outcome: Pre-training on ImageNet consistently improves archaeological semantic segmentation, but transfer learning between archaeological datasets shows mixed results depending on object characteristics.

## Executive Summary
This study investigates transfer learning (TL) for semantic segmentation in archaeology, where limited labeled data is a challenge. The authors compare several TL configurations using U-Net and DeepLabV3+ with backbones ResNet, EfficientNet, and SegFormer on two LiDAR datasets: Chactún (Mexico) and Veluwe (Netherlands). Performance is measured via mean Intersection over Union (mIoU) across 5-fold cross-validation. Pre-training on ImageNet provided consistent gains over training from scratch (Kaiming initialization), with up to 11% improvement on Chactún. However, TL between the two archaeological datasets yielded mixed results—Chactún → Veluwe improved performance, but Veluwe → Chactún degraded it. This inconsistency is attributed to differences in object shapes and elevation patterns between the datasets. The study demonstrates that while general pre-training is beneficial, direct TL between archaeological datasets is not reliably effective due to dataset-specific feature distributions.

## Method Summary
The study employs two semantic segmentation architectures (U-Net and DeepLabV3+) with three backbone variants (ResNet, EfficientNet, SegFormer) to classify LiDAR data into archaeological object categories. Three training strategies are compared: training from scratch with Kaiming initialization, pre-training on ImageNet, and transfer learning between the two archaeological datasets. The Chactún dataset contains aguadas, buildings, and platforms, while Veluwe contains barrows, Celtic fields, and charcoal kilns. All experiments use 5-fold cross-validation with automatic hyperparameter optimization via Ray Tune's Asynchronous HyperBand scheduling, measuring performance using mean Intersection over Union (mIoU).

## Key Results
- Pre-training on ImageNet provided consistent gains over training from scratch, with up to 11% improvement on Chactún dataset
- TL from Chactún to Veluwe improved performance, while TL from Veluwe to Chactún degraded performance
- The effectiveness of cross-dataset TL depends on the similarity of object shapes and elevation patterns between datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on ImageNet provides consistent performance gains over training from scratch for archaeological semantic segmentation.
- Mechanism: ImageNet pre-training initializes the model with general feature detectors (edges, textures, shapes) that are transferable to archaeological objects, reducing the need to learn these low-level features from scratch.
- Core assumption: General visual features learned on ImageNet are sufficiently similar to those needed for archaeological object detection in LiDAR data.
- Evidence anchors:
  - [abstract] "Pre-training on ImageNet provided consistent gains over training from scratch (Kaiming initialization), with up to 11% improvement on Chactún."
  - [section] "Pre-training on natural images resulted in similar or even better performance on both target archaeological datasets compared to no pre-training (Kaiming) with minimal additional cost."
  - [corpus] Weak evidence - no direct comparison of ImageNet pre-training vs Kaiming in corpus papers, though general TL benefits are discussed.
- Break condition: If the target archaeological objects have fundamentally different visual characteristics than natural images, the general features may not transfer effectively.

### Mechanism 2
- Claim: Transfer learning between archaeological datasets can improve performance when datasets share similar object characteristics.
- Mechanism: Models pre-trained on one archaeological dataset can transfer learned features specific to archaeological objects to a second dataset, providing a better starting point than random initialization.
- Core assumption: Archaeological datasets contain sufficiently similar object shapes, textures, and elevation patterns that features learned on one dataset are useful for another.
- Evidence anchors:
  - [abstract] "TL between the two archaeological datasets yielded mixed results—Chactún → Veluwe improved performance, but Veluwe → Chactún degraded it."
  - [section] "Transfer-learned Veluwe models, when subsequently fine-tuned on the Chactún dataset, exhibited a detrimental effect, whereas the opposite was observed in the complementary case, where a performance improvement was observed."
  - [corpus] Weak evidence - corpus papers discuss TL for archaeology but don't specifically address cross-dataset TL effectiveness.
- Break condition: When datasets contain objects with fundamentally different shapes, sizes, or elevation patterns, TL can be detrimental.

### Mechanism 3
- Claim: Dataset-specific feature distributions determine the success of cross-dataset transfer learning.
- Mechanism: The effectiveness of TL depends on whether the source dataset's objects share geometric and topographic characteristics with the target dataset's objects.
- Core assumption: The success of TL is determined by the similarity of object shapes and elevation patterns between source and target datasets.
- Evidence anchors:
  - [section] "This contradiction can be attributed to dissimilarities between the two datasets... Experimental evidence suggests that the distinctive characteristics of each dataset class... also influence the final performance of the model during fine-tuning."
  - [section] "We suppose that the elementary features learned from Veluwe, which predominantly consist of relatively small circular shapes representing barrows and charcoal kilns, inhibit the extraction of additional features that characterise objects in the Chactún dataset."
  - [corpus] Weak evidence - corpus doesn't specifically discuss feature distribution differences between archaeological datasets.
- Break condition: If datasets have similar object characteristics but different environmental contexts, this mechanism may not explain TL success or failure.

## Foundational Learning

- Concept: Transfer learning in deep learning
  - Why needed here: The paper relies on TL to overcome limited labeled data in archaeological datasets, so understanding TL principles is essential.
  - Quick check question: What is the primary benefit of using pre-trained models versus training from scratch when labeled data is limited?

- Concept: Semantic segmentation vs object detection
  - Why needed here: The paper specifically uses semantic segmentation (U-Net, DeepLabV3+) rather than object detection, which affects the approach and evaluation.
  - Quick check question: How does semantic segmentation differ from object detection in terms of output and use cases?

- Concept: Cross-validation methodology
  - Why needed here: The paper uses 5-fold cross-validation to evaluate model performance, which is critical for understanding the reliability of results.
  - Quick check question: Why is cross-validation particularly important when working with small datasets?

## Architecture Onboarding

- Component map: U-Net -> ResNet/EfficientNet/SegFormer, DeepLabV3+ -> ResNet/EfficientNet/SegFormer for semantic segmentation of LiDAR data into archaeological object classes
- Critical path: Pre-training → Fine-tuning on target dataset → 5-fold cross-validation → mIoU evaluation
- Design tradeoffs: Choice between different backbone architectures (ResNet for stability, EfficientNet for efficiency, SegFormer for transformer-based features) and between ImageNet vs archaeological dataset pre-training
- Failure signatures: Mixed or degraded performance when transferring between datasets with different object characteristics; overfitting when training from scratch on small datasets
- First 3 experiments:
  1. Train U-Net with ResNet backbone from scratch (Kaiming initialization) on Chactún dataset with 5-fold CV
  2. Fine-tune ImageNet-pretrained U-Net with ResNet on Chactún dataset with 5-fold CV
  3. Fine-tune Veluwe-pretrained U-Net with ResNet on Chactún dataset with 5-fold CV

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of archaeological objects in different datasets make transfer learning between them ineffective?
- Basis in paper: [explicit] The authors note that "the elementary features learned from Veluwe, which predominantly consist of relatively small circular shapes representing barrows and charcoal kilns, inhibit the extraction of additional features that characterise objects in the Chactún dataset"
- Why unresolved: While the authors identify shape differences (circular vs. square-off) and elevation patterns (positive vs. negative), they don't provide quantitative analysis of which specific features cause the transfer learning failure.
- What evidence would resolve it: Systematic feature analysis showing which specific visual characteristics (shape complexity, size distribution, elevation patterns) correlate with successful or unsuccessful transfer learning performance.

### Open Question 2
- Question: Would transfer learning be more effective if archaeological datasets were specifically curated to have similar object characteristics?
- Basis in paper: [inferred] The authors conclude that "the construction of a large new dataset featuring a wide variety of heterogeneous archaeological objects will encourage the effective use of TL between archaeological datasets," implying that dataset similarity matters.
- Why unresolved: The study only tested two datasets with inherently different characteristics, so it's unclear whether transfer learning would work better between more similar archaeological datasets.
- What evidence would resolve it: Experiments comparing transfer learning performance across multiple pairs of archaeological datasets with varying degrees of similarity in object characteristics.

### Open Question 3
- Question: Can fine-tuning strategies be optimized to better handle the transfer of knowledge between datasets with different object characteristics?
- Basis in paper: [explicit] The authors mention that "the distinctive characteristics of each dataset class... also influence the final performance of the model during fine-tuning" and that mixed results were observed.
- Why unresolved: The study used standard fine-tuning approaches without exploring advanced techniques like curriculum learning or adaptive fine-tuning schedules.
- What evidence would resolve it: Comparative experiments testing various fine-tuning strategies (e.g., gradual unfreezing, learning rate scheduling, contrastive learning) to determine optimal approaches for cross-dataset transfer learning in archaeology.

## Limitations
- The study uses relatively small archaeological datasets (492-1,564 tiles), which may limit generalizability to larger datasets
- The analysis of why Veluwe → Chactún transfer failed is speculative and would benefit from more systematic feature analysis
- The specific characteristics that determine successful cross-dataset TL remain unclear

## Confidence
- **High Confidence**: ImageNet pre-training consistently improves performance over training from scratch for archaeological semantic segmentation
- **Medium Confidence**: Cross-dataset TL effectiveness depends on feature similarity between datasets, but the specific characteristics determining success are not fully characterized
- **Medium Confidence**: The mixed TL results are primarily explained by geometric and topographic differences between Chactún and Veluwe datasets

## Next Checks
1. **Feature Similarity Analysis**: Conduct systematic analysis of feature distributions between archaeological datasets using techniques like t-SNE or UMAP visualization to quantify feature space overlap before transfer learning attempts.

2. **Controlled Shape Transfer**: Create synthetic archaeological datasets with controlled geometric variations (varying shapes, sizes, elevation patterns) to systematically test which features enable or inhibit successful transfer learning between datasets.

3. **Multi-Dataset TL**: Extend experiments to include more than two archaeological datasets to determine if TL effectiveness correlates with specific geographic, temporal, or structural similarities between archaeological sites across different regions.