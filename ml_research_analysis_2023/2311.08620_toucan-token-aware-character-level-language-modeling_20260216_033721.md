---
ver: rpa2
title: 'Toucan: Token-Aware Character Level Language Modeling'
arxiv_id: '2311.08620'
source_url: https://arxiv.org/abs/2311.08620
tags:
- token
- toucan
- character
- tokens
- tokenization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Toucan, a technique for augmenting character-level
  language models to decode entire tokens using only the character embedding layer
  and character decoder, without repeatedly reprocessing the entire sequence. The
  method is applied to the Hourglass Transformer with dynamic token pooling, resulting
  in over 2x faster character decoding without a loss in language modeling performance.
---

# Toucan: Token-Aware Character Level Language Modeling

## Quick Facts
- arXiv ID: 2311.08620
- Source URL: https://arxiv.org/abs/2311.08620
- Authors: Not specified
- Reference count: 11
- Key outcome: Over 2x faster character decoding without loss in language modeling performance using token-aware character-level models

## Executive Summary
This paper introduces Toucan, a method to augment character-level language models for more efficient token generation. The approach leverages the Hourglass Transformer with dynamic token pooling and introduces end-of-token vectors to enable token-aware decoding without reprocessing the full sequence. The method achieves significant speed improvements while maintaining language modeling performance and produces longer tokens compared to traditional subword tokenizers like BPE and WordPiece.

## Method Summary
Toucan augments character-level models by injecting learned end-of-token vectors after each token in up-sampled sequences. The character decoder is trained to predict an end-of-token symbol for the last character of each token, while the injected end-of-token vector predicts the first character of the next token. This allows generation of entire tokens using only the embedding layer and character decoder without reprocessing the full sequence. The approach uses the Hourglass Transformer with dynamic token pooling and a boundary predictor that uses gumbel-sigmoid for differentiable token boundary selection.

## Key Results
- Over 2x faster character decoding compared to baseline character-level models
- Maintains bits per character (BPC) performance comparable to standard character-level models
- Learns tokenization that produces longer sequences as single tokens compared to BPE and WordPiece

## Why This Works (Mechanism)

### Mechanism 1
Character-level models can be augmented to decode entire tokens using only the embedding layer and character decoder without reprocessing the full sequence. The model injects learned end-of-token vectors after each token in the up-sampled sequence. The character decoder is trained to predict an end-of-token symbol for the last character of each token, and the injected end-of-token vector predicts the first character of the next token. This allows generation of an entire token without passing the sequence through the tokenizer and token model again.

### Mechanism 2
Dynamic token pooling with a boundary predictor allows end-to-end unsupervised learning of tokenization at a user-defined compression rate. A separate feed-forward network (boundary predictor) selects segmentation points in character sequences to form tokens. The predictor uses gumbel-sigmoid to enable differentiable selection, allowing the model to learn tokenization boundaries during training based on a compression prior (shortening factor).

### Mechanism 3
Moving the residual connection from the tokenizer to the embedding layer reduces decoder dependence on the tokenizer, enabling token-aware decoding. By connecting the embedding layer directly to the decoder (instead of routing through the tokenizer), the decoder can generate tokens using only character embeddings and its own parameters, without needing contextualized token representations from the tokenizer.

## Foundational Learning

- **Character-level vs subword tokenization**: Understanding the trade-offs between character-level models (no separate tokenizer needed, longer sequences) and subword models (efficient but require tokenization) is crucial to grasp why Toucan's approach is beneficial. *Quick check*: What is the primary efficiency challenge of character-level language models compared to subword-level models?

- **Transformer architecture and quadratic complexity**: The Hourglass Transformer's pooling mechanism addresses the quadratic complexity issue of standard transformers on long sequences, which is why it's the base architecture for Toucan. *Quick check*: How does the Hourglass Transformer reduce computational complexity when processing long sequences?

- **Gumbel-softmax for differentiable selection**: The boundary predictor uses gumbel-sigmoid (a variant) for differentiable token boundary selection, enabling end-to-end training of the tokenizer and language model together. *Quick check*: Why is differentiable selection important for training a model that learns to segment tokens?

## Architecture Onboarding

- **Component map**: Embedding layer → Tokenizer (segmentation with boundary predictor) → Token model (processing) → Upsample → Character decoder (generation)
- **Critical path**: Character → Embedding → Tokenizer (segmentation) → Token model (processing) → Upsample → Character decoder (generation)
- **Design tradeoffs**: Speed vs. performance (higher shortening factors increase generation speed but may reduce language modeling quality); Token length vs. generalization (longer tokens may capture more context but could struggle with rare words); End-of-token overhead (adding EOT vectors increases vocabulary size and model complexity)
- **Failure signatures**: Generation speed doesn't improve (check if end-of-token vectors are properly injected and decoder is using only embedding + decoder layers); Language modeling performance drops (verify shortening factor isn't too high and boundary predictor is selecting reasonable token boundaries); Poor tokenization quality (examine if boundary predictor is learning meaningful segmentation points or producing random splits)
- **First 3 experiments**: 1) Verify baseline Hourglass Transformer with dynamic pooling works as expected on text8/wikib40; 2) Implement Toucan modification (EOT vectors + residual move) and confirm it generates tokens correctly but slowly (using full model); 3) Measure speed improvement with Toucan's token-aware decoding vs. baseline generation speed

## Open Questions the Paper Calls Out
No open questions are explicitly called out in the paper.

## Limitations
- Limited empirical validation with only two datasets and one model size
- Architectural complexity may hinder adoption compared to simpler solutions
- Claims about tokenization quality and longer tokens lack qualitative analysis

## Confidence
- **High Confidence**: The Hourglass Transformer with dynamic token pooling provides a viable foundation for the approach. The basic mechanism of using end-of-token vectors for character prediction is technically sound.
- **Medium Confidence**: The speed improvement claims and language modeling performance preservation are plausible but require more rigorous validation. The comparison with BPE/WordPiece tokenization is interesting but lacks depth in qualitative analysis.
- **Low Confidence**: The claims about "greater amount of longer sequences tokenized as single items" and the overall superiority of learned tokenization over fixed vocabularies are weakly supported by the presented evidence.

## Next Checks
1. **Ablation Study on End-of-Token Vectors**: Remove the end-of-token vector mechanism and measure the speed difference between full model reprocessing vs. the proposed token-aware decoding. This would isolate the contribution of EOT vectors to the claimed 2x speedup.

2. **Tokenization Quality Analysis**: Extract and analyze actual tokens learned by the model at different shortening factors (e.g., SF=2, 4, 8). Compare these to human-readable token boundaries and measure metrics like average token length, coverage of common vs. rare words, and handling of morphological variations.

3. **Scaling and Domain Robustness Test**: Evaluate the method on additional datasets representing different text domains (technical documentation, social media text, multilingual text) and at different model scales (10M, 100M, 1B parameters) to assess generalization beyond the base case.