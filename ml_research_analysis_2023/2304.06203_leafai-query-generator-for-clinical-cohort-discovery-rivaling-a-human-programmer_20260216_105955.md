---
ver: rpa2
title: 'LeafAI: query generator for clinical cohort discovery rivaling a human programmer'
arxiv_id: '2304.06203'
source_url: https://arxiv.org/abs/2304.06203
tags:
- criteria
- leafai
- clinical
- logical
- patients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LeafAI is a novel system that automatically generates SQL queries
  from clinical trial eligibility criteria using natural language processing. It employs
  a hybrid deep learning and rule-based approach to perform named entity recognition,
  relation extraction, and sequence-to-sequence transformation.
---

# LeafAI: query generator for clinical cohort discovery rivaling a human programmer

## Quick Facts
- arXiv ID: 2304.06203
- Source URL: https://arxiv.org/abs/2304.06203
- Reference count: 40
- Primary result: LeafAI matched 43% of enrolled patients on average compared to 27% by a human programmer, while taking minutes instead of 26 hours

## Executive Summary
LeafAI is a novel system that automatically generates SQL queries from clinical trial eligibility criteria using natural language processing. It employs a hybrid deep learning and rule-based approach to perform named entity recognition, relation extraction, and sequence-to-sequence transformation. The system incorporates a knowledge base and semantic metadata mapping to enable data model-agnostic query generation. In a comparison with a human database programmer, LeafAI demonstrated superior performance in identifying patients eligible for clinical trials while significantly reducing the time required for query creation.

## Method Summary
LeafAI uses a staged NLP pipeline to transform clinical trial eligibility criteria into SQL queries. The system first applies named entity recognition (BERT-based) to identify clinical entities, then uses relation extraction to determine relationships between entities. These are transformed into logical forms using a fine-tuned T5 sequence-to-sequence model. A normalization module maps extracted entities to UMLS concepts, enabling reasoning over an integrated knowledge base (UMLS + ontologies) through SPARQL queries. The semantic metadata mapping connects database schema elements to UMLS concepts, allowing the system to generate queries regardless of the underlying data model. The system was trained on the Leaf Logical Forms (LLF) corpus and evaluated against a human programmer on 8 clinical trials.

## Key Results
- LeafAI matched a mean of 43% of enrolled patients across 8 clinical trials, compared to 27% by the human programmer
- Human programmer spent 26 hours crafting queries, while LeafAI took only several minutes
- The system demonstrated potential to rival human programmers in identifying patients eligible for clinical trials, saving time and resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid deep learning plus rule-based NER/RE improves accuracy over pure rule-based or pure ML approaches
- Mechanism: The system first applies NER to extract entities, then RE to identify relations between them, then uses a Seq2Seq model (T5) to transform these into structured logical forms. This staged approach allows each component to specialize and reduces noise propagation
- Core assumption: The intermediate logical form representation is easier for the model to learn than direct text-to-SQL transformation
- Evidence anchors:
  - "We incorporated hybrid deep learning and rule-based modules for these, as well as a knowledge base of the Unified Medical Language System (UMLS) and linked ontologies."
  - "After NER and relation extraction are performed, we leverage T5, a state-of-the-art Seq2Seq architecture we fine-tuned for predicting logical forms on the LLF corpus."
- Break condition: If the logical form syntax is too rigid or does not capture real-world eligibility complexity, the Seq2Seq model performance will degrade sharply

### Mechanism 2
- Claim: Data model-agnostic query generation is achieved by mapping database schema elements to UMLS concepts via a semantic metadata mapping (SMM)
- Mechanism: Each database artifact (table, column, etc.) is tagged with one or more UMLS concepts. When the system normalizes eligibility criteria to UMLS concepts, it can then match them to the SMM and generate SQL accordingly
- Core assumption: A sufficiently comprehensive SMM exists that covers all necessary database schema elements and their UMLS mappings
- Evidence anchors:
  - "To enable data-model agnostic query creation, we introduce a novel method for tagging database schema elements using UMLS concepts."
  - "Each AST criterion is mapped to zero or more corresponding entries in the semantic metadata mapping (SMM), which in turn lists meanings, roles, and relations of a database schema in the form of UMLS concepts."
- Break condition: If the database schema contains elements not covered by UMLS or the SMM is incomplete, the system will fail to generate queries for those criteria

### Mechanism 3
- Claim: The integrated knowledge base enables multi-hop reasoning over non-specific criteria, improving recall
- Mechanism: After normalizing to UMLS concepts, the system uses SPARQL queries over the KB (UMLS + ontologies) to infer additional related concepts (e.g., symptoms of diseases, contraindications). This allows it to expand a simple criterion into a richer set of search terms
- Core assumption: The KB contains sufficient relationships and mappings to support the types of reasoning needed for clinical trial eligibility
- Evidence anchors:
  - "We incorporate hybrid deep learning and rule-based modules for these, as well as a knowledge base of the Unified Medical Language System (UMLS) and linked ontologies."
  - "Using LOINC2HPO mappings further allows us to infer phenotypes by lab test results rather than using ICD-10 or SNOMED codes alone."
- Break condition: If the KB lacks necessary relationships or mappings, the system will miss eligible patients or incorrectly exclude them

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: To identify spans of text representing clinical entities (conditions, procedures, etc.) before they can be processed further
  - Quick check question: What is the difference between a named entity and a relation in the context of this system?

- Concept: Relation Extraction (RE)
  - Why needed here: To determine relationships between identified entities (e.g., "Caused-By", "Numeric-Filter") which are crucial for building logical forms
  - Quick check question: How does the system use relations to construct logical forms?

- Concept: Semantic Metadata Mapping (SMM)
  - Why needed here: To enable the system to generate queries for any database schema by mapping schema elements to UMLS concepts
  - Quick check question: What happens if a database table is not included in the SMM?

## Architecture Onboarding

- Component map: API -> NER module -> RE module -> Logical Form Transformer (T5) -> Normalizer (MetaMapLite + BERT) -> Reasoner (SPARQL queries over KB) -> SMM Mapper -> SQL Compiler
- Critical path: API → NER → RE → Logical Form Transformer → Normalizer → Reasoner → SMM Mapper → SQL Compiler
- Design tradeoffs: Using a hybrid approach (deep learning + rules) vs. pure ML or pure rules; flexibility of data model-agnostic generation vs. potential performance hit; complexity of multi-hop reasoning vs. accuracy gains
- Failure signatures: Low recall due to missing KB mappings; high false positives due to imprecise normalization; query generation failures due to incomplete SMM
- First 3 experiments:
  1. Test NER and RE modules on a small set of eligibility criteria to verify entity and relation extraction accuracy
  2. Validate logical form transformation by comparing predicted forms to ground truth on the LLF corpus
  3. Test SMM mapping by creating a mock database schema and verifying that the system can generate correct SQL queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the true recall and precision of LeafAI in identifying patients eligible for clinical trials?
- Basis in paper: The authors acknowledge that the number of truly eligible patients is unknown, and they used enrolled participants as a proxy for evaluation
- Why unresolved: The study only measured the percentage of enrolled patients matched by LeafAI and the human programmer, without knowing the total number of eligible patients
- What evidence would resolve it: Conducting a prospective study where LeafAI's identified patients are compared to the actual eligibility status of patients in real-world clinical trials

### Open Question 2
- Question: How would LeafAI perform on clinical trials with more than 30 lines of eligibility criteria or those studying diseases outside of the evaluated categories?
- Basis in paper: The authors explicitly state that they limited their evaluation to trials with 30 or fewer lines of eligibility criteria and focused on specific disease categories
- Why unresolved: The study's scope was intentionally restricted, and the performance of LeafAI on more complex eligibility criteria or diverse disease areas is unknown
- What evidence would resolve it: Evaluating LeafAI on a larger and more diverse set of clinical trials, including those with more complex eligibility criteria and studying various diseases

### Open Question 3
- Question: How does LeafAI compare to other NLP-based cohort discovery systems in terms of performance and efficiency?
- Basis in paper: The authors mention that they considered but ultimately did not compare LeafAI to another notable system, Criteria2Query, due to methodological differences
- Why unresolved: The study focused on comparing LeafAI to a human programmer and did not directly benchmark against other NLP-based systems
- What evidence would resolve it: Conducting a comprehensive comparison of LeafAI with other state-of-the-art NLP-based cohort discovery systems using standardized evaluation metrics and datasets

## Limitations

- Data Model Dependence: While claiming data-model agnostic query generation, the SMM mapping appears OMOP-specific, potentially overstating true generalizability
- KB Completeness Dependencies: Multi-hop reasoning relies heavily on UMLS and linked ontologies, with unknown performance degradation when mappings are missing
- Clinical Trial Representativeness: The 8 clinical trials used for evaluation are not described in detail, leaving questions about diversity of eligibility criteria types and medical domains

## Confidence

**High Confidence**: The hybrid NLP pipeline architecture (NER → RE → logical form → SQL) is technically sound and well-supported by citations

**Medium Confidence**: The quantitative comparison with human programmer performance (43% vs 27% patient matching) is methodologically reasonable but may not generalize

**Low Confidence**: The data model-agnostic claim and multi-hop reasoning effectiveness lack sufficient empirical validation across diverse schemas and clinical trial types

## Next Checks

1. **Schema Transfer Test**: Deploy LeafAI on a non-OMOP database schema with documented SMM mapping to verify true data-model agnostic performance and identify any schema-specific limitations

2. **KB Coverage Analysis**: Systematically remove KB mappings and measure performance degradation to quantify the system's dependence on complete knowledge base coverage for different clinical domains

3. **Trial Diversity Benchmark**: Test LeafAI on clinical trials spanning diverse medical specialties (oncology, cardiology, neurology) with varying complexity levels to establish performance bounds and identify failure modes