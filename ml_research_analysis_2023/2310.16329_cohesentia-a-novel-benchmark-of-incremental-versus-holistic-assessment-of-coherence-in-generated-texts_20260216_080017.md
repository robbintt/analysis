---
ver: rpa2
title: 'CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment of
  Coherence in Generated Texts'
arxiv_id: '2310.16329'
source_url: https://arxiv.org/abs/2310.16329
tags:
- coherence
- sentence
- text
- score
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CoheSentia, a new benchmark for evaluating
  the coherence of automatically generated texts. It presents two annotation protocols:
  a holistic approach, which assigns a single coherence score to a text, and an incremental
  approach, which evaluates coherence sentence by sentence, pinpointing specific reasons
  for incoherence.'
---

# CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts

## Quick Facts
- arXiv ID: 2310.16329
- Source URL: https://arxiv.org/abs/2310.16329
- Reference count: 16
- Key outcome: A new benchmark for evaluating coherence in generated texts with incremental and holistic annotation protocols

## Executive Summary
This paper introduces CoheSentia, a benchmark for evaluating coherence in automatically generated texts using two distinct annotation protocols: holistic scoring and incremental sentence-level evaluation. The benchmark consists of 500 GPT-3 generated stories annotated by multiple raters. Results demonstrate that the incremental method yields significantly higher inter-annotator agreement compared to the holistic approach. The study also evaluates fine-tuned language models on both coherence scoring and incoherence detection tasks, finding that while models can capture some coherence aspects, their performance remains unsatisfactory, particularly for identifying specific coherence reasons.

## Method Summary
The authors created a benchmark using 500 GPT-3 generated stories, which were manually cleaned and annotated using two protocols: holistic (assigning a single coherence score) and incremental (evaluating each sentence individually with reasons for incoherence). Annotations were collected via Amazon Mechanical Turk with qualification training. Language models (BERT, DeBERTa, T5, GPT-3) were fine-tuned on the annotated data for both coherence scoring and reasoning tasks using classification and generation-based approaches. Model performance was evaluated using precision, recall, F1 scores, and inter-annotator agreement metrics (ICC, Cohen's kappa, Krippendorff's alpha).

## Key Results
- The incremental annotation method achieved higher inter-annotator agreement than the holistic method
- Language models demonstrated better performance on sentence-level coherence detection than paragraph-level coherence scoring
- Models showed relatively good precision but low recall for cohesion features, struggling particularly with relevance detection
- Cohesive errors exhibited a cascading effect, where early errors led to more subsequent incoherence detections

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incremental annotation yields higher inter-annotator agreement than holistic annotation.
- Mechanism: By breaking evaluation into sentence-level judgments, annotators can more precisely detect and agree on specific incoherence points, reducing ambiguity from global interpretation.
- Core assumption: Coherence assessment benefits from decomposing paragraphs into smaller units where reasons for incoherence can be identified.
- Evidence anchors:
  - [abstract]: "Results show that the incremental method yields higher inter-annotator agreement than the holistic method."
  - [section 4.2.3]: "Our analysis shows that the inter-annotator agreement in the incremental mode is higher than in the holistic alternative..."
- Break condition: If sentences are too short or lack context, the incremental method may not capture global coherence issues.

### Mechanism 2
- Claim: Coherence scoring is harder for LLMs than sentence-level coherence detection.
- Mechanism: LLMs trained on sentence-level coherence tasks can leverage local context, but struggle to capture paragraph-level coherence that depends on global discourse structures.
- Core assumption: Coherence involves both local (sentence-to-sentence) and global (topic, narrative arc) components, and current LLMs are better at the former.
- Evidence anchors:
  - [section 6.1.2]: "Models also demonstrate relatively good precision but low recall for cohesion features... most models struggle to capture the concept of relevance..."
  - [section 6.2.2]: "It demonstrates that while GPT performs poorly on coherence scoring of the entire paragraph, it is a lot better in identifying sentence-level coherence..."
- Break condition: If the training data is too limited or lacks diverse coherence patterns, models may fail to generalize.

### Mechanism 3
- Claim: Cohesive errors have a cascading effect on coherence perception.
- Mechanism: Once a cohesive error is identified, annotators are more likely to perceive subsequent sentences as incoherent due to cohesion, leading to a lower overall coherence score.
- Core assumption: Human perception of coherence is influenced by early errors, causing a bias in evaluating later sentences.
- Evidence anchors:
  - [section 4.3]: "once a cohesion-type error appears in a paragraph, the model identifies more sentences as incoherent due to the same type of reason..."
  - [corpus]: Weak - corpus does not explicitly discuss cascading effects, only reports correlations.
- Break condition: If the model or annotator is aware of the bias, they may adjust their evaluation to mitigate the cascading effect.

## Foundational Learning

- Concept: Coherence as a linguistic property
  - Why needed here: Understanding the theoretical foundations of coherence (Reinhart's conditions: cohesion, consistency, relevance) is crucial for designing annotation protocols and interpreting results.
  - Quick check question: Can you explain the difference between cohesion and consistency in the context of discourse coherence?

- Concept: Incremental vs. holistic evaluation methods
  - Why needed here: The paper contrasts these two methods to show the advantages of incremental evaluation for coherence assessment.
  - Quick check question: What are the potential drawbacks of using a holistic approach for coherence evaluation?

- Concept: Inter-annotator agreement metrics
  - Why needed here: The paper uses ICC, Cohen's kappa, and Krippendorff's alpha to measure agreement between annotators, which is essential for validating the annotation protocol.
  - Quick check question: What is the difference between Cohen's kappa and Krippendorff's alpha, and when would you use each?

## Architecture Onboarding

- Component map: GPT-3 story generation -> Text cleaning -> Annotation (holistic and incremental) -> Consensus labeling -> Benchmark creation -> Model training (BERT, DeBERTa, T5, GPT-3) -> Evaluation

- Critical path:
  1. Generate and clean text data
  2. Annotate data using both holistic and incremental protocols
  3. Compute consensus labels and agreement metrics
  4. Fine-tune LLMs on the annotated data
  5. Evaluate model performance and compare with human annotations

- Design tradeoffs:
  - Annotation granularity: Incremental annotation provides more detailed information but is more time-consuming
  - Model choice: Classification-based vs. generation-based models have different strengths and weaknesses for coherence tasks
  - Data size: Balancing the need for a large dataset with the cost and time of annotation

- Failure signatures:
  - Low inter-annotator agreement: Indicates unclear annotation guidelines or subjective aspects of coherence
  - Poor model performance: Suggests that the model is not capturing the relevant features for coherence assessment
  - Imbalanced dataset: May lead to biased models that perform well on one class but poorly on others

- First 3 experiments:
  1. Fine-tune a BERT model on the holistic annotation data and evaluate its coherence scoring performance
  2. Fine-tune a T5 model on the incremental annotation data and evaluate its sentence-level incoherence detection
  3. Compare the performance of GPT-3 and smaller LLMs on the coherence reasoning task to assess the impact of model size

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided text. However, based on the study's findings, potential open questions include:

- How do different text generation models (beyond GPT-3) compare in terms of the types and frequencies of coherence errors they produce?
- Does the incremental annotation method's higher inter-annotator agreement translate to better performance when training coherence detection models?
- What is the relationship between the length of generated texts and the complexity of coherence errors?

## Limitations

- The benchmark is limited to GPT-3 generated stories, potentially limiting generalizability to other text generation models
- The study relies on crowdworkers whose interpretation of coherence may vary, introducing potential subjectivity
- The cascading effect of cohesive errors on coherence perception, while observed, needs further investigation to determine its impact on different text types and lengths

## Confidence

High confidence in the comparative advantage of incremental vs. holistic annotation methods, supported by clear inter-annotator agreement metrics. Medium confidence in the model performance results, as the evaluation focuses on specific language models without extensive ablation studies. Medium confidence in the cascading effect hypothesis, as the evidence is primarily correlational rather than causal.

## Next Checks

1. **Domain Generalization Test**: Evaluate the benchmark protocols on texts generated by different models (e.g., GPT-4, Claude) and across multiple domains (news, technical writing, dialogue) to assess protocol robustness.

2. **Annotation Protocol Stress Test**: Conduct a controlled experiment where annotators are explicitly trained to detect and mitigate cascading effects, measuring whether agreement improves when bias is accounted for.

3. **Model Architecture Comparison**: Systematically compare classification-based and generation-based approaches across multiple model sizes and architectures to identify optimal configurations for both coherence scoring and reasoning tasks.