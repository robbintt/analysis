---
ver: rpa2
title: The complexity of non-stationary reinforcement learning
arxiv_id: '2307.06877'
source_url: https://arxiv.org/abs/2307.06877
tags:
- states
- learning
- step
- state
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proves that, under the Strong Exponential Time Hypothesis,\
  \ modifying a single state-action pair in a Markov decision process (MDP) to maintain\
  \ an approximate optimal value function requires nearly linear time in the number\
  \ of states\u2014effectively as hard as solving the entire problem from scratch.\
  \ The result captures the computational difficulty of non-stationary reinforcement\
  \ learning in the worst case, particularly when local changes can cause large downstream\
  \ effects."
---

# The complexity of non-stationary reinforcement learning

## Quick Facts
- arXiv ID: 2307.06877
- Source URL: https://arxiv.org/abs/2307.06877
- Reference count: 40
- One-line primary result: Local changes to a single state-action pair in MDPs are computationally as hard as solving the entire problem from scratch under SETH.

## Executive Summary
This paper establishes fundamental computational limits for non-stationary reinforcement learning by proving that maintaining an approximate optimal value function under local changes to a single state-action pair requires nearly linear time in the number of states, unless the Strong Exponential Time Hypothesis is false. The result demonstrates that seemingly minor modifications can trigger computationally expensive updates throughout the entire value function due to the interconnected nature of MDPs. In contrast, the authors show that adding new state-action pairs is significantly easier, requiring only polynomial time in the horizon and approximation error, suggesting that exploration-based strategies may be more viable than direct updates in continual learning settings.

## Method Summary
The authors prove computational hardness through a reduction from the Bichromatic Maximum Inner Product (Max-IP) problem to non-stationary reinforcement learning, leveraging the Strong Exponential Time Hypothesis (SETH). They construct a finite-horizon MDP where YES and NO instances of Max-IP map to high and low value optimal policies, respectively. The reduction encodes sets as state-action transitions, making local updates computationally expensive to distinguish. For the positive result, they design an incremental algorithm that efficiently maintains an approximate value function when adding new state-action pairs by exploiting monotonicity properties and lazy propagation of updates.

## Key Results
- Local changes to a single state-action pair require nearly linear time in the number of states to maintain an approximate optimal value function
- Adding new state-action pairs is computationally easier, requiring only polynomial time in the horizon and approximation error
- The computational hardness results hold unless the Strong Exponential Time Hypothesis is false

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modifying a single state-action pair's transition probabilities requires nearly linear time in the number of states to maintain an approximate optimal value function.
- Mechanism: The proof constructs a reduction from the Bichromatic Maximum Inner Product (Max-IP) problem, which is hard under the Strong Exponential Time Hypothesis (SETH). By encoding sets into an MDP's state-action transitions, any local change can cause a cascade of updates throughout the value function due to the interconnectedness of states in the MDP.
- Core assumption: The worst-case scenario where a small local change (two transition probabilities) can affect the optimal value function computation in a way that is as hard as solving the entire problem from scratch.
- Evidence anchors:
  - [abstract] "Modifying the probabilities or the reward of a single state-action pair in a reinforcement learning problem requires an amount of time almost as large as the number of states"
  - [section] "Our main result (Theorem 3.1) states that, in the worst case, an elementary change in an MDP — just updating two transition probabilities in one action at one state of the MDP — requires time (SAH )1−o(1)"
  - [corpus] Weak - related papers discuss tensor approximations and meta-learning but don't directly address computational hardness of local updates.

### Mechanism 2
- Claim: Adding a new state-action pair is significantly easier than modifying an existing one, requiring only polynomial time in the horizon and approximation error.
- Mechanism: The incremental action change model allows the algorithm to use lazy updates and empirical transition estimation. By propagating changes only when they significantly affect downstream states, and leveraging the monotonicity of value functions under incremental changes, the algorithm maintains an ϵ-approximation efficiently.
- Core assumption: The value function is monotonic under incremental action additions, meaning new actions can only improve or leave unchanged the value of existing state-action pairs.
- Evidence anchors:
  - [abstract] "In contrast, we show that just adding a new state-action pair is considerably easier to implement"
  - [section] "Theorem 4.1 (Efficient algorithm, incremental changes). There is an algorithm with amortized runtime eO(H 5/ϵ3) per update"
  - [corpus] Weak - related papers don't discuss the computational complexity difference between adding vs. modifying state-action pairs.

### Mechanism 3
- Claim: The Strong Exponential Time Hypothesis (SETH) provides a foundation for proving the computational hardness of non-stationary reinforcement learning.
- Mechanism: By reducing the Max-IP problem to NSRL, the paper leverages the hardness result from [ARW17] under SETH. This reduction shows that distinguishing between two cases (YES instance with complete containment vs. NO instance with tiny intersections) in Max-IP translates to distinguishing between high-value and low-value optimal policies in the constructed MDP.
- Core assumption: SETH is true, meaning that for any ϵ > 0, there exists k ≥ 3 such that the k-SAT problem on n variables cannot be solved in time O(2(1−ϵ)n).
- Evidence anchors:
  - [abstract] "unless the strong exponential time hypothesis (SETH) is false"
  - [section] "Theorem 3.3 (Bichromatic Maximum Inner Product (Max-IP) [ARW17]). Let γ > 0 be any constant, and let n ∈ Z+, m = no(1), w = 2(log(n))1−o(1). Given two collections of sets B = {B1, . . . , Bn} and C = {C1, . . . , Bn} over universe [m]... Unless SETH is false, no algorithm can distinguish the following two cases in time O(n2−γ)"
  - [corpus] Weak - related papers don't discuss SETH or its application to reinforcement learning complexity.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and their finite-horizon formulation
  - Why needed here: The paper works with finite-horizon MDPs, where the state and action spaces are defined at each step, and the goal is to maintain an approximate value function over a sequence of updates.
  - Quick check question: In a finite-horizon MDP with horizon H, how many steps are there from the initial state to the terminal state?

- Concept: Computational complexity theory and the Strong Exponential Time Hypothesis (SETH)
  - Why needed here: The paper uses SETH to prove that maintaining an approximate value function under local changes is computationally hard, providing a theoretical foundation for the difficulty of non-stationary reinforcement learning.
  - Quick check question: What is the relationship between SETH and the P ≠ NP conjecture?

- Concept: Value iteration and policy iteration algorithms
  - Why needed here: Understanding how these algorithms compute optimal policies is crucial for grasping why local changes can be so computationally expensive, as the value function must be updated throughout the MDP.
  - Quick check question: In value iteration, how is the value of a state updated based on the values of its successor states?

## Architecture Onboarding

- Component map:
  - MDP construction module -> Update sequence generator -> Value function maintenance algorithm -> Lower bound proof module

- Critical path:
  1. Construct the MDP from the Max-IP instance.
  2. Generate the sequence of local updates.
  3. Attempt to maintain the value function using any algorithm.
  4. Show that distinguishing between YES and NO instances of Max-IP requires nearly linear time.

- Design tradeoffs:
  - Accuracy vs. runtime: Maintaining a more accurate approximation of the value function may require more computation time.
  - Local vs. global updates: Updating only affected states may be faster but less accurate than recomputing the entire value function.
  - Amortized vs. worst-case analysis: Focusing on amortized runtime per update may hide the worst-case complexity.

- Failure signatures:
  - If an algorithm claims to maintain the value function in sublinear time per update, it likely fails to distinguish between high-value and low-value optimal policies.
  - If the MDP construction does not correctly encode the Max-IP problem, the reduction may not hold.
  - If the update sequence does not cover all possible local changes, the lower bound may not apply.

- First 3 experiments:
  1. Implement the MDP construction for a small Max-IP instance and verify that the optimal value function correctly encodes the containment relationship between sets.
  2. Generate a sequence of local updates and measure the runtime of a simple value iteration algorithm to maintain the value function.
  3. Compare the performance of the lazy update algorithm (Algorithm 1) on the incremental action change model with a baseline that recomputes the value function from scratch after each addition.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can function approximation techniques circumvent the computational hardness of non-stationary reinforcement learning demonstrated in this paper?
- Basis in paper: [explicit] The authors note that their negative result leaves open the NSRL problem in the case of function approximation and conjecture that a similar negative result may be provable in this case as well.
- Why unresolved: The current hardness result is established for tabular MDPs, but many practical RL applications use function approximation. It's unclear whether the computational barriers persist when using neural networks or other function approximators.
- What evidence would resolve it: A proof showing either that function approximation does not help (similar lower bounds apply) or that certain function classes enable efficient NSRL updates would resolve this question.

### Open Question 2
- Question: How does the computational complexity of maintaining an exact optimal policy compare to maintaining an approximately optimal policy in non-stationary reinforcement learning?
- Basis in paper: [explicit] The authors prove a lower bound for maintaining O(1/T)-approximation to the value of optimal policy under incremental action changes, but do not establish the complexity of maintaining exact optimal policies.
- Why unresolved: While the paper shows that approximate solutions are computationally easier to maintain than exact solutions, the precise complexity gap between exact and approximate policy maintenance remains unclear.
- What evidence would resolve it: A rigorous complexity analysis comparing the time complexity of exact vs. approximate policy maintenance under various types of MDP changes would provide insight into this question.

### Open Question 3
- Question: Can practical heuristics like exploration-based strategies or periodic restarts effectively overcome the computational barriers identified in this paper?
- Basis in paper: [explicit] The authors suggest that alternating between exploration after each change and restarting when benefits diminish may be a viable approach, noting that some state-of-the-art applications already use similar strategies.
- Why unresolved: While the theoretical result shows worst-case intractability, practical applications often rely on heuristics that may perform well in real-world scenarios despite theoretical limitations.
- What evidence would resolve it: Empirical studies comparing different heuristic approaches (exploration-based, restart-based, hybrid) on realistic NSRL problems would reveal whether these strategies can practically mitigate the computational challenges identified.

## Limitations
- The computational hardness results rely heavily on worst-case MDP constructions and may not capture practical RL scenarios with structured state spaces
- The paper focuses on theoretical bounds rather than empirical validation, leaving questions about real-world applicability
- The reduction from Max-IP may not fully represent the types of updates encountered in practical non-stationary reinforcement learning systems

## Confidence

- Claim cluster (computational hardness of local updates): **High confidence** - The reduction from Max-IP to NSRL is well-established, and the proof structure follows standard complexity-theoretic techniques.
- Claim cluster (ease of adding new state-action pairs): **Medium confidence** - While the theoretical framework is sound, the practical implications and empirical performance of the proposed algorithm are not explored.
- Claim cluster (SETH-based lower bounds): **Medium confidence** - The hardness relies on SETH, which, while widely believed, remains unproven.

## Next Checks

1. Implement the MDP construction and value function maintenance algorithm on small-scale instances to verify the theoretical bounds empirically.
2. Test the algorithm's performance on MDPs with different structural properties (e.g., sparse connectivity, decomposable state space) to identify when the worst-case bounds may not apply.
3. Compare the amortized runtime of the proposed algorithm with baseline methods (e.g., full recomputation) on a range of update sequences to assess practical efficiency.