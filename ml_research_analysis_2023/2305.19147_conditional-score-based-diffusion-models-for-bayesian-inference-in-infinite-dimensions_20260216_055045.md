---
ver: rpa2
title: Conditional score-based diffusion models for Bayesian inference in infinite
  dimensions
arxiv_id: '2305.19147'
source_url: https://arxiv.org/abs/2305.19147
tags:
- conditional
- score
- distribution
- have
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of solving inverse problems
  in infinite-dimensional function spaces using score-based diffusion models (SDMs).
  The authors propose a theoretically grounded method for sampling from the posterior
  distribution of Bayesian linear inverse problems in function spaces, extending the
  conditional SDM framework to infinite dimensions.
---

# Conditional score-based diffusion models for Bayesian inference in infinite dimensions

## Quick Facts
- arXiv ID: 2305.19147
- Source URL: https://arxiv.org/abs/2305.19147
- Reference count: 40
- One-line primary result: The paper extends conditional score-based diffusion models to infinite-dimensional Bayesian linear inverse problems, proving consistency of the conditional denoising estimator and analyzing the forward-reverse SDE framework.

## Executive Summary
This paper addresses the challenge of solving inverse problems in infinite-dimensional function spaces using score-based diffusion models (SDMs). The authors propose a theoretically grounded method for sampling from the posterior distribution of Bayesian linear inverse problems in function spaces, extending the conditional SDM framework to infinite dimensions. They define a conditional score in the infinite-dimensional setting and prove that the conditional denoising estimator is a consistent estimator of the conditional score in infinite dimensions. The authors analyze the forward-reverse conditional SDE framework for both Gaussian and general prior measures, showing that under certain conditions, the reverse SDE converges to the target conditional distribution. They also demonstrate the robustness of the learned distribution against perturbations of observations. The paper concludes with numerical experiments validating the approach, showcasing its ability to approximate non-Gaussian multi-modal distributions.

## Method Summary
The paper proposes a method for learning conditional distributions in infinite-dimensional function spaces using score-based diffusion models. The approach involves defining a conditional score in infinite dimensions and proving the consistency of a conditional denoising estimator. The authors then analyze the forward-reverse conditional SDE framework, showing convergence to the target conditional distribution under certain conditions. They demonstrate the method's ability to approximate non-Gaussian multi-modal distributions and its robustness against observation perturbations. The implementation uses Fourier neural operators to parameterize the conditional score and trains the model using a denoising score matching objective.

## Key Results
- The conditional denoising estimator is proven to be a consistent estimator of the conditional score in infinite dimensions.
- The reverse-time SDE using the conditional score converges to the correct posterior distribution exponentially fast under certain conditions.
- The method can approximate non-Gaussian multi-modal distributions and is robust to perturbations in observations.
- Numerical experiments validate the approach on synthetic problems, demonstrating its ability to capture complex conditional distributions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The conditional score in infinite dimensions can be consistently estimated via a denoising objective, enabling amortized sampling from the posterior.
- Mechanism: By defining the conditional score as \( S(t, x, y) = -(1-e^{-t})^{-1} (x - e^{-t/2} E[X_0 | X_t = x, Y = y]) \), and proving the conditional denoising estimator is consistent in infinite dimensions, the paper enables learning a single amortized model that approximates the posterior across all observations.
- Core assumption: The conditional score estimator converges uniformly in time under certain conditions on the prior measure and observation model.
- Evidence anchors:
  - [abstract] "we prove that the conditional denoising estimator is a consistent estimator of the conditional score in infinite dimensions."
  - [section] "Proposition 4. Under Assumption 1, the minimizer in θ of ... is the same as the minimizer of ..."
  - [corpus] Weak evidence; corpus neighbors focus on related diffusion models but not the specific consistency proof for the conditional case.
- Break condition: If the prior or observation model violates the conditions for uniform boundedness of the conditional score (e.g., noiseless observations causing score blow-up at small t), the estimator becomes inconsistent.

### Mechanism 2
- Claim: The reverse-time SDE using the conditional score converges to the correct posterior distribution exponentially fast if initialized from the invariant distribution.
- Mechanism: For Gaussian priors, the paper explicitly computes the mode dynamics and shows that starting the reverse SDE from the invariant distribution yields exact sampling from the posterior. For general priors, under Assumption 1, the same holds by leveraging martingale convergence.
- Core assumption: The expected squared norm of the conditional score is uniformly bounded in time, ensuring the reverse SDE is well-posed.
- Evidence anchors:
  - [abstract] "we prove that the conditional denoising estimator is a consistent estimator of the conditional score in infinite dimensions."
  - [section] "Proposition 2. Under Assumption 1, the solution of the reverse-time SDE ... satisfies Z_T ~ X_0|Y=y."
  - [corpus] Weak evidence; corpus neighbors discuss infinite-dimensional SDMs but do not provide explicit convergence guarantees for the conditional case.
- Break condition: If the expected squared norm of the conditional score is unbounded (e.g., noiseless observations in Gaussian case), the reverse SDE may not converge to the correct posterior.

### Mechanism 3
- Claim: The conditional score can have singular behavior at small times, unlike the unconditional score, requiring careful analysis and conditions.
- Mechanism: The paper shows that in the noiseless observation case, the conditional score blows up like \(1/t\) as \(t \to 0\), while the unconditional score remains bounded. This necessitates conditions (like bounded \(p(j)(1+q(j))\)) to ensure uniform boundedness.
- Core assumption: The observational model and prior structure allow for a uniform bound on the conditional score across all modes and times.
- Evidence anchors:
  - [abstract] "we show that the extension of SDMs to the conditional setting requires some care because the conditional score typically blows up for small times contrarily to the unconditional score."
  - [section] "Proposition 3. We assume that the conditional version of C_µ ... We make a modified version of assumption in (9) as follows. ... Then Assumption 1 holds true."
  - [corpus] Weak evidence; corpus neighbors do not discuss the singular behavior of the conditional score at small times.
- Break condition: If the observational model has modes with unbounded \(p(j)(1+q(j))\) or if observations are noiseless, the conditional score may become unbounded, breaking the method.

## Foundational Learning

- Concept: Infinite-dimensional Gaussian measures and their properties (e.g., Mercer theorem, Cameron-Martin space).
  - Why needed here: The paper relies heavily on the structure of Gaussian measures in Hilbert spaces to define scores and analyze the forward-reverse SDE framework.
  - Quick check question: What is the relationship between the covariance operator of a Gaussian measure and its eigenfunctions?

- Concept: Stochastic differential equations (SDEs) in infinite dimensions and their time reversal.
  - Why needed here: The method is based on reversing a forward diffusion process using the conditional score, requiring understanding of SDEs in Hilbert spaces.
  - Quick check question: What are the conditions for the time reversal of an infinite-dimensional SDE to be well-posed?

- Concept: Conditional expectations and Radon-Nikodym derivatives in infinite dimensions.
  - Why needed here: The conditional score is defined using conditional expectations, and the prior measure is often given as a density with respect to a Gaussian measure.
  - Quick check question: How is the Radon-Nikodym derivative used to define a prior measure that is absolutely continuous with respect to a Gaussian measure?

## Architecture Onboarding

- Component map: Forward SDE -> Conditional score estimator (FNO) -> Reverse SDE sampler
- Critical path: 1) Define the conditional score in infinite dimensions. 2) Prove consistency of the conditional denoising estimator. 3) Establish conditions for uniform boundedness of the conditional score. 4) Implement the score estimator and reverse SDE sampler. 5) Validate on synthetic and real data.
- Design tradeoffs: Amortized inference vs. data-specific inference (computational cost vs. accuracy), choice of neural network architecture (e.g., Fourier Neural Operators for discretization invariance), and handling of the singular behavior of the conditional score at small times.
- Failure signatures: Inconsistent sampling from the posterior (e.g., mode collapse, incorrect uncertainty quantification), slow convergence of the reverse SDE, or numerical instability due to unbounded conditional scores.
- First 3 experiments:
  1. Validate the consistency of the conditional denoising estimator on a simple synthetic problem with known posterior.
  2. Test the convergence of the reverse SDE to the posterior for a Gaussian prior and noiseless observations.
  3. Evaluate the robustness of the method to perturbations in the observations and compare with data-specific inference methods.

## Open Questions the Paper Calls Out
None explicitly mentioned in the provided content.

## Limitations
- The analysis is currently restricted to linear observation models, with nonlinear observations requiring separate treatment (Appendix D).
- The method relies on strong assumptions about the prior measure and observation noise structure, which may not hold in practice.
- The conditional score's singular behavior at small times poses significant challenges, particularly for noiseless observations.

## Confidence
- High confidence: The consistency proof for the conditional denoising estimator in infinite dimensions is mathematically rigorous and well-supported by the theoretical analysis.
- Medium confidence: The convergence guarantees for the reverse-time SDE hold under Assumption 1, but the practical implications of these conditions for real-world applications require further validation.
- Medium confidence: The analysis of the conditional score's singular behavior is sound, but the proposed modifications to handle this issue may have limitations in practice.

## Next Checks
1. Empirical validation of consistency: Test the learned conditional score estimator on a simple infinite-dimensional problem with known posterior to verify that it consistently approximates the true conditional score across different observation values.
2. Robustness to observation noise: Systematically evaluate the method's performance across a spectrum of observation noise levels, from highly noisy to nearly noiseless cases, to quantify its sensitivity to the singular behavior of the conditional score at small times.
3. Extension to nonlinear observations: Implement and validate the method on a nonlinear observation model (as outlined in Appendix D) to assess the practical feasibility of extending the framework beyond linear inverse problems.