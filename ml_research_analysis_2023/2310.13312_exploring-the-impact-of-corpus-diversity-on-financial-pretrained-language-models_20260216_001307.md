---
ver: rpa2
title: Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models
arxiv_id: '2310.13312'
source_url: https://arxiv.org/abs/2310.13312
tags:
- financial
- corpus
- plms
- tasks
- film
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the impact of corpus diversity on the performance
  of financial pretrained language models (PLMs). We find that existing financial
  PLMs lack diverse training data, resulting in subpar generalization performance.
---

# Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models

## Quick Facts
- arXiv ID: 2310.13312
- Source URL: https://arxiv.org/abs/2310.13312
- Reference count: 28
- Primary result: FiLM achieves 10% performance gains over existing financial PLMs while reducing energy consumption by 82%

## Executive Summary
This study investigates how corpus diversity affects the performance of financial pretrained language models (PLMs). The research reveals that existing financial PLMs suffer from limited training data diversity, resulting in poor generalization across diverse financial tasks. To address this, the authors collect a broad range of financial corpus spanning news, SEC filings, earnings calls, academic papers, and miscellaneous sources. They train FiLM, a financial language model, on these diverse datasets and demonstrate that it outperforms both existing financial PLMs and general domain models on six financial tasks. The study also shows that diverse pretraining reduces energy consumption while improving performance on unseen corpus groups.

## Method Summary
The authors collected 10 financial datasets grouped into five categories: News, SEC filings, Earnings calls, Papers, and MISC. After preprocessing and deduplication (reducing token count from 3.3B to 2.4B), they further pretrained RoBERTa-base on this diverse corpus for one epoch using masked language modeling. The training used a batch size of 16, learning rate of 1e-5, and Adam optimizer without scheduler or warmup. FiLM was then fine-tuned on six financial tasks (FPB, NER, Headline, FiNER, FinQA, FOMC) and compared against existing financial PLMs and general domain models like BERT and RoBERTa.

## Key Results
- FiLM outperforms existing financial PLMs and general domain PLMs on all six evaluated financial tasks
- Diverse pretraining achieves better generalization performance than using numerous tokens from non-diverse corpus
- Incorporating diverse corpora reduces energy consumption by 82% compared to baseline approaches
- Performance improvements extend to unseen corpus groups, demonstrating robust generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse pretraining corpora improve generalization across unseen financial tasks.
- Mechanism: The model learns task-invariant features from varied financial domains, enabling robust performance on tasks outside the pretraining distribution.
- Core assumption: Each financial corpus group contains distinct but complementary linguistic patterns.
- Evidence anchors:
  - [abstract] "Incorporating a diverse corpus, even with fewer tokens, yields a better generalization performance than relying on numerous tokens from a non-diverse corpus."
  - [section §2.1] "None of the corpus groups was distributed over the entire space... it is essential to use data from all groups for pretraining to enable the language model to learn diverse features."
  - [corpus] Sentence embeddings show corpus groups form distinct clusters, indicating unique linguistic patterns.
- Break condition: If corpus groups share high overlap in vocabulary and semantics, diversity benefit diminishes.

### Mechanism 2
- Claim: Deduplication improves efficiency and model accuracy.
- Mechanism: Removing duplicate sentences reduces redundancy in training, allowing the model to learn more diverse patterns within the same computational budget.
- Core assumption: Duplicates provide little new information and may bias the model toward overrepresented patterns.
- Evidence anchors:
  - [section §2.2] "Deduplication could improve the efficiency of pretraining and the accuracy of PLMs... After deduplication, the token count decreased from 3.3B to 2.4B."
  - [corpus] Reduction of 8.26% duplicate ratio across all datasets.
- Break condition: If dataset contains mostly unique content, deduplication yields minimal benefit.

### Mechanism 3
- Claim: Pretraining on diverse corpora reduces energy consumption while improving performance.
- Mechanism: Diverse corpora provide richer signal per token, requiring fewer training steps to reach high performance.
- Core assumption: The model converges faster when exposed to broader data distribution.
- Evidence anchors:
  - [abstract] "Incorporating a diverse corpus, even with fewer tokens, yields a better generalization performance... Furthermore, our experimental results indicate training on diverse corpora reduces energy consumption."
  - [section §4.5] "Compared to FinBERT-Y, FiLM exhibits an 82% reduction in total energy consumption while achieving an average performance of 10% gain."
- Break condition: If pretraining data is already highly diverse, additional diversity yields diminishing returns.

## Foundational Learning

- Concept: Embedding space separation
  - Why needed here: Understanding how corpus groups cluster helps validate the diversity strategy.
  - Quick check question: Can you explain why distinct clusters in embedding space suggest complementary data sources?

- Concept: Pretraining vs fine-tuning distinction
  - Why needed here: The paper focuses on further pretraining RoBERTa, not training from scratch.
  - Quick check question: What is the difference between pretraining and fine-tuning in the context of PLMs?

- Concept: Energy consumption metrics
  - Why needed here: Quantifying efficiency gains is central to the paper's contribution.
  - Quick check question: How is energy consumption calculated for model training?

## Architecture Onboarding

- Component map: RoBERTa-base → deduplication pipeline → multi-corpus pretraining → fine-tuning on financial tasks
- Critical path: Data preprocessing → pretraining on diverse corpus → task-specific fine-tuning → evaluation
- Design tradeoffs: Fewer tokens from diverse sources vs. more tokens from narrow sources; energy efficiency vs. model capacity
- Failure signatures: Poor performance on unseen tasks suggests insufficient corpus diversity; high energy use with marginal gains suggests redundancy in training data
- First 3 experiments:
  1. Compare model performance with varying numbers of corpus groups
  2. Evaluate impact of deduplication on downstream task performance
  3. Test energy consumption difference between diverse vs. narrow pretraining approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance on unseen corpus groups compare to its performance on seen corpus groups?
- Basis in paper: [inferred] The paper states that incorporating diverse corpora can improve performance for unseen tasks, but does not provide specific comparisons between seen and unseen corpus groups.
- Why unresolved: The paper does not provide specific performance metrics for seen versus unseen corpus groups, making it difficult to quantify the impact of diversity on performance.
- What evidence would resolve it: Experimental results comparing the model's performance on seen and unseen corpus groups, with metrics such as accuracy, F1 score, and energy consumption.

### Open Question 2
- Question: What is the optimal balance between corpus diversity and corpus size for training financial PLMs?
- Basis in paper: [inferred] The paper suggests that a more diverse corpus leads to enhanced model performance, but does not provide specific guidance on the optimal balance between diversity and size.
- Why unresolved: The paper does not provide specific guidance on the optimal balance between corpus diversity and size, making it difficult to determine the best approach for training financial PLMs.
- What evidence would resolve it: Experimental results comparing the model's performance on different combinations of corpus diversity and size, with metrics such as accuracy, F1 score, and energy consumption.

### Open Question 3
- Question: How does the model's performance on financial tasks compare to its performance on non-financial tasks?
- Basis in paper: [inferred] The paper focuses on the model's performance on financial tasks, but does not provide comparisons to its performance on non-financial tasks.
- Why unresolved: The paper does not provide specific comparisons between the model's performance on financial and non-financial tasks, making it difficult to determine its generalizability.
- What evidence would resolve it: Experimental results comparing the model's performance on financial and non-financial tasks, with metrics such as accuracy, F1 score, and energy consumption.

## Limitations

- The study demonstrates correlation between diversity and performance but cannot definitively prove causation due to potential confounding factors.
- Energy consumption analysis lacks standardized measurement protocols and doesn't account for full lifecycle costs including data collection and preprocessing energy.
- The claim about "unseen corpus groups" achieving improved performance is based on a single data point rather than systematic testing across multiple unseen domains.

## Confidence

- High Confidence: The finding that deduplication reduces token count from 3.3B to 2.4B (8.26% reduction) and the claim that deduplication improves efficiency are well-supported by direct measurements and reproducible.
- Medium Confidence: The performance superiority of FiLM over existing financial PLMs on the six evaluated tasks is reasonably supported, though the sample size of tasks and the specific datasets used introduce some uncertainty about generalizability.
- Low Confidence: The energy consumption claims lack methodological rigor in measurement and comparison, making it difficult to assess the true efficiency gains across different training scenarios.

## Next Checks

1. Replicate corpus diversity experiments with controlled conditions: Test whether the performance improvements persist when controlling for dataset size by creating matched pairs of diverse vs. non-diverse datasets with equal token counts, and evaluate on multiple unseen task categories beyond the single FPB example.

2. Standardize and verify energy consumption measurements: Implement standardized energy measurement protocols (e.g., GPU power draw monitoring) across all pretraining runs, including data preprocessing and deduplication phases, to establish comparable baseline metrics for FinBERT-Y and other models.

3. Conduct ablation studies on corpus group contributions: Systematically remove individual corpus groups to quantify their marginal contribution to performance, testing whether all five groups are necessary or if a subset provides equivalent benefits, which would inform more efficient training strategies.