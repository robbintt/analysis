---
ver: rpa2
title: Compositional Fusion of Signals in Data Embedding
arxiv_id: '2311.11085'
source_url: https://arxiv.org/abs/2311.11085
tags:
- embedding
- embeddings
- sentence
- word
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We investigate the degree to which vector embeddings can be viewed
  as a fusion of interpretable signals, using two methods: Correlation-based Fusion
  Detection, measuring correlation between known attributes and embeddings, and Additive
  Fusion Detection, viewing embeddings as sums of individual vectors representing
  attributes. We applied these methods to three types of embeddings: word, sentence,
  and graph.'
---

# Compositional Fusion of Signals in Data Embedding

## Quick Facts
- arXiv ID: 2311.11085
- Source URL: https://arxiv.org/abs/2311.11085
- Reference count: 12
- Key outcome: Embeddings can be decomposed into interpretable signals including semantic, morphological, and demographic components using correlation and additive detection methods

## Executive Summary
This paper investigates whether vector embeddings can be decomposed into interpretable signals using two methods: Correlation-based Fusion Detection (CCA) and Additive Fusion Detection (linear decomposition). The authors apply these methods to word, sentence, and graph embeddings, finding that word embeddings combine semantic and morphological signals, BERT sentence embeddings decompose into subject-verb-object word vectors, and graph embeddings contain demographic signals even without explicit training on such attributes. The findings suggest embeddings are fusions of multiple interpretable signals rather than opaque representations.

## Method Summary
The paper presents two complementary methods for detecting compositional structure in embeddings. Correlation-based Fusion Detection uses Canonical Correlation Analysis (CCA) to measure correlations between known attribute embeddings and entity embeddings. Additive Fusion Detection treats embeddings as sums of individual vectors representing attributes, solving a linear system to recover component vectors. Both methods are validated using permutation testing with leave-one-out cross-validation to establish statistical significance. The methods are applied to three embedding types: word embeddings (Word2Vec), sentence embeddings (BERT), and graph embeddings (knowledge graph recommender systems).

## Key Results
- Word2Vec embeddings decompose into semantic and morphological components with cosine similarities above 0.6
- BERT sentence embeddings can be linearly decomposed into subject, verb, and object word vectors
- User embeddings in knowledge graph-based recommender systems contain demographic signals (age, gender) despite no explicit training on these attributes
- Permutation testing confirms statistical significance of compositional signals with p < 0.01 for most test statistics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embeddings can be decomposed into additive combinations of interpretable components
- Mechanism: Linear decomposition via solving ð´ð— = ð” using pseudo-inverse method
- Core assumption: Embedding space has sufficient linear structure for composite attributes
- Evidence anchors: Word embeddings combine semantic and morphological signals; BERT sentences decompose into subject-verb-object vectors
- Break condition: High correlation between attribute vectors or non-linear embedding space

### Mechanism 2
- Claim: Correlation between embeddings and attributes can be detected using CCA
- Mechanism: CCA finds projection vectors maximizing correlation between attribute and embedding matrices
- Core assumption: Attribute and embedding matrices share correlated subspaces
- Evidence anchors: Correlation-based Fusion Detection measures correlation between known attributes and embeddings
- Break condition: Non-linear relationships or noise-dominated signals

### Mechanism 3
- Claim: Permutation testing establishes statistical significance of compositional signals
- Mechanism: Random shuffling of attribute-embedding pairings compared against real performance
- Core assumption: Observed test statistic should be no better than random under null hypothesis
- Evidence anchors: Permutation testing methodology described with 100 random permutations
- Break condition: Small dataset size or high noise levels

## Foundational Learning

- Concept: Linear algebra fundamentals (vector spaces, inner products, projections, pseudo-inverse)
  - Why needed here: All methods rely on linear operations for decomposition and correlation
  - Quick check question: Given a 3Ã—3 matrix ð´ with full column rank and a 3Ã—1 vector ð”, write the formula for the least-squares solution ð—.

- Concept: Canonical Correlation Analysis (CCA)
  - Why needed here: Method 2 uses CCA to detect correlations between attribute and embedding spaces
  - Quick check question: What does the first canonical correlation coefficient represent in terms of the relationship between two datasets?

- Concept: Hypothesis testing and p-values
  - Why needed here: Method 3 uses permutation testing to establish statistical significance
  - Quick check question: If a test statistic is better than all 100 random permutations, what is the one-sided p-value?

## Architecture Onboarding

- Component map: Data preprocessing -> Method selection -> Computation -> Permutation testing -> Result validation
- Critical path: Extract attributes and embeddings â†’ Apply chosen detection method â†’ Compute test statistics â†’ Permutation testing â†’ Validate significance
- Design tradeoffs:
  - Linear vs non-linear methods: Current approach assumes linearity; non-linear methods might capture more complex relationships but sacrifice interpretability
  - Dimensionality: High-dimensional embeddings may require dimensionality reduction before CCA to avoid overfitting
  - Attribute selection: Too many attributes can cause multicollinearity; too few may miss important signals
- Failure signatures:
  - All permutation test results are statistically insignificant (p > 0.01)
  - CCA returns near-zero canonical correlations
  - Linear system solution produces high reconstruction error
  - Attribute-embedding matrices are rank-deficient
- First 3 experiments:
  1. Apply additive fusion detection to Word2Vec embeddings using roots and suffixes as attributes; verify decomposition quality via cosine similarity.
  2. Use CCA to correlate WordNet semantic embeddings with Word2Vec embeddings; examine first canonical correlation coefficient.
  3. Implement permutation test on movie recommender system user embeddings using age and gender attributes; verify p-value < 0.01 for at least one test statistic.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the compositional structure of embeddings vary across different types of data (e.g., text, images, graphs) and what factors contribute to these variations?
- Basis in paper: The paper discusses compositional structure in word, sentence, and graph embeddings but does not extensively compare these variations.
- Why unresolved: The paper focuses on demonstrating the presence of compositional structures rather than exploring the underlying factors causing variations across different data types.
- What evidence would resolve it: Comparative studies analyzing the compositional structure of embeddings across diverse data types and identifying contributing factors.

### Open Question 2
- Question: Can the methods for detecting compositional structure be extended to more complex or hierarchical relationships within embeddings, beyond simple additive models?
- Basis in paper: The paper uses additive models to detect compositional structure but acknowledges the possibility of more complex relationships.
- Why unresolved: The paper primarily focuses on additive models and does not explore more complex or hierarchical relationships within embeddings.
- What evidence would resolve it: Development and application of methods capable of detecting and analyzing more complex compositional structures within embeddings.

### Open Question 3
- Question: How can the findings on compositional structure in embeddings be applied to improve the interpretability and fairness of AI models, particularly in high-stakes applications?
- Basis in paper: The paper mentions the potential for manipulating embeddings to remove bias and explain decisions, but does not provide specific applications.
- Why unresolved: The paper identifies the potential benefits of understanding compositional structure but does not explore practical applications in improving AI interpretability and fairness.
- What evidence would resolve it: Case studies and experiments demonstrating the application of compositional structure findings to improve interpretability and fairness in specific AI models and applications.

## Limitations
- Linear assumption may miss non-linear compositional relationships that exist in real-world embeddings
- MovieLens dataset domain specificity may limit generalizability of demographic signal findings
- Attribute selection guided by domain knowledge rather than systematic exploration
- 100-permutation threshold may be insufficient for small datasets or highly noisy embeddings

## Confidence
- High confidence: Linear decomposition methodology and CCA correlation detection are mathematically sound given their assumptions
- Medium confidence: Application to embedding analysis is reasonable but depends on linearity assumptions holding for real-world embeddings
- Low confidence: Claims about BERT sentence decomposition into subject-verb-object vectors lack rigorous statistical validation

## Next Checks
1. Apply a non-linear decomposition method (e.g., autoencoders) to the same Word2Vec dataset and compare interpretability and reconstruction quality against linear method
2. Test knowledge graph compositional detection on a different domain (e.g., social network data with demographic attributes) to verify generalizability of demographic signal leakage
3. Systematically vary attribute selection for Word2Vec decomposition and measure how quality of compositionality detection changes