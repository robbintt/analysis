---
ver: rpa2
title: 'Real Customization or Just Marketing: Are Customized Versions of Chat GPT
  Useful?'
arxiv_id: '2312.03728'
source_url: https://arxiv.org/abs/2312.03728
tags:
- turbo
- bsvp
- chatgpt
- chatgpt-4
- education
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of a customized ChatGPT-4
  Turbo model (BSVP) as a virtual business statistics professor compared to the standard
  model. The customized model was fine-tuned with specific instructions and contextual
  course materials to provide more tailored responses.
---

# Real Customization or Just Marketing: Are Customized Versions of Chat GPT Useful?

## Quick Facts
- arXiv ID: 2312.03728
- Source URL: https://arxiv.org/abs/2312.03728
- Reference count: 4
- Key outcome: No statistically significant differences found in overall response quality, depth, or personalization between customized BSVP and standard ChatGPT-4 Turbo for general educational queries in basic statistics courses.

## Executive Summary
This study evaluates whether a customized ChatGPT-4 Turbo model (BSVP) provides meaningful advantages over the standard model for educational purposes in business statistics. The customized model was fine-tuned with specific instructions and contextual course materials to provide more tailored responses. While BSVP demonstrated a more relatable communication style and could leverage course documentation for specific queries, no statistically significant differences were found in overall response quality, depth, or personalization compared to ChatGPT-4 Turbo.

## Method Summary
The study prepared a dataset of 136 real student questions from business statistics courses and gathered course-specific materials. Two versions of ChatGPT-4 Turbo were created - one standard and one customized with course materials and communication style instructions via OpenAI's fine-tuning interface. Both models were used to respond to the same set of student questions, which were then evaluated blind by 5 professors using a 0-10 scale across three dimensions: quality, depth, and personalization. Statistical comparison was performed to assess differences between models.

## Key Results
- BSVP provided responses in a more relatable and friendly tone with minor jokes and personalized greetings.
- When explicitly asked for course-specific content, BSVP could leverage contextual documentation to provide superior responses.
- No statistically significant differences were found in overall response quality, depth, or personalization compared to ChatGPT-4 Turbo.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-based fine-tuning changes communication style without altering core task performance.
- Mechanism: Fine-tuning via prompt engineering modifies the model's response tone, greetings, and level of conciseness by conditioning it on specific stylistic instructions, but does not substantially improve the factual correctness or depth of statistical knowledge compared to the base model.
- Core assumption: The base model already has adequate knowledge for basic statistics; fine-tuning mainly influences surface-level expression.
- Evidence anchors:
  - [abstract]: "substantial modification in the style of communication was observed... BSVP provided responses in a more relatable and friendly tone, even incorporating a few minor jokes."
  - [section]: "A substantial modification in the style of communication was indeed observed. As per its training, BSVP provided responses in a much more approachable and friendly tone."

### Mechanism 2
- Claim: Access to contextual course documentation enables targeted responses to specific course content queries.
- Mechanism: The fine-tuned model has access to internal documentation (e.g., course books, R practice exercises), allowing it to generate responses tailored to explicit references to course materials that the base model cannot address.
- Core assumption: The base model lacks access to proprietary or internal documentation; the fine-tuned model is provided with it.
- Evidence anchors:
  - [abstract]: "when explicitly asked for something like 'I would like to practice a programming exercise similar to those in R practice 4,' BSVP was capable of providing a far superior response: having access to contextual documentation, it could fulfill the request, something beyond ChatGPT-4 Turbo's capabilities."
  - [section]: "when explicitly asked for something like 'I would like to practice a programming exercise similar to those in R programming practice 3,' BSVP was capable of providing a much superior response: having access to contextual documentation, it was able to address the request, something that was not possible for ChatGPT-4 Turbo."

### Mechanism 3
- Claim: Fine-tuning does not yield statistically significant gains in overall response quality, depth, or personalization for general queries.
- Mechanism: Despite stylistic and contextual advantages, the fine-tuned model's responses are not measurably better in core academic performance metrics compared to the base model when evaluated blind by external instructors.
- Core assumption: Instructors' evaluations are unbiased and representative of true performance differences.
- Evidence anchors:
  - [abstract]: "no statistically significant differences were found in overall response quality, depth, or personalization compared to ChatGPT-4 Turbo."
  - [section]: "The comparative analysis of the performance of both systems suggests that there are no significant differences in any dimension... no statistically significant differences were observed in the responses between BSVP and ChatGPT-4 Turbo."

## Foundational Learning

- Concept: Fine-tuning vs. prompt engineering
  - Why needed here: The study distinguishes between prompt-based customization and more intensive fine-tuning; understanding the difference is key to interpreting results.
  - Quick check question: Does prompt engineering change the model's learned parameters or only its behavior during inference?

- Concept: Statistical significance testing in educational AI evaluation
  - Why needed here: The study uses t-tests to compare models; understanding p-values and effect sizes is essential for interpreting "no significant difference."
  - Quick check question: What does a p-value > 0.05 imply about the difference between two models' performance?

- Concept: Contextual vs. general knowledge in LLM applications
  - Why needed here: The advantage of BSVP lies in its ability to leverage course-specific documentation; recognizing when this matters is critical for deployment.
  - Quick check question: When would a student benefit from a model with access to course-specific documentation versus a general-purpose model?

## Architecture Onboarding

- Component map: Student question -> Model (standard or customized) -> Response generation -> Blind evaluation by instructors
- Critical path: 1. Student submits question 2. Model checks for course-specific references 3. If found, retrieves relevant documentation 4. Generates response using both prompt conditioning and documentation 5. Response is evaluated for academic quality
- Design tradeoffs:
  - Longer response times for access to documentation vs. real-time interactivity
  - More relatable tone vs. potential for perceived unprofessionalism
  - Contextual accuracy vs. general applicability
- Failure signatures:
  - Style changes without substantive content improvement
  - Documentation access errors or irrelevant retrievals
  - Inconsistent response quality across question types
- First 3 experiments:
  1. Test response quality on a set of general statistics questions with both models, measuring mean scores and standard deviations.
  2. Test response quality on course-specific questions that explicitly reference internal documentation, measuring accuracy and completeness.
  3. Measure response latency for both models under identical conditions to quantify the trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does BSVP's contextual documentation advantage persist when students ask more complex, multi-step questions requiring synthesis across multiple course materials?
- Basis in paper: [explicit] The paper notes BSVP could handle specific queries like "practice exercise similar to R practice 3" better than ChatGPT-4 Turbo due to having access to course documentation, but the study only evaluated basic statistics questions that didn't require this capability.
- Why unresolved: The study focused on basic statistics questions where contextual documentation wasn't necessary, so it didn't test BSVP's advantage in handling more complex, multi-document queries.
- What evidence would resolve it: A study evaluating BSVP on complex questions requiring synthesis across multiple course materials (e.g., combining concepts from different chapters or practices) would determine if the contextual documentation provides meaningful advantages beyond simple referenced queries.

### Open Question 2
- Question: How do students actually perceive and value the differences in communication style between BSVP and ChatGPT-4 Turbo, and does this affect their learning outcomes?
- Basis in paper: [explicit] The paper notes BSVP has a more "relatable and friendly tone" with phrases like "Dear ICADE student" and small jokes, but the evaluation was done by professors, not students, who might be influenced by style.
- Why unresolved: The study used professor evaluators rather than actual students, who are the end users and may have different preferences or be more influenced by communication style when assessing responses.
- What evidence would resolve it: A randomized controlled trial where students use both systems and report their preferences, perceived helpfulness, and learning outcomes would determine if the friendlier communication style actually benefits student learning.

### Open Question 3
- Question: Does the customization advantage of BSVP become more pronounced in advanced or specialized subjects compared to basic statistics?
- Basis in paper: [inferred] The paper suggests the lack of significant differences might be because basic statistics has abundant online information, making responses similar between models, and proposes testing more advanced subjects as future work.
- Why unresolved: The study only tested basic statistics where abundant information online makes both models perform similarly, so the potential advantage of customization for specialized knowledge hasn't been tested.
- What evidence would resolve it: Evaluating BSVP against ChatGPT-4 Turbo on advanced graduate-level courses or highly specialized subjects would determine if customization provides more substantial benefits when general online information is insufficient.

## Limitations
- The study relies on instructor evaluations, which may introduce subjective bias despite blind scoring.
- The fine-tuning process and prompt engineering details are not fully disclosed, making exact replication difficult.
- The sample of student questions may not be representative of all possible educational scenarios, especially for advanced or atypical topics outside the course scope.

## Confidence
- **High Confidence**: The finding that BSVP does not significantly outperform ChatGPT-4 Turbo on general statistics queries (p > 0.05). This is supported by blind instructor evaluations and statistical tests.
- **Medium Confidence**: The advantage of BSVP in course-specific queries is plausible but less robustly supported, as it relies on a single illustrative example rather than systematic testing.
- **Low Confidence**: The claim that fine-tuning only affects style and not core knowledge is inferred but not directly tested; the study does not compare factual accuracy in a controlled way.

## Next Checks
1. **Replicate the evaluation**: Conduct blind scoring of both models on a broader set of course-specific and general questions by multiple instructors, reporting inter-rater reliability and statistical significance.
2. **Test factual accuracy**: Design a set of factual questions where the correct answer is known, and compare both models' accuracy rates, not just perceived quality.
3. **Probe documentation access**: Systematically test whether BSVP can retrieve and correctly apply information from course materials for a variety of query types, and measure response times.