---
ver: rpa2
title: 'Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable
  and Ontologically Based LLMs'
arxiv_id: '2309.05918'
source_url: https://arxiv.org/abs/2309.05918
tags:
- language
- llms
- some
- human
- since
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that stochastic large language models (LLMs) do
  not truly understand language and fail to make correct inferences in several linguistic
  contexts, such as nominal compounds, intensional contexts, copredication, quantifier
  scope ambiguities, and prepositional phrase attachments. To address these limitations,
  the authors propose applying a bottom-up reverse engineering strategy in a symbolic
  setting, which allows for discovering the ontology implicit in natural language.
---

# Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs

## Quick Facts
- **arXiv ID:** 2309.05918
- **Source URL:** https://arxiv.org/abs/2309.05918
- **Reference count:** 28
- **Key outcome:** Stochastic LLMs fail to understand language and make correct inferences in linguistic contexts like nominal compounds, intensional contexts, copredication, and quantifier scope ambiguities.

## Executive Summary
This paper argues that current stochastic large language models lack true language understanding and fail on critical linguistic phenomena including nominal compounds, intensional contexts, copredication, quantifier scope ambiguities, and prepositional phrase attachments. The authors propose a symbolic, explainable approach based on bottom-up reverse engineering of language in a symbolic setting. By discovering language-agnostic primitive relations that form dimensions of meaning, this approach aims to provide more accurate concept similarity and inference capabilities while maintaining explainability through preserved semantic maps of computation.

## Method Summary
The paper proposes a bottom-up reverse engineering strategy that analyzes natural language corpora to discover an implicit ontology. This involves extracting primitive, language-agnostic relations (such as hasProp, inState, agentOf) that serve as dimensions of word meaning. These relations are used to construct concept hierarchies and compute similarity based on shared primitive relations rather than vector-space proximity. The approach aims to overcome LLM limitations by providing explainable, ontologically grounded language models that can handle linguistic phenomena that challenge current systems.

## Key Results
- Stochastic LLMs fail to make correct inferences in several linguistic contexts including nominal compounds and intensional contexts
- Vector-space similarity in LLMs cannot account for concept identity, leading to circular concept representation
- Symbolic representations preserve semantic maps of computation, enabling explainability through traceable inference paths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symbolic representations preserve semantic maps of computation, enabling explainability
- Mechanism: Symbolic systems maintain traceable computation paths through abstract syntax trees, unlike DNNs where tensor decompositions are undecidable
- Core assumption: Semantic maps are necessary for explainability because they allow inference in reverse
- Evidence anchors: Computation in DNNs proceeds by linear combinations and activation functions that are not invertible

### Mechanism 2
- Claim: Bottom-up reverse engineering discovers language-agnostic primitive relations as dimensions of meaning
- Mechanism: Analyzing property distributions across concepts reveals primitive relations that form an underlying ontology
- Core assumption: Natural language usage reflects an underlying ontology that can be reverse-engineered
- Evidence anchors: Properties that can sensibly apply to concepts reveal subset relationships forming primitive relations

### Mechanism 3
- Claim: Concept similarity based on linguistic dimensions outperforms vector-space similarity
- Mechanism: Concepts are represented as weighted sets of primitive relations, with similarity computed through dimension-wise comparisons
- Core assumption: Semantic similarity correlates with shared primitive relations rather than geometric proximity
- Evidence anchors: LLMs can only account for proximity and similarity, not concept identity

## Foundational Learning

- Concept: Subsymbolic vs. Symbolic computation
  - Why needed here: Understanding why DNNs cannot provide explainability requires grasping the fundamental difference in computation handling
  - Quick check question: Can you explain why the computation in a neural network is not invertible, while symbolic computation maintains a semantic map?

- Concept: Bottom-up reverse engineering of language
  - Why needed here: The paper's approach relies on discovering language structure by analyzing actual usage rather than imposing linguistic theories
  - Quick check question: How does analyzing which properties sensibly apply to which concepts help discover an underlying ontology?

- Concept: Intensional vs. Extensional equality
  - Why needed here: Understanding LLM failures in intensional contexts requires distinguishing between equality by value and equality by cognitive content
  - Quick check question: Why does the fact that "Madrid = the capital of Spain" not mean these expressions are interchangeable in all contexts?

## Architecture Onboarding

- Component map: Corpus analysis -> Primitive relation extraction -> Ontology construction -> Concept representation -> Similarity computation
- Critical path: Analyze corpus → Extract primitive relations → Build ontology → Represent concepts → Compute similarity
- Design tradeoffs: Symbolic approach trades computational efficiency for explainability and ontological grounding
- Failure signatures: System fails when linguistic usage doesn't reflect consistent ontology or primitive relations cannot capture semantic phenomena
- First 3 experiments:
  1. Test primitive relation extraction on controlled corpus with known ontological structure
  2. Compare symbolic system similarity scores against human similarity judgments
  3. Evaluate system's ability to handle LLM-struggling linguistic phenomena and compare performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed symbolic bottom-up reverse engineering approach effectively handle all the linguistic phenomena that LLMs struggle with?
- Basis in paper: The paper explicitly lists nominal compounds, intensional contexts, copredication, quantifier scope ambiguities, prepositional phrase attachments, and metonymy as challenges for LLMs
- Why unresolved: The paper presents the approach but doesn't provide concrete evidence or experiments demonstrating its effectiveness across all these phenomena
- What evidence would resolve it: Empirical results showing the symbolic system correctly resolving these linguistic challenges compared to LLM performance

### Open Question 2
- Question: How can the proposed symbolic approach be scaled to handle the full complexity and size of natural language as effectively as LLMs handle syntax and basic semantics?
- Basis in paper: The paper acknowledges LLMs' success in handling syntax at scale but doesn't address how the symbolic approach could achieve similar scalability
- Why unresolved: The paper focuses on theoretical framework without discussing implementation challenges or computational efficiency at scale
- What evidence would resolve it: Demonstration of symbolic system processing large corpora efficiently while maintaining accuracy across diverse linguistic contexts

### Open Question 3
- Question: How does the proposed concept similarity measure based on linguistic dimensions compare to traditional vector-based similarity?
- Basis in paper: The paper introduces this new similarity measure but doesn't provide comparative evaluations against standard approaches
- Why unresolved: The paper describes methodology but lacks empirical validation or comparison studies
- What evidence would resolve it: Systematic comparison of proposed similarity measure against vector-based methods on standard semantic similarity benchmarks

## Limitations
- The symbolic approach lacks concrete implementation details and evaluation metrics
- No empirical validation is provided for claims about LLM limitations or symbolic superiority
- The bottom-up reverse engineering methodology remains abstract without specifying extraction procedures

## Confidence

| Claim | Confidence |
|-------|------------|
| LLMs fail on specific linguistic phenomena | Medium |
| Symbolic representations provide explainability | Low |
| Ontology discovery through reverse engineering works | Low |

## Next Checks

1. Implement prototype system that extracts primitive relations from controlled corpus and tests identification of known ontological structures
2. Conduct systematic evaluations comparing proposed symbolic similarity measure against human judgments and standard vector-based methods on established semantic similarity benchmarks
3. Design experiments directly testing LLM performance on specific linguistic phenomena cited using current state-of-the-art models to establish baseline failure rates before claiming symbolic superiority