---
ver: rpa2
title: 'CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance
  and Calibration'
arxiv_id: '2309.07822'
source_url: https://arxiv.org/abs/2309.07822
tags:
- question
- context
- calibration
- data
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates using large language models (LLMs) to\
  \ generate counterfactual (CF) instances that augment training data for small language\
  \ models (SLMs) in extractive question answering, aiming to improve out-of-domain\
  \ (OOD) performance and calibration. The authors propose two approaches\u2014Solo-QAG\
  \ and Duo-QAG\u2014that prompt LLMs to generate minimally altered CF questions and\
  \ answers."
---

# CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration

## Quick Facts
- arXiv ID: 2309.07822
- Source URL: https://arxiv.org/abs/2309.07822
- Reference count: 40
- Primary result: LLM-generated counterfactuals improve OOD performance by 2-4% and calibration by up to 20% relative improvement

## Executive Summary
This paper introduces CATfOOD, a method for improving out-of-domain (OOD) performance and calibration of small language models (SLMs) in extractive question answering by augmenting training data with counterfactual instances generated by large language models (LLMs). The authors propose two LLM prompting strategies (Solo-QAG and Duo-QAG) that generate minimally altered counterfactual questions and answers. Across various LLM generators, counterfactual augmentation consistently enhances OOD performance and improves model calibration, with gains correlating with higher diversity in counterfactual surface form and semantic content.

## Method Summary
The CATfOOD approach uses LLMs (GPT-JT, LLaMA, Alpaca, GPT-NeoxT, Flan-T5-xxl, Flan-UL2) to generate counterfactual questions and answers that minimally alter original SQuAD instances. Two prompting strategies are employed: Solo-QAG generates CF questions from original questions, while Duo-QAG generates both CF questions and answers together. Generated CFs are filtered for context relevance and noise before augmenting the SQuAD training data. A RoBERTa-base model is trained on the augmented data and evaluated on seven OOD datasets. Calibration is performed using random forest classifiers with various feature sets including probability, SHAP, attention, integrated gradients, and rationale semantics.

## Key Results
- Counterfactual augmentation improves OOD performance by 2-4% EM/F1 across six datasets
- Model calibration improves by up to 20% relative (MacroCE reduction) when using rationale semantics
- Higher diversity in CF surface form and semantic content correlates with larger OOD performance gains (average correlation 0.55)
- Rationale-augmented calibrators prefer concise explanations with lower entropy importance scores

## Why This Works (Mechanism)

### Mechanism 1
LLMs trained on diverse data generate counterfactuals that are more diverse in surface form and semantic content than baseline RGF approaches. The LLMs' extensive pretraining on varied datasets allows them to produce counterfactuals that cover a broader part of the input space, introducing perturbations like metonymy, topic shifts, and lexical variations. Core assumption: Pre-training diversity in LLMs directly translates to more varied counterfactual generation compared to models fine-tuned on narrow datasets.

### Mechanism 2
Counterfactual augmented models possess more precise explanations, improving calibration performance when rationale semantics are introduced to calibrators. Training with diverse counterfactuals forces the base model to discern nuanced differences between original and perturbed instances, resulting in better internal representations and explanations. Core assumption: The process of consolidating discrepancies between instances and their counterfactuals leads to more informative model explanations.

### Mechanism 3
Rationale-augmented calibrators prefer concise explanations with lower entropy importance scores, leading to better calibration performance. Counterfactual augmented models assign more focused importance to a smaller subset of tokens, resulting in explanations that are more decisive and less noisy for calibrators. Core assumption: Lower entropy in token importance scores indicates more informative and decisive explanations preferred by calibrator models.

## Foundational Learning

- Concept: Counterfactual instances and their role in improving OOD performance
  - Why needed here: Understanding how counterfactuals work is crucial for implementing the CATfOOD approach and interpreting results.
  - Quick check question: What are counterfactual instances and how do they help improve model generalization?

- Concept: Model calibration and its importance in assessing model trustworthiness
  - Why needed here: Calibration is a key focus of the paper, and understanding its principles is essential for implementing the calibration experiments.
  - Quick check question: What is model calibration and why is it important for assessing model trustworthiness?

- Concept: Explainability methods and their role in providing insights into model decision-making
  - Why needed here: The paper leverages various explainability methods to generate features for calibrator models, so understanding these methods is crucial.
  - Quick check question: What are explainability methods and how can they provide insights into model decision-making?

## Architecture Onboarding

- Component map: LLM counterfactual generators (GPT-JT, LLaMA, Alpaca, GPT-NeoxT, Flan-T5-xxl, Flan-UL2) -> Counterfactual filtering pipeline -> RoBERTa-base training on augmented data -> Explanation generation (SHAP, attention, gradients) -> Random Forest calibration

- Critical path: 1. Generate counterfactual instances using LLMs 2. Filter and select high-quality counterfactuals 3. Train base model on original and counterfactual data 4. Generate explanations using various methods 5. Train calibrator models with different feature sets 6. Evaluate OOD performance and calibration

- Design tradeoffs:
  - Model size vs. generation quality: Larger LLMs may generate more diverse counterfactuals but are computationally expensive
  - Filtering strictness vs. data quantity: Stricter filtering may result in higher-quality counterfactuals but fewer instances
  - Feature set complexity vs. calibrator performance: More complex feature sets may improve calibration but increase computational cost

- Failure signatures:
  - Poor OOD performance: Counterfactuals may not be diverse enough or may introduce too much noise
  - Calibration failure: Explanations may not be informative enough, or calibrator models may not effectively utilize the features
  - Computational inefficiency: Generating and filtering counterfactuals may be too slow or resource-intensive

- First 3 experiments:
  1. Compare OOD performance of base model vs. model trained with counterfactuals generated by different LLMs
  2. Evaluate calibration performance of base model vs. model trained with counterfactuals using various feature sets
  3. Analyze the relationship between counterfactual diversity and OOD performance/calibration across different LLMs

## Open Questions the Paper Calls Out

### Open Question 1
How does the diversity of counterfactual (CF) instances generated by different LLMs correlate with the out-of-domain (OOD) performance improvement of small language models (SLMs)? The paper shows that more diverse CFs improve OOD performance and model calibration by a large margin, with a high average correlation of 0.55 between diversity metrics and OOD performance gain. However, it does not provide a detailed analysis of the specific mechanisms by which different types of diversity contribute to performance gains, and the study is limited to extractive QA tasks.

### Open Question 2
What is the optimal balance between the comprehensiveness and sufficiency of explanations for effective model calibration? The paper finds that rationale-augmented calibrators prefer concise explanations with lower entropy importance scores, but does not provide a detailed analysis of how the trade-off between comprehensiveness and sufficiency affects calibration performance. It also does not explore whether this preference for concise explanations holds across different types of explanation methods or tasks.

### Open Question 3
How do domain-specific CF instances generated by LLMs fine-tuned on relevant datasets compare to general CF instances in improving OOD performance and calibration? The paper suggests that alignment between the domain expertise of LLMs used to generate CFs and the data distribution of OOD datasets could be important, but does not conduct experiments to directly compare the effectiveness of domain-specific vs general CF instances.

## Limitations

- The evaluation framework doesn't directly compare against strong baseline counterfactual generation approaches (e.g., RGF with learned perturbations)
- Diversity metrics used (self-BLEU, Levenshtein distance, SBERT similarity) are surface-level measures that may not fully capture semantic diversity's impact on OOD performance
- The filtering pipeline's effectiveness is assumed rather than empirically validated - we don't know how many counterfactuals are generated versus filtered

## Confidence

**High Confidence**: OOD performance improvements from CF augmentation are well-supported by multiple experiments across different LLM generators and datasets (2-4% EM/F1 gains).

**Medium Confidence**: The mechanism linking LLM pretraining diversity to counterfactual diversity is plausible but not directly proven - evidence is largely correlational rather than causal.

**Low Confidence**: The claim that counterfactual augmented models have "more precise explanations" is weakly supported - correlation between lower entropy scores and better calibration doesn't establish explanation accuracy.

## Next Checks

1. **Ablation on filtering pipeline**: Run experiments with varying filtering thresholds to determine how much performance gain comes from counterfactual generation versus filtering quality control.

2. **Alternative diversity metrics**: Evaluate semantic diversity using task-specific metrics (e.g., downstream task performance on CF instances) rather than just surface similarity measures.

3. **Cross-LLM consistency**: Test whether CFs generated by smaller/faster models (e.g., Flan-T5-xxl) match the diversity and performance of larger models, establishing practical deployment boundaries.