---
ver: rpa2
title: Developing a Named Entity Recognition Dataset for Tagalog
arxiv_id: '2311.07161'
source_url: https://arxiv.org/abs/2311.07161
tags:
- tagalog
- language
- entity
- nified
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces TLU NIFIED -NER , a high-quality Tagalog\
  \ NER dataset comprising ~7.8k news documents annotated with three entity types\
  \ (PER, ORG, LOC) by native speakers, achieving an inter-annotator agreement of\
  \ 0.81 Cohen's \u03BA. Extensive evaluation across supervised and transfer learning\
  \ settings shows strong performance, especially with context-sensitive models like\
  \ RoBERTa Tagalog, which achieves 90.34 F1 overall."
---

# Developing a Named Entity Recognition Dataset for Tagalog

## Quick Facts
- arXiv ID: 2311.07161
- Source URL: https://arxiv.org/abs/2311.07161
- Reference count: 11
- Key outcome: Native speaker annotation with iterative guidelines achieves IAA of 0.81 Cohen's κ; RoBERTa Tagalog achieves 90.34 F1 overall

## Executive Summary
This work introduces TLU NIFIED -NER, a high-quality Tagalog NER dataset comprising ~7.8k news documents annotated with three entity types (PER, ORG, LOC) by native speakers. The dataset achieves an inter-annotator agreement of 0.81 Cohen's κ through iterative annotation and guideline refinement. Extensive evaluation across supervised and transfer learning settings shows strong performance, especially with context-sensitive models like RoBERTa Tagalog, which achieves 90.34 F1 overall. The dataset is publicly released to advance Tagalog NLP and serves as a gold-standard alternative to noisy existing resources like WikiANN.

## Method Summary
The authors developed TLU NIFIED -NER by filtering news documents from the TLUnified corpus and having three native Tagalog speakers annotate them iteratively using Prodigy. After each annotation round, annotators held retrospective meetings to resolve disagreements and refine guidelines. The resulting dataset was split into train/dev/test sets and used to train spaCy NER models with various embeddings including baseline, fastText, RoBERTa Tagalog, XLM-R, and mBERT. Models were evaluated using F1-score and compared against the silver-standard WikiANN dataset.

## Key Results
- Native speaker annotation with iterative guidelines achieves IAA of 0.81 Cohen's κ
- RoBERTa Tagalog achieves 90.34 F1 overall on TLU NIFIED -NER
- Models trained on TLU NIFIED -NER significantly outperform those trained on noisy WikiANN data

## Why This Works (Mechanism)

### Mechanism 1
Native speaker annotation with iterative guidelines produces high inter-annotator agreement. Three native Tagalog speakers iteratively annotate the same text batches, followed by retrospective meetings to resolve disagreements and refine guidelines. Core assumption: Native linguistic intuition and collaborative refinement reduce systematic annotation errors. Evidence: IAA score of 0.81, iterative annotation process with retrospective meetings. Break condition: If annotators lack consistent linguistic training or if iterative feedback loops are skipped, IAA will drop significantly.

### Mechanism 2
Contextualized embeddings (RoBERTa Tagalog) outperform static embeddings for Tagalog NER. Contextual embeddings capture morphological and syntactic dependencies specific to Tagalog's VSO/VOS word order and agglutinative structure, enabling better entity boundary detection. Core assumption: Static embeddings cannot represent context-dependent entity boundaries in morphologically rich languages. Evidence: RoBERTa Tagalog achieves 90.34 F1 overall; relative error reduction between LOC and ORG entities. Break condition: If the pretrained model does not adequately cover the domain (news text), performance gains may be limited.

### Mechanism 3
Gold-standard annotations yield better downstream model performance than silver-standard noisy data. Clean, consistent annotations reduce model confusion between entity types (e.g., ORG vs PER for "Ombudsman"). Core assumption: Silver-standard data contains systematic labeling errors that mislead model training. Evidence: WikiANN shows ORG F1-score of 0.59 compared to TLU NIFIED -NER; models built from TLU NIFIED -NER are more performant. Break condition: If gold-standard annotation is inconsistent or incomplete, the advantage over silver-standard data diminishes.

## Foundational Learning

- Concept: Named Entity Recognition fundamentals
  - Why needed here: Understanding NER task setup (BILUO encoding, entity types) is essential for working with TLU NIFIED -NER
  - Quick check question: What are the three entity types defined in TLU NIFIED -NER?

- Concept: Inter-annotator agreement metrics
  - Why needed here: IAA (Cohen's κ) is used to validate annotation quality and guide iterative refinement
  - Quick check question: What does a Cohen's κ of 0.81 indicate about annotation consistency?

- Concept: Pretrained language model finetuning
  - Why needed here: Models like RoBERTa Tagalog require finetuning on NER data for optimal performance
  - Quick check question: What is the difference between static fastText vectors and contextualized RoBERTa vectors?

## Architecture Onboarding

- Component map: TLUnified corpus -> manual filtering -> Prodigy annotation -> train/dev/test split -> spaCy NER models with various embeddings -> evaluation
- Critical path: Load TLUnified corpus -> filter to news domain -> annotate with native speakers in iterative rounds -> compute IAA and refine guidelines -> split into train/dev/test -> train spaCy NER with chosen embeddings -> evaluate on test set
- Design tradeoffs: Using only three entity types simplifies annotation but limits downstream use cases; transition-based parser is lightweight but may underperform more complex architectures; gold-standard annotation is expensive but ensures higher model performance
- Failure signatures: Low IAA (<0.7) indicates guideline ambiguity or annotator inconsistency; confusion matrix shows high O-label errors, suggesting context sensitivity issues; WikiANN performance significantly worse than TLU NIFIED -NER suggests dataset quality problems
- First 3 experiments: Train baseline spaCy model without embeddings to establish lower bound; add fastText vectors and compare F1 improvement; fine-tune RoBERTa Tagalog and evaluate relative error reduction for ORG and LOC

## Open Questions the Paper Calls Out
- How would fine-grained and overlapping NER tag sets, such as those used in the ACE project, impact the performance of NER models on Tagalog compared to the current three-entity setup (PER, ORG, LOC)?
- How would expanding TLU NIFIED -NER to include other major Philippine languages impact the generalizability and performance of NER models across the Austronesian language family?
- How effective are large language models (LLMs) like GPT-4 and PaLM for zero-shot or few-shot NER tasks in Tagalog compared to supervised learning approaches?

## Limitations
- Dataset covers only three entity types (PER, ORG, LOC), limiting downstream applicability
- Evaluation focuses solely on the news domain, leaving generalization to other domains unexplored
- Per-entity IAA scores are not reported, making it unclear whether certain entity types were more challenging to annotate consistently

## Confidence

**High Confidence**: The core findings regarding native speaker annotation quality and RoBERTa Tagalog performance are well-supported by the evidence provided.

**Medium Confidence**: The claim that gold-standard annotations significantly outperform silver-standard data is supported but could benefit from additional comparisons with other established Tagalog NER datasets.

**Low Confidence**: The assertion that static embeddings cannot adequately capture context-dependent entity boundaries in morphologically rich languages is plausible but not empirically proven within this work.

## Next Checks

1. Compute and report Cohen's κ scores separately for PER, ORG, and LOC entities to identify whether certain types had lower agreement

2. Evaluate the trained models on Tagalog text from domains outside news (e.g., social media, literature) to assess whether the performance gains transfer beyond the annotated domain

3. Conduct controlled experiments comparing static fastText vectors with RoBERTa Tagalog embeddings on identical model architectures to isolate the contribution of contextualization versus pretraining scale