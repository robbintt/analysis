---
ver: rpa2
title: 'RAPID: Enabling Fast Online Policy Learning in Dynamic Public Cloud Environments'
arxiv_id: '2304.04797'
source_url: https://arxiv.org/abs/2304.04797
tags:
- resource
- workload
- rapid
- workloads
- allocation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAPID enables fast online learning of resource allocation policies
  for cloud workloads by decoupling control decisions from QoS measurements. RAPID
  uses a lightweight QoS predictor based on architectural counters to provide near-instantaneous
  feedback, enabling stable policies to be learned in minutes rather than hours.
---

# RAPID: Enabling Fast Online Policy Learning in Dynamic Public Cloud Environments

## Quick Facts
- **arXiv ID**: 2304.04797
- **Source URL**: https://arxiv.org/abs/2304.04797
- **Reference count**: 40
- **Primary result**: RAPID achieves stable resource allocation policies in minutes rather than hours, improving QoS by 9.0x and BE workload performance by 19-43% compared to prior state-of-the-art.

## Executive Summary
RAPID addresses the challenge of fast online learning for resource allocation in cloud environments by decoupling control decisions from direct QoS measurements. The system uses a lightweight QoS predictor based on architectural counters to provide near-instantaneous feedback, enabling stable policies to be learned in minutes rather than hours. This approach significantly improves both QoS for latency-critical workloads and performance for best-effort workloads compared to prior reinforcement learning approaches.

## Method Summary
RAPID implements a two-stage approach: first, it collects initial QoS prediction data through domain-knowledge-guided sampling of architectural counters; second, it trains a support vector regression model to predict QoS from these counters. The resource controller uses a branching dueling Q-network architecture that operates on the predicted QoS values rather than waiting for actual measurements. A bias correction mechanism continuously adjusts predictions by comparing distributions of past measurements and predictions. The system explores resource allocation policies using a probability-based approach that avoids dangerous random allocations while still discovering optimal configurations.

## Key Results
- RAPID achieves stable policies in minutes rather than hours required by prior RL approaches
- 9.0x improvement in QoS for latency-critical workloads
- 19-43% increase in best-effort workload performance compared to prior state-of-the-art
- Effective bias correction maintains prediction accuracy over extended operation periods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling resource allocation decisions from direct QoS measurements via QoS predictions enables much faster learning rates.
- Mechanism: Instead of waiting for actual QoS measurements that can take 15+ seconds to stabilize, RAPID uses a lightweight QoS predictor trained on architectural counters to provide near-instantaneous feedback (200ms interval). This allows the controller to explore and adapt policies orders of magnitude faster than prior RL approaches that rely on direct QoS measurements.
- Core assumption: Architectural counters can serve as reliable proxies for QoS that can be predicted accurately with limited training data.
- Evidence anchors: Abstract states RAPID "leverages lightweight QoS predictions" to decouple control from conventional feedback sources.

### Mechanism 2
- Claim: Combining multiple architectural counters provides more informative feedback than single counters or direct QoS measurements for guiding policy learning.
- Mechanism: RAPID uses a small set of carefully selected architectural counters that capture different aspects of resource contention. By combining these counters through a non-linear model (support vector regression), RAPID can more accurately predict QoS than using any single counter or direct QoS measurements, especially when trained on limited samples.
- Core assumption: The relationship between QoS and architectural counters, while roughly linear in some cases, can be better modeled by non-linear approaches that capture complex interactions.
- Evidence anchors: Paper shows combining multiple counters yields more accurate QoS estimates than single counters or direct measurements.

### Mechanism 3
- Claim: Domain-knowledge-inspired sampling and bias correction techniques enable effective learning with minimal training data.
- Mechanism: RAPID uses domain knowledge to guide initial sampling toward resource allocations most likely to affect contention and performance, rather than uniform random sampling. It also implements a two-step bias correction approach that estimates and compensates for prediction bias by comparing distributions of past QoS measurements and predictions.
- Core assumption: Domain knowledge about which resource allocations are likely to cause contention can be used to efficiently explore the space without exhaustive sampling.
- Evidence anchors: Paper describes avoiding uniform random sampling and instead leveraging domain knowledge to focus on resource allocations most likely to affect performance.

## Foundational Learning

- Concept: Reinforcement Learning with Deep Q-Networks (DQN)
  - Why needed here: RAPID uses a variant of DQN (Branching Dueling Q-Network) to learn resource allocation policies that balance QoS requirements with performance optimization for best-effort workloads.
  - Quick check question: What is the key advantage of using a dueling architecture in the context of resource allocation?

- Concept: Feature Engineering and Normalization
  - Why needed here: RAPID transforms raw architectural counter values through log-transformation and normalization to create stable, comparable features for the QoS predictor and controller.
  - Quick check question: Why might log-transformation be particularly useful when working with architectural counters that span several orders of magnitude?

- Concept: Bias-Variance Tradeoff in Machine Learning
  - Why needed here: RAPID must balance model complexity against overfitting when learning from limited samples, which is why it uses a relatively simple SVR model with RBF kernel rather than more complex neural networks.
  - Quick check question: How does the choice of regularization parameter C in SVR affect the bias-variance tradeoff?

## Architecture Onboarding

- Component map: QoS Predictor (SVR with RBF kernel) -> Resource Controller (BDQ) -> Bias Correction Module -> Sampling Engine
- Critical path: Initial sampling → QoS predictor training → Controller operation with QoS predictions → Continuous learning with bias correction
- Design tradeoffs:
  - Model complexity vs. sample efficiency: Chose SVR over deep networks for better performance with limited data
  - Granularity of resource allocation vs. action space dimensionality: Used coarse steps (10% memory bandwidth) to keep action space manageable
  - Exploration strategy: Implemented probability-based exploration instead of epsilon-greedy to avoid dangerous random allocations
- Failure signatures:
  - Controller makes erratic decisions → QoS predictor may be poorly trained or biased
  - System performance degrades over time → Bias correction may not be working or workload behavior changed
  - Controller converges to suboptimal policy → Exploration strategy may be too conservative
- First 3 experiments:
  1. Verify QoS predictor accuracy on held-out data from initial sampling period
  2. Test controller behavior with perfect QoS predictions (oracle) to establish upper bound
  3. Run with direct QoS measurements only to quantify improvement from decoupling strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RAPID's QoS prediction accuracy compare when using different numbers of architectural counters as input features?
- Basis in paper: Explicit mention of ablation tests showing removing all counters except IPC incurs 5.9x increase in weighted QoS
- Why unresolved: Paper doesn't provide systematic comparison of prediction accuracy across different numbers of counters
- What evidence would resolve it: Controlled experiments showing QoS prediction error rates as a function of number of architectural counters used

### Open Question 2
- Question: What is the theoretical upper bound on training time reduction that could be achieved with RAPID's approach compared to direct QoS measurements?
- Basis in paper: Explicit claim that RAPID enables "stable policies in minutes, rather than hours"
- Why unresolved: Paper demonstrates practical improvements but doesn't analyze fundamental limits
- What evidence would resolve it: Mathematical analysis comparing convergence rates with perfect QoS predictions versus noisy measurements

### Open Question 3
- Question: How does RAPID's performance scale with increasing numbers of co-scheduled workloads beyond the tested scenarios?
- Basis in paper: Inferred from focus on single HP and BE workload scenarios
- Why unresolved: Paper doesn't evaluate performance with multiple HP workloads or many BE workloads
- What evidence would resolve it: Experiments testing RAPID with 2+ HP workloads and various numbers of BE workloads

### Open Question 4
- Question: How sensitive is RAPID's performance to the choice of exploration probability and bias correction gain parameters?
- Basis in paper: Explicit mention of empirical determination that 0.7≤ α≤ 0.9 yielded desirable behavior
- Why unresolved: Paper uses specific parameter values without systematic analysis of sensitivity
- What evidence would resolve it: Grid search or sensitivity analysis across ranges of exploration probability and bias correction gain values

## Limitations
- Evaluation limited to single dual-socket Intel Xeon platform, limiting generalizability
- Workload selection focuses on specific benchmarks rather than broad real-world cloud workloads
- Claims about effectiveness in "dynamic public cloud environments" not fully validated with actual cloud conditions

## Confidence

- **High confidence**: Core decoupling mechanism (QoS predictions enabling faster learning) is well-supported by experimental evidence
- **Medium confidence**: 9.0x QoS improvement and 19-43% BE performance gains demonstrated but limited hardware diversity
- **Low confidence**: Claims about effectiveness in "dynamic public cloud environments" not fully validated with actual cloud deployments

## Next Checks

1. **Cross-platform validation**: Implement RAPID on different hardware architecture (AMD EPYC or ARM-based system) to verify QoS predictor transferability
2. **Long-term stability test**: Run RAPID continuously for 24+ hours with realistic workload patterns to evaluate bias correction and policy stability
3. **Comparison with neural network approaches**: Implement lightweight neural network QoS predictor to compare sample efficiency and accuracy against RAPID's SVR approach