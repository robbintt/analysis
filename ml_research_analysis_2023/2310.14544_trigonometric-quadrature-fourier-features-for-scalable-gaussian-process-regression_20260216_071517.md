---
ver: rpa2
title: Trigonometric Quadrature Fourier Features for Scalable Gaussian Process Regression
arxiv_id: '2310.14544'
source_url: https://arxiv.org/abs/2310.14544
tags:
- quadrature
- gaussian
- fourier
- error
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Trigonometric Quadrature Fourier Features (TQFF)
  to address the limitation of variance starvation in existing Random Fourier Feature
  (RFF) and Gaussian Quadrature Fourier Feature (QFF) methods for scalable Gaussian
  Process (GP) regression. The core idea is to use a novel trigonometric quadrature
  rule tailored for the Fourier transform, which overcomes the oscillatory behavior
  issues of polynomial interpolation used in Gaussian QFF.
---

# Trigonometric Quadrature Fourier Features for Scalable Gaussian Process Regression

## Quick Facts
- arXiv ID: 2310.14544
- Source URL: https://arxiv.org/abs/2310.14544
- Reference count: 27
- The paper proposes Trigonometric Quadrature Fourier Features (TQFF) to address variance starvation in existing Random Fourier Feature (RFF) and Gaussian Quadrature Fourier Feature (QFF) methods for scalable Gaussian Process regression.

## Executive Summary
This paper addresses a critical limitation in scalable Gaussian Process regression: variance starvation in existing Fourier feature approximations. The authors introduce Trigonometric Quadrature Fourier Features (TQFF), a novel method that uses trigonometric quadrature rules specifically designed for the oscillatory nature of Fourier transforms. TQFF overcomes the oscillatory behavior issues of polynomial interpolation used in Gaussian QFF, achieving more accurate kernel approximations with fewer features. Empirical results demonstrate superior performance compared to RFF and Gaussian QFF across multiple benchmark datasets and synthetic functions.

## Method Summary
TQFF is a scalable approximation method for Gaussian Process regression that replaces the traditional Monte Carlo sampling of RFF and polynomial interpolation of Gaussian QFF with a deterministic trigonometric quadrature rule. The method computes quadrature nodes and weights using the Golub-Welsh algorithm, constructs a feature map using cosine polynomials that exactly match the structure of the Fourier integrand, and uses this to approximate the kernel matrix. The approach is particularly effective for low-dimensional problems with known spectral densities that factorize across dimensions.

## Key Results
- TQFF achieves lower test negative log-likelihood and root mean squared error than RFF and Gaussian QFF across multiple benchmark datasets
- TQFF preserves posterior uncertainty better, with predictive intervals visually indistinguishable from full GP posterior
- TQFF requires fewer features than competing methods to achieve comparable or better performance
- The method shows particular effectiveness for short length-scale kernels where existing methods struggle

## Why This Works (Mechanism)

### Mechanism 1
TQFF reduces variance starvation by using a trigonometric interpolant tailored for the oscillatory Fourier integrand. Unlike Gaussian QFF which uses polynomial interpolation that struggles with high oscillation, TQFF uses cosine polynomials that exactly match the structure of the integrand cos(ωγ[(x-x')/θ]). This leads to more accurate kernel approximation with fewer features.

### Mechanism 2
TQFF achieves higher approximation accuracy than RFF and Gaussian QFF by avoiding Monte Carlo variance and polynomial interpolation error. TQFF uses deterministic quadrature with exact trigonometric degree of exactness (2L-1), which ensures the approximation matches the truncated integral exactly for integer multiples of the scaled distance.

### Mechanism 3
TQFF preserves posterior uncertainty better than RFF and Gaussian QFF, leading to well-calibrated predictions. By accurately estimating covariances for both small and large distances, TQFF avoids both variance starvation (underestimation in data-sparse regions) and overestimation in data-rich regions.

## Foundational Learning

- Gaussian Process regression and kernel approximations: Understanding GP priors, kernels, and the computational bottleneck of full GP inference is essential to grasp why feature approximations are useful. Quick check: What is the computational complexity of full GP training/prediction, and why is it prohibitive for large n?

- Fourier feature approximations (RFF, QFF): Knowing how RFF uses Monte Carlo sampling and how Gaussian QFF uses deterministic quadrature is necessary to understand TQFF's novelty. Quick check: What is "variance starvation" in RFF, and how do Gaussian QFF methods attempt to address it?

- Quadrature rules and interpolation: Understanding how quadrature approximates integrals and the role of interpolation is key to grasping TQFF's mechanism. Quick check: What is the difference between polynomial interpolation (used in Gaussian QFF) and trigonometric interpolation (used in TQFF)?

## Architecture Onboarding

- Component map: Input data -> TQFF quadrature computation -> Feature map Φ(x) -> Low-rank kernel approximation -> GP inference
- Critical path: 1) Compute TQFF quadrature nodes and weights (once, offline) 2) Construct feature map Φ(x) for training and test inputs 3) Approximate kernel matrix as Φ(x)^T Φ(x') 4) Perform GP inference using the low-rank approximation
- Design tradeoffs: Feature count S vs. approximation accuracy; Truncation parameter γ vs. error; Dimensionality d vs. scalability (tensor product quadrature suffers from curse of dimensionality)
- Failure signatures: Poor predictive performance (insufficient features or inappropriate γ); High computational cost (high dimensionality or inefficient implementation); Numerical instability (very large L)
- First 3 experiments: 1) Compare TQFF kernel approximation error vs. RFF and Gaussian QFF on a 1D SE kernel for varying length-scales and feature counts 2) Evaluate TQFF GP regression performance on a synthetic 2D function with known GP ground truth 3) Assess TQFF uncertainty quantification on a real-world time series dataset with hold-out regions

## Open Questions the Paper Calls Out

The paper identifies several open questions: How does TQFF scale with increasing dimensionality compared to existing QFF methods? Can Bayesian quadrature with a kernel designed for trigonometric integrands improve TQFF's accuracy in higher dimensions? How does the choice of truncation parameter γ affect TQFF's approximation error and computational efficiency? These questions remain unresolved and represent important directions for future research.

## Limitations

- TQFF requires a known spectral density and assumes factorization across dimensions, excluding many common kernels like the Matérn family
- The method suffers from the curse of dimensionality from tensor product quadrature, limiting scalability to higher dimensions
- Numerical stability of the Golub-Welsh algorithm for computing trigonometric quadrature rules at high degrees L is not thoroughly investigated

## Confidence

- Mechanism 1 (trigonometric vs polynomial interpolation): Medium - The theoretical advantage is clear, but empirical validation against polynomial interpolation error is missing
- Mechanism 2 (deterministic quadrature accuracy): Medium - Strong theoretical bounds exist, but direct kernel approximation error comparisons are limited
- Mechanism 3 (uncertainty preservation): Medium - Visual and KL-divergence results are promising, but systematic calibration metrics are absent

## Next Checks

1. Test TQFF performance on non-SE kernels (e.g., Matérn) to assess the impact of spectral density factorization assumptions
2. Evaluate numerical stability of the Golub-Welsh algorithm for TQFF at high L values (L > 50) on commodity hardware
3. Benchmark TQFF against RFF and Gaussian QFF on higher-dimensional problems (d > 5) to quantify the curse of dimensionality effects