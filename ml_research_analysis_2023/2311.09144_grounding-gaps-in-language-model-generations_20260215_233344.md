---
ver: rpa2
title: Grounding Gaps in Language Model Generations
arxiv_id: '2311.09144'
source_url: https://arxiv.org/abs/2311.09144
tags:
- grounding
- acts
- arxiv
- clarification
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a "grounding gap" in LLM dialogue, where
  models are less likely to use grounding acts like clarification, acknowledgment,
  and follow-ups compared to humans. The authors curate a set of grounding acts, build
  classifiers to detect them, and simulate LLM turn-taking on three dialogue datasets.
---

# Grounding Gaps in Language Model Generations

## Quick Facts
- arXiv ID: 2311.09144
- Source URL: https://arxiv.org/abs/2311.09144
- Reference count: 27
- Key outcome: LLMs use 77.5% fewer grounding acts than humans in dialogue, with low alignment across all evaluated models

## Executive Summary
This paper identifies a "grounding gap" where language models use significantly fewer grounding acts (clarification, acknowledgment, follow-ups) compared to humans in dialogue. Through simulation of LLM turn-taking on three dialogue datasets and analysis of RLHF preference data, the authors demonstrate that current training methods, particularly RLHF, systematically erode these essential conversational coordination signals. The findings suggest that instruction-following paradigms may inadvertently train models to assume common ground rather than actively establish it.

## Method Summary
The paper curates a taxonomy of grounding acts and builds GPT-4 classifiers to detect them in dialogue. Researchers simulate LLM turn-taking by having models respond to human-human conversation datasets (TSCC, ESConv, Persuasion for Good), controlling for conversation length to prevent in-context learning advantages. They measure grounding act usage through base rates and alignment using Cohen's kappa. The study also analyzes the effects of SFT and RLHF training by examining their impact on grounding act usage, including analysis of RLHF preference data to understand training biases.

## Key Results
- LLMs use 77.5% fewer grounding acts than humans across all evaluated datasets
- Cohen's kappa alignment between human and LM grounding act usage is low across all model families
- RLHF training explicitly penalizes questions (13.77% preferred vs 18.35% dispreferred), eroding grounding
- SFT training shows no positive effect on conversational grounding
- Prompt-based interventions can increase grounding act usage but don't improve alignment with human behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLHF training reduces grounding act usage by optimizing for immediate relevance rather than conversational coordination
- Mechanism: During RLHF, human raters prefer responses that directly address the prompt without questions, leading models to avoid clarification and follow-up acts
- Core assumption: Human preference data captures what makes responses "good" in isolation rather than what maintains conversation flow
- Evidence anchors: RLHF preference data shows questions are significantly less frequent in preferred (13.77%) compared to dispreferred (18.35%) examples

### Mechanism 2
- Claim: LLMs assume common ground instead of verifying it because they are trained to be expert responders
- Mechanism: Instruction-following training teaches models to act as authoritative experts who should already know the context, reducing need for clarification
- Core assumption: The "expert listener" framing assumes the model should have privileged knowledge about the task
- Evidence anchors: The paper hypothesizes that instruction-following paradigms lead models to avoid conversational grounding

### Mechanism 3
- Claim: Base rate differences between human and LM grounding acts don't necessarily indicate misalignment if LMs use different grounding strategies
- Mechanism: LMs might establish common ground through implicit methods rather than explicit dialogue acts like "I understand"
- Core assumption: Grounding can be achieved without using the specific acts measured in the paper
- Evidence anchors: Simply using grounding acts to augment training does not guarantee alignment with human behavior

## Foundational Learning

- Concept: Cohen's Kappa
  - Why needed here: Measures agreement between human and LM grounding act usage while accounting for chance agreement
  - Quick check question: If two raters always agree on all labels, what is their Cohen's Kappa value?

- Concept: Grounding acts taxonomy
  - Why needed here: Provides the framework for measuring conversational grounding through specific dialogue acts
  - Quick check question: What distinguishes a clarification question from a follow-up question in the paper's taxonomy?

- Concept: In-context learning (ICL)
  - Why needed here: Explains why conversation length must be controlled in the simulation to prevent unfair advantages
  - Quick check question: How does controlling conversation length prevent ICL from biasing grounding act usage?

## Architecture Onboarding

- Component map: Grounding act classifier -> Conversation simulator -> Alignment metrics -> Training intervention evaluation
- Critical path: Simulated conversation generation -> Grounding act classification -> Base rate and Kappa calculation -> Model comparison
- Design tradeoffs: Using GPT-4 for classification provides high accuracy but is expensive; using GPT-3.5 for simulation is cheaper but less capable
- Failure signatures: Low Kappa with high base rates indicates overeager grounding; consistently low base rates across all models suggests training bias
- First 3 experiments:
  1. Compare grounding act usage across different GPT model versions (3.5 vs 4) on ESConv dataset
  2. Test SFT-only training on Mistral 7B with UltraChat data and measure grounding alignment
  3. Apply the prompt mitigation to ChatGPT-3.5 and compare base rates and Kappa to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does instruction tuning (SFT) improve grounding alignment with humans in low-resource domains like mental health support?
- Basis in paper: The paper found that SFT does not improve conversational grounding, and RLHF erodes it
- Why unresolved: The study used emotional support conversations but didn't specifically test low-resource domains
- What evidence would resolve it: Training SFT models on low-resource emotional support datasets and measuring grounding alignment compared to humans

### Open Question 2
- Question: Can multi-turn interaction reward models improve grounding alignment beyond single-step RLHF?
- Basis in paper: The paper suggests that current RLHF is trained on single-step interactions, but humans use grounding acts strategically across multiple turns
- Why unresolved: The study didn't explore multi-turn RLHF training
- What evidence would resolve it: Training reward models on multi-turn emotional support conversations and comparing grounding alignment to single-step RLHF models

### Open Question 3
- Question: Does grounding act frequency correlate with task success in goal-oriented dialogues?
- Basis in paper: The paper mentions that grounding acts are crucial in tasks requiring coordination to achieve a goal
- Why unresolved: The study didn't measure task success rates in relation to grounding act usage
- What evidence would resolve it: Analyzing task completion rates in goal-oriented dialogues while varying grounding act frequency and types

## Limitations
- The grounding act taxonomy may not capture all forms of grounding, particularly implicit methods that don't map to explicit dialogue acts
- Causal link between RLHF and grounding erosion is inferred rather than experimentally isolated through controlled ablation studies
- The study doesn't measure whether grounding act frequency correlates with task success or conversation quality

## Confidence

- Grounding gap exists: High
- RLHF specifically causes grounding erosion: Medium
- SFT doesn't improve grounding: Medium

## Next Checks

1. Conduct an ablation study where a base model is RLHF-trained and grounding act usage is measured before/after to isolate RLHF's causal effect

2. Implement metrics that capture implicit grounding (response relevance, coherence) alongside explicit dialogue acts to determine if models use different grounding strategies

3. Create synthetic preference data where raters evaluate turn-taking quality in multi-turn conversations, then train models on this data and measure whether grounding acts are preserved