---
ver: rpa2
title: 'WizardLM: Empowering large pre-trained language models to follow complex instructions'
arxiv_id: '2304.12244'
source_url: https://arxiv.org/abs/2304.12244
tags:
- instruction
- prompt
- data
- instructions
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WizardLM, a method for training large language
  models (LLMs) to follow complex instructions by automatically generating instruction
  data using LLM-based evolution. The core idea is Evol-Instruct, which iteratively
  rewrites initial instructions into more complex ones through six evolutionary operations
  (in-depth evolving, in-breadth evolving, and elimination evolving).
---

# WizardLM: Empowering large pre-trained language models to follow complex instructions

## Quick Facts
- **arXiv ID**: 2304.12244
- **Source URL**: https://arxiv.org/abs/2304.12244
- **Reference count**: 22
- **Key outcome**: WizardLM achieves 41.3% win rate vs Vicuna on balanced test set and 42.9% win rate vs ChatGPT on high-complexity instructions through AI-evolved instruction fine-tuning

## Executive Summary
WizardLM introduces a novel method called Evol-Instruct that uses LLM-based evolution to automatically generate high-quality instruction data for fine-tuning large language models. The approach iteratively transforms simple instructions into more complex ones through six evolutionary operations (in-depth evolving, in-breadth evolving, and elimination evolving), creating a dataset that is more diverse and complex than human-created instruction sets. By fine-tuning LLaMA with 250K AI-evolved instructions, WizardLM significantly outperforms Vicuna on instruction-following tasks and even surpasses ChatGPT on high-complexity instructions, demonstrating that AI-evolved instruction data is a promising direction for enhancing LLMs.

## Method Summary
WizardLM fine-tunes LLaMA 7B on instruction data generated through Evol-Instruct, an LLM-based evolution method. The process starts with the Alpaca 52K instruction dataset and applies four rounds of evolution using ChatGPT API. Each evolution round includes in-depth evolving (making instructions more complex), in-breadth evolving (generating new instruction topics), and elimination evolving (filtering failed instructions). The evolved instructions are merged with the initial set to create 250K training examples, which are then used to fine-tune LLaMA for 3 epochs using DeepSpeed ZeRO-3 on 8 V100 GPUs.

## Key Results
- WizardLM achieves 41.3% win rate vs Vicuna on balanced test set (28.9% for Vicuna)
- On high-complexity instructions, WizardLM reaches 42.9% win rate vs ChatGPT's 35.0%
- Instructions from Evol-Instruct show superior diversity compared to human-created datasets, with more even dispersion in t-SNE analysis
- The method successfully generates more diverse and complex instructions than human-created ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evol-Instruct generates instruction data that is more complex and diverse than human-created data, leading to better instruction-following performance.
- Mechanism: The method uses LLM-based evolution with six operations (in-depth evolving, in-breadth evolving, and elimination evolving) to iteratively transform simple instructions into more complex ones. This creates a larger variety of instruction types and difficulty levels than human annotators typically produce.
- Core assumption: LLMs can successfully rewrite instructions to be more complex while maintaining reasonableness and understandability.
- Evidence anchors:
  - [abstract]: "instructions from Evol-Instruct are superior to human-created ones"
  - [section]: "human evaluation results show that instructions from Evol-Instruct are superior to human-created ones"
  - [corpus]: Weak - no direct corpus evidence comparing evolved vs human instruction distributions
- Break condition: If LLMs fail to generate reasonable complex instructions or produce excessive invalid outputs, the quality advantage disappears.

### Mechanism 2
- Claim: The balance of instruction difficulty levels in training data improves model generalization to complex instructions.
- Mechanism: By mixing instructions from multiple evolution epochs, the training data contains a smooth distribution of difficulty levels rather than being skewed toward easy instructions like human-created datasets.
- Core assumption: Exposure to a balanced difficulty distribution during training enables better handling of complex instructions at test time.
- Evidence anchors:
  - [abstract]: "outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT" on high-complexity instructions
  - [section]: "By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM model are preferred to outputs from OpenAI ChatGPT"
  - [corpus]: Weak - no corpus evidence showing the actual difficulty distribution in the training data
- Break condition: If the evolution process creates too many invalid instructions or the difficulty progression is too steep, the balance advantage is lost.

### Mechanism 3
- Claim: In-breadth evolving increases topic coverage and diversity, addressing the limitation of small instruction datasets.
- Mechanism: The method generates entirely new instructions based on existing ones but with different topics/skills, expanding the instruction space beyond what human annotators would naturally create.
- Core assumption: LLMs can generate meaningful new instruction topics that are relevant to real-world use cases.
- Evidence anchors:
  - [section]: "The primary purpose of In-Breadth Evolving is to increase the topic coverage, skill coverage, and overall diversity"
  - [section]: "instructions generated by our method are more evenly dispersed, while those of ShareGPT and Alpaca are more clustered"
  - [corpus]: Moderate - t-SNE clustering analysis shows better dispersion for evolved instructions
- Break condition: If in-breadth evolving produces irrelevant or nonsensical instruction topics, the diversity benefit disappears.

## Foundational Learning

- Concept: Instruction-following fine-tuning
  - Why needed here: The method builds on instruction-tuning approaches but improves the quality of training data through evolution
  - Quick check question: What distinguishes open-domain instruction-following from closed-domain instruction-following?

- Concept: Reinforcement learning vs supervised fine-tuning
  - Why needed here: The paper uses supervised fine-tuning on evolved instructions rather than RLHF, which is an important design choice
  - Quick check question: How does supervised fine-tuning on evolved instructions differ from RLHF approaches like InstructGPT?

- Concept: Data quality vs data quantity trade-offs
  - Why needed here: The method generates 250K instructions from a small initial set, prioritizing quality through evolution over raw quantity
  - Quick check question: Why might 70K carefully evolved instructions outperform 70K randomly collected user instructions?

## Architecture Onboarding

- Component map:
  Instruction Evolver -> Instruction Filter -> Data Pipeline -> LLaMA Fine-tuning -> WizardLM Evaluation

- Critical path: Initial instruction set → Evol-Instruct evolution → Instruction filtering → Data mixing → LLaMA fine-tuning → WizardLM evaluation

- Design tradeoffs:
  - Evolution iterations vs. instruction validity: More iterations create more complex instructions but risk more invalid outputs
  - Evolution difficulty progression vs. training stability: Gradual increases work better than rapid jumps
  - LLM choice vs. cost: Different LLMs for evolution have different quality/cost trade-offs

- Failure signatures:
  - High proportion of eliminated instructions indicates poor evolution prompts
  - Model performs worse on complex instructions than simple ones suggests imbalance in training data
  - Evolution produces repetitive instructions indicates in-breadth evolving is not working

- First 3 experiments:
  1. Test evolution prompts on a small instruction set and measure valid instruction rate
  2. Train on 1K instructions from 1 evolution round vs. 1K from 4 evolution rounds and compare performance
  3. Compare human evaluation of 100 evolved vs. 100 human-created instructions to validate quality claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Evol-Instruct method ensure that the evolved instructions remain relevant and coherent while increasing in complexity?
- Basis in paper: [explicit] The paper mentions that the evolved instructions must be "reasonable and must be understood and responded by humans," and that the rate of difficulty increase needs to be gradual to avoid damaging the generalization performance of models trained on this instruction set.
- Why unresolved: While the paper states these requirements, it does not provide specific mechanisms or examples of how the method ensures relevance and coherence during the evolution process, especially when dealing with more complex instructions.
- What evidence would resolve it: A detailed explanation or empirical demonstration of how the Evol-Instruct method maintains instruction relevance and coherence during the evolution process, particularly for high-complexity instructions.

### Open Question 2
- Question: What is the impact of the elimination evolving step on the diversity and complexity of the final instruction dataset?
- Basis in paper: [explicit] The paper mentions that elimination evolving is used to filter out failed instructions, which are classified into four situations, including instructions that do not provide any information increment, instructions that make it difficult for the LLM to generate a response, instructions containing only punctuation and stop words, and instructions that obviously copy words from the evolving prompt.
- Why unresolved: While the paper describes the elimination evolving step, it does not provide quantitative data on how this step affects the overall diversity and complexity of the final instruction dataset, nor does it discuss potential biases introduced by this filtering process.
- What evidence would resolve it: A detailed analysis of the impact of elimination evolving on the diversity and complexity of the final instruction dataset, including statistical measures of instruction diversity before and after filtering, and a discussion of potential biases introduced by the filtering process.

### Open Question 3
- Question: How does the Evol-Instruct method compare to other instruction generation techniques, such as human annotation or self-instruct, in terms of cost-effectiveness and quality of the generated instructions?
- Basis in paper: [inferred] The paper mentions that manually creating instruction data is time-consuming and labor-intensive, and that humans may struggle to produce high-complexity instructions. It also states that Evol-Instruct can mass-produce open-domain instructions at a relatively low cost and generate more diverse and complex instructions than human-created ones.
- Why unresolved: While the paper suggests that Evol-Instruct is more cost-effective and produces higher-quality instructions than human annotation, it does not provide a direct comparison with other instruction generation techniques, such as self-instruct, in terms of cost-effectiveness and quality of the generated instructions.
- What evidence would resolve it: A comparative study of Evol-Instruct against other instruction generation techniques, such as human annotation and self-instruct, in terms of cost-effectiveness and quality of the generated instructions, including quantitative measures of instruction diversity, complexity, and relevance.

## Limitations
- The quality of AI-evolved instructions remains uncertain due to limited evaluation methodology and lack of standardized comparison protocols
- The computational cost of generating 250K instructions through 624K API calls represents a significant practical limitation
- Performance comparisons against ChatGPT are based on limited test cases with unknown evaluation methodology

## Confidence
- **High confidence**: The basic premise that LLMs can be fine-tuned on instruction-following data to improve performance. The superiority of WizardLM over Vicuna on the balanced test set is supported by human evaluation, though methodology details are limited.
- **Medium confidence**: The specific mechanism of Evol-Instruct producing more complex and diverse instructions than human creation. While the paper provides some evidence through t-SNE analysis and human evaluation, the evaluation lacks standardization and the t-SNE results are somewhat qualitative.
- **Low confidence**: The claim that WizardLM surpasses ChatGPT on high-complexity instructions. This comparison is based on only 100 test cases with an unknown evaluation methodology, and ChatGPT performance can vary significantly based on version and prompt engineering.

## Next Checks
1. Replicate the human evaluation of evolved vs. human-created instructions using a standardized protocol with multiple annotators and calculate inter-rater reliability (Cohen's kappa) to validate the quality claims.
2. Implement the Evol-Instruct pipeline and generate a small sample (1K instructions) to measure the valid instruction rate and analyze the actual complexity progression across evolution rounds.
3. Conduct an ablation study comparing WizardLM trained on: (a) 70K evolved instructions, (b) 70K human-created instructions, and (c) 70K mixed instructions, using the same test methodology to isolate the effect of data quality vs. quantity.