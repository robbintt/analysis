---
ver: rpa2
title: 'ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy'
arxiv_id: '2311.09215'
source_url: https://arxiv.org/abs/2311.09215
tags:
- clip
- accuracy
- supervised
- imagenet
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares ConvNets and Vision Transformers under supervised
  and CLIP training paradigms, focusing on properties beyond ImageNet accuracy. The
  authors analyze four models with similar ImageNet accuracies and compute requirements,
  evaluating their behaviors across multiple dimensions including types of mistakes,
  calibration, transferability, robustness, and invariance to transformations.
---

# ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy

## Quick Facts
- arXiv ID: 2311.09215
- Source URL: https://arxiv.org/abs/2311.09215
- Authors: 
- Reference count: 40
- Primary result: Comprehensive multi-dimensional evaluation reveals ConvNets and Vision Transformers exhibit substantially different behaviors across calibration, robustness, transferability, and transformation invariance despite similar ImageNet accuracies.

## Executive Summary
This study systematically compares ConvNets and Vision Transformers under supervised and CLIP training paradigms, moving beyond standard ImageNet accuracy to evaluate multiple behavioral dimensions. The authors analyze four models with similar ImageNet performance and compute requirements, finding that standard accuracy metrics fail to capture important differences in model behavior. Key findings include CLIP models being overconfident but making fewer mistakes on out-of-distribution data, supervised models being better calibrated, and ConvNeXt showing architectural advantages on synthetic data and transformation invariance tasks. The work emphasizes that model selection should depend on target use cases rather than just ImageNet performance.

## Method Summary
The study evaluates four pretrained models (ViT-B/16 and ConvNeXt-B under both supervised and CLIP training paradigms) across multiple dimensions including mistakes, calibration, robustness, transferability, synthetic data performance, and transformation invariance. Models are compared using their pretrained weights without further training or finetuning. Evaluation encompasses standard benchmarks (ImageNet-X variants, VTAB) and custom experiments (PUG-ImageNet synthetic dataset, transformation invariance tests). The analysis focuses on properties beyond ImageNet accuracy, computing Expected Calibration Error (ECE), error ratios, shape/texture bias, and robustness scores across various distribution shifts.

## Key Results
- CLIP models make fewer mistakes relative to their ImageNet accuracy but are overconfident in predictions, while supervised models are better calibrated
- ConvNeXt under supervised training outperforms ViT on synthetic data, transformation invariance, and shows competitive transferability to CLIP models
- Models with similar ImageNet accuracy exhibit substantially different behaviors across calibration, robustness, and transferability dimensions
- Shape bias varies significantly between training paradigms, with CLIP models showing higher shape bias than supervised counterparts

## Why This Works (Mechanism)

### Mechanism 1
Model selection should depend on target use cases because standard ImageNet accuracy does not capture important behavioral differences. Models with similar ImageNet accuracy can exhibit substantially different behaviors across calibration, robustness, transferability, and transformation invariance. These differences arise from architectural choices and training paradigms rather than just overall accuracy.

### Mechanism 2
CLIP models make fewer mistakes relative to their ImageNet performance but are overconfident in predictions. CLIP's image-text training creates representations that generalize better to distribution shifts and diverse visual concepts, leading to fewer absolute errors on out-of-distribution data. However, this same training objective creates overconfident predictions on both in-distribution and out-of-distribution data.

### Mechanism 3
ConvNeXt under supervised training outperforms ViT on many benchmarks including synthetic data, transformation invariance, and transferability. The convolutional architecture of ConvNeXt, combined with modern design elements, provides better local feature learning for synthetic data and more stable behavior under transformations, while maintaining competitive performance on standard benchmarks.

## Foundational Learning

- Concept: Model calibration and Expected Calibration Error (ECE)
  - Why needed here: Understanding calibration is crucial for interpreting model confidence and reliability, especially when comparing CLIP and supervised models which show different calibration behaviors.
  - Quick check question: If a model has an ECE of 0.15, what does this tell you about the relationship between its predicted confidence and actual accuracy?

- Concept: Transformation invariance and its measurement
  - Why needed here: The paper evaluates scale, shift, and resolution invariance, which are important for real-world deployment where input data may be transformed.
  - Quick check question: Why would a model that performs well on ImageNet-1K still fail when images are slightly shifted or scaled?

- Concept: Transfer learning benchmarks and linear probing
  - Why needed here: Transferability is evaluated using VTAB and linear probing on frozen features, which requires understanding how to measure a model's ability to adapt to new tasks.
  - Quick check question: What is the key difference between fine-tuning a model versus using linear probing on frozen features when evaluating transferability?

## Architecture Onboarding

- Component map: ViT-sup -> ConvNeXt-sup -> ViT-clip -> ConvNeXt-clip (model configurations) -> Mistakes -> Calibration -> Robustness -> Transferability -> Synthetic data -> Transformation invariance (evaluation dimensions)

- Critical path: ImageNet accuracy → multi-dimensional evaluation → model selection recommendation
  - Start with models having similar ImageNet accuracy
  - Evaluate across 6+ behavioral dimensions
  - Use results to guide model selection based on application needs

- Design tradeoffs:
  - Accuracy vs calibration: CLIP has better accuracy on distribution shifts but worse calibration
  - Architecture vs training: ConvNeXt has architectural advantages on synthetic data and transformations, but CLIP training provides better generalization
  - Computational cost vs behavioral benefits: More comprehensive evaluation requires significant compute but provides better guidance

- Failure signatures:
  - Assuming ImageNet accuracy alone is sufficient for model selection
  - Overlooking calibration differences when deploying CLIP models
  - Ignoring transformation invariance requirements for applications with variable input conditions

- First 3 experiments:
  1. Replicate calibration comparison: Run reliability diagrams and ECE calculation on ImageNet-1K and ImageNet-R for all four models
  2. Verify transformation invariance results: Test scale, shift, and resolution invariance on a subset of ImageNet-1K validation set
  3. Validate synthetic data performance: Evaluate all models on PUG-ImageNet synthetic dataset and compare against reported results

## Open Questions the Paper Calls Out

### Open Question 1
How do model characteristics change when comparing models with equal ImageNet performance but different parameter counts or computational requirements? The study focused on models with similar ImageNet accuracies and compute requirements, but the relationship between model size, computational requirements, and various performance characteristics remains unexplored.

### Open Question 2
What is the optimal balance between model size and training data diversity for achieving superior performance on real-world tasks? While the study demonstrates the advantages of CLIP models trained on diverse data, the specific relationship between model size, training data diversity, and performance on real-world tasks remains unclear.

### Open Question 3
How can we develop new benchmarks that better capture real-world performance and are not biased towards ImageNet-like distributions? Current benchmarks, including ImageNet-based ones, may not fully capture the nuances and challenges of real-world applications, limiting our ability to assess model performance accurately.

## Limitations

- The study relies on pretrained models without training control, making it difficult to isolate whether observed differences stem from architecture or training methodology
- The selection of models with "similar" ImageNet accuracy and compute requirements may not be sufficiently tight, potentially confounding results
- The synthetic data and transformation invariance experiments use specific datasets (PUG-ImageNet) that may not generalize to all real-world scenarios

## Confidence

- High confidence in the core finding that CLIP models are overconfident while supervised models are better calibrated
- Medium confidence in architectural advantages of ConvNeXt over ViT
- Medium confidence in transferability findings

## Next Checks

1. Replicate the calibration experiments using multiple random seeds and bin configurations to verify the consistency of ECE differences between CLIP and supervised models
2. Conduct ablation studies by training both architectures with the same training regime (either both supervised or both CLIP) to isolate architectural effects from training effects
3. Extend the transformation invariance experiments to include additional transformation types (rotation, color jitter, noise) to assess the generality of observed patterns