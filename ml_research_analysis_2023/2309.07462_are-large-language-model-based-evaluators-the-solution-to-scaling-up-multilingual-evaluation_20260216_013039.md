---
ver: rpa2
title: Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual
  Evaluation?
arxiv_id: '2309.07462'
source_url: https://arxiv.org/abs/2309.07462
tags:
- human
- samples
- agreement
- given
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) show promise as multilingual evaluators
  but exhibit bias toward higher scores, particularly for non-Latin and lower-resource
  languages. The study evaluates GPT-4 as an evaluator across three text-generation
  tasks, five metrics, and eight languages, calibrating its judgments against 20K
  human annotations.
---

# Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?

## Quick Facts
- arXiv ID: 2309.07462
- Source URL: https://arxiv.org/abs/2309.07462
- Reference count: 40
- Large language models (LLMs) show promise as multilingual evaluators but exhibit bias toward higher scores, particularly for non-Latin and lower-resource languages.

## Executive Summary
This study evaluates GPT-4 as a multilingual evaluator across three text-generation tasks, five metrics, and eight languages, using 20K human annotations for calibration. While LLM-based evaluators demonstrate high consistency with human judgments when humans agree on positive scores, they show a systematic bias toward assigning higher scores even when human annotators disagree. This bias is particularly pronounced for non-Latin scripts and lower-resource languages like Czech. The research recommends careful calibration of LLM evaluators against human judgments and suggests best practices for future multilingual evaluation work.

## Method Summary
The study uses GPT-4 to evaluate LLM-generated text across three tasks (Open Prompt, Continue Writing, Summarize) and five metrics (Linguistic Acceptability, Output Content Quality, Task Quality, Problematic Content, Hallucinations) in eight languages. Human annotators provide judgments for 20K examples, which are then compared against LLM evaluations using percentage agreement as the primary metric. The researchers test different prompting strategies including single metric vs. compound prompting and zero-shot vs. few-shot approaches, while controlling for temperature at 0 to ensure consistency.

## Key Results
- LLM-based evaluators show high consistency with human judgments when humans agree on positive scores, but tend to assign high scores even when human annotators disagree
- Compound prompting (evaluating all metrics in one call) performs worse than single metric prompting
- LLM evaluators exhibit significant bias toward higher scores, especially for non-Latin scripts and lower-resource languages like Czech

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based evaluators show high consistency with human judgments when humans agree on positive scores
- Mechanism: When human annotators agree on a score (especially high scores like 2), the LLM-based evaluator tends to reproduce this consensus, indicating alignment in quality assessment for clear-cut cases
- Core assumption: Human agreement represents the "ground truth" for quality assessment
- Evidence anchors:
  - [abstract] "LLM-based evaluators demonstrate high consistency but tend to assign high scores even when human annotators disagree"
  - [section] "Overall, we find that the LLM-evaluator prompted using the compound prompt has a lower agreement with human annotators than the single prompt variation"

### Mechanism 2
- Claim: Compound prompting (evaluating all metrics in one call) performs worse than single metric prompting
- Mechanism: Evaluating multiple metrics simultaneously in one LLM call introduces cognitive load or confusion, reducing accuracy compared to focused single-metric evaluation
- Core assumption: LLMs can better focus on individual evaluation criteria when prompts are task-specific
- Evidence anchors:
  - [section] "we find that the LLM-evaluator prompted using the compound prompt has a lower agreement with human annotators than the single prompt variation"
  - [section] "We experiment with several prompting strategies for LLM-based evaluators and find that evaluating a single metric at a time produces better results"

### Mechanism 3
- Claim: LLM-based evaluators show bias toward higher scores, especially for non-Latin scripts and lower-resource languages
- Mechanism: LLMs trained primarily on Latin-script languages may have implicit bias toward generating more favorable assessments for languages they're less familiar with, or may default to optimistic scoring when uncertain
- Core assumption: Training data imbalance creates systematic evaluation bias
- Evidence anchors:
  - [abstract] "This bias is especially pronounced in non-Latin script and lower-resource languages"
  - [section] "We find that PA and bias towards higher scores are particularly evident in non-Latin script languages such as Chinese and Japanese, and lower-resource languages such as Czech"

## Foundational Learning

- Concept: Calibration of automated evaluators against human judgments
  - Why needed here: The study fundamentally relies on comparing LLM evaluator outputs against human annotations to identify biases and calibration needs
  - Quick check question: If an LLM evaluator agrees with humans 90% of the time on positive cases but only 60% on negative cases, what does this indicate about its calibration?

- Concept: Prompt engineering and few-shot learning in LLMs
  - Why needed here: The study tests various prompting strategies (single vs. compound, zero-shot vs. few-shot) to optimize evaluator performance
  - Quick check question: Why might providing few-shot examples not improve performance if the examples are drawn from a different data distribution than the evaluation set?

- Concept: Inter-annotator agreement measurement
  - Why needed here: The study uses percentage agreement between human annotators as a baseline to evaluate LLM evaluator performance
  - Quick check question: If three annotators have 80% pairwise agreement, what is the expected percentage agreement if their ratings are independent?

## Architecture Onboarding

- Component map: Data collection pipeline -> Prompt generation system -> LLM evaluation engine -> Calibration analysis module -> Sensitivity testing framework
- Critical path:
  1. Human annotators score LLM outputs
  2. LLM evaluator scores same outputs with different prompts
  3. Calculate percentage agreement between human and LLM scores
  4. Analyze class distributions to identify bias patterns
  5. Conduct ablation experiments to optimize prompting strategy
- Design tradeoffs:
  - Single vs. compound prompting: Accuracy vs. computational efficiency
  - Simple vs. detailed instructions: Ease of implementation vs. potential reduction in bias
  - Temperature=0 vs. higher temperatures: Consistency vs. exploration of evaluation variations
- Failure signatures:
  - Consistently high scores from LLM regardless of human disagreement
  - Poor agreement specifically for non-Latin scripts or lower-resource languages
  - Degradation in performance when switching from single to compound prompting
  - Temperature sensitivity indicating instability in evaluation consistency
- First 3 experiments:
  1. Compare percentage agreement between human annotators vs. LLM evaluator across all languages and tasks
  2. Test single metric prompting vs. compound prompting on the full dataset
  3. Evaluate sensitivity by randomly shuffling word order in 10% of sentences and checking if scores change

## Open Questions the Paper Calls Out

- Question: How can we create high-quality datasets for calibrating LLM-based evaluators in multiple languages, especially for lower-resource and non-Latin script languages?
  - Basis in paper: [explicit] The authors suggest this as a future research direction, noting the urgent need for such datasets to ensure accurate evaluation of LLM performance across diverse languages.
  - Why unresolved: Creating such datasets requires significant resources, including native speaker annotators proficient in the target languages, clear annotation guidelines, and a variety of evaluation dimensions.
  - What evidence would resolve it: Successful creation and validation of a multilingual dataset for LLM evaluator calibration, demonstrating improved evaluation accuracy for lower-resource and non-Latin script languages.

- Question: Can different prompting strategies, such as persona-based evaluation or consensus-building among multiple evaluators, help address the bias towards higher scores observed in LLM-based evaluators?
  - Basis in paper: [explicit] The authors note that providing more detailed instructions did not eliminate the bias, suggesting the need for alternative prompting approaches. They also mention the potential of using different evaluator personas to represent diverse perspectives.
  - Why unresolved: The effectiveness of various prompting strategies in mitigating bias is not yet established, and the optimal approach may depend on the specific evaluation task and language.
  - What evidence would resolve it: Empirical comparison of different prompting strategies on a multilingual dataset, showing their impact on evaluator bias and accuracy.

- Question: How does the performance of LLM-based evaluators vary across different language families and writing systems, and what factors contribute to these variations?
  - Basis in paper: [inferred] The authors observe lower performance for non-Latin script languages and lower-resource languages, but do not provide a detailed analysis of variations across language families or writing systems.
  - Why unresolved: Understanding the factors influencing LLM evaluator performance across different languages is crucial for developing effective evaluation strategies and addressing potential biases.
  - What evidence would resolve it: Comprehensive analysis of LLM evaluator performance across multiple language families and writing systems, identifying key factors contributing to variations and potential solutions.

## Limitations
- The study is limited to GPT-4 as the LLM evaluator, potentially limiting generalizability to other models
- Evaluation is constrained to text-generation tasks in eight specific languages, which may not capture the full diversity of multilingual evaluation challenges
- The temperature=0 setting, while ensuring consistency, may not reflect real-world usage where some stochasticity is expected

## Confidence
- **High Confidence**: The finding that LLM-based evaluators show high consistency with human judgments when humans agree on positive scores is well-supported by the evidence
- **Medium Confidence**: The claim about compound prompting performing worse than single metric prompting is supported but may depend on specific prompt engineering choices and LLM architecture
- **Medium Confidence**: The bias toward higher scores for non-Latin and lower-resource languages is documented, but the underlying mechanism remains speculative

## Next Checks
1. Test the same evaluation methodology with different LLM architectures (e.g., Claude, LLaMA, or open-source multilingual models) to determine if the observed biases and performance patterns are model-specific or general to LLM-based evaluation
2. Fine-tune an LLM on balanced multilingual evaluation data and repeat the evaluation to determine whether the observed biases toward non-Latin and lower-resource languages can be mitigated through targeted training
3. Conduct a systematic analysis across a range of temperature settings (not just temperature=0) to determine the stability of LLM evaluator judgments and identify the optimal temperature range for balancing consistency with nuanced evaluation