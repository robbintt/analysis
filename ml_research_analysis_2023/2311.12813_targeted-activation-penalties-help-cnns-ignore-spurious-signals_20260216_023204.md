---
ver: rpa2
title: Targeted Activation Penalties Help CNNs Ignore Spurious Signals
arxiv_id: '2311.12813'
source_url: https://arxiv.org/abs/2311.12813
tags:
- spurious
- teacher
- signals
- annotations
- permuted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of neural networks learning to
  rely on spurious signals in training data, leading to poor generalization. The authors
  propose Targeted Activation Penalty (TAP), a novel method that tackles this issue
  by penalizing activations to control the re-emergence of spurious signals in deep
  convolutional neural networks (CNNs).
---

# Targeted Activation Penalties Help CNNs Ignore Spurious Signals

## Quick Facts
- arXiv ID: 2311.12813
- Source URL: https://arxiv.org/abs/2311.12813
- Reference count: 40
- Primary result: TAP outperforms state-of-the-art methods at reducing spurious signal reliance while using less memory and training time

## Executive Summary
This paper addresses the problem of neural networks learning to rely on spurious signals in training data, leading to poor generalization. The authors propose Targeted Activation Penalty (TAP), a novel method that tackles this issue by penalizing activations to control the re-emergence of spurious signals in deep convolutional neural networks (CNNs). TAP is shown to outperform two state-of-the-art baselines, Right for the Right Reasons (RRR) and Right for Better Reasons (RBR), while also reducing training time and memory usage. Additionally, the authors demonstrate that TAP can still be effective when using annotations generated by pre-trained models as substitutes for ground-truth annotations, making it a more practical solution in real-world applications. The method is validated on the MNIST benchmark and two clinical image datasets, using four different CNN architectures.

## Method Summary
TAP addresses spurious signal reliance by adding a penalty term to the loss function that minimizes the sum of activations in regions identified as containing spurious signals. This penalty is applied to specific layers of the CNN where spatial correspondence between input and activation maps is preserved. When ground-truth annotations are unavailable, TAP uses saliency maps from pre-trained teacher models to identify regions to penalize. The method is computationally efficient as it doesn't require computing second-order derivatives like RRR/RBR. TAP is validated on MNIST and two medical imaging datasets (pneumonia and osteoarthritis detection) with various CNN architectures including VGG-16, ResNet-18, and DenseNet-121.

## Key Results
- TAP outperforms RRR and RBR baselines while using less memory and training time
- TAP remains effective when using teacher-generated annotations instead of ground-truth annotations
- On PNEU dataset with 1% clean data, TAP achieves 90.8% F-score compared to 74.1% for baseline models
- TAP reduces performance drop when spurious signals are permuted, indicating reduced reliance on these signals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TAP reduces reliance on spurious signals by penalizing activations in regions corresponding to spurious signals
- Mechanism: The method adds a penalty term to the loss function that minimizes the sum of activations in areas identified by annotation masks. This encourages the network to learn to ignore these regions during classification.
- Core assumption: Convolutional layers preserve spatial relationships, so activations in spurious signal regions can be targeted effectively
- Evidence anchors:
  - [abstract]: "TAP is shown to outperform two state-of-the-art baselines... by penalizing activations to control the re-emergence of spurious signals"
  - [section 4.1]: "TAP takes advantage of the fact that convolutional layers preserve spatial relationships within an image in its activation map"
  - [corpus]: Weak evidence - related work focuses on Jacobian regularization and activation transfer, not activation penalties
- Break condition: If the spatial correspondence between input and activation maps breaks down in deeper layers, or if the downscaling function D fails to preserve annotation spatial information

### Mechanism 2
- Claim: TAP indirectly suppresses input gradients of spurious signals without computing expensive second-order derivatives
- Mechanism: By minimizing activations in spurious regions, TAP pushes ReLU gradients to zero in those areas, which cascades to reduce input gradients. This provides similar protection to RRR/RBR but with lower computational cost.
- Core assumption: ReLU activation function allows optimization of activation map elements to set input gradients to zero
- Evidence anchors:
  - [abstract]: "TAP... lowering training times and memory usage" compared to RRR/RBR
  - [section 4.1]: Theorem 1 shows how targeting activations can set input gradients to zero
  - [section 4.1]: "TAP does not require computing second-order derivatives, which results in faster training times"
- Break condition: If non-ReLU activations are used, or if the receptive field calculations in Theorem 1 don't hold for the architecture

### Mechanism 3
- Claim: Teacher annotations from pre-trained models can effectively substitute for ground-truth annotations when clean data is scarce
- Mechanism: A pre-trained teacher model provides saliency maps that identify unimportant regions, which are then thresholded to create annotations. These annotations guide the student model to ignore spurious signals
- Core assumption: A teacher trained on clean data can identify spurious signals even when they're correlated with labels in contaminated data
- Evidence anchors:
  - [abstract]: "TAP can still be effective when using annotations generated by pre-trained models as substitutes for ground-truth annotations"
  - [section 4.2]: "We propose using saliency maps to identify areas... the teacher model believes to be unimportant, which, if the teacher fulfils Equation 1, should include the spurious signals"
  - [section 6.2]: "On PNEU, TAP with teacher annotations results in strong performances... there is very little difference between using the two sets of teacher annotations"
- Break condition: If the teacher model itself has learned spurious correlations, or if the saliency method fails to identify spurious signals

## Foundational Learning

- Concept: Activation maps in CNNs
  - Why needed here: TAP operates by targeting specific regions of activation maps corresponding to spurious signals
  - Quick check question: How do activation maps in convolutional layers relate spatially to the input image?

- Concept: Receptive fields in CNNs
  - Why needed here: Understanding how activation elements capture input pixels is crucial for Theorem 1's proof
  - Quick check question: How does the receptive field of an activation element in layer l relate to the input image coordinates?

- Concept: Saliency maps and input gradients
  - Why needed here: Teacher annotations are derived from saliency maps, and RRR/RBR baselines target input gradients
  - Quick check question: What's the difference between using saliency maps for explanation versus using them to guide training?

## Architecture Onboarding

- Component map:
  Input -> CNN with L layers producing activation maps Al -> Downscaling function D for annotation masks -> Loss function (task + TAP) -> SGD optimizer

- Critical path:
  1. Forward pass through CNN to compute activations
  2. Downscale annotation masks to match activation dimensions
  3. Compute TAP loss as L1 norm of mask ⊙ activation product
  4. Backpropagate combined loss (task + TAP)
  5. Update weights

- Design tradeoffs:
  - Targeting all vs. subset of layers: Full targeting increases regularization but computational cost
  - Annotation threshold τ: Higher values capture more spurious signals but may include relevant features
  - κ parameter in downscaling: Larger κ increases penalty region but may over-regularize

- Failure signatures:
  - High contamination sensitivity (large Δ-metric) indicates model still relies on spurious signals
  - Poor performance on clean data suggests over-regularization
  - Memory issues when targeting too many layers simultaneously

- First 3 experiments:
  1. Implement TAP on MNIST with ground-truth annotations and compare to No XS baseline
  2. Test different values of λ (loss weight) on validation set
  3. Switch to teacher annotations with 1% clean data and compare performance to ground-truth case

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TAP perform on architectures other than CNNs, such as transformers or recurrent neural networks?
- Basis in paper: [explicit] The authors mention they plan to investigate if a variant of TAP can be generalized to other model architectures besides CNNs
- Why unresolved: The current study focuses exclusively on CNNs, leaving the applicability of TAP to other architectures unexplored
- What evidence would resolve it: Experiments applying TAP to non-CNN architectures like transformers or RNNs, comparing performance against baseline methods

### Open Question 2
- Question: What is the optimal threshold τ for teacher annotations across different datasets and models?
- Basis in paper: [explicit] The authors note that performance depends on τ, but optimal values may vary across datasets and architectures
- Why unresolved: The paper only tests a few τ values (0.01, 0.05, 0.1) and finds performance varies, but doesn't determine optimal thresholds
- What evidence would resolve it: A systematic study varying τ across multiple datasets and architectures to identify patterns or optimal ranges

### Open Question 3
- Question: How does TAP compare to other explainable AI methods in terms of computational efficiency and memory usage?
- Basis in paper: [explicit] The authors note TAP reduces training time and memory usage compared to RRR and RBR, but don't compare to other XAI methods
- Why unresolved: The paper focuses on comparing TAP to RRR and RBR, not other XAI techniques that might also address spurious signals
- What evidence would resolve it: Benchmarks comparing TAP's computational efficiency and memory usage against other XAI methods like LIME or SHAP

## Limitations
- The method's effectiveness depends on reliable spatial correspondence between input and activation maps, which may break down in deeper layers
- Teacher annotation quality is limited by the teacher model's own ability to identify spurious signals, which may be compromised if the teacher has learned spurious correlations
- Performance with teacher annotations varies across datasets and contamination patterns, suggesting dataset-specific limitations

## Confidence
- **High confidence**: TAP's computational efficiency advantage over RRR/RBR baselines (supported by explicit complexity analysis and training time comparisons)
- **Medium confidence**: Effectiveness of teacher-generated annotations (supported by strong results on PNEU but limited testing on KNEE with different contamination patterns)
- **Medium confidence**: Generalizability across architectures (tested on four architectures but limited to classification tasks)

## Next Checks
1. Test TAP's effectiveness when spurious signals appear in non-corner locations or with different spatial patterns to verify the spatial correspondence assumption
2. Evaluate teacher annotation quality by measuring precision/recall of spurious signal detection before student training, and test on datasets where teacher models may have learned different spurious correlations
3. Implement TAP on a regression task or object detection task to validate generalization beyond classification problems