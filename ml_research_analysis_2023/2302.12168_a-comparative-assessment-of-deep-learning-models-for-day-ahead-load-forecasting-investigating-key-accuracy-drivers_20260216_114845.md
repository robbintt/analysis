---
ver: rpa2
title: 'A comparative assessment of deep learning models for day-ahead load forecasting:
  Investigating key accuracy drivers'
arxiv_id: '2302.12168'
source_url: https://arxiv.org/abs/2302.12168
tags:
- forecasting
- load
- time
- data
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study conducts a comparative analysis of four deep learning\
  \ architectures\u2014MLP, LSTM, N-BEATS, and TCN\u2014for day-ahead load forecasting\
  \ of Portugal's national electricity demand. Models are trained on data from 2013\u2013\
  2020 and evaluated on 2021 data."
---

# A comparative assessment of deep learning models for day-ahead load forecasting: Investigating key accuracy drivers

## Quick Facts
- arXiv ID: 2302.12168
- Source URL: https://arxiv.org/abs/2302.12168
- Reference count: 40
- Primary result: N-BEATS achieves 1.90% MAPE, outperforming MLP (2.34%), LSTM (2.22%), and TCN (2.22%) for Portuguese day-ahead load forecasting

## Executive Summary
This study provides a comprehensive comparative analysis of four deep learning architectures (MLP, LSTM, N-BEATS, TCN) for day-ahead load forecasting of Portugal's national electricity demand. Using 2013-2020 training data and 2021 test data, the research demonstrates that N-BEATS consistently outperforms other models with a MAPE of 1.90%, while ensembling 30 models per architecture further improves accuracy and robustness. The study also employs multiple linear regression to identify key accuracy drivers, revealing that holidays and midday hours significantly degrade forecasting accuracy across all models, with N-BEATS and LSTM showing the most resilience to these calendar effects.

## Method Summary
The research trains four deep learning architectures on Portuguese electricity demand data (2013-2020) and evaluates them on 2021 data. Models are optimized using tree-structured parzen estimator (TPE) with 100 hyperparameter tuning trials, then ensembles of 30 models per architecture are created with median aggregation. A multiple linear regression analysis correlates forecast errors with calendar features (time of day, season, holidays, weekends) and temperature to identify accuracy drivers. All models use historical load data only, without exogenous weather forecasts.

## Key Results
- N-BEATS achieves the lowest MAPE at 1.90%, outperforming TCN and LSTM (2.22%) and MLP (2.34%)
- Ensembling 30 models per architecture consistently improves accuracy and reduces variance across all model types
- Holidays and midday hours significantly degrade accuracy for all models, with N-BEATS and LSTM being most resilient
- Temperature negatively impacts forecasts, particularly for simpler architectures like MLP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: N-BEATS consistently outperforms other deep learning models in day-ahead load forecasting due to its double residual stacking and interpretable decomposition of time series patterns.
- Mechanism: The architecture's blocks produce backcast and forecast outputs, with the backcast subtracted from input for the next block. This enables both accurate forecasting and decomposition of target time series, allowing it to capture complex seasonal and trend components more effectively than other architectures.
- Core assumption: The double residual stacking strategy in N-BEATS is superior to other architectures for handling the non-linearity and non-stationarity of electricity demand time series.
- Evidence anchors:
  - [abstract] "Our results suggest that N-BEATS consistently outperforms the rest of the examined models."
  - [section] "N-BEATS is a deep feed-forward neural network... consists of n stacks, which comprise m fully connected non-linear neural regressor blocks, interconnected with double residual links."
- Break condition: If electricity demand patterns become more linear or stationary, or if the double residual stacking mechanism proves less effective for other time series characteristics.

### Mechanism 2
- Claim: Ensembling 30 models per architecture improves forecasting accuracy and robustness by mitigating the uncertainty of neural weight initializations.
- Mechanism: Training multiple models with the same hyperparameter values but different pseudo-random initializations of neural weights, then aggregating forecasts using median operator, reduces variance and prevents poor individual model forecasts from affecting final results.
- Core assumption: The variance in model performance due to different weight initializations is significant enough to impact overall accuracy, and aggregation effectively reduces this variance.
- Evidence anchors:
  - [abstract] "Ensembling 30 models per architecture further improves performance and robustness."
  - [section] "the median operator was used to aggregate the forecasts of the individual models... this analysis provides valuable insights to energy stakeholders interested in... producing forecasts in a selective fashion."
- Break condition: If individual model performance becomes highly consistent across different weight initializations, or if aggregation methods prove less effective than expected.

### Mechanism 3
- Claim: Calendar features (holidays and midday hours) significantly degrade forecasting accuracy across all models, with N-BEATS and LSTM being most resilient to these variations.
- Mechanism: The MLR analysis correlates forecast errors with calendar and weather features, revealing that certain time periods have inherently higher prediction difficulty. N-BEATS and LSTM architectures handle these difficult periods better due to their memory operations and pattern recognition capabilities.
- Core assumption: The difficulty in forecasting during certain calendar periods is consistent and measurable, and some architectures are inherently better at handling these specific patterns.
- Evidence anchors:
  - [abstract] "A multiple linear regression analysis reveals that holidays and midday hours significantly degrade accuracy across all models, with N-BEATS and LSTM being most resilient."
  - [section] "MLR model was estimated to correlate the MAPE values... with nine key forecast accuracy drivers... Three binary (one-hot encoded) features (F1: morning, F2: midday, F3: night), representing the time of day."
- Break condition: If calendar patterns become more predictable or if other external factors become more significant than calendar features in affecting accuracy.

## Foundational Learning

- Concept: Understanding of time series forecasting principles
  - Why needed here: The study compares different deep learning architectures for time series forecasting, requiring knowledge of how models handle temporal dependencies, seasonality, and non-linearity in electricity demand data.
  - Quick check question: What are the key challenges in short-term load forecasting that make it different from general time series forecasting?

- Concept: Knowledge of deep learning architectures (MLP, LSTM, N-BEATS, TCN)
  - Why needed here: The comparative analysis requires understanding how each architecture processes sequential data, their strengths and weaknesses, and why certain architectures perform better for electricity load forecasting.
  - Quick check question: How does the double residual stacking mechanism in N-BEATS differ from the processing approach in LSTM networks?

- Concept: Understanding of hyperparameter optimization and model ensembling
  - Why needed here: The study uses TPE optimization and ensembles of 30 models per architecture, requiring knowledge of how these techniques improve model performance and robustness.
  - Quick check question: Why might ensembling multiple models with the same hyperparameters but different weight initializations improve forecasting accuracy?

## Architecture Onboarding

- Component map: Data preprocessing (hourly load data from 2013-2021) -> Hyperparameter optimization (100 trials) -> Train 30 models per architecture -> Ensemble forecasts (median) -> Evaluate accuracy (MAPE) -> MLR analysis for performance drivers
- Critical path: Data preprocessing → Hyperparameter optimization (100 trials) → Train 30 models per architecture → Ensemble forecasts (median) → Evaluate accuracy (MAPE) → MLR analysis for performance drivers
- Design tradeoffs: Computational cost vs accuracy (TCN is most expensive but not most accurate), complexity vs interpretability (N-BEATS offers both accuracy and interpretability), real-time deployment feasibility (MLP is fastest but less accurate)
- Failure signatures: High variance in individual model performance suggests poor hyperparameter selection or data quality issues; systematic errors during holidays indicate calendar features not properly captured; temperature sensitivity suggests weather patterns not adequately learned
- First 3 experiments:
  1. Train a single MLP model with default parameters to establish baseline performance and verify data preprocessing pipeline
  2. Run TPE optimization for one architecture (e.g., LSTM) with 10 trials to test optimization framework and identify promising hyperparameter ranges
  3. Train an ensemble of 5 N-BEATS models with optimized parameters to validate the ensembling approach and compare with single model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of exogenous variables like weather forecasts impact the accuracy of autoregressive DL models in day-ahead load forecasting?
- Basis in paper: [explicit] The study explicitly mentions that autoregressive models do not require weather forecasts as additional input, focusing instead on the impact of historical load patterns and calendar features.
- Why unresolved: The paper does not explore the potential benefits or drawbacks of incorporating exogenous variables like weather forecasts into autoregressive models, leaving a gap in understanding their full potential.
- What evidence would resolve it: Conducting experiments that compare the performance of autoregressive models with and without weather forecasts as input would provide insights into their impact on accuracy.

### Open Question 2
- Question: How do the DL models perform in forecasting accuracy when applied to different geographical regions with varying load patterns and external factors?
- Basis in paper: [inferred] The study focuses on the Portuguese national load, but the authors suggest that the comparative assessment could be replicated using data from more countries to benchmark the performance of DL models objectively.
- Why unresolved: The paper does not provide data or results from other regions, limiting the generalizability of the findings to other geographical contexts.
- What evidence would resolve it: Replicating the study with data from multiple countries and comparing the performance of DL models across different regions would provide insights into their robustness and adaptability.

### Open Question 3
- Question: What is the impact of different hyperparameter optimization techniques on the performance of DL models in STLF?
- Basis in paper: [explicit] The study uses the tree-structured parzen estimator (TPE) for hyperparameter optimization, but it does not explore other optimization techniques or compare their effectiveness.
- Why unresolved: The paper does not investigate alternative hyperparameter optimization methods, leaving questions about the potential benefits of other techniques.
- What evidence would resolve it: Conducting experiments with different hyperparameter optimization techniques and comparing their impact on model performance would provide insights into the most effective methods for STLF.

## Limitations
- Analysis is restricted to Portuguese electricity demand, limiting generalizability to other regions or markets
- Temperature data from a single location (Lisbon) may not capture regional variations affecting demand
- The study does not explicitly model COVID-19 impacts, which significantly affected 2021 data patterns

## Confidence
- Comparative model performance results: High
- MLR analysis identifying calendar features as key accuracy drivers: Medium
- Generalizability of findings to other regions: Low

## Next Checks
1. Replicate the MLR analysis using different variable selection methods (LASSO, Ridge) to test robustness of calendar feature identification
2. Test model performance on a different geographical region (e.g., Spanish electricity demand) to assess generalizability
3. Conduct ablation studies removing specific architectural components (e.g., N-BEATS stacks) to isolate performance drivers