---
ver: rpa2
title: 'ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by Learning
  to Scale'
arxiv_id: '2310.01217'
source_url: https://arxiv.org/abs/2310.01217
tags:
- scalearn
- learning
- tasks
- task
- adapter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of existing two-stage multi-task
  learning methods, which require a large number of additional parameters for knowledge
  transfer. To overcome this, the authors propose ScaLearn, a simple and highly parameter-efficient
  two-stage MTL method that leverages the knowledge of source adapters by learning
  a minimal set of scaling parameters.
---

# ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by Learning to Scale

## Quick Facts
- arXiv ID: 2310.01217
- Source URL: https://arxiv.org/abs/2310.01217
- Reference count: 40
- Key outcome: ScaLearn achieves GLUE performance of 85.36% with only 0.47% of AdapterFusion's parameters

## Executive Summary
This paper addresses the inefficiency of existing two-stage multi-task learning methods that require large numbers of additional parameters for knowledge transfer. The authors propose ScaLearn, a parameter-efficient method that leverages source adapter knowledge through learned scaling parameters. By linearly scaling output representations of source adapters and combining them through element-wise sum, ScaLearn achieves competitive performance with minimal parameter overhead. Experiments across GLUE, SuperGLUE, and HumSet benchmarks using RoBERTa and XLM-R models demonstrate that ScaLearn consistently outperforms strong baselines while using orders of magnitude fewer parameters.

## Method Summary
ScaLearn implements a two-stage MTL approach where source adapters are first trained on individual tasks, then frozen during target task adaptation. For each target task, ScaLearn learns scaling parameters that control how source adapter outputs are combined. The method applies element-wise scaling to adapter outputs (ScaLearn) or scalar scaling (ScaLearnUniform) before summing them, allowing selective activation of task-specific knowledge. The approach can be further compressed through layer-sharing variants (ScaLearn++ and ScaLearnUniform++) that reduce parameter count to as few as 8 parameters per target task.

## Key Results
- Achieves 85.36% average GLUE score with only 0.47% of AdapterFusion's parameters
- ScaLearnUniform++ variant maintains competitive performance with just 8 parameters per target task
- Consistently outperforms strong baselines across GLUE, SuperGLUE, and HumSet benchmarks
- Demonstrates effectiveness with both RoBERTa and XLM-R architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling output vectors of source adapters enables selective activation of task-specific knowledge.
- Mechanism: By learning scaling coefficients that are applied element-wise to source adapter outputs, the model controls the degree to which each source task's knowledge contributes to solving the target task.
- Core assumption: Optimal scaling coefficients can be learned independently for each target task without constraints.
- Evidence anchors: The paper shows combinations where scaling coefficients don't sum to 1 can achieve better performance.

### Mechanism 2
- Claim: Learning separate scaling parameters per target task prevents destructive interference from multi-task optimization.
- Mechanism: Each target task learns its own scaling parameters independently, avoiding the competition between tasks that occurs in joint MTL optimization.
- Core assumption: Task interference primarily stems from shared optimization rather than incompatible representations.
- Evidence anchors: Two-stage approach allows independent optimization of scaling parameters for each target task.

### Mechanism 3
- Claim: Uniform scaling maintains performance while drastically reducing parameter count.
- Mechanism: ScaLearnUniform reduces each scaling vector to a single scalar parameter, achieving high parameter efficiency while maintaining competitive performance.
- Core assumption: Magnitude of scaling is more important than directional information for effective knowledge combination.
- Evidence anchors: The method achieves competitive results with only 8 parameters per target task.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning
  - Why needed here: ScaLearn builds on adapter-based architectures where small modules adapt PLMs to specific tasks without full fine-tuning.
  - Quick check question: What reduction factor is typically used for adapter modules in transformer architectures?

- Concept: Multi-task learning interference
  - Why needed here: Understanding task interference is crucial because ScaLearn's two-stage approach specifically addresses this problem.
  - Quick check question: What is the primary difference between joint MTL and two-stage MTL in terms of optimization strategy?

- Concept: Linear probing and representation analysis
  - Why needed here: The paper's analysis section uses linear probing to examine how scaling affects transfer learning performance.
  - Quick check question: In linear probing experiments, what metric is typically used to evaluate the effectiveness of different scaling configurations?

## Architecture Onboarding

- Component map: PLM backbone → Source adapters (one per task) → ScaLearn scaling layer → Task head
- Critical path: Gradients flow from task heads through ScaLearn scaling parameters to source adapters, which remain frozen
- Design tradeoffs: Uniform scaling reduces parameters further but may lose task-specific directional information; layer-sharing variants trade more parameters for greater efficiency
- Failure signatures: Poor performance may indicate incompatible source adapters, ineffective scaling coefficient learning, or unresolved task interference
- First 3 experiments:
  1. Implement linear probing: freeze adapters and PLM, train task head with different fixed scaling values
  2. Implement ScaLearn with full vectors: train with learning rate sweep to find optimal learning rate
  3. Implement ScaLearnUniform: compare performance with full vector version to validate parameter efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound on performance improvement when using ScaLearn compared to traditional adapter fusion methods?
- Basis in paper: The paper shows empirical superiority but doesn't analyze theoretical limitations or maximum potential gains
- Why unresolved: Demonstrates empirical results but lacks formal analysis of theoretical performance ceiling
- What evidence would resolve it: Formal proof or empirical study establishing maximum possible performance improvement

### Open Question 2
- Question: How does ScaLearn perform on tasks with highly non-linear relationships between source and target tasks?
- Basis in paper: Focuses on linear scaling but doesn't explore performance on tasks requiring complex, non-linear knowledge transfer
- Why unresolved: Analysis limited to tasks where linear combinations are effective, non-linear task relationships unexplored
- What evidence would resolve it: Systematic experiments across tasks with varying degrees of non-linearity

### Open Question 3
- Question: What is the impact of ScaLearn's performance when source adapters are of varying quality or trained on imbalanced datasets?
- Basis in paper: Mentions potential for reusing existing adapters with different performance characteristics
- Why unresolved: Experiments use uniformly trained adapters, real-world applications likely involve adapters of varying quality
- What evidence would resolve it: Experiments with controlled variations in adapter quality and training conditions

## Limitations
- Linear scaling assumption may not hold for tasks with highly conflicting or incompatible representations
- Results primarily demonstrated on specific PLM architectures (RoBERTa, XLM-R) and benchmarks, raising generalizability questions
- Most parameter-efficient variants (ScaLearnUniform++) may be too restrictive for complex task combinations

## Confidence

**High Confidence**: Core experimental results showing ScaLearn outperforms AdapterFusion with significantly fewer parameters (0.47% of parameters while achieving competitive GLUE scores). The two-stage MTL framework design and parameter counting methodology are well-established.

**Medium Confidence**: Claim that uniform scaling maintains competitive performance while using only 8 parameters per task. Results presented but trade-off between parameter efficiency and potential performance loss not fully explored across diverse task combinations.

**Low Confidence**: Mechanism claim that linear scaling resolves multi-task interference by allowing independent scaling per target task. Works empirically but provides limited theoretical justification for why independent scaling prevents interference.

## Next Checks

1. **Interference Analysis**: Vary the number of source adapters used in combination (from 1 to all available) to quantify how ScaLearn handles increasing task interference and identify break points where performance degrades.

2. **Generalization Test**: Apply ScaLearn to a different PLM architecture (e.g., BERT or DeBERTa) and a different task suite (e.g., SuperGLUE only or domain-specific benchmarks) to validate the approach's architecture and task independence.

3. **Scaling Coefficient Analysis**: Analyze the learned scaling coefficients across tasks to determine if they exhibit meaningful patterns or if they are essentially random, which would suggest the mechanism is less principled than claimed.