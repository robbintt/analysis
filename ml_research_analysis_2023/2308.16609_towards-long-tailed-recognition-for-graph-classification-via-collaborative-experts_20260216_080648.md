---
ver: rpa2
title: Towards Long-Tailed Recognition for Graph Classification via Collaborative
  Experts
arxiv_id: '2308.16609'
source_url: https://arxiv.org/abs/2308.16609
tags:
- learning
- graph
- classes
- class
- long-tailed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of long-tailed graph classification,
  where a few classes dominate the dataset while many classes have few samples. The
  authors propose a novel method called Collaborative Multi-expert Learning (CoMe)
  to address this issue.
---

# Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts

## Quick Facts
- arXiv ID: 2308.16609
- Source URL: https://arxiv.org/abs/2308.16609
- Reference count: 40
- Key outcome: CoMe significantly outperforms existing state-of-the-art methods, especially in severe imbalance settings.

## Executive Summary
This paper tackles the problem of long-tailed graph classification, where a few classes dominate the dataset while many classes have few samples. The authors propose a novel method called Collaborative Multi-expert Learning (CoMe) to address this issue. CoMe jointly optimizes representation learning and classifier training using balanced contrastive learning and balanced predicted probability. It also incorporates multi-expert fusion and disentangled knowledge distillation to enhance the model's performance. Experiments on seven benchmark datasets demonstrate that CoMe significantly outperforms existing state-of-the-art methods, especially in severe imbalance settings. The paper provides valuable insights into the effectiveness of combining balanced contrastive learning and classifier training for long-tailed graph classification.

## Method Summary
The CoMe framework consists of four modules: balanced contrastive learning of individual expert, individual-expert classifier learning, multi-expert fusion module, and inter-expert distillation module. The method involves jointly optimizing representation learning and classifier training using balanced contrastive learning and balanced predicted probability, along with multi-expert fusion and disentangled knowledge distillation. The model is trained on seven benchmark datasets with different imbalance factors (IFs), and its performance is evaluated using top-1 average accuracy over 10 runs.

## Key Results
- CoMe significantly outperforms existing state-of-the-art methods on long-tailed graph classification tasks.
- The method is particularly effective in severe imbalance settings, where it shows substantial improvements over baseline methods.
- CoMe demonstrates strong generalization ability on a balanced test dataset, correctly classifying graphs in all classes, not just the head classes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Balanced contrastive learning (BCL) mitigates class imbalance by equalizing the contributions of head and tail classes during representation learning.
- Mechanism: BCL modifies the contrastive loss to include class-specific learnable anchors and class-frequency-weighted similarity scores, ensuring that tail-class graphs are not overwhelmed by head-class negatives.
- Core assumption: Tail-class graphs are underrepresented in random negatives, so explicit anchor-based balancing improves tail-class discriminative power.
- Evidence anchors:
  - [abstract] "To equilibrate the contributions of head and tail classes, we first develop balanced contrastive learning from the view of representation learning"
  - [section IV-A] Detailed mathematical derivation showing how BCL adjusts probabilities for head vs. tail classes.
- Break condition: If the class anchors collapse or the temperature τ is set too low, the balancing effect diminishes.

### Mechanism 2
- Claim: Balanced predicted probability (BPP) removes classifier bias by incorporating class frequency priors into the softmax output.
- Mechanism: BPP multiplies logits by inverse class frequency before normalization, preventing the model from favoring head classes based on sample size.
- Core assumption: The logits implicitly encode class frequency bias; correcting them during training reduces this bias.
- Evidence anchors:
  - [abstract] "design an individual-expert classifier training based on hard class mining"
  - [section IV-B] Derivation of BPP and comparison with naive softmax.
- Break condition: If class frequencies are misestimated or BPP is applied only at inference, bias reduction fails.

### Mechanism 3
- Claim: Disentangled knowledge distillation (DKL) improves collaboration by separately optimizing target-class and non-target-class knowledge transfer.
- Mechanism: DKL splits the KL divergence into target and non-target components with tunable weights β1 and β2, avoiding mutual suppression.
- Core assumption: Knowledge about distinguishing target vs. non-target classes has different confidence and utility across experts.
- Evidence anchors:
  - [abstract] "execute gated fusion and disentangled knowledge distillation among the multiple experts to promote the collaboration"
  - [section IV-D] Mathematical formulation of DKL and ablation showing performance gains.
- Break condition: If β1 or β2 are set too high/low, distillation may harm rather than help learning.

## Foundational Learning

- Concept: Graph neural networks (GNNs) and message passing.
  - Why needed here: The framework relies on GNN encoders to produce graph-level embeddings that BCL and BPP operate on.
  - Quick check question: Can you explain how a 2-layer GNN aggregates neighbor features to produce a graph representation?

- Concept: Contrastive learning and positive/negative pairs.
  - Why needed here: BCL is built on contrastive learning principles, so understanding how positives/negatives are formed is essential.
  - Quick check question: In standard contrastive loss, how are positive and negative pairs defined for a graph sample?

- Concept: Long-tailed classification and class imbalance.
  - Why needed here: The entire method targets imbalanced graph datasets; knowing how imbalance affects accuracy is key.
  - Quick check question: What happens to a classifier trained on long-tailed data if no rebalancing is applied?

## Architecture Onboarding

- Component map:
  - GNN encoder (GraphSAGE) → embeddings
  - Balanced contrastive learning module (BCL) → representation balancing
  - Classifier with balanced predicted probability (BPP) → bias reduction
  - Gated multi-expert fusion → diversity weighting
  - Disentangled knowledge distillation (DKL) → expert collaboration
- Critical path: GNN encoder → BCL → BPP → expert outputs → gated fusion → final prediction
- Design tradeoffs:
  - More experts increase diversity but add training complexity.
  - Higher α in BCL boosts tail-class emphasis but may underfit head classes.
  - Gating prototypes add learnable parameters; wrong initialization can stall convergence.
- Failure signatures:
  - Loss divergence → check BCL temperature and α settings.
  - All experts predict the same class → verify gating prototype learning.
  - Tail-class accuracy low despite BCL → confirm BPP is applied in training.
- First 3 experiments:
  1. Run CoMe on COLLAB with default hyperparams and measure per-class accuracy.
  2. Disable BCL (use vanilla SupCon) and compare tail-class performance.
  3. Disable DKL and measure if expert collaboration drops.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions. However, based on the content, some potential open questions could include:
- How does the performance of CoMe change when applied to graph datasets with different imbalance factors, particularly in scenarios where the imbalance factor exceeds the range tested in the paper?
- How does the performance of CoMe compare to other state-of-the-art methods when applied to real-world graph datasets with varying degrees of imbalance and different graph characteristics (e.g., size, density, connectivity)?
- How does the performance of CoMe change when applied to graph datasets with different types of node and edge features, particularly in scenarios where the features are noisy, incomplete, or irrelevant?

## Limitations
- The method's reliance on multiple hyper-parameters (α, τ, η, ϵ, β₁, β₂) introduces sensitivity to tuning, and the paper provides limited ablation studies on their optimal ranges.
- The claim of "significantly outperforms" existing methods is supported by experiments on seven datasets, but the comparison with recent specialized long-tailed graph methods is not comprehensive.
- The disentangled knowledge distillation mechanism, while theoretically sound, lacks sufficient empirical validation to confirm its practical benefit over standard distillation.

## Confidence
- The balanced contrastive learning and classifier training mechanisms are well-justified: Medium
- The interaction effects between modules and generalization across diverse graph types require further validation: Medium
- The disentangled knowledge distillation mechanism's practical benefit over standard distillation: Low

## Next Checks
1. Perform sensitivity analysis on hyper-parameters (α, τ, β₁, β₂) to determine robustness ranges and identify potential overfitting to specific settings.
2. Compare CoMe against recent specialized long-tailed graph classification methods like RAHNet to establish relative performance in the most challenging imbalance scenarios.
3. Evaluate CoMe on additional graph datasets with different characteristics (e.g., citation networks, biological graphs) to test generalization beyond the current benchmark suite.