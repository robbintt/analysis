---
ver: rpa2
title: Object-centric Cross-modal Feature Distillation for Event-based Object Detection
arxiv_id: '2311.05494'
source_url: https://arxiv.org/abs/2311.05494
tags:
- distillation
- feature
- event
- object
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of improving event-based object
  detection by leveraging knowledge distillation from grayscale images. The key challenge
  is that event data is sparser and lacks global illumination information, making
  it difficult to directly transfer knowledge from image-based detectors.
---

# Object-centric Cross-modal Feature Distillation for Event-based Object Detection

## Quick Facts
- arXiv ID: 2311.05494
- Source URL: https://arxiv.org/abs/2311.05494
- Reference count: 40
- Key outcome: Cross-modal distillation improves event-based object detection by 2.6-3.4 mAP points by focusing on object-centric features rather than entire image regions.

## Executive Summary
This paper addresses the challenge of improving event-based object detection by leveraging knowledge distillation from grayscale images. Event data is inherently sparse and lacks global illumination information, making it difficult to directly transfer knowledge from image-based detectors. The authors propose a novel approach that uses slot attention to iteratively decouple feature maps into object-centric features and corresponding pixel features, allowing the method to focus on foreground objects where distillation works best. The method significantly improves performance on both synthetic (CARLA) and real-world (DSEC) datasets, nearly halving the performance gap with respect to grayscale teacher detectors.

## Method Summary
The proposed method uses slot attention to decouple FPN features into object-centric features and pixel-level features. It performs coarse-level feature alignment followed by attention-aided fine-level alignment, where attention matrices serve as learning-based masks. The approach also includes object-centric relation distillation, transferring structural knowledge between objects. During training, auxiliary tasks (classification and bounding box regression) are used to stabilize the student model. The method is evaluated on YOLOX and Faster R-CNN detectors using event voxel grids as input and aligned grayscale images as teacher modality.

## Key Results
- Improves YOLOX mAP by 2.6 points on CARLA and 3.4 points on DSEC datasets
- Improves Faster R-CNN mAP by 2.2 points on CARLA dataset
- Nearly halves the performance gap between event-based and grayscale detectors
- Shows competitive results for model compression within a single modality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Event-based detection struggles because events are sparse and lack global illumination, making direct feature distillation from image-based detectors ineffective.
- Mechanism: The proposed method focuses distillation on object-centric features rather than the entire image, using slot attention to decouple features into object-centric and pixel-level components.
- Core assumption: Foreground objects contain more discriminative features across modalities than background regions.
- Evidence anchors: [abstract] "However, RGB detectors still outperform event-based detectors due to the sparsity of the event data and missing visual details."

### Mechanism 2
- Claim: Attention matrices from slot attention can serve as learning-based masks for attention-aided feature alignment during distillation.
- Mechanism: The method uses attention maps to mask feature maps, focusing distillation on regions where the student and teacher agree most strongly.
- Core assumption: The attention matrices learned by slot attention capture the most important regions for object detection across modalities.
- Evidence anchors: [section III-D] "During the slot attention module, the attention maps of the teacher and the student attnT and attnS, learned to focus on the important region of the corresponding objects by interacting with the slot features."

### Mechanism 3
- Claim: Object-centric relation distillation improves performance by transferring structural knowledge between objects.
- Mechanism: The method distills the affinity matrix between object-centric features, capturing relationships between objects rather than just individual object features.
- Core assumption: The structural relationships between objects contain valuable information that can be transferred across modalities.
- Evidence anchors: [section III-E] "Besides the direct distillation of object-centric features, which we tackle using the slot features, also the feature relation between the objects contains information and we propose to use this relation to further improve the distillation."

## Foundational Learning

- Concept: Knowledge distillation
  - Why needed here: The paper uses knowledge distillation to transfer information from image-based detectors to event-based detectors.
  - Quick check question: What is the difference between response distillation, feature distillation, and label assignment distillation?

- Concept: Slot attention
  - Why needed here: Slot attention is used to iteratively decouple feature maps into object-centric features and corresponding pixel features for distillation.
  - Quick check question: How does slot attention differ from traditional attention mechanisms like self-attention?

- Concept: Feature Pyramid Networks (FPN)
  - Why needed here: The method distills knowledge from the FPN output features, which allows it to be applicable to both one-stage and two-stage detectors.
  - Quick check question: What is the purpose of FPN in object detection architectures?

## Architecture Onboarding

- Component map: Event voxel grids -> Slot attention module -> Coarse-level alignment -> Attention-aided fine-level alignment -> Object-centric relation distillation -> Auxiliary tasks -> Detection head
- Critical path: Input → Slot attention → Feature distillation → Detection head
- Design tradeoffs: Using slot attention adds computational complexity but allows for more focused distillation; method requires paired event-image data during training
- Failure signatures: Poor performance on real-world datasets compared to synthetic datasets; degradation in performance when using RGB images as teacher modality; sensitivity to alignment errors between event and image data
- First 3 experiments:
  1. Implement the slot attention module and verify it can extract object-centric features from FPN outputs
  2. Test the coarse-level feature alignment and attention-aided fine-level alignment modules separately
  3. Combine all components and test on a small synthetic dataset to verify the overall pipeline works

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed distillation method be extended to work with unlabeled data, alleviating the dependency on paired event-image datasets during training?
- Basis in paper: [explicit] The paper mentions that a limitation is the reliance on paired event and image datasets during training, and suggests that future work could involve extending the method to work with unlabeled data.
- Why unresolved: The paper does not provide details on how to adapt the method for unlabeled data, and this remains an open area for future research.
- What evidence would resolve it: Successful application of the distillation method on unlabeled event data, demonstrating improved object detection performance without the need for paired image data during training.

### Open Question 2
- Question: Can the distillation approach be extended to transfer knowledge from fused event frames and images into images, or from fused frames and images into event frames?
- Basis in paper: [explicit] The paper suggests that as sensor fusion technology evolves, a promising future work is to extend the method to distill knowledge from fused event frames and images into images, or from fused frames and images into event frames.
- Why unresolved: The paper does not provide any experimental results or implementation details for this extension, leaving it as a future research direction.
- What evidence would resolve it: Experimental results showing improved object detection performance when applying the distillation method to fused event-image data, demonstrating the feasibility and benefits of this approach.

## Limitations
- Performance gap between synthetic (CARLA) and real-world (DSEC) datasets raises questions about generalization
- Individual contribution of each component (slot attention, attention-aided alignment, relation distillation) is not clearly isolated in ablation studies
- Does not explore sensitivity to different teacher model architectures or impact of temporal misalignment between event and image data

## Confidence

- **High Confidence**: The basic framework of using slot attention to decouple features into object-centric and pixel components is well-supported by experimental results
- **Medium Confidence**: The effectiveness of attention-aided fine-level alignment and object-centric relation distillation mechanisms is supported by ablation studies, but individual component contributions are not clearly isolated
- **Low Confidence**: The claim that this approach can be directly applied to model compression within a single modality without further validation across different compression scenarios

## Next Checks

1. **Component Isolation Study**: Design experiments to measure the individual contribution of each component (slot attention, coarse-level alignment, attention-aided alignment, relation distillation) by systematically removing each one and measuring the impact on mAP.

2. **Teacher Model Sensitivity**: Test the method with different teacher model architectures (not just grayscale detectors) to determine how sensitive the approach is to teacher model choice and whether RGB images as teacher modality cause the reported degradation.

3. **Real-World Generalization**: Conduct experiments on additional real-world event-based datasets beyond DSEC to validate the method's generalization capabilities and test the hypothesis that the performance gap between synthetic and real data is primarily due to domain shift rather than method limitations.