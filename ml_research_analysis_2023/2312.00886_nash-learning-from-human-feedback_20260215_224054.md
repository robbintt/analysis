---
ver: rpa2
title: Nash Learning from Human Feedback
arxiv_id: '2312.00886'
source_url: https://arxiv.org/abs/2312.00886
tags:
- policy
- preference
- learning
- human
- nash
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Nash learning from human feedback (NLHF),
  a novel approach for aligning large language models (LLMs) with human preferences.
  Unlike existing RLHF methods that optimize against a learned reward model, NLHF
  directly optimizes against a preference model using Nash equilibrium as the solution
  concept.
---

# Nash Learning from Human Feedback

## Quick Facts
- arXiv ID: 2312.00886
- Source URL: https://arxiv.org/abs/2312.00886
- Reference count: 28
- Primary result: NLHF outperforms existing RLHF approaches on text summarization, with Nash-MD-PG achieving best performance at Î² = 0.125-0.375

## Executive Summary
This paper introduces Nash learning from human feedback (NLHF), a novel approach for aligning large language models with human preferences. Unlike existing RLHF methods that optimize against a learned reward model, NLHF directly optimizes against a preference model using Nash equilibrium as the solution concept. The key idea is to treat preference optimization as a two-player constant-sum game where policies compete against each other, and the Nash equilibrium represents a policy that is preferred over any competing policy according to the preference model. Experiments on text summarization show that NLHF outperforms existing RLHF approaches, particularly when using an intermediate mixture parameter.

## Method Summary
The method learns a preference model P(ğ‘¦ â‰» ğ‘¦â€²|ğ‘¥) from human feedback data that directly models the probability that one response is preferred over another. The policy is then optimized by finding the Nash equilibrium of the two-player game where policies compete against each other. The Nash-MD algorithm uses mirror descent to converge to the regularized Nash equilibrium, while Nash-MD-PG and Nash-EMA-PG provide gradient-based implementations for deep learning architectures. The preference model is KL-regularized to ensure existence and uniqueness of the Nash equilibrium.

## Key Results
- NLHF outperforms existing RLHF approaches on TL;DR text summarization dataset
- Nash-MD-PG with intermediate mixture parameter Î² = 0.125-0.375 achieves best performance
- Preference model approach captures richer human preferences than reward models
- Self-play and best-response strategies underperform the optimal mixture strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The preference model captures richer human preferences than reward models by avoiding Bradley-Terry assumptions
- Mechanism: Instead of learning a scalar reward score from pairwise comparisons, the preference model directly models P(ğ‘¦ â‰» ğ‘¦â€²|ğ‘¥) as the probability that one response is preferred over another, without assuming a Bradley-Terry structure
- Core assumption: Human preferences can be more complex than what Bradley-Terry models capture, including non-transitive preferences
- Evidence anchors: [abstract] states preference models remain invariant to the specific policy employed; [section 3.1] provides examples showing preference models can capture non-transitive preferences

### Mechanism 2
- Claim: Nash equilibrium provides better alignment with diverse human preferences than maximum reward optimization
- Mechanism: By finding the Nash equilibrium of the preference model game, the algorithm finds a policy that is preferred over any competing policy, rather than just maximizing expected reward against a fixed distribution
- Core assumption: The diversity of human preferences is better served by a strategy that performs well against any opponent rather than optimizing for a single fixed distribution
- Evidence anchors: [section 3.2] shows maximizing reward would choose deterministic policy while Nash equilibrium chooses mixture; [abstract] states Nash equilibrium represents policy preferred over any competing policy

### Mechanism 3
- Claim: Regularized preference model with KL divergence enables stable convergence to unique Nash equilibrium
- Mechanism: The KL regularization term KL(ğœ‹, ğœ‡) in the preference model creates a strictly convex-concave game structure that guarantees existence and uniqueness of the Nash equilibrium
- Core assumption: Adding KL regularization to the preference model creates the mathematical conditions needed for guaranteed convergence
- Evidence anchors: [section 4] proves existence and uniqueness of Nash equilibrium; [section 6] shows Nash-MD converges with KL-divergence decaying as O(1/T)

## Foundational Learning

- Concept: Two-player constant-sum game theory and Nash equilibrium
  - Why needed here: The core algorithm treats preference optimization as a game where two policies compete, and the solution concept is the Nash equilibrium
  - Quick check question: In a two-player constant-sum game, what property guarantees that the Nash equilibrium can be found using minimax optimization?

- Concept: Mirror descent and online convex optimization
  - Why needed here: Nash-MD is a variant of mirror descent that converges to the Nash equilibrium by optimizing against a geometric mixture of policies
  - Quick check question: How does mirror descent differ from standard gradient descent when optimizing over probability distributions?

- Concept: Policy gradient methods and KL regularization
  - Why needed here: The algorithm uses policy gradients to optimize the preference model while maintaining KL regularization to ensure stability
  - Quick check question: What role does the KL regularization term play in preventing policy updates from becoming too large or unstable?

## Architecture Onboarding

- Component map:
  Preference Model -> Policy Network -> Alternative Policy Generator -> Gradient Calculator -> Update Rule

- Critical path:
  1. Sample prompt ğ‘¥ from distribution ğœŒ
  2. Generate response ğ‘¦ from current policy ğœ‹ğœƒ
  3. Generate response ğ‘¦â€² from alternative policy ğœ‹â€²
  4. Query preference model to get P(ğ‘¦ â‰» ğ‘¦â€²|ğ‘¥)
  5. Compute policy gradient and update ğœƒ
  6. Repeat until convergence

- Design tradeoffs:
  - Tabular vs parametric policies: Tabular provides theoretical guarantees but doesn't scale; parametric (LLM) scales but requires approximations
  - Geometric mixture vs EMA for alternative policy: Geometric mixture has stronger theoretical guarantees; EMA is computationally simpler
  - Regularization strength ğœ: Higher ğœ provides more stability but may reduce flexibility; lower ğœ allows more adaptation but may be less stable

- Failure signatures:
  - Policy collapse: All generated responses become very similar (likely ğœ too low)
  - No improvement: KL divergence between policies remains high (likely ğœ too high)
  - Oscillations: Policy parameters fluctuate without convergence (learning rate or mixture parameter issues)
  - Preference model bias: Generated responses don't match human preferences (preference model training issues)

- First 3 experiments:
  1. Baseline comparison: Run RLHF baseline vs NLHF with self-play (ğ›½=0) on small dataset
  2. Mixture parameter sweep: Test different ğ›½ values (0.125, 0.25, 0.375, 0.5) to find optimal tradeoff
  3. Regularization sensitivity: Test different ğœ values to find stability-performance balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical gap between the one-step-at-a-time regularized policy used in practice and the true regularized policy defined in Equation (3)? How significant is this discrepancy in terms of convergence guarantees?
- Basis in paper: The paper acknowledges that the one-step-at-a-time approach differs from the true regularized policy and states that analyzing this difference is left for future work.
- Why unresolved: The paper does not provide theoretical analysis of the difference between the two policies or empirical evidence quantifying the impact of this approximation.
- What evidence would resolve it: Theoretical analysis comparing the convergence properties of the one-step-at-a-time approach versus the true regularized policy, or empirical studies measuring the performance difference between the two approaches.

### Open Question 2
- Question: How does the choice of mixing parameter Î² in Nash-MD-PG affect the quality of the learned policy across different tasks and domains? Is there a systematic way to select the optimal Î² value?
- Basis in paper: The paper shows that intermediate values of Î² (0.125-0.375) perform better than extreme values (0-1), but does not provide a systematic method for selecting Î².
- Why unresolved: The paper only provides empirical evidence for one specific task (text summarization) and does not explore how Î² selection might vary across different domains or tasks.
- What evidence would resolve it: Extensive experiments across multiple tasks and domains showing how Î² selection affects performance, and potentially a theoretical framework for choosing optimal Î² values based on task characteristics.

### Open Question 3
- Question: How does NLHF compare to RLHF when human preferences are not perfectly transitive or when the Bradley-Terry model assumption is violated?
- Basis in paper: The paper argues that preference models can capture non-transitive preferences while reward models cannot, and shows that NLHF can outperform RLHF even when the Bradley-Terry model holds.
- Why unresolved: The paper does not provide direct experimental comparisons showing NLHF's advantages when preferences are non-transitive or when the Bradley-Terry model assumption is violated.
- What evidence would resolve it: Experiments comparing NLHF and RLHF performance on tasks with known non-transitive preferences or where the Bradley-Terry model is known to be violated, measuring both final policy quality and robustness to preference distribution changes.

## Limitations
- Limited to single summarization task (TL;DR dataset), restricting generalizability
- Exact prompt engineering details for PaLM 2 Large evaluation not specified
- Computational efficiency comparisons with existing methods not provided
- Optimal mixture parameter Î² = 0.125-0.375 lacks theoretical justification

## Confidence
- High confidence: The mathematical formulation of the preference model game and Nash equilibrium existence/uniqueness proofs
- Medium confidence: The empirical superiority of NLHF over RLHF on the TL;DR dataset
- Low confidence: Generalizability to other tasks and languages, and scalability to larger models

## Next Checks
1. **Cross-task generalization test**: Evaluate NLHF on at least two additional text generation tasks (e.g., dialogue generation, question answering) using the same experimental protocol to verify if the Î² = 0.125-0.375 range remains optimal across different domains.

2. **Preference model ablation study**: Systematically remove the KL regularization term and test different preference model architectures (e.g., Bradley-Terry vs direct probability estimation) to quantify the contribution of each component to the final performance.

3. **Computational efficiency benchmarking**: Measure wall-clock training time and inference latency for NLHF compared to standard RLHF across different model sizes, and analyze the trade-off between performance gains and computational overhead.