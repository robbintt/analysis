---
ver: rpa2
title: Unveiling The Factors of Aesthetic Preferences with Explainable AI
arxiv_id: '2311.14410'
source_url: https://arxiv.org/abs/2311.14410
tags:
- aesthetic
- dataset
- learning
- shap
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study pioneers a novel approach to understanding image aesthetic\
  \ preferences using machine learning models trained on aesthetic attribute scores\
  \ rather than raw image data. The authors employ four regression models\u2014Random\
  \ Forest, XGBoost, SVR, and MLP\u2014and use SHAP (SHapley Additive exPlanations)\
  \ to interpret model predictions."
---

# Unveiling The Factors of Aesthetic Preferences with Explainable AI

## Quick Facts
- arXiv ID: 2311.14410
- Source URL: https://arxiv.org/abs/2311.14410
- Authors: 
- Reference count: 40
- Key outcome: This study pioneers a novel approach to understanding image aesthetic preferences using machine learning models trained on aesthetic attribute scores rather than raw image data.

## Executive Summary
This study pioneers a novel approach to understanding image aesthetic preferences using machine learning models trained on aesthetic attribute scores rather than raw image data. The authors employ four regression models—Random Forest, XGBoost, SVR, and MLP—and use SHAP (SHapley Additive exPlanations) to interpret model predictions. SHAP values quantify the contribution of each attribute to the overall aesthetic score. Experiments on three image aesthetic benchmarks (AADB, EVA, PARA) reveal that high-level attributes like content and semantics consistently rank as the most influential predictors across datasets. Low-level attributes such as symmetry or repetition show minimal impact. Interaction plots further reveal how pairs of attributes jointly influence aesthetic predictions, often amplifying effects when values are high. The results highlight the potential of interpretable machine learning to reveal underlying factors shaping aesthetic judgments, offering insights for both computational aesthetics research and model transparency.

## Method Summary
The authors employ four regression models—Random Forest, XGBoost, SVR, and MLP—to predict aesthetic scores from image attribute inputs. They then use SHAP (SHapley Additive exPlanations) to interpret the model predictions by quantifying the contribution of each attribute to the overall aesthetic score. The method processes aesthetic attributes as inputs to predict aesthetic scores, then analyzes which attributes most influence these predictions across three image aesthetic benchmarks.

## Key Results
- High-level attributes like content and semantics consistently rank as the most influential predictors across all three datasets (AADB, EVA, PARA)
- Low-level attributes such as symmetry or repetition show minimal impact on aesthetic predictions
- Interaction plots reveal that pairs of attributes jointly influence aesthetic predictions, often amplifying effects when values are high
- Multiple regression models consistently identify the same key attributes, strengthening confidence in the findings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model's ability to predict aesthetic scores from attribute inputs works because it transforms subjective human judgments into a structured, continuous output space that machine learning can optimize.
- Mechanism: The regression models (Random Forest, XGBoost, SVR, MLP) learn the mapping from 11 attributes (AADB), 4 attributes (EVA), or 7 attributes (PARA) to an overall aesthetic score, effectively capturing the non-linear relationships between these attributes and perceived beauty.
- Core assumption: Aesthetic preferences can be represented as a function of measurable attribute scores, even though these attributes are themselves subjective ratings.
- Evidence anchors:
  - [abstract] "Our models process these attributes as inputs to predict the aesthetic scores of images."
  - [section 2.1] The paper describes how each regression model (Random Forest, XGBoost, SVR, MLP) is used to predict aesthetic scores from attribute inputs.
- Break condition: If the underlying attribute scores are too subjective or inconsistent across raters, the regression models cannot learn a reliable mapping, leading to poor predictive performance.

### Mechanism 2
- Claim: SHAP values provide reliable explanations for the model's predictions because they quantify each attribute's marginal contribution to the predicted aesthetic score in a theoretically grounded way.
- Mechanism: SHAP computes Shapley values from cooperative game theory to assign a contribution score to each attribute for every prediction, ensuring that the sum of contributions equals the model's prediction.
- Core assumption: The SHAP framework's properties (local accuracy, missingness, consistency) hold for the regression models used, making the explanations meaningful and comparable across different models.
- Evidence anchors:
  - [abstract] "we utilize the popular Explainable AI (XAI) technique known as SHapley Additive exPlanations (SHAP)."
  - [section 2.2] "SHAP values, the key component of this technique, prove to be more consistent with human intuition than other techniques."
- Break condition: If the model's predictions are highly unstable or if the feature interactions are too complex, SHAP's approximations may not accurately reflect true contributions.

### Mechanism 3
- Claim: Comparing multiple models (Random Forest, XGBoost, SVR, MLP) increases confidence in the interpretability results because consistent attribute rankings across models suggest the findings are not model-specific artifacts.
- Mechanism: Each model has different inductive biases (ensemble methods, kernel-based, neural network), but if they all identify the same attributes as most important, it strengthens the claim that these attributes genuinely drive aesthetic preferences.
- Core assumption: The models are sufficiently different in their architectures and learning mechanisms that consistent results across them are unlikely to be coincidental.
- Evidence anchors:
  - [abstract] "By employing multiple models and consistently observing results in conjunction with SHAP, we establish a reliable interpretation of the effects of image aesthetic attributes."
  - [section 4] Describes the implementation of four distinct regression models with different hyperparameter settings.
- Break condition: If all models share a similar bias or limitation, they might consistently miss important attributes or overemphasize others, leading to false confidence in the results.

## Foundational Learning

- Concept: Regression vs Classification
  - Why needed here: The task is predicting continuous aesthetic scores, not discrete categories, so regression models are appropriate.
  - Quick check question: If the output were "aesthetic" vs "not aesthetic" instead of a score from 1-5, would regression still be the right choice?

- Concept: SHAP (Shapley Additive Explanations)
  - Why needed here: SHAP provides interpretable explanations for how each attribute contributes to the model's prediction, addressing the "black box" nature of machine learning.
  - Quick check question: How does SHAP ensure that the sum of all feature contributions equals the model's prediction?

- Concept: Ensemble Learning
  - Why needed here: Random Forest and XGBoost combine multiple weak learners to create a stronger predictor, improving accuracy and robustness for aesthetic score prediction.
  - Quick check question: What is the key difference between bagging (Random Forest) and boosting (XGBoost) in how they combine individual models?

## Architecture Onboarding

- Component map: Data → Preprocessing → Four Regression Models (Random Forest, XGBoost, SVR, MLP) → Prediction → SHAP Explainer → Attribute Importance/Interaction Plots
- Critical path: Raw attribute scores → Model training → SHAP computation → Interpretation of results
- Design tradeoffs: Using multiple models increases confidence but also computational cost; SHAP provides interpretability but is computationally expensive, especially for SVR
- Failure signatures: Inconsistent attribute rankings across models; poor predictive performance (low R², high error); SHAP computation taking excessively long
- First 3 experiments:
  1. Train all four models on AADB dataset and compare R² and error metrics to identify the best performer
  2. Apply SHAP to the best-performing model on AADB and visualize the summary plot to see which attributes are most important
  3. Compare SHAP results across all four models to check for consistency in attribute rankings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the attribute importance rankings change when models are trained on datasets with different cultural or demographic compositions?
- Basis in paper: [inferred] The authors acknowledge that aesthetic preferences are subjective and that dataset quality and diversity impact model performance and interpretability.
- Why unresolved: The study uses three existing datasets without analyzing how attribute importance might vary across different cultural or demographic contexts.
- What evidence would resolve it: Training and comparing models on datasets from diverse cultural backgrounds and analyzing how SHAP rankings of attributes differ across these datasets.

### Open Question 2
- Question: To what extent do interaction effects between aesthetic attributes vary across different image categories (e.g., landscapes vs. portraits)?
- Basis in paper: [explicit] The authors observe interaction effects between attributes but do not analyze whether these interactions differ by image category.
- Why unresolved: The current analysis treats all images uniformly without examining whether the nature of attribute interactions depends on image content type.
- What evidence would resolve it: Analyzing SHAP interaction plots separately for different image categories to identify category-specific interaction patterns.

### Open Question 3
- Question: Can the SHAP-based interpretability approach be extended to identify causal relationships between aesthetic attributes and overall aesthetic scores?
- Basis in paper: [inferred] The authors use SHAP to identify contributions of attributes to predictions but acknowledge this shows correlation rather than causation.
- Why unresolved: SHAP values indicate predictive importance but do not establish whether manipulating an attribute would causally affect aesthetic judgments.
- What evidence would resolve it: Experimental manipulation studies where specific aesthetic attributes are systematically altered to observe corresponding changes in aesthetic ratings, compared with SHAP-based predictions.

## Limitations
- The findings remain correlational rather than causal—the models identify associations between attributes and scores without establishing that manipulating these attributes would change aesthetic judgments
- Model interpretability through SHAP assumes the explanations are faithful to the model's decision-making process, though complex feature interactions may limit the accuracy of these attributions
- The study's claims about identifying key aesthetic attributes rely on subjective human ratings that may vary across raters and cultures

## Confidence
- Model performance and attribute rankings: High
- Cross-dataset consistency: Medium-High
- Causal interpretation of findings: Low-Medium
- SHAP explanation reliability: Medium

## Next Checks
1. Conduct ablation studies removing top-ranked attributes to verify their impact on model performance
2. Test model robustness by training on subsets of each dataset to assess consistency across different data samples
3. Compare SHAP attributions with alternative explanation methods (e.g., LIME) to validate the consistency of attribute importance rankings