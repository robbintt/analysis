---
ver: rpa2
title: Rethinking and Improving Multi-task Learning for End-to-end Speech Translation
arxiv_id: '2311.03810'
source_url: https://arxiv.org/abs/2311.03810
tags:
- task
- training
- speech
- tasks
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates task consistency in multi-task learning
  for end-to-end speech translation. The authors find that auxiliary tasks like ASR
  and MT have inconsistent gradients with the main ST task, particularly in the textual
  encoder.
---

# Rethinking and Improving Multi-task Learning for End-to-end Speech Translation

## Quick Facts
- arXiv ID: 2311.03810
- Source URL: https://arxiv.org/abs/2311.03810
- Authors: 
- Reference count: 13
- Key outcome: This paper investigates task consistency in multi-task learning for end-to-end speech translation. The authors find that auxiliary tasks like ASR and MT have inconsistent gradients with the main ST task, particularly in the textual encoder. To address this, they propose the Looking-Back Mechanism (LBM) to reduce speech length while preserving information, Local-to-Global (L2G) training with noise injection to bridge modality gaps, and task-norm-based weight decay to improve training efficiency. Experiments on MuST-C show their method achieves state-of-the-art performance, and with additional data, they achieve new SOTA on the En-Es task with 20.8% of the training time of previous methods.

## Executive Summary
This paper investigates gradient consistency issues in multi-task learning for end-to-end speech translation. The authors find that while ASR auxiliary tasks help the acoustic encoder, they harm the textual encoder's consistency with the main ST task due to cross-modal conversion challenges. To address this, they propose three key mechanisms: Looking-Back Mechanism (LBM) for stable sequence length reduction, Local-to-Global (L2G) training with noise injection to bridge modality gaps, and task-norm-based weight decay to improve training efficiency. Their method achieves state-of-the-art results on MuST-C and sets new records on En-Es with significantly reduced training time.

## Method Summary
The authors propose an improved multi-task learning framework for end-to-end speech translation that addresses gradient inconsistency between auxiliary tasks (ASR/MT) and the main ST task. The approach uses dynamic task weighting based on gradient consistency metrics, a Looking-Back Mechanism for stable sequence length reduction while preserving information, Local-to-Global training with noise injection to bridge modality gaps, and task-norm-based weight decay to improve training efficiency. The model is trained on MuST-C with additional MT data from WMT and LibriSpeech ASR data, using HuBERT-initialized features and SentencePiece tokenization.

## Key Results
- Achieves state-of-the-art performance on MuST-C across 8 language pairs
- New SOTA on En-Es task with 20.8% of training time compared to previous methods
- Demonstrates improved gradient consistency between auxiliary and main tasks
- Shows stable sequence length reduction without performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASR gradients help the acoustic encoder (A-Enc) but hurt the textual encoder (T-Enc) consistency
- Mechanism: ASR task gradients align with ST in A-Enc for speech modeling, but diverge in T-Enc where cross-modal conversion is needed
- Core assumption: The T-Enc serves a unique cross-modal role distinct from either ASR or MT
- Evidence anchors:
  - [abstract] "We find that the textual encoder primarily facilitates cross-modal conversion, but the presence of noise in speech impedes the consistency between text and speech representations."
  - [section 2.1] "Although the ASR task shares all the parameters with the ST task, only the A-Enc exhibits high consistency with the ST task."
  - [corpus] Weak - neighboring papers focus on joint training but don't explicitly analyze gradient consistency across modules
- Break condition: If T-Enc can be trained to share representations with ASR without hurting ST performance

### Mechanism 2
- Claim: Length inconsistency between speech frames and text tokens creates representation misalignment
- Mechanism: Speech is modeled at frame level while text uses subword tokens, creating different sequence lengths that hurt cross-attention alignment
- Core assumption: Sequence length mismatch is a fundamental barrier to effective cross-modal representation learning
- Evidence anchors:
  - [abstract] "we propose an improved multi-task learning (IMTL) approach for the ST task, which bridges the modal gap by mitigating the difference in length and representation."
  - [section 2.3] "The length disparity arises from modeling granularity (frames for speech while sub-words for text)"
  - [corpus] Weak - length issues mentioned but not deeply analyzed in neighboring work
- Break condition: If a model can learn cross-modal alignment without addressing length mismatch

### Mechanism 3
- Claim: Noisy speech representations impede consistency with clean text embeddings
- Mechanism: Acoustic encoder produces noisy representations while text embeddings are clean, creating an information gap that gradient consistency metrics reveal
- Core assumption: The noise in speech features is the primary driver of representation inconsistency
- Evidence anchors:
  - [abstract] "the presence of noise in speech impedes the consistency between text and speech representations"
  - [section 2.3] "The representation space discrepancy is due to acoustic features extracted by the acoustic encoder lacking text-based information"
  - [corpus] Weak - neighboring papers acknowledge modality gaps but don't quantify noise effects
- Break condition: If clean speech representations can be generated without the proposed noise injection

## Foundational Learning

- Concept: Gradient consistency metrics using cosine similarity
  - Why needed here: To quantify how auxiliary tasks align with the main ST task across different modules and layers
  - Quick check question: How does cosine similarity between flattened gradient vectors indicate task consistency?

- Concept: Multi-task learning optimization trade-offs
  - Why needed here: To understand why auxiliary tasks might help some modules while hurting others
  - Quick check question: What happens to task weights when gradient directions conflict between tasks?

- Concept: Contrastive learning for cross-modal alignment
  - Why needed here: To bridge the representation gap between speech and text modalities
  - Quick check question: How does contrastive loss encourage similar representations for aligned speech-text pairs?

## Architecture Onboarding

- Component map: Acoustic encoder (HuBERT-initialized) → Textual encoder → Decoder
- Critical path: Speech → A-Enc → T-Enc → Decoder → Translation
  - T-Enc is the critical bottleneck for cross-modal consistency
- Design tradeoffs:
  - LBM vs direct shrinking: LBM preserves information but adds complexity
  - L2G noise injection vs clean text: Improves consistency but adds noise
  - Task weighting vs fixed weights: More efficient but requires monitoring
- Failure signatures:
  - Training instability: Often indicates LBM implementation issues
  - Inconsistent BLEU scores: May indicate task weight decay timing problems
  - Memory issues: Can occur with L2G when window sizes are too large
- First 3 experiments:
  1. Measure gradient consistency with and without ASR task on A-Enc vs T-Enc
  2. Compare shrinking vs LBM on sequence length reduction vs BLEU preservation
  3. Test L2G with fixed vs increasing window sizes on representation entropy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Looking-Back Mechanism (LBM) specifically handle the integration of blank tokens in the compressed speech sequence, and what is the impact of this on the overall model performance?
- Basis in paper: [explicit] The paper mentions that the LBM method includes blank positions to prevent error propagation and uses a fusion module to integrate the original and retrieved information.
- Why unresolved: The paper does not provide detailed information on how blank tokens are specifically handled or the impact of this handling on model performance.
- What evidence would resolve it: Detailed analysis of the LBM's handling of blank tokens and its impact on model performance metrics such as BLEU scores or training efficiency.

### Open Question 2
- Question: What is the optimal window size and stride for the Local-to-Global (L2G) extractor, and how do these parameters affect the model's ability to bridge the modality gap?
- Basis in paper: [explicit] The paper mentions that the kernel size c and the increased stride d for the L2G extractor were set to 5 and 3, respectively, but does not explore the impact of different values.
- Why unresolved: The paper does not provide an analysis of how varying the window size and stride affects the model's performance.
- What evidence would resolve it: Experimental results showing the performance of the model with different window sizes and strides for the L2G extractor.

### Open Question 3
- Question: How does the task-norm-based weight decay method affect the training efficiency and final performance of the model compared to traditional fine-tuning methods?
- Basis in paper: [explicit] The paper introduces a task-norm-based weight decay method to improve training efficiency and avoid unnecessary training costs.
- Why unresolved: The paper does not provide a detailed comparison of the task-norm-based weight decay method with traditional fine-tuning methods in terms of training efficiency and final performance.
- What evidence would resolve it: Comparative analysis of training efficiency and final performance metrics between the task-norm-based weight decay method and traditional fine-tuning methods.

## Limitations
- No ablation studies to isolate contribution of each proposed component
- Sparse implementation details that make exact reproduction challenging
- Limited analysis of how techniques perform on languages beyond the 8 tested
- No examination of how these techniques interact with different speech feature extractors

## Confidence
- High confidence: Gradient consistency findings (ASR helps A-Enc but hurts T-Enc) - well-supported by empirical measurements and logical explanation
- Medium confidence: LBM effectiveness - supported by results but lacks ablation studies and implementation details are sparse
- Medium confidence: L2G mechanism benefits - results show improvements but component contributions are unclear
- Low confidence: Task-norm weight decay efficiency claims - lacks comparative baseline data

## Next Checks
1. **Ablation Study of LBM Components**: Implement sequence length reduction without LBM to verify the claimed instability, then test different LBM variants to isolate which components (search matrix, feature fusion) are essential versus nice-to-have.

2. **Cross-Modal Consistency Measurement**: Conduct a controlled experiment measuring gradient consistency across all three tasks (ASR, MT, ST) in both the acoustic and textual encoders, comparing clean vs noisy speech representations to quantify the noise impact claim.

3. **Training Efficiency Benchmarking**: Run parallel training experiments comparing task-norm-based weight decay against standard weight decay and other efficiency techniques, measuring not just BLEU scores but actual wall-clock training time to completion.