---
ver: rpa2
title: Exchanging-based Multimodal Fusion with Transformer
arxiv_id: '2309.02190'
source_url: https://arxiv.org/abs/2309.02190
tags:
- multimodal
- fusion
- muse
- which
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multimodal fusion by proposing a novel exchanging-based
  model MuSE for text-vision fusion using Transformer. The key idea is to use separate
  encoders for text and image, followed by decoders implementing image captioning
  and text-to-image generation tasks to regularize and align embeddings.
---

# Exchanging-based Multimodal Fusion with Transformer

## Quick Facts
- arXiv ID: 2309.02190
- Source URL: https://arxiv.org/abs/2309.02190
- Reference count: 21
- F1 scores of 76.81 on Twitter15 and 88.62 on Twitter17 for MNER, accuracy of 75.80 on MVSA-Single for MSA

## Executive Summary
This paper introduces MuSE, an exchanging-based model for text-vision fusion using Transformer architecture. The model employs separate encoders for text and image modalities, followed by decoders implementing image captioning and text-to-image generation tasks to regularize and align embeddings. The CrossTransformer module then performs knowledge exchange between modalities. Experiments on MNER and MSA tasks show MuSE outperforms state-of-the-art methods with significant performance improvements.

## Method Summary
MuSE uses BERT-based text encoding and ResNet-based image encoding, followed by embedding regularization through two generative decoders: PixelCNN++ for image captioning and NIC-Att for text-to-image generation. The CrossTransformer module performs multimodal exchange by selecting tokens with smallest attention scores to `cls` and replacing them with average embeddings from the other modality. The model is trained with CrossEntropy loss for MNER and MSA prediction, and discretized mix logistic loss for text-to-image generation.

## Key Results
- MNER task: F1 score of 76.81 on Twitter15 and 88.62 on Twitter17
- MSA task: Accuracy of 75.80 on MVSA-Single
- Outperforms state-of-the-art methods on both MNER and MSA tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CrossTransformer performs effective knowledge exchange between text and image modalities by selecting tokens with smallest attention scores to `cls` and replacing them with the average embedding of the other modality.
- Mechanism: The method first learns global contextual information in shallow layers, then performs inter-modal exchange by selecting a proportion of tokens in one modality and replacing their embeddings with the average of embeddings in the other modality. This exchange occurs between layers μ and η, allowing controlled fusion of multimodal information.
- Core assumption: The `cls` token effectively represents sentence-level semantics and can serve as a reliable reference for selecting tokens to exchange.
- Evidence anchors:
  - [abstract] "it performs inter-modal exchange by selecting a proportion of tokens in one modality and replacing their embeddings with the average of embeddings in the other modality"
  - [section] "We first add cls to the beginning of the embeddings generated by the text encoder and the image encoder, which are taken as the input of the CrossTransformer"
  - [corpus] Weak evidence - no direct mention of this specific exchange mechanism in related papers
- Break condition: If the attention scores to `cls` do not reliably indicate token importance, or if the average embedding replacement destroys critical modality-specific information.

### Mechanism 2
- Claim: The image captioning and text-to-image generation tasks effectively regularize embeddings and pull them into the same space.
- Mechanism: Two decoders implement generative tasks that capture correlations between text and images. The image captioning task takes image embeddings as input and generates captions, while the text-to-image generation task does the opposite. These tasks create a shared embedding space through regularization losses.
- Core assumption: Texts can be captions of images and images can be generated from texts, establishing a meaningful correlation between modalities.
- Evidence anchors:
  - [abstract] "we employ two decoders to regularize the embeddings and pull them into the same space. The two decoders capture the correlations between texts and images with the image captioning task and the text-to-image generation task"
  - [section] "inspired by the fact that texts can be captions of images and images can also be generated from texts, we specially design an image captioning task and a text-to-image generation task"
  - [corpus] Weak evidence - while related to multimodal fusion, the specific combination of captioning and generation tasks is not directly supported
- Break condition: If the generative tasks fail to establish meaningful correlations between modalities, or if the regularization pushes embeddings into an unnatural shared space.

### Mechanism 3
- Claim: Separate low-dimensional projections followed by embedding regularization enables effective multimodal fusion where modalities are distant from each other.
- Mechanism: Text and image encoders map inputs into separate low-dimensional spaces, then two generative decoders (regularizers) pull these embeddings into the same space through cross-modal generative tasks. This approach handles the challenge that text and image modalities are distant and correspond to different spaces.
- Core assumption: The generative decoders can effectively bridge the gap between different modality spaces through the designed tasks.
- Evidence anchors:
  - [abstract] "We first use two encoders to separately map multimodal inputs into different low-dimensional spaces. Then we employ two decoders to regularize the embeddings and pull them into the same space"
  - [section] "Since the inputs are in two modalities, they are generally projected into different spaces by Equation 2. Therefore, for information exchange from multimodalities, we first need to pull these embeddings into the same space"
  - [corpus] Weak evidence - the specific approach of separate projections with generative regularization is novel
- Break condition: If the embedding regularization fails to create a meaningful shared space, or if the separate projections create irreconcilable gaps between modalities.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The CrossTransformer relies on multi-head self-attention to compute attention scores and perform token selection for exchange. Understanding how attention works is crucial for implementing and debugging the exchange mechanism.
  - Quick check question: How does the scaling factor √dk in the attention computation affect the attention distribution?

- Concept: Multimodal representation learning
  - Why needed here: The paper addresses the challenge of fusing text and image modalities that exist in different embedding spaces. Understanding multimodal representation learning principles is essential for grasping why separate projections and regularization are necessary.
  - Quick check question: Why might text and image modalities naturally correspond to different embedding spaces?

- Concept: Generative modeling and cross-modal tasks
  - Why needed here: The embedding regularization relies on image captioning and text-to-image generation tasks. Understanding these generative tasks and their role in learning cross-modal correlations is crucial for implementing the regularizers.
  - Quick check question: How do generative tasks like image captioning help establish correlations between different modalities?

## Architecture Onboarding

- Component map: Text Encoder (BERT-based) → Text embeddings → Text Decoder (NIC-Att) → Text-to-image generation
  Image Encoder (ResNet-based) → Image embeddings → Image Decoder (PixelCNN++) → Image captioning
  CrossTransformer → Multimodal exchange and fusion → CRF layer → MNER task prediction
  MLP layer → MSA task prediction

- Critical path: Text/Image Encoder → Embedding Regularization (via Decoders) → CrossTransformer → Task-specific prediction layer
- Design tradeoffs:
  - Separate encoders allow modality-specific feature extraction but require regularization to align spaces
  - Generative regularizers add computational cost but improve cross-modal understanding
  - Exchange proportion θ balances information fusion with preserving modality-specific knowledge
  - Layer boundaries μ and η control when contextual learning stops and exchange begins

- Failure signatures:
  - Performance degrades when θ is too high (over-exchange) or too low (under-exchange)
  - Training instability when regularization losses α and β are misbalanced
  - Poor results when μ is too small (cls hasn't learned meaningful representations) or too large (missed exchange opportunity)

- First 3 experiments:
  1. Train with μ=1, η=4, θ=0.1 to verify basic exchange functionality
  2. Test ablation without regularization tasks to measure their impact
  3. Vary θ from 0.05 to 0.25 to find optimal exchange proportion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MuSE vary with different values of the exchange proportion θ and what is the optimal range?
- Basis in paper: [explicit] The paper conducts a sensitivity analysis on the exchange proportion θ, showing that the model performance first increases and then drops as θ increases. The optimal range is suggested to be about 10% to 20%.
- Why unresolved: The exact optimal value of θ is not determined, and it might vary depending on the specific task or dataset.
- What evidence would resolve it: Additional experiments varying θ across different tasks and datasets to find the optimal value for each scenario.

### Open Question 2
- Question: How does the start layer µ for multimodal exchanging affect the performance of MuSE?
- Basis in paper: [explicit] The paper shows that the model performance improves quickly when the start layer µ increases from 1 to 2, suggesting the importance of using regular Transformer layers without exchanging in the shallow layers of CrossTransformer.
- Why unresolved: The exact impact of varying µ on performance is not fully explored, and it might depend on the specific characteristics of the input data.
- What evidence would resolve it: Further experiments varying µ across different datasets and analyzing the performance changes to determine the optimal start layer.

### Open Question 3
- Question: How does the end layer η for multimodal exchanging influence the performance of MuSE?
- Basis in paper: [explicit] The paper indicates that the model performance first rises and then degenerates as η increases, suggesting that more exchanging layers could degrade the original intra-modal knowledge after the fusion completes.
- Why unresolved: The exact impact of varying η on performance is not fully explored, and it might depend on the specific characteristics of the input data.
- What evidence would resolve it: Additional experiments varying η across different datasets and analyzing the performance changes to determine the optimal end layer.

### Open Question 4
- Question: How does the efficiency of MuSE compare to other cross-attention-based text-vision fusion methods in terms of training and evaluation time?
- Basis in paper: [explicit] The paper compares MuSE to several representative methods in terms of the number of parameters, performance, training time, and evaluation time, showing that MuSE significantly improves performance but also comes with higher time costs.
- Why unresolved: The specific reasons for the increased time costs and potential optimizations are not explored.
- What evidence would resolve it: Further analysis of the computational bottlenecks in MuSE and exploration of optimization techniques to reduce the time costs while maintaining performance.

## Limitations
- The token exchange mechanism using attention scores to `cls` lacks comprehensive validation and ablation studies
- The specific implementation details of the CrossTransformer and decoder components are not fully specified
- The increased computational costs compared to baseline methods may limit practical deployment

## Confidence
- Exchange Mechanism Effectiveness: Low - The token exchange process is innovative but lacks comprehensive ablation studies to validate the specific design choices
- Regularization Through Generative Tasks: Medium - While the concept is sound, the specific implementation details and hyper-parameter settings are not fully specified
- Overall Performance Gains: High - The experimental results are clearly presented and demonstrate significant improvements over state-of-the-art methods

## Next Checks
1. Conduct ablation study on exchange parameters (θ, μ, η) to determine optimal values and validate their importance
2. Implement and test alternative token selection mechanisms for the exchange process
3. Perform detailed analysis of learned embeddings to verify meaningful cross-modal correlations are created by the generative tasks