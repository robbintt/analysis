---
ver: rpa2
title: A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling
arxiv_id: '2310.03298'
source_url: https://arxiv.org/abs/2310.03298
tags:
- fidelity
- sampling
- latent
- multi-fidelity
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops a unified adaptive sampling framework (MuFASa)
  for multi-fidelity (MF) global fitting (GF) and Bayesian optimization (BO) problems.
  The framework leverages a Latent Variable Gaussian Process (LVGP) to map MF models
  into an interpretable latent space, capturing their correlations without assuming
  hierarchical fidelity levels.
---

# A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling

## Quick Facts
- arXiv ID: 2310.03298
- Source URL: https://arxiv.org/abs/2310.03298
- Reference count: 0
- Primary result: Unified multi-fidelity adaptive sampling framework using latent variable Gaussian processes for both global fitting and Bayesian optimization

## Executive Summary
This paper introduces MuFASa, a unified framework for multi-fidelity adaptive sampling that leverages Latent Variable Gaussian Processes (LVGP) to capture correlations between fidelity models without assuming hierarchical relationships. The method employs pre-posterior analysis to quantify the benefit of low-fidelity samples on high-fidelity predictions, enabling cost-effective sampling strategies. MuFASa outperforms traditional single-fidelity approaches in both global fitting and Bayesian optimization tasks, achieving better convergence rates and robustness.

## Method Summary
The method uses a Latent Variable Gaussian Process to map multiple fidelity models into a shared latent space, capturing their correlations without hierarchical assumptions. Pre-posterior analysis quantifies how low-fidelity samples affect high-fidelity predictions, and a two-stage optimization selects samples based on benefit-to-cost ratios. The framework alternates between identifying the most informative location on the high-fidelity model and selecting the optimal fidelity source to sample next, balancing exploration and exploitation across fidelity levels.

## Key Results
- MuFASa achieves RRMSE of 0.01 with cost 4000 on Borehole GF problem vs. 0.05 for single-fidelity GP
- Outperforms benchmark methods in both global fitting and Bayesian optimization convergence rates
- Demonstrates robustness across multiple test problems including Borehole, Wing Weight, Simple-1D, and Sasena

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LVGP captures correlation structure between fidelity models without hierarchical ordering
- Mechanism: Fidelity levels embedded as qualitative variables mapped into 2D latent space, learning continuous surface encoding correlations
- Core assumption: Correlation structure can be represented in low-dimensional latent space
- Evidence anchors: Abstract states LVGP maps fidelities to interpretable latent space; Section 2.2 describes embedding fidelity levels as qualitative variables

### Mechanism 2
- Claim: Pre-posterior analysis quantifies benefit of future LF samples on HF predictions
- Mechanism: Approximates how HF prediction uncertainty would change if LF sample were added, enabling selection of LF samples that maximally reduce HF uncertainty per unit cost
- Core assumption: Predicted LF response accurately approximates true LF response; latent variables remain constant during pre-posterior update
- Evidence anchors: Abstract mentions assessing LF sampling impact with pre-posterior analysis; Section 2.3 describes using predictive mean of response as approximation

### Mechanism 3
- Claim: Two-stage optimization balances exploration of HF space with exploitation of LF sources
- Mechanism: Stage 1 identifies HF location of interest; Stage 2 searches all fidelity levels for sample maximizing HF improvement per unit cost
- Core assumption: HF model location of interest is most critical point for improvement; benefit-to-cost ratio is appropriate selection metric
- Evidence anchors: Section 3.2 describes first-stage optimization using HF surrogate model; Section 3.3 explains second-stage optimization for benefit-to-cost ratio

## Foundational Learning

- Concept: Gaussian Process regression with latent variables
  - Why needed here: LVGP extends standard GP to handle qualitative variables by mapping them to latent space, enabling fusion of multiple fidelity models with different response surfaces
  - Quick check question: How does LVGP handle correlation between points with different fidelity levels?

- Concept: Pre-posterior analysis in GP regression
  - Why needed here: Allows prediction of how posterior GP would change if hypothetical sample were added, enabling evaluation of future sample benefits before they are taken
  - Quick check question: What are mathematical assumptions behind pre-posterior analysis?

- Concept: Acquisition functions for Bayesian optimization
  - Why needed here: Framework uses standard acquisition functions (EI for BO, MMSE for GF) in multi-fidelity context, requiring understanding of exploration-exploitation balance
  - Quick check question: How does Expected Improvement (EI) balance exploration vs. exploitation?

## Architecture Onboarding

- Component map: LVGP model -> Pre-posterior analysis module -> Two-stage optimization engine -> Fidelity selection logic -> Data management layer

- Critical path: 1) Train initial MF-LVGP with initial samples 2) Identify location of interest on HF model 3) Evaluate all candidate LF samples using pre-posterior analysis 4) Select sample with best benefit-to-cost ratio 5) Simulate sample and update training set 6) Repeat until termination

- Design tradeoffs: Latent space dimension (2D default but may need adjustment); computational cost of pre-posterior analysis vs. sampling efficiency; balance between exploration and exploitation in two-stage optimization; handling of biased LF models vs. discarding their data

- Failure signatures: Poor convergence (check latent space learning quality and pre-posterior analysis accuracy); wrong optimum selection (check if HF model is sufficiently accurate); excessive HF sampling (check benefit-to-cost ratio calculations and latent correlation structure)

- First 3 experiments: 1) Implement LVGP on simple two-fidelity problem with known correlation structure 2) Add pre-posterior analysis module and verify it correctly predicts uncertainty reduction 3) Implement two-stage optimization and test on simple BO problem to verify it selects appropriate fidelity sources

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is optimal trade-off between number of latent dimensions and model accuracy for multi-fidelity problems across different dimensionalities?
- Basis in paper: [explicit] Uses 2D latent space but mentions "dimensions of the latent variable vector can be freely chosen" and "two-dimensional (2D) latent vector... is usually sufficient in most engineering designs [58], which is also adopted in this study"
- Why unresolved: Paper doesn't explore impact of varying latent space dimensionality on model performance across different problem scales and dimensionalities
- What evidence would resolve it: Systematic comparison of model accuracy and computational efficiency across different latent space dimensionalities (1D, 2D, 3D+) on diverse test problems with varying input dimensions

### Open Question 2
- Question: How does proposed method perform when HF and LF models have different input spaces (subregion relationship)?
- Basis in paper: [explicit] States "we only demonstrate the case where the HF and LF models share the same design space" and notes this as limitation
- Why unresolved: Paper doesn't test or develop extensions for cases where LF models operate on subregions of HF input space
- What evidence would resolve it: Application and validation of method on benchmark problems where LF models have restricted input domains relative to HF models

### Open Question 3
- Question: What is impact of batch sampling strategies on convergence rate and accuracy compared to sequential sampling in proposed framework?
- Basis in paper: [explicit] Mentions "the ùë•ùë•ùëõùëõùëõùëõùëõùëõùëõùëõ is not limited to represent one infill location" and discusses batch sampling but only demonstrates "one infill sample in each iteration"
- Why unresolved: Paper only demonstrates sequential sampling and doesn't compare with batch sampling approaches
- What evidence would resolve it: Comparative study of sequential vs. batch sampling strategies (various batch sizes) on same benchmark problems measuring convergence speed and final accuracy

## Limitations
- Computational cost of pre-posterior analysis scales poorly with problem dimension
- Requires sufficient initial LF samples to learn accurate latent space
- Performance degrades when LF models are biased or have discontinuous correlation structures
- 2D latent space assumption may be insufficient for complex fidelity relationships

## Confidence
- Core mechanism (LVGP correlation capture): High confidence
- Pre-posterior analysis component: Medium confidence
- Benefit-to-cost ratio metric: Medium confidence

## Next Checks
1. Test framework on problems with known latent structure but non-smooth correlations to validate latent space learning capability
2. Implement full pre-posterior analysis and verify it correctly predicts uncertainty reduction on simple test problems
3. Compare benefit-to-cost ratio selection against alternative metrics (e.g., pure acquisition improvement) on problems with varying cost structures