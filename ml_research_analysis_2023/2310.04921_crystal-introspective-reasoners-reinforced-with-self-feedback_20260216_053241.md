---
ver: rpa2
title: 'Crystal: Introspective Reasoners Reinforced with Self-Feedback'
arxiv_id: '2310.04921'
source_url: https://arxiv.org/abs/2310.04921
tags:
- knowledge
- reasoning
- crystal
- training
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces CRYSTAL, an introspective reasoner for commonsense
  question answering that generates and utilizes knowledge statements to improve performance
  and interpretability. CRYSTAL has two modes: knowledge introspection, which generates
  relevant knowledge statements, and knowledge-grounded reasoning, which uses these
  statements to make predictions.'
---

# Crystal: Introspective Reasoners Reinforced with Self-Feedback

## Quick Facts
- arXiv ID: 2310.04921
- Source URL: https://arxiv.org/abs/2310.04921
- Authors: 
- Reference count: 24
- Key outcome: CRYSTAL achieves up to 2.5% accuracy improvement on 25 commonsense QA datasets through introspective reasoning with self-feedback reinforcement learning

## Executive Summary
CRYSTAL introduces an introspective reasoning approach for commonsense question answering that generates and utilizes knowledge statements to improve performance and interpretability. The model operates in two modes: knowledge introspection generates relevant knowledge statements, while knowledge-grounded reasoning uses these statements to make predictions. Both modes are trained using reinforcement learning with rewards derived from self-feedback, allowing the model to adapt its knowledge generation based on downstream reasoning effectiveness. Experiments demonstrate significant performance improvements over direct QA models and chain-of-thought approaches, with the introspective process providing interpretable reasoning traces.

## Method Summary
CRYSTAL is trained in two stages: an imitation learning stage using supervised knowledge distillation from few-shot GPT-3, followed by a reinforcement learning stage that optimizes both knowledge introspection and knowledge-grounded reasoning modes. The model uses a shared transformer backbone with different heads for language modeling and value regression, enabling efficient PPO training by storing only two models instead of four. Self-feedback rewards are computed by comparing prediction performance with and without generated knowledge, reinforcing introspection when knowledge proves beneficial. The two modes are mutually adapted through an interleaved optimization schedule that mirrors the EM algorithm's structure.

## Key Results
- Achieves up to 2.5% accuracy improvement over direct QA models and chain-of-thought distilled models
- Demonstrates good generalization to unseen datasets (15 out of 25 datasets were unseen during training)
- Shows 34% of generated knowledge supports predictions according to human annotations
- Requires less memory and training time than prior methods due to model sharing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mutual adaptation between knowledge introspection and knowledge-grounded reasoning improves performance.
- Mechanism: The model optimizes two intertwined objectives through an interleaved schedule, where reasoning mode adapts to introspected knowledge and introspection mode adapts to reasoning needs.
- Core assumption: The knowledge generated by introspection is non-differentiable but its effectiveness can be measured via rewards, enabling reinforcement learning to optimize the introspection policy.
- Evidence anchors: [abstract] "knowledge introspection and knowledge-grounded reasoning modes... are tuned via reinforcement learning to mutually adapt"; [section 2.3] "We use the PPO algorithm to optimize this reward."

### Mechanism 2
- Claim: Self-feedback via reinforcement learning enables the model to improve its own reasoning.
- Mechanism: The model samples knowledge, uses it in reasoning, then evaluates whether that knowledge improved its prediction, with rewards comparing performance with and without knowledge.
- Core assumption: The model's own judgments about whether knowledge helped are reliable enough to serve as training signals.
- Evidence anchors: [abstract] "... where the reward derives from the feedback given by the model itself."; [section 2.3] "The desirability of introspected knowledge is determined by its effectiveness on the subsequent knowledge-grounded reasoning."

### Mechanism 3
- Claim: Model sharing between policy, value, and reward models significantly improves memory and time efficiency for PPO training.
- Mechanism: Instead of maintaining separate networks for each PPO component, CRYSTAL reuses its parameters across all three roles with different heads.
- Core assumption: The shared architecture can effectively represent the policy, value function, and reward prediction simultaneously without interference.
- Evidence anchors: [section 2.4] "PPO on CRYSTAL needs to store only two models... consumes less GPU memory, and has faster training speed."

## Foundational Learning

- Concept: Reinforcement Learning with Proximal Policy Optimization (PPO)
  - Why needed here: Standard supervised learning cannot directly optimize the discrete knowledge generation process, requiring RL to optimize for the downstream effect of knowledge on reasoning.
  - Quick check question: How does PPO's clipped surrogate objective prevent destructive policy updates during training?

- Concept: Interleaved optimization schedules
  - Why needed here: Joint optimization of mutually dependent objectives can lead to unstable training; interleaving allows each component to stabilize before adapting to the other.
  - Quick check question: What is the relationship between CRYSTAL's interleaved schedule and the EM algorithm?

- Concept: Knowledge-augmented reasoning
  - Why needed here: Commonsense reasoning often requires implicit knowledge that must be made explicit and leveraged for accurate predictions.
  - Quick check question: How does knowledge-grounded reasoning differ from chain-of-thought reasoning in terms of knowledge utilization?

## Architecture Onboarding

- Component map: Input processor → Knowledge introspection head → Knowledge-grounded reasoning head → Answer prediction
- Critical path: Question → Knowledge introspection → Knowledge-grounded reasoning → Answer prediction
- Design tradeoffs:
  - Knowledge length vs. computational cost: Limited to 32 tokens to control memory usage
  - Knowledge diversity vs. coherence: Nucleus sampling with p=0.5 balances exploration and relevance
  - Model size vs. generalization: Larger models show better performance but require more resources
- Failure signatures:
  - Knowledge generation produces trivial or repetitive statements (indicating poor introspection capability)
  - Reasoning mode ignores generated knowledge (indicating poor adaptation)
  - Performance improvements plateau quickly (indicating insufficient mutual adaptation)
- First 3 experiments:
  1. Train CRYSTAL on a single dataset (e.g., CommonsenseQA) with supervised learning only to establish baseline performance
  2. Add reinforcement learning stage with random knowledge sampling to verify reward signal functionality
  3. Implement interleaved optimization schedule and compare against joint optimization on same data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CRYSTAL's performance on non-commonsense reasoning tasks compare to its performance on commonsense reasoning tasks?
- Basis in paper: [inferred] The paper states that CRYSTAL is intended to solve commonsense QA problems, but its performance on non-commonsense applications is unknown and thus requires further investigation.
- Why unresolved: The paper does not provide any experimental results or analysis on CRYSTAL's performance on non-commonsense reasoning tasks.
- What evidence would resolve it: Conducting experiments on CRYSTAL's performance on a diverse set of non-commonsense reasoning tasks and comparing the results to its performance on commonsense reasoning tasks.

### Open Question 2
- Question: What is the impact of increasing the length of knowledge statements generated by CRYSTAL beyond the current experimental limit?
- Basis in paper: [explicit] The paper mentions that there is a limit on the length of knowledge it generates in the experimental setting, and it has not been tested on generating long and coherent text.
- Why unresolved: The paper does not explore the effects of generating longer knowledge statements or analyze the coherence of such statements.
- What evidence would resolve it: Conducting experiments with varying maximum lengths for knowledge statements and evaluating the impact on CRYSTAL's performance and the coherence of the generated knowledge.

### Open Question 3
- Question: How does the self-feedback mechanism in CRYSTAL compare to human feedback in terms of improving model performance and interpretability?
- Basis in paper: [explicit] The paper discusses the use of self-feedback in training CRYSTAL through reinforcement learning, but it does not compare this approach to using human feedback.
- Why unresolved: The paper does not provide any experimental results or analysis comparing the effectiveness of self-feedback versus human feedback in training CRYSTAL.
- What evidence would resolve it: Conducting experiments where CRYSTAL is trained using both self-feedback and human feedback, and comparing the resulting model performance and interpretability.

## Limitations
- Self-feedback mechanism reliability is uncertain without independent verification of the model's reward judgments
- Human annotation study shows knowledge supports predictions in only 34% of cases without establishing baseline comparison
- Lacks ablation studies comparing performance against variants without model sharing or with different knowledge generation strategies

## Confidence
- **High Confidence**: Architecture design and training procedure are clearly specified with sufficient detail for implementation; efficiency gains from model sharing are well-documented
- **Medium Confidence**: Performance improvements over baseline models are demonstrated across multiple datasets, but mechanisms driving improvements are not fully validated; 2.5% accuracy gain is modest relative to complexity
- **Low Confidence**: Self-feedback providing reliable training signals lacks independent validation; interpretability benefits demonstrated through single human study without comparison to baseline methods

## Next Checks
1. **Self-Feedback Reliability Test**: Implement oracle evaluation where human annotators independently verify whether generated knowledge actually helped predictions, then compare these judgments against the model's self-feedback rewards to quantify the reliability of the self-supervised training signal.

2. **Ablation of Model Sharing**: Create a variant of CRYSTAL that maintains separate networks for policy, value, and reward modeling to quantify the actual performance impact of the efficiency optimization and determine whether any accuracy loss is justified by computational gains.

3. **Knowledge Generation Quality Analysis**: Conduct systematic evaluation of knowledge generation quality by sampling statements across multiple reasoning types and having human judges rate relevance, novelty, and correctness to identify whether the 34% support rate reflects genuine knowledge utility or post-hoc rationalization.