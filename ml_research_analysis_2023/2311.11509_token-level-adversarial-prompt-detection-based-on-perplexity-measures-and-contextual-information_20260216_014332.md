---
ver: rpa2
title: Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual
  Information
arxiv_id: '2311.11509'
source_url: https://arxiv.org/abs/2311.11509
tags:
- adversarial
- prompt
- token
- prompts
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting adversarial prompts
  in large language models (LLMs) by introducing a token-level detection method. The
  core idea is to leverage the LLM's perplexity measure combined with contextual information
  from neighboring tokens.
---

# Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information

## Quick Facts
- arXiv ID: 2311.11509
- Source URL: https://arxiv.org/abs/2311.11509
- Authors: 
- Reference count: 15
- Key outcome: This paper addresses the problem of detecting adversarial prompts in large language models (LLMs) by introducing a token-level detection method. The core idea is to leverage the LLM's perplexity measure combined with contextual information from neighboring tokens. Two approaches are proposed: one based on optimization techniques and another using probabilistic graphical models. Both methods efficiently identify whether each token in a sequence is part of an adversarial prompt. Experimental results show that the proposed methods achieve perfect sequence-level detection performance (precision, recall, F1-score, and AUC all reaching 1) and effective token-level localization with precision of 0.6287, recall of 0.7946, and F1-score of 0.7020. The methods demonstrate that smaller models like GPT-2 are sufficient for this task, reducing computational requirements while maintaining strong detection performance.

## Executive Summary
This paper introduces a token-level adversarial prompt detection method that leverages language model perplexity and contextual information to identify adversarial tokens in input sequences. The core innovation combines perplexity-based scoring with a regularization framework that encourages detection of contiguous adversarial sequences. Two complementary approaches are presented: an optimization-based method using fused lasso regularization and a probabilistic graphical model that provides uncertainty quantification. The method achieves perfect sequence-level detection performance and strong token-level localization, demonstrating that smaller language models like GPT-2 are sufficient for this task.

## Method Summary
The detection method works by computing perplexity scores for each token using a language model, then applying regularization to encourage detection of contiguous adversarial sequences. The optimization approach minimizes negative log-likelihood with a fused lasso regularization term that encourages neighboring token indicators to be similar. The probabilistic graphical model approach extends this to Bayesian inference, computing posterior probabilities for each token being adversarial. Both methods use dynamic programming for efficient O(n) computation. The framework uses GPT-2 (124M parameters) to compute token probabilities and evaluates on a dataset of 107 adversarial prompts combined with natural language queries.

## Key Results
- Perfect sequence-level detection performance: precision=1, recall=1, F1=1, AUC=1
- Strong token-level localization: precision=0.6287, recall=0.7946, F1=0.7020
- Smaller models like GPT-2 are sufficient for effective adversarial prompt detection
- Optimization and PGM approaches achieve comparable performance with different hyperparameter settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High perplexity indicates adversarial tokens because they deviate from the model's learned distribution
- Mechanism: The method leverages the LLM's probability predictions for each token. Adversarial prompts are intentionally crafted to be out-of-distribution, resulting in low probability predictions and thus high perplexity values. Tokens with high perplexity are flagged as potential adversarial tokens.
- Core assumption: Adversarial prompts are deliberately constructed to be out-of-distribution relative to the model's training data
- Evidence anchors:
  - [abstract] "Due to the OOD nature of these adversarial prompts, a salient characteristic of adversarial prompts is their high perplexity."
  - [section] "Since we are not aware of the ground-truth of language distribution, we use a language model as an approximation. Given its preceding tokens x1, . . . , xi− 1, a language model produce a probability represented as pLLM(xi|x1, . . . , xi− 1). For simplicity, we refer to this probability as p0,i."
  - [corpus] Weak - corpus papers focus on jailbreak detection methods but don't directly validate the perplexity-adversarial relationship
- Break condition: If adversarial prompts become indistinguishable from natural text distributions, or if benign but rare tokens consistently trigger high perplexity

### Mechanism 2
- Claim: Contextual regularization improves detection by identifying contiguous adversarial sequences
- Mechanism: The method incorporates a regularization term that encourages neighboring token indicators (ci) to be similar. This captures the observation that adversarial prompts often appear in sequences rather than isolated tokens, improving detection accuracy by considering local context.
- Core assumption: Adversarial prompts typically form contiguous sequences rather than isolated tokens
- Evidence anchors:
  - [abstract] "Furthermore, these adversarial prompts often appear in sequence, as longer adversarial prompts are more effective in leading the model astray."
  - [section] "To encourage the detection of contiguous adversarial prompt sequences, we introduce a regularization term∑n−1 i=1 |ci+1 − ci| inspired by the fused lasso. This regularization aims to make adjacent indicators ci and ci+1 similar, thus considering the context around a token when determining if it's part of an adversarial prompt sequence."
  - [corpus] No direct evidence - corpus focuses on jailbreak detection but doesn't validate sequence continuity assumption
- Break condition: If adversarial prompts are crafted as isolated tokens scattered throughout otherwise normal text, or if the regularization strength λ is poorly tuned

### Mechanism 3
- Claim: Probabilistic graphical models provide uncertainty quantification for detection decisions
- Mechanism: The method extends from binary classification to Bayesian inference, modeling each indicator ci as a random variable with a prior distribution. This allows computing posterior probabilities for each token being adversarial, providing uncertainty estimates alongside detection decisions.
- Core assumption: A probabilistic framework can better capture the uncertainty in adversarial prompt detection compared to deterministic optimization
- Evidence anchors:
  - [section] "While the optimization problem gives us a binary output for each token, it doesn't capture the uncertainty in the prediction. To address this, we can extend our approach to a probabilistic graphical model. This approach has the advantage of providing uncertainty information about the model's predictions with a Bayesian posterior over the indicators ci."
  - [corpus] Weak - corpus papers mention uncertainty in detection but don't specifically validate probabilistic graphical models for this task
- Break condition: If the prior assumptions about indicator distributions are significantly wrong, or if computational complexity of exact inference becomes prohibitive

## Foundational Learning

- Concept: Perplexity as a measure of language model uncertainty
  - Why needed here: The entire detection method relies on perplexity to distinguish adversarial from normal tokens
  - Quick check question: How is perplexity mathematically related to the probability assigned by a language model to a token?

- Concept: Regularization techniques for sequence modeling
  - Why needed here: The fused lasso-inspired regularization is crucial for encouraging detection of contiguous adversarial sequences
  - Quick check question: What is the purpose of the regularization term ∑|ci+1 − ci| in the optimization problem?

- Concept: Probabilistic graphical models and Bayesian inference
  - Why needed here: The PGM approach provides uncertainty quantification for detection decisions
  - Quick check question: How does the Bayesian posterior p(c⃗|x⃗) differ from the optimization-based solution?

## Architecture Onboarding

- Component map: Language model (GPT-2) -> Perplexity computation -> Regularization framework -> Dynamic programming solver -> Detection output
- Critical path: Token input -> Language model probability prediction -> Indicator computation (optimization or PGM) -> Contiguous sequence detection -> Output heatmap
- Design tradeoffs: Smaller models like GPT-2 are sufficient but may miss nuanced adversarial patterns; larger models increase computational cost without improving detection accuracy
- Failure signatures: High false positive rate on rare but benign tokens; poor detection of isolated adversarial tokens; computational bottlenecks with long sequences
- First 3 experiments:
  1. Baseline test: Run detection on sequences known to contain adversarial prompts vs. clean sequences, verify sequence-level performance
  2. Ablation study: Compare detection performance with regularization λ set to 0 (no context) vs. optimal λ to validate contextual benefit
  3. Model dependency test: Swap GPT-2 with larger models like GPT-2 1.5B or Llama2 to confirm model size doesn't significantly impact detection accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of perplexity measure affect adversarial prompt detection performance?
- Basis in paper: [explicit] The paper mentions that perplexity is a key metric for identifying adversarial prompts and notes that adversarial sequences often exhibit high perplexity. It also discusses the choice of language model (GPT-2) for computing perplexity probabilities.
- Why unresolved: The paper uses GPT-2 to compute perplexity but does not explore alternative perplexity measures or their impact on detection performance. It also doesn't discuss whether different perplexity measures (e.g., normalized, entropy-based) would affect results.
- What evidence would resolve it: Comparative experiments using different perplexity measures (e.g., entropy-based, normalized perplexity) across multiple language models, showing detection performance differences.

### Open Question 2
- Question: What is the optimal balance between individual token perplexity and contextual information for adversarial prompt detection?
- Basis in paper: [explicit] The paper introduces a regularization term to balance token perplexity and contextual information, with λ controlling this trade-off. It mentions that different λ values prioritize different aspects of detection.
- Why unresolved: The paper uses fixed λ=10 and µ=0.5 values without exploring the sensitivity of detection performance to these hyperparameters or determining optimal values for different types of adversarial prompts.
- What evidence would resolve it: Systematic hyperparameter optimization experiments showing detection performance across a range of λ and µ values, identifying optimal settings for different adversarial prompt types.

### Open Question 3
- Question: How does the proposed detection method generalize to adversarial prompts from different attack strategies?
- Basis in paper: [explicit] The paper uses adversarial prompts generated by one specific algorithm [13] and evaluates detection performance only on these prompts. It mentions that adversarial prompts can be constructed through various methods.
- Why unresolved: The evaluation dataset consists only of adversarial prompts from one generation algorithm, without testing the method's effectiveness against adversarial prompts from different attack strategies or more sophisticated attacks.
- What evidence would resolve it: Testing the detection method against adversarial prompts generated by multiple attack strategies (optimization-based, gradient-based, heuristic-based) and measuring performance degradation across different attack types.

## Limitations

- Model Dependency and Generalization: The method relies on perplexity measures that may not generalize across different model architectures or training paradigms.
- Evaluation Dataset Limitations: Performance is evaluated on a relatively small dataset of 107 adversarial prompts, potentially limiting generalizability.
- Regularization Hyperparameter Sensitivity: Fixed hyperparameters (λ=10, µ=0.5) are used without systematic tuning or justification.

## Confidence

**High Confidence Claims**:
- The optimization and PGM algorithms are correctly formulated and implementable
- The token-level detection framework with perplexity-based scoring is technically sound
- The sequence-level detection performance (precision=1, recall=1, F1=1, AUC=1) is achievable on the evaluated dataset

**Medium Confidence Claims**:
- The claim that smaller models are sufficient for detection tasks
- The effectiveness of contextual regularization for improving token-level localization
- The generalizability of detection performance across different adversarial prompt types

**Low Confidence Claims**:
- The robustness of detection against adaptive adversarial attacks
- The performance on significantly larger or differently structured language models
- The real-world deployment viability without extensive hyperparameter tuning

## Next Checks

1. **Adversarial Robustness Test**: Generate a new set of adversarial prompts using different generation strategies (e.g., different from Zou et al. 2023) and evaluate detection performance to assess generalization. This would reveal whether the method is overfit to a specific adversarial prompt generation technique.

2. **Model Architecture Transferability**: Implement the detection framework using different language models (e.g., GPT-2 1.5B, Llama2, or BERT-based models) to verify the claim that model size does not significantly impact detection accuracy. This would test the assumption that smaller models are sufficient and identify any architecture-specific limitations.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary the regularization parameter λ and the detection threshold µ across a wide range of values, then measure the impact on both sequence-level and token-level performance metrics. This would quantify the robustness of the method to hyperparameter choices and identify optimal settings for different scenarios.