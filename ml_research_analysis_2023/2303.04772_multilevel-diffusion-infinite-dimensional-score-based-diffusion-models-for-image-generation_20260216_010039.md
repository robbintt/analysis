---
ver: rpa2
title: 'Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for
  Image Generation'
arxiv_id: '2303.04772'
source_url: https://arxiv.org/abs/2303.04772
tags:
- process
- resolution
- usion
- nite-dimensional
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops score-based diffusion models (SBDMs) in the
  infinite-dimensional setting, modeling images as functions supported on a rectangular
  domain. The key motivation is to create a well-posed infinite-dimensional learning
  problem that can be consistently discretized on multiple resolution levels, enabling
  multilevel training for improved efficiency.
---

# Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation

## Quick Facts
- arXiv ID: 2303.04772
- Source URL: https://arxiv.org/abs/2303.04772
- Reference count: 40
- Primary result: Develops infinite-dimensional score-based diffusion models using trace class operators for multilevel image generation training

## Executive Summary
This paper develops score-based diffusion models in the infinite-dimensional setting, modeling images as functions supported on a rectangular domain. The key motivation is to create a well-posed infinite-dimensional learning problem that can be consistently discretized on multiple resolution levels, enabling multilevel training for improved efficiency. The authors address two main challenges: ensuring the latent distribution is well-defined in infinite dimensions using trace class operators, and approximating the score function with an operator network (Fourier neural operators) for effective multilevel training.

## Method Summary
The paper proposes a framework for score-based diffusion models in infinite-dimensional Hilbert spaces, where images are treated as functions. The forward process uses trace class operators to ensure well-defined Gaussian distributions in infinite dimensions, while the reverse process is formulated as an infinite-dimensional SDE. The score function is approximated using Fourier neural operators, which provide resolution-invariant representations. The method enables multilevel training by discretizing the infinite-dimensional problem and showing convergence as resolution increases, allowing efficient training from coarse to fine resolutions.

## Key Results
- Demonstrated consistent discretization of infinite-dimensional forward and reverse processes across resolution levels
- Showed that trace class operators enable well-defined Gaussian distributions in infinite dimensions
- Validated multilevel training approach on MNIST and synthetic datasets with improved efficiency
- Compared different latent distributions (standard Gaussian, inverse Laplacian, FNO prior) and network architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using trace class operators ensures well-defined Gaussian distributions in infinite-dimensional Hilbert spaces, enabling consistent multilevel training.
- Mechanism: Trace class operators guarantee that the sum of squared eigenvalues (trace) remains finite even as dimension increases, preventing the blow-up that occurs with identity covariance matrices. This allows proper discretization at multiple resolution levels.
- Core assumption: The infinite-dimensional problem must be consistently discretizable across resolution levels while maintaining mathematical well-posedness.

### Mechanism 2
- Claim: Fourier neural operators (FNOs) provide resolution-invariant score function approximation, enabling generalization across different image resolutions.
- Mechanism: FNOs operate in frequency space with a fixed number of modes independent of input resolution, allowing the same network architecture to process images at different resolutions without retraining.
- Core assumption: The score function can be effectively approximated by a resolution-invariant operator network rather than a resolution-specific U-Net architecture.

### Mechanism 3
- Claim: Multilevel training converges from coarse to fine resolutions, reducing computational cost while maintaining image quality.
- Mechanism: Training begins on low-resolution data where forward and reverse processes are computationally cheaper, then uses the learned parameters as initialization for higher-resolution training, leveraging the hierarchical structure of the infinite-dimensional problem.
- Core assumption: Solutions to the infinite-dimensional SDE converge as discretization level increases, allowing coarse-level training to provide useful initialization for fine-level training.

## Foundational Learning

- Concept: Trace class operators in infinite-dimensional spaces
  - Why needed here: Standard Gaussian distributions are not well-defined in infinite dimensions because their covariance operator would have infinite trace. Trace class operators ensure the covariance remains finite, making Gaussian measures well-defined.
  - Quick check question: What mathematical property distinguishes trace class operators from other compact operators, and why is this property crucial for infinite-dimensional Gaussian measures?

- Concept: Spectral decomposition of covariance operators
  - Why needed here: The spectral decomposition allows expressing the infinite-dimensional Wiener process as a sum of independent one-dimensional processes, enabling coordinate-wise analysis and discretization.
  - Quick check question: How does the spectral decomposition of a trace class operator enable the decoupling of the infinite-dimensional forward process into scalar SDEs?

- Concept: Martingale solutions vs strong solutions in infinite dimensions
  - Why needed here: In infinite dimensions, the reverse process may only exist as a martingale solution rather than a strong solution, requiring different mathematical treatment than finite-dimensional SDEs.
  - Quick check question: What is the key difference between martingale solutions and strong solutions, and why does this distinction become important in the infinite-dimensional setting?

## Architecture Onboarding

- Component map: Forward process -> Infinite-dimensional SDE with trace class noise operator -> Score network (Fourier neural operator) -> Discretization (spectral truncation) -> Multilevel training (coarse → fine resolution)

- Critical path:
  1. Define trace class covariance operator Q
  2. Implement spectral decomposition of Q
  3. Build FNO architecture for score approximation
  4. Set up multilevel training loop (resolution 16 → 32 → 64)
  5. Implement consistency checks for discretization convergence

- Design tradeoffs:
  - FNO vs U-Net: FNO provides resolution invariance but may be less expressive than U-Net
  - Number of frequency modes: More modes increase expressiveness but also computational cost
  - Number of resolution levels: More levels enable better coarse-to-fine learning but increase training complexity
  - Trace class operator choice: Different operators (inverse Laplacian, FNO prior) may yield different image characteristics

- Failure signatures:
  - Training divergence at higher resolutions indicates poor coarse-level initialization
  - Mode collapse in generated samples suggests score network lacks expressiveness
  - Loss plateaus early suggest insufficient model capacity or inappropriate learning rate
  - Poor FID scores indicate the trace class operator choice is not capturing the data distribution well

- First 3 experiments:
  1. Train FNO with standard Gaussian prior on resolution 16 MNIST, evaluate on resolution 32 to verify resolution invariance
  2. Compare inverse Laplacian vs FNO prior trace class operators on same resolution to assess impact on image quality
  3. Implement two-level training (16→32) vs single-level training at resolution 32 to measure computational savings and performance impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different choices of trace class operators affect the quality and diversity of generated images in practice?
- Basis in paper: [explicit] The paper mentions comparing the inverse Laplacian and FNO-based prior as trace class operators, but notes they were unable to produce good results with the inverse Laplacian and only briefly mentions the FNO prior's performance.
- Why unresolved: The experiments only briefly compare a few trace class operators, and the paper doesn't thoroughly investigate the effects of different choices on image quality and diversity.
- What evidence would resolve it: Comprehensive experiments comparing various trace class operators (e.g., different spectral distributions, fractional Laplacians) on standard image generation benchmarks, analyzing both quantitative metrics (FID, Inception Score) and qualitative sample diversity.

### Open Question 2
- Question: What are the theoretical conditions under which the reverse process in infinite dimensions admits a strong solution rather than just a martingale solution?
- Basis in paper: [explicit] The paper discusses the time reversal of infinite-dimensional diffusions and references [37] for conditions, but doesn't fully characterize when strong solutions exist.
- Why unresolved: The paper adopts a martingale solution framework for the reverse process but doesn't explore the stronger notion of strong solutions or provide conditions for their existence.
- What evidence would resolve it: Rigorous analysis of the conditions under which the reverse SDE admits a strong solution, potentially involving additional regularity assumptions on the score function or the forward process coefficients.

### Open Question 3
- Question: How does the choice of neural operator architecture (e.g., U-Net vs FNO) impact the expressiveness and performance of the score function in infinite-dimensional SBDMs?
- Basis in paper: [explicit] The paper mentions that FNOs may not be well-suited to capture sharp edges in images compared to U-Nets, but doesn't thoroughly investigate the trade-offs between different architectures.
- Why unresolved: The experiments use a simple FNO architecture, and the paper doesn't compare its performance to other architectures like U-Nets or wavelet neural operators in the infinite-dimensional setting.
- What evidence would resolve it: Systematic comparison of different neural operator architectures for approximating the score function, evaluating their performance on image generation tasks and analyzing their ability to capture different image features.

### Open Question 4
- Question: What are the optimal strategies for multilevel training in infinite-dimensional SBDMs, and how do they compare to single-resolution training?
- Basis in paper: [explicit] The paper presents a preliminary multilevel training experiment but doesn't provide a comprehensive analysis of optimal strategies or compare them to single-resolution training.
- Why unresolved: The paper only explores a simple two-level coarse-to-fine approach and doesn't investigate other strategies like adaptive resolution selection or dynamic scheduling.
- What evidence would resolve it: Extensive experiments comparing different multilevel training strategies (e.g., varying the number of levels, resolution ratios, training schedules) against single-resolution training, analyzing both computational efficiency and image quality.

## Limitations
- Limited experimental validation primarily on simple datasets (MNIST, synthetic data) rather than complex natural images
- Preliminary multilevel training experiments without comprehensive analysis of optimal strategies
- Limited comparison of different neural operator architectures for score function approximation

## Confidence
- **High confidence**: The mathematical framework using trace class operators for infinite-dimensional Gaussian distributions is rigorously justified
- **Medium confidence**: The proposed FNO architecture shows promise but needs more extensive comparison with standard approaches
- **Medium confidence**: Multilevel training efficiency gains are demonstrated but would benefit from more comprehensive computational analysis

## Next Checks
1. Implement the forward and reverse SDEs in infinite-dimensional Hilbert space with trace class operators and verify discretization convergence across resolution levels
2. Compare performance of FNO architecture versus U-Net for score approximation on resolution-invariant image generation tasks
3. Conduct comprehensive ablation study on multilevel training strategies, varying number of levels, resolution ratios, and training schedules to quantify computational savings versus single-resolution training