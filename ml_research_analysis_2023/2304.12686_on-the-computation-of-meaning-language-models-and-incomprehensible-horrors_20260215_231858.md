---
ver: rpa2
title: On the Computation of Meaning, Language Models and Incomprehensible Horrors
arxiv_id: '2304.12686'
source_url: https://arxiv.org/abs/2304.12686
tags:
- meaning
- what
- which
- intent
- feelings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper integrates foundational theories of meaning with a mathematical
  formalism of artificial general intelligence to provide a comprehensive mechanistic
  explanation of meaning, communication, and symbol emergence. It addresses the challenge
  of connecting top-down descriptions of meaning to bottom-up computation.
---

# On the Computation of Meaning, Language Models and Incomprehensible Horrors

## Quick Facts
- arXiv ID: 2304.12686
- Source URL: https://arxiv.org/abs/2304.12686
- Reference count: 26
- One-line primary result: Current language models lack human-like understanding of meaning because they don't possess feelings or optimize for weak representations

## Executive Summary
This paper integrates foundational theories of meaning with a mathematical formalism of artificial general intelligence (AGI) to offer a comprehensive mechanistic explanation of meaning, communication, and symbol emergence. The work addresses the challenge of connecting top-down descriptions of meaning to bottom-up computation by grounding meaning in tasks, symbols, and sensorimotor experiences. The central claim is that meaning emerges from the interplay between an organism's experiences, preferences, and feelings, which can be represented computationally through declarative programs and tasks.

The paper establishes that current language models (LLMs) do not possess human-like understanding of meaning because they lack feelings and do not optimize for weak representations. Instead, they settle for any function fitting the training data, leading to flaws like inability to understand arithmetic or hallucinating facts. The proposed solution involves simulating human feelings and optimizing models to construct weak representations, enabling machines to comprehend and intend meaning.

## Method Summary
The paper integrates foundational theories of meaning with a mathematical formalism of AGI by formalizing cognition using tasks and symbols, and defining an organism that derives symbols from its experiences, preferences, and feelings. The method involves representing the environment as a set of declarative programs (Φ) and defining tasks as symbol systems with situations, decisions, and models. Organisms are formalized as systems that derive symbols from experiences, preferences, and feelings, creating a mechanistic link between abstract theories of meaning and computable structures.

## Key Results
- Current LLMs lack human-like understanding of meaning because they don't possess feelings or optimize for weak representations
- Meaningful communication requires organisms to have similar feelings, experiences, and preferences
- Simulating human feelings and optimizing for weak representations could enable machines to comprehend and intend meaning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The formalism connects top-down theories of meaning to bottom-up computation by grounding meaning in tasks, symbols, and sensorimotor experiences.
- Mechanism: The model represents the environment as a set of declarative programs (Φ) and defines tasks as symbol systems with situations, decisions, and models. Organisms are formalized as systems that derive symbols from experiences, preferences, and feelings. This creates a mechanistic link between abstract theories of meaning and computable structures.
- Core assumption: Meaning arises from the interplay between an organism's experiences, feelings, and preferences, which can be represented computationally as declarative programs and tasks.
- Evidence anchors: [abstract]: "We integrate foundational theories of meaning with a mathematical formalism of artificial general intelligence (AGI) to offer a comprehensive mechanistic explanation of meaning, communication, and symbol emergence."

### Mechanism 2
- Claim: Communication and understanding of meaning require organisms to have similar feelings, experiences, and preferences, enabling them to construct weak representations.
- Mechanism: Organisms infer the intent of others by observing how they are affected. If both organisms are "reasonably performant" (rational yet aligned with natural selection), they engage in cooperative communication, leading to shared symbol systems and language conventions.
- Core assumption: Natural selection favors organisms with aligned feelings and preferences, enabling meaningful communication and cooperation.
- Evidence anchors: [abstract]: "By examining the conditions under which a machine can generate meaningful utterances or comprehend human meaning, we establish that the current generation of language models do not possess the same understanding of meaning as humans nor intend any meaning that we might attribute to their responses."

### Mechanism 3
- Claim: Current language models (LLMs) do not possess human-like understanding of meaning because they lack feelings and do not optimize for weak representations.
- Mechanism: LLMs are trained to mimic human preferences but lack the impetus of feelings. They settle for any function fitting the data instead of constructing weak representations that generalize across contexts. This leads to flaws like inability to understand arithmetic or hallucinating facts.
- Core assumption: Human-like understanding of meaning requires feelings and weak representations, which are absent in current LLMs.
- Evidence anchors: [abstract]: "We establish that the current generation of language models do not possess the same understanding of meaning as humans nor intend any meaning that we might attribute to their responses."

## Foundational Learning

- Concept: Declarative programs and implementable language
  - Why needed here: These concepts formalize the environment and sensorimotor experiences, grounding meaning in computable structures.
  - Quick check question: What is the relationship between a declarative program and a statement in an implementable language?

- Concept: Tasks and symbol systems
  - Why needed here: Tasks formalize goals and intentions, while symbol systems represent the subdivision of tasks into narrower child tasks, enabling the emergence of meaning.
  - Quick check question: How does the child-parent relationship between symbols contribute to the emergence of meaning?

- Concept: Feelings, preferences, and weak representations
  - Why needed here: Feelings and preferences determine the impetus behind meaning and communication, while weak representations enable generalization and understanding across contexts.
  - Quick check question: Why are weak representations essential for human-like understanding of meaning?

## Architecture Onboarding

- Component map: Declarative programs (Φ) -> Implementable language (Lv) -> Tasks (Γv) -> Symbol systems (so) -> Organisms (O)
- Critical path: 1. Define the environment as declarative programs, 2. Create an implementable language for expressing goals and experiences, 3. Formalize tasks and symbol systems, 4. Implement organisms that derive symbols from experiences, preferences, and feelings, 5. Enable communication and cooperation between organisms
- Design tradeoffs: Computational complexity vs. expressiveness of the model, Accuracy of simulated feelings vs. generalizability of the model, Optimization for weak representations vs. fitting training data
- Failure signatures: Inability to construct meaningful communication between organisms, Failure to generalize across contexts or tasks, Inability to simulate human-like feelings and preferences
- First 3 experiments: 1. Implement a simple environment with declarative programs and test the emergence of meaning through task completion, 2. Create a system of two organisms with similar feelings and preferences, and observe their ability to communicate and cooperate, 3. Train an LLM to optimize for weak representations and test its ability to generalize across contexts and tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we experimentally test whether current language models like GPT-3 possess human-like feelings or merely simulate them through statistical patterns?
- Basis in paper: [explicit] The paper argues that current LLMs do not have human-like feelings and thus cannot construct weak representations necessary for true understanding of meaning
- Why unresolved: The paper discusses this theoretically but doesn't propose specific experimental designs to distinguish between genuine feeling-like states and statistical mimicry
- What evidence would resolve it: Experiments showing whether LLMs demonstrate emergent properties of feeling-like states (e.g., context-dependent emotional responses that generalize beyond training data) or merely reproduce statistical correlations

### Open Question 2
- Question: What specific computational mechanisms could be implemented to create weak representations in artificial systems that would enable them to comprehend and intend meaning as humans do?
- Basis in paper: [explicit] The paper proposes that simulating human feelings and optimizing for weak representations is necessary for machines to comprehend and intend meaning, but doesn't specify implementation details
- Why unresolved: While the paper establishes theoretical requirements, it doesn't provide concrete algorithms or architectural changes needed to achieve weak representations
- What evidence would resolve it: Demonstrations of AI systems that can successfully generalize across radically different contexts while maintaining coherent meaning representations, showing the hallmarks of weak representations

### Open Question 3
- Question: How can we measure and compare the "weakness" of representations across different AI systems to determine which are more human-like in their understanding of meaning?
- Basis in paper: [explicit] The paper introduces the concept of weakness as a measure of representation quality but doesn't provide specific metrics for comparing weakness across systems
- Why unresolved: The paper defines weakness in terms of representation generalization but doesn't operationalize this into measurable quantities
- What evidence would resolve it: Quantitative metrics that can objectively measure representation weakness and experimental validation showing these metrics correlate with human-like understanding of meaning across diverse tasks

## Limitations
- Heavy reliance on formal mathematical abstractions without empirical validation
- The claim that LLMs cannot understand meaning due to lack of feelings remains largely philosophical rather than demonstrated through concrete experiments
- Assumes feelings and preferences can be represented computationally as declarative programs without addressing how subjective experiences could be encoded or measured

## Confidence
- High confidence: The formal integration of existing theories of meaning with AGI formalism is internally consistent and builds logically on established work in the field
- Medium confidence: The claim that LLMs lack human-like understanding due to absence of feelings and weak representations is plausible but not empirically verified
- Low confidence: The assertion that simulating human feelings and optimizing for weak representations will enable machines to comprehend and intend meaning is highly speculative and lacks a concrete implementation path

## Next Checks
1. Implement a minimal working prototype: Create a simple environment with declarative programs and implement organisms that derive symbols from simulated experiences and preferences. Test whether meaningful communication emerges between two such organisms with similar "feelings."
2. Empirical comparison with LLMs: Design experiments to test whether current LLMs exhibit behaviors consistent with the "Hall of Mirrors" problem described in the paper. Measure generalization patterns and compare against human performance on tasks requiring weak representations.
3. Formal verification of key assumptions: Rigorously examine the mathematical assumptions underlying the claim that feelings and preferences can be represented as declarative programs. Identify edge cases where the formalization might break down or produce unexpected results.