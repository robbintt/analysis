---
ver: rpa2
title: 'Invisible Relevance Bias: Text-Image Retrieval Models Prefer AI-Generated
  Images'
arxiv_id: '2311.14084'
source_url: https://arxiv.org/abs/2311.14084
tags:
- images
- ai-generated
- real
- retrieval
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uncovers an invisible relevance bias in text-image retrieval
  models, where they tend to rank AI-generated images higher than real images, even
  when both have similar visual semantics. To study this bias, the authors construct
  a benchmark by generating semantically similar AI-generated images for real images
  using a novel over-sampling and selection strategy.
---

# Invisible Relevance Bias: Text-Image Retrieval Models Prefer AI-Generated Images

## Quick Facts
- arXiv ID: 2311.14084
- Source URL: https://arxiv.org/abs/2311.14084
- Reference count: 40
- Primary result: Text-image retrieval models rank AI-generated images higher than real images despite similar visual semantics

## Executive Summary
This paper reveals an invisible relevance bias in text-image retrieval models where AI-generated images consistently receive higher relevance scores than real images with similar visual semantics. The authors construct a benchmark by generating semantically similar AI images for real images using stable diffusion and demonstrate that this bias persists across multiple retrieval architectures. They further show that including AI-generated images in training data exacerbates the bias, creating a self-reinforcing feedback loop. To address this, they propose a debiasing method using contrastive loss between real and AI-generated image representations, which effectively reduces the preference for AI-generated content.

## Method Summary
The authors create a benchmark by generating AI images using Stable Diffusion conditioned on merged captions from real image annotations. They evaluate various retrieval models including VSE, NAAF, VILT, FLAVA, ALIGN, and BEIT-3 on this benchmark. To mitigate the bias, they introduce a debiasing method that adds a contrastive loss term between real and AI-generated image representations during training, controlled by a Bernoulli sampling probability. The method is evaluated by measuring changes in retrieval performance and representation distributions across different mixing ratios of AI-generated images in training data.

## Key Results
- Retrieval models consistently rank AI-generated images higher than real images with similar visual semantics
- Including AI-generated images in training data worsens the bias and damages real image retrieval performance
- The proposed debiasing method effectively reduces the preference for AI-generated images
- AI-generated images induce image encoders to embed additional consistent information that amplifies relevance scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-image retrieval models assign higher relevance scores to AI-generated images than real images, even when semantic similarity is controlled.
- Mechanism: The image encoder embeds additional, consistent latent features into AI-generated image representations that are not visually perceptible but amplify relevance scoring.
- Core assumption: AI-generated images contain systematic, model-specific artifacts that neural encoders can extract and use as implicit relevance signals.
- Evidence anchors:
  - [abstract] "the AI-generated images induce the image encoder to embed additional information into their representation. This information exhibits a certain consistency across generated images with different semantics and can make the retriever estimate a higher relevance score."
  - [section] "AI-generated images introduce an invisible relevance bias to text-image retrieval models... AI-generated images do not exhibit more visually relevant features to the query than real images."
- Break condition: If encoder architectures change to ignore low-level artifacts or if generation models are trained to remove consistent embedding cues.

### Mechanism 2
- Claim: Including AI-generated images in training data worsens the invisible relevance bias.
- Mechanism: Training on mixed real/generated datasets teaches the retriever to associate the latent features from AI-generated images with higher relevance, creating a self-reinforcing feedback loop.
- Core assumption: The model learns to generalize the latent embedding patterns from generated images to unseen data, propagating the bias.
- Evidence anchors:
  - [section] "as the mixing ratio of AI-generated images increases, the invisible relevance bias becomes more serious... retrieval performance on the generated images gradually becomes stronger, while the retrieval performance on the real images is gradually damaged."
  - [section] "AI-generated images have a higher chance of being obtained from massive data, which makes them more likely to be mixed into the training of AIGC and retrieval models, leading to more serious bias and forming a vicious cycle."
- Break condition: If debiasing training methods successfully neutralize the latent feature associations or if training datasets are strictly filtered.

### Mechanism 3
- Claim: The debiasing method reduces bias by optimizing the difference in relevance scores between real and AI-generated images during training.
- Mechanism: A contrastive loss term penalizes the model when it assigns higher scores to AI-generated images than their real counterparts, forcing the encoder to reduce the latent feature bias.
- Core assumption: The contrastive term effectively measures and corrects the additional relevance introduced by AI-generated images without harming overall retrieval quality.
- Evidence anchors:
  - [abstract] "we introduce an effective training method aimed at alleviating the invisible relevance bias. Specifically, we introduce a contrastive loss between real and AI-generated images during training."
  - [section] "This contrastive loss measures the invisible relevance bias introduced by the AI-generated images for retrieval models. Optimization of this loss can reduce the invisible relevance bias, thereby alleviating the preference for AI-generated images in text-image retrieval models."
- Break condition: If the contrastive term overfits to training artifacts or if the sampling probability is poorly tuned.

## Foundational Learning

- Concept: Cross-modal embedding alignment
  - Why needed here: The retrieval model relies on aligning text and image embeddings; understanding how different modalities interact is key to diagnosing bias sources.
  - Quick check question: What happens to retrieval scores if the image encoder is replaced with a modality-agnostic transformer?

- Concept: Contrastive learning objectives
  - Why needed here: The debiasing method uses contrastive loss to compare real vs. generated image relevance; knowing how contrastive loss shapes embeddings is critical for tuning.
  - Quick check question: How does the margin in contrastive loss affect the magnitude of bias correction?

- Concept: Dataset contamination and domain shift
  - Why needed here: Mixing AI-generated images into training introduces domain shift that can bias the model; recognizing contamination helps in designing clean benchmarks.
  - Quick check question: If 50% of training images are AI-generated, what measurable shift occurs in the embedding distribution?

## Architecture Onboarding

- Component map:
  - Text encoder (e.g., BERT, CLIP text branch)
  - Image encoder (e.g., CLIP image branch, BEIT-3)
  - Fusion/dual-encoder architecture for scoring
  - Debiasing module: contrastive loss and Bernoulli sampling

- Critical path:
  1. Generate AI images using stable diffusion conditioned on merged captions
  2. Compute image embeddings for real and AI images
  3. Estimate relevance scores and apply contrastive loss if AI image scores higher
  4. Backpropagate through debiasing objective to adjust encoder weights

- Design tradeoffs:
  - Using image rewriting vs. pure text conditioning: rewriting increases semantic similarity but deviates from common generation pipeline
  - Sampling probability β: higher β reduces bias more but may discard useful hard negatives
  - Benchmark construction: balancing semantic similarity vs. avoiding additional semantic cues

- Failure signatures:
  - Retrieval performance degrades sharply on real images but improves on AI images
  - T-SNE shows no separation between real and AI embeddings despite bias in scores
  - Contrastive loss fails to converge, suggesting poor measurement of bias

- First 3 experiments:
  1. Measure NDCG@5 on benchmark with varying β; expect bias reduction as β increases
  2. Visualize T-SNE of image embeddings from debiased vs. non-debiased models; expect clearer separation
  3. Add the reverse transformation vector to real image embeddings and measure if bias flips; expect real images to be ranked higher

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the long-term impact of AI-generated content on information retrieval systems?
- Basis in paper: [explicit] The paper discusses the potential vicious cycle where AI-generated images become more prevalent in training data, leading to increased bias in retrieval models.
- Why unresolved: The paper only explores the immediate effects of this bias and does not provide insights into how this cycle might evolve over extended periods or its ultimate consequences for information retrieval systems.
- What evidence would resolve it: Longitudinal studies tracking the performance and bias of retrieval systems over time as AI-generated content becomes more prevalent in training data.

### Open Question 2
- Question: Can the proposed debiasing method be effectively applied to other types of AI-generated content beyond images?
- Basis in paper: [inferred] The paper focuses on text-image retrieval and AI-generated images, but the concept of invisible relevance bias could potentially apply to other forms of AI-generated content.
- Why unresolved: The paper does not explore the applicability of the debiasing method to other modalities or types of AI-generated content.
- What evidence would resolve it: Experiments applying the debiasing method to text-only retrieval or other modalities like audio or video, and comparing its effectiveness across different types of AI-generated content.

### Open Question 3
- Question: What are the ethical implications of invisible relevance bias in information retrieval systems?
- Basis in paper: [explicit] The paper mentions the risk of users being surrounded by AI-generated images due to the bias, but does not delve into broader ethical considerations.
- Why unresolved: The paper focuses on the technical aspects of the bias and its mitigation, without exploring the potential societal impacts or ethical concerns related to the manipulation of search results.
- What evidence would resolve it: Interdisciplinary studies involving ethicists, social scientists, and information retrieval experts to assess the potential consequences of invisible relevance bias on user behavior, information consumption, and societal discourse.

## Limitations
- The mechanistic explanation of why AI-generated images induce additional embeddings remains partially speculative
- The claim about training data vicious cycles lacks theoretical grounding for why bias amplifies
- The debiasing method's performance on truly out-of-distribution data remains untested

## Confidence
- Invisible relevance bias existence: High
- Mechanism of latent feature embedding: Medium
- Training data vicious cycle: Medium
- Debiasing method effectiveness: Medium

## Next Checks
1. Conduct ablation studies varying the strength of the contrastive debiasing loss to determine optimal bias correction without degrading overall retrieval performance
2. Test the debiased model on datasets with completely different visual domains (e.g., medical images, satellite imagery) to assess generalization of bias correction
3. Perform human evaluation studies comparing relevance rankings between debiased models and standard models to validate that reducing AI preference aligns with human judgment