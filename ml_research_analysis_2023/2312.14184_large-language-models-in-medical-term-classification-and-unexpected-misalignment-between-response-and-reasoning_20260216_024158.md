---
ver: rpa2
title: Large Language Models in Medical Term Classification and Unexpected Misalignment
  Between Response and Reasoning
arxiv_id: '2312.14184'
source_url: https://arxiv.org/abs/2312.14184
tags:
- prompt
- reasoning
- cognitive
- clinical
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated state-of-the-art large language models (LLMs)
  for identifying mild cognitive impairment (MCI) from discharge summaries. Using
  the MIMIC-IV v2.2 database, GPT-4 demonstrated superior performance with an F1 score
  of 0.964 in zero-shot learning, significantly outperforming other models.
---

# Large Language Models in Medical Term Classification and Unexpected Misalignment Between Response and Reasoning

## Quick Facts
- arXiv ID: 2312.14184
- Source URL: https://arxiv.org/abs/2312.14184
- Authors: 
- Reference count: 0
- Key outcome: GPT-4 achieved 0.964 F1 score in zero-shot MCI classification from discharge summaries, significantly outperforming other models, while exhibiting notable reasoning-response inconsistencies.

## Executive Summary
This study evaluates large language models (LLMs) for identifying mild cognitive impairment (MCI) from discharge summaries using the MIMIC-IV v2.2 database. GPT-4 demonstrated superior performance with 0.964 F1 score in zero-shot learning, while fine-tuned models like LLaMA 2 achieved even higher accuracy (0.986 F1). However, GPT-4 exhibited significant inconsistencies between its responses and reasoning, raising concerns about clinical coherence. The research highlights both the potential and limitations of LLMs in healthcare diagnostics, emphasizing the need for improved interpretability and alignment between model outputs and explanations.

## Method Summary
The study used MIMIC-IV v2.2 to extract discharge summaries from patients aged 65+, identifying 870 unique summaries (377 MCI, 493 non-MCI) through ICD code screening followed by expert review. Five prompt variations were tested on GPT-3.5, GPT-4, LLaMA 2, and Falcon models in zero-shot and fine-tuned configurations. Fine-tuning employed 4-bit quantization and LoRA adaptation with 10 epochs, batch size 2, and gradient accumulation step 4. Evaluation metrics included F1 score, recall, precision, and rationalization corrections to address response-reasoning inconsistencies.

## Key Results
- GPT-4 achieved 0.964 F1 score in zero-shot learning, significantly outperforming GPT-3.5 (0.871 F1) and fine-tuned open-source models
- Fine-tuned LLaMA 2 reached 0.986 F1 score, the highest overall performance
- GPT-4 exhibited notable reasoning-response inconsistencies, with some cases showing MCI suggested in reasoning but non-MCI predicted as final output
- Prompt engineering significantly impacted performance, with role-specific and criteria-based prompts achieving superior results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 demonstrates superior precision in identifying MCI compared to GPT-3.5 Turbo and other models when provided with appropriately structured prompts.
- Mechanism: GPT-4's advanced reasoning capabilities allow it to conduct detailed examinations of clinical narratives, identifying relevant data points and making connections that suggest cognitive impairment. The model interprets symptoms and their progression within the clinical context, mirroring human clinical reasoning.
- Core assumption: The structured prompts that specify medical roles and provide methodical instructions elicit more accurate responses from GPT-4 by aligning with its reasoning capabilities.
- Evidence anchors: [abstract]: "GPT-4 demonstrated superior interpretative capabilities, particularly in response to complex prompts, yet displayed notable response-reasoning inconsistencies." [section]: "GPT-4 conducts a detailed examination of clinical narratives, pinpointing and interpreting relevant data points such as memory difficulties, orientation issues, and daily functioning impediments."

### Mechanism 2
- Claim: Fine-tuning open-source models like LLaMA 2 and Falcon significantly improves their performance compared to zero-shot learning approaches.
- Mechanism: Fine-tuning adapts the pre-trained models to the specific domain of MCI identification by updating model parameters on task-specific data, bridging the distribution gap between general pre-training and the specialized medical classification task.
- Core assumption: The training data contains sufficient representative examples of MCI-related clinical narratives to effectively adapt the model's parameters.
- Evidence anchors: [abstract]: "Fine-tuned models like LLaMA 2 and Falcon achieved even higher accuracy, with F1 scores of 0.986 and 0.916, respectively." [section]: "In LLaMA 2 fine-tuning, the number of training epochs is set to 10... These methods collectively greatly enhanced efficiency, making the fine-tuning process more manageable and resource-efficient."

### Mechanism 3
- Claim: Rationalization corrections that align binary predictions with underlying reasoning can significantly improve model performance by addressing inconsistencies between responses and explanations.
- Mechanism: When models produce outputs that contradict their own reasoning, applying a correction step that prioritizes the model's reasoning over its initial binary response resolves these discrepancies and produces more clinically coherent results.
- Core assumption: The model's reasoning process is more reliable than its initial binary prediction, and these inconsistencies are systematic rather than random.
- Evidence anchors: [abstract]: "However, GPT-4 exhibited notable inconsistencies between its responses and reasoning, highlighting the need for further research to ensure clinical coherence." [section]: "By implementing rationalization corrections to align the binary predictions with the associated reasoning, we achieved a significant improvement in F1 performance."

## Foundational Learning

- Concept: Prompt engineering and its impact on LLM performance
  - Why needed here: The study demonstrates that different prompt structures yield vastly different performance outcomes, with role-specific and criteria-focused prompts achieving the best results.
  - Quick check question: Why did prompts specifying medical roles and adhering to diagnostic criteria perform better than creative or complex prompts?

- Concept: Zero-shot vs. fine-tuning approaches in machine learning
  - Why needed here: The study compares these two approaches, showing that while GPT models excel at zero-shot learning, open-source models require fine-tuning to achieve comparable performance.
  - Quick check question: What is the key difference between zero-shot learning and fine-tuning, and why does this matter for medical diagnosis tasks?

- Concept: Model interpretability and the importance of reasoning alignment
  - Why needed here: The study identifies a critical issue where model responses don't align with their reasoning, which is particularly problematic in clinical settings where explainability is essential.
  - Quick check question: Why is it problematic when a model's binary response contradicts its detailed reasoning, especially in medical diagnosis?

## Architecture Onboarding

- Component map: MIMIC-IV extraction -> preprocessing (cleaning, abbreviation expansion, standardization) -> train/validation/test split -> GPT-3.5, GPT-4, LLaMA 2, Falcon models -> prompt engineering (5 variations) -> zero-shot/fine-tuning -> evaluation (F1, recall, precision) -> rationalization corrections

- Critical path: 1. Data preprocessing and annotation 2. Model selection and configuration (zero-shot vs. fine-tuning) 3. Prompt engineering and testing 4. Model training/fine-tuning 5. Evaluation with rationalization corrections 6. Clinical coherence assessment

- Design tradeoffs: Closed-source (GPT) vs. open-source (LLaMA 2, Falcon): GPT provides better reasoning but raises privacy concerns; open-source offers data control but requires fine-tuning. Zero-shot vs. fine-tuning: Zero-shot is faster but less accurate for specialized tasks; fine-tuning is resource-intensive but yields superior performance. Model complexity vs. interpretability: More complex models may achieve higher accuracy but introduce reasoning inconsistencies.

- Failure signatures: High false positive rates indicating reasoning-response misalignment. Inconsistent performance across different prompts for the same model. Poor generalization when training and test data distributions differ. Degradation in performance when clinical narratives contain ambiguous terminology.

- First 3 experiments: 1. Compare zero-shot performance of GPT-4 vs. GPT-3.5 on identical prompts to quantify reasoning capability differences 2. Test different prompt engineering strategies (role specification, criteria-based instructions) on a held-out validation set 3. Apply rationalization corrections to identify and quantify the frequency of reasoning-response inconsistencies across models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific prompt structures influence GPT-4's alignment between responses and reasoning in clinical decision-making tasks?
- Basis in paper: [explicit] The paper explicitly discusses the importance of prompt engineering and notes that discrepancies between responses and reasoning were observed, particularly with prompts 1 and 5 showing higher inconsistency rates.
- Why unresolved: While the paper identifies that certain prompts lead to more inconsistencies, it does not systematically analyze which specific prompt elements (e.g., role specification, detail level, or structure) most significantly impact the alignment between responses and reasoning.
- What evidence would resolve it: A systematic study varying individual prompt components while keeping others constant, measuring the consistency rate between responses and reasoning for each variation, would clarify which elements most affect alignment.

### Open Question 2
- Question: What is the optimal fine-tuning approach for open-source LLMs to achieve both high accuracy and interpretability in medical diagnosis tasks?
- Basis in paper: [inferred] The paper shows that fine-tuned LLaMA 2 and Falcon models achieved high accuracy but lacked explanatory reasoning, suggesting a gap between performance and interpretability that needs addressing.
- Why unresolved: The paper demonstrates that fine-tuning improves accuracy but does not explore whether different fine-tuning techniques, training data compositions, or architectural modifications could enhance interpretability alongside accuracy.
- What evidence would resolve it: Comparative studies testing various fine-tuning strategies (e.g., different data ratios, loss functions, or architectural modifications) while evaluating both accuracy metrics and the quality of generated explanations would identify optimal approaches.

### Open Question 3
- Question: Can hybrid human-LLM collaborative frameworks effectively mitigate response-reasoning inconsistencies in clinical applications?
- Basis in paper: [explicit] The paper proposes developing human-LLM collaborative frameworks where medical experts review and corroborate LLM's initial screening as a potential solution to inconsistency issues.
- Why unresolved: While proposed as a solution, the paper does not provide empirical evidence on whether such frameworks actually reduce inconsistencies or improve overall diagnostic accuracy and clinical coherence.
- What evidence would resolve it: Clinical studies implementing various collaborative frameworks (e.g., expert review of all outputs vs. selective review based on confidence scores) with measured outcomes on consistency rates and diagnostic accuracy would demonstrate their effectiveness.

## Limitations

- The study's generalizability is limited to discharge summaries from a single database, potentially overfitting to MIMIC-IV data patterns and clinical narrative styles.
- The rationalization correction methodology is described but not detailed enough for replication, making it unclear whether corrections consistently improve clinical decision-making.
- Clinical impact assessment is absent, with no evaluation of how these models would perform in real-world diagnostic workflows or their potential to introduce diagnostic bias.

## Confidence

**High Confidence**: The comparative performance of different models (GPT-4 vs. GPT-3.5 vs. fine-tuned LLaMA 2 and Falcon) and the effectiveness of prompt engineering strategies are well-supported by empirical results.

**Medium Confidence**: The observation of response-reasoning misalignment in GPT-4 is supported by examples but lacks systematic quantification across the dataset. The proposed rationalization corrections show promise but are not thoroughly validated for clinical coherence.

**Low Confidence**: The clinical impact assessment is limited, with no evaluation of how these models would perform in real-world diagnostic workflows or their potential to introduce bias in MCI diagnosis.

## Next Checks

1. **Systematic Analysis of Reasoning-Response Inconsistencies**: Conduct a comprehensive audit of GPT-4 outputs to quantify the frequency and types of response-reasoning misalignments across different clinical scenarios, particularly focusing on cases where the model's reasoning suggests MCI but the binary response indicates otherwise.

2. **Cross-Dataset Generalization Testing**: Evaluate model performance on discharge summaries from multiple healthcare systems and time periods to assess whether the high F1 scores (0.964 for GPT-4) hold when the clinical narrative style or terminology changes, addressing concerns about overfitting to MIMIC-IV data.

3. **Clinical Expert Review of Rationalization Corrections**: Engage clinical experts to review cases where rationalization corrections were applied to determine whether the corrected outputs represent improved clinical reasoning or simply represent the model's biases being reinforced, ensuring that the corrections genuinely enhance diagnostic reliability rather than introducing new errors.