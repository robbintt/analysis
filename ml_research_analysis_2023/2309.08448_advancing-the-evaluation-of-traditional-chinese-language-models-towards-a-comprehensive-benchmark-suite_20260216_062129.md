---
ver: rpa2
title: 'Advancing the Evaluation of Traditional Chinese Language Models: Towards a
  Comprehensive Benchmark Suite'
arxiv_id: '2309.08448'
source_url: https://arxiv.org/abs/2309.08448
tags:
- language
- benchmarks
- chinese
- evaluation
- traditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the scarcity of comprehensive benchmarks for
  evaluating Traditional Chinese language models by proposing a new set of benchmarks
  tailored to Traditional Chinese. These benchmarks leverage existing English datasets
  and cover a wide range of tasks, including contextual question-answering, summarization,
  classification, and table understanding.
---

# Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite

## Quick Facts
- arXiv ID: 2309.08448
- Source URL: https://arxiv.org/abs/2309.08448
- Reference count: 8
- Primary result: Proposed benchmarks show Model 7-C achieves performance comparable to GPT-3.5 on certain Traditional Chinese tasks

## Executive Summary
This study addresses the scarcity of comprehensive benchmarks for evaluating Traditional Chinese language models by proposing a new set of benchmarks tailored to Traditional Chinese. These benchmarks leverage existing English datasets and cover a wide range of tasks, including contextual question-answering, summarization, classification, and table understanding. The authors evaluate the performance of GPT-3.5, Taiwan-LLaMa-v1.0, and their proprietary model, Model 7-C, on these benchmarks. The results show that Model 7-C achieves performance comparable to GPT-3.5 on certain tasks, demonstrating the effectiveness of the proposed benchmarks. The authors have open-sourced their benchmark code and datasets to stimulate further research in this field.

## Method Summary
The authors created comprehensive benchmarks for Traditional Chinese language models by translating existing English datasets into Traditional Chinese. They evaluated multiple models (GPT-3.5, Taiwan-LLaMa-v1.0, and Model 7-C) using zero-shot evaluation with greedy decoding across diverse tasks including QA, summarization, classification, and table understanding. Performance was measured using Exact Match for QA and classification tasks, and Rouge-2 for summarization.

## Key Results
- Model 7-C achieves performance comparable to GPT-3.5 on certain Traditional Chinese tasks
- Table understanding tasks revealed significant deficiencies in both Taiwan-LLaMa-v1.0 and Model 7-C with frequent hallucinations
- Summarization tasks showed universally low Rouge-2 scores across all models
- Benchmarks are open-sourced to stimulate further research in Traditional Chinese language modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Leveraging existing English benchmarks and translating them into Traditional Chinese provides a scalable way to create comprehensive evaluation tasks without needing to design new datasets from scratch.
- Mechanism: By using established datasets like XSum, IMDB, and BIG-bench tasks, the authors can focus on translation and adaptation rather than dataset creation, which speeds up benchmark development.
- Core assumption: English datasets can be accurately translated into Traditional Chinese without losing task complexity or nuance.
- Evidence anchors:
  - [abstract] states that the proposed benchmarks "leverage existing English datasets and are tailored to evaluate language models in Traditional Chinese."
  - [section] describes that "we translated the listed English datasets to Traditional Chinese for the evaluation."
- Break condition: If translation introduces errors or cultural context shifts that alter task difficulty or meaning.

### Mechanism 2
- Claim: The proposed benchmarks enable direct comparison of proprietary and open-source models on the same evaluation tasks.
- Mechanism: By applying the same translated datasets to multiple models (GPT-3.5, Taiwan-LLaMa-v1.0, and Model 7-C), the study can fairly assess relative performance across tasks.
- Core assumption: All models are evaluated under identical zero-shot conditions with greedy decoding to ensure fairness.
- Evidence anchors:
  - [section] notes "We carry out all evaluation zero-shot and use greedy decoding for a fair comparison."
  - [section] presents a table comparing the models' performance on each dataset.
- Break condition: If models are not truly comparable due to differences in training data or architecture that affect zero-shot performance.

### Mechanism 3
- Claim: The benchmarks reveal specific weaknesses in model performance, such as poor table understanding and summarization, which can guide future research.
- Mechanism: By testing on diverse tasks like contextual QA, classification, and table understanding, the benchmarks expose areas where models struggle, as seen in low EM scores for table tasks.
- Core assumption: Low scores in certain tasks reflect genuine model limitations rather than benchmark design flaws.
- Evidence anchors:
  - [section] states that "the table understanding task reveals a discernible deficiency in both Taiwan-LLaMa-v1.0 and Model 7-C, with frequent hallucinations evident in numerous samples."
  - [section] mentions "summarization tasks delineated suboptimal results; even though the models were instructed to condense the context into a single concise sentence, they demonstrated low Rouge-2 scores universally."
- Break condition: If poor performance is due to unclear instructions or ambiguous dataset examples rather than model inability.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The benchmarks are evaluated zero-shot to fairly compare models without fine-tuning on the task data.
  - Quick check question: What does zero-shot mean in the context of language model evaluation?

- Concept: Exact Match (EM) metric
  - Why needed here: EM is used to measure how well models match the exact ground truth answer, important for QA and classification tasks.
  - Quick check question: How is Exact Match different from metrics like Rouge-2?

- Concept: Translation of datasets
  - Why needed here: Translating English datasets to Traditional Chinese is necessary because few high-quality Chinese benchmarks exist.
  - Quick check question: Why might translating a dataset introduce evaluation challenges?

## Architecture Onboarding

- Component map:
  - Dataset translation pipeline (English â†’ Traditional Chinese) -> Benchmark evaluation harness (zero-shot inference) -> Model comparison framework (performance metrics aggregation) -> Open-source release package (code + datasets)

- Critical path:
  1. Translate benchmark datasets to Traditional Chinese
  2. Run zero-shot evaluations on all models
  3. Compute metrics (EM, Rouge-2)
  4. Compare and analyze results
  5. Open-source outputs

- Design tradeoffs:
  - Translation vs. native dataset creation (speed vs. cultural relevance)
  - Zero-shot vs. fine-tuning (fairness vs. potential performance gains)
  - Broad task coverage vs. depth in each task (comprehensiveness vs. detailed analysis)

- Failure signatures:
  - Large performance gaps across models suggest possible benchmark bias or dataset issues.
  - Consistently low scores in a task may indicate unclear instructions or dataset noise.
  - Translation artifacts causing model confusion or incorrect answers.

- First 3 experiments:
  1. Translate a small sample of XSum to Traditional Chinese and run zero-shot summarization on GPT-3.5.
  2. Evaluate IMDB-TC classification task on Taiwan-LLaMa-v1.0 and compare with GPT-3.5.
  3. Test table understanding task with Penguins-in-a-Table-TC on all models and analyze hallucination patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed benchmarks perform in evaluating language models' capabilities in tasks beyond those explicitly tested, such as code generation or creative writing?
- Basis in paper: [inferred]
- Why unresolved: The paper focuses on evaluating language models' capabilities in specific tasks like contextual question-answering, summarization, classification, and table understanding. It does not explore the performance of the proposed benchmarks in evaluating other tasks, such as code generation or creative writing.
- What evidence would resolve it: Conducting additional experiments using the proposed benchmarks to evaluate language models' performance in tasks like code generation or creative writing would provide evidence to answer this question.

### Open Question 2
- Question: How do the proposed benchmarks compare to existing benchmarks in terms of their ability to capture the nuances of Traditional Chinese language and culture?
- Basis in paper: [inferred]
- Why unresolved: The paper introduces a new set of benchmarks tailored to Traditional Chinese language models but does not provide a direct comparison with existing benchmarks in terms of their ability to capture the nuances of Traditional Chinese language and culture.
- What evidence would resolve it: Conducting a comparative study between the proposed benchmarks and existing benchmarks, focusing on their ability to capture the nuances of Traditional Chinese language and culture, would provide evidence to answer this question.

### Open Question 3
- Question: How do the proposed benchmarks perform in evaluating language models' capabilities in handling domain-specific knowledge, such as legal or medical terminology?
- Basis in paper: [inferred]
- Why unresolved: The paper evaluates language models' capabilities in various tasks but does not specifically address their performance in handling domain-specific knowledge, such as legal or medical terminology.
- What evidence would resolve it: Conducting additional experiments using the proposed benchmarks to evaluate language models' performance in handling domain-specific knowledge, such as legal or medical terminology, would provide evidence to answer this question.

## Limitations
- Translation-based approach may introduce subtle errors or cultural misalignments affecting model performance
- Zero-shot evaluation may underestimate model capabilities compared to task-specific training
- Current benchmarks may not fully capture the linguistic complexity of Traditional Chinese

## Confidence
- Claim: Translation methodology introduces uncertainty about whether performance differences reflect genuine model capabilities or translation artifacts. **Confidence: Medium**
- Claim: Benchmarks reveal specific weaknesses in table understanding and summarization with clear quantitative evidence. **Confidence: High**
- Claim: Model 7-C achieves performance comparable to GPT-3.5 on certain tasks. **Confidence: Medium**

## Next Checks
1. Conduct a human evaluation study comparing the quality and meaning of translated benchmark datasets against their English originals to identify potential translation-induced artifacts.
2. Perform fine-tuning experiments on select benchmarks to determine whether zero-shot evaluation underestimates model capabilities compared to task-specific training.
3. Develop a cross-lingual consistency check by evaluating the same models on both English and Traditional Chinese versions of benchmark tasks to identify performance gaps attributable to translation quality versus genuine language modeling differences.