---
ver: rpa2
title: 'LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning'
arxiv_id: '2308.03303'
source_url: https://arxiv.org/abs/2308.03303
tags:
- fine-tuning
- lora-fa
- lora
- memory
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LoRA-FA, a memory-efficient fine-tuning method
  for large language models (LLMs) that reduces activation memory consumption without
  sacrificing performance. Unlike traditional LoRA, which requires storing large input
  activations, LoRA-FA freezes the projection-down weight matrix A and only updates
  the projection-up weight matrix B, significantly reducing the activation memory
  needed.
---

# LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning

## Quick Facts
- arXiv ID: 2308.03303
- Source URL: https://arxiv.org/abs/2308.03303
- Reference count: 18
- Reduces activation memory consumption by up to 1.4× while maintaining performance

## Executive Summary
LoRA-FA introduces a memory-efficient fine-tuning method for large language models by freezing the projection-down weight matrix A and only updating the projection-up weight matrix B. This approach significantly reduces activation memory requirements during back-propagation while maintaining the low-rank adaptation space. The method achieves comparable fine-tuning accuracy to full-parameter fine-tuning and standard LoRA across various tasks and model types including RoBERTa, T5, and LLaMA, with overall memory cost reductions of up to 1.4×.

## Method Summary
LoRA-FA modifies the standard LoRA approach by freezing the projection-down weight matrix A while keeping the projection-up weight matrix B trainable. During forward pass, activations are computed as Y = XW + XAB, but during backward pass, only gradients for B are computed using stored XA activations rather than full-rank input X. This eliminates the need to store large input activations, reducing memory consumption. The method maintains weight changes in a low-rank space (∆W = AB) and is mathematically equivalent to low-rank gradient compression applied to full-parameter fine-tuning.

## Key Results
- Reduces activation memory consumption by up to 1.4× compared to standard LoRA
- Maintains close fine-tuning accuracy to full-parameter fine-tuning and LoRA across GLUE, WMT16 En-Ro, MMLU, Alpaca, and FLAN v2 tasks
- Successfully applied to RoBERTa, T5, and LLaMA models with rank sizes up to 128

## Why This Works (Mechanism)

### Mechanism 1
Freezing matrix A eliminates the need to store full-rank input activations during back-propagation. By only computing gradients with respect to B, the method requires storing only the low-rank product XA instead of the full input X, significantly reducing memory usage.

### Mechanism 2
The change in model weights remains in a low-rank space during fine-tuning. Since A is initialized as a low-rank matrix and frozen, the weight change ∆W = AB stays within the column space defined by A, preserving the low-rank adaptation property.

### Mechanism 3
LoRA-FA is equivalent to low-rank gradient compression applied to full-parameter fine-tuning. The gradient computation dB = AT dW compresses the full gradient into a lower-dimensional space, then B is updated using this compressed gradient.

## Foundational Learning

- **Low-rank matrix decomposition**: Understanding how AB represents a low-rank approximation of weight changes is fundamental to grasping LoRA-FA's approach
  - Quick check: If A ∈ R^(d×r) and B ∈ R^(r×d) with r << d, what is the rank of AB?

- **Backpropagation and activation memory**: The key innovation is reducing activation memory by changing which activations need to be stored during the backward pass
  - Quick check: In standard LoRA, which activations must be stored for gradient computation versus LoRA-FA?

- **QR decomposition and orthogonal bases**: The paper uses QR decomposition to show that weight changes stay in the column space of A
  - Quick check: If A = QR where Q has orthonormal columns, what property does Q preserve about the column space of A?

## Architecture Onboarding

- **Component map**: Input -> Linear layer with frozen weight W -> LoRA-FA adapter (frozen A, trainable B) -> Activation (GeLU) -> Layer normalization -> Multi-head attention
- **Critical path**: Forward pass computes Y = XW + XAB, backward pass computes gradients only for B using stored XA activations
- **Design tradeoffs**: Memory vs. expressivity (freezing A reduces memory but may limit adaptation capacity), initialization sensitivity (random initialization of A must produce useful low-rank basis), gradient quality (compressed gradients may affect optimization dynamics)
- **Failure signatures**: Poor convergence despite proper hyperparameter tuning, unexpectedly high memory usage, significantly worse performance than standard LoRA
- **First 3 experiments**: 1) Implement LoRA-FA on single linear layer with synthetic data, verify gradients flow only to B and memory usage matches expectations 2) Compare convergence curves of LoRA-FA vs. full LoRA on small LLM fine-tuning task 3) Profile memory usage during fine-tuning with different rank values to confirm activation memory reduction claims

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations and scope of the work, several important questions remain unanswered.

## Limitations
- The paper focuses exclusively on memory reduction without comprehensive performance degradation analysis across diverse tasks and model architectures
- The comparison against full LoRA is somewhat self-serving given LoRA-FA is presented as a memory-efficient variant
- The 1.4× memory reduction claim needs independent verification across different hardware configurations and batch sizes

## Confidence
- **High confidence**: The memory reduction mechanism is mathematically sound and the theoretical framework is rigorous
- **Medium confidence**: The empirical results showing comparable performance to LoRA, as the evaluation covers multiple tasks but with limited model diversity
- **Medium confidence**: The claimed 1.4× memory reduction, as the measurement methodology is not fully specified

## Next Checks
1. **Memory Profiling Validation**: Conduct detailed memory profiling across different batch sizes and hardware configurations to verify the 1.4× reduction claim, specifically measuring activation memory savings during backpropagation.

2. **Cross-Architecture Generalization**: Test LoRA-FA on additional LLM architectures beyond RoBERTa, T5, and LLaMA (such as GPT-style models) to assess whether the frozen A matrix performs consistently across different transformer designs.

3. **A Matrix Sensitivity Analysis**: Systematically evaluate how different initialization strategies and rank values for matrix A affect both memory savings and fine-tuning performance, particularly for tasks requiring significant parameter adaptation.