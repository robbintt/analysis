---
ver: rpa2
title: Breaking the Entanglement of Homophily and Heterophily in Semi-supervised Node
  Classification
arxiv_id: '2312.04111'
source_url: https://arxiv.org/abs/2312.04111
tags:
- directed
- node
- graph
- learning
- undirected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of semi-supervised node classification
  on graphs where homophily and heterophily are entangled. The authors argue that
  existing GNNs are limited by their focus on undirected graphs, neglecting the rich
  information in directed edges.
---

# Breaking the Entanglement of Homophily and Heterophily in Semi-supervised Node Classification

## Quick Facts
- arXiv ID: 2312.04111
- Source URL: https://arxiv.org/abs/2312.04111
- Reference count: 40
- Key outcome: Proposed method (AMUD + ADPA) outperforms 16 baselines by 3.96% on average on 14 benchmark datasets

## Executive Summary
This paper addresses the challenge of semi-supervised node classification on graphs where homophily and heterophily are entangled, particularly focusing on the limitations of existing GNNs in handling directed edges. The authors argue that the neglect of directed edges results in sub-optimal graph representations and propose a new framework called AMUD that quantifies the correlation between node profiles and topology to guide whether to model directed graphs as undirected or directed. They also introduce ADPA, a new digraph learning paradigm that adaptively discovers personalized directed patterns and uses hierarchical attention mechanisms to aggregate multi-scale messages.

## Method Summary
The proposed method consists of two main components: AMUD and ADPA. AMUD uses Pearson correlation coefficients to quantify the relationship between directed topology and node profiles, deciding whether to preserve directed edges or transform to undirected based on a threshold. ADPA then generates k-order directed pattern operators and performs K-step feature propagation, using two levels of node-adaptive attention (DP attention and hop attention) to fuse multi-granularity representations. The feature propagation is decoupled from training as a pre-processing step to improve computational efficiency.

## Key Results
- ADPA outperforms 16 baseline methods by 3.96% on average across 14 benchmark datasets
- AMUD guidance helps identify whether to preserve directed edges or transform to undirected, with R² scores > 0.5 for homophilous datasets and < 0.5 for heterophilous datasets
- The method achieves SOTA performance on datasets like CoraML, CiteSeer, PubMed, WikiCS, and Amazon-computers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modeling directed edges breaks the homophily-heterophily entanglement by preserving structural information lost in undirected transformations.
- Mechanism: The AMUD framework quantifies the correlation between directed topology and node profiles using Pearson correlation coefficients. When this correlation is high, it preserves directed edges; otherwise, it transforms to undirected.
- Core assumption: Directed edges contain discriminative information about homophily vs heterophily that is obscured when edges are made undirected.
- Evidence anchors:
  - [abstract] "The neglect of directed edges results in sub-optimal graph representations, thereby hindering the capacity of GNNs."
  - [section] "To address this issue, we introduce AMUD, which quantifies the relationship between node profiles and topology from a statistical perspective"
  - [corpus] Weak - corpus neighbors focus on heterophily but don't directly address directed edge modeling
- Break condition: If the correlation between directed topology and node profiles is low for both homophilous and heterophilous datasets, AMUD's guidance becomes ambiguous.

### Mechanism 2
- Claim: Adaptive Directed Pattern Aggregation (ADPA) improves performance by discovering personalized directed patterns and using hierarchical attention mechanisms.
- Mechanism: ADPA generates k-order directed pattern (DP) operators and performs K-step feature propagation. It then uses two levels of node-adaptive attention (DP attention and hop attention) to fuse multi-granularity representations.
- Core assumption: Different nodes require different combinations of directed patterns and propagation steps for optimal representation.
- Evidence anchors:
  - [abstract] "we propose Adaptive Directed Pattern Aggregation (ADPA) as a new directed graph learning paradigm for AMUD"
  - [section] "we propose two hierarchical node-adaptive attention mechanisms to finely fusion multi-granularity node representations"
  - [corpus] Missing - corpus doesn't contain evidence about ADPA's specific mechanisms
- Break condition: If the attention mechanisms fail to learn meaningful weights for pattern aggregation, ADPA's performance gain disappears.

### Mechanism 3
- Claim: The decoupling of feature propagation from training in ADPA significantly reduces computational complexity while maintaining performance.
- Mechanism: Feature propagation using k-order DP operators is performed as a pre-processing step independent of training, using sparse-dense matrix multiplication.
- Core assumption: The expensive feature propagation can be separated from the training phase without sacrificing model quality.
- Evidence anchors:
  - [section] "Since ADPA utilizes k-order DP in the feature propagation, the overall time complexity is O(kKmf ). Notably, this process is independent of the training phase"
  - [section] "Similar design principles have been proven to significantly enhance computational efficiency without compromising predictive performance"
  - [corpus] Missing - corpus doesn't address computational efficiency aspects
- Break condition: If the pre-computed features become stale or if the training requires dynamic feature updates, the decoupling strategy fails.

## Foundational Learning

- Concept: Pearson correlation coefficient
  - Why needed here: Used to quantify the relationship between directed topology and node profiles in AMUD
  - Quick check question: What does a Pearson correlation coefficient close to 1 or -1 indicate about the relationship between two variables?

- Concept: Graph neural networks and message passing
  - Why needed here: ADPA builds upon GNN principles but extends them to directed graphs with adaptive pattern discovery
  - Quick check question: How does the message-passing mechanism in GNNs differ when applied to directed vs undirected graphs?

- Concept: Attention mechanisms in neural networks
  - Why needed here: Two hierarchical attention mechanisms in ADPA are crucial for fusing multi-granularity node representations
  - Quick check question: What is the key difference between node-wise DP attention and node-wise hop attention in ADPA's architecture?

## Architecture Onboarding

- Component map:
  - AMUD: Topological modeling guidance (Pearson correlation calculation, decision threshold)
  - ADPA: Directed pattern discovery (k-order DP operators), feature propagation (K-step), hierarchical attention (DP attention + hop attention), MLP classifier
  - Data flow: Input graph → AMUD decision (undirected/directed) → ADPA processing → predictions

- Critical path:
  1. Compute Pearson correlation between directed topology and node profiles
  2. Apply threshold to decide undirected vs directed modeling
  3. Generate k-order DP operators based on the decision
  4. Perform K-step feature propagation using DP operators
  5. Apply hierarchical attention mechanisms to fuse representations
  6. Feed fused representations to MLP classifier

- Design tradeoffs:
  - Choosing k (number of DP operators) vs K (propagation steps): Higher k captures more patterns but increases complexity; higher K captures more neighborhood information but risks over-smoothing
  - Pre-computation vs on-the-fly computation: Pre-computing feature propagation saves training time but may miss dynamic updates
  - Attention mechanism choice: Different attention mechanisms (gate, recursive, JK) perform differently on homophilous vs heterophilous datasets

- Failure signatures:
  - Poor performance despite correct implementation: Likely issues with DP operator selection or attention mechanism configuration
  - High variance in results: Possible overfitting due to insufficient regularization or too many learnable parameters
  - Slow convergence: May indicate learning rate issues or need for better initialization strategies

- First 3 experiments:
  1. Run AMUD on a small directed dataset (like CoraML) and verify it correctly identifies whether to preserve directed edges or transform to undirected
  2. Implement 1-hop and 2-hop DP operators and validate that they capture expected structural patterns in a toy directed graph
  3. Test the two hierarchical attention mechanisms separately on a small dataset to understand their individual contributions to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed AMUD framework perform in comparison to existing homophily metrics (such as Hnode, Hedge, Hclass, H adj, and LI) when applied to directed graphs with entangled homophily and heterophily?
- Basis in paper: [explicit] The paper introduces AMUD as a new metric that quantifies the correlation between node profiles and topology in directed graphs, aiming to address the limitations of existing homophily metrics.
- Why unresolved: The paper does not provide a direct comparison between AMUD and existing homophily metrics on a variety of directed graphs with different levels of homophily and heterophily.
- What evidence would resolve it: Conduct experiments comparing the performance of AMUD and existing homophily metrics on a diverse set of directed graphs, evaluating their ability to guide graph-based data engineering and model selection.

### Open Question 2
- Question: What is the impact of different directed pattern (DP) operators on the performance of the proposed ADPA model, and how can we identify the most effective DP operators for a given graph dataset?
- Basis in paper: [explicit] The paper introduces the concept of DP operators and their role in capturing multi-scale structural information in directed graphs. It also presents an ablation study on different k-order DP operators.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of different DP operators on ADPA's performance across various graph datasets. It also does not offer a clear guideline for selecting the most effective DP operators.
- What evidence would resolve it: Conduct extensive experiments on a wide range of graph datasets, systematically varying the DP operators used in ADPA. Analyze the results to identify patterns and provide recommendations for selecting the most effective DP operators based on graph characteristics.

### Open Question 3
- Question: How does the proposed ADPA model perform in comparison to other state-of-the-art directed graph neural networks (such as DGCN, NSTE, DIMPA, DirGNN, and A2DUG) on various graph benchmark datasets?
- Basis in paper: [explicit] The paper presents a comprehensive comparison of ADPA with several baselines, including state-of-the-art directed GNNs, on 14 graph benchmark datasets.
- Why unresolved: While the paper provides a comparison, it does not offer a detailed analysis of the strengths and weaknesses of ADPA in relation to each baseline method. It also does not explore the impact of different graph characteristics on the relative performance of ADPA and the baselines.
- What evidence would resolve it: Conduct a thorough analysis of the experimental results, focusing on the performance differences between ADPA and each baseline method across various graph datasets. Investigate the factors that contribute to these differences, such as graph size, density, and the level of homophily or heterophily.

## Limitations
- Lack of detailed implementation specifications for Pearson correlation calculation in AMUD and specific MLP architecture in ADPA's attention mechanisms
- Absence of ablation studies that isolate contributions of AMUD guidance versus ADPA's hierarchical attention mechanisms
- Fixed threshold (θ=0.5) for AMUD's decision-making may not generalize well across diverse graph structures

## Confidence
- **High confidence**: The general framework of using Pearson correlation to guide graph modeling direction (undirected vs directed) is theoretically sound and supported by statistical literature
- **Medium confidence**: The claim that ADPA outperforms baselines by 3.96% on average, as this is based on reported experimental results without detailed methodology or ablation studies provided
- **Low confidence**: The specific computational efficiency gains claimed from decoupling feature propagation from training, as no empirical runtime comparisons are provided against baseline methods

## Next Checks
1. **Replicate AMUD's Pearson correlation guidance**: Implement the AMUD framework on a small directed dataset (e.g., CoraML) and verify that the correlation-based decision correctly identifies whether to preserve directed edges or transform to undirected, comparing against the authors' claimed threshold of θ=0.5.

2. **Ablation study of ADPA components**: Conduct controlled experiments to isolate the contributions of (a) the AMUD guidance decision, (b) the k-order DP operators, (c) the DP attention mechanism, and (d) the hop attention mechanism, to quantify their individual impact on performance across both homophilous and heterophilous datasets.

3. **Computational efficiency validation**: Measure and compare the training time and inference latency of ADPA against the fastest baseline methods (e.g., GraphSAGE, GAT) on datasets of varying sizes, focusing on the claimed benefit of pre-computed feature propagation independent of the training phase.