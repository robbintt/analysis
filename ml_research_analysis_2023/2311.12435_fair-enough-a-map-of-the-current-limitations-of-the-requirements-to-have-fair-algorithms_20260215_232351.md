---
ver: rpa2
title: Fair Enough? A map of the current limitations of the requirements to have fair
  algorithms
arxiv_id: '2311.12435'
source_url: https://arxiv.org/abs/2311.12435
tags:
- discrimination
- fairness
- more
- causal
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work critically examines the challenges of defining and implementing
  "fair" algorithms in Automated Decision-Making systems. While significant progress
  has been made in developing mathematical metrics to assess fairness, the authors
  argue that these metrics are often insufficient and can even be misleading.
---

# Fair Enough? A map of the current limitations of the requirements to have fair algorithms

## Quick Facts
- arXiv ID: 2311.12435
- Source URL: https://arxiv.org/abs/2311.12435
- Reference count: 40
- Primary result: Current requirements for "fair" algorithms are too vague and require further societal discussion and consensus before effective implementation

## Executive Summary
This critical analysis examines why the requirement for "fair" algorithms in Automated Decision-Making (ADM) systems remains fundamentally problematic. While significant progress has been made in developing mathematical fairness metrics, the authors argue these technical solutions are insufficient without resolving deeper societal questions about which attributes should be protected and what constitutes unfair discrimination. The paper highlights that purely observational metrics can be misleading as they fail to capture underlying causal mechanisms of discrimination.

The authors emphasize that fairness is not purely a technical problem but requires collaboration among statisticians, AI experts, ethicists, and legal experts to develop a shared understanding of fairness requirements. They identify key ambiguities including how to define sensitive attributes, the limitations of causal reasoning approaches, and the threshold problem in fairness metrics. The work calls for recognizing that having "fair" algorithms is per se a nearly meaningless requirement without societal consensus on the fundamental questions of what should be protected and what constitutes unjust discrimination.

## Method Summary
The paper provides a critical analysis of fairness requirements in ADM systems, focusing on theoretical and conceptual limitations rather than empirical experimentation. It reviews existing approaches to fairness assessment, identifies key ambiguities in defining fairness requirements, and proposes attention points for stakeholders. The methodology involves synthesizing perspectives from multiple disciplines including statistics, AI, ethics, and law to map the current landscape of fairness requirements and their limitations.

## Key Results
- Purely observational fairness metrics are insufficient as they cannot distinguish between different types of discrimination effects without understanding causal mechanisms
- The vagueness of "fair" algorithms stems from unresolved societal choices about protected attributes and definitions of unfair discrimination
- The threshold problem in fairness metrics creates ambiguity about what level of disparity is acceptable for practical implementation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair-ML fairness metrics are insufficient because they fail to capture the underlying causal mechanisms of discrimination.
- Mechanism: The paper demonstrates that purely observational metrics cannot distinguish between direct and indirect discrimination, or identify spurious correlations, without understanding the causal structure of the data generation process.
- Core assumption: Causal relationships exist and can be meaningfully modeled for human attributes in automated decision-making systems.
- Evidence anchors:
  - [abstract] "they highlight key ambiguities in defining sensitive attributes, understanding what constitutes unfair discrimination, and the limitations of causal reasoning approaches."
  - [section] "Another drawback of purely observational metrics is that they are blind with respect to the underlying generation mechanisms"
  - [corpus] Weak - corpus does not contain relevant papers on causal fairness
- Break condition: If causal relationships among human attributes cannot be reliably identified or modeled, or if the assumptions underlying causal inference are invalid for social phenomena.

### Mechanism 2
- Claim: The vagueness of "fair" algorithms stems from unresolved societal choices about which attributes to protect and what constitutes unfair discrimination.
- Mechanism: The paper argues that "fairness" requires societal consensus on protected attributes and definitions of unfair discrimination, which are currently absent, making technical solutions insufficient.
- Core assumption: Fairness is fundamentally a social construct that cannot be fully resolved through technical means alone.
- Evidence anchors:
  - [abstract] "the authors argue that these metrics are often insufficient and can even be misleading"
  - [section] "what is still fundamentally missing is the awareness that having 'fair' algorithms is per se a nearly meaningless requirement"
  - [corpus] Weak - corpus does not contain relevant papers on societal aspects of fairness
- Break condition: If society reaches consensus on protected attributes and definitions of unfair discrimination, reducing the ambiguity the paper identifies.

### Mechanism 3
- Claim: The threshold problem in fairness metrics creates ambiguity about what level of disparity is acceptable.
- Mechanism: The paper identifies that observational metrics produce real-valued measures, but practitioners lack guidance on what numerical thresholds constitute "fair enough," leading to inconsistent application.
- Core assumption: Practitioners need quantitative thresholds to operationalize fairness requirements.
- Evidence anchors:
  - [abstract] "the authors emphasize the need for a multidisciplinary approach involving statisticians, AI experts, ethicists, and legal experts"
  - [section] "The threshold problem. Once practitioners have computed the values of observational metrics, they face the problem of 'deciding' whether the found numbers are enough to raise a warning or not."
  - [corpus] Weak - corpus does not contain relevant papers on fairness thresholds
- Break condition: If clear, widely-accepted thresholds for fairness metrics are established through regulatory frameworks or industry standards.

## Foundational Learning

- Concept: Causal inference and counterfactual reasoning
  - Why needed here: The paper extensively discusses how causal frameworks can distinguish between different types of discrimination effects (direct, indirect, spurious) that observational metrics cannot detect
  - Quick check question: Can you explain the difference between the three types of effects (direct, indirect, spurious) in causal fairness analysis?

- Concept: Intersectionality in fairness assessment
  - Why needed here: The paper highlights that evaluating discrimination along single sensitive attributes separately can miss intersectional biases that occur when multiple protected characteristics interact
  - Quick check question: Why does the number of subgroups to evaluate grow exponentially with the number of sensitive attributes?

- Concept: Legal frameworks for non-discrimination
  - Why needed here: The paper references various legal definitions (EU Charter, US Civil Rights Act) and distinguishes between direct/indirect discrimination, which informs how fairness should be assessed
  - Quick check question: What is the difference between direct discrimination and disparate impact in US law?

## Architecture Onboarding

- Component map: Statisticians -> AI experts -> Ethicists -> Legal experts -> Policy makers
- Critical path: 1) Define protected attributes and discrimination criteria 2) Collect appropriate data 3) Choose and compute fairness metrics 4) Assess causal mechanisms 5) Implement mitigation if needed
- Design tradeoffs: Balancing statistical parity with business necessity, individual vs group fairness, precision of causal inference vs practical implementation constraints
- Failure signatures: Over-reliance on single metrics, ignoring intersectional effects, assuming causal relationships without validation, setting arbitrary thresholds
- First 3 experiments:
  1. Implement demographic parity calculation on a binary classification dataset with known sensitive attribute
  2. Create a causal graph for a simple loan approval scenario and compute direct vs indirect effects
  3. Simulate an intersectional bias scenario and compare single-attribute vs multi-attribute fairness assessments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a consensus on which human attributes should be protected from discrimination, and what criteria should be used to define these attributes?
- Basis in paper: [explicit] The paper highlights the ambiguity in defining sensitive attributes, stating "It is not that discrimination per se is unjust —this would be trivial and would somehow throw away the entire (data-driven) decision-making paradigm— only discrimination with respect to some attributes that we have either to list and agree upon or to define with some reasonable criterion."
- Why unresolved: The paper emphasizes that no mathematical model can determine which characteristics should be protected from discrimination, as this is ultimately a societal decision. There is no clear consensus on what constitutes a sensitive attribute, and the criteria for defining these attributes are often subjective and context-dependent.
- What evidence would resolve it: A comprehensive study involving diverse stakeholders (e.g., ethicists, legal experts, sociologists, and representatives from affected communities) could provide insights into the societal values and priorities that should guide the selection of sensitive attributes. This could involve surveys, focus groups, and Delphi studies to reach a consensus on the most relevant and justifiable attributes for protection.

### Open Question 2
- Question: What are the most effective strategies for collecting sensitive data while ensuring privacy and addressing the reluctance of individuals to share personal information?
- Basis in paper: [explicit] The paper raises the issue of sensitive data collection, stating "Should developers of ADM systems keep track of all the sensitive attributes that they wouldn’t otherwise record, for the sole purpose of assessing unjust discrimination with respect to those attributes?" and noting that "Buyl and De Bie (2022), sensitive data collection may not only be morally unacceptable, but also extremely unlikely to occur because people are typically reluctant to share personal information."
- Why unresolved: While the paper acknowledges the need for sensitive data to assess discrimination, it also recognizes the ethical and practical challenges of collecting such data. There is a tension between the need for data and the right to privacy, and current methods for collecting sensitive data may not be sufficient or acceptable to individuals.
- What evidence would resolve it: Research into innovative data collection methods that prioritize privacy and transparency could help address this issue. This could involve exploring techniques such as differential privacy, federated learning, or synthetic data generation. Additionally, studies on public attitudes towards data sharing and privacy could inform the development of more acceptable and effective data collection strategies.

### Open Question 3
- Question: How can we develop more robust and reliable fairness metrics that account for the limitations of observational data and the complexities of causal relationships?
- Basis in paper: [explicit] The paper discusses the limitations of observational fairness metrics, stating "Purely observational fairness metrics should be taken with a grain of salt. They can and should be used as a means for a deeper reasoning on the mechanisms underlying a specific observational distribution, rather than a final word on the presence or lack of unjust discrimination."
- Why unresolved: The paper highlights the challenges of relying solely on observational data to assess fairness, as these metrics may not capture the underlying causal mechanisms that contribute to discrimination. Additionally, the choice of fairness metric and the threshold for determining fairness are often subjective and context-dependent.
- What evidence would resolve it: Research into causal inference methods that can be applied to fairness assessment could help address this issue. This could involve developing techniques for estimating causal effects from observational data, or exploring the use of counterfactual reasoning to identify potential sources of discrimination. Additionally, studies on the relationship between different fairness metrics and their implications for real-world decision-making could inform the development of more robust and reliable assessment tools.

## Limitations
- The paper relies heavily on theoretical frameworks without extensive empirical validation of its claims, particularly regarding causal inference approaches to fairness
- The discussion of intersectionality effects is theoretically sound but lacks concrete examples demonstrating the exponential complexity in practice
- The call for multidisciplinary collaboration is compelling but underspecified regarding how competing stakeholder interests would be reconciled

## Confidence
- High Confidence: Claims about the insufficiency of purely observational fairness metrics and the importance of causal reasoning are well-supported by existing literature and theoretical arguments
- Medium Confidence: The assertion that societal consensus is fundamentally required for meaningful fairness definitions, while logically sound, lacks empirical evidence of how such consensus might be achieved in practice
- Low Confidence: Specific numerical thresholds for fairness metrics are dismissed as problematic, but the paper doesn't provide concrete alternatives for practitioners facing real-world implementation challenges

## Next Checks
1. **Empirical Causal Validation**: Test whether causal inference methods (e.g., mediation analysis) can reliably distinguish between direct, indirect, and spurious discrimination effects in real-world datasets with known bias patterns.
2. **Threshold Sensitivity Analysis**: Systematically evaluate how different threshold values for fairness metrics affect downstream outcomes across multiple real-world applications to assess the practical impact of the "threshold problem."
3. **Stakeholder Consensus Experiment**: Conduct structured workshops with diverse stakeholders (legal experts, data scientists, affected communities) to attempt defining concrete fairness criteria for a specific use case and document where consensus breaks down.