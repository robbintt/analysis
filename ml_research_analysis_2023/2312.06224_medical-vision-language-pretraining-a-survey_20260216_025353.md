---
ver: rpa2
title: 'Medical Vision Language Pretraining: A survey'
arxiv_id: '2312.06224'
source_url: https://arxiv.org/abs/2312.06224
tags:
- medical
- image
- text
- learning
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews medical Vision-Language Pretraining
  (VLP), a self-supervised learning approach addressing the scarcity of labeled data
  in the medical domain. VLP leverages paired or unpaired vision (images, videos)
  and text (reports, captions) datasets to acquire vast knowledge and learn robust
  feature representations.
---

# Medical Vision Language Pretraining: A survey

## Quick Facts
- arXiv ID: 2312.06224
- Source URL: https://arxiv.org/abs/2312.06224
- Reference count: 40
- One-line primary result: Comprehensive survey categorizing medical VLP approaches by pretraining objectives, architectures, and downstream tasks

## Executive Summary
This survey comprehensively reviews medical Vision-Language Pretraining (VLP), a self-supervised learning approach addressing the scarcity of labeled data in the medical domain. VLP leverages paired or unpaired vision (images, videos) and text (reports, captions) datasets to acquire vast knowledge and learn robust feature representations. The survey categorizes pretraining objectives into masked prediction, contrastive learning, matching prediction, and hybrid approaches, analyzing their strengths and limitations. It also discusses model architectures (CNNs, ViTs), modality fusion techniques, and downstream evaluation tasks like classification, segmentation, object detection, retrieval, report generation, and visual question answering.

## Method Summary
The survey methodology involved collecting 74 medical VLP papers from 2018 onwards through Google Scholar, PubMed, and IEEE Xplore. Papers were categorized by pretraining objectives (masked prediction, contrastive learning, matching prediction, hybrid approaches), model architectures (CNNs, ViTs), modality fusion techniques, and downstream tasks. The survey synthesizes findings into a comprehensive structure covering pretraining objectives, architectures, datasets, downstream tasks, challenges, and future directions.

## Key Results
- Masked prediction pretraining objectives enable zero-shot downstream task performance by reconstructing masked inputs
- Hybrid pretraining objectives combining masked prediction and contrastive learning lead to improved downstream performance
- Contrastive learning pretraining objectives maximize similarity between paired data and minimize similarity between unpaired data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masked prediction pretraining objectives allow models to learn robust feature representations by reconstructing masked inputs, enabling zero-shot downstream task performance.
- Mechanism: The model learns to predict or reconstruct masked tokens in either the text or image modality, forcing it to learn contextual relationships between modalities.
- Core assumption: The model can learn meaningful representations from masked inputs without explicit labels.
- Evidence anchors:
  - [abstract]: "VLP leverages paired or unpaired vision (images, videos) and text (reports, captions) datasets to acquire vast knowledge and learn robust feature representations."
  - [section]: "MMBERT [24] utilized MLM for vision-language pretraining on radiograph images with corresponding captions... The model is trained with the objective of predicting masked medical text tokens using the image feature as additional context."
- Break condition: If the masking ratio is too high, the model may fail to learn meaningful representations due to excessive information loss.

### Mechanism 2
- Claim: Contrastive learning pretraining objectives maximize similarity between paired data and minimize similarity between unpaired data, leading to discriminative representations.
- Mechanism: The model learns to bring paired image-text representations closer in the embedding space while pushing apart unpaired representations using a contrastive loss function.
- Core assumption: The contrastive loss can effectively distinguish between semantically similar and dissimilar pairs.
- Evidence anchors:
  - [abstract]: "VLP leverages paired or unpaired vision (images, videos) and text (reports, captions) datasets to acquire vast knowledge and learn robust feature representations."
  - [section]: "A common approach in self-supervised medical VLP methods, based on contrastive objectives, involves the use of cross-modal global alignment to maximize the similarity between global embeddings of paired visual and textual data."
- Break condition: If the dataset contains many false negatives (semantically similar but labeled as negative pairs), the contrastive loss may push apart representations that should be similar.

### Mechanism 3
- Claim: Hybrid pretraining objectives combining masked prediction and contrastive learning lead to improved downstream performance by capturing both fine-grained and discriminative information.
- Mechanism: The model is trained with multiple objectives, such as masked language modeling and contrastive learning, allowing it to learn both low-level details and high-level semantic relationships.
- Core assumption: Combining different pretraining objectives leads to complementary learning effects.
- Evidence anchors:
  - [abstract]: "The survey categorizes pretraining objectives into masked prediction, contrastive learning, matching prediction, and hybrid approaches, analyzing their strengths and limitations."
  - [section]: "The learning principles of contrastive and masking-based strategies complement each other. Cross-modal contrastive learning explicitly discriminates positive and negative vision-text pairs, enhancing the discriminative capabilities of the representations. On the other hand, masked prediction objectives promote a more fine-grained representation learning and can better capture low-level modality information."
- Break condition: If the objectives are not properly balanced, one objective may dominate training and undermine the benefits of the other.

## Foundational Learning

- Concept: Multimodal learning
  - Why needed here: Understanding how models can learn from multiple modalities (e.g., images and text) is crucial for medical VLP, where datasets often contain paired image-text data.
  - Quick check question: What are the benefits of using multimodal data compared to unimodal data in medical applications?

- Concept: Self-supervised learning
  - Why needed here: Medical VLP relies on self-supervised learning to pretrain models without requiring labeled data, which is often scarce in the medical domain.
  - Quick check question: How does self-supervised learning differ from supervised learning, and what are its advantages in medical applications?

- Concept: Contrastive learning
  - Why needed here: Contrastive learning is a key component of many medical VLP approaches, allowing models to learn discriminative representations by comparing similar and dissimilar pairs.
  - Quick check question: What is the intuition behind contrastive learning, and how does it help models learn meaningful representations?

## Architecture Onboarding

- Component map: Image encoder (CNN or ViT) -> Text encoder (transformer-based) -> Modality fusion module (early or late fusion) -> Pretraining objective head (masked prediction, contrastive learning, matching prediction, or hybrid) -> Loss computation

- Critical path: Image/text input → Encoder → Fusion → Pretraining objective → Loss computation

- Design tradeoffs:
  - Early fusion vs. late fusion: Early fusion combines modalities before encoding, while late fusion allows separate encoding of each modality. Early fusion may capture more interactions but can be computationally expensive.
  - Single-stream vs. dual-stream architecture: Single-stream uses a unified encoder, while dual-stream uses separate encoders for each modality. Single-stream may be more efficient but may not capture modality-specific features as well.

- Failure signatures:
  - Poor downstream performance: May indicate issues with pretraining objectives, model architecture, or dataset quality.
  - Overfitting to pretraining data: May suggest the need for more diverse pretraining data or regularization techniques.
  - Mode collapse in generative objectives: May indicate issues with the generative model or training process.

- First 3 experiments:
  1. Ablation study on pretraining objectives: Compare the performance of models trained with different pretraining objectives (e.g., masked prediction, contrastive learning, hybrid) on a downstream task.
  2. Modality fusion analysis: Compare the performance of models using early fusion vs. late fusion, and single-stream vs. dual-stream architectures.
  3. Dataset size sensitivity: Evaluate the performance of models trained with different amounts of pretraining data to understand the impact of dataset size on downstream performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can false negatives in contrastive learning be effectively mitigated in highly imbalanced medical datasets?
- Basis in paper: [explicit] The paper discusses the challenge of false negatives in contrastive learning, particularly in medical datasets where class imbalance is prevalent.
- Why unresolved: Existing methods often rely on additional image labels or external tools, which can have limited coverage and applicability. The effectiveness of these methods in real-world scenarios remains uncertain.
- What evidence would resolve it: Comparative studies evaluating the performance of different false negative mitigation strategies across diverse medical datasets, including their impact on downstream task performance.

### Open Question 2
- Question: What are the optimal strategies for data augmentation in medical vision-language pretraining to avoid misalignment between images and text?
- Basis in paper: [inferred] The paper highlights the challenges of applying general data augmentation techniques to medical images, as they can remove crucial regions or cause misalignment with corresponding text descriptions.
- Why unresolved: While some strategies like multimodal augmentations in feature space have been proposed, their effectiveness in medical VLP settings requires further investigation.
- What evidence would resolve it: Ablation studies comparing the performance of different augmentation strategies on downstream medical tasks, considering both unimodal and multimodal augmentations.

### Open Question 3
- Question: How can vision-language models be effectively adapted for whole slide image analysis in computational pathology?
- Basis in paper: [explicit] The paper discusses the challenges of applying VLP models trained on patch-level images to whole slide images (WSIs) due to their gigapixel scale and the need for efficient processing.
- Why unresolved: Current approaches like multi-instance learning are parallelizable but can still be computationally intensive and may not fully capture the complexity of WSI analysis.
- What evidence would resolve it: Comparative studies evaluating the performance of different adaptation strategies for VLP models on WSI analysis tasks, considering factors like computational efficiency and accuracy.

## Limitations

- Limited empirical validation of the proposed hybrid approaches across diverse medical modalities
- Potential bias toward English-language medical datasets and reporting standards
- Incomplete coverage of 3D medical imaging applications despite growing relevance

## Confidence

- Pretraining objectives and their mechanisms: High confidence
- Architecture comparisons and design tradeoffs: Medium confidence
- Downstream task evaluations: Medium confidence
- Future directions and challenges: Medium confidence

## Next Checks

1. **Ablation study validation**: Systematically evaluate the impact of each pretraining objective (masked prediction, contrastive learning, matching prediction) on downstream performance across at least three medical imaging modalities (X-ray, CT, pathology) using standardized datasets.

2. **Modality fusion benchmarking**: Conduct controlled experiments comparing early vs. late fusion approaches on the same backbone architectures and pretraining objectives to quantify the specific performance gains attributable to fusion timing decisions.

3. **Generalization assessment**: Test whether models pretrained on one medical domain (e.g., radiology) maintain performance when transferred to other domains (e.g., pathology, ophthalmology) without domain-specific fine-tuning, measuring both zero-shot and few-shot transfer capabilities.