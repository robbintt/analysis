---
ver: rpa2
title: Generating Explanations in Medical Question-Answering by Expectation Maximization
  Inference over Evidence
arxiv_id: '2310.01299'
source_url: https://arxiv.org/abs/2310.01299
tags:
- evidence
- medical
- explanation
- explanations
- emin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to generate natural language
  explanations for answers predicted by medical question-answering systems. The key
  idea is to extract knowledge from medical textbooks to enhance the quality of explanations.
---

# Generating Explanations in Medical Question-Answering by Expectation Maximization Inference over Evidence

## Quick Facts
- arXiv ID: 2310.01299
- Source URL: https://arxiv.org/abs/2310.01299
- Reference count: 20
- Key outcome: EMIN framework outperforms state-of-the-art models on Rouge-1 and Bleu-4 scores for medical QA explanation generation

## Executive Summary
This paper introduces EMIN, a novel approach for generating natural language explanations in medical question-answering systems by extracting and reasoning over evidence from medical textbooks. The key innovation is using Expectation-Maximization inference to iteratively optimize evidence relevance weights and model parameters, enabling scalable attention over lengthy evidence passages. Experimental results on two medical QA datasets demonstrate significant improvements over existing methods, with EMIN achieving state-of-the-art performance in terms of ROUGE and BLEU metrics.

## Method Summary
The EMIN framework uses a dual encoder-decoder architecture where evidence paragraphs are retrieved using DPR and weighted using EM inference. In the E-step, evidence weights are computed based on inverse cross-entropy between generated and ground-truth explanations. In the M-step, model parameters are optimized via backpropagation. The dual cross-attention decoder allows scalable incorporation of evidence through element-wise multiplication with cross-attention outputs. Training uses ground-truth answers while inference uses predicted answers from BioLinkBERT.

## Key Results
- EMIN achieves significant improvements on Rouge-1 and Bleu-4 scores compared to state-of-the-art models
- The framework effectively generates natural language explanations by reasoning over evidence from medical textbooks
- EM inference provides evidence-level reasoning that outperforms token-level attention approaches for lengthy documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EMIN iteratively refines both evidence weights and model parameters to improve explanation quality.
- Mechanism: Uses Expectation-Maximization algorithm where E-step computes evidence weights based on inverse cross-entropy between generated and ground-truth explanations, and M-step optimizes model parameters via backpropagation.
- Core assumption: Evidence weights can be treated as hidden variables that can be inferred from the quality of generated explanations.
- Evidence anchors:
  - [abstract] "We designed an expectation-maximization approach that makes inferences about the evidence found in these texts"
  - [section 2.3] "In the E-step, we update the evidence weights zi using the ground-truth explanations. In the M-step, we use the back-propagation algorithm to optimize the dual encoder-decoder model's parameters Θ given the expected values of zi."
- Break condition: If evidence weights converge to uniform distribution or KL divergence between iterations doesn't decrease below threshold, the model fails to distinguish relevant evidence.

### Mechanism 2
- Claim: Dual cross-attention decoder enables scalable attention over multiple lengthy evidence documents without full attention concatenation.
- Mechanism: Each evidence paragraph is encoded separately and weighted by learned evidence weights, then incorporated into decoder layers through element-wise multiplication with cross-attention outputs.
- Core assumption: Separating evidence encoding and using learned weights is more efficient than full attention over concatenated evidence.
- Evidence anchors:
  - [abstract] "offering an efficient way to focus attention on lengthy evidence passages"
  - [section 2.2] "Equation (3) becomes: Hl+1 i = zi ⊙ Cross-Attn (CAl i, El i)"
- Break condition: If attention weights become uniform across evidence paragraphs, the model loses the ability to focus on relevant evidence.

### Mechanism 3
- Claim: EM inference provides evidence-level reasoning rather than token-level attention, enabling better explanations for lengthy documents.
- Mechanism: Evidence weights are updated at paragraph level using inverse cross-entropy, capturing importance of entire evidence passages rather than individual tokens.
- Core assumption: Paragraph-level evidence importance is more informative than token-level attention for explanation generation.
- Evidence anchors:
  - [section 3.4] "EM inference offers an elegant solution for reasoning at the paragraph level"
  - [section 2.3] "The evidence weightszi ∈ Rk capture the importance of the evidence paragraphs Ei"
- Break condition: If EM inference fails to converge or produces similar weights across all evidence, it provides no additional benefit over simple averaging approaches.

## Foundational Learning

- Concept: Expectation-Maximization algorithm
  - Why needed here: To iteratively optimize both evidence weights (latent variables) and model parameters when ground-truth evidence weights are unavailable
  - Quick check question: What are the two main steps in EM algorithm and what happens in each step?

- Concept: Cross-attention in transformer architectures
  - Why needed here: To incorporate evidence information into the decoder while generating explanations, allowing the model to attend to relevant evidence
  - Quick check question: How does cross-attention differ from self-attention in transformer models?

- Concept: Inverse cross-entropy as similarity metric
  - Why needed here: To measure how close the generated explanation is to ground truth when using specific evidence, which determines evidence weights
  - Quick check question: Why is inverse cross-entropy used instead of regular cross-entropy for measuring explanation similarity?

## Architecture Onboarding

- Component map: DPR (Evidence Retriever) → BART Encoder (Evidence Encoder) → Dual Cross-Attn Decoder (BART Decoder with EM inference) → Explanation Generator
- Critical path: Question + Answer → Evidence Retrieval → Evidence Encoding → EM Inference (E-step + M-step) → Explanation Generation
- Design tradeoffs:
  - Separate evidence encoding vs. concatenated evidence: More scalable but requires learned evidence weights
  - EM inference vs. uniform weights: Better performance but increased complexity and training time
  - BART base vs. larger models: Good balance of performance and efficiency for medical domain
- Failure signatures:
  - Uniform evidence weights across all paragraphs indicate EM inference isn't learning relevance
  - Explanations that don't reference retrieved evidence suggest poor evidence integration
  - Low ROUGE/BLEU scores with high evidence retrieval scores indicate explanation generation issues
- First 3 experiments:
  1. Compare uniform evidence weights vs. EM-inferred weights on small validation set to verify EM provides benefit
  2. Test different numbers of evidence paragraphs (k=5, 10, 15) to find optimal balance between coverage and efficiency
  3. Compare EM inference convergence speed with different temperature schedules (λ decay rates)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of EMIN compare to other state-of-the-art models on other medical question-answering datasets?
- Basis in paper: [explicit] The paper compares EMIN to other state-of-the-art models on two medical question-answering datasets, MQAE-diag and MQAE.
- Why unresolved: The paper only evaluates EMIN on these two datasets, and it is unclear how it would perform on other medical question-answering datasets.
- What evidence would resolve it: Further experiments on other medical question-answering datasets would provide insights into the generalizability of EMIN.

### Open Question 2
- Question: How does the number of evidence paragraphs selected by EMIN affect its performance?
- Basis in paper: [explicit] The paper mentions that the number of selected evidence paragraphs k is set to 10, but does not explore the impact of different values of k on performance.
- Why unresolved: The optimal number of evidence paragraphs may vary depending on the specific question-answering task and dataset.
- What evidence would resolve it: Experiments with different values of k on the same datasets used in the paper would provide insights into the optimal number of evidence paragraphs for EMIN.

### Open Question 3
- Question: How does the quality of the explanations generated by EMIN compare to human-generated explanations?
- Basis in paper: [explicit] The paper evaluates the quality of explanations generated by EMIN using ROUGE and BLEU scores, but does not compare them to human-generated explanations.
- Why unresolved: Human-generated explanations are often considered the gold standard for evaluating the quality of explanations, and it is unclear how EMIN-generated explanations compare to them.
- What evidence would resolve it: A human evaluation study comparing EMIN-generated explanations to human-generated explanations would provide insights into the quality of EMIN-generated explanations.

## Limitations
- Limited dataset size and scope with only 577 and 101 instances in MQAE and MQAE-diag datasets
- Single-domain evidence corpus relying exclusively on StatPearls medical book
- EM inference convergence uncertainty and sensitivity to initialization and hyperparameters

## Confidence
- High confidence in the core mechanism of using EM inference for evidence weight optimization and dual cross-attention decoder architecture
- Medium confidence in the scalability claims for handling lengthy evidence documents
- Low confidence in generalizability beyond medical domains without additional validation

## Next Checks
1. Cross-domain evaluation: Test the EMIN framework on non-medical QA datasets with different evidence types to assess generalizability
2. EM convergence robustness analysis: Systematically evaluate EM inference convergence across different initialization strategies, temperature schedules, and question categories
3. Ablation study on evidence quality: Compare performance when using different numbers of evidence paragraphs (k=5, 10, 15) and varying DPR retrieval thresholds