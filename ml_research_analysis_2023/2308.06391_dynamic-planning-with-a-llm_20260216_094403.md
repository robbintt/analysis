---
ver: rpa2
title: Dynamic Planning with a LLM
arxiv_id: '2308.06391'
source_url: https://arxiv.org/abs/2308.06391
tags:
- llm-dp
- planning
- language
- goal
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-DP, a neuro-symbolic framework that combines
  a Large Language Model (LLM) with a symbolic planner to solve embodied tasks. The
  approach leverages the LLM's ability to understand natural language and translate
  tasks into PDDL (Planning Domain Definition Language) while using a symbolic planner
  to efficiently find optimal solutions.
---

# Dynamic Planning with a LLM

## Quick Facts
- arXiv ID: 2308.06391
- Source URL: https://arxiv.org/abs/2308.06391
- Reference count: 9
- Key outcome: LLM-DP achieves 96% success rate compared to 64% for ReAct baseline, while using fewer actions on average (13.16 vs 18.69)

## Executive Summary
This paper introduces LLM-DP, a neuro-symbolic framework that combines a Large Language Model (LLM) with a symbolic planner to solve embodied tasks. The approach leverages the LLM's ability to understand natural language and translate tasks into PDDL (Planning Domain Definition Language) while using a symbolic planner to efficiently find optimal solutions. LLM-DP dynamically generates multiple planning problems by sampling possible world states and selects actions based on the generated plans. Experiments on the Alfworld environment show that LLM-DP achieves 96% success rate compared to 64% for a ReAct baseline, while using fewer actions on average (13.16 vs 18.69). The method effectively bridges the gap between LLM-based reasoning and efficient symbolic planning, offering a promising direction for embodied agents that require both language understanding and optimal planning.

## Method Summary
LLM-DP integrates a Large Language Model with a symbolic planner to solve embodied tasks by translating natural language instructions into PDDL goals and sampling possible world states to generate multiple planning problems. The LLM converts task descriptions into executable PDDL goals and samples from belief distributions about unknown predicates to create plausible world states. A symbolic planner (BFS(f)) then finds optimal plans for each sampled state, and the shortest valid plan is selected for execution. The agent iteratively updates its world state based on observations, triggering replanning when new information is discovered. The framework requires a pre-defined PDDL domain file describing actions and predicates, and experiments were conducted on the Alfworld environment using gpt-3.5-turbo-0613 for LLM integration.

## Key Results
- LLM-DP achieves 96% success rate compared to 64% for ReAct baseline
- Uses fewer actions on average (13.16 vs 18.69)
- Successfully translates task descriptions into executable PDDL goals 97% of the time
- Demonstrates effective integration of LLM reasoning with symbolic planning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-DP improves planning efficiency by combining LLM's semantic understanding with symbolic planner's optimal search.
- Mechanism: The LLM translates natural language tasks into PDDL goals, while the symbolic planner efficiently finds optimal plans given these goals and sampled world states.
- Core assumption: The LLM can accurately translate tasks into PDDL goals and sample plausible world states that include relevant objects and predicates.
- Evidence anchors:
  - [abstract] "LLM-DP achieves 96% success rate compared to 64% for a ReAct baseline, while using fewer actions on average"
  - [section 4.3] "LLM-DP uses stored observations W, beliefs B and an LLM to construct different planning problem files in PDDL"
  - [corpus] Weak - related papers discuss similar LLM + planner integration but lack specific performance comparison
- Break condition: LLM fails to generate valid PDDL goals or sample plausible world states, causing the planner to fail or find suboptimal plans.

### Mechanism 2
- Claim: Sampling multiple world states allows the planner to find plans even with incomplete information.
- Mechanism: The LLM samples from beliefs about unknown predicates (like object locations) to generate multiple plausible world states, each converted to a PDDL problem. The planner finds plans for each, and the shortest valid plan is selected.
- Core assumption: Sampling from beliefs produces at least one plausible world state where the goal is achievable.
- Evidence anchors:
  - [section 4.3] "Since B includes possible predicates which are unknown, we sample from B using an LLM to obtain wbelief"
  - [section 5] "The LLM-DP can translate the task description into an executable PDDL goal 97% of the time, but sampling reduces the accuracy further when it fails to select a valid set of possible world states"
  - [corpus] Missing - no direct evidence about sampling effectiveness
- Break condition: All sampled world states are either invalid or already satisfy the goal, preventing the planner from finding actionable plans.

### Mechanism 3
- Claim: Dynamic replanning based on observations enables adaptation to environmental changes.
- Mechanism: After each action, the agent updates its world state representation with new observations. If new objects are discovered, it triggers replanning with the updated state to find new plans.
- Core assumption: Observations provide accurate and complete information about the environment that can be integrated into the world state.
- Evidence anchors:
  - [section 4.6] "LLM-DP uses the result of each action to update its internal state representation... If the agent detects new information from the scene... it triggers a re-planning process"
  - [section 2] "In contrast, modern LLMs cope with noisy observations and high levels of uncertainty when reasoning about a task"
  - [corpus] Weak - related papers discuss replanning but not specifically tied to observation processing
- Break condition: Observations are noisy or incomplete, leading to incorrect world state updates and invalid subsequent plans.

## Foundational Learning

- Concept: PDDL (Planning Domain Definition Language)
  - Why needed here: PDDL is the formal language used to represent planning problems that the symbolic planner can solve. Understanding PDDL structure is essential for creating valid domain and problem files.
  - Quick check question: What are the key components of a PDDL problem file and how do they relate to the planning task?

- Concept: Symbolic planning algorithms (like BFS(f))
  - Why needed here: The planner needs to efficiently search for optimal plans given the PDDL problems generated from sampled world states. Understanding how planners work helps in debugging planning failures.
  - Quick check question: How does BFS(f) differ from other planning algorithms and why might it be preferred for this application?

- Concept: Sampling from belief distributions
  - Why needed here: Since the initial world state is incomplete (unknown object locations), sampling from beliefs allows generating multiple plausible scenarios for planning. Understanding sampling strategies is crucial for effective world state generation.
  - Quick check question: What are the trade-offs between LLM-based sampling and random sampling for generating plausible world states?

## Architecture Onboarding

- Component map: LLM -> World state tracker (W) and belief set (B) -> Plan generator (symbolic planner) -> Action selector -> Environment -> Observation processor -> (updates W and B)

- Critical path:
  1. LLM converts task to PDDL goal
  2. LLM samples beliefs to create plausible world states
  3. Each world state converted to PDDL problem
  4. Symbolic planner finds plans for each problem
  5. Action selector chooses shortest valid plan
  6. Execute action, observe result, update world state
  7. Repeat until goal reached or failure

- Design tradeoffs:
  - LLM sampling vs random sampling for beliefs: LLM sampling may be more accurate but slower and more expensive
  - Number of samples (n): More samples increase chances of finding valid plans but increase computation time
  - Fallback strategy: When no valid plans found, random sampling can help but may reduce success rate

- Failure signatures:
  - LLM generates invalid PDDL goals → check few-shot examples and prompt format
  - No valid plans found → check sampling strategy and whether goal is already satisfied in all sampled states
  - High number of actions per episode → check if planner is finding suboptimal plans or if sampling is generating overly conservative world states

- First 3 experiments:
  1. Test LLM goal generation with various task descriptions to ensure 97%+ accuracy
  2. Compare LLM sampling vs random sampling for beliefs with different values of n
  3. Measure success rate and episode length with and without fallback to random sampling when no valid plans are found

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the LLM-DP framework handle cases where the LLM generates invalid PDDL goals due to semantic misunderstandings?
- Basis in paper: [explicit] The paper provides examples of invalid PDDL goals generated by the LLM, such as identifying a mug as a receptacle.
- Why unresolved: The paper does not discuss how the framework handles or recovers from such invalid goal generations.
- What evidence would resolve it: Experiments showing the framework's performance with invalid goal generations and strategies to handle or recover from them.

### Open Question 2
- Question: How does the LLM-DP framework perform when the environment provides noisy or imperfect observations?
- Basis in paper: [inferred] The paper mentions that Alfworld provides perfect textual descriptions of the current location, but does not discuss performance with noisy or imperfect observations.
- Why unresolved: The paper does not explore the framework's performance in environments with noisy or imperfect observations.
- What evidence would resolve it: Experiments testing the framework's performance in environments with varying levels of observation noise or imperfection.

### Open Question 3
- Question: How does the LLM-DP framework adapt to new environments without a pre-defined domain file?
- Basis in paper: [explicit] The paper mentions that the framework requires a domain file containing action-descriptions and object predicates as a simplifying assumption.
- Why unresolved: The paper does not discuss how the framework would perform in environments where the domain file is not available or needs to be learned.
- What evidence would resolve it: Experiments showing the framework's ability to adapt to new environments and learn the necessary domain information.

## Limitations

- Unknown PDDL domain file specification prevents exact reproduction
- Specific LLM prompts and few-shot examples not provided
- No experiments with noisy or imperfect observations
- Framework requires pre-defined domain file for new environments

## Confidence

- **High**: Performance improvements over ReAct baseline (96% vs 64% success rate, 13.16 vs 18.69 actions)
- **Medium**: Mechanism of combining LLM with symbolic planner
- **Medium**: Dynamic replanning based on observations

## Next Checks

1. **Validate LLM goal generation accuracy**: Test the LLM with diverse Alfworld task descriptions to confirm the 97% PDDL goal generation accuracy claim across all task types.

2. **Compare sampling strategies**: Implement and compare LLM-based sampling versus random sampling for beliefs with varying values of n to quantify the trade-off between accuracy and computational cost.

3. **Test observation integration robustness**: Systematically evaluate how the system handles noisy or incomplete observations by introducing controlled perturbations to the observation stream and measuring plan quality degradation.