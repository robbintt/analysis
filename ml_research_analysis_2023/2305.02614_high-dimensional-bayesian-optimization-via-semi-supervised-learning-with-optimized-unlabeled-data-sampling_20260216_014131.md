---
ver: rpa2
title: High-Dimensional Bayesian Optimization via Semi-Supervised Learning with Optimized
  Unlabeled Data Sampling
arxiv_id: '2305.02614'
source_url: https://arxiv.org/abs/2305.02614
tags:
- data
- unlabeled
- learning
- optimization
- labeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TSBO, a novel semi-supervised Bayesian optimization
  (BO) method that incorporates a teacher-student model to effectively utilize both
  labeled and unlabeled data. The core idea is to use the teacher model to predict
  pseudo labels for unlabeled data, which are then used to train a student model (Gaussian
  Process).
---

# High-Dimensional Bayesian Optimization via Semi-Supervised Learning with Optimized Unlabeled Data Sampling

## Quick Facts
- arXiv ID: 2305.02614
- Source URL: https://arxiv.org/abs/2305.02614
- Reference count: 14
- Primary result: TSBO achieves state-of-the-art sample efficiency in high-dimensional optimization tasks

## Executive Summary
This paper presents TSBO, a novel semi-supervised Bayesian optimization method that leverages both labeled and unlabeled data through a teacher-student model framework. The approach uses a VAE to learn a low-dimensional latent space, then employs a teacher neural network to generate pseudo labels for unlabeled data, which trains a student Gaussian Process model. The student's performance on dynamically selected validation data provides feedback to update the teacher model. Additionally, the paper proposes optimized strategies for selecting validation data and unlabeled data sampling, including using a dynamically fitted extreme value distribution to focus exploration on promising regions.

## Method Summary
TSBO operates in a learned latent space to scale Bayesian optimization to high-dimensional problems. The method pre-trains a VAE on unlabeled data, then iteratively trains teacher-student models where the teacher generates pseudo labels and the student provides feedback for improvement. Validation data is selected as the top-K highest labeled points, and unlabeled data is sampled either from a parameterized distribution or from a dynamically fitted extreme value distribution. The student GP is trained on labeled data plus pseudo-labeled unlabeled data, and an acquisition function guides the selection of the next query point.

## Key Results
- Outperforms existing baselines (LS-BO, W-LBO, T-LBO) on three high-dimensional optimization tasks
- Achieves state-of-the-art sample efficiency by effectively utilizing unlabeled data
- Demonstrates the effectiveness of optimized unlabeled data sampling from extreme value distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The teacher-student model framework improves the quality of pseudo labels by incorporating student feedback into teacher updates.
- Mechanism: The student model evaluates the teacher's pseudo labels on a dynamically selected validation set, and this performance feedback is used to update the teacher model parameters, creating a selective regularization effect.
- Core assumption: The student's performance on validation data is a reliable indicator of the teacher's ability to generate high-quality pseudo labels in regions relevant to the optimization objective.

### Mechanism 2
- Claim: Optimized unlabeled data sampling from a dynamically fitted extreme value distribution improves exploration toward the global optimum.
- Mechanism: By fitting a GEV distribution to the highest validation labels and sampling unlabeled data from this distribution, the method focuses exploration on regions with potentially higher objective values while maintaining diversity.
- Core assumption: The extreme value distribution of validation labels captures the tail behavior of the objective function near promising regions.

### Mechanism 3
- Claim: Operating in a learned latent space with reduced dimensionality makes the method scalable to high-dimensional problems.
- Mechanism: A VAE learns a low-dimensional latent representation from unlabeled data, and BO is performed in this latent space rather than the original high-dimensional input space.
- Core assumption: The latent space preserves the relevant structure for optimization while reducing computational complexity.

## Foundational Learning

- Concept: Gaussian Process surrogate modeling
  - Why needed here: GP provides uncertainty quantification essential for the acquisition function to balance exploration and exploitation in BO.
  - Quick check question: What is the posterior mean and variance formula for a GP given labeled data?

- Concept: Extreme Value Theory (EVT)
  - Why needed here: EVT provides the theoretical foundation for modeling the distribution of extreme values, which is used to sample unlabeled data from promising regions.
  - Quick check question: What are the three types of extreme value distributions and when does each apply?

- Concept: Teacher-student model architecture
  - Why needed here: This framework enables semi-supervised learning by having the teacher generate pseudo labels and the student provide feedback for teacher improvement.
  - Quick check question: How does the student's performance on validation data provide feedback to the teacher in a bi-level optimization framework?

## Architecture Onboarding

- Component map:
  VAE (encoder + decoder) -> Teacher model (neural network) -> Student model (GP) -> Validation data sampler -> Unlabeled data sampler -> Acquisition function

- Critical path:
  1. VAE pre-training on unlabeled data
  2. Initial teacher-student training on labeled data
  3. Iterative loop: validate → sample unlabeled → train teacher-student → update GP → acquisition → query
  4. Add new labeled data and repeat

- Design tradeoffs:
  - VAE capacity vs. training time: Higher capacity VAEs capture more structure but require more data and computation
  - Validation set size: Larger sets provide better feedback but reduce data available for training
  - Unlabeled sampling strategy: GEV sampling focuses on promising regions but may miss global optimum if validation data is poor

- Failure signatures:
  - GP posterior uncertainty remains high throughout: Likely VAE latent space is poor or insufficient data
  - Performance plateaus early: Validation data selection may not align with optimization objective
  - Degradation over iterations: Student feedback may be misleading due to poor pseudo labels

- First 3 experiments:
  1. Baseline test: Run with random unlabeled sampling to establish performance floor
  2. VAE quality check: Measure reconstruction error and latent space visualization
  3. Validation sensitivity: Compare performance with different validation set selection strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TSBO scale with increasing dimensionality of the input space, and what are the theoretical limits of its applicability?
- Basis in paper: [inferred] The paper mentions that TSBO operates in a learned latent space with reduced dimensionality, making it scalable to high-dimensional problems. However, it does not provide a detailed analysis of how performance scales with dimensionality.
- Why unresolved: The paper does not provide empirical results or theoretical analysis on the performance of TSBO as the dimensionality of the input space increases beyond the tested cases.
- What evidence would resolve it: Conducting experiments with increasingly high-dimensional datasets and analyzing the performance of TSBO compared to other methods, along with providing a theoretical analysis of the scalability limits.

### Open Question 2
- Question: What is the impact of the choice of the validation data size (K) on the performance of TSBO, and is there an optimal value for K?
- Basis in paper: [explicit] The paper mentions that the size of the validation data is 10 in the topology and expression tasks and 30 in the chemical design task, but it does not explore the impact of different validation data sizes on the performance.
- Why unresolved: The paper does not provide a systematic study of how varying the size of the validation data affects the performance of TSBO.
- What evidence would resolve it: Conducting experiments with different validation data sizes and analyzing the performance of TSBO to determine the optimal value of K.

### Open Question 3
- Question: How does the choice of the acquisition function affect the performance of TSBO, and is there an optimal acquisition function for this method?
- Basis in paper: [explicit] The paper mentions that the acquisition function used in TSBO is the Expected Improvement (EI) function, but it does not explore the impact of different acquisition functions on the performance.
- Why unresolved: The paper does not provide a comparative study of different acquisition functions in the context of TSBO.
- What evidence would resolve it: Conducting experiments with different acquisition functions and analyzing the performance of TSBO to determine the optimal acquisition function for this method.

## Limitations
- The effectiveness of the teacher-student feedback loop depends on validation data aligning with the optimization objective
- Extreme value distribution sampling relies on the assumption that the fitted GEV accurately captures the tail behavior of the objective function
- Performance may degrade if the VAE latent space does not preserve relevant structure for optimization

## Confidence

- High confidence: The VAE-based dimensionality reduction approach and the general teacher-student framework are well-established techniques that should improve sample efficiency
- Medium confidence: The optimized validation data selection and extreme value distribution sampling strategies will consistently improve performance across diverse high-dimensional optimization problems
- Low confidence: The specific hyperparameters for MCMC sampling and the exact implementation details of the bi-level optimization framework

## Next Checks

1. Test sensitivity to validation data size: Run experiments with varying validation set sizes (e.g., 5%, 10%, 20% of total data) to determine the optimal balance between feedback quality and training data availability

2. Compare sampling strategies: Implement a baseline using random unlabeled data sampling and compare performance against the optimized GEV sampling to quantify the improvement from the proposed strategy

3. Validate latent space quality: Measure VAE reconstruction error and visualize the latent space using t-SNE to ensure that the dimensionality reduction preserves relevant structure for optimization tasks