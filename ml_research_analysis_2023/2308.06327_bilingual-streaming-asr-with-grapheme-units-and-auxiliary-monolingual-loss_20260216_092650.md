---
ver: rpa2
title: Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss
arxiv_id: '2308.06327'
source_url: https://arxiv.org/abs/2308.06327
tags:
- bilingual
- units
- monolingual
- table
- locales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of developing a bilingual automatic
  speech recognition (ASR) system for Latin languages that supports English as a secondary
  language, enabling effective code-mixing scenarios. The core method involves transitioning
  from traditional phone-based pronunciation lexicons to grapheme (letter-based) units,
  enabling better cross-lingual sharing.
---

# Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss

## Quick Facts
- arXiv ID: 2308.06327
- Source URL: https://arxiv.org/abs/2308.06327
- Reference count: 0
- This work develops a bilingual streaming ASR system for Latin languages that supports English code-mixing, achieving 13.8% WER on code-mixed Italian tasks while maintaining near-parity (9.6%) with monolingual Italian performance.

## Executive Summary
This paper addresses the challenge of developing bilingual automatic speech recognition systems for Latin languages that can effectively handle English code-mixing scenarios. The core innovation involves transitioning from traditional phone-based pronunciation lexicons to grapheme (letter-based) units, which enables better cross-lingual sharing across Latin locales. The authors develop a fully bilingual streaming Transformer model with a shared encoder and parallel encoders, trained with auxiliary monolingual losses to specialize each encoder for its respective language. The approach is evaluated on large-scale Spanish and Italian tasks, demonstrating significant improvements in code-mixing WER while maintaining strong monolingual performance.

## Method Summary
The method involves transitioning from phone-based pronunciation lexicons to grapheme units to enable stronger cross-lingual sharing, and developing a streaming Transformer model with shared bottom layers and parallel top encoders. Each parallel encoder is trained with an auxiliary monolingual loss to specialize for its target language. The model is trained end-to-end on large-scale datasets including English, Spanish, and Italian, with data augmentation techniques applied. The approach combines shared bottom layers for common acoustic features with parallel top layers for language-specific modeling, all within a streaming constraint.

## Key Results
- Bilingual Italian model improves code-mixing WER from 46.5% to 13.8% while achieving near-parity with monolingual Italian (9.6% vs 9.5%)
- The bilingual EN model maintains English WER within 10% relative of monolingual EN model
- Streaming bilingual models achieve competitive performance with significantly smaller model size (300 MB) compared to prior work
- Auxiliary monolingual losses prove superior to language ID-based combination for encoder specialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transitioning from phone-based units to grapheme units enables stronger cross-lingual sharing and reduces WER in bilingual code-mixing scenarios.
- Mechanism: Grapheme units (letters) are language-independent, so they naturally share across Latin locales. This reduces the complexity of modeling locale-specific phones and allows the model to generalize better to English loanwords and code-switched content.
- Core assumption: Latin-script languages have sufficient grapheme-unit overlap to make this transition effective.
- Evidence anchors:
  - [abstract] "Our key developments constitute: (a) pronunciation lexicon with grapheme units instead of phone units..."
  - [section 2.1] "In this section we motivate grapheme units and specifically letter units for bilingual work..."
  - [corpus] Weak; no related papers directly discuss grapheme vs phone unit impact on WER.
- Break condition: If the target locales use non-Latin scripts (e.g., Cyrillic, Arabic), grapheme units may not share well and performance could degrade.

### Mechanism 2
- Claim: Parallel encoders with auxiliary monolingual losses specialize encoders for each locale, improving monolingual task performance while maintaining bilingual capability.
- Mechanism: By projecting each parallel encoder's output to a monolingual feature space and adding a cross-entropy loss over acoustic states (chenones), each encoder learns to specialize in its target language. The shared projection layer allows code-mixing by combining the two specialized encoders.
- Core assumption: Specialization improves performance more than a fully shared encoder, without harming code-mixing ability.
- Evidence anchors:
  - [abstract] "...auxiliary loss for monolingual projections...our proposed auxiliary loss is superior in specializing the parallel encoders..."
  - [section 3.3] "We use cross-entropy criterion for auxiliary losses that are jointly optimized with the rest of the model."
  - [section 4.3] "We notice that the PE with auxiliary loss rectifies mistakes over phrases with similar pronunciations..."
- Break condition: If the auxiliary loss weight is too high, it may cause overfitting to monolingual data and hurt code-mixing performance.

### Mechanism 3
- Claim: Streaming Transformer architecture with shared bottom layers and parallel top layers minimizes model size while retaining specialization benefits.
- Mechanism: Bottom layers learn low-level acoustic features shared across languages; parallel top layers learn high-level language-specific features. This design retains good scope for locale-specific improvements while minimizing total model size.
- Core assumption: Initial layers learn low-level common features, whereas upper layers learn high-level features suitable for dedicated monolingual tasks.
- Evidence anchors:
  - [section 3.1] "Our design consideration is to minimize the total model size - we achieve that by sharing bottom model layers..."
  - [section 4.3] "Our bilingual model consists of 18 Transformer layers. For PE work, we consider 15 shared layers and 3 layers per parallel encoder."
  - [corpus] Weak; no related papers discuss parallel encoder architecture tradeoffs in detail.
- Break condition: If the number of shared vs. parallel layers is not optimally tuned, the model may lose either generalization (too many parallel layers) or specialization (too few).

## Foundational Learning

- Concept: Grapheme vs. phoneme units
  - Why needed here: Understanding why graphemes improve cross-lingual sharing vs. phonemes is fundamental to the paper's core innovation.
  - Quick check question: Why do grapheme units enable better sharing across Latin languages compared to phone units?

- Concept: Parallel encoder architecture
  - Why needed here: The parallel encoder design with auxiliary losses is the key technical contribution that enables strong bilingual performance.
  - Quick check question: How does the auxiliary loss help each parallel encoder specialize in its target language?

- Concept: Streaming Transformer models
  - Why needed here: The streaming constraint and Transformer architecture choices impact model design and performance tradeoffs.
  - Quick check question: What is the main performance limitation of streaming models compared to non-streaming models, and how does knowledge distillation help?

## Architecture Onboarding

- Component map:
  - Bottom 15 Transformer layers: Shared across both languages (EN and IT)
  - Top 3 layers per language: Parallel encoders (EN-encoder, IT-encoder)
  - Projection layer: Shared projection for bilingual feature space
  - Auxiliary losses: Monolingual CE losses for each parallel encoder
  - LID module (optional): Predicts language probability for soft combination

- Critical path:
  1. Grapheme lexicon preparation
  2. Bilingual GMM-HMM training for alignments
  3. Bilingual TDNN training for frame alignments
  4. Shared+parallel Transformer training with auxiliary losses
  5. Evaluation on monolingual and code-mixed test sets

- Design tradeoffs:
  - Shared vs. parallel layers: More parallel layers = better specialization but larger model
  - Auxiliary loss weight: Higher weight = better specialization but risk of overfitting
  - LID vs. auxiliary loss: LID enables soft combination but can be unstable; auxiliary loss is more stable but uses a simpler combination method

- Failure signatures:
  - High WER on code-mixed tasks but low WER on monolingual tasks: Parallel encoders too specialized
  - High WER on both monolingual and code-mixed tasks: Auxiliary loss weight too low or model not properly trained
  - Training instability: LID module causing issues, consider using auxiliary loss instead

- First 3 experiments:
  1. Train bilingual GMM-HMM and TDNN models to verify alignment quality
  2. Train streaming Transformer with grapheme units and evaluate baseline WER
  3. Add parallel encoders with auxiliary losses and compare WER improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance gap between bilingual and monolingual models be further reduced for English code-mixing tasks?
- Basis in paper: [explicit] The paper states that the bilingual IT model improves the WER for a code-mix IT task from 46.5% to 13.8%, while achieving close parity (9.6%) with the monolingual IT model (9.5%) over IT tests.
- Why unresolved: While the bilingual model shows significant improvement, there is still a gap between bilingual and monolingual performance, especially for English code-mixing tasks.
- What evidence would resolve it: Additional experiments and analyses that focus on optimizing the bilingual model for English code-mixing tasks, potentially through advanced model architectures or training techniques.

### Open Question 2
- Question: Can the proposed approach be extended to non-Latin locales and end-to-end models?
- Basis in paper: [explicit] The paper mentions that in future work, the approach will be extended to non-Latin locales and applied to end-to-end models.
- Why unresolved: The current study focuses on Latin locales and hybrid models, so the effectiveness and challenges of extending the approach to non-Latin locales and end-to-end models remain unexplored.
- What evidence would resolve it: Experimental results and analyses demonstrating the performance of the approach on non-Latin locales and end-to-end models, along with insights into any unique challenges or benefits.

### Open Question 3
- Question: How can the shared projection layer in the parallel encoder structure be improved to better support code-mixing tasks?
- Basis in paper: [inferred] The paper notes that the simple shared projection layer in the current implementation may contribute to a small regression on the code-mixed task, suggesting room for improvement.
- Why unresolved: The paper suggests that the shared projection layer may not be optimal for combining locale-specific encoders to support code-mixing tasks, but does not provide a definitive solution.
- What evidence would resolve it: Experimental results comparing different shared projection layer designs or attention mechanisms, along with analyses of their impact on code-mixing task performance.

## Limitations

- Data composition and representativeness: The paper mentions large-scale datasets but doesn't specify the distribution of code-mixed vs. monolingual data in training sets, which is critical for bilingual model effectiveness.
- Hyperparameter sensitivity: Critical hyperparameters like auxiliary loss weight and number of shared vs. parallel layers are mentioned but not thoroughly explored with sensitivity analysis.
- Generalizability beyond Latin locales: The effectiveness of grapheme units may vary significantly across different Latin locales with varying phoneme-grapheme correspondences.
- Streaming constraint impact: The paper doesn't adequately compare performance against non-streaming baselines on the same tasks, making it unclear whether improvements are due to streaming architecture or specific technical contributions.

## Confidence

**High confidence**: The core claim that grapheme units enable better cross-lingual sharing than phone units is well-supported by the mechanism analysis and empirical results (WER improvements from 46.5% to 13.8% on code-mixed tasks).

**Medium confidence**: The claim that parallel encoders with auxiliary monolingual losses improve specialization is reasonably supported, but the ablation studies are incomplete and don't fully explore whether this approach is superior to other specialization techniques.

**Low confidence**: Claims about the approach's generalizability to other Latin locales and the assertion that the method provides "the best of both worlds" in a single training step lack sufficient empirical validation across multiple locales or languages.

## Next Checks

**Validation Check 1**: Perform ablation studies varying the auxiliary loss weight (Î») across a wider range (0.1 to 1.0) and systematically evaluate the tradeoff between monolingual specialization and code-mixing performance.

**Validation Check 2**: Train and evaluate the same model architecture on a third Latin locale (e.g., French or Portuguese) to test the generalizability claim and compare performance with and without grapheme units.

**Validation Check 3**: Conduct a controlled experiment comparing streaming vs. non-streaming versions of the same model architecture on identical tasks to isolate the impact of streaming constraints from specific technical contributions.