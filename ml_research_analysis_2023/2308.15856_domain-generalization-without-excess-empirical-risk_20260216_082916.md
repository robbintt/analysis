---
ver: rpa2
title: Domain Generalization without Excess Empirical Risk
arxiv_id: '2308.15856'
source_url: https://arxiv.org/abs/2308.15856
tags:
- generalization
- penalty
- domain
- risk
- empirical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present an algorithm for domain generalization that
  aims to improve out-of-distribution generalization without sacrificing in-distribution
  performance. The key idea is to reformulate the problem as minimizing a domain generalization
  penalty under the constraint of optimality of the empirical risk.
---

# Domain Generalization without Excess Empirical Risk

## Quick Facts
- arXiv ID: 2308.15856
- Source URL: https://arxiv.org/abs/2308.15856
- Authors: 
- Reference count: 40
- Primary result: SDG eliminates excess empirical risk while improving out-of-distribution performance

## Executive Summary
This paper addresses a fundamental limitation in domain generalization (DG) methods: while they aim to improve out-of-distribution (OOD) performance, they often inadvertently increase in-distribution (ID) error. The authors propose Satisficing Domain Generalization (SDG), which reformulates the optimization problem as minimizing a domain generalization penalty under the constraint that the empirical risk is optimally minimized. This approach ensures no degradation of ID performance while potentially improving OOD generalization. The method is evaluated across WILDS and DomainBed benchmark datasets, demonstrating significant improvements over existing DG methods.

## Method Summary
SDG modifies standard penalty-based DG approaches (like CORAL, FISH, VRex) by replacing joint risk-penalty minimization with constrained penalty minimization. The algorithm computes per-domain gradients, then uses a Blahut-Arimoto-style iterative algorithm to approximate the optimal update distribution that minimizes the penalty while staying within a bounded deviation from the empirical gradient. The final update is sampled from this distribution and applied with the standard learning rate. This ensures the solution remains optimal for the empirical risk while reducing the generalization penalty.

## Key Results
- SDG significantly improves OOD performance across WILDS datasets without sacrificing ID performance
- On DomainBed benchmarks, SDG shows consistent improvements over ERM and other DG methods
- The CORAL model combined with SDG achieves the best overall performance across multiple datasets
- SDG eliminates the common problem of DG methods increasing ID error while attempting to improve OOD generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating joint risk-penalty minimization as constrained penalty minimization guarantees no degradation of in-distribution performance.
- Mechanism: The reformulation replaces the unconstrained joint optimization problem with a constrained optimization that explicitly requires the solution to be optimal for the empirical risk, ensuring the penalty cannot increase the empirical risk.
- Core assumption: The constraint "optimality of the empirical risk" can be operationalized via stationarity (local optimality) in non-convex settings.
- Evidence anchors:
  - [abstract] "Instead of jointly minimizing empirical risk with the penalty, we minimize the penalty under the constraint of optimality of the empirical risk."
  - [section 2.3] "Instead of jointly minimizing the empirical risk and the penalty, we minimize the penalty under the constraint of the optimality of the empirical risk."
- Break condition: If the empirical risk surface has multiple disconnected minima, the constrained solution may not find the global best one for domain generalization.

### Mechanism 2
- Claim: Bounded deviation from the empirical gradient enables convergence to a stationary point even when the update is biased toward minimizing the penalty.
- Mechanism: Proposition 2.1 shows that stochastic updates with a bias bounded by D/√t converge to a stationary point, allowing penalty-minimizing updates that stay within the feasible set.
- Core assumption: The bias in the update direction decreases at a rate of 1/√t, ensuring eventual convergence.
- Evidence anchors:
  - [section 2.3] "We adapt the analysis of SGD with bias from Sener & Koltun [37] and show that as long as the norm of the bias is bounded and this bound decreases during training faster than the rate 1/√t, it convergences to a stationary point of a smooth and non-convex risk function."
- Break condition: If the bias does not decrease sufficiently fast or the step size is poorly chosen, convergence to a stationary point is not guaranteed.

### Mechanism 3
- Claim: The connection to rate-distortion theory provides a principled way to balance penalty minimization against deviation from the empirical gradient.
- Mechanism: The optimization problem is analogous to rate-distortion, where the penalty plays the role of distortion and the constraint bounds the allowed deviation (distortion), enabling the use of Blahut-Arimoto-style algorithms.
- Core assumption: Treating the gradient update as a communication channel between the true gradient and the applied update is valid for optimization purposes.
- Evidence anchors:
  - [section 2.4] "This form is strikingly similar to the rate-distortion function, where the (compression) rate is optimized with the constraint of acceptable degradation in decoding."
- Break condition: If the penalty function is not smooth or the approximation used in the algorithm breaks down, the rate-distortion analogy may not hold.

## Foundational Learning

- Concept: Non-convex optimization and stationary points
  - Why needed here: The empirical risk is non-convex, so we can only guarantee convergence to a stationary point, not a global minimum.
  - Quick check question: Why do we use stationarity as the optimality criterion instead of global optimality?

- Concept: Bias in stochastic gradient descent
  - Why needed here: The updates are biased toward minimizing the penalty, and we need to ensure this bias doesn't prevent convergence.
  - Quick check question: How does the decay rate of the bias bound affect convergence guarantees?

- Concept: Information-theoretic rate-distortion tradeoff
  - Why needed here: The optimization problem maps to a rate-distortion problem, enabling the use of information-theoretic algorithms.
  - Quick check question: What is the role of the Lagrange multiplier β in controlling the tradeoff between penalty and distortion?

## Architecture Onboarding

- Component map: Per-domain gradients -> Blahut-Arimoto solver -> Update distribution -> Sampled update -> Model parameters

- Critical path:
  1. Compute per-domain gradients ∇θLe(θ)
  2. Compute penalty gradient ∇θPenalty(θ)
  3. Run BA iterations to compute update distribution
  4. Sample update and apply to model

- Design tradeoffs:
  - Computational cost vs. performance: BA iterations and per-domain gradients increase training time significantly
  - Approximation quality: Using only sign information and independent parameters simplifies computation but may lose some gradient structure
  - Hyperparameter sensitivity: β, D, and γ control the bias-penalty tradeoff and require tuning

- Failure signatures:
  - Training becomes very slow (BD times slower due to per-domain gradients)
  - Performance degrades if the penalty approximation is poor
  - Convergence issues if β or D are not well-tuned

- First 3 experiments:
  1. Apply SDG to CORAL on a small WILDS dataset (e.g., iWildCam) and compare OOD performance to standard CORAL
  2. Vary the bias decay rate (D/t decay) and measure impact on convergence and performance
  3. Test the effect of the number of BA iterations on both training time and final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Satisficing Domain Generalization (SDG) method's computational complexity scale with the number of domains and batch size, and what are the potential bottlenecks?
- Basis in paper: [explicit] The paper states that SDG's training time is up to BD times slower than direct optimization, where B is the batch size and D is the number of domains, due to the need for gradients per domain.
- Why unresolved: The paper only provides empirical wall-clock time measurements for a few datasets and doesn't provide a theoretical analysis of how the computational complexity scales with B and D. It also doesn't explore potential optimization techniques to mitigate the computational overhead.
- What evidence would resolve it: A detailed theoretical analysis of SDG's computational complexity, along with experiments that systematically vary B and D to measure the impact on training time. Additionally, exploring and evaluating techniques like gradient approximation or distributed computing to reduce the computational burden.

### Open Question 2
- Question: How does the choice of the information regularization parameter γ impact SDG's performance across different datasets and domain generalization methods?
- Basis in paper: [explicit] The paper mentions that γ is not treated as a hyperparameter but is set using a heuristic based on the average of (InvP(Gt k) + βd(Gt k, e)) throughout training. However, it doesn't explore the impact of different γ values on the final performance.
- Why unresolved: The paper only uses a single heuristic to set γ and doesn't investigate how different γ values affect the learned model's performance. It's unclear whether the chosen heuristic is optimal or if there are better ways to set γ.
- What evidence would resolve it: A systematic study that varies γ across a range of values and evaluates its impact on the final performance for different datasets and domain generalization methods. This would help understand the sensitivity of SDG to γ and whether the current heuristic is sufficient.

### Open Question 3
- Question: Can the SDG method be extended to handle more complex domain generalization scenarios, such as those with covariate shift, label shift, or domain generalization with label distribution mismatch?
- Basis in paper: [inferred] The paper focuses on penalty-based domain generalization methods and assumes covariate shift, where only the data distribution changes between domains and the conditional distribution of the label given the data stays the same. However, it doesn't discuss how SDG can be adapted to handle more complex scenarios.
- Why unresolved: The paper doesn't explore the applicability of SDG to more challenging domain generalization scenarios. It's unclear whether the current formulation and optimization approach can be extended to handle these cases or if new techniques are needed.
- What evidence would resolve it: Experiments that evaluate SDG's performance on datasets with covariate shift, label shift, or domain generalization with label distribution mismatch. Additionally, theoretical analysis of how SDG's assumptions and optimization approach can be adapted to handle these more complex scenarios.

## Limitations

- The computational overhead of per-domain gradients and BA iterations can make training up to BD times slower than standard methods
- The theoretical guarantees for preserving in-distribution performance rely on stationarity conditions that may not hold in highly non-convex landscapes
- The rate-distortion interpretation, while intuitive, lacks rigorous information-theoretic analysis to fully explain the algorithm's effectiveness

## Confidence

- **High confidence** in empirical results showing SDG improves out-of-distribution performance without degrading in-distribution performance on benchmark datasets
- **Medium confidence** in the theoretical mechanism linking constrained optimization to in-distribution preservation, as the non-convex case analysis is limited
- **Low confidence** in the rate-distortion interpretation as a complete explanation for the algorithm's effectiveness, due to lack of information-theoretic analysis

## Next Checks

1. Test SDG on a synthetic dataset with known multiple minima to verify whether the constraint truly preserves in-distribution optimality or merely finds a stationary point that may be suboptimal
2. Measure the actual computational overhead of SDG versus baseline methods across different dataset sizes and domain counts to quantify the BD scaling factor empirically
3. Perform ablation studies removing the rate-distortion approximation (e.g., using full gradient information) to isolate the contribution of the information-theoretic framework to performance gains