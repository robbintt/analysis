---
ver: rpa2
title: Distilling Universal and Joint Knowledge for Cross-Domain Model Compression
  on Time Series Data
arxiv_id: '2307.03347'
source_url: https://arxiv.org/abs/2307.03347
tags:
- knowledge
- domain
- student
- teacher
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of compressing deep learning
  models for time series data under cross-domain scenarios, where the source domain
  (training) and target domain (deployment) have different data distributions. The
  authors propose a novel framework called Universal and Joint Knowledge Distillation
  (UNI-KD) that transfers both universal feature-level knowledge across domains and
  joint logit-level knowledge shared by both domains from a pre-trained teacher model
  to a compact student model.
---

# Distilling Universal and Joint Knowledge for Cross-Domain Model Compression on Time Series Data

## Quick Facts
- arXiv ID: 2307.03347
- Source URL: https://arxiv.org/abs/2307.03347
- Reference count: 14
- Key outcome: UNI-KD achieves up to 10.3% improvement in macro F1-score while reducing model complexity by ~15x compared to source-only training

## Executive Summary
This paper addresses the challenge of compressing deep learning models for time series data under cross-domain scenarios where source and target domains have different data distributions. The authors propose UNI-KD (Universal and Joint Knowledge Distillation), a novel framework that transfers both universal feature-level knowledge across domains and joint logit-level knowledge shared by both domains from a pre-trained teacher to a compact student model. The approach uses two discriminators - a feature-domain discriminator for aligning representations and a data-domain discriminator for prioritizing domain-shared samples - to achieve superior cross-domain compression performance on four real-world time series datasets across three classification tasks.

## Method Summary
UNI-KD is an end-to-end framework that transfers knowledge from a pre-trained teacher model (trained with DANN on both source and target domains) to a compact student model. The method employs two discriminators: a feature-domain discriminator that aligns teacher and student feature representations through adversarial learning for universal knowledge transfer, and a data-domain discriminator that prioritizes domain-shared samples for joint knowledge transfer. The student model is trained with a combination of feature alignment, weighted logit distillation (based on data-domain discriminator output), and cross-entropy on source data. The framework is evaluated on UCI HAR, HHAR, FD, and SSC datasets, demonstrating significant improvements over state-of-the-art benchmarks while reducing model complexity by approximately 15x.

## Key Results
- Achieves up to 10.3% improvement in macro F1-score compared to source-only training across all four datasets
- Reduces model complexity by approximately 15x in terms of parameters and FLOPs
- Consistently outperforms state-of-the-art benchmarks including DDC, MDDA, HoMM, CDAN, DIRT-T, JKU, AAD, and MobileDA
- Shows particular strength in scenarios with moderate to high domain shift between source and target domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial learning with a feature-domain discriminator effectively transfers universal knowledge across source and target domains.
- Mechanism: The feature-domain discriminator identifies whether input features come from the teacher or student model. During adversarial training, the student is optimized to generate feature maps similar to the teacher's, aligning representations across domains. This forces the student to learn domain-invariant features that capture universal characteristics present in both domains.
- Core assumption: Universal knowledge exists as shared feature representations that can be aligned across domains through adversarial training.
- Evidence anchors: [abstract] "A feature-domain discriminator is employed to align teacher's and student's representations for universal knowledge transfer"; [section] "We train Df and student ψ in an adversarial manner... By minimizing LGEN... the discriminator Df is expected to be incapable of telling whether the features are from Ψ or ψ"

### Mechanism 2
- Claim: The data-domain discriminator prioritizes domain-shared samples for joint knowledge transfer by disentangling domain-specific from domain-shared knowledge.
- Mechanism: The data-domain discriminator outputs probabilities indicating whether samples belong to source or target domain. Samples with similar probabilities for both domains (near decision boundary) are considered to have domain-shared (joint) knowledge. The weight wi = 1 - |pc=0 - pc=1| assigns higher importance to these samples during logit-level distillation.
- Core assumption: Samples that the data-domain discriminator cannot confidently classify contain more domain-shared information than those it classifies with high confidence.
- Evidence anchors: [abstract] "A data-domain discriminator is utilized to prioritize the domain-shared samples for joint knowledge transfer"; [section] "if the data-domain discriminator Dd cannot distinguish certain sample in the feature space, this sample most likely belongs to P(Xsrc)∩Q(Xtgt) and possesses more domain-joint knowledge"

### Mechanism 3
- Claim: Combining universal feature-level knowledge transfer with joint logit-level knowledge transfer yields superior cross-domain performance compared to either approach alone.
- Mechanism: Universal knowledge (via feature alignment) provides domain-invariant representations, while joint knowledge (via weighted logit distillation) ensures the student focuses on domain-shared information. These complementary approaches address both the domain shift and the need to transfer relevant knowledge.
- Core assumption: Universal and joint knowledge are complementary and both necessary for effective cross-domain compression.
- Evidence anchors: [abstract] "we propose to transfer both the universal feature-level knowledge across source and target domains and the joint logit-level knowledge shared by both domains"; [section] "These two types of knowledge are complementary to each other"

## Foundational Learning

- Concept: Adversarial learning for knowledge transfer
  - Why needed here: Enables implicit alignment of teacher and student representations without requiring explicit distance metrics
  - Quick check question: What is the difference between minimizing a distance metric and using adversarial learning for feature alignment?

- Concept: Domain adaptation with discriminators
  - Why needed here: Provides a mechanism to identify and prioritize domain-shared vs domain-specific knowledge
  - Quick check question: How does a discriminator's confidence level relate to whether a sample contains domain-shared or domain-specific knowledge?

- Concept: Kullback-Leibler divergence for distillation
  - Why needed here: Measures the difference between teacher and student probability distributions for knowledge transfer
  - Quick check question: What role does the temperature parameter τ play in smoothing the probability distributions during distillation?

## Architecture Onboarding

- Component map:
  Pre-trained teacher model (Ψ) -> Student model (ψ) -> Feature-domain discriminator (Df) -> Data-domain discriminator (Dd) -> Combined loss function

- Critical path:
  1. Pre-train teacher with DANN on both domains
  2. Initialize student as compact version of teacher
  3. Alternately train Df (fix student) and student (fix Df)
  4. Train Dd and student simultaneously with weighted logit distillation
  5. Optimize with combined loss including cross-entropy on source data

- Design tradeoffs:
  - Feature distillation vs logit distillation: Features contain more information but are higher-dimensional; logits are lower-dimensional but may lose fine-grained information
  - Discriminator accuracy vs student learning: Too accurate discriminators early in training can prevent effective student learning
  - Weighting scheme: The choice of wi = 1 - |pc=0 - pc=1| balances focus between domain-shared and domain-specific knowledge

- Failure signatures:
  - Student performance worse than source-only training on certain transfer scenarios (negative transfer)
  - Slow or unstable convergence during adversarial training
  - Large performance gap between teacher and student despite training
  - Overfitting to source domain when β is too high

- First 3 experiments:
  1. Compare feature-domain discriminator vs L2 distance for feature alignment on a simple cross-domain task
  2. Test different weighting schemes for wi (e.g., exponential, linear) to see impact on domain-shared knowledge prioritization
  3. Evaluate the effect of gradually increasing α (from 0.1 to 0.9) versus using a fixed α value throughout training

## Open Questions the Paper Calls Out
- How does the performance of UNI-KD scale with increasing domain shift between source and target domains?
- Can the proposed adversarial learning scheme be extended to handle multi-source domain adaptation scenarios?
- How sensitive is the proposed method to the choice of hyperparameters (α and β) across different datasets and tasks?

## Limitations
- The evaluation focuses primarily on macro F1-score, though other metrics are also reported in supplementary materials
- The ablation studies could be more comprehensive - for instance, the paper mentions but doesn't fully explore different weighting schemes for the data-domain discriminator
- Limited analysis of how the optimal hyperparameters (α and β) vary across different datasets and tasks

## Confidence

**Confidence Labels:**
- High confidence: The core mechanism of using adversarial learning for feature alignment and the general framework architecture
- Medium confidence: The specific weighting scheme (wi = 1 - |pc=0 - pc=1|) for prioritizing domain-shared samples, as limited direct evidence exists in the literature
- Medium confidence: The claim that universal and joint knowledge are strictly complementary, as this is based on observed performance improvements rather than theoretical proof

## Next Checks
1. Test alternative weighting schemes for the data-domain discriminator (e.g., exponential decay based on confidence scores) to verify the proposed linear weighting is optimal
2. Conduct ablation studies on different teacher pre-training strategies (e.g., comparing DANN vs other domain adaptation methods) to isolate the impact of the pre-trained teacher
3. Evaluate on additional time series datasets with varying domain shifts (both small and large) to test the method's robustness across different transfer scenarios