---
ver: rpa2
title: 'Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of Agents'
arxiv_id: '2309.17207'
source_url: https://arxiv.org/abs/2309.17207
tags:
- agent
- memory
- trxl
- learning
- endless
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Memory Gym introduces a benchmark for assessing memory capabilities\
  \ in Deep Reinforcement Learning agents. It features three 2D partially observable\
  \ environments\u2014Mortar Mayhem, Mystery Path, and Searing Spotlights\u2014that\
  \ require agents to memorize sequences, navigate paths, and track hidden objects."
---

# Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of Agents

## Quick Facts
- arXiv ID: 2309.17207
- Source URL: https://arxiv.org/abs/2309.17207
- Reference count: 34
- Key outcome: Memory Gym introduces a benchmark for assessing memory capabilities in Deep Reinforcement Learning agents, with GRU outperforming TrXL in endless tasks despite TrXL's superior sample efficiency in finite environments.

## Executive Summary
Memory Gym introduces a novel benchmark for evaluating memory capabilities in Deep Reinforcement Learning agents through three 2D partially observable environments: Mortar Mayhem, Mystery Path, and Searing Spotlights. These environments require agents to memorize sequences, navigate paths, and track hidden objects. The benchmark extends finite tasks into endless formats that incrementally increase difficulty, inspired by the game "I packed my bag." The study compares Gated Recurrent Unit (GRU) and Transformer-XL (TrXL) with Proximal Policy Optimization, finding that while TrXL demonstrates superior sample efficiency in finite tasks, GRU outperforms TrXL by significant margins in all endless tasks due to robustness against stale episodic memory accumulation.

## Method Summary
The Memory Gym benchmark uses three 2D partially observable environments with both finite and endless variants. Agents are trained using Proximal Policy Optimization with either Gated Recurrent Unit (GRU) or Transformer-XL (TrXL) as memory encoders. Visual observations are processed through an Atari CNN, with optional observation reconstruction as an auxiliary loss. The endless tasks incrementally increase difficulty by extending episode length and complexity. Performance is evaluated using sample efficiency (steps to reach performance) and effectiveness (performance at convergence), with IQM and 95% bootstrapped confidence intervals for statistical robustness.

## Key Results
- In finite environments, TrXL demonstrates superior sample efficiency in Mystery Path and outperforms in Mortar Mayhem
- GRU is more sample efficient in Searing Spotlights and outperforms TrXL in all endless tasks by significant margins
- Observation reconstruction improves sample efficiency in visually rich environments but adds computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer-XL (TrXL) achieves higher sample efficiency in finite environments due to its episodic memory with sliding window allowing broader context access compared to GRU's sequential updates.
- Mechanism: TrXL's self-attention mechanism attends to past activations stored in episodic memory over a fixed-length sliding window, enabling parallel context aggregation without being limited by sequential recurrence. GRU updates hidden states step-by-step, constraining temporal integration.
- Core assumption: The finite task lengths fit within TrXL's sliding window capacity, and the additional memory access overhead is offset by richer context integration.
- Evidence anchors:
  - [abstract]: "TrXL demonstrates superior sample efficiency in Mystery Path and outperforms in Mortar Mayhem"
  - [section 3.4]: "Leveraging past activations in the input sequence, the agent's memory facilitates the ability to attend to events that extend beyond the boundaries of the fixed-length memory window"

### Mechanism 2
- Claim: GRU outperforms TrXL in endless environments due to robustness against stale episodic memory from off-policy data accumulation during PPO's multiple training epochs.
- Mechanism: PPO uses sampled data for multiple epochs; in endless environments, early episode steps become outdated while the episode continues. GRU's hidden state naturally updates with new data, while TrXL's episodic memory accumulates stale context, harming learning.
- Core assumption: Endless tasks generate episodes longer than PPO's data sequence length, causing meaningful off-policy data retention in episodic memory.
- Evidence anchors:
  - [section 4.3.4]: "PPO is an on-policy algorithm that consumes the training data for a few epochs. Once the second training epoch on the same batch commences, the batch is already considered off-policy"
  - [section 4.2]: "GRU surpassed TrXL by large margins" in endless environments

### Mechanism 3
- Claim: GRU's simpler architecture and fewer parameters enable faster convergence and better sample efficiency in certain environments, offsetting TrXL's potential representational power.
- Mechanism: GRU has ~4.05M parameters vs TrXL's ~2.8M; fewer parameters reduce optimization complexity and overfitting risk, enabling quicker policy learning, especially where memory demands are moderate.
- Core assumption: Task complexity does not require TrXL's full representational capacity; simpler recurrent dynamics suffice for effective memory use.
- Evidence anchors:
  - [section 4.3.1]: "Our TrXL baseline comprises 2.8 million trainable parameters, while GRU consists of 4.05 million"
  - [section 4.1]: "GRU, both with and without observation reconstruction, outperforms its respective TrXL counterparts in Searing Spotlights"

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: Memory Gym environments are designed to be unsolvable without memory, making them POMDPs where agents must infer hidden state from observation history.
  - Quick check question: Why can't an agent solve Mystery Path without memory, even if it knows the path structure?

- Concept: Recurrent Neural Networks (RNNs) and Backpropagation Through Time (BPTT)
  - Why needed here: GRU is used as a memory encoder; understanding how RNNs process sequences and are trained via truncated BPTT is essential for debugging and extending the baseline.
  - Quick check question: How does truncated BPTT in GRU differ from full BPTT, and why is it used in this RL context?

- Concept: Self-Attention and Transformer Architectures
  - Why needed here: TrXL uses self-attention over episodic memory; understanding attention mechanisms, positional encoding, and sliding memory windows is critical for interpreting its behavior.
  - Quick check question: What role does the strictly lower triangular mask play in TrXL's attention computation?

## Architecture Onboarding

- Component map: Observation → Atari CNN → Memory Encoder (GRU/TrXL) → Policy/Value/Reconstruction Heads → Loss → Update
- Critical path: Observation → Encoding → Memory Encoder → Heads → Loss → Update
- Design tradeoffs:
  - TrXL vs GRU: TrXL offers parallel context access but risks stale memory; GRU is simpler and more robust but sequential.
  - Observation reconstruction: Improves sample efficiency in visually rich environments but adds computation.
  - Sliding window size: Larger windows capture longer dependencies but increase memory and compute cost.
- Failure signatures:
  - TrXL: Performance plateaus or degrades in long episodes; instability when episodic memory grows stale.
  - GRU: Slower convergence in tasks with rich long-range dependencies; may underfit complex patterns.
  - Reconstruction loss: Can cause instability if reconstruction target is noisy or sparse.
- First 3 experiments:
  1. Train both GRU and TrXL on Mortar Mayhem Grid (simplified version) to confirm memory is necessary and compare baseline performance.
  2. Vary TrXL's sliding window length on Endless Mortar Mayhem to test sensitivity to stale memory.
  3. Add relative positional encoding to TrXL's query on Endless Mortar Mayhem to test if temporal awareness improves performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Transformer-XL (TrXL) scale with model capacity in endless memory tasks?
- Basis in paper: [inferred] The paper notes that scaling up TrXL did not close the performance gap with GRU, but further investigation into the relationship between model capacity and performance was not conducted.
- Why unresolved: The study only experimented with a limited range of model capacities due to computational constraints and did not explore the potential non-linear relationship between capacity and performance in endless tasks.
- What evidence would resolve it: A systematic study varying TrXL model capacity across a wider range, potentially using different architectures or training regimes, could reveal if there is a capacity threshold where TrXL outperforms GRU in endless tasks.

### Open Question 2
- Question: What is the impact of episodic memory staleness on TrXL's performance in endless memory tasks?
- Basis in paper: [inferred] The paper suggests that off-policy data in TrXL's episodic memory might hinder training progress, but this was not explicitly tested.
- Why unresolved: The study did not investigate the effects of different memory refresh rates or the impact of stale data on TrXL's performance in endless tasks.
- What evidence would resolve it: Experiments comparing TrXL's performance with different memory refresh rates or methods to mitigate the impact of stale data would clarify the role of episodic memory staleness.

### Open Question 3
- Question: How do other memory architectures, such as structured state space models or HCAM, perform on Memory Gym's endless tasks?
- Basis in paper: [explicit] The paper mentions the potential of exploring other memory mechanisms like structured state space models and HCAM, but these were not tested.
- Why unresolved: The study focused on comparing GRU and TrXL, leaving the performance of other memory architectures unexplored in the context of endless tasks.
- What evidence would resolve it: Implementing and benchmarking other memory architectures, such as HCAM or structured state space models, on Memory Gym's endless tasks would provide a more comprehensive understanding of their effectiveness in this domain.

## Limitations

- The comparison between GRU and TrXL is primarily evaluated in 2D partially observable environments, which may not fully capture performance differences in more complex 3D or real-world scenarios.
- The effectiveness of TrXL's episodic memory mechanism depends heavily on task length relative to the sliding window size, creating potential blind spots for very long-range dependencies.
- The observation reconstruction auxiliary loss, while shown to improve sample efficiency in some cases, may not generalize to environments with more complex or high-dimensional observation spaces.

## Confidence

- High confidence: GRU's superior performance in endless tasks (supported by multiple experiments and clear failure mode analysis of TrXL's stale episodic memory)
- Medium confidence: TrXL's sample efficiency advantages in finite tasks (consistent results but limited to specific environment types)
- Medium confidence: The mechanism explaining GRU's advantage in endless tasks through off-policy data accumulation (well-reasoned but requires further validation)

## Next Checks

1. Test TrXL with dynamic sliding window adjustment: Implement an adaptive mechanism that resets or adjusts the episodic memory window based on episode length to address stale memory accumulation in endless tasks.

2. Evaluate on extended task horizons: Create variants of the Memory Gym environments with significantly longer task sequences (exceeding current sliding window limits) to test the theoretical break conditions for both GRU and TrXL mechanisms.

3. Cross-domain generalization study: Apply the GRU and TrXL architectures to a different benchmark domain (such as Atari games or continuous control tasks) to assess whether the observed memory performance patterns hold across diverse RL problem types.