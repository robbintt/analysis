---
ver: rpa2
title: 'MasakhaNEWS: News Topic Classification for African languages'
arxiv_id: '2304.09972'
source_url: https://arxiv.org/abs/2304.09972
tags:
- languages
- language
- news
- african
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MasakhaNEWS, a new benchmark dataset for
  news topic classification covering 16 African languages. The dataset addresses the
  scarcity of evaluation datasets for African languages in NLP research.
---

# MasakhaNEWS: News Topic Classification for African languages

## Quick Facts
- arXiv ID: 2304.09972
- Source URL: https://arxiv.org/abs/2304.09972
- Reference count: 25
- Key outcome: MasakhaNEWS benchmark dataset for news topic classification covering 16 African languages, with evaluation showing AfroXLMR models outperform generic multilingual models

## Executive Summary
This paper introduces MasakhaNEWS, a new benchmark dataset for news topic classification across 16 African languages. The dataset addresses the scarcity of evaluation resources for African languages in NLP research through a multi-stage annotation process involving native speakers. The authors evaluate various baseline models including classical machine learning approaches and fine-tuning multilingual language models, demonstrating that African-centric models like AfroXLMR-base and AfroXLMR-large significantly outperform generic alternatives.

## Method Summary
The MasakhaNEWS dataset was created through a multi-stage annotation process involving native speakers, with inter-annotator agreement scores ranging from 0.55 to 0.85. The authors evaluated multiple approaches including classical ML models (Naive Bayes, MLP, XGBoost), fine-tuning multilingual language models (XLM-R, AfriBERTa, AfroXLMR), and zero/few-shot learning methods (MAD-X, PET, SetFit, ChatGPT). The few-shot learning approach using Pattern Exploiting Training (PET) achieved over 90% of full supervised training performance with as little as 10 examples per label.

## Key Results
- Fine-tuning African-centric models (AfroXLMR-base and AfroXLMR-large) outperforms generic multilingual models for news topic classification
- Few-shot learning with PET achieves 86.0 F1 points with 10 examples per label, compared to 92.6 F1 points with full supervised training
- Zero-shot prompting of ChatGPT achieves 70 F1 points on average across low-resource African languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual transfer with MAD-X improves performance by leveraging shared linguistic features across African languages.
- Mechanism: MAD-X uses modular adapters that can be trained on a source language (e.g., Hausa or Swahili) and applied to a target language without full fine-tuning, capturing cross-lingual generalizations.
- Core assumption: The source and target languages share sufficient typological or lexical overlap to make adapter-based transfer effective.
- Evidence anchors:
  - [abstract] "Our evaluation in zero-shot setting shows the potential of prompting ChatGPT for news topic classification in low-resource African languages, achieving an average performance of 70 F1 points without leveraging additional supervision like MAD-X."
  - [section 6.1] "MAD-X 2.0 (Pfeiffer et al., 2020; 2021) - a parameter efficient approach for cross-lingual transfer leveraging the modularity, and portability of adapters"
- Break condition: If source and target languages are too distant in typological features, transfer benefits diminish and performance may drop below fine-tuning from scratch.

### Mechanism 2
- Claim: Few-shot PET/iPET outperforms full supervised training when few labeled examples are available.
- Mechanism: PET/iPET restructures input sequences into patterns and verbalizers that guide the LLM to map input to labels, and iteratively refines predictions using unlabeled data (iPET), effectively amplifying small labeled sets.
- Core assumption: The language model's pre-training includes sufficient semantic understanding of the target language to generalize from few examples when prompted correctly.
- Evidence anchors:
  - [abstract] "In few-shot setting, we show that with as little as 10 examples per label, we achieved more than 90% (i.e. 86.0 F1 points) of the performance of full supervised training (92.6 F1 points) leveraging the PET approach."
  - [section 6.1] "PET/iPET (Schick & SchÃ¼tze, 2021a;b), also known as (Iterative) Pattern Exploiting Training is a semi-supervised approach that used restructured input sequences to condition language models to better understand a given task"
- Break condition: If the LLM has not seen the target language during pre-training, pattern-based few-shot learning may fail to bootstrap meaningful mappings.

### Mechanism 3
- Claim: Fine-tuning African-centric models (AfroXLMR) significantly outperforms generic multilingual models due to domain-specific language exposure.
- Mechanism: AfroXLMR models are adapted from XLM-R by further training on African language corpora, improving alignment with linguistic structures and vocabularies specific to African languages.
- Core assumption: The additional pre-training on African languages reduces the domain gap between generic multilingual models and the target language distribution.
- Evidence anchors:
  - [abstract] "The results show that fine-tuning African-centric models like AfroXLMR-base and AfroXLMR-large outperform other approaches."
  - [section 5.1] "The African-centric multilingual text encoders are all modeled after XLM-R. AfriBERTa was pretrained from scratch on 11 African languages, AfroXLMR was adapted to African languages through fine-tuning the original XLM-R model on 17 African languages"
- Break condition: If the African-centric model lacks coverage of the target language, gains from adaptation disappear.

## Foundational Learning

- Concept: Text classification fundamentals (feature extraction, label mapping, evaluation metrics)
  - Why needed here: The task is news topic classification; understanding feature representations and evaluation (F1) is essential to interpret results.
  - Quick check question: What is the difference between precision and recall, and why is F1 used here?

- Concept: Fine-tuning vs. prompt-tuning in NLP
  - Why needed here: The paper compares full fine-tuning, adapter-based fine-tuning, and prompt-based methods; understanding the trade-offs is key to interpreting performance differences.
  - Quick check question: When would prompt-tuning be preferred over full fine-tuning?

- Concept: Zero-shot and few-shot learning paradigms
  - Why needed here: The evaluation includes zero-shot transfer and few-shot learning; knowing how these paradigms work explains the experimental design and results.
  - Quick check question: What is the main advantage of few-shot learning over zero-shot learning?

## Architecture Onboarding

- Component map: Data collection -> Annotation -> Model fine-tuning -> Zero/few-shot evaluation -> Prompt-based evaluation
- Critical path: Clean news corpus -> Native speaker annotation -> Baseline model training -> Adapter/few-shot experiments -> ChatGPT prompting
- Design tradeoffs: Headline-only vs. headline+text inputs; model size vs. coverage; labeled data volume vs. annotation cost
- Failure signatures: Low inter-annotator agreement indicates ambiguous labeling; poor cross-lingual transfer indicates insufficient linguistic overlap; few-shot methods failing suggests model unfamiliarity with target language
- First 3 experiments:
  1. Train a simple Naive Bayes classifier on headline+text to establish a low-resource baseline.
  2. Fine-tune AfroXLMR-base on MasakhaNEWS TRAIN split and evaluate on TEST split to verify African-centric model benefit.
  3. Apply MAD-X adapter fine-tuning using Hausa as source language to test cross-lingual transfer to another African language.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ChatGPT compare to other multilingual models like BLOOM or XGLM when evaluated on the MasakhaNEWS dataset?
- Basis in paper: [inferred] The paper mentions plans to extend the evaluation to include bigger multilingual models like BLOOM and XGLM in future work.
- Why unresolved: The paper focuses on evaluating several baseline models and few-shot learning approaches but does not include these larger multilingual models in the current study.
- What evidence would resolve it: Experimental results comparing the performance of ChatGPT, BLOOM, and XGLM on the MasakhaNEWS dataset using both zero-shot and few-shot learning settings.

### Open Question 2
- Question: What is the impact of using different news article content types (e.g., headline only, headline + first paragraph, full article) on the performance of news topic classification models for African languages?
- Basis in paper: [explicit] The paper compares the performance of models using headline-only versus headline+text content and shows improvements with headline+text, especially for classical ML methods.
- Why unresolved: The paper only considers headline and headline+text content types, leaving the potential benefits of using other content types unexplored.
- What evidence would resolve it: Experimental results comparing the performance of models using different content types (e.g., headline, headline+first paragraph, full article) on the MasakhaNEWS dataset.

### Open Question 3
- Question: How does the performance of news topic classification models vary across different African language families (e.g., Niger-Congo, Afro-Asiatic, Indo-European)?
- Basis in paper: [explicit] The paper covers four language families (Niger-Congo, Afro-Asiatic, Indo-European, and English Creole) and provides performance metrics for each language.
- Why unresolved: The paper does not analyze the performance differences across language families, which could provide insights into the challenges and opportunities for news topic classification in different linguistic contexts.
- What evidence would resolve it: A comparative analysis of model performance across different language families in the MasakhaNEWS dataset, highlighting strengths, weaknesses, and potential reasons for performance variations.

## Limitations
- Dataset size varies significantly across languages, with some having only 1,000-2,000 examples
- Inter-annotator agreement scores (0.55-0.85) suggest some ambiguity in topic classification
- Zero-shot ChatGPT results may have reproducibility challenges due to API dependencies
- Results focus on news headlines and short text, may not generalize to longer document classification

## Confidence
- **High confidence**: The dataset creation methodology and baseline model comparisons (AfroXLMR outperforming generic models) are well-supported by experimental results
- **Medium confidence**: The zero-shot and few-shot learning results are promising but may have reproducibility challenges due to API dependencies and prompt engineering sensitivity
- **Medium confidence**: The cross-lingual transfer claims via MAD-X are supported by results but would benefit from more extensive ablation studies across different language pairs

## Next Checks
1. Attempt to replicate the ChatGPT zero-shot results using the exact prompts and evaluation methodology described in the paper to verify the 70 F1 score claim.
2. Examine which of the 16 African languages have sufficient pre-training data in the evaluated models to determine if performance differences correlate with pre-training language coverage.
3. Analyze the class distribution across languages and evaluate whether weighted F1 adequately captures performance differences compared to other metrics like macro F1 or per-class precision/recall.