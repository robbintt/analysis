---
ver: rpa2
title: How much can ChatGPT really help Computational Biologists in Programming?
arxiv_id: '2309.09126'
source_url: https://arxiv.org/abs/2309.09126
tags:
- chatgpt
- code
- asked
- computational
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ChatGPT offers significant potential as a programming assistant\
  \ for computational biologists, helping with tasks like code generation, debugging,\
  \ explanation, conversion, and pipeline creation. While not perfect\u2014occasionally\
  \ providing made-up functions or missing critical details\u2014it can substantially\
  \ reduce development time and improve code understanding, especially for those new\
  \ to programming or bioinformatics tools."
---

# How much can ChatGPT really help Computational Biologists in Programming?

## Quick Facts
- arXiv ID: 2309.09126
- Source URL: https://arxiv.org/abs/2309.09126
- Reference count: 40
- Primary result: ChatGPT shows significant potential as a programming assistant for computational biologists, helping with code generation, debugging, explanation, conversion, and pipeline creation, though users should verify outputs for accuracy.

## Executive Summary
ChatGPT demonstrates substantial potential as a programming assistant for computational biologists, offering capabilities in code generation, debugging, explanation, conversion, and pipeline creation. The large language model can generate functional code snippets, explain complex algorithms in accessible terms, and automate routine programming tasks. While not perfect and occasionally producing made-up functions or missing critical details, ChatGPT can substantially reduce development time and improve code understanding, particularly for researchers new to programming or bioinformatics tools. The tool's ability to generate bash commands, Python scripts, and R code, along with providing clear explanations and documentation, makes it a valuable resource for the computational biology community.

## Method Summary
The paper evaluates ChatGPT's utility as a programming assistant through direct interaction with the model, asking questions and requesting code for various bioinformatics tasks. The authors use illustrative examples and hypothetical scenarios to test ChatGPT's capabilities in code generation, debugging, explanation, conversion, and pipeline creation. Rather than using specific datasets, the assessment relies on qualitative evaluation of ChatGPT's responses to bioinformatics programming challenges. The paper examines the model's performance across multiple use cases including code writing, reviewing, debugging, converting, refactoring, and pipelining from the perspective of computational biologists.

## Key Results
- ChatGPT can generate functional code for common bioinformatics tasks and explain basic programming concepts effectively
- The tool significantly reduces development time by automating routine programming tasks like file format conversion and pipeline creation
- ChatGPT improves code understanding for users from diverse backgrounds through explanation and documentation generation
- The model has limitations including occasional made-up functions, outdated information (pre-2022 knowledge cutoff), and token limits affecting explanation of large codebases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT can serve as a functional programming assistant by generating code, debugging, and explaining algorithms for computational biologists.
- Mechanism: Large language model pretrained on diverse text data (including code) uses attention-based transformer architecture to produce contextually relevant outputs for programming tasks.
- Core assumption: The training data included sufficient computational biology and bioinformatics code examples for the model to learn relevant patterns.
- Evidence anchors:
  - [abstract] "This paper focuses on the potential influence (both positive and negative) of ChatGPT in the mentioned aspects with illustrative examples from different perspectives."
  - [section] "We cover use cases such as code writing, reviewing, debugging, converting, refactoring and pipelining using ChatGPT from the perspective of computational biologists in this paper."
  - [corpus] Weak evidence - corpus contains general programming papers but no specific bioinformatics coding papers.
- Break condition: Model encounters novel bioinformatics tools or algorithms not represented in its training corpus (limited to pre-2022 knowledge).

### Mechanism 2
- Claim: ChatGPT reduces development time by automating routine programming tasks like file format conversion and pipeline creation.
- Mechanism: Autoregressive language modeling generates code snippets based on natural language descriptions, eliminating the need for manual lookup of syntax.
- Core assumption: Computational biology tasks follow predictable patterns that can be encoded in template-like responses.
- Evidence anchors:
  - [section] "We asked ChatGPT to create a pipeline script that (a) uses samtools to downsample a bam file and (b) creates a new bam file from the output bam file of step (a) that only has fragments over length 400."
  - [section] "ChatGPT used samtools and Pysam to achieve this but created the intermediate bam file."
  - [corpus] Weak evidence - corpus lacks specific examples of pipeline automation tools.
- Break condition: Task requires complex error handling or domain-specific knowledge beyond common patterns.

### Mechanism 3
- Claim: ChatGPT improves code understanding for users from diverse backgrounds through explanation and documentation generation.
- Mechanism: Natural language processing capabilities allow the model to translate between technical jargon and plain language explanations.
- Core assumption: Users can formulate their questions clearly enough for the model to provide relevant explanations.
- Evidence anchors:
  - [section] "ChatGPT can make algorithms easier. If you ask ChatGPT to describe BLAST search to you as if you were a 5-year-old, then ChatGPT will tell you about this search using pictures and puzzles, which is really easy to understand."
  - [section] "We asked ChatGPT to explain the 'SeqIO.parse' function without mentioning the function source. It immediately realized that this function was from Biopython library and gave us a sample code with this function."
  - [corpus] No evidence - corpus contains no examples of code explanation capabilities.
- Break condition: User lacks sufficient domain knowledge to verify the accuracy of explanations provided.

## Foundational Learning

- Concept: Transformer architecture with attention mechanism
  - Why needed here: Understanding the underlying technology helps predict limitations and failure modes
  - Quick check question: What is the maximum context length for ChatGPT and how might this limit its ability to explain long codebases?

- Concept: Bioinformatics file formats (BAM, SAM, FASTA, VCF)
  - Why needed here: ChatGPT generates code that manipulates these formats, requiring user understanding to verify correctness
  - Quick check question: What are the key differences between BAM and SAM formats and why would you choose one over the other?

- Concept: Basic programming concepts (loops, conditionals, functions)
  - Why needed here: Users need to understand the generated code to integrate it effectively into their projects
  - Quick check question: How would you identify and fix a syntax error in Python code generated by ChatGPT?

## Architecture Onboarding

- Component map: User prompt → Natural language understanding → Code generation → Output formatting
- Critical path: User prompt → Natural language understanding → Code generation → Output formatting
- Design tradeoffs:
  - Generality vs. specificity: Model trained on broad data may lack domain-specific optimizations
  - Accuracy vs. confidence: Model generates plausible-sounding but potentially incorrect responses
  - Static vs. dynamic: Fixed knowledge base cannot incorporate recent developments
- Failure signatures:
  - Made-up functions or libraries that don't exist
  - Outdated information (pre-2022 knowledge cutoff)
  - Overconfidence in incorrect answers
- First 3 experiments:
  1. Test basic code generation: Ask for simple Python function to count lines in a file
  2. Test domain knowledge: Request explanation of a common bioinformatics algorithm
  3. Test error handling: Present a code snippet with deliberate errors and ask for debugging assistance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How accurate is ChatGPT in generating bioinformatics-specific code compared to specialized tools like Copilot or AlphaCode?
- Basis in paper: [explicit] The paper mentions that while ChatGPT can generate code, it may occasionally provide made-up functions or miss critical details. It also compares ChatGPT's general-purpose nature to specialized tools like OpenAI's Codex and DeepMind's AlphaCode.
- Why unresolved: The paper does not provide a direct comparison of ChatGPT's accuracy in generating bioinformatics-specific code against specialized tools.
- What evidence would resolve it: A controlled study comparing the accuracy and efficiency of code generation for bioinformatics tasks between ChatGPT, Copilot, and AlphaCode.

### Open Question 2
- Question: What are the long-term implications of using ChatGPT for programming assistance in computational biology, particularly regarding the skill development of new researchers?
- Basis in paper: [inferred] The paper discusses how ChatGPT can assist researchers from diverse backgrounds, including those with non-programming backgrounds, in coding tasks. It also mentions the importance of understanding algorithms to use ChatGPT effectively.
- Why unresolved: The paper does not explore the potential impact of relying on ChatGPT for programming tasks on the skill development and learning curve of new computational biologists.
- What evidence would resolve it: Longitudinal studies tracking the skill development of computational biology researchers who use ChatGPT versus those who do not, over several years.

### Open Question 3
- Question: How does the token limit of ChatGPT affect its ability to explain and comment on large codebases in computational biology?
- Basis in paper: [explicit] The paper mentions that ChatGPT has a token limit of 4096 while generating answers, which can be a constraint when explaining long codebases.
- Why unresolved: The paper provides an example of ChatGPT stopping midway while commenting on a 400-line codebase but does not explore the broader implications of this limitation.
- What evidence would resolve it: An analysis of the effectiveness of ChatGPT's explanations and comments on codebases of varying lengths, including those that exceed the token limit.

## Limitations

- The evaluation relies entirely on hypothetical scenarios and illustrative examples rather than systematic testing with real computational biology datasets or projects
- The paper lacks quantitative metrics for evaluating code correctness, performance, or developer productivity improvements
- The assessment does not address security implications of using ChatGPT-generated code in sensitive research environments or discuss potential intellectual property concerns

## Confidence

**High confidence**: ChatGPT can generate functional code for common bioinformatics tasks and explain basic programming concepts. This is well-supported by the paper's examples of working code generation and the established capabilities of transformer-based language models.

**Medium confidence**: ChatGPT significantly reduces development time for computational biologists. While the paper suggests this benefit, it lacks quantitative evidence comparing development time with and without ChatGPT assistance.

**Low confidence**: ChatGPT reliably handles complex bioinformatics pipelines and novel algorithms. The paper acknowledges limitations with made-up functions and outdated information, suggesting reliability decreases for complex or cutting-edge tasks.

## Next Checks

1. **Systematic accuracy testing**: Create a benchmark suite of 50+ real computational biology coding tasks with ground truth solutions, then measure ChatGPT's success rate in generating correct, functional code across different complexity levels.

2. **Time efficiency study**: Conduct a controlled experiment with 20 computational biology students working on identical programming assignments, half using ChatGPT and half working traditionally, measuring both time-to-completion and code quality through peer review.

3. **Knowledge currency assessment**: Design a test suite targeting bioinformatics tools and algorithms released after ChatGPT's training cutoff (post-2022), evaluating the model's ability to correctly identify knowledge gaps and suggest appropriate workarounds or alternatives.