---
ver: rpa2
title: 'ExpeL: LLM Agents Are Experiential Learners'
arxiv_id: '2308.10144'
source_url: https://arxiv.org/abs/2308.10144
tags:
- expel
- agent
- insights
- learning
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ExpeL, a novel LLM agent that learns from
  experience without parameter updates. The agent autonomously gathers experiences
  and extracts insights from training tasks, then uses these insights and recalled
  successful trajectories during inference.
---

# ExpeL: LLM Agents Are Experiential Learners

## Quick Facts
- arXiv ID: 2308.10144
- Source URL: https://arxiv.org/abs/2308.10144
- Reference count: 40
- Key outcome: Novel LLM agent learns from experience without parameter updates, achieving 39% success rate on HotpotQA vs 28% for ReAct

## Executive Summary
This paper introduces ExpeL, a novel LLM agent that learns from experience without parameter updates. The agent autonomously gathers experiences and extracts insights from training tasks, then uses these insights and recalled successful trajectories during inference. Experiments on three diverse domains (HotpotQA, ALFWorld, WebShop) show ExpeL consistently outperforms strong baselines like ReAct and Act, demonstrating emergent abilities like self-correction and world model belief updates.

## Method Summary
ExpeL is a method for training LLM agents for sequential decision-making tasks without parameter updates. The agent gathers experiences via trial and error using Reflexion with ReAct planning, extracts insights by comparing success/failure pairs and analyzing successful trajectories, and retrieves similar past trajectories during inference. The prompt is augmented with extracted insights and in-context examples from retrieved successful trajectories to guide decision-making.

## Key Results
- ExpeL achieves 39% success rate on HotpotQA compared to 28% for ReAct baseline
- Demonstrates positive forward transfer from source to target tasks
- Shows emergent abilities like self-correction and world model belief updates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ExpeL's performance gains stem from its ability to extract cross-task insights from accumulated experiences.
- Mechanism: During training, the agent uses operations (ADD, UPVOTE, DOWNVOTE, EDIT) to iteratively refine insights based on comparisons between failed and successful trajectories. These insights are appended to task specification during inference.
- Core assumption: The LLM can effectively generate and refine actionable insights from natural language trajectories.
- Evidence anchors: Abstract mentions autonomous experience gathering and insight extraction; section 4.2 discusses insight extraction akin to off-policy learning.
- Break condition: If LLM generates irrelevant or misleading insights, or importance count mechanism fails to filter them out.

### Mechanism 2
- Claim: ExpeL benefits from retrieving semantically similar successful trajectories as in-context examples during inference.
- Mechanism: Faiss vectorstore indexes successful trajectories by task similarity. During inference, agent retrieves top-k most similar trajectories and uses them as few-shot examples in the prompt.
- Core assumption: Task similarity in embedding space correlates with task-solving utility in context.
- Evidence anchors: Section 4.2 proposes experience recall to retrieve successful trajectories based on task similarity; section 5.2 shows contrasting quantitative distinctions when restricting to one mode.
- Break condition: If embedding model fails to capture task-relevant features, retrieved examples will be unhelpful or misleading.

### Mechanism 3
- Claim: ExpeL's transfer learning capability allows insights from source tasks to improve performance on target tasks with minimal target task examples.
- Mechanism: Extracted insights from source tasks are "finetuned" using small number of target task demonstrations via prompt engineering.
- Core assumption: Source and target tasks share enough common knowledge for insights to transfer meaningfully.
- Evidence anchors: Section 4.4 hypothesizes target task few-shot examples better ground insights; section 5.4 shows performance gains from transferred knowledge.
- Break condition: If source and target domains are too dissimilar, transferred insights will be ineffective or harmful.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: ExpeL operates in environments formalized as MDPs where agent must learn policy to maximize cumulative reward.
  - Quick check question: In a deterministic MDP with goal-reaching reward (0/1) and γ=1, what does agent's objective reduce to?

- Concept: Autoregressive Language Models
  - Why needed here: The LLM generates actions token-by-token conditioned on trajectory history and augmented prompt.
  - Quick check question: What decoding strategy is used in ExpeL's evaluation phase to ensure deterministic behavior?

- Concept: Vector Similarity Search (Faiss)
  - Why needed here: Efficiently retrieves top-k similar successful trajectories from experience pool for in-context learning.
  - Quick check question: Which embedding model is used to encode tasks for similarity retrieval in ExpeL?

## Architecture Onboarding

- Component map: Experience Pool (Faiss vectorstore) -> Insight Extractor (GPT-4) -> Policy Engine (GPT-3.5-turbo) -> Retriever (kNN with all-mpnet-base-v2)
- Critical path: Experience Gathering → Insight Extraction → Evaluation (prompt assembly with insights + retrieved examples)
- Design tradeoffs:
  - Using GPT-4 for insights improves quality but increases cost; GPT-3.5-turbo is cheaper but less effective
  - Retrieval by task similarity balances relevance and diversity; random sampling is faster but less effective
  - Limiting insight list size prevents context overflow but may omit useful patterns
- Failure signatures:
  - No performance gain over baseline → insight extraction or retrieval is ineffective
  - Degraded performance vs baseline → misleading insights or irrelevant retrieved examples
  - High variance across runs → instability in insight refinement or retrieval ranking
- First 3 experiments:
  1. Run ExpeL on HotpotQA with default parameters; verify it outperforms ReAct
  2. Disable insight extraction (use only retrieval) and compare performance drop
  3. Swap GPT-4 for GPT-3.5-turbo in insight extraction; observe performance change

## Open Questions the Paper Calls Out
- How does the performance of ExpeL change when using different base language models for insight extraction, particularly when comparing GPT-4 with other large language models?
- How does ExpeL perform in environments with visual observations, and what modifications would be necessary to adapt the framework for multimodal inputs?
- How does the number of training tasks and the diversity of experiences impact the performance of ExpeL, and is there an optimal balance between task quantity and quality?

## Limitations
- Claims about insight extraction quality and transfer learning effectiveness not fully validated due to limited ablation studies
- Reliance on GPT-4 for insight extraction raises questions about scalability and cost-effectiveness
- Evaluation only covers three specific domains, limiting generalizability claims

## Confidence
- High Confidence: Core mechanism of combining experience recall with insight extraction is technically sound and well-implemented
- Medium Confidence: Performance improvements over baselines are demonstrated, but exact contribution of each component varies significantly across domains
- Low Confidence: Claims about emergent abilities like self-correction and world model updates are observational and not rigorously quantified

## Next Checks
1. Conduct systematic ablation study isolating contribution of insight extraction versus trajectory retrieval across all three domains
2. Test cross-domain transfer by training on HotpotQA and evaluating on ALFWorld (or vice versa)
3. Implement cost-benefit analysis comparing GPT-4-based insight extraction with cheaper alternatives for scalability assessment