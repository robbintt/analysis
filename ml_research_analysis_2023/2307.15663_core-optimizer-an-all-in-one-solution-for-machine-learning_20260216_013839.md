---
ver: rpa2
title: 'CoRe Optimizer: An All-in-One Solution for Machine Learning'
arxiv_id: '2307.15663'
source_url: https://arxiv.org/abs/2307.15663
tags:
- optimizer
- core
- learning
- accuracy
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive benchmark of the CoRe optimizer
  against nine other state-of-the-art first-order gradient-based optimizers across
  diverse machine learning tasks. The CoRe optimizer combines Adam-like and RPROP-like
  weight-specific learning rate adaptation with step-dependent decay rates and an
  optional stability-plasticity balance.
---

# CoRe Optimizer: An All-in-One Solution for Machine Learning

## Quick Facts
- arXiv ID: 2307.15663
- Source URL: https://arxiv.org/abs/2307.15663
- Reference count: 0
- This paper presents a comprehensive benchmark of the CoRe optimizer against nine other state-of-the-art first-order gradient-based optimizers across diverse machine learning tasks

## Executive Summary
The CoRe optimizer combines Adam-like and RPROP-like weight-specific learning rate adaptation with step-dependent decay rates and an optional stability-plasticity balance. Benchmark results show that CoRe consistently achieves the highest or competitive performance across diverse machine learning tasks including variational autoencoding, image classification, reinforcement learning, super-resolution, and semi-supervised classification. For lifelong machine learning potential training, CoRe with stability-plasticity balance achieves test set RMSE values of (3.4±0.4) meV/atom for energies and (92±4) meV/Å for atomic forces, outperforming Adam and other optimizers.

## Method Summary
The CoRe optimizer implements first-order gradient-based iterative optimization using PyTorch. The benchmark compares CoRe against nine other optimizers (Adam, RPROP, AdaMax, RMSprop, AdaGrad, AdaDelta, NAG, Momentum, SGD) across nine machine learning tasks. The method uses 20 random initializations per task and includes hyperparameter tuning for learning rates and optimizer-specific parameters. The optimizer combines moving averages of gradients with step-size adjustments based on gradient sign changes, and optionally incorporates stability-plasticity balance through weight importance scoring and freezing.

## Key Results
- CoRe consistently achieves highest or competitive performance across all investigated machine learning applications
- For lifelong machine learning potential training, CoRe achieves test set RMSE of (3.4±0.4) meV/atom for energies and (92±4) meV/Å for atomic forces
- CoRe provides generally applicable hyperparameter values requiring only one intuitive parameter adjustment between mini-batch and batch learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoRe optimizer achieves superior performance by combining Adam-like adaptive learning rates with RPROP-like step-size adaptation based on gradient sign changes
- Mechanism: The optimizer uses moving averages of gradients (Adam-like) for individual weight adaptation, then adjusts step sizes based on whether consecutive gradients have the same sign (RPROP-like). This dual approach handles both the magnitude and direction of gradient changes effectively
- Core assumption: Weight-specific learning rates and step-size adjustments based on gradient sign patterns provide complementary benefits for optimization convergence
- Evidence anchors:
  - [abstract]: "CoRe optimizer combines Adam-like and RPROP-like weight-specific learning rate adaptation with step-dependent decay rates"
  - [section]: "The CoRe optimizer combines Adam-like and RPROP-like weight-specific learning rate adaption. Moreover, in the CoRe optimizer step-dependent decay rates are employed in the calculation of Adam-like gradient moving averages, which are the basis of the RPROP-like step size updates"
  - [corpus]: Weak evidence - no directly comparable studies in corpus
- Break condition: If gradient sign patterns become random or the optimization landscape has many saddle points where gradient sign changes frequently without meaningful direction changes

### Mechanism 2
- Claim: Step-dependent decay rates in gradient moving averages provide better handling of stochastic vs deterministic optimization scenarios
- Mechanism: The decay rate β₁ is made a function of individual weight update counter τ, transitioning from βa₁ (higher dependence on current gradient) to βb₁ (lower dependence on current gradient) via a Gaussian function. This allows more aggressive adaptation early and more stable averaging later
- Core assumption: Different optimization scenarios (mini-batch vs batch learning) benefit from different decay rate behaviors, and a dynamic decay rate can adapt to these scenarios
- Evidence anchors:
  - [section]: "In the CoRe optimizer, β₁ is a function of the individual weight update counter τ, βτ₁ = βb₁ + (βa₁ - βb₁) exp(-(τ-1)²/βc²₁), whereby τ can vary from the counter of gradient calculations t if some optimization steps do not update every weight"
  - [abstract]: "The optimizer's performance advantage is attributed to its effective combination of momentum and individually adapted learning rates, with superior handling of both stochastic and deterministic optimization scenarios"
  - [corpus]: Weak evidence - no directly comparable studies in corpus
- Break condition: If the Gaussian transition function doesn't match the actual optimization dynamics, or if the decay rate adaptation introduces instability in certain training regimes

### Mechanism 3
- Claim: Stability-plasticity balance through weight importance scoring and freezing improves lifelong learning performance
- Mechanism: Weights are assigned importance scores based on their contribution to loss reduction. The plasticity factor Pτξ can freeze weights with high importance scores (top-nfrozen,χ) to prevent forgetting while allowing other weights to adapt. This addresses the stability-plasticity dilemma in lifelong learning
- Core assumption: Identifying and protecting important weights during training prevents catastrophic forgetting while still allowing adaptation to new tasks
- Evidence anchors:
  - [section]: "The plasticity factor, Pτξ = 0 for τ > thist ∧ Sτ-1ξ top-nfrozen,χ in Sτ-1χ 1 otherwise, aims to improve the stability-plasticity balance by regularization in the weight updates"
  - [section]: "The importance score value, Sτξ = Sτ-1ξ + (thist)-1 gτξ · uτξ · Pτξ · sτξ for τ ≤ thist 1 - (thist)-1 · Sτ-1ξ + (thist)-1 gτξ · uτξ · Pτξ · sτξ otherwise, ranks the weight importance by taking into account weight-specific contributions to previously estimated loss function decreases"
  - [abstract]: "For lifelong machine learning potential training, CoRe with stability-plasticity balance achieves test set RMSE values of (3.4±0.4) meV/atom for energies and (92±4) meV/Å for atomic forces, outperforming Adam and other optimizers"
- Break condition: If the importance scoring mechanism incorrectly identifies important weights, or if the freezing mechanism prevents necessary adaptation

## Foundational Learning

- Concept: Gradient-based optimization fundamentals
  - Why needed here: Understanding how optimizers use gradient information to update weights is essential for grasping the CoRe algorithm's innovations
  - Quick check question: What is the difference between using the gradient magnitude versus just the gradient sign in weight updates?

- Concept: Moving averages and exponential decay in optimization
  - Why needed here: CoRe uses moving averages of gradients (like Adam) and their squares (like RMSprop) with decay rates that change over time
  - Quick check question: How does the choice of decay rate affect the optimizer's responsiveness to new gradient information?

- Concept: Stability-plasticity tradeoff in machine learning
  - Why needed here: The CoRe optimizer's optional stability-plasticity balance is crucial for lifelong learning applications
  - Quick check question: What is catastrophic forgetting, and why is it a problem in lifelong learning?

## Architecture Onboarding

- Component map: Loss gradient → gradient moving average calculator (gτξ) with step-dependent decay → squared gradient moving average calculator (hτξ) with fixed decay → adaptive learning rate calculator (uτξ) based on moving averages → step size adjuster (sτξ) based on gradient sign changes → plasticity factor calculator (Pτξ) based on importance scores → weight decay calculator (wtξ) based on update magnitude → weight update

- Critical path: Loss gradient → moving averages → adaptive learning rate → step size adjustment → weight update (with optional weight decay and plasticity control)

- Design tradeoffs: The CoRe optimizer trades increased computational complexity (additional moving averages and calculations) for improved convergence speed and final accuracy. The stability-plasticity balance adds another hyperparameter but enables better lifelong learning performance.

- Failure signatures: Optimizer instability when η− is too small, slow initial convergence when smax is too small for batch learning, or poor performance when pfrozen is set too high for tasks requiring significant weight adaptation.

- First 3 experiments:
  1. Compare CoRe optimizer with default hyperparameters against Adam on a simple MNIST classification task to verify basic performance improvements
  2. Test CoRe with different smax values (0.001, 0.01, 0.1, 1) on a mini-batch learning task to find optimal setting for stochastic optimization
  3. Enable stability-plasticity balance with different pfrozen values on a lifelong learning task to evaluate the impact on preventing forgetting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between the stability-plasticity balance parameter (pfrozen) and the learning rate decay rate (β1) in the CoRe optimizer?
- Basis in paper: [inferred] The paper shows that pfrozen = 0.1 significantly improves lMLP training convergence and final accuracy, but the relationship between this parameter and the learning rate decay dynamics is not explicitly quantified.
- Why unresolved: The paper demonstrates the effectiveness of the stability-plasticity balance but does not provide a systematic analysis of how pfrozen interacts with other hyperparameters like β1 or learning rate schedules.
- What evidence would resolve it: A comprehensive sensitivity analysis varying pfrozen across multiple orders of magnitude while keeping other hyperparameters fixed, measuring the resulting convergence speed, final accuracy, and stability for different learning rate decay schedules.

### Open Question 2
- Question: How does the CoRe optimizer's performance compare to second-order optimizers like AdaHessian or Sophia for high-dimensional chemical systems beyond lMLPs?
- Basis in paper: [explicit] The paper mentions these second-order optimizers in the introduction but only benchmarks first-order optimizers, leaving a gap in understanding whether CoRe's advantages extend to second-order methods.
- Why unresolved: The paper focuses exclusively on first-order gradient-based optimizers and does not investigate whether the combination of momentum and adaptive learning rates in CoRe provides advantages over second-order methods.
- What evidence would resolve it: Direct benchmarking of CoRe against AdaHessian and Sophia on the same set of chemical systems and ML tasks used in this paper, measuring both final accuracy and computational efficiency.

### Open Question 3
- Question: What is the theoretical explanation for why the CoRe optimizer performs particularly well in intermediate batch size scenarios (5-10% of training data)?
- Basis in paper: [explicit] The paper identifies that CoRe outperforms both Adam and RPROP in intermediate batch size scenarios but does not provide a theoretical explanation for this advantage.
- Why unresolved: While the empirical results are clear, the paper does not investigate the underlying mathematical properties that make CoRe particularly suited to this specific regime between pure mini-batch and full batch learning.
- What evidence would resolve it: A mathematical analysis of how the combined Adam-like and RPROP-like learning rate adaptation mechanisms interact with gradient noise at different batch sizes, potentially involving variance analysis or stability conditions for the optimization dynamics.

## Limitations
- The benchmark covers diverse tasks but may not fully capture edge cases where the optimizer could fail
- The hyperparameter recommendations are based on the authors' experiments but may not generalize optimally to all architectures and problem domains
- There is no direct comparison to second-order optimizers, which might perform differently on high-dimensional chemical systems

## Confidence

- High Confidence: The theoretical foundations of combining adaptive learning rates with gradient sign-based step-size adjustment are well-established in the literature
- Medium Confidence: The benchmark results showing superior performance across multiple tasks, though convincing, lack independent verification
- Medium Confidence: The stability-plasticity mechanism for lifelong learning shows promise but has been validated only on specific material science applications

## Next Checks
1. **Cross-domain robustness test**: Apply CoRe to optimization problems outside the benchmark set (e.g., natural language processing, graph neural networks) to assess generalizability
2. **Ablation study**: Systematically disable individual components (Adam-like adaptation, RPROP-like step-size adjustment, step-dependent decay) to quantify their independent contributions to performance
3. **Long-term stability analysis**: Monitor weight importance scores and frozen weights over extended training periods to verify that the stability-plasticity balance prevents catastrophic forgetting without overly restricting adaptation