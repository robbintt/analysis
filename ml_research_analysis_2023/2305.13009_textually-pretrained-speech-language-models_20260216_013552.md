---
ver: rpa2
title: Textually Pretrained Speech Language Models
arxiv_id: '2305.13009'
source_url: https://arxiv.org/abs/2305.13009
tags:
- speech
- arxiv
- language
- twist
- speechlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes TWIST, a method for initializing speech language
  models (SpeechLMs) from pretrained textual language models. TWIST replaces the text
  vocabulary with speech tokens and reinitializes the embedding layer, while keeping
  the rest of the network unchanged.
---

# Textually Pretrained Speech Language Models

## Quick Facts
- arXiv ID: 2305.13009
- Source URL: https://arxiv.org/abs/2305.13009
- Reference count: 27
- Primary result: TWIST initialization improves speech language model performance across multiple metrics while enabling scaling to 7B parameters

## Executive Summary
This paper introduces TWIST, a method for initializing speech language models (SpeechLMs) from pretrained textual language models. By replacing the text vocabulary with speech tokens and reinitializing the embedding layer while keeping the rest of the network unchanged, TWIST provides a warm-start initialization that consistently outperforms cold-start SpeechLM training. The approach demonstrates improved convergence speed, better performance across various metrics including perplexity and lexical modeling, and enables scaling to larger models and more training data. The authors present the largest SpeechLM to date (7B parameters, 150k hours of speech data) and introduce spoken versions of the StoryCloze benchmark for evaluating SpeechLM capabilities.

## Method Summary
TWIST works by taking a pretrained textual language model and adapting it for speech processing. The method involves replacing the original text vocabulary with speech tokens generated from a HuBERT-based tokenizer, reinitializing the embedding layer to accommodate the new vocabulary, and keeping all other network parameters unchanged. The speech data used includes LibriSpeech, LibriLight, Spotify podcasts, People dataset, and VoxPopuli, totaling approximately 150k hours after filtering for non-English content. Models are trained using negative log likelihood loss between predicted and true probability distributions. The approach is evaluated across multiple scales (125M to 7B parameters) and compared against cold-start initialization, showing consistent improvements in performance and convergence speed.

## Key Results
- TWIST consistently outperforms cold-start initialization across all tested model scales and datasets
- The method achieves comparable performance with only 10% of the training data compared to cold-start approaches
- Scaling to 7B parameters with 150k hours of speech data produces state-of-the-art SpeechLM performance
- TWIST demonstrates 4x faster convergence compared to cold-start training on validation loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TWIST provides warm-start initialization that improves convergence speed and final performance of SpeechLMs.
- Mechanism: Pretrained textual LMs contain learned representations of language structure (syntax, semantics, lexical patterns) that transfer to speech modality even with different token granularity.
- Core assumption: The underlying linguistic knowledge encoded in textual LMs is sufficiently abstract and transferable to speech tokens, despite different temporal resolutions.
- Evidence anchors: [abstract]: "TWIST outperforms a cold-start SpeechLM across the board"; [section]: "TWIST converges faster. Next, we analyze how textual pretraining affects model convergence. Fig. 2b presents the validation loss training curves for OPT-350M. We observe that using TWIST, the model reaches the same PPL in about one quarter of the training updates compared to COLD-INIT."

### Mechanism 2
- Claim: TWIST leverages transfer learning to achieve sample efficiency, requiring less training data for comparable performance.
- Mechanism: Pretraining on massive text corpora provides rich linguistic priors that reduce the amount of speech data needed to achieve good performance.
- Core assumption: Linguistic patterns learned from text generalize to speech modality, and the benefit scales with model size.
- Evidence anchors: [abstract]: "We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs."; [section]: "Scaling improves SpeechLMs... for all model scales, following the TWIST approach with only 10% of the data yields comparable or superior performance than the corresponding COLD-INIT approach using 100% of the data."

### Mechanism 3
- Claim: TWIST enables scaling to larger model sizes and more training data, achieving state-of-the-art performance.
- Mechanism: By providing a better initialization point, TWIST allows training larger SpeechLMs on more data without running into optimization difficulties that would occur with cold-start initialization.
- Core assumption: Larger models benefit more from pretraining due to increased parameter count and capacity to capture transferred knowledge.
- Evidence anchors: [abstract]: "Based on our observations, we present the largest SpeechLM to date (to the best of our knowledge), both in terms of size (7B parameters) and training data (~150k speech hours)."; [section]: "Scaling improves SpeechLMs... As expected, increasing the model size and the magnitude of the dataset improves model performance in almost all cases."

## Foundational Learning

- Concept: Speech tokenization and discrete representation learning
  - Why needed here: Understanding how speech is converted to discrete tokens is fundamental to grasping how TWIST works
  - Quick check question: What is the difference between the speech tokenizer's role in TWIST vs. traditional text tokenization?

- Concept: Language model pretraining and transfer learning
  - Why needed here: TWIST builds on the principle that pretrained models can be adapted to new tasks/modalities
  - Quick check question: How does the pretraining objective differ between textual LMs and SpeechLMs?

- Concept: Evaluation metrics for generative models (perplexity, sWUGGY, sBLIMP)
  - Why needed here: Understanding these metrics is crucial for interpreting TWIST's performance gains
  - Quick check question: Why might sWUGGY scores be higher for models with lower-frequency speech tokenization?

## Architecture Onboarding

- Component map: Speech data -> HuBERT tokenizer -> discrete speech tokens -> TWIST-initialized SpeechLM -> negative log likelihood loss -> generation -> token-to-speech vocoder -> audio output

- Critical path: Speech data → tokenizer → discrete tokens → TWIST-initialized SpeechLM → generation → vocoder → audio output

- Design tradeoffs:
  - Token frequency vs. model performance (50Hz vs 25Hz)
  - Number of tokens in vocabulary (100 vs 200 vs 500)
  - Model size vs. computational cost
  - Pretrained textual model choice (OPT, BLOOM, Pythia, LLaMA)

- Failure signatures:
  - Poor sWUGGY/sBLIMP scores indicate failure to capture lexical/syntactic patterns
  - High perplexity indicates poor language modeling
  - Degradation in human evaluation (MMOS) suggests generation quality issues
  - Training instability or convergence issues suggest initialization problems

- First 3 experiments:
  1. Compare TWIST vs COLD-INIT on a small model (125M parameters) with 200 tokens at 50Hz to verify basic mechanism
  2. Test different token frequencies (50Hz vs 25Hz) to understand impact on lexical/syntactic modeling
  3. Try different pretrained textual models (BLOOM, Pythia) to validate generality of TWIST approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of semantic understanding are missing from SpeechLMs, and how could these be addressed?
- Basis in paper: [explicit] The paper mentions that the biggest limitation of SpeechLMs is the lack of semantic understanding, which might lead to ungrammatical, off-topic, or inaccurate responses.
- Why unresolved: The paper acknowledges this limitation but does not provide specific details on what aspects of semantic understanding are missing or how to address them.
- What evidence would resolve it: Future research could identify specific semantic understanding capabilities that SpeechLMs lack, propose methods to incorporate these capabilities, and demonstrate improvements in model performance.

### Open Question 2
- Question: How does the granularity of speech tokenizers (e.g., 50Hz vs 25Hz) impact the overall performance of SpeechLMs in terms of language modeling, generation quality, and computational efficiency?
- Basis in paper: [explicit] The paper discusses the effect of different speech tokenizers on model performance, including the impact of downsampling factors on sWUGGY and sBLIMP results.
- Why unresolved: While the paper provides some insights into the impact of tokenization frequency, a comprehensive analysis of how granularity affects various aspects of SpeechLM performance is lacking.
- What evidence would resolve it: Systematic experiments comparing SpeechLMs with different tokenization frequencies across various tasks and metrics, including computational efficiency, would provide a clearer understanding of the impact of granularity.

### Open Question 3
- Question: Are there alternative methods for converting speech tokens to word tokens that could potentially outperform the current approach used in TWIST?
- Basis in paper: [explicit] The paper mentions that more advanced methods for converting speech tokens to word tokens probably exist and hopes this study will motivate researchers to explore them.
- Why unresolved: The paper uses a straightforward approach of replacing the text vocabulary with speech tokens and reinitializing the embedding layer, but does not explore other potential methods for this conversion.
- What evidence would resolve it: Developing and testing alternative methods for converting speech tokens to word tokens, such as using attention mechanisms or incorporating additional linguistic information, and comparing their performance to the current approach would provide insights into potential improvements.

## Limitations

- The paper lacks detailed ablation studies on critical hyperparameters, particularly speech tokenizer configuration and its interaction with model performance
- Evaluation methodology relies on crowdsourced workers who may not represent the target user population for SpeechLM applications
- Computational resources required for scaling to 7B parameters represent a significant barrier to reproducibility and broader adoption
- The study focuses primarily on English data, leaving unclear how TWIST would perform across languages with different phonological and morphological properties

## Confidence

**High Confidence**: TWIST outperforms cold-start initialization across multiple metrics (perplexity, sWUGGY, sBLIMP, MMOS) is well-supported by comprehensive experimental results across multiple model scales and datasets.

**Medium Confidence**: Scaling both model size and training data improves performance is supported by empirical results, but the relationship between scale and performance gains is not fully characterized.

**Medium Confidence**: TWIST enables sample efficiency (achieving comparable performance with 10% of the data) is demonstrated but could benefit from more extensive ablation studies to understand the limits of this efficiency gain across different data regimes and model sizes.

## Next Checks

1. **Tokenizer Ablation Study**: Systematically vary both token frequency (e.g., 12.5Hz, 25Hz, 50Hz, 100Hz) and vocabulary size (e.g., 100, 200, 500, 1000 tokens) to understand their individual and combined effects on lexical and syntactic modeling performance.

2. **Cross-Lingual Transfer Validation**: Train TWIST-initialized SpeechLMs on non-English datasets (e.g., VoxPopuli multilingual data, MLS corpus) to test whether the linguistic transfer benefits generalize across languages with different phonological and morphological characteristics.

3. **Computational Efficiency Analysis**: Conduct a detailed study of the computational trade-offs involved in TWIST initialization versus cold-start training across different scales, including training time, memory requirements, and inference latency.