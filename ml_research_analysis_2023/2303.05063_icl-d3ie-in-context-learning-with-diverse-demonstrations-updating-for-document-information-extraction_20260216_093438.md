---
ver: rpa2
title: 'ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document
  Information Extraction'
arxiv_id: '2303.05063'
source_url: https://arxiv.org/abs/2303.05063
tags:
- demonstrations
- icl-d3ie
- text
- document
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles document information extraction (DIE) using
  large language models (LLMs) with in-context learning. DIE poses challenges due
  to the modality and task gap - LLMs cannot directly process images and may lack
  training on layout information in visually rich documents (VRDs).
---

# ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction

## Quick Facts
- arXiv ID: 2303.05063
- Source URL: https://arxiv.org/abs/2303.05063
- Reference count: 40
- Authors: Various
- Key outcome: ICL-D3IE enables GPT-3/ChatGPT to achieve superior performance on DIE tasks compared to fine-tuned pre-trained methods

## Executive Summary
This paper addresses document information extraction (DIE) from visually rich documents using large language models (LLMs) with in-context learning. DIE poses unique challenges because LLMs cannot directly process images and may lack training on layout information. The authors propose ICL-D3IE, a framework that constructs diverse demonstrations including hard demonstrations highlighting challenging aspects, layout-aware demonstrations describing positional relationships, and formatting demonstrations providing output formatting examples. The framework also iteratively updates hard demonstrations. Experiments on three benchmark datasets show ICL-D3IE enables GPT-3/ChatGPT to achieve superior performance compared to previous pre-trained methods fine-tuned with full training data.

## Method Summary
The ICL-D3IE framework processes document images through OCR to extract textual content and bounding boxes, then uses Sentence-BERT to find similar training documents for each test document. It constructs three types of demonstrations: hard demonstrations based on zero-shot predictions of challenging examples, layout-aware demonstrations describing positional relationships, and formatting demonstrations guiding output structure. The framework iteratively updates hard demonstrations through 20 rounds of in-context learning, then uses the complete demonstration set to prompt the LLM for entity label prediction on test documents.

## Key Results
- ICL-D3IE with GPT-3 achieves 97.88% F1 score on SROIE, outperforming LayoutLMv3 (96.89%)
- Superior performance in both in-distribution and out-of-distribution settings compared to fine-tuned pre-trained methods
- M-H-L-F demonstration ordering (Medium-Hard-Layout-Formatting) consistently outperforms other configurations across datasets

## Why This Works (Mechanism)

### Mechanism 1
Diverse demonstrations improve LLM performance on DIE tasks by providing varied context types that address different challenges in document understanding. The framework constructs three types of demonstrations (hard, layout-aware, formatting) that target specific weaknesses of LLMs in handling visually rich documents - hard demonstrations focus on challenging examples, layout-aware demonstrations provide positional context, and formatting demonstrations guide output structure.

### Mechanism 2
Iterative updating of hard demonstrations through in-context learning improves model performance by focusing on the most challenging aspects of the task. The framework starts with initial hard demonstrations based on zero-shot predictions, then iteratively updates them by identifying text segments with the lowest F1 scores and adding them to the demonstration set.

### Mechanism 3
Label mapping and formatting demonstrations help LLMs handle non-natural labels by translating them into natural language descriptions that the model can process more effectively. The framework converts unnatural labels into natural language descriptions and uses formatting demonstrations to guide the model's output structure.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: ICL is the core paradigm that allows LLMs to perform DIE tasks without fine-tuning, making it essential for understanding the paper's approach
  - Quick check question: What are the key differences between zero-shot, one-shot, and few-shot learning in the context of LLMs?

- Concept: Document Information Extraction (DIE)
  - Why needed here: DIE is the specific task being addressed, and understanding its challenges (modality gap, task gap) is crucial for appreciating the proposed solution
  - Quick check question: What are the main challenges in extracting information from visually rich documents compared to plain text?

- Concept: Multimodal learning and layout understanding
  - Why needed here: DIE requires understanding both textual content and visual layout, which is essential for designing effective demonstrations and evaluating model performance
  - Quick check question: How do different pre-trained models (like LayoutLM, BROS) handle the combination of text and layout information?

## Architecture Onboarding

- Component map: OCR extraction -> Sentence-BERT document representation -> Nearest neighbor selection -> Diverse demonstrations construction -> Iterative hard demonstration updates -> Final inference with complete demonstration set

- Critical path: 1) Document preprocessing and OCR extraction 2) Nearest neighbor selection using Sentence-BERT 3) Initial hard demonstration construction via zero-shot prediction 4) Layout-aware and formatting demonstration creation 5) Iterative hard demonstration updates (20 iterations) 6) Final inference with complete demonstration set

- Design tradeoffs: Demonstration quantity vs. context window limits (more demonstrations provide better guidance but consume more context space); Hard demonstration selection (balance between covering challenging aspects and maintaining diversity); Label mapping verbosity (more natural descriptions improve understanding but increase context usage)

- Failure signatures: Poor performance on specific entity types suggests inadequate demonstration coverage; Inconsistent results across different runs may indicate sensitivity to demonstration ordering; Degradation with too many demonstrations points to context window limitations

- First 3 experiments: 1) Baseline comparison: Run ICL-D3IE vs. standard ICL on FUNSD dataset to verify performance improvement 2) Demonstration type ablation: Test ICL-D3IE with each demonstration type removed to confirm their individual contributions 3) Iteration count optimization: Vary the number of hard demonstration update iterations to find optimal performance point

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of ICL-D3IE vary with different OCR quality levels, and what is the optimal OCR configuration for maximizing DIE accuracy? The paper mentions that ICL-D3IE has greater robustness to OCR errors in document content on the OOD settings, but does not explore the impact of OCR quality on performance.

### Open Question 2
Can ICL-D3IE be extended to handle more complex DIE tasks, such as cross-document information extraction or temporal reasoning across multiple documents? The paper focuses on entity labeling within single documents, but does not explore more complex DIE scenarios involving multiple documents or temporal reasoning.

### Open Question 3
How does the choice of demonstration ordering (e.g., M-H-L-F vs. M-L-H-F) impact ICL-D3IE performance on different document layouts or entity types? The paper investigates the impact of demonstration ordering on ICL-D3IE performance, finding that M-H-L-F consistently outperforms M-L-H-F across datasets.

## Limitations

- Performance claims rest on assumptions about Sentence-BERT representations capturing relevant document features
- Demonstration construction heavily relies on label mapping from provided datasets, limiting applicability to scenarios without such metadata
- Experiments primarily focus on English documents with relatively clean layouts, raising questions about performance on multilingual documents or documents with complex formatting
- Computational cost of multiple iterative updates (20 iterations) raises scalability concerns for larger datasets

## Confidence

- High confidence: The framework's core architecture and demonstration construction methodology are well-specified and logically sound
- Medium confidence: Performance improvements over baseline methods are supported by experimental results, but relative contribution of each demonstration type remains unclear
- Low confidence: Claims about generalizability to out-of-distribution documents and different document types are based on limited experimental evidence

## Next Checks

1. **Ablation study**: Remove each demonstration type (hard, layout-aware, formatting) individually and measure performance degradation to quantify their relative contributions

2. **Error analysis on challenging documents**: Manually examine LLM predictions on documents with the lowest F1 scores to identify systematic failure patterns

3. **Scalability evaluation**: Test the framework on a larger dataset (10x the current size) to measure computational costs and performance changes