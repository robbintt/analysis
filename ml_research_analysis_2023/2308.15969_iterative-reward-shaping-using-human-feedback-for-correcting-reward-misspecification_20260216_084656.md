---
ver: rpa2
title: Iterative Reward Shaping using Human Feedback for Correcting Reward Misspecification
arxiv_id: '2308.15969'
source_url: https://arxiv.org/abs/2308.15969
tags:
- reward
- feedback
- user
- agent
- iters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents ITERS, an iterative approach for correcting
  reward misspecification in reinforcement learning (RL) using human feedback. ITERS
  allows users to provide trajectory-level feedback on agent behavior, augmented with
  explanations, to shape the reward function iteratively during training.
---

# Iterative Reward Shaping using Human Feedback for Correcting Reward Misspecification

## Quick Facts
- arXiv ID: 2308.15969
- Source URL: https://arxiv.org/abs/2308.15969
- Authors: [Not specified in source]
- Reference count: 18
- Primary result: Iterative human feedback approach successfully corrects reward misspecification with minimal user effort across three RL environments

## Executive Summary
This paper introduces ITERS, an iterative approach for correcting reward misspecification in reinforcement learning using human trajectory-level feedback. The method enables users to identify and explain unwanted agent behaviors during training, which are then used to shape the reward function in subsequent iterations. By augmenting user feedback with similar trajectories through randomization and training a supervised model to predict feedback strength, ITERS achieves performance comparable to expert policies trained on true reward functions while requiring only a handful of marked trajectories per environment.

## Method Summary
ITERS iteratively trains a DQN agent while collecting human feedback on undesirable trajectories. After partial training, the system presents the top-m trajectories to the user for feedback and explanations. These marked trajectories are augmented by randomizing unimportant features while preserving important elements identified in explanations, creating a dataset of similar trajectories. A reward shaping model Rs is trained on this augmented data to predict feedback strength, which is then integrated as a shaping reward signal in subsequent training iterations. The process repeats until the agent's behavior aligns with the true reward function.

## Key Results
- ITERS successfully corrected reward misspecification in three environments (GridWorld, highway driving, inventory management) with minimal user effort
- Required only a handful of marked trajectories per environment to achieve convergence to true reward baseline
- Demonstrated effectiveness in steering agents away from undesirable behaviors while maintaining performance comparable to expert policies
- Showed that trajectory-level explanations generalize to similar scenarios through augmentation, reducing overall feedback frequency

## Why This Works (Mechanism)

### Mechanism 1
Reward misspecification can be corrected through iterative human trajectory-level feedback integrated as reward shaping. ITERS trains an RL agent partially, collects user-marked trajectories indicating unwanted behavior, augments these with similar trajectories via randomization, trains a supervised model to predict user feedback, and uses this model to shape the reward in subsequent training iterations. The core assumption is that users can identify unwanted behavior on trajectory level even when they cannot specify the correct reward function, and trajectory-level explanations generalize to similar trajectories.

### Mechanism 2
Augmenting user feedback with randomized similar trajectories reduces user effort and improves generalization. For each marked trajectory, unimportant elements are randomized to generate a dataset of similar trajectories, creating a richer training set for the reward shaping model Rs. The core assumption is that randomizing unimportant trajectory elements preserves the core reason for user feedback while expanding coverage of similar scenarios.

### Mechanism 3
Iterative training with accumulated feedback buffer enables convergence to true reward behavior. ITERS maintains a buffer of all augmented trajectories and their feedback strength, updating the reward shaping model Rs at each iteration to predict feedback based on historical marking frequency. The core assumption is that feedback strength indicates the importance of avoiding that behavior, and accumulated data enables learning robust patterns.

## Foundational Learning

- Concept: Reinforcement Learning with Reward Shaping
  - Why needed here: Understanding how reward shaping modifies agent behavior is essential for grasping how ITERS integrates human feedback into the learning process.
  - Quick check question: How does adding a shaping reward term to the environment's reward affect the optimal policy of an RL agent?

- Concept: Human-in-the-Loop Learning
  - Why needed here: ITERS relies on human feedback being integrated iteratively, so understanding different human-in-the-loop approaches and their limitations is crucial.
  - Quick check question: What are the key differences between state-based, trajectory-based, and preference-based human feedback in RL?

- Concept: Supervised Learning for Reward Function Approximation
  - Why needed here: The reward shaping model Rs is trained via supervised learning to predict human feedback from trajectories, so understanding how this approximation works is essential.
  - Quick check question: How can a supervised learning model trained on human feedback be used to shape rewards in a reinforcement learning context?

## Architecture Onboarding

- Component map: RL Environment -> DQN Agent -> Trajectory Summarizer -> User Interface -> Trajectory Augmenter -> Reward Shaping Model Rs -> Shaped Reward Calculator -> RL Environment
- Critical path: RL training → trajectory summarization → user feedback → trajectory augmentation → Rs training → shaped reward integration → next RL iteration
- Design tradeoffs: Feedback frequency vs. user effort (fewer iterations may miss behaviors, more iterations overwhelm users); augmentation size p vs. computational cost (larger p provides better coverage but increases training time); λ shaping strength vs. exploration (larger λ may over-constrain agent, smaller λ may be ineffective)
- Failure signatures: Agent performance plateaus below true reward baseline despite iterations; user feedback becomes repetitive without improving results; reward shaping model Rs shows high variance or low accuracy on validation data
- First 3 experiments: 1) Run ITERS with λ=0.1 on GridWorld for 10 iterations, verify if agent reaches goal state more frequently; 2) Test trajectory augmentation with different p values (1000, 5000, 10000) on highway environment, measure lane change reduction; 3) Compare ITERS with no feedback baseline on inventory management, measure profit and delivery frequency

## Open Questions the Paper Calls Out

### Open Question 1
How does ITERS perform in non-episodic environments where behavior cannot be easily summarized as discrete trajectories? The paper explicitly states that ITERS is limited to episodic environments and suggests that alternative summarization methods would be needed for non-episodic tasks.

### Open Question 2
Can the reward shaping parameter λ be dynamically adjusted during training instead of being manually tuned? The paper mentions that λ was manually explored during training and suggests future work on dynamically adjusting this hyperparameter.

### Open Question 3
How does the user effort required by ITERS scale with the complexity of the environment or the severity of reward misspecification? The paper reports the average number of trajectories needed in three tasks but does not explore how this scales with environmental complexity or misspecification severity.

### Open Question 4
How does ITERS compare to other reward correction methods, such as inverse reinforcement learning or preference-based RL, in terms of sample efficiency and final policy performance? The paper notes that ITERS is the first approach to use human feedback to correct reward misspecification and does not compare it to other reward correction methods.

## Limitations

- Limited evaluation scope to three relatively simple domains constrains confidence in broader applicability
- Trajectory augmentation mechanism's randomization procedure may not generalize well to complex, high-dimensional environments
- Assumption that trajectory-level feedback generalizes via randomization has limited empirical support with only weak citations to related work

## Confidence

- Mechanism robustness: Medium (Limited empirical support for trajectory augmentation generalization)
- Scalability claims: Low (No evaluation in continuous control or high-dimensional spaces)
- User effort claims: Medium (Supported within tested environments but may not hold for more nuanced corrections)

## Next Checks

1. Test ITERS in a continuous control environment (e.g., MuJoCo locomotion tasks) to evaluate generalization beyond discrete action spaces
2. Conduct ablation studies on augmentation size p and λ shaping strength to identify optimal hyperparameter ranges
3. Implement multiple independent runs with statistical significance testing to verify convergence claims across random seeds