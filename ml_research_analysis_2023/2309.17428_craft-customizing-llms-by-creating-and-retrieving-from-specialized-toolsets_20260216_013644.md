---
ver: rpa2
title: 'CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets'
arxiv_id: '2309.17428'
source_url: https://arxiv.org/abs/2309.17428
tags:
- tool
- function
- tools
- object
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CRAFT, a framework that creates and retrieves
  specialized toolsets to augment large language models (LLMs) for complex tasks.
  It constructs toolsets offline by sampling diverse problems from a dataset, generating
  code solutions with GPT-4, abstracting them into reusable snippets, and validating
  their correctness.
---

# CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets

## Quick Facts
- arXiv ID: 2309.17428
- Source URL: https://arxiv.org/abs/2309.17428
- Reference count: 40
- One-line primary result: Achieves up to 43.16% relative F1 score improvement over baselines by creating and retrieving specialized toolsets for LLM augmentation

## Executive Summary
CRAFT is a framework that improves large language model performance on complex tasks by creating and retrieving specialized toolsets. The approach involves offline generation of code solutions for sampled problems, followed by abstraction, validation, and deduplication to build a reusable toolset. At inference, a multi-view retrieval mechanism selects relevant tools based on problem similarity, function names, and docstrings, which are then integrated into LLM prompts to guide code generation and execution. Experiments on vision-language, tabular processing, and mathematical reasoning tasks demonstrate substantial performance gains over strong baselines.

## Method Summary
CRAFT operates in two phases: offline tool creation and online tool retrieval. During creation, diverse problems are sampled from datasets, GPT-4 generates code solutions, which are then abstracted into reusable snippets, validated for correctness, and deduplicated to form a toolset. At inference, the LLM retrieves relevant tools using a multi-view matching approach that considers the problem, function names, and docstrings, aggregates matches by frequency, and selects the most relevant tools. Retrieved tools are embedded in prompts to guide LLM code generation and execution, enabling more effective problem solving without fine-tuning.

## Key Results
- Achieves up to 43.16% relative improvement in F1 score over strong baselines across vision-language, tabular processing, and mathematical reasoning tasks
- Consistent performance improvement with scaling toolset size and backbone model capability
- Multi-view retrieval (problem, function name, docstring) outperforms single-view similarity matching
- Tools created are well-structured, reliable, and atomic, contributing to robust performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CRAFT improves performance by creating specialized toolsets offline and retrieving them online, avoiding the brittleness of direct code generation.
- Mechanism: Offline, diverse problem samples are collected, GPT-4 generates code solutions, which are abstracted into reusable snippets, validated for correctness, and deduplicated. Online, retrieved snippets are added to the prompt to guide the LLM.
- Core assumption: Code generation errors can be eliminated by validation and deduplication, and specialized toolsets are more effective than general ones.
- Evidence anchors:
  - [abstract] "substantial performance gains—up to 43.16% relative improvement in F1 score—over strong baselines."
  - [section] "At inference time, the language model retrieves snippets from the toolsets and then executes them or generates the output conditioning on the retrieved snippets."
  - [corpus] Weak: related work focuses on tool selection or merging, not offline creation + validation.

### Mechanism 2
- Claim: Multi-view retrieval (problem, function name, docstring) outperforms simple similarity matching.
- Mechanism: Each tool is indexed by its original problem, function name, and docstring. During retrieval, similarity is computed in all three views and aggregated by frequency to select the most relevant tools.
- Core assumption: Different views capture complementary aspects of tool relevance; combining them improves precision over single-view matching.
- Evidence anchors:
  - [abstract] "Experiments on vision-language, tabular processing, and mathematical reasoning tasks show that our approach achieves substantial improvements compared to strong baselines."
  - [section] "CRAFT implements a retrieval component that takes into account the target problem, the names of the tools, and their docstrings through a multi-view matching function."
  - [corpus] Weak: related papers focus on single-view similarity (SimCSE, BM25) or heuristic selection; none combine all three views.

### Mechanism 3
- Claim: Scaling the toolset size and backbone model capability leads to consistent performance gains.
- Mechanism: Larger toolsets increase coverage of problem patterns; stronger backbones better leverage the tools via improved retrieval and execution.
- Core assumption: Performance improvement is monotonic with toolset size and model capability up to some practical limit.
- Evidence anchors:
  - [abstract] "(1) consistent performance improvement can be achieved by scaling up the number of tools and the capability of the backbone models."
  - [section] "Experiments on vision-language, tabular processing, and mathematical reasoning tasks show that our approach achieves substantial improvements."
  - [corpus] Weak: related work focuses on fixed toolsets or fixed model sizes; none systematically vary both.

## Foundational Learning

- Concept: Tool abstraction and code generation
  - Why needed here: CRAFT depends on GPT-4 generating executable Python code that solves problems and can be abstracted into reusable functions. Without understanding code generation, validation, and abstraction, the core pipeline cannot be implemented or improved.
  - Quick check question: Can you write a Python function that, given a list of integers, returns the maximum? Can you then abstract it to work for any numeric iterable?

- Concept: Similarity embeddings and retrieval (SimCSE)
  - Why needed here: CRAFT uses SimCSE embeddings to measure similarity between problems, function names, and docstrings. Without understanding how sentence embeddings work, the retrieval step cannot be tuned or debugged.
  - Quick check question: Given two sentences, how would you compute their cosine similarity using pre-trained embeddings? What is the effect of sentence length on the similarity score?

- Concept: Cyclomatic complexity and code quality metrics
  - Why needed here: CRAFT evaluates the complexity of created tools to ensure they are low-complexity and reliable. Understanding complexity metrics helps maintain tool quality and guides refactoring.
  - Quick check question: What is cyclomatic complexity? How does it relate to code maintainability and bug risk?

## Architecture Onboarding

- Component map: Problem sampling → GPT-4 code generation → Validation → Abstraction → Deduplication → Toolset → Multi-view retrieval → LLM prompt → Execution
- Critical path: Problem → Sampling → Code generation → Validation → Abstraction → Deduplication → Retrieval (online) → LLM prompt → Execution
- Design tradeoffs:
  - Tool creation vs. runtime efficiency: More complex validation and deduplication improves quality but adds upfront cost.
  - Retrieval granularity vs. prompt size: Including more tools increases coverage but risks exceeding context limits.
  - Abstraction level vs. usability: Overly abstract tools may be harder to use; too specific tools reduce reusability.
- Failure signatures:
  - Poor retrieval: Low overlap between retrieved tools and needed functionality; often due to insufficient abstraction or mismatch between problem and tool description.
  - Code execution errors: Validation step missed bugs; abstraction altered semantics; or LLM misuses tool signatures.
  - Performance plateau: Toolsets not diverse enough or retrieval fails to surface relevant tools; can be diagnosed by inspecting tool coverage and retrieval accuracy.
- First 3 experiments:
  1. Run CRAFT with and without the validation step on a small VQA subset; compare success rates to quantify validation impact.
  2. Compare single-view vs. multi-view retrieval on a fixed toolset; measure F1 score difference.
  3. Scale toolset size incrementally (e.g., 100, 200, 500 tools) on the same task; plot performance to verify monotonic improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CRAFT's tool creation and retrieval approach generalize to non-code-based tasks beyond visual question answering, tabular processing, and mathematical reasoning?
- Basis in paper: [inferred] The authors acknowledge a limitation that CRAFT is "currently based on code generation for tool creation" and "only suitable for tasks that can be solved via writing code solutions."
- Why unresolved: The paper's experiments and analysis are limited to tasks that can be solved through code generation, leaving open whether the approach works for other task types.
- What evidence would resolve it: Empirical results demonstrating CRAFT's effectiveness on non-code-based tasks like text generation, dialogue, or multi-modal tasks that require different solution representations.

### Open Question 2
- Question: How does the performance of CRAFT scale with increasingly large and diverse toolsets, and is there a point of diminishing returns?
- Basis in paper: [explicit] The authors show performance improvements as the toolset scales up, but note that "the upward trend of soft accuracy continues, suggesting the potential for further improvement of CRAFT as the toolset keeps expanding."
- Why unresolved: The analysis only examines a limited range of toolset sizes, and the relationship between toolset size, diversity, and performance is not fully characterized.
- What evidence would resolve it: Systematic experiments varying toolset size and diversity across different task types, identifying optimal toolset characteristics and potential performance ceilings.

### Open Question 3
- Question: How sensitive is CRAFT's performance to the choice of backbone language model, and what are the key factors that determine successful tool usage?
- Basis in paper: [explicit] The authors observe that "LLMs can benefit from the guidance of more capable models while gaining no improvement from the guidance of itself" and that some open-source models "achieve near-random performance in the challenging tool-manipulation setting."
- Why unresolved: The analysis is limited to GPT-3.5-Turbo and GPT-4 backbones, and the factors contributing to successful tool usage (e.g., reasoning ability, tool manipulation skills) are not fully explored.
- What evidence would resolve it: Comparative experiments with different backbone models (including open-source alternatives), identifying the key capabilities required for effective tool usage and the extent to which these can be improved through training or prompting.

## Limitations

- The framework's effectiveness depends on the reliability of GPT-4-generated code and the robustness of the validation step, which is not fully characterized.
- The multi-view retrieval mechanism assumes orthogonal and complementary views, but redundancy or correlation between views is not analyzed.
- Performance scaling with toolset size and backbone capability is asserted but not empirically validated across wide ranges or for potential diminishing returns.
- No analysis of tool creation overhead or context length limits is provided, which are critical for real-world deployment.

## Confidence

- **High confidence**: CRAFT's overall design and pipeline (problem sampling → code generation → validation → abstraction → retrieval → LLM integration) is clearly specified and logically sound. The experimental setup (three distinct task types, strong baselines, appropriate metrics) is methodologically robust.
- **Medium confidence**: The empirical gains reported (up to 43.16% F1 improvement) are substantial, but the reproducibility is hampered by missing implementation details (e.g., exact prompts, thresholds, embedding configs). The claim that multi-view retrieval is superior to single-view is plausible but not rigorously tested against alternatives.
- **Low confidence**: The paper's assertion that performance consistently improves with toolset size and model capability is not empirically validated across a wide range of scales, and no analysis of practical limits is provided.

## Next Checks

1. **Validation step robustness**: Run CRAFT on a small VQA subset with and without the validation step; compare success rates to quantify the impact of validation on tool correctness and overall performance.
2. **Multi-view vs. single-view retrieval**: Fix a toolset and compare single-view (problem, name, or docstring only) vs. multi-view retrieval on the same task; measure F1 score difference to test the benefit of combining views.
3. **Scaling toolset size**: Incrementally scale the toolset (e.g., 100, 200, 500 tools) on the same task; plot performance and measure retrieval accuracy to determine if gains are monotonic and if any saturation or diminishing returns emerge.