---
ver: rpa2
title: On Achieving Optimal Adversarial Test Error
arxiv_id: '2306.07544'
source_url: https://arxiv.org/abs/2306.07544
tags:
- adversarial
- optimal
- lemma
- training
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies optimal adversarial predictors and adversarial
  training for achieving optimal adversarial test error. Key results include: (1)
  Optimal adversarial predictor structure: For predictors whose adversarial convex
  loss is almost optimal, choosing an appropriate threshold ensures their adversarial
  zero-one loss is also almost optimal.'
---

# On Achieving Optimal Adversarial Test Error

## Quick Facts
- arXiv ID: 2306.07544
- Source URL: https://arxiv.org/abs/2306.07544
- Reference count: 40
- Primary result: Early stopping in adversarial training on shallow ReLU networks can achieve adversarial test error arbitrarily close to optimal.

## Executive Summary
This paper establishes theoretical foundations for achieving optimal adversarial test error through adversarial training. The key insight is that predictors with near-optimal adversarial convex loss can be thresholded to achieve near-optimal adversarial zero-one loss. The paper proves that adversarial training on shallow ReLU networks with early stopping and optimal adversarial attacks can achieve adversarial test error arbitrarily close to the theoretical optimum over all measurable functions.

## Method Summary
The method combines adversarial training with early stopping on shallow ReLU networks. Training uses gradient descent on network parameters with optimal adversarial attacks at each iteration. Early stopping is applied when parameters exceed a threshold distance from initialization, ensuring low model complexity. The approach leverages the near-initialization regime to derive tight generalization bounds that relate the trained network's performance to the optimal test error over all measurable functions.

## Key Results
- Choosing an appropriate threshold on a predictor with near-optimal adversarial convex loss yields a predictor with near-optimal adversarial zero-one loss
- Early stopping in adversarial training on shallow ReLU networks achieves adversarial test error arbitrarily close to optimal
- Continuous predictors can achieve adversarial risk arbitrarily close to that of measurable functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Choosing an appropriate threshold on a predictor with near-optimal adversarial convex loss yields a predictor with near-optimal adversarial zero-one loss.
- Mechanism: The adversarial convex loss can be expressed as an integral over adversarial zero-one losses across all thresholds. If the convex loss is near-optimal, then the corresponding zero-one loss across all thresholds is also near-optimal. Thresholding at the right value aligns the convex predictor with the optimal zero-one predictor.
- Core assumption: The adversarial convex loss is convex and non-increasing, with a continuous derivative, and the data distribution is Borel measurable.
- Evidence anchors:
  - [abstract]: "choosing an appropriate threshold ensures their adversarial zero-one loss is also almost optimal"
  - [section 3.2]: "when an appropriate threshold is chosen its adversarial zero-one loss is also almost optimal"
- Break condition: If the adversarial convex loss is not sufficiently close to optimal, or if the threshold is poorly chosen, the zero-one loss may deviate significantly from optimal.

### Mechanism 2
- Claim: Early stopping in adversarial training on shallow ReLU networks achieves adversarial test error arbitrarily close to optimal.
- Mechanism: Early stopping keeps the network parameters near initialization, ensuring low model complexity and enabling tight generalization bounds. Combined with optimization bounds comparing against arbitrary reference networks, this allows the trained network's adversarial test error to be bounded in terms of the optimal test error over all measurable functions.
- Core assumption: The network width m is sufficiently large relative to problem parameters, and the training uses an optimal adversarial attack.
- Evidence anchors:
  - [abstract]: "adversarial training on shallow networks with early stopping and an idealized optimal adversary is able to achieve optimal adversarial test error"
  - [section 4.2]: "we use early stopping so that we remain in the near-initialization regime and ensure low model complexity"
- Break condition: If the early stopping criterion is not met or the width is too small, the generalization bound may not converge to the optimal error.

### Mechanism 3
- Claim: Continuous predictors can achieve adversarial risk arbitrarily close to that of measurable functions.
- Mechanism: Using approximation theory for infinite-width networks, a continuous predictor can be constructed that is ϵ-close in risk to the optimal measurable predictor. This closes the potential gap between continuous and measurable functions in terms of achievable adversarial risk.
- Core assumption: The optimal measurable predictor can be approximated by a continuous function within arbitrary accuracy.
- Evidence anchors:
  - [abstract]: "continuous predictors can get arbitrarily close to the optimal adversarial error for both convex and zero-one losses"
  - [section 3.2]: "we prove that continuous functions can get arbitrarily close to the optimal test error given by measurable functions"
- Break condition: If the approximation quality of continuous functions is limited, the gap between continuous and measurable risks may persist.

## Foundational Learning

- Concept: Rademacher Complexity
  - Why needed here: Used to derive generalization bounds for adversarial training in the near-initialization regime.
  - Quick check question: What is the key difference between Rademacher complexity bounds in the adversarial vs. standard setting?

- Concept: Near-Initialization/NTK Regime
  - Why needed here: The analysis relies on properties of neural networks when parameters are close to initialization, allowing linearization and simplifying the optimization and generalization analysis.
  - Quick check question: Why is the near-initialization regime crucial for the adversarial training analysis?

- Concept: Convex Loss Calibration
  - Why needed here: Establishes the relationship between optimal adversarial convex loss predictors and optimal adversarial zero-one loss predictors, enabling the use of convex losses with appropriate thresholding.
  - Quick check question: How does the calibration of convex losses in the adversarial setting differ from the standard setting?

## Architecture Onboarding

- Component map:
  Data Distribution -> Shallow ReLU Network -> Adversarial Training -> Early Stopping -> Optimal Adversarial Test Error

- Critical path:
  1. Sample training data from distribution
  2. For each iteration, find optimal adversarial perturbations
  3. Update network parameters via gradient descent
  4. Stop when parameters exceed early stopping threshold
  5. Output the iterate with best training risk

- Design tradeoffs:
  - Width m: Larger m improves approximation but may hurt generalization
  - Early stopping: Necessary for tight generalization but may limit optimization
  - Optimal attacks: Provide theoretical guarantees but are computationally infeasible

- Failure signatures:
  - Overfitting: Training risk decreases but test risk increases
  - Underfitting: Both training and test risks remain high
  - Sensitivity to initialization: Performance varies significantly with random seed

- First 3 experiments:
  1. Train a shallow ReLU network on a synthetic binary classification task with small ℓ∞ perturbations, using early stopping and PGD attacks.
  2. Compare the adversarial test error of the trained network to the optimal test error over all measurable functions.
  3. Vary the network width and perturbation radius to study their effects on the gap between achieved and optimal adversarial test error.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the necessity of early stopping for achieving optimal adversarial test error extend beyond the near-initialization/NTK regime into later phases of training where robust overfitting occurs?
- Basis in paper: [inferred] The paper notes early stopping is used to remain in the near-initialization regime and avoid robust overfitting, but does not explore whether early stopping remains necessary in later training phases.
- Why unresolved: The analysis is limited to the near-initialization setting, leaving open the question of whether the benefits of early stopping persist as the network moves away from initialization.
- What evidence would resolve it: Empirical studies comparing adversarial test error with and without early stopping across different training phases and network widths.

### Open Question 2
- Question: Is there a fundamental tradeoff between network width and adversarial generalization error that cannot be eliminated even with optimal early stopping and attack strategies?
- Basis in paper: [explicit] The paper shows generalization bounds that scale with width, and discusses how increased width may help approximation but hurt generalization in the adversarial setting.
- Why unresolved: While the paper provides bounds showing width dependence, it does not determine whether this is an inherent limitation or can be circumvented with better techniques.
- What evidence would resolve it: Experiments varying network width while keeping other factors constant, or theoretical analysis proving/disproving width-independent adversarial generalization bounds.

### Open Question 3
- Question: Can weaker adversarial attacks (e.g., PGD) be used during training while still guaranteeing robustness against optimal attacks at test time?
- Basis in paper: [explicit] The paper assumes access to optimal adversarial attacks during training to guarantee robustness against optimal attacks, but notes this is unrealistic in practice.
- Why unresolved: The paper does not explore whether the theoretical guarantees can be extended to weaker attacks that are computationally feasible.
- What evidence would resolve it: Analysis showing bounds on test-time robustness when training with PGD or other practical attacks, or counterexamples demonstrating the impossibility of such guarantees.

## Limitations
- Relies on optimal adversarial attacks, which are computationally infeasible in practice
- Generalization bounds depend on specific assumptions about data distribution and perturbation sets
- Theoretical framework assumes infinite-width networks for approximation results

## Confidence
- High Confidence: The convex loss calibration mechanism (Mechanism 1) is well-established with clear mathematical proofs
- Medium Confidence: The early stopping analysis for shallow ReLU networks (Mechanism 2) is sound but relies on strong assumptions about the near-initialization regime
- Medium Confidence: The approximation results for continuous predictors (Mechanism 3) follow from established theory but may not reflect practical limitations

## Next Checks
1. Empirical validation: Test the early stopping criterion on standard adversarial learning benchmarks to verify if the theoretical bounds hold in practice
2. Width sensitivity: Systematically evaluate how the achieved adversarial test error varies with network width to validate the approximation claims
3. Attack approximation: Study the impact of using practical adversarial attacks (e.g., PGD) instead of optimal attacks on the final test error performance