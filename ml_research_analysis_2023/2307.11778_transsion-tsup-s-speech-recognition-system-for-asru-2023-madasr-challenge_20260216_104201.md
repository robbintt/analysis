---
ver: rpa2
title: Transsion TSUP's speech recognition system for ASRU 2023 MADASR Challenge
arxiv_id: '2307.11778'
source_url: https://arxiv.org/abs/2307.11778
tags:
- challenge
- bengali
- track
- speech
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Transsion TSUP team developed ASR systems for the ASRU 2023
  MADASR Challenge focusing on low-resource Indian languages. For tracks 1 and 2,
  they used a squeezeformer encoder with bidirectional transformer decoder and joint
  CTC-Attention training, incorporating an external KenLM language model during TLG
  beam search decoding.
---

# Transsion TSUP's speech recognition system for ASRU 2023 MADASR Challenge

## Quick Facts
- arXiv ID: 2307.11778
- Source URL: https://arxiv.org/abs/2307.11778
- Reference count: 0
- Low-resource Indian languages ASR system achieving 24.17% WER on Bengali and 19.61% on Bhojpuri

## Executive Summary
The Transsion TSUP team developed speech recognition systems for the ASRU 2023 MADASR Challenge focusing on low-resource Indian languages (Bengali and Bhojpuri). They implemented different approaches for different tracks: squeezeformer-encoder with bidirectional transformer decoder for tracks 1-2, and IndicWhisper fine-tuning for tracks 3-4. The systems incorporated external KenLM language models during decoding, achieving competitive word error rates across all four challenge tracks.

## Method Summary
The team employed a hybrid approach based on track requirements. For tracks 1-2 (data-limited scenarios), they used a squeezeformer encoder architecture with bidirectional transformer decoder, trained with joint CTC-Attention loss and KenLM language model integration during TLG beam search decoding. For tracks 3-4 (full data access), they fine-tuned pretrained IndicWhisper models (Bengali and Hindi) on both challenge and public datasets, applying dropout regularization to prevent overfitting. Both approaches incorporated external KenLM language models trained on challenge data and dialect-rich text corpora, with language model weights optimized based on development set performance.

## Key Results
- Bengali: 24.17% WER (track 3) and 24.43% WER (track 4)
- Bhojpuri: 19.61% WER (track 3) and 19.54% WER (track 4)
- External KenLM integration improved WER by 1.96% absolute for Bengali and 2.06% for Bhojpuri
- Zero-shot Hindi IndicWhisper model achieved reasonable Bhojpuri recognition due to linguistic similarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IndicWhisper models with Hindi pretraining can decode Bhojpuri audio effectively through cross-linguistic transfer
- Mechanism: The linguistic similarity between Hindi and Bhojpuri allows knowledge transfer from the Hindi pretrained model to Bhojpuri speech recognition tasks
- Core assumption: Hindi and Bhojpuri share sufficient phonological and lexical overlap for zero-shot transfer to work reasonably well
- Evidence anchors:
  - [abstract] "Bhojpuri was not included in these models. We hypothesize that due to the linguistic similarity between Hindi and Bhojpuri, it may be possible to use Hindi models to decode Bhojpuri audio."
  - [section] "Table 2 shows the zero-shot performance of IndicWhisper models on the challenge dev datasets, indicating that Bengali and Hindi IndicWhisper models demonstrate satisfactory zero-shot performance."
  - [corpus] Weak evidence - no direct citations found supporting Hindi-Bhojpuri linguistic similarity claims
- Break condition: If Bhojpuri diverges significantly from Hindi in phonology, vocabulary, or grammar, the zero-shot transfer would fail

### Mechanism 2
- Claim: Fine-tuning all layers of large IndicWhisper models with dropout prevents overfitting on limited datasets
- Mechanism: Fine-tuning the entire model allows adaptation to domain-specific patterns while dropout regularization prevents memorizing training data
- Core assumption: The dataset is large enough to support full fine-tuning but small enough to require regularization
- Evidence anchors:
  - [section] "To fine-tune all layers of the models, we used four A100 GPUs with 40GB GPU RAM, with a learning rate of 5e-6 and linearly decay to zero after 3 epochs. However, we found that the fine-tuning process easily led to overfitting due to the large model size and relatively small dataset."
  - [section] "Therefore, it's important to apply dropout of 0.1 during the fine-tuning process to avoid overfitting."
  - [corpus] No direct evidence found for dropout effectiveness in this specific context
- Break condition: If the dataset is too small relative to model capacity, even with dropout, overfitting would occur

### Mechanism 3
- Claim: External KenLM language model integration during Whisper decoding improves performance by leveraging additional text data
- Mechanism: Beam search decoding with language model fusion provides better word sequence probabilities by incorporating domain-specific text corpora
- Core assumption: The dialect-rich text corpora contains relevant language patterns that complement the acoustic model predictions
- Evidence anchors:
  - [section] "The whisper beam search decoding was also modified to support an external KenLM language model, which enabled better utilization of the additional text provided by the challenge."
  - [section] "A 20-gram BPE-level kenlm language model was trained on the challenge speech data text and the dialect-rich text in our experiment."
  - [section] "The results from Table 3 showed that the external language model brought a large margin WER improvement, with a 1.96% absolute reduction for Bengali and a 2.06% absolute reduction for Bhojpuri."
  - [corpus] No citations found supporting KenLM integration with Whisper models
- Break condition: If the external text corpus is not representative of the test domain, the language model could degrade performance

## Foundational Learning

- Concept: Low-resource language adaptation techniques
  - Why needed here: The challenge specifically focuses on low-resource Indian languages with limited training data
  - Quick check question: What are the main challenges when adapting ASR models to low-resource languages?

- Concept: Multilingual transfer learning
  - Why needed here: Using Hindi pretrained models for Bhojpuri demonstrates cross-linguistic transfer learning
  - Quick check question: How does linguistic similarity affect the success of zero-shot transfer between languages?

- Concept: Language model integration with end-to-end ASR
  - Why needed here: Modifying Whisper's decoding to support external KenLM models shows advanced ASR system integration
  - Quick check question: What are the tradeoffs between integrated language models and external language model fusion?

## Architecture Onboarding

- Component map:
  - Squeezeformer encoder + Bidirectional Transformer decoder (Tracks 1-2)
  - IndicWhisper models (Tracks 3-4)
  - External KenLM language model integration
  - Joint CTC-Attention training loss
  - TLG beam search decoding

- Critical path: Acoustic model → Language model fusion → Beam search decoding → WER calculation

- Design tradeoffs:
  - Full fine-tuning vs. adapter-based fine-tuning (memory vs. performance)
  - External LM integration vs. integrated Whisper decoding (flexibility vs. simplicity)
  - Cross-lingual transfer vs. monolingual pretraining (coverage vs. specialization)

- Failure signatures:
  - Overfitting: Training WER << validation WER
  - Poor transfer: Hindi model performs worse on Bhojpuri than random baseline
  - Language model degradation: External LM integration increases WER

- First 3 experiments:
  1. Evaluate zero-shot performance of IndicWhisper Hindi model on Bhojpuri dev set
  2. Fine-tune IndicWhisper model with and without dropout to observe overfitting patterns
  3. Test KenLM integration with different weights on dev set to find optimal fusion parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the IndicWhisper models on low-resource Indian languages change with varying amounts of fine-tuning data, and what is the optimal balance between model capacity and dataset size to prevent overfitting?
- Basis in paper: [inferred] The paper mentions that fine-tuning the large IndicWhisper models on the relatively small dataset led to overfitting, necessitating the application of dropout. This suggests a relationship between model size, dataset size, and overfitting that warrants further investigation
- Why unresolved: The paper does not explore the impact of different amounts of fine-tuning data on model performance or investigate the optimal model size for a given dataset size
- What evidence would resolve it: Systematic experiments varying the amount of fine-tuning data and model sizes, along with analysis of overfitting patterns, would provide insights into the optimal balance between model capacity and dataset size

### Open Question 2
- Question: How does the linguistic similarity between Hindi and Bhojpuri affect the zero-shot performance of IndicWhisper models, and can this similarity be leveraged to improve ASR for other closely related low-resource languages?
- Basis in paper: [explicit] The paper hypothesizes that the linguistic similarity between Hindi and Bhojpuri allows for the use of Hindi models to decode Bhojpuri audio, as evidenced by the zero-shot performance results
- Why unresolved: While the paper demonstrates the potential of using Hindi models for Bhojpuri, it does not explore the broader implications of linguistic similarity on ASR performance for other language pairs
- What evidence would resolve it: Comparative studies of zero-shot and fine-tuned ASR performance across various language pairs with different degrees of linguistic similarity would elucidate the impact of language relatedness on ASR adaptation

### Open Question 3
- Question: What is the impact of incorporating external KenLM language models on the ASR performance for low-resource languages, and how can the language model weights be optimized for different dialects and linguistic contexts?
- Basis in paper: [explicit] The paper demonstrates significant WER improvements by incorporating external KenLM language models and optimizing language model weights based on the dev dataset
- Why unresolved: The paper does not explore the generalizability of these improvements across different dialects or investigate methods for automatically optimizing language model weights for diverse linguistic contexts
- What evidence would resolve it: Experiments comparing ASR performance with and without external language models across various dialects and linguistic contexts, along with the development of automated methods for optimizing language model weights, would address this question

## Limitations
- Limited validation of Hindi-Bhojpuri linguistic similarity claims due to lack of linguistic citations
- Insufficient architectural details for squeezeformer implementation in tracks 1-2
- No exploration of adapter-based fine-tuning alternatives for memory efficiency

## Confidence
- High Confidence: Overall system architecture description and final WER results are clearly stated
- Medium Confidence: Zero-shot transfer performance and fine-tuning methodology are well-documented
- Low Confidence: Specific implementation details of TLG beam search decoding and KenLM integration modifications

## Next Checks
1. Conduct controlled experiments comparing zero-shot transfer performance between Hindi and Bhojpuri against other linguistically distant language pairs to validate the hypothesized similarity advantage
2. Systematically vary dropout rates (0.05, 0.1, 0.2) during fine-tuning to determine optimal regularization and confirm that 0.1 is indeed the best choice for preventing overfitting on these specific datasets
3. Implement a grid search over language model weights (0.1-1.0) during beam search decoding to verify that the reported WER improvements are robust to weight selection rather than dependent on specific fixed values