---
ver: rpa2
title: Computation with Sequences in a Model of the Brain
arxiv_id: '2306.03812'
source_url: https://arxiv.org/abs/2306.03812
tags:
- sequence
- which
- will
- assemblies
- area
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a formal neural model (NEMO) that captures temporal
  sequences through synaptic plasticity, showing that repeated presentation of stimulus
  sequences leads to their memorization and recall. The model demonstrates that sequences
  can be copied between brain areas, and that scaffolding sequences across multiple
  areas accelerates memorization.
---

# Computation with Sequences in a Model of the Brain

## Quick Facts
- arXiv ID: 2306.03812
- Source URL: https://arxiv.org/abs/2306.03812
- Reference count: 12
- Key outcome: The NEMO model proves Turing completeness by showing that any finite state machine can be learned through sequence presentation, with sequence memorization and recall demonstrated experimentally.

## Executive Summary
This paper introduces NEMO (Neural Assembly Computing), a formal model of the brain where temporal sequences are captured through synaptic plasticity. The model demonstrates that repeated presentation of stimulus sequences leads to their memorization and recall, with performance improving when multiple brain areas work together. Most significantly, the authors prove that any finite state machine can be learned through sequence presentation, establishing the model's Turing completeness. The theoretical results are supported by experiments showing successful sequence memorization and FSM simulation, with performance improving as model parameters increase.

## Method Summary
The NEMO model represents sequences as assemblies of neurons that form through Hebbian plasticity during repeated stimulus presentation. When sequences are presented, assemblies corresponding to each element form and strengthen synaptic connections between consecutive elements. The model uses long-range interneurons to control temporal coordination between brain areas, enabling FSM simulation through state transitions encoded as assembly activations. The authors prove theoretical bounds for sequence memorization and FSM simulation, then validate these through experiments measuring recall accuracy and overlap minimization.

## Key Results
- The model successfully memorizes sequences through Hebbian plasticity, with scaffolded learning across multiple areas accelerating the process
- Any finite state machine can be learned through sequence presentation, establishing Turing completeness
- Experimental results show sequence memorization capacity exceeds theoretical bounds, with performance improving as area size and connectivity increase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal sequences are captured through Hebbian plasticity between consecutive stimuli representations
- Mechanism: When a sequence of stimuli is repeatedly presented, synaptic weights between corresponding neuron assemblies increase, encoding temporal order. The activation of any assembly in the sequence triggers recall of subsequent assemblies through these strengthened connections.
- Core assumption: Hebbian plasticity with multiplicative weight updates (1+β) can reliably encode temporal precedence between neural assemblies
- Evidence anchors:
  - [abstract] "time can be captured naturally as precedence through synaptic weights and plasticity"
  - [section] "repeated presentation of a sequence of stimuli leads to the memorization of the sequence through corresponding neural assemblies: upon future presentation of any stimulus in the sequence, the corresponding assembly and its subsequent ones will be activated"
  - [corpus] No direct evidence found; mechanism is theoretical
- Break condition: If β is too high, plasticity becomes unstable and prevents reliable sequence formation; if too low, insufficient strengthening occurs for recall

### Mechanism 2
- Claim: Multiple brain areas working together (scaffolded representation) accelerates sequence memorization
- Mechanism: When two brain areas receive the same sequence simultaneously, assemblies form in both areas with strengthened cross-area connections. This creates redundant pathways for sequence recall, making the memory more robust and faster to form.
- Core assumption: Inter-area connectivity can support parallel assembly formation that reinforces sequence encoding
- Evidence anchors:
  - [abstract] "by involving in this process additional brain areas during presentation...makes memorization faster and more robust"
  - [section] "by involving in this process additional brain areas during presentation (essentially forming a 'scaffold' of interconnected assemblies) makes memorization faster and more robust"
  - [corpus] No direct evidence found; mechanism is theoretical
- Break condition: If cross-area connectivity is too sparse or plasticity parameters are mismatched between areas, the scaffold effect diminishes

### Mechanism 3
- Claim: Long-range interneurons enable precise temporal control for FSM simulation
- Mechanism: Long-range interneurons can inhibit or disinhibit brain areas, controlling which area fires at each time step. This creates an alternation pattern that allows the network to simulate state transitions in a finite state machine.
- Core assumption: Long-range interneurons can provide fast enough inhibition/disinhibition to coordinate sequential area activation
- Evidence anchors:
  - [abstract] "Long-range interneurons (LRIs)...can be recruited by assemblies in adjacent areas, and whose function is to inhibit or disinhibit remote brain areas to achieve synchrony and control of the computation"
  - [section] "LRIs are essential for the onset of γ oscillations, often considered coterminous with brain computation"
  - [corpus] No direct evidence found; mechanism is theoretical
- Break condition: If interneuron response times are too slow relative to neural activation dynamics, temporal precision is lost

## Foundational Learning

- Concept: Hebbian plasticity and synaptic weight dynamics
  - Why needed here: The entire sequence learning mechanism relies on weight updates that encode temporal relationships
  - Quick check question: What happens to synaptic weights between assemblies when they fire in sequence repeatedly?

- Concept: Assembly formation and stability
  - Why needed here: Understanding how stable sets of neurons (assemblies) form and persist is crucial for grasping sequence representation
  - Quick check question: How does the model ensure that assemblies representing different sequence elements remain distinct?

- Concept: Finite state machine theory and regular languages
  - Why needed here: The proof that the model can simulate any FSM requires understanding state transitions and language recognition
  - Quick check question: What are the minimal components needed to simulate an FSM in a neural network?

## Architecture Onboarding

- Component map:
  - Brain areas: Collections of neurons with local inhibition
  - Synapses: Directed connections with weights subject to Hebbian plasticity
  - Long-range interneurons: Control inhibition/disinhibition of remote areas
  - Input areas: Receive external stimuli and excite connected brain areas
  - Assembly: Stable set of highly intraconnected neurons representing concepts

- Critical path: Stimulus → Assembly formation → Synaptic strengthening → Recall through activation cascade
  - Sequence learning: Repeated presentation → Assembly creation → Cross-assembly weight strengthening → Future recall
  - FSM simulation: State/symbol assembly firing → Arc assembly activation → Next state assembly firing → State transition simulation

- Design tradeoffs:
  - Plasticity rate (β): Higher values speed learning but risk instability; lower values ensure stability but slow learning
  - Area size (n) vs. Assembly size (k): Larger areas allow more assemblies but increase computational complexity
  - Connectivity density (p): Higher density improves signal propagation but increases interference between assemblies

- Failure signatures:
  - Poor recall: Assemblies overlap too much or weights insufficiently strengthened
  - Erratic behavior: Plasticity too high causing unstable weight dynamics
  - No learning: Plasticity too low or insufficient stimulus presentations
  - State confusion: Assemblies representing different states/symbols become indistinguishable

- First 3 experiments:
  1. Train on simple sequences (e.g., A-B-C) and test recall by presenting only the first element
  2. Compare single-area vs. scaffolded learning rates on the same sequences
  3. Implement FSM simulation for a simple language recognizer (e.g., even number of 0s) and test on various input strings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between the number of training presentations and the probability of successful sequence memorization in the scaffolded model?
- Basis in paper: [explicit] The paper proves that T ≥ 1/(4β) * sqrt(ln(nL)/ln(n/k)) presentations are needed for perfect recall in the scaffolded model, but this is a worst-case bound that may not reflect experimental observations.
- Why unresolved: The proof provides a conservative upper bound, but experimental results show faster memorization rates than predicted.
- What evidence would resolve it: Systematic experiments varying the number of presentations and measuring recall probability across different sequence lengths and model parameters.

### Open Question 2
- Question: How does the capacity for sequence memorization scale with brain area size (n) beyond the theoretical bounds?
- Basis in paper: [explicit] The paper shows experimentally that sequence memorization capacity exceeds the theoretical bound of n/k, but the scaling behavior at larger n is unclear.
- Why unresolved: Theoretical analysis provides conservative bounds, but experimental results suggest the actual capacity is much higher.
- What evidence would resolve it: Large-scale simulations varying area size while measuring maximum sequence length that can be successfully memorized.

### Open Question 3
- Question: What is the optimal balance between plasticity parameter β and training time for efficient sequence learning?
- Basis in paper: [explicit] The paper shows that the number of presentations needed is inversely proportional to β, but also notes that β cannot be too high.
- Why unresolved: The theoretical bounds provide only a sufficient condition, not an optimal trade-off between plasticity and training efficiency.
- What evidence would resolve it: Systematic experiments varying β while measuring both memorization accuracy and number of presentations needed across different tasks.

## Limitations

- The model relies on idealized assumptions about neural connectivity and plasticity that may not hold in biological systems
- Long-range interneuron mechanisms for precise temporal control lack empirical validation in biological neural circuits
- The theoretical bounds for sequence memorization are conservative and may not reflect practical performance

## Confidence

- High confidence: The mathematical proof of Turing completeness via FSM simulation is sound within the model's assumptions
- Medium confidence: The sequence learning mechanism via assembly formation is theoretically plausible but lacks biological validation
- Low confidence: The long-range interneuron control mechanism for precise temporal coordination has minimal empirical support

## Next Checks

1. **Biological plausibility test**: Compare predicted assembly overlap patterns during sequence learning with empirical neural imaging data from sequence learning tasks
2. **Parameter sensitivity analysis**: Systematically vary β, n, k, and p to identify robust operating regimes and failure thresholds
3. **Biological constraint integration**: Incorporate realistic neural dynamics including synaptic delays, energy constraints, and noise to test whether FSM simulation remains feasible