---
ver: rpa2
title: An Ambiguity Measure for Recognizing the Unknowns in Deep Learning
arxiv_id: '2312.06077'
source_url: https://arxiv.org/abs/2312.06077
tags:
- decision
- space
- boundaries
- feature
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a geometric ambiguity measure to identify unknowns
  in deep learning models by analyzing the learned feature space. The measure quantifies
  ambiguity using distances from training data convex hulls and decision boundaries
  in the latent space, which is obtained via SVD of the last hidden layer.
---

# An Ambiguity Measure for Recognizing the Unknowns in Deep Learning

## Quick Facts
- arXiv ID: 2312.06077
- Source URL: https://arxiv.org/abs/2312.06077
- Reference count: 34
- Primary result: Geometric ambiguity measure using distances from training data convex hulls and decision boundaries in latent space effectively detects model failures

## Executive Summary
This paper introduces a geometric ambiguity measure for identifying unknown inputs in deep learning models by analyzing the learned feature space. The method quantifies ambiguity using distances from training data convex hulls and decision boundaries in a latent space obtained via SVD of the last hidden layer. The authors provide a theoretical framework proving that regions far from decision boundaries guarantee high model confidence, and demonstrate the measure's effectiveness at detecting in-distribution mistakes, adversarial inputs, and out-of-distribution samples across ResNet and Swin Transformer models.

## Method Summary
The method projects inputs into a lower-dimensional feature space using SVD of the last hidden layer, then computes geometric distances to both the convex hull of training data and decision boundaries. An ambiguity score is calculated based on these distances, with inputs far from convex hulls and close to decision boundaries flagged as ambiguous. The paper proves that maintaining a minimum distance from decision boundaries ensures high model confidence, and develops a unified ambiguity measure combining four geometric components. The approach can be used for abstention or ambiguity detection, providing interpretable explanations for model uncertainty.

## Key Results
- The ambiguity measure detects significant portions of in-distribution mistakes, adversarial inputs, and out-of-distribution samples
- Regions far from decision boundaries guarantee high model confidence regardless of proximity to training data
- The simpler rule-of-thumb version performs nearly as well as the full measure
- Computationally efficient implementation achieves 100ms running time for Swin Transformer on ImageNet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ambiguity can be quantified by geometric distances in the learned feature space
- Mechanism: The method projects inputs into a lower-dimensional feature space (via SVD of the last hidden layer), then computes distances to both the convex hull of training data and the decision boundaries. Inputs far from the convex hull and close to decision boundaries are flagged as ambiguous
- Core assumption: The feature space learned by the model preserves geometric relationships relevant to classification confidence
- Evidence anchors: [abstract] "We define the ambiguity based on the geometric arrangements of the decision boundaries and the convex hull of training set in the feature space learned by the trained model"

### Mechanism 2
- Claim: Regions far from decision boundaries have guaranteed high model confidence
- Mechanism: The paper proves that maintaining a minimum distance Î´ from the closest decision boundary ensures the model's confidence (softmax score) exceeds a calculable threshold
- Core assumption: The model's confidence is solely determined by proximity to decision boundaries
- Evidence anchors: [section 4.4] "we prove that only in the vicinity of decision boundaries a model has low confidence, and in all other regions of the domain, a model is guaranteed to have high confidence"

### Mechanism 3
- Claim: The ambiguity measure can detect various failure modes using a single unified framework
- Mechanism: By combining four geometric measures (distance to convex hull, distance to decision boundaries, overlap detection, and knowledge gaps), the method creates a comprehensive ambiguity score that captures different types of uncertainty
- Core assumption: Different failure modes manifest as distinct geometric patterns in the feature space
- Evidence anchors: [abstract] "a single ambiguity measure may detect a considerable portion of mistakes of a model on in-distribution samples, adversarial inputs, as well as out-of-distribution inputs"

## Foundational Learning

- Concept: Feature space geometry and SVD decomposition
  - Why needed here: The method relies on projecting inputs into a lower-dimensional feature space where geometric relationships can be meaningfully computed
  - Quick check question: Can you explain why SVD is used on the last hidden layer and how it affects the geometric relationships in the feature space?

- Concept: Convex hulls and their computational properties
  - Why needed here: The ambiguity measure uses distances to convex hulls of training data as one component, requiring understanding of how to compute these distances efficiently
  - Quick check question: What algorithm does the paper use to approximate distance to convex hulls, and why is an approximation acceptable here?

- Concept: Decision boundary properties in neural network feature spaces
  - Why needed here: The method computes distances to decision boundaries, requiring understanding of their geometric properties and how they partition the feature space
  - Quick check question: How does the paper prove that regions far from decision boundaries have guaranteed high confidence?

## Architecture Onboarding

- Component map: Input preprocessing -> Model inference -> Feature space projection (SVD) -> Geometric computation (distances to convex hulls and decision boundaries) -> Ambiguity score calculation -> Abstention/decision

- Critical path: The geometric computation phase, particularly projecting points to decision boundaries and computing distances, as these operations can be computationally expensive in high-dimensional spaces

- Design tradeoffs:
  - Using SVD for feature space compression trades some geometric precision for computational efficiency
  - The approximation algorithms for convex hull distances trade exactness for speed
  - The unified ambiguity measure trades specificity for broader applicability across failure modes

- Failure signatures:
  - High computational cost when projecting to decision boundaries in very high-dimensional spaces
  - Poor performance if the feature space doesn't preserve meaningful geometric relationships
  - False positives/negatives if the geometric measures don't correlate well with actual model uncertainty

- First 3 experiments:
  1. Compute the ambiguity measure on a small test set and verify it correlates with known misclassifications
  2. Test the abstention mechanism by setting different ambiguity thresholds and measuring accuracy vs. coverage trade-offs
  3. Compare the full ambiguity measure against the simplified rule-of-thumb version on detecting adversarial examples

## Open Questions the Paper Calls Out

- How do the geometric ambiguity measures perform on different types of models beyond ResNet and Swin Transformer, such as models with different architectures or training objectives (e.g., self-supervised models)?
- Can the ambiguity measure be extended to tasks beyond image classification, such as object detection, segmentation, or natural language processing?
- How does the ambiguity measure perform in detecting more subtle or complex failure modes, such as adversarial attacks that are specifically designed to evade detection or out-of-distribution samples that are similar to in-distribution data?

## Limitations
- Geometric assumptions validity: The framework depends on SVD projections preserving meaningful geometric relationships, which lacks empirical validation
- Computational tractability: The exact complexity of distance computations to convex hulls and decision boundaries isn't fully characterized
- Generalization across architectures: Tested only on ResNet and Swin Transformer, limiting claims about broader applicability

## Confidence
- High confidence: The theoretical framework proving high confidence regions exist far from decision boundaries (Section 4.4)
- Medium confidence: The geometric interpretation of ambiguity and its connection to failure modes
- Low confidence: The computational efficiency claims and the universal applicability across model architectures

## Next Checks
1. Implement the SVD projection and compute distances on a small synthetic dataset where ground truth geometric relationships are known to verify the method correctly identifies ambiguous vs. unambiguous regions
2. Profile the exact running time of the distance computations for various dataset sizes and model architectures to compare against the claimed 100ms benchmark and analyze scaling behavior
3. Systematically test the method's ability to detect each failure mode (in-distribution errors, adversarial inputs, out-of-distribution samples) separately rather than just reporting aggregate performance