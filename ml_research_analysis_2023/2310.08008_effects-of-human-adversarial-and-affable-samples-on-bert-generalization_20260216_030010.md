---
ver: rpa2
title: Effects of Human Adversarial and Affable Samples on BERT Generalization
arxiv_id: '2310.08008'
source_url: https://arxiv.org/abs/2310.08008
tags:
- samples
- training
- performance
- data
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the impact of human-adversarial (h-adversarial)
  and human-affable (h-affable) samples on BERT generalization performance in text
  classification and relation extraction tasks. H-adversarial samples are defined
  as pairs with minor differences but different ground-truth labels, while h-affable
  samples are pairs with minor differences but the same label.
---

# Effects of Human Adversarial and Affable Samples on BERT Generalization

## Quick Facts
- arXiv ID: 2310.08008
- Source URL: https://arxiv.org/abs/2310.08008
- Reference count: 24
- Key outcome: Incorporating 10-30% h-adversarial samples improves BERT precision and F1 scores by up to 20 points in text classification and relation extraction tasks

## Executive Summary
This paper examines how human-adversarial (h-adversarial) and human-affable (h-affable) samples impact BERT's generalization performance. H-adversarial samples are pairs with minor differences but different labels, while h-affable samples have minor differences but the same label. Experiments across IMDB sentiment analysis, a custom keyword detection task, and ChemProt relation extraction show that 10-30% h-adversarial samples significantly improve precision and F1 scores, while h-affable samples do not improve performance and can degrade it when exceeding certain thresholds.

## Method Summary
The study uses BERT-based models across three tasks: IMDB sentiment analysis, KDAO keyword detection, and ChemProt relation extraction. H-adversarial and h-affable samples were either manually curated or automatically generated based on word error rates calculated using Levenshtein edit distance. Models were trained with varying proportions of these samples while keeping total training size constant. Training used learning rates of 1e-05 for BERT/BioBERT and 7e-06 for RoBERTa, batch size of 64, and early stopping with patience of 10 epochs.

## Key Results
- Incorporating 10-30% h-adversarial samples improves precision and F1 scores by up to 20 points compared to random samples alone
- H-affable samples do not contribute to model generalization and can degrade performance when their proportion exceeds certain thresholds
- The optimal h-adversarial proportion depends on baseline performance, with larger improvements seen when initial performance is low

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low to moderate proportions (10-30%) of h-adversarial samples improve model precision and F1 scores by up to 20 points by explicitly guiding the model to learn discriminatory features rather than spurious correlations.
- Mechanism: H-adversarial samples force the model to focus on subtle but meaningful differences between similar inputs that have different labels, thereby reducing reliance on superficial patterns that may not generalize to unseen data.
- Core assumption: The model can effectively learn from small but meaningful differences in input when these differences are explicitly tied to label changes.
- Evidence anchors:
  - [abstract] "having 10-30% h-adversarial instances improves the precision, and therefore F1, by up to 20 points in the tasks of text classification and relation extraction."
  - [section] "The intuition behind h-adversarials, as shown in Table 1, is to explicitly guide the model to learn discriminatory features through training data, thus reducing its reliance on spurious features."
- Break condition: If the model cannot effectively learn from subtle input differences, or if the h-adversarial samples are not truly representative of meaningful differences, performance improvements will not be observed.

### Mechanism 2
- Claim: H-affable samples do not contribute to model generalization and can even degrade performance by reinforcing potentially spurious patterns.
- Mechanism: H-affable samples, being very similar inputs with the same label, add redundant information to the training set that reinforces existing patterns, including spurious ones, without providing new learning signals.
- Core assumption: The model benefits more from diverse learning signals than from repeated exposure to similar patterns.
- Evidence anchors:
  - [abstract] "h-affables may not contribute to a model's generalizability and may even degrade generalization performance."
  - [section] "We also hypothesize, that h-affable training samples, or samples that are similar to another but have minor differences that preserve the label, can potentially result in redundant information in the training data, reinforcing possibly spurious patterns and worsening a model's performance."
- Break condition: If the h-affable samples introduce new, meaningful variations that the model can learn from, or if the model is particularly sensitive to repetition, performance may not degrade.

### Mechanism 3
- Claim: The optimal proportion of h-adversarial samples depends on the initial performance of the model, with larger improvements seen when baseline performance is low.
- Mechanism: When baseline performance is low, the model has more room for improvement and can benefit more from the additional learning signals provided by h-adversarial samples. As performance improves, the marginal benefit of additional h-adversarials decreases.
- Core assumption: The relationship between h-adversarial proportion and performance improvement is not linear, but rather depends on the initial performance of the model.
- Evidence anchors:
  - [abstract] "We find that for a fixed size of training samples, as a rule of thumb, having 10-30% h-adversarial instances improves the precision, and therefore F1, by up to 20 points in the tasks of text classification and relation extraction."
  - [section] "When baseline (@rpn_hv=0.0) performance is low, adding smaller percentages of h-adversarials boosts performance compared to adding random samples."
- Break condition: If the model is already performing well, or if the additional h-adversarials introduce noise rather than meaningful learning signals, performance improvements will not be observed.

## Foundational Learning

- Concept: Understanding the difference between h-adversarial and h-affable samples
  - Why needed here: To correctly implement the data curation strategy proposed in the paper, engineers must be able to distinguish between samples that introduce meaningful learning signals (h-adversarials) and those that introduce redundancy (h-affables).
  - Quick check question: Given two similar text samples with different labels, how would you classify this pair as h-adversarial or h-affable?

- Concept: Calculating word error rate based on Levenshtein edit distance
  - Why needed here: To quantify the similarity between samples and determine whether they qualify as h-adversarial or h-affable based on the predefined threshold.
  - Quick check question: Given two text samples, how would you calculate their word error rate using the Levenshtein edit distance formula provided in the paper?

- Concept: Implementing data augmentation strategies
  - Why needed here: To generate h-adversarial and h-affable samples as needed to implement the proposed data curation strategy.
  - Quick check question: Given a set of positive samples, how would you generate h-adversarial samples by making minor changes that alter the label?

## Architecture Onboarding

- Component map: Data curation pipeline -> Model training pipeline -> Evaluation pipeline
- Critical path: Data curation -> Model training -> Evaluation -> Performance analysis
- Design tradeoffs: Balancing the proportion of h-adversarial samples to maximize performance improvements without introducing noise; deciding whether to generate samples automatically or manually based on available resources
- Failure signatures: Performance degradation when h-adversarial proportion exceeds 30%; lack of improvement with h-affable samples; inconsistent performance across runs
- First 3 experiments:
  1. Vary h-adversarial proportion (0%, 10%, 20%, 30%, 40%) and evaluate on held-out test set
  2. Vary h-affable proportion (0%, 10%, 20%, 30%, 40%) and evaluate on held-out test set
  3. Compare mixed h-adversarial/random training to random-only and h-adversarial-only training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal proportion of h-adversarial samples that maximizes BERT generalization performance across different NLP tasks?
- Basis in paper: [explicit] The paper found that 10-30% h-adversarial samples improved precision and F1 scores, but performance plateaus or degrades beyond this range.
- Why unresolved: The study only tested a limited range of h-adversarial proportions (0%, 10%, 20%, 30%, 90%). The optimal proportion may vary depending on the specific task and dataset characteristics.
- What evidence would resolve it: A more extensive empirical study testing a wider range of h-adversarial proportions (e.g., 5%, 15%, 25%, 35%, etc.) across multiple NLP tasks and datasets would help identify the optimal proportion.

### Open Question 2
- Question: How do h-adversarial and h-affable samples impact BERT generalization performance on other NLP tasks beyond text classification and relation extraction?
- Basis in paper: [inferred] The study focused on text classification and relation extraction tasks. The impact of h-adversarial and h-affable samples on other NLP tasks, such as named entity recognition, question answering, or summarization, remains unknown.
- Why unresolved: The study's scope was limited to specific NLP tasks. Extending the analysis to other tasks would provide a more comprehensive understanding of the generalizability of the findings.
- What evidence would resolve it: Conducting similar experiments on other NLP tasks and comparing the results would shed light on the broader applicability of h-adversarial and h-affable samples.

### Open Question 3
- Question: What are the underlying mechanisms by which h-adversarial samples improve BERT generalization performance?
- Basis in paper: [inferred] The paper suggests that h-adversarial samples guide the model to learn discriminatory features and reduce reliance on spurious features. However, the specific mechanisms are not fully elucidated.
- Why unresolved: The study focused on empirical observations rather than theoretical explanations. Understanding the underlying mechanisms would provide insights into how to design more effective training strategies.
- What evidence would resolve it: Conducting in-depth analyses of the learned representations and feature importance in models trained with h-adversarial samples could reveal the mechanisms behind the performance improvements.

## Limitations
- Limited scope of experimental validation across only three specific tasks without statistical significance testing
- Incomplete methodology details for generating h-adversarial and h-affable samples, making exact reproduction challenging
- Unclear generalizability to other domains and model architectures beyond the specific BERT-based models tested

## Confidence

**High Confidence**: The core finding that h-adversarial samples in the 10-30% range improve precision and F1 scores is well-supported by experimental results across multiple tasks.

**Medium Confidence**: The claim that h-affable samples can degrade performance when exceeding certain thresholds is less certain, with the underlying mechanism being more speculative.

**Low Confidence**: The generalizability of these findings to other domains and model architectures remains uncertain due to limited testing scope.

## Next Checks

1. Conduct statistical significance testing (e.g., paired t-tests) across multiple random seeds to confirm that observed performance improvements with 10-30% h-adversarial samples are statistically significant and not due to random variation.

2. Implement ablation studies that systematically vary the quality and diversity of h-adversarial samples (e.g., using automated vs. manual generation methods) to determine the minimum quality threshold required for performance improvements.

3. Test the findings on additional text classification tasks from different domains (e.g., medical text classification, legal document classification) to assess the generalizability of the h-adversarial sampling strategy beyond the three tasks studied in the paper.