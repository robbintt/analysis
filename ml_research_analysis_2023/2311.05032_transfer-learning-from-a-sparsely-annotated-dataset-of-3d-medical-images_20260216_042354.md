---
ver: rpa2
title: Transfer learning from a sparsely annotated dataset of 3D medical images
arxiv_id: '2311.05032'
source_url: https://arxiv.org/abs/2311.05032
tags:
- learning
- transfer
- segmentation
- dataset
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the efficacy of transfer learning from
  a large but sparsely annotated dataset of 3D medical images to improve organ segmentation
  performance in new tasks. A 3D U-Net base model was trained on 556 CT scans with
  annotations of 22 anatomical structures, using a hybrid loss function to handle
  missing annotations.
---

# Transfer learning from a sparsely annotated dataset of 3D medical images

## Quick Facts
- arXiv ID: 2311.05032
- Source URL: https://arxiv.org/abs/2311.05032
- Reference count: 40
- Primary result: Transfer learning from a sparsely annotated dataset significantly improves 3D medical image segmentation performance, especially with limited training data

## Executive Summary
This paper investigates transfer learning from a large but sparsely annotated dataset of 3D medical images to improve organ segmentation performance on new tasks. A 3D U-Net base model was trained on 556 CT scans with annotations of 22 anatomical structures, using a hybrid loss function to handle missing annotations. The base model was then used to initialize networks for four new segmentation tasks (esophagus, vertebrae, lung lobes, and prostate), with varying levels of annotation availability. Results showed that transfer learning significantly improved performance, especially when limited training data was available, with fine-tuning being more effective than vanilla transfer learning.

## Method Summary
The study employed a 3D U-Net architecture trained on a sparsely annotated dataset of 556 CT scans with 22 anatomical structures labeled across different images. A hybrid masked loss function combining Dice and cross-entropy terms was used to handle missing annotations. Transfer learning was applied to four downstream segmentation tasks with varying annotation availability. Two transfer strategies were tested: vanilla transfer learning (updating all weights) and transfer learning with fine-tuning (updating only the last three layers initially, then all layers). Performance was evaluated using Dice score across different training set sizes to simulate scarce data scenarios.

## Key Results
- Transfer learning with fine-tuning increased performance by up to 0.129 (+28%) Dice score compared to training from scratch
- Transfer learning was particularly beneficial when small datasets were available for new tasks
- Fine-tuning the base model was more effective than updating all network weights with vanilla transfer learning
- The base model achieved competitive performance on the sparsely annotated dataset despite missing annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hybrid masked loss function enables effective training on sparsely annotated data by ignoring unlabeled classes while maintaining segmentation quality.
- Mechanism: The loss function is defined only on annotated structures using a binary flag δc, which excludes unlabeled classes from the loss calculation. This prevents the model from incorrectly penalizing predictions for unlabeled structures while still learning from the available annotations.
- Core assumption: The model can learn to segment all structures across different images, even when only a subset is annotated per image.
- Evidence anchors:
  - [abstract] "A base segmentation model (3D U-Net) was trained on a large and sparsely annotated dataset; its weights were used for transfer learning on four new down-stream segmentation tasks"
  - [section] "To mitigate this problem, we trained the base model using a hybrid masked loss function composed of two terms, an average Dice score term and a cross-entropy term. These were computed only for the present foreground structures, ignoring classes that were not annotated in an image."

### Mechanism 2
- Claim: Transfer learning from the base model significantly improves segmentation performance, especially when training data is limited.
- Mechanism: The pre-trained weights provide a strong initialization that captures general anatomical features, allowing the model to converge faster and achieve better performance than training from scratch, particularly with small datasets.
- Core assumption: Features learned from the large but sparsely annotated dataset are general enough to be useful for new segmentation tasks.
- Evidence anchors:
  - [abstract] "Transfer learning with fine-tuning increased the performance by up to 0.129 (+28%) Dice score than experiments trained from scratch"
  - [section] "Results showed that transfer learning from the base model was beneficial when small datasets were available, providing significant performance improvements"

### Mechanism 3
- Claim: Fine-tuning transfer learning outperforms vanilla transfer learning by preventing catastrophic forgetting while allowing task-specific adaptation.
- Mechanism: Fine-tuning first updates only the last three layers to adapt to the new task, then gradually updates all layers, preserving useful low-level features while allowing specialization for the new segmentation task.
- Core assumption: The initial layers of the network contain general features that are useful across multiple segmentation tasks.
- Evidence anchors:
  - [abstract] "where fine-tuning the base model is more beneficial than updating all the network weights with vanilla transfer learning"
  - [section] "By first adjusting only the last few layers to the new task and then fine-tuning the entire network, the network might profit more from transfer learning in a second step"

## Foundational Learning

- Concept: Hybrid loss function design for sparse annotations
  - Why needed here: Standard loss functions assume complete annotations for all classes, which is not the case in the sparsely annotated dataset
  - Quick check question: How does the binary flag δc in the loss function prevent incorrect penalization of unlabeled structures?

- Concept: Transfer learning initialization strategies
  - Why needed here: Different initialization approaches (scratch, vanilla transfer, fine-tuning) have significantly different impacts on performance, especially with limited data
  - Quick check question: What is the key difference between vanilla transfer learning and transfer learning with fine-tuning in terms of which layers are updated?

- Concept: Multi-dataset harmonization and preprocessing
  - Why needed here: The base model is trained on data from multiple sources with different annotation protocols and image characteristics
  - Quick check question: How were kidney annotations harmonized between KiTS19 and other datasets that distinguished left/right kidneys?

## Architecture Onboarding

- Component map: 3D U-Net architecture with 4 resolution levels, starting with 32 filters and doubling after each pooling layer. No batch normalization layers to avoid data-specific normalization issues. Input patch size of 132×132×132 voxels for CT, 108×108×108 for MR. Output patches are smaller due to no padding in convolutions.
- Critical path: Data preprocessing → hybrid loss training on sparse annotations → base model evaluation → transfer learning initialization → fine-tuning strategy → final segmentation task evaluation
- Design tradeoffs: Removed batch normalization to prevent transfer learning issues vs. potential training instability. Chose 3D U-Net for volumetric context vs. computational cost. Used isotropic resampling to 1mm resolution vs. potential information loss.
- Failure signatures: Poor performance on sparsely annotated data may indicate insufficient annotated structures per image. Failure to improve with transfer learning may indicate task dissimilarity. Fine-tuning failure may indicate need for different layer freezing strategy.
- First 3 experiments:
  1. Train base model on sparsely annotated data and evaluate Dice score on validation set to verify hybrid loss effectiveness
  2. Test vanilla transfer learning from base model to esophagus segmentation with small training set (10 scans) to establish baseline transfer benefit
  3. Implement and test fine-tuning transfer learning on the same task to compare performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of transfer learning from a sparsely annotated dataset compare to transfer learning from a fully annotated dataset of similar size when both are available for a new task?
- Basis in paper: [inferred] The paper compares transfer learning from the sparsely annotated base model to training from scratch and to transfer learning from a simpler model (ExpVS90), but does not directly compare to transfer learning from a fully annotated dataset.
- Why unresolved: The paper focuses on demonstrating the efficacy of transfer learning from sparse annotations, but does not provide a direct comparison to the ideal case of having a fully annotated dataset of similar size.
- What evidence would resolve it: A study where transfer learning is performed from both a sparsely annotated dataset and a fully annotated dataset of similar size for the same new task, with direct performance comparison.

### Open Question 2
- Question: Does the order in which anatomical structures are annotated in the sparsely annotated dataset affect the performance of the base model and subsequent transfer learning?
- Basis in paper: [explicit] The paper mentions that "By presenting in consecutive training steps images from different subsets, i.e., with different structures annotated, the network can learn to recognize all 22 structures."
- Why unresolved: The paper does not investigate whether the order of presenting structures during training has any impact on the base model's ability to learn features useful for transfer learning.
- What evidence would resolve it: Experiments where the base model is trained with the same sparsely annotated dataset but with different orders of presenting structures, followed by transfer learning experiments to measure any performance differences.

### Open Question 3
- Question: How does the performance of transfer learning from a sparsely annotated dataset scale with the number of anatomical structures annotated in the dataset?
- Basis in paper: [inferred] The paper uses a base model trained on 22 structures, but does not investigate how performance changes with fewer or more annotated structures.
- Why unresolved: The study does not explore the relationship between the number of annotated structures in the base model and the effectiveness of transfer learning for new tasks.
- What evidence would resolve it: Experiments where base models are trained with varying numbers of annotated structures (e.g., 5, 10, 15, 22, 26) and their performance in transfer learning tasks is compared.

## Limitations

- Limited exploration of why certain tasks (vertebrae, prostate) showed less transfer learning improvement than others
- No direct comparison of hybrid loss function against alternative sparse annotation handling strategies
- Lack of quantitative evidence for feature preservation claims in fine-tuning strategy

## Confidence

- Transfer learning effectiveness with limited data: **High** - Supported by quantitative Dice score improvements across all four tasks
- Fine-tuning superiority over vanilla transfer: **Medium** - Results show improvement but lack detailed analysis of layer-wise adaptation
- Hybrid loss function efficacy: **Medium** - Demonstrated functionality but no comparison to alternative sparse annotation handling methods

## Next Checks

1. Conduct ablation studies comparing the hybrid masked loss against alternative sparse annotation strategies (e.g., weighted loss, pseudo-labeling) to quantify its specific contribution to performance gains.

2. Perform layer-wise analysis of feature similarity between base model and fine-tuned models using techniques like Grad-CAM or activation pattern correlation to verify that fine-tuning preserves useful low-level features while adapting higher-level representations.

3. Test transfer learning effectiveness on a "break condition" task - a segmentation problem significantly different from the base model's training data (e.g., brain tumor segmentation) to establish the boundaries of transfer learning applicability.