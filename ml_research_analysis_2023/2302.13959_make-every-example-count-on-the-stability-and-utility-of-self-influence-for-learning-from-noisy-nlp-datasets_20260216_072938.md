---
ver: rpa2
title: 'Make Every Example Count: On the Stability and Utility of Self-Influence for
  Learning from Noisy NLP Datasets'
arxiv_id: '2302.13959'
source_url: https://arxiv.org/abs/2302.13959
tags:
- uence
- data
- self-in
- training
- ltering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the stability and utility of task-agnostic
  self-influence scores for data cleaning in NLP datasets. The authors show that self-influence
  scores are stable across training hyperparameters and model architectures, making
  them a reliable foundation for data cleaning.
---

# Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets

## Quick Facts
- arXiv ID: 2302.13959
- Source URL: https://arxiv.org/abs/2302.13959
- Reference count: 16
- Key outcome: Self-influence scores are stable across training hyperparameters and model architectures, making them a reliable foundation for data cleaning in NLP datasets.

## Executive Summary
This paper investigates the stability and utility of task-agnostic self-influence scores for data cleaning in NLP datasets. The authors demonstrate that self-influence scores are stable across training hyperparameters and model architectures, making them a reliable foundation for data cleaning. They show that filtering highly self-influential examples is more effective for out-of-distribution evaluation, while for in-distribution tasks, threshold filtering may not be the best approach due to the distribution of natural noise. To address this, the authors propose an automated curriculum learning approach that dynamically identifies and reweights data subsets based on self-influence scores, leading to improved performance on noisy and clean datasets compared to threshold filtering.

## Method Summary
The paper proposes using Arnoldi-based Influence Functions (ABIF) to compute self-influence scores for NLP datasets. These scores are then used to identify and filter outliers or reweight data subsets through automated curriculum learning. The automated curriculum learning approach uses multi-armed bandits to dynamically identify and reweight data subsets based on self-influence scores. The method is evaluated on machine translation (Paracrawl), question answering (Natural Questions, TriviaQA), and text classification (Wikipedia Toxicity) tasks.

## Key Results
- Self-influence scores are stable across training hyperparameters and model architectures, making them a reliable foundation for data cleaning.
- Filtering highly self-influential examples is more effective for out-of-distribution evaluation, with up to +9 BLEU points improvement on Paracrawl-newstest16.
- Automated curriculum learning using self-influence scores improves over threshold-based filtering on noisy and clean datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-influence scores are stable across training hyperparameters for a fixed architecture.
- Mechanism: Stability of self-influence scores allows them to serve as a reliable proxy for data quality regardless of minor training variations.
- Core assumption: Self-influence stability holds when the underlying model architecture remains constant.
- Evidence anchors:
  - [abstract]: "The authors show that self-influence scores are stable across training hyperparameters and model architectures, making them a reliable foundation for data cleaning."
  - [section]: "We find that the self-influence are stable for a fixed architecture across common training hyperparameters, but care should be exercised in transferring findings between architectures of different capacity."
- Break condition: If the model architecture changes significantly (e.g., different capacity or attention mechanisms), the stability assumption breaks down.

### Mechanism 2
- Claim: Filtering highly self-influential examples is more effective for out-of-distribution evaluation.
- Mechanism: Self-influence scores capture outliers that prevent learning systematic noise patterns, improving generalization to unseen distributions.
- Core assumption: The same noise patterns that hurt in-distribution performance also generalize to out-of-distribution scenarios.
- Evidence anchors:
  - [abstract]: "They demonstrate that filtering highly self-influential examples is more effective for out-of-distribution evaluation."
  - [section]: "We show that self-influence filtering using ABIF bring higher improvements for the o.o.d. evaluation setup: up to +9 BLEU points on Paracrawl-newstest16."
- Break condition: If the out-of-distribution test set shares very different characteristics from the training noise, the effectiveness of filtering may decrease.

### Mechanism 3
- Claim: Automated curriculum learning using self-influence scores improves over threshold-based filtering.
- Mechanism: Dynamic data reweighting via multi-armed bandits adapts to the specific needs of the model during training, avoiding the rigidity of fixed thresholds.
- Core assumption: The model's needs change during training, and a fixed threshold cannot adapt to these changes.
- Evidence anchors:
  - [abstract]: "To address this, the authors propose an automated curriculum learning approach that dynamically identifies and reweights data subsets based on self-influence scores."
  - [section]: "We show that dynamically reweighing based on multi-armed bandits which pick self-influence buckets to sample batches from, improves over threshold filtering on noisy and clean datasets."
- Break condition: If the dataset is already clean and well-curated, the benefits of dynamic reweighting may be minimal.

## Foundational Learning

- Concept: Influence Functions
  - Why needed here: Understanding how individual training examples influence the model's predictions is crucial for identifying outliers and improving data quality.
  - Quick check question: Can you explain how influence functions quantify the effect of a training example on a test point's loss?

- Concept: Automated Curriculum Learning
  - Why needed here: Dynamic data scheduling is essential for adapting to the model's changing needs during training and avoiding the limitations of fixed threshold filtering.
  - Quick check question: How does the multi-armed bandit framework in automated curriculum learning learn which data subsets are most useful?

- Concept: Data Quality Assessment
  - Why needed here: Evaluating the effectiveness of data cleaning methods requires understanding the relationship between data quality and model performance.
  - Quick check question: What are some common metrics used to assess data quality in NLP datasets?

## Architecture Onboarding

- Component map:
  Data Ingestion -> Self-Influence Calculation -> Data Bucketing -> Automated Curriculum Learning -> Model Training -> Evaluation

- Critical path:
  1. Compute self-influence scores for the training data.
  2. Split the data into buckets based on self-influence scores.
  3. Implement the multi-armed bandit algorithm to dynamically reweight the buckets.
  4. Fine-tune the model on the reweighted data.
  5. Evaluate the model's performance on various test sets.

- Design tradeoffs:
  - ABIF hyperparameters (number of projectors, iterations) vs. accuracy and computational cost.
  - Number of data buckets vs. granularity of control and computational overhead.
  - Exploration rate in the bandit algorithm vs. convergence speed and final performance.

- Failure signatures:
  - If self-influence scores are unstable across training runs, the data cleaning method may be unreliable.
  - If the multi-armed bandit algorithm fails to learn a meaningful policy, the dynamic reweighting may not improve performance.
  - If the number of data buckets is too large, the computational overhead may outweigh the benefits.

- First 3 experiments:
  1. Compute self-influence scores for a small subset of the Paracrawl dataset and visualize their distribution.
  2. Implement a simple threshold-based filtering method and evaluate its impact on model performance.
  3. Implement a basic version of the multi-armed bandit algorithm and observe its behavior on a toy dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do self-influence scores vary across different NLP tasks and domains beyond those studied in this paper?
- Basis in paper: [explicit] The authors mention that they study stability and utility of self-influence scores across machine translation, question answering, and text classification tasks.
- Why unresolved: The paper only investigates a limited set of tasks and domains. It's unclear how self-influence scores would behave in other NLP applications or with different types of data.
- What evidence would resolve it: Conducting experiments with self-influence scores on a broader range of NLP tasks and domains, comparing stability and effectiveness across different settings.

### Open Question 2
- Question: What is the relationship between self-influence scores and model interpretability or explainability?
- Basis in paper: [inferred] The authors discuss using self-influence scores for data cleaning and curriculum learning, which are related to model interpretability and explainability. However, they don't directly investigate this relationship.
- Why unresolved: While self-influence scores are shown to be useful for data cleaning and curriculum learning, their connection to model interpretability or explainability is not explored.
- What evidence would resolve it: Analyzing how self-influence scores correlate with other interpretability metrics, such as feature importance or saliency maps, and investigating whether they can provide insights into model behavior.

### Open Question 3
- Question: How do self-influence scores perform in low-resource or cross-lingual settings?
- Basis in paper: [inferred] The paper focuses on high-resource settings and doesn't address how self-influence scores might behave in low-resource or cross-lingual scenarios.
- Why unresolved: Low-resource and cross-lingual settings often present unique challenges, and it's unclear whether self-influence scores would be equally effective in these contexts.
- What evidence would resolve it: Conducting experiments with self-influence scores in low-resource or cross-lingual settings, comparing performance to other data cleaning or curriculum learning methods, and analyzing the factors that influence their effectiveness.

## Limitations

- Architecture Transferability: Self-influence scores are stable within the same architecture but may not transfer between architectures of different capacity, requiring recomputation when changing model sizes.
- Dataset Dependency: The effectiveness of self-influence filtering appears strongly tied to the presence of systematic noise patterns, with limited benefits for relatively clean datasets.
- Computational Cost: Arnoldi-based influence functions (ABIF) are computationally expensive, though more efficient than traditional influence functions, with no concrete runtime comparisons provided.

## Confidence

- High Confidence: Claims about self-influence stability within the same architecture across different training hyperparameters.
- Medium Confidence: Claims about automated curriculum learning (AutoCL) outperforming threshold filtering.
- Medium Confidence: Claims about self-influence filtering being more effective for out-of-distribution evaluation.

## Next Checks

1. **Architecture Transferability Test**: Compute self-influence scores using LongT5-base and LongT5-large on the same dataset, then measure the Spearman correlation between rankings to validate stability across model capacities.

2. **Computational Overhead Analysis**: Measure and compare the wall-clock time for computing self-influence scores using ABIF versus traditional influence functions, and benchmark the total time including AutoCL implementation.

3. **Dataset Cleanliness Threshold**: Systematically vary the noise level in a clean dataset (e.g., by injecting synthetic noise into Wikipedia Toxicity) and measure the performance gap between threshold filtering and AutoCL to identify the minimum noise level required for these methods to be beneficial.