---
ver: rpa2
title: 'An Empirical Investigation into Benchmarking Model Multiplicity for Trustworthy
  Machine Learning: A Case Study on Image Classification'
arxiv_id: '2311.14859'
source_url: https://arxiv.org/abs/2311.14859
tags:
- multiplicity
- accuracy
- learning
- dataset
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework to measure and analyze model multiplicity
  in deep learning, a phenomenon where multiple models achieve similar performance
  but exhibit distinct behaviors. The key idea is to translate various trustworthy
  ML metrics into a common scale called "accuracy under intervention" through appropriate
  interventions.
---

# An Empirical Investigation into Benchmarking Model Multiplicity for Trustworthy Machine Learning: A Case Study on Image Classification

## Quick Facts
- arXiv ID: 2311.14859
- Source URL: https://arxiv.org/abs/2311.14859
- Reference count: 40
- Primary result: Model multiplicity persists across trustworthy ML metrics even after enforcing additional specifications, revealing fundamental over-parameterization issues in deep learning.

## Executive Summary
This paper introduces a framework to measure and analyze model multiplicity in deep learning, where multiple models achieve similar accuracy but exhibit distinct behaviors across trustworthy ML metrics. The key innovation is translating diverse trustworthy metrics into a common "accuracy under intervention" scale, enabling direct comparison of multiplicity across dimensions like fairness, robustness, privacy, and security. Through a case study on image classification using UTKFace and CIFAR10 datasets, the authors demonstrate that multiplicity remains severe even after model selection based on multiple trustworthy metrics, highlighting the need for more fundamental research on this phenomenon to ensure trustworthy real-world deployments.

## Method Summary
The framework measures model multiplicity by converting trustworthy ML metrics into a common accuracy scale through appropriate interventions. For each metric, an intervention is defined (e.g., adversarial perturbations for security, output noise for privacy) and model accuracy under this intervention becomes the comparable metric. Multiplicity sheets are constructed at three levels: raw metric scores, pairwise differences within tables, and maximum differences across all scores. The method is applied to image classification tasks using UTKFace and CIFAR10 datasets, training multiple models with varying hyperparameters and random seeds, then analyzing multiplicity across standard and trustworthy metrics.

## Key Results
- Multiplicity exists across trustworthy metrics (fairness, robustness, privacy, security) even after model selection based on these metrics
- Interventions effectively translate diverse metrics to a common accuracy scale for comparison
- Over-parameterization remains a fundamental challenge, as multiplicity persists despite enforcing trustworthy requirements
- The framework reveals significantly higher multiplicity in trustworthy metrics compared to standard accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting trustworthy ML metrics into a common "accuracy under intervention" scale enables direct comparison of model multiplicity across different trustworthiness dimensions.
- Mechanism: By defining an intervention (e.g., fixed adversarial budget for security, fixed output perturbation rate for privacy), each metric becomes an accuracy score that can be measured and compared on the same scale.
- Core assumption: The intervention accurately represents the original metric's intent while remaining computationally tractable and interpretable.
- Evidence anchors:
  - [abstract] "translating various trustworthy metrics into accuracy under appropriate interventions"
  - [section 2.1] "measure the model accuracy under a well-designed intervention that represents a proxy for our original trustworthy metric"
  - [corpus] No direct evidence; assumption based on methodology description.
- Break condition: If the intervention does not meaningfully capture the essence of the original metric, comparisons become misleading.

### Mechanism 2
- Claim: Multiplicity sheets aggregate multiplicity across different axes (hyperparameters, random seeds) in three progressively condensed levels, enabling both detailed and high-level analysis.
- Mechanism: First level shows raw metric scores, second level computes ∆max within each table to show pairwise differences, third level computes ∆all_max across all scores to give a single multiplicity value.
- Core assumption: The choice of axes and their combinations meaningfully capture the sources of multiplicity relevant to deployment.
- Evidence anchors:
  - [section 2.2] "We want a method that can provide both summaries for a quicker scan and detailed results for a more in-depth analysis"
  - [section 2.2] "Finally, the third level further aggregates the complete multiplicity sheet by taking the maximum difference across all raw metric scores"
  - [corpus] No direct evidence; methodology described but not validated.
- Break condition: If important sources of multiplicity are omitted from the sheet structure, the aggregation will miss critical patterns.

### Mechanism 3
- Claim: Even after enforcing model selection based on multiple trustworthy metrics, multiplicity persists across unforeseen metrics, indicating fundamental over-parameterization issues.
- Mechanism: Model selection narrows the candidate pool based on specified metrics, but the remaining models still exhibit significant variation on metrics not used in selection.
- Core assumption: The unforeseen metrics are sufficiently different from the selection metrics that they reveal hidden multiplicity.
- Evidence anchors:
  - [section 4] "despite following the recommendations of recent literature and providing additional specifications using trustworthy metrics, our study reveals that model multiplicity can still create unforeseen failure cases"
  - [section 4] "despite this, we see persistent multiplicity on trustworthy issues not seen during model selection"
  - [corpus] No direct evidence; finding based on experimental results.
- Break condition: If unforeseen metrics are too correlated with selection metrics, they may not reveal additional multiplicity.

## Foundational Learning

- Concept: Over-parameterization and its relationship to multiplicity
  - Why needed here: The paper attributes multiplicity to over-parameterization, so understanding this relationship is fundamental to interpreting the results
  - Quick check question: If a model has fewer parameters than training samples, would we expect multiplicity to be more or less severe?

- Concept: Trustworthy ML metrics (fairness, robustness, privacy, security)
  - Why needed here: The paper converts these metrics to a common scale, requiring understanding of what each metric measures
  - Quick check question: For a facial recognition model, which metric would be most concerned with treating different racial groups equally?

- Concept: Random seeds and their impact on model training
  - Why needed here: The paper emphasizes the importance of random seeds in creating multiplicity, so understanding this source of variation is crucial
  - Quick check question: If you train the same model architecture with the same hyperparameters but different random seeds, should you expect identical performance?

## Architecture Onboarding

- Component map:
  - Data preprocessing pipeline (UTKFace/CIFAR10 loading and splitting) -> Model training module (with configurable hyperparameters and random seeds) -> Metric computation module (fairness, robustness, privacy, security with interventions) -> Multiplicity sheet generation module (aggregation and visualization) -> Model selection module (filtering based on multiple metrics)

- Critical path:
  1. Load and preprocess dataset
  2. Train multiple models with varying configurations and random seeds
  3. Compute all trustworthy metrics with appropriate interventions
  4. Generate multiplicity sheets at three levels of aggregation
  5. Perform model selection based on specified metrics
  6. Evaluate multiplicity on unforeseen metrics

- Design tradeoffs:
  - Computational cost vs. thoroughness: Training many models with different seeds and configurations is expensive but necessary for capturing multiplicity
  - Intervention choice vs. metric fidelity: Interventions must balance computational tractability with accurate representation of the original metric
  - Sheet complexity vs. interpretability: More detailed sheets provide more information but may be harder to interpret

- Failure signatures:
  - High variance across random seeds indicates sensitivity to initialization
  - Large ∆all_max values suggest fundamental multiplicity issues
  - Model selection fails to reduce unforeseen multiplicity, indicating over-parameterization

- First 3 experiments:
  1. Train 5 models with identical configurations but different random seeds, compute accuracy only to establish baseline multiplicity
  2. Train 3 models with different learning rates but same seed, compute fairness metric to see hyperparameter impact
  3. Train models with random seeds and learning rates, compute both accuracy and fairness to build a simple multiplicity sheet

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model multiplicity vary across different datasets and domains beyond image classification?
- Basis in paper: [inferred] The paper focuses on image classification using UTKFace and CIFAR10 datasets, suggesting the need to investigate multiplicity in other domains.
- Why unresolved: The study is limited to two image datasets and does not explore how multiplicity behaves in different domains like NLP, healthcare, or finance.
- What evidence would resolve it: Empirical studies measuring multiplicity across diverse datasets and domains using the proposed framework.

### Open Question 2
- Question: What is the impact of different sources of randomness (e.g., data shuffling, weight initialization) on model multiplicity?
- Basis in paper: [explicit] The paper mentions controlling randomness in model training with a single random seed but leaves decoupled analysis of various sources of randomness for future work.
- Why unresolved: The study does not separate and analyze the effects of different random factors on multiplicity.
- What evidence would resolve it: Experiments isolating and measuring the impact of different random sources on multiplicity.

### Open Question 3
- Question: How does model multiplicity change with different levels of model complexity and architecture choices?
- Basis in paper: [explicit] The paper studies multiplicity across different architectures (ResNet-18, ResNet-50, WideResNet-50x2) but does not provide a comprehensive analysis of how multiplicity scales with model complexity.
- Why unresolved: The study only compares a few architectures and does not explore the relationship between model complexity and multiplicity in depth.
- What evidence would resolve it: Systematic experiments measuring multiplicity across a wide range of architectures and complexity levels.

### Open Question 4
- Question: What are the most effective methods for mitigating model multiplicity beyond the checklist approach of trustworthy requirements?
- Basis in paper: [explicit] The paper shows that even after enforcing additional specifications using trustworthy metrics, multiplicity persists, highlighting the need for better safeguards.
- Why unresolved: The study only demonstrates the limitations of the checklist approach and does not explore alternative methods for addressing multiplicity.
- What evidence would resolve it: Research on novel techniques for reducing multiplicity that go beyond the current approaches of trustworthy requirements.

## Limitations

- The core assumption that interventions meaningfully preserve the original metric semantics remains untested, potentially making multiplicity comparisons unreliable if interventions distort metric meanings.
- The computational burden of the proposed framework is substantial, requiring training dozens of models with multiple random seeds and computing multiple metrics with adversarial attacks and perturbations.
- The focus on image classification tasks with specific datasets raises questions about generalizability to other domains where multiplicity manifestations may differ.

## Confidence

- High confidence: The existence of model multiplicity in deep learning is well-established; the paper's empirical demonstration that multiplicity persists across trustworthy metrics beyond standard accuracy is robust.
- Medium confidence: The framework for converting diverse metrics to a common accuracy scale is methodologically sound, but the fidelity of these translations across all metrics requires further validation.
- Low confidence: The claim that multiplicity is fundamentally rooted in over-parameterization lacks rigorous theoretical backing; alternative explanations may contribute.

## Next Checks

1. **Intervention Fidelity Test**: Select one trustworthy metric (e.g., fairness) and verify that accuracy under intervention correlates strongly with the original metric across a range of model behaviors, ensuring the conversion preserves metric semantics.
2. **Domain Transfer Experiment**: Apply the framework to a different ML domain (e.g., text classification or tabular data) to assess whether multiplicity patterns generalize beyond image classification tasks.
3. **Theoretical Analysis**: Develop mathematical conditions under which the accuracy under intervention framework guarantees metric preservation, particularly for non-differentiable or discontinuous metrics like privacy or security.