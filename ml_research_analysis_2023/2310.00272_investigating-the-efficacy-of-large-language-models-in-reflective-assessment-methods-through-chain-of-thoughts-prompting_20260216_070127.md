---
ver: rpa2
title: Investigating the Efficacy of Large Language Models in Reflective Assessment
  Methods through Chain of Thoughts Prompting
arxiv_id: '2310.00272'
source_url: https://arxiv.org/abs/2310.00272
tags:
- score
- language
- prompting
- feedback
- essays
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the use of Chain of Thought prompting with
  large language models for grading reflective essays of medical students. Four models
  (ChatGPT, Bard, Llama-7b, Llama-30b) were tested using two prompts with varying
  complexity.
---

# Investigating the Efficacy of Large Language Models in Reflective Assessment Methods through Chain of Thoughts Prompting

## Quick Facts
- arXiv ID: 2310.00272
- Source URL: https://arxiv.org/abs/2310.00272
- Reference count: 20
- Models: ChatGPT, Bard, Llama-7b, Llama-30b tested on medical student reflective essays

## Executive Summary
This study evaluates the effectiveness of Chain of Thought (CoT) prompting for large language models (LLMs) in grading reflective essays from third-year medical students. Four models of varying sizes were tested using two CoT prompts - one describing the scoring rubric and another including sample essays. ChatGPT emerged as the most effective model with a Cohen's kappa score of 0.53 and mean squared error of 0.35 when using the more detailed prompt. The findings demonstrate that CoT prompting can successfully guide LLMs to grade reflective essays and provide feedback, though model size and prompt design significantly impact performance. Smaller models like Llama-7b struggled particularly with longer texts.

## Method Summary
The study tested four LLMs (ChatGPT, Bard, Llama-7b, Llama-30b) on 17 reflective essays from third-year medical students, each scored by human markers on a 1-4 scale based on critical thinking. Two CoT prompts were developed - one describing the scoring rubric and another including example essays. The models were evaluated using Cohen's kappa score for agreement with human scores, mean squared error for prediction accuracy, and correlation analysis. The study also examined the quality of feedback provided by the models.

## Key Results
- ChatGPT achieved highest performance with Cohen's kappa of 0.53 and MSE of 0.35 using the detailed prompt
- Llama-7b showed poorest performance, particularly struggling with longer texts
- Larger parameter models (ChatGPT, Bard) generally outperformed smaller models (Llama-7b)
- CoT prompting effectively guided models to grade essays and provide feedback

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain of Thought prompting improves LLM reasoning for grading reflective essays by making reasoning steps explicit.
- Mechanism: CoT prompting provides intermediate reasoning steps, allowing the model to mimic human-like logical progression, which enhances accuracy especially for complex tasks like essay grading.
- Core assumption: LLMs can improve task performance when reasoning steps are made explicit, even for tasks they weren't explicitly trained on.
- Evidence anchors:
  - [abstract] "CoT prompting technique has been proposed as a means to enhance LLMs’ proficiency in complex reasoning tasks"
  - [section] "CoT prompting is only effective in enhancing performance when applied to models with approximately 100 billion parameters"
  - [corpus] No direct evidence in corpus about CoT for essay grading specifically
- Break condition: Models too small to benefit from CoT prompting (as shown with Llama-7b), or prompts that don't effectively capture the reasoning process.

### Mechanism 2
- Claim: Larger parameter models (ChatGPT, Bard) perform better than smaller models (Llama-7b) in essay grading tasks.
- Mechanism: Model size correlates with ability to handle complex reasoning and longer text inputs, leading to better performance in grading tasks.
- Core assumption: Parameter size directly correlates with reasoning capacity and ability to handle longer text sequences.
- Evidence anchors:
  - [abstract] "Conversely, ChatGPT emerges as the superior model, boasting a higher Cohen kappa score value of 0.53"
  - [section] "Our selection included Llama-7b with 7 billion parameters, along with larger Llama-30b with 30 billion parameters"
  - [corpus] No direct evidence in corpus about parameter size effects on grading
- Break condition: If prompt design is poor, even large models may underperform; if text is too long for any model regardless of size.

### Mechanism 3
- Claim: Including sample essays in prompts improves model performance by providing concrete examples of desired output.
- Mechanism: Exemplar essays guide the model's reasoning process by showing the expected format and quality levels, reducing ambiguity.
- Core assumption: LLMs can effectively learn from a small number of examples (few-shot learning) to generalize to new grading tasks.
- Evidence anchors:
  - [section] "For our study, we had a dataset comprised of reflective essays that were evaluated using a predefined scoring rubric"
  - [section] "we created a series of prompts utilising the CoT strategy to assess the model's proficiency with unfamiliar data"
  - [corpus] No direct evidence in corpus about few-shot learning for essay grading
- Break condition: If sample essays are too few or not representative of the grading rubric, or if model cannot effectively learn from examples.

## Foundational Learning

- Concept: Chain of Thought (CoT) prompting
  - Why needed here: Enables LLMs to perform complex reasoning tasks by breaking down the problem into intermediate steps, which is essential for essay grading
  - Quick check question: What is the main difference between standard prompting and CoT prompting for complex reasoning tasks?

- Concept: Cohen's Kappa score
  - Why needed here: Measures inter-rater agreement between human evaluators and model predictions, essential for evaluating grading performance
  - Quick check question: What does a Cohen's Kappa score of 0.53 indicate about model-human agreement in essay grading?

- Concept: Mean Squared Error (MSE)
  - Why needed here: Quantifies the average squared difference between predicted and actual scores, measuring grading accuracy
  - Quick check question: How does MSE help evaluate the performance of LLM grading models compared to human markers?

## Architecture Onboarding

- Component map: Essay text -> Prompt construction -> LLM inference -> Score prediction -> Evaluation metrics
- Critical path: Essay text → Prompt construction → LLM inference → Score prediction → Evaluation metrics
- Design tradeoffs: Larger models provide better performance but are more expensive to run; smaller models are cheaper but may struggle with complex reasoning
- Failure signatures: Llama-7b hanging on longer essays, models providing inconsistent feedback across different instances, high MSE values indicating poor alignment with human scores
- First 3 experiments:
  1. Test each model with the same basic CoT prompt on a small sample of essays to establish baseline performance
  2. Compare performance between the two prompt variants (with and without sample essays) for each model
  3. Evaluate model feedback quality by having human markers assess the relevance and usefulness of generated feedback

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of Chain of Thought (CoT) prompting affect the model's ability to handle longer reflective essays compared to traditional prompting methods?
- Basis in paper: [explicit] The paper discusses the use of CoT prompting to improve the performance of language models in grading reflective essays and mentions that larger models perform better with this approach.
- Why unresolved: The paper provides initial results but does not extensively explore the scalability of CoT prompting for longer texts or compare it directly with other prompting methods.
- What evidence would resolve it: A comparative study using different prompting techniques on essays of varying lengths to evaluate the consistency and effectiveness of CoT prompting.

### Open Question 2
- Question: What is the impact of model parameter size on the accuracy and feedback quality of grading reflective essays?
- Basis in paper: [explicit] The study compares models with different parameter sizes (Llama-7b, Llama-30b, ChatGPT, Bard) and observes variations in performance.
- Why unresolved: While the paper shows that larger models like ChatGPT perform better, it does not quantify the relationship between parameter size and grading accuracy or feedback quality.
- What evidence would resolve it: A detailed analysis correlating model parameter size with grading accuracy and feedback quality across a diverse set of essays.

### Open Question 3
- Question: How can large language models be adapted to assess reflective essays in indigenous languages effectively?
- Basis in paper: [inferred] The paper mentions the lack of support for indigenous languages in current models and the challenges this poses for users.
- Why unresolved: The paper highlights the issue but does not propose solutions or test models in indigenous languages.
- What evidence would resolve it: Development and testing of language models specifically trained on indigenous language datasets to assess their performance in grading reflective essays.

## Limitations

- Limited dataset of only 17 essays restricts generalizability of findings
- Moderate agreement (kappa of 0.53) suggests models are helpful but not yet reliable for fully automated grading
- Results may not transfer to other disciplines or essay types beyond medical student reflective essays

## Confidence

- **High confidence**: Chain of Thought prompting can guide LLMs to grade essays and provide feedback; larger models generally outperform smaller ones on this task
- **Medium confidence**: The specific CoT prompting techniques used in this study are effective; prompt design significantly impacts model performance
- **Low confidence**: These results generalize to other essay types, disciplines, or grading contexts; models can replace human graders

## Next Checks

1. **Scale validation**: Replicate the study with a larger dataset (minimum 100 essays) across multiple disciplines to test generalizability of the CoT prompting approach.

2. **Prompt optimization**: Systematically test variations of the CoT prompts (different numbers of examples, alternative rubric descriptions) to identify optimal prompt design for essay grading.

3. **Human equivalence testing**: Conduct a study comparing LLM grading consistency against multiple human graders over time, measuring whether models provide more consistent evaluations than humans who may experience fatigue or bias.