---
ver: rpa2
title: 'Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations'
arxiv_id: '2305.15035'
source_url: https://arxiv.org/abs/2305.15035
tags:
- zero-shot
- arxiv
- self-icl
- step
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-ICL, a simple framework for zero-shot
  in-context learning (ICL) using self-generated demonstrations. The authors propose
  that large language models (LLMs) have underestimated zero-shot capabilities, and
  demonstrations primarily serve to expose these intrinsic functionalities rather
  than teach new tasks.
---

# Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations

## Quick Facts
- arXiv ID: 2305.15035
- Source URL: https://arxiv.org/abs/2305.15035
- Reference count: 9
- Primary result: Self-ICL outperforms zero-shot baselines on 23 BIG-Bench Hard tasks

## Executive Summary
This paper introduces Self-ICL, a framework for zero-shot in-context learning using self-generated demonstrations. The key insight is that large language models have underestimated zero-shot capabilities, and demonstrations primarily serve to expose these intrinsic functionalities rather than teach new tasks explicitly. By generating pseudo-inputs and pseudo-labels through zero-shot prompting, Self-ICL constructs demonstrations that guide the model toward the target domain without requiring external data sources.

The evaluation demonstrates that Self-ICL achieves superior performance compared to standard zero-shot approaches and, with zero-shot chain-of-thought, matches the effectiveness of using real demonstrations. This suggests the possibility of bootstrapping LLMs' intrinsic capabilities to improve zero-shot performance across diverse tasks.

## Method Summary
Self-ICL operates by generating pseudo-inputs conditioned on a test input and task description, predicting their pseudo-labels via zero-shot prompting, and constructing pseudo-demonstrations for in-context learning. The method eliminates the need for external demonstration pools or pre-defined label sets by self-generating both inputs and labels through instruction-following capabilities. The framework is evaluated on 23 BIG-Bench Hard tasks using InstructGPT (text-davinci-003) via OpenAI API with temperature=0 and max_tokens=1024.

## Key Results
- Self-ICL outperforms zero-shot baselines on average accuracy across 23 BIG-Bench Hard tasks
- Head-to-head comparisons show Self-ICL superiority over zero-shot and zero-shot chain-of-thought baselines
- With zero-shot chain-of-thought, Self-ICL achieves results comparable to using real demonstrations
- The method demonstrates the possibility of bootstrapping LLMs' intrinsic capabilities for better zero-shot performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-ICL works by exposing the model's intrinsic capabilities through pseudo-demonstrations that inform the input and label space.
- Mechanism: The model generates pseudo-inputs conditioned on a test input and task description, predicts their labels via zero-shot prompting, and uses these pairs as demonstrations for ICL. This process exposes the model's internal representations of the task domain.
- Core assumption: Demonstrations primarily serve to expose models' intrinsic functionalities rather than teach new tasks explicitly.
- Evidence anchors:
  - [abstract]: "demonstrations primarily serve to expose models' intrinsic functionalities"
  - [section 1]: "Instead of contributing explicit signals for learning new tasks, demonstrations mainly expose large LMs' intrinsic functionalities and guide models towards target domains"
  - [corpus]: Weak evidence - the corpus neighbors discuss related ICL approaches but don't directly confirm the exposure mechanism claim
- Break condition: If the model lacks sufficient zero-shot generalization capability or the task description fails to constrain the pseudo-input generation appropriately.

### Mechanism 2
- Claim: Self-ICL mitigates the copying effect by generating diverse pseudo-inputs that are sufficiently different from the test input.
- Mechanism: The prompt instructs the model to generate "new, diverse, and creative instances," which empirically produces pseudo-inputs with a similarity gap to real inputs, reducing the likelihood of label copying.
- Core assumption: Diverse pseudo-inputs prevent the model from copying labels from semantically similar inputs.
- Evidence anchors:
  - [section 3.3]: "Most of the tasks falls inside the ±5% similarity range (dotted lines), suggesting our designed prompt is able to encourage the generation of diverse pseudo-inputs"
  - [abstract]: "increasing the pseudo-inputs' diversity is essential"
  - [corpus]: Weak evidence - corpus neighbors discuss ICL improvements but don't specifically address the copying effect mitigation
- Break condition: If the model's diversity generation capability is insufficient or the task domain inherently limits input diversity.

### Mechanism 3
- Claim: Self-ICL enables true zero-shot ICL without requiring external demonstration pools or pre-defined label sets.
- Mechanism: By self-generating both pseudo-inputs and pseudo-labels through zero-shot prompting, the framework eliminates dependency on external data sources while maintaining ICL effectiveness.
- Core assumption: Large LMs can generate meaningful pseudo-demonstrations without access to task-specific training data.
- Evidence anchors:
  - [abstract]: "To the best of our knowledge, we present the first attempt for true zero-shot ICL that does not require any external data from real distribution or pre-defined label sets"
  - [section 2.2]: "Without the requirement of candidate pool for demonstration selection, SELF-ICL bridges the gap for end-user's practical needs"
  - [corpus]: Weak evidence - corpus neighbors discuss ICL methods but most still require some form of external data or pre-defined labels
- Break condition: If the model cannot generate coherent pseudo-inputs or pseudo-labels for certain task types.

## Foundational Learning

- Concept: Zero-shot prompting
  - Why needed here: Self-ICL relies on zero-shot prompting to generate pseudo-inputs and predict pseudo-labels without task-specific demonstrations
  - Quick check question: Can the model follow instructions and perform the task when given only the task description and test input?

- Concept: In-context learning mechanics
  - Why needed here: Understanding how demonstrations influence model behavior is crucial for designing effective pseudo-demonstrations
  - Quick check question: Do demonstrations primarily expose model capabilities or provide explicit learning signals?

- Concept: Prompt engineering for diversity
  - Why needed here: The effectiveness of Self-ICL depends on generating diverse pseudo-inputs to avoid the copying effect
  - Quick check question: Does the prompt successfully encourage the generation of varied and creative instances?

## Architecture Onboarding

- Component map: Input processor -> Pseudo-input generator -> Zero-shot predictor -> ICL executor -> Output extractor
- Critical path: Test input → Pseudo-input generation → Zero-shot label prediction → Pseudo-demonstration construction → ICL execution → Final prediction
- Design tradeoffs:
  - Number of pseudo-inputs (k): More shots improve demonstration coverage but increase inference cost
  - Diversity vs. relevance: Highly diverse inputs may lose task relevance
  - Zero-shot vs. zero-shot CoT: CoT provides reasoning but risks exposing unfaithful explanations
- Failure signatures:
  - Poor pseudo-input diversity: Similarity gap approaches zero, indicating copying effect risk
  - Inconsistent pseudo-labels: Labels don't align with task semantics despite diverse inputs
  - Zero-shot prediction failure: Model cannot generate meaningful pseudo-inputs or labels
- First 3 experiments:
  1. Test pseudo-input generation diversity across different tasks and prompt variations
  2. Compare zero-shot vs. zero-shot CoT performance on a subset of tasks
  3. Measure similarity gap between pseudo-inputs and real inputs for different k values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Self-ICL scale with the number of pseudo-inputs generated (k) for different task types?
- Basis in paper: The paper mentions that k is specified in the instruction for generating pseudo-inputs, but does not explore the impact of varying k on performance across different task types.
- Why unresolved: The authors only mention k as a hyperparameter but do not provide empirical analysis of how different values of k affect performance across various task types.
- What evidence would resolve it: A systematic study varying k across multiple task types, measuring performance and computational efficiency trade-offs.

### Open Question 2
- Question: What is the exact mechanism by which pseudo-demonstrations improve zero-shot performance compared to standard zero-shot prompting?
- Basis in paper: The paper claims that demonstrations expose intrinsic functionalities of LLMs, but does not provide a detailed analysis of how this mechanism works at a technical level.
- Why unresolved: While the paper presents empirical evidence of Self-ICL's effectiveness, it does not explain the underlying reasons for this improvement in terms of the model's internal processes.
- What evidence would resolve it: An analysis of model activations, attention patterns, or other internal representations when using Self-ICL versus standard zero-shot prompting.

### Open Question 3
- Question: How does Self-ICL perform on non-multiple choice tasks compared to multiple choice tasks?
- Basis in paper: The paper explicitly states that Self-ICL can extend to other generation tasks, but only evaluates on multiple choice tasks from BBH.
- Why unresolved: The authors acknowledge the potential for extension to other task types but do not provide empirical evidence of Self-ICL's effectiveness on non-multiple choice tasks.
- What evidence would resolve it: Experiments evaluating Self-ICL on a diverse set of non-multiple choice tasks, such as text generation, summarization, or question answering.

## Limitations

- The core claim about demonstrations exposing rather than teaching capabilities lacks direct empirical validation
- The copying effect mitigation mechanism shows substantial variation across tasks, suggesting it may not be universally reliable
- The "true zero-shot" claim depends on strong instruction-following capabilities that may not generalize to all task domains

## Confidence

**High Confidence**: The empirical results showing Self-ICL outperforming zero-shot baselines on 23 BBH tasks. The methodology is sound, the evaluation is comprehensive, and the results are statistically significant.

**Medium Confidence**: The mechanism explanation that demonstrations expose intrinsic capabilities rather than teach new tasks. While the theoretical framework is compelling and consistent with observations, direct causal evidence is limited.

**Medium Confidence**: The copying effect mitigation through diversity generation. The similarity gap analysis provides supporting evidence, but the effectiveness varies significantly across tasks and the mechanism could fail in practice.

**Low Confidence**: The claim that Self-ICL achieves results "comparable to using real demonstrations" with zero-shot CoT. The comparison to real demonstrations is not directly tested, and the CoT explanations may be unfaithful rather than truly explanatory.

## Next Checks

1. **Direct Mechanism Validation**: Design an ablation study that tests whether the exposure mechanism is necessary by comparing Self-ICL performance when using randomly generated pseudo-inputs versus semantically meaningful ones. If performance drops significantly only with random inputs, this would support the exposure hypothesis.

2. **Diversity Robustness Analysis**: Systematically vary the prompt parameters for encouraging diversity and measure the resulting similarity gap and performance across different task types. Identify which task categories are most vulnerable to copying effects and under what prompt conditions.

3. **Cross-Model Generalization**: Test Self-ICL with multiple model architectures (different sizes, different training paradigms) to determine whether the effectiveness depends on specific model properties like instruction-following capability or general reasoning ability. This would clarify whether the method is broadly applicable or model-specific.