---
ver: rpa2
title: 'Fin-Fact: A Benchmark Dataset for Multimodal Financial Fact Checking and Explanation
  Generation'
arxiv_id: '2309.08793'
source_url: https://arxiv.org/abs/2309.08793
tags:
- financial
- dataset
- fin-fact
- fact-checking
- claims
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FIN-FACT, a new dataset for checking financial
  claims that include both text and images. The dataset contains 3,562 claims across
  multiple financial topics, each labeled by professional fact-checkers as True, False,
  or Not Enough Information, with expert justifications and image bias assessments.
---

# Fin-Fact: A Benchmark Dataset for Multimodal Financial Fact Checking and Explanation Generation

## Quick Facts
- arXiv ID: 2309.08793
- Source URL: https://arxiv.org/abs/2309.08793
- Reference count: 15
- Primary result: Dataset contains 3,562 financial claims labeled by professional fact-checkers as True, False, or Not Enough Information, with explanations and image bias assessments

## Executive Summary
This paper introduces FIN-FACT, a novel dataset for multimodal financial fact-checking that combines textual claims with associated images and expert annotations. The dataset contains 3,562 claims across various financial topics, each labeled by professional fact-checkers and accompanied by detailed justifications and image bias assessments. Experiments with state-of-the-art models including BART, RoBERTa, ELECTRA, and GPT-2 show that current approaches struggle with this task, achieving fact-checking accuracy between 29% and 43%, while explanation generation reaches GLUE scores of 0.062 and ROUGE scores around 0.84, 0.63, and 0.46 for R1, R2, and R3 respectively. These results highlight the need for better models specifically designed for financial fact-checking and demonstrate the value of expert annotations and multimodal information for improving factuality analysis.

## Method Summary
The Fin-Fact dataset was created by collecting 3,562 financial claims from various sources, each containing text and associated images. Professional fact-checkers annotated each claim as True, False, or Not Enough Information, providing detailed justifications and assessing visualization bias in the images. The dataset was then used to evaluate several state-of-the-art models including BART, RoBERTa, ELECTRA, and GPT-2 on both fact-checking and explanation generation tasks. Models were evaluated using accuracy metrics for classification and GLUE and ROUGE scores for explanation quality, with experiments conducted using a maximum token length of 256.

## Key Results
- Current models achieve fact-checking accuracy between 29% and 43% on the Fin-Fact dataset
- Explanation generation reaches GLUE score of 0.062 and ROUGE scores of R1 0.84, R2 0.63, and R3 0.46
- Expert annotations provide high-quality ground truth that distinguishes this dataset from others with noisier or synthetic data
- Multimodal structure combining text and images proves challenging for existing models, indicating need for specialized approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert annotations provide credibility and improve model performance
- Mechanism: Professional fact-checkers annotate claims with justifications and labels, enabling models to learn from high-quality ground truth rather than noisy or synthetic data
- Core assumption: Expert annotations are consistent and cover nuanced financial domain knowledge that general datasets miss
- Evidence anchors: [abstract] "Notably, it includes professional fact-checker annotations and justifications, providing expertise and credibility." [section] "Current financial misinformation datasets lack clear labeling and justifications, raising concerns about result reliability. In contrast, the FIN-FACT dataset is distinct with genuine data and a multimodal structure, combining text and images to encompass a wide range of financial information. Additionally, it includes expert fact-checker comments, enabling comprehensive explanations by models."
- Break condition: If expert annotations introduce significant inter-rater disagreement or are unavailable for new claim types

### Mechanism 2
- Claim: Multimodal inputs enhance factuality analysis beyond text alone
- Mechanism: Combining textual claims with associated images and captions allows models to detect visualization bias and contextual cues that may not be evident from text
- Core assumption: Images in financial claims are not purely decorative but contain meaningful information that influences perceived credibility
- Evidence anchors: [abstract] "With its multimodal nature encompassing both textual and visual content, FIN-FACT provides complementary information sources to enhance factuality analysis." [section] "The rise of misinformation in the financial domain has become a pressing concern... the manipulation of images to exploit visualization bias presents another significant challenge in the verification process."
- Break condition: If images are irrelevant or models cannot effectively integrate visual features with textual reasoning

### Mechanism 3
- Claim: Explanation generation builds trust by revealing model reasoning
- Mechanism: Models generate natural language explanations using expert justifications as ground truth, making fact-checking decisions interpretable
- Core assumption: Users trust models more when they can understand the reasoning behind decisions, not just the labels
- Evidence anchors: [abstract] "By offering insightful explanations, FIN-FACT empowers users, including domain experts and end-users, to understand the reasoning behind fact-checking decisions, validating claim credibility, and fostering trust in the fact-checking process." [section] "Explanation generation plays a pivotal role in facilitating human comprehension of claim credibility."
- Break condition: If generated explanations are too generic or misaligned with human-provided justifications

## Foundational Learning

- Concept: Multimodal representation learning
  - Why needed here: Financial claims combine text and images; models must learn joint representations to reason effectively
  - Quick check question: Can the model extract relevant features from both text and image modalities simultaneously?

- Concept: Natural Language Inference (NLI) for fact-checking
  - Why needed here: Fact-checking is framed as determining entailment between claims and evidence, requiring NLI capabilities
  - Quick check question: Does the model correctly classify claim-evidence pairs into True/False/Not Enough Information?

- Concept: Visualization bias detection
  - Why needed here: Financial misinformation often manipulates charts/images to mislead; models must recognize such biases
  - Quick check question: Can the model identify when an image is being used to distort the meaning of a claim?

## Architecture Onboarding

- Component map: Data loader → Multimodal feature extractor → NLI model → Explanation generator → Evaluation pipeline
- Critical path: Data ingestion → preprocessing (text/image) → multimodal fusion → classification → explanation generation → evaluation
- Design tradeoffs: Trade-off between model complexity and inference speed; choice of multimodal fusion strategy (early vs late vs hybrid); balancing text and image feature importance
- Failure signatures: Low precision on "False" claims suggests poor detection of misinformation; high NEI misclassification indicates difficulty with insufficient evidence; poor explanation quality suggests misalignment with ground truth justifications
- First 3 experiments:
  1. Baseline: Test BART, RoBERTa, ELECTRA, GPT-2 on text-only classification to establish performance ceiling without multimodal input
  2. Multimodal integration: Add image features via cross-attention or multimodal transformer to compare with text-only baseline
  3. Explanation generation: Fine-tune BART on justifications to generate explanations and evaluate using GLUE and ROUGE against expert-provided evidence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does visualization bias in images affect the accuracy of financial fact-checking models?
- Basis in paper: [explicit] The paper discusses the challenge of visualization bias and mentions the inclusion of a "Visualisation Bias Label" in the dataset
- Why unresolved: While the paper acknowledges the presence of visualization bias, it does not provide quantitative analysis on how this bias impacts model performance or the specific ways in which manipulated images influence claim interpretation
- What evidence would resolve it: Conducting experiments that compare model performance on claims with and without visualization bias, and analyzing the types of biases that most affect accuracy

### Open Question 2
- Question: What are the limitations of current multimodal models when applied to the Fin-Fact dataset?
- Basis in paper: [inferred] The paper states that current models struggle with the task, achieving low accuracy rates, but does not delve into specific limitations of multimodal models
- Why unresolved: The paper highlights the need for better models but does not specify the shortcomings of existing multimodal approaches in handling the unique challenges of financial fact-checking
- What evidence would resolve it: Detailed error analysis of multimodal models on the Fin-Fact dataset, identifying common failure modes and areas where these models underperform

### Open Question 3
- Question: How can the Fin-Fact dataset be extended to include more diverse financial topics and real-time data?
- Basis in paper: [explicit] The paper mentions the intention to extract quantitative insights from media and enhance the dataset with tables and charts, suggesting future expansion
- Why unresolved: The paper outlines plans for dataset improvement but does not specify how to incorporate a broader range of financial topics or integrate real-time data effectively
- What evidence would resolve it: Development of a framework for continuously updating the dataset with new claims, including a variety of financial topics and real-time market data, along with methodologies for ensuring data quality and relevance

## Limitations

- The relatively small dataset size (3,562 claims) may limit model generalizability and the ability to capture the full complexity of financial misinformation
- Current models struggle significantly with the task, achieving only 29-43% accuracy, suggesting either the dataset is particularly challenging or existing approaches are inadequate
- The paper does not address potential biases in the expert annotation process or provide inter-rater reliability analysis among professional fact-checkers

## Confidence

- High confidence: The dataset creation methodology and multimodal structure are well-documented and clearly defined
- Medium confidence: The evaluation metrics and reported baseline results are reproducible, though exact hyperparameter configurations are not fully specified
- Low confidence: The practical utility of the dataset for real-world financial fact-checking applications remains uncertain given the current model performance limitations

## Next Checks

1. Conduct inter-rater reliability analysis on expert annotations to quantify annotation consistency and identify potential sources of disagreement

2. Perform ablation studies comparing text-only versus multimodal model performance to quantify the actual contribution of visual information to fact-checking accuracy

3. Test model performance across different financial sub-domains (stock markets, cryptocurrencies, macro indicators) to identify which areas benefit most from multimodal approaches and where challenges persist