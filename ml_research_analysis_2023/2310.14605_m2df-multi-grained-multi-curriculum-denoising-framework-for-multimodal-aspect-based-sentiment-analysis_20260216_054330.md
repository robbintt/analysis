---
ver: rpa2
title: 'M2DF: Multi-grained Multi-curriculum Denoising Framework for Multimodal Aspect-based
  Sentiment Analysis'
arxiv_id: '2310.14605'
source_url: https://arxiv.org/abs/2310.14605
tags:
- m2df
- noise
- denoising
- image
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a denoising framework for multimodal aspect-based
  sentiment analysis. The framework addresses the issue of noisy images in multimodal
  datasets that can negatively impact model learning.
---

# M2DF: Multi-grained Multi-curriculum Denoising Framework for Multimodal Aspect-based Sentiment Analysis

## Quick Facts
- arXiv ID: 2310.14605
- Source URL: https://arxiv.org/abs/2310.14605
- Reference count: 27
- Key outcome: M2DF achieves up to 1.9% increase in F1-score on MASC task and 1.4% on JMASA task for MABSA by denoising multimodal data through curriculum learning

## Executive Summary
This paper addresses the problem of noisy images in multimodal aspect-based sentiment analysis (MABSA) datasets, which can negatively impact model performance. The authors propose M2DF, a Multi-grained Multi-curriculum Denoising Framework that evaluates noise levels at both sentence and aspect levels, then gradually exposes models to noisy data starting from clean data during training. The framework achieves significant improvements over state-of-the-art models on three MABSA subtasks, demonstrating that curriculum learning can effectively mitigate the impact of noisy images in multimodal learning.

## Method Summary
M2DF evaluates image noise through two metrics: a coarse-grained sentence-level similarity measure and a fine-grained aspect-level similarity measure that checks if aspect terms appear in images. These metrics are computed using CLIP embeddings and Mask-RCNN object detection. During training, instances are sorted by noise level and exposed to the model according to a curriculum that prioritizes clean data early in training. The framework dynamically selects between single curricula (one per metric) and multiple curricula based on validation performance, allowing the model to gradually learn from increasingly noisy data while maintaining performance on clean examples.

## Key Results
- M2DF achieves 1.9% increase in F1-score on MASC task compared to state-of-the-art models
- The framework improves JMASA task performance by 1.4% F1-score
- Fine-grained noise metric outperforms coarse-grained metric in most tasks, demonstrating the value of aspect-level denoising

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Noisy images harm model performance by introducing irrelevant visual information that confuses the learning process
- Mechanism: Images unrelated to text content create conflicting signals during training, leading the model to waste capacity on irrelevant visual features instead of learning the true text-image relationships
- Core assumption: The degree of image-text relevance directly correlates with the image's contribution to accurate sentiment prediction
- Evidence anchors:
  - [abstract] "most of the studies overestimate the importance of images since there are many noisy images unrelated to the text in the dataset, which will have a negative impact on model learning"
  - [section 1] "most studies overlooked the fact that there are many noisy images in the dataset that are unrelated to the text, which negatively impacts model learning"
  - [corpus] "Data Uncertainty-Aware Learning for Multimodal Aspect-based Sentiment Analysis" - indicates other researchers are also addressing noise in MABSA

### Mechanism 2
- Claim: Curriculum learning reduces noise impact by prioritizing clean data exposure during early training stages
- Mechanism: By ordering training instances from clean to noisy, the model first learns robust text-image correlations before being exposed to potentially misleading noisy examples
- Core assumption: Early training stages are more influential in shaping model parameters than later stages
- Evidence anchors:
  - [abstract] "CL can achieve denoising by adjusting the order of training data... encourages training more time on clean data and less time on noisy data"
  - [section 3.3.1] "the model no longer presents training data in completely random order during training, but trains more time on clean data and less time on noisy data"
  - [corpus] "Data Uncertainty-Aware Learning for Multimodal Aspect-based Sentiment Analysis" - suggests uncertainty-based curriculum approaches exist for this domain

### Mechanism 3
- Claim: Multi-grained noise metrics capture different aspects of image-text relevance, improving denoising effectiveness
- Mechanism: Sentence-level metrics detect general relevance while aspect-level metrics specifically check if aspect terms appear in images, creating complementary denoising signals
- Core assumption: Image-text relevance manifests differently at different granularities (sentence vs. aspect level)
- Evidence anchors:
  - [section 3.2.1] "the lower the similarity between the image and the text, the more likely the image is noise"
  - [section 3.2.2] "the lower the similarity between the aspect terms and the visual objects, the more likely the image is noise"
  - [section 5.2] "The fine-grained noise metric achieves better performance than the coarse-grained noise metric in most tasks"
  - [corpus] "Data Uncertainty-Aware Learning for Multimodal Aspect-based Sentiment Analysis" - suggests multiple approaches to measuring data quality

## Foundational Learning

- Concept: Curriculum Learning (Bengio et al., 2009)
  - Why needed here: Provides the theoretical foundation for ordering training data by difficulty/noise level to improve learning efficiency
  - Quick check question: How does presenting easier examples first help the model learn more effectively than random ordering?

- Concept: Multimodal representation learning
  - Why needed here: The framework relies on extracting meaningful representations from both text and images before measuring their similarity
  - Quick check question: What pre-trained models are used for extracting text and image features in this framework?

- Concept: Noise-aware training strategies
  - Why needed here: The framework specifically addresses the challenge of noisy labels or irrelevant data in multimodal datasets
  - Quick check question: What are the limitations of threshold-based filtering approaches compared to curriculum-based denoising?

## Architecture Onboarding

- Component map: Noise Metrics module -> Denoising Curriculums module -> Base MABSA model -> CLIP encoders
- Critical path:
  1. Compute noise metrics for all training instances
  2. Sort instances by noise level
  3. Select curriculum based on validation performance
  4. Sample batches according to current curriculum threshold
  5. Update model parameters
  6. Periodically evaluate and adjust curriculum selection

- Design tradeoffs:
  - Granularity vs. computational cost: Aspect-level metrics are more accurate but computationally heavier
  - Static vs. dynamic curriculum selection: Dynamic selection adapts to model performance but adds complexity
  - Single vs. multiple curriculum: Multiple curricula provide better denoising but require more sophisticated selection logic

- Failure signatures:
  - Curriculum selection oscillation between metrics
  - Poor performance on high-noise subsets despite overall improvement
  - Validation performance plateaus early in training
  - Model fails to converge or overfits to clean data

- First 3 experiments:
  1. Implement single curriculum with only coarse-grained metric on a simple baseline model
  2. Add fine-grained metric and compare performance improvements
  3. Implement dynamic curriculum selection and measure improvement over static selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What other noise metrics could be developed to measure the degree of noisy images in multimodal datasets beyond the coarse-grained and fine-grained metrics proposed in this work?
- Basis in paper: [inferred] The authors mention in the Limitations section that "there may be other methods to achieve this goal, which requires us to do further research and exploration."
- Why unresolved: The paper only explores two specific noise metrics and does not investigate other potential approaches for measuring noise levels in multimodal data.
- What evidence would resolve it: Experimental results comparing the performance of M2DF when using different noise metrics (e.g., based on visual saliency, image-text semantic alignment scores, or multimodal consistency measures) on the same MABSA tasks.

### Open Question 2
- Question: How would the M2DF framework perform when combined with other curriculum learning training schedules beyond the single and multiple denoising curricula explored in this work?
- Basis in paper: [explicit] The authors state in the Limitations section that they "have not examined other CL training schedules, such as self-paced learning (Kumar et al., 2010), which may be worth considering in the future."
- Why unresolved: The paper only evaluates two specific curriculum learning strategies and does not explore the potential benefits of other CL approaches.
- What evidence would resolve it: Comparative experiments showing the performance of M2DF when integrated with different CL training schedules (e.g., self-paced learning, difficulty-based curricula) on the same MABSA tasks.

### Open Question 3
- Question: What is the impact of noisy images on model performance at different noise levels, and how does the M2DF framework mitigate this impact across various noise intensities?
- Basis in paper: [inferred] The authors conduct an analysis dividing the test set into subsets with different noise levels, but they do not systematically investigate the relationship between noise intensity and model performance or provide a comprehensive evaluation of M2DF's effectiveness across a wide range of noise levels.
- Why unresolved: The paper provides limited insights into how noise levels affect model performance and does not thoroughly explore M2DF's ability to handle varying degrees of noise.
- What evidence would resolve it: Experiments that systematically vary the noise level in the training and test data, measuring the performance of both the baseline models and M2DF at each noise level, and analyzing the relative improvements achieved by M2DF across different noise intensities.

## Limitations

- The framework's effectiveness depends on the assumption that image-text similarity metrics can accurately identify noisy data, which may not hold for abstract or metaphorical content
- Evaluation is limited to only two Twitter datasets, potentially limiting generalizability to other multimodal domains
- The paper doesn't compare against simple threshold-based filtering approaches, making it difficult to assess the specific benefits of curriculum learning over simpler denoising methods

## Confidence

- **High Confidence**: The claim that noisy images negatively impact MABSA model performance is well-supported by the experimental results showing consistent improvements across all three subtasks (MATE, JMASA, and MASC) when using M2DF
- **Medium Confidence**: The effectiveness of multi-grained metrics over single-grained metrics is supported by the results, but the improvement margins are relatively small (around 0.5% F1-score difference between coarse and fine-grained metrics)
- **Low Confidence**: The claim that dynamic curriculum selection is superior to static selection lacks strong evidence, as the paper doesn't provide ablation studies comparing static versus dynamic curriculum approaches

## Next Checks

1. **Dataset Noise Analysis**: Conduct a human annotation study to empirically verify the proportion and types of noisy images in the TWITTER-15 and TWITTER-17 datasets, comparing the noise detection accuracy of the proposed metrics against human judgments

2. **Baseline Comparison**: Implement and compare M2DF against a simple threshold-based filtering approach that removes instances below a fixed similarity threshold, to quantify the specific benefits of the curriculum learning approach

3. **Generalization Test**: Evaluate M2DF on additional multimodal datasets beyond Twitter, such as product review datasets with images or multimodal sentiment analysis datasets, to assess the framework's generalizability across different domains and noise patterns