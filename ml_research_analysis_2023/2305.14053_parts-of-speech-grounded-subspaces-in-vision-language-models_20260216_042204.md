---
ver: rpa2
title: Parts of Speech-Grounded Subspaces in Vision-Language Models
arxiv_id: '2305.14053'
source_url: https://arxiv.org/abs/2305.14053
tags:
- visual
- clip
- space
- subspace
- subspaces
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using parts of speech to disentangle visual
  modes in CLIP embeddings. By learning linear subspaces that capture variance in
  specific PoS categories while minimizing variance in others, the method isolates
  visual content from appearance.
---

# Parts of Speech-Grounded Subspaces in Vision-Language Models

## Quick Facts
- arXiv ID: 2305.14053
- Source URL: https://arxiv.org/abs/2305.14053
- Authors: 
- Reference count: 40
- Primary result: Parts of speech-grounded subspaces in CLIP disentangle visual modes, enabling controllable text-to-image synthesis and targeted removal of visual themes without fine-tuning

## Executive Summary
This paper proposes using parts of speech to disentangle visual modes in CLIP embeddings. By learning linear subspaces that capture variance in specific PoS categories while minimizing variance in others, the method isolates visual content from appearance. The objective is solved in closed form via eigenvalue decomposition, with a manifold-aware variant respecting the hyperspherical geometry of CLIP embeddings. Qualitative results show the subspaces can visually separate artists' styles from their likenesses in text-to-image models, and selectively remove visual themes like gore or imitation of artists. Class invariance metrics and improved zero-shot classification on 14/15 datasets demonstrate the effectiveness of the noun subspace at isolating image content.

## Method Summary
The method learns subspaces in CLIP's vision-language space to disentangle visual modes of variation using parts of speech as supervision. It constructs matrices for each PoS category using CLIP embeddings of WordNet words, then solves a trace maximization problem that preserves variance in the target PoS while minimizing variance in others via eigenvalue decomposition. A manifold-aware variant operates in the tangent space at the intrinsic mean to respect the hyperspherical geometry. The resulting subspaces can isolate visual concepts like artist styles or gore themes, and projecting onto orthogonal complements removes these themes from text-to-image generation. The approach is evaluated using class invariance metrics, zero-shot classification accuracy, and qualitative text-to-image results.

## Key Results
- Parts of speech-grounded subspaces successfully disentangle visual modes in CLIP embeddings
- Noun subspace improves zero-shot classification on 14/15 datasets compared to vanilla CLIP
- Subspaces enable selective removal of visual themes like artist styles and gore from text-to-image models
- Manifold-aware variant respects hyperspherical geometry while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method isolates visual content from appearance by learning subspaces aligned with parts of speech.
- Mechanism: By constructing a matrix for each part of speech (e.g., nouns, adjectives) using CLIP embeddings of WordNet words, then optimizing a trace maximization problem that preserves variance in the target PoS while minimizing variance in others, the resulting eigenvectors span a subspace capturing only that visual mode.
- Core assumption: CLIP embeddings of words preserve semantic associations such that nouns map to objects and adjectives to visual styles.
- Evidence anchors:
  - [abstract]: "learns subspaces capturing variability corresponding to a specific part of speech, while jointly minimising variability to the rest"
  - [section 2.1.1]: Closed-form solution via leading eigenvectors of the constructed matrix
  - [corpus]: Weak or missing – no direct neighbor papers discuss PoS-grounded subspaces in CLIP
- Break condition: If CLIP embeddings do not consistently encode PoS semantics, the subspaces will fail to isolate modes.

### Mechanism 2
- Claim: The method respects the hyperspherical geometry of CLIP embeddings by operating in the tangent space at the intrinsic mean.
- Mechanism: CLIP embeddings lie on a hypersphere; projecting directly in Euclidean space can leave the sphere. Instead, the method maps embeddings to the tangent space via the Logarithmic Map, performs the subspace analysis there, then maps back via the Exponential Map.
- Core assumption: The manifold structure can be locally approximated by the tangent space at the intrinsic mean.
- Evidence anchors:
  - [abstract]: "respecting the underlying geometry of the manifold on which the representations lie"
  - [section 2.2]: "manifold generalisation of the subspaces [...] that share the property of capturing the variance of only the desired visual attributes, yet better respect the manifold"
  - [corpus]: Weak or missing – no neighbor papers address manifold-aware subspace learning for CLIP
- Break condition: If the manifold approximation is poor (e.g., high curvature), projections may not stay on the sphere.

### Mechanism 3
- Claim: The method enables selective removal of visual themes in text-to-image models by projecting onto orthogonal complements of learned subspaces.
- Mechanism: Once a subspace captures a visual theme (e.g., artist style), projecting a CLIP embedding onto its orthogonal complement removes that theme from the embedding, so the text-to-image model generates without that style.
- Core assumption: CLIP-based text-to-image models are sensitive to CLIP embedding content; removing a theme from the embedding prevents its manifestation in generated images.
- Evidence anchors:
  - [abstract]: "enables the selective removal of entire visual themes from CLIP-based text-to-image synthesis"
  - [section 3.1.2]: "simply modifies the TTIM forward pass to first project the CLIP text representations onto the orthogonal complement of the adjective subspace"
  - [corpus]: Weak or missing – no neighbor papers discuss theme removal via CLIP embedding manipulation
- Break condition: If the text-to-image model uses additional conditioning beyond CLIP embeddings, theme removal may be incomplete.

## Foundational Learning

- Concept: Linear algebra – eigenvectors and eigenvalues
  - Why needed here: The closed-form solution requires finding eigenvectors of a constructed matrix.
  - Quick check question: What do the leading eigenvectors of a symmetric matrix represent in PCA?

- Concept: Manifold geometry – tangent spaces and exponential/logarithmic maps
  - Why needed here: The method must respect the hyperspherical structure of CLIP embeddings.
  - Quick check question: How does the

## Architecture Onboarding

- Component map: WordNet words -> CLIP embeddings -> Subspace matrices -> Eigenvalue decomposition -> Subspaces
- Critical path: Construct subspace matrix Ci -> Solve eigenvalue problem -> Extract leading eigenvectors -> Form subspace W
- Design tradeoffs: Closed-form solution vs. iterative optimization; Euclidean vs. manifold-aware variant; positive variance vs. negative variance guidance
- Failure signatures: Poor disentanglement if λ is too close to 0 or 1.0; low classification accuracy if k dimension is too small
- First experiments: 1) Compute noun subspace and evaluate class invariance; 2) Apply adjective subspace to remove artist style from text-to-image generation; 3) Compare Euclidean vs. manifold-aware variants

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Effectiveness relies on CLIP embeddings preserving PoS-semantics associations, which may not hold across all domains or languages
- Manifold-aware variant assumes local linearity of hyperspherical embedding space, which could break down for embeddings far from intrinsic mean
- Method depends on having sufficient representative words for each PoS category - WordNet coverage may be incomplete for rare visual concepts

## Confidence
High confidence in: Closed-form solution for subspace learning via eigenvalue decomposition, mathematical framework for combining positive and negative variance guidance, basic approach of using orthogonal projection for theme removal.

Medium confidence in: Manifold-aware variant's practical benefits over Euclidean baseline, robustness to CLIP embedding variations across different model versions.

## Next Checks
1. Test method's performance on non-English CLIP embeddings to verify PoS-semantics preservation across languages.

2. Conduct ablation studies varying dimensionality k of subspaces and balance parameter λ to establish optimal ranges for different visual tasks.

3. Evaluate method's effectiveness when applied to CLIP embeddings from different model checkpoints (e.g., CLIP-ViT-B-32 vs CLIP-ViT-B-16) to assess robustness to embedding variations.