---
ver: rpa2
title: A density estimation perspective on learning from pairwise human preferences
arxiv_id: '2311.14115'
source_url: https://arxiv.org/abs/2311.14115
tags:
- annotator
- preferences
- preference
- pairwise
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reframes learning from pairwise human preferences as
  a density estimation problem rather than reinforcement learning. The key insight
  is that under certain generative process assumptions (Luce choice rule and its generalizations
  via preference behavior distribution equations), training a reward function on pairwise
  preferences is equivalent to estimating the annotator's implicit preference distribution.
---

# A density estimation perspective on learning from pairwise human preferences

## Quick Facts
- arXiv ID: 2311.14115
- Source URL: https://arxiv.org/abs/2311.14115
- Reference count: 20
- Primary result: Learning from pairwise preferences is reframed as density estimation rather than reinforcement learning

## Executive Summary
This paper presents a novel perspective on learning from human preferences by reframing the problem as density estimation rather than reinforcement learning. Under specific generative process assumptions (Luce choice rule and its generalizations), training a reward function on pairwise preferences is mathematically equivalent to estimating the annotator's implicit preference distribution. The authors demonstrate this both theoretically and empirically using synthetic experiments, showing that the optimal reward function recovers the annotator's preference distribution. They also identify "annotator misspecification" as a critical failure mode when the model's generative process assumptions don't match the annotator's true behavior, leading to poorly-adapted models.

## Method Summary
The method treats pairwise preference learning as density estimation by assuming a generative process for how preferences are generated. The Luce choice rule states that the probability of preferring one option over another is proportional to their implicit preference distribution. Under this assumption, training a reward function to predict pairwise preferences is equivalent to estimating the log-preference distribution. The authors generalize this to a family of preference behavior distribution equations (PBDEs) that can capture different generative processes. Optimization uses binary cross-entropy loss on predicted preference probabilities, with the theoretical guarantee that the global optimum recovers the true preference distribution when assumptions are met.

## Key Results
- Pairwise preference learning under Luce choice rule is mathematically equivalent to density estimation
- The optimal reward function recovers the annotator's implicit preference distribution
- Annotator misspecification occurs when model assumptions don't match true annotator behavior
- Length bias in autoregressive models violates Luce choice rule assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training a reward function on pairwise preferences is equivalent to density estimation when the generative process follows the Luce choice rule.
- Mechanism: The Luce choice rule assumes that preference probability between two options is proportional to their implicit preference distribution. This creates a Bradley-Terry model parameterization where the reward function's logits directly estimate the log-preference distribution.
- Core assumption: Annotators behave according to the Luce choice rule (Equation 9 in the paper).
- Evidence anchors:
  - [abstract] "We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution."
  - [section 4.1] "Theorem 1. Let p∗(x) be a probability distribution with support S... The loss function is globally minimized when erϕ(x)∝p∗(x), ∀x∈S."
  - [corpus] Weak evidence - related papers discuss preference heterogeneity but don't directly address Luce rule assumptions.
- Break condition: The assumption fails when annotator behavior deviates from Luce choice rule, such as when preferences depend on factors beyond the implicit preference distribution.

### Mechanism 2
- Claim: Different generative process assumptions lead to different optimal models, even with the same observed preference distribution.
- Mechanism: The preference behavior distribution equation (PBDE) family generalizes the Luce choice rule. Different PBDE parameterizations (f(x) and g(x)) create different relationships between the observed preferences and the optimal model.
- Core assumption: The same PBDE governs both annotator behavior and model parameterization.
- Evidence anchors:
  - [section 4.3] "Theorem 2. Let the generative processes for pairwise preferences for the annotator and the model be governed by the same PBDE... Then the global minimizer... is πθ(x) = p∗(x), ∀x."
  - [section 4.3] "We now show how Direct Preference Optimization (DPO; Rafailov et al., 2023) can be recast as an instance of this general density estimation procedure."
  - [corpus] Moderate evidence - papers on DPO and preference optimization discuss different parameterizations but not the PBDE framework.
- Break condition: Mismatch between annotator's PBDE and model's PBDE leads to annotator misspecification, producing models that don't capture the true preference distribution.

### Mechanism 3
- Claim: Annotator misspecification occurs when multiple annotators with different preference distributions are treated as a single annotator.
- Mechanism: When preferences are aggregated across annotators with different implicit distributions, the resulting mixture cannot be captured by a single-annotator model. This creates regions of low probability density in the learned model.
- Core assumption: Multiple annotators contribute to the preference data without identity information.
- Evidence anchors:
  - [section 5.1] "If we adapt πpre(x) on comparison outcomes sampled from Equation 25 using a Luce choice rule generative process for the model, we are in an annotator misspecification setting and the adapted model... fails to capture regions of high density under either of the two annotators' PDFs."
  - [section 5.2] "The first observation we make is that the Luce choice rule is not a realistic generative process for pairwise preferences in the case of autoregressive models applied to language modeling tasks due to 'length bias' in likelihood values."
  - [corpus] Strong evidence - papers on preference heterogeneity directly address this issue with solutions like ternary preferences.
- Break condition: Annotator identity information is available, allowing separate modeling of different annotators, or the model is adapted using a multi-annotator generative process.

## Foundational Learning

- Concept: Probability density estimation
  - Why needed here: The core insight is that learning from pairwise preferences is fundamentally a density estimation problem where the goal is to recover the annotator's implicit preference distribution.
  - Quick check question: If an annotator prefers A over B with probability p(A)/(p(A)+p(B)), what distribution are we trying to estimate from pairwise comparisons?

- Concept: Luce choice rule and Bradley-Terry model
  - Why needed here: These models provide the generative process assumption that links pairwise preferences to an underlying preference distribution.
  - Quick check question: What functional form does the Luce choice rule give for the probability of preferring one option over another?

- Concept: KL-divergence and maximum likelihood
  - Why needed here: The optimization objective is minimizing KL-divergence between the model's predicted preferences and the true annotator preferences, which is equivalent to maximum likelihood estimation.
  - Quick check question: Why does minimizing KL-divergence between predicted and true preference distributions lead to the same result as maximum likelihood estimation?

## Architecture Onboarding

- Component map:
  Preference data acquisition -> Pairwise comparison pairs with labels
  Generative process model -> Luce choice rule or PBDE parameterization
  Reward function -> Neural network estimating log-preference distribution
  Optimization -> Binary cross-entropy loss on preference probabilities
  Model adaptation -> Direct policy optimization or RL

- Critical path:
  1. Collect pairwise preference data (xA, xB, y)
  2. Define generative process assumption (Luce choice rule or PBDE)
  3. Train reward function to predict preference probabilities
  4. Extract implicit preference distribution from trained reward
  5. Use distribution as policy or for further RL optimization

- Design tradeoffs:
  - Single-annotator vs. multi-annotator generative processes
  - Length-normalized vs. raw likelihoods for autoregressive models
  - Direct optimization vs. reward modeling followed by RL
  - Binary cross-entropy vs. alternative losses (hinge, ranking)

- Failure signatures:
  - Model produces outputs in low-probability regions of true distribution
  - Sensitivity to temperature parameter in temperature-smoothed distributions
  - Length bias in autoregressive models favoring shorter sequences
  - Inconsistent preferences across different annotator groups

- First 3 experiments:
  1. Synthetic 1D experiment: Train reward function on pairwise preferences from known distribution, verify recovery of implicit distribution
  2. Multi-annotator toy: Create two synthetic annotators with different preferences, train single-annotator model, observe failure modes
  3. Length bias experiment: Train autoregressive model on pairwise preferences, compare sequence length distributions before and after adaptation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of PBDE (Preference Behavior Distribution Equation) affect the properties of the learned reward function and the final aligned model?
- Basis in paper: [explicit] The paper introduces a family of PBDEs and shows how different choices (e.g., Luce choice rule vs. length-normalized Luce choice rule) lead to different globally optimal models under the same annotator behavior.
- Why unresolved: While the paper demonstrates the theoretical relationship, it doesn't explore the practical implications of choosing different PBDEs for specific alignment tasks or how to select the most appropriate one.
- What evidence would resolve it: Empirical studies comparing alignment quality and failure modes across different PBDE choices on real-world preference datasets.

### Open Question 2
- Question: What is the minimum amount of preference data required to effectively recover the annotator's implicit preference distribution, especially in high-dimensional settings like language modeling?
- Basis in paper: [inferred] The paper notes that practical applications operate in low-data regimes and the number of possible sequences is much larger than the number of synthetic annotator queries, yet approaches still work reasonably well.
- Why unresolved: The theoretical results hold in the infinite data limit, but the practical data efficiency is not well understood.
- What evidence would resolve it: Empirical studies measuring alignment quality as a function of preference dataset size across different model architectures and task complexities.

### Open Question 3
- Question: How can we effectively address annotator misspecification when we don't have access to annotator identities?
- Basis in paper: [explicit] The paper demonstrates that annotator misspecification leads to badly-tuned policies and shows a solution when annotator IDs are available, but notes this is an open problem without IDs.
- Why unresolved: The proposed solution using mixture models with Gaussian noise requires annotator IDs which are typically not available in practice.
- What evidence would resolve it: Development and validation of methods that can automatically detect and account for multiple annotator preferences without explicit ID information.

## Limitations

- The density estimation framework assumes the Luce choice rule, which may not hold for real-world preferences due to cognitive biases and context effects
- Length bias in autoregressive models violates the basic Luce choice rule assumption, requiring additional normalization
- Annotator misspecification is identified as a failure mode but lacks comprehensive empirical validation on real-world datasets

## Confidence

**High confidence**: The theoretical connection between pairwise preference learning and density estimation under Luce choice rule assumptions (Theorem 1). The mathematical derivation is rigorous and the synthetic experiments convincingly demonstrate the mechanism.

**Medium confidence**: The broader PBDE framework and its ability to capture various preference optimization methods including DPO. While the theory is sound, the practical implications and empirical validation across diverse scenarios are less established.

**Low confidence**: The extent and practical significance of annotator misspecification in real-world preference datasets. The paper identifies this as a failure mode but doesn't provide comprehensive empirical evidence about its prevalence or impact on deployed systems.

## Next Checks

1. **Empirical prevalence study**: Analyze real-world preference datasets (e.g., from RLHF datasets) to quantify the degree of preference heterogeneity and identify evidence of annotator misspecification. This would involve clustering preferences by latent annotator characteristics and measuring distributional differences.

2. **Extended generative process validation**: Test the PBDE framework with alternative generative processes beyond Luce choice rule, including models that incorporate context effects, cognitive biases, or multi-option preferences. Validate whether the density estimation perspective still holds under these more complex assumptions.

3. **Length bias mitigation evaluation**: Implement and thoroughly evaluate the length-normalized Luce choice rule on autoregressive language models across multiple tasks. Compare against baseline approaches and measure the impact on sequence length distributions and overall preference satisfaction.