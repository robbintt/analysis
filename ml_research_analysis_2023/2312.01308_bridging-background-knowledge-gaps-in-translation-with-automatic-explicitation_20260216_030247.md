---
ver: rpa2
title: Bridging Background Knowledge Gaps in Translation with Automatic Explicitation
arxiv_id: '2312.01308'
source_url: https://arxiv.org/abs/2312.01308
tags:
- explicitation
- language
- translation
- source
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces automatic techniques for generating "explicitation"
  - adding explanatory context to translations when cultural or background knowledge
  gaps exist between source and target audiences. The authors collect a dataset from
  Wikipedia annotations showing when translators add context (e.g., "the former French
  Prime Minister Dominique de Villepin" vs just "Dominique de Villepin").
---

# Bridging Background Knowledge Gaps in Translation with Automatic Explicitation

## Quick Facts
- arXiv ID: 2312.01308
- Source URL: https://arxiv.org/abs/2312.01308
- Reference count: 35
- Primary result: Automatic generation of explanatory context (explicitation) improves English QA accuracy by 11-48% compared to Polish/Spanish QA when translating culturally specific entities.

## Executive Summary
This paper introduces automatic techniques for generating "explicitation" - adding explanatory context to translations when cultural or background knowledge gaps exist between source and target audiences. The authors collect a dataset from Wikipedia annotations showing when translators add context (e.g., "the former French Prime Minister Dominique de Villepin" vs just "Dominique de Villepin"). They then develop methods to automatically detect when entities need additional explanation and generate appropriate context using structured data from Wikidata. The system is evaluated both intrinsically (human judges rate the quality of generated explicitation) and extrinsically using multilingual question answering, where the explicitation improves English QA accuracy by 11-48% compared to Polish/Spanish QA.

## Method Summary
The system detects potential explicitation candidates by finding unaligned segments near named entities in parallel text using word alignment tools (SimAlign and awesome-align). It then decides if explicitation is needed using heuristics based on entity's proximity to source language country in Wikidata knowledge graph, Wikipedia page popularity metrics, and global recognition. The system generates explicitation by fetching relevant information from Wikidata/Wikipedia (SHORT: 1-2 words, MID: phrase to short sentence, LONG: 1-3 sentences) and integrates it into English translations. Evaluation uses both human annotation (translator ratings on validity/quality) and QA performance comparison between source and target languages using LLaMA model with Expected Wins and Full Input Accuracy metrics.

## Key Results
- Automatic explicitation generation achieves 0.71 human evaluation score for decision quality and 95% appropriateness rating for LONG type explanations
- Explicitation improves English QA accuracy by 11-48% compared to Polish/Spanish QA, with greater improvement indicating more effective bridging of knowledge gaps
- The system correctly identifies culturally bound entities, with explicitation being more effective in target language QA tasks than source language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wikidata-based entity linking enables automatic detection of when explicitation is needed by measuring cultural specificity.
- Mechanism: The system calculates the distance between an entity and language-specific countries in the knowledge graph, combined with Wikipedia page statistics (length and incoming links), to determine if an entity is tightly bound to the source language community.
- Core assumption: Entities with fewer links to target language countries and more links to source language countries need explicitation for target audiences.
- Evidence anchors:
  - [section] "Given a relational knowledge base (KB) graph, this can be implemented as the number of hops from an entity to source and target language-speaking countries."
  - [section] "We implement each of these properties and measure whether the property values prop of the given entity e conditioned on each language l of bitext pairs, and check if the shifts from the source language lsrc to target language ltgt are above the threshold τ"
  - [corpus] Weak - the paper mentions using Wikidata and Wikipedia but doesn't provide specific corpus evidence for the effectiveness of this approach
- Break condition: If the knowledge graph is incomplete or entities have mixed cultural associations, the distance metric may incorrectly identify explicitation needs.

### Mechanism 2
- Claim: Human evaluation provides valid intrinsic assessment of automatic explicitation quality.
- Mechanism: Translators rate both the decision to explicitation and the quality of generated content using a three-point Likert scale, anchored by clear endpoints (e.g., "not necessary/wrong explanation" to "appropriate and well-generated/necessary").
- Core assumption: Human translators can reliably distinguish between necessary and unnecessary explicitation, and judge the quality of generated explanations.
- Evidence anchors:
  - [section] "Annotators score the explicitation decision 0.71 (Table 5). Nega-tive examples (boldface as additional explanation added by explicitation) include too obvious ones..."
  - [section] "The quality of generation is assessed highest on LONG type where the annotator evaluates about 95% of the generated explanations are appropriate and useful."
  - [corpus] Moderate - the paper reports kappa agreement of ~0.7 among annotators, indicating reliable but not perfect agreement
- Break condition: If annotators have inconsistent interpretations of what constitutes necessary explicitation, or if cultural differences affect judgment, the evaluation may be unreliable.

### Mechanism 3
- Claim: Multilingual QA provides extrinsic evaluation of explicitation usefulness by measuring performance differences across languages.
- Mechanism: The system generates explicitation for both source and target languages, then measures QA accuracy improvements in each language using expected wins metrics. Greater improvement in target language indicates more effective explicitation.
- Core assumption: QA performance reflects user understanding, so improvements in target language QA demonstrate the explicitation helped bridge the knowledge gap.
- Evidence anchors:
  - [section] "We hypothesize that a well-generated explicitation will increase the accuracy of the target language QA task while be-ing relatively less effective in the source language"
  - [section] "Explicitation is more effective in English QA tasks than Polish on all metrics in XQB-pl (Figure 3a), which indicates that our decision algorithm effectively selects entities that need explicitation"
  - [corpus] Moderate - the paper shows 11-48% improvement in English QA accuracy, but doesn't provide extensive corpus evidence for this approach
- Break condition: If the QA system already has knowledge about the explicitation content, or if questions are too easy/difficult regardless of explicitation, the metric may not accurately reflect usefulness.

## Foundational Learning

- Concept: Pragmatic explicitation in translation studies
  - Why needed here: The paper builds on translation theory to define when and how to add explanatory context, distinguishing it from other types of explicitation
  - Quick check question: What distinguishes pragmatic explicitation from obligatory or optional explicitation according to Klaudy's typology?

- Concept: Knowledge graph entity linking and distance metrics
  - Why needed here: The system uses Wikidata to measure cultural specificity of entities by calculating distances to language-specific countries
  - Quick check question: How does the system use the number of hops between an entity and language-specific countries to determine explicitation needs?

- Concept: Question answering evaluation metrics (Expected Wins, Full Input Accuracy)
  - Why needed here: The extrinsic evaluation uses these metrics to measure how much explicitation improves QA performance in different languages
  - Quick check question: What's the difference between Expected Wins and Full Input Accuracy in measuring QA system performance?

## Architecture Onboarding

- Component map:
  Input: Parallel text (WikiMatrix) → Entity recognition (WikiNEuRal) → Unaligned segment detection (SimAlign/awesome-align) → Explicitation decision (Wikidata + Wikipedia statistics) → Explicitation generation (Wikidata/Wikipedia API) → Output: Enhanced translations
  Evaluation: Human evaluation (translator ratings) + Extrinsic evaluation (multilingual QA with LLaMA)

- Critical path: Entity detection → Explicitation decision → Generation → Evaluation
- Design tradeoffs: Structured data generation ensures precision but limits flexibility; Wikidata-based decisions are consistent but may miss nuanced cultural context
- Failure signatures: Poor entity recognition leading to missed explicitation opportunities; incorrect distance calculations causing false positives/negatives; QA system not benefiting from explicitation due to knowledge overlap
- First 3 experiments:
  1. Test entity detection and distance calculation on a small set of known entities to verify the decision algorithm works
  2. Generate explicitation for a few examples and have translators rate quality to validate the generation approach
  3. Run the full pipeline on a small parallel corpus and measure QA performance changes to test the extrinsic evaluation approach

## Open Questions the Paper Calls Out
- What is the optimal threshold for deciding when an entity needs explicitation across different languages and cultures?
- How does the effectiveness of explicitation vary based on the type of additional information (hypernym, nationality, occupation, etc.)?
- What is the impact of explicitation on speech-to-speech simultaneous interpretation?
- How can explicitation generation be improved to better match the knowledge level of target audiences?

## Limitations
- The distance-based decision algorithm may oversimplify cultural specificity and produce false positives/negatives for entities with mixed cultural associations
- Human evaluation may reflect subjective cultural biases rather than objective knowledge gaps, despite reasonable inter-annotator agreement
- Extrinsic QA evaluation assumes performance differences directly reflect user understanding, but QA systems may already encode knowledge about common explicitation content

## Confidence
- **High Confidence**: The basic approach of using Wikidata for structured data retrieval and Wikipedia for contextual information is sound and technically feasible
- **Medium Confidence**: The distance-based decision algorithm for identifying explicitation needs shows promise but may produce false positives/negatives in edge cases
- **Medium Confidence**: Human evaluation methodology is appropriate but cultural subjectivity in ratings remains a concern
- **Medium Confidence**: QA-based extrinsic evaluation provides useful signals but may conflate system knowledge with actual knowledge gap bridging

## Next Checks
1. Test the decision algorithm on a curated set of entities with known cultural specificity (e.g., regional politicians, local landmarks) to measure precision/recall of explicitation detection
2. Conduct cross-cultural human evaluation with bilingual annotators from both source and target language communities to identify systematic judgment differences
3. Create a controlled QA evaluation where some questions require the explicitation content to answer correctly, isolating the knowledge gap bridging effect from general QA performance