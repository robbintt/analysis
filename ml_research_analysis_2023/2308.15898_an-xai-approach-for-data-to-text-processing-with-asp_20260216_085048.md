---
ver: rpa2
title: An xAI Approach for Data-to-Text Processing with ASP
arxiv_id: '2308.15898'
source_url: https://arxiv.org/abs/2308.15898
tags:
- zones
- descriptions
- description
- verbosity
- details
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an explainable AI approach for converting time
  series data into natural language text descriptions. The core method uses Answer
  Set Programming (ASP) to model and optimize the selection of key descriptive elements
  from data, balancing accuracy and level of detail.
---

# An xAI Approach for Data-to-Text Processing with ASP

## Quick Facts
- arXiv ID: 2308.15898
- Source URL: https://arxiv.org/abs/2308.15898
- Reference count: 20
- Primary result: Explainable AI system generates fluent text summaries from time series data using Answer Set Programming for controlled accuracy and detail

## Executive Summary
This paper presents an explainable AI approach for converting time series data into natural language descriptions using Answer Set Programming (ASP). The system fits data with function prototypes, optimizes descriptor selection at multiple verbosity levels, and structures the output as a hierarchical narrative. By explicitly encoding rules in ASP programs, the approach provides transparency in how descriptors are selected and how natural language text is generated from mathematical parameters. The method is demonstrated on Google Trends data for terms like "Concert" and "Blockchain," showing the ability to generate fluent text summaries with controlled accuracy measured via RMSE.

## Method Summary
The method uses ASP to model and optimize the selection of key descriptive elements from time series data. It fits data with function prototypes (lines, sinusoids, etc.), then uses two stages of ASP optimization: first to select optimal descriptors at different verbosity levels, and second to balance summary and details while creating narration structure. The system converts mathematical parameters into qualitative natural language descriptions, allowing domain experts to review and adjust the knowledge base. The approach provides multi-resolution descriptions controlled by a verbosity parameter that scales with the number of descriptions.

## Key Results
- System generates fluent text summaries from time series data with controlled accuracy (RMSE)
- ASP-based approach provides explainable control over descriptor selection and narration structure
- Verbosity parameter enables multi-resolution descriptions balancing summary and detail
- Demonstrated on Google Trends data for terms like "Concert" and "Blockchain"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASP provides explainable control over descriptor selection and narration structure
- Mechanism: The system uses ASP programs to explicitly encode rules for descriptor optimization and natural language conversion, making the reasoning transparent
- Core assumption: ASP's rule-based logic can effectively model both the mathematical fitting process and the qualitative narrative generation
- Evidence anchors:
  - [abstract] "We model ASP/Python programs that enable an explicit control of accuracy errors and amount of synthesis"
  - [section] "The core of the system is explainable by design, since we develop a transparent deductive handling of semantic rules that describe features, by exploiting ASP"
  - [corpus] Weak evidence - corpus neighbors focus on general NLG and XAI topics but don't specifically validate ASP's role in this approach
- Break condition: If the ASP programs become too complex to understand or maintain, the explainability benefit is lost

### Mechanism 2
- Claim: Verbosity parameter enables multi-resolution descriptions that balance summary and detail
- Mechanism: The system optimizes descriptor selection at different verbosity levels (number of descriptions), allowing control over the level of detail in the final narration
- Core assumption: Users can effectively control the amount of detail through a simple integer parameter that scales with the number of descriptions
- Evidence anchors:
  - [abstract] "enable an explicit control of accuracy errors and amount of synthesis"
  - [section] "The parameter ℓ (levels of divisions) controls the amount of time resolution is provided to descriptors" and "The verbosity corresponds to the number of sentences used in the final narration"
  - [corpus] No direct evidence - corpus focuses on general data-to-text generation rather than verbosity control mechanisms
- Break condition: If the relationship between verbosity and actual detail becomes non-linear or unpredictable, users cannot effectively control the output

### Mechanism 3
- Claim: Function prototypes with curve fitting provide interpretable descriptions that can be converted to natural language
- Mechanism: The system fits data with function prototypes (line, sinusoid, etc.) and converts the mathematical parameters into qualitative natural language descriptions
- Core assumption: Simple mathematical functions with intuitive parameters can adequately represent complex data patterns and be meaningfully translated to natural language
- Evidence anchors:
  - [abstract] "The system fits data with function prototypes, optimizes descriptor selection at multiple verbosity levels"
  - [section] "Parameters, that control the degrees of freedom of the shape of the curve, allow to build a high level narration" and detailed examples of converting function parameters to qualitative descriptions
  - [corpus] No direct evidence - corpus neighbors don't discuss the specific function prototype approach used here
- Break condition: If data patterns cannot be adequately captured by the available function prototypes, the accuracy and interpretability of the output will suffer

## Foundational Learning

- Concept: Answer Set Programming (ASP)
  - Why needed here: ASP provides the rule-based reasoning framework that makes the system explainable and allows domain experts to review the knowledge base
  - Quick check question: How does ASP differ from traditional procedural programming in terms of knowledge representation and reasoning?

- Concept: Root Mean Square Error (RMSE) optimization
  - Why needed here: RMSE provides a quantitative measure of how well the function prototypes fit the data, which is essential for selecting optimal descriptors
  - Quick check question: Why is RMSE a suitable metric for evaluating the accuracy of function prototypes fitting time series data?

- Concept: Curve fitting and function prototypes
  - Why needed here: The system relies on fitting simple mathematical functions to data segments to create interpretable descriptions
  - Quick check question: What are the advantages and limitations of using function prototypes like lines, sinusoids, and polynomial segments for time series analysis?

## Architecture Onboarding

- Component map: Python configurator -> First ASP optimization program -> Second ASP optimization program -> Function prototype converters -> Python post-processing -> External data source (Google Trends)
- Critical path: Data download → Function fitting → First ASP optimization → Second ASP optimization → Natural language generation → Output
- Design tradeoffs:
  - ASP provides explainability but may be slower than neural approaches
  - Function prototypes are interpretable but may not capture all data patterns
  - Verbosity control is simple but may not capture nuanced detail requirements
- Failure signatures:
  - Poor fitting results (high RMSE) indicate inadequate function prototypes
  - Unnatural text suggests problems in the qualitative conversion rules
  - Slow performance indicates optimization issues in ASP programs
- First 3 experiments:
  1. Test with simple sinusoidal data to verify function fitting and conversion to natural language
  2. Vary verbosity parameter to observe changes in output detail level
  3. Use synthetic data with known patterns to test the accuracy of descriptor selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the system handle uncertainty in the data or lack of knowledge about patterns when fitting functions?
- Basis in paper: [explicit] The paper mentions "Handling of incoherent points of view, inconsistent rules, uncertainty and lack of knowledge can be explicitly supported by the system."
- Why unresolved: The paper does not provide details on how uncertainty is specifically handled in the ASP programs or how the system deals with ambiguous or incomplete data.
- What evidence would resolve it: A detailed explanation of how the ASP programs incorporate uncertainty handling, perhaps through specific rules or constraints that manage ambiguous data or unknown patterns.

### Open Question 2
- Question: What are the limitations of using a fixed set of function prototypes for curve fitting in terms of capturing complex or unexpected data patterns?
- Basis in paper: [inferred] The paper discusses the selection of function prototypes but does not explore the limitations of this approach in capturing diverse or complex patterns.
- Why unresolved: The paper does not address how the system performs with data that does not fit well with the predefined function prototypes, nor does it explore the potential need for additional or alternative prototypes.
- What evidence would resolve it: Empirical results showing the system's performance with various complex datasets, including those with unexpected patterns, and a discussion of the limitations of the current function prototypes.

### Open Question 3
- Question: How does the system ensure that the generated text is both accurate and coherent, especially when dealing with multiple time series or high-dimensional data?
- Basis in paper: [inferred] The paper focuses on single time series and does not address the challenges of handling multiple or high-dimensional data sources.
- Why unresolved: The paper does not explore the system's capability to maintain accuracy and coherence when extending to more complex data scenarios.
- What evidence would resolve it: Testing the system with multiple synchronized time series or high-dimensional data, evaluating the coherence and accuracy of the generated text, and discussing any necessary modifications to the ASP programs to handle such complexity.

## Limitations
- Limited detail on mathematical forms of function prototypes used for curve fitting
- ASP programs described conceptually but not provided in full
- Evaluation limited to small set of Google Trends terms without systematic comparison to baselines
- Does not address handling of multiple or high-dimensional data sources

## Confidence
- High confidence in ASP-based explainable optimization mechanism
- Medium confidence in function prototype approach for curve fitting
- Low confidence in natural language generation quality without full conversion rules

## Next Checks
1. Test the system on synthetic time series data with known patterns to verify accurate descriptor selection and RMSE optimization
2. Compare natural language output quality and accuracy against baseline data-to-text methods using standardized datasets
3. Evaluate computational efficiency of ASP optimization approach compared to neural alternatives for larger datasets