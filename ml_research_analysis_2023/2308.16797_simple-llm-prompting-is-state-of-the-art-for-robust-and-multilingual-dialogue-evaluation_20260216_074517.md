---
ver: rpa2
title: Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue
  Evaluation
arxiv_id: '2308.16797'
source_url: https://arxiv.org/abs/2308.16797
tags:
- evaluation
- dialogue
- metrics
- language
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DIAL EVALML, a novel framework for automatic
  dialogue evaluation that leverages Large Language Models (LLMs) to achieve state-of-the-art
  performance in both robustness and multilingual tasks. The authors propose combining
  multiple submetrics, including Valid Sentence Prediction (VSP), Next Sentence Prediction
  (NSP), Masked Language Modeling (MLM), and Engagement, with ChatGPT to evaluate
  dialogue quality.
---

# Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation

## Quick Facts
- arXiv ID: 2308.16797
- Source URL: https://arxiv.org/abs/2308.16797
- Reference count: 11
- Primary result: Combines ChatGPT with encoder-based submetrics to achieve state-of-the-art performance in robust and multilingual dialogue evaluation

## Executive Summary
This paper introduces DIAL EVALML, a novel framework for automatic dialogue evaluation that leverages Large Language Models (LLMs) to achieve state-of-the-art performance in both robustness and multilingual tasks. The authors propose combining multiple submetrics, including Valid Sentence Prediction (VSP), Next Sentence Prediction (NSP), Masked Language Modeling (MLM), and Engagement, with ChatGPT to evaluate dialogue quality. The framework is trained on self-supervised and supervised data, and the submetrics are weighted based on their correlation with the desired aspect. The authors demonstrate that their approach significantly outperforms previous methods in both tasks, with the best results achieved by combining ChatGPT with encoder-based models.

## Method Summary
The DIAL EVALML framework combines multiple submetrics (VSP, NSP, MLM, ENG) with ChatGPT evaluations, using correlation-based weighting to optimize for specific quality aspects. The submetrics are implemented using encoder-based models (XLM-RoBERTa-large and MBART50) trained on self-supervised tasks and augmented with machine-translated and paraphrased data. ChatGPT provides zero-shot evaluation capabilities through carefully engineered prompts. The framework ensembles these components through weighted averaging, where weights are determined by each submetric's correlation with the target evaluation aspect. The approach is evaluated on DSTC11 Track 4 benchmarks for both robust and multilingual dialogue evaluation tasks.

## Key Results
- Achieved state-of-the-art performance with mean Spearman correlation scores across several benchmarks
- Ranked first place on both the Robust and Multilingual tasks of the DSTC11 Track 4
- Demonstrated superior performance by combining ChatGPT with encoder-based models compared to using either approach alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT can evaluate dialogue quality without needing gold reference responses because it has strong language understanding capabilities from RLHF training.
- Mechanism: The LLM leverages its conversational coherence modeling to assess response appropriateness, relevance, and engagement directly from context-response pairs.
- Core assumption: The instruction-tuned LLM's evaluation capability generalizes across dialogue aspects without explicit fine-tuning on evaluation data.
- Evidence anchors:
  - [abstract] "Empirical results show our framework achieves state of the art results... proving the evaluation capabilities of prompted LLMs."
  - [section] "We show that ChatGPT is a strong evaluator of dialogues, outperforming typical encoder frameworks."
  - [corpus] Weak - no direct corpus evidence that ChatGPT's zero-shot evaluation generalizes across languages.
- Break condition: If the LLM's evaluation capability degrades significantly for non-English languages or specialized domains not covered in its training.

### Mechanism 2
- Claim: Combining multiple submetrics (VSP, NSP, MLM, ENG) with ChatGPT through weighted ensembling improves evaluation robustness and captures different dialogue quality aspects.
- Mechanism: Each submetric captures specific quality dimensions (fluency, coherence, engagement) that complement the LLM's holistic evaluation, and weighted combination optimizes for specific aspects.
- Core assumption: Different submetrics capture orthogonal aspects of dialogue quality that when combined provide better correlation with human evaluation than any single metric.
- Evidence anchors:
  - [section] "Similar to other frameworks... we employ several submetrics to evaluate dialogue responses – ranging from zero-shot prediction using pretrained LLMs to trained models using self-supervised and supervised methods – and weigh them according to the aspect we wish to predict."
  - [section] "We propose a comprehensive framework, incorporating earlier encoder-based approaches and ChatGPT, as illustrated in Figure 1."
  - [corpus] Weak - no corpus evidence showing individual submetric contributions or ensemble effectiveness across languages.
- Break condition: If correlation between submetrics and human evaluation is low for certain languages or aspects, reducing ensemble effectiveness.

### Mechanism 3
- Claim: The proposed framework achieves state-of-the-art performance in both robust and multilingual dialogue evaluation tasks.
- Mechanism: The framework's combination of LLM prompting and encoder-based submetrics, optimized for specific quality aspects through correlation-based weighting, achieves superior Spearman correlation scores compared to baseline methods.
- Core assumption: The correlation-based weighting scheme effectively maps submetric scores to human evaluation dimensions across languages.
- Evidence anchors:
  - [abstract] "Empirical results show our framework achieves state of the art results in terms of mean Spearman correlation scores across several benchmarks and ranks first place on both the Robust and Multilingual tasks of the DSTC11 Track 4."
  - [section] "For the track we submitted 4 different systems, exploring the contribution of the different components of our framework... System 1 (DIAL EVALML): Submetric ensembling of ChatGPT + XLM-R."
  - [corpus] Weak - no corpus evidence that performance generalizes beyond the specific DSTC11 benchmarks used.
- Break condition: If performance degrades significantly on datasets outside DSTC11 or when tested with different LLM models.

## Foundational Learning

- Concept: Spearman correlation as evaluation metric
  - Why needed here: The paper uses Spearman correlation to measure how well the automated metric ranks dialogue quality compared to human judgments.
  - Quick check question: What is the difference between Spearman and Pearson correlation, and why is Spearman more appropriate for dialogue evaluation?

- Concept: Prompt engineering for LLMs
  - Why needed here: The effectiveness of ChatGPT evaluation depends on crafting prompts that elicit consistent quality scores.
  - Quick check question: How does temperature setting affect the determinism of LLM outputs, and why was it set to 0 in this work?

- Concept: Ensemble methods in machine learning
  - Why needed here: The framework combines multiple submetrics through weighted averaging to improve overall evaluation performance.
  - Quick check question: What are the advantages and disadvantages of weighted averaging versus other ensemble methods like stacking or boosting?

## Architecture Onboarding

- Component map: Context/Response → Submetrics (VSP, NSP, MLM, ENG, ChatGPT) → Correlation weighting → Weighted average score
- Critical path: Context/Response → Submetrics → Correlation weighting → Final score
- Design tradeoffs: Simplicity vs. performance (using simple weighted averaging vs. more complex ensemble methods), computational cost (multiple model inferences) vs. accuracy
- Failure signatures: Inconsistent scores across runs (ChatGPT variability), poor correlation with human evaluation (submetric weighting issues), performance degradation on certain languages (multilingual capability limitations)
- First 3 experiments:
  1. Run each submetric independently on a sample dataset and compare correlation with human evaluation
  2. Test ChatGPT with different prompt variations to identify most consistent scoring behavior
  3. Perform ablation study by removing each submetric from the ensemble to quantify individual contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between multilingual data and monolingual data for training robust dialogue evaluation models?
- Basis in paper: [explicit] The authors found that including translations was detrimental to performance for VSP but improved performance for NSP, and the best performance for ENG was obtained with 20% of translated data.
- Why unresolved: The optimal balance likely depends on the specific task and model architecture, and further experimentation is needed to determine the best approach.
- What evidence would resolve it: A comprehensive study comparing different ratios of multilingual to monolingual data for various dialogue evaluation tasks and model architectures.

### Open Question 2
- Question: How can we improve the calibration of LLM-based dialogue evaluation metrics to better align with human judgments?
- Basis in paper: [explicit] The authors found instances where ChatGPT underestimated the quality of responses compared to human evaluations, suggesting a need for better calibration.
- Why unresolved: Current LLMs may have inherent biases or limitations in their evaluation capabilities that need to be addressed through improved prompting, fine-tuning, or alternative approaches.
- What evidence would resolve it: Experiments comparing the performance of calibrated LLM-based metrics against human judgments on a diverse set of dialogue evaluation tasks.

### Open Question 3
- Question: What are the most effective methods for evaluating the "safety" and "trustworthiness" of chatbots beyond traditional dialogue quality metrics?
- Basis in paper: [explicit] The authors discuss the need for evaluating chatbots' "safety" and "trustworthiness," which are abstract concepts not easily captured by current dialogue evaluation protocols.
- Why unresolved: Current dialogue evaluation metrics focus primarily on linguistic competence and may not adequately capture the nuances of safety and trustworthiness in chatbot interactions.
- What evidence would resolve it: Development and validation of new evaluation methods that incorporate measures of safety, trustworthiness, and other relevant factors beyond traditional dialogue quality metrics.

## Limitations

- The evaluation framework is validated only on DSTC11 Track 4 benchmarks without broader cross-dataset validation
- Multilingual capabilities are asserted but not thoroughly validated across diverse language families beyond English, Spanish, and Chinese
- The computational cost of running multiple encoder-based submetrics plus ChatGPT evaluations may limit practical deployment in resource-constrained settings

## Confidence

**High confidence**: The core mechanism of combining multiple submetrics with ChatGPT is technically sound and the weighted ensemble approach is well-established in evaluation literature.

**Medium confidence**: The claimed state-of-the-art performance on DSTC11 benchmarks, while demonstrated, requires broader validation across additional datasets and language pairs.

**Low confidence**: The assertion that ChatGPT's evaluation capability generalizes robustly across all languages without fine-tuning is not empirically validated beyond the specific languages tested.

## Next Checks

1. **Cross-dataset validation**: Test the framework on additional dialogue evaluation datasets beyond DSTC11 (such as PersonaChat, DailyDialog, or ConvAI benchmarks) to verify whether state-of-the-art performance generalizes beyond the specific test conditions.

2. **Language family expansion**: Evaluate the framework on languages from different language families (e.g., Arabic, Hindi, Japanese) to assess whether the claimed multilingual capabilities hold across typologically diverse languages, not just Indo-European and Sinitic languages.

3. **Model version consistency**: Run the evaluation framework using different versions of ChatGPT (or alternative LLMs like Claude or Llama) to determine whether the strong zero-shot evaluation performance is consistent across model providers and versions, or specific to the particular model version used in the study.