---
ver: rpa2
title: Instance-wise Linearization of Neural Network for Model Interpretation
arxiv_id: '2310.16295'
source_url: https://arxiv.org/abs/2310.16295
tags:
- network
- neural
- prediction
- input
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the interpretability challenge of neural networks
  by reformulating their forward computation process into a linear matrix multiplication
  representation. The core idea is based on the observation that each prediction of
  a neural network has only one activation pattern, allowing the complex nonlinear
  operations to be locally linearized.
---

# Instance-wise Linearization of Neural Network for Model Interpretation

## Quick Facts
- arXiv ID: 2310.16295
- Source URL: https://arxiv.org/abs/2310.16295
- Authors: 
- Reference count: 40
- Primary result: Reformulates neural network forward computation into linear matrix multiplication F(x) = W · x + b for instance-wise interpretability

## Executive Summary
This paper addresses neural network interpretability by reformulating the forward computation process into a linear matrix multiplication representation. The core insight is that each prediction uses only one activation pattern, allowing the complex nonlinear operations to be locally linearized. By transforming each layer into a linear operation and aggregating them, the prediction can be expressed as F(x) = W · x + b, where W and b are calculated layer by layer. This formulation provides both a feature attribution map and reveals how each input feature contributes to the prediction.

The method has been validated on multiple datasets including MNIST, CIFAR-10, CIFAR-100, and ImageNet, showing effectiveness in explaining model predictions. It applies to both supervised classification and unsupervised parametric t-SNE dimension reduction. The approach is particularly valuable for understanding the decision process of neural networks and identifying critical features used during prediction.

## Method Summary
The method reformulates neural network layers into linear operations and aggregates them into a global linear equation F(x) = W · x + b. Each layer type (fully connected, convolution, pooling, skip connection, batch normalization) is transformed into a matrix multiplication and bias accumulation. The activation pattern is fixed during a single forward pass, allowing the entire network to be represented as a linear function. The resulting W matrix provides feature attribution by showing how input features contribute to the output, while b captures constant biases from batch normalization and other sources.

## Key Results
- Linearization approach effectively explains model predictions across MNIST, CIFAR-10, CIFAR-100, and ImageNet datasets
- Feature attribution W reveals critical input features used in predictions
- Method applies to both supervised classification and unsupervised parametric t-SNE dimension reduction
- Demonstrated effectiveness on multiple architectures including LeNet, AlexNet, VGG, ResNet, and DenseNet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Each prediction uses only one activation pattern, making the local computation linear
- Mechanism: ReLU and other piecewise-linear activations activate in a fixed pattern for a given input, converting each layer into matrix multiplication with constant scaling
- Core assumption: Activation pattern is determined solely by the input and remains fixed during forward pass
- Evidence anchors: [abstract] "a neural network prediction has only one activation pattern", [section 3] "For a single prediction, each layer can be replaced as a linear matrix multiplication"
- Break condition: If network uses non-piecewise-linear activations (e.g., GELU, SELU, ELU) without approximation, linearization is not exact

### Mechanism 2
- Claim: Overall prediction can be expressed as F(x) = W · x + b by aggregating linearized layers
- Mechanism: Each layer is reformulated as linear operation, then multiplied/accumulated to form global W and b
- Core assumption: Order of operations and linear transformations preserve correct mapping when aggregated
- Evidence anchors: [section 3.6] "After linearization of a neural network prediction, the prediction is represented as F (x) = W · x + b", [section 3.2] shows convolution → sparse matrix multiplication
- Break condition: If network has recurrent or attention layers not covered by linearization, aggregation breaks

### Mechanism 3
- Claim: W provides exact feature attribution because it equals input-output Jacobian under fixed activation
- Mechanism: Since activation pattern is fixed, Jacobian JF(x) is constant and equals W, giving direct sensitivity of output to input
- Core assumption: Piecewise-linear activation region contains input, so Jacobian is constant
- Evidence anchors: [section 3.6] "Under this condition, the feature attribution W is equal to input-output Jacobian matrix JF (x)", [section 3.7] shows ensemble models preserve this property
- Break condition: If input lies near activation boundary, small changes could switch pattern and invalidate attribution

## Foundational Learning

- Concept: Matrix representation of convolution operations
  - Why needed here: To convert convolution into sparse matrix multiplication for linearization
  - Quick check question: Given a 2x2 kernel on a 4x4 input with stride 2, what is the size of resulting matrix W?

- Concept: Piecewise-linear activation functions
  - Why needed here: Linearization relies on ReLU-like activations that are linear in each region
  - Quick check question: What is gradient of ReLU(x) for x > 0 and x < 0?

- Concept: Jacobian matrix for neural networks
  - Why needed here: To justify that W is equivalent to Jacobian under fixed activation patterns
  - Quick check question: For scalar output f(x) and vector input x, what is Jacobian JF(x)?

## Architecture Onboarding

- Component map: Input layer → flattened vector x → Each CNN layer (conv, pool, BN, skip) → linearized matrix + bias → Aggregation step → multiply matrices to form W, accumulate biases to form b → Output layer → final linear map F(x) = W·x + b

- Critical path:
  1. Forward pass to fix activation pattern
  2. Convert each layer to linear form
  3. Multiply layer matrices in order to get W
  4. Sum propagated biases to get b
  5. Use W, b for attribution or prediction

- Design tradeoffs:
  - Exact linearization only for piecewise-linear activations; others require approximation
  - Batch norm layers contribute large bias terms, dominating W·x in some cases
  - Skip connections merge identity matrix into W

- Failure signatures:
  - Input near activation boundaries → sudden attribution changes
  - Non-piecewise-linear activations → linearization error
  - Large bias terms from BN → W·x insufficient for prediction

- First 3 experiments:
  1. Implement linearization on simple 2-layer ReLU network and verify F(x) = W·x + b matches original output
  2. Test attribution on MNIST with LeNet5 and compare to gradient-based methods
  3. Apply linearization to small CNN without batch norm and check if W·x alone suffices for prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can linearization approach be extended to handle activation functions like GELU, SELU, and ELU that are not piecewise linear?
- Basis in paper: [explicit] Paper mentions "activation function such as GELU, SELU and ELU can not use Jacobian matrix to calculate W" and states equation for y = W · x + b needs to be calculated layer by layer for these functions
- Why unresolved: Paper acknowledges limitation but doesn't provide concrete solution for extending linearization to these activation functions
- What evidence would resolve it: Method to transform or approximate these activation functions into linear form that can be incorporated into W matrix calculation, along with experimental validation

### Open Question 2
- Question: What is optimal architecture for supervised learning tasks when training without batch normalization to maximize effectiveness of W matrix for interpretation?
- Basis in paper: [explicit] Paper states "An alternative approach to improve the sufficient of W is to train a neural network without batch normalization layer" and mentions networks can achieve state-of-art performance without batch normalization
- Why unresolved: While paper suggests this as alternative, it doesn't explore architectural changes or training techniques that would optimize this approach for interpretability
- What evidence would resolve it: Comparative studies of different architectures and training techniques (e.g., weight normalization, initialization schemes) trained without batch normalization, showing which configurations produce most interpretable W matrices while maintaining competitive performance

### Open Question 3
- Question: How does linearization approach perform on sequential data (e.g., time series, text) and recurrent neural networks?
- Basis in paper: [inferred] Paper focuses exclusively on convolutional neural networks and mentions "we mainly focus our discussion on convolution neural network" but doesn't address other network architectures or data types
- Why unresolved: Methodology is demonstrated only on image-based CNN architectures, leaving open questions about applicability to other data modalities and network types
- What evidence would resolve it: Experimental results showing linearization approach applied to RNNs, LSTMs, or Transformers on sequential data tasks, with comparisons to traditional interpretability methods for these architectures

## Limitations
- Exact linearization only applies to piecewise-linear networks; smooth activations require approximations
- Batch normalization layers introduce large constant bias terms that can overwhelm linear contribution W·x
- Method assumes stable activation patterns - inputs near decision boundaries may cross into different linear regions

## Confidence
- High confidence: Networks with piecewise-linear activations (ReLU, Leaky ReLU, MaxPool) where activation patterns are stable within input region
- Medium confidence: Networks with batch normalization where W·x contribution is significant relative to bias term
- Low confidence: Networks with smooth activations (GELU, ELU) or inputs near activation boundaries

## Next Checks
1. Verify Jacobian-W equivalence by computing both on simple network and comparing numerical values
2. Test attribution stability across inputs near activation boundaries to quantify pattern-switching effects
3. Measure prediction accuracy when using W·x alone versus W·x + b on batch-normalized networks to quantify bias term's contribution