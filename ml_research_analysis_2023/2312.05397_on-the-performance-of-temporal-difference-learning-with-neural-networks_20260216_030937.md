---
ver: rpa2
title: On the Performance of Temporal Difference Learning With Neural Networks
arxiv_id: '2312.05397'
source_url: https://arxiv.org/abs/2312.05397
tags:
- neural
- lemma
- where
- learning
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper provides a convergence analysis of Neural Temporal Difference\
  \ (TD) Learning for policy evaluation using neural networks as function approximators.\
  \ The authors analyze Neural TD Learning with a projection onto a ball of fixed\
  \ radius around the initial point and show an approximation bound of O(\u03F5) +\
  \ \xD5(1/\u221Am), where \u03F5 is the approximation quality of the best neural\
  \ network in the projection ball and m is the width of all hidden layers."
---

# On the Performance of Temporal Difference Learning With Neural Networks

## Quick Facts
- arXiv ID: 2312.05397
- Source URL: https://arxiv.org/abs/2312.05397
- Reference count: 40
- One-line primary result: Analysis of Neural Temporal Difference Learning with constant-radius projection achieving O(ϵ) + Õ(1/√m) approximation bound

## Executive Summary
This paper provides a convergence analysis of Neural Temporal Difference (TD) Learning for policy evaluation using neural networks as function approximators. The authors analyze Neural TD Learning with a projection onto a ball of fixed radius around the initial point, showing an approximation bound of O(ϵ) + Õ(1/√m). The key innovation is using a convex combination of the D-norm and Dirichlet semi-norm to measure approximation error, which allows convergence analysis without linearization or regularity assumptions on the policy.

## Method Summary
The method involves Neural TD Learning with a projection operator that keeps iterates within a constant-radius ball around the initial point θ₀. The algorithm uses a convex combination of the D-norm and Dirichlet semi-norm for error measurement, enabling stable convergence without linearizing around the initial condition. The analysis relies on NTK-style bounds showing that with increasing width, the neural approximation becomes more linear, allowing gradient-based methods to work effectively.

## Key Results
- Achieves approximation bound of O(ϵ) + Õ(1/√m) where ϵ is the approximation quality of the best neural network in the projection ball
- Shows convergence without requiring the projection radius to decay with network width m
- Demonstrates empirically that using a constant projection radius can substantially improve performance compared to decaying radius

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using the convex combination N(f) = (1 - γ)||f||_D^2 + γ||f||_Dir^2 enables stable convergence without linearization
- Mechanism: The N-norm captures both value approximation error and policy-induced dynamics. TD updates approximate gradient descent on N(f) even for non-reversible policies
- Core assumption: Policy is irreducible and aperiodic (ensures unique stationary distribution)
- Evidence anchors:
  - [abstract] "The key distinguishing feature of this analysis is the use of a convex combination of the D-norm and Dirichlet semi-norm"
  - [section 2.3] "p N(f) is a valid norm: since N(f) is quadratic, we can write N(f) = f^T Nf for some symmetric matrix N"
- Break condition: If policy is reducible or periodic, stationary distribution may not exist

### Mechanism 2
- Claim: Projection onto constant-radius ball B(θ_0, ω) prevents divergence without restricting network expressiveness
- Mechanism: Fixed-radius projection keeps iterates in a region where the neural network remains sufficiently linear for gradient approximation
- Core assumption: Initial weights are within O(√m) of initialization (Assumption 3.1)
- Evidence anchors:
  - [abstract] "Our result improves on previous work because it does not require taking the radius ω to decay with m"
  - [section 6] "empirically that these changes can make a substantial difference in performance"
- Break condition: If initialization is too far from optimal, projection may prevent reaching solution

### Mechanism 3
- Claim: NTK-style bounds ensure network becomes increasingly linear with width, enabling convergence analysis
- Mechanism: As width m increases, the neural network's input-output mapping becomes closer to its linearization, allowing gradient-based methods to work
- Core assumption: Activation function is Lipschitz and smooth (Assumption 2.4)
- Evidence anchors:
  - [section 4] "We use recent NTK-style bounds from Liu et al. (2020) to argue that with increasing width, the neural approximation gets more linear"
  - [section 2.5] "Lemma A.11... the norm of Hessian matrix, ∇^2_θV(s,θ), is uniformly bounded by Õ(m^{-0.5})"
- Break condition: If activation function is not smooth (e.g., ReLU), NTK bounds don't apply

## Foundational Learning

- Concept: Stationary distribution and mixing time of Markov chains
  - Why needed here: The analysis relies on states being sampled from the stationary distribution for the D-norm to be well-defined
  - Quick check question: What condition on the transition matrix P guarantees the existence of a unique stationary distribution?

- Concept: Semi-gradient methods and temporal difference learning
  - Why needed here: The paper analyzes a semi-gradient approach to TD learning, which is the foundation for neural TD methods
  - Quick check question: How does the TD error δ_t relate to the Bellman equation for the value function?

- Concept: Projection methods in optimization
  - Why needed here: The algorithm uses projection onto a ball to stabilize the iterates and prevent divergence
  - Quick check question: What is the mathematical definition of the projection operator Proj(θ) onto a set Θ?

## Architecture Onboarding

- Component map:
  - MDP environment (states, actions, rewards, transition probabilities)
  - Policy π (fixed, provides action probabilities)
  - Neural network V(s, θ) with parameters θ
  - Projection operator Proj onto B(θ_0, ω)
  - TD update rule with gradient approximation

- Critical path:
  1. Initialize θ from N(0,1) with |b_r| ≤ 1
  2. Sample state s from stationary distribution
  3. Compute next state s' and reward r(s)
  4. Calculate TD error δ_t = r(s) + γV(s', θ) - V(s, θ)
  5. Compute gradient g(θ_t) = ∇_θV(s, θ_t) * δ_t
  6. Update θ_{t+1/2} = θ_t + α_t * g(θ_t)
  7. Project: θ_{t+1} = Proj(θ_{t+1/2})
  8. Repeat

- Design tradeoffs:
  - Fixed vs. decaying projection radius: Fixed radius (O(ω)) allows better exploration but may slow convergence
  - Width m vs. depth K: Wider networks are more linear but require more computation
  - Smooth vs. non-smooth activations: Smooth activations enable NTK analysis but may be less biologically plausible

- Failure signatures:
  - Divergence: θ values grow unbounded, projection has no effect
  - Slow convergence: θ remains close to initialization, small updates
  - Oscillation: θ values fluctuate without approaching optimal solution
  - Poor approximation: Final value function has high Bellman error despite convergence

- First 3 experiments:
  1. Verify stationary distribution: Run policy to collect states and check empirical distribution matches theoretical stationary distribution
  2. Test projection radius: Compare convergence with fixed vs. decaying radius on a simple MDP (e.g., gridworld)
  3. Width scaling: Train networks with increasing width m and measure approximation error and convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of Neural TD Learning scale with the depth K of the neural network?
- Basis in paper: [explicit] The paper analyzes Neural TD Learning with a fixed radius projection and shows convergence bounds in terms of width m, but does not explicitly analyze the dependence on depth K.
- Why unresolved: The analysis focuses on the width parameter m and the projection radius ω, but the role of depth K is not thoroughly explored. The paper assumes K grows slower than exponentially in m, but the precise impact of K on convergence is not quantified.
- What evidence would resolve it: Empirical studies varying K while keeping m fixed, or theoretical bounds explicitly showing the dependence of convergence rate on K.

### Open Question 2
- Question: What is the impact of the initialization scheme on the convergence of Neural TD Learning?
- Basis in paper: [explicit] The paper assumes the initialization is not too large (Assumption 3.1) and mentions that this holds with high probability for random Gaussian initialization. However, it does not explore the impact of different initialization schemes on convergence.
- Why unresolved: The analysis relies on a specific initialization assumption but does not investigate how other initialization strategies (e.g., Xavier, He) might affect convergence.
- What evidence would resolve it: Comparative experiments with different initialization schemes and their impact on convergence speed and final approximation error.

### Open Question 3
- Question: How does the choice of activation function affect the convergence of Neural TD Learning?
- Basis in paper: [explicit] The paper assumes the activation function is Lipschitz and smooth (Assumption 2.4) but does not explore the impact of different activation functions on convergence.
- Why unresolved: The analysis relies on smoothness assumptions but does not investigate how non-smooth activations (e.g., ReLU) or different smooth activations (e.g., tanh, sigmoid) affect convergence.
- What evidence would resolve it: Empirical studies comparing convergence rates and final errors using different activation functions, or theoretical analysis of the impact of activation function properties on convergence.

## Limitations
- Relies on smoothness and Lipschitz properties of activation function that may not hold for commonly used ReLU
- Empirical validation is limited to demonstrating performance differences between constant and decaying projection radii
- Theoretical guarantees depend on specific initialization assumptions that may not hold in practice

## Confidence
- High confidence: The theoretical framework using convex combination of D-norm and Dirichlet semi-norm is mathematically sound
- Medium confidence: The NTK-style bounds that enable the analysis are reasonable but may not fully capture deep network behavior
- Low confidence: The practical implications of the theoretical bounds and actual performance gains need more extensive empirical validation

## Next Checks
1. Activation function sensitivity: Systematically test the algorithm with different activation functions (smooth vs. non-smooth) to verify the impact of smoothness assumptions on convergence

2. Width scaling verification: Conduct experiments varying network width m across multiple orders of magnitude to empirically verify the O(1/√m) approximation bound

3. Projection radius sensitivity: Design controlled experiments comparing fixed vs. decaying projection radii across different MDP environments and initializations to better understand the trade-offs highlighted in Section 6