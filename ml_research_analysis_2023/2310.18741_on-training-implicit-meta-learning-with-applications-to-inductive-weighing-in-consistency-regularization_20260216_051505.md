---
ver: rpa2
title: On Training Implicit Meta-Learning With Applications to Inductive Weighing
  in Consistency Regularization
arxiv_id: '2310.18741'
source_url: https://arxiv.org/abs/2310.18741
tags:
- training
- numn
- learning
- validation
- neumann
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically compares different inverse Hessian-vector
  product approximation methods used in implicit meta-learning (IML), including Neumann
  series, conjugate gradient, and identity matrix approximations. Experiments on MNIST
  and CIFAR-10 datasets show that Neumann series with 3 terms and conjugate gradient
  with 3 steps perform best, while increasing iterations worsens performance due to
  estimation inaccuracies beyond convergence.
---

# On Training Implicit Meta-Learning With Applications to Inductive Weighing in Consistency Regularization

## Quick Facts
- arXiv ID: 2310.18741
- Source URL: https://arxiv.org/abs/2310.18741
- Reference count: 0
- One-line primary result: Neumann series with 3 terms and conjugate gradient with 3 steps perform best for inverse Hessian-vector product approximation in implicit meta-learning

## Executive Summary
This paper systematically compares inverse Hessian-vector product approximation methods in implicit meta-learning, including Neumann series, conjugate gradient, and identity matrix approximations. Experiments on MNIST and CIFAR-10 datasets demonstrate that Neumann series with 3 terms and conjugate gradient with 3 steps provide optimal performance, while increasing iterations degrades accuracy due to estimation inaccuracies. The study also reveals catastrophic forgetting when implicit meta-learning is trained beyond convergence. Based on these insights, the authors propose "Fixing FixMatch," a semi-supervised learning algorithm that uses a confidence network to inductively weigh consistency regularization losses, improving baseline FixMatch performance by learning to prioritize relevant training instances.

## Method Summary
The study compares three methods for approximating inverse Hessian-vector products in implicit meta-learning: Neumann series, conjugate gradient, and identity matrix approximations. The experiments use multi-layer perceptrons on MNIST and CIFAR-10 datasets with per-parameter and per-layer L2 regularization as meta-knowledge. The base model is trained to convergence using Adam optimizer, then meta-gradients are computed using implicit differentiation. The confidence network approach builds on FixMatch by learning to weight unlabeled examples based on their relevance, with the confidence network sharing parameters with the base model to reduce computational overhead.

## Key Results
- Neumann series with 3 terms and conjugate gradient with 3 steps provide optimal performance for inverse Hessian-vector product approximation
- Increasing Neumann series terms or conjugate gradient steps beyond 3 degrades performance due to estimation inaccuracies
- Implicit meta-learning exhibits catastrophic forgetting when trained beyond convergence of the base model
- The confidence network approach improves FixMatch baseline by learning to up-weigh useful images and down-weigh out-of-distribution samples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Neumann series with 3 terms provides the most accurate inverse Hessian-vector product approximation in implicit meta-learning (IML).
- **Mechanism:** Neumann series iteratively approximates the inverse Hessian by repeatedly applying the operation (I - H) to the gradient, where H is the Hessian matrix. With only 3 terms, the approximation remains close to the true inverse while avoiding numerical instability from excessive matrix exponentiations.
- **Core assumption:** The base model is trained to convergence, making the gradient sufficiently small for Neumann series to provide accurate estimates without requiring many terms.
- **Evidence anchors:**
  - [abstract]: "Neumann series with 3 terms and conjugate gradient with 3 steps perform best"
  - [section]: "Observation 1: Increasing Neumann Series terms worsens its accuracy" and "Observation 2: Catastrophic forgetting observed when IML is trained beyond convergence can be explained in terms of the accuracy of estimating inverse Hessian-vector products"
  - [corpus]: Weak - no direct evidence about Neumann series performance in corpus

### Mechanism 2
- **Claim:** Using more terms in Neumann series or conjugate gradient steps degrades approximation quality and final solution performance.
- **Mechanism:** As more terms/steps are added, the approximation introduces more numerical errors and floating-point instability. The repeated matrix multiplications and exponentiations amplify small errors, leading to divergence from the true inverse Hessian-vector product.
- **Core assumption:** The loss landscape curvature changes significantly beyond convergence, making higher-order approximations less accurate.
- **Evidence anchors:**
  - [abstract]: "increasing iterations worsens performance due to estimation inaccuracies beyond convergence"
  - [section]: "Increasing the number of Neumann terms paints the algorithm more unstable and exacerbates the ability of the model to overfit the validation set" and "Increasing the number of CG steps worsens its' performance"
  - [corpus]: Weak - no direct evidence about term count effects in corpus

### Mechanism 3
- **Claim:** Catastrophic forgetting in IML occurs because approximation methods cannot accurately estimate loss function curvature beyond convergence.
- **Mechanism:** When IML is trained beyond the point where the base model has converged, the approximation methods (Neumann series, CG, etc.) produce highly variable estimates of the inverse Hessian-vector product. This variability causes the meta-update to move away from the optimal solution, degrading both training and validation performance.
- **Core assumption:** The variance in inverse Hessian estimates is directly correlated with the distance from convergence in the optimization landscape.
- **Evidence anchors:**
  - [abstract]: "exhibits catastrophic forgetting if trained beyond convergence"
  - [section]: "Neumann(3) only exhibits high variance after the algorithm convergences" and "This variance in estimating the inverse Hessian-Vector product is one source of explanation to why training and validation performance worsens"
  - [corpus]: Weak - no direct evidence about catastrophic forgetting in corpus

## Foundational Learning

- **Concept: Implicit Function Theorem (IFT)**
  - Why needed here: IFT allows bypassing unrolled differentiation in meta-learning by providing a way to compute the derivative of the optimal solution with respect to hyperparameters without storing the entire optimization trajectory.
  - Quick check question: What condition must hold for IFT to be applicable in IML, and what does it guarantee about the existence of a solution?

- **Concept: Second-order optimization and Hessian-vector products**
  - Why needed here: IML requires computing second-order derivatives (Hessian) to update hyperparameters, but storing the full Hessian is impractical for deep networks. Understanding Hessian-vector products is crucial for implementing approximation methods.
  - Quick check question: How does Pearlmutter's method enable efficient computation of Hessian-vector products without explicitly forming the Hessian matrix?

- **Concept: Semi-supervised learning and consistency regularization**
  - Why needed here: The proposed Confidence Network method builds on FixMatch, which uses consistency regularization between weakly and strongly augmented versions of unlabeled data.
  - Quick check question: In FixMatch, what is the role of the confidence threshold τ, and how does it affect which unlabeled examples contribute to the unsupervised loss?

## Architecture Onboarding

- **Component map:**
  Base model (fθ) -> Confidence Network (C(u;ω)) -> Meta-knowledge optimizer -> Base model optimizer

- **Critical path:**
  1. Train base model to convergence on labeled data
  2. Compute validation loss gradient with respect to base model parameters
  3. Approximate inverse Hessian-vector product using chosen method (Neumann, CG, or Identity)
  4. Compute meta-gradient and update hyperparameters
  5. Repeat until convergence or early stopping criterion met

- **Design tradeoffs:**
  - Neumann series vs CG: Neumann is faster but less accurate with more terms; CG is slower but can be more precise with fewer steps
  - Convergence assumption vs practical efficiency: Training to full convergence provides better meta-gradients but is computationally expensive
  - Early stopping vs full training: Early stopping prevents catastrophic forgetting but may underutilize available validation data

- **Failure signatures:**
  - High variance in validation accuracy during training (indicates approximation instability)
  - Degradation of both training and validation performance after initial improvement (catastrophic forgetting)
  - Oscillating weight predictions from Confidence Network (indicates instability in meta-updates)
  - Extremely slow convergence or divergence of base model training (indicates poor hyperparameter choices)

- **First 3 experiments:**
  1. Implement Neumann series with 3 terms on a simple MLP for per-parameter L2 regularization, measuring overfitting capability on a small validation set
  2. Compare CG with 3 steps versus Neumann series on the same task, measuring both accuracy and memory consumption
  3. Implement the Confidence Network on CIFAR-10 with 250 labeled examples, measuring improvement over baseline FixMatch and analyzing the weight distribution learned by the Confidence Network

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different Hessian approximation methods affect the geometry of the meta-loss landscape during training?
- Basis in paper: [inferred] The paper mentions the need to study how approximations shape and get shaped by the loss landscape, and references recent work on Hessian eigenvalue density analysis.
- Why unresolved: The paper only provides empirical comparisons of approximation methods on final solution quality, without analyzing how these methods affect the loss landscape geometry during training.
- What evidence would resolve it: Detailed analysis of Hessian eigenvalue distributions throughout training for different approximation methods, showing how they evolve and affect the meta-loss landscape.

### Open Question 2
- Question: What is the optimal strategy for determining the number of inner-loop steps in implicit meta-learning?
- Basis in paper: [explicit] The paper shows that using too few inner steps leads to unstable training, while using too many steps is computationally expensive, but doesn't provide a principled method for choosing this hyperparameter.
- Why unresolved: The paper demonstrates the problem but doesn't propose a systematic solution for determining the optimal number of inner steps.
- What evidence would resolve it: Development of a method to predict the minimum number of inner steps needed for stable training, possibly based on convergence metrics or loss landscape analysis.

### Open Question 3
- Question: How can the need for a large validation set in the Confidence Network approach be eliminated?
- Basis in paper: [explicit] The paper identifies the need for a large validation set as a main shortcoming of the proposed semi-supervised learning algorithm.
- Why unresolved: While the paper proposes sharing parameters between the base model and Confidence Network as a potential solution, it doesn't implement or validate this approach.
- What evidence would resolve it: Experimental validation of the parameter-sharing approach or alternative methods to eliminate the need for a large validation set while maintaining or improving performance.

## Limitations
- Experimental scope limited to MNIST and CIFAR-10 datasets with MLP architectures, limiting generalizability to complex vision tasks
- Neumann series optimality claim relies on single convergence assumption that may not hold for non-convex or poorly conditioned loss landscapes
- Weak corpus evidence linking approximation variance to catastrophic forgetting phenomena
- Confidence Network improvement lacks ablation studies showing individual contributions of weighting versus feature learning

## Confidence
- Neumann series with 3 terms optimality: Medium confidence (limited evidence, convergence assumption)
- Catastrophic forgetting mechanism: Low confidence (weak corpus evidence)
- Confidence Network improvement: Medium confidence (lack of ablation studies)

## Next Checks
1. Implement Neumann series with 3 terms on CIFAR-10 using ResNet-18 to verify if the approximation method maintains stability with deeper architectures
2. Conduct controlled experiments varying the Hessian eigenvalue distribution to test the convergence assumption underlying Neumann series performance
3. Perform ablation studies on the Confidence Network to isolate the effects of inductive weighting versus domain-centric feature learning on FixMatch performance