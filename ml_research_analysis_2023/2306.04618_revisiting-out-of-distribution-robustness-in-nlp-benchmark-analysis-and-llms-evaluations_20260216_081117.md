---
ver: rpa2
title: 'Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and
  LLMs Evaluations'
arxiv_id: '2306.04618'
source_url: https://arxiv.org/abs/2306.04618
tags:
- dataset
- datasets
- performance
- proceedings
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of inadequate out-of-distribution
  (OOD) robustness evaluation in natural language processing (NLP). The authors propose
  a benchmark construction protocol to ensure clear differentiation and challenging
  distribution shifts.
---

# Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations

## Quick Facts
- arXiv ID: 2306.04618
- Source URL: https://arxiv.org/abs/2306.04618
- Authors: 
- Reference count: 40
- Primary result: Vanilla fine-tuning remains a strong baseline, domain-specific models outperform LLMs on ID examples with sufficient data, while LLMs with in-context learning perform better on OOD instances

## Executive Summary
This paper addresses the critical issue of inadequate out-of-distribution (OOD) robustness evaluation in natural language processing by proposing a comprehensive benchmark construction protocol and evaluation framework. The authors introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets with clear distribution shifts. Through extensive experiments, they analyze the relationship between in-distribution (ID) and OOD performance, identifying three distinct correlation patterns. The study provides empirical evidence that vanilla fine-tuning remains competitive, domain-specific fine-tuning outperforms LLMs on ID data when sufficient training data is available, and LLMs with in-context learning show advantages on OOD instances.

## Method Summary
The study constructs the BOSS benchmark following a three-principle protocol ensuring clear differentiation and challenging distribution shifts across 5 NLP tasks. The evaluation analyzes ID-OOD performance correlation through controlled experiments varying model scale, training steps, available samples, and tunable parameters. The study evaluates 5 classic methods and 5 large language models (LLMs) with various adaptation paradigms including zero-shot, in-context learning, few-shot, and full-data approaches. Experiments systematically compare fine-tuned domain-specific models against LLMs across both ID and OOD scenarios, with particular focus on in-context learning effectiveness using ID versus OOD examples.

## Key Results
- Vanilla fine-tuning emerges as a strong baseline for OOD robustness across most tasks
- Fine-tuned domain-specific models significantly outperform LLMs on ID examples when sufficient training data is available
- LLMs with in-context learning from ID examples show better OOD performance compared to fine-tuned domain-specific models
- Three distinct correlation patterns between ID and OOD performance are identified: monotonic linear positive, monotonic piecewise linear positive, and non-monotonic V-shaped

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning from ID examples can improve OOD performance on LLMs
- Mechanism: When provided with examples from the ID training set as context, LLMs can better generalize to OOD data by learning relevant patterns and avoiding overfitting to ID-specific spurious correlations
- Core assumption: The ID examples provided in the context are representative of the underlying task and can help the LLM understand the broader task requirements beyond the ID data
- Evidence anchors:
  - [abstract]: "However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results."
  - [section]: "Takeaway 3: Examples from ID datasets are generally more effective for in-context learning than those from the original training split of the testing OOD dataset."
- Break condition: If the ID examples are not representative of the task or contain strong spurious correlations, in-context learning from ID examples may not improve OOD performance

### Mechanism 2
- Claim: Fine-tuning domain-specific models is superior to LLMs for ID performance when sufficient training data is available
- Mechanism: Domain-specific models are pre-trained on relevant data and fine-tuned on the ID dataset, allowing them to capture task-specific patterns and achieve higher accuracy on ID examples compared to general-purpose LLMs
- Core assumption: The domain-specific pre-training data is relevant to the target task and the ID dataset is large enough to fine-tune the model effectively
- Evidence anchors:
  - [abstract]: "However, when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly."
  - [section]: "Takeaway 1: Fine-tuning small domain-specific models is superior when enough training data is available, while LLMs may be favored in low-resource scenarios."
- Break condition: If the ID dataset is too small or the domain-specific pre-training data is not relevant to the target task, LLMs may outperform fine-tuned domain-specific models on ID examples

### Mechanism 3
- Claim: The correlation between ID and OOD performance can be categorized into three types: monotonic linear positive, monotonic piecewise linear positive, and non-monotonic V-shaped
- Mechanism: The learning dynamics of models during training can lead to different patterns of correlation between ID and OOD performance. In some cases, improvements in ID performance directly translate to improvements in OOD performance. In other cases, there may be a turning point where the relationship changes. And in some cases, there may be an initial negative correlation followed by a positive correlation
- Core assumption: The learning dynamics of models are influenced by factors such as model scale, training steps, available training samples, and tunable parameters, which can lead to different patterns of correlation between ID and OOD performance
- Evidence anchors:
  - [abstract]: "We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets."
  - [section]: "Type I. This is the most prevalent type of correlation observed across all ID-OOD pairs for sentiment analysis, name entity recognition, and the majority for toxic detection. ... Type II. This category is observed on ID-OOD pairs for extractive question answering. ... Type III. The V-shaped fitted lines shown in Figure 1c mainly occurs on ID-OOD pairs of NLI tasks."
- Break condition: If the learning dynamics of models are not influenced by the factors mentioned in the core assumption, the correlation between ID and OOD performance may not follow the three types described

## Foundational Learning

- Concept: Out-of-distribution (OOD) robustness
  - Why needed here: OOD robustness is the main focus of the paper, and understanding it is crucial for interpreting the results and implications of the study
  - Quick check question: What is the difference between in-distribution (ID) and out-of-distribution (OOD) data?

- Concept: Benchmark construction protocol
  - Why needed here: The paper introduces a benchmark construction protocol to ensure clear differentiation and challenging distribution shifts. Understanding this protocol is important for evaluating the validity and generalizability of the study's findings
  - Quick check question: What are the three fundamental principles of the benchmark construction protocol proposed in the paper?

- Concept: Large language models (LLMs)
  - Why needed here: The paper evaluates the performance of LLMs with various adaptation paradigms. Understanding LLMs and their capabilities is essential for interpreting the results and implications of the study
  - Quick check question: What are the key differences between LLMs and traditional pre-trained language models?

## Architecture Onboarding

- Component map: BOSS benchmark -> ID-OOD correlation analysis -> Classic method evaluation -> LLM adaptation paradigm evaluation

- Critical path: Construct BOSS benchmark following the proposed protocol -> Analyze ID-OOD performance correlation on BOSS -> Evaluate robustness-enhanced methods on BOSS -> Evaluate LLMs with various adaptation paradigms on BOSS

- Design tradeoffs: Balancing the challenge and fairness of the OOD benchmarks, choosing the appropriate model architectures and adaptation paradigms for evaluation, ensuring the generalizability of the findings across different tasks and datasets

- Failure signatures: Inadequate challenge in the OOD benchmarks leading to overestimation of model robustness, inconsistent correlation between ID and OOD performance across different tasks and datasets, limited effectiveness of robustness-enhanced methods compared to vanilla fine-tuning, LLMs underperforming on ID examples compared to fine-tuned domain-specific models

- First 3 experiments:
  1. Construct BOSS benchmark following the proposed protocol and evaluate the ID-OOD performance correlation on a subset of tasks and datasets
  2. Evaluate the effectiveness of robustness-enhanced methods on the constructed BOSS benchmark for a specific task
  3. Compare the performance of LLMs with various adaptation paradigms on the BOSS benchmark for a specific task, focusing on the trade-offs between ID and OOD performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically construct adversarial datasets for each task in the BOSS benchmark to ensure clear distribution shifts and challenging evaluation?
- Basis in paper: [explicit] The paper mentions constructing AdvCivil for toxic detection and discusses the need for adversarial datasets in each task
- Why unresolved: While AdvCivil is introduced, the paper does not provide a detailed methodology for creating adversarial datasets across all tasks in BOSS
- What evidence would resolve it: A standardized procedure for generating adversarial samples for each task type, including specific perturbation techniques and validation processes

### Open Question 2
- Question: What is the optimal number and composition of examplars for in-context learning to maximize OOD performance across different LLMs and tasks?
- Basis in paper: [explicit] The paper evaluates LLMs with in-context learning using ID and OOD examples but doesn't explore the optimal number or composition of examplars
- Why unresolved: The study uses a fixed number of examplars (5-shot) without investigating the impact of varying the number or diversity of examples on OOD performance
- What evidence would resolve it: A comprehensive study varying the number and diversity of in-context examplars across tasks and LLM models, measuring their impact on OOD performance

### Open Question 3
- Question: How does the temporal aspect of data affect the performance gap between ID and OOD evaluation, and how can we mitigate potential data contamination issues in LLM evaluations?
- Basis in paper: [explicit] The paper discusses the potential data contamination issue in LLM evaluations and the importance of temporal shifts
- Why unresolved: While acknowledged, the paper doesn't provide a concrete solution or methodology to address data contamination or evaluate the temporal aspect of distribution shifts
- What evidence would resolve it: A systematic approach to curate truly OOD datasets for LLMs, considering temporal aspects and potential contamination, along with evaluation metrics that account for these factors

## Limitations

- The correlation patterns between ID and OOD performance may be sensitive to specific model architectures and training configurations used in the study
- The superiority of fine-tuned domain-specific models over LLMs on ID tasks assumes sufficient and relevant training data, which may not generalize to all domain-specific scenarios
- The effectiveness of in-context learning depends heavily on the quality and representativeness of ID examples used as context, which varies across tasks and datasets

## Confidence

- Claims about fine-tuning superiority: Medium
- Claims about LLM in-context learning advantages: Medium
- Claims about correlation type categorization: Medium
- Claims about benchmark construction protocol effectiveness: High

## Next Checks

1. **Cross-task correlation validation**: Replicate the correlation analysis across additional NLP tasks beyond the five covered in BOSS to verify the generalizability of the three correlation types

2. **Controlled scale ablation study**: Conduct experiments varying only model scale while keeping other hyperparameters constant to isolate the effect of model size on ID-OOD correlation patterns

3. **Temporal stability assessment**: Evaluate whether the observed correlation patterns remain stable across different time periods and dataset versions, addressing potential data shift issues in the underlying datasets