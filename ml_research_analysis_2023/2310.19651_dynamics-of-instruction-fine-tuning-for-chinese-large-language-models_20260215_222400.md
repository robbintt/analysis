---
ver: rpa2
title: Dynamics of Instruction Fine-Tuning for Chinese Large Language Models
arxiv_id: '2310.19651'
source_url: https://arxiv.org/abs/2310.19651
tags:
- data
- arxiv
- abilities
- https
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how different abilities in Chinese LLMs
  develop during instruction tuning, focusing on the impact of data quantity, model
  size, and data construction methods. A new human-curated dataset of 40,000 instruction
  instances across 10 abilities is used to systematically analyze growth patterns.
---

# Dynamics of Instruction Fine-Tuning for Chinese Large Language Models

## Quick Facts
- arXiv ID: 2310.19651
- Source URL: https://arxiv.org/abs/2310.19651
- Authors: 
- Reference count: 40
- Primary result: Investigates how different abilities in Chinese LLMs develop during instruction tuning, finding varying growth patterns across abilities and superior performance of human-curated data over synthetic data.

## Executive Summary
This study systematically investigates the dynamics of instruction tuning for Chinese large language models, focusing on how different abilities develop under varying conditions. Using a new human-curated dataset of 40,000 instruction instances across 10 abilities, the research examines the impact of data quantity, model size, and data construction methods on model performance. The findings reveal that abilities respond differently to scaling factors, with human-curated data proving more efficient than synthetic alternatives, and that instruction tuning enables strong cross-ability generalization.

## Method Summary
The study employs a systematic approach using the LLaMA series models (7b to 33b parameters) with continuous pre-training in Chinese. Researchers constructed a human-curated dataset (DoIT) with over 40,000 instruction instances across 10 distinct abilities. They trained hundreds of model checkpoints with varying data volumes, parameter sizes, and data construction methods (human-curated vs. synthetic from GPT-4). Evaluation used exact-match questions and a semi-automated "comparison with distractors" method for open-ended questions, with validation on public benchmarks CMMLU and AGIEval.

## Key Results
- Different abilities show distinct growth patterns during instruction tuning, with some scaling well with limited data while others like Ethics remain resistant to change
- Human-curated data demonstrates superior efficiency compared to synthetic data from GPT-4, maintaining performance gains with increased volume
- Instruction tuning enables strong cross-ability generalization, with out-of-domain evaluation results mirroring in-domain patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different abilities in LLMs grow at different paces during instruction tuning.
- Mechanism: Each ability responds uniquely to changes in data quantity, parameter size, and data construction method due to its inherent complexity and transferability characteristics.
- Core assumption: Abilities have distinct learning dynamics that are not uniform across the board.
- Evidence anchors:
  - [abstract] "While these factors directly affect overall model performance, some abilities are more responsive to scaling, whereas others demonstrate significant resistance."
  - [section 4.3.1] "Abilities responsive to both factors: As depicted in Fig 3, Code Generation, STEM-Biology, and Humanity-History... show a clear upward trend with the growth of data volume and parameter scale."
  - [corpus] Weak or missing.
- Break condition: If all abilities showed uniform growth patterns regardless of the factor changes, this mechanism would be invalidated.

### Mechanism 2
- Claim: Human-curated data is more efficient than synthetic data from GPT-4 for instruction tuning.
- Mechanism: Human-curated data provides higher quality and more diverse instruction-output pairs that lead to better performance gains and consistent improvement with increased volume.
- Core assumption: The quality and diversity of human-curated data contribute to its superior performance over synthetic data.
- Evidence anchors:
  - [abstract] "Human-curated data is more efficient than synthetic data from GPT-4 and maintains performance gains with increased volume."
  - [section 4.3.2] "Comparing the effectiveness of synthetic and human-curated data, it is evident that the abilities taught by synthetic data are limited, hovering around random scores even with an enlarged data size of 41k."
  - [corpus] Weak or missing.
- Break condition: If synthetic data were to show equal or better performance gains compared to human-curated data, this mechanism would be invalidated.

### Mechanism 3
- Claim: Instruction data promotes powerful cross-ability generalization.
- Mechanism: Models trained on a specific set of abilities can generalize to out-of-domain abilities, with growth characteristics mirroring those of in-domain abilities.
- Core assumption: The skills acquired during instruction tuning are transferable to related but unseen tasks.
- Evidence anchors:
  - [abstract] "Instruction data brings powerful cross-ability generalization, with evaluation results on out-of-domain data mirroring the first two observations."
  - [section 4.3.3] "In Figure 6, scores and growth trends of three out-of-distribution (OOD) abilities illustrate that instruction-tuned models exhibit strong cross-ability generalization on unseen data."
  - [corpus] Weak or missing.
- Break condition: If models failed to generalize to out-of-domain abilities or showed significantly different growth patterns, this mechanism would be invalidated.

## Foundational Learning

- Concept: Data construction methods
  - Why needed here: Different data construction methods (human-curated vs. synthetic) have varying impacts on model performance and learning efficiency.
  - Quick check question: How does the choice between human-curated and synthetic data affect the performance and efficiency of instruction tuning?

- Concept: Ability categorization
  - Why needed here: Categorizing abilities allows for systematic analysis of how each ability responds to different factors.
  - Quick check question: Why is it important to categorize abilities when studying the dynamics of instruction tuning?

- Concept: Cross-ability generalization
  - Why needed here: Understanding how models generalize to unseen abilities is crucial for evaluating the effectiveness of instruction tuning.
  - Quick check question: How does instruction tuning influence a model's ability to perform tasks it was not explicitly trained on?

## Architecture Onboarding

- Component map: Data collection/annotation -> Model training with various configurations -> In-domain evaluation -> Out-of-domain evaluation -> Analysis for data construction strategy
- Critical path: Data curation → Model training → Evaluation → Analysis of results to inform data construction strategies
- Design tradeoffs: Balancing data quality and quantity, choosing between human-curated and synthetic data, deciding on proportions of different ability categories
- Failure signatures: Inconsistent performance across abilities, lack of improvement with increased data volume, poor generalization to out-of-domain tasks
- First 3 experiments:
  1. Train models with varying data quantities on human-curated data to observe ability growth patterns
  2. Compare the performance of models trained on human-curated vs. synthetic data
  3. Evaluate model generalization on out-of-domain abilities to assess cross-ability transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the underlying reason for the 33b model underperforming the 13b model on certain abilities like Chinese and Creative Writing?
- Basis in paper: [explicit] The paper mentions that the 33b model underperforms the 13b model on Chinese and Creative Writing abilities, but does not provide an explanation for this phenomenon.
- Why unresolved: The paper does not explore or hypothesize about the reasons behind this unexpected performance difference between the 33b and 13b models.
- What evidence would resolve it: Further experiments comparing the performance of 33b and 13b models on a wider range of abilities, as well as analysis of the model architectures and training processes, could provide insights into the reasons for this underperformance.

### Open Question 2
- Question: How would extending the data volume beyond 1,000 instances and using process-level evaluations impact the performance of the Chain-of-Thought ability?
- Basis in paper: [inferred] The paper mentions that the Chain-of-Thought ability shows only marginal improvements within the experimental scope, likely due to the high difficulty of grad-math questions and the scoring based solely on exact matches with gold answers. The authors suggest that extending data volume and conducting process-level evaluations may yield further insights.
- Why unresolved: The paper does not explore the potential benefits of extending the data volume and using process-level evaluations for the Chain-of-Thought ability.
- What evidence would resolve it: Experiments comparing the performance of models trained on larger datasets and evaluated using process-level metrics would provide insights into the potential benefits of these approaches for the Chain-of-Thought ability.

### Open Question 3
- Question: What are the potential benefits and limitations of using reinforcement learning from human feedback (RLHF) to enhance abilities like Ethics and Role-play Chat that are resistant to supervised fine-tuning?
- Basis in paper: [explicit] The paper mentions that Ethics and Role-play Chat abilities exhibit stagnant scores across all factor changes, indicating that supervised fine-tuning (SFT) alone may not effectively enhance these abilities. The authors suggest that alternative approaches like RLHF may be necessary for their development.
- Why unresolved: The paper does not investigate the potential benefits and limitations of using RLHF or other alternative approaches to enhance the Ethics and Role-play Chat abilities.
- What evidence would resolve it: Experiments comparing the performance of models trained using RLHF or other alternative approaches on the Ethics and Role-play Chat abilities would provide insights into the potential benefits and limitations of these methods.

## Limitations

- Findings are specific to Chinese language models and may not generalize to other languages or multilingual contexts
- The analysis focuses on a specific set of abilities and may not capture the full complexity of LLM capabilities
- Comparison between human-curated and synthetic data is limited to GPT-4-generated synthetic data, potentially missing performance differences with other synthetic generation methods

## Confidence

**High Confidence:** The observation that different abilities scale differently with data and model size is well-supported by the presented evidence and aligns with prior research on ability-specific learning dynamics in LLMs.

**Medium Confidence:** The claim that human-curated data is more efficient than synthetic data is supported by the evidence, but the comparison is limited to GPT-4-generated synthetic data. The efficiency gains may vary with different synthetic data generation methods.

**Low Confidence:** The assertion of strong cross-ability generalization is based on limited out-of-domain evaluation. The extent and limitations of this generalization are not fully explored, and the results may be influenced by the specific choice of out-of-domain abilities.

## Next Checks

1. **Cross-Lingual Validation:** Evaluate the same instruction tuning dynamics on multilingual or non-Chinese LLMs to assess the generalizability of the findings across languages.

2. **Diverse Synthetic Data Generation:** Compare the efficiency of human-curated data against synthetic data generated by multiple methods (e.g., different prompting strategies, other LLMs) to determine if the observed efficiency gains are consistent across synthetic data sources.

3. **Extended Out-of-Domain Evaluation:** Expand the evaluation of cross-ability generalization to a broader set of out-of-domain abilities and tasks to better understand the scope and limitations of the observed generalization effects.