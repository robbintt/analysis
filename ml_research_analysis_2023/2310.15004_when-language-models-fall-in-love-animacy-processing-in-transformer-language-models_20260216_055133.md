---
ver: rpa2
title: 'When Language Models Fall in Love: Animacy Processing in Transformer Language
  Models'
arxiv_id: '2310.15004'
source_url: https://arxiv.org/abs/2310.15004
tags:
- animacy
- animate
- inanimate
- experiment
- surprisal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformer language models (LMs) process
  animacy, particularly in atypical cases where inanimate objects behave as animate.
  The authors find that while LMs initially struggle with atypical animacy, they can
  adapt their behavior to treat these entities as animate when given sufficient context.
---

# When Language Models Fall in Love: Animacy Processing in Transformer Language Models

## Quick Facts
- arXiv ID: 2310.15004
- Source URL: https://arxiv.org/abs/2310.15004
- Reference count: 30
- Primary result: Transformer LMs can adapt to atypical animacy but show limited human-like processing

## Executive Summary
This paper investigates how transformer language models process animacy, particularly in atypical cases where inanimate objects behave as animate. The authors find that while LMs initially struggle with atypical animacy, they can adapt their behavior to treat these entities as animate when given sufficient context. Even with very short prompts, LMs show some ability to adapt by treating atypically animate entities as animate. Stronger models exhibit better animacy processing, with larger models showing lower surprisal for inanimate entities. The study concludes that despite training only on text, LMs can respond to animacy in surprisingly human-like ways.

## Method Summary
The study uses pre-trained transformer language models (GPT-2 variants, OPT, LLaMA) as test subjects, treating LM surprisal as a proxy for human cognitive effort. Experiments include testing model accuracy on BLiMP animacy datasets, replicating human repetition experiments from Nieuwland and van Berkum (2006), and testing LM responses to short prompts with atypically animate entities. The methodology compares LM surprisal to human N400 brain responses to measure adaptation patterns, using KL divergence between LM output distributions and reference distributions as a metric.

## Key Results
- Transformer LMs show measurable adaptation to atypical animacy when given sufficient context
- Stronger/bigger models perform better on animacy tasks with lower surprisal for inanimate entities
- LMs can partially replicate human-like animacy processing patterns but with limited magnitude compared to humans

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers learn animacy distinctions through selectional constraint violations in text.
- Mechanism: Models adjust surprisal based on how well verb/adjective fits with noun animacy; violations produce higher surprisal.
- Core assumption: Animacy information is sufficiently encoded in selectional patterns within language data.
- Evidence anchors:
  - [abstract] "animacy is not always expressed directly in language: in English it often manifests indirectly, in the form of selectional constraints on verbs and adjectives"
  - [section 2.1] "animacy distinctions in English often take the form of selectional constraints that limit the use of certain verbs or adjectives with in/animate entities"
  - [corpus] Average neighbor FMR=0.421 suggests related papers often discuss selectional constraints and language model behavior
- Break condition: If animacy is marked morphologically in a language rather than through selectional constraints, this mechanism would not apply.

### Mechanism 2
- Claim: Models adapt to atypical animacy through contextual integration over time.
- Mechanism: Initial high surprisal at atypical entity decreases as context accumulates, showing adaptation to new animacy interpretation.
- Core assumption: Transformers can dynamically update their representations based on accumulating context.
- Evidence anchors:
  - [abstract] "even when presented with stories about atypically animate entities, such as a peanut in love, LMs adapt: they treat these entities as animate, though they do not adapt as well as humans"
  - [section 5.1] "by the 3rd and 5th mentions thereof, their N400 responses were so low as to be statistically indistinguishable from the responses to mentions of the animate entity"
  - [corpus] Related papers discuss retrieval augmentation and deeper dives into LM efficacy, suggesting context adaptation is an active research area
- Break condition: If context is too limited or ambiguous, adaptation may fail or be incomplete.

### Mechanism 3
- Claim: Stronger models process animacy more like humans due to better pattern recognition.
- Mechanism: Larger parameter counts and training data allow better capture of subtle animacy cues in language.
- Core assumption: Model size correlates with ability to capture nuanced linguistic patterns.
- Evidence anchors:
  - [abstract] "Stronger models exhibit better animacy processing, with larger models showing lower surprisal for inanimate entities"
  - [section 5.1] "Stronger LMs are more able to replicate the large magnitude of human N400 reduction"
  - [corpus] Weak corpus evidence (max_h_index=None for neighbors) suggests limited external validation of this size-animacy relationship
- Break condition: If model capacity is insufficient for the linguistic complexity of animacy distinctions, performance degrades regardless of size.

## Foundational Learning

- Concept: Selectional restrictions
  - Why needed here: Understanding how verbs and adjectives restrict their arguments based on animacy is fundamental to interpreting the paper's core question about LM processing
  - Quick check question: Can you explain why "The rock walked to the store" sounds odd compared to "The child walked to the store"?

- Concept: N400 brain response
  - Why needed here: The paper uses LM surprisal to model human N400 responses, so understanding what N400 measures is crucial
  - Quick check question: What does an elevated N400 response indicate about semantic processing during language comprehension?

- Concept: Surprisal as cognitive modeling
  - Why needed here: The paper's core methodology compares LM surprisal to human brain responses, requiring understanding of this relationship
  - Quick check question: How does LM surprisal relate to human processing difficulty, and what are the limitations of this comparison?

## Architecture Onboarding

- Component map: Pre-trained transformer LMs (GPT-2, OPT, LLaMA) → BLiMP benchmark → Nieuwland and van Berkum stories → custom short prompt dataset → surprisal computation → human N400 comparison
- Critical path: Train LM → measure surprisal on animacy test sentences → compare to human N400 responses → analyze adaptation patterns
- Design tradeoffs: Using only text-based models limits ecological validity compared to human learning but provides clean experimental control
- Failure signatures: Models show adaptation but lag behind human performance, particularly with limited context or weaker model sizes
- First 3 experiments:
  1. Test LM accuracy on BLiMP animacy datasets to establish baseline performance on typical animacy
  2. Replicate Nieuwland and van Berkum's repetition experiment to measure adaptation to atypical animacy
  3. Test LM responses to short prompts with atypically animate entities to measure adaptation with minimal context

## Open Questions the Paper Calls Out

- How do different linguistic structures or sentence constructions affect the processing of animacy in language models?
- To what extent do larger language models capture the nuances of animacy processing compared to smaller models?
- How do language models process metaphorical animacy, such as when inanimate objects are described with animate characteristics?
- Can language models generalize their understanding of animacy across different contexts and scenarios?
- What are the underlying mechanisms by which language models process animacy, and how do these compare to human cognitive processes?

## Limitations

- The study relies on surrogate data (surprisal scores) to approximate human N400 responses, which may not capture all aspects of human animacy processing
- Adaptation effects shown are limited - models show improvement but don't fully replicate human performance, particularly with shorter contexts
- Focus on English selectional constraints may not generalize to languages with different animacy marking systems
- Use of only text-based models limits ecological validity since humans learn animacy through multimodal experience

## Confidence

**High confidence:** Models show measurable adaptation to atypical animacy when given sufficient context. The finding that stronger/bigger models perform better on animacy tasks is well-supported by the data.

**Medium confidence:** The claim that LMs "can respond to animacy in surprisingly human-like ways" is partially supported but requires qualification - while patterns are similar, the magnitude and completeness of human-like responses is limited.

**Low confidence:** The exact mechanisms by which transformers learn animacy distinctions are not fully characterized, and the claim that selectional constraints are the primary learning mechanism needs further validation across languages.

## Next Checks

1. Test LM performance on languages with morphological animacy marking (e.g., Russian or Swahili) to validate whether selectional constraint learning is the primary mechanism or if other patterns emerge.

2. Conduct ablation studies varying model size and training data quantity to quantify the precise relationship between model capacity and animacy processing capability, moving beyond the general correlation found here.

3. Design experiments with controlled distractor sentences to determine whether LM adaptation to atypical animacy relies on genuine semantic integration or can be explained by pattern-matching on surface features.