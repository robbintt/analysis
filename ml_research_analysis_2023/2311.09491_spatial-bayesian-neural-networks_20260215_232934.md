---
ver: rpa2
title: Spatial Bayesian Neural Networks
arxiv_id: '2311.09491'
source_url: https://arxiv.org/abs/2311.09491
tags:
- process
- spatial
- target
- distribution
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes spatial Bayesian neural networks (SBNNs) as
  a flexible class of spatial process models. SBNNs incorporate a spatial embedding
  layer and possibly spatially-varying network parameters to better capture spatial
  covariances and non-stationary behavior compared to conventional Bayesian neural
  networks.
---

# Spatial Bayesian Neural Networks

## Quick Facts
- arXiv ID: 2311.09491
- Source URL: https://arxiv.org/abs/2311.09491
- Reference count: 11
- One-line primary result: SBNNs match finite-dimensional distributions of target spatial processes through Wasserstein distance minimization

## Executive Summary
This paper introduces spatial Bayesian neural networks (SBNNs) as a flexible framework for spatial modeling that can capture complex spatial processes without requiring explicit parametric assumptions. SBNNs incorporate spatial embedding layers and potentially spatially-varying network parameters to better capture spatial covariances and non-stationary behavior compared to conventional Bayesian neural networks. The key innovation is calibrating SBNNs by matching their finite-dimensional distribution at locations on a fine grid to that of a target process using a Wasserstein distance minimization approach.

## Method Summary
SBNNs extend Bayesian neural networks by adding a spatial embedding layer with radial basis functions and optionally allowing spatially-varying network parameters. The calibration procedure uses a two-stage optimization: first optimizing a neural network to approximate the 1-Lipschitz function for computing Wasserstein distance, then updating the SBNN's hyper-parameters using gradient descent. This approach enables matching the finite-dimensional distribution of the SBNN to that of a target spatial process at grid locations, making SBNNs effective surrogate models for simulation and inference.

## Key Results
- SBNN variants outperform conventional BNNs in matching finite-dimensional distributions of both stationary and non-stationary Gaussian processes
- SBNNs successfully capture spatial covariances and non-stationary behavior through spatial embedding and spatially-varying parameters
- Calibrated SBNNs serve as effective surrogate models for target processes including lognormal spatial processes
- The Wasserstein distance minimization approach enables flexible spatial modeling without requiring explicit parametric assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SBNNs can match finite-dimensional distributions of target spatial processes by calibrating prior distributions through Wasserstein distance minimization
- Mechanism: The calibration procedure adjusts the prior distribution of weights and biases in the SBNN to minimize the Wasserstein-1 distance between the SBNN's finite-dimensional distribution at grid locations and the target process's empirical distribution. This is achieved through a two-stage optimization: first optimizing a neural network to approximate the optimal 1-Lipschitz function for computing the Wasserstein distance, then updating the SBNN's hyper-parameters using gradient descent.
- Core assumption: The Wasserstein distance provides a meaningful measure of dissimilarity between the SBNN's distribution and the target process's distribution, and the approximation using a neural network is sufficiently accurate.
- Evidence anchors:
  - [abstract] "An SBNN is calibrated by matching its finite-dimensional distribution at locations on a fine gridding of space to that of a target process of interest."
  - [section] "Calibration proceeds by minimising this approximate Wasserstein distance. Specifically, we seek ψ∗, where ψ∗ = arg min ψ W1(ψ)."
  - [corpus] No direct evidence for this specific mechanism in corpus papers.
- Break condition: The Wasserstein distance approximation becomes poor, or the optimization gets stuck in local minima that don't correspond to matching the target distribution.

### Mechanism 2
- Claim: Spatial embedding layers and spatially-varying network parameters enable SBNNs to capture complex spatial covariances and non-stationary behavior
- Mechanism: The spatial embedding layer incorporates RBF basis functions centered on a grid, allowing the network to explicitly model spatial proximity. The spatially-varying parameters allow the network's weights and biases to change smoothly across space, capturing non-stationary behavior that would be impossible with spatially-invariant parameters.
- Core assumption: The combination of RBF basis functions and spatially-varying parameters provides sufficient flexibility to model the spatial structure of the target process.
- Evidence anchors:
  - [abstract] "An SBNN leverages the representational capacity of a Bayesian neural network; it is tailored to a spatial setting by incorporating a spatial 'embedding layer' into the network and, possibly, spatially-varying network parameters."
  - [section] "We define f0(s; θ0) ≡ ρ(s; τ) ≡ (ρ1(s; τ), . . . , ρK(s; τ))′, where ρk(· ; τ), k = 1, . . . , K, are K radial basis functions with centroids {ξk ∈ D : k = 1 , . . . , K} that together make up the spatial embedding layer."
  - [corpus] No direct evidence for this specific mechanism in corpus papers.
- Break condition: The RBF basis functions are insufficient to capture the spatial structure, or the smoothness constraints on spatially-varying parameters are too restrictive.

### Mechanism 3
- Claim: The two-stage optimization (inner-loop for Wasserstein metric, outer-loop for hyper-parameters) enables computationally efficient calibration of high-dimensional SBNNs
- Mechanism: By alternating between optimizing the Wasserstein metric (inner-loop) while keeping hyper-parameters fixed, and then updating hyper-parameters (outer-loop) while keeping the Wasserstein metric fixed, the algorithm efficiently navigates the high-dimensional space of SBNN parameters. This approach avoids the computational intractability of directly optimizing over all parameters simultaneously.
- Core assumption: The two-stage optimization converges to a good solution and the Wasserstein metric remains meaningful across iterations.
- Evidence anchors:
  - [section] "We iterate both stages until the Wasserstein distance ceases to notably decrease after several outer-loop optimisation steps."
  - [section] "We only do one step at a time in the outer-loop optimisation stage since the Wasserstein metric needs to be re-established for every new value of ψ."
  - [corpus] No direct evidence for this specific mechanism in corpus papers.
- Break condition: The two-stage optimization fails to converge or converges to poor local minima.

## Foundational Learning

- Concept: Wasserstein distance and its use in distribution matching
  - Why needed here: The calibration procedure relies on minimizing the Wasserstein distance between the SBNN's distribution and the target process's distribution
  - Quick check question: Can you explain the difference between Wasserstein distance and KL divergence, and why Wasserstein might be preferred for this application?

- Concept: Radial basis functions and their role in spatial modeling
  - Why needed here: The spatial embedding layer uses RBF basis functions to incorporate spatial information into the network
  - Quick check question: How do RBF basis functions help capture spatial proximity, and why is this important for modeling spatial processes?

- Concept: Bayesian neural networks and prior specification
  - Why needed here: SBNNs are fundamentally Bayesian neural networks with specific prior distributions over weights and biases
  - Quick check question: Why is the choice of prior distribution particularly important for SBNNs, and how does it differ from standard BNNs?

## Architecture Onboarding

- Component map: Input coordinates -> RBF embedding layer (K basis functions) -> Hidden layers (tanh activation) -> Output layer
- Critical path:
  1. Define target process and grid locations
  2. Initialize SBNN with embedding layer and parameter structure
  3. Generate samples from SBNN and target process
  4. Optimize Wasserstein metric (inner-loop)
  5. Update SBNN hyper-parameters (outer-loop)
  6. Repeat 4-5 until convergence
- Design tradeoffs:
  - Spatially-invariant vs. spatially-varying parameters: More flexibility vs. more hyper-parameters
  - Number of RBF basis functions: Better spatial representation vs. more parameters
  - Network depth and width: More representational capacity vs. harder calibration
  - Sample size N: Better Wasserstein approximation vs. computational cost
- Failure signatures:
  - Poor Wasserstein distance convergence
  - Covariogram mismatch with target process
  - Non-stationary processes poorly captured by spatially-invariant variants
  - Excessive computation time without improvement
- First 3 experiments:
  1. Implement a simple SBNN with 2D input, 1D output, and no embedding layer. Calibrate it to a stationary Gaussian process and verify it fails to capture covariances.
  2. Add an embedding layer with 9 RBF basis functions. Calibrate to the same stationary Gaussian process and verify improved covariance capture.
  3. Implement spatially-varying parameters with 'prior-per-layer' scheme. Calibrate to a non-stationary Gaussian process and verify ability to capture non-stationary covariances.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the results change with different length scale parameters τ for the radial basis functions in the embedding layer?
- Basis in paper: [inferred] The paper states "We did not conduct a detailed experiment to analyse how the results change with τ; provided there is reasonable overlap between the basis functions, as in our case, we do not expect the results to substantially change with τ."
- Why unresolved: The authors did not conduct experiments with varying τ values, so the sensitivity of results to this hyperparameter is unknown.
- What evidence would resolve it: Running the experiments with different τ values and comparing the results to see if and how they change.

### Open Question 2
- Question: How do SBNNs compare to other state-of-the-art spatial modeling approaches for non-Gaussian processes like max-stable processes?
- Basis in paper: [inferred] The paper shows SBNNs can model non-Gaussian processes like lognormal processes, but does not compare to other approaches for more complex processes like max-stable processes.
- Why unresolved: The paper does not provide a comparison to other methods for modeling complex non-Gaussian spatial processes.
- What evidence would resolve it: Comparing SBNNs to other approaches (e.g. max-stable process models) on benchmark datasets or case studies.

### Open Question 3
- Question: How can efficient MCMC algorithms be developed for the SBNN-V variants to facilitate spatial prediction?
- Basis in paper: [explicit] The paper states "Spatial prediction with the calibrated SBNN-VI and the calibrated SBNN-VP is even more onerous, and it would require some alterations to standard algorithms available for BNNs."
- Why unresolved: The authors acknowledge this is a challenge but do not provide a solution.
- What evidence would resolve it: Developing and testing new MCMC algorithms tailored to the SBNN-V architecture that can efficiently sample from the posterior distribution.

## Limitations

- Computational complexity of two-stage optimization may limit scalability to high-dimensional spatial domains
- Limited empirical validation beyond controlled synthetic examples; real-world applications remain untested
- Lack of discussion on uncertainty quantification in calibrated models and their behavior under extrapolation

## Confidence

- **High Confidence**: The theoretical framework for Wasserstein distance minimization and the spatial embedding mechanism are well-established concepts that logically extend to the spatial setting
- **Medium Confidence**: The computational efficiency claims are plausible given the two-stage optimization approach, but would benefit from empirical validation on larger problems
- **Medium Confidence**: The empirical results demonstrating superior performance over conventional BNNs are promising but based on synthetic examples; real-world validation is needed

## Next Checks

1. Apply SBNNs to a real-world spatial dataset (e.g., climate or environmental monitoring data) and compare performance against established spatial models like Gaussian processes with Matérn covariance
2. Conduct scalability experiments by increasing the spatial domain resolution and measuring the computational overhead of the two-stage optimization procedure
3. Perform uncertainty quantification analysis by examining posterior predictive distributions and credible intervals for calibrated SBNNs, particularly for extrapolation scenarios beyond the training domain