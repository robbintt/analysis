---
ver: rpa2
title: Training Data Protection with Compositional Diffusion Models
arxiv_id: '2308.01937'
source_url: https://arxiv.org/abs/2308.01937
tags:
- diffusion
- data
- training
- arxiv
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Compartmentalized Diffusion Models (CDM),
  a method for training multiple diffusion models on different data sources and composing
  them at inference time. The approach enables selective forgetting, continual learning,
  and customized model serving while maintaining performance close to a model trained
  on all data.
---

# Training Data Protection with Compositional Diffusion Models

## Quick Facts
- arXiv ID: 2308.01937
- Source URL: https://arxiv.org/abs/2308.01937
- Authors: 
- Reference count: 40
- One-line primary result: FID scores within 10% of monolithic models for class-conditional generation using compositional diffusion models

## Executive Summary
This paper introduces Compartmentalized Diffusion Models (CDM), a method for training multiple diffusion models on different data sources and composing them at inference time. The approach enables selective forgetting, continual learning, and customized model serving while maintaining performance close to a model trained on all data. Key results include FID scores within 10% of monolithic models for class-conditional generation, 8x faster forgetting, and 14.33% improvement in text-to-image generation alignment. The method uses prompt tuning for efficiency and provides a framework for data protection in large-scale diffusion models.

## Method Summary
The method trains separate diffusion models on distinct data shards and composes them at inference time using a closed-form expression for the reverse diffusion flow of a mixture distribution. A shared U-ViT backbone with shard-specific prompts enables efficient inference, while a classifier-based weight estimation mechanism determines the contribution of each shard to the final output. The approach provides formal guarantees for differential privacy and allows for selective data removal through prompt-based forgetting.

## Key Results
- FID scores within 10% of monolithic models for class-conditional generation
- 8x faster forgetting through prompt-based data removal
- 14.33% improvement in text-to-image generation alignment (TIFA)
- 4.9% improvement in CLIP score for text-to-image generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The compositional diffusion model works by combining the outputs of separately trained diffusion models using a weighted average of their vector fields, where the weights depend on the likelihood that the current noisy sample originated from each model's training distribution.
- Mechanism: Proposition 3.1 derives that the correct composition is vt(x) = Σi λi wi(x,t) v(i)_t(x), where wi(x,t) = p(i)_t(x) / pt(x) is the probability that the current noisy sample xt came from the ith component distribution.
- Core assumption: Each sub-model v(i)_t(x) is a valid flow that satisfies the continuity equation for its own distribution p(i)_t(x), and the mixture forward process is linear.
- Evidence anchors: [abstract], [section] Proposition 3.1 and its derivation in Section 3.2

### Mechanism 2
- Claim: Using prompt tuning (deep prompts) instead of full models for each shard drastically reduces memory and compute cost while maintaining performance.
- Mechanism: The paper attaches learnable prompts to all layers of a pre-trained U-ViT backbone. During inference, prompts from different shards are combined in parallel with the shared backbone.
- Core assumption: The backbone captures general features useful across shards, and the prompts can encode shard-specific information without retraining the whole model.
- Evidence anchors: [abstract], [section] Section 4 describes the architecture with "one set of deep prompts for each class per data distribution"

### Mechanism 3
- Claim: The classifier-based weight estimation provides an efficient way to compute the composition weights wi(x,t) without expensive density estimation.
- Mechanism: A small n-way classifier is trained to predict which shard a noisy sample came from; its softmax outputs approximate the required weights wi(x,t).
- Core assumption: The classifier's predictions p(z=i|x,t) accurately reflect the true posterior p(i)_t(x)/pt(x) at each timestep.
- Evidence anchors: [abstract], [section] Proposition 3.2 and the description in Section 3.3

## Foundational Learning

- Concept: Diffusion models as flow-based generative models
  - Why needed here: The paper's composition method relies on the flow interpretation (continuity equation) rather than just the denoising objective
  - Quick check question: What is the relationship between the vector field vt(x) and the probability path pt(x) in a diffusion model?

- Concept: Linear mixture distributions and their forward processes
  - Why needed here: The derivation in Section 3.2 uses the linearity of the forward process to show that pt(x) = Σi λi p(i)_t(x) for mixture components
  - Quick check question: If p(x) = 0.5*p(birds)(x) + 0.5*p(cars)(x), what is the form of pt(x) for the forward process?

- Concept: Score-based vs flow-based diffusion model interpretations
  - Why needed here: The paper mentions both interpretations and uses the flow-based one for composition; understanding the connection is important
  - Quick check question: How does the continuity equation div(pt(x)vt(x)) = dpt(x)/dt relate to the score function in score-based diffusion models?

## Architecture Onboarding

- Component map:
  Pre-trained U-ViT backbone -> Deep prompts (one set per shard) -> Linear transformation layer (text-to-image) -> Attention-based classifier heads

- Critical path:
  1. Load shared U-ViT backbone
  2. Load prompts for relevant shards
  3. Generate noisy input and timestep embedding
  4. Forward through backbone with prompts
  5. Extract classifier outputs for weight computation
  6. Apply weights to per-shard predictions and combine
  7. Denoise to next timestep

- Design tradeoffs:
  - Full models per shard: highest performance, highest memory/compute
  - Prompt tuning: good performance, low memory/compute, relies on backbone generalization
  - Naive averaging: simplest, but provably incorrect composition (Section 3.2)
  - Classifier-based weights vs direct density estimation: efficient but may be less accurate

- Failure signatures:
  - Poor FID scores with many shards: suggests prompt tuning insufficient for shard differences
  - Unstable or incorrect weights during inference: suggests classifier not well-trained or posterior hard to estimate
  - Memory issues: suggests too many large prompts or inefficient parallelization

- First 3 experiments:
  1. Train prompts for two simple, disjoint datasets (e.g., CUB200 birds and Stanford cars) and verify composition improves over naive averaging
  2. Test weight estimation by visualizing classifier outputs at different timesteps for samples from each shard
  3. Measure memory usage and inference time with 2, 4, and 8 shards to confirm efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of CDMs degrade gracefully as the number of splits increases beyond 8, and what is the theoretical limit of splits before performance becomes unacceptable?
- Basis in paper: [explicit] The paper shows that FID scores increase as the number of splits increases, with 8-splits showing within 10% of monolithic models, but doesn't explore the degradation curve beyond this point.
- Why unresolved: The experiments only tested up to 8-splits, leaving the scalability question unanswered.
- What evidence would resolve it: Systematic experiments testing CDMs with 16, 32, 64+ splits on multiple datasets, measuring both FID and the practical limits of selective forgetting and continual learning.

### Open Question 2
- Question: How does the choice of prompt length and architecture affect the performance and efficiency trade-offs in CDMs across different generation tasks?
- Basis in paper: [explicit] The paper uses different prompt lengths (8 for class-conditional, 64 for text-to-image) but doesn't systematically study the effect of prompt architecture on performance.
- Why unresolved: The paper selects prompt lengths based on parameter count parity rather than optimizing for each task.
- What evidence would resolve it: Ablation studies varying prompt length, depth, and architecture across all generation tasks while measuring FID, inference speed, and storage requirements.

### Open Question 3
- Question: What is the theoretical relationship between the number of shards, data distribution similarity, and the optimal weighting function w(x,t) for CDM composition?
- Basis in paper: [inferred] The paper derives Proposition 3.1 showing the correct weighting function but doesn't provide theoretical analysis of how this function behaves with varying numbers of shards or distribution similarity.
- Why unresolved: The empirical results show that w(x,t) works well in practice, but the theoretical understanding of its behavior in the limit of many shards or similar distributions is lacking.
- What evidence would resolve it: Mathematical analysis proving convergence properties of w(x,t) as shards → ∞, plus empirical validation on synthetic mixtures with controlled similarity metrics.

## Limitations

- The composition mechanism assumes linear mixture distributions and may not generalize to complex, overlapping data distributions
- Prompt-tuning efficiency gains may diminish as shard diversity increases beyond the backbone's generalization capacity
- Differential privacy guarantees depend on implementation details not fully specified in the paper

## Confidence

**High confidence**: The theoretical derivation of the composition mechanism (Proposition 3.1) and the basic prompt-tuning implementation are well-grounded and reproducible.

**Medium confidence**: The claim that CDM matches monolithic model performance within 10% across various tasks, as this depends on specific dataset combinations and implementation details not fully specified.

**Low confidence**: The differential privacy guarantees and their practical effectiveness in real-world data protection scenarios, as these require careful implementation of privacy mechanisms not detailed in the paper.

## Next Checks

1. **Scale and diversity test**: Compose diffusion models trained on increasingly diverse datasets (e.g., adding ImageNet classes to CUB200 and Stanford Cars) and measure FID degradation to determine the practical limits of the composition approach.

2. **Prompt sufficiency analysis**: Systematically vary the size and capacity of prompts relative to the backbone, measuring performance tradeoffs to establish when prompt tuning becomes insufficient for shard-specific features.

3. **Privacy robustness evaluation**: Implement the differential privacy training procedure and test against membership inference attacks to verify that the claimed privacy protection actually holds under adversarial conditions.