---
ver: rpa2
title: A Survey on Fairness in Large Language Models
arxiv_id: '2308.10149'
source_url: https://arxiv.org/abs/2308.10149
tags:
- bias
- llms
- fairness
- proceedings
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of fairness research
  in large language models (LLMs), examining both intrinsic and extrinsic biases.
  The survey categorizes research based on model scale and training paradigms, focusing
  on medium-scale LLMs under pre-training/fine-tuning and large-scale LLMs under prompting.
---

# A Survey on Fairness in Large Language Models

## Quick Facts
- arXiv ID: 2308.10149
- Source URL: https://arxiv.org/abs/2308.10149
- Reference count: 33
- Primary result: Comprehensive survey examining fairness in LLMs, categorizing research by model scale and training paradigms, identifying gaps in evaluation metrics and debiasing strategies

## Executive Summary
This paper provides a comprehensive survey of fairness research in large language models (LLMs), examining both intrinsic and extrinsic biases. The survey categorizes research based on model scale and training paradigms, focusing on medium-scale LLMs under pre-training/fine-tuning and large-scale LLMs under prompting. Key findings include: existing evaluation metrics are insufficient for capturing the complex nature of bias; intrinsic and extrinsic biases are not reliably correlated; and current debiasing strategies are inadequate for large-scale LLMs. The authors highlight the need for more diverse benchmark datasets, improved evaluation methods, and efficient debiasing strategies that consider fairness during the model development phase.

## Method Summary
The survey synthesizes existing research on LLM fairness, organizing it into sections on evaluation metrics, debiasing methods, and fairness considerations for large-scale LLMs. It categorizes fairness research based on model scale (medium vs. large) and training paradigm (pre-training/fine-tuning vs. prompting). The authors review 33 existing studies, identifying gaps in current approaches and proposing future research directions. The survey focuses on both intrinsic bias (in embeddings and representations) and extrinsic bias (in downstream predictions and generated outputs).

## Key Results
- Existing evaluation metrics are insufficient for capturing the complex nature of bias in LLMs
- Intrinsic and extrinsic biases are not reliably correlated, requiring different measurement and mitigation approaches
- Current debiasing strategies are inadequate for large-scale LLMs due to their size, prompting paradigm, and limited accessibility for fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Medium-scale LLMs under pre-training/fine-tuning paradigms exhibit intrinsic bias in embeddings and extrinsic bias in downstream predictions, while large-scale LLMs under prompting paradigm mainly reflect bias in generated outputs.
- Mechanism: The survey divides fairness research based on model scale and training paradigm, allowing for tailored evaluation and debiasing strategies.
- Core assumption: Intrinsic and extrinsic biases are distinct phenomena that require different measurement and mitigation approaches.
- Evidence anchors: [abstract] "First, for medium-scale LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively."
- Break condition: If intrinsic and extrinsic biases are found to be strongly correlated or if the distinction between model scales becomes less relevant.

### Mechanism 2
- Claim: Intrinsic debiasing methods can be categorized into pre-processing, in-processing, and post-processing approaches, each addressing bias at different stages of model development.
- Mechanism: The survey organizes intrinsic debiasing methods based on when they are applied in the model development pipeline, providing a comprehensive overview of available techniques.
- Core assumption: The stage at which bias is addressed affects the effectiveness and efficiency of debiasing methods.
- Evidence anchors: [section] "Intrinsic debiasing, which aims to mitigate the intrinsic bias in the representations before they are applied to downstream tasks, is task-agnostic. Considering the application stage of debiasing techniques, intrinsic debiasing methods can be divided into three categories: pre-processing, in-processing, and post-processing."
- Break condition: If the distinction between pre-processing, in-processing, and post-processing becomes less relevant or if new debiasing stages emerge.

### Mechanism 3
- Claim: Large-scale LLMs present unique challenges in fairness evaluation and debiasing due to their size, prompting paradigm, and limited accessibility for fine-tuning.
- Mechanism: The survey highlights the need for new evaluation methods and debiasing strategies specifically tailored to large-scale LLMs.
- Core assumption: The characteristics of large-scale LLMs necessitate different approaches to fairness research compared to medium-scale LLMs.
- Evidence anchors: [abstract] "Key findings include: existing evaluation metrics are insufficient for capturing the complex nature of bias; intrinsic and extrinsic biases are not reliably correlated; and current debiasing strategies are inadequate for large-scale LLMs."
- Break condition: If large-scale LLMs become more accessible for fine-tuning or if new evaluation methods prove effective for both medium and large-scale models.

## Foundational Learning

- Concept: Intrinsic vs. Extrinsic Bias
  - Why needed here: Understanding the distinction between intrinsic and extrinsic bias is crucial for selecting appropriate evaluation metrics and debiasing strategies.
  - Quick check question: Can you explain the difference between intrinsic bias in embeddings and extrinsic bias in downstream predictions?

- Concept: Pre-processing, In-processing, and Post-processing Debiasing
  - Why needed here: Knowing when to apply different debiasing methods is essential for effective bias mitigation throughout the model development process.
  - Quick check question: Which debiasing approach would you use if you only had access to the pre-trained model and couldn't retrain it?

- Concept: Fairness Evaluation in Large-scale LLMs
  - Why needed here: Large-scale LLMs present unique challenges in fairness evaluation, requiring new methods and benchmark datasets.
  - Quick check question: What are the limitations of current fairness evaluation methods for large-scale LLMs, and how might they be addressed?

## Architecture Onboarding

- Component map: Evaluation metrics (intrinsic and extrinsic) -> Debiasing methods (intrinsic and extrinsic) -> Fairness considerations for large-scale LLMs
- Critical path: Understanding the distinction between medium and large-scale LLMs, their respective biases, and the appropriate evaluation and debiasing strategies
- Design tradeoffs: Balancing the need for comprehensive fairness evaluation with the practical limitations of working with large-scale LLMs
- Failure signatures: Over-reliance on intrinsic metrics for large-scale LLMs, inadequate consideration of the unique challenges posed by the prompting paradigm, or failure to develop diverse benchmark datasets
- First 3 experiments:
  1. Apply a subset of intrinsic and extrinsic evaluation metrics to a medium-scale LLM to understand the differences in bias measurement
  2. Implement a pre-processing debiasing method (e.g., data calibration) on a medium-scale LLM and evaluate its impact on downstream task fairness
  3. Design and conduct a bias evaluation for a large-scale LLM using prompt engineering techniques, comparing results with traditional fine-tuning approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the most effective method to evaluate fairness in large-scale LLMs when the models are not open-source?
- Basis in paper: [explicit] The paper states that for many large-scale LLMs that are not open-source, bias can only be quantified based on the model's response results, and existing methods rely heavily on human judgment.
- Why unresolved: Current evaluation methods are inadequate and resource-intensive, relying on human judgment that may introduce personal bias.
- What evidence would resolve it: Development of automated statistical principles and measurement techniques that can accurately quantify bias without requiring access to model internals or extensive human annotation.

### Open Question 2
- Question: How can we develop comprehensive benchmark datasets specifically for large-scale LLMs that cover diverse tasks and bias types?
- Basis in paper: [explicit] The paper notes that existing datasets like BLOD and Bias-in-Bios were not specifically designed for large-scale LLMs and may not accurately reflect their performance.
- Why unresolved: Current benchmark datasets are limited in scope and task coverage, making it difficult to evaluate the full spectrum of biases in large-scale LLMs.
- What evidence would resolve it: Creation and validation of new benchmark datasets that encompass a wide range of tasks, bias types, and demographic groups specific to large-scale LLM evaluation.

### Open Question 3
- Question: What are the underlying reasons for bias in large-scale LLMs beyond just training data statistics?
- Basis in paper: [explicit] The paper suggests exploring reasons for bias from multiple perspectives including psychological, political, and technical factors, similar to research done on medium-scale LLMs.
- Why unresolved: Current research primarily focuses on data statistics, neglecting other potential sources of bias such as model architecture, training procedures, and external factors.
- What evidence would resolve it: Comprehensive studies investigating the relationship between model size, training procedures, architectural choices, and bias manifestation, combined with psychological and sociological analyses of bias sources.

## Limitations

- The survey relies on existing literature without conducting original experiments, limiting the strength of its conclusions about debiasing effectiveness
- Current evaluation metrics may not fully capture the complex nature of bias, yet the survey still uses these metrics as the basis for its analysis
- The distinction between medium and large-scale LLMs may oversimplify the continuous spectrum of model capabilities

## Confidence

- **High Confidence**: The categorization of fairness research into medium and large-scale LLMs, and the identification of gaps in current evaluation metrics and debiasing strategies
- **Medium Confidence**: The claim that intrinsic and extrinsic biases are not reliably correlated, as this is based on synthesizing existing studies rather than original correlation analysis
- **Low Confidence**: Specific effectiveness claims about debiasing methods, as the survey doesn't conduct comparative experiments or provide quantitative results

## Next Checks

1. Conduct a correlation analysis between intrinsic and extrinsic bias measurements across multiple LLMs to verify the survey's claim about their weak relationship
2. Implement a subset of proposed debiasing methods (one from each category: pre-processing, in-processing, post-processing) on a medium-scale LLM and measure their impact on downstream task fairness
3. Design and execute a fairness evaluation for a large-scale LLM using prompt engineering techniques, comparing results with traditional fine-tuning approaches to assess the survey's claims about large-scale LLM challenges