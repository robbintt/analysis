---
ver: rpa2
title: Theoretical Evaluation of Asymmetric Shapley Values for Root-Cause Analysis
arxiv_id: '2310.09961'
source_url: https://arxiv.org/abs/2310.09961
tags:
- variables
- variance
- interactions
- values
- shap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies Asymmetric Shapley Values (ASV), a variant of
  SHAP that incorporates known causal relationships between variables to improve model
  explanations. The authors analyze ASV theoretically using variance reduction, showing
  that relaxing symmetry in Shapley values can lead to counter-intuitive attributions,
  especially in the presence of complex interactions between variables.
---

# Theoretical Evaluation of Asymmetric Shapley Values for Root-Cause Analysis

## Quick Facts
- arXiv ID: 2310.09961
- Source URL: https://arxiv.org/abs/2310.09961
- Reference count: 40
- Asymmetric Shapley Values can exhibit counter-intuitive attributions when applied to models with complex interactions between variables

## Executive Summary
This paper theoretically evaluates Asymmetric Shapley Values (ASV), a variant of SHAP that incorporates known causal relationships between variables. The authors analyze ASV through a variance reduction framework and identify Generalized Additive Models (GAMs) as a restricted model class where ASV exhibits desirable properties. They prove a sufficient condition for correct behavior when ASV is applied to GAMs and demonstrate through experiments that while ASV is generally effective for root-cause analysis, potential issues should be considered when applying it to models with complex interactions.

## Method Summary
The paper implements ASV using LightGBM with interaction constraints to approximate GAMs, evaluating through variance reduction and comparison with unrestricted models. The method involves training both unrestricted and GAM-restricted models on real-world datasets (Communities and Crime Unnormalized, PM2.5 Data of Five Chinese Cities, Superconductivity, Productivity Prediction of Garment Employees, and a proprietary Mobile Telecommunications dataset), computing ASV contributions, and measuring interactions between variables and feature groups. The variance reduction framework tracks how much variance is reduced when different feature subsets are revealed to quantify each feature's contribution.

## Key Results
- ASV can prioritize distal causes over proximal ones in causal chains by reversing the order of feature revelation
- GAMs effectively mitigate many issues with ASV by restricting complex interactions between variables
- Complex interactions between variables can cause ASV to produce counter-intuitive attributions that violate expected causal ordering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASV can prioritize distal (root) causes over proximal ones in causal chains by reversing the order of feature revelation
- Mechanism: When causal ancestors are revealed earlier in the permutation π, they capture the full contribution of the variance they explain, while descendants only get the residual contribution
- Core assumption: The causal ordering is correctly specified and maintained in the permutation π
- Evidence anchors:
  - [abstract]: "ASV proposes a way to improve model explanations incorporating known causal relations between variables"
  - [section III.B]: "ASV is introduced as a way to prefer certain variables over others by assigning interactions asymmetrically"
  - [corpus]: Weak - corpus mentions "causality-aware" and "global explanations" but lacks specific ASV variance reduction analysis
- Break condition: When the assumed causal ordering is incorrect or when complex interactions violate the additivity assumption

### Mechanism 2
- Claim: Generalized Additive Models (GAMs) restrict complex interactions, making ASV more reliable for root-cause analysis
- Mechanism: By forcing the model to be a sum of univariate functions, GAMs eliminate the kind of complex interactions that cause ASV to misattribute contributions
- Core assumption: The true underlying relationship can be well-approximated by a GAM
- Evidence anchors:
  - [abstract]: "we identify generalized additive models (GAM) as a restricted class for which ASV exhibits desirable properties"
  - [section VI]: "we propose using generalized additive models (GAMs) in an attempt to exclude undesirable complex interactions"
  - [corpus]: Weak - corpus includes GAM-related papers but none specifically about ASV + GAM combinations
- Break condition: When the true relationship contains essential complex interactions that cannot be captured by additive univariate functions

### Mechanism 3
- Claim: Variance reduction framework provides a rigorous way to evaluate ASV behavior and compare it against ground truth
- Mechanism: By tracking how much variance is reduced when different feature subsets are revealed, we can quantify exactly how much contribution each feature deserves
- Core assumption: The model is trained to minimize MSE, making it approximate conditional expectations
- Evidence anchors:
  - [section IV]: "we propose observing the changes in the variance of the target variable T with different feature sets revealed to the model"
  - [section IV]: "we can express the contributions assigned by ASV in terms of variance reduction"
  - [corpus]: Weak - corpus has variance-related papers but not specifically this variance reduction approach to ASV evaluation
- Break condition: When the model is not trained to minimize MSE or when conditional expectations don't capture the true relationship

## Foundational Learning

- Concept: Shapley values and their axioms (efficiency, symmetry, dummy player, additivity)
  - Why needed here: ASV is built on Shapley values but relaxes the symmetry axiom, so understanding the original framework is essential
  - Quick check question: What happens to Shapley value axioms when we relax symmetry, and why might this be useful for causal analysis?

- Concept: Conditional vs marginal expectations in feature attribution
  - Why needed here: The paper uses conditional SHAP throughout, which accounts for feature dependencies differently than marginal SHAP
  - Quick check question: How does the choice between conditional and marginal expectations affect the attribution of shared/redundant information between correlated features?

- Concept: Generalized Additive Models (GAMs) and their restrictions on model expressivity
  - Why needed here: GAMs are proposed as a solution to ASV's problems with complex interactions, so understanding their limitations is crucial
  - Quick check question: What types of relationships can GAMs capture that linear models cannot, and what types of relationships remain inaccessible?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline -> Model training (unrestricted vs GAM-restricted) -> ASV computation with variance tracking -> Analysis and visualization
  Key components: permutation generation, conditional expectation approximation, variance calculation, causal ordering specification

- Critical path:
  1. Prepare data and define causal relationships
  2. Train both unrestricted and GAM-restricted models
  3. Compute ASV contributions using both models
  4. Calculate variance reduction metrics
  5. Analyze differences and identify problematic interactions

- Design tradeoffs:
  - Accuracy vs interpretability: GAMs sacrifice some predictive power for more reliable attributions
  - Computational cost: Computing ASV requires training models for all feature coalitions
  - Causal specification burden: Requires accurate causal graphs from domain experts

- Failure signatures:
  - Large discrepancies between unrestricted and GAM-restricted attributions
  - Positive W_r values indicating complex interactions
  - Contributions that don't align with domain knowledge or expected causal ordering

- First 3 experiments:
  1. Replicate Example B (nonlinearity case) with controlled synthetic data to verify ASV's counter-intuitive behavior
  2. Apply ASV to Communities and Crime dataset with both unrestricted and GAM models, comparing attribution patterns
  3. Test the Telco dataset with known causal structure, examining how well ASV recovers the expected causal ordering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a general method to handle complex interactions in root-cause analysis that works beyond the restricted model class of GAMs?
- Basis in paper: [explicit] The paper identifies Generalized Additive Models (GAMs) as a restricted model class for which ASV exhibits desirable properties, but also acknowledges that some hard-to-interpret interactions can still occur even with the restriction.
- Why unresolved: While GAMs mitigate many issues with ASV, the paper demonstrates that complex interactions can still persist in unusual circumstances, indicating a need for a more general solution.
- What evidence would resolve it: A method that can effectively handle complex interactions across various model classes and demonstrate improved performance compared to GAM-restricted ASV in real-world datasets with complex relationships.

### Open Question 2
- Question: How can we extend the theoretical framework of ASV to classification tasks beyond explaining logits?
- Basis in paper: [explicit] The paper proposes explaining logits instead of probabilities for classification tasks, acknowledging that probabilities are inherently non-additive and explaining them using additive methods like ASV may result in issues similar to Example B.
- Why unresolved: While explaining logits is a practical solution, it doesn't fully address the challenges of applying ASV to classification tasks, especially when dealing with complex interactions between features that affect class probabilities.
- What evidence would resolve it: A theoretical framework that extends ASV to classification tasks while accounting for the non-additivity of probabilities and effectively handling complex interactions between features.

### Open Question 3
- Question: How can we develop a more efficient approach to approximate the restricted conditional expectation Er in practice?
- Basis in paper: [explicit] The paper mentions that approximating Er requires training GAMs, which can be computationally expensive, especially for large datasets and complex models.
- Why unresolved: While the paper provides some methods for approximating Er, such as using interaction constraints in gradient boosting or modifying neural network architectures, these approaches may not be efficient or scalable for all use cases.
- What evidence would resolve it: A more efficient algorithm or approximation method for calculating Er that maintains the desirable properties of ASV while being computationally feasible for large-scale applications.

## Limitations
- The theoretical framework assumes access to true conditional expectations and known causal orderings, which are often unavailable in practice
- Empirical validation relies on synthetic examples and a limited number of real-world datasets
- One proprietary dataset used in experiments cannot be independently verified

## Confidence
- High confidence in the variance reduction framework and its mathematical properties
- Medium confidence in the claim that GAMs effectively mitigate ASV issues, based on limited empirical evidence
- Low confidence in the practical applicability of ASV for root-cause analysis without additional safeguards, given the prevalence of complex interactions in real data

## Next Checks
1. Conduct systematic experiments on synthetic datasets with varying degrees of complex interactions to quantify the relationship between interaction strength and ASV's reliability
2. Develop automated methods to detect when ASV attributions are likely to be unreliable, using metrics like W_r or discrepancies between unrestricted and GAM-restricted models
3. Test ASV with alternative model classes beyond GAMs (e.g., sparse neural networks, decision trees with limited depth) to identify other restricted model classes that might work better for different types of data distributions