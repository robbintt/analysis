---
ver: rpa2
title: 'ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level
  Code Generation'
arxiv_id: '2308.01861'
source_url: https://arxiv.org/abs/2308.01861
tags:
- code
- generation
- class-level
- class
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We make the first attempt to evaluate LLMs on class-level code
  generation, a more challenging scenario than existing method-level code generation.
  We manually construct the first class-level code generation benchmark ClassEval,
  containing 100 tasks and 500 person-hours.
---

# ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation

## Quick Facts
- arXiv ID: 2308.01861
- Source URL: https://arxiv.org/abs/2308.01861
- Reference count: 40
- Primary result: First benchmark for evaluating LLMs on class-level code generation

## Executive Summary
This paper introduces ClassEval, the first manually-crafted benchmark for evaluating large language models (LLMs) on class-level code generation. The benchmark contains 100 Python tasks with 412 methods across various dependency types (field, method, library). The study evaluates 11 state-of-the-art LLMs using three generation strategies (holistic, incremental, compositional) and demonstrates that existing LLMs perform significantly worse on class-level code generation compared to standalone method-level benchmarks like HumanEval. The results show that GPT-4 and GPT-3.5 dominate other models, while method-by-method generation strategies work better for models with limited long-context understanding.

## Method Summary
The researchers manually constructed ClassEval by selecting 100 Python class-level coding tasks from open-source repositories, creating class skeletons with method contracts, developing comprehensive test suites (98%+ statement and branch coverage), and providing canonical solutions. They evaluated 11 LLMs (including GPT-4, GPT-3.5, Code Llama, StarCoder, and others) using three generation strategies: holistic (generate entire class at once), incremental (generate methods sequentially based on previous outputs), and compositional (generate methods independently). The evaluation measured Pass@k metrics at both class and method levels, along with DEP metrics for dependency correctness.

## Key Results
- Existing LLMs exhibit significantly worse performance on class-level code generation compared to method-level benchmarks like HumanEval
- Method-level coding ability does not equivalently reflect class-level coding ability among LLMs
- GPT-4 and GPT-3.5 significantly outperform all other models on both greedy and nucleus sampling
- Method-by-method generation (incremental and compositional) is better for models with limited long-context understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Class-level code generation requires LLMs to manage multiple interdependent methods within a single code artifact, unlike function-level tasks.
- Mechanism: The benchmark ClassEval explicitly tests models on generating classes with 412 methods across 100 tasks, forcing models to handle method dependencies, field access, and cross-method calls simultaneously.
- Core assumption: The dependency complexity in real-world code (as indicated by prior work showing 70%+ methods are dependent) is representative of what models must learn to handle.
- Evidence anchors:
  - [abstract] "We make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e., class-level code generation."
  - [section 2.2] "Previous work [17] indicates that only 30% of methods in open-source projects are independent of other code contexts."
  - [corpus] Weak - no direct citation of dependency complexity studies, but benchmark construction process references this.
- Break Condition: If real-world projects have significantly fewer interdependencies than assumed, or if the benchmark over-represents complex cases.

### Mechanism 2
- Claim: Holistic generation (generating entire class at once) works best only for LLMs with strong instruction-following and long-context handling capabilities.
- Mechanism: GPT-4 and GPT-3.5 significantly outperform other models on holistic generation because they can better understand and utilize the full class skeleton context simultaneously.
- Core assumption: GPT models have superior ability to process long, complex instructions compared to other models.
- Evidence anchors:
  - [section 5.2] "GPT-4 and GPT-3.5 significantly outperform all the other models on solving class-level coding tasks with both greedy sampling and nucleus sampling."
  - [section 4.2] Describes three generation strategies, highlighting holistic vs incremental/compositional differences.
  - [corpus] Weak - no direct comparison of GPT vs other models' long-context processing capabilities.
- Break Condition: If newer models with improved long-context handling emerge, or if GPT models' advantage is specific to this benchmark.

### Mechanism 3
- Claim: Method-by-method generation strategies (incremental/compositional) are better for models with limited long-context understanding because they reduce input complexity.
- Mechanism: By generating methods sequentially or independently, models avoid being overwhelmed by the full class context, leading to better method-level correctness.
- Core assumption: Most studied LLMs have limited capability to handle long input contexts effectively.
- Evidence anchors:
  - [section 5.2] "The main reason is that these models are able to generate much more correct methods...when generating each method in separate iterations compared to generating all methods at once."
  - [section 4.5] "Our experiments are run on a computational infrastructure comprising eight A800-80G GPUs" - indicates resource constraints that may limit context window usage.
  - [corpus] Weak - no direct measurement of input length effects on model performance.
- Break Condition: If models with significantly larger context windows become available, or if input length doesn't impact performance as assumed.

## Foundational Learning

- Concept: Dependency management in object-oriented programming (field dependencies, method dependencies, library dependencies)
  - Why needed here: The benchmark specifically tests models on generating code with diverse dependencies, which is crucial for class-level code generation but not for standalone function generation.
  - Quick check question: What are the three main types of dependencies tested in ClassEval, and why are they important for class-level code generation?

- Concept: Test-driven development and coverage metrics (statement-level and branch-level coverage)
  - Why needed here: The benchmark uses high-coverage test suites (98%+ statement and branch coverage) to validate generated code, requiring understanding of how to write and interpret comprehensive tests.
  - Quick check question: How does ClassEval's test coverage compare to existing benchmarks like HumanEval, and why is this important for reliability?

- Concept: Instruction-following capabilities and prompt engineering for LLMs
  - Why needed here: The study uses different prompts for models with and without instruction-following abilities, requiring understanding of how to effectively communicate tasks to different types of models.
  - Quick check question: What are the key differences in prompt design between LLMs with instruction-following ability and those without, according to the study?

## Architecture Onboarding

- Component map: Class skeletons (containing class-level info and method contracts) -> Generation strategies (holistic, incremental, compositional) -> Test suites (method-level and class-level tests) -> Canonical solutions
- Critical path: Construct class skeleton → Generate code using chosen strategy → Execute test suite → Measure Pass@k and DEP metrics → Analyze error types
- Design tradeoffs: Manual construction (500 person-hours) ensures quality and avoids data leakage but limits scalability. Using greedy vs nucleus sampling trades determinism for exploration of solution space.
- Failure signatures: High rates of AttributeError and TypeError indicate models struggle with understanding syntactic and semantic constraints. KeyError suggests difficulties with dictionary operations and field dependencies.
- First 3 experiments:
  1. Replicate holistic generation results for GPT-4 on ClassEval to verify baseline performance
  2. Compare incremental vs compositional generation for a smaller model (e.g., SantaCoder) to understand strategy preferences
  3. Test a model's ability to generate field-accessing code vs method-invoking code separately to isolate dependency generation challenges

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do large language models (LLMs) perform on class-level code generation compared to standalone method-level code generation, and does method-level coding ability reflect class-level coding ability among LLMs?
- Basis in paper: [explicit] The paper states that existing LLMs show much worse performance on class-level code generation compared to standalone method-level code generation benchmarks like HumanEval, and that method-level coding ability cannot equivalently reflect class-level coding ability among LLMs.
- Why unresolved: This question remains unresolved because the study only compared the performance of LLMs on class-level and method-level code generation tasks, but did not investigate the underlying reasons for the differences in performance or explore ways to improve class-level code generation.
- What evidence would resolve it: Further research is needed to understand the factors that contribute to the lower performance of LLMs on class-level code generation tasks, such as the complexity of generating code that depends on other contexts or the limitations of current LLMs in understanding and utilizing long instructions. Additionally, studies could explore techniques to improve the class-level code generation capabilities of LLMs, such as incorporating context-aware mechanisms or developing specialized training strategies.

### Open Question 2
- Question: How do different generation strategies (holistic, incremental, and compositional) impact the performance of LLMs on class-level code generation tasks?
- Basis in paper: [explicit] The paper investigates the performance of LLMs with three different generation strategies: holistic generation (generating the entire class all at once), incremental generation (generating the class method by method based on previously generated methods), and compositional generation (generating the class method by method independently). The study finds that the best generation strategy varies among different LLMs.
- Why unresolved: This question remains unresolved because the study only compared the performance of LLMs with different generation strategies but did not delve into the underlying reasons for the differences in performance or explore ways to optimize the generation strategies for specific LLMs or code generation tasks.
- What evidence would resolve it: Further research is needed to understand the factors that contribute to the effectiveness of different generation strategies for LLMs on class-level code generation tasks, such as the impact of model size, training data, and task complexity. Additionally, studies could explore techniques to optimize the generation strategies for specific LLMs or code generation tasks, such as incorporating adaptive generation strategies or developing task-specific prompts.

### Open Question 3
- Question: What are the common errors and limitations in the code generated by LLMs for class-level code generation tasks?
- Basis in paper: [explicit] The paper analyzes the errors and limitations in the code generated by LLMs for class-level code generation tasks, finding that most incorrect code encounters AttributeError and TypeError, indicating limited model ability in understanding and satisfying syntactic or semantic constraints in the code context. Additionally, a few cases encounter KeyError due to erroneous operations on dictionary variables.
- Why unresolved: This question remains unresolved because the study only identified the common errors and limitations in the code generated by LLMs but did not investigate the underlying reasons for these errors or explore ways to improve the code generation capabilities of LLMs to reduce these errors.
- What evidence would resolve it: Further research is needed to understand the factors that contribute to the errors and limitations in the code generated by LLMs for class-level code generation tasks, such as the impact of model architecture, training data, and task complexity. Additionally, studies could explore techniques to improve the code generation capabilities of LLMs to reduce these errors, such as incorporating error detection and correction mechanisms or developing specialized training strategies that focus on handling common error types.

## Limitations

- Manual construction of ClassEval required 500 person-hours, limiting scalability and potentially introducing subjective biases
- Benchmark focuses exclusively on Python programming, limiting generalizability to other languages
- Does not explicitly measure model performance on specific dependency types (field vs method dependencies) in isolation

## Confidence

- **High Confidence**: Claims about GPT-4 and GPT-3.5's superior performance on class-level code generation compared to other LLMs
- **Medium Confidence**: Claims about method-by-method generation strategies being superior for models with limited long-context understanding
- **Low Confidence**: Claims about real-world dependency complexity (70%+ interdependent methods) serving as justification for benchmark design

## Next Checks

1. Replicate with expanded scope: Apply ClassEval methodology to evaluate models on at least 5 different programming languages to test language generalizability claims

2. Isolate dependency type performance: Design controlled experiments that separately measure model performance on field-accessing vs method-invoking code to validate the dependency management hypothesis

3. Test prompt engineering impact: Systematically vary prompt complexity and instruction clarity for models with different instruction-following capabilities to quantify the exact impact of prompt design on generation quality