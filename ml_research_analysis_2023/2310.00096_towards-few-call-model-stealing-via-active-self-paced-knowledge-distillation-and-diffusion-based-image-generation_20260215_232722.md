---
ver: rpa2
title: Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation
  and Diffusion-Based Image Generation
arxiv_id: '2310.00096'
source_url: https://arxiv.org/abs/2310.00096
tags:
- data
- learning
- number
- labels
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of replicating black-box classification
  models without access to training data, model architecture, or weights, while limiting
  the number of API calls to the model. The core method generates a synthetic training
  dataset using diffusion models, then employs an active self-paced learning framework
  to train a student model.
---

# Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation and Diffusion-Based Image Generation

## Quick Facts
- arXiv ID: 2310.00096
- Source URL: https://arxiv.org/abs/2310.00096
- Reference count: 40
- Key outcome: Achieves up to 10% accuracy improvements over state-of-the-art methods in few-call scenarios using 32-128 API calls per class

## Executive Summary
This paper addresses the challenge of replicating black-box classification models without access to training data, architecture, or weights while minimizing API calls to the model. The authors propose a novel framework that generates synthetic training data using diffusion models, then employs active learning to select representative samples for teacher queries and self-paced learning to pseudo-label remaining data. The approach is evaluated on CIFAR-10 and Food-101 datasets, demonstrating superior performance compared to existing methods in few-call scenarios.

## Method Summary
The method consists of three stages: first, generating a synthetic proxy dataset using diffusion models (Stable Diffusion v2 or GLIDE) conditioned on class labels; second, applying active learning to select the most informative samples for querying the teacher model based on clustering in the student's latent space; third, using self-paced learning to assign pseudo-labels to unlabeled samples via nearest neighbor procedures in latent space, then training the student model on both teacher-labeled and pseudo-labeled data. The student model (Half-AlexNet or ResNet-18) learns to replicate the teacher's functionality while minimizing API calls.

## Key Results
- Achieves up to 10% accuracy improvements over competing approaches in few-call scenarios
- Demonstrates effectiveness with 32-128 API calls per class on CIFAR-10 and Food-101 datasets
- Shows superior performance compared to state-of-the-art model stealing methods under API call constraints

## Why This Works (Mechanism)

### Mechanism 1
- Diffusion models generate synthetic proxy data that approximates the diversity and realism of the original training set through iterative denoising of random noise into realistic images conditioned on class labels. Core assumption: The diffusion model has sufficient training data for target classes. Evidence: Paper cites diffusion models' success in generating realistic images and their application to various tasks. Break condition: Insufficient training data for target classes leads to unrealistic or non-representative generated images.

### Mechanism 2
- Active self-paced learning selects informative and diverse samples for teacher queries while pseudo-labeling the rest. The method clusters latent vectors per class and samples closer to centroids are more likely to be queried, with remaining samples pseudo-labeled via weighted nearest neighbors. Core assumption: Student's latent space meaningfully captures class structure. Evidence: Clustering approach described for determining relevant samples and self-paced learning procedure using nearest neighbor in latent embedding space. Break condition: Early in training, student's latent representations may be unreliable, making clustering and pseudo-labeling ineffective.

### Mechanism 3
- Joint use of labeled teacher samples and pseudo-labeled samples enables effective knowledge distillation within few-call budget. The student is trained on both teacher-labeled and pseudo-labeled data to maximize use of synthetic dataset. Core assumption: Pseudo-labels are sufficiently accurate to improve student without harmful noise. Evidence: Introduction of strategy assigning pseudo-labels to leftover samples and training on joint dataset. Break condition: Noisy or biased pseudo-labels may cause student to overfit to incorrect patterns.

## Foundational Learning

- Concept: Diffusion model mechanics (forward noising, reverse denoising)
  - Why needed: Pipeline depends on generating realistic proxy images; understanding iterative denoising is key to tuning prompts and assessing image quality
  - Quick check: What is the role of the learned noise schedule in a diffusion model, and how does it affect image realism?

- Concept: Knowledge distillation (teacher-student training with soft/hard labels)
  - Why needed: Core objective is transferring teacher's functionality to student; knowing how temperature scaling, KL divergence, and loss weighting work is essential
  - Quick check: How does using soft labels versus hard labels influence the student's learning trajectory in distillation?

- Concept: Active learning and self-paced learning principles
  - Why needed: Method alternates between querying teacher and pseudo-labeling to maximize limited API calls; understanding sampling strategies and pseudo-label reliability is critical
  - Quick check: What trade-off exists between sample representativeness and diversity in active learning selection?

## Architecture Onboarding

- Component map: Diffusion model -> Student model -> Active learning module -> Self-paced learning module -> Training loop
- Critical path:
  1. Generate proxy dataset with diffusion model
  2. Initialize student and compute latent vectors
  3. Select samples for teacher queries (active learning)
  4. Collect labels from teacher
  5. Train student on labeled samples
  6. Pseudo-label remaining samples (self-paced)
  7. Train student on full proxy dataset
  8. Repeat until API call budget exhausted
- Design tradeoffs:
  - Proxy dataset size vs. diversity: Larger sets increase coverage but may include redundancy
  - Prompt templates: Different wording affects image diversity and quality
  - Sampling probability σ: Controls focus on centroids vs. exploration
  - k nearest neighbors: More neighbors smooth pseudo-labels but risk dilution
  - Distance metric (Euclidean vs. cosine): Impacts pseudo-label accuracy for soft vs. hard labels
- Failure signatures:
  - Poor teacher accuracy: Student accuracy mirrors teacher; check teacher training
  - Unstable student training: Monitor loss; may need learning rate adjustment
  - Pseudo-labels degrade performance: Too many outliers or noisy latent space
  - Limited diversity in proxy images: Check diffusion model training data coverage
- First 3 experiments:
  1. Generate proxy dataset with diffusion model, visualize samples per class, ensure realism
  2. Run student initialization and compute latent vectors for all proxy samples, confirm clustering
  3. Execute one active learning query cycle, verify sample selection and teacher label collection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between the number of diffusion-generated samples and the number of API calls to achieve maximum model extraction accuracy while minimizing detection risk?
- Basis: Paper explicitly discusses trade-off between accuracy and number of model queries but lacks systematic analysis of optimal ratio
- Why unresolved: Focuses on demonstrating effectiveness rather than analyzing optimal balance
- What evidence would resolve it: Empirical studies comparing extraction accuracy across different ratios of synthetic samples to API calls while measuring detection probability

### Open Question 2
- Question: How does the quality of diffusion-generated synthetic data impact the long-term performance of extracted models in real-world applications?
- Basis: Paper demonstrates diffusion models can generate realistic proxy data but doesn't evaluate generalization to real-world scenarios with distribution shifts
- Why unresolved: Evaluation on standard benchmarks where synthetic data matches test set distribution, but real-world applications often involve domain shifts
- What evidence would resolve it: Comparative studies of extracted models trained on diffusion-generated data versus real data when deployed in environments with different data distributions

### Open Question 3
- Question: Can the self-paced learning component be extended to incorporate uncertainty estimation to further improve pseudo-label quality?
- Basis: Paper describes self-paced learning with nearest neighbor pseudo-labeling and acknowledges potential for improvement
- Why unresolved: Current approach uses simple nearest neighbor scheme without confidence or uncertainty measures
- What evidence would resolve it: Experimental results comparing current method with variants incorporating uncertainty estimation (e.g., entropy-based filtering) to demonstrate improvements

## Limitations

- Implementation-specific hyperparameters remain underspecified, including clustering algorithm parameters, sampling probability σ, and k-nearest neighbors count, which likely impact performance significantly
- Latent space quality dependency is fundamental - the entire framework relies on student model's latent space meaningfully capturing class structure, which may be unreliable early in training
- Teacher model quality assumption underlies all improvements - if teacher has inherent weaknesses or biases, student will inherit these limitations regardless of distillation framework sophistication

## Confidence

- High confidence: General framework of using diffusion models for proxy data combined with active learning and self-paced learning is technically sound and well-motivated
- Medium confidence: Specific performance improvements (up to 10% accuracy gains) and superiority over state-of-the-art methods are credible but exact reproducibility depends on unspecified implementation details
- Low confidence: Claim of minimizing API calls is partially supported but lacks rigorous comparison against theoretical limits or alternative optimization strategies

## Next Checks

1. Validate latent space clustering quality by visualizing t-SNE or UMAP projections of student latent vectors for proxy samples, checking whether class clusters form clearly and whether outliers are identifiable
2. Perform ablation studies on the three key components (diffusion-generated proxy data, active learning selection, self-paced pseudo-labeling) to quantify their individual contributions by removing each component in turn
3. Test teacher model sensitivity by evaluating student performance when teacher has varying levels of accuracy (e.g., 70% vs 90% vs 95% top-1 accuracy) to reveal method dependency on teacher quality