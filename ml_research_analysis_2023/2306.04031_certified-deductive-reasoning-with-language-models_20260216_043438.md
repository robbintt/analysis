---
ver: rpa2
title: Certified Deductive Reasoning with Language Models
arxiv_id: '2306.04031'
source_url: https://arxiv.org/abs/2306.04031
tags:
- event
- guide
- reasoning
- prop
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Language models often achieve higher accuracy when reasoning step-by-step,
  but their rationales are often logically unsound or inconsistent. To address this,
  the authors introduce guides: tools that use state and incremental constraints to
  guide language model generation.'
---

# Certified Deductive Reasoning with Language Models

## Quick Facts
- arXiv ID: 2306.04031
- Source URL: https://arxiv.org/abs/2306.04031
- Reference count: 40
- Key outcome: Language models achieve up to 35% accuracy gains on reasoning tasks when using LogicGuide, which enforces sound logical reasoning through formal constraints.

## Executive Summary
Language models often produce incorrect reasoning even when their final answers are correct. LogicGuide addresses this by constraining model generation to only valid logical inferences from formalized assumptions. The approach uses an external theorem prover (Peano) combined with constrained semantic decoding to ensure soundness. Experiments show significant accuracy improvements across multiple datasets while drastically reducing content effects - the interference of prior beliefs with current reasoning tasks.

## Method Summary
LogicGuide is a tool that guides language model generation by constraining outputs to valid logical inferences from formalized assumptions. The system works by having models formalize their assumptions using [[axiom:]] blocks, set goals with [[goal:]] blocks, and then perform [[infer:]] steps constrained by an external theorem prover. The guide uses Constrained Semantic Decoding to ensure the model can only generate tokens representing valid inferences. This approach is tested on PrOntoQA, ProofWriter, Syllogism Validity, and ReClor datasets with GPT-3, GPT-3.5 Turbo, and LLaMA models.

## Key Results
- Accuracy gains up to 35% on PrOntoQA, ProofWriter, and Syllogism Validity datasets
- Drastic reduction in content effects when models formalize assumptions before reasoning
- Successful self-improvement through STaR when training only on certified reasoning traces
- Improved performance on ReClor real-world reasoning dataset

## Why This Works (Mechanism)

### Mechanism 1: Sound reasoning through constraint enforcement
LogicGuide enforces sound reasoning by constraining language model outputs to only valid logical inferences from formalized assumptions. When a model invokes LogicGuide using [[infer:]], the guide queries an external theorem prover for all valid inferences given the current state, and Constrained Semantic Decoding ensures the model can only generate one of these valid inferences, guaranteeing soundness.

### Mechanism 2: Reducing content effects through formalization
LogicGuide reduces content effects by forcing models to reason from formalized assumptions rather than natural language. When models formalize assumptions using [[axiom:]] blocks before reasoning, they must explicitly state their premises, reducing the influence of prior beliefs that typically cause content effects when reasoning directly from natural language descriptions.

### Mechanism 3: Enabling self-improvement through certification
LogicGuide enables self-improvement by allowing models to distinguish certified from uncertified reasoning traces. During STaR self-improvement, LogicGuide can certify which reasoning traces are logically sound by checking if conclusions were formally derived, allowing models to fine-tune only on certified solutions and avoid learning from their own hallucinations.

## Foundational Learning

- **Constrained Semantic Decoding (CSD)**: Why needed - enforces guide constraints by biasing model logits to only generate valid tokens from the guide's allowed set. Quick check: What does CSD do when the model tries to generate a token outside the guide's allowed set?

- **Theorem proving and formal logic**: Why needed - LogicGuide relies on a theorem-proving environment to compute valid logical inferences from formalized assumptions. Quick check: Why is an external theorem-prover necessary rather than having the language model reason directly?

- **Self-Taught Reasoner (STaR)**: Why needed - STaR is the self-improvement method that benefits from LogicGuide's ability to certify reasoning traces. Quick check: How does STaR normally handle cases where correct answers have unsound reasoning?

## Architecture Onboarding

- **Component map**: Language model -> LogicGuide wrapper -> Peano theorem prover; CSD bridges between model and guide; prompts contain [[action:]] blocks for formalization and inference
- **Critical path**: Model generates reasoning problem → formalizes assumptions with [[axiom:]] blocks → sets goal with [[goal:]] → performs [[infer:]] steps constrained by guide → reaches conclusion
- **Design tradeoffs**: Guide adds overhead and complexity vs. pure language model reasoning; requires formal logic expertise vs. natural language flexibility; limits model creativity vs. ensures soundness
- **Failure signatures**: Model gets stuck on [[infer:nothing]] when no valid inferences exist; model misformalizes assumptions leading to dead ends; model fails to prove goal despite correct reasoning
- **First 3 experiments**: 1) Test LogicGuide on simple syllogisms from Syllogism Validity dataset to verify content effect reduction. 2) Run LogicGuide on single-hop problems from ProofWriter to verify basic accuracy improvement. 3) Implement STaR with LogicGuide on PrOntoQA to verify self-improvement benefits.

## Open Questions the Paper Calls Out

### Open Question 1
How can we develop methods to improve the accuracy of language models' formalization of natural language sentences into logical representations? The paper mentions that misformalization of assumptions is a common failure mode, but does not explore techniques to improve formalization accuracy.

### Open Question 2
Can the effectiveness of LogicGuide be improved by incorporating techniques for planning and selecting relevant inferences? The paper suggests that planning techniques are complementary to their work but does not explore their integration.

### Open Question 3
How can we develop methods to detect and correct content effects in language model reasoning? While LogicGuide significantly reduces content effects, the paper does not explore methods to detect and correct them beyond using LogicGuide.

## Limitations

- Implementation details for LogicGuide completion engine and CSD integration are not fully specified
- Claims about STaR self-improvement are based on limited empirical evidence with unclear training details
- Effectiveness depends on models' ability to accurately formalize natural language assumptions into logical representations

## Confidence

- **High Confidence**: The core concept of using formal logic guides to constrain language model reasoning is well-founded and theoretically sound.
- **Medium Confidence**: The accuracy improvements on the tested datasets appear significant, but reproducibility depends heavily on implementation details not fully disclosed.
- **Low Confidence**: The claims about bootstrapping self-improvement and real-world performance on ReClor are based on limited experiments with unclear methodology.

## Next Checks

1. Implement a minimal LogicGuide prototype with Peano theorem prover and test on a small subset of Syllogism Validity problems to verify the core mechanism works as described.

2. Conduct ablation studies comparing accuracy with and without LogicGuide on single-hop reasoning problems to quantify the impact of formalization vs. inference guidance.

3. Test the robustness of LogicGuide by deliberately introducing formalization errors and measuring whether the guide can detect or recover from invalid assumptions.