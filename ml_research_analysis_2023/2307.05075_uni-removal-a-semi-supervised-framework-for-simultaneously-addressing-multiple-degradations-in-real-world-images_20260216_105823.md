---
ver: rpa2
title: 'Uni-Removal: A Semi-Supervised Framework for Simultaneously Addressing Multiple
  Degradations in Real-World Images'
arxiv_id: '2307.05075'
source_url: https://arxiv.org/abs/2307.05075
tags:
- loss
- images
- image
- real-world
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-stage semi-supervised framework, called
  Uni-Removal, for removing multiple degradations (e.g., haze, rain, and blur) from
  real-world images using a unified model and parameters. The first stage uses knowledge
  transfer with a multi-teacher and student architecture, where multiple pre-trained
  teacher networks specialized in different degradation types guide the student network
  to learn through a multi-grained contrastive learning loss.
---

# Uni-Removal: A Semi-Supervised Framework for Simultaneously Addressing Multiple Degradations in Real-World Images

## Quick Facts
- arXiv ID: 2307.05075
- Source URL: https://arxiv.org/abs/2307.05075
- Reference count: 40
- Key outcome: Uni-Removal outperforms state-of-the-art methods for real-world image dehazing, deraining, and deblurring simultaneously, with 14.2 and 5.8 improvements in BRISQUE and PIQE metrics respectively on the SPA dataset.

## Executive Summary
This paper proposes Uni-Removal, a two-stage semi-supervised framework for removing multiple degradations (haze, rain, blur) from real-world images using a unified model. The framework employs knowledge transfer from multiple pre-trained teacher networks through multi-grained contrastive learning, followed by unsupervised fine-tuning on real-world images using extended contrastive learning and adversarial training. Experimental results demonstrate superior performance on real-world datasets compared to both supervised and unsupervised state-of-the-art methods, with significant improvements in no-reference image quality metrics.

## Method Summary
Uni-Removal uses a two-stage semi-supervised framework. In the knowledge transfer stage, multiple pre-trained teacher networks specialized for different degradation types guide a unified student network using multi-grained contrastive learning (MGCL) loss. In the domain adaptation stage, the student network undergoes unsupervised fine-tuning on real-world images using extended multi-grained contrastive learning (EX-MGCL) loss and adversarial training with a discriminator. The framework is trained on synthetic datasets initially and then adapted to real-world data without requiring paired ground truth images.

## Key Results
- Outperforms state-of-the-art supervised methods on real-world datasets for simultaneous dehazing, deraining, and deblurring
- Achieves 14.2 and 5.8 improvements in BRISQUE and PIQE metrics respectively on the SPA dataset
- Demonstrates effectiveness of unified framework compared to task-specific models
- Shows successful domain adaptation from synthetic to real-world images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-grained contrastive learning (MGCL) in the knowledge transfer stage aligns the student network with teacher networks at both feature and image levels, enabling effective multi-degradation learning.
- Mechanism: For each degradation type, MGCL treats teacher features/images as positives and student features/images from other degradation types as negatives, enforcing both similarity to teachers and dissimilarity across degradation types.
- Core assumption: The teacher networks have learned effective degradation-specific representations that can guide the student network.
- Evidence anchors:
  - [abstract] "A multi-grained contrastive loss is introduced to enhance learning from feature and image spaces."
  - [section II] "We propose a MGCL loss in the knowledge transfer stage to enable learning from both positive and negative examples."
  - [corpus] Weak - the corpus doesn't contain direct references to MGCL effectiveness
- Break condition: If teacher networks are poorly trained or the contrastive sampling is not diverse enough, the student may learn incorrect degradation patterns.

### Mechanism 2
- Claim: Extended multi-grained contrastive learning (EX-MGCL) in the domain adaptation stage aligns the student with real-world clear images by treating real clear images as positives and other real degraded images as negatives.
- Mechanism: EX-MGCL replaces the single positive example with a batch of weakly correlated positives (real clear images) to handle the domain gap between synthetic and real-world degradations.
- Core assumption: Real clear images serve as appropriate positive examples despite lacking strong correlation with degraded images.
- Evidence anchors:
  - [abstract] "The integration of an extended multi-grained contrastive loss and generative adversarial loss enables the adaptation of the student network from synthetic to real-world domains."
  - [section II] "Since a strongly correlated positive sample is not available in real-world degradation removal, the extended contrastive loss is used, which replaces the strongly correlated positive with a batch of weakly correlated positives."
  - [corpus] Weak - corpus doesn't provide evidence for EX-MGCL effectiveness
- Break condition: If the batch of real clear images is too diverse or not representative, the contrastive alignment may become ineffective.

### Mechanism 3
- Claim: The two-stage semi-supervised framework bridges the domain gap between synthetic training and real-world application, enabling unified multi-degradation removal.
- Mechanism: First stage (knowledge transfer) trains student on synthetic data with teacher guidance using MGCL; second stage (domain adaptation) fine-tunes on real-world data using EX-MGCL and adversarial loss.
- Core assumption: The domain gap between synthetic and real-world degradations can be effectively reduced through contrastive alignment and adversarial training.
- Evidence anchors:
  - [abstract] "In the knowledge transfer stage... In the domain adaptation stage, unsupervised fine-tuning is performed..."
  - [section II] "Uni-Removal, a two-stage semi-supervised framework for removing multiple degradations from real-world images"
  - [corpus] Weak - corpus doesn't provide evidence for the two-stage framework's effectiveness
- Break condition: If the domain gap is too large or the real-world data is insufficient, the adaptation stage may fail to produce good results.

## Foundational Learning

- Concept: Knowledge distillation
  - Why needed here: Enables student network to learn from multiple specialized teacher networks without inheriting their complexity
  - Quick check question: How does knowledge distillation differ from traditional supervised learning when using teacher-student architecture?

- Concept: Contrastive learning
  - Why needed here: Provides a mechanism to learn representations by comparing positive and negative pairs, crucial for both synthetic-to-synthetic and synthetic-to-real adaptation
  - Quick check question: What is the key difference between image-grained and feature-grained contrastive losses in the context of this framework?

- Concept: Domain adaptation
  - Why needed here: Addresses the performance drop when models trained on synthetic data are applied to real-world images
  - Quick check question: Why is unsupervised domain adaptation particularly important for real-world image enhancement tasks?

## Architecture Onboarding

- Component map:
  - Teacher networks (k specialized networks for each degradation type)
  - Student network (unified architecture shared with teachers)
  - Discriminator (for adversarial training in domain adaptation)
  - VGG-19 feature extractor (for contrastive loss computation)
  - Loss functions (L1 pixel loss, MGCL, EX-MGCL, adversarial loss, identity loss)

- Critical path:
  1. Train k teacher networks on synthetic datasets for each degradation type
  2. Knowledge transfer: Train student using L1 loss + MGCL on synthetic data
  3. Domain adaptation: Fine-tune student using adversarial loss + EX-MGCL on real-world data

- Design tradeoffs:
  - Single unified model vs. multiple specialized models: Unified model reduces parameters but requires more sophisticated training
  - Contrastive learning vs. direct supervision: Contrastive learning handles unlabeled real data but may be slower to converge
  - Feature-grained vs. image-grained losses: Feature-grained provides finer control but may require careful weight scheduling

- Failure signatures:
  - Knowledge transfer stage: Poor PSNR/SSIM on synthetic test sets indicates teacher-student misalignment
  - Domain adaptation stage: High BRISQUE/PIQE scores on real-world data indicates adaptation failure
  - Both stages: Mode collapse in adversarial training or vanishing gradients in contrastive losses

- First 3 experiments:
  1. Train teachers individually on synthetic datasets and evaluate their performance on corresponding test sets
  2. Train student with only pixel-level loss (no contrastive losses) and compare to full knowledge transfer stage
  3. Fine-tune student with only adversarial loss (no EX-MGCL) and evaluate real-world performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MGCL loss compare to other knowledge distillation methods in terms of efficiency and effectiveness for real-world degradation removal?
- Basis in paper: [explicit] The paper introduces the MGCL loss and compares it to other methods, but does not provide a comprehensive comparison of efficiency and effectiveness.
- Why unresolved: The paper focuses on the performance of the MGCL loss within the Uni-Removal framework, but does not explore its efficiency or effectiveness compared to other knowledge distillation methods in the broader context of real-world degradation removal.
- What evidence would resolve it: A detailed comparative study of the MGCL loss with other knowledge distillation methods, evaluating both efficiency (e.g., training time, model size) and effectiveness (e.g., performance on real-world degradation removal tasks) would provide insights into the relative merits of the MGCL loss.

### Open Question 2
- Question: Can the Uni-Removal framework be extended to handle other types of degradations not explicitly mentioned in the paper (e.g., noise, compression artifacts)?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of Uni-Removal on haze, rain, and blur, but does not explore its applicability to other types of degradations.
- Why unresolved: The paper focuses on specific degradations and does not provide a general framework for handling a wide range of degradation types.
- What evidence would resolve it: Experiments applying the Uni-Removal framework to other types of degradations, such as noise or compression artifacts, and evaluating its performance would determine its potential for broader applicability.

### Open Question 3
- Question: How does the performance of Uni-Removal compare to task-specific state-of-the-art methods for each individual degradation type?
- Basis in paper: [explicit] The paper compares Uni-Removal to task-specific methods for each degradation type, but does not provide a comprehensive comparison of overall performance.
- Why unresolved: The paper focuses on the performance of Uni-Removal as a unified framework, but does not explore how it compares to specialized methods for each individual degradation type.
- What evidence would resolve it: A comprehensive comparison of Uni-Removal's performance to task-specific state-of-the-art methods for each degradation type, considering both visual quality and quantitative metrics, would provide insights into the trade-offs between a unified framework and specialized methods.

## Limitations
- The paper relies heavily on non-reference quality metrics (BRISQUE, PIQE) rather than traditional PSNR/SSIM measures for real-world images
- Ablation studies focus on comparing against models trained only on synthetic data, lacking clear baseline for semi-supervised performance
- Implementation details for contrastive learning mechanisms remain unclear, hindering exact reproduction
- Comparative advantages over existing semi-supervised methods are claimed but not thoroughly benchmarked

## Confidence
- **High Confidence**: The two-stage semi-supervised framework architecture and its general training procedure are well-specified and logically sound.
- **Medium Confidence**: The knowledge transfer mechanism using multi-grained contrastive learning is theoretically justified, though implementation details remain unclear.
- **Medium Confidence**: The domain adaptation stage's effectiveness is demonstrated empirically, but the specific mechanisms for bridging the synthetic-to-real domain gap could be more rigorously validated.
- **Low Confidence**: The comparative advantages over existing semi-supervised methods are claimed but not thoroughly benchmarked against relevant alternatives.

## Next Checks
1. Implement the exact contrastive loss calculations (MGCL and EX-MGCL) and verify gradient flow through both feature and image spaces.
2. Train a fully supervised baseline on the same synthetic-to-real adaptation task to establish whether the semi-supervised approach provides meaningful advantages.
3. Test the trained model on additional real-world datasets not seen during training to assess generalization beyond the reported SPA dataset results.