---
ver: rpa2
title: 'MeaeQ: Mount Model Extraction Attacks with Efficient Queries'
arxiv_id: '2310.14047'
source_url: https://arxiv.org/abs/2310.14047
tags:
- uni00000013
- query
- uni00000358
- meaeq
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MeaeQ, a method to improve model extraction
  attacks in NLP by addressing task relevance and data diversity issues. MeaeQ employs
  a zero-shot sequence inference classifier with API service information to filter
  task-relevant data from a public text corpus.
---

# MeaeQ: Mount Model Extraction Attacks with Efficient Queries

## Quick Facts
- **arXiv ID**: 2310.14047
- **Source URL**: https://arxiv.org/abs/2310.14047
- **Reference count**: 22
- **Key outcome**: MeaeQ improves model extraction attacks in NLP by filtering task-relevant queries and reducing redundancy, achieving higher functional similarity to victim models with fewer queries than baselines.

## Executive Summary
This paper addresses critical challenges in model extraction attacks for NLP systems, specifically task relevance and data diversity. MeaeQ introduces a two-stage approach that first filters task-relevant queries from public text corpora using a zero-shot sequence inference classifier, then applies clustering-based data reduction to select representative queries. The method demonstrates consistent improvements across four benchmark datasets and different model architectures, achieving higher agreement and accuracy metrics while requiring fewer queries than existing approaches.

## Method Summary
MeaeQ is a model extraction attack framework for NLP that addresses task relevance and data diversity through a two-stage process. First, it filters task-relevant queries from public text corpora using a zero-shot sequence inference classifier (TRF) that combines NLI model outputs with task-specific prompts. Second, it applies data reduction based on clustering (DRC) to select representative queries by embedding texts, clustering them, and choosing samples closest to cluster centroids. The extracted model is then trained on these filtered and reduced queries along with victim model outputs, achieving higher functional similarity with fewer queries than baseline methods.

## Key Results
- MeaeQ achieves higher functional similarity (Agreement) to victim models than baselines across four benchmark datasets
- The method consistently outperforms random sampling and active learning baselines in both agreement and accuracy metrics
- Improvements are demonstrated across different model architectures and query budgets, with particular effectiveness at lower query budgets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Filtering queries with a zero-shot sequence inference classifier improves task relevance and reduces label imbalance.
- **Mechanism**: The Task Relevance Filter (TRF) uses a pre-trained NLI model to classify whether a premise (text sample) entails a hypothesis (task-specific prompt). Only samples with high entailment probability are retained.
- **Core assumption**: Entailment probability from a zero-shot NLI model correlates with task relevance for the target classification.
- **Evidence anchors**: Abstract states use of zero-shot sequence inference classifier combined with API service information to filter task-relevant data; section describes TRF filtering queries related to target task.
- **Break condition**: If the NLI model's entailment scores poorly align with actual task relevance, the filter will admit irrelevant samples and reject relevant ones, harming performance.

### Mechanism 2
- **Claim**: Clustering-based data reduction selects diverse and informative queries, improving model extraction efficiency.
- **Mechanism**: Data Reduction based on Clustering (DRC) embeds texts, clusters them, and selects the sample closest to each cluster centroid. This maximizes inter-sample distance under the query budget.
- **Core assumption**: Cluster centroids represent diverse and informative data regions, and selecting nearest points yields a good approximation of the maximum distance subset.
- **Evidence anchors**: Abstract mentions clustering-based data reduction to obtain representative data as queries; section describes strategy to achieve approximate maximum distance between candidate samples.
- **Break condition**: If clusters poorly represent the data distribution or centroids are poorly estimated, selected queries may be uninformative or redundant.

### Mechanism 3
- **Claim**: Combining task relevance filtering with data reduction yields better model extraction than either method alone.
- **Mechanism**: TRF narrows the candidate pool to task-relevant samples; DRC then selects the most informative subset within that pool. This two-stage process avoids wasting queries on irrelevant data.
- **Core assumption**: TRF and DRC address complementary problems (relevance vs. redundancy), and their combination is synergistic.
- **Evidence anchors**: Abstract describes MeaeQ as comprising TRF and DRC modules; section shows MeaeQ outperforms two variants particularly in terms of higher agreement.
- **Break condition**: If either module fails (e.g., TRF admits irrelevant samples or DRC selects redundant ones), the combination loses its advantage.

## Foundational Learning

- **Concept**: Natural Language Inference (NLI) and zero-shot classification
  - **Why needed here**: TRF relies on a pre-trained NLI model to filter task-relevant queries without labeled data.
  - **Quick check question**: What is the difference between entailment, contradiction, and neutral in NLI?

- **Concept**: Clustering and centroid-based selection
  - **Why needed here**: DRC uses k-means clustering to group similar queries and select the most representative sample from each cluster.
  - **Quick check question**: How does k-means clustering determine cluster centroids, and why is the nearest point to the centroid selected?

- **Concept**: Cosine similarity and embedding distance
  - **Why needed here**: DRC uses cosine similarity to measure distances between text embeddings and select diverse queries.
  - **Quick check question**: What is the range of cosine similarity, and how does it relate to the angle between vectors?

## Architecture Onboarding

- **Component map**: Corpus (public unannotated text) -> TRF (zero-shot NLI classifier + task-specific prompt) -> DRC (embedding model + clustering algorithm + nearest point selection) -> Extracted model (trained on filtered, reduced queries and victim model outputs)

- **Critical path**:
  1. Sample sentences from corpus → Initialize Qo
  2. TRF filters Qo using NLI model and prompt → Qg
  3. DRC clusters Qg and selects representative samples → Qr
  4. Query API with Qr, collect outputs → Attacker dataset
  5. Train extracted model on attacker dataset

- **Design tradeoffs**:
  - Query budget vs. model performance: Higher budgets allow more diverse queries but increase cost.
  - TRF threshold (ϵ) vs. recall: Higher thresholds yield more relevant queries but may discard useful data.
  - DRC iterations (t) vs. runtime: More iterations improve clustering quality but increase computation time.

- **Failure signatures**:
  - High standard deviation in agreement/accuracy: Indicates noisy or unrepresentative queries.
  - Performance plateau at low query budgets: Suggests TRF or DRC is ineffective.
  - Accuracy much lower than agreement: Indicates the extracted model memorizes the victim's outputs but doesn't generalize.

- **First 3 experiments**:
  1. Verify TRF filters relevant queries: Run TRF on a small corpus, inspect top/bottom samples, and check if they align with task relevance.
  2. Test DRC diversity: Cluster a sample query set, visualize embeddings, and confirm selected samples are well-distributed.
  3. End-to-end extraction: Use MeaeQ on a simple dataset (e.g., SST-2) with a small query budget, and compare agreement/accuracy to random sampling.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does MeaeQ's performance scale with query budgets beyond those tested in the paper?
  - **Basis in paper**: The paper mentions that DRC's effectiveness decreases as query budgets increase, suggesting potential performance limitations at higher budgets.
  - **Why unresolved**: The experiments primarily focus on low query budgets (up to ×0.3 or ×0.02 of the dataset size). Scaling to higher budgets could reveal new insights about MeaeQ's limitations or potential improvements.
  - **What evidence would resolve it**: Additional experiments testing MeaeQ with query budgets of ×0.5, ×1.0, and beyond, comparing its performance to baselines and analyzing the impact on agreement and accuracy metrics.

- **Open Question 2**: How does MeaeQ perform on other NLP tasks beyond text classification, such as named entity recognition or machine translation?
  - **Basis in paper**: The paper explicitly states that the applicability of MeaeQ to other NLP tasks like machine translation or text summarization remains unexplored.
  - **Why unresolved**: The paper focuses solely on text classification tasks, leaving open the question of MeaeQ's effectiveness in other areas of NLP.
  - **What evidence would resolve it**: Applying MeaeQ to various NLP tasks, including named entity recognition, machine translation, and text summarization, and comparing its performance to baselines in these domains.

- **Open Question 3**: What is the impact of different prompt designs in the Task Relevance Filter on MeaeQ's performance?
  - **Basis in paper**: The paper mentions that prompts are designed based on API service information but doesn't explore the impact of different prompt designs.
  - **Why unresolved**: The effectiveness of the Task Relevance Filter relies on the prompt design, but the paper doesn't investigate how different prompt formulations might affect the quality of task-relevant data filtering.
  - **What evidence would resolve it**: Experiments testing various prompt designs for different tasks, measuring their impact on the agreement and accuracy of extracted models, and identifying optimal prompt formulations for different NLP tasks.

## Limitations

- The paper's evaluation focuses on functional similarity metrics rather than direct comparison of extracted model parameters with victim models, limiting understanding of attack depth.
- The zero-shot NLI model's ability to accurately filter task-relevant queries across diverse NLP tasks requires further validation, particularly for tasks where entailment relationships may not cleanly map to classification relevance.
- Claims about MeaeQ's superiority over all existing model extraction approaches should be tempered as the paper doesn't compare against all relevant baselines (e.g., GAN-based approaches, membership inference-based methods).

## Confidence

- **High Confidence**: The core methodology (combining relevance filtering with clustering-based reduction) is well-defined and follows established NLP practices. The experimental design comparing MeaeQ to random sampling and active learning baselines is sound, with consistent improvements across multiple datasets and model architectures.
- **Medium Confidence**: The effectiveness of the zero-shot NLI classifier for task relevance filtering, while theoretically sound, requires more extensive validation across diverse task types. The clustering-based data reduction's impact on extraction efficiency is demonstrated but could benefit from sensitivity analysis on different clustering parameters and distance metrics.
- **Low Confidence**: Claims about MeaeQ's superiority over all existing model extraction approaches should be tempered, as the paper doesn't compare against all relevant baselines (e.g., GAN-based approaches, membership inference-based methods).

## Next Checks

1. **Task Relevance Filter Validation**: Create a small labeled validation set for each task and measure the precision and recall of the TRF module. This will quantify how well the zero-shot NLI classifier aligns with human judgments of task relevance and identify any systematic biases in the filtering process.

2. **Clustering Quality Assessment**: For each dataset, analyze the quality of clusters produced by DRC using silhouette scores and visualize sample distributions with t-SNE plots. Compare the diversity of selected queries against random sampling within the same task-relevant pool to isolate the contribution of the clustering component.

3. **Generalization Across Tasks**: Test MeaeQ on additional NLP tasks not covered in the paper (e.g., relation extraction, named entity recognition) and document any failure modes or performance degradation. This will help identify whether the method's effectiveness is task-dependent or generalizes across the NLP domain.