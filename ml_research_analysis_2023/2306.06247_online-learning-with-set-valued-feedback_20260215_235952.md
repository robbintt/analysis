---
ver: rpa2
title: Online Learning with Set-Valued Feedback
arxiv_id: '2306.06247'
source_url: https://arxiv.org/abs/2306.06247
tags:
- online
- learner
- such
- randomized
- realizable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a new online learning framework where the
  learner predicts a single label but receives a set of correct labels as feedback,
  penalizing only when the predicted label is not in the revealed set. Unlike traditional
  online learning with single-label feedback, this setting exhibits a separation between
  deterministic and randomized learnability, even in the realizable case.
---

# Online Learning with Set-Valued Feedback

## Quick Facts
- arXiv ID: 2306.06247
- Source URL: https://arxiv.org/abs/2306.06247
- Reference count: 3
- This work introduces a new online learning framework where the learner predicts a single label but receives a set of correct labels as feedback, penalizing only when the predicted label is not in the revealed set.

## Executive Summary
This paper establishes a fundamental separation between deterministic and randomized online learnability in a new setting where learners receive set-valued feedback rather than single labels. The authors introduce two new combinatorial dimensions—the Set Littlestone dimension (SLdim) and Measure Shattering dimension (MSdim)—that tightly characterize deterministic and randomized learnability respectively. These dimensions are shown to be tight in both realizable and agnostic settings, with applications to online multilabel ranking with relevance-score feedback and online multilabel classification with ε-insensitive losses.

## Method Summary
The paper introduces a new online learning framework where the learner predicts a single label but receives a set of correct labels as feedback. The 0-1 set loss penalizes only when the predicted label is not in the revealed set. Two algorithms are presented: a deterministic Standard Optimal Algorithm (Algorithm 1) and a randomized Standard Optimal Algorithm (Algorithm 2), both using min-max prediction strategies. The framework characterizes learnability through two new combinatorial dimensions: SLdim for deterministic online learnability and MSdim for randomized online learnability, with the equivalence between deterministic and randomized learnability established when the Helly number of the collection of valid label sets is finite.

## Key Results
- Set-valued feedback creates a fundamental separation between deterministic and randomized online learnability, even in the realizable setting
- The Set Littlestone dimension (SLdim) tightly characterizes deterministic online learnability
- The Measure Shattering dimension (MSdim) tightly characterizes randomized online learnability
- SLdim and MSdim are tight in both realizable and agnostic settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Set-valued feedback fundamentally alters the learnability landscape compared to single-label feedback
- Mechanism: In single-label feedback, deterministic and randomized online learnability are equivalent in the realizable setting due to the Standard Optimal Algorithm. With set-valued feedback, the learner receives multiple correct labels as feedback, creating a richer information structure that can be exploited by randomized algorithms but not by deterministic ones
- Core assumption: The feedback structure allows randomized algorithms to achieve sublinear regret where deterministic algorithms cannot
- Evidence anchors:
  - [abstract]: "deterministic and randomized online learnability are not equivalent even in the realizable setting"
  - [section 4.1]: "there exists a Y, S(Y) and H ⊆ Y^X such that in the realizable setting: (i) H is online learnable (ii) No deterministic algorithm is an online learner for H"
  - [corpus]: Weak evidence - related papers focus on bandit feedback and partial feedback, but don't establish the fundamental equivalence break
- Break condition: When the Helly number of S(Y) is finite, deterministic and randomized learnability become equivalent

### Mechanism 2
- Claim: The Set Littlestone dimension (SLdim) characterizes deterministic online learnability under set-valued feedback
- Mechanism: SLdim extends the classical Littlestone dimension to handle set-valued feedback by defining shattering through X-valued, Y-ary trees where each internal node has Y outgoing edges labeled by sets from S(Y). This captures the temporal dependencies inherent in online learning with set-valued feedback
- Core assumption: The tree-based combinatorial structure can fully capture the online learning complexity
- Evidence anchors:
  - [section 3]: Formal definition of SLdim using Y-ary trees with set-valued edge labeling functions
  - [section 4.2]: "there exists a deterministic online learner which, on any realizable stream, makes at most d mistakes" where d = SLdim(H)
  - [corpus]: Weak evidence - no direct citations to combinatorial dimensions for set-valued feedback
- Break condition: When MSdim < SLdim, randomized algorithms can achieve better regret bounds than deterministic ones

### Mechanism 3
- Claim: The Measure Shattering dimension (MSdim) characterizes randomized online learnability under set-valued feedback
- Mechanism: MSdim is a scale-sensitive dimension defined using X-valued, Π(Y)-ary trees where edges are labeled by probability measures over Y. The shattering condition requires that for every path of measures, there exists a hypothesis whose prediction falls within the labeled sets with probability at least 1-γ
- Core assumption: Randomized algorithms can exploit the measure-theoretic structure of the feedback to achieve better learning bounds
- Evidence anchors:
  - [section 3]: Formal definition of MSdim using Π(Y)-ary trees with measure-based edge labeling
  - [section 4.3]: "There exists a randomized online learner whose expected cumulative loss on any realizable stream is at most inf γ>0 (dγ + γT)"
  - [corpus]: Weak evidence - related work on bandit feedback doesn't establish measure-theoretic characterization
- Break condition: When γ → 0, MSDim approaches SLdim if Y is countable, reducing the gap between deterministic and randomized learnability

## Foundational Learning

- Concept: Online learning framework with set-valued feedback
  - Why needed here: This is the fundamental setting being analyzed - understanding how it differs from standard online learning is crucial
  - Quick check question: What is the key difference between set-valued feedback and standard online multiclass classification feedback?

- Concept: Combinatorial dimensions (Littlestone, SLdim, MSdim)
  - Why needed here: These dimensions provide the mathematical tools to characterize learnability in different settings
  - Quick check question: How does the Set Littlestone dimension differ from the classical Littlestone dimension in terms of tree structure?

- Concept: Helly number and its role in learnability equivalence
  - Why needed here: The Helly number determines when deterministic and randomized learnability become equivalent
  - Quick check question: What property of the collection of valid label sets S(Y) ensures that deterministic and randomized learnability are equivalent?

## Architecture Onboarding

- Component map: Online learning protocol -> Combinatorial dimension definitions -> Algorithmic learners
- Critical path: The core analysis follows this path: define the setting → introduce combinatorial dimensions → prove upper and lower bounds → apply to practical settings
- Design tradeoffs: The framework trades off between computational complexity (min-max predictions in the algorithms) and statistical performance (regret bounds). The scale-sensitive nature of MSdim allows finer-grained analysis but increases complexity
- Failure signatures: If SLdim = MSdim for all γ, the fundamental separation between deterministic and randomized learnability disappears. If the Helly number is infinite, deterministic algorithms cannot match randomized performance even in the realizable setting
- First 3 experiments:
  1. Verify the separation example from Theorem 2 by implementing both deterministic and randomized learners on Y = ℝ, S(Y) = {ℝ \ y : y ∈ ℝ}, H = {constant functions}
  2. Test the SLdim upper bound by implementing Algorithm 1 on a synthetic hypothesis class with known SLdim and measuring mistake count on realizable streams
  3. Validate the MSdim characterization by comparing the expected regret of Algorithm 2 against the theoretical bound inf γ>0 (dγ + γT) on various hypothesis classes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there a practical learning problem beyond multilabel ranking and classification that exhibits a separation between deterministic and randomized learnability under set-valued feedback?
- Basis in paper: [explicit] The authors explicitly state "To the best of our knowledge, this is the ﬁrst online learning problem where there is a qualitative diﬀerence in randomized and deterministic learnability even in the realizable setting" and apply their results to multilabel ranking and classification
- Why unresolved: The paper only provides two specific applications (multilabel ranking and classification), leaving open whether other natural learning problems exhibit this separation
- What evidence would resolve it: A concrete learning problem with set-valued feedback where (1) the hypothesis class is learnable by randomized algorithms but not deterministic ones, and (2) the problem has practical relevance beyond the examples given

### Open Question 2
- Question: Can the Measure Shattering dimension be bounded for specific hypothesis classes beyond what's shown for multilabel problems?
- Basis in paper: [inferred] The paper characterizes learnability via MSdim for general classes but only computes bounds for specific multilabel settings, suggesting broader applicability
- Why unresolved: The paper establishes that MSdim characterizes randomized learnability but doesn't provide general techniques for computing or bounding it for arbitrary hypothesis classes
- What evidence would resolve it: General theorems or techniques that relate MSdim to more familiar combinatorial parameters (like VC dimension, covering numbers, etc.) for broad classes of hypothesis spaces

### Open Question 3
- Question: Does the equivalence between deterministic and randomized learnability under finite Helly number extend to other feedback models beyond set-valued feedback?
- Basis in paper: [explicit] The authors show "deterministic and randomized online learnability are equivalent in the realizable setting if the Helly number of the collection of valid label sets is finite" specifically for their set-valued feedback model
- Why unresolved: The Helly number condition is specific to the set-valued feedback setting; it's unclear whether similar structural conditions could establish equivalence in other feedback models
- What evidence would resolve it: Identification of analogous combinatorial conditions (generalizing Helly number) for other feedback models that guarantee equivalence between deterministic and randomized learnability

## Limitations

- Computational complexity of the min-max prediction strategies in Algorithms 1 and 2 may be intractable for large hypothesis spaces
- Limited empirical validation on real-world multilabel ranking tasks
- The Helly number condition provides a clean theoretical characterization but may be difficult to compute for practical label set collections

## Confidence

- High confidence in the fundamental separation between deterministic and randomized learnability under set-valued feedback
- Medium confidence regarding the practical applicability of the algorithms due to computational complexity concerns
- Low confidence in identifying the Helly number for practical label set collections beyond simple cases

## Next Checks

1. **Empirical Helly Number Verification**: Test the Helly number condition on various practical label set collections (e.g., from real multilabel datasets) to identify when deterministic and randomized learnability become equivalent in practice.

2. **Computational Complexity Analysis**: Implement Algorithm 1 and Algorithm 2 on synthetic hypothesis classes with varying SLdim/MSdim values to measure actual runtime and compare against theoretical complexity bounds.

3. **Real-world Application Testing**: Apply the framework to a standard multilabel ranking dataset (e.g., Delicious or Bibtex) with relevance-score feedback to validate the practical utility of MSdim-based regret bounds against existing methods.