---
ver: rpa2
title: Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical
  Notes
arxiv_id: '2309.00237'
source_url: https://arxiv.org/abs/2309.00237
tags:
- clinical
- notes
- discharge
- synthetic
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Asclepius, a clinical large language model
  trained exclusively on synthetic clinical notes generated from publicly available
  case reports. The authors use GPT-3.5-turbo to convert case reports into synthetic
  discharge summaries and then generate instruction-answer pairs for eight clinical
  NLP tasks.
---

# Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes

## Quick Facts
- arXiv ID: 2309.00237
- Source URL: https://arxiv.org/abs/2309.00237
- Reference count: 40
- One-line primary result: Synthetic clinical notes can effectively substitute real ones for training high-performing clinical language models while preserving privacy

## Executive Summary
This paper introduces Asclepius, a clinical large language model trained exclusively on synthetic clinical notes generated from publicly available case reports. The authors use GPT-3.5-turbo to convert case reports into synthetic discharge summaries and then generate instruction-answer pairs for eight clinical NLP tasks. Asclepius is fine-tuned on these synthetic notes and achieves performance comparable to GPT-3.5-turbo on real clinical notes across multiple benchmarks. When evaluated by GPT-4 and medical professionals, Asclepius shows no significant performance gap compared to a variant trained on real clinical notes, demonstrating that synthetic notes can effectively substitute real ones for training high-performing clinical language models while enabling public sharing of both data and models.

## Method Summary
The authors use GPT-3.5-turbo to transform case reports from the PMC-Patients dataset into synthetic clinical notes, then generate instruction-answer pairs for eight clinical NLP tasks. They pre-train LLaMA on these synthetic notes before fine-tuning on the instruction sets, creating two model variants: Asclepius-7B and Asclepius-13B. The models are evaluated on real clinical notes from MIMIC-III, MIMIC-IV, i2b2, CASI, and DiSCQ datasets using GPT-4 and medical professional assessments. The approach avoids privacy restrictions by using publicly available case reports rather than real clinical notes, enabling open sharing of both data and models.

## Key Results
- Asclepius achieves performance comparable to GPT-3.5-turbo on real clinical notes across multiple benchmarks
- Medical professionals and GPT-4 evaluation show no significant performance gap between Asclepius and a variant trained on real clinical notes
- The model successfully handles diverse clinical NLP tasks including information extraction, relation extraction, and question answering
- Synthetic notes enable privacy-preserving LLM development while maintaining clinical language understanding capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic clinical notes can substitute real ones for training clinical LLMs
- Mechanism: Case reports from PMC-Patients are transformed into synthetic clinical notes that match real note characteristics through prompt engineering with GPT-3.5-turbo
- Core assumption: The transformation preserves clinical content while adopting the format and style of real clinical notes
- Evidence anchors:
  - [abstract] "We first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature"
  - [section 2.1] "we used GPT-3.5 to transform case reports into synthetic clinical notes, giving an instruction to mimic the traits found in real clinical notes"
  - [corpus] Weak - corpus evidence mentions related work on synthetic data but doesn't directly validate this mechanism
- Break condition: Hallucination introduces clinical information not present in source case reports

### Mechanism 2
- Claim: Synthetic notes enable privacy-preserving LLM development while maintaining performance
- Mechanism: Using publicly available case reports avoids privacy restrictions, enabling open sharing of both data and models
- Core assumption: Synthetic notes sufficiently approximate real clinical notes to avoid privacy risks
- Evidence anchors:
  - [abstract] "Unlike real ones, not only enables us to leverage the capabilities of the powerful online LLM... but also allow for the sharing of these resources"
  - [section 2.1] "Our approach does not rely on real clinical notes to create synthetic ones. Instead, we use publicly accessible case reports, which enables us to share the notes publicly"
  - [corpus] Weak - corpus mentions de-identification vs synthetic comparison but doesn't validate this specific approach
- Break condition: Synthetic notes fail to capture essential clinical language patterns

### Mechanism 3
- Claim: Domain adaptation through synthetic note pre-training improves clinical LLM performance
- Mechanism: Pre-training LLaMA on synthetic clinical notes before fine-tuning on clinical instructions helps capture clinical domain patterns
- Core assumption: Synthetic notes contain sufficient clinical language patterns for effective pre-training
- Evidence anchors:
  - [section 3.1] "we applied domain adaptation to LLaMA by pre-training it on synthetic clinical notes before fine-tuning it with clinical instructions"
  - [abstract] "Asclepius-7B and Asclepius-13B, our advanced clinical LLMs capable of handling diverse clinical NLP tasks"
  - [corpus] Weak - corpus doesn't directly address domain adaptation strategy
- Break condition: Synthetic notes lack diversity or complexity of real clinical language

## Foundational Learning

- Concept: Clinical note structure and terminology
  - Why needed here: Understanding differences between case reports and clinical notes is essential for effective transformation
  - Quick check question: What are the key structural differences between case reports and clinical discharge summaries?

- Concept: Instruction tuning methodology
  - Why needed here: Creating effective instruction-answer pairs requires understanding how to frame clinical NLP tasks
  - Quick check question: How does the sequential generation of instructions and answers improve quality compared to simultaneous generation?

- Concept: Evaluation methodology for clinical models
  - Why needed here: Proper evaluation requires understanding clinical accuracy requirements and appropriate scoring criteria
  - Quick check question: Why is GPT-4 evaluation supplemented with medical professional assessment in this work?

## Architecture Onboarding

- Component map: Case reports → Synthetic notes → Instruction-answer pairs → LLaMA → Domain adaptation (synthetic notes) → Instruction fine-tuning → Evaluation
- Critical path: Synthetic note generation → Model pre-training → Instruction fine-tuning → Evaluation
- Design tradeoffs: Synthetic vs real notes (privacy vs authenticity), model size vs performance, sequence length vs memory constraints
- Failure signatures: Hallucination in synthetic notes, domain mismatch in pre-training, instruction generation errors
- First 3 experiments:
  1. Generate synthetic notes from case reports and validate against real clinical note perplexity
  2. Compare model performance with different amounts of synthetic vs real data
  3. Evaluate hallucination detection methods for synthetic note generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance metrics of Asclepius compare when evaluated on clinical note types beyond discharge summaries, such as progress notes or radiology reports?
- Basis in paper: [inferred] The paper explicitly states that the model was "primarily designed and tested only on discharge summaries" and acknowledges this as a limitation, noting it may limit generalizability to other clinical note types.
- Why unresolved: The authors did not conduct experiments on other note types, so the model's effectiveness across diverse clinical documentation remains unknown.
- What evidence would resolve it: Comparative performance evaluations of Asclepius on various clinical note types (progress notes, nursing notes, radiology reports) using the same benchmark tasks would provide clarity on its generalizability.

### Open Question 2
- Question: What is the impact of model hallucination on Asclepius's reliability in clinical decision-making, and how does this compare to GPT-3.5-turbo?
- Basis in paper: [explicit] The authors explicitly state they "did not extensively investigate the model's hallucination capacity" and note this as a critical limitation affecting reliability.
- Why unresolved: The paper does not provide quantitative measures or qualitative assessments of hallucination frequency or severity in Asclepius compared to baseline models.
- What evidence would resolve it: Systematic hallucination analysis using established metrics (e.g., factuality scores, clinical consistency checks) comparing Asclepius to GPT-3.5-turbo on identical clinical queries would quantify this risk.

### Open Question 3
- Question: Can Asclepius maintain or improve its performance when scaled to handle multi-turn conversations typical of clinical consultations?
- Basis in paper: [explicit] The authors state the model "is currently only capable of handling one-turn instruction following tasks" and plan to extend it for interactive dialogues.
- Why unresolved: The paper does not explore or benchmark multi-turn capabilities, leaving uncertainty about the model's effectiveness in dynamic clinical interactions.
- What evidence would resolve it: Performance evaluations of Asclepius on multi-turn clinical dialogue datasets (e.g., medical Q&A with follow-ups) compared to single-turn performance would demonstrate its conversational capabilities.

## Limitations

- Reliance on GPT-3.5-turbo for synthetic note generation limits reproducibility and introduces potential hallucination risks
- Evaluation focused primarily on discharge summaries, limiting generalizability to other clinical note types
- Model currently handles only one-turn instruction following tasks, not multi-turn clinical conversations

## Confidence

- High Confidence: The comparative performance claims between Asclepius and GPT-3.5-turbo on real clinical notes
- Medium Confidence: The privacy-preserving claims regarding publicly available case reports
- Low Confidence: The generalizability claims to all clinical NLP tasks beyond the eight evaluated

## Next Checks

1. **Hallucination Analysis**: Conduct a systematic analysis of synthetic notes to quantify the frequency and severity of hallucinated clinical information, comparing against real clinical notes using medical domain experts.

2. **Cross-Corpus Generalization**: Evaluate Asclepius performance on clinical datasets from different healthcare systems and countries (e.g., non-MIMIC datasets) to assess robustness across different clinical documentation styles and practices.

3. **Long-term Performance Stability**: Test the model's performance consistency over time by evaluating on clinical notes from different years to ensure synthetic pre-training doesn't introduce temporal bias or outdated medical knowledge.