---
ver: rpa2
title: 'CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing'
arxiv_id: '2305.11738'
source_url: https://arxiv.org/abs/2305.11738
tags:
- answer
- question
- evidence
- critic
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CRITIC, a framework that enables frozen
  large language models to self-verify and self-correct their outputs by interacting
  with external tools like search engines and code interpreters. The method iteratively
  checks outputs using tool-based critiques and revises them, improving accuracy across
  diverse tasks: free-form QA (up to 12.4 F1 points), mathematical program synthesis
  (up to 11.4% accuracy gain), and toxicity reduction (79.2% reduction in toxicity).'
---

# CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing

## Quick Facts
- arXiv ID: 2305.11738
- Source URL: https://arxiv.org/abs/2305.11738
- Reference count: 40
- Improves LLM self-correction across diverse tasks using tool interaction

## Executive Summary
This paper introduces CRITIC, a framework that enables frozen large language models to self-verify and self-correct their outputs by interacting with external tools like search engines and code interpreters. The method iteratively checks outputs using tool-based critiques and revises them, improving accuracy across diverse tasks: free-form QA (up to 12.4 F1 points), mathematical program synthesis (up to 11.4% accuracy gain), and toxicity reduction (79.2% reduction in toxicity). Experiments reveal that LLMs struggle to reliably self-verify, underscoring the importance of external feedback for consistent improvement. CRITIC advances trustworthy AI by combining tool interaction with in-context learning for continual self-improvement.

## Method Summary
CRITIC uses an iterative verify-then-correct loop where an LLM generates initial outputs, interacts with external tools to verify those outputs, and then generates corrections based on tool-based critiques. The framework employs in-context learning with few-shot demonstrations to teach LLMs how to use tools without additional training. External tools include search engines, code interpreters, and APIs that provide objective information for verification. The process continues until specific stopping criteria are met, allowing for progressive refinement of outputs across diverse tasks including QA, mathematical problem solving, and toxicity reduction.

## Key Results
- Improves free-form QA performance by up to 12.4 F1 points
- Achieves up to 11.4% accuracy gain in mathematical program synthesis
- Reduces toxicity by 79.2% in text generation tasks

## Why This Works (Mechanism)

### Mechanism 1
Tool interaction provides external feedback that is more reliable than LLM self-evaluation for correcting outputs. LLMs generate critiques by interacting with external tools (e.g., search engines, code interpreters) which provide objective, verifiable information. These critiques are then used to guide corrections. Core assumption: External tools provide accurate and relevant information that the LLM can interpret correctly. Evidence anchors: [abstract]: "Experiments reveal that LLMs struggle to reliably self-verify, underscoring the importance of external feedback for consistent improvement." [section 3.3]: "our proposed CRITIC significantly improves the model's ability to discern facts by incorporating tool interaction, outperforming all previous estimation methods." Break condition: If the tool returns inaccurate or irrelevant information, or if the LLM cannot correctly interpret the tool's output, the mechanism breaks down.

### Mechanism 2
Iterative verify-then-correct cycles enable continuous output improvement. The framework iteratively checks outputs using tool-based critiques and revises them until a stopping condition is met, allowing for progressive refinement. Core assumption: Each iteration provides new information that leads to meaningful improvements. Evidence anchors: [section 4.5]: "Our observations are as follows: 1) Iterative correction generally leads to continuous improvement...2) The marginal benefits of multiple corrections diminish..." [section 3.3]: "We can iterate the process of 'Verify⇒ Correct⇒ Verify' to continuously improve the output until specific stopping criteria are met." Break condition: If iterations stop providing meaningful improvements or if the process becomes unstable (e.g., oscillating between states).

### Mechanism 3
In-context learning with tool interaction is a practical and accessible approach that avoids expensive training. CRITIC uses few-shot in-context learning to teach LLMs to use tools, avoiding the need for task-specific training or large-scale human annotation. Core assumption: LLMs have sufficient in-context learning capabilities to learn tool usage from a few demonstrations. Evidence anchors: [section 2]: "CRITIC avoids task-specific training and employs in-context learning, which is more simple and general." [section 3.2]: "To enable LLMs to use tools, we first construct various external tools...then interleave the LLMs generations with tool use in in-context demonstrations." Break condition: If the LLM fails to learn tool usage from the demonstrations, or if the demonstrations are insufficient.

## Foundational Learning

- Concept: Tool-Augmented Language Models
  - Why needed here: Understanding how tools can enhance LLM capabilities is fundamental to grasping CRITIC's approach.
  - Quick check question: How do tools like search engines or code interpreters improve the fidelity and potency of LLMs?

- Concept: In-Context Learning
  - Why needed here: CRITIC relies on in-context learning to teach LLMs to use tools without additional training.
  - Quick check question: What is the difference between in-context learning and traditional fine-tuning?

- Concept: Iterative Refinement
  - Why needed here: The framework's effectiveness depends on the iterative verify-then-correct process.
  - Quick check question: How does iterative refinement differ from one-shot correction in terms of potential benefits and drawbacks?

## Architecture Onboarding

- Component map: Input → LLM (initial output) → Tool Interaction (verification) → LLM (correction) → Tool Interaction (verification) → ... → Output
- Critical path: Input → LLM → Tool Interaction → LLM (correction) → Output. The most critical part is the tool interaction for generating critiques.
- Design tradeoffs: The framework trades off latency (due to tool interactions) for improved accuracy and reliability. It also relies on the availability and quality of external tools.
- Failure signatures: Poor performance if tools return inaccurate information, LLM fails to interpret tool outputs, or iterative process becomes unstable.
- First 3 experiments:
  1. Implement basic CRITIC for a simple task (e.g., factual QA) with one tool (e.g., search engine).
  2. Compare CRITIC with and without tool interaction on the same task.
  3. Experiment with different stopping criteria for the iterative process.

## Open Questions the Paper Calls Out

### Open Question 1
Question: How can the reliability of LLM self-verification be improved without external tools?
Basis in paper: [inferred] from findings that LLMs struggle to reliably self-verify and that external feedback is crucial for consistent self-improvement.
Why unresolved: The paper shows that self-verification alone is insufficient, but doesn't explore potential methods to improve LLM self-reliability.
What evidence would resolve it: Experiments comparing different self-verification methods (e.g., ensemble approaches, uncertainty calibration) against external tool-based verification.

### Open Question 2
Question: What is the optimal number of iterative corrections needed for different task types?
Basis in paper: [explicit] from observation that marginal benefits diminish after 2-3 rounds and that the optimal number varies by task.
Why unresolved: The paper only provides general guidance on iteration limits without task-specific optimization strategies.
What evidence would resolve it: Task-specific ablation studies measuring performance improvements across different iteration counts.

### Open Question 3
Question: How can tool selection be automated for different inputs rather than being pre-specified?
Basis in paper: [explicit] from discussion of automatic tool selection via in-context learning but implementation using pre-specified tools.
Why unresolved: The paper mentions the possibility but doesn't implement or evaluate automated tool selection.
What evidence would resolve it: Comparison of automated vs. manual tool selection across diverse inputs and tasks.

## Limitations

- Heavy reliance on quality and availability of external tools - mechanism breaks down if tools provide inaccurate information
- Assumes LLMs can reliably interpret tool outputs and generate meaningful critiques
- Iterative nature introduces latency concerns with diminishing returns after multiple corrections

## Confidence

**High Confidence**: The core claim that tool interaction provides more reliable feedback than LLM self-evaluation is well-supported by experimental evidence showing consistent improvements across diverse tasks. The observation that LLMs struggle with self-verification is directly demonstrated in the paper.

**Medium Confidence**: The claim about iterative verify-then-correct cycles enabling continuous improvement has supporting evidence, but the diminishing returns after multiple iterations suggest practical limitations. The specific performance gains (12.4 F1 points, 11.4% accuracy, 79.2% toxicity reduction) are reported but would benefit from independent replication.

**Low Confidence**: The assertion that in-context learning is a universally practical and accessible approach may not hold for all LLMs or all types of tool interactions. The paper doesn't fully explore the learning curve for different tool types or the minimum demonstration requirements for effective learning.

## Next Checks

1. **Tool Reliability Test**: Run CRITIC on a controlled dataset where ground truth tool outputs are known, measuring how often tool inaccuracies propagate into output errors versus cases where the LLM correctly identifies and ignores bad tool information.

2. **Iteration Stability Analysis**: Track the self-correction process across multiple iterations to identify cases where the LLM oscillates between states or makes regressive corrections, quantifying the stability of the iterative process.

3. **Tool Interaction Cost-Benefit Analysis**: Measure the exact latency overhead introduced by tool interactions and compare it against accuracy improvements to determine the practical trade-offs for real-world deployment scenarios.