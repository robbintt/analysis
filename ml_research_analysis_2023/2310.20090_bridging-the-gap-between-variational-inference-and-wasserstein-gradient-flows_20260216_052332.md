---
ver: rpa2
title: Bridging the Gap Between Variational Inference and Wasserstein Gradient Flows
arxiv_id: '2310.20090'
source_url: https://arxiv.org/abs/2310.20090
tags:
- gradient
- variational
- space
- divergence
- path-derivative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a surprising equivalence between two seemingly\
  \ distinct optimization paradigms: black-box variational inference (BBVI) and Wasserstein\
  \ gradient flows. The authors show that under certain conditions\u2014specifically\
  \ with Gaussian variational families and KL divergence\u2014the Bures-Wasserstein\
  \ gradient flow can be recast as a Euclidean gradient flow, where its forward Euler\
  \ scheme corresponds exactly to the standard BBVI algorithm."
---

# Bridging the Gap Between Variational Inference and Wasserstein Gradient Flows

## Quick Facts
- arXiv ID: 2310.20090
- Source URL: https://arxiv.org/abs/2310.20090
- Reference count: 40
- Key outcome: Establishes equivalence between black-box variational inference and Wasserstein gradient flows under Gaussian families

## Executive Summary
This paper reveals a surprising mathematical equivalence between two seemingly distinct optimization paradigms: black-box variational inference (BBVI) and Wasserstein gradient flows. The authors demonstrate that under Gaussian variational families with KL divergence, the Bures-Wasserstein gradient flow can be recast as a Euclidean gradient flow, where its forward Euler scheme corresponds exactly to standard BBVI. This geometric insight arises from a Riemannian submersion that maps Euclidean gradients to Riemannian gradients in the Bures-Wasserstein space. The work further extends these findings to arbitrary f-divergences through a novel "path-derivative gradient estimator" implemented via a distillation procedure, enabling practical applications with modern deep learning libraries.

## Method Summary
The paper develops a theoretical framework connecting BBVI to Wasserstein gradient flows through Riemannian geometry. For Gaussian variational families with KL divergence, it shows that the path-derivative gradient estimator (using stop gradient on parameters) generates the same vector field as the Bures-Wasserstein gradient flow ODE. This equivalence arises from a Riemannian submersion mapping the Euclidean gradient flow in non-singular matrices to the Bures-Wasserstein gradient flow. The authors extend this to f-divergences via a distillation procedure where particles are moved along the Wasserstein gradient flow and projected back to the variational family. The method is evaluated on synthetic distributions (Rosenbrock density, 1D Gaussian mixtures) and Bayesian logistic regression tasks.

## Key Results
- BBVI with path-derivative gradient is mathematically equivalent to forward Euler scheme of Bures-Wasserstein gradient flow under Gaussian families
- Riemannian submersion provides geometric bridge between Euclidean and Bures-Wasserstein geometries
- Novel path-derivative gradient estimator generalizes to arbitrary f-divergences via distillation procedure
- Competitive performance on Bayesian logistic regression with Gaussian mixture variational families

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The path-derivative gradient estimator from BBVI can be reinterpreted as a forward Euler discretization of a Wasserstein gradient flow.
- Mechanism: Under Gaussian variational families and KL divergence, BBVI with reparameterization gradients equals the forward Euler scheme of the Bures-Wasserstein gradient flow. The path-derivative gradient estimator (using stop gradient) generates the same vector field as the Bures-Wasserstein gradient flow ODE.
- Core assumption: Gaussian variational family with mean and scale parameterization (θ = (µ, S)), KL divergence
- Evidence anchors: Abstract states equivalence under Gaussian families and KL divergence; Section 3.2 confirms BBVI equals forward Euler scheme; No neighboring paper evidence found
- Break condition: Non-Gaussian families, non-KL divergences, or different parameterizations

### Mechanism 2
- Claim: Riemannian submersion provides geometric bridge between Euclidean and Bures-Wasserstein geometries.
- Mechanism: Euclidean gradient flow in non-singular matrices (Sn×n) with Frobenius inner product maps to Bures-Wasserstein gradient flow through Riemannian submersion. The horizontal space decomposition ensures Euclidean gradients (horizontal) map to tangent space of Bures-Wasserstein manifold.
- Core assumption: Map π(S) = SS^T is Riemannian submersion between manifolds
- Evidence anchors: Section 3.2.1 establishes connection via Riemannian submersion; Euclidean gradient has no vertical component and maps to tangent space; No neighboring paper evidence found
- Break condition: If π(S) = SS^T fails to be Riemannian submersion or gradients have vertical components

### Mechanism 3
- Claim: Path-derivative gradient estimator generalizes to arbitrary f-divergences through distillation procedure.
- Mechanism: Distillation procedure moves particles along Wasserstein gradient flow vector field and projects back to variational family via Euclidean distance minimization. This yields statistically unbiased gradient estimator for f-divergences implementable with ML libraries.
- Core assumption: Distillation procedure correctly approximates Wasserstein gradient flow marginal with parametric distribution
- Evidence anchors: Abstract describes alternative perspective on path-derivative gradient as distillation; Section 4.2.1 confirms unbiased gradient estimator; No neighboring paper evidence found
- Break condition: If projection step fails to adequately approximate Wasserstein gradient flow marginal

## Foundational Learning

- Concept: Riemannian submersion
  - Why needed here: Establishes geometric connection between Euclidean and Bures-Wasserstein geometries crucial for understanding BBVI's natural Wasserstein geometry
  - Quick check question: What is the key property that makes a map a Riemannian submersion between two manifolds?

- Concept: Wasserstein gradient flows
  - Why needed here: Forms the optimization paradigm over probability measures that BBVI is shown to be equivalent to
  - Quick check question: How does the Wasserstein gradient flow differ from standard gradient descent in Euclidean space?

- Concept: Reparameterization trick and path-derivative gradients
  - Why needed here: The equivalence result depends on path-derivative gradient estimator's treatment of the computational graph
  - Quick check question: What is the difference between path-derivative gradient and standard reparameterization gradient in terms of computational graph treatment?

## Architecture Onboarding

- Component map: KL divergence + Gaussian family → path-derivative gradient → forward Euler scheme → BBVI equivalence; Distillation procedure → generalized gradient estimator for f-divergences

- Critical path: Gaussian variational family with KL divergence enables the geometric interpretation through Riemannian submersion, leading to the path-derivative gradient estimator that implements the forward Euler scheme of the Wasserstein gradient flow

- Design tradeoffs:
  - Gaussian family provides clean geometric interpretation but limits flexibility compared to normalizing flows
  - Path-derivative gradient has lower variance but requires careful handling of stop gradient operator
  - Distillation procedure is more general but computationally more expensive than direct reparameterization

- Failure signatures:
  - Gradient explosion or NaN values: Check stop gradient operator implementation and ensure proper scaling
  - Poor convergence: Verify variational family adequately represents target distribution
  - Incorrect equivalence: Ensure Gaussian parameterization matches specification (µ, S) rather than (µ, Σ)

- First 3 experiments:
  1. Implement 2D Gaussian example from Section 3.3 and verify BBVI with path-derivative gradient and ODE evolution produce identical trajectories
  2. Test distillation procedure on simple f-divergence (e.g., forward KL) with Gaussian family and compare against known results
  3. Extend implementation to Gaussian mixture variational families and evaluate on multimodal target distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does variance of path-derivative gradient estimator behave as variational distribution approaches target distribution?
- Basis in paper: [inferred] Paper mentions path-derivative gradient vanishes when variational distribution closely approximates target, leading to "sticking the landing" property, but lacks detailed variance analysis
- Why unresolved: Paper acknowledges variance analysis of path-derivative gradient remains unresolved matter
- What evidence would resolve it: Empirical studies comparing variance of path-derivative gradient estimator with other estimators under various scenarios where variational distribution approaches target

### Open Question 2
- Question: Can geometric insights from Riemannian submersion framework extend to non-Gaussian variational families?
- Basis in paper: [explicit] Paper discusses Riemannian submersion specifically for Gaussian families and suggests analyzing geometry for general parameter space remains challenging
- Why unresolved: Complexity lies in defining appropriate geometric structures and understanding implications for variational inference
- What evidence would resolve it: Theoretical framework extending Riemannian submersion to other variational families with empirical validation

### Open Question 3
- Question: What are computational trade-offs between path-derivative gradient estimator and other f-divergence gradient estimators?
- Basis in paper: [inferred] Paper introduces path-derivative gradient estimator and compares to existing methods but lacks comprehensive analysis of computational trade-offs
- Why unresolved: Paper presents theoretical foundation and some empirical results but lacks detailed comparison of computational efficiency and accuracy
- What evidence would resolve it: Comparative studies measuring computational resources and accuracy across different models and datasets

## Limitations
- Theoretical equivalence critically depends on Gaussian variational family assumption, limiting practical applicability
- Experiments focused primarily on synthetic distributions and single Bayesian logistic regression task, lacking extensive empirical validation
- Computational overhead of distillation procedure for f-divergences remains unquantified relative to standard approaches

## Confidence
- **High confidence**: Geometric interpretation of BBVI as Riemannian gradient flow over Gaussian distributions is well-supported by mathematical derivations in Section 3.2
- **Medium confidence**: Extension to f-divergences via distillation procedure is theoretically sound but lacks comprehensive empirical validation
- **Low confidence**: Practical advantages over existing VI methods (e.g., normalizing flows) are not thoroughly demonstrated

## Next Checks
1. **Convergence analysis**: Empirically verify claimed equivalence by implementing 2D Gaussian example from Section 3.3 and measuring trajectory convergence rates between BBVI and ODE evolution
2. **Scaling study**: Evaluate computational efficiency of distillation procedure for f-divergences on higher-dimensional problems (e.g., VAEs) and compare against standard reparameterization gradients
3. **Robustness testing**: Assess performance across different f-divergences (reverse KL, forward KL, χ², Hellinger) on multimodal distributions to identify which divergences benefit most from proposed approach