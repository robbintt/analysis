---
ver: rpa2
title: Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical
  Image Segmentation
arxiv_id: '2308.11903'
source_url: https://arxiv.org/abs/2308.11903
tags:
- data
- ssmis
- segmentation
- loss
- dpms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple yet effective approach to boost semi-supervised
  medical image segmentation (SSMIS) performance, dubbed DPMS. The authors argue that
  the key to SSMIS lies in generating substantial and appropriate prediction disagreement
  on unlabeled data.
---

# Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation

## Quick Facts
- arXiv ID: 2308.11903
- Source URL: https://arxiv.org/abs/2308.11903
- Authors: 
- Reference count: 40
- Key outcome: This paper proposes a simple yet effective approach to boost semi-supervised medical image segmentation (SSMIS) performance, dubbed DPMS. The authors argue that the key to SSMIS lies in generating substantial and appropriate prediction disagreement on unlabeled data. To this end, they emphasize the importance of data perturbation and model stabilization in semi-supervised segmentation. DPMS adopts a plain teacher-student framework with a standard supervised loss and unsupervised consistency loss. To produce appropriate prediction disagreements, DPMS perturbs the unlabeled data via strong augmentations to enlarge prediction disagreements considerably. On the other hand, using EMA teacher when strong augmentation is applied does not necessarily improve performance. DPMS further utilizes a forwarding-twice and momentum updating strategies for normalization statistics to stabilize the training on unlabeled data effectively. Despite its simplicity, DPMS can obtain new state-of-the-art performance on the public 2D ACDC and 3D LA datasets across various semi-supervised settings, e.g., obtaining a remarkable 22.62% improvement against previous SOTA on ACDC with 5% labels.

## Executive Summary
This paper introduces DPMS, a simple yet effective approach for semi-supervised medical image segmentation (SSMIS). The authors argue that the key to SSMIS lies in generating substantial and appropriate prediction disagreement on unlabeled data. DPMS adopts a plain teacher-student framework with a standard supervised loss and unsupervised consistency loss. To produce appropriate prediction disagreements, DPMS perturbs the unlabeled data via strong augmentations to enlarge prediction disagreements considerably. On the other hand, using EMA teacher when strong augmentation is applied does not necessarily improve performance. DPMS further utilizes a forwarding-twice and momentum updating strategies for normalization statistics to stabilize the training on unlabeled data effectively. Despite its simplicity, DPMS can obtain new state-of-the-art performance on the public 2D ACDC and 3D LA datasets across various semi-supervised settings, e.g., obtaining a remarkable 22.62% improvement against previous SOTA on ACDC with 5% labels.

## Method Summary
DPMS is a teacher-student framework for semi-supervised medical image segmentation. It uses a standard supervised loss on labeled data and an unsupervised consistency loss on unlabeled data. To produce appropriate prediction disagreements, DPMS perturbs the unlabeled data via strong augmentations. The authors further utilize forwarding-twice and momentum updating strategies for normalization statistics to stabilize the training on unlabeled data effectively.

## Key Results
- DPMS obtains new state-of-the-art performance on the public 2D ACDC and 3D LA datasets across various semi-supervised settings.
- DPMS achieves a remarkable 22.62% improvement against previous SOTA on ACDC with 5% labels.
- The proposed approach outperforms previous SOTA methods on both 2D and 3D medical image segmentation tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Strong data augmentations generate sufficient prediction disagreement to improve SSMIS.
- Mechanism: Applying geometrical transformations, intensity-based augmentations, and copy-paste operations to unlabeled data enlarges the discrepancy between predictions on weakly and strongly augmented views. This disagreement drives the model to learn more robust features by enforcing consistency.
- Core assumption: The original data distribution is preserved under strong augmentations, so the disagreement is label-preserving.
- Evidence anchors:
  - [abstract] "DPMS perturbs the unlabeled data via strong augmentations to enlarge prediction disagreements considerably."
  - [section 3.2.1] "we investigate three popular kinds of data augmentations... In Table 1, we examine the effectiveness of each type of augmentations and their combinations..."
  - [corpus] Weak - only one neighbor (Self-training with dual uncertainty) mentions augmentations but not in the context of strong perturbation.
- Break condition: If augmentations distort the data distribution, the model may learn from incorrect supervision, degrading performance.

### Mechanism 2
- Claim: Updating batch normalization (BN) statistics via exponential moving average (EMA) stabilizes training when strong augmentations are applied.
- Mechanism: By averaging the BN statistics of the student model to update the teacher model's statistics (Equations 4 and 5), the model prevents BN from being severely disturbed by strong augmentations, maintaining stable statistics across training iterations.
- Core assumption: BN statistics drift significantly under strong augmentations, and EMA can mitigate this drift.
- Evidence anchors:
  - [section 3.2.2] "we also updating the batch normalization (BN) statistics via the exponential moving average (EMA) of BN of the student model."
  - [section 3.2.2] "applying EMA-BN can effectively improve the SSMIS performance... Improvements become even more pronounced when strong data augmentation is applied."
  - [corpus] Weak - none of the corpus neighbors discuss BN stabilization in SSMIS.
- Break condition: If EMA momentum is too high or too low, BN statistics may not stabilize properly, leading to training instability.

### Mechanism 3
- Claim: Extra forward pass of weakly augmented inputs to the student model ("Extra-Weak") further stabilizes BN statistics and improves performance.
- Mechanism: Forwarding weakly augmented inputs to the student model in addition to the teacher model provides more stable statistics for BN updates, preventing collapse due to strong augmentations.
- Core assumption: Weak augmentations provide a stable reference for BN statistics that complements strong augmentations.
- Evidence anchors:
  - [section 3.2.2] "we forward the weakly augmented inputs to the student model, not only the teacher model, dubbed as 'Extra-Weak'."
  - [section 3.2.2] "applying all these stabilization strategies can successfully improve the augmentation baseline by a large margin..."
  - [corpus] Weak - no direct mention in neighbors; inferred from the paper's ablation study.
- Break condition: If weak augmentations are too similar to strong ones, the extra forward pass may not provide meaningful stabilization.

## Foundational Learning

- Concept: Consistency regularization in semi-supervised learning.
  - Why needed here: The paper builds on CR-based methods like Mean-Teacher and FixMatch, which rely on prediction consistency across perturbed views.
  - Quick check question: How does consistency loss encourage the model to produce similar predictions on differently augmented views of the same input?

- Concept: Exponential moving average (EMA) for model weight and statistics updates.
  - Why needed here: EMA is used to update both the teacher model's weights and BN statistics, ensuring stable teacher predictions.
  - Quick check question: What is the effect of the EMA momentum parameter on the smoothness of teacher model updates?

- Concept: Batch normalization (BN) statistics drift under data perturbations.
  - Why needed here: Strong augmentations can distort BN statistics, and the paper proposes EMA-BN and Extra-Weak to counteract this.
  - Quick check question: Why might BN statistics become unstable when training with strong augmentations, and how does EMA help?

## Architecture Onboarding

- Component map:
  - Student model: Trained on both labeled data (supervised loss) and unlabeled data (consistency loss)
  - Teacher model: Updated via EMA of student weights and BN statistics
  - Data pipeline: Applies weak augmentations (a) to both labeled and unlabeled data, strong augmentations (A) to unlabeled data
  - Loss functions: Supervised loss on labeled data, consistency loss on unlabeled data with confidence-based filtering

- Critical path:
  1. Forward weakly augmented labeled data through student → compute supervised loss
  2. Forward weakly augmented unlabeled data through both student and teacher → compute consistency loss
  3. Forward strongly augmented unlabeled data through student → compute consistency loss with teacher predictions
  4. Update student weights via total loss
  5. Update teacher weights and BN statistics via EMA

- Design tradeoffs:
  - Strong augmentations improve disagreement but risk BN instability; EMA-BN and Extra-Weak mitigate this
  - Confidence-based filtering reduces confirmation bias but may discard useful weakly confident samples
  - Simple mean-teacher baseline vs. complex methods with extra branches or contrastive losses

- Failure signatures:
  - Degraded performance with strong augmentations alone → BN statistics drift
  - Overfitting to labeled data → insufficient use of unlabeled data
  - High variance in results → unstable training dynamics

- First 3 experiments:
  1. Baseline mean-teacher with weak augmentations only → establish performance floor
  2. Add strong augmentations without stabilization → observe performance drop due to BN drift
  3. Add EMA-BN and Extra-Weak → verify stabilization and performance recovery

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different data perturbation strategies (geometrical, intensity-based, copy-paste) impact the performance of semi-supervised medical image segmentation across various anatomical regions?
- Basis in paper: [explicit] The paper examines three popular kinds of data augmentations and their combinations on 2D ACDC and 3D LA datasets, finding that intensity-based augmentation is most effective.
- Why unresolved: While the paper identifies intensity-based augmentation as most effective overall, it doesn't provide a detailed breakdown of how these augmentations perform on specific anatomical regions or different types of medical images.
- What evidence would resolve it: Detailed experiments showing the effectiveness of each augmentation type on different anatomical regions and medical imaging modalities.

### Open Question 2
- Question: What is the optimal balance between strong data augmentations and model stabilization techniques to maximize segmentation performance without overfitting?
- Basis in paper: [inferred] The paper discusses the importance of model stabilization when using strong data augmentations, but doesn't provide a clear framework for balancing these two aspects.
- Why unresolved: The paper presents individual strategies for data perturbation and model stabilization but doesn't explore their combined effects systematically or provide guidelines for optimal balance.
- What evidence would resolve it: Systematic experiments varying the strength of data augmentations and the degree of model stabilization to find the optimal balance point.

### Open Question 3
- Question: How do different consistency loss types (Dice, CE, compound) affect the performance of semi-supervised medical image segmentation across different datasets and annotation scenarios?
- Basis in paper: [explicit] The paper explores different consistency loss types and finds that Dice loss performs better on ACDC while CE loss is optimal for LA, but doesn't extensively analyze why.
- Why unresolved: The paper shows that different loss types work better for different datasets but doesn't provide a theoretical framework for understanding why certain losses are more suitable for specific scenarios.
- What evidence would resolve it: Theoretical analysis of how different loss functions interact with various dataset characteristics and annotation scenarios.

## Limitations

- The exact implementation details of the strong data augmentation pipeline (specific transformations, intensity ranges, copy-paste configurations) are not fully specified, limiting exact reproduction.
- The forwarding-twice and momentum updating strategies for normalization statistics require precise hyperparameter tuning, and the paper provides limited guidance on optimal values.
- Low confidence in the generalizability of the approach to other medical imaging tasks beyond cardiac and abdominal organ segmentation.

## Confidence

- High confidence in the overall mechanism of strong augmentations + EMA-BN + Extra-Weak improving SSMIS performance, supported by consistent quantitative results across datasets.
- Medium confidence in the specific contribution of each stabilization strategy (EMA-BN vs. Extra-Weak) due to limited ablation studies isolating their individual effects.
- Low confidence in the generalizability of the approach to other medical imaging tasks beyond cardiac and abdominal organ segmentation.

## Next Checks

1. Reproduce the ablation study on data augmentation types (geometrical, intensity-based, copy-paste) to verify their individual contributions to performance gains.
2. Implement and test the EMA-BN and Extra-Weak strategies separately to isolate their effects on BN stabilization and overall performance.
3. Evaluate the approach on a different medical imaging dataset (e.g., brain tumor segmentation) to assess generalizability beyond cardiac and abdominal structures.