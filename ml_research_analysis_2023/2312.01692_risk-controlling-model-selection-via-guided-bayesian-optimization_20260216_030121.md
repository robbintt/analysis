---
ver: rpa2
title: Risk-Controlling Model Selection via Guided Bayesian Optimization
arxiv_id: '2312.01692'
source_url: https://arxiv.org/abs/2312.01692
tags:
- configurations
- optimization
- testing
- pareto
- objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of finding optimal machine learning
  model configurations that satisfy user-specified risk constraints while also optimizing
  additional objectives. The authors propose a novel method that combines Bayesian
  Optimization (BO) with rigorous risk-controlling procedures to efficiently search
  for configurations in a designated region of interest.
---

# Risk-Controlling Model Selection via Guided Bayesian Optimization

## Quick Facts
- arXiv ID: 2312.01692
- Source URL: https://arxiv.org/abs/2312.01692
- Authors: 
- Reference count: 40
- Primary result: Novel method combining Bayesian Optimization with risk-controlling procedures to efficiently find ML model configurations satisfying user-specified risk constraints while optimizing additional objectives.

## Executive Summary
This paper addresses the challenge of finding optimal machine learning model configurations that satisfy user-specified risk constraints while also optimizing additional objectives. The authors propose a novel method that combines Bayesian Optimization (BO) with rigorous risk-controlling procedures to efficiently search for configurations in a designated region of interest. The core idea is to steer BO towards an efficient testing strategy by defining a region aligned with the testing goal and modifying the acquisition function to focus on this region. The resulting configurations are statistically validated, and the best-performing verified configuration is selected with guaranteed risk levels. The proposed method is demonstrated to be effective on various tasks with multiple desiderata, including fairness, robustness, rate and distortion in generative models, and computational cost reduction.

## Method Summary
The method combines Bayesian Optimization with statistical testing to find ML model configurations that satisfy user-specified risk constraints. It defines a region of interest in the objective space using Hoeffding-Bentkus bounds, then uses a modified acquisition function (HVI with adjusted reference point) to focus BO searches within this region. After BO generates candidate configurations, they are filtered for Pareto optimality, ordered by p-values derived from calibration data, and tested using Fixed Sequence Testing to control the family-wise error rate. The best valid configuration is selected based on the free objective.

## Key Results
- Proposed method effectively finds configurations satisfying multiple risk constraints across diverse tasks
- FWER is rigorously controlled at user-specified level δ through Fixed Sequence Testing
- Modified HVI acquisition function successfully focuses BO on the region of interest
- Method demonstrates effectiveness on fairness, robustness, rate/distortion, and computational cost reduction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method controls FWER at level δ by filtering Pareto optimal configurations with p-values and applying Fixed Sequence Testing (FST).
- Mechanism: After BO identifies candidate configurations, they are ordered by increasing p-values (approximated on validation data). FST sequentially tests each configuration with a fixed threshold δ until the first failure. This ensures that any false discovery has probability ≤ δ.
- Core assumption: p-values derived from calibration data are super-uniform under their respective null hypotheses.
- Evidence anchors:
  - [section] "We follow (Angelopoulos et al., 2021; Laufer-Goldshtein et al., 2023) for testing the selected set. Prior to testing we filter and order the candidate set CBO. Specifically, we retain only Pareto optimal configurations from CBO, and arrange the remaining configurations by increasing p-values...Then, Fixed Sequence Testing (FST) (Holm, 1979) is applied over the ordered set..."
  - [section] "Theorem 5.1. Let Dval = {Xi, Yi}k i=1 and Dcal = {Xi, Yi}k+m i=k+1 be two disjoint datasets. Suppose the p-value pλ, derived from Dcal, is super-uniform under Hλ for all λ. Then the output λ∗ of Algorithm D.2 satisfies Eq. (1)."
  - [corpus] Weak or missing direct evidence; the FWER claim relies on cited prior work.
- Break condition: If calibration data is not i.i.d. with respect to the configuration set, p-values may not be super-uniform and FWER control fails.

### Mechanism 2
- Claim: The region of interest R(α, k, m) ensures that configurations inside are likely both valid and efficient.
- Mechanism: For each constrained objective, compute αmax using Hoeffding-Bentkus bound. Expand this to [ℓlow, ℓhigh] using confidence intervals based on validation data size k and calibration data size m. The intersection across all constraints defines R. Configurations in R are expected to satisfy constraints while being Pareto optimal in the free objective.
- Core assumption: Validation data loss ℓ̂ is a good estimator of expected loss ℓ, and Hoeffding-Bentkus provides valid confidence intervals.
- Evidence anchors:
  - [section] "For example, using again Hoeffding's inequality, we obtain the following region of interest: R(α, k, m) = [αmax − r log (1/δ′) / 2k , αmax + r log (1/δ′) / 2k]"
  - [section] "R(α, k, m) = ⋂i=1c R(αi, k, m); α = (α1, . . . , αc)" (multi-constraint case)
  - [corpus] No direct corpus evidence; relies on Hoeffding/Bentkus concentration inequalities.
- Break condition: If loss is not bounded in [0,1] or data is heavy-tailed, Hoeffding-Bentkus may not apply and R may be misspecified.

### Mechanism 3
- Claim: The modified HVI acquisition function focuses BO on the region of interest, producing a dense set of likely-valid configurations.
- Mechanism: Define reference point r with rc+1 set to the minimum free objective among configurations in Rlow (those below the lower bound in all constrained objectives). Maximize HVI(ĝ(λ), P̂; r) to select next λ. This biases search toward Pareto optimal points inside R.
- Core assumption: The surrogate model ĝ accurately predicts the objectives, and maximizing HVI with this r will generate points in R.
- Evidence anchors:
  - [section] "We define the region Rlow = {λ : ĝ1(λ) < ℓlow 1, . . . ,ĝc(λ) < ℓlow c}, where the configurations are likely to be valid but inefficient. Finally, we tightly enclose this region from below in the free dimension: rc+1 = minλ∈Rlow ĝfree(λ)."
  - [section] "We select the next configuration by maximizing the HVI (4) with respect to this reference point: λn = arg maxλ HV I (ĝ(λ), P̂; r)"
  - [corpus] No direct corpus evidence; this is a novel modification of standard HVI.
- Break condition: If surrogate model predictions are poor (e.g., due to insufficient evaluations), HVI maximization may select points outside R.

## Foundational Learning

- Concept: Gaussian Process (GP) surrogate modeling
  - Why needed here: BO relies on GP to estimate objective functions and their uncertainty at untried configurations, enabling acquisition function optimization.
  - Quick check question: In a GP surrogate, what do the posterior mean μ(λ) and variance Σ(λ, λ) represent?

- Concept: Multi-objective Pareto optimality
  - Why needed here: The goal is to find configurations that are Pareto optimal across multiple objectives, then filter them by risk constraints.
  - Quick check question: Given two configurations λ1 and λ2, how do you determine if λ1 dominates λ2?

- Concept: Multiple hypothesis testing (MHT) and FWER control
  - Why needed here: After BO, multiple configurations are tested for risk constraints; MHT ensures overall error rate is controlled.
  - Quick check question: What is the difference between FWER and FDR, and why is FWER control important here?

## Architecture Onboarding

- Component map:
  Input datasets -> Region of interest definition -> BO engine (GP surrogate + modified HVI) -> Candidate pool -> Pareto filtering + p-value ordering -> Fixed Sequence Testing -> Selected configuration

- Critical path:
  1. Generate initial random pool C0, evaluate objectives
  2. Define region of interest R(α, k, m)
  3. Iteratively select λ via modified HVI, evaluate, update pool until budget N
  4. Filter Pareto optimal subset, order by p-values
  5. Apply FST on calibration data to find Cvalid
  6. Choose λ* = arg min_{λ∈Cvalid} ℓfree(λ)

- Design tradeoffs:
  - Larger δ′ → wider R → more configurations but lower density and higher computational cost
  - Smaller N → less exploration, risk of missing good configurations in R
  - Using Hoeffding vs Hoeffding-Bentkus → trade-off between tightness and validity of p-values

- Failure signatures:
  - BO consistently selects points outside R → surrogate model inaccurate or reference point mis-specified
  - All configurations fail FST → risk bounds α too tight or data insufficient for power
  - λ* is null → no configuration satisfies constraints at significance level δ

- First 3 experiments:
  1. Run fairness task (Adult dataset) with α1 = 0.17, δ = 0.1, N = 10. Verify that λ* has DDP ≤ α1 and controlled error.
  2. Vary δ′ ∈ {0.0001, 0.001, 0.01} and observe effect on free objective performance.
  3. Compare HVI vs EHVI acquisition: measure how often configurations fall inside R and final FWER control.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of δ′ affect the performance of the guided BO method in practice?
- Basis in paper: [explicit] The paper discusses δ′ as an empirical choice that determines the width of the region of interest, but does not provide extensive empirical results on its impact.
- Why unresolved: While the paper mentions that the method is generally insensitive to δ′, it does not explore the range of δ′ values or their specific effects on the final model selection.
- What evidence would resolve it: Conducting experiments with varying δ′ values and analyzing the impact on the selected configurations' performance and efficiency.

### Open Question 2
- Question: How does the guided BO method compare to other multi-objective optimization techniques when applied to high-dimensional hyperparameter spaces?
- Basis in paper: [inferred] The paper demonstrates effectiveness on various tasks but does not explicitly compare the method to other techniques in high-dimensional spaces.
- Why unresolved: The paper focuses on demonstrating the method's effectiveness but does not provide a comprehensive comparison with other optimization techniques, especially in high-dimensional settings.
- What evidence would resolve it: Conducting comparative experiments with other multi-objective optimization methods in high-dimensional hyperparameter spaces and analyzing their performance.

### Open Question 3
- Question: Can the guided BO method be extended to handle non-differentiable objective functions or constraints?
- Basis in paper: [inferred] The paper discusses differentiable objective functions and does not address non-differentiable cases.
- Why unresolved: The paper focuses on differentiable objective functions and does not explore the method's applicability to non-differentiable scenarios.
- What evidence would resolve it: Developing an extension of the guided BO method to handle non-differentiable objective functions or constraints and demonstrating its effectiveness on relevant tasks.

## Limitations
- Validity of p-values relies critically on bounded losses in [0,1] and i.i.d. data assumptions
- Method assumes surrogate model accurately captures objective landscape; poor predictions could miss the region of interest
- Calibration data size directly impacts region width, creating tradeoff between validity and efficiency
- Limited empirical validation beyond synthetic and standard benchmark datasets

## Confidence
- **High**: The mechanism of using FST to control FWER after ordering by p-values is well-established (supported by cited prior work)
- **Medium**: The integration of risk-controlling region definition with BO acquisition functions is novel but relies on reasonable assumptions
- **Low**: The empirical demonstration is limited to synthetic and standard benchmark datasets without extensive real-world validation

## Next Checks
1. Test the method on unbounded loss functions (e.g., squared error) to assess robustness of FWER guarantees
2. Conduct ablation studies varying the calibration set size m to quantify the tradeoff between region width and efficiency
3. Implement a diagnostic to detect when BO consistently selects points outside R, indicating potential surrogate model failure