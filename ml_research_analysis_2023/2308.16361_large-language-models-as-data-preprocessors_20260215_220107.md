---
ver: rpa2
title: Large Language Models as Data Preprocessors
arxiv_id: '2308.16361'
source_url: https://arxiv.org/abs/2308.16361
tags:
- data
- llms
- prompting
- language
- preprocessing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the application of Large Language Models (LLMs)
  for data preprocessing tasks, including error detection, data imputation, schema
  matching, and entity matching. A framework integrating zero-shot and few-shot prompting,
  batch processing, and feature selection was proposed to leverage LLMs' capabilities
  in handling tabular data.
---

# Large Language Models as Data Preprocessors
## Quick Facts
- arXiv ID: 2308.16361
- Source URL: https://arxiv.org/abs/2308.16361
- Reference count: 35
- Key outcome: LLM framework for data preprocessing shows GPT-4 achieving 100% accuracy/F1 on 4 of 12 datasets

## Executive Summary
This study proposes an LLM-based framework for data preprocessing tasks including error detection, data imputation, schema matching, and entity matching. The framework integrates advanced prompt engineering techniques with traditional methods like contextualization and feature selection. Through experiments on 12 datasets using GPT-3.5 and GPT-4, the research demonstrates LLMs' potential to handle tabular data preprocessing tasks effectively, particularly when enhanced with chain-of-thought reasoning and batch processing.

## Method Summary
The research develops an LLM-based framework that employs zero-shot and few-shot prompting strategies for four data preprocessing tasks. Zero-shot prompts use chain-of-thought reasoning to specify tasks and answer formats, while few-shot examples condition the model to learn specific criteria. The framework incorporates contextualization to convert tabular data into text format, feature selection for manual attribute prioritization, and batch prompting to reduce computational costs. Experiments compare GPT-3.5 and GPT-4 performance across 12 datasets, measuring accuracy and F1 scores for different preprocessing tasks.

## Key Results
- GPT-4 achieved 100% accuracy or F1 score on 4 out of 12 datasets
- Batch prompting reduced token usage from over 4M to 1.5M while maintaining or improving accuracy
- Zero-shot prompting with chain-of-thought reasoning improved structured data task performance
- Few-shot examples effectively conditioned LLMs for domain-specific preprocessing criteria

## Why This Works (Mechanism)
### Mechanism 1
Zero-shot prompting with chain-of-thought reasoning improves LLM performance by providing explicit task specification and answer formatting. The prompt instructs the model to reason before answering, following a specific format that helps structure output correctly for tasks like data imputation.

### Mechanism 2
Few-shot prompting conditions LLMs to learn specific error criteria, imputation methods, and matching conditions. Labeled examples teach the model expected behavior for tasks like schema matching, reducing the need for task-specific fine-tuning.

### Mechanism 3
Batch prompting reduces token and time costs by processing multiple data instances in a single prompt while maintaining accuracy through consistent reasoning. Multiple instances are presented together, allowing the LLM to identify commonalities and generate consistent solutions.

## Foundational Learning
- **Chain-of-thought prompting**: Helps LLMs structure reasoning process for data preprocessing tasks, leading to more accurate results. Quick check: What are the two parts of the answer format for data imputation? (Reason in first line, value in second line)
- **Few-shot learning**: Conditions LLMs to learn specific criteria without requiring fine-tuning. Quick check: How many few-shot examples are used for schema matching? (3 examples)
- **Contextualization**: Converts tabular data into text format that LLMs can process. Quick check: What format represents each data instance? ([attribute1.name: "value", ..., attributeN.name: "value"])

## Architecture Onboarding
- **Component map**: Raw data -> Contextualization -> Feature selection -> Prompt construction -> LLM inference -> Result parsing
- **Critical path**: Receive raw data → Apply contextualization → Select features → Construct prompt → Send to LLM → Parse response
- **Design tradeoffs**: Zero-shot vs. few-shot (speed vs. accuracy), random vs. cluster batching (simplicity vs. similarity), batch size (cost vs. context limits)
- **Failure signatures**: Incorrect task interpretation, poor generalization, context window overflow, inconsistent reasoning
- **First 3 experiments**: Test zero-shot prompting on single imputation instance, evaluate few-shot impact on schema matching, measure batch prompting cost-benefit tradeoff

## Open Questions the Paper Calls Out
- How do different types of error detection tasks (spelling vs. contextual errors) impact LLM performance?
- What is the optimal balance between computational cost and accuracy when using LLMs for data preprocessing?
- How do LLMs perform on highly specialized or domain-specific datasets compared to general datasets?

## Limitations
- Computational expense constrains practical deployment for large-scale data processing
- Domain specificity remains a concern for specialized or structured data types
- Manual feature selection introduces potential bias and limits automation

## Confidence
- **High confidence**: LLMs can perform basic data preprocessing tasks with reasonable accuracy when properly prompted
- **Medium confidence**: The proposed framework combining zero-shot and few-shot prompting improves performance
- **Medium confidence**: Batch prompting provides significant cost reductions without substantial accuracy loss
- **Low confidence**: Framework's effectiveness on highly domain-specific or complex data structures

## Next Checks
1. Evaluate the framework on industry-specific datasets with complex schemas and domain-specific terminology
2. Conduct cost-benefit analysis comparing LLM-based preprocessing with traditional methods on large-scale datasets
3. Implement automated feature selection and prompt generation to reduce manual intervention and improve reproducibility