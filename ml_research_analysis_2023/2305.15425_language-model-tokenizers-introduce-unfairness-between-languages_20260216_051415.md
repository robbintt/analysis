---
ver: rpa2
title: Language Model Tokenizers Introduce Unfairness Between Languages
arxiv_id: '2305.15425'
source_url: https://arxiv.org/abs/2305.15425
tags: []
core_contribution: Language models tokenize text differently across languages, causing
  unfair disparities in cost, latency, and context length. Some languages require
  up to 15 times more tokens than others for the same content.
---

# Language Model Tokenizers Introduce Unfairness Between Languages

## Quick Facts
- arXiv ID: 2305.15425
- Source URL: https://arxiv.org/abs/2305.15425
- Reference count: 32
- Primary result: Language models tokenize text differently across languages, causing unfair disparities in cost, latency, and context length.

## Executive Summary
Language model tokenizers exhibit significant disparities in how they encode different languages, leading to unfair treatment of users. Some languages require up to 15 times more tokens than others for equivalent content, resulting in higher costs, longer processing times, and reduced context windows. These disparities persist even in tokenizers specifically designed for multilingual support, with premiums exceeding 4 times for certain language pairs. The authors advocate for developing multilingually fair tokenizers that start from Unicode encoding to ensure equitable treatment across all languages.

## Method Summary
The study evaluates 17 different tokenizers using the FLORES-200 parallel corpus (2000 sentences translated into 200 languages) and additional parallel corpora for dialects and creoles. The researchers measure tokenization length disparities by calculating premium ratios between languages, analyze processing times to verify linear correlation with tokenization length, and examine UNK rates across different tokenizers. They compare various tokenizer types including GPT-2, RoBERTa, ChatGPT, GPT-4, multilingual models like XLM-R and M2M100, and byte-level models such as CANINE and ByT5.

## Key Results
- Tokenization premiums exceed 4 times for certain language pairs even with multilingual tokenizers
- Some languages require up to 15 times more tokens than others for the same semantic content
- Disparities lead to higher costs, longer processing times, and reduced context windows for affected languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokenization differences directly translate to cost and latency unfairness
- Mechanism: Tokenizers produce varying token counts for same semantic content across languages. Since commercial APIs charge per token and processing time scales with token count, languages with longer token sequences incur higher costs and longer latencies
- Core assumption: Token count is primary driver of both cost and latency in LLM APIs
- Evidence anchors: Abstract states disparities lead to users paying more and experiencing longer processing times; section shows Arabic users charged 3 times more than English users
- Break condition: If billing models shift to fixed pricing or processing time becomes dominated by non-token factors

### Mechanism 2
- Claim: Multilingual tokenizers can reduce but not eliminate disparities
- Mechanism: Models like XLM-R and M2M100 show lower premiums than English-centric models but still exhibit premiums over 4 for some language pairs
- Core assumption: Multilingual models inherently improve tokenization fairness compared to monolingual ones
- Evidence anchors: Abstract notes disparities persist across all 17 tokenizers evaluated; section shows all five models have languages with premiums larger than 4
- Break condition: If a tokenizer achieves near-parity (<1.5x premium) across all languages in large-scale evaluation

### Mechanism 3
- Claim: Unicode-based subword tokenization is necessary but insufficient for parity
- Mechanism: Tokenizers building vocabularies on Unicode representations can encode all languages, but subword tokenization still produces different token counts due to language-specific structures
- Core assumption: Subword tokenization is best available method for balancing token efficiency and vocabulary size
- Evidence anchors: Abstract mentions character-level and byte-level models also exhibit over 4 times differences; section states variation of subword tokenization is necessary to achieve parity
- Break condition: If new tokenization method achieves significantly better parity without sacrificing model performance

## Foundational Learning

- Concept: Subword tokenization and vocabulary size tradeoffs
  - Why needed here: Understanding how subword tokenization affects token efficiency and model performance is crucial for designing fair tokenizers
  - Quick check question: What is the relationship between vocabulary size and tokenization length for a given language?

- Concept: Unicode encoding standards (UTF-8, UTF-32) and their impact on tokenization
  - Why needed here: Unicode encoding determines base character set available to tokenizers, affecting their ability to represent all languages fairly
  - Quick check question: How does UTF-8's variable-width encoding affect tokenization length for languages with non-ASCII characters?

- Concept: Cross-lingual transfer and its dependence on tokenization
  - Why needed here: Tokenization affects how well models can transfer knowledge between languages, influencing overall multilingual performance
  - Quick check question: How does tokenization parity impact the effectiveness of cross-lingual transfer learning?

## Architecture Onboarding

- Component map: Text input -> Tokenizer -> Token sequence -> Language Model -> Output + Token count -> API pricing and latency calculation

- Critical path: 1) Text input → Tokenizer → Token sequence; 2) Token sequence → Language Model → Output; 3) Output + Token count → API pricing and latency calculation

- Design tradeoffs: Vocabulary size vs. tokenization length (larger vocabularies reduce token count but increase model size); Tokenization granularity (finer tokenization improves language coverage but may increase sequence length); Unicode encoding choice (UTF-8 is space-efficient but can lead to variable token lengths; UTF-32 is uniform but space-inefficient)

- Failure signatures: High premiums for certain language pairs indicate tokenizer bias; Unexpected spikes in processing time for specific languages suggest tokenization inefficiencies; Low multilingual performance despite large vocabulary may indicate poor tokenization design

- First 3 experiments: 1) Measure tokenization premiums across 50+ languages using FLORES-200 corpus; 2) Compare processing times and costs for equivalent tasks in different languages using commercial API; 3) Evaluate multilingual model performance with and without tokenization parity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for designing a parallel corpus that ensures both linguistic diversity and balanced representation of named entities across languages?
- Basis in paper: The authors emphasize importance of well-curated and diverse parallel corpus for developing multilingually fair tokenizers, noting imbalances in topics and named entities can skew tokenization results
- Why unresolved: Designing such corpus requires balancing linguistic diversity, topic coverage, and named entity representation without overburdening with noise or redundancy
- What evidence would resolve it: Empirical studies comparing tokenizer performance trained on corpora with varying levels of topic and named entity balance

### Open Question 2
- Question: Can a multilingually fair tokenizer achieve near-perfect parity across all language pairs, or are there inherent linguistic constraints that make some disparities unavoidable?
- Basis in paper: Authors hypothesize natural languages can reach parity levels close to 1, but acknowledge perfect parity might not be possible due to discrete nature of tokenization and shared linguistic features
- Why unresolved: Paper does not provide definitive answer on whether perfect parity is achievable or if certain linguistic characteristics inherently limit parity
- What evidence would resolve it: Comprehensive analysis of tokenization parity across large number of languages, identifying patterns of unavoidable disparities

### Open Question 3
- Question: How do byte-level tokenizers compare to subword tokenizers in terms of computational efficiency and model performance across diverse languages?
- Basis in paper: Authors discuss trade-offs between byte-level and subword tokenization, noting byte-level models can represent all Unicode codepoints but may introduce inefficiencies
- Why unresolved: Paper highlights potential of byte-level tokenization for multilingual support but does not provide detailed comparison of computational efficiency and performance
- What evidence would resolve it: Empirical studies comparing computational cost, model accuracy, and tokenization parity of byte-level and subword tokenizers across diverse languages

## Limitations

- The study cannot establish causation between tokenization alone and overall model performance degradation
- FLORES-200 corpus represents only 200 sentences per language, which may not capture full variability of real-world usage patterns
- Analysis focuses on tokenization premiums but does not evaluate downstream impact on actual task performance metrics

## Confidence

- High Confidence: Empirical measurements of tokenization length disparities across languages are well-supported by data
- Medium Confidence: Claim that disparities translate directly to cost and latency unfairness is plausible but not experimentally isolated
- Low Confidence: Assertion that Unicode-based subword tokenization is "necessary but insufficient" lacks comparative evidence against alternative strategies

## Next Checks

1. Conduct controlled experiments measuring task performance (translation quality, classification accuracy) for equivalent inputs across languages while controlling for tokenization differences
2. Expand evaluation to include real-world API usage data showing actual cost differences and latency measurements across multiple commercial providers
3. Test alternative tokenization approaches (morphological segmentation, character n-grams, hybrid methods) on same evaluation framework to quantify potential improvements in parity metrics