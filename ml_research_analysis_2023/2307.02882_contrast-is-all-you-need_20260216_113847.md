---
ver: rpa2
title: Contrast Is All You Need
arxiv_id: '2307.02882'
source_url: https://arxiv.org/abs/2307.02882
tags:
- finetuning
- setfit
- vanilla
- features
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the challenge of legal text classification
  with limited and imbalanced labeled data. The authors compare two fine-tuning approaches
  for LegalBERT: a standard fine-tuning setup and a contrastive learning approach
  using SetFit (Sentence Transformer Finetuning).'
---

# Contrast Is All You Need

## Quick Facts
- arXiv ID: 2307.02882
- Source URL: https://arxiv.org/abs/2307.02882
- Reference count: 19
- Key outcome: SetFit outperforms vanilla fine-tuning in legal text classification with limited data while improving interpretability through contrastive learning.

## Executive Summary
This paper addresses the challenge of legal text classification with limited and imbalanced labeled data by comparing standard fine-tuning of LegalBERT with a contrastive learning approach using SetFit (Sentence Transformer Finetuning). SetFit improves few-shot learning by generating positive and negative pairs from small datasets, effectively enlarging the training data through synthetic examples. Experiments on the LEDGAR dataset demonstrate that SetFit achieves better performance than vanilla fine-tuning while using only a fraction of the training samples. LIME-based feature analysis reveals that SetFit assigns higher weights to legally informative features, enhancing model interpretability and trustworthiness in data-scarce legal NLP tasks.

## Method Summary
The study fine-tunes LegalBERT using two approaches: vanilla fine-tuning and SetFit's contrastive learning framework. SetFit generates positive and negative pairs from small labeled datasets by creating triplets where two sentences share the same class label and a third comes from a different class. The authors evaluate both methods on the LEDGAR dataset, using both the original unbalanced dataset (60,000 samples, 100 provision labels) and a balanced version with 32 classes (1000 samples each). Model performance is measured using F1-score and accuracy, while LIME is employed to analyze which features the models use for predictions and assess interpretability.

## Key Results
- SetFit outperforms vanilla fine-tuning on legal text classification while using significantly fewer training samples.
- LIME analysis shows SetFit assigns higher weights to legally informative features, both positive and negative, improving interpretability.
- The contrastive approach achieves comparable performance to vanilla fine-tuning using only a fraction of the training data (SetFit: 1600 samples vs vanilla: 18,734 samples).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning in SetFit improves classification by generating positive and negative pairs from small labeled datasets, thereby enlarging the effective training data.
- Mechanism: For each class, SetFit creates triplets where two sentences are from the same class (positive pair) and another sentence is from a different class (negative pair). This increases the number of training samples exponentially compared to the original labeled data size.
- Core assumption: The quality of positive and negative pairs generated randomly from the small dataset is sufficient to guide the model toward useful representations.
- Evidence anchors:
  - [abstract]: "SetFit improves few-shot learning by generating positive and negative pairs from small datasets."
  - [section]: "Formally, we assume a small set of K-labeled samples... For each class label c âˆˆ C, R positive triplets are generated..."
  - [corpus]: Weak - corpus neighbors do not directly discuss pair generation in SetFit, only general contrastive learning mentions.
- Break condition: If the random pairing process produces misleading or noisy pairs, the contrastive signal may degrade model performance.

### Mechanism 2
- Claim: SetFit's contrastive fine-tuning leads to embeddings that better capture legally informative features, improving interpretability and trustworthiness.
- Mechanism: By training on positive and negative pairs, the model learns to differentiate semantically similar but class-different sentences, pushing the encoder to focus on discriminative features.
- Core assumption: The legal domain contains clear positive/negative feature patterns that contrastive learning can amplify.
- Evidence anchors:
  - [abstract]: "LIME-based feature analysis reveals that SetFit assigns higher weights to legally informative features, both positive and negative, enhancing model interpretability and trustworthiness."
  - [section]: "The results show that a contrastive setup with SetFit performed better than vanilla finetuning while using a fraction of the training samples."
  - [corpus]: Weak - corpus does not provide specific legal feature interpretability evidence.
- Break condition: If legal texts are too ambiguous or the features are not clearly separable, contrastive learning may not yield better interpretability.

### Mechanism 3
- Claim: SetFit achieves comparable or better performance with fewer training samples than vanilla fine-tuning.
- Mechanism: By leveraging pre-trained Sentence Transformer embeddings and fine-tuning them with contrastive pairs, SetFit reduces the need for large labeled datasets while maintaining model performance.
- Core assumption: The pre-trained Sentence Transformer already encodes useful semantic information that can be adapted with minimal labeled data.
- Evidence anchors:
  - [abstract]: "SetFit outperforms vanilla fine-tuning while using only a fraction of the training samples."
  - [section]: "SetFit will generate positive and negative samples randomly from the training set, unless they are explicitly given."
  - [corpus]: Weak - corpus neighbors focus on few-shot learning but do not validate the specific sample efficiency claim for SetFit.
- Break condition: If the pre-trained model is not sufficiently aligned with the legal domain, few-shot adaptation may fail.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: It allows the model to learn from limited data by creating synthetic training pairs, which is essential in low-resource legal NLP tasks.
  - Quick check question: What is the main difference between contrastive learning and standard supervised fine-tuning in terms of data usage?

- Concept: Sentence Transformers and embeddings
  - Why needed here: SetFit builds on Sentence Transformers to generate sentence-level embeddings, which serve as input for the classifier head.
  - Quick check question: Why is it beneficial to use sentence-level embeddings rather than token-level embeddings for few-shot classification?

- Concept: LIME for feature interpretability
  - Why needed here: LIME helps identify which features (words) the model uses for predictions, ensuring that decisions are based on legally relevant terms.
  - Quick check question: How does LIME approximate a complex model locally using interpretable features?

## Architecture Onboarding

- Component map: Pre-trained LegalBERT -> SetFit contrastive fine-tuning (Siamese network) -> Sentence Transformer embeddings -> Logistic regression classifier head -> LIME interpretability
- Critical path: Data preparation -> Contrastive pair generation -> Sentence Transformer fine-tuning -> Classifier head training -> Inference with LIME feature extraction
- Design tradeoffs: Contrastive learning increases training data size but adds computational overhead; fewer samples needed but model complexity rises.
- Failure signatures: Poor pair generation -> degraded embeddings; insufficient legal domain alignment -> misleading features; LIME instability -> unreliable interpretability.
- First 3 experiments:
  1. Validate pair generation logic by inspecting a sample of generated triplets for correctness.
  2. Compare embedding quality before and after contrastive fine-tuning using similarity metrics on legal text pairs.
  3. Run LIME on a small set of predictions to confirm that top features align with legal terminology.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SetFit's performance scale with increasing dataset size compared to vanilla fine-tuning?
- Basis in paper: [explicit] The paper compares SetFit and vanilla fine-tuning on both original and balanced LEDGAR datasets, showing SetFit's comparable performance with significantly fewer training samples.
- Why unresolved: The paper only tests up to 1600 samples for SetFit and 18734 for vanilla fine-tuning. It doesn't explore performance trends with even larger datasets.
- What evidence would resolve it: Experiments comparing SetFit and vanilla fine-tuning on increasingly larger datasets (e.g., 10k, 50k, 100k samples) would show if SetFit's advantage diminishes or remains as data scales up.

### Open Question 2
- Question: Are the "legally informative" features identified by LIME truly meaningful to legal experts, or are they just statistically significant patterns?
- Basis in paper: [explicit] The authors claim that SetFit boosts legally informative features, but this is based on their interpretation of feature importance rather than expert validation.
- Why unresolved: The paper doesn't include any legal domain expert evaluation of the identified features to confirm their legal relevance.
- What evidence would resolve it: A study where legal experts review and rate the importance of features identified by both SetFit and vanilla models would validate whether these features are truly legally informative.

### Open Question 3
- Question: How sensitive is SetFit's performance to the choice of the R parameter (number of positive/negative pairs generated)?
- Basis in paper: [explicit] The paper mentions that R is a hyperparameter but only uses its default value (20) across all experiments without exploring its impact.
- Why unresolved: The authors don't investigate how varying R affects model performance, which is crucial for understanding SetFit's robustness.
- What evidence would resolve it: Experiments systematically varying R (e.g., 5, 10, 20, 50, 100) and measuring the resulting model performance would show the sensitivity to this parameter.

## Limitations

- The evaluation focuses on contract provision texts from the LEDGAR dataset, limiting generalizability to other legal document types.
- Implementation details for SetFit's contrastive learning setup and pair generation are not fully specified, hindering reproducibility.
- The interpretability claims rely on LIME feature analysis without validation from legal domain experts.

## Confidence

**High confidence**: The core claim that SetFit achieves better performance than vanilla fine-tuning with fewer samples on the LEDGAR dataset. This is supported by experimental results and aligns with established few-shot learning principles.

**Medium confidence**: The mechanism that contrastive learning improves interpretability through feature weighting. While LIME analysis shows different feature weights, the legal relevance of these features is not independently verified.

**Low confidence**: Generalization to other legal text classification tasks beyond contract provisions. The paper provides no evidence of performance on different legal document types.

## Next Checks

1. **Pair generation validation**: Manually inspect a random sample of 50 generated triplets to verify that positive pairs genuinely share the same class label and negative pairs come from different classes, ensuring the quality of the contrastive learning signal.

2. **Cross-dataset evaluation**: Test the SetFit approach on at least two other legal text classification datasets (e.g., case law classification, statute categorization) to assess domain generalizability beyond contract provisions.

3. **Expert feature validation**: Conduct a blinded study with legal domain experts to evaluate whether the top-weighted features identified by LIME correspond to legally meaningful distinctions, validating the interpretability claims beyond statistical feature importance.