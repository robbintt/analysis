---
ver: rpa2
title: Explainable Fraud Detection with Deep Symbolic Classification
arxiv_id: '2312.00586'
source_url: https://arxiv.org/abs/2312.00586
tags:
- data
- fraud
- transaction
- performance
- expression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Deep Symbolic Classification (DSC) extends Deep Symbolic Regression
  for fraud detection by adding a sigmoid layer and optimizing a class-imbalanced
  reward function. DSC generates interpretable analytical expressions via RNN-guided
  search, achieving F1 scores comparable to state-of-the-art models while providing
  explainable decision rules.
---

# Explainable Fraud Detection with Deep Symbolic Classification

## Quick Facts
- **arXiv ID:** 2312.00586
- **Source URL:** https://arxiv.org/abs/2312.00586
- **Reference count:** 40
- **Key outcome:** DSC generates interpretable fraud detection rules using only 3 features, achieving F1 scores comparable to state-of-the-art models while avoiding data leakage and eliminating need for sampling techniques.

## Executive Summary
Deep Symbolic Classification (DSC) extends Deep Symbolic Regression for fraud detection by incorporating a sigmoid layer and optimizing a class-imbalanced reward function. The method generates interpretable analytical expressions via RNN-guided search, achieving competitive F1 scores while providing explainable decision rules. The optimal expression uses only three features‚Äîtransaction type, external recipient, and amount difference‚Äîto form a concise fraud detection rule. DSC successfully addresses class imbalance through F1 score optimization and allows explicit balancing between prediction accuracy and explainability via Pareto-optimal expression selection.

## Method Summary
DSC uses an RNN to generate analytical expression syntax trees, which are then optimized through an inner genetic programming loop. Each expression is passed through a sigmoid function to produce probabilities, and a tunable threshold (typically 0.8) converts these to binary fraud/legitimate decisions. The model is trained using reinforcement learning with a reward function equal to the F1 score, which inherently handles class imbalance without requiring oversampling or undersampling techniques. After training, a Pareto front of expression complexity versus F1 score is constructed, allowing users to select optimal expressions that balance interpretability and performance.

## Key Results
- DSC achieves F1 scores comparable to XGBoost and random forest baselines on the PaySim fraud detection dataset
- The optimal expression uses only three features (transaction type, external recipient, and amount difference) to detect fraud
- DSC successfully addresses class imbalance through F1 score optimization without requiring sampling techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DSC replaces traditional sampling-based class imbalance handling with direct optimization of F1 score via reinforcement learning.
- Mechanism: Instead of balancing training data through oversampling or undersampling, the RNN samples expressions based on a reward function equal to the F1 score, which inherently weights precision and recall equally and avoids the minority class being ignored.
- Core assumption: The F1 score is a suitable proxy for fraud detection performance in imbalanced settings and that reinforcement learning can effectively optimize it.
- Evidence anchors:
  - [abstract] "Furthermore, the class imbalance problem is successfully addressed by optimizing for metrics that are robust to class imbalance such as the F1 score."
  - [section] "the class imbalance problem is successfully addressed by optimizing for metrics that are robust to class imbalance such as the F1 score."
  - [corpus] Weak evidence - no direct mention of F1 score or reinforcement learning in corpus titles/abstracts.
- Break condition: If the F1 score does not correlate with business-relevant fraud detection metrics (e.g., cost of false positives vs. false negatives), or if the reward landscape is too noisy for effective RL training.

### Mechanism 2
- Claim: Expressions are optimized for both accuracy and interpretability via Pareto-optimal selection.
- Mechanism: After generating many expressions with varying complexity, the algorithm constructs a Pareto front where no other expression is better in both complexity and performance. The user selects an optimal expression from this front, trading off explainability and predictive power.
- Core assumption: There exists a meaningful tradeoff surface between model complexity and performance, and that simpler expressions can be nearly as accurate as complex ones.
- Evidence anchors:
  - [abstract] "Finally, the model allows to explicitly balance between the prediction accuracy and the explainability."
  - [section] "the set of generated expressions where no other generated expression is superior in both complexity and performance [31]. Such a set is typically known as the Pareto front."
  - [corpus] No mention of Pareto optimization or explainability-performance tradeoff in corpus.
- Break condition: If all generated expressions lie on a single performance peak, making complexity irrelevant, or if complexity is not a good proxy for interpretability in the given domain.

### Mechanism 3
- Claim: Sigmoid layer + tunable threshold enables effective binary classification from regression outputs.
- Mechanism: Analytical expressions output real values; a sigmoid function maps these to probabilities, and a threshold (tuned, e.g., to 0.8) converts them to binary fraud/legitimate decisions. Higher thresholds reduce false positives in imbalanced data.
- Core assumption: The distribution of expression outputs is suitable for sigmoid calibration, and a single threshold can adequately separate classes.
- Evidence anchors:
  - [section] "Each expression ùëì is passed through a sigmoid function ùúé to produce probabilities that are suitable for use in this binary classification problem."
  - [section] "The class prediction ÀÜùë¶ of a transaction with corresponding features ùíô is determined according to the following: ÀÜùë¶ = (1 if ùúé (ùëì (ùíô)) ‚â• ùë° 0 if ùúé (ùëì (ùíô)) < ùë°"
  - [corpus] No mention of sigmoid or threshold tuning in corpus.
- Break condition: If the expression outputs are not well-calibrated probabilities or if the decision boundary is not linear in the sigmoid space, leading to poor classification.

## Foundational Learning

- Concept: Reinforcement learning with risk-seeking policy gradient.
  - Why needed here: Standard supervised learning cannot optimize arbitrary metrics like F1; RL allows direct optimization of non-differentiable reward functions.
  - Quick check question: What is the difference between maximizing expected reward and maximizing the reward of the top fraction of samples in policy gradient methods?

- Concept: Symbolic regression and analytical expression trees.
  - Why needed here: The fraud detection model must be interpretable; analytical expressions are human-readable and can be converted to decision rules.
  - Quick check question: How does a preorder traversal list represent a unique syntax tree for an analytical expression?

- Concept: Class imbalance and its impact on classification metrics.
  - Why needed here: Fraud detection data is highly imbalanced; standard accuracy is misleading, so metrics like F1 are more appropriate.
  - Quick check question: Why does optimizing for accuracy on imbalanced data often lead to models that predict the majority class for everything?

## Architecture Onboarding

- Component map: RNN (expression generator) -> Sigmoid layer (probability mapping) -> Threshold (binary decision) -> Reward (F1 or cross-entropy) -> RL policy update. GP loop refines initial RNN-generated expressions.
- Critical path: RNN sampling -> Expression evaluation on validation set -> Reward calculation -> Risk-seeking gradient update -> New sampling. Each iteration depends on the previous reward signal.
- Design tradeoffs: RNN-guided search reduces randomness but may miss novel expressions; Pareto selection adds interpretability but may discard potentially useful complex models; threshold tuning trades off precision vs recall.
- Failure signatures: Low reward variance across generations (stuck in local optima), high variance in F1 across runs (unstable RL), or expressions that are too complex to be interpretable.
- First 3 experiments:
  1. Run DSC with default threshold and ùëüùê∂ùê∏ reward; observe F1 vs complexity curve to check if Pareto front forms.
  2. Sweep threshold from 0.5 to 0.9 with ùëüùêπ 1; record precision/recall trade-off and expression stability.
  3. Compare DSC Pareto-optimal expressions to baseline models (XGBoost, RF) on held-out test set for both performance and interpretability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the long-term effectiveness of DSC in adapting to evolving fraud tactics, given the limited temporal scope of the PaySim dataset?
- Basis in paper: [inferred] from the discussion of seasonal fraud behavior and the need for future studies to evaluate DSC's adaptability to evolving fraud tactics.
- Why unresolved: The PaySim dataset covers only a single month, which may not capture the seasonal patterns and evolving nature of real-world fraud. The paper suggests that DSC should adapt well to evolving fraud tactics, but this has not been empirically tested due to data set limitations.
- What evidence would resolve it: Long-term evaluation of DSC on datasets spanning multiple years or on real-world fraud data with evolving patterns would provide evidence of its adaptability to changing fraud tactics.

### Open Question 2
- Question: How does the performance of DSC compare to other explainable models on datasets with different characteristics (e.g., different imbalance ratios, feature types)?
- Basis in paper: [explicit] from the discussion of DSC's performance on the PaySim dataset and the suggestion for future studies to evaluate DSC on different datasets.
- Why unresolved: The paper evaluates DSC on the PaySim dataset, which may have specific characteristics that favor DSC's performance. It is unclear how DSC would perform on datasets with different imbalance ratios, feature types, or domain characteristics.
- What evidence would resolve it: Comparative evaluation of DSC with other explainable models on diverse datasets with varying characteristics would provide insights into DSC's generalizability and performance across different fraud detection scenarios.

### Open Question 3
- Question: What are the optimal strategies for incorporating domain knowledge into DSC to improve its performance and interpretability?
- Basis in paper: [inferred] from the discussion of expert interpretation and the potential for incorporating domain knowledge into DSC.
- Why unresolved: While the paper demonstrates that DSC's expression aligns with expert knowledge, it does not explore strategies for systematically incorporating domain knowledge into the model. This could potentially improve performance and interpretability by guiding the search for relevant features and relationships.
- What evidence would resolve it: Empirical studies investigating the impact of incorporating domain knowledge through feature engineering, operator selection, or reward function design would provide insights into optimal strategies for leveraging domain expertise in DSC.

## Limitations
- The fixed threshold of 0.8 is not justified through systematic validation or sensitivity analysis
- Lack of direct experimental evidence linking expression complexity to human interpretability
- The paper's claims about F1 optimization and interpretability are moderately supported but lack corpus evidence for key mechanisms

## Confidence
- **High confidence**: DSC can generate interpretable analytical expressions and achieve competitive F1 scores
- **Medium confidence**: The Pareto front effectively balances accuracy and interpretability; class imbalance is addressed through F1 optimization
- **Low confidence**: The specific threshold of 0.8 is optimal; expression complexity directly correlates with interpretability

## Next Checks
1. Conduct ablation studies comparing DSC with standard sampling-based methods (SMOTE, undersampling) to verify F1 optimization claims
2. Perform user studies or expert evaluation to empirically validate that Pareto-optimal expressions are more interpretable than black-box models
3. Test threshold sensitivity by sweeping from 0.5 to 0.95 and measuring the impact on precision-recall trade-off and expression stability