---
ver: rpa2
title: Minimizing Dynamic Regret on Geodesic Metric Spaces
arxiv_id: '2302.08652'
source_url: https://arxiv.org/abs/2302.08652
tags: []
core_contribution: This paper tackles online convex optimization (OCO) on complete
  Riemannian manifolds, a challenging setting due to curvature and non-Euclidean geometry.
  While offline Riemannian optimization is well-studied, online learning in such spaces
  has received less attention, especially regarding adaptive regret bounds.
---

# Minimizing Dynamic Regret on Geodesic Metric Spaces

## Quick Facts
- arXiv ID: 2302.08652
- Source URL: https://arxiv.org/abs/2302.08652
- Reference count: 40
- Primary result: Adaptive dynamic regret bounds for online convex optimization on Riemannian manifolds with non-positive curvature

## Executive Summary
This paper tackles online convex optimization (OCO) on complete Riemannian manifolds, extending standard Euclidean techniques to curved spaces. While offline Riemannian optimization is well-studied, online learning in such spaces has received less attention, especially regarding adaptive regret bounds. The authors propose a family of algorithms called RADAR (Riemannian Adaptive Dynamic Regret) that achieve sublinear regret by combining optimistic mirror descent, meta-expert frameworks, and geometric averaging techniques.

The key insight is that by allowing improper learning (choosing decisions from a slightly expanded set) and carefully designing optimism terms, the algorithms can avoid the geometric distortion that typically arises in non-Euclidean settings. The framework bridges a gap between Euclidean and Riemannian online learning, providing both algorithmic tools and theoretical guarantees for adaptive decision-making in curved spaces.

## Method Summary
The paper develops RADAR algorithms for OCO on Hadamard manifolds, using a meta-expert framework where multiple experts run optimistic mirror descent with different step sizes. The meta-algorithm aggregates expert predictions using the Fr'echet mean to respect the manifold's geometry. Three variants achieve adaptive regret bounds in terms of path length, gradient variation, and cumulative loss. The analysis handles improper learning to avoid projection distortion and uses optimistic terms to cancel geometric distortion costs.

## Key Results
- Sublinear dynamic regret bounds on Hadamard manifolds with non-positive curvature
- Adaptive regret bounds matching or closely approximating Euclidean results
- Novel analysis of projective distortion through improper learning
- Tight minimax lower bound showing optimality of regret bounds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimistic mirror descent on manifolds with improper learning enables sublinear regret even when decisions are taken from a slightly expanded set.
- **Mechanism:** By allowing the learner to choose decisions from a slightly larger convex set (NδM) around the true feasible set N, the algorithm avoids the geometric distortion that arises when projecting onto N. The optimism term Mt compensates for the mismatch between the gradient evaluated at yt and the gradient transported to xt, ensuring a telescoping sum structure in the regret bound.
- **Core assumption:** The manifold M is Hadamard and the losses are geodesically smooth. The expansion δM must be small enough relative to the gradient bound G and smoothness constant L.
- **Evidence anchors:** [abstract]: "optimistic mirror descent on manifolds with improper learning"
- **Break condition:** If δM is too large, the gradient bound G no longer holds uniformly over NδM, invalidating the regret bound.

### Mechanism 2
- **Claim:** The Fr'echet mean provides a geometrically consistent way to aggregate expert predictions on manifolds.
- **Mechanism:** The meta-algorithm uses the Fr'echet mean (minimizer of the sum of squared distances) to combine outputs from experts running R-OGD with different step sizes. This averaging respects the manifold's curvature and allows the meta-algorithm to track the best expert in hindsight.
- **Core assumption:** The decision set N is geodesically convex and compact, and the manifold is Hadamard so that unique minimizers exist.
- **Evidence anchors:** [abstract]: "a meta-algorithm combining multiple experts with different step sizes via the Fr'echet mean"
- **Break condition:** If the decision set is not geodesically convex or the manifold has positive curvature, the Fr'echet mean may not exist or may not minimize the surrogate loss.

### Mechanism 3
- **Claim:** Optimistic Hedge works for gsc-convex losses on manifolds, enabling adaptive regret bounds in terms of gradient variation.
- **Mechanism:** The algorithm uses a surrogate loss and optimism based on gradients at the meta-algorithm's average point ¯xt. The design ensures that the negative term in the optimistic Hedge regret bound cancels the geometric distortion introduced by curvature.
- **Core assumption:** Losses are L-gsc-smooth on the manifold, and the meta-algorithm's averaging is done in NδG to avoid distortion.
- **Evidence anchors:** [abstract]: "optimistic online learning algorithms which can be employed on geodesic metric spaces"
- **Break condition:** If the optimism term does not account for the tangent space of ¯xt, the distortion cost may dominate.

## Foundational Learning

- **Concept:** Geodesic convexity and Riemannian gradients
  - **Why needed here:** The algorithms rely on gsc-convexity to ensure that local gradient information can be used to bound regret globally. Riemannian gradients replace Euclidean gradients and must be defined via the inner product on the tangent space.
  - **Quick check question:** Why can't we use standard Euclidean convexity on a manifold?

- **Concept:** Hadamard manifolds and sectional curvature
  - **Why needed here:** The analysis assumes non-positive curvature to ensure unique geodesics, convex distance functions, and well-behaved projections. These properties are essential for the regret bounds.
  - **Quick check question:** What goes wrong if the sectional curvature is allowed to be positive?

- **Concept:** Bregman projections and parallel transport
  - **Why needed here:** Projections onto convex sets on manifolds are not as simple as in Euclidean space. Parallel transport is used to compare tangent vectors at different points, and its cost must be bounded to avoid O(T) regret.
  - **Quick check question:** How does parallel transport differ from a simple vector addition on a manifold?

## Architecture Onboarding

- **Component map:** Expert algorithms -> Meta-algorithm (Fr'echet mean) -> Optimistic Hedge weight update
- **Critical path:** Each expert submits xt,i to meta-algorithm -> Meta-algorithm computes weighted average (Fr'echet or geodesic mean) -> Loss ft is observed; gradients are sent back to experts -> Meta-algorithm updates weights using Optimistic Hedge update
- **Design tradeoffs:** Improper vs proper learning: Improper allows better regret bounds but requires δM to be small. Fr'echet vs geodesic mean: Fr'echet is more stable but may be computationally heavier. Optimistic term design: Must be based on ¯xt to avoid distortion, but this adds complexity.
- **Failure signatures:** Regret grows linearly: Likely due to projection distortion or improper δM choice. Weights collapse to one expert: Possible if the optimism term is not well-tuned. Numerical instability in averaging: May occur if N is not strictly convex or manifold is nearly flat.
- **First 3 experiments:**
  1. Test R-OGD on a simple geodesic convex set (e.g., probability simplex with Fisher-Rao metric) with known comparator path length.
  2. Verify that the Fr'echet mean correctly minimizes the sum of squared distances on a small manifold (e.g., sphere).
  3. Compare Optimistic Hedge with standard Hedge on a synthetic gsc-convex loss sequence to confirm the benefit of the optimism term.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the best-of-both-worlds bound be improved to remove the √lnT factor in Theorem 9?
- **Basis in paper:** [explicit] The paper notes in Remark 4 that the √lnT factor comes from hedging two experts and it would be interesting to remove this dependence.
- **Why unresolved:** The authors acknowledge this limitation but do not provide a method to eliminate the √lnT factor.
- **What evidence would resolve it:** A proof showing the best-of-both-worlds bound can be achieved with only lnT dependence, matching the Euclidean setting.

### Open Question 2
- **Question:** Can adaptive regret bounds be established for proper learning on Riemannian manifolds?
- **Basis in paper:** [inferred] The paper focuses on improper learning and mentions in the conclusion that it would be interesting to study adaptive online learning in the proper setting.
- **Why unresolved:** The technical challenges of projection distortion and averaging on manifolds make it unclear if proper learning can achieve the same regret bounds.
- **What evidence would resolve it:** An algorithm and analysis showing sublinear regret for proper learning with adaptive bounds matching the improper learning results.

### Open Question 3
- **Question:** Can adaptive online learning be extended to the bandit setting on Riemannian manifolds?
- **Basis in paper:** [inferred] The conclusion explicitly mentions the bandit setting as an interesting direction for future work.
- **Why unresolved:** The paper focuses on full-information feedback, and extending to bandit feedback would require new techniques to handle gradient estimation in non-Euclidean geometry.
- **What evidence would resolve it:** An algorithm achieving sublinear regret in the bandit setting with adaptive bounds, along with a matching lower bound.

## Limitations
- The improper learning parameter δM must be carefully tuned relative to gradient bounds and manifold geometry; no explicit recipe is provided for this choice.
- The analysis assumes gsc-convexity and geodesic smoothness, but empirical validation on non-simulated manifolds is not shown.
- Computational cost of Fr'echet/geodesic means on high-dimensional manifolds is not addressed; scalability is unclear.

## Confidence
- High: The regret bound derivation for R-OGD on Hadamard manifolds with non-positive curvature.
- Medium: The meta-algorithm using Fr'echet/geodesic mean to aggregate expert predictions; assumes unique minimizers exist.
- Medium: The optimistic Hedge extension for gsc-convex losses; inspired by Euclidean results but requires careful adaptation.

## Next Checks
1. Implement R-OGD on a known Hadamard manifold (e.g., positive definite matrices) and verify sublinear regret on a synthetic gsc-convex sequence.
2. Test the Fr'echet mean computation on a simple manifold (e.g., sphere) with multiple experts to ensure geometric averaging is correct.
3. Compare the regret of Optimistic Hedge vs standard Hedge on a synthetic sequence with varying gradient variation to confirm the benefit of optimism.