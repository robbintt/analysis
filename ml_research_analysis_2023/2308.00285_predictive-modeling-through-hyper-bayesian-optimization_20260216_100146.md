---
ver: rpa2
title: Predictive Modeling through Hyper-Bayesian Optimization
arxiv_id: '2308.00285'
source_url: https://arxiv.org/abs/2308.00285
tags:
- function
- monotonicity
- space
- optimization
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new way to conduct model selection within
  Bayesian optimization (BO) by integrating model selection and optimization into
  a single, joint framework. The core idea is to use a "hyper" BO to optimize the
  hyperparameters of the GP model used in the underlying BO search for the function
  optima.
---

# Predictive Modeling through Hyper-Bayesian Optimization

## Quick Facts
- arXiv ID: 2308.00285
- Source URL: https://arxiv.org/abs/2308.00285
- Authors: 
- Reference count: 22
- One-line primary result: New HyperBO framework jointly optimizes function and GP model hyperparameters through nested BO, improving convergence on real-world datasets.

## Executive Summary
This paper introduces HyperBO, a framework that integrates model selection and optimization within Bayesian optimization (BO) by using a hyper BO to optimize the GP model's hyperparameters. The framework alternates between BO in function space and BO in model space, using a stationary scoring function based on regret bounds to evaluate model performance. Convergence is theoretically proven, and experiments demonstrate significant improvements over standard BO for both length scale tuning and monotonicity discovery tasks.

## Method Summary
The HyperBO framework treats model selection as a hyperparameter optimization problem by running a nested BO in the space of GP models. After every K iterations, the inner BO (function space) evaluates the current model's performance using a regret-normalized scoring function that accounts for the moving nature of the BO search. This score is fed back to the hyper BO (model space), which selects new hyperparameters for the next K iterations. The framework uses GP-UCB for the inner BO and Thompson Sampling for the hyper BO, with convergence guaranteed under smoothness assumptions about the acquisition function with respect to model hyperparameters.

## Key Results
- HyperBO significantly improves BO performance on length scale tuning tasks compared to standard BO
- The framework successfully discovers monotonicity constraints in GP models when needed
- Theoretical convergence is proven with O(sqrt(T logT)) + T*ϵ*L regret bound, where ϵ represents model selection error

## Why This Works (Mechanism)

### Mechanism 1
HyperBO alternates between BO in function space and BO in model space to optimize both the function and the model simultaneously. The algorithm iteratively selects model hyperparameters using a separate BO in the model space (HyperBO), applies them for K iterations in the function space BO, and then scores the model's effectiveness using a regret-normalized scoring function. This score is fed back to HyperBO to guide the next model selection.

### Mechanism 2
The scoring function uses regret bounds to normalize performance across different BO iterations, making model comparison fair. The score function incorporates the O(sqrt(T γT log(|D|))) regret bound of GP-UCB, normalizing the performance gain by sqrt(logT^(d+1)/T). This makes the score independent of the absolute iteration number T, allowing fair comparison of models selected at different times.

### Mechanism 3
The convergence proof accounts for model selection within the BO framework, extending traditional regret analysis. The convergence theorem shows that even with changing models (θt), the cumulative regret is bounded by O(sqrt(T logT)) + T*ϵ*L, where ϵ is the model selection error. This extends standard BO convergence analysis to the case where the model itself is being optimized.

## Foundational Learning

- Concept: Gaussian Process regression and covariance functions
  - Why needed here: The entire framework relies on GP models to represent the black-box function and its hyperparameters
  - Quick check question: What is the form of the squared exponential kernel used in this paper, and what do its parameters represent?

- Concept: Bayesian Optimization acquisition functions
  - Why needed here: Both the inner BO (function space) and outer BO (model space) use acquisition functions to select points/models
  - Quick check question: What is the difference between GP-UCB and Thompson Sampling, and why might each be used in different parts of this framework?

- Concept: Hyperparameter tuning in machine learning
  - Why needed here: The framework treats model selection as hyperparameter optimization in the space of GP models
  - Quick check question: How does this framework's approach to model selection differ from traditional approaches that periodically update models based on observations?

## Architecture Onboarding

- Component map: HyperBO -> Inner BO -> Scoring function -> GP θ
- Critical path: 1. HyperBO selects initial θ vector, 2. Inner BO runs K iterations with this θ, 3. Scoring function evaluates performance, 4. Score fed back to HyperBO, 5. Repeat until convergence
- Design tradeoffs: K iterations per model vs. frequent model updates, scoring function complexity vs. computational efficiency, model space granularity vs. search efficiency
- Failure signatures: HyperBO fails to converge (scores show high variance), inner BO performance degrades (score improvements are small/negative), computational overhead increases significantly, poor model generalization on new data
- First 3 experiments: 1. Implement HyperBO for length scale tuning on a simple 1D synthetic function (e.g., Branin), 2. Test scoring function sensitivity by comparing performance with and without regret normalization, 3. Validate convergence proof empirically by measuring ||θ* − θtO|| over time for a simple function

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- The computational overhead of running two nested BO processes could be prohibitive for high-dimensional problems or expensive function evaluations
- The framework's performance on non-stationary black-box functions is not explored, despite the theoretical framework potentially extending to such cases
- The experimental validation uses limited hyperparameter ranges and fixed discretization, which may not generalize to all BO scenarios

## Confidence
- Framework's claims about stationarity and convergence: Medium confidence (relies on specific smoothness assumptions that may not hold for complex model spaces)
- Scoring function's effectiveness: Medium confidence (practically beneficial but depends heavily on function landscape)
- Practical applicability claims: Low confidence (shows improvements but computational overhead could be prohibitive)

## Next Checks
1. Test the framework's performance when the true function is non-stationary, measuring how quickly HyperBO detects and adapts to changing optimal model hyperparameters
2. Evaluate the sensitivity of results to the K parameter by running experiments with K ranging from 1 to 100 iterations per model, measuring both performance and computational overhead
3. Validate the convergence proof empirically by deliberately violating the smoothness assumptions (e.g., using acquisition functions with sharp discontinuities) and measuring the impact on convergence behavior