---
ver: rpa2
title: Improving Address Matching using Siamese Transformer Networks
arxiv_id: '2307.02300'
source_url: https://arxiv.org/abs/2307.02300
tags:
- address
- bi-encoder
- addresses
- matching
- cross-encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The research introduces a transformer-based model for Portuguese
  address matching to improve parcel delivery accuracy. The model uses a bi-encoder
  for retrieval and a cross-encoder for reranking, achieving over 95% accuracy at
  the door level.
---

# Improving Address Matching using Siamese Transformer Networks

## Quick Facts
- arXiv ID: 2307.02300
- Source URL: https://arxiv.org/abs/2307.02300
- Reference count: 28
- Over 95% accuracy at the door level for Portuguese address matching

## Executive Summary
This research introduces a transformer-based Siamese network model for Portuguese address matching to enhance parcel delivery accuracy. The system employs a two-stage approach: a bi-encoder for efficient retrieval of top candidates and a cross-encoder for precise reranking. With CP4-based filtering and GPU acceleration, the model achieves over 95% accuracy at the door level while being 4.5 times faster than traditional methods like BM25. The architecture significantly reduces manual intervention in address matching, with real-world implementation currently under investigation.

## Method Summary
The proposed model uses a two-part Siamese transformer architecture for Portuguese address matching. First, a bi-encoder creates dense embeddings of unnormalized addresses and normalized database entries, enabling fast cosine similarity comparisons to retrieve the top 10 candidates. Then, a cross-encoder reranks these candidates using DistilBERT to evaluate address pairs holistically. The system incorporates CP4-based filtering (using the first digit of Portuguese postal codes) to reduce the search space by 9x, significantly improving inference speed without compromising accuracy. The model is fine-tuned using contrastive loss for the bi-encoder and cross-entropy for the cross-encoder, with training data consisting of approximately 430k normalized addresses and 1.1 million unnormalized records.

## Key Results
- Achieved over 95% accuracy at the door level for Portuguese address matching
- 4.5x faster inference than traditional BM25 method with GPU acceleration
- Bi-encoder retrieves correct normalized address in top-10 candidates 99.41% of the time
- Manual review rate reduced to under 10% through probability thresholding

## Why This Works (Mechanism)

### Mechanism 1: Bi-encoder Efficient Retrieval
The bi-encoder produces dense embeddings for unnormalized addresses and normalized database entries, enabling fast cosine similarity comparisons. This produces a small candidate set for the cross-encoder to process in detail. The core assumption is that address embeddings preserve semantic similarity such that correct matches are likely within the top 10 retrieved candidates.

### Mechanism 2: Cross-encoder Precise Reranking
The cross-encoder processes full address pairs simultaneously using DistilBERT, allowing attention across all tokens in both addresses to better resolve subtle matching cues. The core assumption is that full pair attention yields higher matching accuracy than independent embeddings, justifying the extra computation.

### Mechanism 3: CP4-based Filtering
Portuguese postal codes start with a digit (CP4). By creating 9 auxiliary databases keyed by this digit, the system only compares a query to addresses in the same CP4 group. The core assumption is that addresses with different CP4 prefixes are unlikely to match, so filtering by CP4 is safe and effective.

## Foundational Learning

- Concept: Cosine similarity in high-dimensional embedding space
  - Why needed here: The bi-encoder relies on cosine similarity to rank candidate addresses; understanding its properties is essential for tuning thresholds and interpreting results.
  - Quick check question: If two address embeddings have a cosine similarity of 0.95, are they more or less likely to be a match than embeddings with similarity 0.60?

- Concept: Siamese network training with contrastive loss
  - Why needed here: The bi-encoder is trained in a siamese fashion to minimize distance between matching pairs and maximize distance between non-matches; this directly shapes retrieval quality.
  - Quick check question: In contrastive loss, what happens to the gradient when two non-matching embeddings are already farther apart than the margin?

- Concept: Knowledge distillation (DistilBERT)
  - Why needed here: DistilBERT is used as the base transformer; understanding distillation explains why it's faster yet retains most performance.
  - Quick check question: If a teacher model has 12 layers and the student has 6, by what factor does inference time typically reduce?

## Architecture Onboarding

- Component map: Input -> Bi-encoder (embeddings) -> CP4 filtering -> Top-10 retrieval -> Cross-encoder (reranking) -> Probability threshold -> Output

- Critical path: 1) Embed unnormalized address via bi-encoder, 2) Filter database by CP4, 3) Compute cosine similarities to get top 10, 4) Cross-encode each pair for reranking, 5) Apply probability threshold and return result

- Design tradeoffs: Speed vs accuracy (adding cross-encoder improves accuracy but slows inference; CP4 filtering mitigates this), model size vs performance (DistilBERT chosen for balance), top-k selection (Top 10 chosen empirically)

- Failure signatures: Low top-1 accuracy despite high top-10 (bi-encoder embeddings poorly discriminate), high discarded rate after thresholding (threshold too strict), slow inference (CP4 filtering ineffective)

- First 3 experiments: 1) Measure top-1 vs top-10 accuracy of bi-encoder alone to quantify retrieval quality, 2) Sweep probability threshold to find optimal balance of accuracy vs manual review rate, 3) Benchmark inference speed with/without CP4 filtering and with/without cross-encoder to quantify cost of each component

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed model vary with different languages or address formats beyond Portuguese? The model's effectiveness on other languages or address formats is not tested or discussed.

### Open Question 2
What are the potential benefits or drawbacks of using a larger or more complex transformer model instead of DistilBERT for this task? The impact of using different transformer architectures on model performance and efficiency is not investigated.

### Open Question 3
How does the model handle addresses with significant errors or omissions, such as missing ZIP codes or street names? The model's ability to accurately match addresses with substantial errors or missing information is not tested.

## Limitations

- Real-world deployment status remains uncertain with no evidence of production monitoring
- Accuracy claims lack field validation across diverse geographic regions and address formats
- CP4 filtering assumption may not hold for all address types, particularly in rural areas
- Manual review rate claim based on test data but lacks real-world validation

## Confidence

- High confidence: The architectural approach (bi-encoder + cross-encoder) is technically sound and well-supported by the described methodology
- Medium confidence: The 4.5x speed improvement claim depends heavily on hardware configuration and implementation details
- Low confidence: The manual review rate claim and model's robustness to address variations outside training distribution remain unproven

## Next Checks

1. Deploy the model in a controlled pilot program across at least three distinct Portuguese regions with varying address densities and complexity to measure actual manual review rates and identify failure patterns.

2. Conduct an ablation study removing CP4 filtering to quantify its impact on accuracy across different CP4 distributions and identify edge cases where valid matches might be excluded.

3. Test model performance on address formats from other Portuguese-speaking countries (Brazil, Angola, Mozambique) to evaluate generalizability and identify necessary adaptation requirements.