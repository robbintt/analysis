---
ver: rpa2
title: A General Model for Aggregating Annotations Across Simple, Complex, and Multi-Object
  Annotation Tasks
arxiv_id: '2312.13437'
source_url: https://arxiv.org/abs/2312.13437
tags:
- aggregation
- annotation
- annotations
- complex
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides a unified, general framework for modeling and
  aggregating annotations across diverse, complex annotation tasks. We propose distance-based
  aggregation models that bypass the challenge of defining task-specific probabilistic
  models by modeling annotation distances, which can often be easily induced from
  existing evaluation metrics.
---

# A General Model for Aggregating Annotations Across Simple, Complex, and Multi-Object Annotation Tasks

## Quick Facts
- arXiv ID: 2312.13437
- Source URL: https://arxiv.org/abs/2312.13437
- Reference count: 25
- This work provides a unified framework for modeling and aggregating annotations across diverse, complex annotation tasks using distance-based methods.

## Executive Summary
This paper introduces a unified framework for aggregating annotations across simple, complex, and multi-object annotation tasks. The key innovation is modeling distances between annotations rather than the annotations themselves, using existing evaluation metrics as distance functions. This task-agnostic approach enables consistent aggregation across diverse annotation types including translations, text sequences, bounding boxes, rankings, and more. The framework includes several variants from simple distance-based selection to more sophisticated probabilistic models like Multidimensional Annotation Scaling (MAS), which can estimate annotator reliability and item difficulty.

## Method Summary
The framework transforms complex annotations into distance matrices using existing evaluation metrics, then applies general probabilistic models to these distances. The MAS model estimates annotator reliability parameters that weight each annotation inversely proportional to its predicted distance from ground truth. For multi-object tasks, annotations are decomposed into primitive objects, partitioned into clusters corresponding to different objects, then merged and recombined. The framework includes simpler variants (SAD, BAU) for tasks where annotator reliability variation is minimal, and a semi-supervised variant (SMAS) that incorporates gold labels to improve reliability estimation when peer agreement is insufficient.

## Key Results
- Distance-based models (SAD, BAU) perform as well as or better than task-specific bespoke models across diverse annotation types
- Weighted aggregation models (MAS) tend to outperform unweighted methods on real datasets with sufficient annotator reliability variation
- Semi-supervised learning (SMAS) significantly improves aggregation accuracy when peer agreement is insufficient to estimate annotator reliability
- The framework handles a wide variety of tasks including translation, text sequences, bounding boxes, keypoints, ranked lists, syntax trees, and numerical/categorical labeling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distance-based modeling bypasses task-specific model complexity by transforming complex annotations into continuous distance values that existing models can process.
- **Mechanism:** The framework converts complex annotations into distance matrices using existing evaluation metrics, then applies general probabilistic models to these distances rather than the annotations themselves.
- **Core assumption:** Existing evaluation metrics for annotation tasks can serve as distance functions that satisfy basic metric properties (non-negativity, symmetry, triangle inequality).
- **Evidence anchors:** [abstract] "This generality is achieved by devising a task-agnostic method to model distances between labels rather than the labels themselves." [section 4.1] "Our key idea to obviate the need for task-specific models is to model distances between labels, rather than the labels themselves."

### Mechanism 2
- **Claim:** Weighted aggregation outperforms unweighted methods when there is sufficient variation in annotator reliability and enough annotations per item to estimate this variation accurately.
- **Mechanism:** The MAS model estimates annotator reliability parameters that weight each annotation inversely proportional to its predicted distance from ground truth, leveraging consensus patterns across items.
- **Core assumption:** Annotator reliability varies sufficiently across workers and can be estimated from distance patterns between annotations.
- **Evidence anchors:** [abstract] "Our experiments show that distance-based models perform as well or better than task-specific bespoke models, and that weighted models tend to perform best on real datasets." [section 4.4] "Annotations may also be graded and ranked according to this Îµiu, which represents the model's predicted distance from annotation Liu to the best possible annotation."

### Mechanism 3
- **Claim:** Semi-supervised learning significantly improves aggregation accuracy when peer agreement is insufficient to estimate annotator reliability, particularly in difficult skew distributions of worker error.
- **Mechanism:** The SMAS method incorporates known gold labels to directly estimate annotator reliability parameters, bypassing the need to infer them from consensus patterns alone.
- **Core assumption:** A small subset of gold labels can provide sufficient signal to overcome estimation errors when worker error distributions are skewed toward low reliability.
- **Evidence anchors:** [abstract] "We contribute a new general semi-supervised learning method for complex label aggregation that outperforms prior work." [section 4.6] "Our approach is to measure each annotator's average distance from known ground truth labels, and assign those values to the annotator error parameters."

## Foundational Learning

- **Concept:** Distance metrics and evaluation functions
  - Why needed here: The framework relies on converting complex annotations to distance values using existing evaluation metrics. Understanding metric properties (non-negativity, symmetry, triangle inequality) is crucial for selecting appropriate distance functions.
  - Quick check question: Given an evaluation metric that outputs values in [0,1], what transformation would you apply to create a valid distance function that satisfies metric properties?

- **Concept:** Probabilistic modeling and Bayesian inference
  - Why needed here: MAS and MADD models use Bayesian inference to estimate parameters like annotator reliability and item difficulty. Understanding priors, likelihood functions, and parameter estimation is essential.
  - Quick check question: In MAS, why does the model prefer placing annotations closer to the origin when there is high consensus among annotators?

- **Concept:** Clustering algorithms and multi-object annotation
  - Why needed here: For multi-object tasks, the framework uses clustering to partition annotations into groups corresponding to different objects. Understanding agglomerative clustering and linkage criteria is important.
  - Quick check question: How does the number of clusters parameter in the partitioning algorithm relate to the maximum number of objects found by any annotator?

## Architecture Onboarding

- **Component map:** Task owner defines distance function -> Distance matrices computed from all annotation pairs -> Probabilistic model estimates parameters (annotator reliability, item difficulty) -> Model selects/merges/combines annotations based on estimated quality -> Unit tests verify model behavior and parameter estimates

- **Critical path:** 1. Task owner defines distance function for annotation type 2. Distance matrices computed from annotation datasets using task-specific distance functions 3. Probabilistic model estimates parameters (annotator reliability, item difficulty) 4. Model selects/merges/combines annotations based on estimated quality 5. Unit tests verify model behavior and parameter estimates

- **Design tradeoffs:**
  - Distance function choice vs. model complexity: Richer distance functions may improve accuracy but increase computational cost
  - Weighted vs. unweighted aggregation: Weighted methods generally perform better but require reliable annotator reliability estimates
  - Model complexity (MAS vs. MADD): MAS is more complex but generally more accurate; MADD is simpler but may underperform on tasks with large label spaces

- **Failure signatures:**
  - Zero or negative correlation between predicted and actual worker reliability (indicates model mis-specification)
  - Extremely narrow distribution of predicted distances (suggests poor model fit)
  - Poor performance on tasks with difficult skew worker error distributions (indicates need for semi-supervised learning)

- **First 3 experiments:**
  1. Run unit tests on MAS model with simulated binary classification data to verify worker reliability estimation
  2. Compare MAS vs. SAD performance on a simple categorical dataset with known annotator reliability variation
  3. Test semi-supervised learning by reserving 10% of items as gold and comparing SMAS vs. MAS performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do alternative distance functions impact aggregation performance across diverse annotation tasks?
- Basis in paper: [explicit] Section 12.2 discusses the potential benefits of exploring alternative distance functions beyond the inverse evaluation function, mentioning HRRASA's use of richer linguistic information from various natural language representation models.
- Why unresolved: The paper primarily uses the inverse evaluation function as the distance function across tasks, with limited exploration of alternative distance functions.
- What evidence would resolve it: Empirical comparison of aggregation performance using various distance functions (e.g., incorporating different linguistic information, kernel functions) across a diverse set of annotation tasks.

### Open Question 2
- Question: How can the MAS model be extended to support multi-modal annotation spaces and subjective tasks?
- Basis in paper: [explicit] Section 12.4.1 discusses the limitation of MAS in assuming isotropic and unimodal annotation spaces, suggesting potential extension to support subjective tasks with multiple valid responses or differing schools of thought.
- Why unresolved: The paper does not explore methods for extending MAS to handle multi-modal annotation spaces or subjective tasks.
- What evidence would resolve it: Development and empirical evaluation of MAS variants that can identify multiple clusters of annotations and effectively handle subjective tasks with diverse valid responses.

### Open Question 3
- Question: What alternative decompositions of complex annotations can be explored to optimize aggregation performance?
- Basis in paper: [explicit] Section 12.3 discusses the flexibility in decomposing complex annotations into alternative, equivalent primitive representations, suggesting potential for exploring different decompositions and corresponding aggregation operators.
- Why unresolved: The paper focuses on a specific decomposition approach using standard object serialization, without exploring alternative decompositions for various complex annotation types.
- What evidence would resolve it: Empirical comparison of aggregation performance using different decompositions and corresponding aggregation operators for various complex annotation types (e.g., free text, tree structures, semantic segmentation).

## Limitations

- The framework assumes existing evaluation metrics can serve as valid distance functions, which may fail for tasks where metrics violate metric properties or when annotation spaces are too sparse
- Weighted aggregation models require sufficient annotator reliability variation and data to estimate parameters reliably, limiting benefits for datasets with minimal reliability variation
- The semi-supervised variant requires gold labels, which may not be available for many real-world annotation tasks
- Model performance may degrade significantly when data is too sparse to reliably estimate annotator reliability parameters

## Confidence

- **High Confidence**: Distance-based modeling framework design and its ability to handle diverse annotation types through task-agnostic distance computation. The experimental results showing SAD and BAU performing comparably to bespoke models across multiple task types.
- **Medium Confidence**: The superiority of weighted aggregation (MAS) over unweighted methods, as this depends heavily on having sufficient annotator reliability variation and data to estimate parameters accurately. The SMAS semi-supervised method's performance gains also fall into medium confidence due to their dependence on gold label availability.
- **Low Confidence**: The framework's performance on extremely sparse annotation spaces or tasks where evaluation metrics cannot be transformed into valid distance functions. The scalability of MAS for very large label spaces remains uncertain.

## Next Checks

1. **Distance Function Validation**: Test the framework on a task where the evaluation metric is known to violate metric properties (e.g., BLEU score for translation). Verify whether distance-based modeling still provides benefits or if the metric transformation step fails.

2. **Data Sparsity Stress Test**: Apply MAS to datasets with varying numbers of annotators per item (from 2 to 20) and varying annotator pool sizes. Measure how parameter estimation accuracy and model performance degrade as data becomes sparser.

3. **Gold Label Sensitivity Analysis**: For SMAS, systematically vary the proportion of gold labels (0% to 30%) and measure the marginal benefit of each increment. Identify the minimum supervision threshold needed for SMAS to outperform MAS on difficult skew worker error distributions.