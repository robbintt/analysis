---
ver: rpa2
title: 'Active Continual Learning: On Balancing Knowledge Retention and Learnability'
arxiv_id: '2305.03923'
source_url: https://arxiv.org/abs/2305.03923
tags:
- learning
- task
- tasks
- forgetting
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Active Continual Learning (ACL), which addresses
  the problem of data annotation in Continual Learning (CL) by integrating Active
  Learning (AL) into the CL framework. ACL leverages AL to select and annotate training
  data for each incoming task, with the goal of improving the trade-off between retaining
  old knowledge and learning new tasks.
---

# Active Continual Learning: On Balancing Knowledge Retention and Learnability

## Quick Facts
- arXiv ID: 2305.03923
- Source URL: https://arxiv.org/abs/2305.03923
- Authors: 
- Reference count: 40
- Primary result: ACL methods using only 30% of training data achieve comparable performance to CL models trained on full datasets.

## Executive Summary
This paper introduces Active Continual Learning (ACL), which addresses the challenge of data annotation in Continual Learning by integrating Active Learning into the CL framework. ACL leverages AL to select and annotate training data for each incoming task, aiming to improve the trade-off between retaining old knowledge and learning new tasks. The study investigates the effectiveness of combining several AL (entropy, min-margin, BADGE, k-means) and CL (fine-tuning, EWC, ER) methods across domain, class, and task-incremental learning scenarios.

## Method Summary
The ACL framework sequentially processes incoming tasks, using AL methods to select informative samples from an unlabelled pool for annotation. Two labeling strategies are compared: sequential labeling (conditioning queries on previous task annotations) and independent labeling (queries based only on current task). The selected samples are annotated and used to update the CL model (fine-tuning, EWC, or ER). This process repeats until the annotation budget is exhausted. Experiments use RoBERTa-base classifiers on two text classification datasets: Aspect Sentiment Classification (ASC) with 19 domains and 20News with 20 topics split into 10 tasks.

## Key Results
- ACL methods using 30% of training data achieve comparable performance to CL models trained on full datasets
- Sequential labeling outperforms independent labeling, especially for uncertainty-based AL methods in domain-IL
- Experience replay (ER) is consistently the best CL method across all scenarios
- Diversity-based AL methods (KMEANS, BADGE) outperform uncertainty-based methods (ENT, MARG) in class-IL due to model miscalibration

## Why This Works (Mechanism)

### Mechanism 1
Conditioning AL queries on previous task annotations (sequential labelling) improves performance in domain-IL by enabling positive transfer. The model uses its current state, which has been updated with prior task knowledge, to select informative samples for the next task. This allows the selection to be informed by what the model already knows and what it needs to learn next.

### Mechanism 2
In class-IL, uncertainty-based AL methods suffer from model miscalibration on new classes, making diversity-based methods more effective. When new classes are introduced, the model's softmax outputs are poorly calibrated because it hasn't seen these classes before. Uncertainty scores based on these miscalibrated probabilities are unreliable, whereas diversity-based methods select samples that span the representation space regardless of confidence.

### Mechanism 3
Experience replay (ER) is consistently the best CL method because it explicitly combats forgetting by rehearsing old examples during new task training. ER maintains a replay buffer of examples from previous tasks and interleaves them with current task data during training. This prevents the model from overwriting parameters important for past tasks while still learning new ones.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper's central problem is balancing retention of old knowledge with learning new tasks; understanding forgetting is essential to grasp why this is hard.
  - Quick check question: What happens to a neural network's performance on task A when it is trained on task B without any special mechanism?

- Concept: Active learning query strategies (uncertainty vs diversity)
  - Why needed here: The paper compares different AL methods and their effectiveness in different CL scenarios; knowing how these strategies work is crucial.
  - Quick check question: How does entropy-based uncertainty sampling differ from diversity-based k-means sampling in selecting which unlabeled examples to label?

- Concept: Continual learning scenarios (domain-IL, class-IL, task-IL)
  - Why needed here: The experiments cover all three scenarios, and the effectiveness of methods varies significantly across them; understanding these distinctions is necessary.
  - Quick check question: In which scenario are new classes introduced versus new input distributions but same classes?

## Architecture Onboarding

- Component map: RoBERTa-base classifier → Continual Learning module (EWC/ER/ft) → Active Learning module (ENT/MARG/BADGE/KMEANS) → Memory buffer (for ER) → Task sequence manager
- Critical path: Task arrives → CL training on current labeled data → AL query selection → Labeling → CL retraining with new data → Repeat until budget exhausted
- Design tradeoffs: Sequential vs independent labeling trades faster learning for potentially more forgetting; uncertainty vs diversity AL trades reliability of scores for coverage of representation space
- Failure signatures: Sequential labeling shows higher forgetting rates in class-IL; uncertainty-based AL performs poorly when model is miscalibrated on new classes
- First 3 experiments:
  1. Run ER with sequential labeling on domain-IL to verify the positive transfer effect
  2. Run ER with independent labeling on class-IL to confirm lower forgetting but slower learning
  3. Run diversity-based AL (KMEANS) with ER on class-IL to test the miscalibration hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do ACL methods perform on more challenging tasks beyond text classification, such as sequence prediction or computer vision?
- Basis in paper: [explicit] The paper acknowledges limitations by stating that it solely studies text classification and hopes that the simplicity of this task helps isolate factors influencing ACL performance. It leaves exploration of more challenging tasks like sequence prediction as future work.
- Why unresolved: The study only examines ACL on English text classification datasets, so its generalizability to other task types remains untested.
- What evidence would resolve it: Experiments applying ACL to tasks like image classification, speech recognition, or time series prediction, comparing performance to standard CL approaches.

### Open Question 2
- Question: What is the impact of varying annotation budgets on the trade-off between forgetting and learning speed in ACL?
- Basis in paper: [inferred] The paper explores ACL with a fixed annotation budget of 30% of the training data for ASC and 20% for 20News. The forgetting-learning profile suggests that ACL methods face challenges balancing forgetting and learning, but the effect of different budget sizes is not investigated.
- Why unresolved: The study uses predetermined annotation budgets, so the relationship between budget size and the forgetting-learning trade-off is unknown.
- What evidence would resolve it: Experiments varying the annotation budget (e.g., 10%, 20%, 50%) and analyzing how the forgetting rate and learning speed change across different budget levels.

### Open Question 3
- Question: How do more advanced AL and CL algorithms affect the performance and learning dynamics of ACL?
- Basis in paper: [explicit] The paper notes that it does not extensively cover all AL and CL methods proposed in the literature, focusing instead on well-studied algorithms. It suggests that exploring more complicated algorithms could be a direction for future work.
- Why unresolved: The study uses a limited set of AL and CL methods, so the potential benefits of more sophisticated techniques remain unexplored.
- What evidence would resolve it: Experiments incorporating advanced AL methods (e.g., meta-learning-based approaches) and CL methods (e.g., parameter isolation, generative replay) and comparing their performance to the methods used in this study.

## Limitations
- The study only examines ACL on text classification datasets, limiting generalizability to other task types
- Fixed annotation budget of 30% across all tasks without exploring how budget size affects the forgetting-learning trade-off
- Sequential labeling's effectiveness assumes meaningful domain overlap, which may not hold for more disparate domain shifts

## Confidence
- **High confidence**: Experience replay being the most effective CL method - supported by consistent performance across all three scenarios and explicitly shown lower forgetting rates
- **Medium confidence**: Sequential labeling superiority in domain-IL - while results show improvement, the assumption of positive transfer depends heavily on domain similarity which isn't fully characterized
- **Low confidence**: Diversity-based AL superiority in class-IL due to miscalibration - the paper demonstrates better performance but doesn't directly measure model calibration statistics to validate the proposed mechanism

## Next Checks
1. **Calibration analysis**: Compute expected calibration error (ECE) scores for uncertainty-based AL methods in class-IL to directly test whether poor calibration explains their underperformance compared to diversity methods
2. **Domain similarity quantification**: Measure domain overlap metrics (e.g., vocabulary overlap, topic distribution distance) between consecutive tasks in domain-IL to verify that sequential labeling's effectiveness correlates with domain similarity
3. **Budget sensitivity analysis**: Vary the annotation budget (e.g., 10%, 30%, 50%, 70%) across all CL scenarios to determine if sequential labeling maintains its advantage and whether the forgetting-learning profile shifts with budget size