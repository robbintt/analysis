---
ver: rpa2
title: Impact of architecture on robustness and interpretability of multispectral
  deep neural networks
arxiv_id: '2309.12463'
source_url: https://arxiv.org/abs/2309.12463
tags:
- fusion
- channels
- images
- image
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work evaluates multispectral fusion neural networks with
  different underlying architectures in terms of how much attention they pay to different
  input spectral bands (RGB and NIR) and how robust they are to naturalistic image
  corruptions affecting one or more input spectral bands. The experiments find that
  the answers to these two questions correlate as expected: paying more attention
  to RGB channels results in greater sensitivity to RGB corruptions.'
---

# Impact of architecture on robustness and interpretability of multispectral deep neural networks

## Quick Facts
- **arXiv ID**: 2309.12463
- **Source URL**: https://arxiv.org/abs/2309.12463
- **Reference count**: 34
- **Key outcome**: This work evaluates multispectral fusion neural networks with different underlying architectures in terms of how much attention they pay to different input spectral bands (RGB and NIR) and how robust they are to naturalistic image corruptions affecting one or more input spectral bands. The experiments find that the answers to these two questions correlate as expected: paying more attention to RGB channels results in greater sensitivity to RGB corruptions. Interestingly, the experimental results for segmentation models on one dataset contrast with those for classification models on another dataset, suggesting that classification and segmentation models may use multispectral information in quite different ways.

## Executive Summary
This paper investigates how different neural network architectures for multispectral image fusion (early vs late fusion) affect both interpretability and robustness. Using perceptual scores to measure channel importance and testing robustness to natural corruptions, the authors find that models that rely more heavily on RGB channels are more sensitive to RGB corruptions. The study uses two datasets (RarePlanes for classification and US3D for segmentation) and reveals an interesting contrast: while classification models on RarePlanes show expected correlations between channel attention and corruption sensitivity, segmentation models on US3D display different patterns, suggesting classification and segmentation tasks may fundamentally use multispectral information differently.

## Method Summary
The study evaluates early and late fusion architectures on multispectral image datasets. Early fusion concatenates RGB and NIR channels before feeding them to a single backbone network, while late fusion processes each spectral band through separate backbones before combining features. The authors use perceptual scores to quantify channel importance by measuring performance degradation when shuffling one channel. They also test robustness to 15 types of natural corruptions (noise, blur, weather, digital) at five severity levels. Two datasets are used: RarePlanes for aircraft classification and US3D for urban scene segmentation, both providing 4-channel (RGB+NIR) inputs.

## Key Results
- Perceptual scores show that models consistently pay more attention to RGB channels than NIR channels across architectures
- There is a positive correlation between perceptual scores and corruption robustness: models more reliant on a channel are more sensitive to corruptions affecting that channel
- Classification models on RarePlanes and segmentation models on US3D show contrasting behavior, with segmentation models displaying unexpected patterns in channel importance and corruption sensitivity
- Early fusion models generally show higher confidence in predictions but also greater sensitivity to input corruptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Perceptual score correlates with robustness to channel-specific corruptions.
- **Mechanism**: Perceptual score quantifies a model's reliance on a specific input channel by measuring performance degradation when that channel is shuffled with data from other examples. This creates a proxy for channel importance, which in turn predicts sensitivity to corruptions affecting that channel.
- **Core assumption**: Shuffling a channel creates a distribution shift that mimics the information loss from a corruption affecting that channel.
- **Evidence anchors**:
  - [abstract]: "paying more attention to RGB channels results in greater sensitivity to RGB corruptions"
  - [section]: "The perceptual scores presented in the previous section aim to quantify our models' dependence on RGB and NIR inputs. A related question is how robust these models are to naturally occurring corruptions that affect either (or both) of the RGB or NIR inputs."
- **Break Condition**: If channel shuffling doesn't adequately simulate real-world corruption effects, the correlation would break down.

### Mechanism 2
- **Claim**: Fusion architecture choice (early vs late) affects spectral band weighting differently for classification vs segmentation tasks.
- **Mechanism**: Early fusion concatenates spectral bands before feature extraction, leading to equal weighting in the initial feature space. Late fusion allows separate feature extraction before fusion, enabling the model to learn different weights for each band's contribution. The classification and segmentation tasks have different feature importance patterns, leading to divergent weighting behaviors.
- **Core assumption**: The tasks have inherently different feature importance distributions across spectral bands.
- **Evidence anchors**:
  - [abstract]: "Interestingly, the experimental results for segmentation models on one dataset contrast with those for classification models on another dataset, suggesting that classification and segmentation models may use multispectral information in quite different ways."
  - [section]: "Our measurements of perceptual score for segmentation models on US3D, shown in fig. 3 are quite different: they suggest the late fusion model pays even less attention (again, from the perspective of perceptual score) to NIR information than the early fusion model."
- **Break Condition**: If the tasks have similar feature importance distributions, the architecture differences wouldn't manifest differently.

### Mechanism 3
- **Claim**: More input channels provide more robustness to natural corruptions.
- **Mechanism**: Additional input channels increase the model's parameter capacity and provide alternative information pathways when one channel is corrupted. This redundancy allows the model to maintain performance even when individual channels are degraded.
- **Core assumption**: Additional parameters and information pathways translate to robustness rather than overfitting.
- **Evidence anchors**:
  - [section]: "Another possible explanation is that our models exhibit (positive) correlation between accuracy on clean test data and accuracy on corrupted data, as has been previously observed in the literature on robust RGB image classifiers."
  - [section]: "We note that the confidence intervals in the early fusion model overlap; however, these results point in the same general direction as our perceptual score conclusions fig. 2: for early fusion models, RGB channels are weighted more heavily in predictions, and hence performance suffers more when RGB inputs are corrupted."
- **Break Condition**: If additional channels introduce more noise or overfitting, they could decrease robustness.

## Foundational Learning

- **Concept**: Multispectral image processing and spectral band relationships
  - Why needed here: Understanding how different spectral bands (RGB, NIR) capture different physical properties is crucial for interpreting why models might weight them differently
  - Quick check question: Why might NIR be particularly useful for distinguishing vegetation from other materials?

- **Concept**: Deep learning model architecture and fusion strategies
  - Why needed here: The paper contrasts early and late fusion approaches, requiring understanding of how information flows through neural networks
  - Quick check question: How does early fusion differ from late fusion in terms of when spectral bands are combined?

- **Concept**: Robustness evaluation metrics and natural image corruptions
  - Why needed here: The paper uses perceptual scores and evaluates robustness to various natural corruptions, requiring understanding of these evaluation methods
  - Quick check question: What is the difference between adversarial robustness and robustness to natural corruptions?

## Architecture Onboarding

- **Component map**: Input preprocessing (pansharpening, rescaling, gamma correction) -> Model backbone (ResNet34 for classification, ResNet50 for segmentation) -> Fusion strategies (early fusion concatenates RGB+NIR, late fusion processes separately then combines) -> Output heads (classification MLP for RarePlanes, DeepLabv3 segmentation head for US3D) -> Evaluation (perceptual scores, corruption robustness tests)

- **Critical path**: Data preprocessing → Model training → Perceptual score calculation → Corruption robustness evaluation

- **Design tradeoffs**:
  - Early fusion provides simpler architecture but may not capture task-specific band importance
  - Late fusion allows more flexible band weighting but increases model complexity
  - Perceptual scores provide interpretability but may not perfectly correlate with real-world corruption effects

- **Failure signatures**:
  - High perceptual scores but low corruption robustness (indicates metric failure)
  - Similar performance across fusion architectures (indicates task may not benefit from multispectral information)
  - Large confidence intervals in corruption experiments (indicates insufficient sample size)

- **First 3 experiments**:
  1. Train a simple early fusion model on RarePlanes to establish baseline performance
  2. Calculate perceptual scores for the early fusion model to understand band importance
  3. Apply RGB-only corruptions to the early fusion model to test the correlation between perceptual scores and corruption robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do classification and segmentation models differ in their use of multispectral information, and what architectural or training factors drive these differences?
- Basis in paper: [explicit] The paper states "our experimental results for segmentation models on the US3D dataset contrast with those for classification models on the RarePlanes dataset" and notes this suggests classification and segmentation models may use multispectral information in quite different ways.
- Why unresolved: The paper observes this difference but does not investigate the underlying reasons or mechanisms driving the contrasting behavior between classification and segmentation models.
- What evidence would resolve it: Comparative ablation studies varying architectural components (backbone depth, fusion methods), training strategies (loss functions, data augmentation), and input channel importance across both tasks would clarify the drivers of these differences.

### Open Question 2
- Question: Does increasing input channel count (e.g., adding more spectral bands beyond RGB+NIR) consistently improve robustness to naturalistic corruptions?
- Basis in paper: [inferred] The paper notes that when all channels are corrupted, performance drops are similar across models, and speculates that "having more input channels (and hence more parameters) provides more robustness."
- Why unresolved: The experiments only consider RGB+NIR fusion and do not systematically vary the number of input spectral bands to test the hypothesis that more channels lead to greater robustness.
- What evidence would resolve it: Experiments training and evaluating models with varying numbers of input spectral bands (e.g., RGB only, RGB+NIR, RGB+NIR+SWIR) under identical corruption regimes would test whether robustness scales with channel count.

### Open Question 3
- Question: How does perceptual score correlate with model performance on corrupted data, and can it predict robustness across different fusion architectures?
- Basis in paper: [explicit] The paper finds that perceptual scores correlate with corruption robustness (e.g., higher NIR perceptual scores correspond to greater sensitivity to NIR corruptions) and notes that models with greater clean accuracy tend to be more robust.
- Why unresolved: While the paper observes correlations between perceptual scores and corruption robustness, it does not establish whether perceptual score can serve as a reliable predictor of robustness across diverse architectures and datasets.
- What evidence would resolve it: Systematic experiments measuring perceptual scores and corruption robustness across multiple architectures, datasets, and corruption types would determine whether perceptual score can reliably predict robustness and guide architecture selection.

## Limitations

- The correlation between perceptual scores and corruption robustness may be dataset-specific, as the paper only tested on two datasets with different tasks (classification vs segmentation)
- The shuffling-based perceptual score metric may not perfectly capture real-world information importance, as channel shuffling creates a different type of distribution shift than actual corruptions
- The preprocessing pipeline for multispectral images (particularly pansharpening and contrast stretching) could significantly impact results but is not fully specified

## Confidence

- High confidence: The general finding that models pay more attention to RGB channels than NIR channels (supported by perceptual scores on RarePlanes)
- Medium confidence: The correlation between perceptual scores and corruption robustness (based on limited experimental evidence)
- Medium confidence: The contrasting behavior between classification and segmentation models (based on only two datasets)

## Next Checks

1. Apply the same perceptual score and corruption robustness methodology to additional multispectral datasets with different tasks to verify the generalization of the findings
2. Test alternative interpretability metrics (e.g., saliency maps, integrated gradients) alongside perceptual scores to validate the channel importance conclusions
3. Experiment with different preprocessing pipelines for multispectral images to assess sensitivity to these choices and ensure results are robust to implementation details