---
ver: rpa2
title: 'FedLED: Label-Free Equipment Fault Diagnosis with Vertical Federated Transfer
  Learning'
arxiv_id: '2312.17451'
source_url: https://arxiv.org/abs/2312.17451
tags:
- domain
- fault
- diagnosis
- target
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fault diagnosis in industrial
  equipment using Federated Transfer Learning (FTL) when dealing with label scarcity
  and heterogeneous feature spaces across different agents. The proposed method, FedLED,
  is an unsupervised vertical FTL approach that leverages unlabeled target domain
  data for effective model transfer.
---

# FedLED: Label-Free Equipment Fault Diagnosis with Vertical Federated Transfer Learning

## Quick Facts
- arXiv ID: 2312.17451
- Source URL: https://arxiv.org/abs/2312.17451
- Reference count: 40
- Key outcome: Unsupervised vertical federated transfer learning approach for equipment fault diagnosis that achieves up to 4.13× higher diagnosis accuracy than state-of-the-art methods.

## Executive Summary
This paper presents FedLED, an unsupervised vertical federated transfer learning approach for equipment fault diagnosis when dealing with label scarcity and heterogeneous feature spaces across different agents. The method addresses the challenge of diagnosing faults in target equipment using only unlabeled data by leveraging labeled data from source equipment with different feature spaces. FedLED employs a vertical federated joint domain adversarial adaptation to map heterogeneous features to a common latent space and a joint domain alignment to minimize the distance between source and target label distributions. Experimental results on real equipment monitoring data demonstrate significant performance improvements over existing methods.

## Method Summary
FedLED is an unsupervised vertical federated transfer learning approach that combines vertical federated joint domain adversarial adaptation with joint domain alignment to address equipment fault diagnosis challenges. The method maps heterogeneous source and target features to a common latent space using a conditional domain discriminator, then minimizes the distance between source label distributions and target classification results. The training process uses pre-training-fine-tuning initialization to accelerate convergence and includes a dynamic sample weight method to avoid negative samples affecting training. The approach is evaluated on two public datasets containing monitoring signals from different equipment types, demonstrating superior performance compared to state-of-the-art methods.

## Key Results
- FedLED achieves up to 4.13× higher diagnosis accuracy than state-of-the-art methods on real equipment monitoring data
- The method successfully handles heterogeneous feature spaces across different agents in the federated setting
- FedLED demonstrates strong generality across various vertical federated transfer learning settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: FedLED addresses both feature space heterogeneity and label scarcity by combining vertical federated joint domain adversarial adaptation with joint domain alignment.
- **Mechanism**: The vertical federated joint domain adversarial adaptation maps heterogeneous source and target features to a latent common space using a conditional domain discriminator. The joint domain alignment then minimizes the distance between the source label distribution and the target classification result distribution, enabling effective transfer without target labels.
- **Core assumption**: The source and target domains share the same fault distribution in the same label space, allowing alignment of label distributions despite feature heterogeneity.
- **Evidence anchors**:
  - [abstract]: "employs a vertical federated joint domain adversarial adaptation to map heterogeneous features to a common latent space and a joint domain alignment to minimize the distance between source and target label distributions"
  - [section]: "In FedLED, a vertical federated joint domain adversarial adaptation is proposed to map heterogeneous source and target features to a common latent space. To enhance the effectiveness of zero fault label model transferring, we construct a novel joint domain alignment that minimizes the distance between the source label distribution and the target classification result distribution"

### Mechanism 2
- **Claim**: FedLED uses pre-training-fine-tuning initialization to accelerate convergence and prevent overfitting.
- **Mechanism**: The federated model initialization adopts pre-training on the source domain data followed by fine-tuning, which reduces training time and speeds up convergence compared to training from scratch.
- **Core assumption**: Pre-training on the source domain provides a good starting point for the model that can be adapted to the target domain.
- **Evidence anchors**:
  - [section]: "The federated initializaiton adopts the pre-training-fine-tuning method demonstrated as effective in [36]. Compared with training from scratch, pre-training the model reduces training time and speeds up the training convergence."

### Mechanism 3
- **Claim**: The dynamic sample weight method in FedLED helps avoid negative samples affecting training.
- **Mechanism**: Sample weights are updated based on the classifier's probability outputs, giving more weight to samples that are harder to classify and less weight to those that might be negatively affecting the training process.
- **Core assumption**: Samples that the classifier finds difficult to classify contain more useful information for adaptation than those it classifies easily.
- **Evidence anchors**:
  - [section]: "At the same time, an additional dynamic sample weight method is used to avoid negative samples from affecting training. The update method of the sample weight is as Eq.(4), where p represents the probability that the classifier finally predicts each category"

## Foundational Learning

- **Concept**: Federated Transfer Learning (FTL)
  - Why needed here: Enables knowledge transfer across agents with privacy constraints while addressing feature heterogeneity and label scarcity
  - Quick check question: What is the key difference between horizontal and vertical federated learning in the context of fault diagnosis?

- **Concept**: Domain adaptation and adversarial learning
  - Why needed here: Required to align feature spaces and distributions across heterogeneous source and target domains
  - Quick check question: How does a conditional domain discriminator differ from a standard domain discriminator in adversarial domain adaptation?

- **Concept**: Unsupervised learning with zero target labels
  - Why needed here: The target domain has zero fault labels, requiring methods that can leverage unlabeled data effectively
  - Quick check question: What are the key challenges of unsupervised domain adaptation compared to supervised domain adaptation?

## Architecture Onboarding

- **Component map**: Feature extractors (FS for source, FT for target) -> Classifiers (shared C) -> Domain discriminator (D) -> Dynamic sample weighting module
- **Critical path**: Source feature extraction → classifier → domain discriminator (adversarial training) → joint domain alignment → target feature extraction → classification
- **Design tradeoffs**:
  - Balancing feature alignment strength (λ) vs. classification accuracy
  - Pre-training benefits vs. potential source domain overfitting
  - Communication overhead in federated setting vs. privacy preservation
- **Failure signatures**:
  - Poor convergence indicated by oscillating loss values
  - Degraded performance on highly heterogeneous feature spaces
  - Overfitting to source domain when pre-training is too strong
- **First 3 experiments**:
  1. Verify feature alignment by checking latent space similarity between source and target features using visualization techniques
  2. Test the impact of dynamic sample weighting by comparing with and without this component
  3. Evaluate the necessity of joint domain alignment by comparing with only adversarial adaptation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The method requires source domain data with labels to be available, which may not always be the case in real-world scenarios
- Performance heavily depends on the assumption that source and target domains share the same fault distribution, which may not hold for all equipment types
- The paper does not thoroughly explore the impact of communication overhead in the federated setting, which is critical for real-world deployment

## Confidence
- **High Confidence**: The core mechanism of using joint domain adversarial adaptation for feature alignment is well-established in the domain adaptation literature
- **Medium Confidence**: The specific combination of vertical federated learning with zero-label target domains is novel, but the individual components have been validated separately
- **Medium Confidence**: The reported performance improvements are based on experiments with two specific datasets (CWRU and Gearbox), which may not generalize to all equipment types

## Next Checks
1. Test FedLED's performance when source and target domains have different fault distributions to verify the robustness of the joint domain alignment mechanism
2. Evaluate the impact of communication efficiency by measuring the total number of gradient exchanges required for convergence across different dataset sizes
3. Validate the method on additional equipment datasets with varying degrees of feature heterogeneity to assess generalizability beyond the two tested datasets