---
ver: rpa2
title: Edit Distance based RL for RNNT decoding
arxiv_id: '2306.01789'
source_url: https://arxiv.org/abs/2306.01789
tags:
- rnn-t
- training
- speech
- loss
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an edit-distance-based reinforcement learning
  (EDRL) approach for RNN-T decoding in speech recognition. The key idea is to minimize
  the discrepancy between training and inference by using an action-level policy gradient
  and edit-distance-based rewards.
---

# Edit Distance based RL for RNNT decoding

## Quick Facts
- arXiv ID: 2306.01789
- Source URL: https://arxiv.org/abs/2306.01789
- Reference count: 0
- Primary result: EDRL achieves 1.4% WER on LibriSpeech dev and 2.6% WER on test-other set for 600M Conformer RNN-T model

## Executive Summary
This paper introduces an edit-distance-based reinforcement learning (EDRL) approach for RNN-T decoding in speech recognition. The method addresses the exposure bias problem in RNN-T models by training at the action level rather than the sentence level, using edit distance as rewards for individual subword predictions. The proposed approach minimizes the discrepancy between training and inference by generating hypotheses via beam search and computing incremental rewards based on alignment quality.

## Method Summary
The EDRL method trains RNN-T models using a two-stage pipeline: first pre-training with standard RNN-T loss, then fine-tuning with a combined RL and RNN-T objective. During fine-tuning, beam search generates hypotheses from which action-level rewards are computed based on edit distance between partial hypotheses and ground truth. These rewards are used in a policy gradient update to optimize the model for beam search inference rather than teacher forcing. The approach uses a 600M parameter W2v-BERT Conformer XL model trained on LibriSpeech with log mel features and 1k WordPiece units.

## Key Results
- Achieves state-of-the-art 1.4% WER on LibriSpeech dev set
- Achieves state-of-the-art 2.6% WER on LibriSpeech test-other set
- Improves over baseline RNN-T models by addressing training-inference discrepancy through action-level RL
- Demonstrates effectiveness of edit-distance-based rewards versus sentence-level MWER

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Action-level RL reduces exposure bias by training the model to handle its own mistakes during inference
- Mechanism: Traditional RNN-T teacher-forcing never allows the model to experience its own predictions as inputs, creating a distribution mismatch between training and inference. The proposed method generates hypotheses via beam search and then computes rewards at the action level based on edit distance, providing training signals for every subword prediction, including errors
- Core assumption: The distribution of errors encountered during beam search approximates the distribution of errors the model will make at inference time
- Evidence anchors: [abstract] "RNN-T's inability to experience mistakes during teacher forcing training makes it more problematic when a mistake occurs in inference"; [section] "the RNN-T model's training objective is to maximize the probabilities of all possible alignments, which differs substantially from the beam search approach"

### Mechanism 2
- Claim: Edit distance-based rewards provide more granular and meaningful training signals than sentence-level MWER
- Mechanism: Instead of waiting until the end of a sentence to compute rewards, the method calculates the incremental edit distance for each action. This assigns specific penalties to incorrect subword predictions and positive rewards to correct ones, creating a richer learning signal
- Core assumption: The edit distance between partial hypotheses and ground truth is a meaningful proxy for the quality of individual actions
- Evidence anchors: [section] "We present a functional RL algorithm for the RNN-T model that generates a training signal for each token rather than for the entire sentence"; [section] "MWER is commonly employed in industry as a crucial step in production models, typically following RNN-T pretraining. Nevertheless, this training approach is a sentence-level policy gradient, which results in poor sample efficiency"

### Mechanism 3
- Claim: The proposed method bridges the gap between training objective and inference method by optimizing beam search directly
- Mechanism: Instead of maximizing all alignment probabilities during training, the method uses beam search to generate hypotheses and then applies RL to optimize the model for finding better alignments during inference. This directly addresses the discrepancy between teacher-forcing and beam search
- Core assumption: Beam search with the proposed reward function will find better alignments than standard RNN-T training
- Evidence anchors: [abstract] "During training, RNN-T maximizes all alignment probabilities by teacher forcing, while during inference, it uses beam search which may not necessarily find the maximum probable alignment"; [section] "The training procedure for our model consists of two stages. Firstly, we conduct regular training of RNN-T model. Once a converged checkpoint is obtained, we proceed to further fine-tune the model using the RL objective"

## Foundational Learning

- Concept: Policy gradient methods in reinforcement learning
  - Why needed here: The method uses actor-critic style policy gradient to update the RNN-T model based on action-level rewards
  - Quick check question: What distinguishes policy gradient from value-based RL methods, and why is policy gradient more suitable for sequence generation tasks?

- Concept: Edit distance computation and its relationship to WER
  - Why needed here: The reward function is based on edit distance between predicted and reference sequences, which directly correlates with word error rate
  - Quick check question: How does edit distance differ from Levenshtein distance, and why is it an appropriate metric for ASR evaluation?

- Concept: Beam search algorithm and its variants
  - Why needed here: The method uses beam search to generate hypotheses for computing rewards, requiring understanding of beam expansion, pruning, and scoring mechanisms
  - Quick check question: What are the key parameters that control beam search behavior, and how do they trade off between search quality and computational cost?

## Architecture Onboarding

- Component map:
  Audio features -> RNN-T encoder -> RNN-T decoder -> Beam search -> Edit distance calculator -> RL loss -> RNN-T model updates

- Critical path:
  1. Audio features â†’ RNN-T encoder
  2. RNN-T decoder generates subword probabilities
  3. Beam search creates hypotheses from probabilities
  4. Edit distance computed between hypotheses and ground truth
  5. Rewards assigned to individual actions
  6. Policy gradient updates applied to RNN-T model

- Design tradeoffs:
  - Reward granularity vs. computational cost: Action-level rewards provide better signals but require more computation than sentence-level rewards
  - Beam search parameters vs. training stability: Wider beams provide better coverage but increase variance in training signals
  - RL loss weight vs. convergence speed: Higher RL weights may accelerate improvements but risk destabilizing RNN-T pretraining

- Failure signatures:
  - Model degradation in WER despite RL training completion
  - Training instability with exploding gradients or NaN values
  - Overfitting to beam search hypotheses that don't generalize to actual inference

- First 3 experiments:
  1. Baseline comparison: Measure WER improvement from RNN-T pretraining alone to RNN-T + RL training
  2. Reward function ablation: Compare action-level vs. sentence-level rewards while keeping other parameters constant
  3. Beam search parameter sweep: Evaluate impact of beam width and top-k on final WER performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed EDRL approach compare to other RL methods, such as MWER, in terms of sample efficiency and convergence speed?
- Basis in paper: [explicit] The paper mentions that MWER suffers from poor sample efficiency due to its sentence-level policy gradient, and that the EDRL approach generates a training signal for each token rather than for the entire sentence
- Why unresolved: The paper does not provide a direct comparison of sample efficiency and convergence speed between EDRL and other RL methods like MWER
- What evidence would resolve it: A direct comparison of the sample efficiency and convergence speed of EDRL and other RL methods, such as MWER, on the same dataset and model architecture

### Open Question 2
- Question: Can the proposed EDRL approach be extended to other sequence-to-sequence tasks beyond speech recognition, such as machine translation or text summarization?
- Basis in paper: [inferred] The paper focuses on applying the EDRL approach to RNN-T decoding in speech recognition, but the underlying concept of minimizing the discrepancy between training and inference objectives using action-level policy gradients and edit-distance-based rewards could potentially be applied to other sequence-to-sequence tasks
- Why unresolved: The paper does not explore the applicability of the EDRL approach to other sequence-to-sequence tasks beyond speech recognition
- What evidence would resolve it: Experiments applying the EDRL approach to other sequence-to-sequence tasks, such as machine translation or text summarization, and comparing the results to state-of-the-art methods in those domains

### Open Question 3
- Question: How does the proposed EDRL approach perform on streaming ASR tasks, where the model needs to generate transcriptions in real-time as the audio is being received?
- Basis in paper: [explicit] The paper mentions that RNN-T models offer seamless support for streaming ASR, but does not specifically address the performance of the EDRL approach on streaming ASR tasks
- Why unresolved: The paper does not provide any experiments or analysis of the EDRL approach on streaming ASR tasks
- What evidence would resolve it: Experiments evaluating the performance of the EDRL approach on streaming ASR tasks, comparing the results to state-of-the-art streaming ASR methods

## Limitations
- Edit distance reward mechanism assumes incremental edit distance meaningfully reflects individual subword prediction quality without empirical validation
- Method requires substantial computational resources (32 TPU v3 cores) and complex two-stage training pipeline
- Paper doesn't provide analysis of model robustness to different error types or varying audio conditions

## Confidence
- High: The core mechanism of using action-level RL to address exposure bias is technically valid and well-grounded in RL theory
- Medium: The experimental results showing WER improvements are reproducible given the specified implementation details, but the absolute performance claims depend on specific model architectures and hyperparameters
- Low: The paper's claims about edit distance-based rewards being superior to sentence-level rewards lack direct comparative ablation studies within the same experimental setup

## Next Checks
1. Implement and compare action-level edit distance rewards vs. sentence-level MWER rewards using identical model architectures and training procedures to quantify the specific contribution of reward granularity

2. Conduct detailed error analysis on the EDRL model to identify which types of errors (insertions, deletions, substitutions) show the most improvement, and whether the model develops any new failure modes

3. Evaluate the EDRL approach on a different speech recognition dataset (such as Common Voice or TED-LIUM) to assess whether the training-inference alignment benefits transfer beyond LibriSpeech