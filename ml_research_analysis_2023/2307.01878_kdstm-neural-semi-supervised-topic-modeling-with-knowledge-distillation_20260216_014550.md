---
ver: rpa2
title: 'KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation'
arxiv_id: '2307.01878'
source_url: https://arxiv.org/abs/2307.01878
tags:
- topic
- modeling
- knowledge
- methods
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes KDSTM, a novel neural semi-supervised topic
  modeling approach that combines knowledge distillation and optimal transport. The
  key idea is to leverage a small set of labeled documents to guide the topic extraction
  process in an unsupervised topic model like ETM.
---

# KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation

## Quick Facts
- arXiv ID: 2307.01878
- Source URL: https://arxiv.org/abs/2307.01878
- Authors: Not specified in the provided text
- Reference count: 20
- Primary result: Outperforms supervised topic modeling methods and achieves comparable performance to weakly supervised text classification methods

## Executive Summary
This paper introduces KDSTM, a novel neural semi-supervised topic modeling approach that combines knowledge distillation and optimal transport. The key innovation is leveraging a small set of labeled documents to guide topic extraction in an unsupervised topic model like ETM. KDSTM aligns topics with labels using optimal transport and transfers knowledge from labeled to unlabeled documents through knowledge distillation. The method achieves strong classification performance while requiring minimal labeled data and no pretrained embeddings.

## Method Summary
KDSTM employs a three-stage training process for semi-supervised topic modeling. First, it trains standard topic modeling using KL divergence and reconstruction loss. Second, it adds an optimal transport loss to align topic distributions with label groupings. Third, it incorporates knowledge distillation to transfer semantic structure from labeled to unlabeled documents. The model uses von Mises-Fisher distribution for better clusterability and employs a bag-of-words representation for documents. The approach is evaluated on three text classification datasets (AG's News, DBLP, 20News) with four classes each.

## Key Results
- Outperforms existing supervised topic modeling methods
- Achieves comparable performance to weakly supervised text classification methods
- Requires no pretrained embeddings and few labeled documents
- Demonstrates efficiency in training compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal transport loss aligns topics with label distributions, improving classification accuracy.
- Mechanism: The cost matrix M in equation 4 penalizes misalignment between topic distributions and label groupings. By minimizing the Sinkhorn distance, the model learns to assign each topic to the most semantically similar label group.
- Core assumption: Label groups contain sufficient semantic coherence for meaningful alignment with topics.
- Evidence anchors:
  - [abstract]: "KDSTM uses optimal transport to align topics with labels"
  - [section 3]: "We adopt optimal transport to assign topics to labels"
  - [corpus]: Weak - corpus lacks explicit mention of optimal transport effectiveness
- Break condition: If label groups are too small or heterogeneous, the cost matrix becomes uninformative and alignment degrades.

### Mechanism 2
- Claim: Knowledge distillation transfers semantic structure from labeled to unlabeled documents.
- Mechanism: Equation 5 computes similarity between unlabeled documents and labeled groups, creating a teacher distribution. Equation 6 then uses cross-entropy to distill this knowledge into the encoder's topic distributions.
- Core assumption: Semantic similarity in embedding space correlates with topic relevance.
- Evidence anchors:
  - [abstract]: "knowledge distillation to transfer the knowledge from the labeled documents to the unlabeled ones"
  - [section 3]: "we borrow the idea from similarity and feature based knowledge distillation"
  - [corpus]: Moderate - neighboring papers discuss similar distillation approaches
- Break condition: If the labeled set is too small, the similarity matrix becomes sparse and distillation loses effectiveness.

### Mechanism 3
- Claim: vMF latent distribution improves topic clustering quality.
- Mechanism: The von Mises-Fisher distribution constrains topic vectors to lie on a hypersphere, encouraging well-separated clusters. The KL divergence term regularizes the encoder to produce these distributions.
- Core assumption: Topic representations benefit from spherical geometry rather than Euclidean space.
- Evidence anchors:
  - [section 3]: "We also use VMF distribution instead of normal distribution as the latent distribution for better clusterability"
  - [appendix C]: Provides mathematical formulation and cites supporting papers
  - [corpus]: Weak - corpus doesn't directly address vMF benefits
- Break condition: If dimensionality is too high, the vMF assumption breaks down and clustering performance degrades.

## Foundational Learning

- Concept: Variational Autoencoders
  - Why needed here: KDSTM uses VAE framework to model document-topic distributions
  - Quick check question: How does the evidence lower bound relate to the reconstruction and KL losses?

- Concept: Optimal Transport Theory
  - Why needed here: The model uses Sinkhorn distance to align topic and label distributions
  - Quick check question: What does the entropy regularization parameter λ control in the transport plan?

- Concept: Knowledge Distillation Principles
  - Why needed here: Soft targets from labeled documents guide unlabeled document representation
  - Quick check question: Why does using temperature τ > 1 in the softmax help knowledge transfer?

## Architecture Onboarding

- Component map:
  - Encoder (ϕ): Bag-of-words → vMF parameters (µ, κ)
  - Decoder: vMF sample × embedding matrices → reconstruction
  - Optimal Transport: Aligns topic distributions with label groups
  - Knowledge Distillation: Transfers semantic structure from labeled to unlabeled
  - Loss functions: Reconstruction + KL + OT + KD

- Critical path: Encoder → Similarity computation → KD loss → Topic quality
- Design tradeoffs: vMF vs Gaussian latent distribution (clusterability vs flexibility)
- Failure signatures: 
  - High KL loss → poor encoder learning
  - Low OT loss but poor classification → misaligned cost matrix
  - High reconstruction loss → embedding mismatch

- First 3 experiments:
  1. Train without KD loss to establish baseline performance
  2. Add OT loss and monitor topic-label alignment quality
  3. Enable full KD and compare classification metrics against baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does KDSTM perform on languages with significantly different structures from English, such as logographic languages like Chinese or morphologically rich languages like Finnish?
- Basis in paper: [inferred] The paper states that KDSTM "does not rely on any transfer learning or pretrained language models" and "is suitable for less common/endangered languages," suggesting it could be applicable to various languages.
- Why unresolved: The experiments were conducted on English text datasets (AG's News, DBLP, 20News). The authors do not provide results or analysis for non-English languages.
- What evidence would resolve it: Empirical results comparing KDSTM's performance across multiple languages with diverse linguistic structures would provide insights into its cross-lingual applicability.

### Open Question 2
- Question: What is the optimal number of labeled documents per topic for achieving the best balance between performance and resource efficiency in KDSTM?
- Basis in paper: [explicit] The paper mentions that KDSTM "only requires a limited number of labeled documents as input" and uses 5 documents per class in experiments, but does not explore the impact of varying this number.
- Why unresolved: The authors only test with a fixed number of labeled documents (5 per class) and do not investigate how performance scales with different amounts of labeled data.
- What evidence would resolve it: A systematic study varying the number of labeled documents per topic while measuring classification accuracy, training time, and model robustness would reveal the optimal trade-off.

### Open Question 3
- Question: How does KDSTM's performance compare to state-of-the-art few-shot learning methods for text classification?
- Basis in paper: [inferred] While the paper compares KDSTM to weakly supervised methods and supervised topic modeling, it does not benchmark against modern few-shot learning approaches specifically designed for text classification.
- Why unresolved: The comparison is limited to methods mentioned in the related work section, excluding more recent few-shot learning techniques that could be relevant.
- What evidence would resolve it: Direct experimental comparison of KDSTM with few-shot learning methods like prototypical networks or matching networks on text classification tasks would clarify its relative effectiveness in low-resource settings.

## Limitations

- Evaluation limited to three datasets with four classes each, constraining generalizability
- Poor performance on DBLP dataset (60.54% accuracy) raises concerns about robustness on smaller datasets
- No systematic sensitivity analysis for critical hyperparameters in the optimal transport and knowledge distillation components

## Confidence

**High Confidence** in the core methodology: The mathematical formulation of the optimal transport and knowledge distillation components is sound and well-defined. The three-stage training procedure is clearly specified and reproducible.

**Medium Confidence** in performance claims: While KDSTM outperforms supervised topic modeling methods, the absolute performance on DBLP is concerning. The paper doesn't adequately explain why this dataset performs significantly worse than the others.

**Low Confidence** in scalability claims: The paper asserts efficiency advantages but provides no computational complexity analysis or runtime comparisons. The optimal transport computation could become prohibitive for larger datasets.

## Next Checks

1. **Dataset Diversity Validation**: Test KDSTM on datasets with varying class distributions, document lengths, and vocabulary sizes to assess robustness beyond the current three datasets. Include both balanced and imbalanced scenarios.

2. **Ablation Study Rigor**: Systematically remove each component (OT loss, KD loss, vMF distribution) and measure the exact contribution of each to classification performance. The current analysis mentions ablation but lacks quantitative breakdowns.

3. **Computational Efficiency Benchmark**: Measure training time, memory usage, and inference latency across different dataset sizes. Compare against stated efficiency claims and provide Big-O complexity analysis for the optimal transport component.