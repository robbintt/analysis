---
ver: rpa2
title: Probing Intersectional Biases in Vision-Language Models with Counterfactual
  Examples
arxiv_id: '2310.02988'
source_url: https://arxiv.org/abs/2310.02988
tags:
- male
- female
- images
- counterfactual
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for probing intersectional biases
  in vision-language models (VLMs) by leveraging text-to-image diffusion models to
  generate counterfactual examples. The key idea is to use Stable Diffusion with cross-attention
  control to create image-text pairs that differ only in social attributes (e.g.,
  race and gender), enabling precise bias measurement.
---

# Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples

## Quick Facts
- arXiv ID: 2310.02988
- Source URL: https://arxiv.org/abs/2310.02988
- Reference count: 32
- Key outcome: This paper introduces a framework for probing intersectional biases in vision-language models (VLMs) by leveraging text-to-image diffusion models to generate counterfactual examples.

## Executive Summary
This paper introduces a framework for probing intersectional biases in vision-language models (VLMs) by leveraging text-to-image diffusion models to generate counterfactual examples. The key idea is to use Stable Diffusion with cross-attention control to create image-text pairs that differ only in social attributes (e.g., race and gender), enabling precise bias measurement. A dataset of 232k counterfactual captions and 23.2M generated images was created, covering occupations and personality traits with race, gender, religion, and physical characteristics. Experiments on state-of-the-art VLMs (ALIP, CLIP, and SLIP) reveal significant intersectional biases, with SLIP showing the highest skew across all attributes and CLIP exhibiting lower but still notable biases. For example, SLIP strongly prefers retrieving images of white male doctors, while ALIP favors Indian and Middle Eastern male doctors. The study highlights the importance of measuring intersectional biases and provides a scalable methodology for future research.

## Method Summary
The methodology leverages text-to-image diffusion models with cross-attention control to generate counterfactual image-text pairs that differ only in intersectional social attributes. Using Stable Diffusion's Prompt-to-Prompt technique, the approach generates highly similar images while varying specific attributes like race and gender. The process involves creating counterfactual captions for occupations and personality traits, generating matching images with controlled attribute differences, filtering with CLIP similarity, and calculating bias metrics (Skew@K and MaxSkew@K) to quantify intersectional biases in state-of-the-art VLMs.

## Key Results
- SLIP shows the highest intersectional bias skew across all attributes, with strong preference for white male doctors
- ALIP exhibits biases favoring Indian and Middle Eastern male doctors
- CLIP demonstrates lower but still notable biases compared to other VLMs tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention control in diffusion models isolates counterfactual changes to only the targeted social attributes.
- Mechanism: Prompt-to-Prompt uses shared cross-attention maps across denoising steps to keep non-social-attribute features (e.g., background, pose) consistent between generated images, while varying only the social attributes specified in the text prompt.
- Core assumption: The cross-attention injection reliably decouples visual differences from non-target attributes during image synthesis.
- Evidence anchors:
  - [abstract] "Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race & gender)."
  - [section] "Brooks et al. [2] noted that some changes require varying the parameter p in Prompt-to-Prompt, which controls the number of denoising steps with shared attention weights."
  - [corpus] Weak evidence for cross-attention isolation robustness in non-occupation domains.
- Break condition: If cross-attention injection fails to maintain subject consistency, the bias measurement becomes confounded by unrelated visual changes.

### Mechanism 2
- Claim: Measuring MaxSkew@K aggregates intersectional bias by comparing the proportion of retrieved images with specific attribute pairs to their expected proportions.
- Mechanism: For each counterfactual caption, a neutral prompt is constructed (removing race/gender terms). The VLM retrieves top-K images, and Skew@K quantifies over- or under-representation of each attribute pair. MaxSkew@K takes the maximum Skew@K across all pairs, highlighting the worst-case bias.
- Core assumption: The neutral prompt accurately represents the desired proportion distribution for each subject-attribute combination.
- Evidence anchors:
  - [section] "Skew@K and MaxSkew@K are then calculated by retrieving the top-K images for the computed text embedding from the set of all images generated for the subject which met our filtering and selection criteria."
  - [section] "We construct neutral prompts in this manner for each unique combination of prefixes and subjects, averaging their CLIP representations across different prefixes to obtain a single text embedding for each subject."
  - [corpus] Weak evidence that averaging CLIP embeddings yields truly neutral semantic representation.
- Break condition: If the neutral prompt inadvertently encodes attribute preferences, the Skew@K metric will misestimate bias magnitude.

### Mechanism 3
- Claim: Synthetic counterfactual examples scale bias probing to exhaustive attribute combinations without reliance on limited real-world datasets.
- Mechanism: By programmatically generating captions for all attribute-subject pairs and then generating matching images with cross-attention control, the approach creates a comprehensive test set covering all intersectional combinations.
- Core assumption: The diffusion model can faithfully represent all specified attribute combinations without mode collapse or generation artifacts.
- Evidence anchors:
  - [abstract] "We conduct extensive experiments using our generated dataset which reveal the intersectional social biases present in state-of-the-art VLMs."
  - [section] "In total, we generate 23.2M images for 232k captions."
  - [corpus] No direct evidence that 23.2M images cover all intersectional modes without generation failure.
- Break condition: If the diffusion model fails to generate certain attribute combinations (e.g., due to training data bias), the dataset will be incomplete and bias estimates will be skewed.

## Foundational Learning

- Concept: Counterfactual examples
  - Why needed here: They isolate the effect of changing one or more attributes while holding others constant, enabling precise bias attribution.
  - Quick check question: In the counterfactual set for "White male doctor" vs. "Black female doctor," which features are held constant by the generation method?

- Concept: Cross-attention in diffusion models
  - Why needed here: Cross-attention maps control which image regions attend to which text tokens, enabling selective modification of specified attributes.
  - Quick check question: What would happen to the background if cross-attention control was not used when switching from "White male" to "Black female" in a prompt?

- Concept: Skew metrics for bias quantification
  - Why needed here: Skew@K and MaxSkew@K provide interpretable numerical measures of over- or under-representation of attribute pairs in retrieval results.
  - Quick check question: If a VLM retrieves 80% White doctors and 20% Black doctors, but the neutral prompt expects 50/50, what is Skew@K for (White, Doctor)?

## Architecture Onboarding

- Component map:
  Caption generator -> Counterfactual caption sets -> Stable Diffusion + Prompt-to-Prompt -> Image pairs with controlled attribute differences -> CLIP filter -> Quality control (cosine similarity ≥ 0.2) -> CLIP directional similarity selector -> Best image pair per counterfactual set -> VLM retrieval engine -> Top-K ranked images for neutral prompts -> Skew@K / MaxSkew@K calculator -> Bias quantification

- Critical path:
  1. Generate counterfactual captions (template-based).
  2. Batch-generate images with cross-attention control.
  3. Filter with CLIP similarity.
  4. Select best image pairs by directional similarity.
  5. For each subject, construct neutral prompt embedding.
  6. Retrieve top-K images from all generated images for that subject.
  7. Compute Skew@K and MaxSkew@K.

- Design tradeoffs:
  - Caption templates ensure coverage but may oversimplify real-world phrasing.
  - Cross-attention control reduces confounding but adds generation complexity.
  - CLIP filtering is fast but may discard subtle valid variations.

- Failure signatures:
  - Low CLIP similarity scores → Generation artifacts or prompt mismatch.
  - MaxSkew@K ≈ 0 for all subjects → Either unbiased model or flawed neutral prompt construction.
  - Uneven MaxSkew@K across occupations → Model or dataset bias tied to occupation semantics.

- First 3 experiments:
  1. Vary the cross-attention parameter p and measure impact on MaxSkew@K variance across occupations.
  2. Replace CLIP filtering threshold (0.2) with stricter or looser criteria and observe bias score changes.
  3. Swap the neutral prompt construction method (e.g., remove averaging) and compare resulting MaxSkew@K distributions.

## Open Questions the Paper Calls Out
- How do the generated counterfactual images compare in quality and realism to real-world images when evaluated by human annotators?
- Does using larger or more diverse diffusion models affect the intersectional bias patterns observed in vision-language models?
- How do intersectional biases change when using different text prompt formulations or template structures?
- What is the temporal stability of the observed intersectional biases when evaluated periodically over time?

## Limitations
- The cross-attention control mechanism's effectiveness across diverse attribute combinations remains uncertain, particularly for complex intersectional attributes like religion and physical characteristics.
- The choice of 0.2 CLIP similarity threshold for filtering may be arbitrary, potentially affecting bias measurements.
- The dataset's coverage of all intersectional modes is assumed but not empirically verified, raising questions about potential mode collapse or generation failures for certain attribute combinations.

## Confidence
- **High Confidence:** The methodology for generating counterfactual examples using Stable Diffusion with cross-attention control is technically sound and well-documented.
- **Medium Confidence:** The effectiveness of cross-attention control in maintaining subject consistency while varying social attributes is supported by theoretical understanding but lacks comprehensive empirical validation across all attribute combinations.
- **Medium Confidence:** The assumption that averaged CLIP embeddings represent neutral prompts is reasonable but not rigorously tested.

## Next Checks
1. Systematically vary the cross-attention parameter p across different occupation-attribute combinations and measure the variance in MaxSkew@K scores to quantify the stability of bias measurements.
2. Conduct experiments with multiple CLIP similarity thresholds (0.1, 0.2, 0.3, 0.4) and analyze how different filtering criteria affect the observed bias distributions across VLMs.
3. Compare bias scores obtained using different neutral prompt construction methods (e.g., removing averaging, using different semantic embedding models) to assess the sensitivity of MaxSkew@K to prompt formulation choices.