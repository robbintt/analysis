---
ver: rpa2
title: Expectation consistency for calibration of neural networks
arxiv_id: '2303.02644'
source_url: https://arxiv.org/abs/2303.02644
tags:
- calibration
- temperature
- data
- idence
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new calibration method for neural networks
  called Expectation Consistency (EC), which adjusts the last-layer weights so that
  the average confidence matches the average accuracy on a validation set. EC is compared
  to Temperature Scaling (TS), a widely used method, across various datasets and architectures.
---

# Expectation consistency for calibration of neural networks

## Quick Facts
- arXiv ID: 2303.02644
- Source URL: https://arxiv.org/abs/2303.02644
- Reference count: 18
- One-line primary result: Expectation Consistency (EC) achieves similar calibration performance to Temperature Scaling (TS) across datasets and architectures, with theoretical advantages in misspecified settings.

## Executive Summary
This paper introduces Expectation Consistency (EC), a post-hoc calibration method for neural networks that adjusts the last-layer weights so that average predicted confidence matches average empirical accuracy on a validation set. EC is compared to Temperature Scaling (TS) across multiple datasets (SVHN, CIFAR10, CIFAR100) and architectures (ResNet, DenseNet, VGG, RepVGG). The results show both methods yield similar calibration performance in terms of Expected Calibration Error (ECE) and Brier Score (BS), with EC offering a principled Bayesian interpretation grounded in the Nishimori identity. Theoretical analysis in a high-dimensional logistic regression setting reveals that EC can outperform TS in certain misspecified scenarios.

## Method Summary
Expectation Consistency calibrates neural networks by finding a temperature parameter T such that the average predicted confidence across the validation set equals the validation accuracy. Specifically, EC finds T where the average of the maximum softmax probability equals the proportion of correct predictions. This is achieved through bisection search. The method is compared against Temperature Scaling, which finds T by minimizing validation cross-entropy loss. Both methods rescale the last-layer logits, but EC is grounded in a Bayesian optimality principle while TS is heuristic.

## Key Results
- EC and TS achieve nearly identical calibration performance across multiple datasets and architectures
- EC provides a principled Bayesian interpretation via the Nishimori identity
- In synthetic logistic regression experiments with misspecified activation functions, EC consistently outperforms TS
- Both methods maintain the same accuracy as the uncalibrated model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: EC calibrates neural networks by enforcing that average predicted confidence equals average empirical accuracy on a validation set.
- **Mechanism**: EC adjusts temperature T until average softmax confidence matches validation accuracy, directly targeting expected calibration error.
- **Core assumption**: Validation accuracy is a reliable proxy for true accuracy and validation set is representative.
- **Evidence anchors**: Abstract notes similar calibration performance; method section specifies finding TEC such that average confidence equals validation accuracy.
- **Break condition**: If validation set is too small or unrepresentative, enforced equality may not generalize.

### Mechanism 2
- **Claim**: EC provides principled Bayesian interpretation grounded in Nishimori identity.
- **Mechanism**: EC mimics Bayesian optimality where expected confidence equals expected accuracy, even when full posterior is unknown.
- **Core assumption**: Bayesian posterior under true data-generating model would satisfy expectation consistency property.
- **Evidence anchors**: Method section argues EC is grounded in Bayesian optimality via Nishimori identity.
- **Break condition**: Severe model misspecification or wrong prior may prevent good calibration.

### Mechanism 3
- **Claim**: EC outperforms TS under model misspecification.
- **Mechanism**: In misspecified settings (e.g., wrong activation function), EC achieves lower ECE than TS because it doesn't rely on cross-entropy minimization assuming correct softmax model.
- **Core assumption**: Calibration performance differs when model likelihood doesn't match true data-generating process.
- **Evidence anchors**: Synthetic experiments show EC consistently outperforms TS in misspecified cases.
- **Break condition**: If model is well-specified, advantage disappears as both methods perform similarly.

## Foundational Learning

- **Concept**: Softmax and temperature scaling in neural networks
  - Why needed here: EC rescales last-layer logits via temperature parameter to adjust predicted probabilities
  - Quick check question: What happens to softmax output when temperature T → 0 or T → ∞?

- **Concept**: Expected Calibration Error (ECE) and Brier Score (BS)
  - Why needed here: These are key metrics used to evaluate calibration performance in experiments
  - Quick check question: How does ECE differ from Brier Score in terms of what they penalize?

- **Concept**: Bayesian inference and Nishimori identity
  - Why needed here: EC's theoretical grounding relies on Bayesian principle that expected confidence equals expected accuracy
  - Quick check question: In Bayesian terms, what does it mean for predictor to satisfy Nishimori condition?

## Architecture Onboarding

- **Component map**: Validation set + trained classifier -> logits -> compute accuracy -> find T via bisection -> calibrated probabilities
- **Critical path**: 1) Run classifier on validation set to get logits and predictions 2) Compute validation accuracy 3) Find temperature T such that average softmax confidence equals accuracy via bisection 4) Use T to rescale logits for new samples
- **Design tradeoffs**: EC is more principled (Bayesian) but may require more iterations to find T; TS is simpler and faster but less theoretically grounded
- **Failure signatures**: Small validation set leads to poor generalization; severe over/underparameterization prevents calibration; data distribution shift invalidates enforced equality
- **First 3 experiments**: 1) Run EC on small CNN trained on CIFAR-10 and compare ECE before/after 2) Compare EC vs TS on logistic regression with misspecified activation functions 3) Test EC on well-specified synthetic dataset matching assumed softmax likelihood

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do EC and TS yield nearly identical results in most practical scenarios despite different theoretical foundations?
- Basis in paper: Abstract notes EC and TS yield similar temperatures and calibration performance across architectures and datasets
- Why unresolved: Paper shows EC can outperform TS in certain misspecified scenarios but doesn't explain practical equivalence
- What evidence would resolve it: Further theoretical work comparing methods in realistic settings or extensive empirical studies across wider range of architectures

### Open Question 2
- Question: Can EC be extended to other neural network architectures or tasks beyond image classification?
- Basis in paper: Paper focuses on image classification with CNNs but doesn't explore other architectures or tasks
- Why unresolved: No theoretical framework or empirical evidence for how EC might perform on other types of networks or tasks
- What evidence would resolve it: Applying EC to diverse architectures (RNNs, transformers) and tasks (NLP, RL) and comparing to TS

### Open Question 3
- Question: How does choice of regularization strength (λ) affect EC performance and is there optimal way to select it?
- Basis in paper: Paper mentions regularization improves calibration but doesn't analyze how different strengths affect EC
- Why unresolved: Only briefly discusses regularization impact without comprehensive study on optimal selection
- What evidence would resolve it: Thorough empirical study examining different regularization strengths on EC's calibration performance

## Limitations
- Theoretical advantage of EC over TS in misspecified settings not validated beyond synthetic logistic regression
- Empirical comparison limited to standard image classification benchmarks
- Corpus lacks direct validation of Bayesian interpretation via Nishimori identity

## Confidence

- EC vs TS performance equivalence: **High** (well-supported by multiple datasets and architectures)
- Bayesian grounding via Nishimori identity: **Medium** (theoretically argued but not empirically validated)
- EC superiority under misspecification: **Medium** (shown in synthetic experiments but not on real-world misspecified models)

## Next Checks

1. Test EC vs TS on real-world misspecified classification task (e.g., training with softmax likelihood on data from different link function)
2. Implement synthetic experiment where validation set distribution differs from test set to check if EC's reliance on validation accuracy breaks down
3. Conduct ablation study removing bisection tolerance constraint to test if EC's performance is sensitive to numerical precision in finding TEC