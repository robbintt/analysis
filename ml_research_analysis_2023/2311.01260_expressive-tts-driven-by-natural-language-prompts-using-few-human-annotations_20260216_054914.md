---
ver: rpa2
title: Expressive TTS Driven by Natural Language Prompts Using Few Human Annotations
arxiv_id: '2311.01260'
source_url: https://arxiv.org/abs/2311.01260
tags:
- style
- speech
- language
- reference
- expressive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FreeStyleTTS (FS-TTS), a controllable expressive
  text-to-speech (TTS) system that leverages a large language model (LLM) to retrieve
  style references from minimal human annotations. The core idea is to reframe expressive
  TTS as a style retrieval task, where the LLM selects the best-matching style reference
  from annotated utterances based on natural language style prompts.
---

# Expressive TTS Driven by Natural Language Prompts Using Few Human Annotations

## Quick Facts
- arXiv ID: 2311.01260
- Source URL: https://arxiv.org/abs/2311.01260
- Reference count: 0
- Primary result: Achieves RMOS score of 3.33±0.13 for style selection, preferred in 66.7% of cases for style inference

## Executive Summary
FreeStyleTTS (FS-TTS) is a controllable expressive text-to-speech system that leverages a large language model (LLM) to retrieve style references from minimal human annotations. The system reframes expressive TTS as a style retrieval task, where the LLM selects the best-matching style reference from annotated utterances based on natural language style prompts. Experiments on a Mandarin storytelling corpus demonstrate that FS-TTS can effectively leverage the LLM's semantic inference ability to retrieve desired styles from either input text or user-defined descriptions, resulting in synthetic speeches closely aligned with the specified styles.

## Method Summary
The proposed method uses an LLM to transform expressive TTS into a style retrieval task, where the LLM selects the best-matching style references from a small set of annotated utterances based on natural language style prompts. A Global Style VAE Encoder extracts style embeddings from reference utterances, learning a continuous latent space that guides the TTS pipeline. The system conditions the TTS model on the VAE latent variable from the retrieved reference to synthesize speech with the intended style. The approach employs KL-annealing to avoid overfitting and KL-vanishing during VAE training.

## Key Results
- RMOS score of 3.33±0.13 for style selection effectiveness
- 66.7% preference rate for style inference compared to baseline
- Effective performance with only 200-300 annotated utterances

## Why This Works (Mechanism)

### Mechanism 1
The LLM retrieves the best-matching style reference from a small set of annotated utterances based on semantic similarity to the style prompt. Natural language prompts are semantically compared to annotated style descriptions via LLM, which returns the index of the most relevant reference. This reference's VAE latent variable guides the TTS model. The core assumption is that the LLM can accurately infer semantic similarity between natural language prompts and annotated style descriptions, even with minimal annotations.

### Mechanism 2
The VAE encoder learns a continuous, informative latent space of speech styles that enables precise control over synthesis. The Global Style VAE Encoder extracts style embeddings from reference utterances, learning a latent variable z = μ + z' ⊙ σ. KL annealing is used to avoid KL-vanishing and overfitting, ensuring a diverse and controllable latent space. The core assumption is that the VAE can learn a disentangled and informative style latent space from a limited number of annotated utterances without overfitting.

### Mechanism 3
The TTS pipeline, guided by the VAE latent variable from the retrieved reference, synthesizes speech that closely matches the specified style. The TTS pipeline conditions on the VAE latent variable from the retrieved reference, adjusting duration, pitch, and other prosodic features to match the style. The core assumption is that the TTS model can effectively condition on the VAE latent variable to synthesize speech with the desired style.

## Foundational Learning

- Concept: Variational Autoencoder (VAE)
  - Why needed here: The VAE is used to learn a continuous, informative latent space of speech styles that enables precise control over synthesis. It extracts style embeddings from reference utterances and learns a latent variable z = μ + z' ⊙ σ.
  - Quick check question: How does the VAE ensure a diverse and controllable latent space, and what is the role of KL annealing in this process?

- Concept: Large Language Model (LLM) Semantic Inference
  - Why needed here: The LLM is used to transform the expressive TTS problem into a style retrieval task. It semantically compares natural language prompts to annotated style descriptions and retrieves the most relevant reference.
  - Quick check question: How does the LLM infer semantic similarity between natural language prompts and annotated style descriptions, and what are the limitations of this approach?

- Concept: Style Control in TTS
  - Why needed here: The overall goal is to enable precise control over speech synthesis styles using natural language prompts. This requires understanding how to extract, model, and apply style information in TTS.
  - Quick check question: What are the key challenges in enabling style control in TTS, and how does the proposed approach address these challenges?

## Architecture Onboarding

- Component map: LLM Prompt Selector -> Global Style VAE Encoder -> TTS Pipeline (txt2vec and vec2wav)
- Critical path: Natural language prompt → LLM Prompt Selector → Retrieved reference → Global Style VAE Encoder → VAE latent variable → TTS Pipeline → Synthesized speech
- Design tradeoffs:
  - Annotation vs. Performance: Fewer annotations reduce workload but may limit style diversity and LLM performance.
  - LLM Complexity vs. Accuracy: More complex LLMs may improve semantic inference but increase computational cost.
  - VAE Latent Space vs. Controllability: A more informative latent space enables better style control but may require more training data.
- Failure signatures:
  - LLM Prompt Selector: Poor semantic inference, leading to irrelevant style references.
  - Global Style VAE Encoder: KL-vanishing or overfitting, leading to uninformative latent space.
  - TTS Pipeline: Poor conditioning on VAE latent variable, leading to unnatural or style-mismatched speech.
- First 3 experiments:
  1. Test LLM Prompt Selector with a small set of annotated utterances and diverse style prompts to evaluate semantic inference accuracy.
  2. Train Global Style VAE Encoder on a limited number of annotated utterances and evaluate the informativeness and controllability of the learned latent space.
  3. Integrate LLM Prompt Selector, Global Style VAE Encoder, and TTS Pipeline, and evaluate the end-to-end performance on style matching and naturalness.

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of style annotation affect the performance of FreeStyleTTS, and what are the key factors that contribute to effective style annotation? The paper mentions that style annotations are done with 1-4 words or phrases, but does not provide a detailed analysis of how the quality of style annotations impacts the system's performance or specify the key factors that contribute to effective style annotation.

### Open Question 2
How does the choice of LLM (e.g., GPT-3.5-turbo-16k vs. GPT-4) affect the style retrieval and overall performance of FreeStyleTTS? The paper mentions that GPT-4 performed better than GPT-3.5-turbo-16k in the style retrieval task, but does not provide a detailed comparison of the performance of FreeStyleTTS using different LLMs or explain why GPT-4 performs better.

### Open Question 3
How does the number of annotated utterances affect the performance of FreeStyleTTS, and is there a point of diminishing returns in terms of annotation quantity? The paper mentions that using 250 annotated utterances resulted in good performance, but does not explore the relationship between annotation quantity and performance in detail or identify a point of diminishing returns.

## Limitations
- Performance heavily dependent on quality and diversity of annotated utterances
- Evaluation limited to single Mandarin storytelling corpus
- Exact annotation burden needed for robust performance across diverse style prompts remains unclear

## Confidence

**High Confidence**: The basic framework of using an LLM for style retrieval combined with VAE-based style modeling is technically sound and builds on established methods in the field.

**Medium Confidence**: The claim that few annotations (200-300) are sufficient for good performance is supported by the reported results but requires additional validation on different corpora and style domains.

**Low Confidence**: The generalizability claims to other languages, domains, and style types are largely speculative with limited supporting evidence.

## Next Checks

1. Test the system on a different TTS corpus (e.g., English audiobooks or conversational speech) with the same annotation strategy to assess generalization beyond the Mandarin storytelling domain.

2. Systematically vary the number of annotated utterances (e.g., 50, 100, 200, 300) while keeping other factors constant to quantify the relationship between annotation effort and system performance.

3. Evaluate system performance on a curated set of diverse and potentially out-of-distribution style descriptions (including culturally-specific, abstract, or compound styles) to identify failure modes and limitations of the LLM-based retrieval approach.