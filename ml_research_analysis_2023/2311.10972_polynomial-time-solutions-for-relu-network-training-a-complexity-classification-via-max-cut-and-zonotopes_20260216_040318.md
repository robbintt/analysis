---
ver: rpa2
title: 'Polynomial-Time Solutions for ReLU Network Training: A Complexity Classification
  via Max-Cut and Zonotopes'
arxiv_id: '2311.10972'
source_url: https://arxiv.org/abs/2311.10972
tags:
- diag
- problem
- optimal
- polynomial-time
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the complexity of training two-layer ReLU\
  \ neural networks with weight decay regularization, showing that finding approximate\
  \ global optimizers is NP-hard for small relative errors (\u22640.006). Using a\
  \ convex reformulation of the problem, it establishes connections to Max-Cut and\
  \ zonotopes, enabling polynomial-time approximation algorithms under specific dataset\
  \ conditions: exact solutions for orthogonal separable datasets, 0.253-relative-error\
  \ approximations for negatively correlated datasets, and approximations governed\
  \ by a geometric ratio for general datasets."
---

# Polynomial-Time Solutions for ReLU Network Training: A Complexity Classification via Max-Cut and Zonotopes

## Quick Facts
- **arXiv ID**: 2311.10972
- **Source URL**: https://arxiv.org/abs/2311.10972
- **Reference count**: 26
- **Primary result**: Finding approximate global optimizers for ReLU network training with weight decay is NP-hard for small relative errors (≤0.006), but polynomial-time algorithms exist for structured datasets

## Executive Summary
This paper establishes a complexity classification for training two-layer ReLU neural networks with weight decay regularization. The key insight is that the problem can be reformulated as a convex dual problem involving Hausdorff distance between zonotopes, which connects it to the Max-Cut problem. The authors prove that finding approximate global optimizers is NP-hard for small relative errors, but polynomial-time approximation algorithms are possible for specific dataset structures: exact solutions for orthogonal separable datasets, 0.253-relative-error approximations for negatively correlated datasets, and approximations governed by a geometric ratio for general datasets.

## Method Summary
The authors reformulate the non-convex ReLU network training problem with weight decay as a convex dual problem. They show that the dual constraints can be expressed as Hausdorff distance between zonotopes, which for certain datasets reduces to Max-Cut. This connection enables polynomial-time approximation algorithms using techniques from Max-Cut literature, including SDP relaxation and randomized rounding. The approach provides the first hardness of approximation results for regularized ReLU networks and establishes polynomial-time approximation guarantees under specific dataset conditions.

## Key Results
- Finding approximate global optimizers for ReLU networks with weight decay is NP-hard for relative errors ≤0.006
- Exact polynomial-time solutions exist for orthogonal separable datasets
- 0.253-relative-error approximations are achievable for negatively correlated datasets
- General datasets admit polynomial-time approximations governed by a geometric ratio

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hardness of ReLU network training with weight decay can be exactly mapped to the Max-Cut problem for specific datasets.
- Mechanism: By formulating the dual problem, constraints become Hausdorff distance between zonotopes, which for certain datasets reduces to Max-Cut.
- Core assumption: Optimal solution found by solving convex dual problem with zero duality gap for wide networks.
- Evidence anchors:
  - [abstract] "hardness of approximation of ReLU networks not only mirrors the complexity of the Max-Cut problem but also, in certain special cases, exactly corresponds to it."
  - [section 3] "maxz∈{−1,1}n+1 zT W z ≤ 1."

### Mechanism 2
- Claim: For orthogonal separable datasets, the dual problem decouples into independent subproblems for positive and negative classes.
- Mechanism: Orthogonality constraint allows maximin problems in dual constraints to be solved in closed form.
- Core assumption: Dataset satisfies orthogonality separability condition.
- Evidence anchors:
  - [abstract] "For orthogonal separable datasets, a precise solution can be obtained in polynomial-time."
  - [section 4] "maxb+∈[0,1]n+ minb−∈[0,1]n− ∥X+diag(λ+)b+ + X−diag(λ−)b−∥2 = ∥X+λ+∥2"

### Mechanism 3
- Claim: For datasets with negative correlation, a polynomial-time approximation with relative error π/2-1 is achievable using SDP rounding.
- Mechanism: Negative correlation simplifies maximin problems, and Goemans-Williamson style SDP rounding provides the approximation guarantee.
- Core assumption: Dataset has negative correlation.
- Evidence anchors:
  - [abstract] "When there is a negative correlation between samples of different classes, we give a polynomial-time approximation with relative error π/2−1≈0.253."
  - [section 5] "maxb+∈[0,1]n+ ∥X+diag(λ+)b+∥2 ≤ 1, maxb−∈[0,1]n− ∥X−diag(λ−)b−∥2 ≤ 1."

## Foundational Learning

- Concept: Zonotopes and Hausdorff distance
  - Why needed here: Dual constraints expressed as Hausdorff distance between zonotopes, connecting to Max-Cut
  - Quick check question: What is the definition of a zonotope generated by matrix A?

- Concept: Convex duality and cone constraints
  - Why needed here: Primal non-convex problem reformulated as convex dual problem with cone constraints
  - Quick check question: What is the dual problem of the max-margin formulation for ReLU networks?

- Concept: Semidefinite programming and rounding
  - Why needed here: Approximation algorithms use SDP relaxations and rounding techniques similar to Max-Cut
  - Quick check question: How does Goemans-Williamson rounding achieve a 0.878 approximation for Max-Cut?

## Architecture Onboarding

- Component map: Primal ReLU network training problem -> Convex dual formulation -> Zonotope-based constraint reformulation -> Approximation algorithms based on Max-Cut techniques
- Critical path: Dataset structure → Dual problem formulation → Constraint simplification → Polynomial-time algorithm selection
- Design tradeoffs: Exact solutions for structured datasets vs. approximation algorithms for general datasets; width of network vs. duality gap
- Failure signatures: Inability to find polynomial-time approximation for small relative errors indicates NP-hardness; duality gap suggests insufficient network width
- First 3 experiments:
  1. Verify the dual formulation for a simple orthogonal separable dataset
  2. Test the SDP-based approximation algorithm on a negatively correlated dataset
  3. Measure the geometric ratio for a general dataset and compare with approximation bounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the approximation ratios established for orthogonal separable and negatively correlated datasets be improved, and if so, under what conditions?
- Basis in paper: [explicit] The paper establishes specific approximation ratios but does not explore whether these can be improved.
- Why unresolved: The paper focuses on establishing the existence of polynomial-time algorithms with certain approximation guarantees, but does not investigate the tightness of these bounds.
- What evidence would resolve it: Developing new algorithms or identifying special cases where the approximation ratios can be tightened, or proving lower bounds on achievable approximation ratios for these dataset classes.

### Open Question 2
- Question: How does the geometric ratio c* behave for real-world datasets, and can it be efficiently estimated in practice?
- Basis in paper: [explicit] The paper introduces the geometric ratio c* to characterize approximability of general datasets, but does not provide empirical analysis of its behavior on real data.
- Why unresolved: While the theoretical framework is established, the paper does not investigate the practical implications or computational feasibility of estimating c* for actual datasets.
- What evidence would resolve it: Empirical studies measuring c* on diverse real-world datasets and developing efficient algorithms to estimate it.

### Open Question 3
- Question: Can the complexity classification and approximation results be extended to deeper neural network architectures beyond two-layer ReLU networks?
- Basis in paper: [explicit] The paper focuses on two-layer ReLU networks and mentions that results can be extended to deeper networks, but does not provide such extensions.
- Why unresolved: The paper establishes results for a specific neural network architecture but does not explore how these results generalize to more complex models.
- What evidence would resolve it: Developing convex reformulations and complexity analyses for deeper networks, or proving limitations on extending these results to more complex architectures.

## Limitations

- The polynomial-time algorithms depend on specific dataset structures (orthogonal separability, negative correlation) that may not hold in real-world scenarios
- The geometric ratio for general datasets may be difficult to compute exactly, leading to suboptimal approximation factors in practice
- The results assume zero duality gap, which may not hold for finite-width networks in practice

## Confidence

**High confidence** in the NP-hardness results for general datasets and small relative errors, as these follow from standard complexity theory arguments and connections to Max-Cut.

**Medium confidence** in the polynomial-time algorithms for structured datasets, as these rely on specific assumptions about dataset structure that may not generalize well.

**Medium confidence** in the approximation guarantees for general datasets, as the geometric ratio depends on dataset-specific properties that may be difficult to compute exactly.

## Next Checks

1. **Empirical validation**: Implement the polynomial-time algorithms for orthogonal separable and negatively correlated datasets on synthetic data, verifying the approximation bounds experimentally.

2. **Duality gap analysis**: Numerically measure the duality gap for finite-width networks on various datasets to understand the practical implications of the theoretical results.

3. **General dataset experiments**: Apply the SDP-based approximation algorithm to real-world datasets and measure the achieved approximation factor compared to the theoretical bound.