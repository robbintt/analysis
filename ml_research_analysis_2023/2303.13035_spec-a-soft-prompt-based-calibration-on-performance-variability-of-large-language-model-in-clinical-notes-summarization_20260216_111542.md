---
ver: rpa2
title: 'SPeC: A Soft Prompt-Based Calibration on Performance Variability of Large
  Language Model in Clinical Notes Summarization'
arxiv_id: '2303.13035'
source_url: https://arxiv.org/abs/2303.13035
tags:
- soft
- prompt
- prompts
- spec
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of performance variability in clinical
  note summarization when using large language models (LLMs) with discrete text prompts.
  The authors propose a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline
  that employs soft prompts to reduce variance while preserving the benefits of prompt-based
  summarization.
---

# SPeC: A Soft Prompt-Based Calibration on Performance Variability of Large Language Model in Clinical Notes Summarization

## Quick Facts
- arXiv ID: 2303.13035
- Source URL: https://arxiv.org/abs/2303.13035
- Reference count: 34
- Key outcome: Reduces performance variability in clinical note summarization by up to 43.1% while maintaining or improving summarization quality across multiple LLMs

## Executive Summary
This paper addresses the critical issue of performance variability when using large language models (LLMs) for clinical note summarization with discrete text prompts. The authors propose SPeC (Soft Prompt-Based Calibration), a model-agnostic pipeline that employs soft prompts to reduce variance while preserving the benefits of prompt-based summarization. SPeC uses a soft prompt encoder to interact with discrete text prompts throughout the embedding space, achieving significant variance reduction across multiple clinical note tasks and LLM architectures. The method demonstrates improved performance and reduced variance compared to base LLMs with LLM-generated prompts, making it particularly valuable for real-world clinical applications where prompt quality may vary.

## Method Summary
SPeC is a soft prompt-based calibration pipeline designed to reduce performance variability in LLM-based clinical note summarization. The method uses a trainable soft prompt encoder that interacts with discrete text prompts to produce consistent embeddings across different prompts. During training, the system minimizes INFO SUM LOSS, which measures the distance between input clinical note embeddings and output summarization embeddings under frozen transformer-based LLMs. The approach is model-agnostic and zero-shot, requiring no ground truth labels during training. The method is evaluated on the MIMIC-CXR dataset using ROUGE-1, ROUGE-2, and ROUGE-L metrics, comparing base LLMs with and without SPeC across multiple LLM architectures including Flan-T5, BART, and Pegasus-xsum.

## Key Results
- SPeC reduces performance variability by up to 43.1% on the Flan-T5 model while maintaining or improving summarization performance
- The method consistently outperforms base LLMs with LLM-generated prompts across multiple clinical note tasks
- In-distribution soft prompt tokens demonstrate superior performance compared to out-of-distribution tokens across all tested LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft prompt encoder reduces variance by aligning embeddings across prompts
- Mechanism: The soft prompt encoder learns a continuous vector that transforms the embedding space so that diverse discrete prompts produce similar embeddings, reducing downstream variance
- Core assumption: The variability in LLM outputs is primarily driven by differences in prompt embeddings rather than inherent model stochasticity
- Evidence anchors:
  - "Our proposed soft prompt encoder is a zero-shot learning model that does not require any of the golden summarization references as ground truth labels during the training phase"
  - "INFO SUM LOSS aims to measure the distance between the embedding distribution of input clinical notes and the embedding distribution of output summarization under the frozen transformer-based LLMs"
- Break condition: If the variance is primarily due to model stochasticity rather than prompt embedding differences, or if the soft prompt cannot adequately align diverse prompt embeddings

### Mechanism 2
- Claim: Model-agnostic design allows SPeC to work across different LLMs
- Mechanism: By freezing the base LLM parameters and only training the soft prompt encoder, SPeC can be applied to any transformer-based LLM without model-specific tuning
- Core assumption: The soft prompt encoder's effect is sufficiently general to work across different LLM architectures
- Evidence anchors:
  - "Experimental findings on multiple clinical note tasks and LLMs indicate that our method not only bolsters performance but also effectively curbs variance for various LLMs"
  - "Our proposed soft prompt encoder is a zero-shot learning model that does not require any of the golden summarization references as ground truth labels during the training phase"
- Break condition: If different LLM architectures respond differently to the same soft prompt encoding approach, or if the frozen LLM assumption fails for certain architectures

### Mechanism 3
- Claim: INFO SUM LOSS provides effective training signal for soft prompt optimization
- Mechanism: The loss function minimizes the divergence between original and prompted embedding distributions, ensuring the soft prompt produces consistent outputs across different discrete prompts
- Core assumption: The distance between embedding distributions correlates with output variance in the final summarization
- Evidence anchors:
  - "INFO SUM LOSS aims to measure the distance between the embedding distribution of input clinical notes and the embedding distribution of output summarization under the frozen transformer-based LLMs"
  - "we can minimize the INFO SUM LOSS by purely updating the parameters of soft prompt encoder θSoft"
- Break condition: If the INFO SUM LOSS does not correlate with actual output variance, or if it leads to over-regularization that harms performance

## Foundational Learning

- Concept: Transformer-based language models
  - Why needed here: Understanding how LLMs process embeddings and generate outputs is crucial for grasping why soft prompts work
  - Quick check question: What is the role of the attention mechanism in transformer models and how does it affect prompt processing?

- Concept: Embedding space alignment
  - Why needed here: The soft prompt works by aligning embeddings from different prompts into a consistent space
  - Quick check question: How do different prompts map to different regions in the embedding space and why does this cause variance?

- Concept: Zero-shot learning
  - Why needed here: SPeC is described as a zero-shot method that doesn't require ground truth labels
  - Quick check question: What distinguishes zero-shot learning from few-shot or supervised learning in the context of prompt engineering?

## Architecture Onboarding

- Component map: Frozen pre-trained LLM (encoder + decoder) -> Soft prompt encoder (trainable) -> INFO SUM LOSS function -> Discrete prompt inputs -> Clinical note inputs

- Critical path: Input clinical notes + LLM-generated prompts → Soft prompt encoder → Transformed embeddings → Frozen LLM → Output summarization

- Design tradeoffs:
  - Flexibility vs performance: Model-agnostic design sacrifices potential gains from model-specific tuning
  - Complexity vs interpretability: Soft prompts are harder to interpret than discrete prompts
  - Training stability vs convergence speed: INFO SUM LOSS may require careful hyperparameter tuning

- Failure signatures:
  - Performance degradation across all prompts (likely over-regularization)
  - Inconsistent performance across different LLMs (architecture mismatch)
  - No reduction in standard deviation despite training (INFO SUM LOSS not effective)

- First 3 experiments:
  1. Test SPeC with a single LLM and single prompt to verify basic functionality
  2. Test with multiple prompts on the same LLM to measure variance reduction
  3. Test across different LLM architectures to verify model-agnostic capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SPeC compare when using in-distribution versus out-of-distribution soft prompt tokens across different LLMs?
- Basis in paper: Table 5 shows that in-distribution soft prompt tokens outperform out-of-distribution ones across all three LLMs (Flan-T5, BART, Pegasus-xsum) in terms of both mean performance and standard deviation reduction
- Why unresolved: While the paper demonstrates the superiority of in-distribution tokens for the specific LLMs tested, it does not explore the full range of potential soft prompt tokens or provide a systematic method for identifying optimal in-distribution tokens for new LLMs
- What evidence would resolve it: A comprehensive study comparing a wide variety of soft prompt tokens (both in-distribution and out-of-distribution) across multiple LLMs, with a clear methodology for identifying the most effective tokens for each model

### Open Question 2
- Question: Can the INFO SUM LOSS function be further optimized to improve the performance and variability reduction of SPeC?
- Basis in paper: The paper mentions that "INFO SUM LOSS aims to measure the distance between the embedding distribution of input clinical notes and the embedding distribution of output summarization under the frozen transformer-based LLMs" but does not explore alternative distance functions or optimization strategies
- Why unresolved: The paper uses mean squared error loss with Flan-T5 and cross-entropy loss with BART and Pegasus-xsum, but does not investigate other potential loss functions or optimization techniques that might yield better results
- What evidence would resolve it: Comparative studies of different loss functions and optimization strategies for the INFO SUM LOSS, with rigorous evaluation of their impact on both performance and variability reduction

### Open Question 3
- Question: How does the length of the soft prompt token affect the performance and variability of SPeC?
- Basis in paper: Figure 2 shows that SPeC achieves lower variability across different LLMs regardless of soft prompt token length, but does not provide a detailed analysis of the relationship between token length and performance
- Why unresolved: While the paper demonstrates that SPeC is effective with varying token lengths, it does not explore the optimal token length for maximizing performance and variability reduction, nor does it investigate potential trade-offs between token length and computational efficiency
- What evidence would resolve it: A systematic study varying the length of soft prompt tokens across multiple LLMs, with detailed analysis of the relationship between token length, performance, variability reduction, and computational efficiency

## Limitations

- The paper lacks detailed hyperparameter specifications, making exact reproduction challenging
- The model-agnostic claims need further validation with a broader range of transformer architectures beyond the three tested
- INFO SUM LOSS function lacks extensive corpus evidence for its effectiveness in this specific clinical application

## Confidence

**High Confidence**: The core mechanism of using soft prompts to reduce embedding space variance is well-supported by the experimental results, particularly the demonstrated reduction in standard deviation of ROUGE scores across different prompts.

**Medium Confidence**: The model-agnostic claims are supported by testing across three different LLMs, but the limited architectural diversity (all being transformer-based) prevents high confidence in universal applicability.

**Low Confidence**: The effectiveness of INFO SUM LOSS as a training signal is theoretically justified but lacks extensive empirical validation beyond the scope of this specific application.

## Next Checks

1. **Architecture Diversity Test**: Evaluate SPeC's performance across a wider range of LLM architectures, including non-transformer models, to rigorously test the model-agnostic claims.

2. **Prompt Diversity Analysis**: Systematically vary the quality and type of discrete prompts used with SPeC to assess its robustness to prompt quality variations and identify potential failure modes.

3. **Longitudinal Stability Assessment**: Conduct a long-term evaluation of SPeC's performance across multiple training iterations and with evolving datasets to assess stability and potential degradation over time.