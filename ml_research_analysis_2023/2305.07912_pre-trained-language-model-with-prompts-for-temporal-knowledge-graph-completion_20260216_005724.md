---
ver: rpa2
title: Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion
arxiv_id: '2305.07912'
source_url: https://arxiv.org/abs/2305.07912
tags:
- temporal
- knowledge
- graph
- timestamps
- hits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PPT, a pre-trained language model-based approach
  for temporal knowledge graph completion. It converts the TKGC task into a masked
  token prediction problem by using prompts to incorporate entities, relations, and
  timestamps into the pre-trained model's input.
---

# Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2305.07912
- Source URL: https://arxiv.org/abs/2305.07912
- Reference count: 17
- Key outcome: PPT achieves competitive results on temporal knowledge graph completion, improving MRR and Hits@1/3/10 metrics through prompt-based conversion of TKGC to masked token prediction

## Executive Summary
This paper introduces PPT (Prompt-based Pre-trained language model for TKGC), a novel approach for temporal knowledge graph completion that leverages pre-trained language models through a prompt-based framework. The method converts quadruples from temporal knowledge graphs into input sequences with entity, relation, and time prompts, then trains the model using masked language modeling. By transforming the TKGC task into a masked token prediction problem, PPT effectively combines semantic information from pre-trained models with temporal relationships from timestamps.

## Method Summary
PPT converts temporal knowledge graph completion into a masked token prediction task using pre-trained language models. The method samples quadruples from TKGs, arranges them chronologically to form Temporal Specialization Graphs (TSGs), then converts these to Time Interval Graphs (TIGs) by calculating time intervals. These TIGs are transformed into input sequences using prompts - ent-prompts for entities, rel-prompts for relations, and time-prompts for temporal information. The model is trained with a 30% random masking strategy using BERT-base-cased and evaluated on ICEWS benchmark datasets.

## Key Results
- PPT achieves competitive performance on ICEWS05-15, ICEWS14, and ICEWS18 datasets
- Model shows improvements in MRR and Hits@1/3/10 metrics compared to existing TKGC methods
- Demonstrates effective leverage of semantic information from pre-trained language models combined with temporal information from timestamps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The masked token prediction task enables the model to leverage semantic information from the pre-trained language model to infer missing entities in temporal knowledge graphs.
- Mechanism: By converting quadruples into input sequences with prompts and applying a masking strategy, the model can use the surrounding context to predict masked tokens, which correspond to missing entities.
- Core assumption: The pre-trained language model's semantic knowledge is transferable to the task of temporal knowledge graph completion.
- Evidence anchors:
  - [abstract] "We train our model with a masking strategy to convert TKGC task into a masked token prediction task, which can leverage the semantic information in pre-trained language models."
  - [section] "By Eq 8, the original knowledge-completion task can be equated to the pre-trained language model masked token prediction task."

### Mechanism 2
- Claim: The use of prompts, particularly time-prompts, allows the model to effectively incorporate temporal information from timestamps into the language model.
- Mechanism: By converting intervals between timestamps into different prompts, the model can make coherent sentences with implicit semantic information, capturing the temporal relationships between events.
- Core assumption: The temporal information in the form of prompts can be effectively captured and utilized by the pre-trained language model.
- Evidence anchors:
  - [abstract] "We convert intervals between timestamps into different prompts to make coherent sentences with implicit semantic information."
  - [section] "We convert each timestamp interval into a prompt. Each prompt contains two parts. The front part is a soft prompt indicating the length of time, such as [SHT] for a short time (less than 60 days),[MID] for a medium time (from 60 days to 365 days), and [LNG] for a long time (above 365 days); the back part is a statement describing the interval."

### Mechanism 3
- Claim: The sampling strategy of converting quadruples into input sequences with prompts allows the model to capture the temporal relationships between events in the knowledge graph.
- Mechanism: By sampling multiple quadruples and arranging them in chronological order, the model can learn the temporal dependencies between events and use this information to predict missing entities.
- Core assumption: The temporal ordering of events in the sampled quadruples is representative of the actual temporal relationships in the knowledge graph.
- Evidence anchors:
  - [section] "We sample the quadruples in TKGs and construct prompts for each type of timestamps, which we call time-prompts. Then we train PLMs with a masking strategy. In this way, TKGC can be converted into a masked token prediction task."
  - [section] "The sampled list is called Temporal Specialization Graph (TSG). TSG can be described as a time-ordered sequence TSG = [q0,q 1...,q n],q i = (si,r i,o i,t i)∈Q ,t i≤ ti+1."

## Foundational Learning

- Concept: Pre-trained Language Models (PLMs)
  - Why needed here: PLMs provide a foundation of semantic knowledge that can be leveraged for the task of temporal knowledge graph completion.
  - Quick check question: What is the main advantage of using a pre-trained language model for the temporal knowledge graph completion task?

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is used as the training strategy to convert the TKGC task into a masked token prediction task, allowing the model to leverage the semantic information in PLMs.
  - Quick check question: How does the masking strategy in MLM help in converting the TKGC task into a masked token prediction task?

- Concept: Prompts in NLP
  - Why needed here: Prompts are used to convert entities, relations, and timestamps into a form suitable for input to PLMs, enabling the model to capture the temporal information effectively.
  - Quick check question: What is the purpose of using prompts in the context of temporal knowledge graph completion?

## Architecture Onboarding

- Component map:
  - Pre-trained Language Model (PLM) -> Prompt Generator -> Masked Language Modeling (MLM) Module -> Temporal Knowledge Graph (TKG) Sampler -> Time Interval Graph (TIG) Converter

- Critical path:
  1. Sample quadruples from the TKG and arrange them in chronological order to form TSGs.
  2. Convert TSGs into TIGs by calculating time intervals between adjacent quadruples.
  3. Convert TIGs into input sequences with prompts using the Prompt Generator.
  4. Apply the MLM Module to perform masked token prediction and train the model.

- Design tradeoffs:
  - Sampling strategy: The choice of sampling strategy (uniform vs. frequency-based) affects the model's ability to capture temporal relationships.
  - Prompt design: The design of prompts, especially time-prompts, impacts the model's ability to incorporate temporal information effectively.
  - Sequence length: The length of the input sequences affects the model's capacity to capture long-term temporal dependencies.

- Failure signatures:
  - Poor performance on tasks requiring long-term temporal reasoning.
  - Inability to capture subtle temporal relationships between events.
  - Overfitting to specific patterns in the training data.

- First 3 experiments:
  1. Ablation study: Remove time-prompts and compare performance to the full model to assess the impact of temporal information.
  2. Hyperparameter tuning: Experiment with different sequence lengths, sampling strategies, and prompt designs to optimize model performance.
  3. Cross-dataset evaluation: Test the model on multiple TKG datasets to assess its generalization ability and robustness to different temporal patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PPT change when using different sampling strategies (uniform vs. frequency-based) for different datasets beyond ICEWS14?
- Basis in paper: [explicit] The paper mentions that frequency-based sampling strategy yields better results on ICEWS14 compared to uniform sampling.
- Why unresolved: The paper only compares these sampling strategies on ICEWS14. It's unclear if the frequency-based approach would consistently outperform uniform sampling across other datasets or if this result is specific to ICEWS14's characteristics.
- What evidence would resolve it: Experiments applying both sampling strategies to ICEWS05-15 and ICEWS18 datasets, comparing their performance across all evaluation metrics.

### Open Question 2
- Question: What is the impact of sequence length on PPT's performance, and is there an optimal range that balances effectiveness and computational efficiency?
- Basis in paper: [explicit] The paper analyzes different sequence lengths (128 and 256) on ICEWS14 but doesn't explore the full range of possible lengths or their computational trade-offs.
- Why unresolved: While the paper finds that sequence length has a smaller effect compared to the number of samples, it doesn't establish the optimal range for different dataset sizes or explore the computational implications of longer sequences.
- What evidence would resolve it: Comprehensive experiments varying sequence length across all three datasets, measuring both performance metrics and training/inference times to identify optimal ranges.

### Open Question 3
- Question: How does PPT's performance compare to other pre-trained language models (e.g., GPT, RoBERTa) when applied to TKGC tasks?
- Basis in paper: [explicit] The paper only tests PPT with different BERT variants (base-cased, base-uncased, large-cased) but doesn't explore other pre-trained language models.
- Why unresolved: The paper demonstrates that PPT is not specific to BERT, but it doesn't establish whether other pre-trained language models could yield better or worse results for TKGC tasks.
- What evidence would resolve it: Experiments applying PPT's methodology to other pre-trained language models like GPT, RoBERTa, or T5, comparing their performance across all three datasets.

## Limitations

- The approach relies heavily on prompt engineering quality, with unclear validation of time-prompt interval boundaries (short: <60 days, medium: 60-365 days, long: >365 days)
- Sampling strategy for constructing Temporal Specialization Graphs is mentioned but not thoroughly explored, with unclear impact of uniform vs. frequency-based approaches
- Evaluation focuses primarily on interpolation (known timestamps) rather than extrapolation (future prediction), missing more challenging temporal reasoning scenarios

## Confidence

**High Confidence**: The core claim that converting TKGC to masked token prediction through prompts is technically sound and the experimental results are properly reported. The mechanism of using language models for semantic reasoning is well-established.

**Medium Confidence**: The effectiveness of the specific prompt engineering approach for temporal information. While the method is reasonable, there's limited ablation or sensitivity analysis to validate the design choices for time-prompts and sampling strategies.

**Low Confidence**: Claims about the model's ability to capture long-term temporal dependencies and its generalization to extrapolation scenarios, as these weren't thoroughly evaluated in the experiments.

## Next Checks

1. **Ablation study on prompt components**: Remove time-prompts and compare performance to assess whether temporal information is actually being utilized, and test different prompt designs (e.g., continuous vs. categorical time representation).

2. **Sampling strategy analysis**: Compare uniform sampling versus frequency-based sampling for constructing TSGs, and evaluate how different sampling strategies affect the model's ability to capture temporal patterns.

3. **Extrapolation evaluation**: Test the model's performance on predicting future events (timestamps beyond the training period) to validate its temporal reasoning capabilities beyond interpolation.