---
ver: rpa2
title: Faster Reinforcement Learning by Freezing Slow States
arxiv_id: '2301.00922'
source_url: https://arxiv.org/abs/2301.00922
tags:
- policy
- state
- value
- states
- approximation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational challenges of solving infinite-horizon
  Markov decision processes (MDPs) with "fast-slow" state structure, where some state
  variables evolve rapidly while others change slowly. The authors propose a novel
  "frozen-state" approximation that periodically freezes slow states during planning,
  solves lower-level finite-horizon MDPs, and applies value iteration to an upper-level
  MDP operating on a slower timescale.
---

# Faster Reinforcement Learning by Freezing Slow States

## Quick Facts
- arXiv ID: 2301.00922
- Source URL: https://arxiv.org/abs/2301.00922
- Reference count: 40
- This paper addresses computational challenges in infinite-horizon MDPs with fast-slow state structure through a frozen-state approximation approach

## Executive Summary
This paper proposes a novel approach to solving infinite-horizon Markov decision processes (MDPs) with fast-slow state structure by periodically freezing slow states during planning. The method reformulates the problem into a hierarchical structure with a lower-level finite-horizon MDP operating on a faster timescale and an upper-level infinite-horizon MDP operating on a slower timescale. This approach offers computational benefits through state re-use, simplified dynamics, and parallelizability while providing theoretical regret bounds for the proposed algorithms.

The authors introduce three algorithmic variants: Frozen-State Value Iteration (FSVI), Nominal FSVI, and Frozen-State Approximate Value Iteration (FSAVI), each with different trade-offs between computational efficiency and approximation accuracy. Empirical evaluation on three domains demonstrates significant computational savings compared to standard approaches while maintaining solution quality.

## Method Summary
The core method involves freezing slow states for T periods and solving lower-level finite-horizon MDPs, then applying value iteration to an upper-level MDP operating on a slower timescale with discount factor γ^T. This hierarchical reformulation simplifies the dynamics by reducing the number of successor states that need to be considered. The approach leverages the fact that slow states change infrequently, allowing their values to be reused across multiple iterations. Three algorithmic variants are proposed: FSVI for exact solutions, Nominal FSVI for problems with factored reward structure, and FSAVI with linear function approximation for scalability to large state spaces.

## Key Results
- Frozen-state methods converge to high-quality policies with significantly less computation than standard approaches like value iteration, Q-learning, and DQN
- Simply omitting slow states performs poorly, while the frozen-state approach effectively balances computational efficiency with solution quality
- Empirical results show 5-10x speedup in convergence time across three benchmark domains (multi-class service allocation, restless bandit, and energy demand response)
- Theoretical regret bounds demonstrate the tradeoff between approximation error and computational savings, with error scaling as O(1/T) for appropriate choices of T

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing slow states reduces computational complexity by limiting the number of successor states in the lower-level MDP.
- Mechanism: When the slow state is frozen, the lower-level MDP only needs to consider transitions of the fast state, dramatically reducing the state space that must be evaluated in each value iteration step.
- Core assumption: The slow state changes slowly enough that freezing it for T periods doesn't significantly impact policy quality.
- Evidence anchors: [abstract]: "computational benefits arise in several ways: (1) re-use of the lower-level policy (which is computed once) when applying value iteration in the upper level, (2) frozen states simplify the dynamics of the lower-level MDP (dramatically fewer successor states), and (3) the lower-level MDP thus becomes separable into independent MDPs, opening the door to speedups via parallel computation."

### Mechanism 2
- Claim: The hierarchical reformulation with slower timescale upper-level MDP improves computational efficiency through better discount factors.
- Mechanism: By aggregating T periods into one step at the upper level, the discount factor changes from γ to γ^T, which is more favorable for convergence and allows the use of value iteration with improved contraction properties.
- Core assumption: The problem can be reasonably approximated by solving T-period subproblems and aggregating their results.
- Evidence anchors: [abstract]: "The proposed approach offers computational benefits through state re-use, simplified dynamics, and parallelizability" and "a slower upper-level timescale allows for a more favorable discount factor."

### Mechanism 3
- Claim: The nominal-state approximation leverages factored reward structure to further reduce computational requirements.
- Mechanism: When the reward function is nearly factored (additive decomposition into slow and fast components), the lower-level MDP only needs to be solved for a nominal slow state, with values for other slow states approximated using the decomposition structure.
- Core assumption: The reward function can be decomposed into components that depend primarily on either the slow state or the fast state.
- Evidence anchors: [section 7]: "One potential drawback of Algorithm 2 is that solving the lower-level problem requires solving an MDP for each slow state x∈X. In this section, we consider the situation where the reward function satisfies a certain nearly-factored assumption."

## Foundational Learning

- Concept: Bellman equation and value iteration
  - Why needed here: The entire algorithm framework is built on solving MDPs using value iteration at both upper and lower levels, with the frozen-state approximation modifying the Bellman operators.
  - Quick check question: What is the contraction factor of the Bellman operator in standard value iteration, and how does it change when we aggregate T periods?

- Concept: Markov decision processes with hierarchical structure
  - Why needed here: The paper reformulates the original MDP into a two-level hierarchy where the upper level operates on a slower timescale, which is fundamental to understanding the computational benefits.
  - Quick check question: How does the action space change when moving from the base MDP to the hierarchical reformulation?

- Concept: Lipschitz continuity and approximation error analysis
  - Why needed here: The theoretical analysis of regret bounds relies heavily on Lipschitz assumptions about the reward and transition functions to bound the approximation errors introduced by freezing states.
  - Quick check question: How does the Lipschitz constant of the optimal value function relate to the Lipschitz constants of the reward and transition functions?

## Architecture Onboarding

- Component map: Base MDP -> Lower-level MDP (frozen slow states) -> Upper-level MDP (slower timescale)
- Critical path:
  1. Solve lower-level MDP with frozen slow states (T-1 steps of value iteration)
  2. Apply value iteration to upper-level MDP using results from lower level
  3. Combine upper-level policy with lower-level policy to form final T-periodic policy
- Design tradeoffs:
  - Choice of T: Larger T improves discount factor but increases approximation error from freezing slow states
  - Exact vs approximate solution: Exact methods (FSVI) provide better performance but don't scale to large state spaces; approximate methods (FSAVI) scale better but introduce additional approximation error
  - Number of nominal states: More nominal states improve approximation quality but increase computational cost
- Failure signatures:
  - If the slow state changes too rapidly, the policy will perform poorly despite computational savings
  - If the reward function is not nearly factored, the nominal-state approximation will introduce significant error
  - If T is chosen too large, the end-of-horizon effect in the lower-level MDP will dominate the policy
- First 3 experiments:
  1. Implement FSVI on a simple two-state fast-slow MDP with known optimal policy to verify computational savings and understand approximation error
  2. Compare FSVI with standard value iteration on the multi-class service allocation problem from the paper to reproduce the performance results
  3. Test the nominal-state approximation on an MDP with known factored reward structure to validate the theoretical error bounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical lower bound on regret for any algorithm that exploits fast-slow structure in infinite-horizon MDPs?
- Basis in paper: [explicit] The authors analyze regret bounds for their frozen-state approach but don't establish fundamental limits on what can be achieved
- Why unresolved: The paper focuses on upper bounds for their proposed methods rather than comparing to information-theoretic lower bounds
- What evidence would resolve it: A proof showing a Ω(f(T, α, dY, Lr, Lf)) regret lower bound for any algorithm exploiting fast-slow structure, where f is some function of the problem parameters

### Open Question 2
- Question: How does the frozen-state approach perform in non-stationary environments where the slow state transitions change over time?
- Basis in paper: [inferred] The paper assumes stationary MDPs but mentions real-world applications where conditions may evolve
- Why unresolved: All theoretical analysis and experiments assume stationary dynamics, though applications like energy markets have time-varying parameters
- What evidence would resolve it: Empirical evaluation on problems with time-varying slow state dynamics, or theoretical analysis extending regret bounds to non-stationary settings

### Open Question 3
- Question: What is the optimal number of periods T to freeze slow states as a function of problem parameters?
- Basis in paper: [explicit] The authors treat T as a parameter and show regret bounds depending on T, but don't provide a principled way to select it
- Why unresolved: The analysis shows T appears in both the approximation error and computational savings, but doesn't solve the trade-off
- What evidence would resolve it: A method to select T that balances the approximation error term ϵr(γ,α,dY, L,T) against computational benefits, possibly as a function of |X|, |Y|, and other problem parameters

## Limitations

- The method assumes perfect knowledge of which states are fast vs slow, which may not hold in real applications where state dynamics need to be learned
- The nominal-state approximation relies on a "nearly-factored reward" assumption that may not be satisfied in many practical problems
- The theoretical regret bounds depend on problem parameters (Lipschitz constants, state space dimensions) that may be difficult to estimate in practice

## Confidence

- **High confidence**: The core computational benefits of freezing slow states are well-supported theoretically and empirically. The three mechanisms (state re-use, simplified dynamics, parallelizability) are clearly demonstrated.
- **Medium confidence**: The regret bounds for FSVI and FSAVI are sound, but their practical tightness and the constants involved need further validation across diverse problem instances.
- **Low confidence**: The nominal-state approximation's practical utility beyond the theoretical analysis is uncertain, particularly for reward functions with complex dependencies between fast and slow states.

## Next Checks

1. **Parameter sensitivity analysis**: Systematically vary T across multiple problem instances to understand the tradeoff between computational savings and approximation error, and identify patterns for selecting T based on problem characteristics.

2. **Robustness testing**: Evaluate frozen-state methods on problems where the nearly-factored reward assumption is violated to quantify the breakdown point and understand when the nominal-state approximation becomes unreliable.

3. **Scalability validation**: Test FSAVI on larger state spaces (10x or more) than those used in the paper to verify that the linear architecture approximation maintains performance while achieving the promised computational benefits.