---
ver: rpa2
title: Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal
arxiv_id: '2306.04502'
source_url: https://arxiv.org/abs/2306.04502
tags:
- training
- learning
- noisy
- labels
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AGRA, a method for adaptive gradient-based
  outlier removal to address the challenge of learning with noisy labels. Unlike traditional
  approaches that pre-clean data, AGRA dynamically adjusts training by comparing individual
  sample gradients with aggregated batch gradients, removing or relabeling samples
  that harm model performance at the current training stage.
---

# Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal

## Quick Facts
- arXiv ID: 2306.04502
- Source URL: https://arxiv.org/abs/2306.04502
- Reference count: 40
- Key outcome: AGRA achieves 93.9% accuracy on YouTube dataset compared to 91.9% for no denoising baseline

## Executive Summary
This paper introduces AGRA (Adaptive Gradient-based Outlier Removal), a method for handling noisy labels by dynamically adjusting training data during model training. Unlike traditional approaches that pre-clean data, AGRA uses gradient similarity between individual samples and comparison batches to decide whether to remove or relabel samples at each training stage. The method shows significant improvements across multiple NLP and image datasets, with particular success on imbalanced datasets using F1 loss and class-weighted sampling.

## Method Summary
AGRA dynamically adjusts training by comparing individual sample gradients with aggregated batch gradients during each training iteration. For each sample, if the cosine similarity between its gradient and the comparison batch gradient is non-positive, the sample is removed or relabeled with an alternative label. The method uses either cross-entropy or F1 loss functions for comparison, with optional class-weighted sampling to ensure balanced representation in the comparison batch. AGRA is evaluated on datasets including YouTube, SMS, TREC, Yoruba, Hausa, CIFAR-10, and CheXpert, showing consistent improvements over baselines like Gold, No Denoising, DP, MeTaL, FlyingSquid, CORES2, and Cleanlab.

## Key Results
- AGRA achieves 93.9% accuracy on YouTube dataset vs 91.9% for no denoising
- Significant F1 score improvements on imbalanced datasets like Hausa (0.642 vs 0.481 for no denoising)
- Outperforms Cleanlab and other baselines across most datasets
- F1 loss function provides additional benefits for imbalanced datasets
- Class-weighted sampling particularly effective for highly imbalanced datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic gradient comparison enables context-aware sample selection
- Mechanism: AGRA compares per-sample gradients to aggregated comparison batch gradients to decide whether a sample should be included in the current update
- Core assumption: A sample's harmfulness is context-dependent and can change as the model evolves during training
- Evidence anchors: [abstract] Dynamic decision based on gradient comparison; [section 3] Individual vs aggregated gradient comparison; [corpus] No direct corpus evidence (0.44 FMR for outlier removal method)

### Mechanism 2
- Claim: F1-based loss functions mitigate class imbalance effects in noisy data
- Mechanism: Using F1 loss instead of cross-entropy reduces the amplification of class bias, allowing minority classes to be better represented
- Core assumption: Cross-entropy loss disproportionately prioritizes majority classes, worsening performance on imbalanced noisy datasets
- Evidence anchors: [section 3.4] Adapted F1 loss to maximize F1 score; [section 4.6] F1 loss adds improvement for most datasets; [corpus] No corpus evidence for F1 loss in noisy label scenarios (0.53 FMR for ordinal correction)

### Mechanism 3
- Claim: Class-weighted sampling ensures balanced representation in the comparison batch
- Mechanism: Sampling weights are inversely proportional to class frequency, increasing likelihood of minority class samples
- Core assumption: Without weighting, majority classes dominate the comparison gradient, making it unrepresentative for minority classes
- Evidence anchors: [section 3.3] Class-weighted sampling for minority class instances; [section 4.6] Balanced datasets profit less from weighted sampling; [corpus] No corpus evidence for class-weighted sampling in noisy label settings (0.51 FMR for active label refinement)

## Foundational Learning

- Concept: Gradient similarity as a proxy for sample utility
  - Why needed here: AGRA relies on comparing gradients to infer whether a sample helps or harms the model
  - Quick check question: What does a negative cosine similarity between a sample's gradient and the comparison gradient indicate in AGRA?

- Concept: Cross-entropy vs F1 loss trade-offs
  - Why needed here: The choice of loss function affects how the model handles noisy labels and class imbalance
  - Quick check question: How does F1 loss differ from cross-entropy in terms of handling imbalanced classes?

- Concept: Batch sampling strategies for noisy data
  - Why needed here: The comparison batch must be representative of all classes to avoid bias
  - Quick check question: Why might random sampling be insufficient for comparison batch construction in imbalanced datasets?

## Architecture Onboarding

- Component map: Data loader -> TF-IDF/CNN encoder -> Model (logistic regression/EfficientNet/ResNet) -> Loss functions (CE/F1) -> AGRA module (gradient computation, similarity scoring, filtering/relabeling) -> Optimizer (Adam)

- Critical path: 1. Sample update batch and comparison batch; 2. Compute comparison gradient from comparison batch; 3. For each sample in update batch, compute gradient and similarity score; 4. Filter or relabel samples based on similarity score; 5. Update model weights using remaining samples

- Design tradeoffs: F1 vs CE loss (F1 better for imbalance but may be slower to converge); Weighted vs random sampling (weighted ensures balance but adds computation); Removal vs relabeling (removal is simpler but relabeling can recover useful samples)

- Failure signatures: Poor performance on minority classes (check comparison batch balance); No improvement over baselines (check gradient similarity threshold and loss function choice); High variance across runs (check random seeds and batch sampling consistency)

- First 3 experiments: 1. Run AGRA with default settings (CE loss, no weighted sampling) on YouTube dataset; 2. Compare AGRA with and without F1 loss on TREC dataset; 3. Test AGRA with weighted sampling on Hausa dataset to observe class balance effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AGRA compare to existing denoising methods on datasets with different types of noise (random, class-conditional, adversarial)?
- Basis in paper: [explicit] AGRA doesn't rely on assumptions about particular noise types, unlike Cleanlab which assumes class-dependent noise
- Why unresolved: Paper only evaluates on weak supervision and synthetic label flip noise patterns
- What evidence would resolve it: Experiments comparing AGRA's performance on datasets with various noise types against existing denoising methods

### Open Question 2
- Question: What is the impact of different comparison loss functions on AGRA's performance and ability to handle noisy labels?
- Basis in paper: [explicit] AGRA is compatible with any loss function and evaluates cross-entropy and F1 loss
- Why unresolved: Limited comparison of different loss functions and unclear how choice affects performance across datasets and noise levels
- What evidence would resolve it: Experiments comparing AGRA with different comparison loss functions (CE, F1, Dice loss) on various datasets and noise levels

### Open Question 3
- Question: How does the choice of alternative label affect AGRA's performance and when is relabeling more beneficial than removal?
- Basis in paper: [explicit] AGRA can use alternative labels for samples with non-positive similarity scores, evaluated on binary datasets
- Why unresolved: No comprehensive analysis of alternative label impact across different datasets and scenarios
- What evidence would resolve it: Experiments comparing AGRA with and without alternative labels on various datasets and scenarios, plus analysis of conditions favoring relabeling over removal

## Limitations

- Dataset Specificity: Performance relies heavily on weak supervision rules for generating noisy labels, which are not fully specified
- Implementation Complexity: Dynamic nature introduces multiple implementation decisions not fully specified in the paper
- Computational Overhead: Requires computing gradients for both comparison batch and individual samples, doubling gradient computations per batch

## Confidence

- Mechanism 1 (Gradient comparison): High confidence - clearly explained and mathematically grounded
- Mechanism 2 (F1 loss benefits): Medium confidence - demonstrated improvements but dataset-dependent
- Mechanism 3 (Class-weighted sampling): Medium confidence - sound theoretical justification with results showing benefits for highly imbalanced datasets

## Next Checks

1. Ablation on comparison batch size: Systematically vary the comparison batch size to determine optimal ratio between comparison and update batches

2. Gradient similarity threshold sensitivity: Test AGRA with different cosine similarity thresholds (0.0, 0.1, 0.2) to assess sensitivity and optimality of default threshold

3. Noise pattern generalization: Test AGRA on datasets with different types of label noise (symmetric vs asymmetric, uniform vs class-conditional) to assess generalization beyond weak supervision rules used in experiments