---
ver: rpa2
title: 'Less is More: Selective Layer Finetuning with SubTuning'
arxiv_id: '2302.06354'
source_url: https://arxiv.org/abs/2302.06354
tags:
- subtuning
- layers
- netuning
- accuracy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SubTuning, a parameter-efficient fine-tuning
  method that selectively trains only a subset of intermediate layers in a neural
  network, keeping the rest frozen. The authors analyze the "fine-tuning profile"
  to identify the most effective layers for each task and architecture.
---

# Less is More: Selective Layer Finetuning with SubTuning

## Quick Facts
- arXiv ID: 2302.06354
- Source URL: https://arxiv.org/abs/2302.06354
- Reference count: 40
- Selective layer fine-tuning achieves accuracy comparable to full fine-tuning while reducing computational costs

## Executive Summary
SubTuning is a parameter-efficient fine-tuning method that selectively trains only a subset of intermediate layers in a neural network while keeping others frozen. The authors analyze "finetuning profiles" to identify the most effective layers for each task and architecture. Experiments across various vision tasks, datasets, and architectures show that SubTuning achieves accuracy comparable to full fine-tuning while reducing inference and training costs. It outperforms both linear probing and full fine-tuning in low-data regimes and enables efficient multi-task learning by minimizing computational overhead.

## Method Summary
SubTuning selectively fine-tunes a subset of consecutive intermediate layers in a pretrained neural network while freezing the rest. The method involves constructing a "finetuning profile" by training each block individually and measuring accuracy, then selecting the optimal consecutive blocks for fine-tuning. At inference, the tuned features are concatenated with the frozen backbone output. The approach includes extensions like Siamese SubTuning for multi-task learning, where different tasks fine-tune different layer subsets without interfering with each other.

## Key Results
- SubTuning achieves accuracy comparable to full fine-tuning while reducing computational overhead
- In low-data regimes, SubTuning outperforms both linear probing and full fine-tuning
- Multi-task learning with SubTuning prevents catastrophic interference between tasks through selective layer adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training only a carefully chosen subset of intermediate layers preserves most of the representational power of the full model while drastically reducing computational cost.
- Mechanism: The pretrained backbone already encodes general visual features in its early layers. By freezing these and tuning only the later layers most relevant to the new task, the network adapts to task-specific patterns without disrupting the general feature extraction. The authors show this by constructing "finetuning profiles" that measure accuracy when training individual blocks, identifying the most effective ones for each task.
- Core assumption: The optimal subset of layers for fine-tuning is task-dependent and can be identified empirically without needing to train the entire network.
- Evidence anchors:
  - [abstract] "subset finetuning (or SubTuning) often achieves accuracy comparable to full finetuning of the model"
  - [section 2] "Analyzing these proﬁles reveals that we can achieve high accuracy without the need to train all the parameters of the network, if we choose correctly which layers to train."
  - [corpus] Weak—no direct supporting evidence found in the related papers.
- Break condition: If the task requires significant changes to early-layer representations (e.g., domain shift affecting low-level features), freezing early layers will hurt performance.

### Mechanism 2
- Claim: SubTuning is especially effective in low-data regimes because it balances expressiveness with parameter efficiency.
- Mechanism: Linear probing (only training the final layer) lacks the capacity to capture complex task-specific transformations, while full fine-tuning overfits when data is scarce. SubTuning strikes a middle ground by training a small number of intermediate layers, allowing the model to learn non-linear mappings without excessive parameters to fit.
- Core assumption: The chosen intermediate layers are sufficient to model the complexity of the target task given limited data.
- Evidence anchors:
  - [abstract] "even surpasses the performance of full ﬁnetuning when training data is scarce"
  - [section 4] "SubTuning with appropriate layer choice performs significantly better than both linear probing and finetuning"
  - [corpus] Weak—no direct evidence in related papers about low-data benefits of selective layer tuning.
- Break condition: If the target task is extremely simple or extremely complex relative to the pretrained model's capacity, the intermediate-layer approach may underperform both extremes.

### Mechanism 3
- Claim: In multi-task settings, SubTuning prevents catastrophic interference between tasks by isolating task-specific adaptations to separate subsets of layers.
- Mechanism: Each task fine-tunes its own subset of intermediate layers while sharing the frozen early layers. At inference, the tuned features are concatenated, allowing each task to have specialized representations without altering the shared backbone. This preserves performance on original tasks while enabling efficient deployment.
- Core assumption: Task-specific features can be extracted from intermediate layers without corrupting shared representations.
- Evidence anchors:
  - [abstract] "This yields a simple and effective method for multi-task learning, where different tasks do not interfere with one another"
  - [section 3.1] "we can 'merge' back the outputs of the two parallel branches (i.e., concatenating them in the 'batch' axis), and use the same network weights for both outputs"
  - [corpus] Weak—related papers discuss parameter-efficient tuning but not interference prevention through selective layer tuning.
- Break condition: If tasks require fundamentally incompatible transformations of the same intermediate features, concatenation may not resolve interference.

## Foundational Learning

- Concept: Finetuning profiles
  - Why needed here: They provide empirical guidance for selecting which layers to train, replacing guesswork with data-driven decisions.
  - Quick check question: What does a finetuning profile graph show, and how is it constructed?
- Concept: Catastrophic forgetting
  - Why needed here: Understanding why naive multi-task fine-tuning fails motivates the selective layer approach.
  - Quick check question: Why does fine-tuning all layers on a new task typically degrade performance on the original task?
- Concept: Parameter-efficient transfer learning
  - Why needed here: SubTuning is a specific instance of this broader class of methods, and understanding the tradeoffs is crucial.
  - Quick check question: How does SubTuning differ from adapter-based methods in terms of which parameters are trained?

## Architecture Onboarding

- Component map: Pretrained backbone (ResNet/ViT) divided into blocks -> frozen early blocks + trained intermediate blocks + new task-specific readout head
- Critical path: 1) Load pretrained model, 2) Construct finetuning profile by training each block individually, 3) Select optimal consecutive blocks, 4) Fine-tune selected blocks with new readout head, 5) At inference, run shared backbone, fork at selected block, concatenate tuned features, apply task-specific head
- Design tradeoffs: Training consecutive blocks simplifies implementation and maximizes computational reuse, but may miss optimal non-consecutive combinations. Longer training per block in profile construction improves accuracy but increases setup cost.
- Failure signatures: 1) If selected blocks are too early, the model underfits the new task; 2) If too late, it may overfit or fail to capture necessary transformations; 3) In low-data regimes, wrong block selection can cause severe degradation; 4) Multi-task concatenation may fail if feature spaces are incompatible.
- First 3 experiments:
  1. Generate finetuning profile on a small validation set by training each block individually and recording accuracy.
  2. Compare full fine-tuning vs. SubTuning with best block(s) on a held-out test set to verify comparable performance.
  3. Test multi-task setup by fine-tuning two tasks with different block selections and measuring inference-time concatenation accuracy.

## Open Questions the Paper Calls Out
- Whether training non-consecutive layers instead of only consecutive blocks could improve performance
- If optimal layer subsets can be predicted based on network properties rather than exhaustive search
- How SubTuning compares to other parameter-efficient fine-tuning methods when composed together

## Limitations
- Finetuning profile methodology may not generalize across all architectures or domains
- Approach hasn't been validated on language models or structured data where layer dependencies differ
- Performance benefits in multi-task settings depend heavily on task similarity and feature compatibility

## Confidence
- Selective layer tuning performance claims: Medium-High - supported by multiple architecture-dataset combinations but limited to vision tasks
- Low-data regime superiority: Medium - claims are plausible given parameter efficiency but lack extensive ablation studies
- Multi-task interference prevention: Medium-Low - theoretical mechanism is sound but practical effectiveness depends heavily on task similarity and feature compatibility

## Next Checks
1. Test SubTuning on a domain-shifted dataset (e.g., natural images → medical imaging) where early-layer features may need modification to assess break conditions for freezing initial layers
2. Implement the multi-task Siamese extension on three diverse tasks and measure both accuracy and inference overhead to verify the claimed efficiency benefits in practice
3. Compare SubTuning's finetuning profile approach against adapter-based methods on the same architectures to quantify whether selective layer tuning offers advantages beyond parameter count reduction