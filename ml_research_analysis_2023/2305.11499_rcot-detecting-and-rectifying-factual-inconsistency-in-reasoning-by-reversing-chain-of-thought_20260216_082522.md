---
ver: rpa2
title: 'RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing
  Chain-of-Thought'
arxiv_id: '2305.11499'
source_url: https://arxiv.org/abs/2305.11499
tags:
- problem
- each
- condition
- feedback
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RCoT is a novel method that automatically detects and rectifies
  factual inconsistency in large language models' reasoning solutions. It works by
  first reconstructing the original problem from the generated solution, then performing
  fine-grained comparisons between the original and reconstructed problems to identify
  overlooked conditions, hallucinated conditions, and question misinterpretations.
---

# RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought

## Quick Facts
- arXiv ID: 2305.11499
- Source URL: https://arxiv.org/abs/2305.11499
- Reference count: 40
- Accuracy improvements of 1.0% to 5.0% across seven arithmetic reasoning datasets

## Executive Summary
RCoT is a novel method that automatically detects and rectifies factual inconsistency in large language models' reasoning solutions. It works by reconstructing the original problem from the generated solution, then performing fine-grained comparisons between the original and reconstructed problems to identify overlooked conditions, hallucinated conditions, and question misinterpretations. Detected inconsistencies are formulated as feedback to guide the model in revising its solution. Experiments on seven arithmetic reasoning datasets show that RCoT consistently outperforms standard chain-of-thought and double-check baselines.

## Method Summary
RCoT detects and rectifies factual inconsistencies in LLM-generated reasoning solutions through a three-step process. First, it reconstructs the original problem from the generated solution using the LLM itself. Second, it performs fine-grained comparison by decomposing both the original and reconstructed problems into condition lists and comparing them element-by-element to identify overlooked or hallucinated conditions and question misinterpretations. Third, it formulates detected inconsistencies into detailed feedback that guides the LLM in revising its solution. The method was evaluated on seven arithmetic reasoning datasets using ChatGPT API with temperature set to 0, comparing against standard CoT, Active-Prompting, and Double-Check baselines.

## Key Results
- RCoT achieves 1.0% to 5.0% accuracy improvements over standard CoT across seven arithmetic reasoning datasets
- Performance gains are consistent across both zero-shot and few-shot settings
- Manual fine-grained feedback can further improve model performance, with ChatGPT reaching 94.6% accuracy on GSM8K
- RCoT successfully detects and corrects overlooked conditions, hallucinated conditions, and question misinterpretations

## Why This Works (Mechanism)

### Mechanism 1: Problem Reconstruction
- Claim: Detecting and rectifying factual inconsistency in reasoning is possible by reconstructing the original problem from the generated solution.
- Mechanism: The LLM reconstructs the original problem from its own generated solution. This reconstruction is then compared with the original problem to identify inconsistencies such as overlooked conditions, hallucinated conditions, and question misinterpretations.
- Core assumption: The LLM can accurately reconstruct the original problem from its generated solution if the solution is logically and factually correct and complete.
- Break condition: The LLM cannot accurately reconstruct the original problem from its generated solution, possibly due to complex reasoning steps or significant factual errors in the solution.

### Mechanism 2: Fine-Grained Comparison
- Claim: Fine-grained comparison between the original and reconstructed problems exposes factual inconsistencies more effectively than coarse-grained comparison.
- Mechanism: The original and reconstructed problems are decomposed into fine-grained condition lists. Each condition in the original problem is compared with the reconstructed conditions to identify overlooked or hallucinated conditions. The questions are also compared to detect misinterpretations.
- Core assumption: Decomposing problems into fine-grained conditions allows for a more thorough and accurate comparison than comparing the entire problems at once.
- Break condition: The fine-grained comparison process is too complex or time-consuming, leading to inefficiencies or errors in detection.

### Mechanism 3: Fine-Grained Feedback
- Claim: Formulating detected factual inconsistencies into fine-grained feedback guides the LLM in revising its solution effectively.
- Mechanism: Detected inconsistencies (overlooked conditions, hallucinated conditions, question misinterpretations) are formulated into detailed feedback that explains the errors and guides the LLM in revising its solution.
- Core assumption: Providing specific, detailed feedback on errors helps the LLM understand and correct its mistakes more effectively than general feedback.
- Break condition: The LLM does not effectively utilize the fine-grained feedback to revise its solution, possibly due to limitations in its learning or reasoning capabilities.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: RCoT builds upon CoT by adding a mechanism to detect and rectify factual inconsistencies in the reasoning process.
  - Quick check question: What is the main purpose of Chain-of-Thought prompting in language models?

- Concept: Problem Reconstruction
  - Why needed here: Reconstructing the original problem from the generated solution is the first step in detecting factual inconsistencies.
  - Quick check question: How does problem reconstruction help in identifying factual inconsistencies in LLM-generated solutions?

- Concept: Fine-Grained Comparison
  - Why needed here: Fine-grained comparison between the original and reconstructed problems is essential for identifying specific types of factual inconsistencies.
  - Quick check question: Why is fine-grained comparison more effective than coarse-grained comparison in detecting factual inconsistencies?

## Architecture Onboarding

- Component map: Problem Reconstruction Module -> Fine-Grained Comparison Module -> Feedback Generation Module -> Solution Revision Module
- Critical path: Problem Reconstruction → Fine-Grained Comparison → Feedback Generation → Solution Revision
- Design tradeoffs:
  - Accuracy vs. Efficiency: Fine-grained comparison is more accurate but also more time-consuming than coarse-grained comparison.
  - Complexity vs. Interpretability: Detailed feedback is more helpful for revision but also more complex to generate and interpret.
- Failure signatures:
  - Inaccurate problem reconstruction leading to missed inconsistencies.
  - Inefficient fine-grained comparison causing slow processing.
  - Vague or unhelpful feedback failing to guide effective solution revision.
- First 3 experiments:
  1. Evaluate the accuracy of problem reconstruction on a set of LLM-generated solutions.
  2. Compare the effectiveness of fine-grained vs. coarse-grained comparison in detecting factual inconsistencies.
  3. Assess the impact of different types of feedback (fine-grained vs. coarse-grained) on the quality of revised solutions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of comparison iterations between the original and reconstructed problems for detecting factual inconsistencies?
- Basis in paper: [inferred] The paper describes a process of comparing condition lists between original and reconstructed problems, suggesting this is a key component of RCoT's effectiveness.
- Why unresolved: The paper does not specify how many comparison iterations are optimal or whether there is a diminishing return on accuracy with additional iterations.
- What evidence would resolve it: Experimental results comparing RCoT performance with varying numbers of comparison iterations between original and reconstructed problems.

### Open Question 2
- Question: How does RCoT perform on non-arithmetic reasoning tasks like logical reasoning or commonsense reasoning?
- Basis in paper: [explicit] The paper states "RCoT could, in principle, be applied to other tasks requiring CoT solutions" and mentions future work exploring other applications.
- Why unresolved: The experiments only evaluate RCoT on arithmetic reasoning datasets, leaving uncertainty about its generalizability to other reasoning domains.
- What evidence would resolve it: Experiments applying RCoT to logical reasoning, commonsense reasoning, or other non-arithmetic reasoning datasets and comparing performance to standard CoT.

### Open Question 3
- Question: What is the computational overhead of RCoT compared to standard CoT prompting?
- Basis in paper: [inferred] The paper mentions "RCoT requires multiple conversations with LLMs" and "may thus slow down the inference speed," but does not provide quantitative measurements.
- Why unresolved: The paper does not provide specific metrics on inference time or computational resources required for RCoT versus standard CoT.
- What evidence would resolve it: Benchmark results showing inference time, API call count, or computational resource usage for RCoT versus standard CoT on comparable datasets.

## Limitations

- The method relies on ChatGPT (GPT-3.5) as the primary evaluation model, limiting generalizability to other LLM architectures
- The effectiveness of problem reconstruction may degrade for solutions with significant factual errors or complex reasoning steps
- Fine-grained feedback quality depends on the LLM's ability to understand and act on detailed error descriptions

## Confidence

- High confidence: The problem reconstruction and fine-grained comparison methodology is clearly specified and reproducible
- Medium confidence: The effectiveness of fine-grained feedback across different LLM architectures
- Low confidence: Generalization of results to non-arithmetic reasoning tasks

## Next Checks

1. Test RCoT's performance on a broader range of LLM architectures (Llama, Claude, Gemini) to assess generalizability of the approach
2. Evaluate the method on non-arithmetic reasoning tasks (logical reasoning, commonsense reasoning) to determine domain applicability
3. Conduct ablation studies comparing fine-grained feedback against different feedback granularities and formats to optimize the feedback mechanism