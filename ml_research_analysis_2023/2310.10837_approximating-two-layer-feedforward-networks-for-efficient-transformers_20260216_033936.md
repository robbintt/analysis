---
ver: rpa2
title: Approximating Two-Layer Feedforward Networks for Efficient Transformers
arxiv_id: '2310.10837'
source_url: https://arxiv.org/abs/2310.10837
tags:
- softmax
- experts
- moes
- expert
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a unified framework to approximate two-layer
  feedforward networks, encompassing methods like Mixture of Experts (MoE) and Product-Key
  Memories (PKM). By viewing these methods as approximations, the authors identify
  design improvements for both MoE and PKM.
---

# Approximating Two-Layer Feedforward Networks for Efficient Transformers

## Quick Facts
- arXiv ID: 2310.10837
- Source URL: https://arxiv.org/abs/2310.10837
- Reference count: 40
- The paper proposes a unified framework to approximate two-layer feedforward networks, encompassing methods like Mixture of Experts (MoE) and Product-Key Memories (PKM), and introduces improvements for both.

## Executive Summary
This paper presents a unified framework for approximating two-layer feedforward networks in Transformers, focusing on Mixture of Experts (MoE) and Product-Key Memories (PKM) as efficient alternatives to dense layers. By viewing these methods as approximations, the authors identify design improvements for both approaches, particularly introducing σ-MoE with sigmoid routing, better initialization, and entropy-based regularization. Unlike prior work comparing MoEs under compute-equal conditions, this study evaluates them under parameter-equal conditions, which is crucial for fair language modeling evaluation. Experiments on WikiText-103 and Enwik8 demonstrate that σ-MoE matches or surpasses dense Transformer-XL baselines while achieving significant reductions in compute and memory usage.

## Method Summary
The paper introduces a unified framework treating MoE and PKM as approximations of two-layer feedforward networks. The core method involves replacing the dense MLP layers with expert-based computation where input channels are partitioned into groups and only top-K groups are activated per token. Key innovations include using sigmoid instead of softmax for routing, entropy-based regularization to encourage balanced expert usage, and expert dropout during training. The authors also release a CUDA kernel implementation for their MoE layers. Training uses modified Transformer-XL architecture with pre-layer norm, 100k training steps, Adam optimizer, gradient clipping, and cosine learning rate decay.

## Key Results
- σ-MoE matches or outperforms dense Transformer-XL baselines on WikiText-103 and Enwik8 while using significantly less compute and memory
- Using sigmoid activation instead of softmax for expert routing improves performance by removing competition between experts
- Entropy-based regularization and expert dropout together prevent expert collapse and encourage balanced expert usage
- The proposed CUDA kernel achieves faster execution and memory reduction compared to dense models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixture-of-Experts layers can approximate the full two-layer feedforward computation while using far fewer operations, because most ReLU activations in the dense layer are zero.
- Mechanism: Instead of computing all $d_{ff}$ channels, MoEs partition them into $N_E$ groups and select the top-$K$ groups based on a learned routing function. Only the selected groups' weights are applied to the input.
- Core assumption: The distribution of ReLU activations is sparse enough that the top-$K$ experts capture most of the signal, and the routing function can be learned to route tokens to the right experts.
- Evidence anchors:
  - [abstract] "We show that our MoEs are competitive with the dense Transformer-XL... while being much more resource efficient."
  - [section] "Concretely, Shen et al. (2023) report that in a Transformer with $d_{model} = 256$ and $d_{ff} = 1024$, 10% of the channels account for 90% of the total activation mass."
  - [corpus] No direct corpus evidence; inferred from experimental claims.
- Break condition: If ReLU sparsity drops significantly (e.g., due to different activation functions or data), the approximation quality degrades.

### Mechanism 2
- Claim: Using sigmoid instead of softmax for expert selection improves performance by removing competition between experts, which aligns better with the original two-layer MLP structure.
- Mechanism: Sigmoid outputs independent scores per expert, so increasing one expert's score does not automatically decrease others'. This matches the ReLU+linear combination in the dense MLP where channels are independent.
- Core assumption: The routing function can be trained effectively with sigmoid, and the independence assumption is valid for the routing task.
- Evidence anchors:
  - [abstract] "We propose methods to improve both MoEs and PKMs... we propose methods to improve both MoEs and PKMs."
  - [section] "Softmax combined with top-K can also encourage expert collapsing... For all these reasons, we opt for sigmoid instead of softmax; we experimentally confirm that this is indeed a good choice."
  - [corpus] No direct corpus evidence; based on ablation study results.
- Break condition: If routing becomes ambiguous (e.g., with many similar experts), sigmoid may lead to overlap and poor specialization.

### Mechanism 3
- Claim: Entropy-based regularization and expert dropout together encourage balanced expert usage and prevent collapse, leading to better performance.
- Mechanism: Entropy regularization maximizes the diversity of expert selection across the batch, while expert dropout randomly disables experts during training, forcing the network to use others.
- Core assumption: Balanced expert usage correlates with better performance and that dropout on experts (not activations) is an effective regularizer.
- Evidence anchors:
  - [abstract] "Our resulting MoE Transformer variant outperforms our improved PKMs, and performs as well as or even outperforms the dense baseline."
  - [section] "We propose to simply maximize the entropy of the selection distribution... Furthermore, we propose to randomly drop complete experts, during training; we refer to this as expert dropout."
  - [corpus] No direct corpus evidence; based on ablation study results.
- Break condition: If the number of experts is too small or the dataset is too simple, regularization may be unnecessary and could even hurt performance.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE)
  - Why needed here: MoE is the core architectural change that enables computation reduction; understanding its routing and grouping mechanism is essential.
  - Quick check question: In MoE, if we have $d_{ff} = 4096$ and $N_E = 16$ with $G = 256$, how many experts are active per token if $K = 4$?
  - Answer: 4 experts are active per token.

- Concept: ReLU sparsity and activation patterns
  - Why needed here: The approximation quality depends on how sparse the activations are; knowing this helps tune $K$ and $G$.
  - Quick check question: If 10% of channels account for 90% of activation mass, what is the minimum $K$ (as a fraction of $N_E$) needed to capture most of the signal?
  - Answer: Roughly $K \geq 0.1 \cdot N_E$.

- Concept: Expert collapse and load balancing
  - Why needed here: Failure modes of MoE include some experts being unused; understanding regularization methods is key to robust training.
  - Quick check question: What is the difference between entropy regularization and expert dropout in preventing expert collapse?
  - Answer: Entropy regularization encourages uniform usage across the batch, while expert dropout forces the network to rely on different experts by randomly disabling some during training.

## Architecture Onboarding

- Component map:
  Input projection: $W_1 \in \mathbb{R}^{d_{ff} \times d_{model}}$ → replaced by $N_E$ expert matrices $W_e^1 \in \mathbb{R}^{G \times d_{model}}$
  Expert selection: $W_3 \in \mathbb{R}^{N_E \times d_{model}}$ with sigmoid activation
  Output projection: $W_2 \in \mathbb{R}^{d_{model} \times d_{ff}}$ → replaced by $N_E$ expert matrices $W_e^2 \in \mathbb{R}^{d_{model} \times G}$
  Top-K routing: selects $K$ experts per token

- Critical path:
  1. Compute expert scores: $s = \sigma(W_3 x)$
  2. Apply Top-K: select $K$ highest-scoring experts
  3. For each selected expert $e$: compute $u_e = W_e^1 x$, then $y_e = W_e^2 \cdot \text{ReLU}(u_e)$
  4. Sum contributions: $\hat{y} = \sum_{e \in \text{top-K}} y_e$

- Design tradeoffs:
  - Larger $N_E$ increases model capacity but may cause expert collapse without strong regularization.
  - Smaller $G$ reduces per-expert compute but may hurt performance if too small.
  - Larger $K$ improves approximation quality but reduces speedup.

- Failure signatures:
  - Degraded performance with no clear training issue → possible expert collapse (check expert usage histograms).
  - Training instability or NaN loss → check initialization of $W_3$ and scaling.
  - Memory errors → verify $G \cdot K$ does not exceed hardware limits.

- First 3 experiments:
  1. Replace one MLP layer in a small Transformer with a MoE layer (fixed $N_E=4$, $G=128$, $K=1$) and compare perplexity on a small dataset.
  2. Sweep $K$ from 1 to 8 while keeping $N_E$ and $G$ fixed; measure speedup vs. accuracy trade-off.
  3. Compare sigmoid vs. softmax routing with entropy regularization on a held-out validation set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sparsity level for Mixture of Experts (MoE) models across different model sizes and datasets?
- Basis in paper: [explicit] The paper discusses that there is a limit on the minimum sparsity level to preserve good performance of MoEs, which is determined by several factors including the number of simultaneously active channels (G * K) and the dataset.
- Why unresolved: The paper shows that the performance of dense baselines can be matched using 25% of the required FLOPs and memory for activations for small models, and 12.5% sparsity for the big one, but does not provide a general formula for determining the optimal sparsity level.
- What evidence would resolve it: Empirical studies testing different sparsity levels (varying G and K) on a wide range of model sizes and datasets to establish a relationship between optimal sparsity and these factors.

### Open Question 2
- Question: How does the choice of activation function in the expert selection function affect the performance and training dynamics of MoE models?
- Basis in paper: [explicit] The paper compares the use of sigmoid and softmax activation functions in the expert selection function and finds that sigmoid performs better, but does not explore other activation functions or their effects in depth.
- Why unresolved: The paper only tests sigmoid and softmax, leaving open the question of whether other activation functions might yield even better results.
- What evidence would resolve it: Experiments comparing the performance of MoE models using various activation functions (e.g., tanh, ReLU, ELU) in the expert selection function across different tasks and model architectures.

### Open Question 3
- Question: Can the resource efficiency of MoE models be further improved by optimizing the CUDA kernel or exploring alternative hardware accelerations?
- Basis in paper: [explicit] The paper mentions that their CUDA kernel is sub-optimal and I/O limited, and that an expert CUDA programmer could improve the speed of the kernel by at least a factor of 2. It also suggests that the kernel does not support tensor cores and mixed precision.
- Why unresolved: The paper does not explore the potential improvements that could be achieved through kernel optimization or alternative hardware accelerations.
- What evidence would resolve it: Implementation and testing of an optimized CUDA kernel with tensor core support and mixed precision, as well as experiments on other hardware accelerators (e.g., TPUs, FPGAs) to compare their efficiency gains.

## Limitations

- The empirical validation is limited to four datasets with relatively modest model sizes, raising questions about scalability to truly large language models
- The performance claims are based on perplexity and bits-per-character metrics, which don't directly measure downstream task performance or robustness to domain shifts
- The generalization of findings to extremely large models (>10B parameters) and diverse downstream tasks remains untested

## Confidence

**High Confidence**: The fundamental observation that ReLU sparsity enables MoE approximation is well-supported by the cited work of Shen et al. (2023) and the mathematical framework presented. The parameter-equal comparison methodology is sound and addresses a genuine gap in prior work. The empirical results showing competitive perplexity scores on standard benchmarks are reproducible given the specified hyperparameters.

**Medium Confidence**: The proposed improvements to MoE (sigmoid routing, entropy regularization, expert dropout) show consistent gains in ablation studies, but the paper doesn't fully explore the interaction between these components or their sensitivity to hyperparameters like regularization strength.

**Low Confidence**: The generalization of these findings to extremely large models (>10B parameters) and diverse downstream tasks remains untested. The paper's assertion that MoEs are relevant "for models of any size" is primarily supported by efficiency metrics rather than comprehensive ablation studies across model scales.

## Next Checks

1. **Scaling Analysis**: Replicate the experiments with model sizes ranging from 100M to 10B parameters on diverse datasets (including multilingual and code datasets) to validate whether the efficiency gains scale proportionally and whether expert collapse becomes more severe at larger scales.

2. **Downstream Task Transfer**: Fine-tune the σ-MoE models on GLUE, SuperGLUE, and code completion benchmarks to assess whether perplexity improvements translate to better downstream performance, and whether the sigmoid routing mechanism affects generalization to non-language tasks.

3. **Ablation of Regularization Components**: Systematically vary the entropy regularization weight and dropout probability across multiple random seeds to determine the stability of the performance gains and identify whether these components are complementary or redundant.