---
ver: rpa2
title: A systematic study comparing hyperparameter optimization engines on tabular
  data
arxiv_id: '2311.15854'
source_url: https://arxiv.org/abs/2311.15854
tags:
- search
- random
- grid
- engines
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper conducts an independent comparison of 11 hyperparameter
  optimization engines from the Ray Tune library on tabular data. It introduces two
  normalization and aggregation methods to rank engines and measure improvements over
  random search.
---

# A systematic study comparing hyperparameter optimization engines on tabular data

## Quick Facts
- **arXiv ID**: 2311.15854
- **Source URL**: https://arxiv.org/abs/2311.15854
- **Reference count**: 21
- **Primary result**: HEBO, AX, and BlendSearch are top performers, with HEBO achieving 50% improvement over random search across all tested models

## Executive Summary
This paper presents an independent comparison of 11 hyperparameter optimization engines from the Ray Tune library on tabular data classification tasks. The study introduces two normalization and aggregation methods to rank engines and measure improvements over random search. Results show that most engines outperform random search, with HEBO, AX, and BlendSearch standing out as the top performers. The study also finds that some engines specialize in specific algorithms, which could bias model comparisons in research studies.

## Method Summary
The study compares 11 hyperparameter optimization engines using a pre-computed grid search approach on five binary classification datasets. Each engine is evaluated on four different models (Random Forest, XGBoost, SVM, PyTorch Tabular) with varying hyperparameter search spaces. The comparison uses two metrics: rank-based (DCG10% to measure probability of beating random search) and score-based (normalized improvement degree). Experiments are run with three different budget levels (m=1,2,3) using 10-fold cross-validation for rank-based metrics and 25 seeds per experiment for score-based metrics.

## Key Results
- HEBO achieves an improvement degree of 50% on all tested models
- AX excels in low-budget scenarios, outperforming other engines when trial budgets are limited
- Some engines show specialization in specific algorithms, potentially biasing model comparisons in research
- Most engines outperform random search across all datasets and models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Using a pre-computed full grid search allows engines to focus optimization without expensive training overhead
- **Mechanism**: The study decouples expensive training-scoring from optimization by pre-computing all validation scores on a finite grid. Engines then read results from this table rather than retraining, enabling rapid iteration and statistical significance testing across many runs.
- **Core assumption**: The grid resolution is fine enough to capture meaningful performance differences between engines while being coarse enough to make the comparison computationally feasible.
- **Evidence anchors**:
  - [abstract] "We introduce two ways to normalize and aggregate statistics across data sets and models"
  - [section] "The operational reason for using a finite grid in this study is that it lets us pre-compute the validation scores"
- **Break Condition**: If the grid is too coarse, engines cannot distinguish fine-grained differences in hyperparameter effects, making the comparison uninformative for real-world deployment.

### Mechanism 2
- **Claim**: Ranking-based metrics abstract away absolute score scales, enabling fair comparison across diverse datasets and models
- **Mechanism**: The study uses DCG10% (discounted cumulative gain on top 10% arms) to create a normalized ranking statistic that compares engines based on how quickly they find good solutions relative to random search, independent of the absolute scale of performance metrics.
- **Core assumption**: The relative ranking of hyperparameter configurations is consistent across different validation folds and seeds, making the aggregated statistics meaningful.
- **Evidence anchors**:
  - [section] "Rank-based metrics answer the question: what is the probability that an engine performs better than random search?"
  - [section] "We use DCG 10% which is a weighted count of top 10% arms present in the L arms generated by the engine"
- **Break Condition**: If the validation score noise is too high relative to the performance differences between configurations, the ranking-based metrics may not reliably distinguish between engines.

### Mechanism 3
- **Claim**: Score-based normalization between random and grid search bounds creates a standardized improvement metric
- **Mechanism**: The study normalizes improvement scores by sandwiching them between random search (rrand) and full grid search (rgrid) performance, creating a percentage improvement metric that can be averaged across different problems.
- **Core assumption**: The gap between random search and grid search represents a meaningful range of achievable improvement that can be proportionally compared across different problems.
- **Evidence anchors**:
  - [section] "Score-based metrics answer the question: how much do we improve the score of random search by using a hyperopt engine?"
  - [section] "To make this metrics easy to aggregate, we normalize it between rrand, the expected best score of the random search with budget L, and rgrid"
- **Break Condition**: If grid search overfits to the validation data, the upper bound becomes unrealistic and the normalization metric becomes misleading.

## Foundational Learning

- **Concept: Hyperparameter optimization as sequential decision making**
  - Why needed here: Understanding that HPO is fundamentally about selecting promising configurations based on previous results is crucial for interpreting why some engines outperform others
  - Quick check question: Why does the study use 10-fold cross-validation rather than a single train/test split?

- **Concept: Bayesian optimization framework**
  - Why needed here: Most engines in the comparison use surrogate models and acquisition functions, so understanding this framework is essential for interpreting their behavior
  - Quick check question: What is the key difference between a GP-based surrogate and a TPE (tree of Parzen estimators) approach?

- **Concept: Statistical significance in experimental comparisons**
  - Why needed here: The study runs 100+ experiments per model-dataset pair to establish statistically significant differences, requiring understanding of proper experimental design
  - Quick check question: Why does the study use three different budget levels (m=1,2,3) rather than a single budget?

## Architecture Onboarding

- **Component map**: Ray Tune provides unified API -> Engines (HEBO, AX, BlendSearch, etc.) -> Training algorithms (RF2, RF3, XGB, SVM, PYTAB) -> Data sets (5 datasets) -> Evaluation (10-fold CV or full CV depending on metric type)
- **Critical path**: Pre-compute full grid scores -> For each engine: initialize -> Select arm -> Read pre-computed score -> Update history -> Repeat until budget exhausted -> Compute metrics -> Aggregate across seeds/folds/datasets
- **Design tradeoffs**: Coarse grid vs. fine grid (computational cost vs. resolution), sequential vs. parallel evaluation (statistical power vs. speed), cross-validation vs. single-fold (noise reduction vs. computational cost)
- **Failure signatures**: Engines that appear to "specialize" in certain models may indicate overfitting to specific hyperparameter spaces; engines that don't beat random search may have poor default settings or be mismatched to the problem characteristics
- **First 3 experiments**:
  1. Run AX engine with default settings on RF2 model using the smallest budget (m=1) on one dataset to verify basic functionality
  2. Compare HEBO and random search on XGB model with medium budget (m=2) to observe the acceleration effect
  3. Test BOHB with default settings on SVM model to understand why it fails to beat random search in this setup

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance rankings change if we used continuous hyperparameter spaces instead of discrete grids?
- Basis in paper: [explicit] The paper explicitly discusses why they chose discrete grids and acknowledges this may disadvantage some engines
- Why unresolved: The authors only tested discrete grids and argue this is a good practice, but didn't explore continuous spaces which some engines are designed for
- What evidence would resolve it: Direct comparison of the same engines on continuous vs discrete versions of the same hyperparameter spaces

### Open Question 2
- Question: Does the superiority of HEBO, AX, and BlendSearch hold across different types of machine learning tasks beyond binary classification?
- Basis in paper: [inferred] The study only tested binary classification with AUC metric, but claims results "should be generalizable" to other settings
- Why unresolved: Limited to binary classification only; no regression, multi-class classification, or other task types tested
- What evidence would resolve it: Replication of the study with regression, multi-class classification, and other task types using appropriate metrics

### Open Question 3
- Question: What is the optimal grid resolution for each hyperparameter optimization engine?
- Basis in paper: [explicit] The authors acknowledge their coarse grids may disadvantage some engines and note some engines perform better with finer grids
- Why unresolved: The study used fixed coarse grids; authors suggest this may not be optimal for all engines
- What evidence would resolve it: Systematic study varying grid resolution for each engine to find optimal resolution that balances exploration and overfitting

## Limitations

- The study's reliance on a pre-computed grid search raises questions about whether the coarse resolution captures all relevant performance differences between engines
- The finding that certain engines "specialize" in specific models could indicate overfitting to the grid structure rather than genuine optimization superiority
- The normalization approach assumes grid search provides a meaningful upper bound, but grid search itself may overfit to validation data

## Confidence

- **High Confidence**: The core finding that HEBO, AX, and BlendSearch consistently outperform random search across multiple models and datasets is robust given the extensive experimental design (100+ runs per configuration)
- **Medium Confidence**: The ranking of engines varies meaningfully by budget level and model type, suggesting genuine performance differences rather than statistical noise
- **Low Confidence**: The interpretation that engine specialization indicates model-specific optimization strengths may be an artifact of the grid-based methodology rather than true algorithmic advantages

## Next Checks

1. Test whether performance rankings persist when using a finer grid resolution or different hyperparameter search spaces for the same models
2. Validate findings on additional tabular datasets with different characteristics (class imbalance, feature types, sample sizes) to assess generalizability
3. Compare results using alternative normalization approaches that don't rely on grid search as an upper bound, such as using nested cross-validation or separate holdout sets