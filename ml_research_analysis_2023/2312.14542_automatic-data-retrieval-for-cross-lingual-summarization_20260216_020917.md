---
ver: rpa2
title: Automatic Data Retrieval for Cross Lingual Summarization
arxiv_id: '2312.14542'
source_url: https://arxiv.org/abs/2312.14542
tags:
- summarization
- language
- pairs
- data
- cross-lingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of creating data for cross-lingual
  summarization from English to Hindi. The authors propose leveraging news events
  covered in both textual articles and video descriptions to automatically generate
  aligned document-summary pairs.
---

# Automatic Data Retrieval for Cross Lingual Summarization

## Quick Facts
- arXiv ID: 2312.14542
- Source URL: https://arxiv.org/abs/2312.14542
- Reference count: 5
- 150K YouTube descriptions and 350K news articles matched to create 28,583 article-summary pairs across English-English, English-Hindi, and Hindi-Hindi language pairs

## Executive Summary
This paper addresses the challenge of creating data for cross-lingual summarization from English to Hindi by leveraging news events covered in both textual articles and video descriptions. The authors propose an automatic method to match and filter document-summary pairs based on publication date, unigram overlap, and semantic similarity, resulting in a dataset of 28,583 aligned pairs. They evaluate several baseline models including IndicBART, mBART, and mT5, with mBART achieving the best performance, particularly for monolingual English-English summarization. Error analysis reveals opportunities for improvement in grammaticality, factuality, and completeness of generated summaries.

## Method Summary
The authors collect 150K YouTube descriptions and 350K news articles, then automatically match pairs based on publication date proximity (within 2.5 days), unigram overlap (fraction of non-stop word unigrams), and semantic similarity using DistUSE embeddings. After filtering based on compression ratio, length-based outliers, and similarity thresholds, they obtain 28,583 article-summary pairs across three language directions. The dataset is then used to fine-tune multilingual models (mBART, IndicBART, mT5) for cross-lingual summarization, with evaluation using ROUGE-1, ROUGE-2, and ROUGE-L scores.

## Key Results
- mBART achieves ROUGE-1 scores of 49.0 for English-English, 25.7 for English-Hindi, and 40.7 for Hindi-Hindi summarization
- Error analysis reveals opportunities for improvement in grammaticality, factuality, and completeness
- Cross-lingual performance (English-Hindi) lags significantly behind monolingual performance (English-English)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Matching news articles and YouTube descriptions based on event coverage enables automatic creation of aligned document-summary pairs for cross-lingual summarization.
- Mechanism: The approach leverages the fact that the same news event is often covered across multiple languages and formats (text and video). By matching articles and video descriptions based on publication date proximity, unigram overlap, and semantic similarity, the system can automatically pair documents with their corresponding summaries.
- Core assumption: When a news event is covered in both text articles and video descriptions, the video descriptions serve as valid summaries of the articles, and the content is sufficiently similar across languages for the matching process to work.
- Evidence anchors:
  - [abstract]: "We propose pairing up the coverage of newsworthy events in textual and video format can prove to be helpful for data acquisition for cross lingual summarization."
  - [section]: "We propose pairing up the coverage of newsworthy events in textual and video format can prove to be helpful for data acquisition for cross lingual summarization."
  - [corpus]: Weak evidence. The corpus analysis shows related papers but doesn't directly validate the assumption that video descriptions serve as valid summaries.
- Break condition: If the semantic content of video descriptions significantly diverges from the articles, or if event coverage differs substantially across languages, the matching process will fail to produce valid pairs.

### Mechanism 2
- Claim: Using multiple matching criteria (date, unigram overlap, semantic similarity) improves the quality of aligned pairs.
- Mechanism: By combining multiple signals - temporal proximity (within 2.5 days), lexical overlap (fraction of non-stop word unigrams), and semantic similarity (cosine similarity of DistUSE embeddings) - the system can filter out poor matches and retain high-quality document-summary pairs.
- Core assumption: Each matching criterion captures a different aspect of relevance, and combining them produces better results than using any single criterion.
- Evidence anchors:
  - [section]: "We have three factors driving the pair matching 1. Date of incidence... 2. Unigram Overlap... 3. Semantic Similarity..."
  - [section]: "After the whole process, we are left with 8K article summary pairs for cross lingual summarization and 14K pairs for monolingual summarization."
  - [corpus]: Weak evidence. Related papers mention similar multi-criteria matching but don't provide direct validation.
- Break condition: If the thresholds for any criterion are too strict or too lenient, the matching process may either miss valid pairs or include poor matches.

### Mechanism 3
- Claim: Pretrained multilingual models can be fine-tuned effectively on the automatically created dataset for cross-lingual summarization.
- Mechanism: The collected dataset, despite being automatically created, provides sufficient quality and quantity for fine-tuning pretrained models like mBART, IndicBART, and mT5 to perform cross-lingual summarization from English to Hindi.
- Core assumption: The automatically created dataset, after filtering, contains enough high-quality examples to effectively train summarization models.
- Evidence anchors:
  - [section]: "We also build and analyze multiple baselines on the collected data and report error analysis."
  - [section]: "Results show mBART outperforms other models, achieving ROUGE-1 scores of 49.0 for English-English, 25.7 for English-Hindi, and 40.7 for Hindi-Hindi summarization."
  - [corpus]: Weak evidence. Related papers discuss similar approaches but don't directly validate the effectiveness of fine-tuning on this specific dataset.
- Break condition: If the dataset quality is too low after filtering, or if the models cannot generalize from the automatically created examples, fine-tuning will not produce effective cross-lingual summarization.

## Foundational Learning

- Concept: Semantic similarity using sentence embeddings
  - Why needed here: To match articles and video descriptions that discuss the same event but may use different wording
  - Quick check question: What would happen to the matching process if the semantic similarity threshold was set too high?

- Concept: ROUGE score for evaluation
  - Why needed here: To measure the quality of generated summaries against reference summaries
  - Quick check question: Why might ROUGE scores be lower for cross-lingual summarization compared to monolingual?

- Concept: Multilingual pretraining and fine-tuning
  - Why needed here: To leverage models that understand multiple languages and adapt them to the summarization task
  - Quick check question: What advantage does mBART have over IndicBART for this cross-lingual task?

## Architecture Onboarding

- Component map: Data collection (web scraping + YouTube API) → Pair matching (date + unigram + semantic similarity) → Filtering (compression ratio + overlap + similarity thresholds) → Model training (fine-tuning mBART/IndicBART/mT5) → Evaluation (ROUGE scores + error analysis)
- Critical path: Data collection → Pair matching → Filtering → Model training
- Design tradeoffs: Automated data collection vs. manual annotation quality, strict filtering vs. dataset size, multilingual model choice vs. resource constraints
- Failure signatures: Low number of matched pairs after filtering, poor ROUGE scores on validation set, high error rates in specific categories (grammar, factuality, etc.)
- First 3 experiments:
  1. Test pair matching with synthetic data where ground truth matches are known
  2. Evaluate the impact of different similarity thresholds on the number and quality of matched pairs
  3. Compare model performance on a small manually annotated subset vs. the full automatically created dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold for cosine similarity between DistUSE embeddings when matching document-summary pairs across different language pairs?
- Basis in paper: [explicit] The authors experimented with different similarity thresholds (0.4 for monolingual, 0.3 for crosslingual, and 0.7 for both title and text) but note these are "reasonable thresholds" without justification
- Why unresolved: The paper doesn't provide systematic analysis of how different threshold values affect the quality and quantity of matched pairs
- What evidence would resolve it: A study varying the cosine similarity threshold values and measuring their impact on downstream summarization quality would identify optimal thresholds

### Open Question 2
- Question: How do the different language models (IndicBART, mBART, mT5) compare when evaluated using human judgments beyond ROUGE scores?
- Basis in paper: [explicit] The authors mention error analysis but primarily report ROUGE scores and note opportunities for improvement in grammaticality, factuality, and completeness
- Why unresolved: ROUGE scores may not fully capture the quality differences in summarization for low-resource languages like Hindi
- What evidence would resolve it: A comprehensive human evaluation study comparing model outputs across multiple dimensions (fluency, adequacy, faithfulness) would provide deeper insights

### Open Question 3
- Question: Can the proposed data collection and matching methodology be effectively applied to other Indian languages beyond Hindi and English?
- Basis in paper: [explicit] The authors suggest in Future Work that "for other Indian Languages, the collected data from this method would be a fraction"
- Why unresolved: The paper only demonstrates the approach for English-Hindi pairs without exploring its scalability to other languages
- What evidence would resolve it: Applying the same methodology to other Indian languages (Bengali, Tamil, Telugu, etc.) and comparing the yield and quality of matched pairs would validate its generalizability

## Limitations
- The methodology relies on the assumption that YouTube descriptions serve as valid summaries of news articles, which may not always hold true
- The filtering thresholds for matching (date proximity, unigram overlap, semantic similarity) appear somewhat arbitrary and may need adjustment for different domains or languages
- The dataset size of 28,583 pairs may be limiting for training robust cross-lingual summarization models, especially for the low-resource Hindi-English direction

## Confidence
**High Confidence**: The methodology for data collection and pair matching is clearly described and follows established NLP practices. The evaluation metrics (ROUGE scores) are standard for summarization tasks, and the error analysis provides concrete insights into model weaknesses.

**Medium Confidence**: The effectiveness of the automatically created dataset for training cross-lingual summarization models. While the paper shows mBART achieves reasonable ROUGE scores, the gap between monolingual (English-English: 49.0) and cross-lingual (English-Hindi: 25.7) performance suggests limitations in the dataset quality or model capacity.

**Low Confidence**: The claim that YouTube descriptions serve as valid summaries of news articles. The paper doesn't provide direct validation of this assumption, and the significant performance gap in cross-lingual vs. monolingual summarization may indicate this assumption doesn't fully hold.

## Next Checks
1. **Dataset Quality Validation**: Manually annotate a small subset (100-200 pairs) of the matched article-summary pairs to verify the quality of the automatic matching process. Calculate precision and recall of the matching algorithm against human judgment.

2. **Threshold Sensitivity Analysis**: Systematically vary the matching thresholds (date proximity, unigram overlap, semantic similarity) and measure their impact on the number of matched pairs and downstream model performance. This will help determine if the current thresholds are optimal or need adjustment.

3. **Cross-Domain Generalization Test**: Evaluate the trained models on a different domain (e.g., scientific articles and their abstracts) to assess whether the approach generalizes beyond news events. This will help determine if the methodology is broadly applicable or domain-specific.