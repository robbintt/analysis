---
ver: rpa2
title: '"Mistakes Help Us Grow": Facilitating and Evaluating Growth Mindset Supportive
  Language in Classrooms'
arxiv_id: '2310.10637'
source_url: https://arxiv.org/abs/2310.10637
tags:
- teacher
- gmsl
- teachers
- mindset
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a growth mindset supportive language (GMSL)
  prompt to coach teachers on language reframing and evaluates the resulting reframings
  on 174 teachers and 1,006 students. The results show that both teacher and model
  reframings are rated more supportive of growth mindset than the original teacher
  responses, with the model reframings outperforming even the teacher reframings.
---

# "Mistakes Help Us Grow": Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms

## Quick Facts
- arXiv ID: 2310.10637
- Source URL: https://arxiv.org/abs/2310.10637
- Authors: 
- Reference count: 23
- Key outcome: LLM reframings outperform both original teacher responses and expert reframings in fostering growth mindset supportive language, with self-critique further improving quality.

## Executive Summary
This paper develops a growth mindset supportive language (GMSL) prompt to coach teachers on language reframing and evaluates the resulting reframings on 174 teachers and 1,006 students. The results show that both teacher and model reframings are rated more supportive of growth mindset than the original teacher responses, with the model reframings outperforming even the teacher reframings. This demonstrates the potential of LLMs to support teachers in fostering growth mindset supportive language.

## Method Summary
The study collects 100 unsupportive teacher utterances from elementary math classroom transcripts, develops a GMSL Guide with expert teachers, and creates reframings using both expert teachers and GPT-4 with and without self-critique. These reframings are then evaluated via surveys of teachers and students measuring perceived teacher mindset, challenge-seeking behavior, shame, and respect. The study compares original responses to reframings by teachers (EXPERT) and models (MODEL (0-CRITIQUES), MODEL (1-CRITIQUE)).

## Key Results
- Both teacher and model reframings are rated more supportive of growth mindset than original teacher responses
- Model reframings outperform teacher reframings, with self-critique further improving model performance
- Students shown model reframings are more likely to choose challenging assignments and feel respected

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM reframings consistently outperform both original teacher responses and even expert teacher reframings in fostering growth mindset supportive language (GMSL).
- **Mechanism:** The prompt engineering approach, incorporating self-critique, enables GPT-4 to generate responses that are more aligned with GMSL principles than human experts. The model successfully integrates elements like empathic validation, positioning as a collaborative resource, and autonomy-supportive language.
- **Core assumption:** The prompt design effectively captures and operationalizes the complex tenets of GMSL in a way that the LLM can reliably reproduce.
- **Evidence anchors:**
  - [abstract] "We also find that model reframings outperform those from the GMSL-trained teachers."
  - [section 5.1] "MODEL (1-CRITIQUE) almost doubles students’ likelihood of choosing hard assignments... in contrast to ORIGINAL."
  - [corpus] Weak corpus evidence - only 5 related papers with low citation counts; this suggests limited external validation of the mechanism.
- **Break condition:** If the model's reframings are perceived as too formulaic or artificial by students, the effectiveness would break down. Additionally, if the specific classroom context or cultural background differs significantly from the training data, the reframings may not be as effective.

### Mechanism 2
- **Claim:** Self-critique significantly improves the quality of LLM-generated GMSL responses.
- **Mechanism:** By prompting the model to improve upon its initial response, the self-critique process leads to more nuanced and effective reframings that better align with GMSL principles.
- **Core assumption:** The model's ability to evaluate and improve its own output is reliable and leads to genuine improvements in GMSL quality.
- **Evidence anchors:**
  - [abstract] "We also find that model reframings slightly surpass even the expert teachers’ reframings and that the model with self-critique performs best."
  - [section 5.1] "We also find that self-critique facilitates GMSL, increasing ratings for MODEL (1-CRITIQUE) compared to MODEL (0-CRITIQUES) by 1-6%."
  - [corpus] Weak corpus evidence - limited research on self-critique in educational contexts.
- **Break condition:** If the self-critique process leads the model to over-optimize for specific tenets at the expense of overall coherence or naturalness, the effectiveness would break down.

### Mechanism 3
- **Claim:** Large-scale human evaluations, particularly involving students, are crucial for assessing the effectiveness of LLM-generated GMSL.
- **Mechanism:** By surveying both teachers and students on their perceptions of the impact of different responses, the evaluation framework captures the nuanced effects of GMSL on student outcomes like challenge-seeking behavior and feelings of respect and shame.
- **Core assumption:** Students' hypothetical judgments of teachers' mindset supportive language are valid predictors of actual classroom behavior and learning outcomes.
- **Evidence anchors:**
  - [abstract] "Our findings also demonstrate the benefit of large-scale human evaluations when applying LLMs in educational domains."
  - [section 4.1] "These constructs operationalize growth mindset culture and were designed and tested by experts (Hecht et al., 2022)."
  - [corpus] Weak corpus evidence - limited research on large-scale human evaluations of LLM-generated educational content.
- **Break condition:** If the hypothetical scenarios presented to students and teachers do not accurately reflect real classroom dynamics, or if there are significant cultural or contextual differences not captured in the evaluation, the effectiveness would break down.

## Foundational Learning

- **Concept:** Growth Mindset Supportive Language (GMSL)
  - Why needed here: GMSL is the core construct being operationalized and evaluated in this work. Understanding its principles is crucial for designing effective prompts and interpreting the results.
  - Quick check question: What are the key tenets of GMSL as outlined in the GMSL Guide?

- **Concept:** Prompt Engineering
  - Why needed here: The effectiveness of the LLM-generated reframings depends heavily on the prompt design. Understanding prompt engineering techniques is crucial for replicating and extending this work.
  - Quick check question: How does the self-critique process work in the prompt design, and what is its intended effect?

- **Concept:** Human Evaluation in NLP
  - Why needed here: The evaluation framework relies on human judgments of the perceived impact of different responses. Understanding best practices for human evaluation in NLP is crucial for interpreting the results and designing future studies.
  - Quick check question: What are the key constructs being measured in the teacher and student surveys, and how are they operationalized?

## Architecture Onboarding

- **Component map:** Data collection and annotation -> Prompt engineering and LLM generation -> Human evaluation framework
- **Critical path:** The critical path for this work is the iterative process of designing prompts, generating reframings, and evaluating them with human raters. This process involves close collaboration between NLP researchers, growth mindset experts, and educators.
- **Design tradeoffs:** The use of hypothetical scenarios in the human evaluation allows for large-scale data collection but may not fully capture the complexity of real classroom interactions. The reliance on expert teachers for annotation ensures high-quality data but limits the scalability of the approach.
- **Failure signatures:** If the LLM-generated reframings are consistently rated lower than the original responses or the expert reframings, it would indicate a failure in the prompt engineering process. If the human evaluation results are inconsistent or do not align with expected outcomes, it would suggest issues with the evaluation framework or the constructs being measured.
- **First 3 experiments:**
  1. Test the effect of different prompt designs on the quality of LLM-generated reframings, holding the evaluation framework constant.
  2. Compare the effectiveness of self-critique versus other prompt engineering techniques for improving GMSL quality.
  3. Conduct a qualitative analysis of the LLM-generated reframings to identify common patterns and areas for improvement.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the performance of model-generated reframings be improved to consistently outperform expert teacher reframings across all metrics and contexts?
  - Basis in paper: [explicit] The paper notes that model reframings outperform expert reframings on average, but there are instances where expert reframings are rated higher, especially in terms of perceived teacher mindset and shame by students.
  - Why unresolved: The study shows variability in model performance, with self-critique not always improving outcomes. The specific conditions under which models excel or falter remain unclear.
  - What evidence would resolve it: A systematic analysis of contexts where model reframings fail, followed by targeted adjustments to the prompting strategy or model architecture to address these weaknesses.

- **Open Question 2:** How can LLMs be adapted to generate age-appropriate language while maintaining effective GMSL, especially for younger students?
  - Basis in paper: [explicit] The lexical analysis indicates that model reframings use more formal language than teacher reframings, which may be harder for younger students to understand.
  - Why unresolved: The study highlights the need for evaluating grade-appropriateness but does not provide solutions for adapting model language to different age groups.
  - What evidence would resolve it: Comparative studies testing model reframings with students of different ages, measuring comprehension and effectiveness, followed by model fine-tuning for age-appropriate language.

- **Open Question 3:** How do cultural and linguistic differences impact the effectiveness of GMSL, and can LLMs be adapted to account for these variations?
  - Basis in paper: [inferred] The study acknowledges the limitation of focusing on elementary math classrooms in New England, predominantly serving low-income students of color, and does not address cultural or linguistic variability.
  - Why unresolved: The paper does not explore how GMSL principles might be interpreted or applied differently across diverse cultural and linguistic backgrounds.
  - What evidence would resolve it: Cross-cultural studies evaluating GMSL reframings with diverse student populations, followed by LLM adaptation strategies that incorporate cultural and linguistic nuances.

## Limitations
- Limited generalizability to contexts beyond elementary math classrooms
- Reliance on hypothetical scenarios may not capture real classroom complexity
- Self-report bias in surveys may not reflect actual classroom dynamics

## Confidence
- **High Confidence:** Both teacher and model reframings are rated more supportive of growth mindset than original responses
- **Medium Confidence:** Model reframings outperform expert teacher reframings, with self-critique improving quality
- **Low Confidence:** Broader implications about LLMs' potential to support teachers in fostering growth mindset language

## Next Checks
1. **Context Transferability Test:** Apply the same prompt engineering approach to classroom transcripts from different subjects (e.g., language arts, science) and grade levels to assess the generalizability of the reframings' effectiveness.

2. **Longitudinal Classroom Implementation:** Conduct a field study where trained teachers use the GMSL reframings in their actual classrooms over a semester, measuring student learning outcomes and mindset changes rather than just hypothetical perceptions.

3. **Cross-Cultural Validation:** Test the effectiveness of the LLM-generated reframings with classroom data and student populations from diverse cultural backgrounds to ensure the approach is not culturally biased or limited to specific contexts.