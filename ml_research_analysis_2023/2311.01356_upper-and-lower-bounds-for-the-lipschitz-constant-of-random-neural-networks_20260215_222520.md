---
ver: rpa2
title: Upper and lower bounds for the Lipschitz constant of random neural networks
arxiv_id: '2311.01356'
source_url: https://arxiv.org/abs/2311.01356
tags:
- networks
- random
- proposition
- constant
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes upper and lower bounds for the Lipschitz
  constant of random ReLU neural networks with He initialization. For shallow networks,
  the Lipschitz constant is characterized up to an absolute numerical constant, showing
  it is proportional to the square root of the input dimension.
---

# Upper and lower bounds for the Lipschitz constant of random neural networks

## Quick Facts
- arXiv ID: 2311.01356
- Source URL: https://arxiv.org/abs/2311.01356
- Reference count: 34
- Primary result: Establishes upper and lower bounds for Lipschitz constants of random ReLU networks, showing they grow at most polynomially with depth and width.

## Executive Summary
This paper provides a comprehensive analysis of Lipschitz constants for random ReLU neural networks with He initialization. For shallow networks, the Lipschitz constant is characterized up to an absolute numerical constant, showing it scales as the square root of the input dimension. For deep networks of sufficiently large width, upper and lower bounds are derived that match up to a logarithmic factor depending on network depth. The key finding is that the Lipschitz constant grows at most polynomially with depth and width, providing insights into the worst-case robustness of neural networks against adversarial perturbations.

## Method Summary
The analysis employs covering number arguments, VC-dimension bounds, and sub-gaussian random variable theory to establish bounds on the expected spectral norm of the gradient chain across network layers. For shallow networks, Hoeffding's inequality ensures concentration of active ReLU units, while for deep networks, matrix deviation inequalities and isotropy properties are leveraged. The proofs rely on Dudley's inequality to relate covering numbers to expected suprema of Gaussian processes.

## Key Results
- Shallow networks (L=1): Lipschitz constant is proportional to sqrt(d) up to absolute constants
- Deep networks (L>1): Lipschitz constant scales as sqrt(d) up to logarithmic factors in width N/d when N ≳ dL²
- Bounds hold with high probability for networks with He initialization and symmetric continuous bias distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Lipschitz constant of random ReLU networks is bounded by the expected spectral norm of the gradient chain across layers.
- Mechanism: By expressing the Lipschitz constant as the supremum of the Euclidean norm of the gradient chain, and using the sub-gaussianity of He-initialized weights, the chain's spectral norm concentrates around its mean with high probability. Dudley's inequality links the covering number of the space of possible gradient chains to the Lipschitz bound.
- Core assumption: The network's gradient chain can be represented as a finite-dimensional random process with bounded sub-gaussian tails, and the bias distributions are continuous and symmetric.
- Evidence anchors:
  - [abstract]: "For deep networks of sufficiently large width, upper and lower bounds are derived that match up to a logarithmic factor depending on the network depth."
  - [section]: "The estimate (4.2) is essential for our derivation of upper bounds for Lipschitz constants of random ReLU networks."
- Break condition: If the network width is too small (less than d + 2 for deep networks), or if bias distributions are not symmetric/continuous, the concentration and isotropy properties fail, invalidating the bounds.

### Mechanism 2
- Claim: In shallow networks, the Lipschitz constant scales as sqrt(d) because the number of active ReLU units in the hidden layer concentrates around N/2, and the spectral norm of a Gaussian matrix with N/2 columns scales as sqrt(d).
- Mechanism: Hoeffding's inequality ensures that the number of active units concentrates around N/2, so the gradient chain is effectively a Gaussian matrix of size d x (N/2). The spectral norm of such a matrix scales as sqrt(d) for large N, and this is tight up to constants.
- Core assumption: The bias distributions are symmetric, ensuring the number of active ReLU units is concentrated around N/2.
- Evidence anchors:
  - [abstract]: "For shallow networks, the Lipschitz constant is characterized up to an absolute numerical constant, showing it is proportional to the square root of the input dimension."
  - [section]: "By Hoeffding's inequality the number of ones in D(0) concentrates around N/2."
- Break condition: If the bias distribution is not symmetric, the number of active units may not concentrate, invalidating the sqrt(d) scaling.

### Mechanism 3
- Claim: In deep networks with width N ≥ C·d·L², the Lipschitz constant scales as sqrt(d) up to a logarithmic factor in N/d, because the product of layer-wise transformations remains almost isometric with high probability.
- Mechanism: The matrix deviation inequality ensures that the product of layer-wise transformations is almost isometric, and the spectral norm of the final output layer's weight vector scales as sqrt(k), where k = min{N,d}. The isotropy of the intermediate layers' transformations ensures that the Lipschitz constant is dominated by the input dimension.
- Core assumption: The width N is sufficiently large relative to d and L (specifically, N ≥ C·d·L²), ensuring the isotropy and concentration properties hold across all layers.
- Evidence anchors:
  - [abstract]: "For deep networks of sufficiently large width, upper and lower bounds are derived that match up to a logarithmic factor depending on the network depth."
  - [section]: "The product matrix D(L−1)W (L−1)···D(0)W (0) is almost isometric with high probability."
- Break condition: If the width is not sufficiently large, the isotropy and concentration properties may fail, and the Lipschitz constant may grow faster than sqrt(d).

## Foundational Learning

- Concept: Sub-gaussian random variables and their properties.
  - Why needed here: The He initialization produces sub-gaussian weights, which are crucial for concentration bounds and isotropy arguments.
  - Quick check question: What is the sub-gaussian norm of a Gaussian random variable with variance σ²?

- Concept: VC-dimension and Sauer's lemma.
  - Why needed here: VC-dimension is used to bound the covering numbers of sets of indicator functions, which are needed to control the complexity of the gradient chain's possible configurations.
  - Quick check question: What is the VC-dimension of the class of homogeneous halfspaces in R^k?

- Concept: Dudley's inequality and Gaussian processes.
  - Why needed here: Dudley's inequality relates the covering numbers of a metric space to the supremum of a Gaussian process, which is used to bound the expected spectral norm of the gradient chain.
  - Quick check question: What is the expected supremum of a Gaussian process with respect to the L² metric?

## Architecture Onboarding

- Component map:
  - He initialization -> weights drawn from N(0, 2/N), biases from continuous symmetric distributions
  -> Gradient chain computation -> product of layer-wise transformations
  -> Spectral norm estimation -> using covering numbers and concentration bounds
  -> Lipschitz constant bound -> up to absolute constants or logarithmic factors

- Critical path:
  1. Initialize weights and biases according to He initialization
  2. Compute the gradient chain for a fixed input
  3. Bound the spectral norm of the gradient chain using Dudley's inequality and covering numbers
  4. Extend the bound to all inputs using concentration of measure

- Design tradeoffs:
  - Shallow vs. deep: Shallow networks have tighter bounds (up to constants) because the gradient chain is simpler, while deep networks require additional assumptions on width
  - Bias distribution: Symmetric bias distributions are crucial for concentration, but may not always be realistic
  - Width: Larger width improves isotropy and concentration, but increases computational cost

- Failure signatures:
  - Lipschitz constant grows faster than sqrt(d): Width is too small, or bias distributions are not symmetric
  - Bounds are loose: Covering numbers are not tight, or the matrix deviation inequality is not applicable

- First 3 experiments:
  1. Verify the Lipschitz constant scales as sqrt(d) for shallow networks with different bias distributions
  2. Test the effect of width on the Lipschitz constant for deep networks, and compare with the theoretical bounds
  3. Check the isotropy of the intermediate layers' transformations for different network architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise dependence of the Lipschitz constant on the depth L of deep ReLU networks, beyond the logarithmic factor in width currently established?
- Basis in paper: [explicit] The paper states "Clarifying the precise dependence of the Lipschitz constant on the network depth L remains an open problem."
- Why unresolved: The upper and lower bounds match up to a factor that grows exponentially with depth, but the exact growth rate is unknown.
- What evidence would resolve it: Tight upper and lower bounds that match up to a constant factor for all depths L, or a proof that such bounds cannot exist.

### Open Question 2
- Question: Do the Lipschitz constant bounds for random ReLU networks extend to non-symmetric or discrete bias distributions?
- Basis in paper: [inferred] The lower bounds require symmetric and continuous bias distributions, while upper bounds allow arbitrary distributions.
- What evidence would resolve it: Lower bounds that match upper bounds for non-symmetric or discrete bias distributions, or a proof that such bounds cannot exist.

### Open Question 3
- Question: Can the assumption N > d+2 for deep networks be relaxed to allow for more narrow networks?
- Basis in paper: [explicit] The upper bound for deep networks requires N > d+2, while the lower bound requires N ≳ dL^2.
- Why unresolved: The proofs for the upper bound rely on covering number arguments that require this width assumption.
- What evidence would resolve it: Upper bounds that hold for N ≤ d+2, or a proof that such bounds cannot exist.

## Limitations
- Bounds are probabilistic rather than deterministic, holding with high probability rather than almost surely
- Width requirement for deep networks (N ≥ CdL²) may be impractical for very deep architectures
- Analysis assumes He initialization specifically; results may not extend to other initialization schemes

## Confidence
- Confidence: Medium-High for shallow network bounds (Theorems 3.1 and 3.2)
- Confidence: Medium for deep network bounds (Theorems 3.3 and 3.4)

## Next Checks
1. Verify the covering number bounds N(L, ||·||₂, ε) explicitly for the specific classes used in Lemmas 5.3 and 5.8
2. Numerically test the width requirement N ≳ dL² by computing empirical Lipschitz constants for networks with varying widths
3. Check sensitivity of the bounds to different symmetric bias distributions beyond the assumed continuous case