---
ver: rpa2
title: Characterizing Out-of-Distribution Error via Optimal Transport
arxiv_id: '2305.15640'
source_url: https://arxiv.org/abs/2305.15640
tags:
- error
- shift
- ppseudo
- distribution
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of estimating a model's out-of-distribution
  (OOD) error when only unlabeled target data is available. The authors introduce
  Confidence Optimal Transport (COT) and its variant COTT, which leverage optimal
  transport theory to provide robust error estimates.
---

# Characterizing Out-of-Distribution Error via Optimal Transport

## Quick Facts
- arXiv ID: 2305.15640
- Source URL: https://arxiv.org/abs/2305.15640
- Reference count: 40
- This paper introduces COT and COTT, which leverage optimal transport theory to provide robust error estimates for out-of-distribution data, achieving up to 3x lower prediction error compared to state-of-the-art methods.

## Executive Summary
This paper addresses the challenge of estimating a model's out-of-distribution (OOD) error when only unlabeled target data is available. The authors introduce Confidence Optimal Transport (COT) and its variant COTT, which leverage optimal transport theory to provide robust error estimates. COT measures the Wasserstein distance between predicted target class probabilities and the true source label distribution, while COTT adds a thresholding mechanism for improved accuracy. These methods are shown to be more reliable than existing approaches, particularly in scenarios with large pseudo-label shift. The authors evaluate COT and COTT on 11 benchmark datasets across multiple domains, demonstrating their effectiveness in estimating OOD error.

## Method Summary
COT computes the Wasserstein distance between predicted target class probabilities and the true source label distribution, providing a more robust error estimate than methods like Average Confidence. COTT extends COT by applying a learned threshold over the optimal transportation costs, further improving accuracy by reducing the impact of outliers. Both methods require only unlabeled target data and a validation set from the source distribution. The approach is validated across various datasets and model architectures, showing significant improvements in OOD error estimation compared to state-of-the-art methods.

## Key Results
- COT and COTT achieve up to 3x lower prediction error compared to state-of-the-art methods
- COT is more robust to pseudo-label shift than existing methods like Average Confidence
- COTT further improves upon COT by reducing the impact of outlier transport costs through thresholding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidence Optimal Transport (COT) provides more robust error estimates than Average Confidence (AC) by measuring the Wasserstein distance between predicted target class probabilities and the true source label distribution.
- Mechanism: COT uses the Wasserstein distance to quantify the difference between the distribution of confidence vectors and the source label distribution, rather than simply averaging the maximum confidence scores. This captures the full structure of the distribution shift.
- Core assumption: The target label distribution is close to the source label distribution.
- Evidence anchors:
  - [abstract]: "COT measures the Wasserstein distance between predicted target class probabilities and the true source label distribution"
  - [section]: "Our approach, Confidence Optimal Transport (COT), leverages optimal transport theory to predict the error of a model as the Wasserstein distance between the predicted target class probabilities and the true source label distribution."
  - [corpus]: No direct corpus evidence found for this specific mechanism.
- Break condition: The assumption that PT(y) ≈ PS(y) is violated significantly, such as when there is substantial label shift between source and target distributions.

### Mechanism 2
- Claim: Pseudo-label shift is a key indicator of error underestimation in existing methods.
- Mechanism: The difference between the predicted target label distribution (Ppseudo(y)) and the true target label distribution (PT(y)) indicates how miscalibrated the model is on the target distribution. Larger pseudo-label shifts correlate with larger underestimation errors in methods like AC.
- Core assumption: The pseudo-label shift is strongly correlated with model miscalibration on the target distribution.
- Evidence anchors:
  - [abstract]: "we identify pseudo-label shift, or the difference between the predicted and true OOD label distributions, as a key indicator to this underestimation."
  - [section]: "we empirically show that there is a strong positive correlation between the pseudo-label shift and|ϵ−ˆϵAC|, which we define as the model's miscalibration."
  - [corpus]: No direct corpus evidence found for this specific mechanism.
- Break condition: The correlation between pseudo-label shift and miscalibration weakens or becomes non-existent, possibly due to specific types of distribution shifts or model architectures.

### Mechanism 3
- Claim: COTT improves upon COT by using a learned threshold over optimal transportation costs, making it more robust to outliers.
- Mechanism: COTT computes a threshold on the validation set such that the fraction of samples with transport costs above the threshold matches the source error. This threshold is then applied to the target set to estimate error, reducing the impact of outlier costs.
- Core assumption: The transport cost distribution has outliers that can skew the mean, and a threshold can effectively mitigate this issue.
- Evidence anchors:
  - [abstract]: "we introduce an empirically-motivated variant of COT, Confidence Optimal Transport with Thresholding (COTT), which applies thresholding to the individual transport costs and further improves the accuracy of COT's error estimates."
  - [section]: "we introduce COT with Thresholding, COTT... which introduces a learned threshold over the optimal transportation costs in line with prior work [13] and empirically improves upon COT's performance."
  - [corpus]: No direct corpus evidence found for this specific mechanism.
- Break condition: The transport cost distribution on the target set has a significantly different shape than the validation set, making the learned threshold ineffective.

## Foundational Learning

- Concept: Wasserstein distance (optimal transport)
  - Why needed here: COT and COTT rely on computing the Wasserstein distance between distributions of confidence vectors and label distributions. Understanding this metric is crucial for grasping how these methods estimate OOD error.
  - Quick check question: What is the Wasserstein distance between two point masses at positions 0 and 1 on the real line?
    - Answer: 1

- Concept: Pseudo-label shift
  - Why needed here: Pseudo-label shift is a central concept in understanding why existing methods underestimate OOD error and how COT addresses this issue. It's the difference between the predicted and true target label distributions.
  - Quick check question: If a model predicts 80% of OOD samples as class A and the true distribution is 50% class A, what is the pseudo-label shift in terms of KL divergence?
    - Answer: KL((0.8, 0.2) || (0.5, 0.5)) = 0.8*log(0.8/0.5) + 0.2*log(0.2/0.5) ≈ 0.029

- Concept: Model calibration
  - Why needed here: Existing methods like AC rely heavily on the model being well-calibrated. Understanding what calibration means and how it affects error estimation is important for appreciating COT's advantages.
  - Quick check question: If a model predicts class probabilities of [0.9, 0.1] for a sample and it belongs to class 0, is the model calibrated on this sample?
    - Answer: Yes, because the predicted probability of the correct class (0.9) matches the actual correctness likelihood (1.0) reasonably well.

## Architecture Onboarding

- Component map: Trained model -> Unlabeled target data -> COT/COTT error estimate
- Critical path:
  1. Train model on source data
  2. Collect unlabeled target data
  3. Compute COT or COTT error estimate
  4. Compare to ground truth error (for evaluation)

- Design tradeoffs:
  - COT vs AC: COT is more robust but assumes label distribution stability
  - COTT vs COT: COTT is more robust to outliers but requires a validation set
  - Computational cost: COT and COTT are more expensive than AC due to optimal transport computation

- Failure signatures:
  - Large pseudo-label shift with significant label distribution difference
  - Transport cost distribution on target set differs significantly from validation set (for COTT)
  - Severe miscalibration that isn't captured by pseudo-label shift

- First 3 experiments:
  1. Implement COT and verify it produces larger error estimates than AC on a simple synthetic dataset with known distribution shift
  2. Test COT on CIFAR-10 with synthetic corruptions, comparing to AC and ground truth error
  3. Implement COTT and verify it reduces MAE compared to COT on a dataset with outlier transport costs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise theoretical relationship between pseudo-label shift and miscalibration beyond the observed correlation?
- Basis in paper: [inferred] The paper states "The relationship between miscalibration and pseudo-label shift is currently only empirical in nature, not theoretical."
- Why unresolved: While the paper demonstrates a strong empirical correlation between pseudo-label shift and miscalibration, it does not provide a theoretical foundation for this relationship. This limits the generalizability of their findings.
- What evidence would resolve it: A theoretical proof showing that pseudo-label shift is a necessary and/or sufficient condition for miscalibration, or at least deriving bounds on miscalibration as a function of pseudo-label shift.

### Open Question 2
- Question: How does the proposed method perform when there is a significant label shift between source and target distributions?
- Basis in paper: [explicit] "In addition, while we have derived a lower bound for error estimates, we have not currently formulated an upper bound; however, we have not observed overestimating the error to be a common problem empirically." Also, "Note that the WILDS benchmark datasets contain label shift, meaning PS(y)≠PT (y). This leads COTT to overestimate the error on the CivilComments-WILDS."
- Why unresolved: The paper acknowledges the limitations of their method in scenarios with label shift but does not provide a comprehensive evaluation of its performance in such settings. This is a significant gap given that label shift is common in real-world applications.
- What evidence would resolve it: Extensive experiments on datasets with varying degrees of label shift, and the development of theoretical bounds for the method's performance under label shift.

### Open Question 3
- Question: What are the computational implications of scaling the proposed method to larger datasets and more complex models?
- Basis in paper: [explicit] "Computationally, COT (or Wasserstein Distance in general) is implemented as a two-step process: 1) calculating individual transport costs for all samples; 2) returning the mean across all transport costs as the error estimate. While we have seen that COT has some protection against miscalibration, it is not completely immune."
- Why unresolved: The paper mentions computational complexity issues but does not provide a detailed analysis of how the method scales with dataset size and model complexity. This is crucial for understanding the method's practical applicability.
- What evidence would resolve it: Empirical studies comparing the computational cost of the proposed method with other OOD error estimation techniques on large-scale datasets and complex models, along with theoretical analysis of the method's computational complexity.

## Limitations
- The method assumes label distribution stability between source and target domains, which may not hold in scenarios with significant label shift.
- Computational complexity of optimal transport could be prohibitive for very large-scale applications.
- The theoretical relationship between pseudo-label shift and miscalibration is empirical, not rigorously proven.

## Confidence
- **High confidence**: The empirical superiority of COT over AC on benchmark datasets is well-demonstrated with quantitative results showing up to 3x lower MAE.
- **Medium confidence**: The theoretical justification for why COT captures distribution shifts better than AC is sound, but the correlation between pseudo-label shift and miscalibration, while observed empirically, lacks rigorous theoretical grounding.
- **Medium confidence**: The effectiveness of COTT's thresholding mechanism is supported by experimental results, but the method's sensitivity to validation set size and transport cost distribution differences isn't thoroughly characterized.

## Next Checks
1. **Label shift robustness**: Test COT and COTT on datasets with controlled, varying degrees of label shift to quantify performance degradation and validate the claim that label distribution stability is a key assumption.
2. **Computational scaling**: Evaluate the computational overhead of COT and COTT on larger datasets (e.g., ImageNet-scale) and assess the impact of batch size on both accuracy and runtime to determine practical scalability.
3. **Distribution sensitivity**: Systematically vary the transport cost distribution on the target set relative to the validation set to test COTT's robustness to distribution differences and identify failure modes.