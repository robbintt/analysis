---
ver: rpa2
title: 'Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition
  for Children VS. Adults'
arxiv_id: '2309.07927'
source_url: https://arxiv.org/abs/2309.07927
tags:
- speech
- myst
- children
- whisper
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automatic speech recognition
  (ASR) for children, which lags behind adult ASR due to limited child-specific datasets
  and unique speech characteristics. The authors improve upon previous work by efficiently
  preprocessing the My Science Tutor (MyST) children's speech corpus, increasing usable
  data from 65 to 179.2 hours.
---

# Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults

## Quick Facts
- arXiv ID: 2309.07927
- Source URL: https://arxiv.org/abs/2309.07927
- Reference count: 0
- Fine-tuning Whisper on filtered children's speech reduces WER from 13.93% to 9.11% (Whisper-Small) and from 13.23% to 8.61% (Whisper-Medium)

## Executive Summary
This paper addresses the challenge of automatic speech recognition (ASR) for children, which consistently underperforms adult ASR due to limited child-specific datasets and unique speech characteristics. The authors improve upon previous work by efficiently preprocessing the My Science Tutor (MyST) children's speech corpus, increasing usable data from 65 to 179.2 hours through careful filtering of mistranscribed and low-quality audio files. By fine-tuning Whisper models on this enhanced dataset, they achieve significant WER reductions on the MyST test set and demonstrate that improvements generalize to unseen datasets, establishing Whisper's viability for effective children's speech recognition.

## Method Summary
The authors preprocess the MyST corpus by filtering out files with WER > 50% (identified using Whisper-Large), removing files with fewer than 3 words, and concatenating short files within recording sessions to approximately 30 seconds while maintaining conversational context. The data is split into train (132.5 hours), development (20.9 hours), and test (25.8 hours) sets. They fine-tune Whisper Small and Medium models (both English and multilingual variants) on the filtered MyST data using learning rates of 1e-5, batch sizes of 64 (Small) or 32 (Medium), and gradient accumulation step of 1 until convergence. The models are evaluated on MyST, CSLU Kids scripted data, and Librispeech test-clean to assess performance and generalization.

## Key Results
- WER on MyST test set reduced from 13.93% to 9.11% with Whisper-Small and from 13.23% to 8.61% with Whisper-Medium
- Improvements generalize to unseen datasets including CSLU Kids scripted portion
- Children's speech recognition WER (9-14%) remains 6-10 times higher than adult speech recognition on Librispeech

## Why This Works (Mechanism)

### Mechanism 1
Improved preprocessing increases usable training data from 65 to 179.2 hours by removing mistranscribed and low-quality audio files. Filtering out files with high WER (>50%) and short files (<3 words) reduces noise in the training set, allowing the model to focus on reliable speech-text pairs. Core assumption: High WER files are mostly mistranscribed or contain poor audio quality that harms model learning. Break condition: If the filtering threshold is too strict, it might remove valid but challenging utterances, reducing dataset diversity.

### Mechanism 2
Concatenating short files into ~30-second chunks improves batch efficiency and context retention. Whisper processes audio in 30-second chunks; concatenating short utterances avoids excessive padding and preserves conversational context within a recording session. Core assumption: Most files in MyST are too short (<8 seconds on average), leading to inefficient training with padding. Break condition: If concatenation disrupts natural turn-taking or introduces incorrect speaker labels, it could confuse the model.

### Mechanism 3
Fine-tuning Whisper on child speech reduces the performance gap between child and adult ASR. Whisper's large pre-training corpus includes diverse speech patterns, making it adaptable to children's speech variability when fine-tuned on child-specific data. Core assumption: Whisper's architecture and pre-training data provide sufficient generalization capacity to adapt to under-represented speech patterns like those of children. Break condition: If the child speech variability is too far from Whisper's pre-training distribution, fine-tuning may plateau without achieving adult-level performance.

## Foundational Learning

- **Word Error Rate (WER) calculation**: Why needed here - WER is the primary metric for evaluating ASR performance and tracking improvements. Quick check: If a transcript has 3 errors in 20 words, what is the WER?

- **Text normalization for ASR**: Why needed here - Standardizing transcriptions (e.g., "you're" → "you are", spelling out numbers) ensures consistent evaluation and prevents penalizing stylistic differences. Quick check: Why is it important to convert all text to lowercase before training?

- **Dataset filtering and quality control**: Why needed here - Removing low-quality or mistranscribed data prevents the model from learning incorrect patterns. Quick check: What could happen if you include files with WER > 50% in your training set?

## Architecture Onboarding

- **Component map**: Audio input → Encoder → Decoder → Text prediction
- **Critical path**: Data preprocessing → model loading → fine-tuning loop (forward pass, loss calculation, backward pass, parameter update) → evaluation on test sets
- **Design tradeoffs**: Using multilingual Whisper vs. English-only affects performance on child speech; larger models generalize better but require more compute
- **Failure signatures**: High WER on both seen and unseen datasets indicates overfitting; very low WER on training but high on test indicates poor generalization
- **First 3 experiments**:
  1. Run zero-shot evaluation of Whisper on MyST to establish baseline WER
  2. Apply preprocessing filters and verify dataset size reduction and quality improvement
  3. Fine-tune Whisper-Small on filtered MyST data and evaluate WER reduction on test set

## Open Questions the Paper Calls Out

- What is the impact of different types of noise (e.g., classroom babble, environmental sounds) on Whisper's performance for children's speech recognition? The authors plan to explore noise robustness, specifically babble noise and typical classroom nonspeech sounds.

- Does Whisper exhibit bias towards specific age groups, genders, or racial groups in children's speech recognition? The authors intend to investigate potential biases across different demographic groups of children.

- Can grade-specific or age-group specific ASR models outperform general "children's speech" models for recognizing speech from different developmental stages? The authors reference grade-specific ASR models that propose grouping different age groups separately.

## Limitations

- Dataset Quality and Representativeness: Children's speech datasets remain significantly smaller than adult speech datasets, and the MyST corpus may not fully represent the diversity of children's speech across different ages, accents, and speaking contexts.

- Generalization Claims: The evaluation on the CSLU spontaneous test set is not reported, which would provide a stronger test of generalization to more natural child speech patterns.

- Model Architecture Limitations: The paper doesn't explore whether Whisper's architecture is fundamentally optimal for children's speech, suggesting current approaches may have inherent limitations.

## Confidence

- **High Confidence**: The preprocessing methodology and dataset filtering procedures are well-documented and reproducible. The WER improvements on the MyST test set are directly measurable and verifiable.

- **Medium Confidence**: The generalization to unseen datasets shows promise but is limited by the scope of evaluation. The comparison with adult speech performance provides useful context but doesn't account for all confounding factors.

- **Low Confidence**: Claims about the need for audio-conditional language models are speculative and not empirically validated in this work. The long-term implications for real-world deployment are not explored.

## Next Checks

1. Evaluate on CSLU Spontaneous Test Set: Complete the evaluation by testing the fine-tuned models on the CSLU spontaneous test set to provide a more rigorous test of generalization to natural child speech patterns.

2. Cross-Age Performance Analysis: Analyze model performance across different age groups within the MyST dataset to determine if improvements are uniform or if certain age ranges benefit more than others.

3. Audio Quality Impact Assessment: Systematically evaluate how different audio quality levels in the filtered dataset affect final WER, particularly testing whether the filtering threshold might be excluding challenging but valid utterances that could improve model robustness.