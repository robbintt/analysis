---
ver: rpa2
title: Augmenting Ad-Hoc IR Dataset for Interactive Conversational Search
arxiv_id: '2311.06119'
source_url: https://arxiv.org/abs/2311.06119
tags:
- clarifying
- user
- questions
- interactions
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating conversational
  search systems at scale, which requires datasets containing mixed-initiative interactions
  like system-generated clarifying questions. Existing datasets are too small for
  training neural models.
---

# Augmenting Ad-Hoc IR Dataset for Interactive Conversational Search

## Quick Facts
- arXiv ID: 2311.06119
- Source URL: https://arxiv.org/abs/2311.06119
- Reference count: 40
- Key outcome: Automatically generates large-scale conversational IR datasets by augmenting ad-hoc IR datasets with simulated user-system interactions

## Executive Summary
This paper addresses the challenge of evaluating conversational search systems at scale by proposing a methodology to automatically generate large-scale conversational IR datasets from existing ad-hoc IR datasets. The authors develop a two-component system consisting of a query clarification generator and user simulation to create clarifying questions and corresponding answers. Applied to the MsMarco dataset, their approach generates the MiMarco dataset, which includes mixed-initiative interactions. Human evaluation shows their clarifying questions are more relevant than baseline approaches, and a neural ranking model leveraging these interactions significantly outperforms strong baselines, demonstrating the utility of the simulated interactions for IR tasks.

## Method Summary
The methodology consists of a two-step pipeline: first, facets are extracted from relevant and irrelevant passages using Sentence-BERT embeddings, selecting top-K similar words as facets. Second, a T5 model is fine-tuned on the ClariQ dataset to generate clarifying questions conditioned on the query and facet. A separate T5 model is fine-tuned on MsMarco to generate yes/no answers conditioned on the query, clarifying question, and sampled relevant passage as intent. The offline evaluation methodology generates interactions for training while the online methodology generates interactions for testing, preventing information leakage. The resulting dataset is used to train a neural ranking model (MonoT5) which is evaluated on IR tasks using BM25 for initial retrieval.

## Key Results
- Human evaluation shows the proposed clarifying questions are more relevant than baseline approaches
- Neural ranking model leveraging the generated interactions significantly outperforms strong baselines
- The methodology successfully creates a large-scale conversational IR dataset (MiMarco) from the MsMarco ad-hoc dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method creates large-scale conversational IR datasets by automatically generating mixed-initiative interactions from existing ad-hoc IR datasets.
- Mechanism: The system uses two key components - a query clarification generator (trained on ClariQ dataset) and a user simulation - to create clarifying questions and corresponding answers. These interactions are then added to the MsMarco dataset to form MiMarco.
- Core assumption: The generated clarifying questions and answers will be relevant and useful for training neural ranking models.
- Evidence anchors:
  - [abstract] "Our approach uses a query clarification generator and user simulation to create clarifying questions and answers."
  - [section 3] "We propose a methodology to automatically build large-scale conversational IR datasets from ad-hoc IR datasets"
  - [corpus] Found related papers on conversational query clarification and interactive dialogue systems, supporting the relevance of this approach.
- Break condition: If the generated interactions are not deemed relevant or useful by human evaluation or fail to improve ranking performance in extrinsic tasks.

### Mechanism 2
- Claim: The clarifying model generates questions conditioned on both the query and extracted facets to ensure relevance.
- Mechanism: Facets are extracted from relevant and irrelevant passages. The clarifying model (CM) is trained to generate questions given the query and a facet. This constrains the generation to be about specific aspects of the query.
- Core assumption: Facets extracted from passages accurately represent sub-topics or aspects of the query.
- Evidence anchors:
  - [section 3.2] "Facets might be explicit or implicit depending on the dataset... We formulate it as a bijective function ðœ“ (ð‘ƒ) :â†’ F that maps a set ð‘ƒ of passages to a set of facets."
  - [section 3.3.1] "The generation of clarifying questions ð‘ð‘ž is conditioned on the initial query ð‘ž and a facet ð‘“"
  - [corpus] Related papers on extracting query facets and using them for clarification support this approach.
- Break condition: If the facet extraction fails to capture meaningful aspects of the query, or if the CM cannot generate coherent questions conditioned on the facets.

### Mechanism 3
- Claim: The user simulation generates answers consistent with the search intent and the relevance of the facet.
- Mechanism: The user simulation (US) takes as input the clarifying question, the search intent (represented by a sampled relevant passage), and the facet. It outputs a 'yes' or 'no' answer, which is 'yes' if the facet is from a relevant passage and 'no' otherwise.
- Core assumption: The relevance of a passage can be used as a proxy for the user's search intent.
- Evidence anchors:
  - [section 3.3.2] "The user simulation aims at estimating the probability of an answer ð‘Ž given a query ð‘ž, a search intent ð‘–ð‘›ð‘¡, and a clarifying question"
  - [section 3.4.1] "The user simulation presented in Section 3.3.2 is replaced by a simple heuristic matching answers ð‘Ž with the relevance of facets ð‘“"
  - [corpus] Related papers on user simulation in dialogue systems and conversational search support this approach.
- Break condition: If the sampled relevant passage does not accurately represent the user's intent, or if the US cannot generate plausible answers.

## Foundational Learning

- Concept: Information Retrieval (IR) basics and evaluation metrics (NDCG, MRR)
  - Why needed here: The paper is about creating datasets for conversational IR and evaluating ranking models on these datasets.
  - Quick check question: What is the difference between NDCG@1 and NDCG@10, and why are both important?

- Concept: Neural ranking models and cross-encoder architectures
  - Why needed here: The paper proposes a neural ranking model (CLART5) that leverages the generated mixed-initiative interactions for document ranking.
  - Quick check question: How does a cross-encoder model differ from a dual-encoder model in terms of input and computation?

- Concept: Query clarification and user simulation in dialogue systems
  - Why needed here: The core of the paper is generating clarifying questions and simulating user answers, which are key components of dialogue systems.
  - Quick check question: Why is it important to condition the generation of clarifying questions on both the query and a facet?

## Architecture Onboarding

- Component map: Ad-hoc IR dataset (MsMarco) -> Facet Extraction -> Clarifying Model -> User Simulation -> Dataset Augmentation -> Neural Ranking Model (MonoT5)

- Critical path: Ad-hoc IR dataset â†’ Facet Extraction â†’ Clarifying Model â†’ User Simulation â†’ Dataset Augmentation â†’ Neural Ranking Model â†’ Improved ranking performance

- Design tradeoffs:
  - Single-turn vs. multi-turn interactions: The paper focuses on single-turn for simplicity, but multi-turn could provide more context.
  - 'Yes'/'no' answers vs. more complex answers: Simpler answers are easier to generate and evaluate but may be less realistic.
  - Using relevance judgments vs. pseudo-relevance feedback: The paper uses both for training and testing, respectively, to avoid bias.

- Failure signatures:
  - Generated clarifying questions are not deemed relevant or useful by human evaluation.
  - Neural ranking model does not improve performance on the augmented dataset.
  - Facet extraction fails to capture meaningful aspects of the query.
  - User simulation generates implausible or inconsistent answers.

- First 3 experiments:
  1. Run the facet extraction on a sample of MsMarco queries and manually verify if the extracted facets are meaningful.
  2. Generate clarifying questions for a sample of queries using the CM and compare them to the ClariQ references using METEOR and COSIM.
  3. Train the CLART5 model on a subset of the augmented dataset and evaluate its ranking performance on a held-out test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed methodology be extended to generate clarifying questions that ask about multiple topics or facets simultaneously, rather than focusing on a single facet?
- Basis in paper: [explicit] The authors mention that real clarifying questions might also question about multiple topics/facets in a single turn, but their current methodology is limited to single facet questions.
- Why unresolved: The paper only demonstrates the methodology with single facet clarifying questions. Generating questions that address multiple facets simultaneously is a more complex task that requires additional research.
- What evidence would resolve it: Developing and evaluating a methodology for generating multi-facet clarifying questions, and comparing its effectiveness to single facet questions in improving document ranking and user satisfaction.

### Open Question 2
- Question: How can the user simulation be improved to generate more complex and varied answers beyond simple 'yes' or 'no' responses?
- Basis in paper: [explicit] The authors acknowledge that the user simulation is limited to 'yes' or 'no' answers and that simulating more complex user responses is a challenge for the future.
- Why unresolved: The current user simulation's simplicity limits the realism and diversity of the generated interactions. More complex user responses would better reflect real conversational search scenarios.
- What evidence would resolve it: Developing and evaluating a user simulation that can generate a wider range of responses, including open-ended answers and explanations, and assessing its impact on the quality of the generated dataset and the performance of conversational search models.

### Open Question 3
- Question: How can the dependency between turns in multi-turn interactions be modeled to generate more realistic and coherent conversations?
- Basis in paper: [explicit] The authors mention that their multi-turn interactions do not consider the dependency between turns, and that more reasonable simulations should take this into account.
- Why unresolved: Real conversational search involves context-dependent interactions where later turns build upon previous ones. The current methodology generates independent interactions, which may not capture the nuances of real conversations.
- What evidence would resolve it: Developing and evaluating a methodology for generating multi-turn interactions that considers the context and dependencies between turns, and comparing its effectiveness to independent interactions in improving document ranking and user satisfaction.

## Limitations
- The methodology simplifies conversational interactions to single-turn clarifying questions with binary answers, which may not fully capture the complexity of real user-system interactions
- The approach relies on relevance judgments as a proxy for user intent, which may not capture the full complexity of search goals
- The effectiveness depends heavily on the quality of facet extraction from passages, which may vary across different datasets

## Confidence
- High confidence: The core methodology of augmenting ad-hoc datasets with simulated interactions is sound and addresses a genuine need in conversational IR research
- Medium confidence: The quality of generated interactions based on human evaluation metrics, as human judgment introduces variability
- Medium confidence: The downstream IR performance improvements, as these depend on the specific ranking model and evaluation setup used

## Next Checks
1. Conduct a larger-scale human evaluation comparing the MiMarco dataset interactions against naturally occurring conversational search data (if available) to validate the realism of generated interactions
2. Test the methodology across multiple ad-hoc IR datasets (beyond MsMarco) to assess generalizability and identify any dataset-specific biases in the augmentation process
3. Evaluate the neural ranking model's performance when incorporating interactions generated with different facet extraction methods to determine sensitivity to this critical component