---
ver: rpa2
title: Learning to Act without Actions
arxiv_id: '2312.10812'
source_url: https://arxiv.org/abs/2312.10812
tags:
- latent
- action
- learning
- policy
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Latent Action Policies from Observation (LAPO),
  a method for learning latent action policies purely from observational data without
  action labels. The key idea is to train an inverse dynamics model (IDM) and a forward
  dynamics model (FDM) jointly using a predictive consistency loss, where the IDM
  learns to predict latent actions that explain observed transitions and the FDM learns
  to predict future observations given past observations and latent actions.
---

# Learning to Act without Actions

## Quick Facts
- arXiv ID: 2312.10812
- Source URL: https://arxiv.org/abs/2312.10812
- Reference count: 33
- One-line primary result: Latent action policies learned purely from observational data can be rapidly fine-tuned to recover expert-level performance in Procgen benchmark environments.

## Executive Summary
This paper introduces Latent Action Policies from Observation (LAPO), a method for learning latent action policies purely from observational data without action labels. The key innovation is a joint training framework where an inverse dynamics model (IDM) and forward dynamics model (FDM) learn to predict latent actions that explain observed transitions. Vector quantization is applied to encourage state-invariant representations. Experiments show that LAPO can learn interpretable latent action spaces that closely correspond to true action spaces, and the resulting policies can be rapidly fine-tuned to recover expert-level performance, significantly outperforming training from scratch.

## Method Summary
LAPO trains an inverse dynamics model and forward dynamics model jointly using a predictive consistency loss. The IDM predicts latent actions from sequences of observations, while the FDM predicts future observations given past observations and latent actions. Vector quantization is applied to the latent actions to encourage a state-invariant representation. The learned latent action space is then used to train a policy through behavior cloning, which can be fine-tuned using reinforcement learning or mapped to true actions using a small labeled dataset.

## Key Results
- LAPO achieves expert-level performance after fine-tuning with only 4M frames, while PPO from scratch reaches only 44% of expert performance in the same period
- The learned latent action spaces exhibit clusters that closely align with the true action space across Procgen benchmark environments
- A small action-labeled dataset (1% of full dataset) can be used to train a decoder that maps latent actions to true actions, achieving good performance without environment interaction

## Why This Works (Mechanism)

### Mechanism 1
The predictive consistency loss between IDM and FDM forces the IDM to learn a compressed representation of the action-relevant differences between consecutive observations. The IDM sees both ot and ot+1, while the FDM only sees ot and the latent action zt. To minimize the prediction error, the IDM must encode the transition information into zt in a way that the FDM can use to reconstruct ot+1. Vector quantization further constrains this encoding to reuse a limited set of latents across different states. Core assumption: The environment is sufficiently deterministic and the agent's actions are the primary source of change between consecutive observations.

### Mechanism 2
The learned latent action space captures the structure of the true action space because actions that produce similar state transitions map to nearby latents. Actions that produce similar observable effects will result in similar IDM predictions, causing them to map to similar latents. VQ clusters these similar latents, creating a discrete structure that mirrors the true action space. Core assumption: Actions have distinguishable and consistent effects on the observation space across different states.

### Mechanism 3
Behavior cloning on the latent action space produces a policy that can be rapidly fine-tuned to the true action space. The latent policy learns to imitate the expert's behavior in the latent space. Since the latent space captures the action structure, fine-tuning only the last layers to map from the true observation space to the latent space is sufficient to recover expert performance. Core assumption: The latent action space learned by LAPO preserves the behavioral information necessary for task completion.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: The paper operates in a setting where the agent only observes partial information about the environment state, which is fundamental to understanding the challenges of learning from observation-only data.
  - Quick check question: What is the difference between a Markov Decision Process (MDP) and a POMDP, and why does POMDP formulation matter for observation-only learning?

- Concept: Inverse and Forward Dynamics Models
  - Why needed here: LAPO relies on learning both an inverse dynamics model (predicting actions from state transitions) and a forward dynamics model (predicting next state from current state and action) to establish the predictive consistency loss.
  - Quick check question: How do inverse and forward dynamics models differ in their inputs and outputs, and why are both needed for LAPO?

- Concept: Vector Quantization (VQ)
  - Why needed here: VQ is applied to the continuous latent actions to encourage a state-invariant representation by forcing the model to reuse a limited set of discrete latents across different states.
  - Quick check question: What is the purpose of vector quantization in LAPO, and how does it help create a state-invariant latent action representation?

## Architecture Onboarding

- Component map: IDM (Inverse Dynamics Model) → VQ Layer → FDM (Forward Dynamics Model) → Loss → Gradient Update
- Critical path: IDM generates latent actions that are quantized and passed to the FDM, which predicts the next observation. The loss between predicted and actual next observation drives learning.
- Design tradeoffs:
  - Continuous vs discrete latent actions: Continuous allows more flexibility but may not capture discrete action structure; discrete is more interpretable but may miss nuances
  - VQ commitment cost: Higher values encourage more stable embeddings but may slow adaptation
  - Context window size k: Larger values provide more information but increase computational cost
- Failure signatures:
  - FDM loss plateaus at high value: IDM may not be providing useful information or environment is too stochastic
  - Latent space lacks structure: VQ may not be effective or actions have context-dependent effects
  - Fine-tuning fails: Latent space may not capture necessary behavioral information
- First 3 experiments:
  1. Train IDM and FDM on a simple environment (e.g., CartPole) and visualize the latent action space structure
  2. Test behavior cloning on the learned latent actions and evaluate performance on the same environment
  3. Fine-tune the latent policy on a slightly modified version of the training environment to test transfer capability

## Open Questions the Paper Calls Out
No open questions were explicitly called out in the paper.

## Limitations
- The method's reliance on deterministic environments and the assumption that actions produce consistent state transitions may not hold in more complex, stochastic domains
- Experiments focus on Atari-style games with relatively simple action spaces, leaving uncertainty about scalability to continuous control tasks
- The paper's assertion that LAPO can learn meaningful latent policies from "vast amounts of action-free data" is not fully substantiated, as experiments use a single fixed dataset size (8M frames)

## Confidence
**High Confidence**: The predictive consistency mechanism between IDM and FDM is well-grounded, with clear mathematical formulation and intuitive explanation. The experimental results showing superior fine-tuning performance compared to training from scratch are robust across multiple Procgen games.

**Medium Confidence**: The interpretability of learned latent action spaces is demonstrated qualitatively through visualizations, but quantitative metrics for evaluating the correspondence between latent and true action spaces are lacking. The claim that VQ creates state-invariant representations needs more rigorous validation across different environmental conditions.

**Low Confidence**: The paper's assertion that LAPO can learn meaningful latent policies from "vast amounts of action-free data" is not fully substantiated, as experiments use a single fixed dataset size (8M frames) without exploring scaling properties or data efficiency.

## Next Checks
1. Test LAPO on environments with stochastic transitions to evaluate robustness when the core assumption of deterministic state changes breaks down.

2. Conduct ablation studies on the VQ layer by comparing continuous vs discrete latent spaces and measuring the impact on both performance and interpretability.

3. Evaluate transfer learning capabilities by fine-tuning policies trained on one environment to perform in semantically similar but visually distinct environments.