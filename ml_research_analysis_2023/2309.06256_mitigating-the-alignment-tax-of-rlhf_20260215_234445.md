---
ver: rpa2
title: Mitigating the Alignment Tax of RLHF
arxiv_id: '2309.06256'
source_url: https://arxiv.org/abs/2309.06256
tags:
- fine-tuning
- arxiv
- learning
- forgetting
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates catastrophic forgetting in large language
  models during fine-tuning, where models lose abilities acquired during pre-training.
  Existing methods to mitigate forgetting often trade off alignment performance with
  forgetting mitigation.
---

# Mitigating the Alignment Tax of RLHF

## Quick Facts
- arXiv ID: 2309.06256
- Source URL: https://arxiv.org/abs/2309.06256
- Reference count: 40
- Key outcome: Heterogeneous Model Averaging (HMA) achieves the strongest alignment-forgetting Pareto front by combining different layers of pre- and post-fine-tuning models

## Executive Summary
This paper addresses catastrophic forgetting in large language models during RLHF fine-tuning, where models lose abilities acquired during pre-training. The authors propose Heterogeneous Model Averaging (HMA), which heterogeneously combines different layers of pre- and post-fine-tuning models to maximize alignment performance while minimizing forgetting. HMA outperforms existing methods across various RLHF algorithms and model sizes, achieving the strongest alignment-forgetting Pareto front. The method is validated on OpenLLaMA-3B and extended to Mistral-7B using open-source preference models and GPT4.

## Method Summary
The Heterogeneous Model Averaging (HMA) method works by combining different layers of pre-trained and fine-tuned models to create a continuum of models that balance task performance with preserved generality. The approach builds on model averaging by introducing heterogeneous layer-wise interpolation, where different layers can be combined with different ratios. The method searches for optimal layer combinations to maximize alignment performance while minimizing catastrophic forgetting, creating a Pareto-optimal frontier between these competing objectives.

## Key Results
- HMA outperforms existing methods including L1/L2/KD penalties, LoRA, and Wise-FT across various RLHF algorithms
- The method achieves the strongest alignment-forgetting Pareto front among competing methods
- Validated on OpenLLaMA-3B and extended to Mistral-7B using open-source preference models and GPT4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heterogeneous model averaging preserves pre-trained knowledge by combining different layers of the model.
- Mechanism: Different transformer layers capture different types of information. Low-level layers contain more general features while higher layers contain more task-specific features. By averaging different combinations of layers from pre- and post-fine-tuning models, HMA can selectively preserve general capabilities while incorporating task-specific improvements.
- Core assumption: Different transformer layers have distinct roles in representing information, and these roles can be exploited through layer-wise interpolation.
- Evidence anchors:
  - [abstract] "Building on the analysis and the observation that averaging different layers of the transformer leads to significantly different alignment-forgetting trade-offs, we propose Heterogeneous Model Averaging (HMA) to Heterogeneously find various combination ratios of model layers."
  - [section] "Empirical evidence corroborates our analysis by showing the benefits of averaging low-level transformer layers."

### Mechanism 2
- Claim: Model averaging creates a Pareto-optimal frontier between alignment performance and forgetting mitigation.
- Mechanism: Linear interpolation between pre-trained (θ0) and fine-tuned (θ) parameters creates a continuum of models that balance task performance with preserved generality. The interpolation coefficient α controls the trade-off.
- Core assumption: The optimal model lies on a continuum between the pre-trained and fine-tuned extremes rather than at either endpoint.
- Evidence anchors:
  - [abstract] "model averaging, which simply interpolates between pre and post RLHF model weights, surprisingly achieves the most strongest alignment-forgetting Pareto front among a wide range of competing methods."
  - [section] "Our findings show that both continual learning and Wise-ft methods effectively mitigate catastrophic forgetting and preserve generality during fine-tuning. Among them, Wise-ft achieves the best performance on speciality-generality trade-off."

### Mechanism 3
- Claim: Layer-specific averaging exploits the modular structure of transformer architectures to selectively preserve capabilities.
- Mechanism: By averaging specific layers (e.g., low-level layers) while keeping others unchanged, HMA can preserve general feature representations while allowing task-specific adaptations in higher layers.
- Core assumption: Transformer architectures have modular properties where different layers can be modified independently without breaking overall functionality.
- Evidence anchors:
  - [abstract] "we offer theoretical insights into model averaging, revealing that it enhances performance Pareto front by increasing feature diversity on the layers where tasks share overlapped feature spaces."
  - [section] "Building on the analysis and the observation that averaging different layers of the transformer leads to significantly different alignment-forgetting trade-offs, we propose Heterogeneous Model Averaging (HMA) to Heterogeneously find various combination ratios of model layers."

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why fine-tuning causes loss of pre-trained abilities is central to the problem being solved
  - Quick check question: What happens to a neural network's performance on previous tasks when it's trained on new tasks without any regularization?

- Concept: Pareto optimality and trade-off frontiers
  - Why needed here: The paper explicitly seeks to find models that simultaneously optimize two competing objectives (alignment performance and forgetting mitigation)
  - Quick check question: If method A performs better than method B on both objectives, can method B be on the Pareto frontier?

- Concept: Transformer architecture and layer functions
  - Why needed here: The proposed solution exploits the layered structure of transformers by averaging specific layers
  - Quick check question: What is the typical function of lower vs. higher layers in transformer models?

## Architecture Onboarding

- Component map:
  Pre-trained model weights (θ0) -> Fine-tuned model weights (θ) -> Layer-wise interpolation mechanism -> Search algorithm for optimal layer combinations -> Evaluation framework for alignment-forgetting trade-off

- Critical path:
  1. Load pre-trained and fine-tuned models
  2. Extract layer weights from both models
  3. Apply heterogeneous averaging to create candidate models
  4. Evaluate each candidate on alignment and forgetting metrics
  5. Select optimal combination based on Pareto front

- Design tradeoffs:
  - Computational cost vs. granularity of layer-wise averaging
  - Number of candidate combinations vs. search efficiency
  - Preservation of pre-trained knowledge vs. adaptation to new task

- Failure signatures:
  - Degradation in both alignment performance and forgetting mitigation (over-regularization)
  - Improved alignment but severe forgetting (under-regularization)
  - Numerical instability in layer-wise interpolation
  - Loss of coherence in model outputs

- First 3 experiments:
  1. Implement basic model averaging (Wise-ft) with single α parameter and validate against baselines
  2. Add layer-wise granularity by averaging specific layer groups and observe impact on trade-off
  3. Implement heterogeneous search over different layer combinations and validate Pareto optimality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of LoRA in mitigating forgetting depend on the fine-tuning task's difficulty, and if so, what specific characteristics of the task make it more or less amenable to LoRA?
- Basis in paper: [explicit] The authors note that LoRA's performance varies significantly depending on whether it is fine-tuned on MedMCQA or PubMedQA, with better results on PubMedQA. They speculate that PubMedQA might possess a better low-rank structure, making it easier for LoRA to adapt to the fine-tuning task.
- Why unresolved: While the authors observe a correlation between task difficulty and LoRA's effectiveness, they do not provide a definitive explanation for why certain tasks are more suitable for LoRA. Further investigation is needed to identify the specific characteristics that influence LoRA's performance.
- What evidence would resolve it: Empirical studies comparing LoRA's performance across a wide range of fine-tuning tasks with varying levels of difficulty, analyzing the low-rank structure of the tasks, and identifying common features that contribute to LoRA's success or failure.

### Open Question 2
- Question: How does the size of the foundation model impact the severity of catastrophic forgetting and the effectiveness of mitigation methods?
- Basis in paper: [inferred] The authors mention that one limitation of their work is not exploring the impact of varying model sizes on the forgetting issue and corresponding methods. This suggests that model size could be a relevant factor.
- Why unresolved: The authors did not investigate how different model sizes affect catastrophic forgetting and the performance of mitigation methods. It remains unclear whether larger models experience more or less forgetting and whether the effectiveness of methods like LoRA and Wise-FT scales with model size.
- What evidence would resolve it: Experiments comparing the performance of foundation models of different sizes on various fine-tuning tasks, analyzing the relationship between model size and forgetting, and evaluating the effectiveness of mitigation methods across different model scales.

### Open Question 3
- Question: Can the heterogeneous model averaging approach (HMA) be further improved by incorporating task-specific layer combinations or adaptive layer selection strategies?
- Basis in paper: [explicit] The authors propose HMA as a method to heterogeneously combine different layers of pre- and post-fine-tuning models to maximize alignment performance while minimizing forgetting. However, they do not explore task-specific or adaptive layer selection strategies.
- Why unresolved: While HMA shows promise in mitigating forgetting, the authors did not investigate whether tailoring the layer combinations to specific tasks or using adaptive strategies to select layers could further enhance its effectiveness. It remains an open question whether such refinements could lead to better performance.
- What evidence would resolve it: Experiments comparing the performance of HMA with task-specific layer combinations and adaptive layer selection strategies against the original HMA approach, analyzing the impact of different layer selection methods on forgetting mitigation and alignment performance.

## Limitations

- The theoretical justification for why different layers should be combined differently is limited and could benefit from more rigorous analysis
- The experimental validation focuses primarily on alignment-forgetting trade-offs without extensively examining other potential side effects such as changes in model coherence or reasoning capabilities
- The generalizability of the approach to other model architectures beyond transformers and other types of fine-tuning beyond RLHF is not thoroughly explored

## Confidence

**High Confidence**: The empirical observation that model averaging (both homogeneous and heterogeneous) improves the alignment-forgetting Pareto front. The experimental results are well-documented and reproducible across multiple baselines and model sizes.

**Medium Confidence**: The theoretical mechanism explaining why layer-wise averaging works - specifically the claim that different transformer layers capture distinct types of information that can be selectively preserved. While plausible, this mechanism could benefit from more rigorous analysis of layer representations.

**Medium Confidence**: The generalizability of the approach to other model architectures beyond transformers and other types of fine-tuning beyond RLHF. The paper provides limited discussion of these extensions.

## Next Checks

1. **Architectural Dependency Analysis**: Systematically test HMA on different transformer variants (DeBERTa, GPT-NeoX, etc.) and non-transformer architectures to determine whether the layer-wise averaging benefits are architecture-specific or more general.

2. **Extended Capability Preservation**: Evaluate the fine-tuned models on a broader suite of capabilities beyond the reported tasks, including logical reasoning, mathematical problem-solving, and creative generation to identify any unintended degradation in capabilities not captured by the current evaluation framework.

3. **Dynamic Layer Weighting**: Implement and test methods that dynamically adjust layer combination weights during fine-tuning rather than using static ratios, to determine whether the optimal layer combinations are fixed or context-dependent.