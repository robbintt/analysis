---
ver: rpa2
title: 'GRANDE: Gradient-Based Decision Tree Ensembles for Tabular Data'
arxiv_id: '2309.17130'
source_url: https://arxiv.org/abs/2309.17130
tags:
- grande
- data
- learning
- methods
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRANDE combines gradient-based optimization with decision tree
  ensembles for tabular data. It learns hard, axis-aligned decision trees using a
  dense representation and straight-through gradient propagation.
---

# GRANDE: Gradient-Based Decision Tree Ensembles for Tabular Data

## Quick Facts
- arXiv ID: 2309.17130
- Source URL: https://arxiv.org/abs/2309.17130
- Reference count: 21
- Key outcome: GRANDE achieves mean F1-score of 0.807 on 19 classification datasets, outperforming XGBoost (0.792) and NODE (0.775)

## Executive Summary
GRANDE introduces a gradient-based approach to decision tree ensembles that learns all parameters end-to-end using backpropagation. Unlike traditional gradient boosting which builds trees sequentially, GRANDE uses a dense matrix representation of the entire ensemble, enabling parallel computation and joint optimization of all tree parameters. The method introduces instance-wise weighting where each leaf has its own weight, allowing simple rules to specialize on easily classified instances while complex rules handle difficult cases. This architecture achieves state-of-the-art performance on tabular classification tasks while maintaining the interpretability benefits of decision trees.

## Method Summary
GRANDE learns hard, axis-aligned decision trees using gradient descent by representing trees as arithmetic functions with dense matrix/tensor parameters. The method uses softsign activation for splits, straight-through operators for non-differentiable operations like hardmax and rounding, and instance-wise weighting where each leaf has its own weight. During training, samples are weighted by the softmax of the leaf weights they fall into, allowing different trees to specialize on different data subsets. The model is optimized using Adam with cosine decay learning rate, stochastic weight averaging, and various regularization techniques including feature subset selection, data subset selection, and dropout.

## Key Results
- Achieves mean F1-score of 0.807 across 19 classification datasets, outperforming XGBoost (0.792) and NODE (0.775)
- Instance-wise weighting significantly improves performance and enables local interpretability by allowing simple rules to handle easily classified instances
- Softsign split function provides superior gradients compared to sigmoid and entmoid alternatives
- Computationally efficient for large and high-dimensional datasets with lower runtime than NODE

## Why This Works (Mechanism)

### Mechanism 1
Gradient-based optimization allows end-to-end learning of tree ensembles by formulating trees as arithmetic functions with dense representations. The straight-through operator bypasses non-differentiable operations during backpropagation while maintaining hard splits in the forward pass, enabling joint optimization of all parameters.

### Mechanism 2
Instance-wise weighting allows each tree to specialize on different subsets of data by learning weights per leaf rather than per estimator. This enables simple rules to handle easily classified instances while complex rules tackle difficult ones, improving both performance and interpretability.

### Mechanism 3
Softsign activation provides more informative gradients than sigmoid or entmoid during optimization, maintaining responsive gradients for large differences between feature values and thresholds while having pronounced gradients when samples are close to the threshold.

## Foundational Learning

- Concept: Dense representation of decision trees using matrix/tensor operations
  - Why needed here: Enables efficient parallel computation of the entire ensemble during training, replacing the traditional nested tree structure with arithmetic operations
  - Quick check question: How does the dense representation convert a tree's feature index vector into a matrix form, and why is this beneficial for gradient computation?

- Concept: Straight-through gradient estimation for non-differentiable operations
  - Why needed here: Decision trees require hard splits (Heaviside function), which are non-differentiable, but GRANDE needs gradients for optimization
  - Quick check question: What specific operations use the straight-through operator in GRANDE, and how does this affect the forward vs backward pass?

- Concept: Instance-wise weighting vs global ensemble weighting
  - Why needed here: Traditional ensemble methods use a single weight per estimator or equal weighting, but GRANDE needs each tree to specialize on different data subsets
  - Quick check question: How does GRANDE's instance-wise weighting scheme differ mathematically from standard ensemble weighting approaches?

## Architecture Onboarding

- Component map:
  - Input preprocessing: Gaussianization via quantile transformation, one-hot encoding for low-cardinality categorical features, leave-one-out encoding for high-cardinality categorical features
  - Tree representation: Dense matrices/tensors for split thresholds (T) and feature indices (I), leaf weight matrices (L)
  - Split function: Softsign activation with straight-through operator
  - Weighting: Instance-wise leaf weights with softmax normalization
  - Optimization: Adam optimizer with cosine decay learning rate schedule, focal factor option, stochastic weight averaging
  - Regularization: Feature subset selection per tree, data subset selection per tree, dropout

- Critical path:
  1. Preprocess input features (Gaussianization + encoding)
  2. Initialize tree parameters (thresholds, feature indices, leaf weights)
  3. For each batch: compute predictions using dense tree representation
  4. Apply instance-wise weighting to combine predictions
  5. Compute loss and backpropagate gradients
  6. Update parameters using Adam optimizer
  7. Apply regularization (feature/data subset selection, dropout)
  8. Early stopping when validation performance plateaus

- Design tradeoffs:
  - Hard splits vs soft splits: Hard splits provide axis-aligned inductive bias but require straight-through operators; soft splits are differentiable but may learn overly smooth functions
  - Instance-wise vs global weighting: Instance-wise allows specialization but increases parameter count; global weighting is simpler but forces homogeneity
  - Dense representation vs sparse: Dense enables parallel computation but uses more memory; sparse is memory-efficient but sequential

- Failure signatures:
  - Training loss plateaus early: Likely issues with gradient flow through straight-through operators or inappropriate learning rate
  - All trees learn similar representations: Instance-wise weighting not functioning properly or regularization too weak
  - Poor performance on small datasets: Model complexity too high relative to data size, try reducing depth or number of estimators

- First 3 experiments:
  1. Compare softsign vs sigmoid vs entmoid split functions on a small dataset to verify gradient behavior advantage
  2. Test instance-wise weighting vs global weighting on a simple binary classification task to confirm specialization benefit
  3. Validate dense representation computation by comparing predictions with a traditional tree implementation on a small ensemble

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GRANDE's performance scale with increasing dataset size and dimensionality compared to traditional gradient boosting methods?
- Basis in paper: The paper mentions that GRANDE is computationally efficient for large and high-dimensional datasets, achieving significantly lower runtime compared to NODE, but notes that GBDT frameworks like XGBoost are highly efficient on GPU and achieve significantly lower runtimes.
- Why unresolved: While the paper provides runtime comparisons for a set of datasets, it does not systematically explore the scaling behavior of GRANDE with respect to dataset size and dimensionality, nor does it provide GPU performance comparisons.
- What evidence would resolve it: Systematic experiments varying dataset size and dimensionality, along with GPU performance benchmarks comparing GRANDE to traditional GBDT methods.

### Open Question 2
- Question: What is the impact of different tree depths and ensemble sizes on GRANDE's performance and interpretability?
- Basis in paper: The paper uses a fixed tree depth of 4-6 and ensemble sizes up to 2048 estimators, but does not systematically explore the impact of these hyperparameters on performance and interpretability.
- Why unresolved: The ablation study focuses on different split functions and weighting schemes, but does not investigate how varying tree depths and ensemble sizes affects GRANDE's ability to learn simple rules and its overall performance.
- What evidence would resolve it: Systematic experiments varying tree depths and ensemble sizes, along with interpretability analysis of the learned rules at different configurations.

### Open Question 3
- Question: How does GRANDE's instance-wise weighting scheme affect its ability to handle noisy and imbalanced datasets?
- Basis in paper: The paper mentions that GRANDE can handle class imbalance by including class weights, but does not specifically investigate how the instance-wise weighting scheme impacts performance on noisy and imbalanced datasets.
- Why unresolved: While the paper demonstrates GRANDE's performance on a benchmark with class imbalance, it does not isolate the effect of the instance-wise weighting scheme on handling noise and imbalance.
- What evidence would resolve it: Experiments comparing GRANDE's performance with and without instance-wise weighting on datasets with varying levels of noise and class imbalance.

## Limitations

- The evaluation relies on 19 classification datasets from OpenML, which may not fully represent the diversity of tabular data challenges in real-world applications
- The comparison against XGBoost and NODE uses default implementations, but hyperparameters were optimized via Optuna, potentially creating an unfair advantage
- The straight-through gradient approximation for hardmax and rounding operations may not provide optimal gradient flow in practice, particularly for deep ensembles

## Confidence

**High Confidence**: The dense representation approach for tree ensembles is mathematically sound and the experimental methodology (5-fold cross-validation on 19 datasets with standardized preprocessing) is rigorous. The improvement over traditional gradient boosting methods is consistently demonstrated.

**Medium Confidence**: The claimed advantages of softsign over sigmoid/entmoid split functions are supported by theoretical arguments about gradient behavior, but the practical significance in final model performance needs more extensive validation across diverse problem types.

**Low Confidence**: The specific mechanism by which instance-wise weighting enables both improved performance and enhanced interpretability is plausible but not definitively proven. The paper shows correlation between weighting and performance but doesn't isolate the interpretability benefits from the performance gains.

## Next Checks

1. **Gradient Flow Analysis**: Instrument the training process to measure gradient magnitudes through the straight-through operators for hardmax and rounding operations. Track whether gradients vanish or explode as ensemble depth increases, particularly comparing softsign vs sigmoid.

2. **Ablation Study on Weighting Scheme**: Train GRANDE with global weighting (one weight per estimator), instance-wise weighting with random weights, and instance-wise weighting with learned weights. Compare not just final performance but also the learned tree specializations to isolate the contribution of the weighting mechanism.

3. **Scalability Testing**: Evaluate GRANDE on datasets with varying numbers of features (10 to 1000+) and samples (1000 to 1,000,000) to identify the practical limits of the dense representation approach. Measure memory usage and training time scaling to determine when the approach becomes impractical.