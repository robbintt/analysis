---
ver: rpa2
title: 'Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to
  Answers Faster'
arxiv_id: '2311.08263'
source_url: https://arxiv.org/abs/2311.08263
tags:
- decoding
- tokens
- approximate
- parallel
- window
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FastCoT is a model-agnostic framework that improves the efficiency
  of Chain-of-Thought (CoT) reasoning tasks by utilizing parallel decoding. It combines
  exact tokens obtained through autoregressive decoding with approximate tokens from
  parallel decoding to provide a glimpse of the future to the language model, leading
  to faster answers.
---

# Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers Faster

## Quick Facts
- arXiv ID: 2311.08263
- Source URL: https://arxiv.org/abs/2311.08263
- Authors: 
- Reference count: 6
- Key outcome: Up to 20% reduction in inference time for Chain-of-Thought reasoning tasks with negligible performance drop

## Executive Summary
FastCoT is a model-agnostic framework that improves the efficiency of Chain-of-Thought (CoT) reasoning tasks by utilizing parallel decoding. The framework combines exact tokens from autoregressive decoding with approximate tokens from parallel decoding to provide a glimpse of the future to the language model, leading to faster answers. Extensive experiments demonstrate that FastCoT achieves significant speedup while maintaining comparable performance to traditional autoregressive decoding methods.

## Method Summary
FastCoT integrates exact tokens obtained through autoregressive decoding with approximate tokens generated via parallel decoding within a sliding context window. The framework uses an Answer Probe mechanism that collects attention weights to determine when sufficient reasoning content has been generated, enabling early termination of the iteration process. The method supports batch processing and KV-Cache to further enhance efficiency, and the approximate tokens generated through parallel decoding exhibit high quality, containing important information for the reasoning task.

## Key Results
- Up to 20% reduction in inference time compared to regular autoregressive decoding
- Only negligible drop in task performance across commonsense reasoning datasets
- Significant speedup improvement while maintaining comparable answer quality
- Parallel decoding approximate tokens contain important information for reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Approximate tokens from parallel decoding provide useful future context that helps the model generate correct answers faster than autoregressive decoding alone.
- Mechanism: The model first generates a mix of exact tokens via autoregressive decoding and approximate tokens via parallel decoding within a sliding context window. The approximate tokens, while not exact, often contain keywords or reasoning steps that are semantically similar to the correct future tokens. This "glimpse of the future" allows the model to jump ahead in reasoning without generating every intermediate step exactly.
- Core assumption: A partial or approximate rationale containing key information is sufficient for the model to reach correct answers, especially in commonsense reasoning tasks.
- Evidence anchors:
  - [abstract] "approximate tokens generated through parallel decoding exhibit high quality, containing important information for the reasoning task"
  - [section] "We observed that even a randomly sampled oracle rationale can lead to correct answers, indicating that the LLMs are able to make accurate predictions without having access to the complete rationale"
  - [corpus] Weak. Corpus contains parallel decoding papers but none directly test approximate tokens in reasoning tasks.
- Break condition: If the reasoning task requires exact step-by-step computation (e.g., mathematical reasoning), approximate tokens may introduce errors that lead to incorrect answers.

### Mechanism 2
- Claim: Parallel decoding reduces the number of forward passes needed to generate a full rationale by guessing multiple future tokens at once.
- Mechanism: Instead of generating one token at a time autoregressively, parallel decoding predicts a block of future tokens in a single forward pass using the causal attention mask. This reduces iteration count, and when combined with KV-cache, amortizes the cost of computing attention weights over multiple tokens.
- Core assumption: The marginal cost of computing logits for additional tokens in the context window is low compared to the cost of an extra autoregressive iteration.
- Evidence anchors:
  - [section] "parallel decoding takes into account additional tokens after the generating position, resulting in a corresponding increase in the number of outputs"
  - [section] "We conducted a single-inference task to measure the time overhead for a single forward inference under different window sizes... there is almost no change in time consumption from autoregressive decoding to parallel decoding with a window sizes up to a certain threshold"
  - [corpus] Moderate. The corpus contains parallel decoding papers but focuses on translation; the claim about iteration reduction in reasoning is not directly supported.
- Break condition: If the context window is too large, GPU parallelism is underutilized and time overhead increases, negating the benefit.

### Mechanism 3
- Claim: The attention weights collected from the Answer Probe can reliably detect when the model has generated enough information to answer the question, allowing early termination.
- Mechanism: After each parallel decoding iteration, the model inspects attention scores from a special "Answer Probe" token. If high attention is paid to tokens containing answer-relevant content, the iteration stops early, skipping unnecessary decoding steps.
- Core assumption: The attention distribution in the model's last layers reliably signals when sufficient reasoning content has been generated.
- Evidence anchors:
  - [section] "This probe collects attention weights for the approximate tokens within the Context Window during the forward inference... these attention weight data exhibit distinct characteristics that can help determine whether it is appropriate to end the iteration"
  - [section] "We have designed three different iteration termination conditions... Score based on the attention weight collected by the Answer Probe"
  - [corpus] Weak. No corpus evidence for attention-based stopping in reasoning tasks; this is a novel contribution.
- Break condition: If the attention signal is noisy or the task requires deeper reasoning, the probe may trigger too early or too late.

## Foundational Learning

- Concept: Autoregressive decoding
  - Why needed here: FastCoT still relies on autoregressive decoding for the initial exact tokens; understanding the difference between AR and parallel decoding is key to grasping the speedup mechanism.
  - Quick check question: In autoregressive decoding, how many tokens are generated per forward pass? (Answer: one)

- Concept: KV-cache and attention efficiency
  - Why needed here: FastCoT uses KV-cache to avoid recomputing attention for past tokens across iterations; this is critical for the claimed speedup.
  - Quick check question: What is the time complexity of attention computation with and without KV-cache? (Answer: O(n²) vs O(n))

- Concept: Context window and causal masking
  - Why needed here: Parallel decoding relies on a sliding context window with causal masking to generate multiple tokens at once; understanding this is essential to tune window size.
  - Quick check question: What is the role of the causal mask in parallel decoding? (Answer: Prevents attending to future tokens within the same window)

## Architecture Onboarding

- Component map:
  - Input prompt (CoT + question) -> Exact token buffer (autoregressive output) -> Approximate token buffer (parallel decoding output) -> Context window manager (sliding window size control) -> Answer probe (attention-based stopping) -> Final answer generator (LLM with combined context + answer trigger)

- Critical path:
  1. Start with exact token buffer containing prompt and any initial exact tokens
  2. In each iteration: perform parallel decoding to fill context window with approximate tokens
  3. Verify which tokens match future autoregressive output (exact vs approximate)
  4. Update buffers and window
  5. Check Answer Probe attention; if high score, stop
  6. Feed combined exact + approximate + trigger to LLM for final answer

- Design tradeoffs:
  - Larger context window → more parallel tokens but higher GPU cost and lower token quality at window end
  - Smaller window → fewer skipped iterations but more passes needed
  - Answer Probe sensitivity → early stopping vs. missing needed reasoning steps

- Failure signatures:
  - Too many incorrect answers → approximate tokens too noisy; reduce window size
  - No speedup observed → context window too small or GPU underutilized; increase window size within memory limits
  - GPU OOM → context window too large; reduce batch size or window size

- First 3 experiments:
  1. Run FastCoT on a small commonsense dataset (e.g., CSQA) with context window size 5, 10, 15; measure speedup and accuracy vs AR.
  2. Test Answer Probe sensitivity by varying the attention threshold; observe impact on iteration count and accuracy.
  3. Benchmark KV-cache integration by comparing memory usage and time with/without KV-cache on batch size 4 and 8.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal Context Window size for different reasoning tasks?
- Basis in paper: [inferred] The paper mentions that the choice of Context Window size is pre-defined and affected by the GPU and language model, and that too large or too small a window can lead to inefficiency.
- Why unresolved: The paper suggests that the process of controlling the Context Window size can be seen as a Markov Decision Process, but does not provide a definitive answer on the optimal size for different tasks.
- What evidence would resolve it: Experiments testing different Context Window sizes on a variety of reasoning tasks to determine the optimal size for each task.

### Open Question 2
- Question: How can the quality of approximate tokens be improved for mathematical reasoning tasks?
- Basis in paper: [explicit] The paper states that approximate rationales are more suitable for commonsense reasoning tasks rather than mathematical reasoning tasks, and that incorrect data in the Approximate Tokens can lead to incorrect final answers.
- Why unresolved: The paper does not provide a solution for improving the quality of approximate tokens specifically for mathematical reasoning tasks.
- What evidence would resolve it: Experiments testing different methods for generating approximate tokens and their impact on the performance of mathematical reasoning tasks.

### Open Question 3
- Question: Can reinforcement learning algorithms be used to regulate the Context Window size during the iteration process?
- Basis in paper: [explicit] The paper suggests that the process of controlling the Context Window size throughout the iteration can be seen as a Markov Decision Process and that using reinforcement learning algorithms to regulate the Context Window size could be an intriguing problem.
- Why unresolved: The paper does not explore this possibility and leaves it as an open question.
- What evidence would resolve it: Experiments testing the use of reinforcement learning algorithms to regulate the Context Window size and their impact on the efficiency and performance of the reasoning task.

## Limitations

- The effectiveness of FastCoT varies significantly between commonsense and mathematical reasoning tasks
- Approximate tokens may introduce errors in tasks requiring exact step-by-step computation
- The Answer Probe mechanism adds computational overhead that needs careful cost-benefit analysis

## Confidence

**High Confidence Claims:**
- FastCoT reduces inference time compared to standard autoregressive decoding on commonsense reasoning tasks
- The framework is compatible with KV-Cache and batch processing
- Approximate tokens contain some relevant information for reasoning tasks

**Medium Confidence Claims:**
- Up to 20% speedup is achievable with FastCoT
- The Answer Probe mechanism reliably detects when sufficient reasoning content has been generated
- Context window size can be tuned to optimize the tradeoff between quality and speed

**Low Confidence Claims:**
- FastCoT's performance on mathematical reasoning tasks will be comparable to commonsense tasks
- The speedup scales linearly with larger context windows
- The framework's effectiveness generalizes to other LLM architectures beyond Llama2-13B

## Next Checks

1. **Cross-dataset generalization test**: Apply FastCoT to three additional reasoning datasets (including at least one mathematical reasoning dataset and one multi-hop reasoning dataset) with varying context window sizes. Measure both speedup and accuracy drop across all datasets to quantify the framework's generalization capability and identify task-specific limitations.

2. **Ablation study on Answer Probe**: Implement versions of FastCoT with and without the Answer Probe mechanism. Compare iteration counts, total inference time, and answer accuracy to isolate the probe's contribution to overall performance. Additionally, test different probe sensitivity thresholds to find optimal stopping criteria.

3. **Resource utilization analysis**: Instrument FastCoT to measure GPU memory usage, FLOPs per iteration, and parallel efficiency at different batch sizes and context window sizes. Compare these metrics against theoretical speedup calculations to identify whether observed improvements come from algorithmic efficiency or hardware utilization effects.