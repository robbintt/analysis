---
ver: rpa2
title: Active Learning for Classifying 2D Grid-Based Level Completability
arxiv_id: '2309.04367'
source_url: https://arxiv.org/abs/2309.04367
tags:
- learning
- active
- levels
- sampling
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies active learning to classify level completability
  in procedurally generated game levels. The authors use uncertainty sampling to select
  the most informative examples for labeling, and compare its performance to random
  sampling and passive learning.
---

# Active Learning for Classifying 2D Grid-Based Level Completability

## Quick Facts
- arXiv ID: 2309.04367
- Source URL: https://arxiv.org/abs/2309.04367
- Reference count: 22
- Key outcome: Active learning achieves higher accuracy with less labeled data than random sampling for classifying level completability in procedurally generated game levels

## Executive Summary
This paper applies active learning to classify the completability of procedurally generated 2D grid-based game levels. The authors use uncertainty sampling (specifically margin sampling) to select the most informative examples for labeling, comparing its performance against random sampling and passive learning approaches. They evaluate their method on three games: Super Mario Bros., Kid Icarus, and a Zelda-like game. Results demonstrate that active learning achieves higher classification accuracy with fewer labeled examples than random sampling, approaching the performance of passive learning while being more data-efficient.

## Method Summary
The study uses pool-based active learning with binary convolutional neural networks to classify level completability. The classifier consists of 3 convolutional layers with max pooling and ReLU activation, followed by fully connected layers with softmax output. For active learning, margin sampling selects examples with the smallest difference between class probabilities (maximum uncertainty). The process starts with 10 pre-training examples, then iteratively queries 400 samples (50 at a time with 50% pool selection), retraining the classifier after each batch. Evaluation uses 5-fold cross-validation with 5 trials per fold, measuring accuracy, false discovery rate (FDR), and false omission rate (FOR).

## Key Results
- Active learning achieves higher accuracy with fewer labeled examples than random sampling across all three games tested
- The active learning approach approaches passive learning performance while requiring less labeled data
- Active learning has higher false discovery rate (unplayable levels classified as playable) than false omission rate (playable levels classified as unplayable)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uncertainty sampling selects the most informative examples for labeling by choosing instances where the classifier is least certain.
- Mechanism: The classifier outputs probability scores for each class. For binary classification, margin sampling selects the sample with the smallest difference between the two class probabilities, indicating maximum uncertainty. This targeted selection focuses labeling effort on examples that will most improve classifier performance.
- Core assumption: The classifier's uncertainty correlates with the information gain from labeling that instance.
- Evidence anchors:
  - [abstract] "Through an active learning approach, we train deep-learning models to classify the completability of generated levels for Super Mario Bros., Kid Icarus, and a Zelda-like game. We compare active learning for querying levels to label with completability against random queries."
  - [section] "When querying for unlabeled samples with margin sampling, this strategy selects the sample with the smallest margin (i.e. difference in certainty between the two classes), since the smaller the decision margin is, the more unsure the decision."

### Mechanism 2
- Claim: Active learning achieves higher accuracy with fewer labeled examples than random sampling by focusing on informative instances.
- Mechanism: The active learning process iteratively selects the most uncertain examples for labeling, updates the classifier with this new information, and repeats. This targeted approach accumulates informative examples faster than random sampling, which may select redundant or uninformative examples.
- Core assumption: The most uncertain examples provide the most information for improving classifier performance.
- Evidence anchors:
  - [abstract] "Our results show using an active learning approach to label levels results in better classifier performance with the same amount of labeled data."
  - [section] "our results demonstrate that with the same number of instances and model hyperparameters, the learner that uses an uncertainty sampling strategy achieves higher accuracy with less labeled data than random sampling."

### Mechanism 3
- Claim: Active learning reduces the overall cost of building a completability classifier by minimizing the number of examples requiring agent testing.
- Mechanism: Agent-based completability testing is computationally expensive and time-consuming. By using a classifier to filter out clearly unplayable levels and focus agent testing only on uncertain or borderline cases, active learning reduces the total number of agent evaluations needed.
- Core assumption: Agent testing is the primary bottleneck in the generate-and-test approach, and classifier filtering can significantly reduce this burden.
- Evidence anchors:
  - [abstract] "Determining the completability of levels generated by procedural generators such as machine learning models can be challenging, as it can involve the use of solver agents that often require a significant amount of time to analyze and solve levels."
  - [section] "Our work shows that using active learning to query labeled data from an oracle results in better efficiency of the learner, hence resulting in more efficient models that are less expensive as they need less labeled data."

## Foundational Learning

- Concept: Pool-based active learning
  - Why needed here: The study uses pool-based sampling where all unlabeled examples are available at the start, and the algorithm iteratively selects the most informative ones for labeling.
  - Quick check question: What is the difference between pool-based and stream-based active learning?

- Concept: Uncertainty sampling and margin sampling
  - Why needed here: The paper specifically uses margin sampling as a representative of uncertainty sampling strategies to select the most uncertain examples for labeling.
  - Quick check question: How does margin sampling differ from least confidence sampling and entropy sampling for binary classification?

- Concept: Convolutional neural networks for grid-based level classification
  - Why needed here: The classifier uses CNN architecture with convolutional layers, max pooling, and fully connected layers to extract features from 2D level representations.
  - Quick check question: Why are CNNs well-suited for processing 2D grid-based game level data?

## Architecture Onboarding

- Component map:
  Binary matrix (level representation) -> CNN (3 conv layers + max pooling + ReLU) -> Fully connected layers (softmax output) -> Active learning loop (uncertainty sampling + oracle labeling + retraining)

- Critical path:
  1. Pre-training with 10 randomly selected examples
  2. Iterative querying (400 samples total, 50 at a time with 50% pool selection)
  3. Classifier retraining after each new labeled example
  4. Performance evaluation after each iteration

- Design tradeoffs:
  - Accuracy vs. labeled data efficiency: Uncertainty sampling achieves higher accuracy with fewer labeled examples but may have higher false discovery rate
  - Exploration vs. exploitation: Selecting only uncertain examples may miss diverse examples; the 50% pool selection introduces randomness
  - Model complexity vs. training time: Deeper CNNs may capture more complex patterns but increase training time

- Failure signatures:
  - High false discovery rate: Unplayable levels classified as playable, requiring agent verification
  - Plateau in accuracy improvement: Classifier may have reached its performance limit with available data
  - Slow convergence: Uncertainty sampling may be selecting redundant examples or the initial model is poorly calibrated

- First 3 experiments:
  1. Compare uncertainty sampling vs. random sampling on a small subset (first 50 queries) to observe early performance differences
  2. Test different uncertainty sampling strategies (least confidence, margin, entropy) to verify they perform similarly for binary classification
  3. Evaluate the impact of initial pre-training size (vary from 5 to 20 examples) on final classifier performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of active learning for level completability classification compare when using an external oracle (e.g., a gameplay agent) instead of pre-labeled data?
- Basis in paper: [explicit] The authors mention they would like to integrate their model with completability agents as an oracle rather than pre-labeled data in future work to evaluate the impact on timing.
- Why unresolved: The current study used pre-labeled data to focus on the performance of active learning without the implications of using an external oracle. The timing and efficiency of using an external oracle are not evaluated.
- What evidence would resolve it: Conduct experiments using an external oracle (e.g., a gameplay agent) to label levels and compare the performance and efficiency of active learning with the current approach using pre-labeled data.

### Open Question 2
- Question: How does the performance of active learning for level completability classification change when dealing with imbalanced datasets?
- Basis in paper: [explicit] The authors mention they balanced the datasets in their study and would like to explore the impact of active learning on imbalanced datasets in the future.
- Why unresolved: The current study used balanced datasets to evaluate the performance of active learning. The impact of active learning on imbalanced datasets is not explored.
- What evidence would resolve it: Conduct experiments using imbalanced datasets and evaluate the performance of active learning compared to random sampling and passive learning.

### Open Question 3
- Question: How does the performance of active learning for level completability classification change when dealing with multi-label classification tasks?
- Basis in paper: [explicit] The authors mention they would like to explore the impact of active learning on multi-label classification tasks in the future.
- Why unresolved: The current study focused on binary classification (playable vs. unplayable levels). The performance of active learning on multi-label classification tasks is not evaluated.
- What evidence would resolve it: Conduct experiments using multi-label classification tasks (e.g., discrete ranges of difficulty) and evaluate the performance of active learning compared to random sampling and passive learning.

## Limitations

- The study focuses on binary classification accuracy without exploring multi-level difficulty classification or more complex game mechanics
- The false discovery rate remains relatively high compared to passive learning, which could limit practical deployment where misclassifying unplayable levels as playable is costly
- The study uses three specific games with relatively simple mechanics, and performance may vary significantly for games with more complex interactions or larger state spaces

## Confidence

- Active learning efficiency claims: **High** - Results are clearly demonstrated across multiple games with statistical significance
- Mechanism validity: **Medium** - The uncertainty sampling approach is well-established, but the specific implementation details and their impact are not fully explored
- Generalizability claims: **Low** - Limited to three games with simple mechanics; performance on more complex games is unknown

## Next Checks

1. Evaluate on games with more complex mechanics and larger state spaces to test generalizability
2. Compare different uncertainty sampling strategies (least confidence, entropy) against margin sampling for this specific binary classification task
3. Test the impact of initial pre-training size (vary from 5 to 20 examples) on final classifier performance and convergence speed