---
ver: rpa2
title: 'Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual
  Pre-training'
arxiv_id: '2306.05278'
source_url: https://arxiv.org/abs/2306.05278
tags:
- data
- intent
- language
- shot
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the conventional wisdom that continual pre-training
  is essential for few-shot intent detection. Through empirical investigation, the
  authors demonstrate that directly fine-tuning pre-trained language models (PLMs)
  on small datasets often yields surprisingly competitive results, with the performance
  gap to continual pre-training methods narrowing rapidly as labeled data increases.
---

# Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training

## Quick Facts
- arXiv ID: 2306.05278
- Source URL: https://arxiv.org/abs/2306.05278
- Reference count: 29
- Key outcome: Directly fine-tuning PLMs on small datasets often yields competitive results, with DFT++ outperforming state-of-the-art continual pre-training methods using only limited data.

## Executive Summary
This paper challenges the conventional wisdom that continual pre-training is essential for few-shot intent detection. Through empirical investigation, the authors demonstrate that directly fine-tuning pre-trained language models (PLMs) on small datasets often yields surprisingly competitive results, with the performance gap to continual pre-training methods narrowing rapidly as labeled data increases. To further enhance direct fine-tuning, the authors propose a framework called DFT++ that introduces context augmentation (generating contextually relevant unlabeled utterances via a generative PLM) and sequential self-distillation (exploiting multi-view data structure). Experiments on four benchmarks show that DFT++ outperforms state-of-the-art continual pre-training methods using only the limited available data, without relying on external training corpora.

## Method Summary
The paper proposes DFT++, a framework that combines direct fine-tuning with context augmentation and sequential self-distillation. Context augmentation uses GPT-J to generate contextually relevant unlabeled utterances based on few-shot prompts, which are then used for masked language modeling to adapt to the target distribution. Sequential self-distillation iteratively distills knowledge between model generations by matching output logits, allowing the model to learn multiple features for each class. The framework is evaluated on four intent classification datasets using BERT-base-uncased and RoBERTa-base models.

## Key Results
- Directly fine-tuning PLMs on small datasets yields surprisingly competitive results compared to continual pre-training methods
- DFT++ outperforms state-of-the-art continual pre-training methods on four intent classification benchmarks
- The performance gap between direct fine-tuning and continual pre-training narrows rapidly as labeled data increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Directly fine-tuning PLMs on small datasets does not necessarily lead to severe overfitting
- Mechanism: The PLM's pre-training provides strong generalization capabilities that persist even with limited fine-tuning data
- Core assumption: Pre-trained PLMs have learned sufficiently general representations that they can adapt to new tasks without extensive domain-specific pre-training
- Evidence anchors: [abstract] "directly fine-tuning pre-trained language models (PLMs) on small datasets often yields surprisingly competitive results"
- Break condition: If the target domain is significantly different from the pre-training corpus, or if the task requires highly specialized knowledge not captured in pre-training

### Mechanism 2
- Claim: Context augmentation through generative PLMs can effectively model target data distribution without external labeled data
- Mechanism: The few labeled examples are used to prompt a generative PLM to create contextually relevant unlabeled utterances, which are then used for masked language modeling to adapt to the target distribution
- Core assumption: The generative PLM can produce utterances that are semantically relevant to the target domain, even if not perfectly matching the labels
- Evidence anchors: [abstract] "introduce context augmentation (generating contextually relevant unlabeled utterances via a generative PLM)"
- Break condition: If the generative PLM produces mostly irrelevant or noisy data, or if the task requires very specific label matching that context alone cannot provide

### Mechanism 3
- Claim: Sequential self-distillation can exploit the multi-view structure in utterance data to improve model performance
- Mechanism: The model is trained iteratively, with each generation distilling knowledge from the previous one by matching output logits, allowing the model to learn multiple features for each class
- Core assumption: Utterances have inherent multi-view structure where multiple features can indicate the same class label
- Evidence anchors: [abstract] "sequential self-distillation (exploiting multi-view data structure)"
- Break condition: If the data lacks meaningful multi-view structure, or if the model converges too quickly to prevent effective distillation

## Foundational Learning

- Concept: Pre-trained Language Models (PLMs)
  - Why needed here: Understanding PLMs is crucial because the paper's approach relies on their generalization capabilities and ability to be fine-tuned with limited data
  - Quick check question: What is the primary advantage of using PLMs over training models from scratch, especially in few-shot scenarios?

- Concept: Few-shot learning
  - Why needed here: The paper addresses few-shot intent detection, so understanding the challenges and approaches in few-shot learning is essential
  - Quick check question: What is the main challenge in few-shot learning, and why is overfitting a particular concern?

- Concept: Data augmentation
  - Why needed here: The paper introduces context augmentation as an alternative to traditional data augmentation, so understanding different augmentation techniques is important
  - Quick check question: How does context augmentation differ from traditional data augmentation methods, and why might it be more effective in certain scenarios?

## Architecture Onboarding

- Component map: Input Utterances -> PLM Encoder (BERT/RoBERTa) -> Linear Classifier -> Output Intent Classification
  - Context Augmentation: GPT-J generates unlabeled utterances -> MLM Loss
  - Self-distillation: Iterative KL divergence matching between model generations

- Critical path:
  1. Fine-tune PLM on limited labeled data
  2. Generate contextually relevant unlabeled data using GPT-J
  3. Combine labeled and generated data for masked language modeling
  4. Apply sequential self-distillation to improve performance
  5. Evaluate on test set

- Design tradeoffs:
  - Using larger PLMs vs. computational cost
  - Generating more context data vs. quality of generated samples
  - Number of self-distillation iterations vs. diminishing returns
  - Balancing between cross-entropy loss and MLM loss

- Failure signatures:
  - Performance degradation with increased self-distillation iterations
  - Poor results when context augmentation generates irrelevant data
  - Overfitting despite direct fine-tuning (contradicting the paper's findings)

- First 3 experiments:
  1. Direct fine-tuning of BERT on a small labeled dataset, measuring performance vs. dataset size
  2. Implementation of context augmentation using GPT-J, comparing generated data quality with traditional augmentation methods
  3. Application of sequential self-distillation to the model, analyzing the impact of different temperature parameters and iteration numbers

## Open Questions the Paper Calls Out
The paper acknowledges several limitations including computational overhead and the need for additional labeled data for generating context. It suggests future work could explore combining DFT++ with other semi-supervised learning methods and investigating its performance on more diverse domains and languages.

## Limitations
- The effectiveness of direct fine-tuning may not generalize to domains significantly different from pre-training corpus
- Computational overhead of context augmentation and self-distillation may limit scalability
- Performance depends heavily on the quality of generated utterances from GPT-J, which is not systematically evaluated

## Confidence
**High Confidence:** The observation that direct fine-tuning performance improves rapidly with increasing labeled data is well-supported by experimental results
**Medium Confidence:** The claim that DFT++ consistently outperforms continual pre-training methods warrants additional validation on more diverse datasets
**Low Confidence:** The explanation of why PLMs don't overfit in few-shot scenarios lacks theoretical grounding and relies primarily on empirical observation

## Next Checks
1. Conduct controlled experiments comparing training loss trajectories and generalization gaps between direct fine-tuning and continual pre-training methods across varying dataset sizes
2. Implement human evaluation or automated relevance scoring to measure semantic similarity between generated utterances and true intent categories
3. Evaluate DFT++ on datasets from domains significantly different from the original pre-training corpus to test generalization limits