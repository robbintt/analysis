---
ver: rpa2
title: Blending adversarial training and representation-conditional purification via
  aggregation improves adversarial robustness
arxiv_id: '2306.06081'
source_url: https://arxiv.org/abs/2306.06081
tags:
- adversarial
- training
- attacks
- carso
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CARSO blends adversarial training and representation-conditional
  purification to improve adversarial robustness. The method conditions a VAE on the
  internal representation of an adversarially-trained classifier, learns to map potentially
  perturbed inputs onto a distribution of clean reconstructions, and aggregates multiple
  samples' predictions for robust classification.
---

# Blending adversarial training and representation-conditional purification via aggregation improves adversarial robustness

## Quick Facts
- arXiv ID: 2306.06081
- Source URL: https://arxiv.org/abs/2306.06081
- Reference count: 40
- Method blends adversarial training with representation-conditional VAE purification to improve robustness against AutoAttack

## Executive Summary
CARSO introduces a novel adversarial defense mechanism that combines adversarially-trained classifiers with conditional variational autoencoders (VAEs) to improve robustness. The method conditions a VAE on the internal representation of an adversarially-trained classifier, learns to map potentially perturbed inputs onto a distribution of clean reconstructions, and aggregates multiple samples' predictions for robust classification. Experiments across MNIST, Fashion MNIST, and CIFAR-10 demonstrate significant improvements in robust accuracy against AutoAttack while maintaining tolerable clean accuracy, and show effectiveness against both unforeseen attacks and adapted end-to-end white-box attacks.

## Method Summary
CARSO works by conditioning a VAE on the internal representation of an adversarially-trained classifier, where the VAE learns to map potentially perturbed inputs onto a distribution of clean reconstructions. During inference, multiple samples are drawn from this distribution and their predictions are aggregated. The method uses adversarially-balanced batches during training, mixing clean and perturbed examples (using FGSM and PGD at different strengths), with reconstruction loss computed as pixel-wise channel-wise binary cross entropy. The architecture consists of pre-encoders for input images and internal representations, a conditional VAE, a sampler for generating multiple reconstructions, and an aggregator for combining predictions.

## Key Results
- CARSO significantly improves state-of-the-art robust accuracy against AutoAttack across MNIST, Fashion MNIST, and CIFAR-10
- Maintains tolerable clean accuracy while achieving robust improvements
- Effectively shields against unforeseen attacks and end-to-end white-box attacks adapted for stochastic defenses
- Demonstrates gradient interference between internal representation extraction and classification disrupts adversarial optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CARSO improves robustness by introducing gradient interference between internal representation extraction and classification.
- Mechanism: The classifier's internal representation serves dual roles - for denoising and for classification. This forces conflicting gradient directions during an attack, disrupting the adversary's optimization.
- Core assumption: The gradients for extracting the internal representation and for classifying the reconstructions are not aligned in direction.
- Evidence anchors:
  - [abstract]: "it is likely that – at least in expectation – partial gradient interference occurs, preventing perturbations towards directions of maximum vulnerability of the undefended classifier."
  - [section 4.2]: "it is the result of those neurons playing two roles at once: within the input/output mapping determining the classification of reconstructions, and as a feature in the internal representation extraction process."
- Break condition: If the classifier's internal representation becomes separable from classification gradients, the interference effect diminishes.

### Mechanism 2
- Claim: Conditioning VAE denoising on classifier internal representation improves reconstruction accuracy for adversarial inputs.
- Mechanism: The internal representation captures semantic features that guide the VAE to generate reconstructions closer to the clean data manifold.
- Core assumption: The internal representation encodes sufficient information about the clean input to guide effective denoising.
- Evidence anchors:
  - [abstract]: "The method builds upon an adversarially-trained classifier, and learns to map its internal representation associated with a potentially perturbed input onto a distribution of tentative clean reconstructions."
  - [section 4.1]: "The key element of novelty lies in the intertwined operation of model and VAE – aimed at learning a distribution of purified examples (to be generated from each potentially-perturbed input), and to classify samples harvested from it"
- Break condition: If the internal representation becomes too noisy or uninformative, VAE conditioning provides no advantage over unconditioned denoising.

### Mechanism 3
- Claim: Adversarial purification creates a stochastic defense that is harder to attack end-to-end.
- Mechanism: Multiple stochastic reconstructions from the VAE require the attacker to optimize over an ensemble of possible outputs rather than a deterministic mapping.
- Core assumption: The attacker cannot efficiently compute gradients through the stochastic sampling process.
- Evidence anchors:
  - [abstract]: "Experimental evaluation by a well-established benchmark of strong adaptive attacks... shows that CARSO is able to defend itself against adaptive end-to-end white-box attacks devised for stochastic defences."
  - [section 5.2]: "End-to-end attacks against the whole architecture appear less effective w.r.t. attacks towards the classifier alone defended a posteriori."
- Break condition: If the attacker can approximate or bypass the stochastic sampling through differentiable alternatives.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: CARSO uses a conditional VAE to learn the mapping from internal representations to clean reconstructions.
  - Quick check question: What are the two main components of a VAE's loss function and what does each optimize?

- Concept: Adversarial Training (PGD)
  - Why needed here: The base classifier must be adversarially trained to provide useful internal representations and classification.
  - Quick check question: In PGD adversarial training, what is the inner loop optimizing and what is the outer loop optimizing?

- Concept: Conditional Generation
  - Why needed here: The VAE must generate reconstructions conditioned on the classifier's internal representation.
  - Quick check question: How does conditioning a VAE differ from a standard VAE in terms of input and training process?

## Architecture Onboarding

- Component map: Input → Classifier → Internal representation → Pre-encoders → VAE → Multiple reconstructions → Classifier predictions → Aggregation → Final prediction
- Critical path: Input → Classifier → Internal representation → Pre-encoders → VAE → Multiple reconstructions → Classifier predictions → Aggregation → Final prediction
- Design tradeoffs:
  - More reconstruction samples → better robustness but higher inference time
  - Larger internal representation subset → potentially better denoising but more VAE parameters
  - Stronger adversarial training → better base robustness but potentially higher clean accuracy cost
- Failure signatures:
  - High clean accuracy drop: VAE may be overfitting to adversarial examples
  - Low robust accuracy: Internal representation may be too noisy or VAE insufficiently trained
  - Slow inference: Too many reconstruction samples or inefficient VAE architecture
- First 3 experiments:
  1. Implement CARSO on MNIST with a simple CNN classifier, using only the final layer as internal representation
  2. Add multiple reconstruction samples and measure robustness against FGSM and PGD attacks
  3. Experiment with different internal representation subsets (e.g., final layer vs. multiple layers) and measure impact on clean vs. robust accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CARSO's defense mechanism work effectively for adversarial attacks beyond the ℓ∞ norm-bound threat model, such as ℓ2 or ℓ1 attacks?
- Basis in paper: [inferred] The paper focuses on the ℓ∞ norm-bound threat model and mentions that CARSO improves robust accuracy against AutoAttack, which primarily uses ℓ∞ bounded attacks. However, it does not explicitly evaluate CARSO's effectiveness against other norm-bounded attacks.
- Why unresolved: The paper does not provide experimental results or theoretical analysis on CARSO's performance against adversarial attacks with different norm bounds. This leaves open the question of whether CARSO's defense mechanism is generalizable to other types of adversarial attacks.
- What evidence would resolve it: Conducting experiments to evaluate CARSO's performance against adversarial attacks with different norm bounds, such as ℓ2 or ℓ1 attacks, would provide evidence to resolve this question. Comparing the robust accuracy of CARSO against these attacks with the robust accuracy against ℓ∞ attacks would help determine its generalizability.

### Open Question 2
- Question: How does the choice of internal representation subset affect CARSO's performance, and can a general criterion be developed for selecting the most informative neurons?
- Basis in paper: [explicit] The paper mentions that a carefully-chosen subset of layers is used instead of the whole internal representation to ease scalability. It also suggests that future work could explore the use of layerwise intrinsic dimension analysis or ad-hoc metrics to quantify the most informative neurons.
- Why unresolved: The paper does not provide a systematic approach or empirical analysis for selecting the most informative subset of internal representation layers. This leaves open the question of how different choices of internal representation subsets impact CARSO's performance and whether a general criterion can be developed for selecting the most informative neurons.
- What evidence would resolve it: Conducting experiments to compare CARSO's performance using different subsets of internal representation layers would provide evidence to resolve this question. Analyzing the relationship between the choice of internal representation subset and the resulting robust accuracy would help develop a general criterion for selecting the most informative neurons.

### Open Question 3
- Question: Can CARSO's defense mechanism be extended to non-image data, such as text or tabular data, and what modifications would be required?
- Basis in paper: [explicit] The paper focuses on image data and mentions that the promising evaluation of CARSO suggests potential future extensions beyond images as input. However, it does not provide specific details on how CARSO could be adapted for non-image data.
- Why unresolved: The paper does not provide experimental results or theoretical analysis on CARSO's performance with non-image data. This leaves open the question of whether CARSO's defense mechanism can be effectively extended to other data modalities and what modifications would be required.
- What evidence would resolve it: Conducting experiments to evaluate CARSO's performance on non-image data, such as text or tabular data, would provide evidence to resolve this question. Analyzing the modifications required to adapt CARSO's architecture and training process for different data modalities would help determine its generalizability.

## Limitations

- The robust accuracy improvements are measured primarily against AutoAttack, with limited evaluation of other attack families
- The analysis of failure modes and ablation studies is incomplete, particularly regarding hyperparameter sensitivity
- Computational overhead of the conditional VAE and sampling process is not quantified
- The paper does not provide a systematic approach for selecting the most informative internal representation subsets

## Confidence

- CARSO improves robust accuracy against AutoAttack: High
- Gradient interference mechanism provides robustness benefit: Medium
- CARSO defends against unforeseen attacks: Low
- End-to-end white-box attacks are less effective: Medium

## Next Checks

1. Evaluate CARSO against gradient-free attacks (e.g., SPSA, One-Pixel) and decision-based attacks (e.g., Boundary Attack) to verify robustness claims extend beyond gradient-based methods.
2. Perform detailed ablation studies varying the number of reconstruction samples (2, 4, 8, 16) and internal representation subsets to quantify their impact on both robust accuracy and computational overhead.
3. Test CARSO on additional datasets (e.g., SVHN, Tiny ImageNet) and against transfer attacks from different architectures to assess generalization beyond the three benchmark datasets used.