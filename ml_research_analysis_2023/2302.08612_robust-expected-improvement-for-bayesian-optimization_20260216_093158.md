---
ver: rpa2
title: Robust expected improvement for Bayesian optimization
arxiv_id: '2302.08612'
source_url: https://arxiv.org/abs/2302.08612
tags:
- robust
- surrogate
- figure
- acquisition
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of robust optimization in Bayesian
  optimization (BO), specifically targeting solutions with wider domains of attraction.
  The proposed method, robust expected improvement (REI), introduces an adversarial
  objective to the BO framework, favoring solutions in broader basins of attraction.
---

# Robust expected improvement for Bayesian optimization

## Quick Facts
- arXiv ID: 2302.08612
- Source URL: https://arxiv.org/abs/2302.08612
- Reference count: 5
- One-line primary result: REI outperforms traditional BO methods like EI in identifying robust solutions across test functions and a robot pushing problem

## Executive Summary
This paper addresses the challenge of robust optimization in Bayesian optimization by proposing robust expected improvement (REI). REI modifies the acquisition function to favor solutions in wider basins of attraction, targeting more robust optima. The method constructs an adversarial surrogate model that penalizes sharp local minima by using the maximum predictive mean over a neighborhood as the response value. This approach effectively "flattens" the surrogate surface around sharp peaks, making wider troughs more attractive to the acquisition function.

The primary results demonstrate that REI consistently outperforms standard expected improvement in identifying robust solutions. Across various test functions and a real-world robot pushing problem, REI minimizes regret and converges to lower values, indicating its effectiveness in finding robust optima. The method also shows superior performance compared to a competitor, StableOPT, in both solution quality and computational efficiency.

## Method Summary
The method builds on Gaussian Process regression to create a two-layer surrogate model. First, a standard GP is fit to observed data. Then, an adversarial surrogate is constructed by maximizing the GP prediction over an α-neighborhood around each point. This adversarial surrogate is used to compute REI, which balances exploitation and exploration while favoring wider basins of attraction. The algorithm can integrate over a range of α values to reduce sensitivity to the exact choice of neighborhood size. The method iteratively selects new evaluation points by optimizing the REI acquisition function until convergence.

## Key Results
- REI consistently outperforms standard expected improvement in minimizing regret and converging to lower values across test functions
- REI demonstrates superior performance compared to StableOPT in both solution quality and computational efficiency
- The method successfully identifies robust optima in a real-world robot pushing problem

## Why This Works (Mechanism)

### Mechanism 1
- Claim: REI modifies the acquisition objective to favor solutions in wider domains of attraction, thus targeting robust optima.
- Mechanism: By building an adversarial surrogate model, REI penalizes sharp local minima. The adversarial surrogate uses the maximum predictive mean over a neighborhood (α-ball) as the response value, effectively "flattening" the surrogate surface around sharp peaks and making wider troughs more attractive to the acquisition function.
- Core assumption: The surrogate model's predictive uncertainty is sufficient to identify and penalize sharp local minima when combined with neighborhood-based adversarial responses.
- Evidence anchors:
  - [abstract]: "robust expected improvement (REI), introduces an adversarial objective to the BO framework, favoring solutions in broader basins of attraction."
  - [section]: "The crux of our idea is as follows. A GP that would typically be fit to a collection of n evaluations of f in a BO framework, can be used to define the adversarial realization of those same values following the fitted analog of g: ˆg(xi,α) = max x∈xαi ˆf(x)."
  - [corpus]: Weak. No direct mention of adversarial objectives in neighboring papers.
- Break condition: If the predictive variance of the GP is too low or the neighborhood α is poorly chosen, the adversarial surrogate may not effectively penalize sharp minima, and REI collapses to standard EI.

### Mechanism 2
- Claim: REI improves robustness without requiring the practitioner to pre-specify the exact robustness level α.
- Mechanism: REI can integrate over a range of α values (e.g., via Monte Carlo or grid averaging), allowing the method to remain robust to the choice of α while still favoring wider basins of attraction.
- Core assumption: Averaging EI over a range of α values still maintains the preference for robust minima while smoothing out sensitivity to the exact α choice.
- Evidence anchors:
  - [section]: "A better method is to base REI acquisitions on all α-values between zero and αmax by integrating over them: EIαmax n = ∫αmax 0 EIα n(x) dF(α)."
  - [corpus]: Weak. No neighboring papers discuss integration over robustness parameters.
- Break condition: If the integration over α values is too coarse or the range too wide, the method may lose the ability to distinguish between robustly optimal and non-robust solutions.

### Mechanism 3
- Claim: REI reduces the number of local maxima in the acquisition surface, simplifying the inner optimization problem.
- Mechanism: The adversarial surrogate tends to smooth over peaked regions, leading to fewer local maxima in the REI surface compared to EI. This makes the inner optimization (finding xn+1 = argmax EIα n(x)) easier and less prone to local optima.
- Core assumption: The adversarial surrogate's smoothing effect on the acquisition surface translates into fewer and more well-separated local maxima.
- Evidence anchors:
  - [section]: "Generally speaking, REI surfaces have fewer local maxima compared to EI surfaces because ˆfα n (x) smooths over the peaked regions."
  - [corpus]: Weak. No direct evidence from neighboring papers.
- Break condition: If the adversarial surrogate is not smooth enough or if the original function has many narrow peaks, REI may still have a complex acquisition surface.

## Foundational Learning

- Concept: Gaussian Process (GP) regression and its role in Bayesian Optimization
  - Why needed here: REI is built on top of GP surrogates, and understanding how GPs model uncertainty and make predictions is crucial for understanding how the adversarial surrogate works.
  - Quick check question: How does the GP predictive mean and variance change as you move away from observed data points?
- Concept: Expected Improvement (EI) acquisition function and its exploration-exploitation tradeoff
  - Why needed here: REI is a modification of EI, so understanding how EI balances exploration and exploitation is key to understanding what REI changes.
  - Quick check question: What happens to the EI acquisition value at a point with high predictive uncertainty but mean higher than the current best?
- Concept: Adversarial optimization and worst-case reasoning
  - Why needed here: The core innovation of REI is the introduction of an adversarial objective, so understanding how adversarial optimization works is essential.
  - Quick check question: In the context of Eq. (2), what does the adversary g(x,α) represent and how does it modify the original objective f(x)?

## Architecture Onboarding

- Component map:
  - Input data -> GP surrogate fit -> Adversarial surrogate construction -> REI acquisition computation -> Next evaluation point selection
- Critical path:
  1. Fit GP surrogate to observed data
  2. Construct adversarial responses (max over neighborhood)
  3. Fit adversarial GP surrogate
  4. Compute REI acquisition
  5. Optimize REI to select next point
- Design tradeoffs:
  - Fixed α vs. integrated α: Fixed α is simpler but less robust; integrated α is more robust but computationally heavier.
  - Neighborhood resolution: Coarse neighborhoods are faster but less accurate; fine neighborhoods are more accurate but slower.
  - GP kernel choice: Stationary kernels are simpler but may mismatch non-stationary surfaces; non-stationary kernels may fit better but are more complex.
- Failure signatures:
  - REI behaves like standard EI: Likely the adversarial surrogate is not effectively penalizing sharp minima (e.g., due to low predictive variance or poor α choice).
  - REI acquisition is multimodal and hard to optimize: Likely the adversarial surrogate is not smooth enough or the original function has many narrow peaks.
  - REI does not improve over standard BO: Likely the robustness consideration is not relevant for the problem at hand, or the implementation has a bug.
- First 3 experiments:
  1. Run REI on a simple 1D test function (e.g., the piecewise function from Fig. 1) with known α and compare to standard EI. Verify that REI targets the wider basin.
  2. Run REI with integrated α on the same test function and compare to fixed α. Verify that integrated α is more robust to the choice of α.
  3. Run REI on a higher-dimensional test function (e.g., 2D Rosenbrock) and visualize the acquisition points. Verify that REI explores the wider basin more than standard EI.

## Open Questions the Paper Calls Out
No open questions were explicitly called out in the provided content.

## Limitations
- The primary assumption that wider basins of attraction correspond to more robust solutions may not hold in all cases, particularly for noisy or stochastic functions
- The choice of α (neighborhood size) significantly impacts REI's behavior, and while the paper proposes integrating over α, the optimal integration strategy is not thoroughly explored
- The computational overhead of constructing and optimizing the adversarial surrogate may become prohibitive in high-dimensional spaces or with complex functions

## Confidence
- Theoretical soundness: Medium
- Empirical validation: Medium
- Practical applicability: Low

## Next Checks
1. Test REI on a wider range of real-world optimization problems to assess its practical utility
2. Conduct a sensitivity analysis on the choice of α and the integration strategy to determine their impact on REI's performance
3. Evaluate the computational efficiency of REI in high-dimensional spaces and compare it to standard BO methods to understand the trade-offs involved