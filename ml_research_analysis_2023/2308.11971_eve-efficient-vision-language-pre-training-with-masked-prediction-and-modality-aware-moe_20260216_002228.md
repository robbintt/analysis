---
ver: rpa2
title: 'EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware
  MoE'
arxiv_id: '2308.11971'
source_url: https://arxiv.org/abs/2308.11971
tags:
- masked
- pre-training
- image
- tasks
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes EVE, an efficient vision-language foundation
  model that achieves state-of-the-art performance while improving training speed.
  EVE uses a unified multimodal Transformer integrated with modality-aware sparse
  Mixture-of-Experts (MoE) modules, and is pre-trained solely by masked signal modeling.
---

# EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE

## Quick Facts
- arXiv ID: 2308.11971
- Source URL: https://arxiv.org/abs/2308.11971
- Authors: Yihang Xu, Yiheng Zhu, Li Dong, Shaohan Huang, Wenhui Wang, Yaru Hao, Saksham Singhal, Barun Patra, Shuming Ma, Wanxiang Che, Furu Wei
- Reference count: 28
- Key outcome: EVE achieves state-of-the-art performance on vision-language tasks while improving training speed by 3.5x through masked signal modeling and modality-aware MoE

## Executive Summary
EVE introduces an efficient vision-language foundation model that achieves state-of-the-art performance while significantly improving training speed. The model uses a unified multimodal Transformer integrated with modality-aware sparse Mixture-of-Experts (MoE) modules and is pre-trained solely by masked signal modeling. This approach eliminates the need for complex contrastive losses and momentum teachers, resulting in 3.5x faster pre-training compared to traditional methods while maintaining or exceeding performance on downstream tasks including visual question answering, visual reasoning, and image-text retrieval.

## Method Summary
EVE is a unified multimodal Transformer that encodes both vision and language within a shared architecture integrated with modality-aware sparse MoE modules. The model is pre-trained using masked signal modeling, which reconstructs masked image pixels and text tokens from visible signals. The modality-aware MoE selectively routes tokens to different experts based on their modality while maintaining shared attention layers for cross-modal fusion. The architecture uses raw pixel reconstruction targets and an asymmetric decoder design that only reconstructs masked image patches, improving efficiency without sacrificing performance. Pre-training is conducted on 4M-21M image-text pairs from MSCOCO, Visual Genome, SBU Captions, and Conceptual Captions.

## Key Results
- Achieves 3.5x faster pre-training speed compared to models using Image-Text Contrastive and Image-Text Matching losses
- State-of-the-art performance on VQA2.0, NLVR2, and image-text retrieval tasks
- EVE-Large outperforms larger models (SimVLM-Huge) pre-trained on significantly more data
- Effective scaling behavior demonstrated with EVE-Base and EVE-Large variants

## Why This Works (Mechanism)

### Mechanism 1: Modality-Aware MoE Routing
- Claim: Modality-aware MoE improves performance by selectively routing tokens to experts based on modality while maintaining shared attention
- Mechanism: The architecture uses shared Multi-Head Self-Attention across all modalities, but the Feed-Forward Network is split into modality-specific experts. The modality routing technique adds modality-specific embeddings to the router input, allowing it to distinguish between vision and language tokens. This enables deep fusion through shared attention while preserving modality-specific processing through expert selection.
- Core assumption: Different modalities benefit from different processing strategies, but fusion requires shared attention layers
- Evidence anchors:
  - [abstract]: "EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts"
  - [section]: "Using a unified shared Transformer is more concise and flexible, which simplifies the extension to additional modalities and facilitates cross-modal alignment. By incorporating MoE, we can take into account the differences between modalities and capture more modality-specific information"
  - [corpus]: Weak evidence - no direct mention of modality-aware MoE in corpus papers, though related MoE approaches exist
- Break condition: If the modality routing becomes unbalanced or if the router fails to distinguish between modalities effectively

### Mechanism 2: Masked Signal Modeling with Raw Pixel Targets
- Claim: Masked signal modeling with raw pixel targets accelerates training while maintaining performance
- Mechanism: Instead of using complex contrastive losses or discrete visual tokens, EVE reconstructs raw pixels for masked image regions and text tokens for masked language. This simplification eliminates the need for additional models (like visual tokenizers or momentum teachers) and reduces computational overhead while maintaining representational quality through the reconstruction task.
- Core assumption: Raw signal reconstruction is sufficient for learning meaningful representations without additional complex objectives
- Evidence anchors:
  - [abstract]: "EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses"
  - [section]: "In order to maintain simplicity and effectiveness, we choose to utilize the image pixels themselves as the reconstruction target"
  - [corpus]: Weak evidence - corpus papers mention related approaches but don't directly address raw pixel reconstruction
- Break condition: If raw pixel targets prove insufficient for capturing high-level semantic information or if reconstruction quality degrades with model scale

### Mechanism 3: Asymmetric Decoder Design for MIM
- Claim: Asymmetric decoder design for MIM improves efficiency without sacrificing performance
- Mechanism: The model uses a lightweight decoder to reconstruct only masked image patches from partial image representations and complete text. This asymmetric design reduces computational cost while maintaining the ability to learn rich multimodal representations through the reconstruction task.
- Core assumption: Reconstructing only masked regions is sufficient for learning meaningful representations without processing all image tokens
- Evidence anchors:
  - [section]: "Following MAE (He et al. 2022a), we adopt an asymmetric design for MIM, where only observed image patches and all text tokens are fed into the encoder. A lightweight decoder is used to reconstruct raw pixels on masked positions from partial image representation and masked tokens"
  - [corpus]: No direct evidence in corpus papers about asymmetric decoder designs
- Break condition: If the lightweight decoder cannot capture sufficient information from partial representations or if the reconstruction quality degrades significantly

## Foundational Learning

- Concept: Masked Autoencoding
  - Why needed here: Forms the basis of the pre-training objective, teaching the model to reconstruct missing information from context
  - Quick check question: What is the key difference between standard autoencoding and masked autoencoding in this context?

- Concept: Mixture-of-Experts (MoE)
  - Why needed here: Enables efficient scaling by activating only relevant experts for each token while maintaining specialized processing
  - Quick check question: How does the modality-aware routing in this model differ from standard MoE routing?

- Concept: Cross-modal alignment
  - Why needed here: Essential for learning unified representations that work across vision and language tasks
  - Quick check question: What architectural component ensures proper fusion between modalities while preserving their distinct characteristics?

## Architecture Onboarding

- Component map: Input tokens → Shared attention layers → Modality-Aware MoE (soft router for top layers, hard router for bottom) → Masked signal modeling (MLM + MIM) → Lightweight decoder for MIM
- Critical path: Image/text tokenization → Shared attention processing → Expert routing → Reconstruction objectives → Downstream fine-tuning
- Design tradeoffs: Unified architecture simplifies scaling but requires careful handling of modality differences; raw pixel targets simplify training but may limit semantic abstraction
- Failure signatures: Poor routing balance (one expert dominates), reconstruction failure (pixel/text targets not learned), downstream task degradation
- First 3 experiments:
  1. Ablation study removing modality routing to measure its contribution
  2. Varying masking ratios for image and text independently
  3. Testing different numbers of experts and top-k values in MoE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of EVE scale with increasing model size and pre-training data?
- Basis in paper: [inferred] The paper mentions that EVE is "easy to scale up" and demonstrates this with EVE-Large achieving better performance than SimVML-Huge which is larger and pre-trained on much more data.
- Why unresolved: The paper only compares EVE-Base and EVE-Large, which are relatively small models. The scaling behavior of EVE for much larger models and datasets remains unexplored.
- What evidence would resolve it: Pre-training and evaluating EVE models with hundreds of billions of parameters and training on massive web-scale datasets, then comparing their performance to other large-scale vision-language models.

### Open Question 2
- Question: What is the optimal masking ratio for images and text in masked signal modeling for different vision-language tasks?
- Basis in paper: [explicit] The paper mentions that a higher vision masking ratio leads to improved performance, but the impact of different masking ratios on various downstream tasks is not fully explored.
- Why unresolved: The paper only investigates the impact of masking ratios on VQA and retrieval tasks, and the optimal masking ratio may vary depending on the specific characteristics of each task.
- What evidence would resolve it: Systematically varying the masking ratios for images and text in pre-training, then fine-tuning and evaluating the models on a wide range of vision-language tasks to identify the optimal masking strategy for each task.

### Open Question 3
- Question: How does the Modality-Aware MoE compare to other approaches for handling modality-specific information in vision-language models?
- Basis in paper: [explicit] The paper introduces Modality-Aware MoE as a simple way to apply MoE to multimodal learning and demonstrates its effectiveness compared to shared FFN and hard router designs.
- Why unresolved: The paper does not compare Modality-Aware MoE to other state-of-the-art approaches for handling modality-specific information, such as LIMoE, uni-perceiver-moe, or VLMO.
- What evidence would resolve it: Pre-training and evaluating EVE with different approaches for handling modality-specific information, including Modality-Aware MoE, LIMoE, uni-perceiver-moe, and VLMO, then comparing their performance on various vision-language tasks.

## Limitations

- Limited ablation studies on individual architectural components, making it difficult to quantify the contribution of each design choice
- Restricted evaluation to vision-language tasks without demonstrating generalization to other modalities
- No comprehensive comparison with alternative efficient pre-training methods beyond contrastive learning approaches

## Confidence

- High Confidence: Overall architecture design and theoretical motivation, use of masked signal modeling as pre-training objective, performance improvements on established benchmarks
- Medium Confidence: Effectiveness of modality-aware MoE routing, efficiency gains from raw pixel reconstruction, contribution of asymmetric decoder design
- Low Confidence: Scalability to larger models and datasets, robustness to different data distributions, comparison with alternative efficient methods

## Next Checks

1. **Routing Balance Analysis**: Conduct a detailed analysis of the modality routing distribution across experts during training to verify that the soft routing mechanism maintains balanced activation patterns and effectively distinguishes between vision and language tokens.

2. **Ablation Study on Pre-training Objectives**: Systematically compare the masked signal modeling approach against other pre-training objectives (including contrastive methods) across different model sizes and training budgets to validate the claimed 3.5x speedup while maintaining performance.

3. **Downstream Task Robustness Testing**: Evaluate the model's performance across a broader range of vision-language tasks and data distributions, including out-of-domain datasets and tasks requiring fine-grained visual reasoning, to assess generalization capabilities beyond the reported benchmarks.