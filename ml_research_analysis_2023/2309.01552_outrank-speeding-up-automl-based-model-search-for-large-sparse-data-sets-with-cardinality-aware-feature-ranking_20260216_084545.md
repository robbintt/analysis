---
ver: rpa2
title: 'OutRank: Speeding up AutoML-based Model Search for Large Sparse Data sets
  with Cardinality-aware Feature Ranking'
arxiv_id: '2309.01552'
source_url: https://arxiv.org/abs/2309.01552
tags:
- feature
- data
- features
- ranking
- automl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OutRank is a feature ranking system for speeding up AutoML-based
  model search in large sparse data sets. It uses a cardinality-aware variant of mutual
  information to rank features, overcoming biases introduced by varying feature cardinalities.
---

# OutRank: Speeding up AutoML-based Model Search for Large Sparse Data sets with Cardinality-aware Feature Ranking

## Quick Facts
- arXiv ID: 2309.01552
- Source URL: https://arxiv.org/abs/2309.01552
- Reference count: 17
- Key outcome: Up to 30% speedup with no performance loss, up to 50% speedup with minimal loss (~0.5% F1), outperforming random forest-based baselines

## Executive Summary
OutRank is a feature ranking system designed to accelerate AutoML-based model search for large sparse datasets. It introduces a cardinality-aware variant of mutual information (CardMI) to rank features while mitigating biases from varying feature cardinalities. The system extends this with a 3MR heuristic that incorporates feature redundancy and interaction information for more informed feature selection. Experiments demonstrate that OutRank can speed up AutoML by up to 50% while maintaining or slightly improving model performance.

## Method Summary
OutRank combines cardinality-aware mutual information with a 3MR heuristic to rank features for AutoML model selection. The method uses CardMI, which normalizes mutual information scores by the expected noise of features with the same cardinality, to overcome bias from high-cardinality features. The 3MR heuristic extends this by incorporating feature similarity and combined relevance through redundancy (MRMR) and interaction (MR) components. For scalability, OutRank uses hash-based encoding of feature interactions to enable fast profiling of higher-order interactions without explicit enumeration. The system processes data in batches, constructing combined features via hashing, computing CardMI scores, applying the 3MR heuristic, and aggregating results to produce the final feature ranking.

## Key Results
- Achieves up to 30% speedup in AutoML with no performance loss
- Achieves up to 50% speedup with minimal performance loss (~0.5% F1)
- Enables exploration of up to 300% larger feature spaces compared to AutoML-only approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Normalizing mutual information by the expected noise of features with the same cardinality mitigates bias introduced by high-cardinality features.
- Mechanism: CardMI adjusts raw mutual information scores by subtracting the average mutual information of a random sample of the same cardinality, effectively normalizing for the noise inherent to high-cardinality features.
- Core assumption: The noise profile of features is consistent across features of the same cardinality, allowing the subtraction of expected noise to reveal the true signal.
- Evidence anchors: [abstract] "It uses a cardinality-aware variant of mutual information to rank features, overcoming biases introduced by varying feature cardinalities."
- Break condition: If the noise profile varies significantly across features of the same cardinality, the normalization may not effectively remove bias, leading to inaccurate rankings.

### Mechanism 2
- Claim: The 3MR heuristic improves feature ranking by incorporating redundancy and interaction information beyond individual feature relevance.
- Mechanism: 3MR re-weights CardMI scores based on the redundancy between features (MRMR component) and the relevance of their interactions (MR component), using statistical functions to aggregate these effects.
- Core assumption: Features that are redundant or form strong interactions with already selected features should be deprioritized, even if they have high individual relevance.
- Evidence anchors: [abstract] "We further extend the similarity measure by incorporating information on feature similarity and combined relevance."
- Break condition: If the statistical function used to aggregate redundancy and interaction scores does not accurately reflect their impact on model performance, the re-weighting may lead to suboptimal feature selection.

### Mechanism 3
- Claim: Hashing-based encoding of feature interactions enables scalable profiling of higher-order interactions without explicit enumeration.
- Mechanism: OutRank constructs combined features by hashing tuples of feature values, allowing for the efficient computation of interaction effects using CardMI while avoiding the computational complexity of explicit feature combination.
- Core assumption: The hash function used to combine features preserves the relevant information for mutual information calculation, and collisions do not significantly impact the accuracy of interaction profiling.
- Evidence anchors: [section] "Combining insights used by this branch of algorithms (hashing trick) with the efficiency of feature ranking, OutRank enables fast profiling of tens of thousands of interactions based on millions of instances."
- Break condition: If the hash function leads to a high rate of collisions, the interaction profiling may become inaccurate, leading to the selection of suboptimal features.

## Foundational Learning

- Concept: Mutual Information (MI)
  - Why needed here: MI is the core similarity measure used by OutRank to quantify the relevance of features to the target variable.
  - Quick check question: What is the formula for mutual information between two random variables U and V?

- Concept: Factorization Machines (FMs)
  - Why needed here: FMs are the target model type for which OutRank is optimizing feature selection, and they explicitly model feature interactions.
  - Quick check question: How do factorization machines model feature interactions differently from traditional linear models?

- Concept: Feature Redundancy and Relevance
  - Why needed here: Understanding the concepts of feature redundancy and relevance is crucial for grasping the motivation behind the 3MR heuristic and its ability to improve feature selection.
  - Quick check question: What is the difference between feature redundancy and feature relevance in the context of feature selection?

## Architecture Onboarding

- Component map: Data ingestion pipeline -> Feature construction module -> Ranking module -> Aggregation module
- Critical path: Streaming data, constructing combined features, computing CardMI scores, applying 3MR heuristic, aggregating scores
- Design tradeoffs: Hashing for feature interactions trades potential hash collisions for computational efficiency; batch-based processing enables scalability but introduces approximation error
- Failure signatures: Incorrect feature rankings, slow performance, or memory issues may indicate problems with the hashing function, batch size, or statistical aggregation functions
- First 3 experiments:
  1. Run OutRank on a small synthetic dataset with known feature relevances to verify that the CardMI scores accurately reflect the true feature importances.
  2. Compare the performance of OutRank with and without the 3MR heuristic on a medium-sized dataset to assess the impact of incorporating redundancy and interaction information.
  3. Profile the memory and runtime of OutRank on a large dataset with varying batch sizes to identify the optimal configuration for scalability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of 3MR change when using different statistical functions (SF) like mean, median, or 90th percentile in the formula?
- Basis in paper: [explicit] The paper mentions that the SF function in 3MR can be mean, median, or 90th percentile, and shows results for different SF functions.
- Why unresolved: The paper does not provide a detailed comparison of how different SF functions affect the performance of 3MR.
- What evidence would resolve it: A systematic comparison of 3MR performance using different SF functions across various datasets and tasks.

### Open Question 2
- Question: How does the proposed CardMI heuristic compare to other feature ranking methods in terms of computational efficiency and scalability?
- Basis in paper: [explicit] The paper mentions that CardMI is designed to be fast and scalable for large sparse datasets, but does not provide a detailed comparison with other methods.
- Why unresolved: The paper does not provide a comprehensive comparison of CardMI with other feature ranking methods in terms of computational efficiency and scalability.
- What evidence would resolve it: A detailed comparison of CardMI with other feature ranking methods in terms of computational time and memory usage on large sparse datasets.

### Open Question 3
- Question: How does the performance of OutRank change when using different hashing schemes for encoding interactions?
- Basis in paper: [explicit] The paper mentions that simple hash-based encoding is used for encoding interactions, but does not explore other hashing schemes.
- Why unresolved: The paper does not provide a detailed comparison of OutRank performance using different hashing schemes for encoding interactions.
- What evidence would resolve it: A systematic comparison of OutRank performance using different hashing schemes for encoding interactions across various datasets and tasks.

## Limitations
- The reported speedups rely on AutoML algorithms whose internal evaluation procedures are not fully specified, limiting our ability to isolate OutRank's contribution.
- The effectiveness of CardMI normalization assumes consistent noise profiles across features of the same cardinality, which may not hold in practice.
- The hash-based interaction encoding may introduce collisions that could impact ranking accuracy, though this risk is not quantified in the experiments.

## Confidence
- CardMI normalization effectiveness: Medium
- 3MR heuristic performance: Low
- Generalizability across datasets: Low
- Speedup claims: Medium

## Next Checks
1. Verify CardMI normalization effectiveness by comparing feature rankings on synthetic datasets with controlled noise profiles across different cardinalities.
2. Test the sensitivity of the 3MR heuristic to different statistical aggregation functions for combining redundancy and interaction scores.
3. Evaluate OutRank's performance when integrated with multiple AutoML frameworks to assess framework-independence of the reported speedups.