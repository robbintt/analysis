---
ver: rpa2
title: Quantifying Uncertainty in Deep Learning Classification with Noise in Discrete
  Inputs for Risk-Based Decision Making
arxiv_id: '2310.06105'
source_url: https://arxiv.org/abs/2310.06105
tags:
- uncertainty
- prediction
- learning
- errors
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses quantifying prediction uncertainty in deep
  neural networks when input variables contain discrete errors, which is critical
  for risk-based decision making in applications like healthcare and finance. The
  authors propose a mathematical framework that models uncertainty arising from both
  errors in predictors (following known discrete distributions) and model parameter
  uncertainty.
---

# Quantifying Uncertainty in Deep Learning Classification with Noise in Discrete Inputs for Risk-Based Decision Making

## Quick Facts
- arXiv ID: 2310.06105
- Source URL: https://arxiv.org/abs/2310.06105
- Reference count: 8
- Primary result: EIV uncertainty framework outperforms MC-dropout in identifying misclassifications for TB treatment outcome prediction

## Executive Summary
This paper addresses quantifying prediction uncertainty in deep neural networks when input variables contain discrete errors, which is critical for risk-based decision making in applications like healthcare and finance. The authors propose a mathematical framework that models uncertainty arising from both errors in predictors (following known discrete distributions) and model parameter uncertainty. They use a quadratic Taylor approximation of the sigmoid function and ensemble learning to estimate prediction uncertainty. In a case study predicting tuberculosis treatment outcomes, their method (EIV uncertainty) outperforms Monte Carlo dropout in identifying misclassifications. The framework successfully distinguishes cases where predictions are sensitive to errors in smear test results, enabling better risk assessment for medical decision making.

## Method Summary
The authors develop a framework for quantifying prediction uncertainty in deep neural networks when input variables contain discrete errors. They train an ensemble of LSTM models on bootstrap samples of TB treatment outcome data, then use a quadratic Taylor approximation of the sigmoid function to compute two types of uncertainty: EIV (which captures uncertainty from discrete input errors) and non-EIV (which captures model parameter uncertainty). The method assumes known discrete error distributions for predictors and explicitly models these to separate aleatoric from epistemic uncertainty. They demonstrate their approach on predicting TB treatment outcomes using longitudinal data from 19,252 patients in Moldova.

## Key Results
- EIV uncertainty framework successfully separates aleatoric uncertainty (from input errors) from epistemic uncertainty (from model parameters)
- The method outperforms Monte Carlo dropout in identifying misclassifications in TB treatment outcome prediction
- EIV and non-EIV uncertainties differ significantly for cases sensitive to errors in smear test results, enabling risk-sensitive decision making
- Quadratic Taylor approximation of sigmoid function captures higher-order effects on predictive distribution compared to linear approximations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The quadratic Taylor approximation of the sigmoid function captures uncertainty more accurately than linear approximations used in Bayesian DNNs.
- **Mechanism**: By expanding the sigmoid function around the mean outputs (μ₀, μ₁) to second order, the method captures variance effects on the predictive distribution, whereas standard Bayesian DNNs only use the first-order term.
- **Core assumption**: The sigmoid function's curvature near its operating point is sufficiently smooth for a Taylor expansion to be valid.
- **Evidence anchors**:
  - [abstract] "They use a quadratic Taylor approximation of the sigmoid function and ensemble learning to estimate prediction uncertainty."
  - [section] "In classification Bayesian DNNs, the predictive distribution derived by applying an elementwise sigmoid function to the estimated expected values, μᵢ's. Consequently, the quantified prediction uncertainty in these models corresponds to the first term of Equation (7)... This indicates that Bayesian DNNs quantify prediction uncertainty resulting from model parameters as a linear Taylor approximation of the sigmoid function, while our model quantifies the uncertainty through a quadratic Taylor approximation of sigmoid function."
- **Break condition**: If the sigmoid function exhibits sharp nonlinearities or the operating point is near saturation, the Taylor expansion may poorly approximate the true behavior.

### Mechanism 2
- **Claim**: The EIV (Errors-in-Variables) uncertainty framework separates aleatoric uncertainty from epistemic uncertainty by modeling discrete input errors explicitly.
- **Mechanism**: By incorporating a known discrete error distribution p_ζ(x) into the predictive distribution calculation, the method isolates uncertainty due to measurement errors from uncertainty due to model parameters.
- **Core assumption**: Errors in predictors follow a known finite discrete distribution that can be parameterized and incorporated into the model.
- **Evidence anchors**:
  - [abstract] "They use a quadratic Taylor approximation of the sigmoid function and ensemble learning to estimate prediction uncertainty."
  - [section] "Our research is focused on the uncertainty that should be considered in model-based decision-support... We assume that p_ζ(x) which represents the probability distribution of true values of features, x, when features are observed as ζ, is known."
- **Break condition**: If the discrete error distribution is misspecified or unknown, the separation between aleatoric and epistemic uncertainty becomes unreliable.

### Mechanism 3
- **Claim**: Ensemble learning with bootstrap sampling provides a practical approximation to the intractable posterior distribution of model parameters.
- **Mechanism**: By training multiple DNN models on bootstrap samples and averaging their outputs, the method approximates the integral over parameter uncertainty without requiring MCMC or variational inference.
- **Core assumption**: Bootstrap samples adequately represent the variability in the training data and parameter space.
- **Evidence anchors**:
  - [abstract] "They use a quadratic Taylor approximation of the sigmoid function and ensemble learning to estimate prediction uncertainty."
  - [section] "To illustrate, we adopt an ensemble learning model in the training phase. The model obtains T bootstrap samples from the trainset, each generating a set of trained parameters."
- **Break condition**: If the training data is small or highly imbalanced, bootstrap sampling may not capture the true parameter variability, leading to overconfident uncertainty estimates.

## Foundational Learning

- **Concept**: Uncertainty decomposition (aleatoric vs epistemic)
  - Why needed here: The paper explicitly distinguishes between uncertainty from input errors (aleatoric) and uncertainty from model parameters (epistemic) to provide more actionable risk assessment.
  - Quick check question: What is the difference between aleatoric and epistemic uncertainty, and why does this distinction matter for risk-based decision making?

- **Concept**: Taylor series approximation
  - Why needed here: The method uses a quadratic Taylor expansion of the sigmoid function to capture higher-order effects of variance on the predictive distribution.
  - Quick check question: How does a quadratic Taylor approximation differ from a linear one, and why might this matter for uncertainty quantification?

- **Concept**: Ensemble learning and bootstrap sampling
  - Why needed here: The method uses ensemble learning with bootstrap samples to approximate the intractable posterior distribution of model parameters without requiring complex Bayesian inference.
  - Quick check question: How does bootstrap sampling help approximate parameter uncertainty, and what are the limitations of this approach?

## Architecture Onboarding

- **Component map**: Data preprocessing -> LSTM model training on bootstrap samples -> Output mean/variance calculation -> Quadratic Taylor approximation -> EIV and non-EIV uncertainty computation -> Risk assessment

- **Critical path**: (1) Train LSTM models on bootstrap samples, (2) Compute mean and variance of outputs for each class, (3) Apply quadratic Taylor approximation to estimate prediction uncertainty, (4) Compute EIV and non-EIV uncertainties, (5) Use these uncertainties for risk-based decision making

- **Design tradeoffs**: The method trades computational complexity (training multiple models) for more accurate uncertainty quantification. It also assumes discrete input errors are known, which may not always be the case.

- **Failure signatures**: If EIV and non-EIV uncertainties are very close for all data points, it may indicate that the model is not sensitive to input errors. If uncertainties are consistently high, it may indicate model overfitting or poor generalization.

- **First 3 experiments**:
  1. Compare EIV and non-EIV uncertainties on a synthetic dataset with known discrete input errors to validate the separation of aleatoric and epistemic uncertainty.
  2. Test the sensitivity of the method to different bootstrap sample sizes to find the optimal number of ensembles for a given dataset size.
  3. Evaluate the method on a real-world dataset (e.g., tuberculosis treatment outcomes) to assess its practical utility for risk-based decision making.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content and limitations discussed, several implicit questions arise regarding the generalizability and practical implementation of the method.

## Limitations

- The method assumes discrete error distributions are known and parameterized, which may not hold in many real-world applications.
- Computational cost scales linearly with ensemble size due to training multiple models.
- The method may be sensitive to bootstrap sampling when training data is limited or imbalanced.
- Quadratic Taylor approximation may break down when sigmoid operates near saturation regions.

## Confidence

**High Confidence**: The mathematical framework for separating EIV and non-EIV uncertainty is sound when discrete error distributions are known and the Taylor expansion is valid.

**Medium Confidence**: The empirical demonstration that EIV uncertainty outperforms Monte Carlo dropout in identifying misclassifications, as this depends on the specific characteristics of the TB dataset and may not generalize to all domains.

**Low Confidence**: The claim that this approach is generally superior to all other uncertainty quantification methods, as the comparison is limited to MC-dropout and does not include other Bayesian or conformal prediction methods.

## Next Checks

1. **Distribution Sensitivity Test**: Systematically vary the assumed discrete error distributions (sensitivity/specificity) in the TB smear test results and measure how EIV uncertainty estimates change, to validate the method's robustness to distribution misspecification.

2. **Ensemble Size Stability Analysis**: Evaluate the convergence of EIV and non-EIV uncertainty estimates as ensemble size increases from 50 to 500 models, identifying the minimum ensemble size needed for stable uncertainty quantification.

3. **Cross-Dataset Generalization**: Apply the method to at least two additional datasets with known discrete measurement errors (e.g., medical imaging with known noise characteristics, or sensor data with documented error distributions) to test generalizability beyond the TB case study.