---
ver: rpa2
title: 'Representations Matter: Embedding Modes of Large Language Models using Dynamic
  Mode Decomposition'
arxiv_id: '2309.01245'
source_url: https://arxiv.org/abs/2309.01245
tags:
- text
- embedding
- modes
- inaccurate
- wiki
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We examine the embedding space properties of hallucinated content
  generated by large language models (LLMs) using dynamic mode decomposition (DMD).
  By analyzing the evolution of sentence embeddings across paragraphs, we find that
  hallucinated text exhibits significantly lower-rank embedding patterns compared
  to ground-truth text.
---

# Representations Matter: Embedding Modes of Large Language Models using Dynamic Mode Decomposition

## Quick Facts
- **arXiv ID**: 2309.01245
- **Source URL**: https://arxiv.org/abs/2309.01245
- **Reference count**: 11
- **Primary result**: Hallucinations occur when ground-truth embeddings contain more dynamic modes than LLM-generated text can approximate

## Executive Summary
This paper introduces a novel framework using Dynamic Mode Decomposition (DMD) to analyze the embedding space properties of hallucinated content in large language models. By examining how sentence embeddings evolve across paragraphs, the authors find that hallucinated text exhibits significantly lower-rank embedding patterns compared to ground-truth text. The key insight is that hallucinations occur when the ground-truth embedding contains more dynamic modes than the LLM-generated text can approximate, suggesting hallucinations result from both generation techniques and learned representations.

## Method Summary
The method constructs embedding matrices from sentence-level embeddings of both GPT-3-generated and ground-truth Wikipedia text from the Wikibio dataset. DMD is applied using the PyDMD package with optimal rank (svd_rank=0) to extract eigenvalues and dynamics. The analysis compares rank, eigenvalue distributions, and mode dynamics between generated and ground-truth text across three annotation categories: major inaccurate, minor inaccurate, and accurate. The framework leverages the electromagnetic wave analogy where complex eigenvalues indicate far-field propagation modes (factual persistence) while real eigenvalues indicate near-field modes (quick decay).

## Key Results
- Hallucinations occur when ground-truth embeddings contain more dynamic modes than LLM-generated text can approximate
- Generated text exhibits consistently lower-rank embedding patterns compared to ground-truth text
- Generated embeddings with hallucinations show fewer modes propagating across sentences compared to ground-truth embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hallucinations occur when the ground-truth embedding contains more dynamic modes than the LLM-generated text can approximate
- Mechanism: DMD extracts the number and persistence of embedding modes across sentences. Ground-truth text embeddings exhibit more modes with slower decay, while LLM-generated text has fewer modes that decay faster. Hallucinations arise when the LLM tries to approximate a high-mode ground-truth embedding with only a few of its own modes
- Core assumption: DMD properly captures essential dynamics of text embeddings across sentences, and mode persistence correlates with factual consistency
- Evidence anchors: [abstract] "hallucinations occur when the ground-truth embedding contains more dynamic modes than the LLM-generated text can approximate"; [section] "hallucination occurs when the embedding modes of the Wiki text have a significant number of modes that cannot be well approximated by those of the GPT-3 text"
- Break condition: If mode persistence does not correlate with factual accuracy, or if DMD fails to capture meaningful dynamics of embeddings

### Mechanism 2
- Claim: Generated text exhibits consistently lower-rank embedding patterns compared to ground-truth text
- Mechanism: GPT-3-generated embeddings have lower rank (fewer linearly independent vectors) than ground-truth embeddings, limiting the model's ability to capture the full complexity of ground-truth embedding dynamics
- Core assumption: Rank of embedding matrices reflects representational capacity and correlates with hallucination likelihood
- Evidence anchors: [section] "it is seen that the rank of the Wiki embedding is higher than that of the GPT-3 generated embedding"; [section] "the higher hallucination of GPT-3 is associated with a higher rank of the ground-truth Wiki embeddings"
- Break condition: If rank does not predict hallucination likelihood, or if other factors dominate

### Mechanism 3
- Claim: Complex eigenvalues in DMD indicate far-field propagation modes (factual persistence), while real eigenvalues indicate near-field modes (quick decay)
- Mechanism: Drawing on electromagnetic wave theory, complex DMD eigenvalues correspond to modes that propagate across sentences (like far-field waves), while real eigenvalues correspond to evanescent modes that decay quickly. Hallucinations occur when ground-truth has many complex-mode structures that the LLM cannot reproduce
- Core assumption: Electromagnetic wave analogy properly maps to text embedding dynamics, and complex eigenvalues truly indicate more persistent, factual content
- Evidence anchors: [section] "Unlike the discrete Fourier transform (DFT) whose eigenvalues are roots of unity, the DMD eigenvalues are characterized by both a frequency and a magnitude, which together correspond to a complex frequency"; [section] "By contrasting Figures 3a and 4a, it is seen that generated samples with hallucinations correspond to generated embeddings with a few modes propagating across sentences as opposed to the Wiki embeddings, where many more embeddings survive across sentences"
- Break condition: If electromagnetic analogy does not hold for text embeddings, or if eigenvalue type does not correlate with factuality

## Foundational Learning

- **Dynamic Mode Decomposition (DMD)**: Provides mathematical framework for extracting modes and their temporal dynamics from embedding matrices, central to understanding how text representations evolve across sentences. *Quick check: How does DMD differ from standard Fourier analysis when applied to embedding dynamics?*
- **Text embedding dimensionality and rank**: Understanding how embeddings form vector spaces and how rank relates to representational capacity is essential for interpreting findings about low-rank hallucination patterns. *Quick check: What does it mean when an embedding matrix has low rank versus high rank in terms of information content?*
- **Hallucination detection in LLMs**: The paper assumes familiarity with hallucination problem in LLMs and builds on existing detection approaches to propose novel embedding-space analysis method. *Quick check: What are main challenges in detecting hallucinations in black-box LLMs without access to internal states?*

## Architecture Onboarding

- **Component map**: Embedding generation (sentence-transformers) → DMD analysis (PyDMD) → Rank analysis and spectrum visualization → Hallucination correlation
- **Critical path**: Embedding generation → DMD computation → Mode analysis → Hallucination correlation. Most critical step is ensuring embeddings capture meaningful semantic content before DMD analysis
- **Design tradeoffs**: Sentence-level embeddings versus token-level embeddings trades granularity for computational efficiency; 384-dimensional embeddings balance expressiveness against overfitting risk; DMD's linear approximation of non-linear dynamics is a necessary simplification
- **Failure signatures**: DMD eigenvalues clustering near unit circle boundary indicates unstable embedding dynamics; rank analysis showing no clear distinction between hallucinated and non-hallucinated text suggests method may not generalize; complex eigenvalues not correlating with factual persistence indicates electromagnetic analogy fails
- **First 3 experiments**:
  1. Replicate Wikibio analysis with different embedding models (BERT, RoBERTa) to test robustness
  2. Apply same DMD analysis to token-level embeddings rather than sentence-level to check if granularity affects results
  3. Test on different dataset (CNN/DailyMail) with different topics to verify generalizability across domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does prompting strategy significantly influence embedding mode characteristics and hallucination rates in LLM-generated text?
- Basis in paper: [inferred] from discussion of limitations regarding prompting strategies
- Why unresolved: Paper used existing annotated dataset without investigating how different prompting strategies might affect embedding properties and hallucination patterns
- What evidence would resolve it: Systematic experiments comparing DMD analysis results across different prompting strategies while keeping underlying model constant would show if prompting influences embedding mode dynamics and hallucination occurrence

### Open Question 2
- Question: Are DMD-based embedding mode patterns generalizable across different LLM architectures and embedding models?
- Basis in paper: [inferred] from suggestion to test on other datasets and contexts
- Why unresolved: Current study only examined GPT-3 with sentence-transformers embeddings, leaving open whether patterns hold for other models like GPT-4, Claude, or LLaMA
- What evidence would resolve it: Repeating DMD analysis with different LLMs and embedding models on same Wikibio dataset would determine if observed mode dynamics and hallucination patterns are consistent across architectures

### Open Question 3
- Question: Can DMD eigenvalue patterns be used as real-time detection mechanism for hallucinations in LLM outputs?
- Basis in paper: [explicit] from discussion of eigenvalues being inside unit circle and electromagnetic wave analogy
- Why unresolved: While paper identifies characteristic DMD patterns for hallucinated vs. non-hallucinated text, it doesn't explore whether these patterns can be computed efficiently enough for real-time detection or if they generalize beyond Wikibio dataset
- What evidence would resolve it: Developing and benchmarking real-time hallucination detection system based on DMD eigenvalue analysis across multiple domains and datasets would demonstrate practical applicability

## Limitations
- Electromagnetic wave analogy, while elegant, may not fully capture non-linear semantic relationships in text embeddings
- Study relies on sentence-transformers all-MiniLM-L6-v2 (384-dim) embeddings which may not capture all nuances of hallucinated content
- Wikibio dataset represents specific domain (biographies) that may not generalize to other text types

## Confidence

- **High confidence**: Generated text exhibits consistently lower-rank embedding patterns compared to ground-truth text (supported by direct empirical observations)
- **Medium confidence**: Hallucinations occur when ground-truth embeddings contain more dynamic modes than LLM-generated text can approximate (mechanism is sound but requires more extensive validation)
- **Medium confidence**: Complex eigenvalues indicate far-field propagation modes and correlate with factual persistence (theoretical basis exists but empirical validation is limited)

## Next Checks

1. Apply DMD analysis framework to token-level embeddings rather than sentence-level embeddings to determine if granularity affects mode detection and hallucination identification
2. Test method on multiple datasets across different domains (news articles, scientific abstracts, dialogue) to evaluate generalizability beyond biographical text
3. Conduct ablation studies by varying embedding dimensions and models (BERT, RoBERTa, etc.) to determine sensitivity of DMD-based hallucination detection to embedding choices