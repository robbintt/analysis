---
ver: rpa2
title: Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading
  Comprehension
arxiv_id: '2312.10126'
source_url: https://arxiv.org/abs/2312.10126
tags:
- text
- systems
- evaluation
- questions
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reading comprehension-based framework to
  evaluate whether text simplification systems preserve the meaning of the original
  text. The approach involves creating multiple-choice questions that target key information
  from the original text and then measuring how well participants can answer these
  questions after reading either the original text or a simplified version.
---

# Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension

## Quick Facts
- arXiv ID: 2312.10126
- Source URL: https://arxiv.org/abs/2312.10126
- Authors: 
- Reference count: 29
- Primary result: Human evaluation via reading comprehension shows that even the best-performing text simplification systems do not fully preserve the meaning of the original text, with at least 14% of questions marked as unanswerable based on simplified content.

## Executive Summary
This paper introduces a reading comprehension-based framework to evaluate whether text simplification systems preserve meaning. The approach involves creating multiple-choice questions targeting key information from original text and measuring how well participants can answer these questions after reading either original or simplified versions. The framework is used to evaluate human-written simplifications and nine state-of-the-art automatic text simplification systems, revealing that supervised systems leveraging pre-training knowledge achieve the highest accuracy on reading comprehension tasks, approaching human-written text scores.

## Method Summary
The study uses the OneStopQA dataset (30 articles, 162 paragraphs) and evaluates 9 text simplification systems including human-written simplifications, supervised models (ControlSup, EditNTS, KIS, MUSS, T5, BART), unsupervised models (DMASS, EDIT), and black-box models (ChatGPT). Participants (112 native English speakers on Prolific) answer 6 multiple-choice questions from 3 randomly selected passages across different conditions. The evaluation measures Accuracy (percentage of correct answers) and Answerability (percentage of questions marked "unanswerable" by annotators). The study also investigates automatic metrics like SARI, BLEU, and BERTScore for correlation with human judgments.

## Key Results
- Supervised systems leveraging pre-training knowledge achieve the highest accuracy on reading comprehension tasks, approaching human-written text scores
- Even the best-performing systems do not fully preserve meaning, with at least 14% of questions marked as "unanswerable" based on simplified content
- SARI's 3-way comparison makes it a reliable metric for system-level evaluation at the paragraph level

## Why This Works (Mechanism)

### Mechanism 1
Reading comprehension questions can reliably assess whether text simplification systems preserve meaning by testing if readers can answer correctly after reading simplified text, directly measuring information retention. Core assumption: Readers proficient in the target language will accurately report their comprehension through RC tasks.

### Mechanism 2
Pre-trained knowledge in supervised systems leads to better meaning preservation in text simplification because systems leveraging pre-training (e.g., BART, T5, ChatGPT) generate simplified texts that maintain more original meaning compared to models trained from scratch. Core assumption: Pre-trained models have better language understanding and generation capabilities that transfer to text simplification.

### Mechanism 3
The 3-way comparison in SARI metric makes it reliable for system-level evaluation at the paragraph level because it compares system output with references and the original text, capturing both simplicity and meaning preservation. Core assumption: The balance between added, deleted, and kept n-grams relative to both original and reference texts provides a comprehensive evaluation.

## Foundational Learning

- Concept: Reading comprehension evaluation
  - Why needed here: To directly measure if simplified texts preserve meaning by testing reader understanding
  - Quick check question: What is the purpose of using multiple-choice questions in this evaluation framework?

- Concept: Text simplification systems
  - Why needed here: Understanding different approaches (supervised, unsupervised, black-box) and their strengths/weaknesses
  - Quick check question: How do pre-trained models differ from models trained from scratch in text simplification?

- Concept: Evaluation metrics (SARI, BLEU, BERTScore)
  - Why needed here: To compare automatic metrics with human judgments and determine which best capture meaning preservation
  - Quick check question: What is the key difference between SARI and other metrics like BLEU or BERTScore?

## Architecture Onboarding

- Component map: RC question generation -> Text simplification system -> Simplified text output -> RC question answering -> Accuracy and answerability metrics
- Critical path: Text simplification system output -> RC question answering -> Accuracy/Answerability metrics
- Design tradeoffs: Sentence-level vs. paragraph-level simplification; automatic vs. human evaluation; pre-trained vs. trained-from-scratch models
- Failure signatures: Low accuracy scores; high percentage of unanswerable questions; poor correlation between automatic metrics and human judgments
- First 3 experiments:
  1. Compare RC accuracy of human-written vs. system-generated simplifications on a small set of questions
  2. Evaluate different TS systems using both RC tasks and automatic metrics to identify correlations
  3. Test the impact of question support (unigram overlap) on answerability in system-generated texts

## Open Questions the Paper Calls Out

### Open Question 1
How can the over-deletion tendency in text simplification systems be effectively mitigated to preserve more meaning from the original text? Basis in paper: The paper identifies over-deletion as a key issue causing questions to become unanswerable after simplification, with 14-65% of questions marked as unanswerable depending on the system. Why unresolved: While the paper demonstrates that over-deletion is a major problem and even proposes a method to detect it automatically using unigram overlap, it does not provide a concrete solution for preventing deletions that remove critical information needed to answer comprehension questions. What evidence would resolve it: A follow-up study that implements and tests various techniques to reduce over-deletion and measures their impact on both answerability scores and human judgments of meaning preservation.

### Open Question 2
Can the reading comprehension-based evaluation framework be effectively automated to scale evaluations across more systems, datasets, and languages? Basis in paper: The paper shows moderate correlation (0.838-0.744) between model-based QA answers and human judgments, but notes the system's limited ability to distinguish closely competing systems and potential confounding factors. Why unresolved: The paper demonstrates that current model-based QA can approximate human judgments but with limitations in discriminative power. The scalability potential is clear, but the reliability and validity of fully automated evaluation remain uncertain. What evidence would resolve it: Comparative studies that test automated RC evaluation against human judgments across multiple datasets, languages, and system types, measuring correlation strength, discriminative ability, and identifying specific failure modes of automated approaches.

### Open Question 3
Does text simplification that preserves meaning actually improve reading comprehension for target audiences (L2 learners, children, or people with reading disabilities)? Basis in paper: The paper evaluates meaning preservation but focuses on native English speakers and uses expert-crafted questions. It notes that this is a direction for future work, implying the connection between meaning preservation and actual comprehension benefits is unproven. Why unresolved: The study establishes that systems can preserve meaning (as measured by RC questions answered by native speakers) but doesn't test whether these same simplifications benefit the actual target populations who need simplified text. What evidence would resolve it: Controlled studies with actual target audiences comparing comprehension scores when reading original versus simplified texts, controlling for text difficulty and measuring both accuracy and reading time.

## Limitations

- Sample size and generalizability: The study uses 30 articles from OneStopQA with 162 paragraphs, which may not represent full diversity of text simplification scenarios
- Question design and bias: Multiple-choice question quality and difficulty may introduce bias, potentially misrepresenting true performance of simplification systems
- System implementation variability: Lack of detailed implementation parameters for each system affects reproducibility and makes it difficult to isolate the impact of pre-training versus other factors

## Confidence

- High Confidence: Supervised systems leveraging pre-training knowledge achieve higher RC accuracy approaching human-written text levels
- Medium Confidence: SARI's 3-way comparison being reliable for system-level evaluation at the paragraph level
- Medium Confidence: At least 14% of questions become unanswerable in simplified text

## Next Checks

1. Cross-Dataset Validation: Replicate the study using a different text simplification dataset (e.g., Newsela or WikiLarge) to test whether findings hold across different domains and text types

2. Alternative Question Types: Conduct complementary evaluation using open-ended questions or summary-based questions instead of multiple-choice to determine if observed patterns of meaning preservation are consistent across different question formats

3. Longitudinal System Comparison: Track performance of evaluated systems over time as they receive updates and improvements, particularly focusing on whether newer versions of pre-trained models continue to show superior performance in meaning preservation compared to specialized simplification models