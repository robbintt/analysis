---
ver: rpa2
title: Scalable Neural Network Training over Distributed Graphs
arxiv_id: '2302.13053'
source_url: https://arxiv.org/abs/2302.13053
tags:
- retexo
- training
- clients
- graph
- gnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of training Graph Neural Networks
  (GNNs) on fully-distributed graphs where each node's data is stored locally on a
  separate device. The challenge is that standard GNN training in this setup incurs
  prohibitively high communication costs due to frequent message-passing between devices.
---

# Scalable Neural Network Training over Distributed Graphs

## Quick Facts
- arXiv ID: 2302.13053
- Source URL: https://arxiv.org/abs/2302.13053
- Reference count: 40
- Key outcome: RETEXO reduces GNN communication costs by up to 10^6x while maintaining accuracy within 1.2% of standard GNNs

## Executive Summary
This paper introduces RETEXO, a framework for training Graph Neural Networks (GNNs) on fully-distributed graphs where each node's data is stored locally on separate devices. The key innovation is transforming existing GNN architectures to decouple edge structure from layer training, enabling sequential layer-wise training with message-passing only after each layer is complete. This approach drastically reduces communication costs while maintaining comparable accuracy to standard GNNs.

## Method Summary
RETEXO transforms existing GNN architectures by introducing lazy message passing, where each layer is trained sequentially rather than simultaneously. After each layer is trained, embeddings are propagated over edges to neighboring nodes, and these aggregated embeddings become the features for training the next layer. This reduces message-passing rounds from O(R) to just K (number of layers). The framework is compatible with standard federated learning protocols, making it suitable for real-world distributed environments.

## Key Results
- RETEXO achieves 1-2 orders of magnitude reduction in network data costs compared to standard GNN training
- Communication costs reduced by up to 10^6x while maintaining node classification accuracy within 1.2% of baseline GNNs
- Total data transferred per client is up to 20x lower across both client-to-client and client-to-server channels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RETEXO reduces message-passing rounds from O(R) to K, drastically cutting communication costs.
- **Mechanism:** RETEXO transforms existing GNNs by decoupling edge structure from layer training. Instead of training all layers simultaneously with message-passing in each round, RETEXO trains each layer sequentially. After each layer is fully trained, embeddings (logits) are propagated over edges, and the aggregated embeddings are used as features for the next layer.
- **Core assumption:** The aggregation and combine functions of the base GNN can be preserved even when applied to embeddings from previous layers rather than raw features.
- **Evidence anchors:**
  - [abstract] "The key idea in RETEXO GNNs is to disentangle the aggregation of intermediate representations over graph edges from the end-to-end training of the model."
  - [section] "The key is a new training procedure, lazy message passing, that reorders the sequence of training GNN elements."
- **Break condition:** If the aggregation functions of the base GNN require raw features rather than embeddings, the accuracy of RETEXO GNNs may degrade significantly.

### Mechanism 2
- **Claim:** RETEXO GNNs achieve 1-2 orders of magnitude reduction in network data costs while retaining accuracy.
- **Mechanism:** By reducing the number of message-passing rounds to K (number of layers), RETEXO GNNs minimize the total data transferred during training. This is particularly beneficial in fully-distributed setups where each node's data is stored locally on a separate device, and network communication is costly.
- **Core assumption:** The number of training rounds (R) is much larger than the number of layers (K), making the reduction in message-passing rounds significant.
- **Evidence anchors:**
  - [abstract] "RETEXO achieves 1-2 orders of magnitude reduction in network data costs compared to standard GNN training, while retaining accuracy."
  - [section] "The total data transferred on client-to-client communication channels in RETEXO GNNs is up to 4 to 7 orders of magnitude less compared to vanilla GNNs across all evaluated configurations."
- **Break condition:** If the number of layers (K) becomes large relative to the number of training rounds (R), the communication cost reduction may become less significant.

### Mechanism 3
- **Claim:** RETEXO GNNs are compatible with existing federated learning protocols and optimizations.
- **Mechanism:** RETEXO GNNs mimic the typical federated learning setup, where each client trains on its local data and communicates with a central server. The only difference is the message-passing rounds for propagating embeddings between clients after each layer is trained. This compatibility allows RETEXO GNNs to benefit from existing federated learning improvements such as federated optimization, asynchronous training, and defenses against data poisoning.
- **Core assumption:** The standard federated learning protocols can be extended to handle the message-passing rounds required by RETEXO GNNs without significant modifications.
- **Evidence anchors:**
  - [abstract] "RETEXO scales gracefully with increasing decentralization and decreasing bandwidth. It is the first framework that can be used to train GNNs at all network decentralization levels."
  - [section] "RETEXO naturally extends to the federated setup wherein a large sub-graph is stored at every client instead of a single node. Therefore, RETEXO remains compatible with training improvements developed for the federated learning setup as well."
- **Break condition:** If the existing federated learning protocols cannot efficiently handle the message-passing rounds required by RETEXO GNNs, the compatibility and performance benefits may be compromised.

## Foundational Learning

- **Concept:** Graph Neural Networks (GNNs)
  - Why needed here: RETEXO is a transformation of existing GNN architectures, so understanding how GNNs work is crucial to grasping the key idea behind RETEXO.
  - Quick check question: How do GNNs aggregate information from a node's neighbors during the message-passing process?

- **Concept:** Federated Learning
  - Why needed here: RETEXO GNNs are designed to be compatible with federated learning protocols, so understanding the basics of federated learning is important for implementing and deploying RETEXO GNNs in real-world scenarios.
  - Quick check question: What is the main difference between centralized and federated learning in terms of data storage and processing?

- **Concept:** Communication Costs in Distributed Systems
  - Why needed here: RETEXO aims to reduce communication costs in distributed GNN training, so understanding the factors that contribute to communication costs is essential for evaluating the effectiveness of RETEXO.
  - Quick check question: What are the main sources of communication costs in distributed GNN training, and how do they impact the overall training time and efficiency?

## Architecture Onboarding

- **Component map:** Base GNN (e.g., GCN, GraphSAGE, GAT) -> RETEXO transformation -> Message-passing rounds (K rounds, one for each layer) -> Federated learning protocol (e.g., FedSGD) -> Central server -> Distributed clients (each holding a single node's data)

- **Critical path:**
  1. Initialize base GNN and RETEXO transformation.
  2. Perform K message-passing rounds, one for each layer.
  3. In each message-passing round, clients share embeddings with their neighbors.
  4. Clients aggregate received embeddings and use them as features for training the next layer.
  5. Train each layer sequentially using the federated learning protocol.
  6. Aggregate locally updated models on the central server.

- **Design tradeoffs:**
  - Accuracy vs. communication cost: RETEXO GNNs may have slightly lower accuracy compared to base GNNs, but the communication cost reduction is significant.
  - Number of layers (K) vs. communication cost: Increasing the number of layers in the base GNN will increase the number of message-passing rounds in RETEXO GNNs, potentially reducing the communication cost benefit.
  - Compatibility with existing federated learning protocols: RETEXO GNNs are designed to be compatible with existing protocols, but modifications may be required to handle the message-passing rounds efficiently.

- **Failure signatures:**
  - Significant accuracy degradation compared to base GNNs.
  - Communication cost reduction is not as significant as expected.
  - RETEXO GNNs are not compatible with existing federated learning protocols or optimizations.
  - Message-passing rounds are not handled efficiently, leading to increased training time or communication costs.

- **First 3 experiments:**
  1. Implement a simple RETEXO GNN based on GCN and evaluate its accuracy and communication cost on a small homophilous dataset (e.g., Cora).
  2. Compare the performance of RETEXO GNN with the base GNN (GCN) on a larger homophilous dataset (e.g., PubMed) to assess the scalability of the approach.
  3. Evaluate the compatibility of RETEXO GNN with an existing federated learning protocol (e.g., FedSGD) on a heterophilous dataset (e.g., Wiki-cooc) to ensure that the communication cost reduction is maintained in different scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of missing neighbors on RETEXO GNNs' accuracy in different types of graph datasets?
- Basis in paper: [explicit] The paper shows RETEXO GNNs are robust to missing neighbors, maintaining accuracy when 70% of neighbors are available and only dropping by 1.5% when 50% are available.
- Why unresolved: The paper only tests on homophilous-transductive datasets and doesn't explore how this robustness varies across heterophilous or inductive settings.
- What evidence would resolve it: Testing RETEXO GNNs on heterophilous and inductive datasets with varying levels of neighbor availability to quantify accuracy degradation.

### Open Question 2
- Question: How does RETEXO's communication efficiency scale with graph size and density?
- Basis in paper: [inferred] The paper demonstrates significant communication cost reduction but only evaluates on datasets up to 22,662 nodes and doesn't explore scaling properties.
- Why unresolved: The evaluation focuses on moderate-sized graphs, leaving uncertainty about performance on massive graphs with millions of nodes or sparse graphs with very few edges per node.
- What evidence would resolve it: Systematic experiments varying graph size (10^3 to 10^7 nodes) and density (average degree 2 to 1000) to measure communication costs and accuracy retention.

### Open Question 3
- Question: Can RETEXO be extended to handle dynamic graphs where edges/nodes are added or removed during training?
- Basis in paper: [explicit] The paper mentions inductive settings for evolving graphs but doesn't address how RETEXO handles graph changes during training.
- Why unresolved: RETEXO relies on static message-passing rounds and sequential layer training, which may not adapt well to continuous graph evolution.
- What evidence would resolve it: Implementing RETEXO with mechanisms to handle edge/node insertions/deletions during training and measuring accuracy and communication efficiency trade-offs.

### Open Question 4
- Question: What is the optimal number of message-passing rounds (K) for RETEXO GNNs across different graph types?
- Basis in paper: [explicit] The paper uses K=2 for homophilous datasets and K=5 for heterophilous datasets but doesn't systematically explore the relationship between K and performance.
- Why unresolved: The choice of K appears heuristic and may not be optimal for all graph types or architectures, potentially leaving accuracy or efficiency on the table.
- What evidence would resolve it: Grid search over K values (2 to 10) across various graph types and architectures to find Pareto-optimal trade-offs between accuracy and communication costs.

## Limitations
- The 10^6x communication cost reduction claim is based on controlled experimental conditions and may vary significantly with real-world network topologies, bandwidth constraints, and graph sizes.
- The paper does not fully address how RETEXO handles dynamic graphs or graph changes during training, which could impact both accuracy and communication efficiency.
- While compatibility with federated learning protocols is claimed, specific optimizations or modifications required for different protocols are not detailed.

## Confidence

- **High Confidence**: The core mechanism of decoupling edge structure from layer training and the sequential layer-wise training approach is well-defined and theoretically sound.
- **Medium Confidence**: The empirical results showing 1-2 orders of magnitude communication cost reduction are supported by experiments, but the exact conditions and generalizability to other scenarios need further validation.
- **Low Confidence**: The claim about compatibility with all existing federated learning protocols and optimizations lacks detailed implementation specifics and may require significant modifications in practice.

## Next Checks
1. Implement RETEXO on a dynamic graph dataset and measure how accuracy and communication costs change as the graph structure evolves during training.
2. Test RETEXO's performance on graphs with varying degrees of heterogeneity and sparsity to assess the robustness of the communication cost reduction claims.
3. Conduct a comparative study of RETEXO's compatibility with different federated learning protocols (e.g., FedAvg, FedProx) to identify any modifications or optimizations required for optimal performance.