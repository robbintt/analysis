---
ver: rpa2
title: 'QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized
  Self-Supervised Speech Representation Learning'
arxiv_id: '2309.00126'
source_url: https://arxiv.org/abs/2309.00126
tags:
- speech
- supervised
- data
- audio
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents QS-TTS, a semi-supervised TTS framework that
  significantly improves synthesis quality while reducing supervised data requirements
  through vector-quantized self-supervised speech representation learning (VQ-S3RL).
  The framework employs two generative VQ-S3R learners: a principal learner combining
  contrastive S3RL with Multi-Stage Multi-Codebook (MSMC) VQ-GAN to produce high-quality
  MSMCR, and an associate learner that further compresses this representation into
  a highly-compact VQ form using VQ-VAE.'
---

# QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning

## Quick Facts
- arXiv ID: 2309.00126
- Source URL: https://arxiv.org/abs/2309.00126
- Reference count: 40
- Key outcome: Semi-supervised TTS framework achieving state-of-the-art performance with reduced supervised data requirements through VQ-S3RL

## Executive Summary
QS-TTS is a semi-supervised TTS framework that leverages vector-quantized self-supervised speech representation learning (VQ-S3RL) to significantly reduce the need for supervised training data while maintaining high synthesis quality. The system employs two complementary VQ-S3R learners: a principal learner that combines contrastive S3RL (HuBERT) with Multi-Stage Multi-Codebook (MSMC) VQ-GAN, and an associate learner that further compresses the representation using VQ-VAE. The framework demonstrates superior performance in both standard and low-resource scenarios, achieving the highest MOS scores against both supervised and semi-supervised baselines while showing slower quality degradation as supervised data decreases.

## Method Summary
QS-TTS employs a two-stage VQ-S3RL approach where a principal learner (combining HuBERT with MSMC-VQ-GAN) extracts compact representations from unlabeled speech data, which are then further compressed by an associate learner (VQ-VAE) to create highly-compact VQ representations. These pre-trained models are fine-tuned with supervised data to create the acoustic model and vocoder components. The system uses a FastSpeech-based architecture with a multi-stage decoder for predicting MSMCR representations, and a MSMC-VQ-GAN-based vocoder for waveform synthesis. Training involves pre-training on AIShell-3 dataset (85 hours) followed by fine-tuning on supervised datasets like CSMSC.

## Key Results
- Achieved highest MOS scores over both supervised and semi-supervised baselines in standard conditions
- Reached FD-AC of 0.19 and CER of 6.05% in standard settings with 10-hour supervised data
- Maintained superior performance in low-resource settings with only 15 minutes supervised data (FD-AC 0.25, CER 8.98%)
- Demonstrated slower quality decay compared to alternatives when decreasing supervised data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VQ-S3RL provides compact, discrete representations that are easier to predict from text while maintaining high-quality audio reconstruction
- Core assumption: Compact discrete representations preserve sufficient phonetic information for intelligible synthesis
- Evidence anchors: Abstract mentions "VQ-S3RL utilizing more unlabeled speech audio" and "highly-compact VQ representation"; Section states VQ-S3RL "provide profitable speech representations for TTS" that are "easier to predict from the text"
- Break condition: If quantization discards critical phonetic information needed for intelligibility

### Mechanism 2
- Claim: Pre-training on large unlabeled speech data with VQ-S3RL provides effective pre-trained models that enhance both acoustic modeling and vocoding
- Core assumption: Knowledge from large-scale unlabeled speech data transfers effectively to supervised TTS tasks
- Evidence anchors: Abstract states VQ-S3RL provides "effective pre-trained models to enhance the acoustic model and the vocoder"; Section mentions "pre-trained models for the prediction and synthesis module training"
- Break condition: If pre-trained models' representations are too specialized to unlabeled data distribution and don't generalize well

### Mechanism 3
- Claim: Combination of contrastive S3RL (HuBERT) with generative VQ-S3RL (MSMC-VQ-GAN) provides complementary benefits
- Core assumption: Contrastive and generative SSL methods provide complementary benefits that enhance each other
- Evidence anchors: Abstract mentions combining "contrastive S3RL model, HuBERT... and the proposed generative S3RL model, Multi-Stage Multi-Codebook (MSMC) VQ-GAN"; Section explains contrastive S3RL extracts general representations while generative VQ-S3RL converts them into high-quality discrete representations
- Break condition: If combination introduces conflicting objectives that degrade performance

## Foundational Learning

- Concept: Self-supervised learning (SSL) for speech representation
  - Why needed here: SSL enables learning from large amounts of unlabeled speech data, crucial for reducing supervised data requirements
  - Quick check question: What's the key difference between contrastive and generative SSL approaches for speech?

- Concept: Vector quantization and discrete representation learning
  - Why needed here: VQ provides compact, discrete representations that are easier to model and transfer than continuous representations
  - Quick check question: How does vector quantization affect the trade-off between representation compactness and information preservation?

- Concept: Transfer learning in speech synthesis
  - Why needed here: Pre-trained models from SSL tasks need to be effectively adapted for TTS through fine-tuning on supervised data
  - Quick check question: What are the key considerations when transferring pre-trained speech representations to TTS tasks?

## Architecture Onboarding

- Component map: Text → Acoustic model (predicts MSMCR) → Vocoder (converts MSMCR to waveform)
- Critical path: Text input flows through acoustic model to predict MSMCR, then vocoder converts MSMCR to final waveform
- Design tradeoffs: Compactness vs. information preservation in VQ representations; number of codebooks and stages vs. model complexity; pre-training data quality vs. model robustness
- Failure signatures: High CER/PER indicates loss of phonetic information in quantization; low MOS with reasonable CER/PER suggests reconstruction quality issues; slow convergence during fine-tuning may indicate poor transfer from pre-trained models
- First 3 experiments:
  1. Compare MSMCR-based synthesis vs. direct HuBERT feature synthesis to validate VQ compression benefits
  2. Ablation study: Remove either contrastive or generative component to measure contribution
  3. Vary codebook size in associate learner to find optimal balance between compactness and quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does QS-TTS performance scale with increasing amounts of unlabeled speech data beyond tested experiments?
- Basis in paper: Paper states QS-TTS utilizes more unlabeled speech data but only evaluates on specific datasets with fixed amounts
- Why unresolved: Paper focuses on demonstrating low-resource effectiveness rather than exploring full potential of scaling unlabeled data
- What evidence would resolve it: Systematic experiments varying unlabeled training data amounts while keeping supervised data constant, measuring MOS, CER across different data scales

### Open Question 2
- Question: What is the impact of language mismatch between unlabeled dataset and target language in cross-lingual scenarios?
- Basis in paper: Paper explicitly tests cross-lingual performance using Cantonese with Mandarin unlabeled dataset, showing QS-TTS performs better than alternatives, but doesn't systematically explore different degrees of language similarity
- Why unresolved: Only tests one cross-lingual pair, effectiveness across different language families or degrees of similarity unexplored
- What evidence would resolve it: Experiments testing QS-TTS across multiple language pairs with varying degrees of similarity with systematic measurement of performance degradation

### Open Question 3
- Question: How does choice of codebook size in associate VQ-S3R learner affect synthesis quality across different languages and data regimes?
- Basis in paper: Paper includes experiment varying codebook sizes (4, 64, 1024) and notes different languages prefer different sizes, but doesn't provide comprehensive analysis of relationship between codebook size, language similarity, and data availability
- Why unresolved: Experiment shows codebook size matters but only tests three discrete values and doesn't explore interaction between codebook size and other factors
- What evidence would resolve it: Systematic study varying codebook sizes across grid of language pairs and supervised data amounts, potentially including method to automatically select optimal codebook size

## Limitations

- Framework relies heavily on pre-training with large unlabeled datasets (85 hours AIShell-3), which may not be readily available for all languages or domains
- Two-stage quantization process introduces multiple potential failure points where information could be lost, particularly for languages with complex phonological systems
- All experiments conducted on Mandarin datasets, effectiveness for other languages with different phonological properties or writing systems remains unproven

## Confidence

**High confidence** (Strong empirical support):
- Framework achieves state-of-the-art MOS scores in both standard and low-resource scenarios as measured by reported experiments
- Objective metrics (FD-AC, CER) show consistent improvements over baseline methods
- Slow quality decay with decreasing supervised data is empirically validated

**Medium confidence** (Supported but with caveats):
- Two-stage VQ-S3RL approach provides complementary benefits - logically sound but could benefit from more ablation studies
- Transfer learning effectiveness from unlabeled to supervised data - demonstrated but specific mechanisms could be better characterized
- Combination of contrastive and generative SSL methods provides complementary benefits - theoretically plausible but limited direct evidence

**Low confidence** (Weak or indirect support):
- Generalization to languages other than Mandarin - no evidence provided
- Performance in multi-speaker scenarios - only single-speaker data used
- Robustness to different recording conditions - limited environmental variability in datasets

## Next Checks

1. **Cross-lingual validation**: Test QS-TTS on non-Mandarin languages (e.g., English, languages with different phonological systems) to verify generalization of the VQ-S3RL approach across linguistic families

2. **Information preservation analysis**: Quantify exactly how much phonetic information is retained through each quantization stage by comparing phoneme recognition accuracy at each representation level and identifying which components of speech signal are most vulnerable to information loss

3. **Multi-speaker scalability test**: Evaluate QS-TTS performance as number of speakers increases, measuring how pre-training benefits scale and whether framework maintains low-resource advantages in multi-speaker scenarios with limited per-speaker data