---
ver: rpa2
title: 'Cross-Domain HAR: Few Shot Transfer Learning for Human Activity Recognition'
arxiv_id: '2310.14390'
source_url: https://arxiv.org/abs/2310.14390
tags:
- data
- target
- source
- learning
- labeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of human activity recognition
  (HAR) with limited labeled data by leveraging existing labeled datasets. The authors
  propose Cross-Domain HAR, a transfer learning framework that uses a teacher-student
  self-training paradigm to adapt representations learned from a source domain to
  a target domain with limited annotations.
---

# Cross-Domain HAR: Few Shot Transfer Learning for Human Activity Recognition

## Quick Facts
- arXiv ID: 2310.14390
- Source URL: https://arxiv.org/abs/2310.14390
- Authors: Various
- Reference count: 40
- Primary result: Achieves 7-8% better F1-scores than naive transfer learning and 20% better than supervised learning on benchmark HAR datasets

## Executive Summary
This paper addresses the challenge of human activity recognition (HAR) when labeled data is scarce in the target domain. The authors propose Cross-Domain HAR, a transfer learning framework that leverages existing labeled datasets through a teacher-student self-training paradigm. The method combines self-supervision, consistency regularization, and source data augmentation to adapt representations from source to target domains. Experimental results on six benchmark datasets demonstrate significant performance improvements, with the framework outperforming state-of-the-art methods by substantial margins in few-shot learning scenarios.

## Method Summary
Cross-Domain HAR employs a teacher-student self-training approach where a teacher model trained on augmented source data generates pseudo-labels for unlabeled target data. The student model learns from these pseudo-labels while simultaneously performing self-supervised learning on the target data using SimCLR. The framework incorporates consistency regularization by matching predictions between weak and strong augmentations of target data. Finally, the learned student model is fine-tuned on the limited labeled target data. The approach addresses the domain gap problem in HAR by leveraging both labeled source data and unlabeled target data through self-training and self-supervision.

## Key Results
- Achieves up to 20% higher F1-scores compared to supervised learning with limited target labels
- Outperforms naive transfer learning by 7-8% across benchmark datasets
- Ablation studies show each component (self-supervision, consistency regularization, augmentation) contributes significantly to performance

## Why This Works (Mechanism)

### Mechanism 1: Dual Self-Training and Self-Supervision
The combination of teacher-student self-training with SimCLR-based self-supervision enables effective adaptation across large domain gaps. The teacher model generates pseudo-labels for weakly augmented target data, while the student model learns from these labels and performs self-supervised learning on the same data. This dual signal bridges source-target domain differences by providing both supervised guidance and unsupervised representation learning.

### Mechanism 2: Consistency Regularization
The framework enforces consistency between weak and strong augmentations of target data by matching teacher predictions on weak augmentations with student predictions on strong augmentations. This invariance constraint forces the student to learn more robust and generalizable features that can handle sensor heterogeneity and data recording differences between domains.

### Mechanism 3: Source Data Augmentation
Eight different signal transformations (noise addition, scaling, rotation, reversal, negation, warping, shuffling, perturbation) are applied to source data to increase diversity and generalization capability. This augmentation strategy helps the teacher model learn more robust representations that transfer better to target domains with different sensor positions and activities.

## Foundational Learning

- **Semi-supervised learning through self-training**: Needed because target domains have very limited labeled data (2-100 windows per class), making purely supervised approaches ineffective while still requiring supervision for accurate activity recognition. Quick check: How does the teacher-student paradigm in this paper differ from standard self-training approaches?

- **Contrastive self-supervised learning (SimCLR)**: Required because target domain activities may be completely different from source activities, necessitating representation learning directly from unlabeled target data. Quick check: What is the role of the projection head in SimCLR, and why is it discarded after pre-training?

- **Domain adaptation and transfer learning**: Essential due to differences in sensor positions (wrist vs waist vs ankle), activity sets, and user populations between source and target domains. Quick check: What makes cross-domain HAR more challenging than user-specific adaptation within the same dataset?

## Architecture Onboarding

- **Component map**: Source data augmentation → Teacher model training → Pseudo-label generation (with confidence filtering) → Student model training (self-training + self-supervision) → Few-shot fine-tuning

- **Critical path**: The complete pipeline flows from data preparation through teacher training, pseudo-label generation, student training with consistency regularization, and final fine-tuning on limited labeled data.

- **Design tradeoffs**: Balancing augmentation strength vs. training stability, confidence threshold for pseudo-labels vs. sufficient target data, and self-training vs. self-supervision loss balance.

- **Failure signatures**: Poor target performance indicates either teacher model didn't learn generalizable features or student training failed to adapt them; high variance suggests instability in pseudo-label selection or augmentation strategy.

- **First 3 experiments**:
  1. Baseline comparison: Implement Cross-Domain HAR with Mobiact as source and HHAR as target, comparing against CPC, SimCLR, Naive transfer, and Conv. Classifier baselines
  2. Ablation study: Remove self-supervised loss component and measure performance degradation on Myogym and Motionsense datasets
  3. Encoder architecture impact: Replace convolutional encoder with DeepConvLSTM and evaluate on PAMAP2 and Motionsense target datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different source datasets compare in terms of effectiveness for transfer learning in HAR, and what factors determine their suitability for specific target domains?
- Basis in paper: The paper discusses using Mobiact and HHAR as source datasets and compares their performance across different target domains.
- Why unresolved: While the paper provides some insights, a comprehensive analysis of factors determining source dataset suitability for specific target domains is lacking.
- What evidence would resolve it: A systematic study comparing performance of different source datasets across a wide range of target domains, considering factors like dataset size, diversity, and activity overlap.

### Open Question 2
- Question: How does the choice of encoder architecture impact the performance of Cross-Domain HAR, and what factors should be considered when selecting an encoder for a specific target domain?
- Basis in paper: The paper discusses the impact of using different encoders (convolutional vs. DeepConvLSTM) and suggests that encoder choice should be driven by performance on the source domain.
- Why unresolved: The paper only explores two encoder architectures without comprehensive analysis of factors influencing encoder choice for specific target domains.
- What evidence would resolve it: A systematic study comparing performance of various encoder architectures (e.g., Transformers, Graph Neural Networks) across different target domains, considering factors like dataset size, complexity, and activity types.

### Open Question 3
- Question: How do different combinations of weak and strong augmentations during consistency regularization impact the performance of Cross-Domain HAR, and what is the optimal strategy for selecting augmentations for a specific target domain?
- Basis in paper: The paper discusses using consistency regularization by matching teacher and student model predictions on weakly and strongly augmented target data, but does not explore the impact of different augmentation combinations.
- Why unresolved: The paper uses a fixed set of augmentations without investigating the impact of different combinations or strategies for selecting augmentations based on target domain characteristics.
- What evidence would resolve it: A systematic study exploring the impact of different combinations of weak and strong augmentations on Cross-Domain HAR performance across various target domains, considering factors like sensor noise, activity types, and data distribution shifts.

## Limitations

- The augmentation parameters for the eight transformations are not specified, making exact reproduction difficult
- The confidence threshold of 30% for pseudo-label filtering appears arbitrary without sensitivity analysis
- The method assumes meaningful alignment between source and target activities, which may not hold for completely disjoint activity sets
- Computational cost of training the teacher model with augmented data and the student model with consistency regularization is not discussed

## Confidence

**High Confidence**: The core empirical findings showing 7-8% improvement over naive transfer learning and 20% over supervised learning are well-supported by experimental results across six benchmark datasets. The ablation study demonstrating the importance of each component is convincing.

**Medium Confidence**: The claim that the dual self-training and self-supervised approach is essential for bridging large domain gaps. While the ablation study supports this, the mechanism could benefit from more theoretical grounding.

**Low Confidence**: The assertion that this approach will generalize to domains with completely disjoint activity sets. The paper focuses on cases where some activity overlap exists, and performance in truly zero-shot scenarios is not explored.

## Next Checks

1. **Ablation of confidence threshold**: Systematically vary the pseudo-label confidence threshold (10%, 30%, 50%, 70%) and measure its impact on final F1-scores across all target datasets to determine if 30% is optimal or conservative.

2. **Extreme domain gap test**: Evaluate Cross-Domain HAR performance when source and target have completely disjoint activity sets (e.g., Mobiact activities vs. Myogym activities with no overlap) to test the limits of the adaptation mechanism.

3. **Computational complexity analysis**: Measure and compare training times and memory requirements for teacher model training with augmentation, student model training with consistency regularization, and the complete pipeline versus baseline methods.