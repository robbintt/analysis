---
ver: rpa2
title: Recurrent Multi-scale Transformer for High-Resolution Salient Object Detection
arxiv_id: '2308.03826'
source_url: https://arxiv.org/abs/2308.03826
tags:
- hrsod
- high-resolution
- images
- salient
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses high-resolution salient object detection (HRSOD)
  by introducing both a new large-scale dataset and a novel recurrent multi-scale
  transformer framework. The authors contribute HRS10K, a dataset of 10,500 high-quality
  images (2K-8K resolution) that addresses the lack of sufficient training data for
  HRSOD.
---

# Recurrent Multi-scale Transformer for High-Resolution Salient Object Detection

## Quick Facts
- arXiv ID: 2308.03826
- Source URL: https://arxiv.org/abs/2308.03826
- Reference count: 40
- Key outcome: New HRS10K dataset and RMFormer architecture achieve state-of-the-art performance on high-resolution SOD benchmarks

## Executive Summary
This paper addresses the challenge of high-resolution salient object detection (HRSOD) by introducing both a new large-scale dataset and a novel recurrent multi-scale transformer framework. The authors contribute HRS10K, a dataset of 10,500 high-quality images (2K-8K resolution) that addresses the lack of sufficient training data for HRSOD. To improve HRSOD performance, they propose the Recurrent Multi-scale Transformer (RMFormer), which uses shared transformers and multi-scale refinement to progressively enhance saliency predictions from coarse to high-resolution outputs. Their approach incorporates an Image Guided Encoder, a Dual-flow Guided Decoder, and a Pixel-wise Refiner to enhance boundary details. Experiments show that RMFormer outperforms existing methods on both high-resolution and low-resolution benchmarks, achieving significant improvements in metrics like MAE, F-measure, E-measure, and boundary accuracy.

## Method Summary
The Recurrent Multi-scale Transformer (RMFormer) framework consists of a Coarse Prediction Stage (CPS) and two Recurrent Refinement Stages (RRS1 and RRS2). The CPS generates an initial coarse prediction at low resolution using a Swin-B backbone. Each RRS refines the prediction using shared transformers, Image Guided Encoders (IGE), Dual-flow Guided Decoders (DGD), and Pixel-wise Refiners (PR). The IGE captures high-resolution information, while the DGD uses both inter-stage flow (previous stage predictions) and inner-stage flow (edge predictions) to restore high-resolution details. The PR specifically enhances boundary predictions by focusing on edge pixels and using global features for re-prediction. The model is trained using SGD with learning rate 0.001 for the backbone and 0.01 for other parts, with a batch size of 3, for 32 epochs using cosine annealing.

## Key Results
- RMFormer achieves state-of-the-art performance on HRS10K with MAE of 0.0328 and FmaxŒ≤ of 0.9498
- The method shows significant improvements on high-resolution benchmarks, outperforming existing methods by large margins
- On low-resolution benchmarks, RMFormer also achieves competitive performance, demonstrating its versatility
- The Pixel-wise Refiner improves boundary accuracy (mBA) by 2.44% compared to the baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RMFormer uses a recurrent multi-scale refinement approach to progressively improve saliency predictions from coarse to high-resolution.
- Mechanism: The framework has three stages: a Coarse Prediction Stage (CPS) and two Recurrent Refinement Stages (RRS1 and RRS2). The CPS generates an initial coarse prediction at low resolution, which then guides the refinement stages. Each RRS uses shared Transformers and multi-scale refinement architectures to enhance the high-resolution predictions, leveraging both lower-resolution predictions and high-resolution input information.
- Core assumption: Lower-resolution predictions can effectively guide higher-resolution refinements, and shared Transformers can capture both coarse and fine details.
- Evidence anchors:
  - [abstract] "high-resolution saliency maps can be generated with the guidance of lower-resolution predictions"
  - [section 4.1] "the Swin Transformer generates four feature maps with different resolutions"
  - [section 4.2] "The CPS is used recurrently, to constrain the saliency information to be consistent in different stages"
- Break condition: If the lower-resolution guidance is poor or the shared Transformer cannot capture sufficient detail, the refinement stages will not improve predictions.

### Mechanism 2
- Claim: The Image Guided Encoder (IGE) and Dual-flow Guided Decoder (DGD) enhance multi-scale feature representations for better HRSOD performance.
- Mechanism: The IGE captures high-resolution information through several Encoder Blocks (EB) that concatenate previous features with resized input images. The DGD uses both inter-stage flow (previous stage predictions) and inner-stage flow (edge predictions from previous decoder blocks) to restore high-resolution details. This dual guidance helps recover fine details lost in the initial coarse prediction.
- Core assumption: High-resolution input information and edge guidance are critical for recovering fine details in the final saliency map.
- Evidence anchors:
  - [section 4.2] "The IGE is composed of several Encoder Blocks (EB)" and "DB also takes the stage prediction Pùëó ‚àí1 ùëÜ from the previous stage as strong prior knowledge"
  - [section 4.2] "the Dual-flow Guided Decoder (DGD) to boost multi-scale representations"
- Break condition: If the high-resolution input is not effectively encoded or the dual-flow guidance is not properly implemented, the final predictions will lack detail.

### Mechanism 3
- Claim: The Pixel-wise Refiner (PR) specifically enhances boundary predictions by focusing on edge pixels and using global features for re-prediction.
- Mechanism: The PR selects feature pixels around object boundaries using edge prediction indices, generates global features through self-attention operations, and then performs pixel-wise re-prediction using both selected local features and global information. This targeted refinement corrects boundary errors that persist from low-resolution predictions.
- Core assumption: Boundary pixels are most prone to errors and benefit most from targeted refinement using both local and global context.
- Evidence anchors:
  - [section 4.2] "the Pixel-wise Refiner (PR), as shown in Figure 5" and "It can be observed that if boundary areas are wrongly predicted in the low-resolution, it is hard to correct them in later high-resolution refinements"
  - [section 4.2] "The PR consists of three units, i.e., HR Feature Pixel Selection, Global Feature Generation and Pixel-wise Re-prediction"
- Break condition: If the edge prediction is inaccurate or the global features do not provide useful context, the PR will not effectively correct boundary errors.

## Foundational Learning

- Concept: Swin Transformer architecture and how it generates multi-scale features
  - Why needed here: The paper uses Swin-B as the backbone and relies on its multi-scale feature extraction capabilities
  - Quick check question: How many feature maps does the Swin Transformer generate in this implementation, and at what resolutions relative to the input?

- Concept: Self-attention mechanisms and their role in feature enhancement
  - Why needed here: The PR module uses self-attention operations to generate enhanced global features for pixel-wise re-prediction
  - Quick check question: What is the purpose of the self-attention operation in the Global Feature Generation unit of the PR?

- Concept: Loss function design for multi-task learning (saliency and edge prediction)
  - Why needed here: The model predicts both saliency maps and edge maps, requiring appropriate loss functions for both tasks
  - Quick check question: What are the two loss functions used for supervising both saliency and edge predictions in this framework?

## Architecture Onboarding

- Component map:
  - Input: High-resolution image (1536√ó1536) and low-resolution version (384√ó384)
  - Coarse Prediction Stage: Swin-B backbone ‚Üí Coarse prediction head ‚Üí Low-res saliency and edge predictions
  - Recurrent Refinement Stages (2): IGE ‚Üí Shared CPS ‚Üí DGD with PR ‚Üí High-res saliency and edge predictions
  - Output: High-resolution saliency map with enhanced boundaries

- Critical path: Input ‚Üí CPS ‚Üí RRS1 ‚Üí RRS2 ‚Üí Output
  - The shared CPS is the critical component that ensures consistency across stages
  - The DGD with PR is critical for recovering high-resolution details

- Design tradeoffs:
  - Using shared Transformers across stages reduces parameters but may limit stage-specific optimizations
  - The PR adds computational overhead but significantly improves boundary accuracy
  - Multi-scale refinement provides better results but increases inference time

- Failure signatures:
  - Poor boundary predictions despite good overall saliency: PR module may not be working correctly
  - Inconsistent predictions across stages: Shared CPS may not be properly constraining the refinement
  - Blurry high-resolution outputs: IGE may not be effectively capturing high-resolution information

- First 3 experiments:
  1. Test the CPS alone on high-resolution images to establish baseline performance
  2. Add one RRS stage and compare against CPS-only to verify refinement effectiveness
  3. Enable the PR module and measure improvement in boundary accuracy metrics (mBA)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed HRS10K dataset's diverse subject distribution affect the generalization of high-resolution salient object detection models?
- Basis in paper: [explicit] The paper discusses the creation of HRS10K with 7 subjects (Stuff, Plant, Person, Object, Mobile, Food, Animal) to address performance bias from previous datasets.
- Why unresolved: The paper mentions the dataset's diversity but does not provide quantitative analysis of how this diversity improves model generalization across different object types.
- What evidence would resolve it: Comparative experiments showing model performance on different object categories within HRS10K and other datasets, demonstrating the impact of subject diversity.

### Open Question 2
- Question: What is the impact of the edge information on the final saliency detection results, and how can the model be improved without relying on edge predictions?
- Basis in paper: [explicit] The paper includes ablation studies on edge information and proposes a PR module for boundary refinement.
- Why unresolved: While the paper shows edge information improves results, it does not explore alternative methods for boundary refinement that don't rely on explicit edge predictions.
- What evidence would resolve it: Experiments comparing models with and without edge information, using different boundary refinement techniques, and evaluating their impact on final performance.

### Open Question 3
- Question: How can the computational efficiency of the proposed RMFormer be improved while maintaining its performance advantages?
- Basis in paper: [explicit] The paper acknowledges that RMFormer has higher computational cost (563.14G MACs) compared to other methods.
- Why unresolved: The paper does not propose solutions to reduce computational complexity while preserving the model's effectiveness.
- What evidence would resolve it: Experiments testing different architectural modifications, pruning techniques, or quantization methods to reduce computational cost without significantly impacting performance.

## Limitations

- The model introduces significant computational overhead through multi-scale refinement and the Pixel-wise Refiner, which may limit practical deployment
- Reliance on a newly created dataset (HRS10K) without extensive validation on other high-resolution datasets raises questions about true generalization
- The effectiveness of shared transformers across refinement stages is assumed but not rigorously tested against stage-specific alternatives

## Confidence

- **High Confidence**: The mechanism of progressive refinement from coarse to high-resolution predictions is well-supported by the architecture description and experimental results
- **Medium Confidence**: The assertion that the HRS10K dataset significantly improves HRSOD research is plausible given its scale and quality, but requires validation through broader community adoption
- **Low Confidence**: The claim that shared transformers are optimal for multi-stage refinement lacks comparison with stage-specific transformer variants

## Next Checks

1. Cross-dataset validation: Test RMFormer on high-resolution images from other SOD datasets (e.g., DUTS, ECSSD) to verify generalization beyond HRS10K
2. Ablation on shared transformers: Compare shared transformer performance against stage-specific transformers to validate the architectural efficiency claim
3. Computational efficiency analysis: Measure actual inference time and memory usage on high-resolution images to assess practical deployment feasibility