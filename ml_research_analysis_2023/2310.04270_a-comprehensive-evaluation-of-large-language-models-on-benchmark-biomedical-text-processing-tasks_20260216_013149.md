---
ver: rpa2
title: A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical
  Text Processing Tasks
arxiv_id: '2310.04270'
source_url: https://arxiv.org/abs/2310.04270
tags:
- biomedical
- llms
- tasks
- text
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study comprehensively evaluates four popular large language
  models (LLMs) - GPT-3.5, PaLM-2, Claude-2, and LLaMA-2 - on 26 biomedical text processing
  benchmark datasets across six diverse tasks: named entity recognition, relation
  extraction, entity linking, text classification, question answering, and text summarization.
  The results show that while LLMs generally underperform fine-tuned biomedical models
  on tasks with large training sets, they can even outperform them on datasets with
  limited training data.'
---

# A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks

## Quick Facts
- arXiv ID: 2310.04270
- Source URL: https://arxiv.org/abs/2310.04270
- Authors: 
- Reference count: 35
- Key outcome: This study comprehensively evaluates four popular large language models (LLMs) - GPT-3.5, PaLM-2, Claude-2, and LLaMA-2 - on 26 biomedical text processing benchmark datasets across six diverse tasks: named entity recognition, relation extraction, entity linking, text classification, question answering, and text summarization. The results show that while LLMs generally underperform fine-tuned biomedical models on tasks with large training sets, they can even outperform them on datasets with limited training data. Notably, PaLM-2 achieved the highest F1 score (54.30) in relation extraction on the BC5CDR dataset, surpassing the previous state-of-the-art BioGPT model. However, no single LLM consistently outperformed others across all tasks, highlighting the importance of task-specific model selection. The findings demonstrate the potential of LLMs as valuable tools for biomedical tasks lacking large annotated datasets, while also revealing their current limitations compared to fine-tuned domain-specific models.

## Executive Summary
This paper presents a comprehensive evaluation of four popular large language models (LLMs) - GPT-3.5, PaLM-2, Claude-2, and LLaMA-2 - on 26 benchmark biomedical text processing tasks across six task types: named entity recognition, relation extraction, entity linking, text classification, question answering, and text summarization. The study focuses on zero-shot evaluation, meaning the LLMs are tested without any fine-tuning or domain-specific adaptation. The researchers manually designed prompts for each task and dataset, and evaluated the LLMs using standard evaluation metrics such as F1 score, precision, recall, accuracy, ROUGE, BERTScore, and Recall@1. The results show that while LLMs generally underperform fine-tuned biomedical models on tasks with large training sets, they can even outperform them on datasets with limited training data. The study highlights the importance of prompt design in achieving optimal performance and demonstrates the potential of LLMs as valuable tools for biomedical tasks lacking large annotated datasets.

## Method Summary
The study evaluates four large language models (GPT-3.5, PaLM-2, Claude-2, and LLaMA-2-13B) on 26 biomedical text processing benchmark datasets across six task types: named entity recognition, relation extraction, entity linking, text classification, question answering, and text summarization. The evaluation uses a zero-shot approach, where the LLMs are tested without any fine-tuning or domain-specific adaptation. Task-specific prompts are manually designed for each dataset based on examples from the original papers. For tasks with parsable outputs, automatic evaluation is performed using standard metrics (F1, precision, recall, accuracy, ROUGE, BERTScore, Recall@1). For non-parsable tasks like summarization, human evaluation is conducted by three annotators. The study compares LLM performance against previous state-of-the-art fine-tuned biomedical models where available.

## Key Results
- PaLM-2 achieved the highest F1 score (54.30) in relation extraction on the BC5CDR dataset, surpassing the previous state-of-the-art BioGPT model.
- LLMs generally underperform fine-tuned biomedical models on tasks with large training sets but can outperform them on datasets with limited training data.
- No single LLM consistently outperformed others across all tasks, highlighting the importance of task-specific model selection.
- GPT-3.5 showed the most balanced performance across tasks, while LLaMA-2, despite being the smallest model, demonstrated competitive results in certain tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs leverage their large-scale pretraining to perform well on biomedical tasks even without fine-tuning.
- Mechanism: Zero-shot LLMs use their broad pretraining knowledge to recognize and handle biomedical concepts, especially in low-resource settings.
- Core assumption: Pretraining on diverse corpora includes sufficient biomedical vocabulary and context for LLMs to understand domain-specific tasks.
- Evidence anchors:
  - [abstract]: "This suggests that pretraining on large text corpora makes LLMs quite specialized even in the biomedical domain."
  - [section 4.1]: "Our findings demonstrate that LLMs are sensitive to prompts, as variations in prompts led to a noticeable difference in results."
- Break Condition: If biomedical concepts are too specialized or underrepresented in pretraining data, zero-shot performance degrades significantly.

### Mechanism 2
- Claim: Task-specific prompt design significantly impacts LLM performance on biomedical tasks.
- Mechanism: Constructing detailed prompts with explicit instructions and domain-specific context improves the model's ability to generate accurate outputs.
- Core assumption: LLMs can follow detailed instructions and benefit from explicit context in prompts.
- Evidence anchors:
  - [abstract]: "However, the evaluation of LLMs in the biomedical domain would require a proper understanding of the complex linguistic characteristics of biomedical texts."
  - [section 4.1]: "For biomedical tasks, the effective construction of prompts is important to best utilize these LLMs in biomedical applications."
- Break Condition: If prompts are too vague or missing critical domain context, model performance drops.

### Mechanism 3
- Claim: LLMs perform better on tasks with smaller training datasets compared to fine-tuned models.
- Mechanism: In low-resource settings, zero-shot LLMs outperform fine-tuned models due to their ability to generalize from pretraining.
- Core assumption: The generalization ability of LLMs is stronger than the specialization of fine-tuned models in small datasets.
- Evidence anchors:
  - [abstract]: "Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art fine-tuned biomedical models."
  - [section 5.2]: "These findings suggest that LLMs can be useful in low-resource biomedical tasks."
- Break Condition: If the dataset is large enough to allow fine-tuning, specialized models outperform LLMs.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: Understanding zero-shot learning is crucial to grasp how LLMs can perform biomedical tasks without fine-tuning.
  - Quick check question: What is the difference between zero-shot and few-shot learning?

- Concept: Prompt engineering
  - Why needed here: Prompt engineering is essential for optimizing LLM performance on specific biomedical tasks.
  - Quick check question: How does prompt design influence the output quality of LLMs?

- Concept: Biomedical text processing
  - Why needed here: Familiarity with biomedical text processing tasks (e.g., NER, relation extraction) is necessary to understand the evaluation context.
  - Quick check question: What are the common tasks in biomedical text processing?

## Architecture Onboarding

- Component map:
  - LLM Models: GPT-3.5, PaLM-2, Claude-2, LLaMA-2
  - Tasks: NER, Entity Linking, Relation Extraction, Text Classification, Question Answering, Text Summarization
  - Datasets: 26 benchmark biomedical datasets
  - Evaluation Metrics: Precision, Recall, F1, ROUGE, BERTScore, Accuracy

- Critical path:
  1. Construct task-specific prompts
  2. Input prompts to LLMs
  3. Generate responses
  4. Evaluate responses using appropriate metrics

- Design tradeoffs:
  - Zero-shot vs. fine-tuned models: Zero-shot is more flexible but may underperform in high-resource settings.
  - Prompt complexity: Detailed prompts improve performance but increase design effort.
  - Model size vs. cost: Larger models may perform better but are more expensive to run.

- Failure signatures:
  - Poor performance on tasks requiring deep domain expertise
  - Inconsistent results across different datasets
  - Over-reliance on prompt quality for performance

- First 3 experiments:
  1. Evaluate a simple zero-shot prompt on a small biomedical dataset.
  2. Compare performance with and without detailed prompt instructions.
  3. Test different LLMs on the same task to identify the best model for that task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can prompt design be optimized to maximize the performance of large language models across different biomedical tasks?
- Basis in paper: [explicit] The paper demonstrates that LLMs are sensitive to prompts and that variations in prompts led to noticeable differences in results.
- Why unresolved: While the paper shows that descriptive prompts are helpful, it does not provide a systematic approach to optimizing prompts for different tasks.
- What evidence would resolve it: A comprehensive study evaluating various prompt engineering techniques (e.g., different prompt structures, instructions, examples) on a wide range of biomedical tasks and datasets, demonstrating the impact on LLM performance.

### Open Question 2
- Question: Can fine-tuning open-source LLMs like LLaMA-2 on biomedical datasets significantly improve their performance compared to existing fine-tuned domain-specific models?
- Basis in paper: [explicit] The paper suggests that fine-tuning open-source LLMs like LLaMA-2 could potentially outperform existing fine-tuned state-of-the-art models in various biomedical tasks.
- Why unresolved: The paper only evaluates zero-shot capabilities of LLMs and does not investigate the impact of fine-tuning on their performance.
- What evidence would resolve it: A comparative study evaluating the performance of fine-tuned open-source LLMs (e.g., LLaMA-2) on biomedical tasks against existing fine-tuned domain-specific models (e.g., BioBERT, BioGPT, BioBART).

### Open Question 3
- Question: What are the ethical implications of using large language models in the biomedical domain, particularly regarding privacy concerns?
- Basis in paper: [explicit] The paper mentions the need to study the ethical implications of using LLMs in the biomedical domain, specifically mentioning privacy concerns.
- Why unresolved: The paper does not delve into the specific ethical considerations or potential risks associated with using LLMs in healthcare and biomedical research.
- What evidence would resolve it: A thorough analysis of the ethical implications of LLM usage in the biomedical domain, including privacy risks, biases, and potential harm to patients, along with proposed mitigation strategies.

## Limitations
- The evaluation relies on zero-shot prompting without any fine-tuning or adaptation of the models to biomedical domains.
- The manual prompt design process introduces potential subjectivity and may not represent optimal prompt configurations.
- The study's reliance on publicly available datasets may not fully capture the complexity of real-world biomedical text processing challenges.

## Confidence

**High Confidence**: The finding that LLMs generally underperform fine-tuned biomedical models on tasks with large training sets is well-supported by the empirical results across multiple datasets and tasks.

**Medium Confidence**: The observation that LLMs can outperform fine-tuned models on low-resource tasks is supported by the data, but the specific conditions under which this occurs warrant further investigation with additional datasets.

**Medium Confidence**: The conclusion that no single LLM consistently outperforms others across all tasks is based on comprehensive testing, though the evaluation is limited to the four specific models examined.

## Next Checks

1. **Prompt Optimization Study**: Conduct a systematic ablation study on prompt variations to determine the sensitivity of each LLM to different prompt formulations and identify optimal prompt structures for each task type.

2. **Few-shot Learning Evaluation**: Evaluate the same LLMs using few-shot learning approaches with 5-10 examples per task to determine if this bridges the performance gap between zero-shot and fine-tuned models.

3. **Domain Adaptation Analysis**: Test whether lightweight domain adaptation (e.g., continued pretraining on biomedical corpora) improves LLM performance on high-resource tasks while maintaining their advantages on low-resource tasks.