---
ver: rpa2
title: 'Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge
  Adapting over Heterogeneous Sources'
arxiv_id: '2305.13269'
source_url: https://arxiv.org/abs/2305.13269
tags:
- language
- knowledge
- query
- question
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents Chain-of-Knowledge (CoK), a framework that
  enhances large language models (LLMs) by dynamically incorporating grounding information
  from heterogeneous sources, particularly structured knowledge bases like Wikidata.
  The core method involves three stages: reasoning preparation, dynamic knowledge
  adapting, and answer consolidation.'
---

# Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources

## Quick Facts
- arXiv ID: 2305.13269
- Source URL: https://arxiv.org/abs/2305.13269
- Reference count: 17
- Key outcome: CoK framework improves LLM factual accuracy by 6.5% on AdvHotpotQA through dynamic knowledge adaptation from structured sources

## Executive Summary
Chain-of-Knowledge (CoK) is a framework that enhances large language models by dynamically incorporating grounding information from heterogeneous sources, particularly structured knowledge bases like Wikidata. The core method involves three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. CoK uses a query generator model with contrastive instruction-tuning to produce accurate knowledge base queries in formats like SPARQL and SQL. The framework breaks down complex questions into sub-questions, retrieves relevant facts from knowledge bases, and guides the LLM to generate more factual answers. Experiments show CoK significantly improves factual correctness and reduces hallucination in LLMs across knowledge-intensive tasks.

## Method Summary
The Chain-of-Knowledge framework processes natural language questions through a three-stage pipeline: (1) reasoning preparation where complex questions are decomposed into sub-questions, (2) dynamic knowledge adapting where a query generator model translates sub-questions into structured queries to retrieve relevant facts from knowledge bases, and (3) answer consolidation where the LLM synthesizes retrieved information to generate final answers. The query generator uses contrastive instruction-tuning to learn the difference between valid and invalid queries, improving accuracy when querying structured data sources like Wikidata. The framework is designed to be modular, allowing easy adaptation to various knowledge sources and LLMs.

## Key Results
- CoK achieves 6.5% improvement in exact match accuracy on AdvHotpotQA compared to standard prompting methods
- Framework significantly reduces hallucination in LLMs by grounding outputs with structured knowledge from knowledge bases
- Modular design allows easy adaptation to various knowledge sources and LLM architectures
- Three-stage pipeline enables effective multi-hop reasoning over structured knowledge bases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoK reduces hallucination by grounding LLM outputs with structured knowledge from external knowledge bases
- Mechanism: The framework retrieves relevant facts from structured sources like Wikidata using SPARQL/ SQL queries, then guides the LLM to generate answers based on these facts rather than relying solely on internal training data
- Core assumption: Structured knowledge bases contain more reliable factual information than unstructured web documents
- Evidence anchors:
  - [abstract] "CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation" and "Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information"
  - [section] "Compared to web documents which may contain irrelevant or conflicting information, knowledge bases offer direct factual statements"
- Break condition: The knowledge base becomes outdated or contains errors, or the LLM ignores the retrieved facts during answer generation

### Mechanism 2
- Claim: The query generator model improves accuracy of knowledge base queries through contrastive instruction tuning
- Mechanism: The model is trained with both correct queries and artificially generated incorrect queries, learning to differentiate between valid and invalid query patterns
- Core assumption: LLMs are not optimized for structured query formats and need specialized training to generate accurate queries
- Evidence anchors:
  - [abstract] "we propose a query generator model with contrastive instruction-tuning" and "to ensure that the query generator can produce effective queries, we propose a contrastive instruction tuning method to differentiate between valid and invalid queries"
  - [section] "As LLMs are not optimized for querying structured data such as knowledge bases, we finetune a query generator model that translates a natural question into a structured query"
- Break condition: The contrastive training data becomes unrepresentative of real query patterns, or the model overfits to the training distribution

### Mechanism 3
- Claim: The three-stage framework (reasoning preparation → dynamic knowledge adapting → answer consolidation) enables effective multi-hop reasoning over knowledge bases
- Mechanism: Complex questions are broken down into sub-questions, relevant knowledge is retrieved for each sub-question, and the LLM synthesizes information from multiple knowledge facts to answer the original question
- Core assumption: Breaking down complex reasoning tasks into simpler sub-tasks improves overall accuracy and enables better utilization of retrieved knowledge
- Evidence anchors:
  - [abstract] "CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains"
  - [section] "we instruct the LLM to generate a chain of sub-questions which can be answered more easily" and "the LLM is guided by the retrieved knowledge facts to answer the original question"
- Break condition: The sub-question decomposition becomes too complex, leading to error propagation, or the LLM fails to properly synthesize information from multiple knowledge facts

## Foundational Learning

- Concept: Structured Query Languages (SPARQL, SQL)
  - Why needed here: The query generator must translate natural language questions into these structured formats to retrieve information from knowledge bases
  - Quick check question: What is the key difference between SPARQL and SQL when querying knowledge bases?

- Concept: Contrastive Learning
  - Why needed here: The query generator uses contrastive instruction tuning to learn the difference between valid and invalid queries
  - Quick check question: How does contrastive learning differ from traditional supervised learning in this context?

- Concept: Chain-of-Thought Reasoning
  - Why needed here: Complex questions require breaking down into sub-questions before knowledge retrieval can be effective
  - Quick check question: What are the potential failure modes when decomposing complex questions into sub-questions?

## Architecture Onboarding

- Component map: Query Generator (trained separately) → Knowledge Base Retriever → LLM (frozen) → Answer Consolidator
- Critical path: Natural Question → Query Generator → Structured Query → Knowledge Base → Retrieved Facts → LLM with Facts → Final Answer
- Design tradeoffs: Using frozen LLM maximizes modularity but limits fine-tuning capabilities; using structured knowledge bases improves reliability but requires query generation
- Failure signatures: Incorrect query generation leading to irrelevant knowledge retrieval; LLM ignoring retrieved facts; error propagation through sub-question decomposition
- First 3 experiments:
  1. Test query generator accuracy on held-out question-query pairs
  2. Evaluate end-to-end performance on simple fact verification tasks
  3. Measure improvement from adding contrastive training to the query generator

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CoK vary when applied to different types of knowledge bases (e.g., Wikidata vs. proprietary databases)?
- Basis in paper: [explicit]
- Why unresolved: The paper mentions that CoK can be adapted to various knowledge sources, but does not provide comparative results across different types of knowledge bases.
- What evidence would resolve it: Conducting experiments using CoK with different types of knowledge bases and comparing the performance results.

### Open Question 2
- Question: What is the impact of the query generator model's size on the overall performance of CoK?
- Basis in paper: [inferred]
- Why unresolved: The paper uses LLaMA-7B as the base model for the query generator but does not explore the effects of using different model sizes.
- What evidence would resolve it: Training and evaluating CoK with query generator models of various sizes and comparing the results.

### Open Question 3
- Question: How does CoK perform on real-world applications outside of the controlled experimental settings?
- Basis in paper: [explicit]
- Why unresolved: The paper focuses on controlled experiments with specific datasets, but does not discuss the application of CoK in real-world scenarios.
- What evidence would resolve it: Implementing CoK in real-world applications and measuring its performance and effectiveness in practical use cases.

### Open Question 4
- Question: How does the contrastive instruction-tuning method in the query generator model affect its ability to handle ambiguous or complex queries?
- Basis in paper: [explicit]
- Why unresolved: The paper introduces contrastive instruction-tuning for the query generator but does not delve into its effectiveness in handling complex or ambiguous queries.
- What evidence would resolve it: Testing the query generator model with a variety of complex and ambiguous queries and analyzing its performance.

### Open Question 5
- Question: What are the potential limitations or drawbacks of using structured knowledge bases compared to unstructured text sources in CoK?
- Basis in paper: [inferred]
- Why unresolved: The paper highlights the benefits of using structured knowledge bases but does not discuss any potential limitations or drawbacks.
- What evidence would resolve it: Conducting a comparative analysis of CoK's performance and limitations when using structured vs. unstructured knowledge sources.

## Limitations
- The paper does not provide detailed ablation studies to quantify the individual contributions of each component
- Limited evaluation on diverse knowledge domains beyond the tested datasets
- No comparison against other knowledge-grounding approaches that use unstructured sources

## Confidence
- High confidence: Effectiveness of structured knowledge bases for reducing hallucination compared to unstructured sources
- Medium confidence: Specific benefits of contrastive instruction tuning for query generation
- Medium confidence: Three-stage framework's superiority for multi-hop reasoning

## Next Checks
1. Conduct controlled experiments comparing CoK's performance using structured vs. unstructured knowledge sources on the same tasks
2. Perform ablation studies to isolate the impact of contrastive training on query generator accuracy
3. Test the framework's generalization across diverse knowledge domains and query types not present in the training data