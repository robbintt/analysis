---
ver: rpa2
title: 'LLMaAA: Making Large Language Models as Active Annotators'
arxiv_id: '2310.19596'
source_url: https://arxiv.org/abs/2310.19596
tags:
- data
- learning
- language
- llms
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes LLM AAA, a framework that employs large language
  models (LLMs) as active annotators to address the challenge of data scarcity in
  natural language processing (NLP) tasks. LLM AAA integrates active learning into
  the LLM annotation process, enabling the LLM to efficiently annotate more informative
  instances that benefit model performance.
---

# LLMaAA: Making Large Language Models as Active Annotators

## Quick Facts
- **arXiv ID**: 2310.19596
- **Source URL**: https://arxiv.org/abs/2310.19596
- **Reference count**: 40
- **Primary result**: LLM AAA framework enables task-specific models to outperform teacher LLMs within hundreds of annotated examples on NER and RE tasks.

## Executive Summary
This paper proposes LLM AAA, a framework that employs large language models as active annotators to address data scarcity in NLP tasks. The framework integrates active learning with LLM annotation, using uncertainty-based acquisition, k-NN prompt engineering, and automatic reweighting to produce high-quality task-specific models. Experiments show LLM AAA achieves superior performance with significantly fewer annotated examples compared to traditional approaches, making it a cost-effective solution for leveraging LLMs in practical scenarios.

## Method Summary
LLM AAA employs a three-component framework: (1) an LLM annotator enhanced with k-NN example retrieval and label verbalization for improved annotation quality, (2) an active acquisition module using uncertainty and diversity metrics to select the most informative samples for annotation, and (3) a robust training process with automatic reweighting to handle noisy labels. The framework operates iteratively, with the active module selecting samples from a large unlabeled pool, the annotator labeling them using carefully engineered prompts, and the training module updating the task-specific model while learning sample weights to minimize validation loss.

## Key Results
- LLM AAA outperforms previous approaches on NER and RE tasks, achieving better performance with significantly fewer annotated examples
- Task-specific models trained from LLM-generated labels can outperform their teacher LLMs within hundreds of annotated examples
- The framework demonstrates efficiency gains by focusing annotation effort on the most informative instances through uncertainty-based active learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Active learning with uncertainty-based acquisition enables LLMs to focus annotation effort on samples that provide the most learning signal for the task-specific model
- **Mechanism**: The LLM annotator uses uncertainty metrics (entropy or least confidence) to select which unlabeled samples to label next, prioritizing those where the model is most uncertain. This creates a curriculum of increasingly challenging examples that maximizes information gain per annotation
- **Core assumption**: The model's uncertainty estimates correlate with the actual difficulty and informativeness of samples for improving performance
- **Evidence anchors**:
  - [abstract]: "LLM AAA enables the LLM to annotate more informative instances that benefit model performance"
  - [section 3.2]: "uncertainty-based methods leverage model predictions to select hard examples"
  - [corpus]: "Found 25 related papers... Top related titles: ActiveLab: Active Learning with Re-Labeling by Multiple Annotators"
- **Break condition**: If the model's uncertainty estimates become decoupled from actual sample difficulty (e.g., due to noisy labels or distribution shift), the active selection becomes ineffective

### Mechanism 2
- **Claim**: Prompt engineering with k-NN example retrieval and label verbalization improves annotation quality by providing contextual guidance
- **Mechanism**: Before generating labels, the LLM retrieves semantically similar examples from a demonstration pool to include in-context, and uses natural language descriptions of labels rather than raw category names. This provides richer semantic context for the LLM to ground its annotations
- **Core assumption**: Semantically similar examples and natural language label descriptions help the LLM better understand task requirements and produce higher-quality annotations
- **Evidence anchors**:
  - [section 3.1]: "we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting technique to assign training samples with learnable weights"
  - [section 5.3.1]: "Without k-NN example retrieval module (i.e. in zero-shot manners), the LLM annotator is unable to extract entities well in NER task, shown by a drastic drop in F1 scores (21% on OntoNotes and 25% on CoNLL)"
  - [corpus]: "Found 25 related papers... Average neighbor FMR=0.386"
- **Break condition**: If the demonstration pool lacks relevant examples or if the label verbalization templates are poorly designed, the prompt engineering may not improve (or could even harm) annotation quality

### Mechanism 3
- **Claim**: Automatic reweighting allows robust learning from noisy labels by adapting sample importance during training
- **Mechanism**: The training process learns sample weights that minimize validation loss on a small clean validation set, giving higher importance to samples that help the model generalize while downweighting noisy annotations
- **Core assumption**: A small clean validation set provides a reasonable proxy for the true data distribution, and the model can learn to distinguish reliable from unreliable samples
- **Evidence anchors**:
  - [abstract]: "we adopt automatic reweighting to assign learnable weights to training samples, reducing the impact of noisy annotations"
  - [section 5.3.3]: "automatic reweighting plays a crucial role when the LLM annotators are relatively poor (as in OntoNotes and Re-TACRED)"
  - [corpus]: "Found 25 related papers... Average citations=0.0"
- **Break condition**: If the validation set is too small or unrepresentative, or if the reweighting process becomes unstable, it may not effectively mitigate label noise

## Foundational Learning

- **Concept**: Active learning pool-based paradigm
  - Why needed here: The framework relies on iteratively selecting and annotating samples from a large unlabeled pool, which is the standard active learning setting
  - Quick check question: What are the key differences between pool-based and stream-based active learning, and why is pool-based more suitable for LLM annotation scenarios?

- **Concept**: In-context learning and prompt engineering
  - Why needed here: The LLM annotator operates entirely through prompting without fine-tuning, making prompt design critical for performance
  - Quick check question: How do different prompting strategies (few-shot vs zero-shot, chain-of-thought vs direct instruction) affect LLM performance on structured prediction tasks?

- **Concept**: Noisy label learning and sample reweighting
  - Why needed here: LLM-generated labels are inherently noisy, requiring techniques to train models robustly despite label errors
  - Quick check question: What are the theoretical guarantees (if any) for sample reweighting techniques when dealing with arbitrary label noise?

## Architecture Onboarding

- **Component map**: LLM annotator (with k-NN retrieval and label verbalization) -> Active acquisition module (uncertainty/diversity metrics) -> Robust training with automatic reweighting -> Validation set evaluation
- **Critical path**: The core loop is: unlabeled pool → active selection → LLM annotation → training with reweighting → validation → repeat. Each iteration produces a stronger task-specific model and a larger labeled dataset.
- **Design tradeoffs**: The framework trades off annotation cost (number of samples to label) against model performance. Using uncertainty-based acquisition reduces cost but requires reliable uncertainty estimates. Prompt engineering improves quality but adds complexity. Reweighting handles noise but needs clean validation data.
- **Failure signatures**: Performance plateaus despite more annotations (active acquisition failing), systematic annotation errors on certain patterns (prompt engineering issues), or model overfitting to noisy labels despite reweighting (validation set issues).
- **First 3 experiments**:
  1. Implement the basic active learning loop with random acquisition to establish baseline performance
  2. Add uncertainty-based acquisition to measure efficiency gains
  3. Incorporate prompt engineering with k-NN examples to evaluate quality improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM AAA scale with different pool sizes of unlabeled data?
- Basis in paper: Inferred - The paper mentions using "massive unlabeled data" and a "large pool of unlabeled data Dpool" but doesn't explore how performance varies with pool size.
- Why unresolved: The experiments use the full training sets as Dpool but don't systematically vary or analyze the impact of different pool sizes.
- What evidence would resolve it: Experiments showing LLM AAA performance across different Dpool sizes (e.g., 1k, 10k, 100k samples) while keeping other parameters constant.

### Open Question 2
- Question: What is the impact of demonstration set size on LLM AAA's performance?
- Basis in paper: Explicit - The paper states "We randomly sample 100 examples from the original validation sets as gold data, reusing the same set for demonstration Ddemo and validation Dval" but doesn't explore different sizes.
- Why unresolved: Only one demonstration set size (100 examples) is tested, leaving questions about the minimum effective size and optimal trade-off with validation data.
- What evidence would resolve it: Experiments varying the demonstration set size (e.g., 10, 50, 100, 200 examples) while measuring the resulting model performance.

### Open Question 3
- Question: How does the k-NN retrieval parameter k affect annotation quality and downstream model performance?
- Basis in paper: Explicit - The paper states "We set k to 5 for all experiments" but doesn't explore the sensitivity to this hyperparameter.
- Why unresolved: The choice of k=5 appears arbitrary and the paper doesn't analyze how different values affect results.
- What evidence would resolve it: Experiments varying k (e.g., 1, 3, 5, 10, 20) and measuring both LLM annotation accuracy and final TAM performance.

### Open Question 4
- Question: How does the choice of active acquisition strategy vary across different NLP tasks?
- Basis in paper: Inferred - The paper compares multiple acquisition strategies but only on NER and RE tasks.
- Why unresolved: Results are task-specific and may not generalize to other NLP tasks like text classification, summarization, or question answering.
- What evidence would resolve it: Applying LLM AAA to diverse NLP tasks and comparing active acquisition strategy effectiveness across domains.

### Open Question 5
- Question: What is the theoretical limit of student model performance relative to teacher LLM capability?
- Basis in paper: Explicit - The paper discusses this in Section 6.2, providing a theoretical analysis but acknowledging it's "much-relaxed" and "leaves rigorous theoretical analysis for future work."
- Why unresolved: The paper provides intuition but no formal bounds on the gap between teacher and student performance.
- What evidence would resolve it: Mathematical analysis establishing upper bounds on TAM performance given teacher LLM accuracy and training data constraints.

## Limitations

- Empirical validation is limited to two NLP tasks (NER and RE) on specific datasets, raising questions about generalizability to other domains and task types
- The framework assumes access to a demonstration pool for k-NN retrieval, but doesn't adequately address how to construct or maintain this pool for new domains
- The automatic reweighting mechanism requires a clean validation set, but the paper doesn't explore how sensitive the approach is to validation set quality or size

## Confidence

- **High confidence**: The core mechanisms of active learning with uncertainty-based acquisition and sample reweighting are well-established techniques. The observation that prompt engineering with k-NN examples improves LLM annotation quality is supported by multiple experiments.
- **Medium confidence**: The claim that task-specific models can outperform teacher LLMs within hundreds of examples needs further validation across more diverse tasks and datasets. The efficiency gains relative to traditional active learning approaches are promising but may be task-dependent.
- **Low confidence**: The scalability analysis to larger datasets and more complex tasks is minimal, and the paper doesn't adequately address potential failure modes when the LLM annotator's capabilities are mismatched to the task difficulty.

## Next Checks

1. Test the framework on a broader range of NLP tasks (e.g., text classification, question answering) and domains (e.g., biomedical, legal) to assess generalizability beyond NER and RE.
2. Conduct ablation studies systematically varying the size and quality of the demonstration pool and validation set to quantify their impact on performance and identify failure thresholds.
3. Implement a cost-benefit analysis comparing total computational resources (annotation + training) against performance gains to evaluate practical deployment considerations.