---
ver: rpa2
title: 'Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization
  Approach'
arxiv_id: '2312.11865'
source_url: https://arxiv.org/abs/2312.11865
tags:
- count
- units
- starcraft
- game
- protoss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Chain of Summarization method for large
  language model (LLM) agents to play StarCraft II. It proposes a textual environment,
  TextStarCraft II, where LLMs can interact with game data.
---

# Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach

## Quick Facts
- arXiv ID: 2312.11865
- Source URL: https://arxiv.org/abs/2312.11865
- Authors: 
- Reference count: 40
- Key outcome: Introduces Chain of Summarization method enabling LLM agents to defeat built-in StarCraft II AI at harder difficulty levels through efficient text-based gameplay

## Executive Summary
This paper presents a novel approach for large language models to play StarCraft II through a Chain of Summarization (CoS) method. The authors create a textual environment called TextStarCraft II where game observations are converted into text that LLMs can process. The CoS method compresses raw observations through single-frame and multi-frame summarization, enabling efficient decision-making by reducing the frequency of LLM queries while maintaining strategic coherence. Experiments demonstrate that this approach allows LLM agents to defeat the built-in AI at harder difficulty levels while providing interpretable decision-making processes.

## Method Summary
The Chain of Summarization method converts StarCraft II gameplay into a text-based environment where LLMs can interact with game data. The approach involves two levels of summarization: single-frame summarization distills raw observations into structured information about resources, units, and buildings, while multi-frame summarization aggregates K frames into a period summary. The LLM then performs Chain of Thought reasoning through six phases - Situation Overview, Analysis, Strategic Planning, Opponent Strategy Analysis, Recommendations, and Decision-Making - to generate actions. These actions are mapped back to valid StarCraft II commands through an action extractor, with a queue buffering sequential execution.

## Key Results
- LLM agents using Chain of Summarization defeated built-in AI at harder difficulty levels
- The method reduces API call frequency by aggregating K observations before querying the LLM
- Human experts (30 grandmaster-level players) evaluated the strategic behaviors and decision-making interpretability of LLM agents
- Various model sizes tested including GPT-3.5, GPT-4, Claude2, and Bard showed different performance levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain of Summarization reduces per-step LLM inference burden while preserving strategic coherence.
- Mechanism: Multi-frame summarization aggregates K observations into one structured summary, which the LLM then reasons over once per K frames instead of K times.
- Core assumption: The aggregated summary captures all salient game state changes over K frames without losing time-sensitive details.
- Evidence anchors:
  - [abstract] "instead of querying LLMs at each time step... aggregate the observations for K steps and generate K actions next corresponding to each time step by querying LLMs one time."
  - [section 4.1] "Every K frames... the level-1 summarized observations... are aggregated into a period summarization... the CoT reasoning is then performed to process the complex reasoning for decision-making."
  - [corpus] Weak: No direct corpus evidence of similar aggregation patterns in other RTS environments.
- Break condition: If K is too large, critical in-game events between summaries are missed, causing poor real-time responses.

### Mechanism 2
- Claim: Single-frame summarization converts raw observation text into compact, LLM-friendly structured information.
- Mechanism: Rule-based or LLM-based extraction filters noisy text into a distilled key-value representation of resources, units, buildings, and enemy data.
- Core assumption: The summarization rules preserve all decision-relevant variables while removing irrelevant verbosity.
- Evidence anchors:
  - [abstract] "single-frame summarization for processing raw observations"
  - [section 4.1] "single-frame summarization distills key information from raw data to help the LLM agent quickly grasp the current status of the game."
  - [corpus] Weak: No corpus evidence of text-based RTS observation summarization in prior work.
- Break condition: If summarization rules drop variables essential for strategy (e.g., supply count, enemy unit composition), the LLM will make poor tactical choices.

### Mechanism 3
- Claim: Structured CoT prompts guide LLM through a six-phase reasoning chain that produces executable game actions.
- Mechanism: The prompt enforces Situation Overview → Situation Analysis → Strategic Planning → Opponent Strategy Analysis → Strategic Recommendations → Decision-Making, with each phase feeding into the next.
- Core assumption: The LLM has sufficient StarCraft II knowledge embedded from pretraining to fill in each phase meaningfully.
- Evidence anchors:
  - [section 4.1] "the CoT process performs below reasoning steps by step with deliberately designed prompts: 1. Situation Overview: ... 2. Situation Analysis: ... 3. Strategic Planning: ..."
  - [section 5.4] "The models evaluated were GPT-3.5, GPT-4, Claude2, and Bard... These encompass extensive textual knowledge about StarCraft II..."
  - [corpus] Weak: No corpus evidence of CoT applied to RTS games; evidence limited to simple sequential reasoning tasks.
- Break condition: If the LLM lacks relevant game knowledge, the reasoning chain produces incoherent or unactionable suggestions.

## Foundational Learning

- Concept: POMDP formulation of StarCraft II as partially observable, multi-agent decision process.
  - Why needed here: Clarifies that the agent only sees textual observations, not full game state, and must act in real-time with delayed reward signals.
  - Quick check question: What is the difference between the state space S and observation space Ω in this POMDP formulation?

- Concept: Text-to-action adapter mapping LLM-generated decisions to valid StarCraft II commands.
  - Why needed here: Ensures that LLM outputs can be translated into executable game actions; without it, reasoning is useless.
  - Quick check question: How does the action extractor E parse LLM decisions into valid StarCraft II commands?

- Concept: Chain of Summarization architecture and frame-skipping strategy.
  - Why needed here: Explains why K frames are aggregated and how this trades off latency for reasoning depth.
  - Quick check question: Why does aggregating K frames reduce API call costs and improve response quality?

## Architecture Onboarding

- Component map: TextStarCraft II environment -> Observation-to-Text adapter -> Single-frame summarizer -> Multi-frame aggregator -> LLM reasoning engine -> Action extractor -> Action queue -> Game execution

- Critical path: Observation → L1 summary → L2 aggregation → LLM reasoning → Action extraction → Action queue → Game execution

- Design tradeoffs:
  - K frame count: larger K reduces API calls but risks missing rapid events
  - Rule-based vs. LLM-based summarization: faster but less precise vs. slower but more accurate
  - Prompt length: more detailed prompts improve reasoning but risk truncation in smaller models

- Failure signatures:
  - Excessive latency: LLM responses lag behind game state changes
  - Poor decisions: Summarization drops key variables or prompts are too vague
  - Inaction: Action extractor fails to parse LLM output into valid commands

- First 3 experiments:
  1. Run LLM with K=1 (no summarization) to establish baseline latency and decision quality
  2. Vary K from 2 to 10 and measure win rate vs. built-in AI and average decision time
  3. Swap rule-based summarizer with few-shot LLM summarizer and compare action coherence and cost

## Open Questions the Paper Calls Out

- Question: How would the Chain of Summarization method perform if applied to different game genres beyond RTS, such as MOBA or FPS games?
  - Basis in paper: [inferred] The paper demonstrates effectiveness of CoS in StarCraft II but does not test generalization to other genres
  - Why unresolved: The method's effectiveness relies on specific StarCraft II mechanics and observation structures that may not translate to other genres
  - What evidence would resolve it: Experiments applying CoS to at least two other game genres with performance metrics and comparison to genre-specific baselines

- Question: What is the minimum model size required for the Chain of Summarization method to achieve competitive performance against built-in AI?
  - Basis in paper: [explicit] The paper tests various model sizes (GPT3.5, GPT4, finetuned models) but does not systematically explore the size-performance relationship
  - Why unresolved: While the paper shows some models work better than others, it doesn't establish the threshold where CoS becomes effective
  - What evidence would resolve it: A controlled study testing CoS with progressively smaller models until performance drops below baseline, with detailed analysis of bottlenecks

- Question: How does the Chain of Summarization method handle imperfect information scenarios compared to perfect information games?
  - Basis in paper: [inferred] The paper focuses on StarCraft II which has imperfect information, but doesn't compare performance against perfect information versions or analyze the impact of information uncertainty
  - Why unresolved: The method's information compression and multi-frame summarization strategies may behave differently when information is uncertain
  - What evidence would resolve it: Comparative experiments testing CoS in both perfect and imperfect information variants of StarCraft II, measuring decision quality and response times

## Limitations

- The paper does not provide the full prompt templates or examples, which are critical for reproducing the Chain of Summarization method
- Specific rule-based logic for single-frame summarization is not detailed, limiting faithful replication of the results
- Human expert evaluation for strategy and knowledge mastery is subjective and not easily reproducible or scalable

## Confidence

- **High confidence**: The paper's claim that Chain of Summarization improves LLM reasoning efficiency is supported by the mechanism description and experimental results showing wins against harder AI levels. The TextStarCraft II environment and CoS method are clearly described.

- **Medium confidence**: The claim that the CoS method accelerates LLM reasoning and enables victories at higher difficulty levels is based on experimental results, but the lack of detailed prompt templates and rule-based logic introduces uncertainty in reproducibility.

- **Low confidence**: The interpretability of LLM decision-making and the strategic behaviors observed are based on human expert evaluation, which is subjective and not easily verifiable.

## Next Checks

1. Reproduce baseline performance: Implement the TextStarCraft II environment and Chain of Summarization method with the available details. Run experiments against built-in AI at various difficulty levels to verify win rates and compare with reported results.

2. Prompt and summarization validation: Obtain or reconstruct the full prompt templates and rule-based summarization logic. Test the impact of different prompt structures and summarization strategies on decision quality and API efficiency.

3. Scalability and robustness testing: Evaluate the method's performance across different LLM models (e.g., GPT-3.5, GPT-4) and frame skip intervals (K). Measure latency, API call costs, and decision quality to assess practical deployment feasibility.