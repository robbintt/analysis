---
ver: rpa2
title: Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs
arxiv_id: '2312.05934'
source_url: https://arxiv.org/abs/2312.05934
tags:
- knowledge
- arxiv
- fine-tuning
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares two methods for injecting knowledge into pre-trained
  large language models: unsupervised fine-tuning and retrieval-augmented generation
  (RAG). The evaluation focuses on knowledge-intensive tasks across various topics,
  including anatomy, astronomy, biology, chemistry, and current events.'
---

# Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs

## Quick Facts
- arXiv ID: 2312.05934
- Source URL: https://arxiv.org/abs/2312.05934
- Reference count: 21
- Key outcome: Retrieval-augmented generation (RAG) consistently outperforms unsupervised fine-tuning for injecting knowledge into pre-trained LLMs across various knowledge-intensive tasks.

## Executive Summary
This study systematically compares two approaches for injecting knowledge into pre-trained large language models: unsupervised fine-tuning and retrieval-augmented generation (RAG). The evaluation spans multiple knowledge-intensive tasks including anatomy, astronomy, biology, chemistry, and current events. The results demonstrate that RAG consistently outperforms fine-tuning in both scenarios where the model has encountered knowledge during pre-training and when dealing with entirely new knowledge. While fine-tuning offers some improvement, it cannot match RAG's performance. The study also reveals that LLMs struggle to learn new factual information through fine-tuning, but this limitation can be partially alleviated by exposing the model to multiple variations of the same fact during training.

## Method Summary
The study evaluates knowledge injection methods using pre-trained models (Llama2-7B, Mistral-7B, Orca2-7B) across multiple tasks. For fine-tuning, models are trained on Wikipedia knowledge bases for 5 epochs with learning rates ranging from 1e-6 to 5e-5. For RAG, a bge-large-en embedding model retrieves relevant Wikipedia chunks which are appended to queries. Both approaches are evaluated on multiple-choice questions using log-likelihood accuracy. The knowledge bases are created by cleaning and chunking Wikipedia articles into 256-token segments.

## Key Results
- RAG consistently outperforms fine-tuning across all tested tasks and models
- Fine-tuning shows some improvement but is not competitive with RAG
- LLMs struggle to learn new factual information through unsupervised fine-tuning
- Data augmentation through paraphrasing during fine-tuning improves performance on new knowledge tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning struggles to learn new factual information from limited data.
- Mechanism: During fine-tuning, the model's weights adjust to minimize loss on the training data, but without sufficient exposure to variations of the same fact, the model fails to generalize or retain the new information effectively.
- Core assumption: The model's ability to learn new facts is limited by the diversity and quantity of training examples.
- Evidence anchors:
  - [abstract] "Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning..."
  - [section 6] "In order to teach pre-trained LLMs new knowledge, the knowledge must be repeated in numerous ways."
- Break condition: If the fine-tuning data contains abundant paraphrases or variations of the same fact, the model may overcome this limitation.

### Mechanism 2
- Claim: Retrieval-augmented generation (RAG) outperforms fine-tuning by providing relevant context alongside the model's knowledge.
- Mechanism: RAG uses an embedding model to retrieve documents similar to the input query and appends them to the query, giving the model additional context to draw upon when generating a response.
- Core assumption: The retrieved documents contain relevant information that the model can use to answer the question more accurately.
- Evidence anchors:
  - [abstract] "Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it..."
  - [section 3.3] "RAG uses information retrieval techniques to enable LLMs to obtain relevant information from a knowledge source and incorporate it into generated text."
- Break condition: If the retrieval model fails to find relevant documents or the documents are of poor quality, RAG's performance may degrade.

### Mechanism 3
- Claim: Data augmentation through paraphrasing improves fine-tuning performance by increasing the diversity of training examples.
- Mechanism: By generating multiple paraphrases of the same fact, the model is exposed to the fact in various forms, increasing the likelihood that the relationships in the data appear naturally and are learned by the model.
- Core assumption: The model benefits from seeing the same fact expressed in different ways, as it helps capture the underlying meaning rather than just memorizing specific sentences.
- Evidence anchors:
  - [section 6] "By providing the information in numerous forms... the various relationships in the data (e.g., a =⇒ b, b ̸=⇒ c) stand a higher chance of appearing naturally."
  - [section 6] "Our experimentation revealed a compelling trend... the accuracy was a monotonically increasing function of the number of paraphrases used."
- Break condition: If the paraphrases are not diverse enough or do not capture the full meaning of the original fact, the benefit of data augmentation may be limited.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding catastrophic forgetting is crucial for interpreting the results of fine-tuning experiments and the limitations of unsupervised fine-tuning for knowledge injection.
  - Quick check question: What happens to a model's performance on previously learned tasks when it is fine-tuned on a new task without proper regularization?

- Concept: In-context learning
  - Why needed here: RAG is a form of in-context learning, where the model uses additional information provided in the input to improve its performance on a task.
  - Quick check question: How does providing relevant context alongside a question affect a model's ability to answer the question accurately?

- Concept: Knowledge representation in LLMs
  - Why needed here: The study focuses on injecting knowledge into LLMs and evaluating their ability to learn and use this knowledge, which requires an understanding of how knowledge is represented and stored in these models.
  - Quick check question: How do LLMs store and retrieve factual knowledge, and what factors influence their ability to learn new facts?

## Architecture Onboarding

- Component map:
  Pre-trained language model (Llama2-7B, Mistral-7B, Orca2-7B) -> Unsupervised fine-tuning pipeline OR Retrieval-augmented generation pipeline -> Embedding model (bge-large-en) -> Vector store (FAISS) -> Knowledge base (Wikipedia articles) -> Evaluation framework (LM-Evaluation-Harness)

- Critical path:
  1. Prepare the knowledge base by cleaning and chunking Wikipedia articles.
  2. For fine-tuning: Train the pre-trained model on the knowledge base using unsupervised fine-tuning.
  3. For RAG: Create embeddings for the knowledge base chunks and store them in a vector store.
  4. Evaluate the performance of the fine-tuned model and the RAG pipeline on the knowledge-intensive tasks.

- Design tradeoffs:
  - Fine-tuning vs. RAG: Fine-tuning may be more efficient for tasks with limited knowledge base size, while RAG may be more effective for larger knowledge bases or when the model needs to access information not present in its weights.
  - Number of retrieved documents (K) in RAG: Increasing K may provide more context but also introduce more noise and increase computational cost.

- Failure signatures:
  - Fine-tuning: Degradation in performance on previously learned tasks, inability to learn new facts effectively.
  - RAG: Poor retrieval quality, over-reliance on retrieved context, failure to integrate retrieved information with model's existing knowledge.

- First 3 experiments:
  1. Evaluate the performance of the pre-trained models on the knowledge-intensive tasks without any knowledge injection.
  2. Fine-tune the models on the knowledge base and evaluate their performance on the same tasks.
  3. Implement the RAG pipeline and evaluate its performance on the knowledge-intensive tasks, comparing it to the fine-tuned models.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the discussion section identifies several important limitations and areas for future work, including the generalizability of results to different model architectures and knowledge sources, and the need to explore more sophisticated fine-tuning approaches.

## Limitations
- Limited to a specific set of knowledge-intensive tasks and three pre-trained model variants
- Evaluation timeframe may not represent long-term knowledge retention patterns
- Focus primarily on multiple-choice question answering may miss other knowledge-intensive task types

## Confidence
- High confidence: The comparative performance advantage of RAG over fine-tuning across the tested scenarios
- Medium confidence: The claim about LLMs' difficulty learning new factual information through fine-tuning
- Medium confidence: The effectiveness of data augmentation through paraphrasing

## Next Checks
1. Test the RAG vs. fine-tuning comparison across a broader range of knowledge-intensive tasks beyond multiple-choice questions
2. Evaluate the knowledge retention of fine-tuned models over extended time periods
3. Experiment with more sophisticated fine-tuning approaches, such as parameter-efficient methods or continual learning techniques