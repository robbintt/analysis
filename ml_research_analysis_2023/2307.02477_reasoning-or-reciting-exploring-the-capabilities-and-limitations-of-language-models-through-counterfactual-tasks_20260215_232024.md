---
ver: rpa2
title: Reasoning or Reciting? Exploring the Capabilities and Limitations of Language
  Models Through Counterfactual Tasks
arxiv_id: '2307.02477'
source_url: https://arxiv.org/abs/2307.02477
tags:
- default
- counterfactual
- task
- step
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LMs) possess
  abstract, generalizable reasoning skills or rely on narrow, task-specific procedures
  by evaluating their performance on "counterfactual" task variants that deviate from
  default assumptions. The authors design a suite of 11 diverse tasks (arithmetic,
  programming, syntax, logic, spatial reasoning, drawing, music, chess, and SET game)
  and compare LM performance under both default and counterfactual conditions.
---

# Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks

## Quick Facts
- arXiv ID: 2307.02477
- Source URL: https://arxiv.org/abs/2307.02477
- Reference count: 40
- Key outcome: Language models show substantial performance degradation on counterfactual task variants, suggesting task-specific rather than abstract reasoning abilities.

## Executive Summary
This paper investigates whether large language models (LLMs) possess abstract, generalizable reasoning skills or rely on narrow, task-specific procedures by evaluating their performance on "counterfactual" task variants that deviate from default assumptions. The authors design a suite of 11 diverse tasks (arithmetic, programming, syntax, logic, spatial reasoning, drawing, music, chess, and SET game) and compare LM performance under both default and counterfactual conditions. Across four LMs (GPT-4, GPT-3.5, Claude, PaLM-2), results show that while LMs maintain above-random performance on counterfactual tasks, their accuracy consistently and substantially degrades compared to default conditions. This pattern holds across task categories and is only partially mitigated by zero-shot chain-of-thought prompting or few-shot demonstrations. The findings suggest that current LMs exhibit task-specific reasoning rather than fully general abstract reasoning abilities, highlighting the need for more nuanced evaluation of their capabilities.

## Method Summary
The paper evaluates four language models (GPT-4, GPT-3.5, Claude, PaLM-2) on 11 diverse tasks with both default and counterfactual variants. For each task, the authors generate prompts that clearly specify the counterfactual conditions and include comprehension checks to verify that models understand the altered task parameters. Models are evaluated using zero-shot prompting with and without chain-of-thought reasoning, and responses are compared against ground truth answers. The evaluation includes both accuracy metrics and counterfactual comprehension checks (CCC) to distinguish between task comprehension failures and reasoning failures. Human evaluation is used for the drawing task.

## Key Results
- Models show substantial and consistent performance degradation on counterfactual task variants compared to default conditions
- Zero-shot chain-of-thought prompting partially but not completely mitigates the performance gap
- Performance degradation correlates with the commonality of counterfactual conditions in pretraining data
- Models maintain above-random performance on counterfactual tasks, indicating some generalization ability
- The pattern holds across diverse task categories including arithmetic, programming, syntax, logic, and spatial reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models show systematic performance degradation on counterfactual task variants due to overfitting to default conditions in pretraining data.
- Mechanism: Models memorize specific input-output mappings for frequently encountered task conditions during pretraining, leading to poor generalization when task parameters deviate from these defaults.
- Core assumption: Pretraining corpora contain abundant examples of default task conditions but relatively fewer examples of counterfactual variants.
- Evidence anchors:
  - [abstract] "performance substantially and consistently degrades compared to the default conditions"
  - [section] "Our results parallel the classic train-test gap in machine learning" and "models tend to perform better on task variants that are closer to the default instantiation"
  - [corpus] Weak evidence - corpus neighbors show related work on causality and reasoning but lack direct evidence about pretraining data composition

### Mechanism 2
- Claim: Zero-shot chain-of-thought prompting partially mitigates but does not eliminate the default-counterfactual performance gap.
- Mechanism: CoT prompting encourages explicit reasoning steps that can help models work through unfamiliar task conditions, but cannot overcome fundamental memorization of specific input-output mappings.
- Core assumption: Explicit reasoning through CoT can access more abstract task representations than implicit next-token prediction.
- Evidence anchors:
  - [abstract] "only partially mitigated by zero-shot chain-of-thought prompting"
  - [section] "0-shot CoT to be helpful for most cases" but "there are, however, exceptions"
  - [corpus] Related work on CoT prompting effectiveness shows mixed results depending on task complexity

### Mechanism 3
- Claim: The proximity of counterfactual conditions to default conditions strongly influences model performance, with more common counterfactuals showing less degradation.
- Mechanism: Models leverage partial overlap between default and counterfactual conditions, applying learned procedures where possible while struggling with novel elements.
- Core assumption: Models can transfer knowledge between similar task conditions when sufficient structural overlap exists.
- Evidence anchors:
  - [section] "small variations on the default instantiations of tasks are challenging for models" and "models tend to perform better on task variants that are closer to the default instantiation"
  - [section] "For example, in the arithmetic task, all models perform better in bases 8 and 16, likely due to their relative abundance compared to bases 9 and 11"
  - [corpus] Related work on transfer learning suggests models can leverage shared structure across similar tasks

## Foundational Learning

- Concept: Counterfactual evaluation framework
  - Why needed here: This paper introduces counterfactual evaluation as a method to distinguish between abstract reasoning and task-specific memorization in language models
  - Quick check question: How does counterfactual evaluation differ from traditional instance-level generalization testing?

- Concept: Chain-of-thought prompting
  - Why needed here: The paper uses zero-shot chain-of-thought prompting as one method to probe whether explicit reasoning can help models handle counterfactual conditions
  - Quick check question: Why might chain-of-thought prompting partially but not completely bridge the default-counterfactual performance gap?

- Concept: Task generalization vs. instance generalization
  - Why needed here: The paper distinguishes between models' ability to handle new instances of known tasks versus their ability to adapt to new task variants with different underlying assumptions
  - Quick check question: What key insight about language model capabilities does the distinction between task and instance generalization reveal?

## Architecture Onboarding

- Component map: Task generators -> Prompt templates -> Model interfaces -> Evaluation metrics
- Critical path: Generate counterfactual task variants → Design prompts with comprehension checks → Query models with and without CoT → Parse and evaluate responses → Analyze performance gaps and patterns
- Design tradeoffs: The paper chose 11 diverse tasks to balance breadth of coverage with depth of analysis, but this limits the number of examples per task category. The comprehension checks help distinguish task comprehension failures from reasoning failures but add complexity.
- Failure signatures: Large performance drops on counterfactual variants while maintaining near-perfect performance on defaults indicates memorization rather than abstract reasoning. High comprehension check accuracy with low counterfactual performance suggests understanding of task conditions but inability to apply them.
- First 3 experiments:
  1. Implement arithmetic task with base-9 counterfactual and comprehension check to verify prompt understanding
  2. Create Python program execution task with 1-based indexing counterfactual and test comprehension of indexing rules
  3. Generate spatial reasoning task with rotated coordinate systems and verify model's ability to map cardinal directions to new coordinates

## Open Questions the Paper Calls Out
The paper identifies several open questions for future research, including whether more advanced prompting techniques (beyond 0-shot CoT) can fully eliminate the default-counterfactual performance gap, how model size and architecture affect robustness to counterfactual perturbations, and what is the precise relationship between pretraining data frequency and model performance on counterfactual tasks. These questions point to important directions for understanding and potentially overcoming the limitations identified in this study.

## Limitations
- Evaluation relies on a specific set of 11 tasks that may not fully capture reasoning capabilities across all domains
- Results may not generalize to other model architectures or training approaches beyond the four tested LMs
- Cannot completely rule out the possibility that models are exploiting subtle cues in counterfactual prompts despite comprehension checks

## Confidence

**High Confidence**: The observation that models show substantial performance degradation on counterfactual variants compared to default conditions is well-supported by the empirical results across multiple tasks and models.

**Medium Confidence**: The interpretation that this performance gap indicates task-specific rather than abstract reasoning is plausible but not definitively proven.

**Low Confidence**: The specific mechanisms proposed for why models fail on counterfactual tasks (e.g., overfitting to default conditions in pretraining data) are speculative and lack direct verification.

## Next Checks

1. **Probe Pretraining Data Distribution**: Analyze the frequency of default versus counterfactual conditions in pretraining corpora to verify whether the memorization hypothesis is supported by empirical evidence about training data composition.

2. **Test Model Scaling Effects**: Evaluate whether larger models with more parameters show reduced performance gaps on counterfactual tasks, which would indicate whether the observed limitations are architectural or capacity-related.

3. **Examine Cross-Task Transfer**: Design experiments where models are first trained or prompted on one counterfactual variant, then tested on a structurally similar but novel counterfactual condition, to assess whether models can develop more abstract representations through experience.