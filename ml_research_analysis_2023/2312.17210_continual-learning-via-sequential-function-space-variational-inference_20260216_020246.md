---
ver: rpa2
title: Continual Learning via Sequential Function-Space Variational Inference
arxiv_id: '2312.17210'
source_url: https://arxiv.org/abs/2312.17210
tags:
- task
- learning
- s-fsvi
- variational
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of continual learning in neural
  networks, where models must learn new tasks while retaining knowledge from previous
  ones. The core method, Sequential Function-Space Variational Inference (S-FSVI),
  frames continual learning as sequential Bayesian inference over predictive functions.
---

# Continual Learning via Sequential Function-Space Variational Inference

## Quick Facts
- arXiv ID: 2312.17210
- Source URL: https://arxiv.org/abs/2312.17210
- Reference count: 34
- Key outcome: S-FSVI achieves superior predictive accuracy compared to related approaches across diverse task sequences, requiring less reliance on coreset maintenance

## Executive Summary
This paper addresses continual learning by framing it as sequential Bayesian inference over predictive functions. The proposed method, Sequential Function-Space Variational Inference (S-FSVI), optimizes an objective expressed in terms of distributions over predictive functions rather than network parameters directly. This approach allows for more flexible variational distributions and more effective regularization, achieving superior predictive accuracy across diverse task sequences including split MNIST, split Fashion MNIST, permuted MNIST, split CIFAR, and sequential Omniglot. The method also requires less reliance on maintaining representative points from previous tasks (coresets), with some multi-head setups achieving state-of-the-art performance even without coresets.

## Method Summary
S-FSVI frames continual learning as sequential Bayesian inference over predictive functions. The method optimizes an objective that includes KL divergence terms between distributions over functions, encouraging similarity between the posterior distribution over functions for the current task and the previous task's posterior. Unlike existing methods that regularize neural network parameters directly, S-FSVI allows parameters to vary widely during training, enabling better adaptation to new tasks while maintaining performance on previous tasks. The approach uses variational inference with flexible distributions over functions, computed at context points sampled from previous tasks.

## Key Results
- S-FSVI achieves superior predictive accuracy compared to related approaches across diverse task sequences
- The method requires less reliance on maintaining representative points from previous tasks (coresets)
- Some multi-head setups achieve state-of-the-art performance even without coresets
- S-FSVI demonstrates strong performance on split MNIST, split Fashion MNIST, permuted MNIST, split CIFAR, and sequential Omniglot

## Why This Works (Mechanism)

### Mechanism 1
- Claim: S-FSVI improves continual learning by regularizing predictive functions rather than network parameters directly.
- Mechanism: Instead of penalizing parameter changes between tasks, S-FSVI encourages the posterior distribution over functions to remain close to the previous task's posterior distribution over functions through KL divergence terms.
- Core assumption: The predictive function, not the network parameters, is the true quantity of interest for maintaining performance across tasks.
- Evidence anchors:
  - [abstract] "In contrast to existing methods that regularize neural network parameters directly, this objective allows parameters to vary widely during training, enabling better adaptation to new tasks."
  - [section] "Unlike existing methods that regularize network parameters directly, this objective allows parameters to vary widely during training, enabling better adaptation to new tasks."
  - [corpus] Weak corpus evidence - only 5 related papers found, average neighbor FMR=0.393.

### Mechanism 2
- Claim: S-FSVI achieves superior predictive accuracy by allowing more flexible variational distributions.
- Mechanism: The S-FSVI objective allows direct optimization of variational variance parameters, unlike FROMP which is restricted to Laplace approximation.
- Core assumption: More flexible variational distributions lead to better approximation of the true posterior, resulting in improved performance.
- Evidence anchors:
  - [abstract] "Compared to objectives that directly regularize neural network predictions, the proposed objective allows for more flexible variational distributions and more effective regularization."
  - [section] "Unlike FROMP, it allows direct optimization of variational variance parameters."
  - [corpus] Moderate corpus evidence - some related work on function-space variational inference.

### Mechanism 3
- Claim: S-FSVI requires less reliance on maintaining representative points from previous tasks.
- Mechanism: By encouraging similarity between distributions over functions at context points from previous tasks, S-FSVI can maintain performance with smaller coresets or even with noise coresets in multi-head settings.
- Core assumption: Regularizing the predictive function at context points is sufficient to maintain performance across tasks, reducing the need for large representative datasets.
- Evidence anchors:
  - [abstract] "S-FSVI also requires less reliance on maintaining representative points from previous tasks (coresets), with some multi-head setups achieving state-of-the-art performance even without coresets."
  - [section] "We demonstrated that—unlike existing function-space regularization methods—S-FSVI does not rely on careful coreset selection and, in multi-head settings, can achieve state-of-the-art performance even without coresets collected on previous tasks."
  - [corpus] Weak corpus evidence - limited discussion of coreset requirements in related work.

## Foundational Learning

- Concept: Bayesian inference and posterior distributions
  - Why needed here: S-FSVI frames continual learning as sequential Bayesian inference over predictive functions, requiring understanding of how to update beliefs based on new evidence.
  - Quick check question: What is the difference between the prior and posterior distributions in Bayesian inference, and how are they related?

- Concept: Variational inference and KL divergence
  - Why needed here: S-FSVI uses variational inference to approximate the posterior distribution, and the objective function includes KL divergence terms to measure the difference between distributions.
  - Quick check question: How does variational inference approximate a posterior distribution, and what role does the KL divergence play in this approximation?

- Concept: Function-space vs. parameter-space inference
  - Why needed here: S-FSVI operates in function space, which is different from traditional parameter-space inference methods. Understanding this distinction is crucial for grasping the method's advantages.
  - Quick check question: What is the difference between function-space and parameter-space inference, and why might function-space inference be advantageous for continual learning?

## Architecture Onboarding

- Component map: Neural network architecture -> Variational distributions over parameters -> Coreset of representative points -> Optimization objective (KL divergence terms) -> Optimized variational parameters
- Critical path: Define neural network architecture -> Initialize variational distributions -> Construct coreset -> Compute objective function (including KL divergence terms) -> Optimize variational parameters using gradient-based methods
- Design tradeoffs: Flexibility vs. computational cost (more flexible variational distributions may lead to better performance but are more expensive to optimize), Coreset size vs. memory usage (larger coresets may lead to better performance but require more memory), Function-space vs. parameter-space regularization (function-space regularization may be more effective but requires careful design of the objective function)
- Failure signatures: Overfitting to current task (if KL divergence term is not strong enough), Poor approximation of posterior (if variational family is not expressive enough), Computational issues (if optimization objective is not well-behaved)
- First 3 experiments:
  1. Implement S-FSVI on split MNIST and compare performance to EWC baseline
  2. Vary coreset size and measure impact on performance to understand method's reliance on representative data
  3. Compare function-space regularization to parameter-space regularization on permuted MNIST where function changes are large but parameter changes are small

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise theoretical conditions under which function-space inference outperforms parameter-space inference in continual learning scenarios?
- Basis in paper: [explicit] The paper mentions that function-space inference "may be more effective than parameter-space inference at transferring prior knowledge from one task to another" and notes that "this may offset the information loss in the KL divergence between distributions over functions compared to the KL divergence between distributions over parameters."
- Why unresolved: The paper demonstrates empirically that function-space inference performs better but doesn't provide rigorous theoretical analysis of when and why this advantage occurs.
- What evidence would resolve it: Mathematical proofs establishing conditions where function-space KL divergence minimization preserves more task-relevant information than parameter-space KL divergence minimization, particularly in overparameterized networks.

### Open Question 2
- Question: What is the optimal coreset size and selection strategy for S-FSVI across different task sequences and neural network architectures?
- Basis in paper: [explicit] The paper shows that "S-FSVI does not require large coresets to perform well" and that it achieves strong performance with randomly selected coresets, but doesn't provide systematic analysis of optimal coreset strategies.
- Why unresolved: While the paper demonstrates that S-FSVI is less sensitive to coreset selection than other methods, it doesn't establish optimal coreset sizes or selection criteria for different scenarios.
- What evidence would resolve it: Comprehensive empirical studies systematically varying coreset sizes and selection methods across diverse task sequences and architectures, potentially coupled with theoretical analysis of coreset information-theoretic properties.

### Open Question 3
- Question: How does S-FSVI scale to real-world continual learning scenarios with massive datasets and many tasks?
- Basis in paper: [inferred] The paper demonstrates S-FSVI's effectiveness on several benchmark task sequences but doesn't evaluate its performance on large-scale, real-world datasets or scenarios with hundreds or thousands of tasks.
- Why unresolved: The experiments focus on controlled benchmark tasks with limited data and task numbers, leaving scalability to real-world scenarios unexplored.
- What evidence would resolve it: Experiments on large-scale datasets (e.g., ImageNet, WebVision) with hundreds or thousands of tasks, measuring computational requirements and performance degradation as task count increases.

## Limitations
- Computational efficiency of function-space computations not fully quantified compared to parameter-space methods
- Scalability to very large networks or extremely long task sequences remains unexplored
- Experiments limited to relatively small architectures and moderate numbers of tasks (maximum 25 in Omniglot experiments)

## Confidence

- Core claims about effectiveness: **High** - Strong experimental results across multiple benchmarks and ablation studies
- Computational efficiency claims: **Medium** - Shows good performance with smaller coresets but lacks detailed runtime comparisons
- Scalability claims: **Low** - Experiments limited to small architectures and moderate task counts

## Next Checks

1. **Runtime Benchmarking**: Measure and compare wall-clock training time per task between S-FSVI and baseline methods like EWC, focusing on how function-space computations scale with network size.

2. **Very Long Task Sequences**: Test S-FSVI on task sequences with 50+ tasks to validate whether the method maintains effectiveness over extended learning periods, particularly examining if KL divergence regularization remains sufficient for preventing catastrophic forgetting.

3. **Large-Scale Architecture Testing**: Evaluate S-FSVI on larger architectures (e.g., ResNet-18 or Transformer-based models) to verify the method scales effectively to modern deep learning architectures beyond the small MLPs used in current experiments.