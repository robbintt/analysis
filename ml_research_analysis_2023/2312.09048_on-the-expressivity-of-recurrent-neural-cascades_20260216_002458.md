---
ver: rpa2
title: On The Expressivity of Recurrent Neural Cascades
arxiv_id: '2312.09048'
source_url: https://arxiv.org/abs/2312.09048
tags:
- tanh
- state
- then
- neuron
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors analyze the expressive power of Recurrent Neural Cascades
  (RNCs), which are recurrent neural networks with no cyclic dependencies among neurons.
  They show that RNCs with sign and tanh activation functions, even with positive
  recurrent weights, can only capture star-free regular languages, a proper subset
  of all regular languages.
---

# On The Expressivity of Recurrent Neural Cascades

## Quick Facts
- arXiv ID: 2312.09048
- Source URL: https://arxiv.org/abs/2312.09048
- Reference count: 40
- Primary result: RNCs with sign/tanh activation capture only star-free regular languages, but adding group neurons extends expressivity to all regular languages

## Executive Summary
This paper analyzes the expressive power of Recurrent Neural Cascades (RNCs), a type of recurrent neural network with no cyclic dependencies among neurons. The authors establish that RNCs with sign or tanh activation functions, even with positive recurrent weights, can only capture star-free regular languages—a proper subset of all regular languages. The key insight is that RNCs can be analyzed through the lens of semigroup and group theory, where each neuron implements a flip-flop or group-like transformation. By introducing neurons that implement groups, the expressivity can be extended to capture all regular languages.

## Method Summary
The authors develop a novel framework for analyzing RNCs through semigroup and group theory, characterizing their expressive power in terms of the algebraic structures they can implement. The approach involves interpreting RNN states as elements of semigroups or groups, using homomorphisms to connect the continuous dynamics to discrete algebraic structures. The analysis distinguishes between flip-flop neurons (implementing read, set, and reset functionalities) and group neurons (implementing finite simple groups). The framework provides a principled way to establish expressivity results by studying which semigroups and groups a single neuron can implement.

## Key Results
- RNCs with sign or tanh activation with positive recurrent weights can only capture star-free regular languages
- The acyclicity of RNCs provides a favorable sample complexity compared to fully-connected architectures
- Introducing neurons that implement groups extends expressivity to all regular languages
- Each neuron in an RNC can be characterized as implementing specific semigroup elements (flip-flops or group transformations)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RNCs with sign or tanh activation can only capture star-free regular languages, even with positive recurrent weights.
- **Mechanism:** The acyclicity of the cascade architecture limits the system's ability to generate periodic behavior. Each neuron implements a flip-flop or group-like transformation, and the composition of these transformations cannot generate non-identity permutations required for non-group-free languages.
- **Core assumption:** The state interpretation of each neuron forms a homomorphism from the RNN dynamics to a semiautomaton.
- **Evidence anchors:**
  - [abstract]: "RNCs with sign or tanh activation with positive recurrent weights are the star-free regular languages."
  - [section]: "If a semiautomaton that is not group-free is homomorphically represented by dynamics ⟨U, X, f⟩, with homomorphism ψ, then there exist u ∈ U and x0 ∈ X such that, for xi = f(xi−1, u), the disequality ψ(xi) ̸= ψ(xi+1) holds for every i ≥ 0."
- **Break condition:** Introducing neurons that can implement groups (e.g., second-order neurons) or allowing cyclic dependencies extends expressivity beyond star-free languages.

### Mechanism 2
- **Claim:** RNCs can achieve the expressivity of all regular languages by introducing neurons that implement groups.
- **Mechanism:** Group neurons can implement finite simple groups, enabling the representation of periodic behavior and non-identity permutations. This extends the expressivity beyond what flip-flop neurons alone can capture.
- **Core assumption:** The composition of group neurons can homomorphically represent any semiautomaton, including those that are not group-free.
- **Evidence anchors:**
  - [abstract]: "RNCs can achieve the expressivity of all regular languages by introducing neurons that can implement groups."
  - [section]: "It suffices to identify appropriate recurrent neurons. In particular, neurons that can implement finite simple groups."
- **Break condition:** If the group neurons cannot be instantiated with practical activation functions or if the group structure is too complex for the network to learn.

### Mechanism 3
- **Claim:** The acyclicity of RNCs provides a favorable sample complexity compared to fully-connected architectures.
- **Mechanism:** The reduced number of connections in a cascade (half of the fully-connected) directly impacts the VC dimension, which in turn affects the sample complexity. Fewer connections mean fewer parameters to learn, leading to better generalization.
- **Core assumption:** The VC dimension of a network is directly proportional to the number of connections.
- **Evidence anchors:**
  - [abstract]: "Acyclicity amounts to a structural prior that even for the same number of neurons yields a more favourable sample complexity compared to a fully-connected architecture."
  - [section]: "The VC dimension of recurrent networks depends directly on the number of connections (Koiran and Sontag 1998)."
- **Break condition:** If the reduction in connections significantly limits the network's ability to learn complex patterns, negating the benefit of lower sample complexity.

## Foundational Learning

- **Concept: Semigroup and Group Theory**
  - Why needed here: Understanding the algebraic structures (semigroups, groups, flip-flops) that underlie the expressivity of RNCs.
  - Quick check question: What is the difference between a semigroup and a group, and how does this difference impact the expressivity of RNNs?

- **Concept: Homomorphism**
  - Why needed here: Homomorphisms are used to map the continuous dynamics of RNNs to discrete algebraic structures, enabling the analysis of their expressivity.
  - Quick check question: What properties must a function have to be considered a homomorphism between two dynamical systems?

- **Concept: Symbol Grounding**
  - Why needed here: Symbol grounding connects the subsymbolic level (real-valued inputs/outputs) of RNNs to the symbolic level (discrete inputs/outputs) required for analyzing regular languages.
  - Quick check question: How does a symbol grounding function bridge the gap between the continuous and discrete representations in RNNs?

## Architecture Onboarding

- **Component map:** Input function -> Core flip-flop neurons (read, set, reset) / Core group neurons -> State updates -> Output function

- **Critical path:**
  1. Input preprocessing (symbol grounding)
  2. Neuron activation and state updates
  3. State interpretation and homomorphism
  4. Output generation (symbol grounding)

- **Design tradeoffs:**
  - Expressivity vs. Sample complexity: More expressive architectures (e.g., with group neurons) may require more data to learn
  - Simplicity vs. Functionality: Flip-flop neurons are simpler but less expressive than group neurons

- **Failure signatures:**
  - Inability to learn periodic patterns: May indicate insufficient expressivity (e.g., only flip-flop neurons)
  - Overfitting: May indicate too many connections or complex activation functions

- **First 3 experiments:**
  1. Train an RNC with only flip-flop neurons on a star-free regular language (e.g., even-length strings). Expect high accuracy.
  2. Train an RNC with group neurons on a non-group-free regular language (e.g., odd-length strings). Expect high accuracy.
  3. Compare the sample complexity of an RNC vs. a fully-connected RNN on the same task. Expect the RNC to require fewer samples.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can flip-flop neurons with positive weights be extended to capture more complex regular languages beyond the star-free ones, perhaps by introducing additional types of neurons?
- **Basis in paper:** The paper discusses that flip-flop neurons with positive weights can only capture star-free regular languages, but mentions that neurons implementing groups can extend expressivity to all regular languages.
- **Why unresolved:** The paper only provides examples of neurons implementing cyclic groups of order two, but does not explore other potential group structures or combinations that might further extend expressivity.
- **What evidence would resolve it:** Experiments or theoretical analysis showing the expressivity of flip-flop neurons with positive weights when combined with neurons implementing other types of groups or semigroups beyond cyclic groups of order two.

### Open Question 2
- **Question:** What is the precise relationship between the expressivity of RNCs with negative weights and the expressivity of flip-flop neurons with negative weights?
- **Basis in paper:** The paper states that RNCs with negative weights can capture regular languages beyond the star-free ones, but it is unclear whether this is solely due to the negative weights or if other factors contribute.
- **Why unresolved:** The paper does not provide a detailed analysis of how negative weights specifically contribute to the expressivity of RNCs, and whether this is equivalent to using flip-flop neurons with negative weights.
- **What evidence would resolve it:** A formal proof or counterexample demonstrating that RNCs with negative weights are equivalent in expressivity to flip-flop neurons with negative weights, or identifying the specific mechanisms by which negative weights enhance expressivity.

### Open Question 3
- **Question:** Can the framework for analyzing RNCs be extended to other types of recurrent neural networks, such as those with fully-connected architectures or different activation functions?
- **Basis in paper:** The paper focuses on RNCs with sign and tanh activations, but mentions that the framework could be applied to other activation functions.
- **Why unresolved:** The paper does not provide explicit results or analysis for other types of recurrent neural networks or activation functions, leaving open the question of how the framework can be generalized.
- **What evidence would resolve it:** Theoretical results or empirical studies demonstrating the applicability of the framework to other types of recurrent neural networks and activation functions, and how this affects their expressivity.

## Limitations

- The theoretical analysis relies heavily on abstract algebraic structures without empirical validation
- Key assumptions about neuron implementation (particularly noise tolerance and parameter optimization) remain unproven
- The claim about favorable sample complexity lacks quantitative comparison with fully-connected architectures
- The paper does not address practical considerations like training dynamics, convergence guarantees, or scalability of group neurons

## Confidence

- **High Confidence:** The core mathematical framework connecting RNCs to semigroup and group theory is sound and well-established. The characterization of star-free languages as the limit of RNCs with flip-flop neurons follows logically from the theory.
- **Medium Confidence:** The expressivity extension through group neurons is theoretically valid but lacks implementation details and empirical verification. The claim about reduced sample complexity needs quantitative validation.
- **Low Confidence:** The practical feasibility of implementing and training group neurons with real activation functions remains unproven. The paper's analysis assumes idealized conditions without addressing noise, finite precision, or learning dynamics.

## Next Checks

1. Implement and test the noise tolerance of flip-flop neurons with sign and tanh activation functions, measuring how parameter variations (a and b in Proposition 4) affect stability and performance on benchmark regular languages.

2. Conduct controlled experiments comparing sample complexity between RNCs and fully-connected RNNs on learning regular languages, quantifying the actual VC dimension reduction and its impact on generalization.

3. Develop a practical implementation of group neurons (e.g., implementing cyclic groups) and test their expressivity on non-group-free regular languages that cannot be captured by flip-flop neurons alone.