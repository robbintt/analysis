---
ver: rpa2
title: Retrosynthetic Planning with Dual Value Networks
arxiv_id: '2301.13755'
source_url: https://arxiv.org/abs/2301.13755
tags:
- value
- pdvn
- molecules
- network
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Retrosynthesis planning aims to find synthesis routes from building
  block molecules to target molecules. The authors propose a reinforcement learning
  method to train a single-step predictor to optimize complete synthesis routes, while
  retaining single-step accuracy.
---

# Retrosynthetic Planning with Dual Value Networks

## Quick Facts
- arXiv ID: 2301.13755
- Source URL: https://arxiv.org/abs/2301.13755
- Reference count: 11
- Primary result: Achieved 98.95% success rate on USPTO dataset, significantly improving over existing planners

## Executive Summary
Retrosynthesis planning aims to find synthesis routes from building blocks to target molecules. This paper introduces Planning with Dual Value Networks (PDVN), a reinforcement learning approach that combines a two-branch policy network with dual value networks to predict synthesizability and cost separately. The method significantly improves performance on the USPTO dataset, increasing success rates from 85.79% to 98.95% for Retro* and reducing model calls by half while solving 99.47% of molecules for RetroGraph.

## Method Summary
The method alternates between planning and updating phases. In the planning phase, Monte Carlo Tree Search (MCTS) generates experiences using a two-branch policy network (one frozen reference branch providing valid reactions, one learnable branch optimizing over them) and dual value networks (predicting synthesizability and cost separately). In the updating phase, the networks are trained using cross-entropy loss for the policy and binary/mean squared error for the value networks. The approach maintains single-step accuracy while optimizing complete synthesis routes through careful decomposition of the value function.

## Key Results
- Increased success rate from 85.79% to 98.95% for Retro* planner
- Reduced model calls by half while achieving 99.47% success rate for RetroGraph
- Found shorter synthesis routes (average length reduced from 5.76 to 4.83 for Retro*)

## Why This Works (Mechanism)

### Mechanism 1: Dual Value Network Decomposition
The total synthesis value is decomposed into synthesizability probability and cost given synthesizability using the law of total expectation. This allows separate optimization of whether a molecule can be synthesized and how much it costs if synthesizable, providing more targeted learning signals.

### Mechanism 2: Two-Branch Policy Network
One branch uses a frozen single-step model trained by supervised learning to provide realistic reactions, while the other branch is learnable and optimizes the probability distribution over these valid reactions. This structure maintains single-step accuracy while enabling route-level optimization.

### Mechanism 3: PUCT Selection with Dual Values
The selection rule combines synthesizability probability and cost estimates to balance exploration and exploitation, prioritizing reactions that are likely to lead to synthesizable routes while considering cost when synthesizability is high.

## Foundational Learning

- **Tree-shaped MDP**: Retrosynthesis planning creates a tree structure as molecules are recursively broken down, requiring different state transitions and value function formulation compared to standard MDPs.
- **RL with MCTS**: The algorithm uses MCTS to generate simulated experiences for training, requiring understanding of tree search and backup mechanisms that differ from standard MDPs.
- **Law of Total Expectation**: This mathematical principle enables the decomposition of the complex value function into synthesizability and cost components, making the learning problem more tractable.

## Architecture Onboarding

- **Component map**: Morgan fingerprints -> Two-branch policy network (frozen reference + learnable branch) -> Dual value networks (synthesizability and cost) -> MCTS planner (PUCT selection) -> Training loop
- **Critical path**: Initialize with frozen reference model → Generate experiences via MCTS → Extract training targets from successful routes → Update policy network (CE loss) → Update synthesizability network (BCE loss) → Update cost network (MSE loss) → Repeat
- **Design tradeoffs**: Two-branch policy maintains accuracy but adds complexity; dual value networks provide better decomposition but require more parameters; MCTS depth limit balances efficiency vs completeness
- **Failure signatures**: Unrealistic reactions from learnable branch (check reference model diversity); poor value network guidance (verify backup mechanism); MCTS not finding good routes (check PUCT parameters)
- **First 3 experiments**: 1) Validate single-step accuracy of two-branch policy on held-out test set; 2) Test MCTS with synthetic small molecule trees to verify PUCT selection; 3) Train on small USPTO subset to verify dual value network decomposition improves over single value network

## Open Questions the Paper Calls Out
- How can PDVN be extended to other single-step models like template-free models?
- How does the choice of building block molecules impact success rate and route quality?
- What are the limitations of dual value networks in capturing retrosynthesis complexity, and how can they be improved?

## Limitations
- Evaluation limited to USPTO dataset without testing on alternative retrosynthesis benchmarks
- Heavy reliance on quality and diversity of reference single-step model
- Computational cost of MCTS with dual value networks may limit scalability

## Confidence
- **High confidence**: Two-branch policy network effectively maintains single-step accuracy while enabling route optimization
- **Medium confidence**: Dual value network decomposition provides meaningful benefits for retrosynthesis planning
- **Low confidence**: Method's generalizability to other retrosynthesis datasets and real-world applications

## Next Checks
1. Evaluate PDVN on a held-out subset of USPTO or alternative retrosynthesis datasets (e.g., Reaxys) to assess performance beyond training distribution
2. Conduct ablation study isolating contribution of dual value network decomposition versus other architectural changes
3. Measure computational efficiency as function of molecule size and tree depth to understand practical limitations