---
ver: rpa2
title: 'Neural Collapse Terminus: A Unified Solution for Class Incremental Learning
  and Its Variants'
arxiv_id: '2308.01746'
source_url: https://arxiv.org/abs/2308.01746
tags:
- learning
- incremental
- session
- class
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified solution to three class incremental
  learning tasks (CIL, LTCIL, FSCIL) and a generalized case where the total number
  of classes and data distribution are unknown. The core idea is to pre-assign an
  optimal feature-classifier alignment based on neural collapse, called neural collapse
  terminus, as a consistent target throughout incremental training.
---

# Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants

## Quick Facts
- arXiv ID: 2308.01746
- Source URL: https://arxiv.org/abs/2308.01746
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance on CIL, LTCIL, FSCIL, and a generalized case with unknown class numbers and data distributions through pre-assigned neural collapse terminus structure

## Executive Summary
This paper proposes a unified solution to three class incremental learning tasks (CIL, LTCIL, FSCIL) and a generalized case where the total number of classes and data distribution are unknown. The core idea is to pre-assign an optimal feature-classifier alignment based on neural collapse, called neural collapse terminus, as a consistent target throughout incremental training. For CIL and LTCIL, a prototype evolving scheme called flying to collapse is proposed to drive backbone features smoothly into the neural collapse terminus. For FSCIL, a projection layer and feature mean memory are introduced. Theoretical analysis shows the global optimality satisfies neural collapse regardless of incremental training or data imbalance. Experiments on multiple datasets demonstrate the effectiveness of the unified method on all three tasks and the generalized case, achieving state-of-the-art performance.

## Method Summary
The method pre-assigns a neural collapse terminus (NCT) as a fixed classifier structure using simplex equiangular tight frame (ETF) for the entire label space. For CIL and LTCIL, it employs a flying to collapse (FtC) strategy that gradually evolves class prototypes from nearest class mean positions to NCT positions using an epoch-dependent weight. A misalignment loss directly minimizes cosine distance between features and prototypes instead of cross-entropy. For FSCIL, a projection layer is trained while keeping the backbone frozen, using stored feature means. The approach claims to eliminate the misalignment dilemma by providing a consistent target throughout incremental training.

## Key Results
- Achieves state-of-the-art performance on CIFAR-100, ImageNet-100, ImageNet-1k, and CUB-200 for CIL, LTCIL, and FSCIL tasks
- Outperforms baseline methods like LUCIR, EEIL, and SS-IL across all three incremental learning scenarios
- Demonstrates effectiveness in generalized case with unknown class numbers and varying data distributions (normal, long-tail, few-shot)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-assigning a neural collapse terminus (NCT) eliminates the misalignment dilemma by providing a fixed optimal feature-classifier structure for the entire label space.
- Mechanism: By initializing classifier prototypes as a simplex equiangular tight frame (ETF) for all K0+K' classes, the model trains toward the same geometric structure throughout incremental sessions, avoiding the need to re-partition feature space.
- Core assumption: The global optimality of neural collapse under MSE loss ensures that features will align with their corresponding prototypes in the NCT regardless of incremental training or data imbalance.
- Evidence anchors:
  - [abstract]: "We propose neural collapse terminus that is a fixed structure with the maximal equiangular inter-class separation for the whole label space. It serves as a consistent target throughout the incremental training to avoid dividing the feature space incrementally."
  - [section]: "Suppose that for a class incremental learning problem the base session contains a label space of K0 classes... we randomly initialize class prototypes ˆWETF ∈ Rd×(K0+K′) by Eq. (2) for the whole label space."
- Break condition: If the feature dimension d < K-1, the simplex ETF structure cannot be maintained, violating the equiangular separation requirement.

### Mechanism 2
- Claim: The flying to collapse (FtC) strategy gradually evolves prototypes from nearest class mean (NCM) to NCT, preventing sharp feature shifts that cause catastrophic forgetting.
- Mechanism: FtC uses an epoch-dependent weight η to smoothly transition class prototypes from NCM positions (close to current features) to NCT positions, allowing features to adapt gradually without forgetting old knowledge.
- Core assumption: Starting from NCM positions ensures that new-class prototypes begin near their features, minimizing initial loss dominance by novel classes.
- Evidence anchors:
  - [abstract]: "For CIL and LTCIL, we further propose a prototype evolving scheme to drive the backbone features into our neural collapse terminus smoothly."
  - [section]: "wc = η ˆw(N CT ) c + (1 − η) ˆw(N CM ) c , η = e E , (5)"
- Break condition: If E is too small, the transition happens too quickly, causing sharp feature shifts and forgetting.

### Mechanism 3
- Claim: Using misalignment loss instead of cross-entropy loss better aligns features with prototypes by directly minimizing angular distance.
- Mechanism: The misalignment loss Lalign = 1/2(cos(wyi, µi) - 1)² directly minimizes cosine distance between normalized features and prototypes, creating stronger feature-classifier alignment than CE loss.
- Core assumption: Minimizing angular distance creates more robust feature-classifier alignment than probabilistic classification objectives, especially under incremental learning.
- Evidence anchors:
  - [abstract]: "We adopt a novel misalignment loss for both aligning and distillation."
  - [section]: "We use a misalignment loss to align the backbone features with their prototypes, which can be formulated as: Lalign (ˆµi, ˆwyi) = 1/2(cos(wyi, µi) - 1)²"
- Break condition: If features become too collapsed, the model may lose discriminative power between classes with similar appearances.

## Foundational Learning

- Concept: Neural Collapse Phenomenon
  - Why needed here: Neural collapse provides the theoretical foundation for why pre-assigned NCT creates optimal feature-classifier alignment. Without understanding this phenomenon, the motivation for fixed classifiers would be unclear.
  - Quick check question: What are the four properties of neural collapse that make it optimal for classification?

- Concept: Simplex Equiangular Tight Frame (ETF)
  - Why needed here: The ETF structure ensures maximal equiangular separation between class prototypes, which is crucial for the NCT to serve as an effective consistent target.
  - Quick check question: How does the pairwise cosine similarity of -1/(K-1) in ETF structure contribute to class discriminability?

- Concept: Catastrophic Forgetting in Incremental Learning
  - Why needed here: Understanding why standard incremental learning causes forgetting is essential to appreciate why NCT eliminates the misalignment dilemma.
  - Quick check question: Why does adjusting old-class prototypes to separate from new-class prototypes cause misalignment with backbone features?

## Architecture Onboarding

- Component map:
  Backbone network f -> Projection layer g (FSCIL only) -> Neural collapse terminus ˆWETF (fixed classifier) -> Misalignment loss Lalign + Knowledge distillation loss Ldistill (CIL/LTCIL only)

- Critical path:
  1. Initialize ˆWETF with simplex ETF structure for entire label space
  2. For each session:
     - CIL/LTCIL: Apply FtC to evolve prototypes from NCM to NCT
     - FSCIL: Fix backbone, train projection layer with stored feature means
  3. Compute loss = Lalign + λ·Ldistill (CIL/LTCIL) or Lalign (FSCIL)
  4. Update model parameters
  5. Evaluate using cosine similarity with NCT

- Design tradeoffs:
  - Fixed vs. learnable classifier: Fixed NCT eliminates misalignment but may reduce flexibility for complex distributions
  - FtC vs. direct NCT: FtC prevents forgetting but adds training complexity
  - Misalignment vs. CE loss: Misalignment loss provides better alignment but may reduce probabilistic interpretability

- Failure signatures:
  - Poor performance on new classes: NCT may be too restrictive for novel distributions
  - Severe forgetting: FtC transition may be too aggressive or λ too small
  - Feature collapse: Misalignment loss may cause features to become too similar within classes

- First 3 experiments:
  1. Test NCT with learnable classifier (compare to full method): Validates that NCT structure is the primary performance driver
  2. Test direct NCT initialization without FtC: Validates that gradual evolution is necessary to prevent forgetting
  3. Test CE loss instead of misalignment loss: Validates that the loss function choice is important for feature-classifier alignment

## Open Questions the Paper Calls Out

- How does the neural collapse terminus method perform when the total number of classes is unknown and significantly larger than the actual number of classes encountered?
- How does the neural collapse terminus method compare to other methods in terms of computational efficiency, especially when dealing with a large number of classes?
- How does the neural collapse terminus method perform in scenarios where the data distribution in each session is not known in advance and can vary significantly between sessions?

## Limitations
- The flying to collapse strategy may not work well if initial class prototypes are not close enough to their corresponding features, leading to sharp shifts and forgetting
- The misalignment loss may be ineffective if backbone features are not well-aligned with their corresponding class prototypes
- Pre-assigning a large number of prototypes for unknown class spaces may face scalability issues in high-dimensional settings

## Confidence
- Neural collapse terminus eliminating misalignment: Medium confidence
- Flying to collapse preventing forgetting: Medium confidence  
- Misalignment loss vs cross-entropy: Low confidence

## Next Checks
1. Ablation Study: Compare NCT with learnable classifier baseline to isolate the contribution of fixed structure vs. other components like FtC and misalignment loss.

2. Distribution Shift Test: Evaluate performance when incremental sessions contain data from significantly different distributions than the base session to test the robustness of fixed NCT.

3. Memory Efficiency Analysis: Measure computational and storage costs of pre-assigning large prototype matrices for unknown class spaces, and test performance degradation when reducing the number of pre-assigned prototypes.