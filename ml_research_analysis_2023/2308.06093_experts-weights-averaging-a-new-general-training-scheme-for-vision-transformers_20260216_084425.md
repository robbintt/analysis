---
ver: rpa2
title: 'Experts Weights Averaging: A New General Training Scheme for Vision Transformers'
arxiv_id: '2308.06093'
source_url: https://arxiv.org/abs/2308.06093
tags:
- training
- experts
- arxiv
- performance
- averaging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new general training scheme for Vision Transformers
  (ViTs) that achieves performance improvement without increasing inference cost.
  The key idea is to replace some Feed-Forward Networks (FFNs) of the ViT with more
  efficient Mixture-of-Experts (MoEs) during training, and perform Experts Weights
  Averaging (EWA) on these MoEs.
---

# Experts Weights Averaging: A New General Training Scheme for Vision Transformers

## Quick Facts
- arXiv ID: 2308.06093
- Source URL: https://arxiv.org/abs/2308.06093
- Reference count: 40
- Key outcome: Achieves 1.72% improvement on image classification and 1.74% mIoU improvement on point cloud semantic segmentation without increasing inference cost

## Executive Summary
This paper proposes a novel training scheme for Vision Transformers that achieves performance improvement without increasing inference cost by replacing some FFN layers with Mixture-of-Experts (MoE) during training and performing Experts Weights Averaging (EWA). The key innovation is using Random Uniform Partition (RUP) for token assignment to experts, eliminating the need for learned routing or auxiliary losses. After training, MoE layers are converted back to standard FFN layers by averaging expert weights, maintaining the original ViT architecture for inference. Comprehensive experiments validate the effectiveness across various 2D and 3D visual tasks, ViT architectures, and datasets.

## Method Summary
The proposed method replaces some FFN layers in Vision Transformers with Mixture-of-Experts (MoE) layers during training, using Random Uniform Partition (RUP) for token assignment. Experts Weights Averaging (EWA) is performed at the end of each iteration with a share rate β. After training, each MoE layer is converted back to a single FFN layer by averaging the experts' weights. This approach achieves performance improvement without increasing inference cost, as the final model maintains the original ViT architecture. The method is validated across various 2D and 3D visual tasks, different ViT architectures, and multiple datasets.

## Key Results
- Achieves 1.72% improvement on image classification tasks (CIFAR-100, Tiny-ImageNet)
- Achieves 1.74% mIoU improvement on point cloud semantic segmentation (S3DIS dataset)
- Improves small dataset performance by 2.6% on CIFAR-10 and 4.1% on CIFAR-100
- Demonstrates consistent improvements across 2D and 3D visual tasks with various ViT architectures

## Why This Works (Mechanism)

### Mechanism 1
Replacing FFNs with MoEs and averaging expert weights during training improves ViT performance without increasing inference cost. During training, ViT's FFN layers are replaced with MoE layers using Random Uniform Partition for token assignment. At the end of each iteration, Experts Weights Averaging (EWA) is performed on each MoE layer. During inference, each MoE layer is converted back to a single FFN by averaging expert weights. The averaging acts as regularization that improves generalization, and random uniform partition prevents overfitting while maintaining computational efficiency.

### Mechanism 2
EWA introduces layer-wise weight decay and aggregates multi-expert historical exponential average weights, improving model generalization. Mathematical analysis shows that after m training steps with EWA, each expert's weight includes a decay term (1-β)^m+1 and an aggregation term that accumulates weighted averages of other experts' historical weights. The weight decay prevents overfitting while the aggregation of expert weights creates a smoother optimization trajectory.

### Mechanism 3
RUP-based MoE training is more efficient than top-k routing because it requires no additional parameters or auxiliary losses. Instead of learning routing scores and selecting top-k experts, RUP randomly and uniformly partitions tokens among all experts. This eliminates the need for routing networks, capacity ratios, and load balancing losses. Random uniform assignment provides sufficient diversity in expert specialization without the complexity of learned routing.

## Foundational Learning

- **Concept**: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding how MoE works is essential to grasp why replacing FFNs with MoEs and then averaging them back can improve performance.
  - Quick check question: What is the key difference between top-k routing and random uniform partition in MoE?

- **Concept**: Structural re-parameterization in neural networks
  - Why needed here: The paper draws parallels between structural re-parameterization for CNNs and the proposed EWA method for ViTs, so understanding this concept helps explain the motivation.
  - Quick check question: How does structural re-parameterization for CNNs achieve performance improvement without increasing inference cost?

- **Concept**: Weight averaging techniques in deep learning
  - Why needed here: EWA is the core technique, so understanding how weight averaging works (including SWA, EMA) is crucial for understanding its mechanism.
  - Quick check question: How does stochastic weight averaging (SWA) differ from the Experts Weights Averaging proposed in this paper?

## Architecture Onboarding

- **Component map**: Input tokens -> Standard ViT layers (attention + LayerNorm) -> EWA-modified layers (MoE with RUP) -> EWA operation -> Final layers -> Output

- **Critical path**: 1) Token embeddings enter ViT 2) Pass through standard layers (attention + LayerNorm) 3) Reach EWA-modified layers where MoE with RUP is applied 4) After weight update, EWA is performed on MoE layers 5) Final layers process tokens normally 6) After training, convert MoE layers back to FFN layers

- **Design tradeoffs**:
  - MoE vs FFN: MoE provides more capacity during training but requires conversion back
  - RUP vs top-k routing: RUP is simpler and more efficient but may provide less specialized expert knowledge
  - Share rate β: Higher values provide more regularization but may reduce expert diversity
  - Number of MoE layers: More layers provide more benefit but increase training complexity

- **Failure signatures**:
  - Training instability: Could indicate β is too high or MoE layers are not properly integrated
  - No performance improvement: May suggest insufficient share rate or improper MoE placement
  - Increased inference cost: Would indicate improper conversion of MoE layers back to FFN

- **First 3 experiments**:
  1. Replace every other FFN layer with a 4-expert MoE using RUP, set β=0.2, linear increasing schedule
  2. Test on CIFAR-100 with ViT-S architecture, compare to vanilla training
  3. After training, verify that MoE layers have been correctly converted back to FFN layers by checking model architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed EWA technique compare to other weight averaging methods like Stochastic Weight Averaging (SWA) or Model Soup in terms of performance and computational efficiency?
- Basis in paper: [inferred] The paper mentions that EWA performs weight averaging at the expert level in MoE during training, while other methods like SWA and Model Soup perform weight averaging at the model level.
- Why unresolved: The paper does not provide a direct comparison between EWA and other weight averaging methods, nor does it discuss the computational efficiency of EWA compared to these methods.
- What evidence would resolve it: Experiments comparing the performance and computational efficiency of EWA to other weight averaging methods on various tasks and datasets would provide evidence to resolve this question.

### Open Question 2
- Question: Can the EWA technique be extended to other architectures beyond ViTs, such as CNNs or other transformer-based models?
- Basis in paper: [inferred] The paper focuses on applying EWA to ViTs and mentions that structural re-parameterization, a similar concept, is primarily used for CNNs. However, it does not explore the potential application of EWA to other architectures.
- Why unresolved: The paper does not provide any experiments or theoretical analysis on the application of EWA to architectures other than ViTs.
- What evidence would resolve it: Experiments applying EWA to other architectures like CNNs or other transformer-based models and comparing their performance to the original models would provide evidence to resolve this question.

### Open Question 3
- Question: What is the impact of the share rate and share schedule on the performance of EWA training, and how can they be optimized for different tasks and datasets?
- Basis in paper: [explicit] The paper mentions that the share rate is adjusted within {0.1, 0.2, 0.3, 0.4, 0.5} for different ViT architectures to get the best performance, and it compares the linear increasing schedule with the constant schedule.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of share rate and share schedule on the performance of EWA training, nor does it discuss how to optimize them for different tasks and datasets.
- What evidence would resolve it: A systematic study on the impact of share rate and share schedule on the performance of EWA training across various tasks and datasets, along with guidelines for optimizing these hyperparameters, would provide evidence to resolve this question.

## Limitations

- Generalization boundaries are unclear, with limited understanding of when performance gains plateau or reverse across different architectures and dataset scales
- Mechanism clarity gap exists regarding the practical impact of weight decay and expert aggregation on optimization dynamics
- Implementation sensitivity to critical hyperparameters like share rate β and number of MoE layers lacks thorough exploration

## Confidence

- **High confidence**: The core claim that EWA can convert MoE layers back to FFN layers without increasing inference cost is well-supported with sound mathematical derivation
- **Medium confidence**: The claim that RUP-based MoE training is more efficient than top-k routing is reasonable but lacks direct comparative analysis
- **Low confidence**: The generalizability claim across "various 2D and 3D visual tasks, ViT architectures, and datasets" is overstated given the modest performance improvements and limited experimental scope

## Next Checks

1. **Ablation study on share rate β**: Run experiments varying β from 0.05 to 0.5 on CIFAR-100 with ViT-S to quantify the relationship between regularization strength and performance, revealing whether the linear increasing schedule is optimal.

2. **Expert count sensitivity analysis**: Systematically vary the number of experts (2, 4, 8, 16) while keeping total FFN capacity constant across all MoE layers to validate whether random uniform partition maintains performance advantages as expert count scales.

3. **Direct routing method comparison**: Implement a top-k routing MoE baseline with identical capacity and compare against RUP on the same tasks (e.g., CIFAR-100 and ModelNet40) to provide concrete evidence for efficiency claims and reveal potential performance tradeoffs.