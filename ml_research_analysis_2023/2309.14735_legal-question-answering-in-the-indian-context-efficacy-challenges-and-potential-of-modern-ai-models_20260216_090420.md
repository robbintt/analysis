---
ver: rpa2
title: 'Legal Question-Answering in the Indian Context: Efficacy, Challenges, and
  Potential of Modern AI Models'
arxiv_id: '2309.14735'
source_url: https://arxiv.org/abs/2309.14735
tags:
- legal
- question
- retrieval
- answering
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a comparative analysis of AI models for Indian
  Legal Question Answering (AILQA) using different retrieval and QA approaches. The
  authors evaluated eight experimental settings combining various embedding models
  (OpenAI Ada, Instructor-XL) with QA models (GPT-3 Davinci, Flan-UL2, Longformer)
  and retrieval methods (semantic similarity, BM25).
---

# Legal Question-Answering in the Indian Context: Efficacy, Challenges, and Potential of Modern AI Models

## Quick Facts
- arXiv ID: 2309.14735
- Source URL: https://arxiv.org/abs/2309.14735
- Reference count: 24
- Key outcome: GPT-3 Davinci-based models outperformed other approaches in Indian legal QA, scoring 0.242 Rouge-1, 0.566 MPNet semantic similarity, and 3.74 expert rating out of 5

## Executive Summary
This study presents a comprehensive analysis of AI models for Indian Legal Question Answering (AILQA) using eight experimental settings that combine different embedding models (OpenAI Ada, Instructor-XL) with QA models (GPT-3 Davinci, Flan-UL2, Longformer) and retrieval methods (semantic similarity, BM25). The system achieved high performance with GPT-3 Davinci-based models, which consistently outperformed other approaches including lawyer-provided answers. Expert evaluation confirmed that generative models are particularly effective for legal question-answering tasks, with semantic-based retrieval showing superior performance over traditional BM25 approaches.

## Method Summary
The study evaluated eight experimental settings combining different embedding models with QA models and retrieval methods. The authors used a corpus of 7,221 Supreme Court judgments (1947-2020) along with Indian legal acts and articles from various sources. They created document chunks using LangChain's CharacterTextSplitter and built vector stores using OpenAI Ada and Instructor-XL embeddings. The system was tested on 50 lawyer-provided QA pairs from VidhiKarya, with performance measured using Rouge scores, BLEU, MPNet semantic similarity, and expert ratings from practicing legal professionals.

## Key Results
- GPT-3 Davinci-based models achieved the highest performance with 0.242 Rouge-1, 0.566 MPNet semantic similarity, and 3.74 expert rating out of 5
- GPT-3 Davinci consistently outperformed other models including lawyer-provided answers across all evaluation metrics
- Semantic-based retrieval outperformed BM25 in legal QA tasks, demonstrating the importance of semantic understanding over keyword matching
- Combining Davinci with either Ada or Instructor embeddings consistently delivered superior results, while Longformer performed poorly in legal QA tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining GPT-3 Davinci with either Ada or Instructor embeddings consistently produces superior performance in legal question-answering tasks.
- Mechanism: The combination leverages Davinci's strong generative capabilities with embeddings that capture semantic meaning more effectively than syntactic approaches, enabling better contextual understanding and response generation.
- Core assumption: The semantic embeddings (Ada/Instructor) provide more relevant document retrieval that aligns with Davinci's reasoning patterns.
- Evidence anchors: [abstract] states "The authors evaluated eight experimental settings combining various embedding models (OpenAI Ada, Instructor-XL) with QA models (GPT-3 Davinci, Flan-UL2, Longformer)"; [section] shows "Combining Davinci with either Ada or Instructor or BM25 consistently produces good results"; [corpus] indicates this is a novel approach specific to Indian legal domain.

### Mechanism 2
- Claim: Expert evaluation is essential for assessing generative models in specialized domains like law, where syntactic metrics fail.
- Mechanism: Human legal experts can judge the practical relevance, accuracy, and completeness of answers in ways that automated metrics cannot capture, especially for domain-specific nuances.
- Core assumption: Legal expertise provides a more valid assessment of answer quality than semantic similarity or syntactic metrics alone.
- Evidence anchors: [abstract] states "empirical evaluations are complemented by feedback from practicing legal professionals"; [section] explains "incorporating expert opinion becomes essential to assess the performance of generative models in this context"; [corpus] shows this is an identified gap in existing evaluation approaches.

### Mechanism 3
- Claim: Retrieval method significantly impacts performance, with semantic-based retrieval outperforming BM25 in legal QA tasks.
- Mechanism: Semantic embeddings capture meaning relationships that BM25's term frequency-based approach misses, leading to more contextually appropriate document chunks for answer generation.
- Core assumption: Legal documents require semantic understanding rather than just keyword matching for effective retrieval.
- Evidence anchors: [abstract] mentions "semantic similarity, BM25" as retrieval methods being compared; [section] states "semantic-based retrieval, employing embedding similarity scores to extract the top-k relevant documents" performs better; [corpus] indicates this is a novel comparison in the Indian legal context.

## Foundational Learning

- Concept: Vector store databases and embedding generation
  - Why needed here: The system relies on efficient retrieval of relevant document chunks through semantic similarity in high-dimensional space
  - Quick check question: How does the dimensionality of embeddings (1536 for Ada vs 768 for Instructor) affect retrieval performance and computational requirements?

- Concept: Prompt engineering for specialized domain tasks
  - Why needed here: The effectiveness of generative models depends heavily on well-crafted prompts that guide them to provide accurate, relevant legal responses
  - Quick check question: What specific elements should be included in prompts for legal QA to ensure answers are contextually appropriate and jurisdiction-specific?

- Concept: Evaluation metrics for generative models in specialized domains
  - Why needed here: Traditional metrics like BLEU and ROUGE may not capture the nuances of legal answer quality
  - Quick check question: Why did the authors choose MPNet for semantic evaluation and what limitations did they encounter with other metrics like BertScore?

## Architecture Onboarding

- Component map: Document collection → Preprocessing → Chunking → Vector store creation → Query embedding → Similarity search → Document chunk retrieval → Context + Query → Prompt → LLM → Answer → Syntactic metrics → Semantic metrics → Expert ratings
- Critical path: Document chunking and embedding → Query processing and retrieval → Answer generation → Expert evaluation
- Design tradeoffs: Open-source vs commercial embeddings (Instructor-XL vs OpenAI Ada); Generative vs extractive approaches (GPT-3 vs Longformer); Retrieval method selection (semantic vs BM25); Cost vs performance (token-based pricing for commercial models)
- Failure signatures: Low semantic similarity scores indicate poor embedding quality or retrieval issues; Expert ratings of 1-2 suggest the model misunderstood the question or lacks domain knowledge; High BLEU/ROUGE but low expert ratings indicate superficial similarity without substantive accuracy
- First 3 experiments: 1) Test document chunking parameters (chunk size, overlap) on retrieval accuracy; 2) Compare semantic similarity thresholds for document retrieval (top-k selection); 3) Evaluate prompt variations for the Davinci model on answer quality and hallucination rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Flan-UL2 and LongFormer models be fine-tuned on specialized legal QA datasets to improve their semantic understanding and alignment with expert judgments?
- Basis in paper: explicit
- Why unresolved: The paper acknowledges that fine-tuning these models on dedicated legal QA datasets could enhance their performance, but such datasets are currently unavailable.
- What evidence would resolve it: Development and release of specialized legal QA datasets, followed by empirical studies demonstrating improved performance of these models after fine-tuning.

### Open Question 2
- Question: What specific techniques, such as Chain-of-Thought prompting (CoT) and few-shot learning, can be incorporated to further improve the reasoning ability and interpretability of the AILQA system?
- Basis in paper: explicit
- Why unresolved: While the paper suggests that these techniques could improve the system, it does not provide concrete implementations or results.
- What evidence would resolve it: Implementation and evaluation of these techniques in the AILQA system, demonstrating measurable improvements in reasoning and interpretability.

### Open Question 3
- Question: How can the evaluation methodology be extended to other legal domains beyond criminal law, such as civil law and family law, to provide a more comprehensive assessment of AILQA systems?
- Basis in paper: explicit
- Why unresolved: The paper focuses on criminal law due to resource constraints, leaving the applicability of the evaluation methodology to other legal domains unexplored.
- What evidence would resolve it: Application of the evaluation methodology to other legal domains, with results showing its effectiveness and adaptability across different areas of law.

## Limitations
- The evaluation relied on only 50 legal QA pairs from a single source, which may not represent the diversity of Indian legal questions
- Expert evaluation involved an unspecified number of legal professionals, raising questions about inter-rater reliability and potential bias
- The study focused exclusively on criminal law judgments from the Supreme Court, limiting generalizability to other legal domains or lower courts

## Confidence
**High Confidence**: The finding that GPT-3 Davinci consistently outperforms other models (including lawyer-provided answers) across multiple evaluation metrics. The experimental setup is clearly specified and the performance gap is substantial.

**Medium Confidence**: The claim that semantic-based retrieval outperforms BM25 in legal QA tasks. While supported by results, the comparison could benefit from additional retrieval methods and parameter tuning.

**Low Confidence**: The assertion that this approach generalizes well to other Indian legal domains beyond criminal law. The corpus is limited to Supreme Court criminal judgments, and no cross-domain validation was performed.

## Next Checks
1. **Expand Evaluation Corpus**: Test the system on 200+ diverse legal QA pairs spanning civil law, constitutional law, and administrative law to assess domain generalization.
2. **Expert Inter-rater Reliability**: Conduct a formal inter-rater reliability analysis among 5-10 legal experts to quantify consistency in expert ratings and identify potential bias sources.
3. **Retrieval Ablation Study**: Perform a systematic comparison of retrieval methods (including newer approaches like SPLADE or ColBERT) with varying semantic similarity thresholds to optimize the retrieval pipeline.