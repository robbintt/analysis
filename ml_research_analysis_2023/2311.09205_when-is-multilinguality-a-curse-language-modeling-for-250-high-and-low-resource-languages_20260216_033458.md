---
ver: rpa2
title: When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource
  Languages
arxiv_id: '2311.09205'
source_url: https://arxiv.org/abs/2311.09205
tags:
- language
- languages
- latn
- multilingual
- medlow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work pre-trains over 10,000 language models for 252 languages,
  varying monolingual data, multilingual data, model size, and linguistic similarity.
  It finds that adding moderate amounts of multilingual data in similar languages
  improves low-resource language modeling, similar to increasing dataset sizes by
  up to 33%.
---

# When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages

## Quick Facts
- arXiv ID: 2311.09205
- Source URL: https://arxiv.org/abs/2311.09205
- Authors: 
- Reference count: 40
- Pre-trains over 10,000 models for 252 languages, finding multilingual data helps low-resource languages but hurts high-resource languages

## Executive Summary
This paper investigates how multilingual pre-training affects language modeling performance across 252 languages, varying monolingual data, multilingual data, model size, and linguistic similarity. The authors find that adding moderate amounts of multilingual data from similar languages improves low-resource language modeling performance, equivalent to increasing dataset sizes by up to 33%. However, high-resource languages consistently perform worse in multilingual pre-training scenarios, with degradations similar to reducing dataset sizes by over 85%. These effects are attributed to limited model capacity.

## Method Summary
The authors pre-train over 10,000 GPT-2 models (tiny, mini, small) across 252 languages with varying conditions: monolingual data (1M, 10M, 100M, 1B tokens), multilingual data (0, 10M, 100M, 1B tokens), linguistic similarity (similar vs. dissimilar languages), and model size. They establish monolingual baselines for each language, then pre-train multilingual models and compare performance using estimated equivalent monolingual token counts. Linguistic similarity is measured using syntactic, geographic, and lexical features from lang2vec, with top/bottom 10 similar languages selected for each target language.

## Key Results
- Moderate multilingual data (100M-1B tokens) from similar languages improves low-resource language modeling by up to 33% equivalent dataset size
- High-resource languages consistently perform worse in multilingual pre-training, equivalent to 85%+ dataset size reduction
- Syntactic similarity of added languages accounts for 24.2% of variance in multilingual model performance
- Performance degradations are larger for smaller models and when more multilingual tokens are added

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding multilingual data improves low-resource language modeling performance when the model has sufficient capacity.
- Mechanism: Low-resource languages benefit from shared linguistic structures with similar languages during pre-training, effectively expanding the training data.
- Core assumption: Model capacity is sufficient to accommodate both the target language and multilingual data without performance degradation.
- Evidence anchors:
  - [abstract]: "We find that in moderation, adding multilingual data improves low-resource language modeling performance, similar to increasing low-resource dataset sizes by up to 33%."
  - [section 6.1]: "Performance improvements are significantly larger when the added languages are similar vs. dissimilar to the target language."
  - [corpus]: Limited - corpus doesn't directly test capacity thresholds, but mentions "low-resource language modeling" which implies capacity constraints.
- Break condition: When model capacity is exceeded, performance degrades for both low-resource and high-resource languages.

### Mechanism 2
- Claim: Syntactic similarity between languages is the primary driver of multilingual benefits.
- Mechanism: Shared syntactic structures allow models to transfer grammatical knowledge across languages, improving language modeling performance.
- Core assumption: Syntactic similarity correlates with actual transfer of grammatical knowledge during pre-training.
- Evidence anchors:
  - [abstract]: "Improvements depend on the syntactic similarity of the added multilingual data, with marginal additional effects of vocabulary overlap."
  - [section 6.1]: "We find that syntactic similarity of the added languages accounts for 24.2% of variance in multilingual model performance."
  - [corpus]: Limited - corpus mentions "linguistic similarity" but doesn't provide detailed syntactic analysis.
- Break condition: When syntactic structures diverge significantly, benefits diminish or disappear.

### Mechanism 3
- Claim: High-resource languages consistently perform worse in multilingual pre-training scenarios due to capacity limitations.
- Mechanism: Adding multilingual data creates competition for model parameters, degrading performance in high-resource languages.
- Core assumption: High-resource languages require more model capacity to maintain performance levels.
- Evidence anchors:
  - [abstract]: "high-resource languages consistently perform worse in multilingual pre-training scenarios... similar to reducing dataset sizes by over 85% in some cases."
  - [section 6.2]: "Degradations are larger when more multilingual tokens are added. Degradations are also larger for smaller models."
  - [corpus]: Limited - corpus discusses "limited model capacity" but doesn't provide specific capacity measurements.
- Break condition: When using sufficiently large models that can accommodate all languages without parameter competition.

## Foundational Learning

- Concept: Language modeling performance metrics (perplexity, log-likelihood)
  - Why needed here: The paper uses these metrics to evaluate and compare monolingual vs. multilingual models across 252 languages.
  - Quick check question: If Model A has perplexity 100 and Model B has perplexity 50, which model assigns higher probability to the evaluation data?

- Concept: Scaling laws in language modeling
  - Why needed here: The paper's results about dataset size effects on performance follow power law relationships.
  - Quick check question: According to scaling laws, does doubling the training data size always double model performance?

- Concept: Linguistic similarity metrics
  - Why needed here: The paper uses syntactic, geographic, and lexical similarity to select multilingual training languages.
  - Quick check question: Would you expect Spanish and Portuguese to be more linguistically similar than Spanish and Chinese based on syntactic features?

## Architecture Onboarding

- Component map: 252 monolingual baseline models (varying dataset sizes and model sizes) + 8,454 multilingual models (varying monolingual data, multilingual data, model size, linguistic similarity)
- Critical path: Pre-train monolingual baselines → Estimate performance curves → Pre-train multilingual models → Compare performance using estimated token counts
- Design tradeoffs: Fixed monolingual tokenizers for evaluation vs. multilingual tokenizers for training; computational cost vs. coverage of low-resource languages
- Failure signatures: Performance degradation when adding multilingual data; inconsistent results across model sizes; poor evaluation when tokenization quality varies
- First 3 experiments:
  1. Pre-train tiny model with 1M monolingual tokens for a low-resource language, evaluate performance
  2. Add 100M tokens of multilingual data from similar languages, re-evaluate performance
  3. Add 100M tokens of multilingual data from dissimilar languages, compare performance to step 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which syntactic similarity drives multilingual language model performance improvements?
- Basis in paper: [explicit] The authors state that syntactic similarity of added languages accounts for 24.2% of variance in multilingual model performance, with marginal additional effects of lexical and geographic similarity.
- Why unresolved: While the authors establish that syntactic similarity is a significant factor, they do not investigate the underlying reasons why this is the case. They note that syntactic similarity might reflect other typological features or serve as a proxy for taxonomic relatedness.
- What evidence would resolve it: Detailed linguistic analysis of how specific syntactic features (e.g., word order, case systems, agreement patterns) are transferred or shared across languages during multilingual pre-training. Controlled experiments isolating individual syntactic features would help identify the most important contributors.

### Open Question 2
- Question: How does the "curse of multilinguality" manifest in larger language models beyond 45M parameters?
- Basis in paper: [explicit] The authors state that larger models have smaller performance degradations for high-resource languages and larger improvements for low-resource languages in multilingual scenarios, suggesting capacity limitations drive the curse. However, they only test models up to 45M parameters.
- Why unresolved: The study does not explore how the curse scales with model size beyond 45M parameters. The authors acknowledge that massively multilingual models may require far larger models to avoid capacity limitations, but do not test this hypothesis.
- What evidence would resolve it: Pre-training and evaluating multilingual models with hundreds of millions or billions of parameters on the same dataset would reveal how performance scales with model size and whether the curse can be mitigated in very large models.

### Open Question 3
- Question: How do the findings generalize to downstream tasks beyond language modeling?
- Basis in paper: [explicit] The authors note that their results apply primarily to language modeling performance in individual languages and that effects of multilingual pre-training may be different for specific downstream tasks or cross-lingual transfer learning.
- Why unresolved: The study focuses solely on language modeling performance, but multilingual models are widely used for various NLP tasks. The optimal pre-training strategy may differ depending on the target task.
- What evidence would resolve it: Evaluating the downstream task performance (e.g., sentiment analysis, named entity recognition, machine translation) of models pre-trained with different multilingual strategies on the same dataset would reveal whether the findings generalize beyond language modeling.

## Limitations

- The fixed monolingual tokenizer evaluation approach may underestimate multilingual model capabilities when multilingual data is added during training
- Conclusions about capacity limitations are based on models up to 45M parameters, leaving uncertainty about larger model behaviors
- The 24.2% variance explained by syntactic similarity leaves 75.8% unexplained by studied similarity metrics

## Confidence

**High Confidence**: The finding that multilingual data consistently hurts high-resource language performance is well-supported across multiple experiments and model sizes. The quantitative estimates (up to 85% dataset size reduction equivalent) are robust and consistently observed.

**Medium Confidence**: The conclusion that moderate multilingual data helps low-resource languages is supported but has important caveats. The "moderation" threshold varies significantly by language family and similarity, and the exact optimal amount depends on model capacity and linguistic distance. The 33% dataset size improvement is an average that masks substantial variation.

**Low Confidence**: The claim that syntactic similarity is the primary driver of multilingual benefits (accounting for 24.2% of variance) requires more scrutiny. This percentage, while statistically significant, leaves 75.8% of variance unexplained by the studied similarity metrics.

## Next Checks

1. **Tokenizer Sensitivity Analysis**: Re-run a subset of multilingual experiments using the same multilingual tokenizers for both training and evaluation. Compare performance differences to quantify how much the fixed monolingual tokenizer evaluation underestimates multilingual model capabilities.

2. **Capacity Threshold Validation**: Systematically test whether the performance degradation in high-resource languages can be eliminated by scaling model size. Use a few high-resource languages and test whether performance curves cross when model size increases, directly testing the capacity limitation hypothesis.

3. **Transfer Mechanism Isolation**: Design controlled experiments that isolate specific aspects of linguistic similarity (syntactic, lexical, phonetic) by creating artificial language pairs with controlled feature overlap. This would determine which similarity dimensions actually drive transfer vs. which ones merely correlate with effective transfer.