---
ver: rpa2
title: Scaling Laws for Sparsely-Connected Foundation Models
arxiv_id: '2309.08520'
source_url: https://arxiv.org/abs/2309.08520
tags:
- sparsity
- scaling
- training
- data
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper derives the first scaling laws for sparsely-connected
  foundation models, relating sparsity, number of non-zero parameters, and training
  data. It validates these laws empirically on ViT/JFT-4B and T5/C4, and uses them
  to characterize optimal sparsity levels.
---

# Scaling Laws for Sparsely-Connected Foundation Models

## Quick Facts
- arXiv ID: 2309.08520
- Source URL: https://arxiv.org/abs/2309.08520
- Authors: 
- Reference count: 21
- Key outcome: Derives scaling laws for sparsely-connected foundation models, showing sparsity acts as a multiplicative constant on model size scaling and optimal sparsity increases with longer training.

## Executive Summary
This paper establishes the first scaling laws for sparsely-connected foundation models, deriving a formula L(S, N, D) that relates sparsity S, number of non-zero parameters N, and training data D. The authors validate these laws empirically on ViT/JFT-4B and T5/C4 datasets, showing that sparsity affects each model size similarly as a multiplicative constant while not significantly interacting with the data scaling term. The work demonstrates that optimal sparsity increases with longer training, with 50% sparsity optimal for 2-3x longer training than the dense compute optimal, and provides a framework for analytically deriving optimal sparsity levels for any given model size and training budget.

## Method Summary
The authors conduct extensive sweeps across sparsity levels (0%, 50%, 75%, 87.5%), model sizes (7 targets for ViT in 2x increments, 4 targets for T5 in 4x increments), and training steps (4 different lengths for ViT, 3 for T5) on the specified datasets. They implement gradual magnitude pruning with a cubic schedule, starting at 25% of training and ending at 75%, using AdaFactor optimizer and sparsity-aware RMS calculation. The scaling law formula L(S, N, D) = (aS(1 - S)^bS + cS) * (1/N)^bN + (aD/D)^bD + c is fit using Huber-loss minimization to validate the law by comparing predicted and actual validation losses, including extrapolation tests on larger models.

## Key Results
- Sparsity acts as a multiplicative constant on model size scaling, independent of data scaling
- Optimal sparsity increases with longer training, providing efficiency gains beyond dense compute optimal
- The scaling law extends to structured n:m sparsity and pruning pretrained dense models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparsity acts as a multiplicative constant on model size scaling, independent of data scaling.
- Mechanism: The scaling law L(S, N, D) separates sparsity into a multiplicative term that modifies the N scaling, while the D term remains unchanged.
- Core assumption: The power law exponents bN and bD are consistent across different sparsity levels.
- Evidence anchors:
  - [abstract]: "sparsity affects each model size similarly as a multiplicative constant, while not significantly interacting with the data scaling term."
  - [section]: "the first multiplicative term captures the impact of sparsity, here expressed as remaining density (1 âˆ’ S), which itself follows a saturating power-law with coefficient aS, exponent bS and limit constant cS."
- Break condition: If sparsity affects bN or bD differently across sparsity levels, the multiplicative constant model breaks down.

### Mechanism 2
- Claim: Optimal sparsity increases with longer training, providing efficiency gains beyond dense compute optimal.
- Mechanism: The scaling law allows analytical derivation of optimal sparsity Sopt(N, C), showing that iso-contours of optimal sparsity run parallel to the dense compute optimal Chinchilla line.
- Core assumption: The training cost relationship between dense and sparse models follows the derived cmul(S) function.
- Evidence anchors:
  - [abstract]: "optimal sparsity increases with longer training, with 50% sparsity optimal for 2-3x longer training than the dense compute optimal."
  - [section]: "the optimal sparsity at a given point (N, C) is lower for dense than sparse FLOPs since the former assumes that sparsity provides no benefits during training."
- Break condition: If the actual training cost relationship deviates significantly from cmul(S), optimal sparsity predictions become inaccurate.

### Mechanism 3
- Claim: The scaling law extends to structured n:m sparsity and pruning pretrained dense models.
- Mechanism: The same L(S, N, D) form applies to n:m sparsity with refitted sparsity coefficients, and pruning from pretrained checkpoints is more efficient than training from scratch.
- Core assumption: The dense base scaling is preserved when changing sparsity structure or pruning strategy.
- Evidence anchors:
  - [abstract]: "Our findings shed light on the power and limitations of weight sparsity across various parameter and computational settings."
  - [section]: "if the dense base scaling is known, one only has to fit aS, bS and cS (just 3 rather than 7 parameters) to find the corresponding L(S, N, D)."
- Break condition: If n:m sparsity or pruning from pretrained models fundamentally changes the scaling relationship, the unified law fails.

## Foundational Learning

- Concept: Scaling laws for foundation models
  - Why needed here: Understanding how model performance scales with parameters and data is essential for predicting sparse model performance.
  - Quick check question: What is the form of the scaling law for dense foundation models?

- Concept: Weight sparsity and its impact on model efficiency
  - Why needed here: Sparsity is the primary mechanism for achieving computational efficiency in large models.
  - Quick check question: How does weight sparsity affect the number of non-zero parameters in a model?

- Concept: Compute-optimal training and its relationship to sparsity
  - Why needed here: The paper derives optimal sparsity levels for a given training budget, which requires understanding compute-optimal training.
  - Quick check question: What is the Chinchilla line and how does it relate to optimal model size and training data?

## Architecture Onboarding

- Component map: Vision Transformers (ViT) and T5 models are Transformer-based architectures with attention mechanism, feed-forward networks, and layer normalization. Sparsity is applied to the linear layers within these components.

- Critical path: The critical path for understanding the paper is:
  1. Understand the scaling law L(S, N, D) and its components
  2. Validate the law empirically using the provided experimental setup
  3. Derive optimal sparsity levels and interpret their implications
  4. Extend the analysis to structured sparsity and pruning from pretrained models

- Design tradeoffs: The paper trades off between model size, training data, and sparsity to achieve optimal performance. Increasing sparsity can reduce computational costs but may also affect model quality. The scaling law helps balance these tradeoffs.

- Failure signatures: If the scaling law does not fit the empirical data well, it suggests that sparsity affects model scaling differently than assumed. If optimal sparsity levels are not achieved in practice, it may indicate issues with the pruning strategy or training setup.

- First 3 experiments:
  1. Fit the L(S, N, D) scaling law to the T5/C4 dataset and visualize the fit quality.
  2. Derive optimal sparsity contours for the ViT/JFT-4B dataset and compare them to the Chinchilla line.
  3. Extend the analysis to n:m structured sparsity and compare the results to unstructured sparsity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the scaling law coefficients change for different sparsity patterns or pruning strategies?
- Basis in paper: [explicit] The authors fit new laws for 2:4 and 1:4 as well as 4:8 and 2:8 n:m sparsity patterns based only on a subset of their full grid data, suggesting the sparsity coefficients may differ from unstructured sparsity.
- Why unresolved: The authors only provide sparse scaling law coefficients for unstructured sparsity in the main paper. While they mention fitting laws for n:m sparsity patterns, the actual coefficient values are not shown.
- What evidence would resolve it: Additional experimental results showing the fitted scaling law coefficients for various n:m sparsity patterns and pruning strategies would allow direct comparison with unstructured sparsity coefficients.

### Open Question 2
- Question: What is the optimal sparsity for very large model sizes and training budgets?
- Basis in paper: [inferred] The authors show that optimal sparsity increases with longer training, and that for training beyond the dense compute optimal regime, higher sparsity can be beneficial. However, they do not explore extremely large models and budgets.
- Why unresolved: The authors' largest models have around 170M non-zero parameters and training budgets around 10^23 FLOPs. It is unclear if the trends they observe continue to hold at much larger scales.
- What evidence would resolve it: Scaling law experiments on foundation models with 1B+ non-zero parameters and training budgets of 10^25+ FLOPs would show if the optimal sparsity continues to increase with model size and budget.

### Open Question 3
- Question: How do the scaling laws change when accounting for additional factors like data repetition or fine-tuning?
- Basis in paper: [explicit] The authors note that their scaling laws assume an ideal training setup with no data repetition and focus on the non-bottlenecked regime. They also mention that sparsity could be particularly practical when data is limited and has to be repeated.
- Why unresolved: The authors' scaling laws do not account for effects like data repetition, which is common in practice, or the fine-tuning stage that typically follows pretraining. These factors could significantly impact the relationship between sparsity, model size, and performance.
- What evidence would resolve it: Additional scaling law experiments that incorporate data repetition, fine-tuning, and other practical considerations would show how the laws change and what the implications are for sparse foundation models.

## Limitations

- Empirical Scope Limitations: The scaling laws are validated only on ViT/JFT-4B and T5/C4 datasets with specific model sizes and sparsity levels.
- Pruning Schedule Specificity: The study uses a specific gradual magnitude pruning schedule that may not generalize to different pruning methods.
- Training Compute Constraints: All experiments use 8x8 TPUv4 chips, which may introduce practical constraints that affect the scaling relationships.

## Confidence

**High Confidence**: The empirical validation of the scaling law form L(S, N, D) across multiple sparsity levels and model sizes.

**Medium Confidence**: The extension of scaling laws to structured n:m sparsity and pruning from pretrained dense models.

**Medium Confidence**: The optimal sparsity contours and their relationship to the Chinchilla line.

## Next Checks

1. **Cross-architecture validation**: Test the scaling laws on additional foundation model architectures (e.g., BERT, GPT-style models) and datasets to verify the generalizability of the L(S, N, D) form beyond ViT and T5.

2. **Different pruning strategies**: Validate the scaling laws using alternative pruning methods such as iterative magnitude pruning, lottery ticket rewinding, or structured pruning to assess the robustness of the multiplicative constant assumption.

3. **Hardware-specific validation**: Measure actual sparse vs dense training speedups on different hardware platforms (e.g., GPUs, specialized sparse accelerators) to refine the cmul(S) function and verify the practical applicability of the optimal sparsity predictions.