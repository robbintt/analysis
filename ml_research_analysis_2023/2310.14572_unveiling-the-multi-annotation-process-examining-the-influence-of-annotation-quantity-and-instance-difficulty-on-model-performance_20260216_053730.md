---
ver: rpa2
title: 'Unveiling the Multi-Annotation Process: Examining the Influence of Annotation
  Quantity and Instance Difficulty on Model Performance'
arxiv_id: '2310.14572'
source_url: https://arxiv.org/abs/2310.14572
tags:
- confidence
- variability
- ambiguous
- hard
- easy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how the performance of NLP models trained on
  multi-annotator datasets changes as the number of annotations per instance increases.
  A simulation method is proposed to generate datasets with varying annotation budgets
  when annotator-specific labels are not available.
---

# Unveiling the Multi-Annotation Process: Examining the Influence of Annotation Quantity and Instance Difficulty on Model Performance

## Quick Facts
- arXiv ID: 2310.14572
- Source URL: https://arxiv.org/abs/2310.14572
- Reference count: 40
- Key outcome: This paper explores how the performance of NLP models trained on multi-annotator datasets changes as the number of annotations per instance increases. A simulation method is proposed to generate datasets with varying annotation budgets when annotator-specific labels are not available. Experiments with BERT, RoBERTa, XLNet, ALBERT, and DistilBERT on the ChaosNLI dataset show that increasing annotations does not always improve performance, and the optimal number of annotators varies by dataset and model. Dataset cartography and V-Information analyses reveal that additional annotations change instance difficulty, influencing performance. For example, RoBERTa’s accuracy on ChaosNLI-S plateaus after ~20 annotations, and for ChaosNLI-α, models like BERT achieve peak accuracy with only 3 annotations.

## Executive Summary
This paper investigates the relationship between annotation quantity and model performance in multi-annotator NLP datasets. Through simulation methods and experiments with various PLMs on the ChaosNLI dataset, the authors demonstrate that increasing annotations does not always improve model performance and that the optimal number of annotators varies by dataset and model. The study introduces dataset cartography and V-Information analyses to understand how additional annotations change instance difficulty and influence model performance.

## Method Summary
The study employs a simulation-based approach to generate datasets with varying annotation budgets when annotator-specific labels are unavailable. The authors simulate multi-annotator datasets using label distributions and fine-tune pretrained models (BERT, RoBERTa, XLNet, ALBERT, DistilBERT) on these datasets using both Majority Label (ML) and Label Distribution (LD) training strategies. Performance is evaluated through accuracy metrics, V-Information scores, and dataset cartography analysis to understand instance difficulty transitions and model predictability.

## Key Results
- Model performance does not monotonically improve with increasing annotations; RoBERTa on ChaosNLI-S plateaus after ~20 annotations
- Optimal annotation count varies by dataset and model; BERT achieves peak accuracy with only 3 annotations on ChaosNLI-α
- V-Information scores correlate with model performance and indicate dataset difficulty, with saturation suggesting diminishing returns from additional annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Additional annotations do not always improve model performance; performance can plateau or even decline.
- Mechanism: The inclusion of more annotations alters the difficulty distribution of instances, shifting from ambiguous to easy or hard-to-learn categories, which affects the model's ability to generalize.
- Core assumption: The relationship between annotation quantity and model performance is non-linear and dataset-specific.
- Evidence anchors:
  - [abstract] "Our findings challenge the popular belief that models trained on multi-annotation examples always lead to better performance than models trained on single or few-annotation examples."
  - [section 5.1] "The trends observed in the performance of ChaosNLI-S, ChaosNLI-M, and ChaosNLI-α challenge the prevailing belief that increased annotations invariably lead to improved performance."
  - [corpus] Weak: Only indirect evidence via related works on annotation quality; no direct citation for the plateau claim.

### Mechanism 2
- Claim: The optimal number of annotations varies by dataset and model.
- Mechanism: Different datasets have varying inherent ambiguity and label distributions, which interact with the model's capacity to learn from multiple perspectives. Some models may saturate earlier, while others may benefit from more diverse annotations.
- Core assumption: The dataset's inherent characteristics (e.g., ambiguity, label distribution) and the model's architecture influence the optimal annotation count.
- Evidence anchors:
  - [section 5.1] "The average performance gain for ChaosNLI-M, ChaosNLI-S and ChaosNLI-α is 0.106, 0.177, and 0.031, respectively."
  - [section 5.1] "For ChaosNLI-α with BERT and DistilBERT, it is interesting to note that the optimal performance is achieved with just three annotations."
  - [corpus] Weak: No direct evidence in corpus for model-specific saturation points.

### Mechanism 3
- Claim: V-Information scores correlate with model performance and indicate dataset difficulty.
- Mechanism: V-Information quantifies the ease with which a model can predict the output based on input. Higher V-Information implies easier datasets for the model, leading to better performance. Saturation of V-Information suggests diminishing returns from additional annotations.
- Core assumption: V-Information is a reliable measure of dataset difficulty and correlates with model performance.
- Evidence anchors:
  - [section 5.1] "Models that exhibit higher V-information scores also tend to yield higher accuracy scores in the LD-based performance evaluation."
  - [section 5.1] "The saturation of V-information scores starting at k = 20 for the ChaosNLI-S dataset effectively explains the observed saturation of LD-based accuracy after 20 annotations."
  - [corpus] Weak: No direct citation for V-Information methodology; relies on prior work (Ethayarajh et al., 2022).

## Foundational Learning

- Concept: Dataset cartography
  - Why needed here: To understand how the addition of annotations changes the difficulty distribution of instances, which affects model performance.
  - Quick check question: How does dataset cartography differentiate between easy-to-learn, hard-to-learn, and ambiguous instances?

- Concept: Label distribution vs. majority label training
  - Why needed here: To evaluate how different training strategies (ML vs. LD) interact with varying annotation quantities and affect model performance.
  - Quick check question: What is the difference between training on majority labels versus label distributions in multi-annotator settings?

- Concept: V-Information and dataset difficulty
  - Why needed here: To quantify the ease with which a model can predict outputs based on inputs, providing insight into the relationship between annotation quantity and performance.
  - Quick check question: How does V-Information measure the difficulty of a dataset for a specific model?

## Architecture Onboarding

- Component map:
  Data simulation module -> Model training module -> Evaluation module -> Analysis module

- Critical path:
  1. Simulate multi-annotator datasets (Dk's) for k ∈ [1, 100]
  2. Train base models on combined SNLI/MNLI or α-NLI datasets
  3. Finetune base models on each Dk using ML and LD strategies
  4. Evaluate model performance on test sets
  5. Analyze results using V-Information and dataset cartography

- Design tradeoffs:
  - Using simulated annotations vs. real annotator-specific labels
  - Training with majority labels vs. label distributions
  - Computational cost of finetuning models for each k vs. sampling strategies

- Failure signatures:
  - Performance plateaus or declines with increasing annotations
  - High variance in performance across different Dk's
  - Negative V-Information scores indicating overfitting

- First 3 experiments:
  1. Replicate the results for ChaosNLI-S using RoBERTa with ML strategy to verify the non-monotonic performance trend
  2. Compare V-Information scores for ChaosNLI-M and ChaosNLI-α to understand dataset difficulty differences
  3. Analyze dataset cartography for ChaosNLI-α with BERT to observe the e → a and a → h transitions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed multi-annotator simulation process generalize to other NLP tasks beyond NLI, such as text classification or question answering?
- Basis in paper: [explicit] The simulation process is introduced and tested only on NLI tasks.
- Why unresolved: The paper does not provide evidence or experiments for other NLP tasks, so the generalizability of the simulation process remains unknown.
- What evidence would resolve it: Testing the simulation process on diverse NLP tasks (e.g., text classification, sentiment analysis, question answering) and comparing performance with real multi-annotator datasets.

### Open Question 2
- Question: What is the impact of annotator expertise or demographic information on the model's performance when increasing the number of annotations?
- Basis in paper: [inferred] The paper does not consider annotator expertise or demographic information, which could influence the difficulty of instances and model performance.
- Why unresolved: The simulation process treats all annotators equally, ignoring potential differences in expertise or background that may affect annotation quality and model outcomes.
- What evidence would resolve it: Experiments incorporating annotator expertise or demographic information into the simulation process and analyzing its effect on model performance and instance difficulty.

### Open Question 3
- Question: How does the choice of the number of annotators affect the model's ability to generalize to unseen data or handle out-of-distribution samples?
- Basis in paper: [explicit] The paper explores the relationship between annotation quantity and model performance but does not investigate generalization or out-of-distribution handling.
- Why unresolved: The study focuses on in-domain performance and does not evaluate the model's robustness to unseen or out-of-distribution data.
- What evidence would resolve it: Evaluating model performance on unseen or out-of-distribution datasets for different annotation budgets to assess generalization and robustness.

### Open Question 4
- Question: What are the computational and resource implications of using more annotations per instance, and how do they scale with the size of the dataset and model?
- Basis in paper: [inferred] The paper does not discuss the computational cost or resource requirements of using more annotations.
- Why unresolved: The study focuses on the impact of annotations on performance but does not consider the practical aspects of computational resources or scalability.
- What evidence would resolve it: Analyzing the computational cost and resource requirements of training models with varying annotation budgets and dataset sizes, and exploring techniques to optimize resource usage.

## Limitations

- The simulation methodology for generating multi-annotator datasets when annotator-specific labels are unavailable is not fully specified
- The optimal annotation count appears highly dataset-specific, suggesting findings may not generalize across all NLP tasks or domains
- The study does not consider annotator expertise or demographic information, which could influence annotation quality and model outcomes

## Confidence

- Claims about non-monotonic performance trends: **Medium** - Supported by experimental results but dependent on specific simulation assumptions
- Claims about model-specific saturation points: **Medium** - Observed patterns exist but may vary with different training procedures
- Claims about V-Information correlating with difficulty: **Medium** - Correlation shown but mechanistic explanation requires further validation

## Next Checks

1. Implement ablation study comparing simulated annotations against real annotator-specific labels when available to validate the simulation methodology
2. Test the framework on additional NLP datasets beyond NLI (e.g., sentiment analysis, question answering) to assess generalizability
3. Analyze the impact of different random seeds in the simulation process on performance trends to quantify robustness