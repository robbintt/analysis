---
ver: rpa2
title: 'OMNI: Open-endedness via Models of human Notions of Interestingness'
arxiv_id: '2306.01711'
source_url: https://arxiv.org/abs/2306.01711
tags:
- tasks
- interesting
- success
- learning
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OMNI, an approach that uses large language
  models to model human notions of interestingness to accelerate open-ended learning.
  The insight is that LLMs have internalized human concepts of interestingness from
  training on vast amounts of human-generated data.
---

# OMNI: Open-endedness via Models of human Notions of Interestingness

## Quick Facts
- arXiv ID: 2306.01711
- Source URL: https://arxiv.org/abs/2306.01711
- Reference count: 40
- One-line primary result: LLM-based model of interestingness improves open-ended learning by focusing on tasks that are both learnable and interesting

## Executive Summary
This paper proposes OMNI, an approach that uses large language models to model human notions of interestingness to accelerate open-ended learning. The insight is that LLMs have internalized human concepts of interestingness from training on vast amounts of human-generated data. OMNI uses LLMs to focus on tasks that are both learnable and interesting, outperforming baselines based on uniform task sampling or learning progress alone. Experiments in a modified Minecraft environment show that OMNI learns more tasks with higher success rates than baselines, achieving results nearly on par with an oracle model of interestingness. The approach has potential to advance open-ended learning by intelligently selecting interesting tasks for AI agents to focus on.

## Method Summary
OMNI combines learning progress-based curriculum with a model of interestingness (MoI) implemented using an LLM. In a modified Minecraft-like Crafter environment with 15 interesting tasks plus 90 boring repetitive tasks and 1023 extremely challenging tasks, OMNI uses GPT-3 to predict which tasks are interesting based on the agent's existing proficiency. The approach selects tasks that have both high learning progress (indicating learnability) and high predicted interestingness, training a PPO agent with a CNN-LSTM architecture using this curriculum. The MoI is prompted in a few-shot manner to make interestingness judgments.

## Key Results
- OMNI outperforms baselines based on uniform task sampling or learning progress alone in the Crafter environment
- OMNI learns more tasks with higher success rates than baselines, achieving results nearly on par with an oracle model of interestingness
- The approach shows promise for advancing open-ended learning by intelligently selecting which tasks to focus on next

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can model human notions of interestingness by leveraging their training on vast amounts of human-generated text data.
- Mechanism: LLMs are trained on extensive human-generated text corpora where humans naturally discuss what they find interesting or boring. This allows LLMs to internalize human concepts of interestingness, which can then be used to filter tasks that are both learnable and interesting for AI agents.
- Core assumption: The human-generated text data used to train LLMs contains sufficient information about what humans consider interesting, and this information can be extracted through appropriate prompting.
- Evidence anchors:
  - [abstract]: "The insight is that we can utilize foundation models (FMs) as a model of interestingness (MoI), because they already internalize human concepts of interestingness from training on vast amounts of human-generated data, where humans naturally write about what they find interesting or boring."
  - [section]: "OMNI leverages the power of LMs that have already been trained on extensive human-generated data and have an inherent understanding of human notions of interestingness."
- Break condition: If the human-generated text data lacks sufficient information about interestingness, or if the LLM cannot extract this information through prompting, the mechanism would fail.

### Mechanism 2
- Claim: Combining learning progress with a model of interestingness (MoI) improves task selection over using learning progress alone.
- Mechanism: Learning progress identifies tasks at the frontier of the agent's capabilities, but can be distracted by uninteresting tasks. The MoI filters out these uninteresting tasks, allowing the agent to focus on tasks that are both learnable and interesting.
- Core assumption: There exists a subset of tasks that are both learnable (have high learning progress) and interesting, and filtering for this subset improves learning outcomes.
- Evidence anchors:
  - [abstract]: "We show that FM-based MoIs improve open-ended learning by focusing on tasks that are both learnable and interesting, outperforming baselines based on uniform task sampling or learning progress alone."
  - [section]: "OMNI outperforms baselines based on uniform task sampling or learning progress alone. This approach has the potential to dramatically advance the ability to intelligently select which tasks to focus on next."
- Break condition: If the MoI incorrectly identifies tasks as interesting or uninteresting, or if the intersection of learnable and interesting tasks is empty or too small to be useful, the mechanism would fail.

### Mechanism 3
- Claim: LMs can be used to automatically analyze performance data and update their predictions of interestingness without human intervention.
- Mechanism: LMs are prompted with information about an agent's performance on tasks, including success rates. They can then analyze this data, identify patterns (e.g., different success rates for synonymous tasks), and update their predictions of what is interesting accordingly.
- Core assumption: LMs have the capability to analyze numerical performance data and make logical inferences from it, which can then be used to refine their interestingness predictions.
- Evidence anchors:
  - [section]: "Preliminary investigations indicate that LMs, such as GPT-3 and GPT-4, possess the ability to automatically analyze numerical results and adjust their understanding of interestingness."
  - [section]: "These models generated plausible analyses of the differences in task success rates between synonymous tasks, and adjusted their predictions of what is interesting based on these analyses."
- Break condition: If the LM cannot accurately analyze the performance data or make valid inferences from it, or if the adjustments to interestingness predictions are incorrect, the mechanism would fail.

## Foundational Learning

- Concept: Reinforcement Learning (RL)
  - Why needed here: The paper uses RL to train task-conditioned agents in the Crafter environment. Understanding RL is crucial for grasping how the agent learns from tasks and how the curriculum affects its learning process.
  - Quick check question: What is the difference between on-policy and off-policy RL algorithms, and which category does PPO (the algorithm used in the paper) belong to?

- Concept: Auto-curriculum learning
  - Why needed here: The paper builds upon and extends existing auto-curriculum learning methods, specifically the learning progress curriculum. Understanding auto-curriculum learning is essential for comprehending how the paper addresses the challenge of selecting interesting tasks in open-ended environments.
  - Quick check question: How does learning progress-based curriculum differ from other auto-curriculum approaches like regret-based or threshold-based methods?

- Concept: Foundation models (FMs) / Large Language Models (LLMs)
  - Why needed here: The paper's key innovation is using FMs/LLMs as a model of interestingness (MoI). Understanding the capabilities and limitations of FMs/LLMs is crucial for evaluating the effectiveness and generalizability of the OMNI approach.
  - Quick check question: What are the key characteristics that distinguish foundation models from traditional machine learning models, and how do these characteristics enable FMs to model human notions of interestingness?

## Architecture Onboarding

- Component map: Crafter Environment -> PPO Agent -> Learning Progress Curriculum -> Model of Interestingness (MoI) -> OMNI Algorithm
- Critical path: The RL agent learns from tasks selected by the OMNI algorithm, which combines the learning progress curriculum with the MoI. The agent's performance on tasks is used to update the learning progress estimates, which in turn affects the task sampling distribution.
- Design tradeoffs: The paper uses a bag-of-words encoding for tasks instead of a pre-trained natural language encoder, which may limit the agent's ability to understand synonymous tasks. However, this design choice allows for a clearer demonstration of the MoI's ability to recognize and filter out synonymous tasks.
- Failure signatures: If the OMNI algorithm fails to improve learning outcomes compared to baselines, it could indicate issues with the MoI's ability to correctly identify interesting tasks, the learning progress curriculum's effectiveness, or the combination of the two.
- First 3 experiments:
  1. Compare OMNI to uniform task sampling and learning progress alone on the Crafter environment, measuring average task success rates and the number of tasks with success rates above a threshold.
  2. Evaluate the performance of the MoI by comparing OMNI to an oracle model of interestingness.
  3. Test OMNI's robustness by using different types of boring tasks (e.g., compound tasks, synonymous tasks) and measuring its performance relative to baselines.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The paper demonstrates OMNI outperforms baselines in a controlled Minecraft environment, but there's no external validation that LLMs actually capture "human notions of interestingness" as claimed.
- Experiments use a modified Crafter environment with hand-designed interesting vs. boring tasks, with no evidence OMNI would work with naturally occurring task distributions or in more complex, open-ended environments.
- The approach relies on GPT-3's ability to judge interestingness without testing whether other LLMs would perform similarly or if the approach is sensitive to the specific model chosen.

## Confidence
- High confidence: The empirical demonstration that combining learning progress with LLM-based filtering outperforms either approach alone in the Crafter environment.
- Medium confidence: That LLMs can serve as useful models of interestingness for task selection in open-ended learning, given the limited external validation of this core claim.
- Low confidence: That the approach would generalize to more complex environments or that the LLM is truly capturing human notions of interestingness rather than some proxy signal.

## Next Checks
1. Conduct a human study where participants rate the interestingness of tasks in the Crafter environment, then compare these ratings to GPT-3's predictions to directly test whether the LLM captures human notions of interestingness.
2. Implement OMNI using multiple different LLMs (GPT-3, GPT-4, Claude, LLaMA) and compare performance to determine if the approach depends on specific model properties or generalizes across models.
3. Modify the evaluation to measure not just task completion but the discovery of novel, interesting agent behaviors that weren't explicitly in the task set, testing whether OMNI enables true open-ended exploration beyond predefined objectives.