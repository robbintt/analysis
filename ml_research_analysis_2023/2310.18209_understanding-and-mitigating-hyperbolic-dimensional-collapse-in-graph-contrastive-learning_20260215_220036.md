---
ver: rpa2
title: Understanding and Mitigating Hyperbolic Dimensional Collapse in Graph Contrastive
  Learning
arxiv_id: '2310.18209'
source_url: https://arxiv.org/abs/2310.18209
tags:
- hyperbolic
- space
- learning
- graph
- collapse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the dimensional collapse problem in hyperbolic
  graph contrastive learning, where feature embeddings become overly concentrated
  in limited regions of the hyperbolic space. The authors propose a novel framework
  that combines hyperbolic distance-based alignment with an outer shell isotropy loss.
---

# Understanding and Mitigating Hyperbolic Dimensional Collapse in Graph Contrastive Learning

## Quick Facts
- arXiv ID: 2310.18209
- Source URL: https://arxiv.org/abs/2310.18209
- Reference count: 25
- Key outcome: Novel framework preventing dimensional collapse in hyperbolic graph contrastive learning achieves 94.50% accuracy on Disease dataset and 93.55% on Airport

## Executive Summary
This paper addresses the critical problem of dimensional collapse in hyperbolic graph contrastive learning, where feature embeddings become overly concentrated in limited regions of hyperbolic space. The authors propose a novel framework that combines hyperbolic distance-based alignment with an outer shell isotropy loss. The key innovation enforces an isotropic Gaussian distribution in the tangent space at the origin of the Poincaré ball, which translates to an isotropic ring density near the boundary of the hyperbolic manifold. This approach effectively prevents both "leaf collapse" and "height collapse" in tree-structured data, significantly outperforming state-of-the-art baselines on graph node classification tasks.

## Method Summary
The HyperGCL framework operates by projecting graph encoder outputs into the Poincaré ball model of hyperbolic space. The method employs two complementary loss terms: an alignment loss that minimizes hyperbolic distance between positive pairs while maximizing it for negative pairs, and an outer shell isotropy loss that enforces an isotropic Gaussian distribution in the tangent space at the origin. This tangent space distribution maps to an isotropic ring density near the boundary of the hyperbolic manifold, preventing dimensional collapse. The framework is trained using Riemannian stochastic gradient descent to respect the non-Euclidean geometry of the Poincaré ball.

## Key Results
- Achieves 94.50% accuracy on Disease dataset and 93.55% on Airport, compared to 90.30% and 92.35% for previous best methods
- Prevents dimensional collapse as evidenced by higher Effective Rank values across all tested datasets
- Outperforms Euclidean-based contrastive learning methods on all evaluated graph node classification benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enforcing an isotropic Gaussian distribution in the tangent space at the origin prevents dimensional collapse by creating an isotropic shell density near the boundary.
- Mechanism: The exponential map from the tangent space to the hyperbolic manifold transforms the isotropic Gaussian into a ring-shaped density that naturally fills the ambient space without collapsing to specific points or subspaces.
- Core assumption: The conformal factor of the Poincaré ball preserves the isotropy of the distribution when mapped to the ambient space.
- Evidence anchors:
  - [abstract] "we propose a substitute of uniformity metric to prevent the so-called dimensional collapse" and "enforcing an isotropic Gaussian distribution in the tangent space at the origin of the Poincaré ball, which translates to an isotropic ring density near the boundary"
  - [section] "We are interested in imposing high density of features uniformly distributed along the ring circumference close to the boundary of Poincaré ball" and the derivation in Theorem 2 showing the analytical distribution in the ambient space
  - [corpus] No direct evidence found; this is the paper's novel contribution
- Break condition: If the exponential map does not preserve the isotropy property or if the conformal factor distorts the distribution asymmetrically.

### Mechanism 2
- Claim: Using hyperbolic distance as the alignment metric captures hierarchical data-invariant information better than Euclidean distance.
- Mechanism: Hyperbolic distance naturally approximates tree distances, so minimizing it between positive pairs preserves the hierarchical structure while repelling unrelated samples.
- Core assumption: The underlying graph structure exhibits hierarchical properties that can be well-approximated by tree distances in hyperbolic space.
- Evidence anchors:
  - [abstract] "we design the alignment metric that effectively captures the hierarchical data-invariant information"
  - [section] "We adopt the Hyperbolic distance to measure the alignment in the hyperbolic space" and Theorem 1 stating "any tree can be embedded into a Poincaré disk with low distortion"
  - [corpus] Weak evidence; the corpus neighbors discuss hyperbolic models but don't specifically address this alignment mechanism
- Break condition: If the graph lacks hierarchical structure or if the hyperbolic distance fails to capture the relevant similarities.

### Mechanism 3
- Claim: The outer shell isotropy loss term (KL divergence between learned and target distributions) increases the Effective Rank of representations, preventing dimensional collapse.
- Mechanism: By minimizing the KL divergence between the empirical distribution in tangent space and an isotropic Gaussian, the method forces the learned representations to span the full dimensional space rather than collapsing to lower-dimensional subspaces.
- Core assumption: Higher Effective Rank correlates with better utilization of the embedding space and prevents collapse.
- Evidence anchors:
  - [abstract] "imposing an isotropic ring density towards boundaries of Poincaré ball" and "imposing the isotropic Gaussian loss on the tangent space increases the Effective Rank"
  - [section] Theorem 3 showing that minimizing the outer isotropy term provides a lower bound on Effective Rank, and Table 3 demonstrating correlation between Effective Rank in ambient and tangent spaces
  - [corpus] No direct evidence found; this is the paper's theoretical contribution
- Break condition: If the Effective Rank metric does not correlate with actual collapse or if the KL divergence optimization fails to prevent concentration of representations.

## Foundational Learning

- Concept: Riemannian geometry and hyperbolic spaces
  - Why needed here: The entire framework operates in hyperbolic space rather than Euclidean space, requiring understanding of Poincaré ball model, exponential/logarithmic maps, and hyperbolic distance
  - Quick check question: What is the relationship between the Euclidean distance and hyperbolic distance in the Poincaré ball model?

- Concept: Contrastive learning and alignment-uniformity tradeoff
  - Why needed here: The method builds on contrastive learning principles but adapts them to hyperbolic space, requiring understanding of how alignment and uniformity work in Euclidean space first
  - Quick check question: How does the uniformity term in Euclidean contrastive learning differ from the outer shell isotropy in hyperbolic space?

- Concept: Dimensionality reduction and Effective Rank
  - Why needed here: The method specifically addresses dimensional collapse, which is measured using Effective Rank, requiring understanding of what this metric captures
  - Quick check question: What does a low Effective Rank indicate about the learned representations?

## Architecture Onboarding

- Component map: Graph encoder (GCN layers) → Projection to Poincaré ball → Alignment loss (hyperbolic distance) → Outer shell isotropy loss (KL divergence in tangent space) → Parameter update via Riemannian SGD
- Critical path: The encoder output must be projected to the Poincaré ball, then both alignment and isotropy losses computed and combined before backpropagation
- Design tradeoffs: Using hyperbolic space allows better modeling of hierarchical structures but requires more complex distance computations and Riemannian optimization; the isotropy loss adds computational overhead but prevents collapse
- Failure signatures: Low Effective Rank values, poor performance on hierarchical datasets, representations collapsing to specific regions of the Poincaré ball
- First 3 experiments:
  1. Train with only alignment loss (L_D^c A) to verify it causes collapse as predicted
  2. Train with only outer shell isotropy loss (L_D^c U) to verify it prevents collapse but may hurt alignment
  3. Train with both losses at different weight ratios to find optimal balance for a specific dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between Effective Rank measured in the ambient space of the hyperbolic manifold versus the Effective Rank measured in the tangent space at the origin?
- Basis in paper: [explicit] The paper states "we show that the Effective Rank measured in the ambient space correlates with the effective rank measured on the tangent plane" and "Effective Rank in the ambient space of the hyperbolic manifold is correlated (just a non-linear mapping) with the Effective Rank in the tangent space"
- Why unresolved: The paper only claims correlation exists but does not provide quantitative analysis or the exact functional form of this relationship. The proof in Theorem 3 only addresses one direction (tangent to ambient via the isotropic Gaussian loss).
- What evidence would resolve it: Experimental results showing scatter plots with regression lines comparing Effective Ranks from both spaces across different datasets, and/or mathematical derivation of the precise relationship.

### Open Question 2
- Question: How sensitive is HyperGCL performance to the choice of curvature parameter c in the Poincaré ball model?
- Basis in paper: [explicit] Section 5.3 includes a discussion on "Impact of Curvature c" with Figure 8 showing performance varies with different c values, noting that "graphs with relatively larger density require smaller c"
- Why unresolved: The paper provides limited empirical analysis with only 3 datasets and suggests the relationship is data-dependent but does not provide a principled method for selecting c or characterize the relationship between graph properties (density, diameter, etc.) and optimal c.
- What evidence would resolve it: A systematic study across diverse graph datasets characterizing how graph properties relate to optimal curvature, or theoretical analysis deriving optimal c based on graph characteristics.

### Open Question 3
- Question: Does the proposed isotropic Gaussian loss in the tangent space generalize to other non-Euclidean geometries beyond hyperbolic spaces?
- Basis in paper: [inferred] The paper specifically addresses hyperbolic spaces and discusses limitations of applying Euclidean uniformity to hyperbolic manifolds, but the concept of using tangent space distributions to control feature spread could potentially apply to other Riemannian manifolds.
- Why unresolved: The method is presented and evaluated only for hyperbolic spaces, and the theoretical analysis relies on specific properties of the Poincaré ball model (exponential map, conformal factor, etc.).
- What evidence would resolve it: Experimental evaluation of the method on other non-Euclidean manifolds (sphere, product manifolds, etc.) and/or theoretical extension of the framework to general Riemannian manifolds.

## Limitations
- Limited evaluation to node classification tasks; potential performance on other graph learning tasks remains unknown
- The hyperparameter sensitivity of the isotropy loss weight λ is not thoroughly explored
- No ablation studies on how the method performs with non-hierarchical graph structures

## Confidence
- High: The mathematical framework for hyperbolic distance and its relationship to tree embeddings (Theorem 1)
- Medium: The connection between outer shell isotropy and Effective Rank improvement (Theorem 3)
- Medium: The experimental performance improvements on benchmark datasets

## Next Checks
1. Cross-domain evaluation: Test HyperGCL on molecular graphs and knowledge graphs to verify performance beyond citation networks
2. Hyperparameter sensitivity analysis: Systematically vary λ and the curvature parameter c to understand their impact on dimensional collapse prevention
3. Ablation on graph structure: Evaluate performance on datasets with varying degrees of hierarchical structure to identify when the hyperbolic approach provides the most benefit