---
ver: rpa2
title: When a Language Question Is at Stake. A Revisited Approach to Label Sensitive
  Content
arxiv_id: '2311.10514'
source_url: https://arxiv.org/abs/2311.10514
tags:
- language
- offensive
- tweets
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to annotating sensitive content
  in Ukrainian tweets related to the Russian-Ukrainian war using pseudo-labeling techniques.
  The study addresses the challenge of creating high-quality datasets for offensive
  language detection in under-resourced languages, which is crucial for developing
  effective content moderation models.
---

# When a Language Question Is at Stake. A Revisited Approach to Label Sensitive Content

## Quick Facts
- arXiv ID: 2311.10514
- Source URL: https://arxiv.org/abs/2311.10514
- Reference count: 19
- Primary result: ELECTRA + ReLU Dense layer model achieves 72% accuracy for offensive language detection in Ukrainian tweets

## Executive Summary
This paper presents a novel approach to annotating sensitive content in Ukrainian tweets related to the Russian-Ukrainian war using pseudo-labeling techniques. The study addresses the challenge of creating high-quality datasets for offensive language detection in under-resourced languages, which is crucial for developing effective content moderation models. The proposed method involves three stages of data annotation, combining human expertise and machine learning models to label 2043 tweets as offensive or neutral. The primary results show that the ELECTRA + ReLU Dense layer model achieves the best performance, with a Fleiss Kappa score of 0.814 for inter-annotator agreement and overall classification accuracy of 72%.

## Method Summary
The study employs a three-stage annotation process combining human expertise and machine learning models. First, 300 tweets are manually annotated by human experts. Second, transformer models (ELECTRA, RoBERTa, and BERT) are fine-tuned on the initial dataset and used to generate pseudo-labels for an additional 700 tweets. Human annotators then verify uncertain labels (those with probabilities between 0.40 and 0.55). Finally, the best-performing model is trained on the combined dataset to create the final corpus of 2043 tweets. The approach uses a four-category annotation scheme distinguishing between offensive/neutral language and offensive/neutral sense to capture both explicit and implicit offensive content.

## Key Results
- ELECTRA + ReLU Dense layer model achieves the best performance with 72% classification accuracy
- Fleiss Kappa score of 0.814 indicates strong inter-annotator agreement
- Dataset contains 500 offensive and 1543 neutral tweets, providing a valuable resource for Ukrainian offensive language detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-labeling with transformer models reduces the need for human annotation in sensitive content domains.
- Mechanism: A transformer model is fine-tuned on a small manually annotated dataset, then used to generate probabilistic labels for a larger dataset. Human annotators only verify uncertain labels (probabilities between 0.40 and 0.55), reducing overall human effort.
- Core assumption: The transformer model's probabilities are sufficiently reliable to trust for most examples, requiring human intervention only for borderline cases.
- Evidence anchors: [abstract] "The proposed method involves three stages of data annotation, combining human expertise and machine learning models to label 2043 tweets"; [section 2.3] "We follow a methodology offered by Kuligowska and Kowalczuk (2021), where authors use the DistilBERT model to distinguish questions from answers."

### Mechanism 2
- Claim: Using ELECTRA with ReLU Dense layer architecture provides the best balance of performance and overfitting resistance for small Ukrainian datasets.
- Mechanism: ELECTRA's discriminator training objective is more sample-efficient than masked language modeling, allowing better performance on limited data. The ReLU activation in the dense layer prevents vanishing gradients while maintaining computational efficiency.
- Core assumption: ELECTRA's architecture is inherently better suited for Ukrainian language tasks than other transformer variants when training data is limited.
- Evidence anchors: [section 2.3] "We utilize a simple ELECTRA + ReLU Dense layer model at this stage as it slightly outperforms the previous ELECTRA architecture."; [section 2.3] "The best-performing architecture in the second stage is ELECTRA + BiLSTM; however, it tends to overfit due to the small corpus size"

### Mechanism 3
- Claim: Four-category annotation scheme (offensive language/neutral language × offensive sense/neutral sense) captures both explicit and implicit offensive content better than binary classification.
- Mechanism: By separating language content from semantic meaning, annotators can distinguish between tweets with offensive words but neutral intent versus neutral words with offensive intent, leading to more nuanced labeling.
- Core assumption: Offensive content in Ukrainian war tweets often appears through contextual implications rather than explicit profanity, requiring this nuanced categorization.
- Evidence anchors: [section 2.3] "These labels allow people to make more accurate judgments about the context."; [section 2.3] "Tweets under the labels 'Offensive language, offensive sense' and 'Neutral language, offensive sense' are straightforward in their semantic manifestation, which can be conveyed through explicit or implicit markers."

## Foundational Learning

- Concept: Inter-annotator agreement (IAA) measurement
  - Why needed here: The study requires validation that different annotators are consistently labeling tweets as offensive or neutral, which is critical for dataset quality
  - Quick check question: What does a Fleiss Kappa score of 0.814 indicate about annotator agreement compared to a score of 0.384?

- Concept: Transformer model fine-tuning
  - Why needed here: The pseudo-labeling approach depends on adapting pre-trained Ukrainian language models to the specific task of offensive language detection
  - Quick check question: Why might ELECTRA be preferred over BERT for this task when training data is limited?

- Concept: Probability calibration and threshold selection
  - Why needed here: The pseudo-labeling process relies on selecting appropriate probability thresholds (0.40-0.55) for human verification
  - Quick check question: What would happen if the probability threshold for human verification was set too high (e.g., 0.90) versus too low (e.g., 0.30)?

## Architecture Onboarding

- Component map: Data collection → Initial manual annotation (Stage 1) → Model fine-tuning → Pseudo-label generation → Human verification of uncertain cases (Stage 2) → Final model training → Dataset compilation (Stage 3)
- Critical path: Manual annotation → Model training → Pseudo-label generation → Human verification → Final dataset creation
- Design tradeoffs: More complex annotation scheme (4 categories) vs. simpler binary classification; higher initial human effort vs. reduced total annotation burden
- Failure signatures: Low IAA scores indicating inconsistent labeling; model overfitting on small datasets; poor probability calibration leading to incorrect pseudo-labels
- First 3 experiments:
  1. Test different probability thresholds for human verification to find optimal balance between automation and accuracy
  2. Compare ELECTRA + ReLU Dense layer performance against other architectures on held-out validation set
  3. Measure IAA using different annotation schemes to validate the 4-category approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of pseudo-labeling for offensive language detection in Ukrainian compare to other low-resource languages?
- Basis in paper: [explicit] The paper discusses the use of pseudo-labeling techniques for annotating sensitive content in Ukrainian tweets, but does not compare its performance to other low-resource languages.
- Why unresolved: The study focuses on Ukrainian tweets and does not provide a comparative analysis with other low-resource languages.
- What evidence would resolve it: Comparative studies of pseudo-labeling techniques across different low-resource languages would provide insights into the generalizability and effectiveness of the approach.

### Open Question 2
- Question: What are the long-term effects of using pseudo-labeling techniques on the quality and consistency of offensive language detection models?
- Basis in paper: [inferred] The paper mentions the use of pseudo-labeling techniques but does not discuss the long-term effects on model quality and consistency.
- Why unresolved: The study focuses on the immediate results of pseudo-labeling but does not address potential long-term impacts on model performance.
- What evidence would resolve it: Longitudinal studies tracking the performance of models trained with pseudo-labeled data over time would provide insights into the long-term effects of this approach.

### Open Question 3
- Question: How do cultural and contextual factors influence the annotation of offensive language in Ukrainian tweets?
- Basis in paper: [explicit] The paper discusses the challenges of annotating sensitive content related to the Russian-Ukrainian war, indicating the influence of cultural and contextual factors.
- Why unresolved: The study acknowledges the impact of cultural and contextual factors but does not provide a detailed analysis of their specific influence on annotation.
- What evidence would resolve it: In-depth qualitative studies examining the role of cultural and contextual factors in the annotation process would provide a better understanding of their impact on offensive language detection.

## Limitations
- Limited dataset size (2043 tweets) restricts generalizability to broader offensive language detection tasks
- Pseudo-labeling effectiveness depends on initial model quality without intermediate validation metrics
- Four-category annotation scheme lacks empirical validation showing superiority over simpler binary classification

## Confidence
**High Confidence**: The core methodology of combining human annotation with pseudo-labeling is sound and well-established. The reported Fleiss Kappa score of 0.814 for inter-annotator agreement indicates strong consensus among annotators, supporting the reliability of the dataset.

**Medium Confidence**: The ELECTRA + ReLU Dense layer architecture's superiority over other models is demonstrated within the study's specific context, but the limited dataset size and single-task focus mean these results may not generalize to other Ukrainian NLP tasks or larger datasets.

**Low Confidence**: The effectiveness of the four-category annotation scheme compared to simpler alternatives is asserted but not empirically validated. The probability threshold selection for human verification (0.40-0.55) appears arbitrary without sensitivity analysis showing why these specific values were chosen.

## Next Checks
1. **Probability Threshold Sensitivity Analysis**: Systematically vary the probability thresholds for human verification (e.g., 0.30-0.40, 0.55-0.65, 0.70-0.80) and measure the impact on final dataset quality and model performance to validate the chosen range.

2. **Annotation Scheme Comparison**: Conduct a controlled experiment comparing the four-category annotation scheme against binary classification using the same tweets and annotators to empirically measure any performance improvements.

3. **Cross-Validation on Expanded Data**: Test the ELECTRA + ReLU Dense layer architecture on an expanded dataset (minimum 5000 tweets) with k-fold cross-validation to assess whether the reported performance advantages hold with larger sample sizes and better statistical power.