---
ver: rpa2
title: On the Convergence of Bounded Agents
arxiv_id: '2307.11044'
source_url: https://arxiv.org/abs/2307.11044
tags:
- agent
- state
- convergence
- agents
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores the concept of convergence for bounded reinforcement
  learning agents in environments without explicit state. It introduces two new definitions
  of agent convergence: (1) behavior convergence, measured by the minimal number of
  states needed to describe future agent behavior, and (2) performance convergence,
  measured by the maximum change in agent performance across repeated visits to the
  same agent state.'
---

# On the Convergence of Bounded Agents

## Quick Facts
- arXiv ID: 2307.11044
- Source URL: https://arxiv.org/abs/2307.11044
- Reference count: 20
- Key outcome: Introduces behavior and performance convergence definitions for bounded reinforcement learning agents in environments without explicit state.

## Executive Summary
This paper explores the concept of convergence for bounded reinforcement learning agents in environments without explicit state. It introduces two new definitions of agent convergence: behavior convergence, measured by the minimal number of states needed to describe future agent behavior, and performance convergence, measured by the maximum change in agent performance across repeated visits to the same agent state. The paper proves that both convergence types exist for all agent-environment pairs and demonstrates that they can occur at different times in general environments, establishing them as distinct concepts.

## Method Summary
The paper develops a theoretical framework for analyzing bounded agents with finite state spaces interacting with environments. It defines behavior convergence through a non-increasing sequence measuring the minimal number of states required to reproduce future behavior, and performance convergence through another non-increasing sequence measuring performance distortion when returning to the same internal state. The authors prove that both limiting sequences exist for all agent-environment pairs using formal mathematical arguments.

## Key Results
- Behavior and performance convergence can occur at different times in general environments
- Both limiting size and limiting distortion sequences exist for all agent-environment pairs
- The framework provides a natural way to define convergence without explicit environment states
- Results are agnostic to the choice of performance function v

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Convergence in bounded agents can be detected by tracking minimal state size needed to reproduce future behavior.
- Mechanism: The algorithm defines a non-increasing sequence $c_t(\lambda,e)$ that measures the smallest number of states required to mimic the agent's behavior from time $t$ onward. When this sequence stabilizes, the agent has converged behaviorally.
- Core assumption: The agent's behavior can be fully characterized by a finite state machine, and the minimal size reflects behavioral equivalence.
- Evidence anchors:
  - [abstract] "The first view says that a bounded agent has converged when the minimal number of states needed to describe the agent's future behavior cannot decrease."
  - [section 3.1] "The minimal size from time t of agent $\lambda$ in environment $e$ is denoted $c_t(\lambda,e) = \min\{n \in \mathbb{N} : \forall h \in \bar{H}_t^\infty \exists \lambda_n \in \Lambda_n \forall h' \in \bar{H}_{\lambda}(hh') = \lambda_n(hh')\}$"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: If the agent's policy becomes more complex over time, the minimal size could increase, violating the non-increasing property.

### Mechanism 2
- Claim: Convergence in bounded agents can be detected by tracking performance distortion across repeated agent states.
- Mechanism: The algorithm defines a non-increasing sequence $\delta_t(\lambda,e)$ that measures the maximum performance gap when the agent returns to the same internal state. When this sequence reaches zero, the agent has converged performance-wise.
- Core assumption: Performance can be captured as a function of agent state, and non-stationarity in performance indicates lack of convergence.
- Evidence anchors:
  - [abstract] "The second view says that a bounded agent has converged just when the agent's performance only changes if the agent's internal state changes."
  - [section 4.1] "The distortion from time t of $\lambda$ in $e$ is $\delta_t(\lambda,e) = \sup_{(h,h') \in H_t^\circ} |v(\lambda,e|h) - v(\lambda,e|hh')|$"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: If the environment introduces performance-relevant non-stationarity that correlates with agent states, the distortion may never reach zero.

### Mechanism 3
- Claim: Bounded agents provide a natural framework for defining convergence without explicit environment states.
- Mechanism: By modeling agents with finite state spaces and history-to-state mappings, the framework can define convergence in terms of agent-centric properties rather than environment-centric ones.
- Core assumption: Real agents are bounded in their computational resources and memory, making finite state representations appropriate.
- Evidence anchors:
  - [abstract] "This paper inspects the concept of agent convergence in a framing of RL focused on bounded agents."
  - [section 2.1] "One advantage of the perspective on RL that emphasizes agents is that it invites questions regarding the nature of the agents we are interested in."
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: If agents can be effectively modeled with unbounded representations, the bounded framework may be unnecessarily restrictive.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The paper contrasts convergence definitions in general environments with standard MDP convergence notions to validate the new definitions.
  - Quick check question: In an MDP, what does it mean for an agent to have converged in the standard sense?

- Concept: Partially Observable MDPs (POMDPs)
  - Why needed here: The paper mentions that its framework can express the same problems modeled by infinite-state POMDPs, providing context for the generality of the approach.
  - Quick check question: How does the absence of explicit environment state in this framework differ from a POMDP?

- Concept: Reinforcement Learning (RL) problem formulation
  - Why needed here: The paper builds its definitions on the standard RL problem structure of agents interacting with environments to maximize performance.
  - Quick check question: What are the key components of the RL problem formulation used in this paper?

## Architecture Onboarding

- Component map:
  - History space: All possible action-observation sequences
  - Environment function: Maps histories and actions to observation distributions
  - Agent function: Maps histories to action distributions
  - State-update function: Maps current state, action, and observation to next state
  - Policy function: Maps states to action distributions
  - Performance function: Maps histories to scalar performance values

- Critical path: History → State Update → Policy → Action → Environment → Observation → History

- Design tradeoffs:
  - Deterministic vs. stochastic state updates (paper uses deterministic for bounded agents)
  - Memory vs. computational efficiency in state representations
  - Granularity of state space (too fine-grained may not capture behavioral equivalence)

- Failure signatures:
  - Performance distortion remains high despite apparent behavioral convergence
  - Minimal size sequence decreases indefinitely (should stabilize)
  - Agent state space becomes too large to be practical

- First 3 experiments:
  1. Implement a simple bandit environment and test whether a greedy agent converges behaviorally (minimal size = 1) and in performance (distortion = 0).
  2. Create an MDP with two states where one agent memorizes the last observation and another doesn't; compare their convergence properties.
  3. Design an environment where performance-relevant non-stationarity exists but isn't captured by agent state; verify that distortion remains high despite behavioral convergence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we design efficient algorithms to estimate the limiting size or distortion of an actual agent-environment pair?
- Basis in paper: [explicit] The authors suggest this as a potential future research direction in the conclusion.
- Why unresolved: The paper establishes the theoretical existence of these measures but doesn't provide practical estimation methods.
- What evidence would resolve it: Development of algorithms that can accurately approximate limiting size or distortion in finite time for real-world agent-environment interactions.

### Open Question 2
- Question: How do behavior and performance convergence relate in environments that cannot be modeled as MDPs?
- Basis in paper: [explicit] Proposition 4.6 proves they can occur at different times in general environments.
- Why unresolved: The paper only shows they can differ but doesn't characterize when or how they differ systematically.
- What evidence would resolve it: Analysis of specific non-MDP environments showing systematic relationships or tradeoffs between behavior and performance convergence.

### Open Question 3
- Question: What canonical agents converge to in general environments beyond bandits and MDPs?
- Basis in paper: [inferred] The paper notes this is an open question in the analysis of limiting size.
- Why unresolved: The paper only characterizes limiting size for simple cases (bandits and MDPs).
- What evidence would resolve it: Identification of general agent classes or convergence patterns that emerge in various classes of non-MDP environments.

## Limitations
- The framework assumes deterministic state updates for bounded agents, which may not capture all practical implementations
- The analysis focuses on theoretical existence proofs rather than empirical validation on specific algorithms
- The performance function v is left abstract, though authors claim results are agnostic to its choice

## Confidence
- High confidence in the mathematical proofs establishing convergence existence
- Medium confidence in the practical relevance of the convergence definitions, as no empirical validation is provided
- Low confidence in how well these definitions capture convergence in complex, non-stationary environments

## Next Checks
1. Implement a simple bounded Q-learning agent and empirically verify whether the minimal size and distortion sequences converge as predicted
2. Test the framework on an environment with performance-relevant non-stationarity to validate that distortion remains high despite behavioral convergence
3. Apply the convergence definitions to a POMDP and compare with standard convergence notions in that setting