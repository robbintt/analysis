---
ver: rpa2
title: 'BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with
  Non-textual Features for CTR Prediction'
arxiv_id: '2308.11527'
source_url: https://arxiv.org/abs/2308.11527
tags:
- non-textual
- features
- data
- textual
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BERT4CTR, a framework that combines pre-trained
  language models with non-textual features for click-through rate (CTR) prediction.
  Traditional approaches either treat the language model and non-textual features
  separately (cascading or shallow interaction) or directly feed non-textual features
  as tokens to the transformer layers (NumBERT), both of which have limitations.
---

# BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction

## Quick Facts
- arXiv ID: 2308.11527
- Source URL: https://arxiv.org/abs/2308.11527
- Reference count: 40
- Key outcome: Achieves 0.6% AUC gain over NumBERT with 64% training cost and 52% inference cost reduction on 90 non-textual features

## Executive Summary
BERT4CTR is a framework that efficiently combines pre-trained language models with non-textual features for click-through rate prediction. Traditional approaches either handle modalities separately or embed non-textual features as tokens, both with limitations. BERT4CTR introduces Uni-Attention, which uses non-textual features as Queries and textual features as Keys/Values, enabling cross-modal learning while avoiding positional semantics issues. Combined with dimensionality reduction and two-steps joint-training, BERT4CTR achieves higher accuracy and efficiency than state-of-the-art methods.

## Method Summary
BERT4CTR integrates pre-trained language models with non-textual features through a three-component approach. First, Uni-Attention allows non-textual features to attend to textual features without positional embeddings, enabling cross-modal learning. Second, dimensionality reduction projects high-dimensional non-textual features into fixed-size embeddings, significantly reducing computational costs. Third, two-steps joint-training pre-trains each modality separately before fine-tuning them jointly, improving convergence and accuracy. The framework uses BERT-based architectures (RoBERTa-24 for Bing Ads, BERT-12 for KDD CUP 2012) and is trained on large-scale datasets with millions of samples.

## Key Results
- Achieves 0.6% AUC gain over NumBERT on commercial dataset with 90 non-textual features
- Reduces training time by 64% and inference time by 52% compared to NumBERT
- Outperforms cascading and shallow interaction approaches on both commercial and public datasets
- Maintains statistically significant accuracy improvements while achieving substantial efficiency gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uni-Attention enables cross-information learning between textual and non-textual features early in the fine-tuning process.
- Mechanism: Uni-Attention uses the non-textual tokens as Queries and textual tokens as Keys and Values. This hybrid attention allows non-textual features to access semantic context from textual features while avoiding the positional semantics problem of non-textual inputs.
- Core assumption: The semantic relationships between textual tokens are better learned by standard bidirectional self-attention, while cross-modal interactions are best learned through the Uni-Attention mechanism.
- Evidence anchors:
  - [abstract] "Our approach is based on a Uni-Attention mechanism integrating the semantic extraction from textual features, with cross-information between textual and non-textual features."
  - [section 3.2.2] "In the uni-attention mechanism, the non-textual components have no positional-embedding, which avoids the issue on positional semantics described above."
  - [corpus] Weak evidence; the cited papers focus on non-textual modalities but not directly on Uni-Attention for CTR.
- Break condition: If Uni-Attention weights collapse to uniform or zero values, cross-modal learning fails.

### Mechanism 2
- Claim: Dimensionality reduction preserves most predictive power while reducing training and inference costs.
- Mechanism: Dense non-textual features are normalized and embedded into fixed-size vectors via an embedding table; sparse features use direct lookup tables. This mapping reduces the total number of input tokens.
- Core assumption: The embedding space (e.g., 512 dimensions) is rich enough to encode the relevant information from the original high-dimensional non-textual features.
- Evidence anchors:
  - [section 3.2.3] "Table 3 reports that AUC and RIG for both alternative models are close... no one of the performance differences is statistically significant."
  - [section 3.2.3] "Table 4... dimensionality reduction reduces strongly the time-cost, up to 45% of training cost and 24% of inference cost..."
  - [corpus] Weak; cited papers discuss embeddings but not the specific reduction used here.
- Break condition: If the embedding dimension is too small, information loss causes accuracy drop.

### Mechanism 3
- Claim: Two-steps joint-training improves accuracy by pre-training each modality separately before joint fine-tuning.
- Mechanism: Warm-up step pre-trains the textual model (MLM) and non-textual model (MLP) separately; joint-training step initializes from these weights and fine-tunes jointly with a small learning rate.
- Core assumption: Separate pre-training stabilizes each modality before they interact, preventing catastrophic forgetting.
- Evidence anchors:
  - [section 3.2.4] "We demonstrate this two-steps joint-training in Figure 4. The results in Section 4 will show that the two-steps joint-training provides significant AUC gain on both commercial and public data sets."
  - [section 4.2.4] "Table 5 shows the AUC/RIG performance... two-steps joint-training brings significant gain... AUC gain is more than 0.3% , and more than 0.4% over KDD CUP 2012 data."
  - [corpus] Weak; similar strategies exist but not specifically for CTR with Uni-Attention.
- Break condition: If joint-training step diverges due to learning rate misconfiguration, gains vanish.

## Foundational Learning

- Concept: Transformer self-attention mechanism
  - Why needed here: BERT4CTR extends the transformer architecture with Uni-Attention, so understanding how QKV attention works is essential.
  - Quick check question: In standard self-attention, what matrices are used as Queries, Keys, and Values?
- Concept: Embedding lookup tables for sparse features
  - Why needed here: Non-textual sparse features are converted to dense vectors via lookup tables; this requires understanding embedding tables.
  - Quick check question: How do you handle unseen sparse feature values during inference?
- Concept: Dimensionality reduction techniques (embedding projection)
  - Why needed here: Non-textual features are projected from high-dimensional space to lower dimension; knowing projection matrices and normalization is key.
  - Quick check question: Why is max-min normalization used before embedding dense features?

## Architecture Onboarding

- Component map: Input layer -> Embedding layer -> Textual self-attention + Uni-Attention -> Fusion layer -> MLP -> Output
- Critical path: Input -> Embedding -> Textual self-attention + Uni-Attention -> Fusion -> MLP -> Prediction
- Design tradeoffs:
  - Uni-Attention vs full bidirectional attention on all tokens: Uni-Attention reduces token count and avoids positional semantics issues but may limit some cross-modal interactions.
  - Dimensionality reduction vs full feature representation: Lower dimensions speed up training/inference but risk losing predictive signal.
  - Two-step training vs single-step: Better convergence but longer total training time.
- Failure signatures:
  - Uni-Attention weights near zero -> no cross-modal learning
  - Embedding dimension too low -> accuracy drop, stable training
  - Large learning rate in joint step -> training instability
- First 3 experiments:
  1. Replace Uni-Attention with standard self-attention on all tokens; measure accuracy and training time.
  2. Remove dimensionality reduction; train with full non-textual feature count; compare training/inference latency.
  3. Skip two-step training; initialize randomly and train jointly; observe convergence and final accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BERT4CTR scale with the number of non-textual features beyond the 90 used in the commercial dataset?
- Basis in paper: [explicit] The paper states that BERT4CTR "scales well with the growing of non-textual features" but does not provide empirical data beyond the 90-feature commercial dataset.
- Why unresolved: The paper only evaluates BERT4CTR on datasets with 57 and 90 non-textual features. Scaling to larger feature sets (e.g., 200+ features) is a common scenario in industrial applications, but the framework's performance and efficiency in these cases are not empirically validated.
- What evidence would resolve it: Experiments evaluating BERT4CTR on datasets with significantly more non-textual features (e.g., 200, 500, or 1000 features) would demonstrate how well the framework scales in terms of accuracy, training time, and inference latency.

### Open Question 2
- Question: How robust is BERT4CTR to noisy or irrelevant non-textual features in the input data?
- Basis in paper: [inferred] The paper does not discuss feature selection or noise handling mechanisms. In real-world CTR prediction, non-textual features often include noisy or irrelevant attributes, which could impact model performance.
- Why unresolved: The paper assumes clean and relevant non-textual features but does not address scenarios where features may be noisy, redundant, or irrelevant. This is a critical aspect for practical deployment in industrial settings.
- What evidence would resolve it: Experiments testing BERT4CTR's performance on datasets with intentionally injected noise or irrelevant features would clarify its robustness and the need for feature selection or denoising mechanisms.

### Open Question 3
- Question: How does BERT4CTR compare to other transformer-based models (e.g., ViT, GPT-style models) when integrating non-textual features for CTR prediction?
- Basis in paper: [explicit] The paper compares BERT4CTR to NumBERT, Shallow Interaction, and cascading frameworks but does not evaluate it against other transformer-based architectures like Vision Transformers (ViT) or GPT-style models adapted for multi-modal inputs.
- Why unresolved: The paper focuses on BERT-based architectures, leaving open the question of whether other transformer designs could achieve similar or better performance when handling non-textual features.
- What evidence would resolve it: Comparative experiments between BERT4CTR and other transformer-based models (e.g., ViT, GPT-style models) adapted for CTR prediction would provide insights into the relative strengths and weaknesses of different architectures for multi-modal integration.

## Limitations
- The Uni-Attention mechanism's unidirectional design may limit richer cross-modal interactions that could arise from bidirectional attention between all tokens.
- Dimensionality reduction analysis is limited to two alternative dimensions without exploring the full tradeoff space between accuracy and efficiency.
- Two-steps joint-training hyperparameters are not fully specified, making it unclear whether gains are robust to hyperparameter variations.

## Confidence
- **High confidence**: Experimental results demonstrating BERT4CTR's superior AUC performance over NumBERT and other baselines are well-supported by reported metrics on both commercial and public datasets.
- **Medium confidence**: Efficiency gains from dimensionality reduction are supported by time-cost measurements, but analysis is limited to two alternative dimensions.
- **Medium confidence**: Uni-Attention mechanism's design rationale is clearly explained, but lack of ablation studies comparing it to alternative attention architectures limits confidence in its optimality.

## Next Checks
1. **Ablation study on attention architecture**: Implement and train a variant of BERT4CTR using standard bidirectional self-attention on all tokens (textual and non-textual) instead of Uni-Attention. Compare AUC, training time, and inference time to assess whether the unidirectional design is necessary or optimal.

2. **Dimensionality reduction sensitivity analysis**: Train BERT4CTR with multiple intermediate embedding dimensions (e.g., 256, 384, 640, 768) between the current 512 and the original 1792. Measure AUC and time-costs across this range to identify whether the 512-dimension choice represents the best accuracy-efficiency tradeoff.

3. **Hyperparameter robustness testing**: Systematically vary the learning rates and epoch counts in both the warm-up and joint-training phases. Train models with different hyperparameter combinations and assess whether the reported AUC gains from two-steps joint-training persist across a range of settings or are sensitive to specific values.