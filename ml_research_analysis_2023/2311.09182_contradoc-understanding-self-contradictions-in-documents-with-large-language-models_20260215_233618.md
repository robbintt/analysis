---
ver: rpa2
title: 'ContraDoc: Understanding Self-Contradictions in Documents with Large Language
  Models'
arxiv_id: '2311.09182'
source_url: https://arxiv.org/abs/2311.09182
tags:
- document
- documents
- self-contradictions
- self-contradiction
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CONTRA DOC, the first human-annotated dataset
  for studying self-contradictions in long documents. The dataset covers multiple
  domains, varying document lengths, and different types of self-contradictions.
---

# ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models

## Quick Facts
- arXiv ID: 2311.09182
- Source URL: https://arxiv.org/abs/2311.09182
- Reference count: 26
- This paper introduces CONTRA DOC, the first human-annotated dataset for studying self-contradictions in long documents.

## Executive Summary
This paper introduces CONTRA DOC, the first human-annotated dataset for studying self-contradictions in long documents. The dataset covers multiple domains, varying document lengths, and different types of self-contradictions. The authors evaluate four state-of-the-art LLMs (GPT3.5, GPT4, PaLM2, and LLaMAv2) on this dataset using three evaluation tasks: binary judgment, self-contradiction top-k, and judge-then-find. While GPT4 performs the best and can outperform humans on this task, the results show that all models struggle with detecting self-contradictions, especially those requiring more nuance and context. The dataset and associated code are released to the community for further research and development of better document-level reasoning capabilities in LLMs.

## Method Summary
The authors create the CONTRA DOC dataset through a human-machine collaborative framework, generating self-contradictory documents across three domains (News, Stories, Wikipedia) with varying lengths and eight types of contradictions. They then evaluate four LLMs (GPT3.5, GPT4, PaLM2, LLaMAv2) using zero-shot prompting on three tasks: Binary Judgment (determining if a document is self-contradictory), Self-Contradiction Top-k (identifying and ranking the most probable sentences indicating self-contradictions), and Judge-then-Find (combining judgment with evidence-based verification). The evaluation uses metrics including precision, recall, F1 score, accuracy, and Evidence Hit Rate (EHR).

## Key Results
- GPT4 performs the best among evaluated models and can outperform humans on self-contradiction detection tasks
- All models struggle with detecting self-contradictions that require more nuance and context, especially subjective types like Emotion/Mood/Feeling and Perspective/View/Opinion
- Document length does not significantly impact model performance, but global contradictions (far apart in document) are easier to detect than local or intra contradictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle with detecting self-contradictions that require nuanced understanding of subjective emotions or perspectives.
- Mechanism: The models fail to integrate contextual cues and emotional nuances when evaluating contradictory statements, leading to poor performance on contradiction types like "Emotion/Mood/Feeling" and "Perspective/View/Opinion".
- Core assumption: Pre-training data and tasks emphasize factual verification over subjective reasoning, limiting models' ability to handle subjective contradictions.
- Evidence anchors:
  - [abstract]: "we find that it is still unreliable and struggles with self-contradictions that require more nuance and context"
  - [section 4.5]: "more subjective self-contradiction types like Emotion/Mood/Feeling and Perspective/View/Opinion are hard"
  - [corpus]: Weak - corpus does not contain direct evidence on subjective reasoning limitations
- Break condition: If models are fine-tuned on subjective reasoning tasks or given explicit context about emotional/perspective cues, their performance on these contradiction types may improve.

### Mechanism 2
- Claim: Document length does not significantly impact the models' ability to detect self-contradictions.
- Mechanism: The models can process and analyze long documents effectively, maintaining performance across varying document lengths (100-2200 tokens).
- Core assumption: LLMs' long context windows allow them to handle document-level reasoning without significant performance degradation.
- Evidence anchors:
  - [section 4.5]: "For both GPT3.5 and GPT4, there is no significant drop in EHR as the document length increases"
  - [abstract]: "varying document lengths" are included in the dataset to test this hypothesis
  - [corpus]: Weak - corpus does not contain direct evidence on context window performance
- Break condition: If the models' context windows are exceeded or if the documents contain complex nested contradictions, performance may degrade.

### Mechanism 3
- Claim: Global contradictions (those far apart in the document) are easier for models to detect than local or intra contradictions.
- Mechanism: Global contradictions often involve more objective types like "Negation" and "Numeric", which are easier for LLMs to detect, while local and intra contradictions involve more subjective types.
- Core assumption: The distribution of contradiction types across scopes influences model performance, with objective types being easier to detect.
- Evidence anchors:
  - [section 4.5]: "global self-contradictory documents had a higher EHR than local and intra"
  - [section 4.5]: "Global contradictions have more Negation and Numeric self-contradiction types, which could be easier for LLMs to detect"
  - [corpus]: Weak - corpus does not contain direct evidence on contradiction type distributions
- Break condition: If the distribution of contradiction types changes or if models are trained to handle subjective contradictions better, the performance gap between global and local contradictions may decrease.

## Foundational Learning

- Concept: Natural Language Inference (NLI) and Recognizing Textual Entailment (RTE)
  - Why needed here: Understanding the framework for detecting contradictions between text pairs is crucial for grasping the task of self-contradiction detection in documents.
  - Quick check question: What is the difference between NLI and RTE, and how do they relate to detecting contradictions in text?

- Concept: Document-level reasoning and long-context processing
  - Why needed here: The ability to process and reason over long documents is essential for detecting self-contradictions that may be far apart in the text.
  - Quick check question: How do LLMs handle long documents, and what challenges arise when reasoning over extended text?

- Concept: Types of self-contradictions and their detection
  - Why needed here: Understanding the different types of self-contradictions (e.g., negation, numeric, content, perspective) is crucial for evaluating model performance and designing effective detection strategies.
  - Quick check question: What are the main types of self-contradictions, and how do they differ in terms of detection difficulty for LLMs?

## Architecture Onboarding

- Component map:
  - Dataset creation pipeline: LLM-based contradictory statement generation -> human verification and tagging
  - Evaluation tasks: Binary Judgment -> Self-Contradiction Top-k -> Judge-then-Find
  - Models: GPT3.5 -> GPT4 -> PaLM2 -> LLaMAv2
  - Metrics: Precision, Recall, F1 Score, Accuracy, Evidence Hit Rate (EHR)

- Critical path:
  1. Create self-contradictory documents using the human-machine collaborative framework
  2. Evaluate models on the three tasks using the specified metrics
  3. Analyze model performance based on document types, lengths, contradiction ranges, and types

- Design tradeoffs:
  - Dataset creation: Balancing the diversity of contradiction types and document sources with the quality and reliability of human annotations
  - Evaluation tasks: Choosing between simple binary judgments and more complex evidence-based tasks to assess model capabilities
  - Model selection: Using a mix of open-source and closed-source models to provide a comprehensive evaluation of current LLM capabilities

- Failure signatures:
  - Models consistently failing to detect self-contradictions, especially those requiring nuanced understanding
  - Performance degradation as document length increases (although this was not observed in the study)
  - Models struggling with specific contradiction types, such as those involving subjective emotions or perspectives

- First 3 experiments:
  1. Evaluate models on the Binary Judgment task to assess their ability to distinguish between self-contradictory and non-contradictory documents
  2. Test models on the Self-Contradiction Top-k task to determine their ability to identify and rank the most probable sentences indicating self-contradictions
  3. Assess models on the Judge-then-Find task to evaluate their performance in both judging the presence of self-contradictions and providing supporting evidence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs' performance on self-contradiction detection vary across different document domains (news, stories, Wikipedia) and why?
- Basis in paper: [explicit] The paper explicitly compares model performance across news, stories, and Wikipedia domains, finding differences in Evidence Hit Rate.
- Why unresolved: While the paper shows performance differences, it doesn't deeply investigate the underlying reasons for these domain-specific variations or how to improve performance in weaker domains.
- What evidence would resolve it: Detailed analysis of model attention patterns and reasoning steps across domains, plus experiments testing targeted improvements for weaker domains.

### Open Question 2
- Question: What is the relationship between document length and self-contradiction detection difficulty, and how does this vary by contradiction type?
- Basis in paper: [explicit] The paper tests different document lengths but finds no significant drop in performance, suggesting length alone isn't the main factor.
- Why unresolved: The paper doesn't explore how document length interacts with different contradiction types or whether certain types become harder at different length scales.
- What evidence would resolve it: Systematic experiments varying document length and contradiction type combinations, plus analysis of model attention patterns across different length/type combinations.

### Open Question 3
- Question: How can LLMs be improved to better detect subjective contradiction types like Emotion/Mood/Feeling and Perspective/View/Opinion?
- Basis in paper: [explicit] The paper shows these subjective types are hardest for models to detect and suggests LLMs may lack sufficient training on emotion-checking tasks.
- Why unresolved: While the paper identifies this weakness, it doesn't propose or test specific methods for improving performance on these harder contradiction types.
- What evidence would resolve it: Experiments testing various training approaches, prompt engineering techniques, or specialized fine-tuning for subjective contradiction types.

## Limitations

- Dataset size: The CONTRA DOC dataset, while diverse, is relatively small (891 documents total), which may limit generalizability
- Zero-shot evaluation: Using only zero-shot prompting may underestimate the true capabilities of LLMs
- Human baseline methodology: The specific methodology for human evaluation is not detailed, making it difficult to assess the validity of comparisons

## Confidence

- LLMs struggle with nuanced self-contradictions: High Confidence
- Document length does not significantly impact performance: Medium Confidence
- Global contradictions are easier to detect than local ones: Medium Confidence

## Next Checks

1. Expand the CONTRA DOC dataset to include a larger and more diverse set of documents, particularly focusing on different contradiction types and document lengths. Validate the dataset using multiple human annotators to ensure reliability.

2. Conduct experiments with fine-tuning the LLMs on the CONTRA DOC dataset to assess whether model performance improves significantly compared to zero-shot evaluation.

3. Replicate the human evaluation study to establish a more robust baseline for comparison. Use multiple human annotators and detailed annotation guidelines to ensure consistency and reliability of results.