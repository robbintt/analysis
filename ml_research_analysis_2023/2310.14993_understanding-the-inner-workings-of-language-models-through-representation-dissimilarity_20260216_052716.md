---
ver: rpa2
title: Understanding the Inner Workings of Language Models Through Representation
  Dissimilarity
arxiv_id: '2310.14993'
source_url: https://arxiv.org/abs/2310.14993
tags:
- stitching
- layer
- layers
- solu
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Representation dissimilarity measures like model stitching and
  CKA can effectively probe internal features of language models, revealing insights
  invisible to standard test-set performance. Model stitching exposed an asymmetry
  between GeLU and SoLU activations: representations from SoLU models incur larger
  penalties when fed into GeLU models, suggesting GeLU activations preserve more useful
  information.'
---

# Understanding the Inner Workings of Language Models Through Representation Dissimilarity

## Quick Facts
- arXiv ID: 2310.14993
- Source URL: https://arxiv.org/abs/2310.14993
- Reference count: 26
- Primary result: Representation dissimilarity measures (model stitching, CKA) reveal internal model features invisible to test-set performance, including activation function asymmetries and feature divergence in larger models.

## Executive Summary
This paper introduces representation dissimilarity measures—model stitching and centered kernel alignment (CKA)—as tools to probe the internal features of language models beyond standard test-set performance. These methods expose structural differences between models, such as an asymmetry in representational capacity between GeLU and SoLU activation functions, and localize divergences between generalizing and heuristic BERT models. The analysis also uncovers an unexpected drop in feature similarity for a 2.8B-parameter Pythia model, likely due to architectural differences.

## Method Summary
The paper employs two primary representation dissimilarity techniques. Model stitching involves freezing two pre-trained models and training a small, learnable linear layer to connect intermediate features, with stitching loss indicating representational compatibility. CKA compares the statistical structure of activation distributions across models or layers using centered kernel alignment, providing a task-agnostic similarity score. Experiments use the Pile dataset for SoLU/GeLU and Pythia models, and MNLI for BERT models, with CKA computed using the unbiased HSIC1 estimator.

## Key Results
- Model stitching reveals an asymmetry: SoLU→GeLU stitching incurs larger penalties than GeLU→SoLU, suggesting GeLU activations preserve more information.
- CKA and stitching localize representational differences between generalizing and heuristic BERT models to later layers.
- Pythia scaling experiments show high feature similarity up to 1B parameters, with an unexpected drop at 2.8B likely due to architectural changes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model stitching with a learnable linear layer effectively probes whether two models share compatible internal representations.
- Mechanism: By freezing the original model weights and only training the stitching layer, the experiment isolates the transformation needed to map one model's representations to another's usable form, revealing representational similarity.
- Core assumption: If two models have similar representations, a simple (linear) transformation can bridge them without large performance loss.
- Evidence anchors:
  - [abstract] "Model stitching extracts features from earlier layers of model f and plugs them into the later layers of model g (possibly mediated by a small, learnable, connecting layer)"
  - [section 3] "we set our stitching layer φ to be a learnable linear layer (all other parameters of the stitched model are frozen)"
- Break condition: If the stitching layer requires excessive complexity or fails to converge, the representations are not sufficiently similar.

### Mechanism 2
- Claim: CKA measures structural similarity between representations by comparing statistical properties of activation distributions, independent of task performance.
- Mechanism: CKA computes the centered kernel alignment between feature matrices, yielding a value that reflects how similarly two models encode the same inputs.
- Core assumption: High CKA values indicate that the internal geometry of the representations is preserved between models, regardless of task-specific utility.
- Evidence anchors:
  - [section 2] "CKA compares the statistical structure of the representations obtained in two different models/layers from a fixed set of input datapoints, ignoring any relationship to performance"
  - [section 4] "CKA values between generalizing and heuristic models significantly exceeds its value within the generalizing and heuristic groups"
- Break condition: If the models use different tokenizations or preprocessing, CKA comparisons may be invalid.

### Mechanism 3
- Claim: Differences in activation functions (GeLU vs SoLU) lead to asymmetric representational capacity, detectable through stitching penalties.
- Mechanism: SoLU's softmax normalization shrinks small activations, reducing representational expressiveness; this asymmetry appears when stitching from SoLU to GeLU models incurs larger penalties than vice versa.
- Core assumption: The activation function fundamentally shapes the geometry of the feature space, and this difference manifests in representational interchangeability.
- Evidence anchors:
  - [section 3] "when f is a SoLU model... larger penalties are incurred than when f uses GeLUs"
  - [abstract] "an apparent asymmetry in the internal representations of model using SoLU and GeLU activation functions"
- Break condition: If the models are too small or the dataset too limited, the stitching penalty differences may not emerge clearly.

## Foundational Learning

- Concept: Representation dissimilarity measures
  - Why needed here: These metrics (model stitching, CKA) are the core tools for comparing internal features without relying on downstream performance.
  - Quick check question: Can you explain the difference between a task-centric measure (stitching) and a statistical measure (CKA)?

- Concept: Model stitching procedure
  - Why needed here: Understanding how stitching works (freezing models, learning a connector) is essential for interpreting the asymmetric results between GeLU and SoLU.
  - Quick check question: What would happen if you didn't freeze the original model weights during stitching?

- Concept: Centered Kernel Alignment (CKA)
  - Why needed here: CKA's computation and interpretation are key to understanding how feature similarity is quantified across model scales.
  - Quick check question: Why does CKA ignore task performance, and what does that imply for its use cases?

## Architecture Onboarding

- Component map:
  - Input: Tokenized text sequences (from Pile or MNLI datasets)
  - Models: Pre-trained transformers (GeLU/SoLU variants, BERT, Pythia families)
  - Analysis tools: Model stitching (learnable linear connector), CKA (statistical similarity)
  - Output: Loss/penalties (stitching), similarity scores (CKA), visualization of feature evolution

- Critical path:
  1. Load pre-trained models and prepare datasets
  2. Extract intermediate activations for given inputs
  3. For stitching: freeze models, train connector, evaluate loss
  4. For CKA: compute centered kernels, estimate similarity, aggregate results
  5. Analyze patterns across layers, models, and scales

- Design tradeoffs:
  - Stitching is task-dependent but can reveal functional representational gaps; CKA is task-agnostic but may miss utility differences.
  - Larger models increase computational cost for both methods.
  - Choice of dataset (in-distribution vs out-of-distribution) affects interpretability.

- Failure signatures:
  - Stitching layer fails to converge or requires excessive complexity
  - CKA scores are uniformly low across all comparisons (possible preprocessing mismatch)
  - Unexpected asymmetry in stitching penalties not reproducible with different seeds or datasets

- First 3 experiments:
  1. Replicate the GeLU vs SoLU stitching asymmetry on a small dataset (e.g., first 1000 examples of Pile)
  2. Compute CKA similarity between early and late layers within a single model to verify "block structure"
  3. Perform identity stitching between generalizing and heuristic BERT models on MNLI to reproduce the penalty difference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the asymmetry in information preservation between GeLU and SoLU activations be observed in larger transformer models?
- Basis in paper: [explicit] The paper demonstrates that SoLU models incur larger penalties when feeding their representations into GeLU models compared to the reverse, suggesting GeLU activations preserve more useful information. This was tested on small 3-4 layer models.
- Why unresolved: The experiments were conducted on relatively small models (9.4M-13M parameters), and larger models may exhibit different behavior due to increased capacity and architectural differences.
- What evidence would resolve it: Running similar stitching experiments on larger transformer models (e.g., 1B+ parameters) to see if the asymmetry persists or diminishes.

### Open Question 2
- Question: Can CKA or stitching reliably predict out-of-distribution generalization performance without access to OOD test sets?
- Basis in paper: [inferred] The paper shows that both CKA and stitching can distinguish between generalizing and heuristic models trained on MNLI using only in-distribution data, localizing the divergence to later layers.
- Why unresolved: While the experiments show promise, they only tested on one dataset (MNLI) and one type of distribution shift (HANS-LO). The generalizability of these findings to other tasks and shifts remains unknown.
- What evidence would resolve it: Applying CKA/stitching analysis to multiple model families across various tasks and distribution shifts to see if the method consistently predicts OOD generalization.

### Open Question 3
- Question: What causes the anomalous intermediate feature similarity in the pythia-2.8b model compared to other Pythia models?
- Basis in paper: [explicit] The paper observes that pythia-2.8b exhibits unusually low intermediate feature similarity with other Pythia models (1B, 1.4B, 6.9B) despite similar performance, and notes its architectural differences (smaller attention head dimension).
- Why unresolved: The exact mechanism by which the architectural differences lead to divergent features is not explained, and it's unclear if this is a fundamental limitation or can be mitigated through architectural adjustments.
- What evidence would resolve it: Systematically varying architectural parameters (attention head size, depth, width) in Pythia-like models to isolate which changes cause the feature similarity anomaly.

## Limitations

- The observed asymmetry between GeLU and SoLU stitching penalties may be sensitive to model scale, dataset composition, or training duration.
- CKA's ability to capture "meaningful" representational similarity is limited by its statistical nature—it may identify structural similarity without guaranteeing functional interchangeability.
- The attribution of stitching penalties solely to activation function properties, without ruling out other factors like layer normalization or positional embeddings.

## Confidence

- **High Confidence**: The effectiveness of model stitching and CKA as general tools for probing internal features, supported by multiple experiments and established methodology.
- **Medium Confidence**: The specific claims about GeLU/SoLU asymmetry and Pythia 2.8B feature similarity drop, as these require careful control of confounding variables (architecture, scale, training).
- **Low Confidence**: The attribution of stitching penalties solely to activation function properties, without ruling out other factors like layer normalization or positional embeddings.

## Next Checks

1. Apply the GeLU/SoLU stitching experiment to a different model family (e.g., GPT-2 variants) to test if the asymmetry generalizes beyond the tested models.
2. Replicate the CKA analysis on Pythia models, systematically varying model size to pinpoint where and why the 2.8B model diverges from the trend.
3. Modify SoLU models to use GeLU activations (or vice versa) layer-by-layer, then compare stitching penalties to isolate the contribution of the activation function.