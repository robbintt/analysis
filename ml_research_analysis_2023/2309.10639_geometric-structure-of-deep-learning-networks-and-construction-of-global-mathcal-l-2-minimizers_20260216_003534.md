---
ver: rpa2
title: Geometric structure of Deep Learning networks and construction of global ${\mathcal
  L}^2$ minimizers
arxiv_id: '2309.10639'
source_url: https://arxiv.org/abs/2309.10639
tags:
- where
- function
- cost
- networks
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper constructs explicit minimizers for the L2 cost function
  in underparametrized Deep Learning (DL) networks with L hidden layers, ReLU activation,
  and equal-dimensional input/output spaces. The key idea is to reinterpret the network's
  hidden layers as a recursive application of a "truncation map" that curates training
  inputs by minimizing their noise-to-signal ratio.
---

# Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers

## Quick Facts
- arXiv ID: 2309.10639
- Source URL: https://arxiv.org/abs/2309.10639
- Authors: 
- Reference count: 10
- Primary result: Constructs explicit global minimizers for L2 cost in underparametrized ReLU networks with clustered training inputs

## Executive Summary
This paper presents a geometric construction of global minimizers for the L2 cost function in underparametrized Deep Learning networks with L hidden layers, ReLU activation, and equal-dimensional input/output spaces. The key innovation is interpreting the network's hidden layers as a recursive application of a "truncation map" that curates training inputs by minimizing their noise-to-signal ratio. The authors construct a family of global minimizers parametrized by µ ∈ (−D, −2δ|uQ|)Q when L ≥ Q, and explicitly determine 2^Q − 1 distinct degenerate local minima.

## Method Summary
The method involves recursively applying "truncation maps" τW,b to clustered training inputs, parameterized by weights W and biases b, to curate inputs and minimize noise-to-signal ratio. For L ≥ Q, the construction uses weight matrices W(ℓ) = W*Rℓ and biases b(ℓ) = -W(ℓ)x0,ℓ + µℓRℓfℓ, with Rℓ rotating fj to uQ/|uQ|. The global minimum is achieved for µ ∈ (-D, -2δ|uQ|)Q, while local minima correspond to µ ∈ (2δ|uQ|, ∞)Q. The construction requires training inputs to satisfy specific geometric clustering conditions (δ < c0 min_j|x0,j - x|, max_j θ*,j < π) and relies on rank preservation properties of the truncation map.

## Key Results
- Explicit construction of global minimizers for L2 cost in underparametrized ReLU networks
- Characterization of 2^Q − 1 distinct degenerate local minima
- Geometric interpretation of input-output matching via a network-induced metric
- Proof that cost remains zero under infinitesimal perturbations of weights and biases at global minimum

## Why This Works (Mechanism)

### Mechanism 1
The global minimum is achieved when each training input cluster is mapped to a single point via the truncation map, effectively collapsing intra-class variance. The truncation map τW,b "curates" training inputs by recursively applying affine transformations followed by ReLU activation. When parameters are chosen such that the ReLU kills all variation within each cluster (µ < -2δ‖W‖op), all inputs in a cluster map to the same point, eliminating the residual term ∆(L)1 and thus achieving zero cost.

### Mechanism 2
Local minima arise when truncation maps preserve input variation within clusters, maintaining intra-class variance. When µ > 2δ‖W‖op, the truncation map becomes the identity on input variations (σ(W∆X0,j + B) = W∆X0,j + B), preserving the original ∆X0 structure. This creates a degenerate local minimum with non-zero cost determined by the original data geometry.

### Mechanism 3
The input space is endowed with a metric induced by the trained network that determines classification boundaries. The network implicitly defines a distance metric d(x, x') = ||Y(Xred0[µ])^{-1}(x - x')|| between inputs. Classification is performed by finding the nearest cluster center in this metric space, analogous to nearest-neighbor classification but in a transformed space.

## Foundational Learning

- Concept: Rank preservation in linear transformations
  - Why needed here: The truncation map must maintain full rank Q to ensure the network can represent all output classes
  - Quick check question: What happens to the cost function if the truncation map reduces rank from Q to Q-1?

- Concept: Convex geometry and cone conditions
  - Why needed here: The proof relies on geometric conditions (cones Cθ[fj]) to ensure inputs remain in regions where truncation maps behave predictably
  - Quick check question: Why is the condition maxj θ*,j < π necessary for maintaining convexity under small perturbations?

- Concept: Degenerate critical points in optimization
  - Why needed here: The constructed minima are degenerate (zero Hessian), which affects the stability and uniqueness of the solution
  - Quick check question: How does degeneracy of the minimum affect the convergence of gradient-based training methods?

## Architecture Onboarding

- Component map: Input layer (Q-dim clustered inputs) -> Hidden layers (recursive truncation maps) -> Output layer (linear transformation) -> Cost function (weighted L2 norm) -> Parameter µ (controls truncation behavior)

- Critical path:
  1. Verify geometric conditions on input clustering (δ, θ*,j)
  2. Construct weight matrices W(ℓ) = W*Rℓ with Rℓ rotating fj to uQ/|uQ|
  3. Set shifts b(ℓ) = -W(ℓ)x0,ℓ + µℓRℓfj
  4. For global minimum: choose µ ∈ (-D, -2δ|uQ|)Q
  5. For local minimum: choose µ ∈ (2δ|uQ|, ∞)Q

- Design tradeoffs:
  - Width vs depth: Fixed width Q but variable depth L ≥ Q
  - Cluster separation: Larger δ allows more aggressive truncation but risks losing rank preservation
  - Parameter range: Wider µ range provides more minima but may include non-optimal solutions

- Failure signatures:
  - Cost remains positive despite parameter tuning → rank preservation violated
  - Classification accuracy poor despite zero cost → metric d not aligned with true class boundaries
  - Numerical instability in matrix inversions → δ too large or clusters too close

- First 3 experiments:
  1. Verify rank preservation: Test τW,b on synthetic clustered data with varying δ, confirm rank(Q) holds
  2. Global minimum construction: Generate Q clusters, apply construction with µ ∈ (-D, -2δ|uQ|)Q, verify CN = 0
  3. Local minimum verification: Use same data, apply µ ∈ (2δ|uQ|, ∞)Q, verify CN > 0 but constant across µ range

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the results extend to the case where M ≠ Q (i.e., input/output dimensions differ from hidden layer dimensions)?
- Basis in paper: [explicit] The paper explicitly states that generalization to Mℓ > Q is left for future work.
- Why unresolved: The current analysis relies on the special case where all dimensions are equal (M = Mℓ = Q), which simplifies the geometric structure and truncation map analysis.
- What evidence would resolve it: A mathematical proof showing how the construction of minimizers and the geometric interpretation change when M ≠ Q, including whether global minima still exist and how their degeneracy is affected.

### Open Question 2
- Question: Are there additional local minima beyond the 2^Q − 1 explicitly constructed ones?
- Basis in paper: [inferred] The authors state that their analysis does not provide information about the existence or inexistence of further local minima beyond the 2^Q − 1 constructed.
- Why unresolved: The proof technique only constructs a specific set of local minima through the truncation map with different parameter regimes, but does not rule out other minima that might exist due to different network configurations.
- What evidence would resolve it: A complete characterization of the loss landscape, potentially through analysis of critical points or numerical exploration of the cost function's behavior for various parameter settings.

### Open Question 3
- Question: How robust are the global minimizers to perturbations in the training data clustering condition?
- Basis in paper: [explicit] The construction assumes training inputs are "sufficiently clustered" and satisfies condition (2.51) on the geometry of training inputs.
- Why unresolved: The analysis assumes a specific geometric condition on training input clustering, but does not explore how deviations from this condition affect the existence and properties of global minimizers.
- What evidence would resolve it: Mathematical analysis showing the sensitivity of the truncation map and minimizer construction to variations in the training data distribution, or empirical studies testing performance under different clustering scenarios.

## Limitations

- The construction requires highly structured training data with precisely clustered inputs that may not reflect real-world datasets
- The theoretical framework depends on strict geometric conditions (δ < c0 min_j|x0,j - x|, max_j θ*,j < π) that may be difficult to verify in practice
- While zero cost is guaranteed, the practical generalization performance and relationship to gradient-based training methods remains unclear

## Confidence

- Existence of global minima under stated conditions: High
- Characterization of degenerate local minima: Medium
- Input-output matching mechanism via induced metric: Medium

## Next Checks

1. **Robustness testing**: Apply the construction to synthetic datasets with controlled noise and varying cluster separation to identify the practical limits of δ and θ*,j constraints.

2. **Numerical verification**: Implement the truncation map construction and verify rank preservation properties across multiple random cluster configurations and dimensionalities.

3. **Gradient landscape analysis**: Compare the constructed minima to local minima found by gradient descent on the same cost function to understand the relationship between analytical and empirical solutions.