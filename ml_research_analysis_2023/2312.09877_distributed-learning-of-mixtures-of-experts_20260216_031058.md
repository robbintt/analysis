---
ver: rpa2
title: Distributed Learning of Mixtures of Experts
arxiv_id: '2312.09877'
source_url: https://arxiv.org/abs/2312.09877
tags:
- estimator
- local
- function
- experts
- reduction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a distributed learning approach for Mixture
  of Experts (MoE) models that enables parallel training on decentralized data subsets
  and aggregates local models into a global one via optimal transport. The method
  addresses the challenge of aggregating MoE models with correct number of components
  and provides a well-posed, consistent reduction estimator.
---

# Distributed Learning of Mixtures of Experts

## Quick Facts
- arXiv ID: 2312.09877
- Source URL: https://arxiv.org/abs/2312.09877
- Authors: 
- Reference count: 40
- Key outcome: Distributed MoE training achieves 3-10x speedup with comparable accuracy to centralized training

## Executive Summary
This paper presents a distributed learning framework for Mixture of Experts (MoE) models that enables parallel training on decentralized data subsets. The approach aggregates local MoE estimators into a global model using optimal transport theory, ensuring the final model has the correct number of components. The method demonstrates that distributed training can achieve comparable performance to centralized approaches while significantly reducing computation time, with experiments showing 3-10x speedups across various dataset sizes.

## Method Summary
The method involves distributing data across M local machines, each running EM to obtain local MoE estimators. These local parameters are then aggregated on a central server using a Majorization-Minimization (MM) algorithm that minimizes expected transportation divergence between the weighted average of local models and a K-component MoE. The aggregation process uses a supporting sample drawn from the full dataset to compute empirical expectations, ultimately producing a globally consistent reduction estimator that approximates the true MoE model.

## Key Results
- Distributed approach achieves 3-10x computation time reduction compared to centralized training
- Maintains similar accuracy metrics (MSE, RPE, ARI) to global centralized training
- With 10^6 samples and 128 machines: 6.32 minutes vs 57.32 minutes runtime
- RPE of 1.45% (distributed) vs 1.03% (centralized) with comparable accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The reduction estimator is consistent as soon as local estimators are consistent
- **Mechanism**: By minimizing the expected transportation divergence between the weighted average of local MoE models and a K-component MoE, the algorithm finds a global K-component MoE that approximates the true model. The MM algorithm converges to this minimizer, and under consistency of local estimators, the reduction estimator converges to the true parameter.
- **Core assumption**: Local estimators are consistent (converge to true parameters) and the cost function satisfies continuity and convexity properties.
- **Evidence anchors**:
  - [abstract] "We show that the provided reduction estimator is consistent as soon as the local estimators to be aggregated are consistent"
  - [section 3.2] "Let ¯θR be the parameter of the reduction density ¯f R... Suppose assumptions A1.-A2. are satisfied. Then ¯θR is a consistent estimator of θ∗"
  - [corpus] Weak - related works focus on HS-MoE, EM perspective, and robustness, not consistency proofs
- **Break condition**: If local estimators are inconsistent (e.g., due to insufficient data or poor initialization), the reduction estimator will also be inconsistent.

### Mechanism 2
- **Claim**: The MM algorithm monotonically decreases the objective function and converges to a local minimum
- **Mechanism**: At each iteration, the algorithm constructs a majorant function that upper bounds the objective at the current point and touches it at that point. Minimizing this majorant yields a point with equal or lower objective value, creating a descent sequence that converges.
- **Core assumption**: The majorant function is well-defined and the objective function is continuous and convex in the parameter space
- **Evidence anchors**:
  - [section 4] "The MM algorithm in our problem consists of starting from an initial model g(0) ∈ MK, then at each iteration t, find a majorant function for Rc(g) at g(t), and minimize it to obtain the next model g(t+1). The generated sequence ( g(t))t⩾1 satisfies Rc(g(0)) ⩾ Rc(g(1)) ⩾ . . ."
  - [appendix A.4] Proof of Proposition 4.1 showing the majorant property
  - [corpus] Weak - no direct evidence about MM convergence properties in related works
- **Break condition**: If the majorant function is not properly constructed or the objective function is non-convex, the algorithm may converge to a suboptimal solution.

### Mechanism 3
- **Claim**: The expected transportation divergence provides a well-posed aggregation framework
- **Mechanism**: By defining the divergence between MoE models through optimal transport plans that match weighted averages of local gating functions to global gating functions, the framework ensures the reduced model has the correct number of components while being close to the weighted average model.
- **Core assumption**: The cost function satisfies the properties required for optimal transport (non-negativity, zero only when densities are equal) and the problem is convex in the parameter space
- **Evidence anchors**:
  - [section 3.1] "The expected transportation divergence between two MoE models h and g is defined by... where c(·, ·) is a real-valued bivariate function defined on Φ × Φ satisfies c(φ1, φ2) ⩾ 0 for all φ1, φ2 ∈ Φ, and the equality holds if and only if φ1 = φ2"
  - [section 3.2] "Proposition 3.3. Assume there exists ∆ ∈ R+ such that I(g, x) ⩽ ∆ for all g ∈ MK, x ∈ X. Then Rc(g) is continuous and convex as a function of g ∈ MK"
  - [corpus] Weak - related works focus on HS-MoE and robustness, not optimal transport formulations
- **Break condition**: If the cost function does not satisfy the required properties or the problem becomes non-convex, the aggregation may fail to produce a meaningful reduction.

## Foundational Learning

- **Concept**: Mixtures of Experts (MoE) models
  - **Why needed here**: The entire paper builds on understanding MoE as a conditional density estimation framework where different experts are activated based on input features
  - **Quick check question**: What is the mathematical form of a MoE model and how does it differ from a standard mixture model?

- **Concept**: Majorization-Minimization (MM) algorithms
  - **Why needed here**: The aggregation algorithm uses MM to minimize the expected transportation divergence, requiring understanding of how majorant functions work and guarantee convergence
  - **Quick check question**: How does an MM algorithm differ from gradient descent, and what guarantees does it provide?

- **Concept**: Optimal transport and Wasserstein distances
  - **Why needed here**: The aggregation framework uses expected transportation divergence, which is based on optimal transport theory between probability distributions
  - **Quick check question**: What is the Kantorovich formulation of optimal transport and how does it apply to comparing mixture models?

## Architecture Onboarding

- **Component map**:
  - Local machines -> Each runs EM algorithm on subset of data to obtain local MoE estimators
  - Central server -> Aggregates local estimators using MM algorithm based on optimal transport
  - Supporting sample -> Small sample drawn from full dataset used for empirical expectations in aggregation
  - Communication layer -> Transfers local parameters and supporting sample from local to central

- **Critical path**: 
  1. Data distributed to M local machines
  2. Each machine runs EM to get local MoE estimator
  3. Local parameters and supporting sample sent to central server
  4. Central server runs MM algorithm to aggregate into global K-component MoE
  5. Output reduction estimator

- **Design tradeoffs**:
  - Number of local machines (M) vs aggregation quality: More machines reduce computation time but may increase aggregation error
  - Size of supporting sample vs accuracy: Larger samples provide better empirical estimates but increase communication and computation
  - Choice of cost function: KL divergence is computationally tractable but may not capture all aspects of model similarity

- **Failure signatures**:
  - Slow convergence of MM algorithm: May indicate poor initialization or ill-conditioned problem
  - Large gap between local and global performance: Suggests aggregation is not effective
  - High communication costs: May occur with very large supporting samples or many machines

- **First 3 experiments**:
  1. Verify EM convergence on local subsets with varying sizes and initializations
  2. Test MM algorithm convergence with synthetic data where ground truth is known
  3. Compare aggregation quality with varying numbers of machines and supporting sample sizes on benchmark datasets

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text.

## Limitations
- Consistency proof relies on strong assumptions about local estimator quality that may not hold in practice
- MM algorithm convergence guarantees assume convexity properties that may be violated in high-dimensional settings
- Optimal transport framework requires careful tuning of cost function and supporting sample size

## Confidence
- Mechanism 1 (Consistency proof): Medium - Theoretical proof is sound but relies on ideal conditions
- Mechanism 2 (MM convergence): High - Standard results apply, but empirical verification is limited
- Mechanism 3 (Optimal transport framework): Medium - Well-defined mathematically but practical challenges exist

## Next Checks
1. **Robustness to initialization**: Test the full pipeline with systematically poor initializations (e.g., random expert assignments, uniform gating) to quantify how often the algorithm recovers from bad starts vs. requiring restarts.

2. **Scaling behavior**: Measure communication costs and aggregation time as M grows beyond 128 machines with varying supporting sample sizes to identify the point where aggregation overhead outweighs distributed computation benefits.

3. **High-dimensional performance**: Evaluate the method on d > 100 dimensional datasets to test whether the optimal transport-based aggregation maintains quality when the parameter space becomes sparse and the KL divergence calculations become numerically unstable.