---
ver: rpa2
title: In-Context Demonstration Selection with Cross Entropy Difference
arxiv_id: '2305.14726'
source_url: https://arxiv.org/abs/2305.14726
tags:
- in-context
- demonstrations
- selection
- examples
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method for selecting in-context demonstrations
  (ICDs) using cross entropy difference (CED), based on the observation that effective
  ICDs have lower perplexity when a language model finetuned on them evaluates a test
  example. The method trains small models on each candidate ICD using parameter-efficient
  finetuning, computes CED scores between each ICD and the test example, and selects
  the ICD with the lowest CED score.
---

# In-Context Demonstration Selection with Cross Entropy Difference

## Quick Facts
- arXiv ID: 2305.14726
- Source URL: https://arxiv.org/abs/2305.14726
- Authors: 
- Reference count: 13
- Key outcome: CED method improves ICD selection over baselines on 8 datasets across 4 tasks and transfers to larger models like GPT-3.5

## Executive Summary
This paper introduces a method for selecting in-context demonstrations (ICDs) using cross entropy difference (CED), based on the observation that effective ICDs have lower perplexity when a language model finetuned on them evaluates a test example. The method trains small models on each candidate ICD using parameter-efficient finetuning, computes CED scores between each ICD and the test example, and selects the ICD with the lowest CED score. This approach approximates gradient alignment between training and test examples, which is shown to be effective for ICD selection. The method is evaluated on 8 datasets from 4 tasks, showing improvements over nearest neighbor baselines. It also transfers to larger models like GPT-3.5, improving performance even on GPT-3.5. The proposed CED method outperforms baselines on macro average across 8 datasets and on each of the 3 GPT model sizes evaluated.

## Method Summary
The CED method selects ICDs by training parameter-efficient models on each candidate demonstration, then computing the cross entropy difference between test examples and each demonstration. For each training example, a small T-Few 0.3B parameter model is trained using parameter-efficient finetuning. When a test example arrives, its perplexity is computed under each finetuned model, and the demonstration corresponding to the model yielding lowest perplexity is selected as the ICD. This approach leverages the insight that effective demonstrations are those on which the model achieves lower perplexity when finetuned, approximating gradient alignment between training and test examples.

## Key Results
- CED method outperforms nearest neighbor and random selection baselines on macro average across 8 datasets
- CED-selected demonstrations transfer to larger models including GPT-3.5, improving performance
- Improvements observed across all 3 GPT model sizes evaluated (code-davinci-002, text-davinci-003, gpt-3.5-turbo)
- CED method effective on 4 task types: binary classification, extractive QA, abstractive QA, and multiple choice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross entropy difference (CED) approximates gradient alignment between training and test examples.
- Mechanism: CED computes the difference in log probabilities of a test example when evaluated by a model finetuned on each training example versus a base model. This difference approximates the dot product between the gradient of the test example and the gradient of each training example, effectively measuring their alignment.
- Core assumption: The gradient of a test example can be approximated by comparing its log probabilities under two models - one finetuned on a single training example and one that is not.
- Evidence anchors:
  - [section]: "Grangier (2019); Wang et al. (2020) describe cross entropy difference as an approximation of the dot product between the gradient of the target domain and the gradient of a single training example."
  - [abstract]: "Our method is based on the observation that the effectiveness of in-context demonstrations negatively correlates with the perplexity of the test example by a language model that was finetuned on that demonstration."
  - [corpus]: Weak - the corpus does not contain direct evidence about the gradient alignment approximation mechanism.
- Break condition: If the base model and finetuned model are not sufficiently close, the CED approximation of gradient alignment breaks down.

### Mechanism 2
- Claim: In-context demonstrations act as "meta-gradients" on the frozen LLM.
- Mechanism: Including in-context demonstrations in the prompt effectively updates the attention mechanism weights of the frozen LLM, similar to how finetuning would update model parameters.
- Core assumption: The attention mechanism can be reparameterized to show that in-context demonstrations modify the attention weights in a way analogous to parameter updates.
- Evidence anchors:
  - [section]: "Dai et al. (2022) describes in-context demonstrations as 'implicit finetuning', where a component of the attention mechanism can be interpreted as a 'meta-gradient'."
  - [abstract]: "Our method is based on the observation that the effectiveness of in-context demonstrations negatively correlates with the perplexity of the test example by a language model that was finetuned on that demonstration."
  - [corpus]: Weak - the corpus does not contain direct evidence about the meta-gradient interpretation of in-context demonstrations.
- Break condition: If the LLM architecture does not support this reparameterization of the attention mechanism, the meta-gradient interpretation breaks down.

### Mechanism 3
- Claim: Parameter-efficient finetuning (PEFT) allows training a model on a single example without overfitting.
- Mechanism: PEFT methods update only a small fraction of model parameters, allowing each training example to be treated as its own "domain" and a separate model to be trained on just that example.
- Core assumption: Updating only a small fraction of parameters prevents catastrophic forgetting and overfitting to a single example.
- Evidence anchors:
  - [section]: "To train each model on a single example, we use the (IA)Â³ PEFT method with a T-Few 0.3B parameter model (Liu et al., 2022a). The effectiveness of PEFT to train a model on a small dataset without catastrophic forgetting allows us to train a model on a single example."
  - [abstract]: "We utilize parameter efficient finetuning to train small models on training data that are used for computing the cross-entropy difference between a test example and every candidate in-context demonstration."
  - [corpus]: Weak - the corpus does not contain direct evidence about the PEFT method's effectiveness on single examples.
- Break condition: If the PEFT method updates too many parameters or the single example is too complex, overfitting may occur.

## Foundational Learning

- Concept: Perplexity as a measure of "in-domain-ness"
  - Why needed here: Perplexity measures how well a language model predicts a test example. Lower perplexity indicates the example is more similar to the training distribution, which is crucial for selecting effective in-context demonstrations.
  - Quick check question: If a test example has lower perplexity when evaluated by a model finetuned on training example A versus training example B, which training example would be a better in-context demonstration?

- Concept: Gradient alignment for domain adaptation
  - Why needed here: The paper builds on previous work showing that selecting training examples with gradients most similar to the test domain improves performance. This concept is adapted to the in-context learning setting.
  - Quick check question: Why does selecting examples with gradients similar to the test example improve in-context learning performance?

- Concept: Parameter-efficient finetuning (PEFT)
  - Why needed here: PEFT enables training separate models on each training example without requiring storing full model copies, making the CED computation feasible.
  - Quick check question: How does PEFT enable training a model on a single example without overfitting or requiring excessive storage?

## Architecture Onboarding

- Component map:
  Input -> PEFT Models -> CED Computation -> Selection -> LLM

- Critical path:
  1. Train PEFT model on each training example
  2. Compute CED scores for test example against all PEFT models
  3. Select training example with lowest CED score
  4. Use selected demonstration with LLM for inference

- Design tradeoffs:
  - Storage vs. accuracy: Storing PEFT parameters for each training example vs. computing them on-the-fly
  - Model size: Smaller PEFT models require less storage but may be less accurate
  - Number of demonstrations: Selecting more demonstrations may improve performance but increases context length

- Failure signatures:
  - High variance in CED scores across training examples
  - Selected demonstrations consistently from out-of-domain
  - No improvement over random demonstration selection

- First 3 experiments:
  1. Compute CED scores for a test example against all training examples and verify they correlate with oracle selection
  2. Train PEFT models on single examples and verify they don't overfit
  3. Test transfer of CED-selected demonstrations to larger LLMs and measure performance improvement

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions but identifies areas for future work including exploring multiple-demonstration selection and applying the method to other tasks beyond text generation.

## Limitations

- Gradient alignment approximation: The relationship between CED and actual gradient alignment remains theoretical rather than empirically validated
- Transfer mechanism ambiguity: The paper shows transfer to larger models but the mechanism for this transfer is unclear
- PEFT method sensitivity: The effectiveness of CED relies on PEFT methods that can train on single examples without overfitting, but this sensitivity is not thoroughly explored

## Confidence

**High Confidence**: 
- CED improves over random and nearest neighbor baselines across multiple datasets and tasks
- The method successfully transfers to larger LLMs like GPT-3.5
- CED computation is feasible with parameter-efficient finetuning

**Medium Confidence**:
- CED approximates gradient alignment (theoretical claim with indirect evidence)
- Perplexity negatively correlates with demonstration effectiveness
- Single-example PEFT doesn't overfit (supported by results but not thoroughly validated)

**Low Confidence**:
- Meta-gradient interpretation of in-context demonstrations
- Exact mechanism of transfer to larger models
- CED's robustness across different PEFT methods

## Next Checks

1. **Direct gradient alignment validation**: Compute actual gradient dot products between training and test examples (where feasible) and compare them against CED scores to verify the approximation relationship.

2. **CED sensitivity analysis**: Systematically vary PEFT method parameters (model size, training epochs, optimization settings) and measure how CED performance changes to establish robustness boundaries.

3. **Cross-task transferability test**: Select CED demonstrations using data from one task (e.g., binary classification) and evaluate their effectiveness on a different task (e.g., QA) to better understand what properties make demonstrations transferable.