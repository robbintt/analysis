---
ver: rpa2
title: Federated Class-Incremental Learning with Prompting
arxiv_id: '2310.08948'
source_url: https://arxiv.org/abs/2310.08948
tags:
- learning
- prompt
- task
- clients
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses federated class-incremental learning (FCIL),
  where local and global models face catastrophic forgetting of old classes due to
  new class arrivals and non-iid data distributions. The proposed method, FCILPT,
  uses prompts instead of memory buffers to preserve old and new knowledge.
---

# Federated Class-Incremental Learning with Prompting

## Quick Facts
- arXiv ID: 2310.08948
- Source URL: https://arxiv.org/abs/2310.08948
- Reference count: 40
- Key outcome: FCILPT achieves up to 38.6% accuracy improvement over state-of-the-art methods by using prompts instead of memory buffers to preserve old and new knowledge in federated class-incremental learning.

## Executive Summary
This paper addresses federated class-incremental learning (FCIL), where models face catastrophic forgetting of old classes due to new class arrivals and non-iid data distributions across clients. The proposed FCILPT method uses prompts instead of memory buffers to preserve both old and new knowledge. Task-relevant and task-irrelevant knowledge are encoded into prompts, and an instance-wise prompt query mechanism selects appropriate prompts without knowing task identity. The key innovation is sorting prompt pools before global aggregation to align task information across clients, solving non-iid issues. Experiments on CIFAR-100, Mini-ImageNet, and Tiny-ImageNet show FCILPT significantly outperforms state-of-the-art methods.

## Method Summary
FCILPT uses a pre-trained ViT-B/16 backbone with frozen parameters and encodes knowledge into task-relevant prompts (task-specific and task-similar) and task-irrelevant prompts (common knowledge capture). The instance-wise prompt query mechanism uses learnable keys and cosine distance matching to select relevant prompts from the prompt pool. A critical innovation is sorting prompt pools based on selection frequency before global aggregation to align task information across clients with non-iid data distributions. The method optimizes using cross-entropy loss with a trade-off parameter λ and evaluates using average accuracy across all phases after learning new tasks.

## Key Results
- Achieves up to 38.6% accuracy improvement over state-of-the-art methods on CIFAR-100, Mini-ImageNet, and Tiny-ImageNet
- Effectively addresses catastrophic forgetting without using memory buffers
- Solves non-iid problems through prompt pool sorting before global aggregation
- Maintains privacy while enabling knowledge transfer across clients

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sorting prompt pools before global aggregation aligns task information across clients, solving non-iid problems caused by class imbalance.
- Mechanism: Prompts are sorted by selection frequency before aggregation, ensuring prompts in the same position from different clients embed information about the same task.
- Core assumption: Task-irrelevant prompts aggregate by simple averaging because each client has only one unique prompt.
- Evidence anchors: [abstract]: "We first sort the task information in the prompt pool in the local clients to align the task information on different clients before global aggregation."
- Break condition: If clients have significantly different prompt selection frequencies for the same task, sorting may not properly align task information.

### Mechanism 2
- Claim: Instance-wise prompt query mechanism accurately selects task-relevant prompts without knowing task identity.
- Mechanism: Uses learnable key-value mapping where query features from input images are matched to prompt keys using cosine distance to select top-N relevant prompts.
- Core assumption: The pre-trained vision transformer embedding layer can extract meaningful query features for prompt selection.
- Evidence anchors: [abstract]: "an instance-wise prompt query mechanism selects appropriate prompts without knowing task identity."
- Break condition: If the embedding layer fails to capture task-relevant features, prompt selection will be inaccurate.

### Mechanism 3
- Claim: Task-irrelevant prompts capture common knowledge across all tasks, improving generalization.
- Mechanism: A separate prompt with different token length aggregates knowledge from all arrived tasks to uncover potentially useful common knowledge.
- Core assumption: Common knowledge exists across tasks and can be effectively captured in a single prompt.
- Evidence anchors: [abstract]: "Task-irrelevant information encodes all the tasks that have arrived, and obtains the possible common information between all the tasks."
- Break condition: If tasks are too dissimilar, common knowledge may not exist or be effectively captured.

## Foundational Learning

- Concept: Federated Learning
  - Why needed here: The paper addresses federated class-incremental learning where data is distributed across multiple clients with privacy constraints.
  - Quick check question: What is the main challenge in federated learning that makes class-incremental learning particularly difficult?

- Concept: Catastrophic Forgetting
  - Why needed here: The model needs to retain knowledge of old classes while learning new ones without using memory buffers.
  - Quick check question: How does the use of prompts help mitigate catastrophic forgetting compared to traditional rehearsal-based methods?

- Concept: Prompt Learning
  - Why needed here: Prompts encode task-relevant and task-irrelevant knowledge to preserve old and new knowledge without retraining the entire model.
  - Quick check question: What is the advantage of using prompts over adapter-based methods for incremental learning?

## Architecture Onboarding

- Component map: Pre-trained Vision Transformer -> Task-relevant prompt pool (task-specific and task-similar prompts) -> Task-irrelevant prompt (common knowledge capture) -> Instance-wise prompt query mechanism -> Global aggregation with prompt pool sorting

- Critical path: Local training → Prompt selection → Feature expansion → Classification → Prompt update → Global aggregation

- Design tradeoffs:
  - Memory vs Performance: Larger prompt pools capture more knowledge but increase memory usage
  - Privacy vs Knowledge transfer: Sharing prompts enables knowledge transfer but requires careful aggregation to maintain privacy
  - Complexity vs Effectiveness: Instance-wise query adds complexity but enables task-agnostic prompt selection

- Failure signatures:
  - Performance degradation indicates poor prompt selection or inadequate task-relevant knowledge encoding
  - Non-iid problems suggest prompt pool sorting is not properly aligning task information
  - Privacy issues may arise from improper prompt aggregation or sharing

- First 3 experiments:
  1. Test prompt selection accuracy with synthetic data where ground truth task identity is known
  2. Validate prompt pool sorting by checking if prompts in same position across clients contain similar task information
  3. Measure catastrophic forgetting by comparing performance on old vs new classes after incremental learning steps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sorting mechanism for prompt pools impact performance in extreme non-IID scenarios where some clients have drastically fewer classes than others?
- Basis in paper: [explicit] The paper states that sorting prompt pools aligns task information from different clients and solves the non-IID problem caused by the lack of classes among different clients in the same incremental task.
- Why unresolved: The paper does not provide experiments or analysis on extreme non-IID scenarios or quantify the impact of sorting on performance under such conditions.
- What evidence would resolve it: Experimental results showing performance degradation with and without sorting in scenarios where some clients have significantly fewer classes than others.

### Open Question 2
- Question: What is the impact of prompt pool size (M) and selection size (N) on the scalability and performance of FCILPT as the number of tasks increases?
- Basis in paper: [inferred] The paper mentions that M and N are hyperparameters that affect the prompt pool configuration, but does not explore their impact on scalability or performance with a large number of tasks.
- Why unresolved: The paper only provides limited hyperparameter tuning results and does not discuss scalability issues or performance trends with increasing task numbers.
- What evidence would resolve it: A comprehensive study varying M and N across different numbers of tasks, analyzing performance and memory usage to identify optimal configurations for scalability.

### Open Question 3
- Question: How does the performance of FCILPT compare to methods that use memory buffers in federated settings where privacy constraints are relaxed?
- Basis in paper: [explicit] The paper emphasizes that FCILPT is rehearsal-free and does not use memory buffers due to privacy constraints, but does not compare its performance to buffer-based methods in settings where privacy is less strict.
- Why unresolved: The paper focuses on the privacy-preserving aspect of FCILPT and does not provide a direct comparison with buffer-based methods under relaxed privacy conditions.
- What evidence would resolve it: Experimental results comparing FCILPT with buffer-based methods in federated settings where memory buffers are allowed, to quantify the trade-off between privacy and performance.

## Limitations

- Limited ablation studies showing individual component contributions to performance
- Evaluation on simplified non-iid scenarios rather than real-world heterogeneous conditions
- Lack of theoretical analysis for convergence guarantees or performance bounds

## Confidence

- High Confidence: The basic premise that prompts can encode task knowledge without memory buffers is well-supported by prompt tuning literature.
- Medium Confidence: The effectiveness of prompt pool sorting for aligning task information across clients is demonstrated empirically but could benefit from more rigorous validation.
- Low Confidence: Claims about task-irrelevant prompts capturing common knowledge across all tasks lack strong empirical support and theoretical justification.

## Next Checks

1. Run controlled experiments removing the prompt pool sorting mechanism while keeping other components constant to quantify its specific contribution to performance improvements.

2. Evaluate FCILPT on extreme non-iid scenarios where clients have completely disjoint class sets (0% overlap) rather than the 60% overlap used in current experiments to test robustness claims.

3. Analyze the instance-wise query mechanism's selection accuracy by computing precision@k for prompt retrieval on held-out validation data with known task identities, comparing against random selection baselines.