---
ver: rpa2
title: 'RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation'
arxiv_id: '2310.04408'
source_url: https://arxiv.org/abs/2310.04408
tags:
- documents
- compressor
- language
- retrieved
- summary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RECOMP, a method that improves retrieval-augmented
  language models (RALMs) by compressing retrieved documents into textual summaries
  before prepending them to the input. The core idea is to use an extractive or abstractive
  compressor trained to produce concise, faithful summaries that guide the LM to generate
  correct outputs while reducing computational costs.
---

# RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation

## Quick Facts
- arXiv ID: 2310.04408
- Source URL: https://arxiv.org/abs/2310.04408
- Reference count: 40
- Key outcome: Achieves compression rates as low as 6% with minimal performance loss on language modeling and open-domain QA tasks, outperforming heuristic methods and off-the-shelf summarization models.

## Executive Summary
This paper introduces RECOMP, a method to improve retrieval-augmented language models (RALMs) by compressing retrieved documents into textual summaries before prepending them to the input. The core innovation is using trained compressors—either extractive (sentence selection) or abstractive (summary generation)—that produce concise, faithful summaries to guide the LM while reducing computational costs. The compressors are trained using end-task signals and can implement selective augmentation by generating empty summaries when retrieved documents are irrelevant. Experiments show RECOMP significantly outperforms heuristic compression methods and off-the-shelf summarization models while maintaining task performance with substantially fewer tokens.

## Method Summary
RECOMP improves RALMs by replacing full retrieved documents with compressed textual summaries. Two compressors are proposed: an extractive compressor that selects relevant sentences using contrastive learning, and an abstractive compressor that generates summaries through distillation from extreme-scale LMs. Both are trained end-to-end using task-specific performance signals without requiring human annotations. The method can selectively augment by outputting empty summaries when retrieved documents are irrelevant, avoiding the inclusion of distracting information. Key to the approach is that these textual summaries can be prepended to the input and used with different language models, enabling transfer across LMs while maintaining faithfulness to the original documents.

## Key Results
- Achieves compression rates as low as 6% with minimal performance loss on both language modeling and open-domain QA tasks
- Significantly outperforms heuristic compression methods (Bag-of-Words, Next Sentence) and off-the-shelf summarization models
- Demonstrates successful transfer of compressors across different language models while maintaining performance
- Shows selective augmentation capability by generating empty summaries when retrieved documents are unhelpful

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compressing retrieved documents into textual summaries before in-context integration reduces computational costs while preserving task performance.
- Mechanism: The compressor generates a concise summary that captures the core information relevant to the input query. This summary is prepended to the input instead of the full retrieved documents, reducing the number of tokens the language model needs to process.
- Core assumption: The summary contains sufficient information for the language model to generate the correct output, and the compression preserves the essential information needed for the task.
- Evidence anchors:
  - [abstract]: "We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summarization models."
  - [section]: "Our experiments show that RECOMP can improve performance of frozen LMs on language modeling and three question answering datasets, while prepending significantly fewer tokens compared to RALM without compression."
- Break condition: If the summary omits critical information needed for the task, or if the language model cannot effectively use the compressed summary to generate the correct output.

### Mechanism 2
- Claim: Selective augmentation, where the compressor generates an empty summary when retrieved documents are irrelevant or unhelpful, improves performance by avoiding the inclusion of distracting information.
- Mechanism: The compressor is trained to output an empty string when the retrieved documents do not contain relevant information or retrieval augmentation is not necessary. This prevents the language model from being distracted by irrelevant context.
- Core assumption: Prepending irrelevant documents can hurt performance, and the compressor can reliably determine when retrieved documents are unhelpful.
- Evidence anchors:
  - [abstract]: "If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can return an empty string, implementing selective augmentation."
  - [section]: "Our compressors also show promising results. For language modelling, both trained compressors achieve a compression ratio of 25% with minimal performance drop. When applied to QA datasets, our best model compresses the documents to 5 - 10% of the original tokens with at most less than 10% relative performance drop."
- Break condition: If the compressor incorrectly generates empty summaries when the retrieved documents are actually helpful, or if the language model performs worse without any retrieved context.

### Mechanism 3
- Claim: The compressors can transfer across different language models, allowing for the use of a single compressor with multiple LMs.
- Mechanism: The textual summaries generated by the compressor can be used as input to different language models, even if the compressor was trained with a specific LM. This allows for the reuse of compressors and simplifies deployment.
- Core assumption: The language models can effectively use the summaries generated by a compressor trained with a different LM, and the summaries contain information that is generally useful across LMs.
- Evidence anchors:
  - [abstract]: "We show that our compressors trained for one LM can transfer to other LMs on the language modeling task and provide summaries largely faithful to the retrieved documents."
  - [section]: "We evaluate whether our compressors trained to achieve high performance with respect to a specific LM (GPT2 for language modeling, FlanUL2 for open domain QA) can transfer to other LMs. For language modeling, we find that trained compressor transfers well to other LMs (GPT2-XL and GPT-J), despite they are much larger LMs."
- Break condition: If the summaries are not useful for the target LM, or if the target LM performs significantly worse with the transferred summaries compared to its own compressor.

## Foundational Learning

- Concept: Text summarization, including extractive and abstractive approaches.
  - Why needed here: The compressors generate textual summaries of the retrieved documents, so understanding summarization techniques is crucial for implementing and improving RECOMP.
  - Quick check question: What is the difference between extractive and abstractive summarization, and when might each be more appropriate?

- Concept: Contrastive learning and distillation for training models without labeled data.
  - Why needed here: The compressors are trained using contrastive learning objectives and distillation from extreme-scale LMs, as there are no human annotations for the desired summaries.
  - Quick check question: How does contrastive learning work, and how can it be used to train a model to select relevant sentences from retrieved documents?

- Concept: Retrieval-augmented language models and the challenges of incorporating retrieved context.
  - Why needed here: RECOMP is a method for improving RALMs, so understanding the limitations of RALMs and how they use retrieved context is important for appreciating the value of RECOMP.
  - Quick check question: What are some of the main challenges in using retrieved documents with language models, and how does RECOMP address these challenges?

## Architecture Onboarding

- Component map: Retriever -> Compressor -> Language Model
- Critical path: Input query → Retriever → Compressor → Language Model → Output
  The retriever and compressor work together to provide the language model with a concise, relevant summary of the retrieved documents.

- Design tradeoffs:
  - Extractive vs. Abstractive Compression: Extractive compression is more faithful to the original documents but may be less concise. Abstractive compression can be more concise but may introduce hallucinations or omit important information.
  - Selective Augmentation: Generating empty summaries when retrieved documents are irrelevant can improve performance, but the compressor must be able to reliably determine when documents are unhelpful.
  - Transferability: Designing compressors that can transfer across different LMs allows for reuse but may sacrifice some performance compared to LM-specific compressors.

- Failure signatures:
  - Performance degradation: If the compressor is not effective at selecting relevant information or if the language model cannot use the compressed summaries effectively, the overall performance may be worse than using the full retrieved documents.
  - Hallucinations: Abstractive compression may introduce hallucinations or incorrect information, especially when summarizing multiple documents.
  - Over-compression: If the compressor is too aggressive in reducing the length of the summary, it may omit critical information needed for the task.

- First 3 experiments:
  1. Train an extractive compressor using contrastive learning and evaluate its performance on the language modeling task. Compare the performance with using the full retrieved documents and with using a random sentence from the documents.
  2. Train an abstractive compressor using distillation from an extreme-scale LM and evaluate its performance on the QA task. Compare the performance with using the full retrieved documents, with using a T5 model for summarization, and with using GPT-3.5 for summarization.
  3. Evaluate the transferability of the compressors by training them with one LM and testing them with different LMs. Compare the performance with using LM-specific compressors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RECOMP compare to retrieval-augmented language models (RALMs) when the retrieved documents are highly relevant to the input query?
- Basis in paper: The paper states that RECOMP achieves a compression rate of as low as 6% with minimal loss in performance for both language modeling and open domain QA tasks, significantly outperforming heuristic compression methods and off-the-shelf summarization models.
- Why unresolved: The paper does not provide a direct comparison between RECOMP and RALMs in terms of performance when the retrieved documents are highly relevant. The focus is on the compression aspect and the comparison is made with heuristic compression methods and off-the-shelf summarization models.
- What evidence would resolve it: A controlled experiment comparing the performance of RECOMP and RALMs when the retrieved documents are highly relevant to the input query.

### Open Question 2
- Question: Can the abstractive compressor of RECOMP handle multi-hop reasoning tasks effectively, or is it limited to simpler question-answering tasks?
- Basis in paper: The paper mentions that the abstractive compressor is trained using end-task signals and can generate summaries by synthesizing information from multiple documents. However, it also notes that the effectiveness of summarization depends on the dataset, with summaries being most faithful for TQA and least faithful for HotpotQA, which requires multi-hop understanding.
- Why unresolved: While the paper demonstrates the effectiveness of the abstractive compressor on language modeling and QA tasks, it does not explicitly test its performance on multi-hop reasoning tasks. The mention of HotpotQA suggests that the abstractive compressor might struggle with such tasks, but this is not confirmed.
- What evidence would resolve it: A comprehensive evaluation of the abstractive compressor on a variety of multi-hop reasoning tasks, comparing its performance to other state-of-the-art methods.

### Open Question 3
- Question: How does the faithfulness of the summaries generated by RECOMP impact the overall performance of the retrieval-augmented language models?
- Basis in paper: The paper states that the summaries generated by RECOMP are largely faithful to the retrieved documents. However, it also mentions that the effectiveness of summarization depends on the dataset, with summaries being most faithful for TQA and least faithful for HotpotQA.
- Why unresolved: While the paper demonstrates the effectiveness of RECOMP in terms of compression and task performance, it does not explicitly analyze the relationship between the faithfulness of the summaries and the overall performance of the retrieval-augmented language models.
- What evidence would resolve it: A detailed analysis of the relationship between the faithfulness of the summaries and the performance of the retrieval-augmented language models on various tasks.

## Limitations

- Corpus Quality: Retrieved documents contain noise that can hurt performance even with compression, and RECOMP's selective augmentation addresses this partially but cannot eliminate it entirely.
- Abstractive Compressor Reliability: The abstractive compressor is prone to hallucinations and may omit critical information when summarizing multiple documents, performing worse than extractive compression on some tasks.
- Transferability Constraints: Compressor transfer is only tested to larger LMs of similar architecture, not to different LM types or architectures.

## Confidence

**High Confidence** (Extensive experimental support):
- Mechanism 1 (Computational efficiency through compression): Supported by quantitative results showing 6-25% compression rates with minimal performance loss across multiple datasets and tasks.
- Mechanism 2 (Selective augmentation effectiveness): Demonstrated through both trained compressors showing ability to return empty summaries when appropriate, with performance improvements over heuristic methods.

**Medium Confidence** (Some experimental support, theoretical gaps):
- Mechanism 3 (Cross-LM transferability): Supported by language modeling experiments but limited to similar transformer architectures and not tested on truly different LM types.

**Low Confidence** (Limited or indirect evidence):
- No evidence on scaling to very large LMs (GPT-4, Claude) or different architectures (RNNs, hybrids)
- No analysis of compression quality vs. document complexity (longer vs. shorter documents)
- No ablation on optimal compression ratios for different task types

## Next Checks

1. **Cross-Retriever Validation**: Test RECOMP's compressors with a completely different retriever (e.g., sparse vs. dense) to validate that performance gains are not tied to specific retriever-document characteristics. This would confirm the method's robustness to retrieval noise.

2. **Extreme Compression Analysis**: Systematically test compression ratios below 5% (potentially down to 1-2%) to identify the minimum viable compression level where performance degradation becomes unacceptable. This would establish practical bounds for RECOMP deployment.

3. **Long-Context LM Validation**: Evaluate RECOMP with modern long-context LMs (context windows > 100K tokens) to determine if compression remains beneficial when LMs can natively process more retrieved content. This addresses whether RECOMP is a stopgap for context limitations or provides inherent advantages.