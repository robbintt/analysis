---
ver: rpa2
title: 'AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning'
arxiv_id: '2308.03526'
source_url: https://arxiv.org/abs/2308.03526
tags:
- offline
- learning
- agents
- behavior
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes AlphaStar Unplugged, a challenging benchmark
  for offline reinforcement learning on StarCraft II, leveraging a large dataset of
  human replays. It provides standardized API tools, a training setup, and evaluation
  metrics, along with baseline agents including behavior cloning, offline actor-critic,
  and MuZero variants.
---

# AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2308.03526
- Source URL: https://arxiv.org/abs/2308.03526
- Reference count: 33
- Primary result: Offline RL agents achieve 90% win rate against previous AlphaStar behavior cloning baseline

## Executive Summary
This paper introduces AlphaStar Unplugged, a benchmark for offline reinforcement learning using StarCraft II human replays. The authors provide standardized tools, a training setup, and evaluation metrics, along with baseline agents including behavior cloning, offline actor-critic, and MuZero variants. Experiments demonstrate that offline RL agents significantly outperform behavior cloning baselines, achieving 90% win rate against the previously published AlphaStar behavior cloning agent. The work highlights the effectiveness of one-step offline RL approaches and reveals challenges such as limited action space coverage and weak learning signals in the offline RL setting.

## Method Summary
The AlphaStar Unplugged benchmark uses a dataset of 1.4 million StarCraft II games from high-skill players (MMR > 3500). The benchmark provides standardized API tools, training setup, and evaluation metrics. The paper introduces several baseline agents: behavior cloning (BC), offline actor-critic (OAC), and MuZero variants. Key design choices include using V-Trace for clipped importance sampling, autoregressive action modeling for the combinatorial action space, and using the behavior value function as critic instead of the target policy value function for stability. The offline RL agents follow a two-step recipe: first train a model to estimate the behavior policy and behavior value function, then use the behavior value function to improve the policy.

## Key Results
- Offline RL agents achieve 90% win rate against previous AlphaStar behavior cloning baseline
- One-step offline RL approaches outperform multi-step methods in this setting
- MuZero with MCTS at inference time improves over pure behavior cloning without destabilizing training
- OAC with behavior value function as critic is stable, while using target policy value function leads to divergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: One-step offline RL outperforms multi-step approaches because the dataset already contains near-optimal trajectories.
- Mechanism: Behavior cloning captures the underlying expert policy, and one-step correction using the value function leverages the existing high-quality trajectories without diverging.
- Core assumption: The dataset is rich enough that most state-action pairs seen during inference are covered in the data.
- Evidence anchors:
  - [abstract] "one-step offline RL approaches ... follow a two-step recipe: first train a model to estimate the behavior policy and behavior value function. Then, use the behavior value function to improve the policy"
  - [section] "We believe sharing these insights will be valuable to anyone interested in offline RL, especially at large scale"
- Break condition: If the dataset coverage drops below a threshold, the value function extrapolation becomes unreliable and performance collapses.

### Mechanism 2
- Claim: Offline Actor-Critic diverges when using the target policy value function as critic but stabilizes with the behavior value function.
- Mechanism: The behavior value function is fixed and trained only on observed returns, preventing the critic from drifting away from the data distribution.
- Core assumption: The behavior value function can be accurately estimated from the logged data.
- Evidence anchors:
  - [section] "Using the value functionð‘‰ ðœ‹ leads to divergence during training, as shown on Figure 7"
  - [section] "we usedð‘‰ ðœ‡ as a critic, and keep it fixed, instead ofð‘‰ ðœ‹"
- Break condition: If the dataset contains significant distribution shift, even the behavior value function may be biased.

### Mechanism 3
- Claim: MuZero with MCTS at inference time improves over pure behavior cloning without destabilizing training.
- Mechanism: MCTS searches over actions sampled from the behavior policy, leveraging the value function for guidance without updating the policy during training.
- Core assumption: The learned latent dynamics model is accurate enough for meaningful planning.
- Evidence anchors:
  - [section] "Using MCTS always outperforms not using it, even at the beginning of training"
  - [section] "Using MCTS at inference time, on the other hand, is stable and leads to better policies"
- Break condition: If the dynamics model is inaccurate, MCTS will search in irrelevant areas of the action space.

## Foundational Learning

- Concept: Importance sampling in offline RL
  - Why needed here: To correct for the mismatch between behavior and target policies without introducing too much variance.
  - Quick check question: What happens if the importance sampling ratio becomes very large?

- Concept: V-Trace and clipped importance sampling
  - Why needed here: To stabilize training by bounding the contribution of off-policy corrections.
  - Quick check question: How does V-Trace prevent policy collapse in the offline setting?

- Concept: Autoregressive action modeling
  - Why needed here: StarCraft II's action space is combinatorial, requiring sequential sampling of arguments.
  - Quick check question: Why can't we sample all action arguments independently in StarCraft II?

## Architecture Onboarding

- Component map: Encoder (MLP/Transformer/ConvNet) -> Latent space -> Prediction heads (function/delay) -> Autoregressive samplers -> MCTS (optional)
- Critical path: Data loading -> Behavior cloning phase -> Value function training -> Policy improvement phase
- Design tradeoffs: Using the behavior value function instead of target value function trades potential policy improvement for training stability
- Failure signatures: Training divergence when using target value function, poor win rate against built-in bots, high importance sampling variance
- First 3 experiments:
  1. Run behavior cloning with different minibatch sizes to verify the scaling trend
  2. Compare win rates with and without value function in the BC baseline
  3. Test OAC with behavior vs target value function as critic to reproduce the divergence finding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different offline RL methods scale when applied to environments with larger state-action spaces than StarCraft II?
- Basis in paper: [inferred] The paper notes that StarCraft II has a large action space (~1026 actions per step) and limited state-action coverage, which poses significant challenges for offline RL methods.
- Why unresolved: The paper only tests methods on StarCraft II and does not explore other environments with even larger state-action spaces.
- What evidence would resolve it: Experiments comparing offline RL methods on environments with progressively larger state-action spaces would reveal scalability limits and performance trends.

### Open Question 2
- Question: What modifications to offline RL algorithms could improve performance in partially observable environments with weak learning signals like StarCraft II?
- Basis in paper: [explicit] The paper identifies partial observability, weak learning signals (win/loss only), and large action spaces as key challenges for offline RL in StarCraft II.
- Why unresolved: While the paper tests some variants (e.g., using behavior value function as critic instead of target policy value function), it does not explore a wide range of algorithmic modifications or architectures tailored to these specific challenges.
- What evidence would resolve it: Systematic ablation studies of algorithmic components and architectural choices in StarCraft II or similar environments would identify which modifications yield the most benefit.

### Open Question 3
- Question: How does the quality and diversity of offline data affect the performance of offline RL methods in complex environments?
- Basis in paper: [explicit] The paper discusses the importance of data quality and filtering, showing that fine-tuning on high-quality data improves performance, but also notes that the dataset has limited coverage and varying quality.
- Why unresolved: The paper only uses a single dataset of human replays and does not investigate how different data distributions or coverage levels impact learning.
- What evidence would resolve it: Experiments training offline RL agents on datasets with varying quality, diversity, and coverage would reveal how data characteristics influence performance and generalization.

## Limitations
- The analysis is based on a filtered dataset of high-skill human replays, which may not generalize to broader or noisier datasets
- The study focuses on a single game (StarCraft II) and may not transfer directly to other domains
- While offline RL agents outperform behavior cloning, the absolute performance against top human players is not reported

## Confidence
- High confidence: The finding that offline RL agents significantly outperform behavior cloning baselines (90% win rate) is well-supported by the experimental results and multiple independent runs.
- Medium confidence: The superiority of one-step offline RL over multi-step approaches is well-justified but relies on assumptions about dataset coverage that are not fully validated.
- Medium confidence: The recommendation to use behavior value functions instead of target value functions for stability is supported by ablation studies but may not generalize to all offline RL settings.

## Next Checks
1. Test the offline RL agents on datasets with varying skill levels and coverage to verify the robustness of the one-step approach.
2. Conduct an ablation study on the importance of each design choice (e.g., V-Trace, autoregressive action modeling) to isolate their individual contributions.
3. Evaluate the learned policies against a wider range of opponents, including top human players, to assess practical applicability.