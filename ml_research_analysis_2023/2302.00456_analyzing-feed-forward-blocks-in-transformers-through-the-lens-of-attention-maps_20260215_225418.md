---
ver: rpa2
title: Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention
  Maps
arxiv_id: '2302.00456'
source_url: https://arxiv.org/abs/2302.00456
tags:
- layer
- attn
- pairs
- each
- contextualization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study analyzed the contextualization effects of feed-forward
  (FF) blocks in Transformer models using a norm-based method to visualize attribution
  changes. Experiments with masked language models revealed that FFs do modify input
  contextualization, particularly amplifying interactions between related tokens such
  as subwords from the same word or named entity components.
---

# Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps

## Quick Facts
- **arXiv ID:** 2302.00456
- **Source URL:** https://arxiv.org/abs/2302.00456
- **Reference count:** 8
- **Primary result:** Feed-forward blocks in Transformers modify contextualization by amplifying specific token-to-token interactions, with surrounding components canceling these effects

## Executive Summary
This study analyzes the contextualization effects of feed-forward (FF) blocks in Transformer models using a norm-based attribution visualization method. The researchers found that FFs do modify input contextualization, particularly amplifying interactions between related tokens such as subwords from the same word or named entity components. Notably, FFs and their surrounding components (residual and normalization layers) tend to cancel out each other's contextualization effects, suggesting potential redundancy in the Transformer layer's design.

## Method Summary
The study employs a norm-based attribution analysis method to visualize how each component in a Transformer layer affects contextualization. The method measures the L2 norm of transformed input vectors to quantify token contributions. For the non-linear feed-forward block, Integrated Gradients decomposition is used to break down the activation function's effect. Attribution maps are created by comparing token contributions before and after each component, with similarity scores (Spearman's rank correlation) measuring how contextualization changes across different analysis scopes.

## Key Results
- FFs modify contextualization by amplifying interactions between related tokens like subwords from the same word or named entity components
- RES2 and LN2 components tend to cancel out FF's contextualization effects through norm differences and dimension shrinking
- The entire Transformer layer shows potential redundancy from a contextualization perspective due to component cancellation effects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** FFs modify input contextualization by amplifying specific token-to-token interactions
- **Mechanism:** FFs use nonlinear transformations (GELU) that cannot be additively decomposed, so Integrated Gradients (IG) is applied to decompose the activation function. This reveals that FFs boost interactions between related tokens like subwords from the same word or named entity components
- **Core assumption:** The FF's nonlinear activation function can be decomposed via IG to reveal its effect on contextualization
- **Evidence anchors:** [abstract] "Our experiments with masked language models revealed that FF networks modify the input contextualization to emphasize specific types of linguistic compositions"; [section] "FFs did not modify the token-to-token interactions randomly, but rather modified interactions associated with some linguistic relationships"
- **Break condition:** If IG decomposition fails to capture the FF's contextualization effects, or if FFs show no amplification pattern for related tokens

### Mechanism 2
- **Claim:** Components surrounding FFs (RES2 and LN2) tend to cancel out each other's contextualization effects
- **Mechanism:** RES2 bypasses FF input vectors to the output, and if bypassed vectors have larger norms than FF outputs, the transformation is diluted. LN2 can shrink specific dimensions where FF produces outliers, canceling FF's effect
- **Core assumption:** RES2 and LN2 have opposing effects on contextualization that lead to cancellation
- **Evidence anchors:** [abstract] "FF and its surrounding components tend to cancel out each other's effects, suggesting potential redundancy in the processing of the Transformer layer"; [section] "RES2 dilutes the contextualization effect caused by FF" and "LN2 cancels the FF's effect by shrinking these special dimensions"
- **Break condition:** If RES2 or LN2 do not show norm differences or dimension shrinking that would cause cancellation

### Mechanism 3
- **Claim:** The entire Transformer layer appears redundant from a contextualization perspective due to component cancellation
- **Mechanism:** When considering all components (LN1, FF, RES2, LN2), their contextualization effects largely cancel out, making ATTN + RES1 appear sufficient for contextualization
- **Core assumption:** Component cancellation is so effective that it creates redundancy in the layer's processing
- **Evidence anchors:** [abstract] "FF and its surrounding components tend to cancel out each other's effects, suggesting potential redundancy in the processing of the Transformer layer"; [section] "the Transformer layer appears as if the block of LN1, FF, RES2, and LN2 do not work as significantly as expected... suggesting the redundancy of Transformers"
- **Break condition:** If component cancellation is not as strong as observed, or if other functions beyond contextualization justify these components

## Foundational Learning

- **Concept: Vector norm-based attribution analysis**
  - **Why needed here:** This method quantifies how much each input token contributes to each output token by measuring transformed input vector norms
  - **Quick check question:** How does measuring ∥F(xi)∥ help understand token contributions compared to just using attention weights?

- **Concept: Integrated Gradients for nonlinear decomposition**
  - **Why needed here:** FFs contain nonlinear activation functions that cannot be additively decomposed, so IG is used to break down the FF's effect
  - **Quick check question:** Why can't we simply additively decompose FF outputs like we do with linear components?

- **Concept: Component-by-component attribution map analysis**
  - **Why needed here:** Comparing attribution maps before and after each component reveals how each component modifies contextualization
  - **Quick check question:** What does a low similarity score between attribution maps before and after a component indicate about that component's effect?

## Architecture Onboarding

- **Component map:** Input → LN1 → RES1 → ATTN → LN2 → RES2 → FF → Output
- **Critical path:** Forward pass through model to get outputs → Apply IG decomposition only to FF's nonlinear activation → Calculate vector norms for attribution → Compare attribution maps across different component scopes → Aggregate results across layers and input sequences
- **Design tradeoffs:** IG decomposition adds computational cost but is necessary for FF analysis; simple decomposition works for linear components but not FF; normalizing attribution maps by column sums enables inter-method comparison; sampling strategies (e.g., same-distance pairs) control for positional biases
- **Failure signatures:** If similarity scores between scopes are consistently 1.0, components may not be modifying contextualization; if IG decomposition produces unrealistic values, the method may be flawed; if word-split or named entity pairs don't show amplification, FF may not have the hypothesized linguistic sensitivity; if RES2 or LN2 don't show norm differences, cancellation may not occur
- **First 3 experiments:**
  1. Compare ATTN-W (attention weights only) vs ATTN-N (norm-based) to establish baseline contextualization differences
  2. Measure similarity between ATTN-RESLN and ATTN-RESLNFFRES to test if RES2 cancels FF effects
  3. Analyze FF-amp scores for word-split pairs vs random pairs to verify FF's sensitivity to subword relationships

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the feed-forward (FF) block in Transformer models serve primarily a contextualization role, or does it function as a knowledge storage/access mechanism as suggested by other studies?
  - **Basis in paper:** [explicit] The paper states that "studies analyzing the input contextualization with FF out of scope might have provided incomplete conclusions" and notes that FF has been analyzed both from contextualization and knowledge storage perspectives
  - **Why unresolved:** The paper shows FF modifies contextualization but also notes FF's substantial parameters and references to FF as "key-value memories," suggesting dual potential functions that aren't fully reconciled
  - **What evidence would resolve it:** Systematic ablation studies comparing models with FF disabled versus FF repurposed for knowledge storage, measuring performance on both contextualization tasks and knowledge retrieval tasks

- **Open Question 2:** What is the fundamental mechanism by which layer normalization (LN) cancels out the contextualization effects of feed-forward (FF) and attention blocks?
  - **Basis in paper:** [explicit] The paper states "FF's contextualization effect is realized using very specific dimensions, and LN2 cancels the effect by shrinking these special dimensions" and that "LN2 cancels both LN1 and FF"
  - **Why unresolved:** The paper observes this cancellation effect but doesn't explain why this architectural design exists or what purpose it serves in the overall model function
  - **What evidence would resolve it:** Analysis of alternative normalization schemes that preserve FF's contextualization effects, or theoretical analysis of whether this cancellation serves a regularization or representational stability function

- **Open Question 3:** How do the contextualization effects of feed-forward (FF) blocks vary across different types of Transformer architectures (e.g., BERT, RoBERTa, GPT) and tasks?
  - **Basis in paper:** [inferred] The paper tested 11 variants of masked language models but focused detailed analysis on BERT-base, noting that "we found similar results for the other models" without providing detailed comparisons
  - **Why unresolved:** The paper provides preliminary evidence of generalization but doesn't explore how FF's contextualization effects might differ based on model architecture, training objectives, or downstream task performance
  - **What evidence would resolve it:** Comparative analysis of FF's contextualization effects across multiple model architectures and task types, correlating these effects with task performance metrics

## Limitations
- The norm-based attribution method is indirect and may not fully capture semantic processing
- IG decomposition assumes linearity in the interpolation path, which may not hold for all activation functions
- The analysis focuses on subword-level interactions and may miss higher-level compositional patterns

## Confidence
- **High confidence:** The methodology for norm-based attribution analysis is sound and the observation that FF blocks amplify specific token relationships is well-supported by evidence
- **Medium confidence:** The cancellation effects between components are real but their interpretation as functional redundancy requires further validation
- **Low confidence:** The broader claim about Transformer layer redundancy based solely on contextualization effects

## Next Checks
1. Validate FF-amp scores using alternative attribution methods (e.g., attention rollout, direct logit attribution) to confirm whether the amplification pattern is consistent across techniques
2. Test the cancellation hypothesis by ablating individual components (RES2, LN2) and measuring the actual performance impact on downstream tasks, not just attribution map changes
3. Extend the analysis to encoder-decoder Transformers and larger models to verify whether the observed patterns generalize beyond BERT/RoBERTa base models