---
ver: rpa2
title: 'eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language
  Models'
arxiv_id: '2309.00964'
source_url: https://arxiv.org/abs/2309.00964
tags:
- tensor
- weight
- compression
- memory
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes eDKM, a memory-efficient implementation of
  Differentiable KMeans Clustering (DKM) for compressing large language models (LLMs).
  The key challenge is that DKM's memory complexity is prohibitively high for LLM
  fine-tuning due to the large attention map between weights and centroids.
---

# eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models

## Quick Facts
- arXiv ID: 2309.00964
- Source URL: https://arxiv.org/abs/2309.00964
- Authors: [List not provided in input]
- Reference count: 24
- Primary result: Achieves 130x memory reduction while maintaining accuracy when compressing LLaMA 7B to 3 bits

## Executive Summary
This paper addresses the prohibitive memory complexity of Differentiable KMeans Clustering (DKM) when compressing large language models like LLaMA 7B. The authors propose eDKM, which introduces two key optimizations: cross-device tensor marshaling to eliminate redundant GPU-CPU copies, and weight uniquification with sharding to exploit the 2^16 unique values in 16-bit weights. These techniques reduce the attention map memory from O(|W|·|C|) to O(|C| + |W|/|L|) while adding negligible runtime overhead. Experiments show eDKM compresses LLaMA 7B from 12.6 GB to 2.5 GB with maintained accuracy across multiple LLM benchmarks.

## Method Summary
eDKM implements two novel techniques to address DKM's memory bottleneck during LLM fine-tuning. First, cross-device tensor marshaling avoids redundant GPU-CPU copies by checking for existing CPU tensors with identical data storage before transfers. Second, weight uniquification leverages the fact that 16-bit weights have only 2^16 unique values to compress the attention map representation, with sharding distributing the compressed index lists across multiple learners. During backward propagation, an all-gather operation reconstructs the full attention map from the compressed form. The method was evaluated on LLaMA 7B using 8× A100-80GB GPUs with AdamW optimizer (lr=5e-5, weight_decay=0, betas=(0.9, 0.95)) for 2 epochs.

## Key Results
- Achieves 130x reduction in memory footprint during DKM training
- Compresses LLaMA 7B from 12.6 GB to 2.5 GB (3-bit compression) while maintaining accuracy
- Outperforms other state-of-the-art compression techniques on LLM benchmarks (PIQA, HellaSwag, Winograde, ARC-e, ARC-c, TriviaQA, MMLU)
- Runtime overhead from additional computation/communications is reported as insignificant

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-device tensor marshaling eliminates redundant GPU-CPU copies by reusing existing CPU tensors when their underlying data storage matches a new tensor being transferred.
- Mechanism: When a tensor is about to be copied to CPU, the marshaling layer checks if a tensor with identical data storage already exists on CPU within a few hops of data-storage-invariant operations (e.g., view, transpose). If found, it returns a reference to the existing CPU tensor plus the necessary ops to reconstruct the desired shape; otherwise, it performs the copy.
- Core assumption: Tensors with identical data storage but different metadata (shape, stride) are functionally equivalent for DKM's backward pass.
- Evidence anchors:
  - [abstract] "we compressed the tensor by applying uniquification and sharding after checking if there is no duplicated tensor previously copied to CPU"
  - [section] "we turn to the forward graph and check if there exists another tensor that is already on CPU and is reachable via only data-storage invariant operations"
  - [corpus] Weak; no corpus papers discuss cross-device tensor marshaling directly.
- Break condition: If the computation graph changes such that no matching data-storage path exists within the search horizon, redundant copies will occur.

### Mechanism 2
- Claim: Weight uniquification compresses the attention map by exploiting the fact that 16-bit weights have only 2^16 unique values, reducing the per-tensor attention map from O(|W|·|C|) to O(|C| + |W|/|L|) after sharding.
- Mechanism: For each unique 16-bit weight value, compute attention to centroids once, store in a compact attention table, and use the 16-bit value as an offset in an index list. Sharding the index list across learners further reduces memory per learner.
- Core assumption: During synchronous training, all learners have identical weight states at any step, so sharding is safe.
- Evidence anchors:
  - [abstract] "weights in 16 bits have only 2^16 unique values to reduce the attention map (in Fig 1) representation"
  - [section] "the 16bit value, BA45 of wi and wk can serve as an offset to the attention table in the index list"
  - [corpus] Weak; corpus papers mention quantization but not the 16-bit uniqueness trick.
- Break condition: If asynchronous updates occur or mixed-precision training is used, the assumption of identical weights across learners fails.

### Mechanism 3
- Claim: Reconstructing the attention map for backward propagation from the compressed form adds negligible runtime overhead compared to the memory savings achieved.
- Mechanism: During backward pass, perform an all-gather of sharded index lists and a lookup into the attention table to restore the full attention map before gradient computation.
- Core assumption: All-gather and lookup costs are small relative to GPU-CPU traffic avoided by compression.
- Evidence anchors:
  - [abstract] "although these steps require extra computation/communications...the runtime overhead is insignificant, as the traffic between GPU and CPU has decreased substantially"
  - [section] "Such sharding will bring down the memory complexity to O(|W|/|L|)" and "extra computation/communications...the runtime overhead is insignificant"
  - [corpus] Weak; no direct evidence in corpus about backward pass reconstruction cost.
- Break condition: If the number of learners grows large or network bandwidth is limited, all-gather latency may dominate.

## Foundational Learning

- Concept: Differentiable KMeans Clustering (DKM) and its attention map O(|W|·|C|) memory complexity
  - Why needed here: Understanding DKM's memory bottleneck is key to appreciating why eDKM's optimizations matter.
  - Quick check question: In DKM, what is the size of the attention map for a 7B parameter model with 1K centroids in FP16? (Answer: 7B × 1K × 2 bytes ≈ 14 TB)

- Concept: Cross-device tensor marshaling and data-storage invariance in PyTorch
  - Why needed here: Engineers must know how eDKM detects and reuses tensors across CPU/GPU to implement or debug the system.
  - Quick check question: If tensor x0 is on GPU and x1 = x0.view(-1,1) is created, do they share data storage on GPU? (Answer: Yes)

- Concept: Mixed-precision training and synchronous data parallelism
  - Why needed here: eDKM's sharding assumes all learners have identical weights; understanding FSDP and sync training is essential.
  - Quick check question: In FSDP, what guarantees that all learners have identical weight states before each backward pass? (Answer: All-reduce synchronization after optimizer step)

## Architecture Onboarding

- Component map: PyTorch forward graph traversal module -> Attention table and index list builders -> Sharding logic -> All-gather + lookup module -> Integration hooks into DKM's forward/backward passes

- Critical path:
  1. Forward pass: Unique weights → attention table + index list (sharded)
  2. Backward pass: All-gather index lists → lookup → reconstruct full attention map → gradient computation

- Design tradeoffs:
  - Memory vs. compute: Compression saves memory but adds lookup and all-gather cost
  - Search depth in marshaling: Deeper search finds more reuse but costs more graph traversal
  - Sharding granularity: Larger shard size reduces all-gather cost but increases memory per learner

- Failure signatures:
  - Memory spikes during backward pass → attention map reconstruction not working
  - GPU idle time during forward pass → marshaling search too deep or ineffective
  - Gradient mismatch across learners → sharding logic broken or sync lost

- First 3 experiments:
  1. Verify that marshaling eliminates redundant CPU copies for a small DKM layer with known tensor reuse patterns.
  2. Measure memory reduction and all-gather overhead when uniquifying weights for a 1M parameter dummy model with 1K centroids.
  3. End-to-end compression of LLaMA 7B with eDKM, comparing accuracy and memory against baseline DKM on a small dataset.

## Open Questions the Paper Calls Out
None explicitly stated in the provided input.

## Limitations
- Limited empirical validation to a single model (LLaMA 7B) and training setup (8× A100-80GB)
- No systematic exploration of scalability to larger models beyond 7B parameters
- Lack of detailed profiling data to support the claim of "insignificant" runtime overhead

## Confidence
- Memory reduction claim: Medium confidence (well-reasoned mechanism but limited validation)
- Runtime overhead claim: Medium confidence (supported by reasoning but lacks detailed profiling)
- Scalability claims: Low confidence (not empirically validated beyond 7B parameters)

## Next Checks
1. **Memory Profiling Validation**: Instrument eDKM to measure actual memory usage during training across all phases (forward, backward, optimizer step) and compare against theoretical predictions for different model sizes and centroid counts.

2. **Numerical Stability Test**: Run eDKM training with perturbed initializations and measure the variance in final weights and accuracy across multiple seeds to assess sensitivity to compression-induced noise.

3. **Scalability Benchmark**: Apply eDKM to progressively larger models (7B → 13B → 30B parameters) and measure how memory reduction scales with model size while tracking accuracy degradation patterns.