---
ver: rpa2
title: 'Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis
  on Reranking for Few-Shot In-Context Learning'
arxiv_id: '2311.09619'
source_url: https://arxiv.org/abs/2311.09619
tags:
- utility
- training
- llms
- task
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes different utility functions for demonstration
  selection in few-shot in-context learning (ICL). The authors compare output probability
  of generating ground-truth, downstream metric reward, and their proposed incremental
  utility, which estimates the additional knowledge a demonstration brings.
---

# Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning

## Quick Facts
- arXiv ID: 2311.09619
- Source URL: https://arxiv.org/abs/2311.09619
- Reference count: 32
- Primary result: Incremental utility improves few-shot ICL by contrasting LLM performance with and without demonstrations across classification, segmentation, and translation tasks in five languages.

## Executive Summary
This paper analyzes different utility functions for demonstration selection in few-shot in-context learning (ICL). The authors compare three approaches: output probability of generating ground-truth, downstream metric reward, and their proposed incremental utility which estimates the additional knowledge demonstrations bring. Experiments across five languages (Arabic, English, Finnish, Japanese, Spanish) and three task types show that output probability works well for short outputs with well-distributed probabilities, while downstream metrics are more robust for longer outputs with nuanced rewards. The incremental utility further improves performance by training with contrastive examples that capture how demonstrations change LLM predictions.

## Method Summary
The method uses a cross-attention encoder to rerank demonstration candidates retrieved by an off-the-shelf text retriever. Three utility functions are compared: output probability (OP) which measures the likelihood of generating ground-truth given a demonstration, downstream metric (DM) which uses task-specific rewards correlated with evaluation metrics, and incremental utility which computes the difference between 1-shot and 0-shot utility scores. The reranking model is trained using logistic regression to predict whether demonstrations improve LLM performance. The approach is evaluated across classification, segmentation, and translation tasks in five languages using Flan-PaLM 2 as the base LLM.

## Key Results
- Output probability utility is effective for classification tasks when probability values are well-distributed across the value range
- Downstream metric utility is more robust for longer outputs with nuanced reward values
- Incremental utility improves ICL performance by providing contrastive training examples that capture demonstration effects
- Constrained retrieval increases the number of contrastive examples and improves performance when retrieved candidates are imbalanced

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Output probability (OP) utility works well for classification tasks because the distribution of probability values spans the full range, providing dense training signals.
- **Mechanism**: When probability values are well-distributed, the reranking model receives more nuanced gradients during training, leading to better demonstration selection.
- **Core assumption**: Dense distribution of probability values correlates with improved downstream ICL performance.
- **Evidence anchors**:
  - [abstract] "the output probability of generating the ground-truth output is indeed good proxy for the utility of a demonstration, especially when the probability values are distributed across the whole value range with short outputs (e.g., classification)"
  - [section] "Figure 2 shows distributions of uDM and uOP on the training sets of ISD (en), EDOS-B, and CLINC; the OP-based utility values are well distributed across the value range."
- **Break condition**: If probability values cluster narrowly (e.g., [0.0, 0.05) for long outputs), the utility function fails to provide discriminative training signals.

### Mechanism 2
- **Claim**: Downstream metric (DM) utility is more robust for longer outputs because it directly optimizes task-specific metrics rather than relying on probability distributions.
- **Mechanism**: DM uses a task-specific reward function correlated with the evaluation metric, providing targeted feedback even when output probabilities are not discriminative.
- **Core assumption**: Task-specific reward functions can effectively guide demonstration selection even when output probabilities are poorly distributed.
- **Evidence anchors**:
  - [abstract] "the downstream metric is more robust when nuanced reward values are provided with long outputs (e.g., text generation tasks)"
  - [section] "Figure 4 and Figure 5 show the distributions of the utility values on SSENT (es) and XML-MT (ja) as in Section 5.1. Unlike the classification tasks, the direct utility values are well distributed by the task-oriented reward."
- **Break condition**: If the reward function is poorly aligned with the evaluation metric or if generating long outputs leads to high variance in reward values.

### Mechanism 3
- **Claim**: Incremental utility improves ICL by contrasting LLM performance with and without demonstrations, effectively training the reranking model with contrastive examples.
- **Mechanism**: By computing the difference between 1-shot and 0-shot utility, incremental utility captures the actual knowledge gain from demonstrations, providing stronger training signals.
- **Core assumption**: Contrastive training examples where demonstrations have both positive and negative effects are essential for effective incremental utility.
- **Evidence anchors**:
  - [abstract] "our proposed incremental utility further improves ICL given contrastive training examples to effectively train the reranking model"
  - [section] "Table 4 shows the counting results, along with the improvement (averaged across k = 1, 3, 5) over uOP by u+OP in Table 3, and we can see a positive correlation as expected."
- **Break condition**: If there are few contrastive training examples (e.g., demonstrations rarely change LLM predictions), incremental utility cannot learn effective ranking.

## Foundational Learning

- **Concept**: Cross-attention encoder for reranking
  - Why needed here: To compute utility scores for demonstration candidates by attending to both the demonstration and the input.
  - Quick check question: What is the input representation for the cross-attention encoder in the reranking model?

- **Concept**: Output probability estimation
  - Why needed here: To measure how likely the LLM is to generate the ground-truth output given a demonstration, used as a proxy for demonstration utility.
  - Quick check question: How is the output probability computed in practice when the LLM only provides APIs?

- **Concept**: Task-specific reward functions
  - Why needed here: To provide nuanced feedback for non-classification tasks where output probabilities are not discriminative.
  - Quick check question: What are the characteristics of an effective reward function for text generation tasks?

## Architecture Onboarding

- **Component map**: Text retriever -> Top-n candidates -> Cross-attention reranking model -> Utility scores -> LLM with prompt construction -> Final prediction
- **Critical path**:
  1. Retrieve top-n candidates using text similarity
  2. Rerank candidates using utility scores
  3. Select top-k demonstrations
  4. Construct prompt and generate prediction
- **Design tradeoffs**:
  - Using n > k allows exploration but increases computation
  - Simple pointwise regression vs. ranking losses for training
  - Caching predictions for validation vs. real-time computation
- **Failure signatures**:
  - Poor retrieval leads to low-quality candidates regardless of reranking
  - Misaligned utility function causes ineffective demonstration selection
  - Insufficient contrastive examples prevent incremental utility from learning
- **First 3 experiments**:
  1. Compare 0-shot vs. 1-shot performance on a simple classification task
  2. Evaluate OP utility distribution on classification vs. generation tasks
  3. Test incremental utility with constrained retrieval on imbalanced datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of incremental utility vary across different LLM architectures and sizes beyond Flan-PaLM 2 and Flan-T5?
- Basis in paper: The authors conduct cross-LLM experiments replacing Flan-PaLM 2 with Flan-T5, showing consistent advantages of u+OP but acknowledging the need for broader investigation.
- Why unresolved: The experiments are limited to only two specific models (Flan-PaLM 2 and Flan-T5), and the authors explicitly note the need to pursue cross-LLM rerankers as an interesting future direction.
- What evidence would resolve it: Systematic experiments testing incremental utility across a diverse set of LLM architectures (e.g., GPT models, LLaMA, BLOOM) and scales, measuring performance differences and identifying patterns.

### Open Question 2
- Question: What is the optimal method for estimating compositional utility when multiple demonstrations are combined (k > 1) in few-shot ICL?
- Basis in paper: The authors discuss the limitation of current utility functions that estimate utility independently for each demonstration, noting that only 27.4% of independent demonstrations change predictions when k=1, increasing to 40.1% with k=2.
- Why unresolved: The paper focuses on utility estimation for individual demonstrations and acknowledges that compositional effects are not well understood, particularly how multiple demonstrations interact to influence LLM predictions.
- What evidence would resolve it: Development and validation of compositional utility estimation methods that account for interactions between demonstrations, potentially using techniques like attention-based aggregation or contrastive learning across demonstration combinations.

### Open Question 3
- Question: How can constrained retrieval be optimally integrated into the training pipeline to maximize the number of contrastive training examples while maintaining retrieval quality?
- Basis in paper: The authors propose a constrained retrieval strategy that increases contrastive examples from 39.5% to 53.5% when n' increases from 10 to 50, showing improved performance.
- Why unresolved: While the paper demonstrates that constrained retrieval can increase contrastive examples, it doesn't determine the optimal parameters (N, n') or fully explore integration methods that balance demonstration diversity with retrieval effectiveness.
- What evidence would resolve it: Systematic analysis of constrained retrieval parameters across multiple datasets and tasks, including ablation studies on different selection criteria, and comparison with alternative diversity-promoting retrieval methods.

## Limitations

- The analysis relies heavily on comparisons between utility functions without extensive ablation studies to isolate individual contributions
- Cross-attention encoder implementation details are not fully specified, making it difficult to assess architectural impact
- The effectiveness of output probability utility critically depends on the assumption of well-distributed probability values, which may not hold for all tasks

## Confidence

**High confidence**: Output probability utility works well for classification tasks with short outputs and well-distributed probabilities, directly supported by experimental evidence across multiple datasets and languages.

**Medium confidence**: Downstream metric utility is more robust for longer outputs with nuanced rewards, theoretically sound but needs more systematic evaluation of different reward function designs.

**Medium confidence**: Incremental utility shows improvements through contrastive training, concept is well-founded but experimental evidence showing performance gains is somewhat indirect.

## Next Checks

1. **Distribution sensitivity analysis**: Systematically evaluate how output probability utility performance degrades as probability value distribution becomes more concentrated, identifying failure thresholds.

2. **Ablation of incremental components**: Conduct controlled experiments isolating the contribution of contrastive learning from the utility function design itself to clarify the source of improvements.

3. **Cross-task generalization test**: Evaluate utility functions on additional task types not covered in the original study (e.g., reasoning tasks, code generation) to assess broader ICL applicability.