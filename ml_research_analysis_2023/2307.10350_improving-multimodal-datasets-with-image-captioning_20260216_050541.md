---
ver: rpa2
title: Improving Multimodal Datasets with Image Captioning
arxiv_id: '2307.10350'
source_url: https://arxiv.org/abs/2307.10350
tags:
- captions
- blip2
- training
- data
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using synthetic captions to improve multimodal
  dataset quality, focusing on web-scraped data with poor text annotations. The authors
  experiment with mixing raw and generated captions using CLIP models at three scales
  (12.8M, 128M, and 1.28B samples).
---

# Improving Multimodal Datasets with Image Captioning

## Quick Facts
- arXiv ID: 2307.10350
- Source URL: https://arxiv.org/abs/2307.10350
- Authors: Multiple
- Reference count: 40
- Key outcome: Synthetic captions combined with raw captions improve CLIP training performance by over 2x on retrieval tasks

## Executive Summary
This paper investigates using synthetic captions to improve multimodal dataset quality, focusing on web-scraped data with poor text annotations. The authors experiment with mixing raw and generated captions using CLIP models at three scales (12.8M, 128M, and 1.28B samples). They find that combining raw and synthetic captions outperforms filtering methods on both ImageNet and 38 other tasks. Notably, synthetic captions improve retrieval performance by over 2x. They also show that model performance on captioning benchmarks doesn't correlate with utility for multimodal training, and that image quality becomes increasingly important at larger scales.

## Method Summary
The authors generate synthetic captions for web-scraped images using captioning models (BLIP, BLIP2, OpenCLIP-CoCa) with top-K sampling. They train CLIP models at three scales on datasets combining raw and synthetic captions, filtering by image-text cosine similarity. The mixing strategy combines high-quality raw and synthetic captions subject to a cosine similarity threshold. They evaluate performance using zero-shot ImageNet classification and 38 DataComp tasks, including retrieval benchmarks.

## Key Results
- Combining raw and synthetic captions outperforms filtering methods on ImageNet and 38 other tasks
- Synthetic captions improve retrieval performance by over 2x
- CLIP performance on image captioning benchmarks doesn't correlate with utility for multimodal training
- Image quality becomes increasingly important at larger scales (1.28B samples)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generated captions reduce noise by providing better image-text alignment than raw web captions
- Mechanism: Image captioning models trained on aligned image-text data learn to produce captions that describe visual content more accurately, reducing semantic misalignment that exists in raw web captions
- Core assumption: Image captioning models can generalize to describe diverse web images beyond their training distribution
- Evidence anchors:
  - [abstract] "raw web data is noisy" and "generated captions can increase the utility of web-scraped datapoints with nondescript text"
  - [section 6.1] "synthetic captions are less noisy and contain more visual information" and "synthetic captions have much higher image-text cosine similarity (mean similarity 0.251 vs 0.208)"
  - [corpus] Found 25 related papers, average neighbor FMR=0.494 indicates moderate relatedness

### Mechanism 2
- Claim: Combining raw and synthetic captions provides both diversity and quality benefits
- Mechanism: Raw captions offer text diversity that captures varied language patterns from web sources, while synthetic captions provide quality through better alignment; combining them gives the best of both
- Core assumption: The diversity gap between raw and synthetic captions can be bridged through mixing strategies
- Evidence anchors:
  - [abstract] "using both sources of captions helps improve the overall caption quality, measured in terms of text diversity as well as image-text alignment"
  - [section 6] "At the population level, synthetic captions are less diverse than raw captions" but "Using both sources of captions helps improve the overall caption quality"
  - [section 5] "mixing raw and synthetic captions, subject to a cosine similarity threshold" outperforms other methods

### Mechanism 3
- Claim: CLIP performance on image captioning benchmarks does not reliably predict caption utility for multimodal training
- Mechanism: Standard captioning metrics like CIDEr optimize for reference similarity rather than visual alignment, while CLIP training requires captions that align well with visual features
- Core assumption: Reference-based metrics capture different aspects of caption quality than what matters for multimodal representation learning
- Evidence anchors:
  - [abstract] "the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable indicator of the utility of the captions it generates for multimodal training"
  - [section 4] "better performance on image captioning benchmarks does not necessarily mean better generated captions for CLIP training" and "fine-tuning pre-trained networks on the task of image captioning end up producing synthetic captions that are worse for CLIP"
  - [section 4] "captioning models that are only pre-trained have very poor CIDEr scores; going with this metric would have suggested that these models are not suitable for caption generation at all"

## Foundational Learning

- Concept: Image-text alignment metrics (cosine similarity between CLIP embeddings)
  - Why needed here: The paper uses image-text cosine similarity extensively to measure caption quality and filter training data
  - Quick check question: How would you compute the cosine similarity between image and text embeddings using CLIP?

- Concept: Text diversity metrics (unique trigrams, unique nouns)
  - Why needed here: The paper measures diversity of caption sets using unique n-grams to understand the trade-off between noise reduction and diversity loss
  - Quick check question: What's the difference between measuring diversity at the individual caption level versus the population level?

- Concept: Multimodal representation learning objectives
  - Why needed here: Understanding why CLIP training benefits from specific caption properties requires knowing what the contrastive loss optimizes for
  - Quick check question: Why might a caption that scores well on CIDEr not necessarily produce good CLIP representations?

## Architecture Onboarding

- Component map: Caption generation → Data filtering/mixing → CLIP model training → Zero-shot evaluation
- Critical path: Caption generation quality → Cosine similarity filtering → Training data composition → Downstream task performance
- Design tradeoffs: Raw caption diversity vs. synthetic caption quality, compute cost of caption generation vs. training efficiency, filtering threshold vs. dataset size
- Failure signatures: Performance degradation when synthetic captions hallucinate, diversity collapse when over-filtering, overfitting when training set becomes too small
- First 3 experiments:
  1. Generate captions for a small subset of images using different captioning models and measure cosine similarity distributions
  2. Train CLIP on pure synthetic captions vs. pure raw captions with CLIP score filtering to establish baseline performance gap
  3. Mix raw and synthetic captions at different ratios with CLIP score thresholds to find optimal combination strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity gap between model-generated and web-scraped text scale with increasing data quantities, and what are the specific mechanisms behind this divergence?
- Basis in paper: [inferred] The paper notes that at larger scales (400M and 1.28B), the diversity gap becomes more significant and may hinder performance gains from synthetic captions.
- Why unresolved: The paper only provides initial observations about the diversity gap but doesn't fully explain the underlying mechanisms or provide detailed scaling analysis.
- What evidence would resolve it: Systematic analysis of diversity metrics (unique nouns, trigrams) across multiple scales, along with experiments isolating the impact of diversity on model performance.

### Open Question 2
- Question: What is the optimal balance between text quality (noise reduction) and text diversity when combining raw and synthetic captions, and how does this balance shift across different data scales?
- Basis in paper: [explicit] The paper discusses the trade-off between noise reduction and diversity loss in existing filtering methods, and explores different mixing strategies for raw and synthetic captions.
- Why unresolved: While the paper identifies the trade-off and experiments with mixing strategies, it doesn't provide a comprehensive framework for determining the optimal balance across scales.
- What evidence would resolve it: Systematic ablation studies varying the mixing ratios and filtering thresholds across multiple scales, coupled with performance analysis.

### Open Question 3
- Question: How do different image captioning model architectures and training approaches (e.g., fine-tuning vs. pre-trained only) affect the utility of generated captions for multimodal training, beyond just CLIP performance?
- Basis in paper: [explicit] The paper shows that fine-tuning captioning models on MS-COCO reduces diversity and hurts CLIP performance, and that CLIP-S metric better predicts utility than CIDEr.
- Why unresolved: The paper only explores a limited set of captioning models and doesn't investigate other architectural choices or training approaches.
- What evidence would resolve it: Comprehensive comparison of multiple captioning architectures and training strategies, including newer approaches like instruction tuning or reinforcement learning from human feedback.

## Limitations

- Performance improvements measured primarily on CLIP models may not transfer to other multimodal architectures
- Study focuses on English-language captions, leaving multilingual generalization open
- Synthetic caption generation relies on models trained on curated datasets, potentially limiting coverage of diverse web content

## Confidence

**High Confidence**: The core finding that combining raw and synthetic captions outperforms either source alone, supported by consistent performance gains across three model scales and 38 evaluation tasks.

**Medium Confidence**: The claim that captioning benchmark performance doesn't predict training utility, based on specific models and metrics tested.

**Low Confidence**: The assertion that synthetic captions improve retrieval performance by "over 2x" lacks specific baseline comparisons in the abstract.

## Next Checks

1. **Cross-Architecture Validation**: Test whether synthetic caption improvements transfer to non-CLIP multimodal models (e.g., SigLIP, ALIGN, or BERT-based vision-language models) to establish architecture independence.

2. **Diversity Coverage Analysis**: Conduct systematic analysis of concept coverage by comparing semantic space of raw vs. synthetic captions using t-SNE visualization or topic modeling.

3. **Multilingual Generalization**: Evaluate synthetic caption performance on non-English web data by translating captions or training captioning models on multilingual datasets.