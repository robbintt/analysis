---
ver: rpa2
title: 'Machine-assisted quantitizing designs: augmenting humanities and social sciences
  with artificial intelligence'
arxiv_id: '2309.14379'
source_url: https://arxiv.org/abs/2309.14379
tags:
- data
- research
- llms
- cation
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a machine-assisted mixed methods framework
  that combines qualitative analysis with rigorous statistical modeling to analyze
  large datasets in humanities and social sciences. The approach uses large language
  models as on-demand annotators to scale up qualitative coding tasks, followed by
  statistical analysis that incorporates machine error rates.
---

# Machine-assisted quantitizing designs: augmenting humanities and social sciences with artificial intelligence

## Quick Facts
- arXiv ID: 2309.14379
- Source URL: https://arxiv.org/abs/2309.14379
- Reference count: 40
- Primary result: LLM-assisted mixed methods framework enables scaling of qualitative research while maintaining replicability and transparency

## Executive Summary
This paper proposes a machine-assisted mixed methods framework that combines qualitative analysis with rigorous statistical modeling to analyze large datasets in humanities and social sciences. The approach uses large language models as on-demand annotators to scale up qualitative coding tasks, followed by statistical analysis that incorporates machine error rates. The method is demonstrated across 16 case studies in 9 languages, covering tasks like topic classification, event detection, interview analysis, text reuse detection, and multimodal visual analytics.

## Method Summary
The framework follows a quantitizing mixed methods design where qualitative coding tasks are first assigned to either human or machine annotators, then combined with statistical modeling that incorporates classification error rates. The process involves designing coding schemes, unitizing data, applying LLM prompts for annotation, evaluating against human-coded test sets, and using bootstrapping procedures to incorporate error rates into uncertainty estimates. The approach emphasizes transparency through systematic documentation of coding schemes, prompts, models, and statistical procedures.

## Key Results
- Current LLMs can match or exceed human-level performance on many humanities and social science tasks
- Zero-shot learning with instructable LLMs removes the need for large annotated training datasets
- The framework enables processing of much larger datasets than traditional human-only approaches while maintaining replicability through systematic documentation and error modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can serve as viable research instruments for complex cultural and social data annotation.
- Mechanism: Zero-shot learning with instructable LLMs removes the need for large annotated training datasets by using natural language prompts to classify text.
- Core assumption: Current generative LLMs have sufficient understanding of diverse languages and contexts to perform classification tasks at human-level or above.
- Evidence anchors: current LLMs can match or exceed human-level performance on many tasks; the zero-shot approach scores 1.5-2x above the state of the art in lexical semantic change detection.
- Break condition: If LLM training data lacks sufficient coverage of target language/domain, performance degrades.

### Mechanism 2
- Claim: Machine assistance enables scaling of qualitative research to larger datasets than human-only approaches.
- Mechanism: Automating the qualitative coding/annotation step in mixed methods frameworks while maintaining rigorous statistical modeling of classification errors.
- Core assumption: Machine annotation errors can be quantified and incorporated into statistical uncertainty estimates.
- Evidence anchors: tasks previously requiring potentially months of team effort can now be accomplished by an LLM-assisted scholar in a fraction of the time; bootstrapping approach is discussed for incorporating machine annotator error rates.
- Break condition: If classification error rates are too high relative to research question sensitivity, results become unreliable.

### Mechanism 3
- Claim: The framework promotes transparency and replicability in qualitative and machine-assisted research.
- Mechanism: Systematic documentation of coding schemes, prompts, models, and statistical procedures enables reproduction.
- Core assumption: Open science practices can be adopted across disciplines to make research processes transparent.
- Evidence anchors: fostering replicability and transparency through proposed good practices in unitizing, analysis and methods documentation; the way to get around these issues is to strive towards maximal transparency and good open science practices.
- Break condition: If source data is sensitive/protected, complete transparency becomes impossible.

## Foundational Learning

- Concept: Mixed methods research design (specifically quantitizing/integrating design)
  - Why needed here: Framework builds on this paradigm to combine qualitative coding with quantitative modeling
  - Quick check question: What distinguishes quantitizing mixed methods from purely qualitative or quantitative approaches?

- Concept: Statistical modeling with random effects for repeated measures
  - Why needed here: Controls for clustering in humanities/social science data (multiple responses per participant, texts from same author, etc.)
  - Quick check question: When would you need to use mixed effects models instead of simple regression?

- Concept: Error-in-variables modeling and bootstrapping for uncertainty estimation
  - Why needed here: Incorporates machine classification error rates into statistical inference
  - Quick check question: How does bootstrapping from a confusion matrix help estimate confidence intervals when using machine annotations?

## Architecture Onboarding

- Component map: Research question → coding scheme design → data unitization → machine/human annotation → error estimation → statistical modeling → interpretation
- Critical path: Coding scheme design → annotation (machine or human) → statistical modeling with error incorporation
- Design tradeoffs: Simpler prompts save costs but may reduce accuracy; more complex preprocessing pipelines may improve results but require more effort
- Failure signatures: High classification error rates; Simpson's Paradox when ignoring hierarchical structure; spurious correlations from uncontrolled confounds
- First 3 experiments:
  1. Replicate the topic classification case study on a small dataset to test prompt engineering
  2. Apply the bootstrapping procedure to a simple classification task to verify error incorporation
  3. Test the relevance filtering approach on a sample of OCR-processed historical texts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific limitations and challenges of using LLM-based approaches for analyzing highly ambiguous or context-dependent qualitative data?
- Basis in paper: The paper acknowledges that LLM annotations may contain errors and variation, and some tasks like emoji interpretation are difficult even for humans.
- Why unresolved: The paper provides limited detailed analysis of LLM performance on highly ambiguous qualitative data, and doesn't explore specific failure modes or mitigation strategies.
- What evidence would resolve it: Systematic testing of LLM performance on datasets with known ambiguity levels, comparison with human annotator agreement rates, and analysis of error patterns in ambiguous cases.

### Open Question 2
- Question: How do different LLM models (e.g., GPT-3.5 vs GPT-4) compare in their ability to handle tasks across different languages and cultural contexts?
- Basis in paper: The paper compares GPT-3.5 and GPT-4 on some tasks and notes that GPT-4 performs better, but doesn't provide comprehensive model comparisons across all languages and tasks.
- Why unresolved: The paper only tests a limited set of models and languages, leaving questions about generalizability to other models and cultural contexts.
- What evidence would resolve it: Large-scale benchmarking of multiple LLM models across diverse languages and cultural contexts, with detailed analysis of performance variations.

### Open Question 3
- Question: What are the ethical implications and potential biases introduced by using LLM-based approaches in humanities and social sciences research?
- Basis in paper: The paper briefly mentions potential biases in LLMs due to training data but doesn't delve into specific ethical concerns or mitigation strategies.
- Why unresolved: The paper doesn't explore the ethical dimensions of LLM use in research, such as perpetuating stereotypes or reinforcing power imbalances.
- What evidence would resolve it: In-depth analysis of LLM outputs for potential biases, case studies of ethical issues in LLM-assisted research, and development of guidelines for responsible LLM use in humanities and social sciences.

## Limitations
- Framework's effectiveness for truly low-resource languages remains untested
- Dependence on quality and coverage of LLM training data, particularly for specialized domains
- Computational costs of LLM API calls could become prohibitive for large-scale research

## Confidence

**High Confidence Claims:**
- LLMs can perform zero-shot classification tasks at human-level or above for common languages
- The mixed methods framework with error incorporation is methodologically sound
- Transparency through systematic documentation improves replicability

**Medium Confidence Claims:**
- The framework scales to genuinely large humanities datasets
- Error rates from bootstrapping accurately capture uncertainty in statistical inference
- The approach works equally well across all demonstrated task types

**Low Confidence Claims:**
- The framework's effectiveness for truly low-resource languages
- Long-term sustainability given potential LLM API cost increases
- Performance consistency across highly specialized academic domains

## Next Checks

1. **Cross-linguistic robustness test**: Apply the framework to a controlled set of texts in 5 languages spanning different language families with known classification difficulties. Compare LLM performance against human annotators on the same material, explicitly measuring performance degradation in low-resource languages.

2. **Error propagation validation**: For a simple classification task, systematically vary the machine annotator error rate (from 0% to 30% in 5% increments) and observe how statistical inference results change. Verify that the bootstrapping procedure correctly captures increasing uncertainty and that confidence intervals remain valid as error rates approach research question sensitivity thresholds.

3. **Cost-benefit scaling analysis**: Process a dataset at increasing sizes (1,000 → 10,000 → 100,000 examples) while tracking both computational costs (API calls, processing time) and accuracy degradation. Identify the point where LLM costs outweigh benefits compared to traditional human annotation, providing practical guidelines for when the framework is most cost-effective.