---
ver: rpa2
title: 'Reward Dropout Improves Control: Bi-objective Perspective on Reinforced LM'
arxiv_id: '2310.04483'
source_url: https://arxiv.org/abs/2310.04483
tags:
- reward
- policy
- behavior
- dropout
- pareto
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper frames controllable language models as a bi-objective
  optimization problem over reward and likelihood objectives. It shows that optimal
  performance occurs when the reward is negatively logarithmic to the likelihood,
  and derives a Pareto improvement condition that guarantees policy improvement.
---

# Reward Dropout Improves Control: Bi-objective Perspective on Reinforced LM

## Quick Facts
- arXiv ID: 2310.04483
- Source URL: https://arxiv.org/abs/2310.04483
- Reference count: 40
- Key outcome: Reward Dropout improves controllable language model performance by framing CLM training as bi-objective optimization over reward and likelihood objectives, showing optimal performance occurs when reward is negatively logarithmic to likelihood.

## Executive Summary
This paper introduces Reward Dropout, a novel approach for improving controllable language models (CLMs) by framing the training problem as a bi-objective optimization over reward and likelihood objectives. The authors prove that optimal performance occurs when the reward and likelihood objectives are negatively logarithmic at optimal points, and derive a Pareto improvement condition that guarantees policy improvement. Reward Dropout selectively zeroes out low-reward samples during training to enforce this condition, leading to improved optimization. Extensive experiments on five benchmark datasets show consistent improvements in average rewards, especially when using quantile-based dropout, with human evaluation confirming both high quality and reliable control in generated texts.

## Method Summary
The method frames CLM training as maximizing both reward and likelihood objectives simultaneously. It introduces Reward Dropout, which selectively zeros out low-reward samples during training to improve optimization. The approach uses a pre-trained behavior LLM (fixed) to sample trajectories, a pre-trained reward model to evaluate them, and a target LLM (initialized to the behavior model) that is fine-tuned using policy-based RL algorithms. The key insight is that by manipulating the reward objective to satisfy Eτ∼π[R(τ)] + Eτ∼π[lnβ(τ)] > 0, Pareto improvement is guaranteed. Three policy gradient methods are evaluated: deterministic policy gradient (DPG), stochastic policy gradient (SPG), and top-k policy gradient (KPG).

## Key Results
- Reward Dropout consistently improved average rewards across all five benchmark datasets (sentiment, politeness, toxicity, emotion, topic)
- Quantile-based dropout outperformed random dropout in all tested configurations
- Human evaluation confirmed generated texts met human standards for both naturalness and attribute control
- The approach showed robust performance across multiple decoding strategies (DPG, SPG, KPG)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward Dropout improves optimization by enforcing Pareto improvement via reward manipulation.
- Mechanism: Reward Dropout selectively zeros out low-reward samples, creating a bias toward high-reward trajectories that satisfy the condition Eτ∼π[R(τ)] + Eτ∼π[lnβ(τ)] > 0, which Theorem 4.2 shows guarantees Pareto improvement.
- Core assumption: The reward objective R(τ) can be effectively manipulated without breaking the likelihood objective β(τ), and the behavior policy β(τ) remains well-defined.
- Evidence anchors:
  - [abstract] "Reward Dropout, which zeroes out low-reward samples during training to improve optimization."
  - [section 5] "we can optimize θ by manipulating R(·) or β(·) such that Eτ∼π[R(τ)] + Eτ∼π[lnβ(τ)] > 0 is always satisfied."
  - [corpus] Weak or missing direct evidence; no related work explicitly discusses reward dropout in bi-objective RL optimization.
- Break condition: If the behavior policy is ill-defined (e.g., β(τ) = 0 for all τ), Proposition 4.2 shows the optimal policy collapses to uniform, making reward dropout ineffective.

### Mechanism 2
- Claim: Pareto optimality requires the reward and likelihood objectives to be negatively logarithmic at optimal points.
- Mechanism: At optimal policy π∗, the reward R(τ) must equal -lnβ(τ) (Theorem 4.3), ensuring a trade-off that defines the Pareto frontier.
- Core assumption: The likelihood objective β(τ) is well-behaved and non-zero where needed, so that lnβ(τ) is defined.
- Evidence anchors:
  - [section 4.2] "the optimal policy π∗(τ) must yield a result that the two objectives are negatively related."
  - [section 4.2] "∀τ∗ ∼ π∗, R(τ) = −lnβ(τ) holds."
  - [corpus] No direct evidence; bi-objective optimization literature does not usually enforce this specific log-negative relationship.
- Break condition: If β(τ) = 0 for some τ, lnβ(τ) is undefined, violating the Pareto optimality condition.

### Mechanism 3
- Claim: Higher-entropy behavior policies enable greater maximal expected rewards.
- Mechanism: Proposition 4.1 shows that if Eτ∗∼π∗[R(τ)] = -Eτ∗∼π∗[lnβ(τ)], then Eτ∗∼π∗[R(τ)] = H[β(τ)], meaning the more uniform the behavior policy, the higher the upper bound on reward.
- Core assumption: The behavior policy has sufficient entropy and covers the token space adequately.
- Evidence anchors:
  - [section 4.3] "the higher the entropy of the behavior policy, the higher the maximal expected reward."
  - [section 4.3] "a highly informative behavior policy increases the maximal level of expected reward."
  - [corpus] No direct evidence; related work does not explicitly link behavior policy entropy to reward upper bounds in this way.
- Break condition: If the behavior policy is concentrated on a narrow token set, entropy is low and the reward upper bound is reduced.

## Foundational Learning

- Concept: Bi-objective optimization and Pareto optimality
  - Why needed here: The paper frames CLM training as maximizing both reward and likelihood objectives, requiring Pareto-optimal solutions where one cannot improve without hurting the other.
  - Quick check question: What condition must hold for a policy to be Pareto optimal in this context?

- Concept: Kullback-Leibler Divergence (KLD) and its role in policy improvement
  - Why needed here: Theorem 4.1 bounds the expected reward by KLD between target and behavior policies, providing a theoretical foundation for policy improvement guarantees.
  - Quick check question: How does minimizing KLD relate to maximizing expected reward in this setup?

- Concept: Off-policy reinforcement learning and the behavior/target policy distinction
  - Why needed here: The target LM is updated using trajectories sampled from a pre-trained behavior LM, so understanding the off-policy setup is essential for applying the theorems.
  - Quick check question: Why must the behavior policy be fixed while updating the target policy?

## Architecture Onboarding

- Component map: Pre-trained behavior LLM (fixed βθ) -> Reward model Rϕ -> Target LLM πθ (initialized to βθ) -> Dropout module -> Policy gradient update

- Critical path:
  1. Sample trajectories from behavior LLM
  2. Compute rewards via reward model
  3. Apply Reward Dropout (zero out low rewards)
  4. Compute policy gradient w.r.t. dropped rewards
  5. Update target LLM parameters

- Design tradeoffs:
  - Random dropout is simpler but less effective than quantile dropout
  - Higher dropout rates improve reward but may reduce sample efficiency
  - Greedy decoding (DPG) is worse than stochastic (SPG) or top-k (KPG) decoding

- Failure signatures:
  - If reward growth plateaus early, behavior policy may be ill-defined or too narrow
  - If reward oscillates, dropout rate may be too high
  - If human evaluation fails, likelihood objective may be sacrificed

- First 3 experiments:
  1. Run with no dropout, stochastic decoding, small dataset to confirm baseline performance
  2. Add quantile dropout with γ=0.8, compare reward growth curves
  3. Vary dropout rate γ ∈ {0.8, 0.9, 0.95}, measure average final reward and human evaluation scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal dropout rate for Reward Dropout in terms of the trade-off between reward improvement and text quality degradation?
- Basis in paper: [explicit] The paper tests three dropout rates (0.80, 0.90, 0.95) but doesn't provide a clear recommendation for the optimal value.
- Why unresolved: The paper shows performance improves with higher dropout rates but doesn't analyze the point where text quality might start degrading.
- What evidence would resolve it: Comparative analysis showing reward improvement plateaus while text quality metrics (e.g., perplexity, human evaluation scores) start declining at higher dropout rates.

### Open Question 2
- Question: How does Reward Dropout perform when applied to non-language control tasks like molecule generation or music composition?
- Basis in paper: [inferred] The paper mentions Reward Dropout is applicable to any reward maximization problem but only evaluates on text generation benchmarks.
- Why unresolved: The theoretical framework is general but empirical validation is limited to text generation tasks.
- What evidence would resolve it: Experiments applying Reward Dropout to at least one non-text control task (e.g., molecule generation) with performance comparison to baseline methods.

### Open Question 3
- Question: Does Reward Dropout maintain its effectiveness when applied to smaller, non-pretrained language models rather than large pre-trained models?
- Basis in paper: [inferred] The paper uses pre-trained GPT-2 as the behavior model based on Proposition 4.1 about high-entropy behavior policies, but doesn't test smaller models.
- Why unresolved: The paper assumes pre-trained models provide better behavior policies but doesn't validate this assumption across different model sizes.
- What evidence would resolve it: Experiments comparing Reward Dropout performance using behavior models of varying sizes (e.g., GPT-2 small, medium, large) while keeping all other conditions constant.

## Limitations

- The theoretical framework relies on assumptions about well-defined behavior policies and sufficient entropy that may not hold in practice
- The experimental scope is limited to five benchmark datasets with a restricted set of decoding strategies
- Key implementation details (reward model architecture, hyperparameters) are missing, making faithful reproduction challenging

## Confidence

**High Confidence:**
- The theoretical framework of bi-objective optimization for CLMs is sound and well-founded
- The improvement in average rewards with Reward Dropout is consistently observed across experiments
- The Pareto improvement condition (Eτ∼π[R(τ)] + Eτ∼π[lnβ(τ)] > 0) is correctly derived and applied

**Medium Confidence:**
- The superiority of quantile dropout over random dropout
- The relationship between behavior policy entropy and maximal expected reward
- The consistency of results across different decoding strategies

**Low Confidence:**
- The generalizability of results to more complex text generation tasks
- The robustness of the approach to violations of theoretical assumptions
- The specific conditions under which human evaluation scores meet standards

## Next Checks

1. **Behavior Policy Robustness Test:** Systematically vary the entropy of the behavior policy by using different pre-trained models (e.g., smaller LMs, more specialized LMs) and measure the impact on reward improvement with Reward Dropout. This would validate Proposition 4.1 and identify when the approach breaks down.

2. **Reward Model Sensitivity Analysis:** Conduct ablation studies on the reward model architecture and training procedure. Test with different reward model sizes, training objectives, and label distributions to understand how reward model quality affects the effectiveness of Reward Dropout.

3. **Cross-task Generalization Experiment:** Apply Reward Dropout to a more challenging text generation task (e.g., long-form story generation with multiple attributes) and compare performance against the current benchmarks. This would test the scalability and practical utility of the approach beyond the current experimental scope.