---
ver: rpa2
title: How to Handle Different Types of Out-of-Distribution Scenarios in Computational
  Argumentation? A Comprehensive and Fine-Grained Field Study
arxiv_id: '2309.08316'
source_url: https://arxiv.org/abs/2309.08316
tags:
- language
- learning
- conference
- tasks
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work comprehensively evaluates out-of-distribution (OOD) generalization
  capabilities of language models (LMs) across multiple shift types (topic, domain,
  language) in computational argumentation tasks. It introduces 11 diverse OOD classification
  tasks and three fine-grained evaluation metrics (Applicability, Reliability, Stability)
  to identify generalization flaws.
---

# How to Handle Different Types of Out-of-Distribution Scenarios in Computational Argumentation? A Comprehensive and Fine-Grained Field Study

## Quick Facts
- arXiv ID: 2309.08316
- Source URL: https://arxiv.org/abs/2309.08316
- Reference count: 40
- Primary result: P+FT consistently outperforms vanilla fine-tuning across most tasks, achieving better performance (+2.3 F1), reliability (+5.8 Kendall correlation), and stability

## Executive Summary
This work comprehensively evaluates out-of-distribution (OOD) generalization capabilities of language models (LMs) across multiple shift types (topic, domain, language) in computational argumentation tasks. It introduces 11 diverse OOD classification tasks and three fine-grained evaluation metrics (Applicability, Reliability, Stability) to identify generalization flaws. Experiments with BERT, RoBERTa, and DeBERTa-v3 show that prompt-based fine-tuning (P+FT) consistently outperforms vanilla fine-tuning across most tasks, achieving better performance (+2.3 F1), reliability (+5.8 Kendall correlation), and stability. P+FT is particularly effective when training and test splits differ semantically. In-context learning with ChatGPT excels for domain shifts but underperforms gradient-based methods overall. Analysis reveals P+FT preserves more semantic information in encoder layers and shows less bias toward surface features compared to vanilla fine-tuning.

## Method Summary
The study evaluates out-of-distribution generalization for LMs using 11 classification tasks covering topic, domain, and language shifts in computational argumentation. Three learning paradigms are compared: vanilla fine-tuning (FT), prompt-based fine-tuning (P+FT), and in-context learning (ICL). P+FT wraps inputs with cloze templates and uses the pre-trained MLM head for predictions rather than initializing new classification heads. Three fine-grained metrics assess performance: Applicability (F1 score), Reliability (Kendall correlation between loss and F1), and Stability (standard deviation of F1 and correlation across runs). Experiments use three seeds and three-fold cross-validation with BERT, RoBERTa, and DeBERTa-v3 models.

## Key Results
- P+FT consistently outperforms vanilla fine-tuning across most tasks, achieving better performance (+2.3 F1), reliability (+5.8 Kendall correlation), and stability
- In-context learning with ChatGPT excels for domain shifts but underperforms gradient-based methods overall
- P+FT is particularly effective when training and test splits differ semantically
- P+FT preserves more semantic information in encoder layers and shows less bias toward surface features compared to vanilla fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-based fine-tuning (P+FT) improves out-of-distribution (OOD) generalization by leveraging pre-trained MLM heads instead of initializing new classification heads.
- Mechanism: P+FT uses the pre-trained MLM head to predict masked tokens, which allows the model to retain semantic information in encoder layers during training, leading to better generalization across different shifts.
- Core assumption: The pre-trained MLM head contains semantic knowledge that is useful for downstream tasks and that this knowledge is preserved during fine-tuning.
- Evidence anchors:
  - [abstract] "P+FT consistently outperforms vanilla fine-tuning across most tasks, achieving better performance (+2.3 F1), reliability (+5.8 Kendall correlation), and stability."
  - [section 4.3] "We see that P+FT predicts with a similar certainty as ID fine-tuning (FT-ID) and a higher one than FT. Thus, we see P+FT being less confused by the distribution shift than FT."
  - [corpus] Weak evidence - no direct mention of MLM heads or semantic retention.
- Break condition: If the pre-trained MLM head does not contain relevant semantic knowledge for the downstream task, or if fine-tuning significantly alters the MLM head's weights.

### Mechanism 2
- Claim: P+FT leads to more evenly distributed token attributions compared to vanilla fine-tuning, resulting in better generalization.
- Mechanism: P+FT distributes the attribution of input tokens more evenly across the model, preventing the model from relying too heavily on specific tokens or features, which improves its ability to handle distribution shifts.
- Core assumption: Evenly distributed token attributions lead to better generalization.
- Evidence anchors:
  - [section 4.3] "We can observe this pattern already when we compare the token attribution of the pre-trained model (raw vs. P+raw). We see that, on average, FT-ID and FT have a higher average attribution than P+FT. Therefore, attributions are more evenly distributed for P+FT."
  - [section 4.3] "With these results, we assume LMs applied in prompt-based or vanilla fine-tuning fundamentally differ in how inputs are processed."
  - [corpus] Weak evidence - no direct mention of token attributions or their distribution.
- Break condition: If the distribution of token attributions does not significantly impact the model's generalization ability, or if other factors play a more important role.

### Mechanism 3
- Claim: In-context learning (ICL) with ChatGPT excels for domain shifts but underperforms gradient-based methods overall because it leverages pre-training data that already covers certain domains.
- Mechanism: ICL with ChatGPT benefits from its massive pre-training corpus, which likely includes instances from various domains, allowing it to perform well on domain shifts. However, it may not generalize as well to truly unseen data or different types of shifts.
- Core assumption: ChatGPT's pre-training corpus includes a wide variety of domains, allowing it to perform well on domain shifts.
- Evidence anchors:
  - [abstract] "In-context learning with ChatGPT excels for domain shifts but underperforms gradient-based methods overall."
  - [section 4.2] "Regarding the different types of shifts, ICL underperforms them on topic shifts while performing on par or better for tasks exhibiting a domain shift."
  - [corpus] Weak evidence - no direct mention of ChatGPT's pre-training corpus or its coverage of different domains.
- Break condition: If ChatGPT's pre-training corpus does not cover a wide variety of domains, or if the model does not effectively leverage this knowledge for domain shifts.

## Foundational Learning

- Concept: Out-of-distribution (OOD) generalization
  - Why needed here: The paper focuses on evaluating and improving LMs' ability to generalize to unseen data distributions, which is a crucial aspect of real-world applications.
  - Quick check question: What is the difference between in-distribution (ID) and out-of-distribution (OOD) generalization?

- Concept: Distribution shifts
  - Why needed here: The paper investigates three types of distribution shifts (topic, domain, and language) and their impact on OOD generalization.
  - Quick check question: How do topic, domain, and language shifts differ in terms of their impact on OOD generalization?

- Concept: Prompt-based fine-tuning (P+FT)
  - Why needed here: P+FT is a key method proposed in the paper for improving OOD generalization, and understanding its mechanism is crucial for implementing and evaluating it.
  - Quick check question: How does P+FT differ from vanilla fine-tuning, and what are its advantages for OOD generalization?

## Architecture Onboarding

- Component map: BERT/RoBERTa/DeBERTa-v3 -> Learning Paradigm (LP/FT/P/P+FT/ICL) -> Evaluation Metrics (Applicability/Reliability/Stability) -> OOD Tasks (Topic/Domain/Language shifts)
- Critical path: Select LM and learning paradigm → Design OOD tasks → Evaluate performance using metrics → Analyze results to identify strengths and weaknesses
- Design tradeoffs: Choice of LM and learning paradigm involves tradeoffs between performance, efficiency, and generalization ability. P+FT may offer better generalization but requires more careful design of prompts and verbalizers.
- Failure signatures: Poor performance on certain OOD tasks, low reliability (misalignment of loss and performance), high stability (large deviations across runs), or over-reliance on surface features (Flesch score, word count) may indicate weaknesses in the model's generalization ability.
- First 3 experiments:
  1. Evaluate the baseline performance of different LMs using vanilla fine-tuning on a subset of OOD tasks.
  2. Implement and evaluate P+FT on the same tasks, comparing its performance to vanilla fine-tuning.
  3. Analyze the token attributions and semantic information retention of P+FT compared to vanilla fine-tuning to understand its mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which prompt-based fine-tuning (P+FT) retains more semantic information in encoder layers compared to vanilla fine-tuning (FT)?
- Basis in paper: [inferred] from the analysis showing P+FT preserves semantic information until the last layers while FT eliminates it across all layers during training.
- Why unresolved: The paper provides empirical evidence of this difference but does not explain the underlying mechanism causing P+FT to preserve semantic information better.
- What evidence would resolve it: Detailed analysis of attention weights, gradient flow, or other internal representations during P+FT vs FT training could elucidate the mechanism.

### Open Question 2
- Question: How does the size of the vocabulary in a language model (e.g., DeBERTa-v3's 120k tokens vs RoBERTa's 50k tokens) contribute to its superior performance in prompt-based fine-tuning?
- Basis in paper: [explicit] The paper notes DeBERTa-v3's significantly larger vocabulary as a potential reason for its success with P+FT.
- Why unresolved: While the paper mentions the larger vocabulary as a possible factor, it does not quantify its impact or compare models with similar vocabularies.
- What evidence would resolve it: Experiments comparing models with varying vocabulary sizes using P+FT could isolate the effect of vocabulary size on performance.

### Open Question 3
- Question: What is the relationship between the distribution shift types (topic, domain, language) and the effectiveness of in-context learning (ICL) with large language models like ChatGPT?
- Basis in paper: [explicit] The paper shows ICL excels for domain shifts but underperforms for topic shifts compared to gradient-based methods.
- Why unresolved: The paper demonstrates this relationship but does not explain why ICL performs better for domain shifts and worse for topic shifts.
- What evidence would resolve it: Further analysis of the training data and capabilities of large language models, as well as experiments varying the degree of distribution shift within each type, could clarify this relationship.

## Limitations
- Mechanism explanations rely heavily on indirect evidence rather than direct architectural analysis
- Prompt template and verbalizer selection process lacks sufficient detail for precise replication across all tasks
- In-context learning comparisons use ChatGPT specifically without exploring how other few-shot approaches might perform
- Analysis doesn't address computational efficiency trade-offs between methods

## Confidence
- **High Confidence**: Empirical findings showing P+FT outperforms vanilla fine-tuning across multiple metrics (F1, reliability, stability)
- **Medium Confidence**: Explanations for why P+FT works better, particularly claims about semantic information preservation in encoder layers
- **Low Confidence**: Specific claims about how pre-trained MLM heads retain semantic knowledge during P+FT and why this leads to better generalization

## Next Checks
1. **Direct Mechanism Validation**: Conduct ablation studies removing the MLM head during P+FT to test whether semantic information retention is indeed the key mechanism, and compare against using a randomly initialized classification head.

2. **Prompt Template Sensitivity Analysis**: Systematically vary prompt templates and verbalizer selection methods across all tasks to quantify their impact on P+FT performance and determine whether the benefits are robust to template choice.

3. **Computational Efficiency Comparison**: Measure and compare the computational costs (training/inference time, memory usage) of P+FT versus vanilla fine-tuning across different tasks and model sizes.