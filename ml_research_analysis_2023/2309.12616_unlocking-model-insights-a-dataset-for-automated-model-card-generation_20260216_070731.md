---
ver: rpa2
title: 'Unlocking Model Insights: A Dataset for Automated Model Card Generation'
arxiv_id: '2309.12616'
source_url: https://arxiv.org/abs/2309.12616
tags:
- dataset
- galactica
- cards
- answers
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a dataset of 500 question-answer pairs for
  25 ML models to automate model card generation. The dataset covers crucial aspects
  like training configurations, datasets, biases, architecture details, and training
  resources.
---

# Unlocking Model Insights: A Dataset for Automated Model Card Generation

## Quick Facts
- **arXiv ID:** 2309.12616
- **Source URL:** https://arxiv.org/abs/2309.12616
- **Reference count:** 16
- **Primary result:** Introduces dataset of 500 Q&A pairs for 25 ML models to automate model card generation, revealing significant LLM limitations in factual extraction from research papers.

## Executive Summary
This paper introduces a dataset of 500 question-answer pairs for 25 machine learning models to automate model card generation. The dataset covers crucial aspects like training configurations, datasets, biases, architecture details, and training resources. The authors evaluate large language models (ChatGPT-3.5, LLaMa, and Galactica) in a zero-shot setting to assess their ability to generate model cards from research papers. Initial experiments reveal a significant gap in LLMs' understanding of research papers and generating factual responses, highlighting the need for further development in automated model card generation.

## Method Summary
The authors created a dataset of 500 question-answer pairs for 25 ML models through a three-stage annotation pipeline: question formulation, preliminary annotation by students, and expert annotation. They evaluated four LLMs (ChatGPT-3.5, LLaMa 7B, Galactica 125M, and Galactica 1.3B) by prompting them with the question-answer pairs in a zero-shot setting, restricting outputs to 1000 tokens. Automatic evaluation metrics (BLEU, ROUGE-L, BERT-Score) and qualitative assessment by subject expert annotators were used to measure performance.

## Key Results
- The dataset of 500 Q&A pairs for 25 ML models can be used to train models for automating model card generation from paper text
- Initial experiments with ChatGPT-3.5, LLaMa, and Galactica showcase significant gaps in understanding research papers and generating factual responses
- The two-stage annotation pipeline (preliminary and expert annotation) ensures quality and accuracy of the dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset's question-answer pairs enable LLMs to learn structured information extraction from research papers for model card generation.
- Mechanism: By providing specific questions targeting crucial aspects of ML models (training configurations, datasets, biases, architecture details, and training resources), the dataset teaches LLMs to identify and extract relevant information from research papers in a structured format. This structured extraction is essential for generating comprehensive model cards.
- Core assumption: LLMs can learn to answer specific questions about ML models by being trained on a dataset of question-answer pairs extracted from research papers.
- Evidence anchors:
  - [abstract]: "Our dataset can be used to train models to automate the generation of model cards from paper text and reduce human effort in the model card curation process."
  - [section]: "Our dataset can be used for instruction tuning LLMs for generating ML model cards by prompting specific questions about different aspects of the model."
- Break condition: If the LLM fails to understand the context of the research papers or the specific questions, it will not be able to extract the relevant information accurately.

### Mechanism 2
- Claim: The two-stage annotation pipeline (preliminary and expert annotation) ensures the quality and accuracy of the dataset.
- Mechanism: The preliminary annotation stage involves undergraduate and graduate students with knowledge of traditional ML and basic knowledge of DL extracting answers from research papers. The expert annotation stage involves a subject expert with expertise in DL reviewing and assessing the answers for accuracy, completeness, and relevance. This two-stage process combines different perspectives and expertise levels to establish a comprehensive and reliable ground truth dataset.
- Core assumption: Combining the perspectives of annotators with varying levels of expertise (undergraduate/graduate students and subject experts) results in a more accurate and comprehensive dataset.
- Evidence anchors:
  - [section]: "By incorporating both the preliminary and expert annotation stages, the curation pipeline adds an extra layer of annotation and combines different perspectives and expertise levels to establish a comprehensive and reliable ground truth dataset for model cards."
- Break condition: If the annotators lack sufficient knowledge of the domain or the research papers are not well-written, the quality of the dataset may be compromised.

### Mechanism 3
- Claim: Evaluating LLMs in a zero-shot setting reveals their current limitations in understanding research papers and generating factual responses.
- Mechanism: By prompting ChatGPT-3.5, LLaMa, and Galactica to answer model card questions without any fine-tuning or additional training, the authors assess their ability to comprehend research papers and generate accurate, factual responses. The results show a significant gap in the understanding of research papers by these LLMs, indicating the need for further development and training.
- Core assumption: Zero-shot evaluation provides an unbiased assessment of an LLM's current capabilities in understanding research papers and generating factual responses.
- Evidence anchors:
  - [abstract]: "Our initial experiments with ChatGPT-3.5, LLaMa, and Galactica showcase a significant gap in the understanding of research papers by these aforementioned LMs as well as generating factual textual responses."
- Break condition: If the LLM has been exposed to similar research papers during its training, it may perform better than expected in the zero-shot setting, skewing the results.

## Foundational Learning

- Concept: Understanding of transformer architectures and their variations (encoder-only, decoder-only, encoder-decoder)
  - Why needed here: The dataset focuses on language models, which are successors of the transformer model. Annotators and LLMs need to understand the basic transformer architecture and its variations to accurately extract and generate information about model architectures.
  - Quick check question: Can you explain the difference between encoder-only, decoder-only, and encoder-decoder transformer architectures?

- Concept: Familiarity with common datasets, tasks, and evaluation metrics in the NLP domain
  - Why needed here: The dataset includes questions about the datasets used to train and evaluate the models, as well as the tasks and evaluation metrics employed. Annotators and LLMs need to be familiar with common datasets, tasks, and metrics in the NLP domain to accurately extract and generate this information.
  - Quick check question: Name three popular datasets used for training and evaluating language models in the NLP domain.

- Concept: Knowledge of training configurations, hyperparameters, and computational resources
  - Why needed here: The dataset includes questions about the training setup, such as learning rate, steps, epochs, optimizer, and computational resources used. Annotators and LLMs need to understand these concepts to accurately extract and generate information about the model training process.
  - Quick check question: What are some common hyperparameters that need to be set when training a language model?

## Architecture Onboarding

- Component map:
  Dataset of question-answer pairs extracted from research papers -> Two-stage annotation pipeline (preliminary and expert annotation) -> LLMs (ChatGPT-3.5, LLaMa, and Galactica) for evaluation -> Automatic evaluation metrics (BLEU, ROUGE-L, BERT-Score) -> Qualitative evaluation by subject expert annotator

- Critical path:
  1. Curate a list of popular language models and their research papers
  2. Formulate a standardized set of questions covering crucial aspects of the models
  3. Employ annotators to extract preliminary answers from the research papers
  4. Have a subject expert review and assess the answers for accuracy, completeness, and relevance
  5. Evaluate the performance of LLMs in generating answers for the model card questions
  6. Analyze the results and identify areas for improvement

- Design tradeoffs:
  - Using a two-stage annotation pipeline increases the quality and reliability of the dataset but also increases the time and effort required for annotation.
  - Evaluating LLMs in a zero-shot setting provides an unbiased assessment of their current capabilities but may not reflect their potential performance after fine-tuning or additional training.
  - Focusing on language models limits the applicability of the dataset to other domains but allows for a more targeted and specific evaluation.

- Failure signatures:
  - If the dataset contains inaccurate or incomplete information, the generated model cards will also be inaccurate or incomplete.
  - If the LLMs fail to understand the context of the research papers or the specific questions, they will not be able to extract the relevant information accurately.
  - If the evaluation metrics or qualitative assessment are not comprehensive enough, they may not capture all aspects of the LLM's performance.

- First 3 experiments:
  1. Evaluate the performance of LLMs in generating answers for a subset of the model card questions using both automatic and qualitative evaluation methods.
  2. Analyze the results to identify common failure modes and areas where the LLMs struggle the most (e.g., understanding specific technical concepts, generating factual responses).
  3. Based on the analysis, refine the question set, annotation guidelines, or LLM evaluation methodology to address the identified issues and improve the overall quality of the dataset and evaluation process.

## Open Questions the Paper Calls Out

- **Open Question 1**: How effective are LLMs at extracting factual details about model training resources from research papers compared to human annotators?
  - Basis in paper: Explicit
  - Why unresolved: The paper evaluates LLM performance on model card generation but does not directly compare their factual extraction accuracy against human annotators on a per-detail basis. This would require a more granular analysis of specific training resource details.
  - What evidence would resolve it: A study comparing the factual accuracy of LLM-extracted training resource details against human-annotated ground truth for a subset of the dataset.

- **Open Question 2**: What are the key limitations of current LLMs in understanding and generating factual responses about complex model architectures described in research papers?
  - Basis in paper: Inferred
  - Why unresolved: While the paper notes that LLMs struggle with factual generation, it does not deeply analyze the specific architectural comprehension challenges. This requires further investigation into the types of architectural details that pose difficulties for LLMs.
  - What evidence would resolve it: An analysis of LLM responses on architectural questions, identifying common errors and patterns of misunderstanding related to specific architectural concepts.

- **Open Question 3**: How does the performance of LLMs on model card generation vary across different domains of machine learning models (e.g., NLP, computer vision, robotics)?
  - Basis in paper: Inferred
  - Why unresolved: The current dataset focuses on NLP models. Evaluating LLM performance on model cards for other domains would reveal potential domain-specific strengths and weaknesses.
  - What evidence would resolve it: Extending the dataset to include model cards for models from various domains and benchmarking LLM performance across these domains.

## Limitations

- The evaluation methodology relies heavily on human annotation for quality control, but lacks detailed information about annotator selection criteria, inter-annotator agreement metrics, and specific annotation guidelines
- Zero-shot evaluation approach may not fully capture the potential performance of LLMs after fine-tuning or additional training on similar research papers
- Automatic evaluation metrics (BLEU, ROUGE-L, BERT-Score) are known to have limitations in capturing semantic similarity and factual correctness, particularly for technical content with specialized vocabulary

## Confidence

- **High Confidence**: The dataset creation methodology and the two-stage annotation pipeline are well-specified and align with established practices in dataset curation. The use of multiple evaluation metrics and qualitative assessment by subject experts provides a robust evaluation framework.
- **Medium Confidence**: The claim about LLMs' significant gap in understanding research papers is supported by the experimental results, but the zero-shot setting may underestimate their true capabilities. The effectiveness of the dataset in automating model card generation will depend on the specific use case and the quality of the fine-tuned models.
- **Low Confidence**: The generalizability of the dataset and evaluation methodology to other domains beyond language models is uncertain, as the questions and annotation guidelines are tailored to the specific characteristics of language models and their research papers.

## Next Checks

1. **Inter-annotator Agreement Analysis**: Calculate and report inter-annotator agreement scores (e.g., Cohen's kappa) for both the preliminary and expert annotation stages to quantify the consistency and reliability of the ground truth data. This will help assess the quality of the dataset and identify potential sources of annotation errors or disagreements.

2. **Fine-tuning Experiment**: Fine-tune the evaluated LLMs (ChatGPT-3.5, LLaMa, and Galactica) on a subset of the curated dataset and re-evaluate their performance on the remaining questions. Compare the fine-tuned results with the zero-shot evaluation to determine the impact of additional training on the LLMs' ability to generate accurate model card answers. This will provide insights into the potential benefits of using the dataset for model card generation and the expected performance gains from fine-tuning.

3. **Cross-domain Applicability Test**: Select a set of research papers from a different domain (e.g., computer vision or reinforcement learning) and apply the same question set and annotation pipeline to create a domain-specific dataset. Evaluate the performance of the LLMs on this new dataset and compare it with their performance on the language model dataset. This will assess the generalizability of the methodology and the potential for adapting it to other domains beyond language models.