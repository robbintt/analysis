---
ver: rpa2
title: Stochastic Unrolled Federated Learning
arxiv_id: '2305.15371'
source_url: https://arxiv.org/abs/2305.15371
tags:
- learning
- unrolled
- which
- algorithm
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel federated learning method called Stochastic
  UnRolled Federated Learning (SURF) that combines algorithm unrolling with federated
  learning. The key idea is to unfold the distributed gradient descent (DGD) algorithm
  into a learnable graph neural network (GNN) architecture.
---

# Stochastic Unrolled Federated Learning

## Quick Facts
- arXiv ID: 2305.15371
- Source URL: https://arxiv.org/abs/2305.15371
- Reference count: 40
- Key outcome: Novel federated learning method (SURF) combining algorithm unrolling with federated learning, achieving comparable performance to baselines with fewer unrolled layers

## Executive Summary
This paper proposes Stochastic UnRolled Federated Learning (SURF), a method that combines algorithm unrolling with federated learning by unfolding the distributed gradient descent (DGD) algorithm into a graph neural network (GNN) architecture. SURF addresses two key challenges: computational efficiency by using stochastic mini-batches instead of entire datasets for unrolled layers, and preserving decentralized federated learning structure through GNN-based unrolling. The method is theoretically proven to converge to a near-optimal region infinitely often, and experiments on CIFAR-10 demonstrate comparable performance to centralized and FedAvg baselines with fewer unrolled layers.

## Method Summary
SURF implements a U-DGD optimizer that unfolds DGD into L learnable GNN layers, where each layer receives stochastic mini-batches and performs neighbor aggregation via graph filters and local gradient computation via fully-connected perceptrons. The method uses descent constraints to ensure convergence despite mini-batch noise, and is trained through alternating minimization of primal and dual variables. Experiments use 100 agents on a 3-degree regular graph with CIFAR-10 datasets divided evenly, applying data augmentation and training for 300 epochs with ADAM optimizer.

## Key Results
- SURF achieves comparable performance to centralized and FedAvg baselines with fewer unrolled layers
- Theoretical proof shows SURF converges to a near-optimal region infinitely often
- Experiments on CIFAR-10 demonstrate effectiveness in collaborative training of image classifiers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stochastic unrolling reduces computational overhead by feeding mini-batches to each unrolled layer instead of entire datasets.
- Mechanism: Each unrolled layer receives a fixed-size batch sampled uniformly from the training dataset, enabling fixed-size inputs and overcoming the need to accommodate varying-size datasets.
- Core assumption: The mini-batches provide sufficient statistical information to approximate descent directions effectively.
- Evidence anchors:
  - [abstract] "We circumvent the former challenge by feeding stochastic mini-batches to each unrolled layer and imposing descent constraints to mitigate the randomness induced by using mini-batches."
  - [section 4] "In stochastic unrolling, the main idea is to feed each layer l ∈ {1, . . . , L} of the unrolled network with a fixed-size batchBl sampled independently and uniformly from ϑ."
  - [corpus] Weak evidence - corpus contains papers on unrolled networks but none specifically address the mini-batch strategy for federated learning.
- Break condition: If mini-batches become too small or unrepresentative, the descent direction estimation becomes unreliable, causing poor convergence.

### Mechanism 2
- Claim: Descent constraints ensure convergence despite stochastic mini-batch noise.
- Mechanism: Imposes supermartingale-like constraints requiring expected gradient norms to decrease by a factor (1-ϵ) at each layer, forcing the unrolled optimizer to learn descent directions despite mini-batch randomness.
- Core assumption: The constrained formulation maintains sufficient descent properties even with stochastic gradients.
- Evidence anchors:
  - [abstract] "We circumvent the former challenge by feeding stochastic mini-batches to each unrolled layer and imposing descent constraints to mitigate the randomness induced by using mini-batches."
  - [section 4.1] "These constraints force the gradients to decrease despite the randomness in the layers' outputs introduced by relying on a few data points to estimate a descent direction."
  - [section 4.2] "Theorem 1. Given are a probably approximately correct unrolled optimizer θ∗ that satisfies (13) and a sequence of random variables W1, W2, . . . representing the outputs of the unrolled layers."
  - [corpus] Weak evidence - corpus has papers on unrolled networks and stochastic methods but none specifically prove convergence with descent constraints in federated settings.
- Break condition: If ϵ is set too large (aggressive reduction), the constraints become too restrictive and prevent learning; if too small, convergence guarantees weaken.

### Mechanism 3
- Claim: Graph neural network unrolling preserves decentralized federated learning structure.
- Mechanism: Unfolds DGD algorithm using GNN architectures, maintaining the decentralized nature where each agent only communicates with neighbors while achieving consensus through weighted aggregation.
- Core assumption: GNN-based unrolling can effectively learn the aggregation and gradient computation steps of DGD while preserving permutation equivariance and transferability.
- Evidence anchors:
  - [abstract] "We address the latter challenge by unfolding the distributed gradient descent (DGD) algorithm in a graph neural network (GNN)-based unrolled architecture, which preserves the decentralized nature of training in federated learning."
  - [section 5] "We construct our proposed U-DGD optimizer by unfolding these two steps in learnable neural layers, as described in Algorithm 1."
  - [section 5] "The weights αij are chosen such thatPn j=1 αij = 1 for all i to ensure that (5) converges [Nedic and Ozdaglar, 2009]."
  - [corpus] Weak evidence - corpus has papers on unrolled networks and GNNs but none specifically demonstrate DGD unfolding for federated learning.
- Break condition: If the graph structure changes significantly or communication patterns break down, the GNN-based unrolling may fail to maintain consensus.

## Foundational Learning

- Concept: Algorithm unrolling
  - Why needed here: Provides interpretable, efficient neural network architectures by unfolding iterative optimization algorithms into learnable layers, reducing the number of required iterations compared to standard algorithms.
  - Quick check question: How does algorithm unrolling differ from training a standard neural network from scratch?

- Concept: Graph neural networks
  - Why needed here: Enables distributed execution of unrolled DGD while preserving the decentralized communication structure inherent to federated learning.
  - Quick check question: What properties of GNNs make them suitable for modeling decentralized optimization algorithms?

- Concept: Stochastic optimization and mini-batching
  - Why needed here: Allows computational efficiency by using subsets of data while maintaining statistical convergence properties, critical for scaling to large federated datasets.
  - Quick check question: How does the variance of mini-batch gradients scale with batch size?

## Architecture Onboarding

- Component map: Data flows from sampled mini-batches through each unrolled layer where agents aggregate neighbor information and compute local updates, with the entire process constrained to ensure gradient descent properties and executed distributively across the network graph
- Critical path: Data flows from sampled mini-batches through each unrolled layer where agents aggregate neighbor information and compute local updates, with the entire process constrained to ensure gradient descent properties and executed distributively across the network graph
- Design tradeoffs: Fixed-size mini-batches enable computational efficiency but may reduce gradient accuracy; descent constraints ensure convergence but may limit learning flexibility; GNN architecture preserves decentralization but may constrain expressiveness compared to centralized approaches
- Failure signatures: Loss divergence despite descent constraints suggests mini-batches are too small or unrepresentative; communication bottlenecks indicate graph structure issues; poor transfer performance suggests GNN parameters are overfit to specific graph properties
- First 3 experiments:
  1. Train U-DGD with varying mini-batch sizes (B=5, 10, 20) on CIFAR-10 to find optimal batch size balancing computational efficiency and gradient quality
  2. Compare U-DGD performance with and without descent constraints to verify the impact of stochastic unrolling on convergence stability
  3. Test transferability by training on 100-agent 3-degree graphs and evaluating on 400-agent 6-degree graphs to measure GNN generalization across graph structures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SURF perform in heterogeneous federated learning scenarios where agents have different data distributions?
- Basis in paper: [inferred] The paper mentions that SURF could be extended to heterogeneous settings through loss reweighting techniques like Zhao and Joshi 2022, but does not provide experimental results.
- Why unresolved: The paper only tested SURF in homogeneous settings where all agents have the same data distribution.
- What evidence would resolve it: Experiments comparing SURF's performance to other methods (FedAvg, centralized training) on heterogeneous datasets like non-IID CIFAR-10 or FEMNIST.

### Open Question 2
- Question: How does the performance of SURF scale with increasing number of agents and graph size?
- Basis in paper: [inferred] The paper mentions transferability properties of GNNs but only tested on graphs of size 100 agents. The convergence guarantees are asymptotic but no empirical scaling study was provided.
- Why unresolved: The experiments were limited to 100 agents. The theoretical analysis assumes a fixed graph structure.
- What evidence would resolve it: Experiments testing SURF on graphs of varying sizes (e.g., 50, 200, 500 agents) and comparing convergence rates and final accuracy.

### Open Question 3
- Question: How sensitive is SURF to the choice of hyperparameters like the constraint parameter ϵ and batch size B?
- Basis in paper: [explicit] The paper mentions that the near-optimal region size depends on ϵ and that batch size B is a hyperparameter, but does not provide a sensitivity analysis.
- Why unresolved: The experiments used a fixed ϵ = 0.05 and B = 10 without exploring the sensitivity to these choices.
- What evidence would resolve it: Experiments varying ϵ and B systematically and measuring the impact on convergence speed, final accuracy, and robustness to noise.

## Limitations
- Theoretical convergence guarantees rely on restrictive assumptions about the meta-learning algorithm and graph structure
- Computational overhead of descent constraints may limit scalability to larger graphs
- Limited empirical validation on heterogeneous datasets and varying graph topologies

## Confidence
- Theoretical convergence claims: Medium - supported by proof but with restrictive assumptions
- Computational efficiency claims: Medium - demonstrated on CIFAR-10 but not extensively benchmarked
- Generalization across graph topologies: Low - only tested on regular graphs

## Next Checks
1. Test SURF on heterogeneous graph structures with varying degrees and connectivity patterns
2. Evaluate performance sensitivity to ϵ parameter across different problem scales
3. Compare computational overhead against standard federated learning baselines on larger datasets (e.g., ImageNet subset)