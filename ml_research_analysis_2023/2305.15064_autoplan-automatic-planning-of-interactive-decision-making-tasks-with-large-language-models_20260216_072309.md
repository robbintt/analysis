---
ver: rpa2
title: 'AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large
  Language Models'
arxiv_id: '2305.15064'
source_url: https://arxiv.org/abs/2305.15064
tags:
- task
- plan
- action
- llm-po
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-PO optimizes LLM-based agents for interactive decision-making
  tasks without requiring gradients or in-context demonstrations. It iteratively refines
  a text-based task-solving plan through experience collection, reflection, and strategy
  updates.
---

# AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models

## Quick Facts
- arXiv ID: 2305.15064
- Source URL: https://arxiv.org/abs/2305.15064
- Authors: 
- Reference count: 4
- Key outcome: LLM-PO optimizes LLM-based agents for interactive decision-making tasks without requiring gradients or in-context demonstrations. Experiments show success rates on par with ICL baselines on ALFWorld, and 8% better on HotpotQA, while using at least twice less inference cost.

## Executive Summary
AutoPlan introduces LLM-PO, a novel method for optimizing large language models (LLMs) in interactive decision-making tasks without gradient access or in-context demonstrations. The approach iteratively refines a text-based task-solving plan through experience collection, reflection, and strategy updates. By maintaining and updating a concise text-based strategy rather than relying on in-context examples, LLM-PO achieves competitive performance while being more efficient and explainable than traditional in-context learning approaches.

## Method Summary
LLM-PO operates by maintaining a text-based task-solving plan that guides the LLM's interactions with the environment. In each iteration, the LLM uses the current plan to collect experiences by interacting with the task environment. These experiences are then used for reflection, where the LLM analyzes the successes and failures to generate insights. Based on these reflections, the strategy is updated to improve future performance. This process repeats until convergence or a stopping criterion is met. The method avoids the need for gradient computations or human-written demonstrations by leveraging the LLM's ability to self-improve through this iterative process.

## Key Results
- Achieves success rates on par with ICL baselines using human-written demonstrations on ALFWorld
- Outperforms ICL baselines by 8% on HotpotQA
- Requires at least twice as less inference cost compared to ICL baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-PO achieves competitive success rates without gradient access or in-context demonstrations by iteratively refining a text-based task-solving plan.
- Mechanism: The method maintains a text-based plan that is updated through iterative experience collection, reflection, and strategy updates. Each iteration involves the LLM interacting with the environment using the current plan, collecting experiences, reflecting on successes and failures, and then revising the plan based on these reflections.
- Core assumption: LLMs can effectively reflect on their own experiences and generate meaningful improvements to a text-based plan without requiring external demonstrations or gradient-based optimization.
- Evidence anchors:
  - [abstract]: "AutoPlan augments the LLM prompt with a task-solving plan and optimizes it through iterative experience collection and reflection."
  - [section]: "We propose LLM-PO to optimize the prompt γ without gradient computations and human demonstrations... The key idea is to maintain a text-based plan and ask LLMs to reflect on pros and cons of the current plan based on experience collected with it, to update the plan, and to collect more experiences with the new plan."
  - [corpus]: Weak evidence. No direct citations in corpus neighbors about iterative plan refinement without demonstrations.
- Break condition: If the LLM fails to generate meaningful reflections or the reflections do not lead to plan improvements, the optimization process will stagnate or degrade.

### Mechanism 2
- Claim: Batching multiple experiences together stabilizes the optimization process by reducing the variance in plan updates.
- Mechanism: By processing multiple task instances in each iteration, the LLM can learn more general patterns and avoid overfitting to specific instances. This batch processing leads to more stable and consistent plan updates.
- Core assumption: Aggregating experiences from multiple task instances provides a more robust signal for plan improvement than individual experiences.
- Evidence anchors:
  - [abstract]: "LLM-PO achieves success rates on par with ICL baselines using human-written demonstrations... It also requires at least twice as less inference cost compared to ICL baselines."
  - [section]: "The update of text-based strategy is extremely versatile and unstable due to the nature of natural language. As mentioned in 2.4, we batch multiple task instances together in one iteration to stabilize the optimization."
  - [corpus]: Weak evidence. No direct citations in corpus neighbors about batching for stability in LLM plan optimization.
- Break condition: If the batch size is too small, the optimization may remain unstable. If too large, the process may become computationally expensive without proportional gains in stability.

### Mechanism 3
- Claim: The text-based plan form is more efficient and explainable than in-context demonstrations.
- Mechanism: Instead of providing multiple demonstrations in the prompt, LLM-PO uses a concise text-based strategy that describes the appropriate actions for different scenarios. This reduces the prompt length and makes the policy more interpretable.
- Core assumption: LLMs can effectively follow text-based instructions and that these instructions can capture the necessary policy without the need for extensive demonstrations.
- Evidence anchors:
  - [abstract]: "Unlike in-context learning, where γ is comprised of in-context examples, we consider γ in the form of a task-solving strategy that defines the appropriate action to take in different scenarios."
  - [section]: "This prompt form has several advantages. First, text-based strategy resembles the policy network in RL but more explainable. Second, instruction-tuned LLMs are able to follow the text-based strategy faithfully. Third, text-based strategy describes the policy more efficiently than using in-context demonstrations in terms of input length."
  - [corpus]: Weak evidence. No direct citations in corpus neighbors about text-based plans vs. in-context demonstrations.
- Break condition: If the text-based plan becomes too complex or ambiguous, the LLM may fail to follow it correctly, leading to suboptimal performance.

## Foundational Learning

- Concept: Iterative optimization through experience and reflection
  - Why needed here: LLM-PO relies on the LLM's ability to learn from its own experiences and improve its strategy over time, which is a form of experiential learning.
  - Quick check question: Can you explain how the reflection process in LLM-PO is similar to the reflection stage in Kolb's Experiential Learning Theory?

- Concept: In-context learning (ICL) and its limitations
  - Why needed here: Understanding ICL is crucial to appreciate why LLM-PO's approach of not using demonstrations is novel and potentially more efficient.
  - Quick check question: What are the main drawbacks of ICL that LLM-PO aims to address?

- Concept: Reinforcement learning (RL) concepts applied to language models
  - Why needed here: LLM-PO draws inspiration from RL but adapts it to work with LLMs that lack gradient access, requiring a different optimization approach.
  - Quick check question: How does LLM-PO's approach differ from traditional RL in terms of policy optimization?

## Architecture Onboarding

- Component map: Experience Collection -> Reflection Module -> Strategy Update -> Plan Storage
- Critical path: Plan → Experience Collection → Reflection → Strategy Update → New Plan
- Design tradeoffs:
  - Batch size vs. stability: Larger batches improve stability but increase computational cost
  - Reflection depth vs. efficiency: More detailed reflections may lead to better plans but require more tokens
  - Plan conciseness vs. expressiveness: Shorter plans are more efficient but may lack necessary detail
- Failure signatures:
  - Plan stagnation: No improvement in success rate across iterations
  - Reflection irrelevance: Reflections do not lead to meaningful plan changes
  - Experience collection failure: LLM fails to follow the plan correctly, leading to invalid experiences
- First 3 experiments:
  1. Verify that the LLM can follow a simple text-based plan on a basic task
  2. Test the reflection process by providing predefined experiences and checking the quality of reflections
  3. Implement a single iteration of experience collection, reflection, and plan update to ensure the components work together

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-PO scale with larger batch sizes during training?
- Basis in paper: [explicit] The paper mentions that batching improves learning stability and achieves higher success rates earlier in the number of iterations.
- Why unresolved: The paper only provides results for batch sizes of 2 and 8 on the Light task of ALFWorld. It does not explore a wider range of batch sizes or other tasks.
- What evidence would resolve it: Running experiments with various batch sizes (e.g., 4, 16, 32) on multiple tasks and analyzing the trade-off between learning stability and computational efficiency.

### Open Question 2
- Question: Can LLM-PO be extended to handle more complex tasks that require long-term planning and multi-step reasoning?
- Basis in paper: [inferred] The paper demonstrates LLM-PO's effectiveness on HotpotQA (multi-hop reasoning) and ALFWorld (long-horizon decision making), but it's unclear how well it would perform on tasks requiring even more complex reasoning and planning.
- Why unresolved: The paper only evaluates LLM-PO on two specific tasks. It does not explore its limitations or potential for handling more complex scenarios.
- What evidence would resolve it: Applying LLM-PO to more challenging tasks, such as solving puzzles, playing strategy games, or planning complex robot actions, and analyzing its performance and limitations.

### Open Question 3
- Question: How sensitive is LLM-PO to the quality and diversity of the initial task-solving plan?
- Basis in paper: [explicit] The paper mentions that LLM-PO starts with an empty strategy and iteratively refines it. It does not explore the impact of starting with different initial plans.
- Why unresolved: The paper only uses an empty initial plan for all experiments. It does not investigate how different initial plans might affect the learning process or final performance.
- What evidence would resolve it: Running experiments with various initial plans (e.g., random, partially correct, or expert-designed) and comparing their impact on LLM-PO's learning efficiency and final performance.

### Open Question 4
- Question: How does LLM-PO compare to other methods for optimizing LLM prompts, such as prompt tuning or reinforcement learning?
- Basis in paper: [explicit] The paper mentions that LLM-PO is a novel approach that does not require gradient access or in-context demonstrations. It does not directly compare its performance to other optimization methods.
- Why unresolved: The paper only compares LLM-PO to in-context learning (ICL) baselines. It does not explore its performance relative to other optimization techniques.
- What evidence would resolve it: Conducting a comprehensive comparison of LLM-PO with other optimization methods (e.g., prompt tuning, reinforcement learning) on a variety of tasks and analyzing their strengths and weaknesses.

## Limitations

- Limited evaluation on only two benchmarks with relatively small task variations
- No comparison against RL-based LLM fine-tuning methods despite claiming to be "RL-free"
- Unclear how the method scales to more complex, long-horizon decision-making tasks

## Confidence

- **High confidence**: The core iterative optimization framework is technically sound and well-motivated
- **Medium confidence**: The claim of reduced inference cost is supported by the reported numbers but lacks detailed breakdowns
- **Low confidence**: The generalizability claim beyond ALFWorld and HotpotQA is not adequately tested

## Next Checks

1. Implement ablation studies varying batch sizes, reflection depth, and plan representation formats to identify optimal configurations
2. Test the method on a more diverse set of decision-making tasks, including those requiring longer horizons and more complex reasoning
3. Compare LLM-PO against RL-based LLM fine-tuning approaches to validate the claimed advantages of avoiding gradients