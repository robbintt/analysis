---
ver: rpa2
title: 'AlpaGasus: Training A Better Alpaca with Fewer Data'
arxiv_id: '2307.08701'
source_url: https://arxiv.org/abs/2307.08701
tags:
- data
- alpagasus
- instruction
- arxiv
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of low-quality data in instruction-following
  datasets, which can hinder the performance of large language models (LLMs) during
  instruction-finetuning (IFT). The authors propose a data selection strategy that
  automatically filters out low-quality data using a strong LLM (e.g., ChatGPT) as
  an auto-grader.
---

# AlpaGasus: Training A Better Alpaca with Fewer Data

## Quick Facts
- arXiv ID: 2307.08701
- Source URL: https://arxiv.org/abs/2307.08701
- Reference count: 34
- Primary result: AlpaGasus, finetuned on only 9k high-quality data filtered from the 52k Alpaca data, significantly outperforms the original Alpaca model

## Executive Summary
This paper addresses the problem of low-quality data in instruction-following datasets, which can hinder the performance of large language models (LLMs) during instruction-finetuning (IFT). The authors propose a data selection strategy that automatically filters out low-quality data using a strong LLM (e.g., ChatGPT) as an auto-grader. The filtered data are then used to finetune the LLM, resulting in a model called AlpaGasus. The primary result is that AlpaGasus, finetuned on only 9k high-quality data filtered from the 52k Alpaca data, significantly outperforms the original Alpaca model. The 13B variant of AlpaGasus matches over 90% performance of its teacher LLM (Text-Davinci-003) on test tasks. Additionally, AlpaGasus provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for Alpaca) to 14 minutes.

## Method Summary
The method involves using ChatGPT to rate the quality of each (instruction, input, response) triplet based on accuracy. The high-quality data are then used to fine-tune a LLaMA model using the same training configuration as the original Alpaca model. The data selection process filters out low-quality instruction-response pairs based on LLM-based scoring, resulting in a high-quality subset of 9k samples from the original 52k Alpaca data. The fine-tuned model, AlpaGasus, is then evaluated on test sets using GPT-4 as a judge to compare its performance with the original Alpaca model and other baselines.

## Key Results
- AlpaGasus, finetuned on only 9k high-quality data, significantly outperforms the original Alpaca model
- The 13B variant of AlpaGasus matches over 90% performance of its teacher LLM (Text-Davinci-003) on test tasks
- AlpaGasus provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes to 14 minutes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data quality outweighs quantity in instruction-finetuning (IFT) when using a strong LLM for filtering
- Mechanism: Filtering low-quality instruction-response pairs based on LLM-based scoring improves model performance despite fewer training examples
- Core assumption: LLM scoring (ChatGPT) accurately distinguishes high-quality from low-quality instruction data
- Evidence anchors:
  - [abstract] "AlpaGasus, finetuned on only 9k high-quality data filtered from the 52k Alpaca data, significantly outperforms the original Alpaca model"
  - [section 4.1] "ALPAGASUS -9k achieves much better performance than ALPACA-52k on all four test sets"
  - [corpus] Weak - only neighbor papers mention data selection but no direct evidence of quality-over-quantity effect
- Break condition: If LLM scorer cannot distinguish quality differences or introduces systematic bias in scoring

### Mechanism 2
- Claim: Strong LLM teachers (like Text-Davinci-003) generate high-variance data with many low-quality examples
- Mechanism: Teacher LLM responses contain errors or irrelevance that mislead IFT when used directly
- Core assumption: Teacher-generated responses inherently contain quality issues that affect downstream model performance
- Evidence anchors:
  - [abstract] "widely used IFT datasets (e.g., ALPACA's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses"
  - [section 2.1] "Alpaca-cleaned is the pioneer of filtering bad data in ALPACA dataset though it requires humans fully involved"
  - [corpus] Weak - neighbor papers discuss data construction but don't address teacher quality variance
- Break condition: If teacher LLM quality improves to near-perfect response generation

### Mechanism 3
- Claim: Automated evaluation with GPT-4 judge provides reliable comparison between models
- Mechanism: Position-robust prompt design allows GPT-4 to compare model outputs without bias
- Core assumption: GPT-4 can reliably judge response quality and maintain >80% agreement with human preferences
- Evidence anchors:
  - [section 3.3] "we employ GPT-4 as our judge for the evaluation" and "GPT-4 judge could match with both the controlled and crowdsourced human preferences ( > 80% agreement)"
  - [appendix A] Detailed prompt structure shows position-agnostic comparison format
  - [corpus] Weak - no corpus evidence directly supporting GPT-4 judge reliability for this specific task
- Break condition: If judge exhibits systematic bias or performance drops below human agreement threshold

## Foundational Learning

- Concept: Instruction-FineTuning (IFT) pipeline
  - Why needed here: Core training methodology being improved by data selection
  - Quick check question: What distinguishes IFT from standard fine-tuning in terms of training objectives?

- Concept: LLM-based scoring and filtering
  - Why needed here: Primary mechanism for identifying high-quality training data
  - Quick check question: How does the threshold τ in Eq. (1) determine which data get filtered?

- Concept: Automated evaluation protocols
  - Why needed here: Required to validate improvements without expensive human annotation
  - Quick check question: Why does the evaluation prompt include both models' responses in random order?

## Architecture Onboarding

- Component map:
  Data source (Alpaca 52k) -> LLM scorer (ChatGPT) -> Filter (threshold τ=4.5) -> Selected dataset (9k) -> Training pipeline (same as Alpaca) -> Model (AlpaGasus) -> Evaluation: Test sets (4 sources) -> Judge (GPT-4) -> Win-Tie-Lose metric

- Critical path: Data selection -> Model training -> Automated evaluation
  - Most time-sensitive: Data filtering (can be parallelized)
  - Most resource-intensive: Model training (scales with model size)

- Design tradeoffs:
  - Threshold τ selection: Higher τ -> fewer, higher-quality samples but risk underfitting
  - LLM scorer choice: Stronger scorer -> better filtering but higher cost per sample
  - Training configuration: Same as baseline ensures fair comparison but may miss optimization opportunities

- Failure signatures:
  - Poor filtering: Performance matches or drops below baseline (Alpaca)
  - Judge bias: Systematic win/loss pattern correlated with response position
  - Overfitting: Performance gap between training and test sets widens

- First 3 experiments:
  1. Run filtering with τ=4.0 and τ=5.0 to observe performance sensitivity to threshold
  2. Compare filtered results using different LLM scorers (ChatGPT vs Claude)
  3. Test training on random 9k subset from Alpaca to validate quality-over-quantity effect

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several potential open questions can be inferred from the content, such as the generalizability of AlpaGasus's performance to other tasks and domains, the impact of different threshold values on the quality and quantity of the filtered data, and the effect of using different auto-grader LLMs on the data filtering process.

## Limitations
- The threshold selection (τ=4.5) appears somewhat arbitrary without ablation studies showing sensitivity to different thresholds
- Reliance on LLM-based scoring introduces potential systematic bias without human validation of the filtering quality
- The evaluation protocol using GPT-4 judge, while claiming >80% human agreement, lacks direct human evaluation comparison for this specific task
- The paper only tests with LLaMA as base model, limiting generalizability to other architectures

## Confidence
- **High confidence**: Data filtering improves Alpaca performance with same training configuration
- **Medium confidence**: 9k filtered data achieves comparable results to full 52k dataset (limited ablation testing)
- **Medium confidence**: Automated GPT-4 evaluation provides reliable model comparison (no direct human validation)

## Next Checks
1. Conduct ablation study with multiple threshold values (τ=4.0, 4.5, 5.0) to verify robustness of data selection
2. Perform human evaluation on a sample of filtered vs unfiltered data to validate LLM scorer accuracy
3. Test the filtering method with alternative base models (e.g., Mistral, Falcon) to assess generalizability