---
ver: rpa2
title: Collaboratively Learning Linear Models with Structured Missing Data
arxiv_id: '2307.11947'
source_url: https://arxiv.org/abs/2307.11947
tags:
- data
- local
- imputation
- each
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies collaborative learning of linear models when
  different agents observe different subsets of features, a scenario common in sensor
  networks with varying resolution. The authors propose a communication-efficient
  algorithm, Collab, which leverages local least squares estimates and weighted aggregation
  to achieve nearly asymptotically optimal estimation error without requiring labeled
  data exchange.
---

# Collaboratively Learning Linear Models with Structured Missing Data

## Quick Facts
- arXiv ID: 2307.11947
- Source URL: https://arxiv.org/abs/2307.11947
- Reference count: 40
- Primary result: Achieves nearly asymptotically local minimax optimal estimation without communicating labeled data

## Executive Summary
This paper addresses collaborative learning of linear models when agents observe different subsets of features, a common scenario in sensor networks with varying resolution. The authors propose Collab, a communication-efficient algorithm that aggregates local least squares estimates using weighted aggregation based on feature covariance and label variance structure. The algorithm achieves nearly asymptotically optimal estimation error without requiring labeled data exchange, matching local minimax lower bounds even compared to methods with full access to all agents' labeled data.

## Method Summary
The Collab algorithm operates in three steps: local agents compute OLS estimates and estimate their local covariance matrices from unlabeled data, then communicate these estimates along with residual sums of squares to a coordinating server. The server computes weights based on the covariance and label variance structure, performs weighted aggregation to obtain a global model, and distributes this model back to each agent. The key innovation is leveraging unlabeled data for covariance estimation while avoiding communication of labeled data, achieving communication efficiency scaling as Θ(d²ᵢ) per agent.

## Key Results
- Collab achieves nearly asymptotically local minimax optimal estimation without communicating labeled data
- The algorithm outperforms imputation methods even when those methods have full data access
- Communication cost scales as Θ(d²ᵢ) per agent, making it practical for high-dimensional settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Collab achieves nearly asymptotically local minimax optimal estimation by weighting local OLS estimates based on feature covariance and label variance structure.
- Mechanism: Each agent computes local OLS estimate, then a coordinating server aggregates using weighted de-biasing where weights depend on Σᵢ₊ and empirical label variance (Rᵢ). This weighting balances between variance reduction and bias control.
- Core assumption: Features follow Gaussian distribution so optimal weights have closed form W*g = Σᵢ₊/Rᵢ and aggregation preserves asymptotic normality.
- Evidence anchors:
  - [abstract] "Our procedure is nearly asymptotically local minimax optimal—even among estimators allowed to communicate the labeled data such as imputation methods."
  - [section] "We show that with some additional assumptions, Collab is also asymptotically locally minimax optimal among estimators that have access to all of the training data of all agents."
  - [corpus] Weak signal; corpus neighbors discuss multi-agent learning but not specifically weighted aggregation with structured missing data.
- Break condition: If feature distribution is non-Gaussian, optimal weights no longer have closed form and empirical estimation of E[xᵢ₊θᵢ₋zᵢ₊zᵢ₊₊θᵢ₋xᵢ₊₊] becomes required, which is not implemented.

### Mechanism 2
- Claim: Collab avoids communication of labeled data by leveraging unlabeled data for covariance estimation.
- Mechanism: Each agent sends only local OLS estimate, estimated covariance Σᵢ₊, and residual sum of squares Rᵢ to central server. Server computes weights and distributes aggregated estimate back.
- Core assumption: Agents have access to sufficient unlabeled data to estimate their local covariance Σᵢ₊ accurately.
- Evidence anchors:
  - [abstract] "Our procedure does not require communicating the labeled data, making it communication efficient and useful in settings where the labeled data is inaccessible."
  - [section] "Our algorithm is communication-efficient: each agent i ∈ [m] syncs twice with a coordinating server and incurs communication cost scaling like Θ(d²ᵢ)."
  - [corpus] Weak signal; corpus neighbors mention federated learning but not specifically covariance estimation from unlabeled data.
- Break condition: If unlabeled data is insufficient or unavailable, Σᵢ₊ estimates become noisy, weights become suboptimal, and performance degrades toward naive averaging.

### Mechanism 3
- Claim: Collab's weighted aggregation outperforms imputation methods even when imputation has full data access.
- Mechanism: Collab aggregates local biased estimates with optimal weights that account for missing-feature structure, while imputation methods either impute locally (introducing bias) or globally (requiring O(n) communication).
- Core assumption: Structured missingness pattern is known and agents can compute Tᵢ = [I_dᵢ Σ⁻¹ᵢ₊Σᵢ₊₊]Πᵢ for their missing-feature structure.
- Evidence anchors:
  - [abstract] "Despite this handicap, our procedure is nearly asymptotically local minimax optimal—even among estimators allowed to communicate the labeled data such as imputation methods."
  - [section] "We prove local minimax lower bounds which prove that Collab is (nearly) instance-optimal. We choose to study this problem in a stylized linear setting so that we can provide stronger guarantees."
  - [corpus] Weak signal; corpus neighbors discuss imputation in different contexts but not structured missing data with known patterns.
- Break condition: If missingness pattern is unknown or varies unpredictably, Tᵢ cannot be computed correctly and aggregation becomes biased.

## Foundational Learning

- Concept: Linear regression with missing features and MAR assumption
  - Why needed here: Problem setup assumes agents observe different feature subsets with known missingness pattern, requiring adaptation of standard regression theory.
  - Quick check question: What is the difference between MCAR and MAR missingness, and why does this problem assume MAR?

- Concept: Weighted empirical risk minimization and asymptotic normality
  - Why needed here: Collab's aggregation step uses weighted WERM, and theoretical guarantees rely on establishing asymptotic normality of the aggregated estimator.
  - Quick check question: How does the choice of weighting matrices Wᵢ affect the asymptotic covariance of the aggregated estimator?

- Concept: Fisher information and local minimax lower bounds
  - Why needed here: Theoretical optimality proofs use local asymptotic minimax framework, comparing Collab's asymptotic covariance to information-theoretic lower bounds.
  - Quick check question: What is the relationship between Fisher information and asymptotic variance in the local asymptotic minimax framework?

## Architecture Onboarding

- Component map:
  - Local agents -> Coordinating server -> Local agents
  - Each agent computes local OLS and covariance -> Server aggregates with weights -> Server distributes aggregated model

- Critical path:
  1. Agents compute local OLS: ˆθᵢ = (Xᵢ₊ᵀXᵢ₊)⁻¹Xᵢ₊ᵀyᵢ
  2. Agents compute Σᵢ₊ and Rᵢ from data
  3. Agents send (ˆθᵢ, Σᵢ₊, Rᵢ) to server
  4. Server computes ˆW_gᵢ = Σᵢ₊/Rᵢ for each agent
  5. Server computes ˆθ_clb = argmin_θ Σᵢ TᵢᵀˆW_gᵢTᵢ(θ − ˆθᵢ)²
  6. Server distributes Tᵢˆθ_clb to each agent

- Design tradeoffs:
  - Communication vs. statistical efficiency: Collab trades labeled data communication for unlabeled data usage and weight computation
  - Gaussian assumption vs. generality: Closed-form weights require Gaussian features; non-Gaussian requires sampling-based estimation
  - Centralization vs. privacy: Single coordinating server simplifies aggregation but creates single point of failure

- Failure signatures:
  - Poor weight estimation: If Σᵢ₊ is poorly estimated, weights become unstable and aggregation degrades to naive averaging
  - Communication bottlenecks: If d²ᵢ is large, communication cost becomes prohibitive despite being sublinear in n
  - Missingness pattern errors: Incorrect Tᵢ computation due to wrong missingness assumptions leads to biased aggregation

- First 3 experiments:
  1. Synthetic data with known covariance structure: verify that Collab matches theoretical asymptotic covariance predictions
  2. Real data with artificial missingness: compare Collab against imputation baselines on prediction error
  3. Communication cost scaling: measure actual communication cost vs. theoretical Θ(d²ᵢ) as d and m vary

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Collab algorithm be extended to non-linear models beyond linear regression?
- Basis in paper: [explicit] The paper discusses potential generalizations to non-linear settings in Section 7, suggesting a weighted aggregation approach for non-linear models.
- Why unresolved: The authors acknowledge this as an open direction for future work and do not provide theoretical guarantees or empirical validation for non-linear models.
- What evidence would resolve it: Theoretical analysis proving asymptotic optimality of the generalized weighted aggregation approach for specific non-linear model families, or empirical results demonstrating improved performance compared to existing federated learning methods on non-linear datasets.

### Open Question 2
- Question: How does the performance of Collab change under non-Gaussian feature distributions?
- Basis in paper: [explicit] The authors mention in Section 7 that the closed-form optimal weights in Collab rely on Gaussianity and suggest potential estimation approaches using unlabeled data.
- Why unresolved: The paper does not provide theoretical analysis or experiments for non-Gaussian feature settings, leaving the optimal weights and convergence guarantees unknown.
- What evidence would resolve it: Empirical evaluation of Collab's performance on real-world datasets with non-Gaussian features, or theoretical bounds on estimation error when using plug-in estimates of optimal weights from unlabeled data.

### Open Question 3
- Question: What is the impact of privacy constraints on the communication efficiency of Collab?
- Basis in paper: [explicit] The authors acknowledge in the introduction that privacy considerations are important for real-world systems but choose to focus on sensor settings where privacy is less of a concern.
- Why unresolved: The paper does not analyze how privacy-preserving techniques (e.g., differential privacy, secure aggregation) would affect Collab's communication cost or statistical performance.
- What evidence would resolve it: Implementation and evaluation of privacy-preserving variants of Collab, comparing their communication costs and prediction errors against the original algorithm and other privacy-preserving federated learning methods.

## Limitations

- Gaussian feature assumption: Optimal weights have closed form only under Gaussian features; non-Gaussian requires sampling-based estimation not implemented
- Limited empirical validation: Only tested on synthetic data and one real dataset (US Census) without exploring failure modes
- Centralization dependency: Single coordinating server creates single point of failure and communication bottleneck

## Confidence

Our confidence in the main claims is **Medium-High**. The theoretical analysis establishing asymptotic optimality is rigorous, with clear proofs of local minimax lower bounds and the Collab algorithm achieving near-optimal performance. However, the Gaussian feature assumption for optimal weights is a significant limitation - the paper acknowledges that non-Gaussian features would require empirical estimation of E[xᵢ₊θᵢ₋zᵢ₊zᵢ₊₊θᵢ₋xᵢ₊₊], which is not implemented. The empirical validation, while showing Collab outperforming baselines, is limited to synthetic and one real dataset (US Census), and doesn't explore failure modes like insufficient unlabeled data or incorrect missingness pattern assumptions. The communication cost analysis assumes d²ᵢ scaling but doesn't account for potential numerical stability issues in matrix operations.

## Next Checks

1. Test Collab with non-Gaussian features to verify performance degradation and necessity of sampling-based weight estimation
2. Evaluate sensitivity to unlabeled data quantity by varying the ratio of unlabeled to labeled samples per agent
3. Implement Collab with incorrect missingness pattern assumptions to quantify bias introduction in the aggregation step