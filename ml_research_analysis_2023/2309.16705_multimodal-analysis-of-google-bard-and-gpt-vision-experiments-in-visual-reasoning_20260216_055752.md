---
ver: rpa2
title: 'Multimodal Analysis Of Google Bard And GPT-Vision: Experiments In Visual Reasoning'
arxiv_id: '2309.16705'
source_url: https://arxiv.org/abs/2309.16705
tags:
- image
- bard
- visual
- text
- black
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a challenge-response study to assess the visual
  comprehension abilities of large language models (LLMs), specifically Google Bard
  and GPT-Vision. The study subjects these models to 64 visual tasks spanning categories
  like "Visual Situational Reasoning" and "Next Scene Prediction." Key findings include
  that while these models can solve visual CAPTCHAs that stump ChatGPT alone, they
  struggle with recreating visual elements like ASCII art or analyzing Tic Tac Toe
  grids, suggesting an over-reliance on educated visual guesses.
---

# Multimodal Analysis Of Google Bard And GPT-Vision: Experiments In Visual Reasoning

## Quick Facts
- arXiv ID: 2309.16705
- Source URL: https://arxiv.org/abs/2309.16705
- Reference count: 40
- Primary result: Multimodal LLMs like Google Bard and GPT-Vision can solve visual CAPTCHAs that stump ChatGPT alone, but struggle with recreating visual elements like ASCII art or analyzing Tic Tac Toe grids.

## Executive Summary
This paper conducts a challenge-response study to assess the visual comprehension abilities of large language models (LLMs), specifically Google Bard and GPT-Vision. The study subjects these models to 64 visual tasks spanning categories like "Visual Situational Reasoning" and "Next Scene Prediction." Key findings include that while these models can solve visual CAPTCHAs that stump ChatGPT alone, they struggle with recreating visual elements like ASCII art or analyzing Tic Tac Toe grids, suggesting an over-reliance on educated visual guesses. The paper provides experimental insights into the current capacities and areas for improvement in multimodal LLMs, highlighting their limitations in visual reasoning tasks despite their proficiency in other areas.

## Method Summary
The study uses a challenge-response methodology where Google Bard and GPT-Vision are presented with 64 visual tasks across diverse categories. Each task involves an image that the model must analyze, interpret, or use to predict outcomes. The researchers record responses and analyze accuracy, reasoning quality, and failure patterns. The methodology focuses on understanding how these models process visual information and identify strengths and limitations in their visual reasoning capabilities.

## Key Results
- Multimodal LLMs can solve visual CAPTCHAs that challenge text-only models like ChatGPT
- Models struggle significantly with recreating visual elements like ASCII art and analyzing game boards
- Next-scene prediction tasks reveal deficiencies in object recognition and visual context understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Google Bard relies on OCR and text-based reasoning rather than deep visual object recognition for many visual tasks.
- Mechanism: When presented with an image, Bard first attempts to extract text using OCR (likely Google's Vision API). If successful, it proceeds with linguistic reasoning. If OCR fails or text is minimal, Bard defaults to educated guesses based on visual cues.
- Core assumption: OCR and text extraction are more reliable and well-trained than pure visual object recognition in Bard's architecture.
- Evidence anchors:
  - [abstract]: "Unlike other models like GPT4, Bard does not appear to rely on optical character recognition libraries like Tesseract but recognizes text in complex images like deep learning models such as Google Lens and Visual API."
  - [section]: "Unlike other models like GPT4, Bard does not appear to rely on optical character recognition libraries like Tesseract but recognizes text in complex images like deep learning models such as Google Lens and Visual API."
  - [corpus]: Weak - neighbor papers don't provide direct evidence about Bard's OCR reliance, only general multimodal evaluation approaches.
- Break condition: When presented with purely visual tasks lacking text (like ASCII art or complex puzzles), Bard fails to recognize patterns and resorts to incorrect educated guesses.

### Mechanism 2
- Claim: Bard's visual reasoning struggles with context-dependent and temporal reasoning tasks.
- Mechanism: Bard can recognize individual visual elements but fails to understand their relationships, sequences, or implications over time. This manifests in poor performance on "Next Scene Prediction" and multistep reasoning tasks.
- Core assumption: Bard's training data and architecture emphasize isolated visual recognition rather than understanding visual narratives or cause-effect relationships.
- Evidence anchors:
  - [abstract]: "the prediction problem based on visual inputs appears particularly challenging with no common-sense guesses for next-scene forecasting based on current 'next-token' multimodal models."
  - [section]: "Next scene prediction, a powerful capability if LLMs master it, also shows deficiency that points to a lack of object recognition in any broad understanding or visual context."
  - [corpus]: Weak - neighbor papers focus on general multimodal evaluation rather than specific temporal reasoning failures.
- Break condition: When asked to predict outcomes or next steps in visual sequences, Bard provides generic or incorrect responses that don't account for visual context.

### Mechanism 3
- Claim: Bard exhibits overconfidence in its visual reasoning, providing plausible-sounding but incorrect answers.
- Mechanism: When Bard cannot confidently recognize or reason about a visual element, it generates responses that sound reasonable but are factually wrong, often maintaining consistency with incorrect premises.
- Core assumption: Bard's training encourages response generation even when uncertain, prioritizing conversational flow over accuracy.
- Evidence anchors:
  - [section]: "One remarkable aspect of this generation of chatbots is when they fail, and their creators claim that the user should improve the question and context."
  - [section]: "The reference to 'OIL' on the cap seems to be language recall as it never appears clearly in the submitted image."
  - [corpus]: Weak - neighbor papers don't address confidence calibration in multimodal models.
- Break condition: When users probe Bard's reasoning with follow-up questions, inconsistencies in its responses become apparent.

## Foundational Learning

- Concept: OCR (Optical Character Recognition)
  - Why needed here: Understanding how Bard extracts text from images is crucial for predicting its behavior on text-heavy visual tasks.
  - Quick check question: What happens when OCR fails to recognize text in an image - does Bard default to pure visual reasoning or make guesses?

- Concept: Multimodal model architecture
  - Why needed here: Bard's performance depends on how text and vision components interact - understanding this helps explain its strengths and weaknesses.
  - Quick check question: Does Bard process text and images through separate transformers that are later merged, or does it use a unified multimodal embedding space?

- Concept: Visual context and object recognition
  - Why needed here: Bard's failures in visual reasoning often stem from inability to understand relationships between objects or their broader context.
  - Quick check question: How does Bard determine which visual elements are important for answering a question versus background noise?

## Architecture Onboarding

- Component map:
  - Image input module → OCR/text extraction → Text reasoning module → Response generation
  - Alternative path: Image input → Visual feature extraction → Pattern recognition → Response generation
  - Safety/filtering layer → Response refinement → Output

- Critical path: Image upload → Text extraction → Context understanding → Response generation
  - The system prioritizes text extraction and then applies language models to reason about the extracted content.

- Design tradeoffs:
  - Speed vs accuracy: Quick text extraction may miss complex visual patterns
  - Generalization vs specialization: Broad training data vs domain-specific visual reasoning
  - Confidence vs uncertainty: Generating responses even when uncertain vs admitting limitations

- Failure signatures:
  - Over-reliance on OCR when visual context would be more appropriate
  - Incorrect pattern recognition in visual puzzles
  - Overconfidence in incorrect answers
  - Failure to understand visual sequences or temporal relationships

- First 3 experiments:
  1. Upload an image with clear text and ask Bard to describe what's happening - observe if it relies on OCR or visual understanding
  2. Present a visual puzzle without text (like Sudoku) and analyze the response quality
  3. Show a sequence of images and ask for prediction of the next scene - test temporal reasoning capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific deep learning techniques does Bard use for visual text recognition, as opposed to traditional OCR tools like Tesseract?
- Basis in paper: [inferred] The paper states that Bard recognizes text in complex images using deep learning models similar to Google Lens and Visual API, unlike other models that rely on OCR libraries like Tesseract.
- Why unresolved: The paper mentions Bard's use of deep learning techniques but does not specify which techniques or models are employed.
- What evidence would resolve it: Detailed technical documentation or research papers from Google explicitly describing the visual text recognition algorithms and models used in Bard.

### Open Question 2
- Question: How does Bard's performance on visual reasoning tasks compare to other multimodal models like GPT-4 when given the same visual challenges?
- Basis in paper: [explicit] The paper compares Bard's performance to ChatGPT, noting that Bard can solve visual CAPTCHAs that stump ChatGPT, but it does not provide a direct comparison to GPT-4 on the same visual tasks.
- Why unresolved: While the paper discusses Bard's strengths and weaknesses, it does not include a side-by-side comparison with GPT-4 or other multimodal models on the specific visual reasoning tasks presented.
- What evidence would resolve it: A study directly comparing Bard and GPT-4 (or other multimodal models) on the same set of visual reasoning tasks, with quantitative results and analysis.

### Open Question 3
- Question: What are the limitations of Bard's visual reasoning capabilities in real-world applications, such as medical diagnosis or autonomous driving?
- Basis in paper: [inferred] The paper mentions that Bard struggles with analyzing Tic Tac Toe grids and predicting next scenes, suggesting limitations in visual reasoning. It also notes that Bard should be used as a supplementary tool rather than a primary diagnostic entity for medical reports.
- Why unresolved: The paper provides experimental insights into Bard's visual reasoning capabilities but does not explore its limitations in specific real-world applications or use cases.
- What evidence would resolve it: Case studies or field tests of Bard's performance in real-world applications, such as medical diagnosis or autonomous driving, highlighting its strengths, weaknesses, and potential risks.

## Limitations
- Study relies on a fixed set of 64 visual tasks, which may not comprehensively represent the full spectrum of visual reasoning challenges
- Limited comparative analysis with other multimodal models beyond ChatGPT, GPT-Vision, and Google Bard
- No detailed information on the specific evaluation methodology or scoring criteria used for task responses

## Confidence
- High Confidence: Findings about models' ability to solve visual CAPTCHAs that challenge ChatGPT alone
- Medium Confidence: Claims about Bard's reliance on OCR and text-based reasoning over deep visual recognition
- Low Confidence: Assertions about Bard's overconfidence in responses and specific failure patterns without detailed statistical backing

## Next Checks
1. Replicate the study with a larger and more diverse set of visual tasks, including domain-specific challenges and real-world scenarios
2. Conduct a comparative analysis with additional multimodal models to better contextualize Bard and GPT-Vision's performance
3. Implement human evaluation of model responses to establish baseline performance and validate automated scoring methods