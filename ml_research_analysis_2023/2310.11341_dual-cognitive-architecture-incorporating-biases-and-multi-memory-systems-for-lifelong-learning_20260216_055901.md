---
ver: rpa2
title: 'Dual Cognitive Architecture: Incorporating Biases and Multi-Memory Systems
  for Lifelong Learning'
arxiv_id: '2310.11341'
source_url: https://arxiv.org/abs/2310.11341
tags:
- learning
- duca
- dataset
- knowledge
- shape
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework called Dual Cognitive Architecture
  (DUCA) for continual learning that incorporates concepts inspired by cognitive architectures,
  high-level cognitive biases, and the multi-memory system. DUCA consists of multiple
  subsystems with dual knowledge representation, including an explicit module that
  processes standard input data and an implicit module that specializes in acquiring
  and sharing contextual knowledge indirectly.
---

# Dual Cognitive Architecture: Incorporating Biases and Multi-Memory Systems for Lifelong Learning

## Quick Facts
- arXiv ID: 2310.11341
- Source URL: https://arxiv.org/abs/2310.11341
- Authors: 
- Reference count: 40
- Primary result: DUCA achieves 50% gain over DER++ on Seq-CIFAR100 and Seq-CIFAR10 in Class-IL setting

## Executive Summary
This paper introduces Dual Cognitive Architecture (DUCA), a novel framework for continual learning that addresses catastrophic forgetting through dual knowledge representation. DUCA consists of an explicit module processing standard input data and an implicit module with specialized submodules for inductive bias and semantic memory. The architecture shows improved performance across multiple datasets while reducing task recency bias. The authors also introduce DN4IL, a challenging domain-incremental dataset to test distribution shift robustness, where DUCA demonstrates superior performance with a 50% gain over baseline methods.

## Method Summary
DUCA implements a dual cognitive architecture with explicit and implicit knowledge representation modules. The explicit module (working model) processes standard RGB input, while the implicit module contains an inductive bias learner that extracts shape information using Sobel filters and a semantic memory submodule for gradual knowledge consolidation. Knowledge sharing occurs bidirectionally between modules through alignment losses, with semantic memory updated stochastically from the working model. During training, three networks operate simultaneously, but inference uses only the semantic memory submodule, making deployment efficient.

## Key Results
- DUCA achieves 50% performance gain over DER++ on Seq-CIFAR100 and Seq-CIFAR10 in Class-IL setting
- Reduced task recency bias without requiring extra information beyond input data
- Superior performance on DN4IL domain-incremental dataset demonstrating robustness to distribution shift
- Effective balance between plasticity (learning new tasks) and stability (retaining old knowledge)

## Why This Works (Mechanism)

### Mechanism 1
Dual representation allows specialized processing of different knowledge types. The explicit module handles direct sensory input while the implicit module processes contextual knowledge through specialized submodules. Different knowledge types benefit from different computational representations, with the implicit module extracting information that isn't directly accessible in raw input.

### Mechanism 2
Shape-based inductive bias counters texture bias in standard ANNs. The inductive bias learner extracts shape information from images and shares it with the explicit module, providing global semantic context that helps overcome local texture-based learning. This addresses the tendency of ANNs to rely on local cues and textures rather than global semantic information.

### Mechanism 3
Stochastic semantic memory consolidation provides stability without sacrificing plasticity. The semantic memory submodule gradually accumulates knowledge from the working model at stochastic intervals, acting as regularization that maintains old information while allowing new learning. This mimics beneficial aspects of human memory consolidation dynamics.

## Foundational Learning

- Concept: Catastrophic forgetting in sequential learning
  - Why needed here: The paper addresses this fundamental problem by introducing mechanisms to retain old knowledge while learning new tasks
  - Quick check question: What happens to standard neural networks when trained on task A then task B without any continual learning mechanisms?

- Concept: Knowledge distillation and feature alignment
  - Why needed here: DUCA uses knowledge sharing objectives (LbiKS, LKSW M, LKSIBL) to align representations between modules, which is a form of knowledge distillation
  - Quick check question: How does mean squared error between module outputs help in knowledge sharing?

- Concept: Domain adaptation and distribution shift
  - Why needed here: The DN4IL dataset specifically tests robustness to distribution shift across domains, requiring understanding of how models handle covariate shift
  - Quick check question: Why is maintaining performance across different domains (real, clipart, sketch) more challenging than incremental class learning?

## Architecture Onboarding

- Component map: Working Model (explicit module) <- bidirectional knowledge sharing -> Inductive Bias Learner (implicit module) -> Semantic Memory (implicit module) -> stochastic consolidation from Working Model

- Critical path: During training, RGB data flows through working model, shape data flows through IBL, both modules share knowledge bidirectionally, semantic memory stochastically consolidates from working model; during inference, only semantic memory is used

- Design tradeoffs: Complexity vs performance (three networks during training vs one during inference), shape extraction quality impacts inductive bias effectiveness, update rate r balancing stability vs plasticity

- Failure signatures: Working model performance degrades rapidly on new tasks (plasticity too low), working model forgets old tasks quickly (stability too low), overall performance plateaus below baseline (knowledge sharing ineffective), semantic memory update causes catastrophic forgetting (update rate too high)

- First 3 experiments: 1) Train DUCA on Seq-CIFAR10 with small buffer size, compare working model vs semantic memory performance on task-wise accuracy to verify plasticity-stability balance, 2) Remove inductive bias learner (train only on RGB) and measure performance drop to quantify shape information contribution, 3) Vary stochastic update rate r and observe stability-plasticity tradeoff curve on DN4IL dataset

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal way to select and integrate inductive biases for different modalities beyond vision (e.g., audio, text)? The paper mentions extending DUCA to other domains like audio using spectrograms and phonemes as inductive biases, but lacks concrete methods or experiments. Conducting experiments with DUCA on different modalities and comparing inductive bias selection methods would provide evidence to resolve this question.

### Open Question 2
How can the knowledge sharing mechanisms between modules in DUCA be further improved to enhance continual learning performance? While the paper demonstrates benefits of knowledge sharing, the specific mechanisms and their optimization remain unexplored. Further research is needed to understand how to improve these mechanisms for better continual learning performance.

### Open Question 3
How can DUCA be adapted to handle more complex and realistic continual learning scenarios, such as those with noisy or incomplete data, or with a large number of tasks? The paper's experiments are limited to relatively controlled settings, and it is unclear how DUCA would perform in more challenging real-world scenarios. Further research is needed to understand the limitations of DUCA and how it can be adapted to handle more complex continual learning tasks.

## Limitations

- The paper lacks specific hyperparameter values for learning rates, batch sizes, and update frequencies, making exact reproduction challenging
- The DN4IL dataset has limited domain diversity (6 domains with 20 classes each) compared to real-world complexity
- Shape extraction quality depends heavily on the Sobel filter implementation, which may not capture all relevant shape information

## Confidence

- **High Confidence**: The dual representation architecture design and its core components (explicit/implicit modules, knowledge sharing mechanisms) are well-defined and theoretically sound
- **Medium Confidence**: The effectiveness of shape-based inductive bias in improving generalization across domains, as this depends on dataset-specific characteristics
- **Low Confidence**: The scalability of DUCA to very large-scale datasets and real-world scenarios with continuous data streams

## Next Checks

1. Conduct ablation studies varying the stochastic update rate r across multiple orders of magnitude to identify optimal stability-plasticity balance
2. Test DUCA on datasets with more diverse domains (e.g., DomainNet with 345 classes across 6 domains) to validate scalability of domain adaptation claims
3. Measure the computational overhead of maintaining three networks during training and one during inference to assess practical deployment viability