---
ver: rpa2
title: 'Functional Neural Networks: Shift invariant models for functional data with
  applications to EEG classification'
arxiv_id: '2301.05869'
source_url: https://arxiv.org/abs/2301.05869
tags:
- data
- functional
- neural
- class
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces functional neural networks (FNNs), a new
  class of neural networks that are shift invariant and preserve smoothness of functional
  data. The authors extend multi-layer perceptrons and convolutional neural networks
  to functional data using methods from functional data analysis (FDA).
---

# Functional Neural Networks: Shift invariant models for functional data with applications to EEG classification

## Quick Facts
- arXiv ID: 2301.05869
- Source URL: https://arxiv.org/abs/2301.05869
- Reference count: 40
- Key outcome: FNNs achieve over 99.6% accuracy on simulated data and outperform EEGNet by 1.97% to 4.69% accuracy on EEG classification

## Executive Summary
This paper introduces functional neural networks (FNNs), a new class of neural networks that are shift invariant and preserve smoothness of functional data. The authors extend multi-layer perceptrons and convolutional neural networks to functional data using methods from functional data analysis (FDA). They propose different FNN architectures and demonstrate that these models outperform a benchmark FDA model on simulated data and achieve state-of-the-art results on EEG classification tasks.

## Method Summary
The authors propose FNNs that use FDA methods to extend traditional neural network architectures to functional data. The key innovation is functional convolutional layers that achieve shift invariance through convolutional kernels that slide across the functional domain. The models use local linear estimation for preprocessing, standardization layers, functional convolutional layers with Legendre polynomial basis functions, and functional dense layers. The framework is applied to both simulated data and real EEG recordings, demonstrating superior performance compared to existing methods.

## Key Results
- FNNs achieve over 99.6% accuracy on simulated datasets while benchmark FDA models range from 76.9% to 99.3%
- On EEG classification, FNNs outperform EEGNet by 1.97% to 4.69% accuracy with similar parameter counts
- The models demonstrate successful shift invariance on non-aligned functional data
- Different FNN architectures are validated, showing consistent improvements over baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Functional convolutional layers achieve shift invariance by using convolutional kernels that slide across the functional domain.
- Mechanism: The kernel function u(j,k)(s-t) computes the convolution integral ∫ u(j,k)(s-t)H^(ℓ-1)(j)(t) dt. Since convolution is translation-invariant, the same kernel can detect a signal regardless of its position in the interval [0,1].
- Core assumption: The underlying signal of interest is smooth and can be represented as a convolution with a learnable kernel.
- Evidence anchors:
  - [abstract] "functional neural networks (FNNs). For this, we use methods from functional data analysis (FDA) to extend multi-layer perceptrons and convolutional neural networks to functional data."
  - [section] "These functional convolutional layers are shift invariant in the sense that a filter, which is capable of detecting a certain signal, would detect it independently of its position in the interval [0, 1]."

### Mechanism 2
- Claim: Local linear estimation provides smooth functional representations that preserve derivative information.
- Mechanism: The local polynomial estimator (ˆf(x), ˆf'(x), ..., ˆf^(p)(x)) = arg min β0,...,βp Σ (Xt - Σ βj((t/T-x)/j)² Kh((t/T-x)/h) estimates the function and its derivatives by minimizing a weighted least squares criterion. This creates smooth functional inputs for the neural network layers.
- Core assumption: The underlying physical process generating the data is smooth enough that local polynomial approximation is valid.
- Evidence anchors:
  - [section] "We use local polynomial regression to estimate the functions f^(n)_i and their first derivative(s)"
  - [section] "If the data is generated by some smooth process, this additional structure should be taken into account."

### Mechanism 3
- Claim: Functional layers with linear combinations of base functions reduce dimensionality while preserving expressiveness.
- Mechanism: Weight functions w^(ℓ)(j,k)(t) = Σ w^(ℓ,i)(j,k)ϕ_i(t) represent the layer weights as linear combinations of basis functions (Legendre polynomials, Fourier basis, etc.). This reduces the infinite-dimensional optimization problem to a finite-dimensional one with q+1 parameters per weight function.
- Core assumption: The true functional relationships can be well-approximated by linear combinations of the chosen basis functions.
- Evidence anchors:
  - [section] "An efficient approach to simplify computations and simultaneously reduce the dimension of the weights' space, is to replace the weights w^(ℓ)(j,k)(t) by linear combinations of a finite set of base functions."
  - [section] "With this representation, the fully functional neural network can be described through scalar weights and we are able to use the standard scalar backpropagation."

## Foundational Learning

- Concept: Functional Data Analysis (FDA) and the L²([0,1]) space
  - Why needed here: The paper treats time series data as samples from underlying smooth functions in L²([0,1]), which is the mathematical foundation for extending neural networks to functional data.
  - Quick check question: What is the key difference between treating data as discrete samples versus treating it as discretized samples of underlying functions?

- Concept: Shift invariance in convolutional neural networks
  - Why needed here: Understanding how convolution achieves translation invariance in standard CNNs is crucial for grasping how functional convolutional layers extend this property to functional data.
  - Quick check question: Why does the convolution operation naturally provide shift invariance for signal detection?

- Concept: Local polynomial regression and bandwidth selection
  - Why needed here: The preprocessing step uses local linear estimation to smooth data and estimate derivatives, which is essential for creating functional inputs that preserve signal structure.
  - Quick check question: How does the choice of bandwidth affect the bias-variance tradeoff in local polynomial estimation?

## Architecture Onboarding

- Component map: Input layer (local linear estimator) -> Standardization -> Functional convolutional layer -> Functional convolutional layer -> Functional dense layer -> Output layer
- Critical path: The data flows from raw functional input → local linear estimation → standardization → functional convolution → functional convolution → functional dense → output. Each step is essential for the shift-invariant functional processing.
- Design tradeoffs:
  - More convolutional layers increase shift invariance and feature hierarchy but add parameters and training complexity
  - Larger kernel bandwidths capture broader patterns but may miss fine-grained features
  - More basis functions increase expressiveness but also increase parameter count and risk overfitting
- Failure signatures:
  - Poor performance on non-smooth signals (violates smoothness assumption)
  - Overfitting when too many basis functions are used relative to data size
  - Shift-invariance failures when kernel bandwidth is too small to capture the signal pattern
- First 3 experiments:
  1. Train a simple FNN on the simulated dataset (I) with 2 convolutional layers and 1 dense layer, varying the number of Legendre polynomial basis functions to find optimal expressiveness.
  2. Compare FNN performance on dataset (II) versus standard CNN to verify the shift invariance advantage on non-aligned signals.
  3. Test different kernel bandwidths in the local linear estimation step to optimize the bias-variance tradeoff for the EEG classification task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do functional neural networks (FNNs) perform on other types of functional data beyond EEG and simulated data, such as stock prices or temperature curves?
- Basis in paper: [inferred] The paper suggests that FNNs could be applied to other types of functional data like stock prices or temperature curves, but does not provide empirical results.
- Why unresolved: The paper focuses on demonstrating the effectiveness of FNNs on EEG data and simulated data, leaving the question of their performance on other types of functional data unanswered.
- What evidence would resolve it: Empirical results comparing the performance of FNNs on various types of functional data, such as stock prices or temperature curves, to other state-of-the-art methods.

### Open Question 2
- Question: What is the optimal choice of base functions for the weight functions in FNNs, and how does it affect the model's performance?
- Basis in paper: [explicit] The paper mentions that the choice of base functions is crucial for the performance of the network and that prior simulation studies showed good results with the Fourier base and Legendre polynomials, but it does not provide a definitive answer.
- Why unresolved: The paper does not conduct a comprehensive study on the impact of different base functions on the performance of FNNs, leaving the optimal choice unclear.
- What evidence would resolve it: A systematic comparison of the performance of FNNs using different base functions on various datasets, identifying the most effective choices for different scenarios.

### Open Question 3
- Question: Can the proposed FNN framework be extended to other types of neural networks, such as recurrent neural networks or transformers, and how would their performance compare to FNNs?
- Basis in paper: [explicit] The paper suggests that the framework could be extended to other types of neural networks but does not provide any empirical results or comparisons.
- Why unresolved: The paper does not explore the potential of applying the FNN framework to other neural network architectures, leaving the question of their performance unanswered.
- What evidence would resolve it: Empirical results comparing the performance of FNNs to other neural network architectures, such as recurrent neural networks or transformers, on the same datasets used in the paper.

## Limitations
- The theoretical analysis is limited, with only brief discussion of the mathematical foundations
- The framework assumes smoothness of underlying functional data, which may not hold for all real-world applications
- No comprehensive comparison with other functional data analysis methods like functional PCA or functional linear models

## Confidence
- **High confidence** in the simulation results and accuracy comparisons
- **Medium confidence** in the generalizability to other functional data types
- **Low confidence** in the theoretical guarantees of shift invariance and smoothness preservation

## Next Checks
1. Test FNNs on non-smooth functional data to verify robustness beyond the smoothness assumption
2. Compare FNN performance against other functional data analysis methods (e.g., functional PCA, functional linear models) on multiple benchmark datasets
3. Conduct ablation studies to isolate the contributions of shift invariance, smoothness preservation, and architectural choices to overall performance