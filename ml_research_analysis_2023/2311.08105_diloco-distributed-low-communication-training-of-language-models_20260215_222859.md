---
ver: rpa2
title: 'DiLoCo: Distributed Low-Communication Training of Language Models'
arxiv_id: '2311.08105'
source_url: https://arxiv.org/abs/2311.08105
tags:
- diloco
- training
- outer
- steps
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiLoCo introduces a distributed training algorithm enabling language
  models to be trained across poorly connected islands of devices. The method builds
  on federated averaging but uses AdamW as the inner optimizer, Nesterov momentum
  as the outer optimizer, and performs hundreds of local steps before communicating.
---

# DiLoCo: Distributed Low-Communication Training of Language Models

## Quick Facts
- **arXiv ID**: 2311.08105
- **Source URL**: https://arxiv.org/abs/2311.08105
- **Reference count**: 17
- **Primary result**: Achieves 15.02 perplexity on C4 with 8 workers, matching or outperforming fully synchronous training while communicating 500Ã— less.

## Executive Summary
DiLoCo introduces a distributed training algorithm that enables language models to be trained across poorly connected devices by dramatically reducing communication frequency. Building on federated averaging, it uses AdamW as the inner optimizer, Nesterov momentum as the outer optimizer, and performs hundreds of local steps before communicating. This approach achieves the same or better perplexity than fully synchronous training while reducing communication by 500Ã—, making it practical for training LLMs across heterogeneous devices with limited connectivity.

## Method Summary
DiLoCo is a distributed training algorithm that extends federated averaging by using AdamW as the inner optimizer, Nesterov momentum as the outer optimizer, and performing hundreds of local steps (H=500) before communicating. Each worker trains independently using its local data shard with AdamW, then synchronizes only model parameters with a coordinator after H steps. The coordinator averages the parameters and applies Nesterov momentum updates before redistributing the new global parameters. This design allows workers to operate independently for long periods, reducing communication frequency while maintaining good generalization through the linear mode connectivity property.

## Key Results
- Achieves 15.02 perplexity on C4 with 8 workers, matching or outperforming fully synchronous training
- Reduces communication by 500Ã— compared to fully synchronous training
- Performance improves with more workers up to 8, with diminishing returns beyond that point
- Maintains robustness to non-i.i.d. data distributions and worker failures
- Generalizes across model sizes and can accelerate single-worker training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DiLoCo achieves good generalization by leveraging linear mode connectivity over long inner steps (H=500).
- **Mechanism:** With large H, local worker models follow different optimization trajectories but stay in the same or nearby loss basins. Averaging their outer gradients at the end of each phase yields a point with low loss due to the linear connectivity property.
- **Core assumption:** Large H (hundreds of steps) keeps worker models in the same or nearby basins, so their parameter space interpolation remains loss-free.
- **Evidence anchors:** [abstract] "DiLoCo builds on federated averaging but uses AdamW as the inner optimizer, Nesterov momentum as the outer optimizer, and performs hundreds of local steps before communicating." [section 3.1] "we observe that communicating more frequently improves generalization performance. However, communicating more frequently than ð» = 500 steps leads to diminishing returns."
- **Break condition:** If H is too large, worker models diverge into separate basins, breaking the interpolation property and hurting generalization.

### Mechanism 2
- **Claim:** Nesterov momentum in the outer optimizer improves robustness to non-i.i.d. data by correcting gradients over many steps.
- **Mechanism:** Outer Nesterov momentum incorporates a look-ahead correction term that compensates for gradient noise introduced by heterogeneous shards, stabilizing the averaged update.
- **Core assumption:** Outer Nesterov momentum can correct accumulated noise from non-i.i.d. shards better than plain SGD or Adam.
- **Evidence anchors:** [abstract] "the outer optimizer is Nesterov momentum for best performance." [section 3.1] "We found Nesterov optimizer... to perform the best... we hypothesize that the Nesterovâ€™s gradient correction is particularly helpful with the outer gradient that span hundred of training steps."
- **Break condition:** If outer optimizer loses momentum tuning or is replaced with unstable optimizers (e.g., Adam with high beta2), convergence degrades.

### Mechanism 3
- **Claim:** Keeping AdamW states local to each worker reduces communication while maintaining effective inner optimization.
- **Mechanism:** AdamW requires first and second moment estimates; synchronizing these would triple communication cost with negligible gain, so DiLoCo keeps them local and only syncs parameters.
- **Core assumption:** Inner optimizer states can be kept local without harming convergence if outer updates are infrequent and properly tuned.
- **Evidence anchors:** [section 6.1] "Each replica in our method has a separate Adam state... DiLoCo synchronizes the parameters of the model, but we also considered synchronizing the inner optimizer states. It did not lead to significant improvements while significantly increasing the communication cost."
- **Break condition:** If H is too small or data is highly non-i.i.d., stale Adam states may slow convergence.

## Foundational Learning

- **Concept:** Linear mode connectivity (interpolation of models without loss increase).
  - Why needed here: Explains why averaging models after many local steps still yields a good solution.
  - Quick check question: If you linearly interpolate between two models trained on different shards, under what condition does the loss stay low?
- **Concept:** Federated averaging and local SGD dynamics.
  - Why needed here: DiLoCo is a variant of FedAvg with large H; understanding local SGD convergence is essential.
  - Quick check question: What happens to convergence when H increases from 1 to 500 in local SGD?
- **Concept:** Momentum-based outer optimizers vs. plain SGD.
  - Why needed here: Nesterov momentum is used in outer optimizer; knowing its bias-variance trade-off is critical.
  - Quick check question: How does Nesterov momentumâ€™s look-ahead correction differ from standard momentum in terms of gradient stability?

## Architecture Onboarding

- **Component map:** Workers -> Coordinator -> Workers
- **Critical path:**
  1. Workers compute H inner steps â†’ local parameters
  2. Workers send parameters â†’ coordinator
  3. Coordinator averages â†’ Nesterov update â†’ new global parameters
  4. Parameters broadcast â†’ repeat
- **Design tradeoffs:**
  - H large â†’ less comm, but risk of basin divergence
  - No inner state sync â†’ faster comm, possible stale moments
  - Nesterov outer â†’ more robust to non-i.i.d., needs momentum tuning
- **Failure signatures:**
  - High perplexity spikes: Likely H too large or momentum too low
  - Slow convergence: Outer learning rate too small or Nesterov momentum too aggressive
  - Comm bottleneck: Try pruning outer gradients (up to 50% works)
- **First 3 experiments:**
  1. Run DiLoCo with H=100 on a small model to verify comm pattern and loss trend
  2. Vary H=50, 500, 1000 on same setup to find sweet spot
  3. Test Nesterov vs. SGD vs. Adam outer optimizers to confirm stability

## Open Questions the Paper Calls Out

- **Open Question 1:** Does DiLoCo scale to trillion-parameter models?
  - Basis in paper: [inferred] from "it would be interesting to see how DiLoCo works at larger scale" and "at the time of writing state-of-the-art language models use 3 orders of magnitude more parameters"
  - Why unresolved: The paper only validates DiLoCo on models up to 400M parameters, and extrapolates that it may perform better at larger scales but hasn't empirically tested this.
  - What evidence would resolve it: Training a trillion-parameter model using DiLoCo and comparing its performance and communication efficiency to synchronous training.

- **Open Question 2:** How does DiLoCo perform with heterogeneous worker speeds?
  - Basis in paper: [explicit] from "DiLoCo assumes that all workers are homogeneous. However, in practice workers might operate at wildly different speed"
  - Why unresolved: The paper only considers homogeneous workers and doesn't explore asynchronous variants or how speed differences affect convergence.
  - What evidence would resolve it: Implementing an asynchronous version of DiLoCo and measuring performance degradation when workers have different compute speeds.

- **Open Question 3:** Can DiLoCo balance wall-clock time efficiency with compute and data efficiency?
  - Basis in paper: [explicit] from "Another avenue of future research is on balancing wall-clock time efficiency with compute efficiency and data efficiency"
  - Why unresolved: The paper shows DiLoCo is fast in wall-clock time but uses more compute than alternatives like 8Ã— updates, without addressing this trade-off.
  - What evidence would resolve it: Developing metrics that jointly optimize wall-clock time, compute efficiency (FLOPs), and data efficiency, then evaluating DiLoCo against baselines using these metrics.

## Limitations
- The linear mode connectivity assumption for large H values is inferred rather than empirically validated
- Analysis focuses primarily on perplexity as the sole generalization metric, with no examination of downstream task performance
- Assumes homogeneous worker speeds, which may not hold in practical distributed settings

## Confidence

- **High confidence**: DiLoCo achieves stated perplexity results on C4 with reduced communication (directly measured and reported)
- **Medium confidence**: The generalization advantage comes from linear mode connectivity over long local steps (inferred from results, not directly tested)
- **Medium confidence**: Nesterov momentum in outer optimizer improves non-i.i.d. robustness (supported by ablation, but mechanism unclear)
- **Low confidence**: DiLoCo's approach generalizes to all LLM architectures and scales without modification (extrapolation from limited experiments)

## Next Checks

1. **Linear mode connectivity validation**: Measure actual loss along interpolation paths between worker models trained for different H values to confirm the basin connectivity assumption.
2. **Optimizer comparison**: Systematically compare Nesterov momentum against other momentum variants (SGD momentum, AdamW) and learning rate schedules in the outer optimizer to isolate the source of non-i.i.d. robustness.
3. **Downstream task generalization**: Evaluate trained models on GLUE, SuperGLUE, or other standard NLP benchmarks to verify that perplexity improvements translate to practical generalization benefits beyond the training domain.