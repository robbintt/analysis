---
ver: rpa2
title: Gradient-Free Structured Pruning with Unlabeled Data
arxiv_id: '2303.04185'
source_url: https://arxiv.org/abs/2303.04185
tags:
- data
- pruning
- arxiv
- accuracy
- structured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a gradient-free structured pruning framework
  for large language models (LLMs) that does not require labeled data or retraining.
  The method uses only the trained model weights and unlabeled data to rank and prune
  filters in the model.
---

# Gradient-Free Structured Pruning with Unlabeled Data

## Quick Facts
- arXiv ID: 2303.04185
- Source URL: https://arxiv.org/abs/2303.04185
- Authors: 
- Reference count: 39
- Primary result: Reduces up to 40% of original FLOP count with less than 4% accuracy loss on BERTBASE and DistilBERT using only unlabeled data

## Executive Summary
This paper proposes R2D2, a gradient-free structured pruning framework for large language models that operates without labeled data or retraining. The method combines two ranking techniques: Representative Ranking (R2) using model weights and Data-Driven (D2) ranking using layer-wise output statistics from unlabeled data. Experiments on GLUE and SQuAD benchmarks demonstrate that R2D2 can achieve up to 40% FLOP reduction while maintaining less than 4% accuracy loss, outperforming pruning methods that require labeled data.

## Method Summary
The method works by first using R2 to rank filters based on how well each filter can be represented by others in weight space, treating filters as data points in a high-dimensional space. D2 then uses layer-wise output statistics computed from unlabeled data to provide an additional ranking. These two rankings are merged and the top-k filters are selected based on a FLOPs constraint. After pruning, a scaling transformation is applied to the remaining active filters using only the unlabeled data to reconstruct layer-wise outputs and recover accuracy loss. The framework dynamically decides how many neurons to prune from each layer based on the merged ranking.

## Key Results
- Achieves up to 40% FLOP reduction on BERTBASE and DistilBERT models
- Maintains less than 4% accuracy loss across GLUE and SQuAD benchmarks
- Outperforms pruning methods requiring labeled data
- Shows dynamic layer-wise pruning (KCM) is more effective than uniform pruning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: R2D2 combines model weights and unlabeled data to rank filters without labeled data or retraining
- Mechanism: R2 uses model weights to rank filters as a representative selection problem in high-dimensional space, while D2 uses layer-wise output statistics from unlabeled data. Merging these rankings identifies important filters
- Core assumption: Filters that are well-represented by others in weight space are less important, and layer-wise output statistics correlate with filter importance
- Evidence anchors:
  - [abstract] "R2 uses the model weights to rank filters, while D2 uses layer-wise output statistics from unlabeled data"
  - [section 3.2.1] "R2 considers the filters of a Feed-Forward Network (FFN) in the trained model as data points in a high-dimensional space, and ranks these by how well a filter can be represented by others"
- Break condition: If filter weights don't correlate with importance or if layer-wise outputs don't reflect filter contribution, the ranking would fail

### Mechanism 2
- Claim: The scaling transformation applied after masking recovers accuracy loss without labeled data
- Mechanism: Scaling uses unlabeled data to reconstruct layer-wise outputs by scaling the outputs of active filters, reducing feature map loss
- Core assumption: Scaling active filters can approximate the original layer-wise outputs without requiring labeled data
- Evidence anchors:
  - [abstract] "we apply an existing scaling transformation method (Kwon et al., 2022) to mitigate the effect of their removal"
  - [section 3.2.3] "we apply a scaling transformation to the selected filters. Such scaling uses only the unlabeled data and, based on the generated mask, aims to reconstruct the layer-wise outputs by scaling the outputs of the active filters"
- Break condition: If scaling cannot approximate original outputs well, accuracy would degrade significantly

### Mechanism 3
- Claim: Dynamic neuron selection across layers optimizes the pruning for each task
- Mechanism: KCM merges R2D2 rankings across all layers and selects top-k filters based on FLOPs constraint, allowing different layers to be pruned differently
- Core assumption: Different layers have different importance for different tasks, and this can be captured by the merged ranking
- Evidence anchors:
  - [section 4.3] "Another important feature of our KCM is the fact that it dynamically decides how many neurons from each layer to prune"
  - [section 4.3] "Figure 4 illustrates how KCM affects different layers of the BERTBASE. Clearly more pruning occurs over the last three layers, and more than half of the filters in the first two layers are pruned"
- Break condition: If all layers require similar numbers of filters for all tasks, dynamic selection provides no benefit

## Foundational Learning

- Concept: Structured pruning vs unstructured pruning
  - Why needed here: The paper focuses on structured pruning (removing groups of parameters) which maintains hardware compatibility while unstructured pruning may not offer significant inference speedup
  - Quick check question: What's the key difference between structured and unstructured pruning in terms of hardware utilization?

- Concept: Representative selection problem in computational geometry
  - Why needed here: R2 translates structured pruning into a representative selection problem where the goal is to find a subset of filters that can well represent the full set
  - Quick check question: How does viewing filters as data points in high-dimensional space help with the pruning problem?

- Concept: Feature map loss as optimization objective
  - Why needed here: Without labeled data, the method minimizes feature map loss between original and pruned model outputs instead of supervised loss
  - Quick check question: Why can't we use supervised loss when we don't have labeled data, and what's the alternative?

## Architecture Onboarding

- Component map: Trained model weights + unlabeled data + FLOPs constraint → R2D2 ranking (R2 + D2) → filter importance scores → Merge rankings → select top-k filters → Scaling transformation → Pruned model
- Critical path: Model weights → R2 ranking → D2 ranking → Merge → Filter selection → Scaling → Pruned model
- Design tradeoffs:
  - Accuracy vs FLOPs: More aggressive pruning reduces FLOPs but may hurt accuracy
  - R2 vs D2 weighting: Different combinations affect which filters are selected
  - Kernel width σ and convergence rate α in R2 affect ranking quality
- Failure signatures:
  - Poor accuracy after pruning: Check if R2D2 rankings are meaningful or if scaling is ineffective
  - High variance across runs: Check seed stability and ranking consistency
  - Slow convergence: Check kernel parameters and unlabeled data quality
- First 3 experiments:
  1. Run KCM with only R2 ranking (no D2) to verify R2's contribution
  2. Run with only D2 ranking (no R2) to verify D2's contribution
  3. Vary unlabeled data sample size to find minimum needed for good performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed R2D2 ranking method compare to other unsupervised pruning techniques that don't use labeled data or gradients?
- Basis in paper: [explicit] The paper mentions that R2D2 combines Representative Ranking (R2) and Data-Driven (D2) to estimate the importance of filters without labeled data or retraining. It also compares to other pruning methods that use labeled data or retraining
- Why unresolved: The paper only compares KCM (which uses R2D2) to methods that require labeled data or retraining. It doesn't compare to other unsupervised pruning techniques that don't use gradients or labeled data
- What evidence would resolve it: A thorough comparison of R2D2 to other unsupervised pruning methods like convex post-processing (Aghasi et al., 2020), self-representation based pruning (You et al., 2020), or clustering-based methods (Browne et al., 2020, 2021)

### Open Question 2
- Question: How sensitive is the R2D2 ranking method to the choice of hyperparameters like the width of the Gaussian kernel (σ) and convergence rate (α)?
- Basis in paper: [explicit] The paper mentions that σ = 1.0 and α = 0.01 work for all tasks considered, and on average it takes less than 20 iterations to converge. However, it doesn't explore the sensitivity to these hyperparameters
- Why unresolved: The paper doesn't provide an ablation study on how different values of σ and α affect the pruning performance. It's unclear if the chosen values are optimal or if the method is robust to variations in these hyperparameters
- What evidence would resolve it: An ablation study showing the pruning performance (accuracy and FLOPs reduction) for different values of σ and α. This would reveal if the method is sensitive to these hyperparameters and what the optimal values might be

### Open Question 3
- Question: Can the R2D2 ranking method be extended to prune other components of the transformer model beyond the FFN layers?
- Basis in paper: [explicit] The paper focuses on pruning the filters of the FFN layers in BERT, as they have a huge impact on model size and inference latency. It doesn't explore pruning other components like attention heads or embedding layers
- Why unresolved: The paper doesn't investigate if the R2D2 ranking method can be adapted to prune other components of the transformer model. It's unclear if the method would be effective for pruning attention heads or embedding layers, which are also important for model efficiency
- What evidence would resolve it: An extension of the R2D2 method to prune attention heads or embedding layers, followed by an evaluation of the pruning performance on the same benchmarks used in the paper. This would show if the method is generalizable to other components of the transformer model

## Limitations

- Limited evaluation to BERTBASE and DistilBERT architectures, with unclear generalizability to larger or different LLM architectures
- No detailed implementation specifications for the critical scaling transformation component
- Potential sensitivity to hyperparameter choices (σ, α) without systematic ablation studies

## Confidence

**High Confidence Claims:**
- The R2D2 ranking framework can reduce FLOPs by 40% with <4% accuracy loss on BERTBASE/DistilBERT models
- The combination of R2 (weight-based) and D2 (data-based) rankings improves over either method alone
- Dynamic layer-wise pruning (KCM) outperforms uniform pruning across layers

**Medium Confidence Claims:**
- The method generalizes to various downstream tasks (GLUE, SQuAD)
- Unlabeled data alone is sufficient for meaningful filter ranking
- Scaling transformation effectively recovers accuracy without labeled data

**Low Confidence Claims:**
- Performance would scale similarly to larger LLM architectures (GPT-3, LLaMA)
- The method works equally well with minimal unlabeled data (2K samples)
- R2D2 rankings would remain stable across different random seeds

## Next Checks

1. **Scaling Transformation Verification**: Implement and validate the exact scaling transformation procedure from Kwon et al., 2022, including all hyperparameters and convergence criteria, to confirm the accuracy recovery claims

2. **Architecture Generalization Test**: Apply the R2D2 framework to a GPT-style architecture (e.g., OPT or LLaMA) and compare FLOPs reduction vs accuracy trade-offs to establish broader applicability beyond BERT-family models

3. **Unlabeled Data Sensitivity Analysis**: Systematically vary the amount of unlabeled data (from 100 to 10,000 samples) to determine the minimum data requirement for maintaining the <4% accuracy loss threshold at 40% FLOPs reduction