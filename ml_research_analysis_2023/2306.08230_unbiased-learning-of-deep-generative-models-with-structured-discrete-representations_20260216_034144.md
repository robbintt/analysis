---
ver: rpa2
title: Unbiased Learning of Deep Generative Models with Structured Discrete Representations
arxiv_id: '2306.08230'
source_url: https://arxiv.org/abs/2306.08230
tags:
- parameters
- which
- data
- inference
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes optimization techniques for training structured
  variational autoencoders (SVAEs) to overcome challenges posed by their expressivity.
  The authors introduce an implicit differentiation scheme that enables memory-efficient
  and robust training, even with incomplete optimization.
---

# Unbiased Learning of Deep Generative Models with Structured Discrete Representations

## Quick Facts
- arXiv ID: 2306.08230
- Source URL: https://arxiv.org/abs/2306.08230
- Reference count: 40
- Key outcome: Introduces optimization techniques for SVAEs including implicit differentiation and unbiased natural gradients, enabling competitive performance with interpretable discrete representations on time series data

## Executive Summary
This paper addresses fundamental challenges in training structured variational autoencoders (SVAEs) by introducing memory-efficient implicit differentiation and unbiased natural gradient methods. The authors demonstrate that these innovations enable SVAEs to handle multimodal uncertainty when data is missing by incorporating discrete latent variables, while maintaining computational tractability. The proposed methods allow for the first comprehensive comparisons of SVAEs against state-of-the-art time series models, where SVAEs perform competitively while learning interpretable and structured discrete data representations.

## Method Summary
The paper proposes two key optimization techniques for SVAEs: implicit differentiation via the implicit function theorem to avoid memory-intensive backpropagation through iterative inference, and unbiased natural gradient updates computed through automatic differentiation in unconstrained parameter space. The SVAE framework uses structured variational inference with conjugate exponential family distributions, where the generative model follows a graphical model prior (such as switching linear dynamical systems) and the recognition network outputs natural parameters for surrogate likelihoods. The method leverages parallel inference for temporal models and demonstrates robustness to incomplete optimization while enabling seamless handling of discrete latent variables without biased relaxations.

## Key Results
- SVAEs with the proposed optimization techniques achieve competitive performance against state-of-the-art time series models on motion capture and audio spectrogram datasets
- The structured mean field approximation enables handling of multimodal uncertainty when data is missing, outperforming methods requiring continuous relaxations
- Memory-efficient implicit differentiation reduces computational overhead by 10-100x while maintaining training stability and convergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Implicit differentiation makes SVAE training tractable by avoiding memory-intensive backpropagation through iterative inference
- Mechanism: Uses the implicit function theorem to compute gradients via solving a linear system using Richardson iterations, requiring only endpoint evaluations
- Core assumption: Stationary conditions of block updating routines can be approximated numerically without full convergence
- Evidence anchors: [abstract] "memory-efficient implicit differentiation scheme makes the SVAE tractable to learn via gradient descent" [section] "We instead apply the implicit function theorem (IFT [28]) to compute implicit gradients ∂ω/∂η, ∂ω/∂ϕ without storing intermediate states"
- Break condition: Forward pass requires many iterations to converge or linear system becomes ill-conditioned

### Mechanism 2
- Claim: Unbiased natural gradients accelerate learning by properly scaling parameter updates
- Mechanism: Computes Fisher-scaled updates automatically via forward-mode differentiation without manual derivations in unconstrained parameter space
- Core assumption: Fisher information matrix accurately captures geometry of parameter space for exponential family distributions
- Evidence anchors: [abstract] "derive a method for computing natural gradients without manual derivations, which avoids biases found in prior work" [section] "derive unbiased natural gradient updates that are easily and efficiently implemented for any SVAE model via automatic differentiation"
- Break condition: Reparameterization introduces singularities or Fisher information matrix becomes singular

### Mechanism 3
- Claim: Structured VI enables seamless handling of discrete latent variables without biased relaxations
- Mechanism: Factorizes posterior and only requires continuous samples for reconstruction, marginalizing discrete variables via structured VI
- Core assumption: Structured mean field approximation adequately captures dependencies between discrete and continuous variables
- Evidence anchors: [abstract] "SVAEs can utilize them seamlessly" [section] "SVAE training only requires reparameterized samples of those latent variables which are direct inputs to the generative network"
- Break condition: Discrete-continuous dependencies are too strong or non-linear

## Foundational Learning

- Concept: Exponential family distributions and their natural parameters
  - Why needed here: SVAEs rely on conjugacy between priors and likelihoods for tractable inference; understanding natural parameters is essential for computing gradients
  - Quick check question: Given a Gaussian distribution N(μ,Σ), what are its natural parameters in terms of μ and Σ?

- Concept: Variational inference and the evidence lower bound (ELBO)
  - Why needed here: SVAEs optimize the ELBO; understanding how structured VI modifies ELBO for graphical models is crucial
  - Quick check question: How does the surrogate ELBO in SVAEs differ from the standard ELBO in terms of the likelihood term?

- Concept: Belief propagation and structured mean field approximation
  - Why needed here: SVAEs use structured VI to handle complex graphical model structure; understanding BP and mean field updates is necessary
  - Quick check question: In a switching LDS, how do the mean field updates for discrete states differ from those for continuous states?

## Architecture Onboarding

- Component map: x → encoder → λϕ(x) → structured VI → q(z; ω) → reconstruction → loss → gradients → parameter update
- Critical path: Input data flows through encoder to obtain natural parameters, structured VI computes posterior, reconstruction generates output, loss computes gradients for parameter update
- Design tradeoffs:
  - Memory vs. speed: Unrolled gradients are more accurate but memory-intensive; implicit gradients trade accuracy for scalability
  - Expressivity vs. tractability: Structured mean field enables discrete variables but introduces approximation error
  - Automatic vs. manual: Unbiased natural gradients avoid manual derivations but require careful reparameterization
- Failure signatures:
  - Training diverges: Check gradient conditioning, learning rate, and numerical stability of implicit differentiation
  - Poor reconstructions: Verify encoder outputs, structured VI convergence, and conjugate likelihood choice
  - Collapsed discrete states: Inspect initialization, mean field updates, and KL regularization
- First 3 experiments:
  1. Train basic LDS SVAE on synthetic linear data to verify inference and reconstruction
  2. Add discrete switching states and test structured VI with missing data imputation
  3. Scale to real temporal data (e.g., motion capture) and compare against baselines using FID scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the implicit differentiation scheme be extended to other SVAE models beyond SLDS?
- Basis in paper: [explicit] Authors mention innovations "leverage automatic differentiation for broad applicability" and enable learning of SVAEs with "rich, non-temporal graphical structure in other domains"
- Why unresolved: Paper focuses on SLDS model without explicit experiments for other SVAE structures
- What evidence would resolve it: Experimental results comparing performance and computational efficiency across various SVAE architectures

### Open Question 2
- Question: How does choice of reparameterization for natural parameters affect convergence and performance?
- Basis in paper: [explicit] Authors propose using unconstrained reparameterizations but don't explore impact of different choices
- Why unresolved: Paper doesn't systematically compare different reparameterization strategies
- What evidence would resolve it: Systematic comparison of different reparameterization strategies on range of SVAE models and datasets

### Open Question 3
- Question: What is theoretical justification for robustness of capped implicit gradient estimator?
- Basis in paper: [inferred] Authors propose "capped implicit gradient estimator" but provide empirical evidence without theoretical analysis
- Why unresolved: Paper lacks theoretical analysis of convergence properties
- What evidence would resolve it: Theoretical analysis of convergence properties using optimization theory or numerical analysis techniques

### Open Question 4
- Question: How does SVAE performance compare to other methods for handling missing data in time series?
- Basis in paper: [explicit] Authors demonstrate SVAE's ability to handle multimodal uncertainty but don't compare to alternative approaches
- Why unresolved: Paper focuses on SVAE advantages without comprehensive comparison to alternative methods
- What evidence would resolve it: Thorough experimental comparison with other state-of-the-art missing data handling methods

## Limitations
- The implicit differentiation scheme's robustness to incomplete optimization needs empirical validation across diverse model architectures
- The memory-efficient approach may sacrifice some accuracy for scalability, particularly in high-dimensional parameter spaces
- Implementation details for unbiased natural gradient computation via automatic differentiation remain underspecified

## Confidence

**High**: The structured VI approach for handling discrete latent variables without biased relaxations is well-supported by evidence and addresses genuine limitations in prior work.

**Medium**: The memory-efficient implicit differentiation provides computational benefits, but robustness to incomplete optimization needs empirical validation across diverse model architectures.

**Medium**: The unbiased natural gradient updates avoid known biases in prior work, but practical gains over biased alternatives require systematic comparison.

## Next Checks

1. **Numerical Stability Test**: Systematically evaluate the implicit differentiation scheme across varying iteration counts and condition numbers to identify stability boundaries.

2. **Generalization Across Architectures**: Test the unbiased natural gradient implementation on non-temporal graphical models (e.g., Bayesian networks) to verify robustness beyond the presented switching LDS case.

3. **Ablation on Structured VI**: Compare the proposed structured mean field approximation against alternative structured VI methods (e.g., structured wake-sleep) to quantify the specific contribution of the factorization approach.