---
ver: rpa2
title: Active Cost-aware Labeling of Streaming Data
arxiv_id: '2304.06808'
source_url: https://arxiv.org/abs/2304.06808
tags:
- algorithm
- data
- labeling
- active
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of actively labeling streaming data
  with costly labels. On each round, an input arrives and the learner chooses whether
  to incur a labeling cost to observe a label or predict based on past data.
---

# Active Cost-aware Labeling of Streaming Data

## Quick Facts
- arXiv ID: 2304.06808
- Source URL: https://arxiv.org/abs/2304.06808
- Reference count: 40
- Key outcome: Achieves Õ(B^{1/3} K^{1/3} T^{2/3}) worst-case loss for discrete inputs and Õ(B^{1/(d+3)} T^{(d+2)/(d+3)}) for RKHS functions with squared exponential kernels

## Executive Summary
This paper studies active labeling of streaming data where observing labels is costly. On each round, an input arrives and the learner chooses whether to incur a labeling cost to observe the label or predict based on past data. The goal is to minimize the sum of labeling costs and prediction errors. The authors propose simple algorithms that label inputs when uncertainty exceeds a threshold dependent on labeling cost and arrival patterns. For discrete inputs, the algorithm achieves a worst-case upper bound that matches a lower bound. For continuous inputs modeled by smooth RKHS functions, the loss is bounded by functions of the kernel type and dimensionality.

## Method Summary
The paper proposes two algorithms for active cost-aware labeling. Algorithm 1 handles discrete inputs from K types by maintaining sample means and confidence intervals for each type, labeling when uncertainty exceeds a cost-dependent threshold. Algorithm 2 extends this to continuous inputs using Gaussian processes with RKHS functions, labeling when the GP posterior standard deviation exceeds a time-varying threshold. Both algorithms balance the trade-off between labeling cost and prediction error by adapting the labeling threshold based on observed arrival patterns and accumulated uncertainty.

## Key Results
- Achieves Õ(B^{1/3} K^{1/3} T^{2/3}) worst-case upper bound for discrete inputs matching lower bound
- For RKHS functions with squared exponential kernels: Õ(B^{1/(d+3)} T^{(d+2)/(d+3)}) loss bound
- For Matérn kernels: Õ(B^{1/(2d+3)} T^{(2d+2)/(2d+3)}) loss bound
- Empirical results on synthetic and real datasets demonstrate superior performance compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm achieves sublinear loss by adaptively labeling inputs when uncertainty exceeds a cost-dependent threshold.
- Mechanism: The threshold function balances the labeling cost B against the accumulated uncertainty, which decreases as more samples of a type are observed. This ensures labeling is frequent enough to reduce prediction error but not so frequent as to incur excessive labeling cost.
- Core assumption: Sub-Gaussian noise in the label observations allows concentration inequalities to bound uncertainty with high probability.
- Evidence anchors: [abstract] "achieves a worst-case upper bound of O(B^{1/3} K^{1/3} T^{2/3})", [section 2.1] "We will maintain an estimate µ̂t,k for each mean µk along with confidence intervals. If the uncertainty for µk ... is larger than a certain threshold, we will label xt"

### Mechanism 2
- Claim: In the RKHS setting, Gaussian process posterior uncertainty guides labeling to achieve sublinear loss.
- Mechanism: The GP posterior standard deviation quantifies uncertainty; labeling occurs when this exceeds a threshold that scales with cost and time. Discretization of the input space bounds the number of required labels per region.
- Core assumption: The underlying function f lies in an RKHS with bounded norm, enabling GP-based uncertainty quantification.
- Evidence anchors: [abstract] "loss is bounded by O(B^{1/(d+3)} T^{(d+2)/(d+3)}) in an RKHS with a squared exponential kernel", [section 3.1] "we will choose to label xt if the GP standard deviation σt−1(xt), which quantifies the uncertainty of µt−1(xt), is larger than a cost and time-dependent threshold τ(t)"

### Mechanism 3
- Claim: The algorithm adapts to arrival patterns, improving performance when certain types appear more frequently.
- Mechanism: The threshold function includes Mt,k (number of arrivals of type k), so more frequent types are labeled more often, reducing cumulative prediction error.
- Core assumption: The learner observes the arrival pattern over time and can adjust labeling accordingly.
- Evidence anchors: [abstract] "achieves better performance when the arrival pattern is more favorable", [section 2.1] "The threshold increases with B, discouraging frequent labeling when it is expensive, and decreases with Mt,xt, encouraging frequent labeling the more times a type is observed"

## Foundational Learning

- Concept: Sub-Gaussian concentration inequalities
  - Why needed here: To bound the uncertainty of sample means and GP posterior estimates with high probability.
  - Quick check question: What is the form of a sub-Gaussian tail bound for a sample mean with variance σ²?
- Concept: Reproducing kernel Hilbert spaces (RKHS)
  - Why needed here: To model smooth functions and enable GP-based uncertainty quantification in the continuous input setting.
  - Quick check question: What is the reproducing property of an RKHS and how does it relate to kernel functions?
- Concept: Maximum information gain (MIG)
  - Why needed here: To quantify the complexity of the GP model and bound the cumulative regret in the RKHS setting.
  - Quick check question: How does MIG scale with the number of observations for squared exponential vs Matérn kernels?

## Architecture Onboarding

- Component map: Input stream processor -> Uncertainty estimator -> Threshold comparator -> Labeling controller -> Predictor
- Critical path: xt arrives -> uncertainty estimated -> threshold compared -> labeling decision -> prediction output
- Design tradeoffs:
  - Fixed vs adaptive threshold: adaptive allows better performance on skewed arrival patterns but adds complexity
  - Sample mean vs GP: GP provides better uncertainty quantification in continuous space but requires kernel tuning
  - Discrete vs continuous input: discrete is simpler but less realistic; continuous requires discretization for analysis
- Failure signatures:
  - Labeling too often: high labeling cost, low prediction error
  - Labeling too rarely: low labeling cost, high prediction error
  - Incorrect kernel: GP uncertainty estimates are unreliable, leading to suboptimal labeling
- First 3 experiments:
  1. Simulate K discrete types with known means, vary arrival patterns (uniform vs lopsided), and measure loss vs B.
  2. Implement Algorithm 2 with SE kernel on synthetic smooth functions, compare to baselines under uniform and skewed arrival patterns.
  3. Apply Algorithm 2 to the Parkinsons dataset, tune kernel hyperparameters via marginal likelihood, and evaluate average loss vs labeling cost B.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Theoretical bounds assume known arrival patterns and RKHS parameters, which may not hold in practice
- Performance analysis focuses on worst-case bounds rather than average-case behavior on realistic data distributions
- Extension to multi-dimensional outputs or structured prediction tasks is not addressed

## Confidence

**Confidence: Medium** on the discrete-input bounds. While the algorithm's design is theoretically sound, the paper does not fully justify the constant factors in the O(B^{1/3} K^{1/3} T^{2/3}) bound or provide a matching lower bound proof.

**Confidence: Low** on the RKHS generalization. The paper assumes functions lie in a bounded-norm RKHS and uses Gaussian process uncertainty, but does not verify these assumptions on real datasets.

**Confidence: Medium** on the practical utility. The algorithms require knowledge of arrival patterns and kernel parameters, which may not be available in practice.

## Next Checks

1. **Robustness to Unknown Arrival Patterns**: Implement a variant of Algorithm 1 that learns the arrival distribution online and compare its performance to the known-pattern version under changing arrival rates.

2. **Kernel Misspecification Analysis**: Evaluate Algorithm 2 with intentionally misspecified kernels (wrong length scale or smoothness) on smooth test functions to quantify performance degradation.

3. **Lower Bound Verification**: Construct explicit adversarial arrival patterns and function families that match the upper bound O(B^{1/3} K^{1/3} T^{2/3}) to verify tightness of the discrete-input regret bound.