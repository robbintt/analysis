---
ver: rpa2
title: 'Reinforcement Replaces Supervision: Query focused Summarization using Deep
  Reinforcement Learning'
arxiv_id: '2311.17514'
source_url: https://arxiv.org/abs/2311.17514
tags:
- bart
- document
- query
- dataset
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a reinforcement learning framework for query-focused
  summarization, addressing the conflict between RL and teacher forcing in transformers
  by using scheduled sampling with a two-pass decoder. The method employs policy gradient
  training with rewards from ROUGE, BLEU, and semantic similarity, including a novel
  passage embedding-based reward using cluster hypothesis.
---

# Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2311.17514
- Source URL: https://arxiv.org/abs/2311.17514
- Reference count: 40
- Primary result: RL-based models achieve 10-point ROUGE-L improvement over supervised baselines for query-focused summarization

## Executive Summary
This paper introduces a reinforcement learning framework for query-focused summarization that addresses the fundamental conflict between RL and Teacher Forcing in transformer-based text generation. The authors propose a scheduled sampling approach with a two-pass decoder using Gumbel reparametrization, enabling gradient backpropagation while simulating sequential action sampling. The method employs policy gradient training with rewards from ROUGE, BLEU, and semantic similarity metrics, including a novel passage embedding-based reward using cluster hypothesis. Experiments on the ELI5 dataset demonstrate substantial improvements over state-of-the-art supervised models, with human evaluation confirming better correctness and fluency.

## Method Summary
The method employs reinforcement learning with policy gradient networks trained on reward signals including ROUGE, BLEU, and semantic similarity. To resolve the conflict between RL and Teacher Forcing in transformers, the authors implement scheduled sampling with a two-pass decoder that uses Gumbel reparametrization for gradient backpropagation. A novel passage embedding model trained on Reddit data using cluster hypothesis provides semantic similarity rewards. The approach uses BART-large as the base architecture and trains on the ELI5 dataset with triplets in <query, document, answer> format.

## Key Results
- RL-based models achieve 10-point ROUGE-L improvement over state-of-the-art supervised models on ELI5 dataset
- Human evaluation on RQFT dataset shows RL models generate more relevant and less verbose summaries than supervised baselines
- Zero-shot transfer to DebatePedia demonstrates robust performance without additional fine-tuning
- Scheduled sampling with two-pass decoder effectively resolves Teacher Forcing vs RL conflict in transformers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement Learning generalizes token-by-token matching into phrase/sentence/semantic matching, enabling better query-focused summarization.
- Mechanism: RL replaces cross-entropy loss with policy gradient training that uses reward signals (ROUGE, BLEU, semantic similarity) to guide summary generation. This allows the model to optimize for higher-level semantic relevance rather than exact token matching.
- Core assumption: Reward functions like ROUGE and BLEU adequately capture semantic relevance for query-focused summaries.
- Evidence anchors:
  - [abstract]: "While Cross-Entropy loss utilizes token-by-token match, RL helps us generalize this further to phrase/sentence/semantic match."
  - [section 3.1]: "MLE attempts to reward a token-by-token match. We take the RL path to generalize this rewarding mechanism using more sophisticated lexical and/or semantic rewards."
  - [corpus]: Weak evidence - related works primarily use SL approaches, limited empirical comparison of RL vs SL generalization for QFS.
- Break condition: If reward signals do not correlate with human judgment of summary quality, RL optimization will fail to produce meaningful improvements.

### Mechanism 2
- Claim: Scheduled sampling resolves the conflict between Teacher Forcing and RL in transformer-based text generation.
- Mechanism: Instead of pure Teacher Forcing (feeding ground truth tokens) or pure RL (feeding generated tokens), scheduled sampling uses a two-pass decoder where the second pass uses sampled embeddings via Gumbel reparametrization. This allows gradient backpropagation while simulating sequential action sampling.
- Core assumption: Two-pass decoding with Gumbel reparametrization effectively approximates true RL training without the computational burden.
- Evidence anchors:
  - [abstract]: "We propose a way to employ RL for text generation without Teacher Forcing, based on Scheduled Sampling."
  - [section 3.1]: "We simulate the sequential action sampling through scheduled sampling for Transformers... This enables gradient backpropagation for the first pass through the decoder too."
  - [corpus]: Weak evidence - no direct comparison with alternative approaches to resolve Teacher Forcing vs RL conflict.
- Break condition: If Gumbel reparametrization sampling does not adequately represent the true distribution of generated tokens, the RL signal will be inaccurate.

### Mechanism 3
- Claim: Cluster Hypothesis-based passage embedding provides better semantic similarity rewards than compositional methods.
- Mechanism: Dual encoder architecture trained on Reddit data clusters similar passages together, creating embeddings where cosine similarity reflects semantic relevance. This is used as a reward signal in RL training.
- Core assumption: Passages answering similar queries cluster together in the embedding space, making cosine similarity a meaningful reward.
- Evidence anchors:
  - [abstract]: "To aid the RL training, we propose a better semantic similarity reward, enabled by a novel Passage Embedding scheme developed using Cluster Hypothesis."
  - [section 3.2]: "We use the Cluster Hypothesis... summaries of the same query on the same document should be clustered together and have a high similarity score."
  - [section 5.1]: "We find that similarity obtained from this embedding generation scheme leads to better results in human and automatic evaluation."
- Break condition: If the Reddit corpus does not adequately represent the semantic space of query-focused summaries, the embedding space will not generalize well.

## Foundational Learning

- Concept: Reinforcement Learning policy gradient optimization
  - Why needed here: To replace cross-entropy loss with reward-based optimization that can capture semantic relevance
  - Quick check question: What is the key difference between MLE loss and policy gradient loss in terms of what they optimize for?

- Concept: Scheduled sampling for transformer training
  - Why needed here: To resolve the Teacher Forcing vs RL conflict while maintaining computational tractability
  - Quick check question: How does the two-pass decoder with Gumbel reparametrization differ from pure Teacher Forcing or pure RL generation?

- Concept: Dual encoder architectures for semantic similarity
  - Why needed here: To create passage embeddings where cosine similarity reflects semantic relevance for use as RL rewards
  - Quick check question: Why is the Cluster Hypothesis appropriate for training passage embeddings for query-focused summarization?

## Architecture Onboarding

- Component map: Encoder-decoder transformer (BART-large) for QFS, dual encoder BERT for passage embeddings, reward computation module
- Critical path: Input (query + document) → Encoder → Decoder (two-pass with scheduled sampling) → Generated summary → Reward computation (ROUGE/BLEU/semantic) → Policy gradient update
- Design tradeoffs: Computational cost vs reward accuracy (two-pass decoding), dataset size vs embedding quality (Reddit corpus), lexical vs semantic rewards
- Failure signatures: Low ROUGE improvement despite training, high hallucination rates, model ignoring document content, slow convergence
- First 3 experiments:
  1. Train baseline BART with cross-entropy loss on ELI5 dataset
  2. Implement scheduled sampling two-pass decoder with ROUGE reward
  3. Add passage embedding reward using pretrained dual encoder

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scheduled sampling technique affect the convergence rate and quality of generated summaries in the long-term?
- Basis in paper: [explicit] The paper introduces a two-pass decoding using scheduled sampling to resolve the conflict between RL and Teacher Forcing in transformers, stating it leads to better results in automatic evaluation.
- Why unresolved: The paper does not provide a detailed analysis of the convergence rate or the long-term effects of scheduled sampling on the quality of generated summaries.
- What evidence would resolve it: Experiments comparing the convergence rate and long-term quality of summaries generated using different sampling techniques, including scheduled sampling, would provide insights into its effectiveness.

### Open Question 2
- Question: What is the impact of using different semantic similarity measures, such as SBERT and SimCSE, on the quality of generated summaries?
- Basis in paper: [explicit] The paper mentions that BART R-S FPEG, which uses a novel passage embedding-based reward, achieves the highest ROUGE-L score, suggesting that semantic similarity measures can influence summary quality.
- Why unresolved: The paper does not provide a detailed comparison of the impact of different semantic similarity measures on the quality of generated summaries.
- What evidence would resolve it: Experiments comparing the performance of models using different semantic similarity measures, such as SBERT and SimCSE, on the same dataset would provide insights into their effectiveness.

### Open Question 3
- Question: How does the size and diversity of the training dataset affect the performance of the passage embedding model?
- Basis in paper: [explicit] The paper mentions that the passage embedding model is trained on a dataset scraped from Reddit, which may have limitations in terms of size and diversity.
- Why unresolved: The paper does not provide a detailed analysis of the impact of the size and diversity of the training dataset on the performance of the passage embedding model.
- What evidence would resolve it: Experiments training the passage embedding model on datasets of varying sizes and diversities, and comparing their performance on downstream tasks, would provide insights into the impact of dataset characteristics.

## Limitations

- Data provenance uncertainty: RPEDT dataset construction details are limited, affecting reproducibility and generalizability of passage embedding rewards
- Reward signal independence: The paper lacks ablation studies showing which reward components contribute most to performance improvements
- Evaluation scope limitation: Human evaluation was conducted only on curated RQFT dataset (250 instances), potentially introducing selection bias

## Confidence

**High confidence**: The technical implementation of scheduled sampling with two-pass decoder and Gumbel reparametrization is well-specified and theoretically sound. The 10-point ROUGE-L improvement over supervised baselines on ELI5 is substantial and likely robust.

**Medium confidence**: The superiority of cluster hypothesis-based passage embeddings over compositional methods is supported by experimental results but lacks direct comparison with alternative semantic similarity approaches (e.g., SBERT, USE).

**Low confidence**: The claim that RL models generate "more relevant" content than supervised models is based on ROUGE metrics rather than direct semantic relevance judgments. The relationship between ROUGE improvement and human-perceived relevance is not rigorously established.

## Next Checks

1. **Ablation study on reward components**: Systematically disable each reward component (ROUGE, BLEU, semantic similarity) to quantify their individual contributions to performance improvements. This will clarify whether the semantic similarity reward is truly essential or if lexical rewards alone suffice.

2. **Cross-dataset generalization test**: Evaluate the trained models on additional query-focused summarization datasets beyond ELI5 and DebatePedia, such as CNN/DailyMail with query modifications or other Reddit-based datasets, to assess true generalization capability.

3. **Human evaluation on automatic outputs**: Conduct human evaluation on the full test set outputs (not just curated instances) to verify that ROUGE improvements correlate with human judgments of relevance, fluency, and correctness across the entire distribution.