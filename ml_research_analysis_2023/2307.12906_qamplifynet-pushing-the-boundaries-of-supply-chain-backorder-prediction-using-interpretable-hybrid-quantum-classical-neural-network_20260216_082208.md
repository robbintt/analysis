---
ver: rpa2
title: 'QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using
  Interpretable Hybrid Quantum-Classical Neural Network'
arxiv_id: '2307.12906'
source_url: https://arxiv.org/abs/2307.12906
tags:
- quantum
- data
- dataset
- backorder
- classical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces QAmplifyNet, a novel hybrid quantum-classical
  neural network for supply chain backorder prediction. QAmplifyNet leverages quantum-inspired
  techniques to effectively handle short, imbalanced datasets, achieving 90% accuracy,
  79.85% AUC-ROC, and superior F1-scores compared to classical models, quantum ensembles,
  and deep reinforcement learning.
---

# QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum-Classical Neural Network

## Quick Facts
- **arXiv ID**: 2307.12906
- **Source URL**: https://arxiv.org/abs/2307.12906
- **Reference count**: 40
- **Primary result**: 90% accuracy, 79.85% AUC-ROC, and superior F1-scores compared to classical models, quantum ensembles, and deep reinforcement learning for supply chain backorder prediction

## Executive Summary
QAmplifyNet introduces a novel hybrid quantum-classical neural network for supply chain backorder prediction. The model combines classical dense layers with a 2-qubit quantum layer using amplitude embedding and strongly entangling gates. Trained on a highly imbalanced dataset, QAmplifyNet achieves 90% accuracy and 79.85% AUC-ROC, outperforming traditional ML models, quantum ensembles, and deep reinforcement learning approaches. The architecture leverages quantum-inspired techniques to handle short, imbalanced datasets effectively while maintaining interpretability through SHAP and LIME.

## Method Summary
QAmplifyNet is a hybrid quantum-classical neural network that processes supply chain data through a three-stage pipeline. First, classical dense layers (512 → 256 → 4 units) with ReLU activation perform feature transformation. Second, a quantum layer uses amplitude embedding to map 4 principal components into a 2-qubit quantum state, followed by strongly entangling layers with parameterized rotations. Third, the quantum output passes through a softmax layer for binary classification. The model is trained using Adam optimizer on balanced data (3:1 ratio after NearMiss undersampling) and evaluated on the original imbalanced test set.

## Key Results
- Achieves 90% accuracy and 79.85% AUC-ROC on supply chain backorder prediction
- Outperforms traditional ML models (Catboost, LGBM, RF, XGBoost, ANN, KNN, SVM, DT)
- Demonstrates superior performance compared to quantum ensembles and deep reinforcement learning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QAmplifyNet improves backorder prediction accuracy by encoding classical data into quantum amplitudes using Amplitude Embedding (AE).
- Mechanism: AE maps up to 2^n classical features into the amplitudes of an n-qubit quantum state, enabling the quantum layer to process high-dimensional information efficiently.
- Core assumption: Classical features are normalized such that the sum of squared amplitudes equals one, preserving valid quantum state representation.
- Evidence anchors:
  - [abstract] "The model's architecture combines classical dense layers with a quantum layer using amplitude embedding and strongly entangling gates."
  - [section] "The AE technique transforms the 4 features obtained from the classical component into the amplitudes of a quantum state with 2 qubits."
- Break condition: If feature values are not normalized (sum of squares ≠ 1), the quantum state preparation will fail or yield incorrect results.

### Mechanism 2
- Claim: Strongly Entangling Layers (SEL) provide trainable parameters that allow the quantum circuit to learn complex correlations between features.
- Mechanism: SEL applies parameterized rotations (RY, RZ) followed by entangling CNOT gates across qubits, enabling the quantum circuit to represent non-linear relationships.
- Core assumption: The depth of SEL (number of layers) is sufficient to capture the complexity of the feature interactions.
- Evidence anchors:
  - [section] "The SEL utilizes a circuit-centric approach in its design. In this approach, each individual qubit... is represented by a 2 × 2 unitary matrix..."
  - [section] "The model has precisely 3 × n × L number of trainable parameters."
- Break condition: If SEL depth is too shallow for the data complexity, the quantum layer will underfit and fail to improve prediction accuracy.

### Mechanism 3
- Claim: NearMiss undersampling balances the dataset without losing critical minority-class information, improving QML training stability.
- Mechanism: NearMiss selects majority-class samples that are closest to minority-class samples, preserving decision boundary information while reducing class imbalance.
- Core assumption: The selected majority-class samples after NearMiss are representative of the true underlying distribution.
- Evidence anchors:
  - [section] "We employ the NearMiss undersampling method, which involves deliberately reducing the majority of class occurrences."
  - [section] "Given the imbalanced nature of the dataset, we employed the 'imblearn' module from scikit-learn to gain insights into specificity, Gmean, and IBA values."
- Break condition: If NearMiss removes too many majority samples, the model may overfit to the minority class or lose generalization ability.

## Foundational Learning

- Concept: Quantum state amplitude encoding
  - Why needed here: To map classical features into a quantum representation that the quantum layer can process.
  - Quick check question: What condition must classical feature values satisfy before amplitude encoding?

- Concept: Strongly Entangling Layers (SEL)
  - Why needed here: To provide trainable parameters that allow the quantum circuit to learn feature interactions.
  - Quick check question: How many trainable parameters does an SEL with n qubits and L layers have?

- Concept: NearMiss undersampling
  - Why needed here: To balance the dataset while preserving decision boundary information for QML training.
  - Quick check question: What is the main difference between NearMiss and random undersampling?

## Architecture Onboarding

- Component map:
  - Input layer: 4 neurons (PCA-reduced features)
  - Dense Layer 1: 512 units, ReLU, non-trainable (embedding)
  - Dense Layer 2: 256 units, ReLU, non-trainable
  - Dense Layer 3: 4 units, ReLU, non-trainable
  - QNN KerasLayer: 2-qubit quantum circuit with AE and SEL
  - Output layer: 2 units, softmax

- Critical path: Input → Dense 1 → Dense 2 → Dense 3 → QNN KerasLayer → Output

- Design tradeoffs:
  - Quantum depth vs. noise sensitivity: Deeper SEL improves expressiveness but increases susceptibility to noise.
  - PCA reduction vs. information loss: Reduces dimensionality for quantum processing but may discard useful features.
  - Undersampling vs. data retention: Balances classes for training but reduces overall dataset size.

- Failure signatures:
  - Low training accuracy but high validation accuracy: Likely overfitting due to aggressive undersampling.
  - High training accuracy but low validation accuracy: Model memorizing training data, possibly due to insufficient regularization.
  - Quantum circuit execution errors: Feature normalization issues or incompatible quantum device configuration.

- First 3 experiments:
  1. Train with only classical layers (remove QNN KerasLayer) to establish baseline performance.
  2. Train with QNN KerasLayer but without AE (use alternative encoding) to test encoding impact.
  3. Train with full architecture but vary SEL depth (L=1, L=2, L=3) to find optimal quantum circuit complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the scalability limitations of QAmplifyNet when applied to significantly larger datasets or real-time applications, and how can these be addressed?
- Basis in paper: [explicit] The authors mention that quantum computing is still in its early stages and that hardware limitations, such as noise and limited qubit connectivity, can hinder scalability and practical implementation of quantum algorithms.
- Why unresolved: The paper does not provide specific data or experiments to quantify the scalability limitations of QAmplifyNet on larger datasets or in real-time scenarios.
- What evidence would resolve it: Empirical studies comparing QAmplifyNet's performance on datasets of increasing size, benchmarking against classical models, and analyzing computational resources required for larger datasets would help quantify scalability limitations.

### Open Question 2
- Question: How does the interpretability of QAmplifyNet compare to traditional black-box models, and what specific insights can be gained from using SHAP and LIME techniques?
- Basis in paper: [explicit] The authors mention that QAmplifyNet has a black-box nature, but they also apply SHAP and LIME techniques to enhance interpretability.
- Why unresolved: The paper does not provide a detailed comparison of the interpretability of QAmplifyNet with traditional black-box models, nor does it explain the specific insights gained from using SHAP and LIME.
- What evidence would resolve it: A detailed comparison of the interpretability of QAmplifyNet with traditional black-box models, along with case studies showcasing the specific insights gained from using SHAP and LIME, would provide a clearer understanding of the model's interpretability.

### Open Question 3
- Question: What are the potential applications of QAmplifyNet in other domains beyond supply chain management, and how can the model be adapted to handle different types of imbalanced datasets?
- Basis in paper: [explicit] The authors mention that the model can be adapted to other supervised binary classification tasks, such as credit card default prediction or fraud detection, where imbalanced datasets and limited feature sets are common challenges.
- Why unresolved: The paper does not provide specific examples or case studies of QAmplifyNet being applied to other domains or adapted to handle different types of imbalanced datasets.
- What evidence would resolve it: Case studies or empirical results demonstrating the application of QAmplifyNet in other domains, along with a detailed explanation of how the model can be adapted to handle different types of imbalanced datasets, would provide valuable insights into its potential applications and adaptability.

## Limitations
- The 2-qubit quantum layer implementation may face scalability constraints when applied to datasets with higher feature dimensions.
- Performance improvements are demonstrated primarily through comparison with traditional ML methods, with limited direct comparison to other quantum approaches.
- The quantum advantage over classical models is demonstrated on a single supply chain dataset with specific preprocessing strategies.

## Confidence
- **High Confidence**: The hybrid architecture design combining classical dense layers with quantum amplitude embedding is technically sound and well-documented.
- **Medium Confidence**: The reported performance improvements (90% accuracy, 79.85% AUC-ROC) are likely accurate for the specific dataset and preprocessing pipeline used, but may not generalize to all supply chain scenarios.
- **Low Confidence**: Claims about QAmplifyNet's superiority over other quantum approaches are limited by the narrow scope of comparative quantum architectures tested.

## Next Checks
1. **Cross-dataset validation**: Test QAmplifyNet on multiple supply chain datasets with varying class imbalance ratios to verify generalizability beyond the original dataset.
2. **Quantum resource scaling**: Evaluate performance as quantum circuit depth increases beyond 2 qubits to assess scalability limitations and noise sensitivity.
3. **Ablation study**: Systematically remove quantum components (QNN KerasLayer) to quantify the specific contribution of quantum processing versus classical methods alone.