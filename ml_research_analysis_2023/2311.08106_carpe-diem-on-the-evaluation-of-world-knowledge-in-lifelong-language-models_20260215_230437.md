---
ver: rpa2
title: 'Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language Models'
arxiv_id: '2311.08106'
source_url: https://arxiv.org/abs/2311.08106
tags:
- knowledge
- learning
- language
- arxiv
- updated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EvolvingQA, a novel benchmark designed to
  evaluate the ability of language models to adapt to dynamically evolving knowledge
  over time. The benchmark focuses on training and evaluating models on an evolving
  Wikipedia database, incorporating question-answering as a downstream task to emulate
  real-world applications.
---

# Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language Models

## Quick Facts
- arXiv ID: 2311.08106
- Source URL: https://arxiv.org/abs/2311.08106
- Reference count: 40
- Language models struggle to update and forget outdated knowledge when pretrained on evolving knowledge

## Executive Summary
This paper introduces EvolvingQA, a novel benchmark for evaluating language models' ability to adapt to dynamically evolving knowledge over time. The benchmark uses Wikipedia snapshots from February to August 2023 and automatically generates question-answer pairs using large language models. The key finding is that existing continual learning baselines struggle to update outdated knowledge, particularly for numerical and temporal information, due to insufficient weight gradients during training. The work highlights the need for better methods to dynamically accumulate and revise information in language models.

## Method Summary
The EvolvingQA benchmark is constructed by collecting Wikipedia snapshots over time, filtering for significant changes (>500 characters), and using GPT-3.5 to automatically generate question-answer pairs. The benchmark evaluates models on three knowledge categories: UNCHANGED (information that remains valid), NEW (completely new information), and EDITED (outdated information that needs updating). Models are pretrained on CHANGED sets and fine-tuned on the QA pairs, with evaluation using Exact Match and F1 metrics. Baselines include initialization from pre-trained checkpoints, full training, K-Adapter, LoRA, and DPR.

## Key Results
- Existing continual learning baselines fail to update outdated knowledge and forget old information
- Models show near-zero weight gradients when learning edited knowledge compared to new knowledge
- Language models particularly struggle with numerical and temporal knowledge updates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual pretraining on CHANGED sets enables language models to update knowledge incrementally over time.
- Mechanism: By sequentially training on differences between Wikipedia snapshots, the model gradually adjusts parameters to reflect new information while attempting to maintain previously learned knowledge.
- Core assumption: Weight updates during training are sufficient to encode temporal knowledge changes when provided with evolving data streams.
- Evidence anchors:
  - [abstract] "continually training language models with evolving world knowledge has become a significant research direction"
  - [section 2.1] "We collect CHANGED sets, pretraining corpora consisting of changes between two consecutive Wikipedia snapshots"
  - [corpus] Weak - the corpus mentions related work on knowledge editing but doesn't directly address this incremental updating mechanism
- Break condition: When weight gradients become too small to effect meaningful parameter changes, as evidenced by the analysis showing minimal gradient updates for EDITED knowledge compared to NEW knowledge.

### Mechanism 2
- Claim: Using question-answering as a downstream task provides an intuitive measure of temporal knowledge adaptation.
- Mechanism: QA pairs force the model to retrieve specific factual information, making it possible to test whether outdated knowledge has been properly forgotten and replaced with updated information.
- Core assumption: QA performance directly reflects the model's internal knowledge state regarding temporal facts.
- Evidence anchors:
  - [abstract] "Our benchmark incorporates question-answering as a downstream task to emulate real-world applications"
  - [section 2.2] "We construct a question-answering benchmark to measure the model's capability of answering correctly while learning temporally changing knowledge"
  - [corpus] Moderate - related papers discuss knowledge editing but don't specifically validate through QA benchmarks
- Break condition: If the model learns to pattern-match questions rather than truly updating knowledge, performance may not reflect actual knowledge state.

### Mechanism 3
- Claim: Automated pipeline construction using LLMs enables scalable benchmark creation across multiple time steps.
- Mechanism: GPT-3.5 generates and validates question-answer pairs from Wikipedia changes, eliminating manual annotation requirements and allowing continuous expansion.
- Core assumption: LLM-generated QA pairs accurately capture the factual changes between time steps without introducing bias or hallucination.
- Evidence anchors:
  - [abstract] "The construction of EvolvingQA is automated with our pipeline using large language models"
  - [section 2.2] "We construct UNCHANGED, NEW, EDITED evaluation sets... using LLM"
  - [corpus] Strong - multiple related papers (EvoEdit, Lifelong Knowledge Editing) also use LLM-based approaches for similar tasks
- Break condition: When LLM hallucinations or biases become systematic, potentially creating evaluation artifacts that don't reflect true knowledge changes.

## Foundational Learning

- Concept: Continual Learning and Catastrophic Forgetting
  - Why needed here: The benchmark directly tests whether models can update knowledge without losing previously learned information, which is the core challenge in continual learning
  - Quick check question: What happens to model performance on UNCHANGED knowledge after training on EDITED knowledge?

- Concept: Gradient-based Parameter Updates
  - Why needed here: The analysis shows that small weight gradients prevent effective knowledge updating, making understanding gradient dynamics essential
  - Quick check question: How do gradient norms differ when learning NEW versus EDITED knowledge?

- Concept: Question-Answer Pair Construction and Evaluation
  - Why needed here: The benchmark relies on automatically generated QA pairs, requiring understanding of how to prompt LLMs effectively and validate their outputs
  - Quick check question: What filtering steps are applied to remove hallucinated or semantically equivalent answers?

## Architecture Onboarding

- Component map: Wikipedia snapshot collection -> CHANGED set filtering -> LLM-based QA generation -> Evaluation set construction -> Model training -> Fine-tuning -> Evaluation -> Analysis

- Critical path: Wikipedia snapshot collection -> CHANGED set creation -> QA pair generation -> Model training -> Evaluation -> Analysis

- Design tradeoffs:
  - Automation vs. accuracy: Using LLMs for QA generation enables scalability but introduces potential bias
  - Granularity vs. efficiency: Filtering only articles with >500 character changes balances dataset size with meaningful updates
  - Single-task vs. multi-task: Focusing on QA simplifies evaluation but may miss other temporal reasoning capabilities

- Failure signatures:
  - Minimal gradient updates during EDITED knowledge training (near-zero gradients)
  - Similar performance on OUTDATED and UPDATED questions indicating failure to update
  - Systematic bias toward numerical/temporal answer types in EDITED sets

- First 3 experiments:
  1. Run gradient analysis comparing NEW vs EDITED knowledge training to confirm mechanism 1
  2. Test multiple choice evaluation format to verify mechanism 2's assumption about QA reflecting knowledge state
  3. Generate additional time steps using the automated pipeline to stress-test mechanism 3's scalability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can language models be effectively trained to update numerical and temporal knowledge without suffering from catastrophic forgetting?
- Basis in paper: [explicit] The paper explicitly states that language models struggle to update numerical and temporal knowledge due to small weight gradients.
- Why unresolved: While the paper identifies this as a key challenge, it does not propose a specific solution to overcome this limitation.
- What evidence would resolve it: A proposed method that successfully improves the model's ability to update numerical and temporal information without forgetting previously learned knowledge, along with experimental results demonstrating this improvement.

### Open Question 2
- Question: What are the trade-offs between continual learning approaches and retrieval-based methods for adapting language models to evolving knowledge?
- Basis in paper: [explicit] The paper discusses how retrieval-based methods like DPR perform better on their benchmark, but acknowledges that this is due to the benchmark's design favoring retrieval methods.
- Why unresolved: The paper does not provide a comprehensive comparison of the strengths and weaknesses of both approaches in real-world scenarios.
- What evidence would resolve it: A detailed study comparing the performance of continual learning and retrieval-based methods on a variety of tasks that require complex reasoning and integration of multiple knowledge sources.

### Open Question 3
- Question: How can the construction of evolving knowledge benchmarks be further automated to reduce costs and enable continuous evaluation?
- Basis in paper: [explicit] The paper presents an automated pipeline for constructing the EvolvingQA benchmark, but acknowledges that human annotation is still required for some steps.
- Why unresolved: The paper does not explore how to fully automate the benchmark construction process, which could make it more scalable and cost-effective.
- What evidence would resolve it: A proposed method that automates the entire process of benchmark construction, including the generation of high-quality questions and answers, and experimental results demonstrating the effectiveness of the automated approach.

## Limitations

- The automated QA generation pipeline may introduce LLM biases and hallucinations that affect evaluation quality
- The >500 character change threshold for filtering Wikipedia updates may exclude smaller but significant knowledge changes
- The benchmark focuses only on Wikipedia domain knowledge, limiting generalizability to other domains

## Confidence

**High Confidence:** The core finding that existing continual learning baselines struggle with updating outdated knowledge while maintaining new information is well-supported by empirical results across multiple experimental conditions. The gradient analysis showing near-zero updates for edited knowledge is particularly robust.

**Medium Confidence:** The automated pipeline's effectiveness in capturing temporal knowledge changes through LLM-generated QA pairs is supported by systematic filtering steps, but the potential for systematic bias in generation remains a concern. The choice of >500 character threshold for meaningful changes is reasonable but somewhat arbitrary.

**Low Confidence:** The specific mechanisms explaining why numerical and temporal information updates are particularly challenging require further investigation. While the paper observes this pattern, the underlying causes (e.g., representation similarity, update frequency) are not definitively established.

## Next Checks

1. Re-run the gradient norm comparison across NEW, EDITED, and UNCHANGED knowledge categories with multiple random seeds to verify the near-zero gradient findings for edited knowledge.

2. Apply the EvolvingQA framework to a different knowledge source (e.g., news articles or scientific publications) to test whether the observed continual learning challenges generalize beyond Wikipedia.

3. Conduct human evaluation of a sample of LLM-generated QA pairs to quantify hallucination rates and assess whether automated filtering effectively removes problematic instances.