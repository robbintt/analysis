---
ver: rpa2
title: Levels of AGI for Operationalizing Progress on the Path to AGI
arxiv_id: '2311.02462'
source_url: https://arxiv.org/abs/2311.02462
tags:
- tasks
- level
- levels
- performance
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for classifying Artificial General
  Intelligence (AGI) models based on performance and generality. The authors propose
  a two-dimensional leveling system that considers the depth (performance) and breadth
  (generality) of AI capabilities, defining six levels of AGI ranging from "Emerging
  AGI" to "Artificial Superintelligence (ASI)".
---

# Levels of AGI for Operationalizing Progress on the Path to AGI

## Quick Facts
- arXiv ID: 2311.02462
- Source URL: https://arxiv.org/abs/2311.02462
- Reference count: 29
- This paper introduces a framework for classifying Artificial General Intelligence (AGI) models based on performance and generality.

## Executive Summary
This paper presents a framework for operationalizing progress toward Artificial General Intelligence (AGI) by introducing a two-dimensional leveling system that considers both performance depth and generality breadth. The authors propose six levels of AGI ranging from "Emerging AGI" to "Artificial Superintelligence (ASI)" and emphasize focusing on capabilities rather than processes. The framework aims to provide a common language for comparing models, assessing risks, and measuring progress toward AGI while highlighting the importance of carefully selecting Human-AI Interaction paradigms for responsible deployment of highly capable AI systems.

## Method Summary
The authors analyzed existing definitions of AGI and distilled principles for a useful ontology, then developed a leveled taxonomy based on two dimensions: performance (five levels from Emerging to Superhuman) and generality (Narrow vs. General). They introduced corresponding Levels of Autonomy that are unlocked but not determined by progression through the Levels of AGI. The framework emphasizes cognitive and metacognitive tasks and ecological validity in benchmarking, while acknowledging measurement challenges and the need for a living benchmark that can evolve.

## Key Results
- Proposes a two-dimensional taxonomy (performance x generality) with six levels of AGI capability
- Maps AGI capability levels to autonomy levels and associated risk profiles
- Emphasizes ecological validity and cognitive/metacognitive tasks over traditional AI metrics
- Acknowledges measurement challenges while establishing principles for a living benchmark framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework provides operational clarity by defining AGI through measurable dimensions rather than abstract qualities.
- Mechanism: By establishing a two-dimensional taxonomy (performance x generality) with explicit level criteria, the framework creates a common reference point for comparing models, enabling risk assessment, and guiding development priorities.
- Core assumption: Performance and generality are sufficient and separable dimensions for characterizing AGI-relevant capabilities.
- Evidence anchors:
  - [abstract] "This framework introduces levels of AGI performance, generality, and autonomy, providing a common language to compare models, assess risks, and measure progress along the path to AGI."
  - [section] "We introduce our Levels of AGI ontology, which offers a more nuanced way to define our progress toward AGI by considering generality (either Narrow or General) in tandem with five levels of performance (Emerging, Competent, Expert, Virtuoso, and Superhuman)."
- Break condition: If AGI requires dimensions beyond performance and generality (e.g., embodiment, consciousness) that cannot be captured in this framework, or if the proposed levels prove insufficient for distinguishing meaningful capability differences.

### Mechanism 2
- Claim: The framework addresses practical deployment concerns by linking capability levels to autonomy levels and risk profiles.
- Mechanism: By mapping AGI capability levels to corresponding autonomy levels (from "AI as a Tool" to "AI as an Agent") and identifying associated risks, the framework enables informed decisions about safe and responsible deployment.
- Core assumption: Autonomy levels are meaningfully correlated with capability levels, and specific risk profiles can be associated with each combination.
- Evidence anchors:
  - [section] "We introduced Levels of Autonomy that are unlocked, but not determined by, progression through the Levels of AGI. We illustrated how considering AGI Level jointly with Autonomy Level can provide more nuanced insights into likely risks associated with AI systems."
  - [section] "As we advance along our capability levels toward ASI, new risks are introduced, including misuse risks, alignment risks, and structural risks."
- Break condition: If the correlation between capability and autonomy proves unreliable, or if risk profiles cannot be consistently associated with level combinations.

### Mechanism 3
- Claim: The framework provides a practical path forward by acknowledging measurement challenges while establishing a living benchmark framework.
- Mechanism: By recognizing that comprehensive AGI benchmarking is challenging but establishing principles for a "living benchmark" that can evolve, the framework creates a framework for ongoing measurement that avoids the paralysis of waiting for perfect metrics.
- Core assumption: A benchmark that measures a substantial portion of AGI-relevant capabilities can provide useful operational guidance even if it cannot capture everything.
- Evidence anchors:
  - [section] "We suspect that these latter classes of complex, open-ended tasks, though difficult to benchmark, will have better ecological validity than traditional AI metrics, or than adapted traditional measures of human intelligence."
  - [section] "Determining that something is not an AGI at a given level simply requires identifying several tasks that people can typically do but the system cannot adequately perform."
- Break condition: If the benchmark cannot evolve to capture new capabilities as they emerge, or if the measurement becomes too resource-intensive to be practical.

## Foundational Learning

- Concept: Matrixed taxonomy design
  - Why needed here: Understanding how to structure a taxonomy that captures both depth (performance) and breadth (generality) of capabilities is fundamental to grasping this framework.
  - Quick check question: Why did the authors choose a matrix structure rather than a linear scale for defining AGI levels?

- Concept: Ecological validity in benchmarking
  - Why needed here: The framework emphasizes measuring real-world, valuable tasks rather than artificial metrics, which is central to its approach.
  - Quick check question: How does the framework define "ecological validity" and why is it important for AGI benchmarking?

- Concept: Risk assessment through capability-autonomy mapping
  - Why needed here: Understanding how different combinations of capabilities and autonomy levels create different risk profiles is key to applying this framework.
  - Quick check question: What is the relationship between capability levels and autonomy levels in this framework?

## Architecture Onboarding

- Component map: Capability levels (performance x generality) -> Autonomy levels -> Risk profiles -> Benchmark principles
- Critical path: 1) Establish capability levels based on performance and generality; 2) Map these to autonomy levels; 3) Identify risk profiles for each combination; 4) Develop benchmark criteria that measure these dimensions.
- Design tradeoffs: The framework prioritizes practical operationalization over theoretical completeness, potentially sacrificing some nuance for usability. It also focuses on cognitive tasks rather than physical ones, which may not capture all aspects of intelligence.
- Failure signatures: The framework may fail if capability levels prove difficult to measure consistently, if risk profiles don't align with the proposed mapping, or if benchmark development stalls.
- First 3 experiments:
  1. Apply the framework to a current frontier model (e.g., GPT-4) to identify which capability levels it reaches and what autonomy level would be appropriate.
  2. Map a specific use case (e.g., medical diagnosis) to the framework to identify potential risks at different capability-autonomy combinations.
  3. Propose a small set of benchmark tasks that would be appropriate for testing the "Competent AGI" level according to the framework's principles.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum set of tasks and the proportion of these tasks that must be mastered to achieve a given level of generality in the proposed AGI taxonomy?
- Basis in paper: [explicit] The paper discusses the importance of generality and performance in defining AGI levels but does not specify the exact set of tasks or the proportion needed to achieve each level.
- Why unresolved: Determining the minimum set of tasks and the proportion required for each level is a complex challenge that involves defining the scope of cognitive and metacognitive abilities, ensuring ecological validity, and considering the practical relevance of these tasks.
- What evidence would resolve it: A comprehensive study that defines a set of tasks, benchmarks AI systems against these tasks, and determines the performance thresholds required for each AGI level would provide the necessary evidence.

### Open Question 2
- Question: How should benchmarks for potentially dangerous capabilities (e.g., deception, persuasion, advanced biochemistry) be included in the AGI framework without enabling malicious actors?
- Basis in paper: [explicit] The paper acknowledges the controversy around including dangerous capabilities in benchmarking and suggests that such capabilities should be included to ensure comprehensive evaluation, but also raises concerns about the potential misuse of such benchmarks.
- Why unresolved: Balancing the need for comprehensive benchmarking with the risk of enabling malicious actors is a complex ethical and technical challenge that requires careful consideration of safety measures and regulatory frameworks.
- What evidence would resolve it: Research that develops secure and controlled benchmarking environments for dangerous capabilities, along with policies and guidelines for their responsible use, would help address this issue.

### Open Question 3
- Question: How does the choice of human-AI interaction paradigms influence the risks and benefits associated with different levels of AGI?
- Basis in paper: [explicit] The paper introduces the concept of Levels of Autonomy and discusses how the choice of interaction paradigms can influence the deployment and risks of AGI systems, but does not provide a detailed analysis of the relationship between autonomy levels and specific risks.
- Why unresolved: Understanding the nuanced relationship between interaction paradigms, autonomy levels, and associated risks requires extensive empirical research and analysis of real-world deployment scenarios.
- What evidence would resolve it: Studies that examine the impact of different interaction paradigms on the risks and benefits of AGI systems in various contexts, along with guidelines for selecting appropriate paradigms based on AGI levels, would provide the necessary insights.

## Limitations

- Measurement challenges: The framework acknowledges significant difficulties in establishing clear thresholds between levels and quantifying ecological validity for complex tasks.
- Scope limitations: The framework focuses primarily on cognitive and metacognitive tasks, potentially missing other aspects of intelligence such as emotional intelligence, physical embodiment, or consciousness.
- Risk assessment validity: The correlation between capability levels and autonomy levels may not hold in practice, and the proposed risk profiles may oversimplify complex interactions.

## Confidence

**High Confidence**:
- The two-dimensional taxonomy (performance x generality) provides a useful structure for comparing AI systems
- The framework successfully addresses the need for operational definitions of AGI progress
- The emphasis on ecological validity in benchmarking represents a valuable improvement over traditional metrics

**Medium Confidence**:
- The proposed levels adequately capture meaningful distinctions in AGI capabilities
- The risk profiles associated with different capability-autonomy combinations are accurate and useful
- The framework can be practically implemented with current benchmark development approaches

**Low Confidence**:
- The framework will remain relevant as AI capabilities continue to evolve
- The proposed levels will effectively guide responsible deployment decisions
- The benchmark principles will lead to actionable measurement approaches

## Next Checks

1. **Framework Application Test**: Apply the framework to a current frontier model (e.g., GPT-4 or Claude) to assess which capability levels it reaches and determine appropriate autonomy levels. Document any ambiguities or measurement challenges encountered.

2. **Risk Profile Validation**: Select three specific use cases (e.g., medical diagnosis, financial advising, autonomous vehicles) and map them through the framework to identify potential risks at different capability-autonomy combinations. Validate these risk assessments with domain experts.

3. **Benchmark Development Pilot**: Design and pilot test a small set of benchmark tasks appropriate for testing "Competent AGI" level according to the framework's principles. Evaluate the feasibility, reliability, and ecological validity of these benchmarks.