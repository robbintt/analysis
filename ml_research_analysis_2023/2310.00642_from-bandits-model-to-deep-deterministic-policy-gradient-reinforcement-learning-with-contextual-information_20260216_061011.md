---
ver: rpa2
title: From Bandits Model to Deep Deterministic Policy Gradient, Reinforcement Learning
  with Contextual Information
arxiv_id: '2310.00642'
source_url: https://arxiv.org/abs/2310.00642
tags:
- contextual
- algorithm
- distribution
- reward
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of sequential decision-making
  in financial markets, where traditional reinforcement learning methods suffer from
  high resource consumption and slow convergence to optimal solutions. To overcome
  this, the authors propose two approaches: contextual Thompson sampling with asymmetric
  alpha-stable distribution, and a modified deep deterministic policy gradient (DDPG)
  algorithm incorporating the Constant Proportion Portfolio Insurance (CPPI) strategy.'
---

# From Bandits Model to Deep Deterministic Policy Gradient, Reinforcement Learning with Contextual Information

## Quick Facts
- arXiv ID: 2310.00642
- Source URL: https://arxiv.org/abs/2310.00642
- Reference count: 33
- The paper proposes two methods: contextual Thompson sampling with asymmetric alpha-stable distributions and CPPI-integrated DDPG for financial sequential decision-making, showing improved convergence and risk-adjusted returns.

## Executive Summary
This paper addresses sequential decision-making in financial markets by proposing two novel reinforcement learning approaches that incorporate contextual information. The first method extends Thompson sampling with contextual features and asymmetric alpha-stable distributions to better model heavy-tailed financial returns. The second method integrates the Constant Proportion Portfolio Insurance (CPPI) strategy into the Deep Deterministic Policy Gradient (DDPG) algorithm to accelerate learning while maintaining risk controls. Both approaches are evaluated on stock market data, demonstrating superior performance compared to traditional methods in terms of convergence speed, profitability, and risk-adjusted returns.

## Method Summary
The paper presents two complementary approaches for sequential decision-making in financial markets. The first approach, contextual Thompson sampling with asymmetric alpha-stable distributions, incorporates contextual information into Bayesian posterior updates while modeling reward distributions using heavy-tailed distributions that better capture financial market behavior. The second approach modifies the DDPG algorithm by integrating CPPI risk management principles, using CPPI's protection floor and risk budget concepts to constrain the policy output and guide exploration. Both methods leverage contextual information - financial ratios, market indicators, and historical data - to improve decision-making and accelerate learning convergence.

## Key Results
- Contextual Thompson sampling achieves 9.68% annual return with improved convergence speed over traditional Thompson sampling
- CPPI-DDPG demonstrates superior risk-adjusted performance with better Sharpe ratios and lower maximum drawdowns compared to standard DDPG
- Both methods outperform traditional approaches in handling heavy-tailed reward distributions characteristic of financial markets
- Contextual information integration significantly reduces uncertainty in reward estimation and improves posterior update accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextual information accelerates Thompson sampling convergence by reducing uncertainty in reward estimation
- Mechanism: By incorporating contextual features into the reward model, the algorithm can condition estimates on relevant information, effectively reducing the dimensionality of the exploration space and improving the precision of posterior updates
- Core assumption: The relationship between context and expected reward is either linear or semi-parametric and can be captured by the chosen model structure
- Evidence anchors:
  - [abstract] "contextual Thompson sampling algorithm uses contextual information and asymmetric alpha-stable distributions to model reward distributions more accurately"
  - [section] "The key advantages of linear contextual bandits is their simplicity and efficiency... they can operate with incomplete information, which is useful when data is missing or incomplete"
  - [corpus] Weak - no direct corpus papers discussing contextual acceleration in Thompson sampling

### Mechanism 2
- Claim: CPPI integration into DDPG accelerates learning by providing risk-aware constraints that guide exploration
- Mechanism: CPPI strategy defines a protection floor and risk budget, which is translated into constraints on the DDPG policy output. This reduces the effective action space and prevents the agent from taking extreme actions that would be harmful in early learning stages
- Core assumption: The CPPI parameters (floor, multiple) are reasonably calibrated to the market conditions and do not overly restrict profitable exploration
- Evidence anchors:
  - [abstract] "the CPPI-DDPG algorithm leverages CPPI's risk management principles to accelerate learning"
  - [section] "In order to investigate strategic trading in quantitative markets, we merged the earlier financial trading strategy known as constant proportion portfolio insurance (CPPI) into deep deterministic policy gradient (DDPG)"
  - [corpus] Weak - no direct corpus papers discussing CPPI-DDPG hybrid approaches

### Mechanism 3
- Claim: Asymmetric alpha-stable distributions better model heavy-tailed reward distributions in financial markets than symmetric distributions
- Mechanism: By allowing skewness (β parameter) and heavy tails (α parameter), the model can capture both the magnitude and direction of extreme events, leading to more accurate posterior updates and better exploration-exploitation trade-offs
- Core assumption: Financial market returns exhibit asymmetric heavy-tailed behavior that cannot be adequately captured by Gaussian or symmetric alpha-stable distributions
- Evidence anchors:
  - [section] "The alpha-stable distribution is an important non-Gaussian distribution that is often used to model both impulsive and skewed data... In finance, the α-stable distribution is used to model fluctuations in asset prices and returns"
  - [section] "Compared with symmetric α-Thompson algorithm, asymmetric α-Thompson algorithm can not only cover the asymmetry in data... but also greatly improve the accuracy of reward distribution assumptions"
  - [corpus] Weak - no direct corpus papers discussing asymmetric alpha-stable distributions in reinforcement learning

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: Provides the formal framework for sequential decision-making under uncertainty, which is the foundation for both Thompson sampling and reinforcement learning approaches
  - Quick check question: What are the five components of an MDP tuple?

- Concept: Bayesian inference and posterior updating
  - Why needed here: Essential for understanding how Thompson sampling updates beliefs about reward distributions based on observed data
  - Quick check question: How does Bayes' rule update a prior distribution to a posterior distribution?

- Concept: Exploration-exploitation trade-off
  - Why needed here: Central to understanding why contextual information and risk constraints can accelerate learning - they help balance exploration and exploitation more effectively
  - Quick check question: What is the fundamental tension between exploration and exploitation in sequential decision-making?

## Architecture Onboarding

- Component map: Data Preprocessing -> Contextual Feature Extraction -> Thompson Sampling Module (Contextual, Asymmetric Alpha-Stable) OR DDPG Module (Actor-Critic with CPPI Integration) -> Action Selection -> Environment Interaction -> Reward Observation -> Posterior Update / Critic Update -> Policy Improvement
- Critical path: Data → Feature extraction → Contextual Thompson sampling / CPPI-DDPG → Action selection → Environment interaction → Reward → Posterior update / Critic update → Policy improvement
- Design tradeoffs: Model complexity vs. sample efficiency (asymmetric alpha-stable vs. Gaussian), risk control vs. profit potential (CPPI parameters), contextual information richness vs. computational overhead
- Failure signatures: High regret bounds indicating poor learning, unstable Sharpe ratios indicating excessive risk-taking, slow convergence indicating inadequate exploration or overly restrictive constraints
- First 3 experiments:
  1. Synthetic data experiment comparing symmetric vs. asymmetric alpha-stable Thompson sampling with and without contextual information
  2. Portfolio management experiment comparing CPPI-DDPG vs. standard DDPG with varying CPPI parameters
  3. Adversarial contextual bandit experiment comparing MDP-contextual asymmetric alpha-Thompson sampling vs. standard contextual Thompson sampling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of asymmetric alpha-Thompson sampling with contextual information compare to deep reinforcement learning methods like DDPG in long-term quantitative trading scenarios?
- Basis in paper: [inferred] The paper shows that DDPG outperforms other methods in long-term performance, but does not directly compare it with asymmetric alpha-Thompson sampling with contextual information in long-term scenarios
- Why unresolved: The paper only compares short-term performance of asymmetric alpha-Thompson sampling with contextual information to DDPG, and long-term performance of DDPG to other methods, but does not provide a direct comparison between the two methods in long-term scenarios
- What evidence would resolve it: A direct comparison of the long-term performance of asymmetric alpha-Thompson sampling with contextual information and DDPG in quantitative trading scenarios, using metrics such as annual return, Sharpe ratio, and maximum drawdown

### Open Question 2
- Question: How does the inclusion of contextual information impact the performance of Thompson sampling algorithms in financial applications?
- Basis in paper: [explicit] The paper states that "the context information can have a great impact on the regret bound, and help to extract the information faster" for synthetic asymmetric data, but also mentions that "it is more unstable than the model without context information"
- Why unresolved: The paper provides some evidence of the impact of contextual information on Thompson sampling algorithms, but does not provide a comprehensive analysis of its effects across different types of financial data and scenarios
- What evidence would resolve it: A systematic study of the impact of contextual information on Thompson sampling algorithms across various financial applications, including stock prices, recommendation data, and synthetic data, using metrics such as regret bound, annual return, and Sharpe ratio

### Open Question 3
- Question: How does the performance of adversarial contextual bandits compare to other reinforcement learning methods in complex financial environments?
- Basis in paper: [explicit] The paper states that "the AC-TS algorithm suitable for MDP, although not performing as well as reinforcement learning algorithms such as DQL and QL, is superior to CB-TS algorithm that does not provide feedback with action changes"
- Why unresolved: The paper provides some evidence of the performance of adversarial contextual bandits compared to other methods, but does not provide a comprehensive analysis of its performance in various financial environments and scenarios
- What evidence would resolve it: A systematic study of the performance of adversarial contextual bandits compared to other reinforcement learning methods, such as DQL, QL, and DDPG, across various financial environments and scenarios, using metrics such as annual return, Sharpe ratio, and maximum drawdown

## Limitations

- Limited empirical validation across diverse market conditions and asset classes, with evaluation focused primarily on a single stock pair (HSI and AIA)
- Lack of comprehensive comparison against state-of-the-art contextual bandit algorithms and modern RL methods like SAC and PPO
- Heavy reliance on assumptions about market behavior (heavy-tailed distributions, linear contextual relationships) that are asserted but not rigorously tested across different market regimes

## Confidence

- **High confidence**: The mathematical formulation of asymmetric alpha-stable distributions and their theoretical properties; the CPPI risk management framework and its integration principles with DDPG
- **Medium confidence**: The convergence acceleration claims for contextual Thompson sampling, as these are supported by experimental results but lack ablation studies to isolate the effects of contextual information vs. distribution assumptions
- **Low confidence**: The generalizability of results to other financial markets and asset classes, given the narrow scope of empirical validation

## Next Checks

1. Conduct ablation studies comparing contextual Thompson sampling with and without contextual features, and with different distribution assumptions (Gaussian vs. asymmetric alpha-stable) to isolate the contribution of each component

2. Expand CPPI-DDPG evaluation to multiple asset classes (commodities, forex, cryptocurrencies) and different market conditions (bull vs. bear markets) to assess robustness

3. Implement and compare against state-of-the-art contextual bandit algorithms (LinUCB, NeuralUCB) and reinforcement learning baselines (SAC, PPO) to establish relative performance in the same experimental framework