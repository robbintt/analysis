---
ver: rpa2
title: Sparse Decentralized Federated Learning
arxiv_id: '2308.16671'
source_url: https://arxiv.org/abs/2308.16671
tags:
- node
- learning
- communication
- algorithm
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses communication and computational inefficiencies
  in Decentralized Federated Learning (DFL) by proposing Sparse Decentralized Federated
  Learning (SDFL) with the CEPS algorithm. The core method introduces sparsity constraints
  on shared models and leverages one-bit compressive sensing (1BCS) to transmit compressed
  one-bit information between selected neighbor nodes, significantly improving communication
  efficiency.
---

# Sparse Decentralized Federated Learning

## Quick Facts
- arXiv ID: 2308.16671
- Source URL: https://arxiv.org/abs/2308.16671
- Reference count: 40
- The paper addresses communication and computational inefficiencies in Decentralized Federated Learning (DFL) by proposing Sparse Decentralized Federated Learning (SDFL) with the CEPS algorithm.

## Executive Summary
The paper addresses communication and computational inefficiencies in Decentralized Federated Learning (DFL) by proposing Sparse Decentralized Federated Learning (SDFL) with the CEPS algorithm. The core method introduces sparsity constraints on shared models and leverages one-bit compressive sensing (1BCS) to transmit compressed one-bit information between selected neighbor nodes, significantly improving communication efficiency. The algorithm integrates differential privacy for trustworthiness and employs theoretical guarantees for convergence and privacy. Key results demonstrate that CEPS achieves effective communication and computation efficiency while maintaining high training accuracy.

## Method Summary
The CEPS algorithm introduces sparsity constraints on the shared model parameters and uses one-bit compressive sensing (1BCS) for communication between neighbor nodes. Each node communicates with neighbors only at specific steps, transmitting one-bit signals and a scalar rather than full model updates. The method employs closed-form solutions for subproblems to improve computational efficiency and integrates differential privacy mechanisms. The algorithm updates local parameters at designated steps while reducing the number of communication rounds and selectively picking partial neighbor nodes to participate in training, enhancing robustness against stragglers.

## Key Results
- CEPS achieves effective communication efficiency by reducing communication rounds and using compressed one-bit information transmission
- The algorithm maintains high training accuracy while significantly reducing communication costs compared to perfect communication methods
- Computational efficiency is improved through closed-form solutions and reduced frequency of complex computations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sparsity constraint on the model enables one-bit compressive sensing (1BCS) to be used for communication.
- Mechanism: By constraining the model to have at most s non-zero entries, the 1BCS technique can recover the model from one-bit measurements with high probability when the number of measurements exceeds a certain threshold.
- Core assumption: The model is sparse enough (s << n) and the measurement matrix is appropriately constructed.
- Evidence anchors: The sparsity constraint facilitates the use of one-bit compressive sensing (1BCS), allowing transmission of one-bit information among neighbour nodes. It has been shown that solution wj can be decoded successfully with high probability if Φi is a randomly generated Gaussian matrix and di exceeds a certain threshold.

### Mechanism 2
- Claim: Communication efficiency is achieved by reducing the number of communication rounds and the size of transmitted data.
- Mechanism: Each node communicates with its neighbors only at specific steps (k ∈ Ki = {0, κi, 2κi, ...}) instead of every step, and only transmits one-bit signals and a scalar during communication.
- Core assumption: The algorithm can still converge with less frequent communication and with compressed data.
- Evidence anchors: Communication between neighbour nodes occurs only at certain steps, reducing the number of communication rounds. Taking di = n = 104 as an example, transmitting a dense vector w ∈ Rn with double-precision floating entries directly between two neighbours would need 64 × 104 bits. In contrast, Algorithm 2 only transmits (64 + 104)-bit content.

### Mechanism 3
- Claim: Computational efficiency is achieved by computing complex items (e.g., gradients) less frequently and using closed-form solutions for subproblems.
- Mechanism: Complex elements are computed at designated steps and remain unchanged during other steps. Subproblems of the inexact ADMM are solved inexactly using closed-form solutions.
- Core assumption: The closed-form solutions provide a good approximation to the true solutions, and the infrequent computation of complex items does not significantly impact convergence.
- Evidence anchors: Complex items are computed only once for several consecutive steps and subproblems are solved inexactly using closed-form solutions, resulting in high computational efficiency. While the computation for wk+1 i is quite cheap, with computational complexity about O(n|Mk+1 i |+ s log(s)).

## Foundational Learning

- Concept: Decentralized Federated Learning (DFL)
  - Why needed here: The paper proposes a new algorithm for DFL, so understanding the basics of DFL is crucial.
  - Quick check question: What is the main difference between centralized and decentralized federated learning?

- Concept: Sparsity and compressive sensing
  - Why needed here: The paper uses sparsity constraints and one-bit compressive sensing to improve communication efficiency.
  - Quick check question: What is the main advantage of using sparse models in federated learning?

- Concept: Alternating Direction Method of Multipliers (ADMM)
  - Why needed here: The paper builds upon the framework of inexact ADMM to solve the optimization problem.
  - Quick check question: What is the main idea behind the ADMM algorithm?

## Architecture Onboarding

- Component map: Nodes -> Local computation -> Communication with neighbors -> Aggregation -> Local parameter update
- Critical path: Local computation → Communication with neighbors → Aggregation of received information → Local parameter update
- Design tradeoffs:
  - Sparsity level (s): Higher sparsity leads to better communication efficiency but may impact model accuracy.
  - Communication frequency (κi): Less frequent communication improves efficiency but may slow down convergence.
  - Measurement matrix (Φi): The choice of Φi affects the success of 1BCS recovery.
- Failure signatures:
  - Poor model accuracy: May indicate insufficient sparsity or inaccurate 1BCS recovery.
  - Slow convergence: May indicate too infrequent communication or inaccurate closed-form solutions.
  - High computational cost: May indicate inefficient implementation of closed-form solutions.
- First 3 experiments:
  1. Verify the 1BCS recovery: Generate a sparse model, compress it using 1BCS, and attempt to recover it. Measure the recovery error as a function of the sparsity level and the number of measurements.
  2. Evaluate communication efficiency: Implement the algorithm with different sparsity levels and communication frequencies. Measure the communication cost and compare it to a baseline algorithm.
  3. Assess computational efficiency: Implement the algorithm with different choices of measurement matrices and closed-form solutions. Measure the computational cost and compare it to a baseline algorithm.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology and results suggest several areas for future research regarding scalability, non-convex optimization, and compression trade-offs.

## Limitations
- The algorithm's performance depends critically on appropriate parameter tuning (sparsity level, communication frequency, measurement matrix) which is not fully specified
- The computational efficiency claims lack quantitative comparison to baseline methods
- The paper only demonstrates performance on convex problems (linear regression) without extending to non-convex deep learning applications

## Confidence

**High Confidence**: The fundamental mechanism of using sparsity constraints to enable one-bit compressive sensing is theoretically well-established. The paper correctly identifies that sparse models can be recovered from fewer measurements, and the integration with DFL provides a novel application.

**Medium Confidence**: The communication efficiency claims are supported by theoretical analysis but depend heavily on parameter tuning. The claim that communication rounds are reduced is clearly demonstrated, but the practical impact on convergence speed and accuracy requires careful parameter selection.

**Low Confidence**: The computational efficiency claims lack quantitative comparison to baseline methods. While the paper argues that closed-form solutions reduce computational cost, there's no empirical evidence comparing the total computational cost against standard DFL approaches.

## Next Checks

1. Parameter Sensitivity Analysis: Systematically vary the sparsity level s, measurement dimension di, and communication frequency κi to identify the operating regime where the algorithm maintains accuracy while achieving efficiency gains.

2. Computational Cost Breakdown: Measure and compare the wall-clock time for (a) standard DFL with dense communication, (b) SDFL with GPSP decoding, and (c) the total time including all local computations. This would validate whether the claimed computational efficiency is realized in practice.

3. Convergence Robustness: Test the algorithm on non-IID data distributions and with varying network topologies to verify the claims about robustness to stragglers and data heterogeneity. Specifically, measure convergence rates when neighbor nodes have significantly different data distributions or when some nodes fail to communicate.