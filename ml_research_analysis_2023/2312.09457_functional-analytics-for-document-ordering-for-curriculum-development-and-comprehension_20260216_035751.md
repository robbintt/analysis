---
ver: rpa2
title: Functional Analytics for Document Ordering for Curriculum Development and Comprehension
arxiv_id: '2312.09457'
source_url: https://arxiv.org/abs/2312.09457
tags:
- document
- documents
- similarity
- sequence
- most
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents methods for automatic document ordering to
  improve curriculum development and learning comprehension. It proposes two main
  techniques: (1) document similarity matrices using cosine, Jaccard, and relative
  probability metrics, and (2) entropy-based ordering using LDA topics and Kullback-Leibler
  divergence.'
---

# Functional Analytics for Document Ordering for Curriculum Development and Comprehension

## Quick Facts
- arXiv ID: 2312.09457
- Source URL: https://arxiv.org/abs/2312.09457
- Reference count: 6
- Primary result: Document ordering can be predicted significantly better than random for structured collections like dissertations and textbooks using similarity matrices and entropy-based methods.

## Executive Summary
This paper presents methods for automatic document ordering to improve curriculum development and learning comprehension. The authors propose two main techniques: (1) document similarity matrices using cosine, Jaccard, and relative probability metrics, and (2) entropy-based ordering using LDA topics and Kullback-Leibler divergence. Tested on various document types including textbooks, dissertations, courses, journals, biographies, novels, and Wikipedia articles, the methods showed significant predictability for dissertations (p < 0.05) and some predictability for textbooks, while control documents could not be predicted reliably. Summarized documents (20% Luhn) performed similarly to full documents for ordering purposes.

## Method Summary
The paper proposes two main approaches for document ordering. The first uses similarity matrices built from cosine, Jaccard, and relative probability metrics to capture pairwise document similarities, then iteratively selects documents based on maximum similarity to already-chosen documents. The second approach uses entropy calculations on LDA-generated topic distributions, ordering documents by their entropy values or KL divergence from uniform distributions to reflect topical generality or prerequisite complexity. The methods were tested on various document types including dissertations, textbooks, journals, courses, biographies, novels, and Wikipedia articles, with results compared against random ordering baselines using five normalized metrics.

## Key Results
- Dissertation orders could be predicted significantly better than random (p < 0.05) using all metrics
- Journal and textbook orderings also showed some predictability above random baselines
- Control documents (biographies, novels, Wikipedia) could not be predicted reliably
- Summarized documents (20% Luhn) performed similarly to full documents for ordering purposes
- Entropy-based methods were most effective for ordering test documents, particularly dissertations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Document similarity matrices can reveal latent topical progression and support automatic ordering.
- Mechanism: Cosine, Jaccard, and relative probability metrics compute pairwise document similarities. The similarity matrix is iteratively reduced by selecting the document most similar to the already-selected set (or most similar to the last chosen document) until all documents are ordered. This exploits the assumption that consecutive documents in an intended reading order share higher topical similarity.
- Core assumption: Documents in the intended sequence exhibit decreasing dissimilarity from their neighbors, i.e., topics transition smoothly and the similarity matrix structure mirrors the reading order.
- Evidence anchors:
  - [abstract] "The first uses document similarities through various methods."
  - [section] Step Two: Generate the Sequence describes selecting documents based on maximum similarity to chosen documents.
- Break condition: If documents are largely independent (e.g., journal papers by different authors) or the similarity matrix lacks a clear structure, the ordering will not reliably match the intended sequence.

### Mechanism 2
- Claim: Entropy and Kullback-Leibler divergence on LDA topic distributions can order documents by topical generality or prerequisite complexity.
- Mechanism: LDA generates a topic distribution for each document. Entropy of the topic distribution indicates topical breadth (high entropy = broad introduction). KL divergence measures deviation from a uniform topic distribution (high KL = focused, specialized content). Documents are ordered from high-to-low entropy or KL, or vice versa, to reflect progression from general to specific.
- Core assumption: Introductory material is broader (higher entropy) and later material is more focused (lower entropy), matching the assumed prerequisite chain of knowledge.
- Evidence anchors:
  - [abstract] "The second uses entropy against the backdrop of topics generated through Latent Dirichlet Allocation (LDA)."
  - [section] Sequence Most to Least Entropic and Sequence Most to Least KL-Divergent describe iterative ordering based on entropy and KL divergence.
- Break condition: If entropy does not correlate with topical generality (e.g., a focused introduction vs. a broad advanced chapter), the ordering may misalign with the intended reading order.

### Mechanism 3
- Claim: Summarized documents can stand in for full documents in ordering tasks without significant loss of topical structure.
- Mechanism: Luhn extractive summarization reduces document length while preserving salient terms. Similarity and entropy calculations are performed on the summaries. Because the most important terms are retained, the topical distribution remains largely intact, allowing the same ordering algorithms to apply.
- Core assumption: The top 20% of sentences capture enough of the full document's topical structure to preserve relative similarities and entropy values.
- Evidence anchors:
  - [abstract] "Our results showed that summarized documents were good stand-ins for the complete documents for the purposes of ordering."
  - [section] Variation: Similarity Matrix-based Content Sequencing Method Using Summaries and Variation: Entropy Ordering Method of Summarized Documents describe applying the algorithms to summaries.
- Break condition: If the summary omits key transitions or prerequisite topics, the ordering may diverge from that of the full documents.

## Foundational Learning

- Concept: Cosine similarity as a measure of document similarity.
  - Why needed here: It provides a continuous similarity score used to build the similarity matrix for ordering.
  - Quick check question: Given two documents represented as term frequency vectors a and b, what is the formula for their cosine similarity?
    - Answer: cos(a, b) = (a·b) / (||a|| ||b||)

- Concept: Latent Dirichlet Allocation (LDA) for topic modeling.
  - Why needed here: LDA extracts a set of topics and their probability distributions for each document, which are the basis for entropy and KL divergence calculations.
  - Quick check question: In LDA, what does the Dirichlet prior control?
    - Answer: The Dirichlet prior controls the distribution over topics for each document and the distribution over words for each topic.

- Concept: Kullback-Leibler (KL) divergence as a measure of distribution difference.
  - Why needed here: KL divergence quantifies how much information is lost when approximating a document's topic distribution with a uniform distribution, indicating topical focus.
  - Quick check question: What is the formula for KL divergence between two discrete distributions p and q?
    - Answer: KL(p||q) = Σ p(x) log(p(x)/q(x))

## Architecture Onboarding

- Component map: Input Documents -> Preprocessing (stopword removal, lemmatization) -> (Summarization) -> Topic Modeling (LDA) -> Similarity/Entropy Calculation -> Iterative Ordering -> Evaluation -> Output Order
- Critical path: Preprocessing → (Summarization) → Topic modeling → Similarity/Entropy calculation → Iterative ordering → Evaluation
- Design tradeoffs:
  - Summarization reduces compute but may lose subtle ordering cues.
  - LDA topic count fixed at 20% is a heuristic; optimal count may vary by corpus.
  - Using full documents vs. summaries: full documents capture more detail but are costlier to process.
- Failure signatures:
  - Low p-values across all metrics for a collection: algorithm cannot predict the intended order (e.g., independent articles).
  - High similarity between random and proposed orders: ordering is no better than chance.
  - High variance in metrics: unstable ordering results.
- First 3 experiments:
  1. Run the full pipeline on a dissertation with known chapter order; verify low p-values with all metrics.
  2. Run on a set of journal papers; expect high p-values (no predictable order).
  3. Run on textbook chapters with and without summarization; compare p-values to assess summary fidelity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of topics for LDA when ordering documents of different genres?
- Basis in paper: [explicit] The paper mentions that while 20% of the total number of documents was used as a standardized number of topics, they acknowledge this is a rough approximation and note that "optimizing for the ideal number of topics for the use of entropy calculations is left as a topic for future research."
- Why unresolved: The paper only used a standardized approach (20% of document count) without exploring optimal topic numbers for different document types.
- What evidence would resolve it: Systematic experiments testing various topic numbers (e.g., 5%, 10%, 20%, 50%, 100% of document count) across different genres (dissertations, textbooks, journals, etc.) and measuring ordering accuracy using multiple metrics.

### Open Question 2
- Question: How does the effectiveness of document ordering algorithms vary across different academic departments?
- Basis in paper: [explicit] The paper conducted ANOVA tests showing variations in predictability across 32 departments and found that "Computer Science dissertations are most different from the rest of the departments."
- Why unresolved: While the paper identified variations, it only presented results for one metric (NMWOE) and one ordering method, leaving open questions about how other methods perform across departments.
- What evidence would resolve it: Comprehensive analysis using all ordering methods and metrics across all departments to identify patterns and create department-specific ordering algorithms.

### Open Question 3
- Question: Can Large Language Models (LLMs) improve document ordering accuracy compared to traditional methods?
- Basis in paper: [explicit] The paper explicitly suggests in the "Further Experimentation and Research" section that "it would be prudent in future to experiment with functional comparisons of results from these models."
- Why unresolved: The paper did not test any LLM-based approaches, only traditional methods like cosine similarity, Jaccard similarity, and entropy-based ordering.
- What evidence would resolve it: Comparative experiments testing document ordering using LLMs (like ChatGPT) versus traditional methods across various document types and metrics.

## Limitations
- Limited generalizability to independent document sets like journals where topical progression doesn't follow assumed patterns
- Effectiveness of entropy-based methods relies on correlation between topical breadth and document position, which may not hold across all domains
- The choice of 20% topics and 20% document summarization length appears heuristic without systematic validation
- Relative probability similarity metric is introduced but not extensively validated against cosine and Jaccard methods

## Confidence
- **High confidence**: The core methodology for similarity-based and entropy-based ordering is clearly specified and validated on multiple document types. The statistical significance testing framework (p < 0.05) is appropriate and consistently applied.
- **Medium confidence**: The claim that summarized documents are good stand-ins for full documents is supported but limited to the specific Luhn 20% summarization approach tested. The relative probability similarity metric shows promise but lacks comparative analysis with other metrics.
- **Low confidence**: The generalizability of these methods to highly diverse document collections or domains where topical progression doesn't follow the assumed patterns (general-to-specific or broad-to-focused).

## Next Checks
1. **Cross-domain validation**: Apply the entropy-based ordering to domains where topical progression is known to be non-linear (e.g., reference manuals, encyclopedias) to test the generality of the entropy correlation assumption.
2. **Parameter sensitivity analysis**: Systematically vary the LDA topic count (beyond the fixed 20% heuristic) and summarization ratio to determine optimal parameters for different document types and collection sizes.
3. **Metric comparison study**: Conduct a head-to-head comparison of cosine, Jaccard, and relative probability metrics across diverse document collections to quantify the relative performance and identify scenarios where each metric excels.