---
ver: rpa2
title: Biomedical knowledge graph-optimized prompt generation for large language models
arxiv_id: '2311.17330'
source_url: https://arxiv.org/abs/2311.17330
tags:
- performance
- biomedical
- context
- knowledge
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a token-optimized and robust Knowledge Graph-based
  Retrieval Augmented Generation (KG-RAG) framework that leverages a massive biomedical
  knowledge graph (SPOKE) with large language models (LLMs) such as Llama-2-13b, GPT-3.5-Turbo
  and GPT-4 to generate meaningful biomedical text rooted in established knowledge.
  The framework uses minimal graph schema for context extraction and employs embedding
  methods for context pruning, resulting in more than 50% reduction in token consumption
  without compromising accuracy.
---

# Biomedical knowledge graph-optimized prompt generation for large language models

## Quick Facts
- arXiv ID: 2311.17330
- Source URL: https://arxiv.org/abs/2311.17330
- Reference count: 0
- Primary result: KG-RAG framework achieves >50% token reduction while improving LLM accuracy on biomedical questions

## Executive Summary
This paper introduces a Knowledge Graph-based Retrieval Augmented Generation (KG-RAG) framework that integrates the SPOKE biomedical knowledge graph with large language models like Llama-2-13b, GPT-3.5-Turbo, and GPT-4. The framework uses minimal graph schema for context extraction and embedding-based pruning to reduce token consumption by over 50% while maintaining accuracy. KG-RAG significantly enhances LLM performance across diverse biomedical prompts, achieving a 71% improvement on multiple-choice questions with Llama-2 and demonstrating effectiveness with both open-source and proprietary models.

## Method Summary
The KG-RAG framework combines explicit knowledge from the SPOKE biomedical knowledge graph with implicit knowledge from LLMs through a token-optimized retrieval-augmented generation process. It extracts relevant biomedical context using minimal graph schema, prunes this context using embedding methods to reduce token consumption, and integrates the enriched prompt with the LLM. The system is task-agnostic and was tested on various biomedical question types including one-hop and two-hop prompts, drug repurposing queries, true/false questions, and multiple-choice questions across different model sizes.

## Key Results
- KG-RAG achieved >50% reduction in token consumption without compromising accuracy
- 71% performance boost for Llama-2 on multiple-choice questions
- Consistent performance enhancement across Llama-2-13b, GPT-3.5-Turbo, and GPT-4 models
- Effective across diverse biomedical prompt types including one-hop, two-hop, drug repurposing, and multiple-choice questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The KG-RAG framework achieves more than 50% reduction in token consumption while maintaining accuracy by using minimal graph schema for context extraction and embedding-based context pruning.
- Mechanism: By leveraging a minimal graph schema, the system extracts only the most relevant Subject-Predicate-Object triples from SPOKE, reducing unnecessary information. Embedding-based context pruning then filters these triples using cosine similarity to retain only the most relevant context for the prompt, further reducing token usage.
- Core assumption: That the embedding similarity threshold (75th percentile with minimum 0.5) effectively identifies the most relevant context without losing accuracy.
- Evidence anchors:
  - [abstract] "utilizes minimal graph schema for context extraction and uses embedding methods for context pruning"
  - [section] "By leveraging a minimal graph schema, the system extracts only the most relevant Subject-Predicate-Object triples from SPOKE"
  - [corpus] Weak evidence. No direct mention of token reduction mechanisms in neighbor papers.
- Break condition: If the embedding similarity threshold is set too high or too low, the system may either lose accuracy or fail to reduce token consumption sufficiently.

### Mechanism 2
- Claim: The KG-RAG framework improves LLM performance by combining explicit knowledge from the knowledge graph with implicit knowledge from the LLM, leading to more accurate and knowledge-grounded responses.
- Mechanism: The framework retrieves relevant biomedical context from SPOKE and integrates it with the LLM's pre-existing knowledge. This combination allows the LLM to generate responses that are not only linguistically coherent but also factually accurate and grounded in established biomedical knowledge.
- Core assumption: That the integration of explicit and implicit knowledge leads to a synergistic effect, enhancing the LLM's ability to generate accurate responses.
- Evidence anchors:
  - [abstract] "combines explicit and implicit knowledge of KG and LLM in a token-optimized fashion"
  - [section] "We believe that this performance improvement arises from the fusion of the explicit knowledge from the KG and the implicit knowledge from the LLM"
  - [corpus] Weak evidence. Neighbor papers discuss KG-RAG but do not provide direct evidence of the synergistic effect on accuracy.
- Break condition: If the knowledge graph contains outdated or incorrect information, the integration with the LLM's knowledge could lead to inaccurate responses.

### Mechanism 3
- Claim: The KG-RAG framework's task-agnostic nature allows it to handle a wide range of biomedical tasks without the need for additional pre-training or fine-tuning of the LLM.
- Mechanism: By using a general-purpose LLM and a comprehensive biomedical knowledge graph (SPOKE), the framework can adapt to various tasks such as true/false questions, multiple-choice questions, and drug repurposing queries. The system's ability to extract relevant context from SPOKE and integrate it with the LLM's knowledge allows it to perform well across different task types.
- Core assumption: That the general-purpose LLM, when combined with the comprehensive SPOKE knowledge graph, has sufficient knowledge to handle a wide range of biomedical tasks without task-specific training.
- Evidence anchors:
  - [abstract] "task-agnostic nature of our approach"
  - [section] "In our analysis, we found an enhancement in LLM performance as a function of the model size in terms of the number of parameters"
  - [corpus] Weak evidence. Neighbor papers discuss task-specific applications of KG-RAG but do not provide direct evidence of task-agnostic performance.
- Break condition: If the tasks require highly specialized knowledge that is not covered by the general-purpose LLM or the SPOKE knowledge graph, the framework may not perform well without additional task-specific training.

## Foundational Learning

- Concept: Knowledge Graphs (KGs)
  - Why needed here: Understanding the structure and use of KGs is crucial for comprehending how the KG-RAG framework retrieves and integrates biomedical knowledge.
  - Quick check question: What are the main components of a knowledge graph, and how are they used in the KG-RAG framework?
- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG is the foundational technique used in the KG-RAG framework to enhance the LLM's responses with external knowledge.
  - Quick check question: How does RAG differ from traditional fine-tuning approaches, and what are its advantages?
- Concept: Embedding Models
  - Why needed here: Embedding models are used in the KG-RAG framework for context pruning and similarity matching, which are essential for reducing token consumption and improving accuracy.
  - Quick check question: What is the role of embedding models in the KG-RAG framework, and how do they contribute to context pruning?

## Architecture Onboarding

- Component map: SPOKE KG -> Context Extraction Module -> Embedding-based Pruning Module -> LLM Integration
- Critical path: Context extraction from SPOKE -> Embedding-based pruning -> LLM integration for response generation
- Design tradeoffs: Balancing comprehensiveness of SPOKE KG against computational efficiency of context extraction and pruning
- Failure signatures: Inaccurate responses, increased token consumption, inability to handle certain task types
- First 3 experiments:
  1. Test the context extraction module with a small subset of SPOKE to ensure it retrieves relevant information accurately.
  2. Evaluate the embedding-based context pruning module to verify that it effectively reduces token consumption without losing accuracy.
  3. Integrate the context extraction and pruning modules with the LLM and test the framework on a simple biomedical task to assess overall performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the KG-RAG framework maintain its performance advantage when applied to entirely different biomedical domains beyond diseases, such as drug-drug interactions or protein-protein interactions?
- Basis in paper: [explicit] The paper states current limitations to disease-centered questions and suggests future expansion to all biomedical concepts in SPOKE.
- Why unresolved: The framework was only tested on disease-related prompts; performance on other biomedical entity types remains unknown.
- What evidence would resolve it: Empirical evaluation of KG-RAG on prompts involving drugs, proteins, or other biomedical entities, comparing accuracy to current baselines.

### Open Question 2
- Question: How does the performance of KG-RAG scale with increasing complexity of knowledge graph queries, such as four-hop or higher-order relationships?
- Basis in paper: [explicit] The paper mentions higher-order relations could grow exponentially and affect token space usage, but does not test beyond two-hop prompts.
- Why unresolved: Only one-hop and two-hop prompts were evaluated; scalability to more complex queries is untested.
- What evidence would resolve it: Benchmarking KG-RAG on datasets with multi-hop queries (three or more) while monitoring accuracy and token efficiency.

### Open Question 3
- Question: What is the impact of temporal drift in LLM performance on KG-RAG's effectiveness, particularly when using GPT-4 across different model versions?
- Basis in paper: [explicit] The paper references a study showing GPT-4's instruction-following behavior changed over time, potentially affecting performance.
- Why unresolved: The paper does not analyze KG-RAG's robustness to such temporal changes in LLM behavior.
- What evidence would resolve it: Longitudinal studies comparing KG-RAG performance with different GPT-4 versions across multiple time points.

## Limitations

- The framework's performance depends heavily on the quality and completeness of the SPOKE knowledge graph
- Evaluation was primarily conducted on multiple-choice and true/false questions, with limited testing on complex reasoning tasks
- The 50% token reduction claim lacks detailed methodology and breakdown across different prompt types

## Confidence

- **High confidence**: The framework's ability to improve accuracy over baseline LLMs (71% improvement on MCQ) - supported by multiple datasets and cross-validated with different model sizes
- **Medium confidence**: The 50% token reduction claim - based on internal measurements but lacking detailed methodology
- **Medium confidence**: Task-agnostic nature - demonstrated across multiple task types but limited to specific biomedical question formats

## Next Checks

1. Conduct ablation studies to isolate the individual contributions of context extraction, pruning, and LLM integration to overall performance
2. Test the framework on open-ended biomedical questions and clinical decision support scenarios to evaluate real-world applicability
3. Implement systematic evaluation of token consumption across different biomedical domains and prompt complexities to verify the 50% reduction claim under various conditions