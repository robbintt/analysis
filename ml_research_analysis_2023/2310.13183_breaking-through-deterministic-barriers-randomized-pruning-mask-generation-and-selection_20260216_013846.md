---
ver: rpa2
title: 'Breaking through Deterministic Barriers: Randomized Pruning Mask Generation
  and Selection'
arxiv_id: '2310.13183'
source_url: https://arxiv.org/abs/2310.13183
tags:
- pruning
- mask
- randomness
- strategy
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel pruning strategy that generates and
  selects multiple pruning masks in a random but controllable way. It addresses the
  problem that traditional deterministic pruning relies solely on magnitude and lacks
  diversity.
---

# Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection

## Quick Facts
- arXiv ID: 2310.13183
- Source URL: https://arxiv.org/abs/2310.13183
- Reference count: 34
- This paper proposes a novel pruning strategy that generates and selects multiple pruning masks in a random but controllable way, achieving state-of-the-art results on GLUE benchmark at high sparsity levels.

## Executive Summary
This paper introduces a novel model pruning strategy that breaks away from traditional deterministic pruning methods by incorporating controlled randomness in mask generation. The key innovation is sampling from a multinomial distribution based on weight magnitudes to generate multiple candidate pruning masks, followed by a selection strategy to choose the optimal mask. This approach addresses the limitation of deterministic magnitude-based pruning, which may miss locally optimal configurations due to its rigid threshold-based approach.

The proposed method demonstrates significant improvements over traditional Iterative Magnitude Pruning (IMP), particularly at high sparsity levels on the GLUE benchmark. The authors introduce three key components: randomized mask generation with controllable randomness, a Mask Candidate Selection Strategy (MCSS) to ensure beneficial randomness, and an Early Mask Evaluation Pipeline (EMEP) for efficiency. Experimental results show that this approach outperforms other pruning techniques while maintaining competitive accuracy.

## Method Summary
The method generates multiple pruning masks through multinomial distribution sampling of weight magnitudes, introducing controlled randomness into the pruning process. At each pruning stage, N candidate masks are created and evaluated using an early evaluation strategy that fine-tunes each mask for one epoch with a large learning rate. The best-performing mask is selected through MCSS and used for the next training stage, with knowledge distillation maintaining model accuracy throughout the process.

## Key Results
- Achieves state-of-the-art results on GLUE benchmark at high sparsity levels
- Outperforms traditional deterministic pruning methods, especially at 16x sparsity
- Maintains competitive accuracy while significantly reducing model size
- Demonstrates effectiveness across multiple GLUE tasks (MNLI, QQP, QNLI, SST-2, CoLA, STS-B, MRPC)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing randomness into mask generation expands the search space for locally optimal pruning masks, leading to better model accuracy than deterministic methods.
- Mechanism: The method samples from a multinomial distribution based on weight magnitudes, generating multiple candidate masks. By ensemble-averaging these masks and selecting the top-k elements, the search space is expanded while controlling randomness.
- Core assumption: Weights with similar magnitudes near the pruning threshold may have different contributions to model accuracy, and deterministic pruning may miss better configurations.
- Evidence anchors:
  - [abstract] "Instead, in this paper, we propose a model pruning strategy that first generates several pruning masks in a designed random way."
  - [section 3.2.1] "Different from the deterministic mask generation, we seek to infuse controllable randomness into this process. In essence, our approach is to sample the retained elements from a multinomial distribution without replacement."
  - [corpus] Weak evidence: related works focus on semi-structured pruning or score-space methods, not directly on randomized mask generation.

### Mechanism 2
- Claim: Mask Candidate Selection Strategy (MCSS) ensures introduced randomness always guides pruning in a beneficial direction by selecting the best mask from candidates.
- Mechanism: At each pruning stage, N candidate masks are generated and evaluated. The best-performing mask on validation data is selected for the next stage, with deterministic mask as fallback.
- Core assumption: Even with randomness, there exists at least one mask among candidates that improves or matches deterministic performance.
- Evidence anchors:
  - [abstract] "Subsequently, along with an effective mask-selection rule, the optimal mask is chosen from the pool of mask candidates."
  - [section 3.3.1] "To address this, we propose Mask Candidate Selection Strategy (MCSS) to ensure the introduced randomness always guides the model optimization in a beneficial direction."
  - [corpus] Weak evidence: adversarial pruning and subnetwork annealing exist but do not use candidate selection for randomness control.

### Mechanism 3
- Claim: Early Mask Evaluation Pipeline (EMEP) accelerates mask selection by reducing computational cost without sacrificing mask quality.
- Mechanism: Each candidate mask is fine-tuned for only one epoch with a large learning rate, and the one with highest validation score is selected. Weights and learning rate are then restored before final training.
- Core assumption: One epoch of high learning rate fine-tuning provides sufficient signal to distinguish good masks from bad ones.
- Evidence anchors:
  - [abstract] "To further enhance efficiency, we introduce an early mask evaluation strategy, mitigating the overhead associated with training multiple masks."
  - [section 3.3.2] "Specifically, we only fine-tune the model one epoch with a large learning rate for each candidate mask."
  - [corpus] Weak evidence: no direct mention of one-epoch evaluation in related works.

## Foundational Learning

- Concept: Multinomial distribution sampling
  - Why needed here: Enables controlled randomness in selecting weights to retain during pruning
  - Quick check question: If we have 1000 weights and want to keep 500, what distribution do we sample from to introduce randomness while biasing toward larger magnitudes?

- Concept: Pruning schedule design
  - Why needed here: Determines how sparsity increases across stages and how much randomness to introduce at each stage
  - Quick check question: Why might a decreasing randomness schedule (less randomness in later stages) be beneficial compared to increasing randomness?

- Concept: Knowledge distillation in pruning
  - Why needed here: Helps maintain model accuracy when weights are removed by transferring knowledge from larger models
  - Quick check question: What specific knowledge is distilled from teacher to student in this pruning framework?

## Architecture Onboarding

- Component map:
  Dense model training -> Mask generation pipeline -> Mask candidate selection -> Early evaluation -> Iterative pruning with knowledge distillation -> Final sparse model

- Critical path:
  1. Train dense model
  2. For each pruning stage:
     - Generate N candidate masks
     - Fine-tune each for one epoch (EMEP)
     - Select best mask (MCSS)
     - Restore weights and continue training
  3. Output final sparse model

- Design tradeoffs:
  - More mask candidates → better exploration but higher computation
  - Higher sampling ratio → more randomness but risk of poor performance
  - Earlier pruning stages → can tolerate more randomness, later stages need precision

- Failure signatures:
  - Accuracy drops sharply at high sparsity → too much randomness in late stages
  - No improvement over deterministic baseline → MCSS failing to select better masks
  - Training instability → learning rate or distillation parameters misconfigured

- First 3 experiments:
  1. Run deterministic IMP baseline on GLUE with 16x sparsity to establish reference
  2. Implement randomized mask generation with MCSS but without EMEP (full fine-tuning) to validate selection mechanism
  3. Add EMEP and test with varying sampling ratios to find optimal randomness level

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of randomized pruning vary across different model architectures (e.g., ResNet, Transformer, MLP) and datasets?
- Basis in paper: [inferred] The paper only evaluates on BERT-based models and GLUE benchmark tasks, leaving open the question of generalization to other architectures and datasets.
- Why unresolved: The experiments are limited to a specific model type (BERT) and natural language understanding tasks, without exploring other domains or architectures.
- What evidence would resolve it: Conducting experiments on diverse model architectures (CNNs, RNNs, Transformers of different sizes) and datasets (computer vision, speech, reinforcement learning) to compare the effectiveness of randomized pruning across different scenarios.

### Open Question 2
- Question: What is the optimal schedule for introducing randomness during the pruning process?
- Basis in paper: [explicit] The paper proposes two simple schedules (decrease and increase) but acknowledges that a more fine-designed schedule could potentially yield better results.
- Why unresolved: The paper only explores two basic schedules and doesn't investigate more complex or adaptive approaches to controlling randomness throughout pruning.
- What evidence would resolve it: Developing and testing various scheduling strategies (e.g., adaptive based on performance, curriculum learning approaches, or data-driven methods) to determine the most effective way to introduce randomness at different pruning stages.

### Open Question 3
- Question: How does the proposed method scale to extremely large models with billions of parameters?
- Basis in paper: [explicit] The authors explicitly state that their method is not yet adaptable to models with billions of parameters due to computational costs of training under multiple pruning masks.
- Why unresolved: The paper doesn't provide solutions or experimental results for pruning extremely large models, which are becoming increasingly common in modern deep learning.
- What evidence would resolve it: Developing efficient parallel implementations of the randomized pruning process or proposing alternative approaches that reduce the computational overhead for large-scale models, followed by empirical validation on billion-parameter models.

## Limitations
- Limited generalization beyond BERT models to other transformer architectures or vision models
- Computational overhead from training multiple masks, especially problematic for billion-parameter models
- No rigorous ablation studies to quantify individual contributions of MCSS and EMEP components

## Confidence

- **High Confidence**: The paper successfully implements an iterative pruning framework with knowledge distillation on GLUE benchmark. The methodology for generating multiple masks and selecting among them is clearly described and reproducible.

- **Medium Confidence**: The claim that randomized pruning outperforms deterministic magnitude pruning at high sparsity levels is supported by experimental results, though the magnitude of improvement and statistical significance across all GLUE tasks is not fully established.

- **Low Confidence**: The assertion that MCSS "always" guides optimization beneficially and that EMEP provides sufficient signal in one epoch are theoretical claims lacking rigorous ablation studies. The paper does not demonstrate robustness when candidate masks all perform poorly.

## Next Checks
1. **Ablation study of individual components**: Run experiments with (a) randomized mask generation without MCSS (random selection), (b) MCSS with full fine-tuning instead of EMEP, and (c) EMEP alone without mask selection to quantify each component's contribution to performance gains.

2. **Robustness testing of MCSS**: Intentionally create scenarios where most candidate masks perform poorly and verify whether MCSS consistently falls back to the deterministic mask or if it can still identify good masks from a pool of mostly bad candidates.

3. **Cross-architecture generalization**: Apply the same randomized pruning approach to RoBERTa, GPT-2, and vision transformer models on respective benchmarks to validate whether the benefits extend beyond BERT on GLUE tasks.