---
ver: rpa2
title: Spike-based computation using classical recurrent neural networks
arxiv_id: '2306.03623'
source_url: https://arxiv.org/abs/2306.03623
tags:
- networks
- spiking
- neural
- neurons
- spikes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Spiking Recurrent Cell (SRC), a differentiable
  recurrent neural network cell designed for spike-based computation. Unlike traditional
  spiking neural networks, SRC neurons communicate via events while remaining fully
  differentiable, enabling direct backpropagation training.
---

# Spike-based computation using classical recurrent neural networks

## Quick Facts
- arXiv ID: 2306.03623
- Source URL: https://arxiv.org/abs/2306.03623
- Reference count: 24
- Key outcome: Spiking Recurrent Cell (SRC) achieves up to 97.99% accuracy on MNIST while enabling direct backpropagation training of spiking networks

## Executive Summary
This paper introduces the Spiking Recurrent Cell (SRC), a differentiable recurrent neural network cell designed for spike-based computation. Unlike traditional spiking neural networks, SRC neurons communicate via events while remaining fully differentiable, enabling direct backpropagation training. The cell is derived from the Bistable Recurrent Cell and incorporates gating mechanisms and a spike-generation mechanism to produce sparse binary outputs. Experiments demonstrate that SRC-based networks achieve competitive performance on MNIST and its variants, matching or exceeding other non-convolutional spiking neural networks.

## Method Summary
SRCs are derived from the Bistable Recurrent Cell (BRC) and GRU, using leaky integrators for input integration and generating spikes via differentiable equations. The cells maintain cellular memory through separate fast and slow hidden states (h and hs) connected by a sigmoid gate. Input spikes are integrated through leaky integrators with learnable weights and passed through a rescaled hyperbolic tangent to prevent saturation. Training uses backpropagation with cross-entropy loss and the Adam optimizer. The architecture enables both rate-based and latency-based spike encoding of inputs while maintaining gradient flow for learning.

## Key Results
- SRC networks achieve up to 97.99% accuracy on MNIST after 30 epochs
- Performance matches or exceeds other non-convolutional spiking neural networks on MNIST variants
- Deep spiking networks with up to 10 hidden layers can be successfully trained using backpropagation
- SRC enables training of spiking networks without converting to rate-based networks or using surrogate gradients for spikes

## Why This Works (Mechanism)

### Mechanism 1
The Spiking Recurrent Cell maintains differentiability by replacing the non-differentiable spike threshold with a soft gating mechanism that tracks membrane potential via a bistable hidden state. Instead of a hard reset at a fixed threshold, SRC uses a fast hidden state h and a lagging state hs connected by a sigmoid gating zs. The output is obtained via ReLU(h), but during backpropagation zs is bypassed so gradients flow even when h < 0, effectively acting as a surrogate gradient. Core assumption: ReLU can isolate spike events without destroying gradient flow if the gate dynamics are smooth and zs is chosen to be high during depolarization.

### Mechanism 2
The combination of cellular memory (separate h and hs) and bistability creates event-based communication while preserving gradient-based training. h stores the current membrane potential; hs lags behind and is updated with zs(t). When h rises above zero, ReLU emits a spike and hs begins to catch up slowly (zs low), mimicking a refractory period. This produces sparse binary outputs but keeps h differentiable. Core assumption: A bistable cell can generate sparse events without losing the memory capacity of classical RNNs.

### Mechanism 3
Event-based encoding of inputs (rate or latency coding) allows SRC to process neuromorphic data without losing gradient flow. Input spikes are integrated via leaky integrators with learnable weights, then passed through a rescaled tanh. This keeps input currents bounded and smooth, enabling backpropagation through time to update weights even with sparse spike inputs. Core assumption: Leaky integration of spike trains produces continuous signals suitable for gradient-based learning.

## Foundational Learning

### Concept: Leaky integrate-and-fire (LIF) neuron dynamics
- Why needed here: SRC builds on LIF-style membrane potential integration but replaces the hard reset with differentiable gates
- Quick check question: In a LIF model, what happens to the membrane potential V[t] when the input current exceeds the threshold?

### Concept: Gating mechanisms in recurrent neural networks
- Why needed here: SRC uses update and reset gates (z, r) from GRU/BRC to control memory flow and spike generation
- Quick check question: In a GRU cell, what is the effect of the update gate z[t] on the hidden state?

### Concept: Backpropagation through time (BPTT) and gradient clipping
- Why needed here: Training SRC involves unfolding in time and applying BPTT; exploding gradients must be managed
- Quick check question: Why do RNNs suffer from exploding gradients more than feedforward networks?

## Architecture Onboarding

### Component map
Input encoder → Leaky integrator → tanh → gates (z, r, rs, zs) → spike generation (h, hs) → ReLU output → readout integrators

### Critical path
Input → i[t] → x[t] → spike generation → sout[t] → readout integrator → prediction

### Design tradeoffs
- ReLU isolation vs. gradient flow: ReLU blocks small gradients but bypassing zs during backward pass restores them
- Sparsity vs. signal strength: High sparsity reduces energy but may weaken gradient signals
- Bistability strength vs. firing frequency: Strong bistability increases event sparsity but can slow learning

### Failure signatures
- Vanishing gradients: If ReLU blocks too many timesteps, loss stops decreasing
- Saturating inputs: If tanh bounds are exceeded, learning plateaus
- Overactive spiking: If bias bh is too high, neurons fire every timestep and lose event-driven advantage

### First 3 experiments
1. Train a single SRC layer on MNIST with rate encoding; verify convergence and inspect spike train sparsity
2. Add a second SRC layer; measure effect on accuracy and epoch duration; check for vanishing gradients
3. Switch to latency encoding; confirm that readout layer still achieves >90% accuracy and analyze activity distribution

## Open Questions the Paper Calls Out
- How does the performance of SRC-based networks compare to state-of-the-art convolutional SNNs on more complex image classification tasks beyond MNIST variants?
- What is the optimal initialization strategy for synaptic weights and biases in SRC networks to improve training stability and convergence speed?
- How does the addition of feedback connections in SRC networks affect computational power and learning capabilities compared to purely feedforward architectures?
- What is the impact of different spike encoding schemes (beyond rate and latency coding) on SRC network performance and efficiency?
- How does the learnable parameterization of the gating mechanisms and spike-generation dynamics affect SRC network adaptability and task performance?

## Limitations
- Core mechanism of maintaining differentiability through soft gating lacks independent validation from neighboring work
- Hyperparameter details are omitted, creating uncertainty about reproducibility of reported results
- Method's effectiveness on more complex tasks beyond MNIST is untested
- Claims about energy efficiency and scalability to deeper networks are not experimentally validated

## Confidence

| Claim | Confidence |
|-------|------------|
| Architectural derivation from BRC/GRU is clearly specified and internally consistent | High |
| Experimental results on MNIST variants are reported but lack ablation studies | Medium |
| Claims about energy efficiency and scalability to deeper networks are not validated | Low |

## Next Checks
1. Implement ablation studies removing either the bistable memory or the spike-generation gating to quantify their individual contributions to performance
2. Test the model on a temporal sequence task (e.g., sequential MNIST or character-level prediction) to validate BPTT effectiveness
3. Measure actual spike sparsity and energy consumption metrics during inference to verify claimed efficiency benefits