---
ver: rpa2
title: Embarassingly Simple Dataset Distillation
arxiv_id: '2311.07025'
source_url: https://arxiv.org/abs/2311.07025
tags:
- dataset
- data
- distillation
- performance
- distilled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of dataset distillation by treating
  it as a bilevel optimization problem and improving upon the foundational back-propagation
  through time (BPTT) method. The proposed Random Truncated Backpropagation Through
  Time (RaT-BPTT) incorporates truncation and randomization to stabilize gradients,
  speed up optimization, and cover long-term dependencies.
---

# Embarassingly Simple Dataset Distillation

## Quick Facts
- arXiv ID: 2311.07025
- Source URL: https://arxiv.org/abs/2311.07025
- Reference count: 33
- This work introduces Random Truncated Backpropagation Through Time (RaT-BPTT) for dataset distillation, achieving state-of-the-art performance across CIFAR-10, CIFAR-100, CUB, and TinyImageNet with various data budgets.

## Executive Summary
This paper addresses the challenge of dataset distillation by formulating it as a bilevel optimization problem and proposing an embarrassingly simple yet effective method called Random Truncated Backpropagation Through Time (RaT-BPTT). The key insight is that by randomly sampling a truncated window from the full training trajectory, the method stabilizes gradients and speeds up optimization while still capturing long-term dependencies. The approach achieves new state-of-the-art results across multiple datasets and data budgets, demonstrating both simplicity and effectiveness.

## Method Summary
The method treats dataset distillation as a bilevel optimization problem where the outer loop optimizes synthetic training data to minimize test loss when the inner loop trains a neural network on this data. RaT-BPTT improves upon standard BPTT by unrolling the inner loop for T steps but only backpropagating through a randomly positioned window of M steps. This truncation with randomization stabilizes gradients and reduces memory usage while maintaining performance. The method uses Adam optimizer with learning rate 0.001 for both inner and outer loops, with batch sizes varying by dataset size. Optional linear basis parameterization and a boosting mechanism (Boost-DD) can further improve results by addressing intercorrelation issues in distilled datasets.

## Key Results
- RaT-BPTT achieves state-of-the-art performance on CIFAR-10, CIFAR-100, CUB, and TinyImageNet across various data budgets
- The boosting mechanism (Boost-DD) significantly improves subset performance, addressing the intercorrelation problem in distilled datasets
- Hardness analysis reveals that distilled datasets focus on easy examples, suggesting opportunities for hardness-aware distillation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random Truncated Backpropagation Through Time (RaT-BPTT) stabilizes gradients and speeds up optimization compared to standard BPTT by using a truncated window with random positioning.
- Mechanism: RaT-BPTT samples M consecutive Hessian products from the full BPTT trajectory instead of backpropagating through all T steps. Randomizing the window placement ensures coverage of the entire trajectory while limiting the maximum number of Hessian products to M, reducing memory burden and gradient instability.
- Core assumption: The gradient information from any contiguous M-step window is sufficiently representative of the full T-step trajectory for effective optimization.
- Evidence anchors:
  - [abstract] "RaT-BPTT incorporates a truncation coupled with a random window, effectively stabilizing the gradients and speeding up the optimization while covering long dependencies."
  - [section] "RaT-BPTT is a subsample version of BPTT, spanning the entire learning trajectory."
- Break condition: If the sampled window consistently misses critical gradient information (e.g., early training phase), performance may degrade compared to full BPTT.

### Mechanism 2
- Claim: Intercorrelation in distilled datasets causes subsets to perform worse than directly distilled smaller datasets of the same size.
- Mechanism: Joint optimization of all synthetic samples creates dependencies between them, such that subsets lack the full information content needed for optimal performance.
- Core assumption: Synthetic samples in a distilled dataset are not independent and contain overlapping or complementary information that is lost when taking subsets.
- Evidence anchors:
  - [abstract] "subsets of distilled datasets tend to exhibit much worse performance than directly distilled smaller datasets of the same size."
  - [section] "When training on a subset of distilled data, for instance 10 images per class extracted from a 50-image per class distilled dataset we observe a large degradation in test accuracy."
- Break condition: If boosting mechanism successfully decouples samples, subsets should perform comparably to independently distilled smaller datasets.

### Mechanism 3
- Claim: Hardness scores reveal that distilled datasets focus on easy examples and fail to capture harder examples effectively.
- Mechanism: Distilled data optimizes for overall accuracy, which may disproportionately capture easy examples (low forgetting score) while neglecting harder examples that require more nuanced information.
- Core assumption: The optimization objective for dataset distillation implicitly weights easy examples more heavily than hard ones.
- Evidence anchors:
  - [section] "one would have hoped that adding more distilled data would help to distill more of the hardness tail, but this is not the case."
  - [section] "This suggests that future works might benefit from focusing on how one can distill data that is better adapted to larger hardness scores."
- Break condition: If a hardness-aware sampling strategy during distillation improves performance on harder examples, this mechanism is validated.

## Foundational Learning

- Concept: Bilevel optimization
  - Why needed here: Dataset distillation is formulated as minimizing test loss subject to the constraint that the distilled data must train a model to a certain performance.
  - Quick check question: What are the outer and inner optimization problems in dataset distillation?

- Concept: Backpropagation Through Time (BPTT)
  - Why needed here: BPTT is the standard method for computing gradients in bilevel optimization by unrolling the inner optimization and backpropagating through all steps.
  - Quick check question: Why does standard BPTT become computationally expensive for dataset distillation?

- Concept: Truncated BPTT
  - Why needed here: Truncated BPTT limits backpropagation to a fixed window to reduce memory usage and computational cost while maintaining gradient information.
  - Quick check question: What is the trade-off between window size and gradient accuracy in truncated BPTT?

## Architecture Onboarding

- Component map:
  - Distilled data initialization -> Inner loop training -> Gradient accumulation (RaT-BPTT) -> Outer loss computation -> Distilled data update

- Critical path:
  1. Initialize distilled data (Gaussian or linear basis)
  2. For each outer loop iteration:
     - Sample window size N âˆˆ [M, T] and position randomly
     - Sample mini-batch from distilled data
     - Run inner loop for N steps, accumulating gradients from step N-M onwards
     - Compute outer loss on target dataset
     - Update distilled data with clipped gradients
  3. Return distilled dataset

- Design tradeoffs:
  - Window size vs. gradient stability: Larger windows capture more information but increase variance
  - Unrolling length vs. computational cost: Longer unrolling captures longer dependencies but requires more memory
  - Random vs. fixed window positioning: Random provides better coverage but may miss critical phases

- Failure signatures:
  - Exploding gradients: Check gradient clipping and learning rate scaling
  - Poor performance on subsets: Indicates strong intercorrelation requiring boosting
  - No improvement over iterations: Check initialization, learning rate, and window size

- First 3 experiments:
  1. Compare RaT-BPTT vs. standard BPTT on CIFAR-10 IPC10 to verify gradient stability and performance improvements
  2. Test boosting mechanism by comparing subsets from boosted vs. jointly distilled IPC50 datasets
  3. Implement hardness-aware sampling during distillation to improve performance on harder examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the intercorrelation problem in distilled datasets be further minimized, especially for larger IPC datasets?
- Basis in paper: [explicit] The paper discusses the intercorrelation problem, noting that subsets of distilled datasets perform worse than directly distilled smaller datasets of the same size. It introduces a boosting mechanism (Boost-DD) to reduce this correlation, but acknowledges that some correlation remains between blocks.
- Why unresolved: While Boost-DD reduces intercorrelation, the paper suggests that further research is needed to minimize correlations, especially in larger IPC datasets.
- What evidence would resolve it: Evidence would include new methods or techniques that demonstrate a significant reduction in intercorrelation for larger IPC datasets, with experimental results showing improved performance of subsets compared to directly distilled smaller datasets.

### Open Question 2
- Question: What factors are bottlenecking the gains when scaling up distilled data, and how can these be addressed to improve performance on harder examples?
- Basis in paper: [explicit] The paper analyzes the performance of dataset distillation on examples that are easy or hard to learn, using a hardness score to stratify the data. It finds that increasing the number of images per class helps with easier examples but shows minimal improvement for harder examples.
- Why unresolved: The paper suggests that future work might benefit from focusing on how one can distill data that is better adapted to larger hardness scores, but does not provide a definitive solution.
- What evidence would resolve it: Evidence would include new methods or techniques that effectively distill data for harder examples, with experimental results showing improved performance on these examples compared to current methods.

### Open Question 3
- Question: How can the GPU memory usage of the RaT-BPTT method be further optimized for larger models?
- Basis in paper: [explicit] The paper acknowledges that RaT-BPTT still requires unrolling and backpropagating over several steps, which can exceed the memory requirements of directly training the model. It suggests that checkpointing techniques might be necessary for larger models.
- Why unresolved: While the paper mentions potential solutions like checkpointing, it does not provide a detailed exploration or validation of these techniques.
- What evidence would resolve it: Evidence would include the implementation and validation of memory optimization techniques, such as checkpointing, with experimental results demonstrating reduced memory usage and maintained or improved performance for larger models.

## Limitations

- The intercorrelation problem in distilled datasets is identified but the fundamental cause remains theoretical rather than empirically validated through ablation studies
- The hardness analysis reveals limitations in distilled data focusing on easy examples, but the proposed hardness-aware sampling strategy lacks comprehensive validation
- RaT-BPTT improves computational efficiency but still requires unrolling and backpropagation over multiple steps, limiting scalability to larger models

## Confidence

- **High confidence**: The effectiveness of RaT-BPTT in stabilizing gradients and improving computational efficiency is well-supported by ablation studies comparing different window sizes and unrolling lengths. The state-of-the-art results across multiple datasets provide strong empirical evidence.
- **Medium confidence**: The hardness analysis and proposed hardness-aware sampling strategy show interesting patterns but lack comprehensive validation. The connection between distilled data focusing on easy examples and the optimization objective is plausible but not rigorously proven.
- **Low confidence**: The theoretical explanation for intercorrelation in distilled datasets is primarily observational. While the boosting mechanism addresses the symptom (poor subset performance), the root cause and whether intercorrelation is an inherent property of the optimization landscape versus an artifact of specific implementation choices remains unclear.

## Next Checks

1. **Ablation study on intercorrelation**: Create distilled datasets using different optimization strategies (joint vs. independent per-sample optimization) and measure subset performance to determine if intercorrelation is an inherent property or optimization artifact.

2. **Hardness-aware distillation validation**: Implement and test the proposed hardness sampler on CIFAR-10 IPC10, measuring performance specifically on harder examples to validate whether the hardness-aware approach improves tail performance.

3. **Gradient stability analysis**: Track gradient norms and variance throughout training with different RaT-BPTT configurations to quantitatively demonstrate gradient stabilization compared to standard BPTT.