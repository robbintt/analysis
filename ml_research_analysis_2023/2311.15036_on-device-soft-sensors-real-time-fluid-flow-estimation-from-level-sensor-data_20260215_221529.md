---
ver: rpa2
title: 'On-Device Soft Sensors: Real-Time Fluid Flow Estimation from Level Sensor
  Data'
arxiv_id: '2311.15036'
source_url: https://arxiv.org/abs/2311.15036
tags:
- sensors
- soft
- data
- inference
- sensor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying soft sensors for
  real-time fluid flow estimation in IoT environments, where network latency and data
  security concerns limit the effectiveness of cloud-based approaches. The authors
  propose on-device soft sensors implemented on a microcontroller-FPGA heterogeneous
  platform, enabling AI inference directly on edge devices.
---

# On-Device Soft Sensors: Real-Time Fluid Flow Estimation from Level Sensor Data

## Quick Facts
- arXiv ID: 2311.15036
- Source URL: https://arxiv.org/abs/2311.15036
- Reference count: 12
- FPGA-based soft sensors achieve inference times between 1.04-12.04 µs, up to 28× faster than MCU-based implementations

## Executive Summary
This paper addresses the challenge of deploying soft sensors for real-time fluid flow estimation in IoT environments, where network latency and data security concerns limit the effectiveness of cloud-based approaches. The authors propose on-device soft sensors implemented on a microcontroller-FPGA heterogeneous platform, enabling AI inference directly on edge devices. Using a multi-layer perceptron (MLP) model trained on level sensor data, the FPGA-based soft sensors achieved inference times between 1.04-12.04 µs, up to 28× faster than MCU-based implementations while consuming only 29 mW. This performance demonstrates the feasibility of real-time on-device inference for fluid flow estimation, offering a compelling alternative to cloud deployments with reduced latency and improved energy efficiency.

## Method Summary
The authors developed a multi-layer perceptron (MLP) model to estimate fluid flow from level sensor data, training models with varying hidden neurons (10, 30, 60, 120) under TensorFlow and PyTorch frameworks. They deployed these models on both microcontroller (MCU) and FPGA platforms, using 8-bit quantization for the MCU and fixed-point quantization for the FPGA. The system was implemented on an Elastic Node V5 hardware platform featuring a Cortex-M0 MCU (RP2020) and Spartan-7 S15 FPGA. The MCU handled sensor data acquisition and preprocessing, while the FPGA performed the computationally intensive neural network inference. Model performance was evaluated based on inference time, power consumption, and energy efficiency across different model sizes.

## Key Results
- FPGA-based soft sensors achieved inference times between 1.04-12.04 µs, up to 28× faster than MCU-based implementations
- FPGA inference consumed only 29 mW, significantly less than the MCU's consistent 119 mW power consumption
- On-device inference demonstrated feasibility for real-time fluid flow estimation with reduced latency compared to cloud deployments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FPGA-based soft sensors achieve 1.04-12.04 μs inference time, up to 28× faster than MCU-based implementations.
- Mechanism: FPGA hardware acceleration enables parallel computation of neural network operations, reducing latency significantly compared to sequential MCU execution.
- Core assumption: The FPGA design efficiently maps the MLP model operations to parallel hardware resources.
- Evidence anchors:
  - [abstract] "Empirical evidence from our real-world use case demonstrates that FPGA-based soft sensors achieve inference times ranging remarkably from 1.04 to 12.04 microseconds."
  - [section 4.2] "In stark contrast, the inference on FPGA is expedient, being up to 28.44 × faster than its MCU counterpart depending on the neuron count."
  - [corpus] Weak evidence - no direct comparison of FPGA vs MCU speed in corpus papers.
- Break condition: If the FPGA cannot efficiently map the neural network operations, or if memory bandwidth becomes a bottleneck, the speed advantage will diminish.

### Mechanism 2
- Claim: On-device inference reduces latency and improves real-time performance compared to cloud-based approaches.
- Mechanism: By processing data locally on the device, the system eliminates network transmission delays and enables immediate decision-making.
- Core assumption: The on-device processing time is less than the sampling interval and network transmission time.
- Evidence anchors:
  - [abstract] "This performance demonstrates the feasibility of real-time on-device inference for fluid flow estimation, offering a compelling alternative to cloud deployments with reduced latency and improved energy efficiency."
  - [section 1] "However, Cloud service effectiveness is closely linked to the stability and reliability of network conditions. Even slight instability in network connections can significantly delay the transmission and processing of sensor data..."
  - [corpus] Weak evidence - no direct measurement of latency improvement in corpus papers.
- Break condition: If the on-device processing time exceeds the required real-time constraints, or if network conditions improve significantly, the latency advantage may be lost.

### Mechanism 3
- Claim: Integration of MCU and FPGA creates a heterogeneous platform that balances control and computation.
- Mechanism: The MCU handles sensor data acquisition and preprocessing, while the FPGA performs the computationally intensive neural network inference.
- Core assumption: The MCU-FPGA communication overhead is minimal compared to the computation time.
- Evidence anchors:
  - [section 3.2] "The MCU predominantly handles physical sensor data transmission to the FPGA and triggers the inference process, while the FPGA undertakes the computational-heavy tasks, signaling the MCU once finished."
  - [section 4.2] "While the power consumption of the inference on MCU remains consistently at 119 mW regardless of neuron number, the consumption of the inference on FPGA peaks at a mere 29 mW."
  - [corpus] Weak evidence - no specific mention of MCU-FPGA integration in corpus papers.
- Break condition: If the communication overhead between MCU and FPGA becomes significant, or if the FPGA cannot efficiently handle the neural network computations, the benefits of this heterogeneous approach will be diminished.

## Foundational Learning

- Concept: Neural Network Quantization
  - Why needed here: Quantization reduces the model size and computational complexity, making it suitable for deployment on resource-constrained devices like MCUs and FPGAs.
  - Quick check question: What is the difference between post-training quantization (PTQ) and quantization-aware training (QAT)?

- Concept: FPGA-based Hardware Acceleration
  - Why needed here: FPGAs can be configured to perform specific computations in parallel, significantly speeding up neural network inference compared to sequential processor execution.
  - Quick check question: How does the parallel nature of FPGA computation contribute to its speed advantage over MCUs for neural network inference?

- Concept: Real-time Systems and Latency Constraints
  - Why needed here: The system must process sensor data and produce results within the sampling interval to meet real-time requirements.
  - Quick check question: If the sampling rate is 10 kHz, what is the maximum allowable inference time to meet real-time constraints?

## Architecture Onboarding

- Component map:
  Sensors (ultrasonic, radar) → MCU (data acquisition, preprocessing) → FPGA (neural network inference) → Output (fluid flow estimation)
  Elastic Node V5 hardware platform: Cortex-M0 MCU (RP2020) + Spartan-7 S15 FPGA

- Critical path:
  Sensor data acquisition → MCU preprocessing → FPGA activation and configuration → Neural network inference → Result transmission to MCU → Output

- Design tradeoffs:
  MCU vs FPGA: MCUs offer flexibility and ease of programming but are slower for neural network inference. FPGAs provide high-speed parallel computation but require more complex design and programming.
  Quantization: 8-bit quantization reduces model size and computation time but may impact accuracy. Fixed-point quantization (6,8) used for FPGA implementation.

- Failure signatures:
  Slow inference times: May indicate FPGA design inefficiencies or communication bottlenecks between MCU and FPGA.
  High power consumption: Could suggest suboptimal FPGA resource utilization or excessive MCU involvement in computation.
  Low accuracy: May result from aggressive quantization or inadequate model training.

- First 3 experiments:
  1. Measure inference time and power consumption for different model sizes (10, 30, 60, 120 hidden neurons) on both MCU and FPGA to establish baseline performance.
  2. Test the system's ability to meet real-time constraints by measuring the total processing time from sensor data acquisition to output generation.
  3. Evaluate the impact of quantization on model accuracy by comparing the performance of full-precision and quantized models on both MCU and FPGA platforms.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quantization strategy (fixed-point vs integer) affect the accuracy of FPGA-based soft sensors for fluid flow estimation?
- Basis in paper: [explicit] The paper mentions that PTquant models, which use fixed-point (6,8) quantization-aware training, exhibit a significant increase in MSE compared to PT F P 32 models.
- Why unresolved: The paper only provides a comparison between PTquant and PT F P 32 models, but does not explore other quantization strategies or their impact on accuracy.
- What evidence would resolve it: A comprehensive study comparing different quantization strategies (e.g., different bit widths, quantization-aware training techniques) and their impact on accuracy and performance for FPGA-based soft sensors.

### Open Question 2
- Question: What is the impact of model size (number of hidden neurons) on the accuracy and performance of on-device soft sensors?
- Basis in paper: [explicit] The paper investigates the impact of model size on performance by training MLP models with 10, 30, 60, and 120 hidden neurons and comparing their inference times and power consumption on MCU and FPGA platforms.
- Why unresolved: While the paper provides insights into the trade-off between model size and performance, it does not explore the impact on accuracy or provide guidelines for selecting the optimal model size for a given application.
- What evidence would resolve it: A systematic study analyzing the relationship between model size, accuracy, and performance for various on-device soft sensor applications, along with guidelines for selecting the optimal model size.

### Open Question 3
- Question: How can on-device soft sensors be adapted to handle different types of fluid flow estimation tasks or sensor data?
- Basis in paper: [inferred] The paper focuses on fluid flow estimation using level sensor data in a specific experimental setup, but does not explore the generalizability of the approach to other types of fluid flow estimation tasks or sensor data.
- Why unresolved: The paper does not provide insights into the adaptability of the on-device soft sensor approach to different types of fluid flow estimation tasks or sensor data, which is crucial for real-world applications.
- What evidence would resolve it: A study evaluating the performance of on-device soft sensors for different types of fluid flow estimation tasks or sensor data, along with guidelines for adapting the approach to new scenarios.

## Limitations
- The study focuses specifically on fluid flow estimation from level sensor data, limiting generalizability to other soft sensing applications.
- The paper does not provide a detailed comparison of model accuracy between MCU and FPGA implementations, leaving uncertainty about potential precision trade-offs.
- The communication overhead between MCU and FPGA in the heterogeneous platform is not thoroughly characterized or analyzed.

## Confidence
- **High Confidence**: The FPGA-based implementation achieves significantly faster inference times (1.04-12.04 µs) compared to the MCU-based approach, with up to 28× speed improvement. This is directly supported by empirical measurements presented in the paper.
- **Medium Confidence**: The claim that on-device inference reduces latency and improves real-time performance compared to cloud-based approaches is plausible based on the elimination of network transmission delays, but lacks direct measurement or comparison of latency improvements in the paper.
- **Low Confidence**: The assertion that the MCU-FPGA heterogeneous platform creates an optimal balance between control and computation is not fully substantiated, as the paper does not provide a detailed analysis of the communication overhead between the MCU and FPGA or compare it to alternative architectures.

## Next Checks
1. **Cross-domain Validation**: Test the FPGA-based soft sensor approach on a different type of soft sensing problem (e.g., temperature estimation from pressure sensor data) to assess the generalizability of the methodology and performance gains.
2. **Accuracy vs. Speed Trade-off Analysis**: Conduct a detailed comparison of model accuracy between the MCU and FPGA implementations across different quantization levels to quantify the precision cost of the speed improvements.
3. **Communication Overhead Characterization**: Measure and analyze the communication overhead between the MCU and FPGA in the heterogeneous platform to determine its impact on overall system performance and identify potential bottlenecks.