---
ver: rpa2
title: On the Stability of Expressive Positional Encodings for Graphs
arxiv_id: '2310.02579'
source_url: https://arxiv.org/abs/2310.02579
tags:
- graph
- neural
- stability
- positional
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Stable and Expressive Positional Encodings
  (SPE) for graph neural networks. The key challenge addressed is the instability
  of Laplacian eigenvector-based positional encodings due to non-unique eigendecompositions
  and sensitivity to perturbations.
---

# On the Stability of Expressive Positional Encodings for Graphs

## Quick Facts
- arXiv ID: 2310.02579
- Source URL: https://arxiv.org/abs/2310.02579
- Reference count: 40
- One-line primary result: SPE achieves stability and expressiveness for graph neural networks through soft partitioning of eigenspaces

## Executive Summary
This paper introduces Stable and Expressive Positional Encodings (SPE) for graph neural networks to address the instability of Laplacian eigenvector-based positional encodings. SPE uses eigenvalues to "softly partition" eigenspaces, achieving both stability and expressiveness. The method is theoretically proven to be stable, universally expressive for basis-invariant functions, and capable of counting graph substructures. Empirically, SPE significantly outperforms existing methods on molecular property prediction tasks and demonstrates improved generalization on out-of-distribution ligand-based affinity prediction tasks.

## Method Summary
SPE processes eigenvectors and eigenvalues from Laplacian decomposition through a soft partitioning mechanism. The architecture consists of m instances of ϕℓ functions that transform eigenvalues, which are then combined with eigenvectors through a weighted sum to produce positional encodings. These encodings are processed by a permutation equivariant function ρ to generate final node embeddings. The soft partitioning is achieved by applying continuous, eigenvalue-dependent weights to eigenvector sums, avoiding the discontinuity of hard partitioning and ensuring stability while maintaining expressive power.

## Key Results
- SPE significantly outperforms existing methods on molecular property prediction tasks (ZINC, Alchemy datasets)
- Demonstrates improved generalization on out-of-distribution ligand-based affinity prediction tasks (DrugOOD)
- Theoretically proven to be stable, universally expressive for basis-invariant functions, and capable of counting graph substructures

## Why This Works (Mechanism)

### Mechanism 1: Stability via Soft Partitioning of Eigenspaces
SPE achieves stability by performing a "soft partition" of eigenspaces using eigenvalue-dependent weights, avoiding the discontinuity of hard partitioning. Instead of treating each eigensubspace independently, SPE applies a weighted sum of eigenvectors that depends on eigenvalues. This continuous weighting prevents sudden changes in output when eigenvalues converge. The Lipschitz continuity of ϕℓ and ρ ensures that small perturbations in eigenvalues lead to proportionally small changes in the output.

### Mechanism 2: Expressive Power through Eigenvalue-Eigenvector Interaction
SPE maintains expressive power by allowing interactions between different eigenvalues through the ϕℓ functions. The ϕℓ functions process all eigenvalues together, capturing relationships between them rather than treating each eigensubspace separately. This enables counting substructures and distinguishing graphs with different eigenvalue spectra. The ϕℓ functions can learn meaningful relationships between eigenvalues that capture graph structure.

### Mechanism 3: Basis Invariance through Permutation Equivariance
SPE is basis invariant because its architecture is permutation equivariant, ensuring consistent outputs regardless of eigenvector basis choice. Both ϕℓ and ρ are permutation equivariant functions. This means applying any orthogonal transformation to the eigenvectors (which represents a different basis choice) doesn't change the final output. The permutation equivariance of ϕℓ and ρ is sufficient to guarantee basis invariance for the entire SPE architecture.

## Foundational Learning

- Concept: Eigenvalue Decomposition of Graph Laplacians
  - Why needed here: The entire SPE architecture is built on processing eigenvectors and eigenvalues from Laplacian decomposition. Understanding how these capture graph structure is fundamental.
  - Quick check question: What information about a graph is encoded in its Laplacian eigenvalues and eigenvectors?

- Concept: Lipschitz Continuity and Stability
  - Why needed here: The stability proofs and theoretical guarantees rely on Lipschitz continuity assumptions. Understanding this concept is crucial for grasping why SPE is stable.
  - Quick check question: How does Lipschitz continuity of a function relate to its stability under input perturbations?

- Concept: Universal Approximation Theory
  - Why needed here: The proof that SPE can universally approximate basis invariant functions draws on universal approximation theory for neural networks.
  - Quick check question: What conditions must a neural network architecture satisfy to be a universal approximator for a class of functions?

## Architecture Onboarding

- Component map: Input layer → ϕℓ processing → Weighted eigenvector combination → ρ processing → Final embeddings → Base GNN
- Critical path: V,λ → [ϕ1(λ),...,ϕm(λ)] → [V diag(ϕ1(λ))V⊤,...,V diag(ϕm(λ))V⊤] → ρ → final embeddings
- Design tradeoffs:
  - More ϕℓ channels (larger m) increases expressive power but also computational cost
  - Using more eigenvectors (larger d) captures more structural information but may reduce stability
  - Simpler ϕℓ functions (e.g., elementwise MLPs) are more stable but potentially less expressive
- Failure signatures:
  - Training instability or divergence suggests ϕℓ or ρ functions are too complex
  - Poor generalization on OOD data indicates insufficient stability
  - Inability to distinguish certain graph structures suggests insufficient expressive power
- First 3 experiments:
  1. Implement SPE with m=1 and elementwise MLPs for ϕℓ on a simple graph regression task
  2. Test stability by comparing outputs for slightly perturbed Laplacians with different eigengaps
  3. Evaluate expressive power by testing on a graph substructure counting task with known ground truth

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important questions remain unresolved based on the content:

### Open Question 1
- Question: How does the stability of SPE affect its performance on dynamic graphs where the Laplacian matrix changes over time?
- Basis in paper: [inferred] The paper discusses the stability of SPE with respect to perturbations in the Laplacian matrix, but does not address dynamic graphs where the Laplacian changes over time.
- Why unresolved: The paper focuses on static graphs and does not explore the application of SPE to dynamic graphs.
- What evidence would resolve it: Experimental results comparing SPE's performance on dynamic graphs versus static graphs, or a theoretical analysis of how SPE's stability properties extend to time-varying Laplacians.

### Open Question 2
- Question: What is the impact of using different types of permutation equivariant functions for ρ in SPE on its overall performance?
- Basis in paper: [explicit] The paper mentions that any permutation equivariant tensor neural networks can be applied for ρ, but does not compare the performance of different choices.
- Why unresolved: The paper uses GIN as ρ but does not explore the impact of other permutation equivariant functions.
- What evidence would resolve it: Experimental results comparing SPE's performance using different permutation equivariant functions for ρ, such as Deep Sets or graph attention networks.

### Open Question 3
- Question: How does the choice of eigenvalues to use in SPE (e.g., the number of smallest eigenvalues) affect its expressive power and stability?
- Basis in paper: [explicit] The paper mentions that using fewer eigenvalues leads to a dependence on the eigengap, but does not explore the impact of different choices of eigenvalues on performance.
- Why unresolved: The paper uses a fixed number of eigenvalues in its experiments but does not systematically vary this choice.
- What evidence would resolve it: Experimental results comparing SPE's performance using different numbers of eigenvalues, or a theoretical analysis of how the choice of eigenvalues affects the trade-off between expressive power and stability.

## Limitations
- The stability proof relies on Lipschitz continuity assumptions that may not hold for all practical implementations of ϕℓ and ρ.
- Empirical validation focuses primarily on molecular datasets where graphs have relatively stable eigendecompositions, leaving uncertainty about performance on graphs with closely-spaced eigenvalues.
- The universal approximation claim depends on specific architectural choices for ϕℓ and ρ that aren't fully specified in the paper.

## Confidence

- **High confidence** in the core mechanism of soft partitioning for stability, as this follows directly from the mathematical formulation and stability proof.
- **Medium confidence** in the universal expressiveness claim, as the proof assumes idealized function classes that may not translate directly to practical neural network implementations.
- **Medium confidence** in empirical results, as they demonstrate strong performance on standard benchmarks but lack extensive ablation studies on the critical architectural components.

## Next Checks

1. Test SPE stability on synthetic graphs with controlled eigengap sizes to quantify the relationship between eigengap magnitude and output sensitivity.
2. Implement and compare multiple architectural variants for ϕℓ and ρ (e.g., different MLP depths, attention mechanisms) to identify which designs best balance stability and expressiveness.
3. Evaluate SPE on non-molecular graph datasets with known structural properties to assess generalization beyond the chemistry domain where the method was primarily validated.