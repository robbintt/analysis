---
ver: rpa2
title: Low-Rank Multitask Learning based on Tensorized SVMs and LSSVMs
arxiv_id: '2308.16056'
source_url: https://arxiv.org/abs/2308.16056
tags:
- tasks
- learning
- where
- problem
- tensor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel multitask learning framework using
  tensorized support vector machines and least squares support vector machines. The
  key idea is to represent tasks with multiple indices as a high-order tensor and
  apply CP decomposition to factorize the coefficient tensor into shared and task-specific
  factors.
---

# Low-Rank Multitask Learning based on Tensorized SVMs and LSSVMs

## Quick Facts
- arXiv ID: 2308.16056
- Source URL: https://arxiv.org/abs/2308.16056
- Reference count: 40
- Primary result: Novel multitask learning framework using tensorized SVMs and LSSVMs that factorizes coefficient tensors into shared and task-specific factors for improved prediction accuracy

## Executive Summary
This paper introduces a novel multitask learning framework that represents tasks with multiple indices as high-order tensors and applies CP decomposition to factorize the coefficient tensor into shared and task-specific factors. The method extends both support vector machines and least squares support vector machines to the multitask setting by solving alternating convex subproblems. Experimental results on both simulated and real-world datasets demonstrate superior performance compared to existing state-of-the-art matrix-based and tensor-based multitask learning methods in terms of prediction accuracy, correlation, and F1 scores.

## Method Summary
The framework constructs a high-order tensor representation of tasks, where each mode corresponds to a task index. Using CP decomposition, the coefficient tensor W is factorized into shared factors (L) and task-specific factors (U1,...,UN). The model solves an alternating optimization scheme where each subproblem becomes convex (either quadratic programming or linear system) by fixing all but one factor. This approach allows tasks to share latent features while preserving task-specific structure, with the final prediction using weighted kernel functions that capture task relationships.

## Key Results
- Outperforms matrix-based and other tensor-based MTL methods on simulated data with higher prediction accuracy and F1 scores
- Achieves superior performance on real-world datasets including School dataset and Alzheimer's Disease data
- Successfully applies the framework to disease progression prediction with promising results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CP decomposition enables sharing of latent features across tasks while preserving task-specific structure
- Mechanism: Weight tensor W is factorized as sum of rank-one components, each being product of shared factor L and task-specific factors U1,...,UN
- Core assumption: Task relationships can be captured through linear combinations of shared latent factors weighted by task-specific factors
- Evidence anchors: [abstract] mentions tensor representation and CP decomposition; [section] shows explicit factorization formula
- Break condition: Fails if task relationships are non-linear or hierarchical

### Mechanism 2
- Claim: Alternating optimization leads to convex subproblems that can be solved efficiently
- Mechanism: Fixing all but one factor creates convex quadratic programs or linear systems
- Core assumption: Alternating minimization converges to good local optimum
- Evidence anchors: [section] describes transformation to convex problems; shows resulting QP formulation
- Break condition: Gets stuck in poor local optima if problem has many such optima

### Mechanism 3
- Claim: Tensor representation naturally captures task relationships across multiple modes
- Mechanism: High-order tensor structure allows capturing relationships along each mode independently
- Core assumption: Multi-mode tensor structure better captures task relationships than single-mode matrices
- Evidence anchors: [abstract] explains natural representation of multi-index tasks; [section] demonstrates mode-specific updates
- Break condition: Adds unnecessary complexity if tasks lack meaningful multi-index structure

## Foundational Learning

- Concept: Tensor decomposition (specifically CP decomposition)
  - Why needed here: Entire method relies on factorizing coefficient tensor to model task relationships
  - Quick check question: Can you explain how CP decomposition differs from Tucker decomposition and why CP was chosen here?

- Concept: Support Vector Machines and Least Squares SVMs
  - Why needed here: Framework extends both SVM and LSSVM to multitask learning
  - Quick check question: What is the key difference between optimization problems in SVM and LSSVM that makes LSSVM more computationally efficient?

- Concept: Alternating optimization and block coordinate descent
  - Why needed here: Method solves non-convex problem by alternating optimization over each factor
  - Quick check question: Under what conditions does alternating optimization converge to stationary point for non-convex problems?

## Architecture Onboarding

- Component map: Tensor construction -> CP factorization initialization -> Alternating optimization loop -> Convergence check -> Prediction
- Critical path: Data → Tensor construction → CP factorization initialization → Alternating optimization loop (solve QP/linear systems) → Convergence check → Prediction
- Design tradeoffs: CP decomposition vs Tucker decomposition (simplicity vs flexibility), SVM vs LSSVM (accuracy vs efficiency), linear vs kernel methods (interpretability vs expressiveness)
- Failure signatures: Poor convergence (check initialization and step sizes), overfitting (check regularization parameters), incorrect task relationships (check tensor structure)
- First 3 experiments:
  1. Implement CP decomposition of synthetic tensor and verify reconstruction
  2. Implement alternating optimization for simple convex problem and check convergence
  3. Run full MTL pipeline on small synthetic dataset with known task relationships

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance vary with different CP rank values and what is optimal rank for different datasets?
- Basis in paper: Paper mentions CP rank R is set as 3 in some experiments but doesn't provide systematic analysis of rank-performance relationship
- Why unresolved: Limited experimental results for different rank values, doesn't explore relationship between rank and performance in depth
- What evidence would resolve it: Thorough experimental study testing wide range of CP rank values on various datasets, analyzing trade-off between model complexity and performance

### Open Question 2
- Question: How do tensorized SVMs and LSSVMs compare to other state-of-the-art MTL methods in computational efficiency and scalability?
- Basis in paper: Mentions main computational complexity but doesn't provide detailed comparison with other methods for large-scale datasets
- Why unresolved: Focuses on effectiveness but doesn't thoroughly analyze computational efficiency compared to other approaches
- What evidence would resolve it: Comprehensive experimental study comparing computational time and memory usage with other MTL approaches on various dataset sizes

### Open Question 3
- Question: How does choice of kernel function affect performance and are there guidelines for selecting appropriate kernel?
- Basis in paper: Mentions exploration of linear and rbf kernels but doesn't provide detailed analysis of kernel impact or selection guidelines
- Why unresolved: Demonstrates effectiveness with different kernels but doesn't investigate relationship between kernel choice and performance in depth
- What evidence would resolve it: Systematic experimental study testing various kernel functions on different datasets, analyzing impact on performance and developing selection guidelines

## Limitations

- Framework assumes task relationships can be captured through linear combinations of shared latent factors, which may not hold for non-linear or hierarchical structures
- Alternating optimization only guarantees convergence to local optima for the non-convex problem
- Performance heavily depends on proper selection of regularization parameters and kernel functions requiring extensive tuning

## Confidence

- Mechanism 1 (CP decomposition for task modeling): Medium - Well-specified but limited evidence for superiority over alternatives
- Mechanism 2 (Alternating optimization efficiency): High - Standard approach with established theoretical backing
- Mechanism 3 (Tensor representation benefits): Low - Assumption-driven with minimal comparative evidence

## Next Checks

1. Implement baseline MTL methods (matrix-based, other tensor-based) and conduct controlled experiments comparing prediction performance across varying task relationships
2. Analyze sensitivity to regularization parameters through systematic grid search and report stability metrics
3. Test the framework on datasets with known non-linear task relationships to evaluate failure modes