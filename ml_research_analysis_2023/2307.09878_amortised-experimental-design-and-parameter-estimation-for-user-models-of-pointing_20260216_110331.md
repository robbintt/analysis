---
ver: rpa2
title: Amortised Experimental Design and Parameter Estimation for User Models of Pointing
arxiv_id: '2307.09878'
source_url: https://arxiv.org/abs/2307.09878
tags:
- user
- parameter
- design
- parameters
- analyst
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an approach for efficient estimation of user
  model parameters in HCI by automating the selection of experimental designs. The
  method amortizes the computational cost of designing experiments by training a policy
  for choosing experimental designs with simulated participants.
---

# Amortised Experimental Design and Parameter Estimation for User Models of Pointing

## Quick Facts
- arXiv ID: 2307.09878
- Source URL: https://arxiv.org/abs/2307.09878
- Authors: 
- Reference count: 40
- One-line primary result: Amortised experimental design enables efficient parameter estimation for HCI pointing models by training policies to select informative experiments with simulated users.

## Executive Summary
This paper introduces an amortised experimental design approach for estimating parameters in human-computer interaction (HCI) models, specifically targeting pointing tasks. The method trains a reinforcement learning agent (Analyst) to select experimental designs that maximize information gain for parameter estimation, using simulated users rather than requiring vast amounts of human data. The approach is demonstrated across three progressively complex pointing models, showing it can estimate both capacity parameters (like noise levels) and preference parameters (like speed-accuracy trade-offs) from the same experiments, with learned policies outperforming random experimental designs.

## Method Summary
The approach uses a two-phase method where an ensemble model of possible user behaviors is first trained, then an Analyst agent is trained using reinforcement learning to select informative experiments. The Analyst interacts with in-silico agents sampled from the parameter space to learn which experimental designs provide the most useful data for parameter estimation. The method uses PPO (Proximal Policy Optimization) with custom policy networks (MLPs or relation networks) and trains on synthetic data generated by sampling parameters from prior distributions. Evaluation compares parameter estimation accuracy against random design baselines using correlation coefficients and error reduction metrics.

## Key Results
- Learned policies lead to more accurate parameter estimates than random experimental designs
- The approach can estimate both capacity parameters (oculomotor and perceptual noise) and preference parameters (speed-accuracy trade-off) from the same experiments
- Amortised design enables non-myopic experimental strategies that plan sequences of experiments for maximum total information gain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The approach learns a policy for choosing experiments that maximizes expected information gain (EIG) for parameter estimation.
- Mechanism: The Analyst agent is trained using reinforcement learning to select experimental designs that minimize the discrepancy between estimated and true parameters of the user model. By interacting with in-silico agents sampled from the model space, it learns which experiments provide the most useful data for parameter estimation.
- Core assumption: The ensemble model of possible user models accurately represents the distribution of real users, and the Analyst can effectively learn to infer parameters by designing informative experiments.
- Evidence anchors:
  - [abstract] "Our solution learns which experiments provide the most useful data for parameter estimation by interacting with in-silico agents sampled from the model space thereby using synthetic data rather than vast amounts of human data."
  - [section] "In phase 2, the Analyst is trained to conduct the best sequence of experiments for determining the model parameters. Simulated users are randomly sampled given the parameter distribution and the Analyst learns to fit the model parameters to the simulated user."
- Break condition: If the ensemble model does not accurately represent the real user distribution, or if the reinforcement learning fails to converge on an optimal policy.

### Mechanism 2
- Claim: The approach can estimate both capacity parameters (e.g., oculomotor and perceptual noise) and preference parameters (e.g., speed-accuracy trade-off) from the same experiments.
- Mechanism: The user model is extended to include both capacity and preference parameters, and the Analyst is trained to estimate all parameters simultaneously. The model's behavior changes in identifiable ways with changes in these parameters, allowing the Analyst to infer their values from observed behavior.
- Core assumption: The user model's behavior is sufficiently sensitive to changes in both capacity and preference parameters, and these changes are distinguishable from each other.
- Evidence anchors:
  - [abstract] "The approach is demonstrated for three progressively complex models of pointing... The results show that the learned policies lead to more accurate parameter estimates than random experiments, and that the approach can estimate both capacity parameters (e.g., oculomotor and perceptual noise) and preference parameters (e.g., speed-accuracy trade-off) from the same experiments."
  - [section] "In Study 3, we test to see whether Analyst can discover user model preferences... In this study both performance time and errors are used in the estimation of model parameters."
- Break condition: If the model's behavior is not sufficiently sensitive to changes in capacity and preference parameters, or if the changes are not distinguishable.

### Mechanism 3
- Claim: The approach can learn non-myopic and adaptive experimental design strategies.
- Mechanism: The Analyst is trained to select a sequence of experiments that maximize the total information gain over the entire sequence, rather than just the next experiment. It can also adapt its design choices based on the outcomes of previous experiments.
- Core assumption: The Analyst can effectively learn to plan a sequence of experiments that maximizes the total information gain, and it can adapt its design choices based on previous outcomes.
- Evidence anchors:
  - [abstract] "In the current article, we investigate a variant of these methods that amortises the computational cost of designing experiments by training a policy for choosing experimental designs with simulated participants."
  - [section] "One advantage of amortisation is that experimental design can be non-myopic. This means that, rather than maximising EIG for each individual experiment, instead it can be maximised for a whole sequence of experiments."
- Break condition: If the Analyst fails to learn an effective non-myopic strategy, or if it cannot effectively adapt its design choices based on previous outcomes.

## Foundational Learning

- Concept: Reinforcement learning
  - Why needed here: The approach uses reinforcement learning to train the Analyst agent to select experimental designs. Understanding RL concepts like policies, rewards, and training algorithms is crucial for implementing and improving the approach.
  - Quick check question: What is the difference between on-policy and off-policy reinforcement learning, and which is used in this approach?

- Concept: User modeling in HCI
  - Why needed here: The approach is applied to user modeling in HCI, specifically for estimating parameters of pointing models. Understanding the basics of user modeling and the challenges involved is important for applying and extending the approach.
  - Quick check question: What are some common challenges in estimating parameters of user models in HCI, and how does this approach address them?

- Concept: Bayesian experimental design
  - Why needed here: The approach is inspired by Bayesian optimal experimental design, which aims to maximize the expected information gain from experiments. Understanding the concepts and limitations of BOED is important for understanding the motivation and potential advantages of the RL-based approach.
  - Quick check question: What are the key differences between Bayesian optimal experimental design and the RL-based approach presented in this paper?

## Architecture Onboarding

- Component map: User model -> Ensemble model -> Analyst agent -> Experimental design selection
- Critical path: 1. Train ensemble user models over parameter space; 2. Train Analyst RL agent to select experiments maximizing information gain; 3. Deploy trained Analyst with real users for parameter estimation
- Design tradeoffs: Training time vs. inference time (3-9 hours training vs. milliseconds inference); model complexity vs. data efficiency; generalizability vs. task-specificity
- Failure signatures: Poor parameter estimates from ineffective policies; slow convergence from poorly tuned training; overfitting to training data reducing generalization
- First 3 experiments: 1. Implement user model and verify realistic behavior generation; 2. Train ensemble model on simple parameter distribution and verify behavior capture; 3. Train Analyst on ensemble model and verify accurate parameter estimation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed approach generalize beyond pointing tasks to other HCI domains like menu search, decision making, or biomechanical control?
- Basis in paper: [explicit] The paper discusses potential generalization but only demonstrates the approach on abstract problems and pointing tasks.
- Why unresolved: The authors explicitly state that further empirical work is needed to test generalization to other HCI tasks.
- What evidence would resolve it: Successful application and validation of the approach on diverse HCI tasks like menu search, decision making, and biomechanical control would demonstrate generalizability.

### Open Question 2
- Question: How does the performance of the Analyst model compare to Bayesian experimental design approaches in terms of accuracy and efficiency?
- Basis in paper: [inferred] The paper mentions Bayesian approaches as having mathematical rigor and posterior distributions but doesn't provide a direct comparison with the proposed RL-based method.
- Why unresolved: The authors discuss advantages of their approach but don't empirically compare it to Bayesian methods.
- What evidence would resolve it: A controlled experiment comparing the RL-based approach to Bayesian methods on the same tasks would provide direct performance comparisons.

### Open Question 3
- Question: What is the impact of hyperparameter selection on the performance of the Analyst model, and can an automated method be developed to optimize these parameters?
- Basis in paper: [explicit] The authors mention that hyperparameter tuning was involved in generating results and finding the best performance was beyond the scope of the current article.
- Why unresolved: The paper acknowledges the challenge of hyperparameter selection but doesn't explore automated optimization methods.
- What evidence would resolve it: Developing and testing an automated hyperparameter optimization method (e.g., Bayesian optimization) for the Analyst model would address this question.

## Limitations
- The approach relies heavily on the quality of the ensemble model to represent real user distributions, which may not generalize well if simulations don't capture actual human behavior
- The computational cost of training the Analyst (3-9 hours) may be prohibitive for some applications despite amortization benefits
- Effectiveness for more complex models with many parameters remains untested, as demonstrated models are relatively simple

## Confidence
- High confidence: The approach can learn effective experimental design policies for parameter estimation
- Medium confidence: The method can estimate both capacity and preference parameters from the same experiments
- Medium confidence: The approach enables non-myopic and adaptive experimental design

## Next Checks
1. Test the approach on a user model with 10+ parameters to evaluate scalability and identify potential identifiability issues
2. Conduct a user study with real participants to validate that policies trained on simulated users transfer effectively to human behavior
3. Compare the amortized approach against traditional Bayesian experimental design methods on the same tasks to quantify computational efficiency gains