---
ver: rpa2
title: Gated recurrent neural networks discover attention
arxiv_id: '2309.01775'
source_url: https://arxiv.org/abs/2309.01775
tags:
- linear
- gated
- rnns
- gating
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that gated recurrent neural networks (RNNs)
  with multiplicative gating can exactly implement linear self-attention, the core
  mechanism in Transformers. The authors provide a constructive proof showing how
  gated RNNs can replicate the behavior of linear self-attention layers using finite
  neurons.
---

# Gated recurrent neural networks discover attention

## Quick Facts
- arXiv ID: 2309.01775
- Source URL: https://arxiv.org/abs/2309.01775
- Reference count: 33
- Key outcome: Gated RNNs with multiplicative gating can exactly implement linear self-attention

## Executive Summary
This paper demonstrates that gated recurrent neural networks (RNNs) with multiplicative gating can exactly implement linear self-attention, the core mechanism in Transformers. Through a constructive proof, the authors show how gated RNNs can replicate linear self-attention layers using finite neurons, with input gating generating key-value products and queries, recurrent units accumulating key-values, and output gating combining these elements. The key finding is that RNNs equipped with linear recurrence and multiplicative gating can, in principle, implement the same attention-based computations as Transformers, challenging the conventional view of these architectures as fundamentally different.

## Method Summary
The paper employs teacher-student experiments where a student RNN is trained to emulate a teacher linear self-attention layer. The RNN uses input/output gating with diagonal linear recurrence, and the training employs AdamW optimizer with cosine annealing learning rate. Post-processing identifies key-value and query neurons from trained weights. The in-context linear regression task trains RNNs on simple regression problems to test whether they discover attention-based algorithms, with performance measured against optimal gradient descent baselines.

## Key Results
- Gated RNNs can exactly implement linear self-attention through constructive proof with O(d⁴) parameters
- Trained RNNs learn attention-based in-context learning algorithms similar to Transformers
- In-context linear regression tasks show RNNs implementing one-step gradient descent via attention mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gated RNNs can exactly implement linear self-attention through finite neuron constructions.
- Mechanism: Input gating generates key-value products and queries, recurrent units with λ=1 accumulate key-values while those with λ=0 return queries, and output gating combined with linear readout multiplies key-values with queries.
- Core assumption: The gating mechanism can be linearized to produce the required polynomial degree interactions.
- Evidence anchors:
  - [abstract] "gated recurrent neural networks (RNNs) with multiplicative gating can exactly implement linear self-attention"
  - [section] "We demonstrate in this section how a gated recurrent layer followed by a linear readout as in Equation 2 can implement any linear self-attention layer through a constructive proof"
  - [corpus] Weak evidence - corpus focuses on different architectural variations but doesn't directly validate the exact implementation claim
- Break condition: If gating nonlinearities cannot be adequately linearized or if recurrent states cannot store the required polynomial combinations

### Mechanism 2
- Claim: Trained gated RNNs discover attention-based in-context learning algorithms similar to Transformers.
- Mechanism: During training on in-context linear regression tasks, gradient descent instills attention-compatible configurations where RNNs learn to implement one step of gradient descent through attention mechanisms.
- Core assumption: The in-context learning task structure naturally guides networks toward attention-based solutions.
- Evidence anchors:
  - [abstract] "trained gated RNNs learn to implement attention, particularly excelling in in-context linear regression tasks"
  - [section] "we find that gradient descent instills in our RNNs the same attention-based in-context learning algorithm used by Transformers"
  - [corpus] Moderate evidence - several papers discuss attention mechanisms in RNNs but fewer validate specific in-context learning algorithm discovery
- Break condition: If the training task doesn't sufficiently constrain the solution space or if alternative algorithms perform equally well

### Mechanism 3
- Claim: The equivalence between gated RNNs and attention requires O(d^4) parameters due to redundancy in the construction.
- Mechanism: The gated RNN implementation requires d^2 neurons for key-value elements and d for queries, with additional redundancy from multiple equivalent configurations and invariances.
- Core assumption: The parameter inefficiency is inherent to the constructive proof rather than a practical limitation.
- Evidence anchors:
  - [section] "Overall, the output gating requires O(d^2) input and output entries for the gated RNN to match a linear self-attention layer. The RNN thus requires O(d^4) parameters in total"
  - [section] "It comes as no surprise that numerous equivalent configurations exist within the gated RNN we study"
  - [corpus] Weak evidence - corpus papers discuss parameter efficiency but don't specifically address the O(d^4) scaling in this context
- Break condition: If practical implementations can exploit structural sparsity in attention matrices to achieve better scaling

## Foundational Learning

- Concept: Linear recurrence with multiplicative gating
  - Why needed here: Forms the basis for implementing attention mechanisms within RNN architectures
  - Quick check question: How does the diagonal linear recurrence differ from standard RNN recurrence in terms of information storage and processing?

- Concept: Polynomial degree matching in neural computations
  - Why needed here: Understanding why attention computations correspond to specific polynomial degrees helps explain the construction requirements
  - Quick check question: What polynomial degree does the linear self-attention computation (WV x)(WKx)⊤(WQx) correspond to, and why does this matter for the gated RNN implementation?

- Concept: Hebbian learning and weight accumulation
  - Why needed here: The attention mechanism's weight accumulation rule resembles Hebbian learning, which is implemented through the recurrent state updates
  - Quick check question: How does the attention weight update rule W_ff_t = W_ff_t-1 + v_t k_t^⊤ relate to Hebbian learning principles?

## Architecture Onboarding

- Component map: Input → Input gating (W_in_m, W_in_x) → Recurrent state (λ parameter determines memory vs query neurons) → Output gating (W_out_m, W_out_x) → Linear readout (D matrix) → Output

- Critical path: The architecture processes inputs through input gating to generate key-values and queries, stores key-values in recurrent states with λ=1, maintains queries with λ=0, and combines them through output gating and linear readout to produce the final output.

- Design tradeoffs: Diagonal recurrence simplifies the construction but may limit expressiveness compared to full recurrence. The gating mechanism enables attention implementation but requires careful parameter initialization. The O(d^4) parameter scaling is a significant cost.

- Failure signatures: Poor in-context learning performance suggests the network hasn't discovered the attention algorithm. Training instability may indicate issues with gating linearization. Overfitting with small datasets suggests the architecture is too expressive for the task.

- First 3 experiments:
  1. Teacher-student experiment: Train a gated RNN to mimic a linear self-attention layer and verify functional equivalence through polynomial analysis
  2. In-context regression: Train on simple regression tasks and check if the network implements gradient descent through attention
  3. Architecture comparison: Compare different gated RNN variants (LSTM, GRU, LRU) on the same tasks to understand which architectures naturally discover attention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much compression of the gating weights is possible in practical scenarios beyond the theoretical O(d^4) parameter scaling?
- Basis in paper: [inferred] The paper shows that in the in-context linear regression task, gated RNNs learn compressed representations compared to the naive construction, leveraging sparse structure in the attention matrices. However, it notes that understanding the extent of compression in real-world scenarios requires further investigation.
- Why unresolved: The paper only demonstrates compression in a simplified toy task and acknowledges that analyzing compression in more complex, realistic attention mechanisms is an open question requiring further study.
- What evidence would resolve it: Experiments applying gated RNNs to real-world sequence modeling tasks (e.g., language modeling, machine translation) and analyzing the resulting parameter efficiency compared to standard Transformers would provide evidence.

### Open Question 2
- Question: What is the precise relationship between the inductive bias toward attention and in-context learning performance across different gated RNN architectures?
- Basis in paper: [inferred] The paper finds that nonlinear gated RNNs (LSTMs, GRUs, LRUs) outperform one-step gradient descent in in-context learning, but notes that the relationship between attention implementation and in-context learning ability is not straightforward, as GRUs (which cannot implement attention) perform best.
- Why unresolved: The paper observes a correlation between attention bias and performance for different LRU variants, but the overall relationship is complex and requires further controlled experiments to understand.
- What evidence would resolve it: Systematic ablation studies varying the attention bias and nonlinear components of different RNN architectures while measuring in-context learning performance on standardized benchmarks would clarify this relationship.

### Open Question 3
- Question: How do the proposed linear gated RNNs compare to existing linearized attention models in terms of computational efficiency and performance?
- Basis in paper: [inferred] The paper introduces a construction showing how gated RNNs can implement linear self-attention, but does not compare this approach to other linearized attention methods (e.g., Performer, Linear Transformers) that have been developed for computational efficiency.
- Why unresolved: While the paper demonstrates theoretical equivalence and practical learning capabilities, it does not benchmark against existing efficient attention implementations or analyze computational complexity trade-offs.
- What evidence would resolve it: Empirical comparisons of the proposed gated RNN attention implementation with existing linearized attention models on long-sequence tasks, measuring both computational efficiency (memory, runtime) and task performance, would provide resolution.

## Limitations

- The O(d⁴) parameter scaling makes the construction highly inefficient compared to direct attention implementation
- The constructive proof assumes perfect linearization of gating mechanisms that may not hold in practice
- Teacher-student experiments show functional similarity but don't conclusively prove identical computational mechanisms
- Claims about attention discovery are based on behavioral similarity rather than mechanistic proof

## Confidence

**High Confidence:** The mathematical construction showing that gated RNNs can implement linear self-attention is rigorous and well-supported. The polynomial analysis and parameter counting are sound, and the theoretical framework is clearly articulated.

**Medium Confidence:** The teacher-student experimental results demonstrating functional equivalence between trained RNNs and attention layers. While the methodology is sound, the experiments rely on post-processing to identify attention components, which introduces potential interpretation errors.

**Low Confidence:** The claim that trained RNNs "discover" attention-based in-context learning algorithms similar to Transformers. The evidence shows behavioral similarity but doesn't establish mechanistic equivalence or prove that gradient descent specifically guides networks toward attention-based solutions rather than alternative algorithms.

## Next Checks

1. **Ablation study on gating mechanisms:** Systematically remove or modify the input/output gating components while keeping other architecture elements fixed, then measure degradation in the ability to implement attention and perform in-context learning. This would test whether gating is truly essential or if alternative mechanisms could achieve similar results.

2. **Cross-architecture generalization test:** Train different recurrent architectures (LSTM, GRU, Simple RNN) on the same in-context learning tasks and analyze whether they discover similar attention-based algorithms or converge to different computational strategies. This would reveal whether the attention discovery is specific to the proposed gated RNN formulation or a more general phenomenon.

3. **Dynamic parameter analysis:** Monitor the evolution of gating parameters (λ) and weight matrices throughout training to identify whether networks consistently converge to attention-compatible configurations, and whether these configurations emerge early in training or develop gradually. This would provide insight into whether attention is a natural attractor in the optimization landscape or requires specific initialization conditions.