---
ver: rpa2
title: 'The Wizard of Curiosities: Enriching Dialogues with Fun Facts'
arxiv_id: '2309.11283'
source_url: https://arxiv.org/abs/2309.11283
tags:
- curiosities
- curiosity
- user
- task
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to enrich dialogues in task-oriented
  conversational assistants by incorporating curated curiosities. The authors created
  a dataset of over 1,350 curiosities for cooking and DIY domains, carefully designed
  to be concise and engaging.
---

# The Wizard of Curiosities: Enriching Dialogues with Fun Facts

## Quick Facts
- arXiv ID: 2309.11283
- Source URL: https://arxiv.org/abs/2309.11283
- Reference count: 8
- Primary result: A/B test shows 9.7% relative rating improvement with curiosity insertion

## Executive Summary
This paper introduces a novel approach to enrich task-oriented dialogues in conversational assistants by incorporating curated curiosities. The authors created a dataset of over 1,350 curiosities for cooking and DIY domains, carefully designed to be concise and engaging. They propose methods to automatically match curiosities to dialogues using text-based and semantic similarity approaches, and a strategy to naturally insert curiosities into the conversation flow. An A/B test with over 1,000 conversations showed that the proposed approach increases user engagement and leads to an average relative rating improvement of 9.7%. The findings suggest that introducing curiosities in a non-intrusive manner can enhance the user experience in task-oriented dialogues.

## Method Summary
The method involves manually curating curiosities for cooking and DIY domains, then automatically matching them to task steps using either text-based (bag-of-words + cosine similarity) or semantic similarity (Sentence-BERT embeddings + cross-encoder re-ranking) approaches. Curiosities are inserted into dialogues using a carefully designed offer/backoff strategy that avoids cognitive overload by limiting insertions to every 6th step and avoiding early or final steps. The system uses predefined openers and closers to frame curiosities naturally within the conversation flow.

## Key Results
- A/B test with over 1,000 conversations showed curiosities increase user engagement
- Average relative rating improvement of 9.7% compared to baseline system without curiosities
- Manual curation ensures factual accuracy and domain relevance, avoiding LLM hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextualized curiosity insertion increases user engagement without disrupting task completion
- Mechanism: Curiosities are matched to the current dialogue step using semantic similarity and inserted only after every 6th step, avoiding early or final steps to reduce cognitive load
- Core assumption: Users are receptive to curiosity offers when they are not in the middle of processing complex instructions
- Evidence anchors:
  - [abstract] "According to an A/B test with over 1000 conversations, curiosities not only increase user engagement, but provide an average relative rating improvement of 9.7%."
  - [section] "To ensure the overall users’ satisfaction, and avoid non-intrusive behaviors, we designed a dialogue curiosity offer/backoff strategy... we never introduce curiosities at the beginning of a dialog, or when the user is listening to long steps (≥200 words)."
- Break condition: If the semantic similarity matching fails to find a relevant curiosity, or if the user declines curiosity offers repeatedly, engagement gains may diminish

### Mechanism 2
- Claim: Offering curiosities in a conversational manner with pre-defined openers and closers improves acceptance rates
- Mechanism: A fixed opener ("Did you know that:") introduces the curiosity, and a closer ("Interesting right?") smoothly transitions back to the task flow
- Core assumption: Structured linguistic framing increases curiosity appeal and user acceptance
- Evidence anchors:
  - [abstract] "According to an A/B test with over 1000 conversations, curiosities not only increase user engagement, but provide an average relative rating improvement of 9.7%."
  - [section] "To deliver a curiosity with the right tone of voice, we select an opener from a pre-defined list, to introduce the curiosity. Similarly, to gracefully end the insertion of a curiosity, we appended a closer phrase after the curiosity sentence."
- Break condition: If the opener/closer phrases feel repetitive or robotic, user engagement may drop

### Mechanism 3
- Claim: Manual curation of curiosities ensures factual accuracy and domain relevance, avoiding LLM hallucinations
- Mechanism: Curiosities are manually searched, filtered, and validated to ensure they are concise, simple, and contextually appropriate
- Core assumption: High-quality, verified curiosities are more effective than automatically generated ones that may be incorrect
- Evidence anchors:
  - [abstract] "All the curiosities were manually curated to fit the characteristics and specifications identified, ensuring their quality and appropriateness for dialog and its domain."
  - [section] "As an alternative to manually curated curiosities, current LLMs can generate curiosities contextualized to the conversation. We tested this strategy but observed that, often these curiosities are false and incorrect."
- Break condition: If manual curation becomes unsustainable as the task set grows, scalability may limit this approach

## Foundational Learning

- Concept: Cosine similarity and semantic embeddings
  - Why needed here: Used to match curiosities to task steps based on semantic relevance rather than keyword overlap
  - Quick check question: What is the difference between cosine similarity and Euclidean distance in high-dimensional embedding spaces?

- Concept: Bag-of-words vectorization and preprocessing
  - Why needed here: Used in the text-based method to match curiosities by filtering out stopwords, verbs, and domain-specific common words
  - Quick check question: How does removing stopwords affect the semantic matching of short text snippets like curiosities?

- Concept: A/B testing methodology and user engagement metrics
  - Why needed here: Used to evaluate whether curiosity insertion improves user ratings and engagement compared to a baseline without curiosities
  - Quick check question: Why is it important to balance user assignment across A and B groups when measuring engagement impact?

## Architecture Onboarding

- Component map: Task content extractor → Curiosity matcher (text-based or semantic) → Curiosity offer/backoff controller → Dialogue manager
- Critical path:
  1. Extract title, steps, and ingredients from the current task
  2. Encode task content and curiosities using Sentence-BERT
  3. Compute similarity scores and select top-m matches
  4. Check curiosity offer/backoff conditions (step count, cognitive load)
  5. Insert curiosity with opener/closer if conditions are met
- Design tradeoffs:
  - Manual curation ensures quality but limits scalability
  - Semantic similarity is more robust than keyword matching but requires more compute
  - Curiosity offers are limited to every 6 steps to avoid annoyance, which may delay engagement benefits
- Failure signatures:
  - Low curiosity acceptance rate → curiosity phrasing or timing may be off
  - No improvement in ratings → matching quality or relevance may be poor
  - System slowdown → Sentence-BERT inference cost may be too high for real-time use
- First 3 experiments:
  1. Test the curiosity offer/backoff algorithm with simulated dialogue steps to verify timing logic
  2. Evaluate semantic vs text-based matching accuracy on a small curated test set
  3. Run a small A/B test with 50 conversations to measure rating impact before full deployment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the long-term impact of introducing curiosities in conversational task assistants on user engagement and satisfaction?
- Basis in paper: [explicit] The paper mentions that the long-term effect of curiosities was not studied due to privacy issues preventing tracking of recurring users
- Why unresolved: The study only covered a 6-month period with new users each time, lacking data on how regular users' engagement changes over extended use
- What evidence would resolve it: Longitudinal studies tracking the same users over a longer period, measuring changes in engagement, satisfaction, and curiosity acceptance rates

### Open Question 2
- Question: How can we effectively mitigate selection bias in studies evaluating the impact of curiosities in conversational assistants?
- Basis in paper: [explicit] The paper acknowledges a "selection bias" where users who accept curiosities might already be more engaged, potentially skewing results
- Why unresolved: The current study design cannot fully separate the curiosity feature's impact from users' pre-existing engagement levels
- What evidence would resolve it: A study design that randomly assigns curiosities to users regardless of their initial engagement level, or a more sophisticated statistical analysis to control for engagement bias

### Open Question 3
- Question: Can large language models be reliably used to generate factually correct curiosities for task-oriented dialogues?
- Basis in paper: [explicit] The paper tested using LLMs for generating curiosities but found them often false or incorrect, making them unreliable for a system where accuracy is crucial
- Why unresolved: Current LLMs still struggle with factual accuracy, especially in niche domains like cooking and DIY tasks
- What evidence would resolve it: Developing and testing a method to ensure factual correctness of LLM-generated curiosities, possibly through fact-checking mechanisms or improved prompting strategies

### Open Question 4
- Question: How can curiosities be effectively contextualized based on a graph of entities in task-oriented dialogues?
- Basis in paper: [explicit] The paper suggests investigating methods to contextualize curiosities according to a graph of entities as future work
- Why unresolved: The current approach uses text-based and semantic similarity methods, which may not fully capture the nuanced relationships between entities in tasks
- What evidence would resolve it: Implementing and testing a system that uses entity graphs to contextualize curiosities, measuring improvements in relevance and user engagement compared to current methods

## Limitations
- Manual curation process may not scale well as the task set grows, potentially limiting long-term applicability
- Specific hyperparameters for matching were not detailed, making exact replication challenging
- Study focused on cooking and DIY domains, leaving uncertainty about effectiveness in other task-oriented contexts

## Confidence
- High confidence: Contextualized curiosity insertion increases engagement when properly timed
- Medium confidence: Specific phrasing strategy (openers/closers) effectiveness
- Medium confidence: Matching methodology effectiveness relative to alternatives

## Next Checks
1. Test the offer/backoff strategy with varying intervals (every 4, 6, or 8 steps) to identify optimal curiosity insertion frequency
2. Conduct A/B testing with alternative curiosity openers/closers to measure impact on acceptance rates
3. Evaluate the system's performance on a broader set of task domains beyond cooking and DIY to assess generalizability