---
ver: rpa2
title: Learning Thresholds with Latent Values and Censored Feedback
arxiv_id: '2312.04653'
source_url: https://arxiv.org/abs/2312.04653
tags:
- threshold
- distribution
- value
- reward
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the query complexity of learning optimal thresholds
  in latent space, where the reward depends on both the proposed threshold and an
  unknown latent value, and can only be achieved if the threshold is lower than or
  equal to the latent value. The authors first prove that the query complexity can
  be infinitely large even when the reward function is monotone with respect to both
  threshold and latent value.
---

# Learning Thresholds with Latent Values and Censored Feedback

## Quick Facts
- arXiv ID: 2312.04653
- Source URL: https://arxiv.org/abs/2312.04653
- Reference count: 40
- One-line primary result: Proves query complexity can be infinite for monotone rewards, provides tight O(1/ε³) bounds under Lipschitz assumptions

## Executive Summary
This paper studies the fundamental limits of learning optimal thresholds when the reward depends on both the proposed threshold and an unknown latent value, but can only be observed if the threshold is below or equal to the latent value. The authors prove that even when the reward function is monotone, the query complexity can be infinite due to the censored nature of the feedback. They provide tight query complexity bounds of O(1/ε³) for two important cases: monotone reward functions with Lipschitz value distributions, and right-Lipschitz reward functions. The paper also extends the model to an online learning setting with a tight O(T²/³) regret bound.

## Method Summary
The paper analyzes a threshold learning problem where a learner proposes thresholds γ ∈ [0,1] and receives censored feedback: the reward g(γ,v) if the latent value v ≥ γ, otherwise 0. The learner must find the threshold maximizing expected utility E[g(γ,V)] where V follows an unknown distribution F. The impossibility result uses a "needle in haystack" construction where the optimal threshold is hidden within an interval of equal expected utility. The upper bounds use discretization approaches that exploit Lipschitz continuity of either the value distribution CDF or the reward function.

## Key Results
- Query complexity can be infinite even when g(γ,v) is monotone with respect to both arguments
- Tight Θ̃(1/ε³) query complexity bound when g is monotone and CDF of value distribution is Lipschitz
- Tight Θ̃(1/ε³) query complexity bound when g satisfies right-Lipschitzness
- O(T²/³) regret bound in the online learning setting

## Why This Works (Mechanism)

### Mechanism 1
The learner cannot determine the exact latent value distribution using finite queries even when the reward function is monotone. The paper constructs a reward function with a discontinuous "bump" at a specific point and a value distribution with a mass at the same point, creating a unique maximum utility at that point. However, by placing this maximum within an interval of equal expected utility, the learner cannot distinguish the optimal point from other points in the interval using finite queries.

### Mechanism 2
The query complexity for learning the optimal threshold is O(1/ε³) when the reward function is monotone and the CDF of the value distribution is Lipschitz. The Lipschitz property allows discretization of the threshold space with bounded error. By querying each discretized threshold sufficiently, the learner can estimate expected utility within additive error ε. The union bound over all discretized thresholds ensures small probability of error.

### Mechanism 3
The query complexity for learning the optimal threshold is O(1/ε³) when the reward function is right-Lipschitz. The right-Lipschitz property ensures the expected utility function is also right-Lipschitz, enabling a similar discretization approach where the threshold space is discretized and each threshold is queried sufficiently to estimate expected utility within additive error ε.

## Foundational Learning

- **Concept**: Lipschitz continuity
  - Why needed here: Crucial for establishing Lipschitzness of expected utility function, enabling discretization approach
  - Quick check question: What is the definition of Lipschitz continuity for a function f(x) on [a,b]?

- **Concept**: Expected utility maximization
  - Why needed here: Learner's goal is finding threshold that maximizes expected reward under value distribution
  - Quick check question: How is expected utility U(γ) defined in terms of reward function g(γ,v) and value distribution F?

- **Concept**: Query complexity
  - Why needed here: Measures number of queries required to learn optimal threshold within error bound
  - Quick check question: What is definition of (ε,δ)-estimator and query complexity in threshold learning?

## Architecture Onboarding

- **Component map**: Value distribution class C -> Reward function class G -> Learner -> Query algorithm -> Threshold selection
- **Critical path**: (1) Select threshold, (2) Query threshold and receive reward feedback, (3) Update expected utility estimate, (4) Repeat until optimal threshold found within error bound
- **Design tradeoffs**: Accuracy vs number of queries required; higher accuracy needs more queries, especially with complex value distributions or reward functions
- **Failure signatures**: Insufficient queries, incorrect assumptions about value distribution or reward function class, adversarial distributions or reward functions
- **First 3 experiments**:
  1. Implement algorithm for monotone reward with Lipschitz value distribution, verify O(1/ε³) query complexity
  2. Implement algorithm for right-Lipschitz reward function, verify O(1/ε³) query complexity
  3. Construct impossibility example with monotone reward and general distribution, verify infinite query complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is exact query complexity for learning optimal thresholds when reward is monotone but value distribution is not Lipschitz?
- Basis in paper: Paper proves infinite complexity for monotone reward with general distribution, but doesn't bound case with monotone reward and non-Lipschitz distribution
- Why unresolved: Paper only considers Lipschitz vs general distributions, not intermediate cases
- What evidence would resolve it: Tight bound on query complexity for monotone reward with non-Lipschitz distribution

### Open Question 2
- Question: What is optimal algorithm for learning optimal thresholds in online setting with monotone reward and Lipschitz distribution?
- Basis in paper: Paper provides regret bound but not optimal algorithm
- Why unresolved: Only provides regret bound without optimal algorithm
- What evidence would resolve it: Algorithm achieving optimal regret bound in online setting

### Open Question 3
- Question: What is relationship between query complexity and regret in online setting?
- Basis in paper: Provides regret bound but no relationship to query complexity
- Why unresolved: Only provides regret bound without relationship to query complexity
- What evidence would resolve it: Relationship between query complexity and regret in online setting

## Limitations

- The fundamental limitation of censored feedback creates significant challenges even with monotone rewards
- The "needle in haystack" impossibility construction may be difficult to implement or verify in practice
- The analysis assumes known Lipschitz constants for the upper bound results

## Confidence

- **High confidence**: Impossibility result for monotone rewards with general distributions, O(1/ε³) query complexity bounds for Lipschitz cases
- **Medium confidence**: Online learning extension and O(T²/³) regret bound, practical implementability of impossibility construction
- **Low confidence**: Specific implementation details for impossibility construction, exact form of perturbations in lower bound proofs

## Next Checks

1. **Implement the impossibility construction**: Create concrete "needle in haystack" example where optimal threshold is hidden in interval of equal expected utility, verify learner cannot distinguish optimal point using finite queries.

2. **Test Lipschitz discretization algorithm**: Implement discretization-based algorithm for Lipschitz value distribution case, run experiments with varying ε to empirically verify O(1/ε³) query complexity scaling.

3. **Validate censored feedback mechanism**: Create simulation environment correctly implementing censored feedback (only observing rewards when threshold ≤ latent value), test handling of boundary cases where optimal threshold is at support boundary.