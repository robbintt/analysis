---
ver: rpa2
title: Estimating label quality and errors in semantic segmentation data via any model
arxiv_id: '2307.05080'
source_url: https://arxiv.org/abs/2307.05080
tags:
- label
- quality
- segmentation
- errors
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting annotation errors
  in semantic segmentation datasets, where human annotators struggle to label every
  pixel correctly. The authors propose a model-agnostic label quality scoring method
  that uses probabilistic predictions from a trained segmentation model to prioritize
  images for review.
---

# Estimating label quality and errors in semantic segmentation data via any model

## Quick Facts
- arXiv ID: 2307.05080
- Source URL: https://arxiv.org/abs/2307.05080
- Authors: 
- Reference count: 7
- One-line primary result: The softmin score consistently outperforms other methods in detecting mislabeled images with high precision and recall, achieving AUROC values up to 0.951 for detecting Drop errors and AUPRC values up to 0.888 for detecting Swap errors.

## Executive Summary
This paper addresses the problem of detecting annotation errors in semantic segmentation datasets, where human annotators struggle to label every pixel correctly. The authors propose a model-agnostic label quality scoring method that uses probabilistic predictions from a trained segmentation model to prioritize images for review. The key innovation is the softmin score, which computes a soft minimum of the model-estimated likelihoods of each pixel's annotated class. This approach is robust to minor statistical fluctuations in per-pixel model outputs. The authors evaluate their method on synthetic versions of the SYNTHIA dataset with three types of annotation errors (Drop, Swap, Shift). The softmin score consistently outperforms other methods in detecting mislabeled images with high precision and recall.

## Method Summary
The paper proposes a model-agnostic method for detecting annotation errors in semantic segmentation datasets. The method uses probabilistic predictions from any trained segmentation model to compute label quality scores for images. The key innovation is the softmin score, which computes a soft approximation of the minimum over per-pixel confidence values, controlled by a temperature parameter. This allows it to focus on the worst-labeled regions without being overly sensitive to noise in well-labeled regions. The method is evaluated on synthetic versions of the SYNTHIA dataset with three types of annotation errors (Drop, Swap, Shift) and applied to the Cityscapes dataset for real-world validation.

## Key Results
- The softmin score consistently outperforms other methods in detecting mislabeled images with high precision and recall.
- Achieves AUROC values up to 0.951 for detecting Drop errors and AUPRC values up to 0.888 for detecting Swap errors.
- Successfully identifies naturally-occurring label errors in the Cityscapes dataset.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The softmin score is robust to minor statistical fluctuations in per-pixel model outputs while still emphasizing the least confident pixels.
- Mechanism: The softmin operator computes a soft approximation of the minimum over per-pixel confidence values, controlled by a temperature parameter. This allows it to focus on the worst-labeled regions without being overly sensitive to noise in well-labeled regions.
- Core assumption: A well-trained model will produce lower confidence values for incorrectly labeled pixels compared to correctly labeled ones.
- Evidence anchors:
  - [abstract] "This approach is robust to minor statistical fluctuations in per-pixel model outputs"
  - [section 3.4] "Thus their mean, sCIL (x), may be undesirably sensitive to such sij variations in correctly-labeled regions of an image... Instead of taking the minimum of these scores, we can instead form a soft approximation of the minimum"
- Break condition: If the model systematically assigns low confidence to certain classes regardless of annotation correctness, the softmin score will fail to distinguish good from bad labels.

### Mechanism 2
- Claim: Using any trained segmentation model makes the method widely applicable across domains and future architectures.
- Mechanism: The method only requires probabilistic predictions from a trained model and the annotated labels, without needing special training procedures or synthesized images.
- Core assumption: The quality of the segmentation model's predictions is directly related to its ability to detect annotation errors.
- Evidence anchors:
  - [abstract] "Widely applicable, our label quality scores rely on probabilistic predictions from a trained segmentation model – any model architecture and training procedure can be utilized"
  - [section 2] "Here we consider universal LED methods that can be applied to any segmentation dataset for which someone has trained any standard type of segmentation model"
- Break condition: If the model is poorly trained or overfit to the noisy labels, its predictions will not be useful for detecting annotation errors.

### Mechanism 3
- Claim: Treating each pixel as an independent instance allows direct application of classification LED methods to segmentation data.
- Mechanism: By treating each pixel as a separate instance, the authors can apply confident learning and other classification LED techniques to estimate which pixels are mislabeled.
- Core assumption: The independence assumption holds sufficiently well for pixel-level analysis to be effective.
- Evidence anchors:
  - [section 3] "In this context, we adopt effective LED methodologies for standard classification scenarios (Northcutt et al., 2021b; Kuan & Mueller, 2022), treating every pixel as a distinct, independent instance"
  - [section 3.5] "Treating each pixel as an independent instance in a classification task (ignoring which image it belongs to), we apply Confident Learning to infer a binary mask estimating which pixels in image x are mislabeled"
- Break condition: If there are strong spatial correlations between pixel labels that violate the independence assumption, pixel-level methods may be less effective.

## Foundational Learning

- Concept: Semantic segmentation and its annotation challenges
  - Why needed here: Understanding the task and annotation process is crucial for grasping why annotation errors are prevalent and how they can be detected.
  - Quick check question: What is the main difference between semantic segmentation and object detection in terms of annotation?

- Concept: Model-agnostic approaches in machine learning
  - Why needed here: The method's ability to work with any trained model is a key innovation, so understanding what this means is important.
  - Quick check question: What are the advantages and disadvantages of model-agnostic approaches compared to model-specific ones?

- Concept: Label error detection (LED) in machine learning
  - Why needed here: The paper's core contribution is an LED method for segmentation data, so understanding LED concepts is essential.
  - Quick check question: How does label error detection differ from robust learning with noisy labels?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Load segmentation dataset, handle annotations
  - Model training: Train segmentation model(s) on dataset
  - Prediction generation: Produce out-of-sample predictions for all images
  - Pixel-level analysis: Compute per-pixel scores and flags
  - Image-level aggregation: Apply softmin or other methods to get image scores
  - Evaluation: Compute precision/recall metrics on detected errors

- Critical path:
  1. Train segmentation model(s)
  2. Generate out-of-sample predictions
  3. Compute per-pixel scores
  4. Aggregate to image-level scores
  5. Rank images by scores and evaluate

- Design tradeoffs:
  - Using pixel-level vs. component-level analysis (computational complexity vs. spatial context)
  - Choice of temperature parameter in softmin (sharpness of minimum approximation)
  - Whether to use cross-validation or separate train/validation split

- Failure signatures:
  - Softmin scores not discriminating between good and bad labels
  - High computational cost for large datasets
  - Model overfitting leading to poor error detection

- First 3 experiments:
  1. Verify softmin scores on a small, manually corrupted dataset
  2. Compare softmin performance to baseline methods on SYNTHIA variants
  3. Apply method to Cityscapes and inspect top-ranked errors visually

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the softmin label quality score compare to other methods when applied to real-world semantic segmentation datasets with naturally occurring annotation errors, beyond the synthetic errors introduced in the SYNTHIA dataset?
- Basis in paper: [explicit] The paper mentions that the softmin method was applied to the Cityscapes dataset and successfully identified naturally occurring label errors, but does not provide a comprehensive evaluation or comparison with other methods on real-world data.
- Why unresolved: The paper focuses on synthetic errors in the SYNTHIA dataset and only briefly mentions the application to Cityscapes. A thorough evaluation on real-world datasets with diverse types of annotation errors is needed to confirm the generalizability of the softmin method.
- What evidence would resolve it: Conduct experiments applying the softmin method and other label quality scoring methods to multiple real-world semantic segmentation datasets with known annotation errors, comparing their performance using metrics such as AUROC, AUPRC, and Lift.

### Open Question 2
- Question: How does the performance of the softmin label quality score vary with different segmentation model architectures and training procedures, beyond the DeepLabV3+ and FPN models used in the experiments?
- Basis in paper: [explicit] The paper states that the softmin method is model-agnostic and can be applied with any segmentation model, but the experiments only use two specific models (DeepLabV3+ and FPN).
- Why unresolved: While the paper demonstrates the effectiveness of the softmin method with two models, it is unclear how well it would perform with other popular segmentation architectures or with models trained using different procedures.
- What evidence would resolve it: Evaluate the softmin method and other label quality scoring methods using a diverse set of segmentation models, including state-of-the-art architectures and models trained with different procedures, on multiple datasets.

### Open Question 3
- Question: How does the softmin label quality score perform when the proportion of mislabeled images in the dataset varies, and how does this affect the trade-off between precision and recall in detecting annotation errors?
- Basis in paper: [inferred] The paper introduces synthetic errors in the SYNTHIA dataset with varying proportions (20%, 30%, and 20% for Drop, Swap, and Shift errors, respectively), but does not explore how the performance of the softmin method changes with different error proportions.
- Why unresolved: The paper does not investigate the robustness of the softmin method to different levels of annotation errors in the dataset, which is important for understanding its practical applicability.
- What evidence would resolve it: Conduct experiments with datasets containing different proportions of annotation errors, ranging from low to high, and evaluate the performance of the softmin method and other label quality scoring methods using precision-recall curves and other relevant metrics.

## Limitations
- Limited validation on real-world datasets with naturally occurring annotation errors
- Unclear how method performs with different segmentation model architectures and training procedures
- Computational efficiency not thoroughly evaluated, especially for Connected Components method

## Confidence
- High: Core claim that softmin outperforms other image-level scoring methods on synthetic SYNTHIA datasets with controlled error types
- Medium: Claims about real-world applicability to Cityscapes dataset
- Low: Computational efficiency claims, as detailed runtime comparisons are not provided

## Next Checks
1. Apply softmin scoring to a dataset with independently verified ground truth annotations (where true error locations are known) to quantify precision/recall on naturally occurring errors
2. Conduct a systematic study of temperature parameter τ sensitivity across different model architectures and dataset characteristics
3. Perform runtime analysis comparing softmin to other methods on datasets of increasing size to establish computational scaling properties