---
ver: rpa2
title: Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits
arxiv_id: '2308.03212'
source_url: https://arxiv.org/abs/2308.03212
tags:
- attention
- circuit
- average-hard
- circuits
- merrill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that average-hard attention transformers can be
  simulated by uniform TC0 circuits, settling an open question from previous work.
  The key insight is that while Merrill et al.
---

# Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits

## Quick Facts
- arXiv ID: 2308.03212
- Source URL: https://arxiv.org/abs/2308.03212
- Reference count: 2
- Primary result: Average-hard attention transformers can be uniformly simulated by TC0 circuits

## Executive Summary
This paper establishes that average-hard attention transformers can be simulated by uniform TC0 circuits, resolving an open question from previous work. While Merrill et al. (2022) showed average-hard attention transformers are in TC0, their construction was non-uniform. By building on the log-precision uniform TC0 simulation of Merrill and Sabharwal (2023a) and carefully analyzing average-hard attention operations, the paper constructs a uniform circuit that simulates an average-hard attention transformer layer, yielding a uniform TC0 simulation of the entire transformer model.

## Method Summary
The paper constructs a uniform TC0 circuit family that simulates average-hard attention transformers by decomposing the attention mechanism into uniform TC0-computable subcomponents (max, eq, sel, sum, div) and wiring them appropriately. The circuit family can be generated in logarithmic space by a Turing machine that iterates over input positions and constructs the sub-circuits and their interconnections. The key insight is that average-hard attention transformers produce only logarithmic-size floating-point numbers, allowing them to be treated as log-precision transformers and enabling the uniform TC0 simulation results to apply.

## Key Results
- Average-hard attention transformers can be uniformly simulated by TC0 circuits
- The circuit family can be generated in logarithmic space
- Average-hard attention transformers recognize exactly the uniform TC0 languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Average-hard attention transformers can be uniformly simulated by TC0 circuits
- Mechanism: The paper constructs a uniform TC0 circuit that simulates each transformer layer by decomposing the attention mechanism into uniform TC0-computable subcomponents (max, eq, sel, sum, div) and wiring them appropriately
- Core assumption: All internal computations use O(log n) precision floating-point numbers
- Evidence anchors:
  - [abstract] "Our paper shows that the first result can be extended to yield uniform circuits as well"
  - [section 3] "A schematic representation of the circuit structure for input size n, illustrating how it computes the attention vector"
  - [corpus] Weak evidence - no direct corpus papers discussing uniform circuit construction for average-hard attention
- Break condition: If any subcomponent cannot be computed by uniform TC0 circuits, or if precision requirements exceed O(log n)

### Mechanism 2
- Claim: The circuit family can be generated in logarithmic space
- Mechanism: A Turing machine can construct the circuit family using O(log n) space by iterating over input positions with loop variables and generating the sub-circuits and their interconnections
- Core assumption: Each sub-circuit can be constructed in logarithmic space
- Evidence anchors:
  - [section 3] "it is necessary to maintain a variable that tracks the index i" and "an additional loop variable... is required to keep track of the index j"
  - [corpus] Weak evidence - no direct corpus papers discussing logarithmic space circuit generation for transformers
- Break condition: If the circuit generation requires more than O(log n) space due to complex interconnections

### Mechanism 3
- Claim: Average-hard attention is computationally equivalent to log-precision transformers for language recognition
- Mechanism: Since average-hard attention transformers produce only logarithmic-size floating-point numbers (Theorem 4 in Merrill et al. 2022), they can be treated as log-precision transformers, allowing the uniform TC0 simulation results to apply
- Core assumption: The precision limitation from Theorem 4 applies to the entire computation
- Evidence anchors:
  - [abstract] "both the underlying assumptions and the specific attention mechanisms differ between the two studies"
  - [section 2] "we consider the implications of the results presented in Merrill and Sabharwal (2023a) and Merrill et al. (2022)"
  - [corpus] Moderate evidence - related papers discuss precision limitations and circuit complexity classes
- Break condition: If average-hard attention requires higher precision for correct computation

## Foundational Learning

- Concept: Circuit complexity classes (AC0, TC0)
  - Why needed here: The paper proves that average-hard attention transformers recognize exactly the uniform TC0 languages
  - Quick check question: What is the key difference between AC0 and TC0 circuits?

- Concept: Uniform circuit families
  - Why needed here: The result shows that average-hard attention transformers can be simulated by uniform (not just non-uniform) circuit families
  - Quick check question: What does it mean for a circuit family to be "uniform"?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The paper analyzes average-hard attention transformers specifically, which differ from standard softmax attention
  - Quick check question: How does average-hard attention differ from standard softmax attention?

## Architecture Onboarding

- Component map:
  - Input positional encoding circuit (uniform TC0)
  - Transformer layers (each with attention heads and feed-forward networks)
  - Attention head components: scoring function, max computation, equality checking, selection, summation, division
  - Output decision circuit (uniform TC0)

- Critical path: Input encoding → Transformer layers (with attention heads) → Final classification

- Design tradeoffs:
  - Precision vs. circuit complexity: Using O(log n) precision enables uniform TC0 simulation but may limit computational power
  - Uniformity vs. expressiveness: Uniform circuits are more robust but potentially less powerful than non-uniform circuits

- Failure signatures:
  - If attention heads cannot be computed by uniform TC0 circuits
  - If precision requirements exceed O(log n) for some inputs
  - If the circuit generation requires more than O(log n) space

- First 3 experiments:
  1. Implement the uniform TC0 circuit for a single average-hard attention head and verify it computes the same output as the original attention mechanism
  2. Test the circuit generation algorithm on small input sizes to verify it runs in O(log n) space
  3. Compare the language recognition power of average-hard attention transformers versus softmax attention transformers on benchmark problems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are average-hard attention transformers strictly less powerful than softmax attention transformers?
- Basis in paper: [explicit] The paper mentions "exploring the distinction between average-hard and softmax attention mechanisms, with the potential to unveil a clear demarcation between the two."
- Why unresolved: The paper establishes that average-hard attention transformers can be simulated by uniform TC0 circuits, but does not directly compare their expressive power to softmax attention transformers.
- What evidence would resolve it: A formal proof showing that softmax attention transformers can compute functions outside uniform TC0, or that they can simulate average-hard attention transformers but not vice versa.

### Open Question 2
- Question: Can the uniform TC0 simulation of average-hard attention transformers be optimized to reduce circuit depth or size?
- Basis in paper: [inferred] The paper presents a uniform TC0 simulation but does not explore optimization possibilities.
- Why unresolved: The construction follows a specific approach based on previous work, but alternative circuit constructions or optimizations are not explored.
- What evidence would resolve it: A more efficient uniform TC0 circuit construction for average-hard attention transformers, with improved depth or size bounds compared to the presented construction.

### Open Question 3
- Question: Does the result extend to transformers with attention mechanisms that lie between average-hard and softmax (e.g., sparse attention variants)?
- Basis in paper: [inferred] The paper focuses specifically on average-hard attention and its uniform TC0 simulation, without exploring intermediate attention mechanisms.
- Why unresolved: The paper's techniques are tailored to average-hard attention's specific properties, and it's unclear if they generalize to other attention variants.
- What evidence would resolve it: A formal proof showing that transformers with a specific intermediate attention mechanism can or cannot be simulated by uniform TC0 circuits, or that they have different computational power than both average-hard and softmax variants.

## Limitations
- The result relies on precision bounds from Theorem 4 in Merrill et al. (2022) that have not been independently verified
- The circuit construction is described at a high level of abstraction with some implementation details left unspecified
- The space complexity claim of O(log n) for the circuit generation algorithm has not been fully detailed

## Confidence
- High Confidence: The theoretical framework connecting average-hard attention transformers to uniform TC0 circuits is well-established
- Medium Confidence: The claim that average-hard attention transformers recognize exactly the uniform TC0 languages is contingent on the precision bounds holding across all inputs
- Low Confidence: The uniform circuit generation algorithm's space complexity claim of O(log n) has not been fully detailed in the paper

## Next Checks
1. Implement a comprehensive test suite to empirically verify that average-hard attention transformers consistently produce logarithmic-size floating-point numbers across diverse input distributions and transformer configurations.

2. Conduct a detailed space complexity analysis of the uniform circuit generation algorithm, including edge cases and worst-case scenarios, to confirm the O(log n) space claim.

3. Test the uniform TC0 simulation framework on multiple transformer architectures (different attention mechanisms, layer configurations) to verify that the results hold beyond the specific average-hard attention model analyzed in the paper.