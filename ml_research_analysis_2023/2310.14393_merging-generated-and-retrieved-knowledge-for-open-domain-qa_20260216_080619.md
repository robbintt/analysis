---
ver: rpa2
title: Merging Generated and Retrieved Knowledge for Open-Domain QA
arxiv_id: '2310.14393'
source_url: https://arxiv.org/abs/2310.14393
tags:
- passages
- passage
- retrieved
- knowledge
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of knowledge conflicts in open-domain
  question answering (QA) caused by the hallucination of large language models (LLMs)
  when combining retrieved and generated knowledge. The proposed COMBO framework aims
  to effectively leverage the two knowledge sources by matching LLM-generated passages
  with retrieved counterparts into compatible pairs based on discriminators trained
  with silver compatibility labels.
---

# Merging Generated and Retrieved Knowledge for Open-Domain QA

## Quick Facts
- arXiv ID: 2310.14393
- Source URL: https://arxiv.org/abs/2310.14393
- Reference count: 40
- Key outcome: COMBO outperforms competitive baselines on three out of four tested open-domain QA benchmarks

## Executive Summary
This paper addresses the challenge of knowledge conflicts in open-domain question answering when combining retrieved passages with LLM-generated passages. The proposed COMBO framework introduces compatibility-guided matching to pair retrieved and generated passages based on whether they contain compatible evidence. By training discriminators to identify evidential and consistent passage pairs using silver labels mined through prediction manipulation, COMBO effectively resolves knowledge conflicts. Experiments demonstrate superior performance on multiple QA benchmarks, particularly in scenarios with high knowledge conflict degrees.

## Method Summary
COMBO integrates retrieved and LLM-generated passages through a two-stage process: first, evidentiality and consistency discriminators are trained on silver compatibility labels mined by observing prediction changes when passages are removed; second, passages are optimally matched using a bipartite graph maximum-weighted matching algorithm based on compatibility scores. A Fusion-in-Decoder reader then processes these compatible passage pairs to generate final answers. The framework handles knowledge conflicts by prioritizing passage pairs that both contain correct evidence while filtering out incompatible or non-evidential passages.

## Key Results
- COMBO achieves state-of-the-art performance on NQ, TriviaQA, and WebQuestions datasets
- The framework shows greater efficacy in high-conflict scenarios where traditional merging approaches fail
- Ablation studies confirm that both discriminators and compatibility-guided matching contribute significantly to performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reader model learns to prioritize compatible passage pairs by observing that compatible pairs have higher attention scores during training
- Mechanism: The compatibility discriminators assign higher scores to passage pairs that both contain correct evidence, and the reader model observes this during training through the optimal matching process
- Core assumption: The reader model can learn to associate higher attention with higher compatibility scores
- Evidence anchors:
  - [abstract] "Further analysis reveals that our proposed framework demonstrates greater efficacy in scenarios with a higher degree of knowledge conflicts."
  - [section 4.3] "Our model assigns higher attention scores to compatible pairs than incompatible ones."
  - [corpus] Weak - no direct corpus evidence found

### Mechanism 2
- Claim: Silver compatibility labels can be automatically mined without human annotation by observing changes in model predictions when passages are removed
- Mechanism: Passages are labeled as compatible if removing either passage from a pair causes the prediction to become incorrect, indicating both contain correct evidence
- Core assumption: A QA model's prediction correctness change reliably indicates whether a passage contains correct evidence
- Evidence anchors:
  - [section 3.2] "we estimate the silver compatibility by checking whether the prediction correctness of a QA model would flip if one or both passages from a target pair were to be removed"
  - [section 3.2] "We consider (lpi, rpj) as consistent if the model's prediction is incorrect with II and all correct with I, III, and IV"
  - [corpus] Weak - the approach is novel and lacks external validation

### Mechanism 3
- Claim: The evidentiality-cutoff heuristic improves performance by binarizing the evidentiality decision before computing consistency
- Mechanism: Non-evidential retrieved passages are scored as zero compatibility, preventing them from being matched with LLM-generated passages
- Core assumption: Raw predicted probabilities from the evidentiality discriminator are not well-calibrated
- Evidence anchors:
  - [section 3.3] "Equation 1 directly multiplies the two probabilities given by the two discriminators together. It demonstrates that models benefit from a binarized decision by DE"
  - [section 3.3] "Equation 1 directly multiplies the two probabilities given by the two discriminators together"
  - [corpus] Weak - the calibration claim references external work (Guo et al., 2017; Jiang et al., 2021) but lacks direct evidence

## Foundational Learning

- Concept: Compatibility-oriented matching
  - Why needed here: Direct merging of retrieved and generated passages ignores knowledge conflicts that can mislead the reader model
  - Quick check question: Why can't we just concatenate all passages and let the reader figure out conflicts on its own?

- Concept: Silver label mining through prediction manipulation
  - Why needed here: Human annotation of compatibility labels would be too expensive at scale
  - Quick check question: How does removing passages help us determine if they contain correct evidence?

- Concept: Bipartite graph maximum-weighted matching
  - Why needed here: We need to pair passages optimally while balancing usage across all passages
  - Quick check question: Why use a matching algorithm instead of just ranking all possible pairs?

## Architecture Onboarding

- Component map: Retriever → LLM generator → Compatibility discriminators (evidentiality + consistency) → Optimal matching → FiD reader
- Critical path: Question → Retrieved passages + LLM passages → Compatibility scoring → Matching → Reader prediction
- Design tradeoffs: Training discriminators adds complexity but enables better knowledge conflict resolution; matching algorithm ensures balanced passage usage
- Failure signatures: If performance doesn't improve over direct merging, likely issues are noisy silver labels or ineffective compatibility scoring
- First 3 experiments:
  1. Run with just evidentiality discriminator to see if filtering non-evidential passages helps
  2. Test random matching vs optimal matching to validate matching algorithm importance
  3. Try removing the evidentiality-cutoff to see if binarization actually helps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can COMBO be extended to few-shot QA settings using in-context learning?
- Basis in paper: [explicit] The paper mentions plans to extend the framework to few-shot QA settings by employing an LLM with in-context learning to compute compatibility scores instead of training a discriminator from scratch.
- Why unresolved: The paper only mentions this as a future direction without providing concrete implementation details or experimental results.
- What evidence would resolve it: Experimental results showing the performance of COMBO in few-shot settings with in-context learning, and a comparison to the current approach.

### Open Question 2
- Question: Can COMBO be applied to other knowledge-intensive NLP tasks such as fact checking or knowledge-enhanced text generation?
- Basis in paper: [explicit] The paper states that it only evaluates the knowledge merging framework on open-domain QA tasks and suggests it would be interesting to apply the method to other tasks like fact checking and knowledge-enhanced text generation.
- Why unresolved: The paper does not provide any experiments or analysis on these other tasks, so the effectiveness of COMBO in these domains is unknown.
- What evidence would resolve it: Experiments applying COMBO to fact checking and knowledge-enhanced text generation tasks, with performance comparisons to baselines.

### Open Question 3
- Question: How can the silver label mining approach be improved to handle smaller datasets with limited labels?
- Basis in paper: [explicit] The paper mentions that the silver label mining approach could result in a limited amount of labels on a small dataset, which could not provide sufficient data to train a dataset-specific discriminator.
- Why unresolved: The paper does not propose any solutions to address this limitation, so the impact of limited labels on the performance of COMBO is unclear.
- What evidence would resolve it: Experiments evaluating the performance of COMBO on small datasets with limited labels, and analysis of the impact of label scarcity on the discriminators and overall framework.

## Limitations
- The silver label mining approach may produce noisy labels when the base reader model is unreliable
- The framework requires additional training of compatibility discriminators, increasing computational complexity
- Performance depends on the quality of LLM-generated passages, which may vary across different prompts and models

## Confidence
- **High confidence**: The basic architecture design (discriminators → matching → FiD reader) and the core claim that knowledge conflicts hurt performance
- **Medium confidence**: The specific mechanism of using prediction changes to mine silver labels, as this is novel but intuitively sound
- **Medium confidence**: The efficacy of compatibility-guided matching over direct merging, supported by experiments but with room for ablation studies
- **Low confidence**: The calibration assumptions and the need for evidentiality-cutoff binarization, which lacks direct validation

## Next Checks
1. Ablation study on matching: Compare COMBO performance with random matching vs. compatibility-guided matching to isolate the matching algorithm's contribution
2. Silver label quality analysis: Measure how often the leave-one-out prediction changes actually indicate true evidence presence vs. random noise
3. Evidentiality discriminator calibration: Test whether raw probability outputs from the evidentiality discriminator are actually poorly calibrated as claimed