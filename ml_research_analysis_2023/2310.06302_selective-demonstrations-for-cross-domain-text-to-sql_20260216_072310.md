---
ver: rpa2
title: Selective Demonstrations for Cross-domain Text-to-SQL
arxiv_id: '2310.06302'
source_url: https://arxiv.org/abs/2310.06302
tags:
- examples
- in-domain
- arxiv
- synthetic
- codex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how to best leverage in-domain and out-of-domain
  demonstrations in cross-domain text-to-SQL tasks using large language models (LLMs).
  The authors find that SQL distribution is more important than NLQ distribution or
  task knowledge for in-domain demonstrations.
---

# Selective Demonstrations for Cross-domain Text-to-SQL

## Quick Facts
- arXiv ID: 2310.06302
- Source URL: https://arxiv.org/abs/2310.06302
- Reference count: 13
- One-line primary result: ODIS framework improves text-to-SQL execution accuracy by 1.1 and 11.8 points using SQL-guided demonstration retrieval

## Executive Summary
This paper addresses the challenge of cross-domain text-to-SQL by analyzing the importance of different demonstration characteristics in in-context learning. The authors propose ODIS, a framework that retrieves demonstrations from both out-of-domain and synthetically generated in-domain data, with a focus on SQL distribution rather than NLQ distribution or task knowledge format. The framework uses SQL-guided retrieval methods (SimSQL and CovSQL) to select relevant demonstrations, achieving state-of-the-art performance on two cross-domain text-to-SQL datasets.

## Method Summary
ODIS is a demonstration selection framework for cross-domain text-to-SQL that leverages both out-of-domain demonstrations from Spider training set and synthetically generated in-domain examples. The method uses SQL-guided retrieval (SimSQL for out-of-domain, CovSQL for in-domain) to select demonstrations based on SQL similarity rather than NLQ similarity. The framework generates synthetic examples using SHiP or GAZPSQL SQL synthesis methods, then uses LLM-based NLQ generation. Demonstrations are combined with database schema information in a prompt to guide SQL generation, improving execution accuracy on unseen databases.

## Key Results
- ODIS improves execution accuracy by 1.1 points on one dataset and 11.8 points on another compared to state-of-the-art methods
- SQL distribution in demonstrations is more important than NLQ distribution or task knowledge format for in-domain demonstrations
- The framework shows robustness to NLQ perturbations across different LLMs including Codex, ChatGPT, and CodeLlama
- Combining out-of-domain task knowledge with synthetic in-domain SQL distribution provides complementary benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SQL distribution in demonstrations is more important than NLQ distribution or task knowledge format.
- Mechanism: The model learns SQL patterns and schema-specific structures through demonstrations, enabling it to generalize to unseen databases.
- Core assumption: The LLM can learn SQL generation patterns independently of NLQ understanding.
- Evidence anchors:
  - [abstract] "we propose ODIS, a framework that retrieves demonstrations from both out-of-domain and synthetically generated in-domain data"
  - [section] "Our experiments show that SQL distribution plays a pivotal role in in-domain demonstrations"
  - [corpus] Weak - related work focuses on retrieval methods but doesn't directly validate SQL distribution importance
- Break condition: If SQL generation requires specific NLQ understanding that cannot be generalized from SQL patterns alone.

### Mechanism 2
- Claim: Combining out-of-domain demonstrations with synthetic in-domain examples provides complementary benefits.
- Mechanism: Out-of-domain examples provide task knowledge and format, while synthetic examples provide SQL distribution without requiring annotated data.
- Core assumption: Task knowledge can be effectively learned from out-of-domain examples, allowing synthetic examples to focus on SQL distribution.
- Evidence anchors:
  - [section] "ODIS leverages the advantages of both, showcasing its effectiveness compared to baseline methods that rely on a single data source"
  - [section] "ODIS outperforms state-of-the-art approaches on two cross-domain text-to-SQL datasets"
  - [corpus] Weak - limited discussion of hybrid demonstration approaches in related work
- Break condition: If task knowledge from out-of-domain examples is insufficient for SQL generation.

### Mechanism 3
- Claim: SQL-guided retrieval methods (SimSQL and CovSQL) improve demonstration selection effectiveness.
- Mechanism: Retrieval based on SQL similarity or coverage ensures demonstrations are relevant to the test SQL structure.
- Core assumption: SQL similarity is a better indicator of demonstration relevance than NLQ similarity.
- Evidence anchors:
  - [section] "We further hypothesize that the similarity of the output SQL queries between test examples and demonstration examples is more important than input similarity"
  - [section] "Retrieving out-of-domain examples with similar predicted SQL queries outperforms random selection"
  - [corpus] Weak - limited empirical comparison of SQL vs NLQ similarity for retrieval
- Break condition: If SQL similarity doesn't correlate with generation performance for certain SQL patterns.

## Foundational Learning

- Concept: In-context learning mechanics
  - Why needed here: Understanding how LLMs use demonstrations to generate SQL without fine-tuning
  - Quick check question: What happens if demonstration examples are shuffled or mismatched?

- Concept: Text-to-SQL task structure
  - Why needed here: Need to understand schema linking, SQL syntax, and natural language question patterns
  - Quick check question: How does the model map database schema elements to natural language concepts?

- Concept: Synthetic data generation
  - Why needed here: Creating in-domain examples requires understanding SQL template extraction and NLQ generation
  - Quick check question: What makes synthetic SQL queries "realistic" for the target database schema?

## Architecture Onboarding

- Component map: LLM (Codex/ChatGPT/CodeLlama) -> Out-of-domain demonstration pool (Spider) -> Synthetic in-domain generator (SHiP/GAZPSQL) -> Retrieval algorithms (SimSQL/CovSQL) -> Database schema parser and prompt constructor

- Critical path: 
  1. Generate initial SQL prediction with zero-shot model
  2. Retrieve out-of-domain demonstrations using SimSQL
  3. Generate and retrieve synthetic in-domain examples using CovSQL
  4. Construct prompt with hybrid demonstrations
  5. Generate final SQL prediction

- Design tradeoffs:
  - Quality vs quantity of synthetic examples
  - Retrieval accuracy vs computational cost
  - Demonstration diversity vs similarity
  - Zero-shot model quality vs retrieval effectiveness

- Failure signatures:
  - Low SQL coverage in demonstrations → poor generation on complex queries
  - Poor synthetic data quality → SQL generation errors
  - Overfitting to demonstration SQL patterns → poor generalization
  - Retrieval bias toward certain SQL patterns → uneven performance

- First 3 experiments:
  1. Compare SQL vs NLQ similarity for demonstration retrieval
  2. Test impact of synthetic data quality on performance
  3. Evaluate robustness to NLQ perturbations with different demonstration sources

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ODIS scale with increasing amounts of synthetic data?
- Basis in paper: [inferred] The paper mentions that the quality of synthetic data is crucial for the effectiveness of ODIS, but does not explore the impact of varying the quantity of synthetic data.
- Why unresolved: The paper focuses on the quality of synthetic data and compares two SQL synthesis methods, but does not investigate the effect of using different amounts of synthetic data.
- What evidence would resolve it: Experiments varying the number of synthetic examples used in ODIS, measuring execution accuracy on text-to-SQL datasets.

### Open Question 2
- Question: How does ODIS perform on text-to-SQL tasks in languages other than English?
- Basis in paper: [inferred] The paper only evaluates ODIS on English text-to-SQL datasets, so its performance on non-English tasks is unknown.
- Why unresolved: The experiments and datasets used in the paper are all in English, so the effectiveness of ODIS for other languages is not addressed.
- What evidence would resolve it: Evaluating ODIS on text-to-SQL datasets in various languages, comparing execution accuracy to baseline methods.

### Open Question 3
- Question: Can ODIS be extended to few-shot, parameter-efficient fine-tuning by leveraging hybrid data sources?
- Basis in paper: [explicit] The paper mentions this as a potential direction for future work in the conclusion section.
- Why unresolved: The paper focuses on in-context learning with demonstrations and does not explore fine-tuning approaches using the hybrid data sources.
- What evidence would resolve it: Experiments comparing the performance of fine-tuning models using the hybrid data sources (out-of-domain and synthetic in-domain) to traditional fine-tuning and in-context learning approaches.

## Limitations

- Limited empirical validation of SQL distribution hypothesis through direct ablation studies
- Synthetic data quality concerns and lack of comprehensive quality metrics
- Retrieval method limitations with insufficient comparison to alternative strategies
- Performance evaluation limited to English text-to-SQL datasets

## Confidence

**High confidence**: The overall framework design and experimental methodology are well-structured. The execution accuracy improvements (1.1 and 11.8 points) are measurable and reproducible. The hybrid approach of combining out-of-domain and synthetic in-domain demonstrations is logically sound.

**Medium confidence**: The SQL distribution hypothesis and the effectiveness of SQL-guided retrieval methods. While the paper provides supporting evidence, more direct validation through controlled ablation studies would strengthen these claims.

**Low confidence**: The quality and representativeness of synthetic in-domain examples, and the framework's performance on databases with significantly different characteristics than those in the training and demonstration sets.

## Next Checks

1. **Ablation study on demonstration components**: Conduct controlled experiments isolating the impact of SQL distribution, NLQ distribution, and task knowledge in demonstrations by systematically varying each component while holding others constant. This would directly test the core hypothesis about SQL distribution importance.

2. **Synthetic data quality analysis**: Implement comprehensive metrics to evaluate synthetic SQL quality, including coverage of different SQL patterns, realism of generated NLQs, and alignment between NLQ-SQL pairs. Compare the performance impact of different synthetic generation methods beyond the SHiP vs GAZPSQL comparison.

3. **Retrieval method stress testing**: Design experiments that specifically target failure modes for SQL-guided retrieval, such as testing on databases with novel SQL patterns, evaluating retrieval performance across different query complexity levels, and comparing against alternative retrieval strategies including NLQ similarity and hybrid approaches.