---
ver: rpa2
title: Global Hierarchical Neural Networks using Hierarchical Softmax
arxiv_id: '2308.01210'
source_url: https://arxiv.org/abs/2308.01210
tags:
- hierarchical
- softmax
- global
- classifier
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using hierarchical softmax to build a global
  hierarchical classifier for tasks with natural class hierarchies. The method replaces
  the regular softmax in neural networks with a hierarchical softmax, allowing the
  network to classify at each level of the hierarchy.
---

# Global Hierarchical Neural Networks using Hierarchical Softmax

## Quick Facts
- arXiv ID: 2308.01210
- Source URL: https://arxiv.org/abs/2308.01210
- Authors: 
- Reference count: 39
- Primary result: Hierarchical softmax improves macro-F1 and macro-recall on four text classification datasets

## Executive Summary
This paper proposes using hierarchical softmax as a replacement for regular softmax in neural networks for classification tasks with natural class hierarchies. The approach allows neural networks to classify at each level of the hierarchy, improving performance on four text classification datasets in terms of macro-F1 and macro-recall. The hierarchical softmax enables more efficient learning by focusing on smaller class subsets at each level, reducing the number of parameters compared to local classifiers. The method is applicable to any classification task with a natural class hierarchy.

## Method Summary
The paper proposes replacing the regular softmax with a hierarchical softmax in neural networks for classification tasks with natural class hierarchies. The hierarchical softmax uses a softmax at every node to calculate conditional probabilities, allowing the model to focus on classifying smaller subsets of classes at each level rather than distinguishing between all classes at once. This approach improves performance on four text classification datasets in terms of macro-F1 and macro-recall, with three datasets also showing higher micro-accuracy and macro-precision. The hierarchical softmax enables more efficient learning by focusing on smaller class subsets at each level, reducing the number of parameters compared to local classifiers.

## Key Results
- Hierarchical softmax improves macro-F1 and macro-recall on four text classification datasets
- Three datasets show higher micro-accuracy and macro-precision compared to regular softmax
- The hierarchical softmax enables more efficient learning by focusing on smaller class subsets at each level

## Why This Works (Mechanism)

### Mechanism 1
- Hierarchical softmax replaces regular softmax, allowing classification at each level of the hierarchy
- Uses softmax at every node to calculate conditional probabilities
- Focuses on smaller class subsets rather than all classes at once
- Core assumption: Class taxonomy has natural hierarchical structure
- Evidence: Limited direct research on hierarchical softmax for classification (FMR score 0.645)

### Mechanism 2
- Enables efficient learning by focusing on smaller class subsets
- Reduces parameters compared to local classifiers
- Each weight vector specializes in discriminating within its subgroup
- Core assumption: Parent nodes have manageable number of child nodes
- Evidence: Performance improvements on four datasets

### Mechanism 3
- Truly global classifier as whole network updates based on all relevant parent nodes
- Gradients calculated for all parent nodes leading to correct class
- Core assumption: Hierarchical structure is a tree
- Evidence: Limited research on global hierarchical classification

## Foundational Learning

- Concept: Hierarchical classification
  - Why needed: Paper's main contribution is global hierarchical classifier
  - Quick check: What is the difference between local and global hierarchical classifiers?

- Concept: Softmax function
  - Why needed: Proposes replacing regular softmax with hierarchical softmax
  - Quick check: How does regular softmax calculate class probabilities?

- Concept: Cross-entropy loss
  - Why needed: Used as loss function for training hierarchical softmax
  - Quick check: What is the mathematical formula for cross-entropy loss?

## Architecture Onboarding

- Component map: Input text -> GloVe embeddings -> LSTM/BiLSTM -> Hierarchical softmax -> Class probabilities
- Critical path: Forward pass: Input text → Embeddings → Recurrent layer → Hierarchical softmax → Class probabilities; Backward pass: Calculate loss → Compute gradients → Update weights
- Design tradeoffs: Number of levels in hierarchy, hidden state dimension, dropout rate
- Failure signatures: Poor performance on unbalanced datasets, slow convergence, overfitting
- First 3 experiments:
  1. Implement LSTM with regular softmax on 20NewsGroups to establish baseline
  2. Replace regular softmax with hierarchical softmax using provided class hierarchy
  3. Compare hierarchical softmax vs regular softmax on macro-F1, macro-recall, micro-accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does hierarchical softmax performance compare to state-of-the-art methods?
- Basis: Paper mentions results were close to SOTA but doesn't provide direct comparison
- Resolution needed: Table comparing hierarchical softmax results with SOTA results on same datasets

### Open Question 2
- Question: How does performance vary with different hierarchical structures in class taxonomy?
- Basis: Only explores two-level hierarchical taxonomy
- Resolution needed: Experiments comparing performance with different hierarchical structures

### Open Question 3
- Question: How does hierarchical softmax compare to other hierarchical classification methods?
- Basis: Mentions local vs global methods but doesn't compare to them
- Resolution needed: Experiments comparing hierarchical softmax to local classifiers and other hierarchical methods

## Limitations

- Empirical validation limited to four text classification datasets
- No ablation studies examining impact of hierarchy depth or structure
- Comparison with local classifiers is theoretical rather than empirical
- No experiments demonstrating failure scenarios

## Confidence

- **High confidence**: Mathematical formulation and theoretical soundness of hierarchical softmax
- **Medium confidence**: Performance improvements on four tested datasets with limited generalizability
- **Low confidence**: Claim of applicability to any classification task with natural class hierarchy

## Next Checks

1. Conduct ablation study on hierarchy depth to identify optimal structures and failure modes
2. Test approach on non-text classification tasks (e.g., image classification) to verify generality claim
3. Implement and empirically compare global hierarchical classifier against local classifier approach