---
ver: rpa2
title: 'Machine Learning for SAT: Restricted Heuristics and New Graph Representations'
arxiv_id: '2307.09141'
source_url: https://arxiv.org/abs/2307.09141
tags:
- time
- learning
- graph-q-sat
- steps
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces several techniques to reduce computational
  costs when using Graph-Q-SAT for SAT solving. The key idea is to run the heavy reinforcement
  learning agent only for a few initial steps and then release control to a classical
  SAT solver.
---

# Machine Learning for SAT: Restricted Heuristics and New Graph Representations

## Quick Facts
- arXiv ID: 2307.09141
- Source URL: https://arxiv.org/abs/2307.09141
- Authors: 
- Reference count: 36
- The paper introduces techniques to reduce computational costs in Graph-Q-SAT by limiting RL agent usage and releasing control to classical solvers

## Executive Summary
This paper presents several techniques to reduce the computational overhead of Graph-Q-SAT, a reinforcement learning approach for SAT solving that uses graph neural networks. The key insight is that early branching decisions are most influential, so the heavy RL agent should only be run for initial steps before releasing control to a classical SAT solver. The paper introduces an action pool mechanism to precompute multiple actions from a single GNN run, reducing expensive evaluations, and proposes a compact graph representation specifically for open shop scheduling problems that significantly reduces model size while maintaining solving effectiveness.

## Method Summary
The method builds on Graph-Q-SAT by implementing three main modifications: (1) running the GNN-based RL agent for only a limited number of initial steps before switching to MiniSAT, (2) introducing an action pool to precompute multiple branching decisions from a single GNN evaluation, and (3) creating a compact graph representation for open shop scheduling problems that operates directly on the original problem instance rather than the SAT encoding. The RL agent is trained on random 3-SAT and coloring problems, then tested on SR(n) benchmarks, industrial instances, and OSSP datasets, with the goal of reducing both iteration counts and runtime while maintaining solution quality.

## Key Results
- Significant reduction in the number of iterations and running time compared to baseline MiniSAT and original Graph-Q-SAT
- Action pool mechanism reduces expensive GNN evaluations while maintaining solution quality
- Compact OSSP graph representation reduces model size while improving SAT solving performance on scheduling problems
- Using predicted Q-values to initialize MiniSAT's activity scores provides additional performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-Q-SAT reduces computational costs by running the heavy RL agent for only a few initial steps before releasing control to a classical SAT solver
- Mechanism: The GNN-based RL agent provides valuable guidance during the critical early steps when classical heuristics lack meaningful statistics. By limiting GNN usage to the initial phase, computational overhead is significantly reduced while still benefiting from learned heuristics
- Core assumption: The most influential decisions come at the beginning, and this is exactly when classical SAT solving heuristics such as MiniSAT activity scores are not yet meaningful
- Evidence anchors: [abstract]: "The key idea is to run the heavy reinforcement learning agent only for a few initial steps and then release control to a classical SAT solver"

### Mechanism 2
- Claim: The action pool modification reduces the number of expensive GNN evaluations by precomputing multiple actions from a single GNN run
- Mechanism: Since the formula structure changes only slightly after each SAT solving step, the list of top actions with highest Q-values will also change only slightly. Therefore, multiple actions can be precomputed and executed sequentially, reducing the frequency of expensive GNN evaluations
- Core assumption: The Q-value predictions remain relevant for multiple consecutive steps due to minimal changes in the formula structure
- Evidence anchors: [abstract]: "Additionally, an action pool is introduced to precompute multiple actions from a single GNN run, reducing the number of expensive GNN evaluations"

### Mechanism 3
- Claim: The new compact graph representation for open shop scheduling problems (OSSP) significantly reduces model size and improves SAT solving efficiency
- Mechanism: Instead of representing the SAT formula directly, the GNN operates on the original OSSP instance graph, which is much smaller. Vertices represent operations with labels containing operation time, earliest start time, and deadline, while edges represent temporal dependencies
- Core assumption: The original problem structure contains sufficient information for effective branching decisions, and the smaller graph representation maintains the necessary information while being more computationally efficient
- Evidence anchors: [abstract]: "A new compact graph representation tailored for open shop scheduling problems is also proposed, significantly reducing model size compared to the standard encoding"

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) for SAT solving
  - Why needed here: GNNs are used to learn Q-values for branching decisions in SAT solvers, providing a way to incorporate structural information from the formula into the decision-making process
  - Quick check question: How does a GNN propagate information through a variable-clause graph representation of a SAT formula?

- Concept: Reinforcement Learning for heuristic selection
  - Why needed here: RL is used to train the GNN to predict Q-values that guide branching decisions, learning from the solver's performance rather than explicit labels
  - Quick check question: What is the reward structure used to train the RL agent in Graph-Q-SAT, and how does it differ from standard Q-learning approaches?

- Concept: Markov Decision Process formulation for SAT solving
  - Why needed here: The SAT solving process is modeled as an MDP where states are defined by unassigned variables and unsatisfied clauses, actions are variable assignments, and rewards are based on solving time
  - Quick check question: How are the states and actions defined in the Graph-Q-SAT MDP formulation, and what constitutes a terminal state?

## Architecture Onboarding

- Component map: Graph-Q-SAT Agent -> Action Pool -> CoPilot Mechanism -> MiniSAT Solver -> OSSP Graph Representation
- Critical path:
  1. Initialize SAT problem and construct appropriate graph representation
  2. Run GNN-based RL agent for initial steps (or until CoPilot signals release)
  3. Precompute action pool if enabled
  4. Release control to MiniSAT with initialized activity scores
  5. Solve remaining problem using classical heuristics

- Design tradeoffs:
  - Number of initial steps vs. computational overhead: More steps provide better guidance but increase cost
  - Action pool size vs. prediction relevance: Larger pools reduce GNN calls but may include less relevant actions
  - Graph representation complexity vs. model size: More detailed representations provide better guidance but increase computational requirements

- Failure signatures:
  - If action pool leads to invalid or suboptimal decisions due to significant environment changes
  - If CoPilot mechanism releases control too early or too late
  - If GNN predictions are unreliable for the specific problem domain

- First 3 experiments:
  1. Implement and test the basic Graph-Q-SAT agent with fixed number of initial steps (1-5) on SR(n) benchmarks
  2. Add action pool modification and evaluate impact on both step count and runtime for SR(500) instances
  3. Implement CoPilot mechanism and test on industrial benchmarks to evaluate when to release control optimally

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed approach scale to industrial-sized SAT instances beyond the tested benchmarks?
- Basis in paper: [explicit] The paper mentions evaluating on industrial SAT instances from the maris05 subset of the SAT 2005 Competition benchmark, but notes that Graph-Q-SAT graphs for realistic sized problems become so large that they do not fit into GPU memory for available hardware
- Why unresolved: The paper only tested on relatively small industrial instances and did not evaluate on larger industrial problems that are more representative of real-world applications
- What evidence would resolve it: Results on larger industrial SAT instances (e.g., with 10,000+ variables and 50,000+ clauses) would show how well the approach scales

### Open Question 2
- Question: Can the action pool approach be extended to dynamically adapt the number of actions per model run based on the current solver state?
- Basis in paper: [explicit] The paper mentions that the number k (actions per agent run) is a constant hyperparameter, but notes as a possible extension that k could also be adaptive
- Why unresolved: The paper only experimented with fixed values of k and did not explore adaptive approaches
- What evidence would resolve it: An adaptive approach that chooses k based on features of the current solver state (e.g., formula complexity, progress so far) could be implemented and compared against fixed k on various benchmarks

### Open Question 3
- Question: How does the OSSP-specific graph representation compare to the standard SAT graph representation on general SAT instances that are not OSSP-derived?
- Basis in paper: [explicit] The paper introduces a new compact graph representation tailored for OSSP instances, but only evaluates it on OSSP datasets
- Why unresolved: The paper does not test the OSSP graph representation on standard SAT benchmarks
- What evidence would resolve it: Running Graph-Q-SAT with the OSSP graph representation on standard SAT benchmarks (e.g., SR(n), industrial instances) and comparing results to the standard SAT graph representation would show if the OSSP representation is beneficial more broadly

## Limitations
- Effectiveness depends on the assumption that early branching decisions are most influential, which may not hold for all SAT problem classes
- Action pool mechanism assumes minimal changes in Q-value predictions between steps, but this could break down for problems with complex interdependencies
- The compact OSSP graph representation may lose critical information for effective decision-making

## Confidence
- High confidence in the computational efficiency gains from limiting RL agent steps
- Medium confidence in the action pool mechanism
- Medium confidence in the OSSP compact representation

## Next Checks
1. Test the action pool mechanism across diverse SAT problem classes to verify the assumption about minimal Q-value changes holds generally
2. Conduct ablation studies on the OSSP compact representation to quantify exactly what information is lost versus gained in computational efficiency
3. Evaluate the CoPilot release mechanism on problems where classical heuristics are known to perform well early, to verify it doesn't prematurely release control in cases where GNN guidance remains valuable