---
ver: rpa2
title: 'Domain Watermark: Effective and Harmless Dataset Copyright Protection is Closed
  at Hand'
arxiv_id: '2310.14942'
source_url: https://arxiv.org/abs/2310.14942
tags:
- dataset
- domain
- samples
- watermark
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a harmless method for dataset ownership verification
  using domain watermarking. Unlike existing backdoor-based approaches that introduce
  harmful misclassifications, the proposed method ensures watermarked models correctly
  classify 'hard' samples that benign models misclassify.
---

# Domain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand

## Quick Facts
- arXiv ID: 2310.14942
- Source URL: https://arxiv.org/abs/2310.14942
- Reference count: 40
- Primary result: Introduces harmless dataset copyright protection via domain watermarking that achieves >90% benign accuracy and verification success rates while being resistant to adaptive attacks.

## Executive Summary
This paper presents a novel approach to dataset copyright protection using domain watermarking, addressing the fundamental limitation of existing backdoor-based methods that introduce harmful misclassifications. The proposed method ensures watermarked models correctly classify "hard" samples that benign models misclassify, rather than forcing misclassification of "easy" samples. Through bi-level optimization, the approach finds a hardly-generalized domain for the original dataset and embeds it as a watermark. Experimental results on CIFAR-10, Tiny-ImageNet, and STL-10 demonstrate the method achieves high benign accuracy (>90%) and verification success rates (90.45% on CIFAR-10) while maintaining harmlessness and resistance to adaptive attacks.

## Method Summary
The method involves finding a hardly-generalized domain for the original dataset through bi-level optimization, then using this domain as a watermark embedded in the training dataset via visually indistinguishable modified samples. The approach consists of a transformation module that generates domain-watermarked samples, a bi-level optimization framework that optimizes both the domain watermark and protected dataset, and a hypothesis-test-guided verification method that compares model predictions on benign versus domain-watermarked samples. The key innovation is that watermarked models are trained to correctly classify hard samples from the hardly-generalized domain, while benign models fail on them, enabling effective ownership verification without introducing harmful misclassifications.

## Key Results
- Achieves 90.45% verification success rate on CIFAR-10 with 100% watermarking rate
- Maintains high benign accuracy (>90%) across all tested datasets
- Demonstrates resistance to adaptive attacks including fine-tuning and pruning
- Shows only 0.31% harmful degree on CIFAR-10 with 100% watermarking rate

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The method achieves harmlessness by training watermarked models to correctly classify "hard" samples that benign models misclassify, rather than forcing misclassification of "easy" samples as in backdoor-based methods.
- **Mechanism**: The approach finds a "hardly-generalized domain" for the original dataset and uses it as a domain watermark. Models trained on the protected dataset containing samples from this domain learn to correctly classify these hard samples, while benign models fail on them.
- **Core assumption**: The existence of a hardly-generalized domain that is difficult for models trained on the original dataset to classify correctly, but becomes learnable when the training set includes samples from this domain.
- **Evidence anchors**:
  - [abstract] states the method makes watermarked models "correctly classify some 'hard' samples that will be misclassified by the benign model."
  - [section 3.2] defines the domain watermark property requiring watermarked models to correctly classify domain-watermarked samples while benign models cannot.
  - [corpus] shows related watermarking work focuses on protecting copyright through backdoor attacks, but this paper explicitly avoids that harmful mechanism.
- **Break condition**: If the hardly-generalized domain becomes learnable by any model through standard training, or if the difference in classification performance between watermarked and benign models disappears.

### Mechanism 2
- **Claim**: The domain watermark can be embedded in the training dataset through modified samples that are visually indistinguishable from original data while inducing the desired hard-domain effect.
- **Mechanism**: The method optimizes a set of visually-indistinguishable modified data that have similar effects to domain-watermarked samples from the hardly-generalized domain. This is done via bi-level optimization that ensures the modified samples are imperceptible while maintaining the watermark's effectiveness.
- **Core assumption**: It is possible to generate modified samples that are visually indistinguishable from originals while still belonging to the hardly-generalized domain in feature space.
- **Evidence anchors**:
  - [section 3.4] describes optimizing "visually-indistinguishable modified data having similar effects to domain-watermarked samples."
  - [section 3.3] explains using a transformation module with convolutional operations to generate domain-watermarked images that minimize mutual information between features.
  - [section 5.1] shows the method maintains high benign accuracy (>90%) while achieving verification success rates, indicating the watermark doesn't significantly degrade model performance.
- **Break condition**: If the modified samples become visually detectable or if their effect on model behavior becomes too weak to distinguish watermarked from benign models.

### Mechanism 3
- **Claim**: The method enables effective ownership verification through hypothesis testing that compares model predictions on benign versus domain-watermarked samples.
- **Mechanism**: The verification process uses a hypothesis test to determine if a suspicious model's prediction behavior differs significantly between benign samples and their domain-watermarked counterparts. A model trained on the protected dataset should show similar prediction confidence for both types, while a benign model should show lower confidence on domain-watermarked samples.
- **Core assumption**: The difference in prediction behavior between watermarked and benign models on domain-watermarked samples is statistically significant and detectable through hypothesis testing.
- **Evidence anchors**:
  - [section 4] describes the hypothesis-test-guided method comparing posterior probabilities on ground-truth labels between benign and domain-watermarked samples.
  - [section 5.2] shows experimental results with ∆P values and p-values demonstrating successful verification, with malicious cases showing ∆P ≈ 0 and p-values ≪ 0.01.
  - [section 3.2] defines the domain watermark properties that enable this verification approach.
- **Break condition**: If the statistical test cannot reliably distinguish between watermarked and benign models, or if adaptive attacks successfully mask the difference in prediction behavior.

## Foundational Learning

- **Concept**: Domain adaptation and generalization theory
  - Why needed here: The method relies on finding a hardly-generalized domain and understanding how models generalize across domains, which requires knowledge of domain adaptation theory and generalization bounds.
  - Quick check question: What is the relationship between mutual information between source and target domain features and the generalization bound on the target domain?

- **Concept**: Backdoor attacks and watermarking techniques
  - Why needed here: Understanding existing backdoor-based watermarking methods is crucial to appreciate why this approach is different and harmless, as well as to implement the proposed method effectively.
  - Quick check question: How do poisoned-label and clean-label backdoor attacks differ in their mechanism and stealthiness?

- **Concept**: Hypothesis testing and statistical verification
  - Why needed here: The ownership verification mechanism relies on statistical hypothesis testing to distinguish between watermarked and benign models, requiring understanding of t-tests and significance levels.
  - Quick check question: What is the relationship between the verification success rate, sample size, and the ability to reject the null hypothesis in the proposed verification method?

## Architecture Onboarding

- **Component map**: Transformation module -> Bi-level optimization framework -> Hypothesis testing module -> Training pipeline

- **Critical path**:
  1. Generate hardly-generalized domain using transformation module and bi-level optimization
  2. Create protected dataset by optimizing modified samples that are visually indistinguishable
  3. Train models on protected dataset
  4. Conduct ownership verification using hypothesis testing on suspicious models

- **Design tradeoffs**:
  - Watermark invisibility vs. verification effectiveness (tradeoff between perturbation budget and VSR)
  - Watermarking rate vs. computational cost (higher rates improve verification but increase training time)
  - Complexity of transformation module vs. quality of hardly-generalized domain (more complex transformations may yield better domains but are harder to optimize)

- **Failure signatures**:
  - Low verification success rate despite high watermarking rate (suggests the hardly-generalized domain isn't sufficiently different)
  - Significant drop in benign accuracy (indicates the watermark is too intrusive)
  - High p-values in malicious cases (suggests the watermark isn't effectively distinguishing models)
  - Visually detectable watermark (indicates the perturbation budget is too high)

- **First 3 experiments**:
  1. Test the transformation module's ability to generate domain-watermarked samples by measuring mutual information reduction and classification accuracy on those samples
  2. Verify the bi-level optimization produces visually indistinguishable modified samples by conducting human visual inspection and measuring perceptual similarity metrics
  3. Evaluate the hypothesis testing mechanism by comparing prediction distributions between benign and watermarked models on a validation set of domain-watermarked samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed domain watermark method compare to existing backdoor-based approaches in terms of practical implementation and ease of use?
- Basis in paper: [explicit] The paper mentions that the proposed method is the first non-backdoor-based approach, but does not provide a detailed comparison with existing methods.
- Why unresolved: The paper does not provide a detailed comparison of the practical implementation and ease of use of the proposed method compared to existing backdoor-based approaches.
- What evidence would resolve it: A detailed comparison of the implementation and ease of use of the proposed method and existing backdoor-based approaches would help resolve this question.

### Open Question 2
- Question: How does the proposed method perform in terms of scalability and efficiency when applied to larger and more complex datasets?
- Basis in paper: [inferred] The paper mentions that the proposed method is evaluated on benchmark datasets, but does not provide information on its performance on larger and more complex datasets.
- Why unresolved: The paper does not provide information on the scalability and efficiency of the proposed method when applied to larger and more complex datasets.
- What evidence would resolve it: Experiments evaluating the performance of the proposed method on larger and more complex datasets would help resolve this question.

### Open Question 3
- Question: How does the proposed method handle cases where the suspicious model has been trained on a combination of the protected dataset and other datasets?
- Basis in paper: [inferred] The paper mentions that the proposed method is evaluated under different scenarios, but does not provide information on its performance when the suspicious model has been trained on a combination of the protected dataset and other datasets.
- Why unresolved: The paper does not provide information on the performance of the proposed method when the suspicious model has been trained on a combination of the protected dataset and other datasets.
- What evidence would resolve it: Experiments evaluating the performance of the proposed method when the suspicious model has been trained on a combination of the protected dataset and other datasets would help resolve this question.

## Limitations

- The theoretical justification for finding and reliably discovering hardly-generalized domains lacks rigorous mathematical proof
- The bi-level optimization approach has high computational complexity that may limit practical scalability
- The robustness evaluation focuses on specific adaptive attacks and may not capture all possible attack vectors

## Confidence

- **High confidence**: The core mechanism of using domain watermarking instead of backdoor attacks to achieve harmlessness (supported by clear theoretical distinction and experimental validation)
- **Medium confidence**: The effectiveness of the bi-level optimization approach for generating hardly-generalized domains (some theoretical gaps remain in justifying why these domains are reliably discoverable)
- **Low confidence**: The robustness against all adaptive attacks (current evaluation focuses on specific attack types, may not capture more sophisticated approaches)

## Next Checks

1. Conduct ablation studies to quantify the contribution of each component (transformation module, bi-level optimization, hypothesis testing) to the overall performance
2. Test the method on additional diverse datasets beyond the three used in experiments to evaluate generalizability
3. Implement and evaluate against more sophisticated adaptive attacks, including transfer learning and knowledge distillation-based approaches, to thoroughly assess robustness claims