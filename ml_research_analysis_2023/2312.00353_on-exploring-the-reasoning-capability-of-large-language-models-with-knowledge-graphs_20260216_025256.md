---
ver: rpa2
title: On Exploring the Reasoning Capability of Large Language Models with Knowledge
  Graphs
arxiv_id: '2312.00353'
source_url: https://arxiv.org/abs/2312.00353
tags:
- knowledge
- relation
- entity
- context
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the reasoning capabilities of large language
  models (LLMs) with knowledge graphs using their internal knowledge graph learned
  during pre-training. The authors formulate two research questions: (1) to what extent
  can LLMs accurately recall information from knowledge graphs, and (2) to what extent
  can LLMs infer knowledge graph relations from context.'
---

# On Exploring the Reasoning Capability of Large Language Models with Knowledge Graphs

## Quick Facts
- arXiv ID: 2312.00353
- Source URL: https://arxiv.org/abs/2312.00353
- Reference count: 25
- Key outcome: LLMs can successfully perform knowledge graph reasoning tasks from their internal knowledge and infer relations from context, with GPT-4 achieving 74.2% hard accuracy and 83.5% soft accuracy on contextual relation generation.

## Executive Summary
This paper investigates whether large language models can perform knowledge graph reasoning using their internal knowledge graph learned during pre-training. The authors formulate two research questions: to what extent LLMs can recall factual knowledge from knowledge graphs and infer relations from context. They employ LLMs to perform four distinct knowledge graph reasoning tasks - tail entity prediction, relation prediction, relation extraction, and contextual path generation - identifying two types of hallucinations (content and ontology) that may occur during reasoning. The experimental results demonstrate that LLMs, particularly GPT-4, can successfully tackle both simple and complex knowledge graph reasoning tasks from their own memory as well as infer from input context.

## Method Summary
The authors use DBpedia triples and Wikipedia context paragraphs to evaluate LLM knowledge graph reasoning capabilities through four tasks: tail entity prediction (mask tail entities), relation prediction (mask relations), relation extraction (extract relations from context), and contextual path generation (generate multi-step paths). They employ zero-shot prompting with temperature 0 for text-davinci-003 and ChatGPT, comparing performance across models using hard accuracy (exact match), soft accuracy (synonymous relations), NGEO (path validity), %IF (formatting), and %IV (ontology compliance). The study uses multi-step prompting for complex tasks, decomposing them into support sentence extraction, entity linking, and path generation subtasks.

## Key Results
- LLMs can successfully perform simple knowledge graph reasoning tasks (tail entity prediction, relation prediction) from their internal knowledge with high accuracy
- LLMs can infer knowledge graph relations from context with good accuracy, with GPT-4 achieving 74.2% hard accuracy and 83.5% soft accuracy
- Multi-step prompting reduces hallucination in complex reasoning tasks by decomposing them into simpler subtasks
- GPT-4 outperforms earlier models significantly in complex tasks when using multi-step prompting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can recall factual knowledge graph relations from their internal knowledge graph learned during pre-training
- Mechanism: During pre-training, LLMs encode structured knowledge graph data into their parameters. When prompted with entity-relation queries, they can retrieve this encoded knowledge to generate accurate predictions.
- Core assumption: The pre-training corpus included substantial knowledge graph data that the LLM could encode into its parameters.
- Evidence anchors:
  - [abstract] "LLMs can successfully tackle both simple and complex knowledge graph reasoning tasks from their own memory"
  - [section 2.1.4] "Our results indicate that more advanced model result in improved relation generation accuracy"
  - [corpus] Weak evidence - no direct mentions of knowledge graph pre-training in neighbor papers

### Mechanism 2
- Claim: LLMs can infer knowledge graph relations from context by combining contextual understanding with their internal knowledge graph
- Mechanism: The LLM processes the input context to understand the semantic relationship between entities, then uses its internal knowledge graph to map this relationship to a valid knowledge graph relation.
- Core assumption: The LLM has learned both contextual understanding and knowledge graph structure during pre-training.
- Evidence anchors:
  - [abstract] "LLMs can successfully tackle... as well as infer from input context"
  - [section 3.1.4] "LLMs are able to infer knowledge graph relation from context with a good level of accuracy"
  - [corpus] Moderate evidence - neighbor paper "Aligning Vision to Language" discusses multimodal knowledge graph construction for LLMs reasoning

### Mechanism 3
- Claim: Multi-step prompting reduces hallucination by decomposing complex reasoning tasks into simpler subtasks
- Mechanism: Complex tasks are broken into support sentence extraction, entity linking, and path generation. Each subtask focuses the LLM on specific aspects, reducing cognitive load and preventing irrelevant generation.
- Core assumption: LLMs perform better on simpler, focused tasks than on complex, multi-faceted tasks.
- Evidence anchors:
  - [section 3.2.4] "Our proposed multi-step prompting strategy performs CPG with the following subtasks"
  - [section 3.2.5] "GPT-4 outperforms its predecessors by a significant margin" when using multi-step prompting
  - [corpus] Moderate evidence - neighbor paper "Plan-on-Graph" discusses self-correcting adaptive planning on knowledge graphs

## Foundational Learning

- Concept: Knowledge Graph Structure
  - Why needed here: Understanding entities, relations, and triples is essential for formulating the reasoning tasks and evaluating results
  - Quick check question: What is the difference between a head entity and a tail entity in a knowledge graph triple?

- Concept: Prompt Engineering
  - Why needed here: Effective prompts are crucial for eliciting accurate knowledge graph reasoning from LLMs
  - Quick check question: How does temperature setting affect the randomness of LLM outputs in knowledge graph tasks?

- Concept: Hallucination Types
  - Why needed here: Identifying content vs ontology hallucination is critical for evaluating and improving LLM performance
  - Quick check question: Why is a relation linking a Person entity to a Location entity considered ontology hallucination?

## Architecture Onboarding

- Component map: LLM (text-davinci-003, ChatGPT, GPT-4) → Prompt → Knowledge Graph Reasoning Task → Evaluation Metrics (H-ACC, S-ACC, NGEO, %IF, %IV)
- Critical path: Context/Document → Support Sentence Extraction → Entity Linking → Path Generation → Output Path
- Design tradeoffs: Zero-shot vs few-shot prompting (simplicity vs performance), single-step vs multi-step prompting (complexity vs accuracy), hard vs soft accuracy metrics (strictness vs leniency)
- Failure signatures: High NGEO values (incorrect paths), high %IF (poor formatting), high %IV (ontology violations), significant H-ACC/S-ACC gap (hallucination)
- First 3 experiments:
  1. Replicate tail entity prediction task with a small dataset to verify basic recall capability
  2. Test contextual relation generation with simple examples to validate inference mechanism
  3. Apply multi-step prompting to a CPG task to assess hallucination reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively reduce hallucinations in LLM-based knowledge graph reasoning tasks?
- Basis in paper: [explicit] The authors identify two types of hallucinations: content hallucination (non-factual or non-existent relations) and ontology hallucination (relations invalid according to DBpedia ontology). They note that reducing hallucinations remains a challenge requiring further investigation.
- Why unresolved: While the authors acknowledge the issue and suggest that prompt engineering might help, they do not provide specific methods or solutions for reducing hallucinations in practice.
- What evidence would resolve it: Successful implementation and evaluation of prompt engineering techniques or other methods that significantly reduce both content and ontology hallucinations in knowledge graph reasoning tasks.

### Open Question 2
- Question: How do different prompting strategies affect the performance of LLMs in complex knowledge graph reasoning tasks like Contextual Path Generation?
- Basis in paper: [explicit] The authors compare single-step and multi-step prompting strategies for GPT-4, ChatGPT, and text-davinci-003 in Contextual Path Generation tasks, finding that multi-step prompting improves performance for some models.
- Why unresolved: The paper only explores a limited set of prompting strategies and models. There could be other effective prompting techniques that were not tested.
- What evidence would resolve it: Comprehensive evaluation of various prompting strategies (e.g., chain-of-thought, retrieval-augmented) across multiple LLM architectures on complex knowledge graph reasoning tasks.

### Open Question 3
- Question: Can LLMs effectively reason over proprietary knowledge graphs that were not seen during pre-training?
- Basis in paper: [inferred] The authors suggest in their conclusion that their prompt design could be extended to address reasoning over proprietary knowledge graphs, but this is not experimentally verified in the paper.
- Why unresolved: The experiments are limited to publicly available knowledge graphs (DBpedia), and the authors do not test their approach on proprietary or unseen knowledge graphs.
- What evidence would resolve it: Experimental results demonstrating successful knowledge graph reasoning by LLMs on proprietary knowledge graphs that were not part of their pre-training data, using the proposed prompting strategies.

## Limitations

- The paper does not specify the sampling methodology for DBpedia triples, which could bias results toward relations the model knows better
- Manual evaluation of soft accuracy relies on subjective human judgment without detailed annotation guidelines or inter-rater reliability measures
- The study assumes knowledge graphs were part of pre-training data but does not verify this assumption or test on proprietary knowledge graphs

## Confidence

- High confidence in the finding that LLMs can successfully perform basic knowledge graph reasoning tasks (tail entity prediction and relation prediction) from their internal knowledge
- Medium confidence in the claim that multi-step prompting significantly reduces hallucination in complex reasoning tasks
- Low confidence in the assertion that LLMs can accurately infer knowledge graph relations from context

## Next Checks

1. Replicate the tail entity prediction task using a fixed sampling methodology (e.g., stratified random sampling across relation types) with a smaller, well-documented dataset to verify that performance is consistent and not biased by sampling artifacts.

2. Conduct a separate study with multiple annotators evaluating the same set of LLM outputs for soft accuracy to quantify inter-rater agreement and establish more objective criteria for synonym determination.

3. Test the same reasoning tasks using LLMs with known differences in pre-training data (e.g., models explicitly trained on knowledge graphs vs. those that weren't) to determine the extent to which observed capabilities depend on knowledge graph exposure during pre-training.