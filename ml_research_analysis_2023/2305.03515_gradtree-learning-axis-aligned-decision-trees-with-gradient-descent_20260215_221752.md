---
ver: rpa2
title: 'GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent'
arxiv_id: '2305.03515'
source_url: https://arxiv.org/abs/2305.03515
tags:
- datasets
- tree
- optimization
- trees
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a gradient-based method for learning axis-aligned
  decision trees, overcoming the non-convex and non-differentiable challenges of traditional
  greedy tree induction. Their approach uses backpropagation with a straight-through
  operator on a dense tree representation, enabling joint optimization of all tree
  parameters (split thresholds, feature indices, and leaf class probabilities).
---

# GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent

## Quick Facts
- arXiv ID: 2305.03515
- Source URL: https://arxiv.org/abs/2305.03515
- Reference count: 40
- Primary result: Gradient-based axis-aligned decision tree learning that outperforms greedy methods on binary classification tasks while avoiding overfitting

## Executive Summary
This paper presents GradTree (GDT), a novel gradient-based method for learning axis-aligned decision trees that overcomes the non-convex and non-differentiable challenges of traditional greedy tree induction. The approach uses backpropagation with a straight-through operator on a dense tree representation, enabling joint optimization of all tree parameters including split thresholds, feature indices, and leaf class probabilities. Through efficient matrix-based tree routing, GDT achieves superior performance on binary classification tasks compared to existing greedy and non-greedy tree learning approaches, while demonstrating competitive results on multi-class problems. The method is particularly effective at avoiding overfitting compared to alternatives.

## Method Summary
GDT uses a dense decision tree representation where categorical split indices are transformed into differentiable matrices and thresholds are extended into feature-wise matrices. The method employs a straight-through estimator to handle non-differentiable operations, using hardmax in the forward pass and identity in the backward pass. Tree routing is performed through matrix operations that process entire batches in parallel. The model is trained using Adam optimizer with cross-entropy loss incorporating a focal factor, and includes early stopping and random restarts. Post-hoc pruning is applied after training to further refine the tree structure.

## Key Results
- Outperforms greedy methods (CART) and non-greedy methods (GeneticTree, DL8.5) on binary classification tasks across 35 UCI datasets
- Demonstrates competitive performance on multi-class problems while avoiding overfitting
- Shows robust performance across datasets with varying feature dimensions (4-60 features) and sample sizes (101-45,211 samples)
- Provides better interpretability than ensemble methods while maintaining strong predictive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient descent can optimize all tree parameters jointly without relying on greedy splitting.
- Mechanism: The dense DT representation transforms categorical split indices into differentiable matrices (I ∈ R^(2d-1) × n) and extends thresholds into feature-wise matrices (T ∈ R^(2d-1) × n). Backpropagation with straight-through estimation allows gradients to flow through the argmax and Heaviside operations.
- Core assumption: The dense representation preserves all valid tree structures and that the ST operator sufficiently approximates the non-differentiable operations.
- Evidence anchors:
  - [abstract]: "uses backpropagation with a straight-through operator on a dense DT representation, to jointly optimize all tree parameters"
  - [section 3.2]: "We extend the vector ι ∈ R^(2d-1) to a matrix I ∈ R^(2d-1) × n. This is achieved by one-hot encoding the feature index"
  - [corpus]: Weak evidence; no corpus neighbors explicitly describe dense representation with ST estimation.

### Mechanism 2
- Claim: The ST operator allows gradient-based optimization of non-differentiable operations without explicit search.
- Mechanism: During forward pass, hardmax is applied to enforce valid categorical splits. During backward pass, the gradient is passed through the pre-hardmax continuous values, ignoring the discrete selection. Similarly, the rounded Heaviside output is used in forward pass, but gradients are computed for the logistic approximation.
- Core assumption: The mismatch between forward and backward passes is small enough that the optimizer can still find good parameters.
- Evidence anchors:
  - [section 3.3]: "For the backward pass, however, we exclude this operation and directly propagate back the gradients"
  - [section 3.3]: "we use the ST operator to assure hard splits... by excluding⌊·⌉ for the backward pass"
  - [corpus]: No explicit corpus evidence; this is a novel contribution.

### Mechanism 3
- Claim: The tree routing allows efficient batch processing via matrix operations.
- Mechanism: Instead of routing samples individually through the tree, all routing decisions are encoded in a dense matrix representation, allowing the entire batch to be processed with a single set of matrix multiplications.
- Core assumption: Balanced trees allow fixed routing logic that can be precomputed and applied uniformly.
- Evidence anchors:
  - [abstract]: "novel tree routing scheme that allows parallel batch processing using matrix operations"
  - [section 3.4]: "Our novel tree routing allows calculating the tree pass function over a complete batch as a single set of matrix operations"
  - [corpus]: Weak evidence; no corpus neighbors describe matrix-based tree routing explicitly.

## Foundational Learning

- Concept: Decision tree optimization as non-convex, non-differentiable problem
  - Why needed here: Understanding why traditional greedy algorithms are limited and why gradient descent is non-trivial for trees
  - Quick check question: Why can't we directly apply gradient descent to vanilla decision trees?

- Concept: Straight-through estimator in neural networks
  - Why needed here: Core mechanism for handling non-differentiable operations in GDT
  - Quick check question: What is the key difference between forward and backward pass when using ST estimator?

- Concept: Matrix representation of categorical choices
  - Why needed here: Dense representation transforms discrete split indices into continuous matrices
  - Quick check question: How does one-hot encoding of split indices enable gradient-based optimization?

## Architecture Onboarding

- Component map:
  Dense representation layer (I, T matrices) -> Straight-through estimator module -> Tree routing engine -> Leaf parameter layer (L) -> Optimization loop (Adam with weight averaging)

- Critical path:
  1. Initialize dense parameters (I, T, L)
  2. Forward pass with ST operator and matrix routing
  3. Compute cross-entropy loss with focal factor
  4. Backward pass with ST estimation
  5. Update parameters with Adam
  6. Early stopping based on validation loss

- Design tradeoffs:
  - Dense representation increases parameters but enables joint optimization
  - ST estimator introduces forward/backward mismatch but allows gradient flow
  - Balanced tree requirement simplifies routing but limits flexibility
  - Matrix operations improve efficiency but require fixed tree depth

- Failure signatures:
  - Poor convergence: ST mismatch too large or learning rate inappropriate
  - Overfitting: Dense representation has too many parameters for dataset size
  - Memory issues: Tree depth too large for matrix operations
  - Runtime explosion: Unbalanced trees break matrix routing assumptions

- First 3 experiments:
  1. Verify ST estimator works: Compare forward pass outputs with and without ST operator on a small synthetic dataset
  2. Test matrix routing: Ensure batch routing produces identical results to individual sample routing
  3. Validate optimization: Train on a simple binary classification problem and verify convergence to reasonable tree structure

## Open Questions the Paper Calls Out
- Extension to ensembles as a performance-interpretability trade-off (future work)
- Application to datasets with extremely high-dimensional features
- Integration with oblique splits while maintaining interpretability

## Limitations
- Dense representation may lead to overfitting on smaller datasets due to parameter explosion
- Performance on truly high-dimensional datasets (thousands of features) remains untested
- Limited evaluation of multi-class problems against specialized multi-class tree algorithms

## Confidence

- Mechanism 1 (Joint optimization): High - The mathematical formulation is sound and the ST operator approach is well-established in the literature
- Mechanism 2 (ST operator effectiveness): Medium - While theoretically valid, the paper lacks ablation studies showing how sensitive results are to ST approximation errors
- Mechanism 3 (Matrix routing efficiency): Medium - The claim is supported by complexity analysis but not empirically validated against sequential routing implementations

## Next Checks
1. Ablation study: Train GDT with and without ST operators on a simple dataset to quantify the approximation error introduced
2. Parameter sensitivity: Systematically vary tree depth and measure overfitting on datasets of different sizes to establish safe depth bounds
3. Multi-class benchmarking: Compare GDT against specialized multi-class tree algorithms (not just binary methods adapted to multi-class) to validate competitive performance claims