---
ver: rpa2
title: Function Basis Encoding of Numerical Features in Factorization Machines
arxiv_id: '2305.14528'
source_url: https://arxiv.org/abs/2305.14528
tags:
- field
- functions
- numerical
- function
- basis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of incorporating numerical features
  into factorization machines (FMs) for recommendation systems. The authors propose
  a systematic approach that encodes numerical features into vectors of function values
  from a chosen set of basis functions, such as B-Splines.
---

# Function Basis Encoding of Numerical Features in Factorization Machines

## Quick Facts
- **arXiv ID:** 2305.14528
- **Source URL:** https://arxiv.org/abs/2305.14528
- **Reference count:** 40
- **Primary result:** Encoding numerical features as vectors of basis function values (e.g., B-Splines) improves accuracy over traditional binning in factorization machines while preserving computational efficiency.

## Executive Summary
This paper addresses the challenge of incorporating numerical features into factorization machines (FMs) for recommendation systems. The authors propose a systematic approach that encodes numerical features into vectors of function values from a chosen set of basis functions, such as B-Splines. This method leverages the approximation power of splines to learn segmentized functions of numerical features, where the spanning coefficients vary between segments. The approach is shown to be theoretically justified and practically efficient, preserving fast training and inference. Experiments on synthetic and real datasets, including a web-scale A/B test, demonstrate improved performance compared to traditional binning methods. The paper also presents a technique for integrating the proposed model into existing systems that use binning, with controllable accuracy reduction.

## Method Summary
The method transforms numerical features by evaluating them through a set of basis functions (B-Splines), creating field-specific embedding vectors. These vectors are aggregated per field using either identity or sum reduction, then interacted pairwise using field-interaction matrices. The spanning properties of B-Splines ensure the model learns segmentized functions with segment-specific coefficients, while the non-zero basis functions at any point are bounded by a constant, preserving computational efficiency. The approach can be integrated into existing binning-based systems by simulating binning with high-resolution discretization of the basis-encoded model.

## Key Results
- The proposed method outperforms traditional binning approaches on synthetic and real-world datasets in terms of prediction accuracy
- The method preserves fast training and inference by maintaining a small number of effective parameters
- Integration with existing binning-based systems is possible with controllable accuracy reduction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The proposed method preserves fast training and inference by keeping the number of learned parameters linear in the number of basis functions rather than quadratic.
- **Mechanism:** The spanning properties (Lemma 1 and Lemma 2) ensure that the model learns segmentized functions that are linear combinations of the chosen basis functions. Each field's embedding vector is multiplied by basis functions, and the summing reduction aggregates these contributions. Since at any point only a constant number of basis functions (four for cubic B-Splines) are non-zero, the number of effective parameters remains small.
- **Core assumption:** The non-zero basis functions at any point are bounded by a constant, ensuring computational efficiency is preserved.
- **Evidence anchors:**
  - [abstract] "Our technique preserves fast training and inference, and requires only a small modification of the computational graph of an FM model."
  - [section 3.2] "Another important property of the B-Spline basis is that at any point only four basis functions are non-zero, as is apparent in Figure 3."
  - [corpus] Weak: No direct evidence in corpus papers about basis function efficiency in FM variants.
- **Break condition:** If basis functions have wide support or are not localized, computational efficiency is lost and the method no longer preserves fast inference.

### Mechanism 2
- **Claim:** The use of B-Spline basis functions provides strong approximation power with a small number of basis functions, avoiding sparsity issues.
- **Mechanism:** B-Splines can approximate smooth functions with k continuous derivatives up to an error of O(∥g^(k)∥∞/ℓ^k), where ℓ is the number of subintervals. This is faster decay than binning's O(1/ℓ), allowing accurate approximation with fewer intervals and thus avoiding sparsity problems.
- **Core assumption:** The functions being approximated are smooth enough (have small high-order derivatives).
- **Evidence anchors:**
  - [section 3.2] "It is known [13, pp 149], that cubic splines can approximate an arbitrary function g with k ≤ 4 continuous derivatives up to an error bounded by O(∥g^(k)∥∞/ℓ^k)..."
  - [section 3.2] "The spanning property (Lemma 1) ensures that the model's segmentized outputs are splines spanned by the same basis, and therefore their power to approximate the optimal segmentized outputs."
  - [corpus] Weak: No direct evidence in corpus papers about spline approximation in FM contexts.
- **Break condition:** If the target functions are not smooth (e.g., have discontinuities or large high-order derivatives), spline approximation becomes ineffective.

### Mechanism 3
- **Claim:** The method can be seamlessly integrated into existing systems that use binning by simulating binning with high-resolution discretization.
- **Mechanism:** By computing the embedding vector p(z) for each numerical value z and evaluating it at the midpoints of binning intervals, a model trained with the basis encoding method can produce the same per-bin embeddings as a traditionally binned model. This allows integration without changing the serving infrastructure.
- **Core assumption:** The binning intervals are known and fixed, and the transformation Tf that maps numerical values to the unit interval is consistent between training and serving.
- **Evidence anchors:**
  - [section 3.4] "Suppose we would like to obtain a model which employs binning of the field f into a large number N of intervals... we can generate such a model from another model trained using our scheme to make initial integration easier."
  - [section 3.4] "The idea is best explained by referring, again, to Figure 2... we simply compute N corresponding embedding vectors by evaluating p at the mid-point of each interval."
  - [corpus] Weak: No direct evidence in corpus papers about integration strategies for FM variants.
- **Break condition:** If the binning intervals change between training and serving, or if the transformation Tf is inconsistent, the integration will fail.

## Foundational Learning

- **Concept:** Factorization Machines (FMs) and their variants (FFM, FwFM, FmFM)
  - **Why needed here:** The paper builds upon the FM family and introduces modifications to their computational graph. Understanding the base model is essential to grasp the proposed changes.
  - **Quick check question:** What is the key difference between FM and FFM in terms of field interaction matrices?

- **Concept:** B-Splines and spline approximation theory
  - **Why needed here:** The paper advocates for B-Splines as the basis function due to their strong approximation properties. Understanding spline theory is crucial to appreciate why this choice is effective.
  - **Quick check question:** What is the maximum number of basis functions that are non-zero at any given point for cubic B-Splines?

- **Concept:** Discretization and binning in machine learning
  - **Why needed here:** The paper compares its approach to traditional binning methods and highlights the limitations of binning (sparsity issues). Understanding binning is necessary to appreciate the advantages of the proposed method.
  - **Quick check question:** What is the main limitation of binning when the number of bins is increased?

## Architecture Onboarding

- **Component map:** Numerical feature values -> Basis function evaluation -> Field reduction (identity/sum) -> Pairwise interactions (field-interaction matrices) -> Bias and linear terms -> Final prediction

- **Critical path:**
  1. Transform numerical input values using basis functions.
  2. Apply field reductions to aggregated vectors.
  3. Compute pairwise interactions between fields.
  4. Combine interactions with bias and linear terms.
  5. Output prediction.

- **Design tradeoffs:**
  - Basis function choice: B-Splines offer strong approximation power but require careful selection of knots and transformations for numerical fields.
  - Number of basis functions: More basis functions increase approximation power but also increase computational cost and risk of overfitting.
  - Field reduction type: Identity reduction preserves vector information but increases dimensionality; sum reduction reduces dimensionality but may lose some information.

- **Failure signatures:**
  - Poor approximation accuracy: Indicates that the chosen basis functions are not suitable for the target functions or that the number of basis functions is insufficient.
  - Sparsity issues: Suggests that the number of basis functions is too large, leading to insufficient data per segment.
  - Integration failure: Points to inconsistencies between training and serving transformations or binning intervals.

- **First 3 experiments:**
  1. **Synthetic data experiment:** Generate synthetic data with known segmentized functions and compare the learned functions using binning vs. B-Splines to validate the spanning properties and approximation accuracy.
  2. **External dataset experiment:** Apply the method to benchmark datasets (e.g., California Housing, Adult Income) and compare performance with traditional binning methods to assess real-world effectiveness.
  3. **A/B test simulation:** Simulate an A/B test by splitting traffic between a model using the proposed method and a model using traditional binning to measure the impact on prediction accuracy and business metrics (e.g., CTR).

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- The method offers no advantage over existing approaches for sparse numerical features (like ZIP codes).
- The performance impact on truly massive-scale systems has not been fully characterized.
- The method's sensitivity to basis function choice and hyperparameter tuning across diverse datasets is not systematically explored.

## Confidence
- Core theoretical claims (spanning properties, approximation bounds): **High confidence** based on established spline theory and clear empirical validation on synthetic data.
- Practical efficiency claims (fast training/inference): **Medium confidence** supported by theoretical arguments about basis function localization, though direct computational benchmarks are limited.
- Integration claims with existing binning systems: **Medium confidence** based on algorithmic description but lack extensive deployment validation.

## Next Checks
1. **Scale Validation:** Run controlled A/B tests comparing the proposed method against binning across different traffic volumes and feature distributions to quantify performance gains and computational overhead at scale.

2. **Robustness Analysis:** Systematically evaluate the method's sensitivity to basis function choice (not just B-Splines), knot placement, and hyperparameter settings across diverse datasets to establish practical guidelines.

3. **Deployment Benchmark:** Measure end-to-end latency and resource utilization in a production serving environment to validate the claimed preservation of fast inference and identify any hidden costs.