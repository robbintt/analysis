---
ver: rpa2
title: 'The Stable Signature: Rooting Watermarks in Latent Diffusion Models'
arxiv_id: '2303.15435'
source_url: https://arxiv.org/abs/2303.15435
tags:
- image
- images
- watermark
- arxiv
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Stable Signature, a method to embed watermarks
  into images generated by Latent Diffusion Models (LDMs) by fine-tuning the latent
  decoder. The approach enables detection and identification of generated images even
  after transformations like cropping or compression.
---

# The Stable Signature: Rooting Watermarks in Latent Diffusion Models

## Quick Facts
- arXiv ID: 2303.15435
- Source URL: https://arxiv.org/abs/2303.15435
- Authors: 
- Reference count: 40
- Key outcome: Embedding watermarks into images generated by Latent Diffusion Models (LDMs) by fine-tuning the latent decoder, achieving 90%+ detection accuracy with false positive rates below 10^-6

## Executive Summary
This paper proposes Stable Signature, a method to embed watermarks into images generated by Latent Diffusion Models (LDMs) by fine-tuning the latent decoder. The approach enables detection and identification of generated images even after transformations like cropping or compression. Watermark extraction is done via a pre-trained extractor, and detection uses statistical tests. Results show 90%+ accuracy in detecting cropped images with false positive rates below 10^-6. The method is robust, compatible with various generative tasks, and incurs minimal impact on image quality.

## Method Summary
The method embeds watermarks by fine-tuning the LDM decoder to generate images whose watermark extractor outputs a fixed binary signature. The fine-tuning optimizes both perceptual similarity (using Watson-VGG loss) and message consistency (matching a target signature). Detection uses statistical tests based on matching bit counts between extracted and target signatures, while identification selects the model with highest matching score. The approach is compatible with various generative tasks and shows robustness to common image transformations.

## Key Results
- 90%+ accuracy in detecting cropped images
- False positive rates below 10^-6 under statistical assumptions
- Minimal impact on image quality (PSNR, SSIM maintained)
- Robustness to transformations including cropping, compression, and scaling

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning the latent decoder can embed a persistent watermark that survives image transformations. During fine-tuning, the decoder is optimized to produce images whose watermark extractor outputs a fixed binary signature. The perceptual loss ensures the watermark is placed in perceptually masked regions, while the message loss enforces signature consistency. Core assumption: The watermark extractor's output bits are i.i.d. Bernoulli(0.5) on non-watermarked images.

### Mechanism 2
Statistical tests using matching bit counts can reliably detect or identify watermarked images. For detection, a threshold on the number of matching bits between the extracted message and the signature determines if an image is flagged. For identification, the model with the highest matching score is selected. Core assumption: Bit-wise independence and symmetry under the null hypothesis.

### Mechanism 3
Model collusion can be detected via averaging model outputs due to predictable changes in extracted watermark bits. When two models are averaged, the watermark extractor's output for bits that disagree between the models becomes random, while agreeing bits remain deterministic. This "marking assumption" holds even without explicit design for traitor tracing.

## Foundational Learning

- **Latent Diffusion Models (LDMs) and autoencoders**: The watermark is embedded by fine-tuning the decoder of the LDM autoencoder, so understanding the diffusion process and latent space is essential. Quick check: How does the LDM autoencoder transform a latent vector into a final image?

- **Perceptual similarity metrics (e.g., LPIPS, Watson-VGG)**: The perceptual loss controls how much the watermark distorts the image, balancing robustness and quality. Quick check: Why might Watson-VGG be preferred over MSE for perceptual quality in this context?

- **Statistical hypothesis testing and binomial distributions**: Detection and identification rely on thresholds derived from binomial CDFs under the null hypothesis. Quick check: How does the false positive rate scale with the number of users in identification mode?

## Architecture Onboarding

- **Component map**: Pre-trained watermark extractor (W) -> LDM encoder (E) -> LDM decoder (D) -> Fine-tuned decoder (Dm)

- **Critical path**:
  1. Pre-train W with HiDDeN-style encoder/decoder, discard encoder
  2. Fine-tune Dm using perceptual + message loss
  3. Generate images, extract signatures, apply statistical test

- **Design tradeoffs**:
  - Perceptual loss weight λi: higher = better image quality, lower = better watermark robustness
  - Bit length k: longer = more robust detection, but higher FPR if bits aren't perfectly i.i.d.
  - Whitening: essential for statistical validity but may reduce robustness slightly

- **Failure signatures**:
  - High FPR despite theoretical guarantees → bits not i.i.d. after whitening
  - Low bit accuracy on transformations → extractor not robust to that transformation
  - Visible artifacts → perceptual loss too low or whitening too aggressive

- **First 3 experiments**:
  1. Train W on 256×256 COCO images with 48-bit signatures and whitening; verify i.i.d. assumption empirically.
  2. Fine-tune LDM decoder with λi=2.0; measure PSNR, SSIM, and bit accuracy on transformations.
  3. Run detection test with varying τ; plot TPR vs FPR and compare to theoretical curve.

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of perceptual loss function impact the balance between image quality and watermark robustness in Stable Signature? The paper mentions using Watson-VGG, Watson-DFT, LPIPS, MSE, and LPIPS+MSE as perceptual losses but doesn't quantify the trade-off between image quality (PSNR, SSIM) and watermark robustness (bit accuracy) for each loss function across different transformations.

### Open Question 2
What is the impact of colluding users on the accuracy of identifying the original model used to generate an image? The paper discusses model collusion as a potential attack and shows that averaging model weights leads to random bits in the extracted watermark for positions where the original models disagree, but only demonstrates this for two models.

### Open Question 3
How does the watermarking process affect the diversity and creativity of the generated images? The paper evaluates the FID score to measure the quality and diversity of generated images but doesn't explicitly analyze the impact of watermarking on the creative aspects of the images.

## Limitations

- Statistical assumptions (i.i.d. Bernoulli outputs, marking assumption for collusion) are not empirically validated for the specific model architectures used
- No ablation studies comparing different perceptual loss functions or their impact on quality-robustness trade-off
- Limited testing against adversarial attacks on the watermark extractor itself
- Choice of λi=2.0 is heuristic without sensitivity analysis

## Confidence

**High Confidence**: The overall framework of embedding watermarks during generation rather than post-hoc is technically sound and the implementation details are clearly specified.

**Medium Confidence**: The specific architectural choices and reported performance metrics are credible but lack comprehensive ablation studies and empirical validation of statistical assumptions.

**Low Confidence**: The statistical assumptions underlying false positive rate guarantees are not empirically validated, and robustness to unknown future transformations is asserted but not tested.

## Next Checks

1. Generate 100,000 images from unwatermarked LDMs and run them through the detection system to measure actual false positive rate, comparing this to the theoretical 10^-6 claim.

2. For a sample of 10,000 vanilla images, extract watermark bits and perform statistical tests (chi-square, autocorrelation) to verify the i.i.d. Bernoulli(0.5) assumption holds for the specific extractor architecture.

3. Design targeted attacks against the watermark extractor (e.g., adversarial examples, universal perturbations) and measure how much bit accuracy degrades under these attacks compared to benign transformations.