---
ver: rpa2
title: Adversarial Item Promotion on Visually-Aware Recommender Systems by Guided
  Diffusion
arxiv_id: '2312.15826'
source_url: https://arxiv.org/abs/2312.15826
tags:
- recommender
- image
- attack
- adversarial
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IPDGI, a novel attack method that employs
  guided diffusion models to generate adversarial samples designed to deceive visually-aware
  recommender systems. The key innovation lies in integrating a conditional constraint
  into the reverse process of the diffusion model, ensuring generated adversarial
  images closely resemble original images while maintaining attack effectiveness.
---

# Adversarial Item Promotion on Visually-Aware Recommender Systems by Guided Diffusion

## Quick Facts
- arXiv ID: 2312.15826
- Source URL: https://arxiv.org/abs/2312.15826
- Reference count: 40
- One-line primary result: IPDGI achieves significant improvements in promoting long-tailed items across visually-aware recommender systems with enhanced image quality

## Executive Summary
This paper introduces IPDGI, a novel attack method that employs guided diffusion models to generate adversarial samples designed to deceive visually-aware recommender systems. The key innovation lies in integrating a conditional constraint into the reverse process of the diffusion model, ensuring generated adversarial images closely resemble original images while maintaining attack effectiveness. IPDGI outperforms existing ranker-targeted attacks, achieving significant improvements in promoting long-tailed (unpopular) items across three representative visually-aware recommender systems on two real-world datasets. The method demonstrates both enhanced attack performance and superior image quality compared to baseline attacks, with FID scores improving by up to 91.51%.

## Method Summary
IPDGI leverages a pre-trained diffusion model to generate adversarial images for item promotion attacks on visually-aware recommender systems. The method integrates perturbations into Gaussian noise during the diffusion process, using conditional constraints to maintain image fidelity. A k-means clustering approach selects reference images semantically similar to target items, ensuring effective perturbation optimization. The attack is evaluated on VBPR, DVBPR, and AMR recommender systems using Amazon Beauty and Amazon Baby datasets, measuring effectiveness through Exposure Rate at Rank K (ER@K) and image quality through FrÃ©chet Inception Distance (FID).

## Key Results
- IPDGI achieves up to 91.51% improvement in FID scores compared to baseline AIP attack
- ER@5 increases from 1.18% to 17.27% on Amazon Beauty dataset using IPDGI
- No significant degradation in NDCG@K observed, indicating minimal side effects on recommendation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Guided diffusion preserves visual fidelity while introducing adversarial perturbations
- Mechanism: By integrating conditional constraints during the reverse diffusion process, the model maintains similarity to original images while embedding targeted feature shifts
- Core assumption: The diffusion model can effectively separate and preserve perturbation signals during denoising while removing random noise
- Evidence anchors:
  - [abstract]: "The generated adversarial images have high fidelity with original images, ensuring the stealth of our IPDGI"
  - [section 4.2]: "we have opted for the Mean Squared Error (MSE) loss as the condition: ð‘ (x0|xð‘¡adv) = exp(ðœ‰ âˆ¥x0 âˆ’ xð‘¡advâˆ¥)"
  - [corpus]: Weak - no direct diffusion-based attack papers found
- Break condition: If perturbation magnitude exceeds the diffusion model's denoising capacity, or if conditional guidance becomes too restrictive and fails to embed the adversarial signal

### Mechanism 2
- Claim: Clustering-based reference image selection ensures semantic consistency and attack effectiveness
- Mechanism: K-means clustering groups visually similar items, then the most popular item from the target's cluster serves as reference point for perturbation optimization
- Core assumption: Items in the same cluster share sufficient visual similarity that small perturbations can effectively shift features without causing distortion
- Evidence anchors:
  - [section 4.3]: "we first employ k-means cluster analysis on the images within the dataset... choose the image of the most popular items whose feature vectors are in the same cluster"
  - [section 5.6]: "The reduction in ER@5 observed in the 'IPDGI w/o Clustering' setting highlights the significance of reference image selection"
  - [corpus]: Weak - clustering is standard but specific application to adversarial recommendation is novel
- Break condition: If clusters are too heterogeneous or target items belong to semantically diverse clusters, making it impossible to find suitable reference images

### Mechanism 3
- Claim: Diffusion-based perturbation generation outperforms direct perturbation methods in both stealth and effectiveness
- Mechanism: Perturbations are first optimized through feature alignment, then embedded within diffusion noise rather than applied directly to clean images
- Core assumption: The diffusion model's denoising process preferentially preserves structured perturbations while removing random noise artifacts
- Evidence anchors:
  - [section 4.4]: "we incorporate the perturbation within the diffusion model... the corrupted image generated by the forward process of the diffusion model contains the perturbation"
  - [section 5.4]: "IPDGI demonstrates an 87.56% and 91.51% improvement over AIP in terms of image quality"
  - [section 5.6]: "the comparison between 'IPDGI' and 'IPDGI w/o Attack' implies the effectiveness of our perturbation generator"
- Break condition: If perturbation optimization fails to create sufficiently strong signal, or if diffusion model fails to preserve perturbations during denoising

## Foundational Learning

- Concept: Diffusion models and their forward/reverse processes
  - Why needed here: Understanding how noise is gradually added and removed is crucial for grasping how IPDGI maintains image quality while embedding adversarial content
  - Quick check question: What distinguishes the forward process from the reverse process in diffusion models, and why is this distinction important for IPDGI's approach?

- Concept: Visual feature extraction and embedding spaces
  - Why needed here: IPDGI relies on comparing and aligning feature vectors in the visual embedding space of recommender systems
  - Quick check question: How does IPDGI use visual feature extraction to determine which reference image to use for perturbation optimization?

- Concept: Recommender system ranking mechanisms
  - Why needed here: Understanding how visual features influence item ranking is essential to comprehend IPDGI's attack objective
  - Quick check question: What role do visual features play in the ranking mechanisms of VBPR, DVBPR, and AMR models?

## Architecture Onboarding

- Component map: Pre-trained diffusion model -> K-means clustering -> Perturbation generator -> Guided diffusion module -> Recommender system interface
- Critical path: 1. Cluster target item -> 2. Select reference image -> 3. Optimize perturbation -> 4. Embed in diffusion noise -> 5. Generate adversarial image -> 6. Evaluate ranking impact
- Design tradeoffs:
  - Image quality vs. attack effectiveness: Higher perturbation magnitude improves attack but reduces image quality
  - Diffusion steps vs. computation time: More steps improve quality but increase generation time
  - Guidance scale vs. stealth: Higher guidance improves fidelity but may reduce attack strength
- Failure signatures:
  - Low FID scores with no ER@K improvement: Perturbations are too subtle to affect rankings
  - High FID scores with moderate ER@K improvement: Tradeoff between stealth and effectiveness is suboptimal
  - ER@K degradation below baseline: Perturbations are too aggressive and harm recommender performance
- First 3 experiments:
  1. Generate adversarial images for a single target item across different epsilon values (16, 32, 64) and measure FID vs ER@5 tradeoff
  2. Compare IPDGI with baseline AIP attack on a single recommender system using identical target items
  3. Test different cluster sizes (k=5, k=10, k=20) to determine optimal semantic grouping for reference image selection

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Evaluation limited to specific datasets (Amazon Beauty and Amazon Baby) without testing on larger, more diverse e-commerce platforms
- Focus on item promotion attacks without exploring potential side effects on overall recommendation quality in long-term scenarios
- Reliance on specific clustering parameters and generalizability of results across different dataset characteristics

## Confidence

**Major Uncertainties:**
The paper's evaluation focuses primarily on item promotion attacks, with limited exploration of potential side effects on overall recommendation quality. While NDCG@K is reported to show no significant degradation, the long-term impact of repeated adversarial attacks on system stability remains unclear. The reliance on specific clustering parameters (k-value selection) and the generalizability of results across different dataset characteristics present additional uncertainty.

**Confidence Labels:**
- **High confidence**: IPDGI's ability to generate high-fidelity adversarial images (supported by 87.56-91.51% FID improvements)
- **Medium confidence**: Attack effectiveness on ranker promotion (demonstrated across multiple models but limited to specific datasets)
- **Medium confidence**: Clustering-based reference selection mechanism (proven effective but parameters not fully explored)

## Next Checks
1. Test IPDGI's performance across a broader range of dataset characteristics, including different item popularity distributions and visual feature spaces
2. Evaluate the attack's robustness against common adversarial defense mechanisms in recommender systems
3. Conduct ablation studies on diffusion model parameters (guidance scale, step count) to optimize the tradeoff between image quality and attack effectiveness