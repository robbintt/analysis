---
ver: rpa2
title: 'Mindful Explanations: Prevalence and Impact of Mind Attribution in XAI Research'
arxiv_id: '2312.12119'
source_url: https://arxiv.org/abs/2312.12119
tags:
- sentence
- algorithm
- mind
- cluster
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates mind attribution in XAI research by analyzing
  3,533 XAI articles. Using semantic shift detection on SciBERT embeddings, three
  types of mind-attributing language were identified: metaphorical ("to learn"), awareness
  ("to consider"), and agency ("to make decisions").'
---

# Mindful Explanations: Prevalence and Impact of Mind Attribution in XAI Research

## Quick Facts
- arXiv ID: 2312.12119
- Source URL: https://arxiv.org/abs/2312.12119
- Reference count: 40
- Key outcome: Mind-attributing language in XAI research increases AI awareness ratings and resists responsibility shifts toward human experts

## Executive Summary
This paper investigates the prevalence and impact of mind-attributing language in XAI research. Using semantic shift detection on SciBERT embeddings, the authors identify three types of mind-attributing language: metaphorical, awareness, and agency. An experiment with 199 participants demonstrates that mind-attributing explanations increase AI awareness ratings and resist reductions in AI responsibility after considering human involvement. The findings highlight the need for careful language in XAI to avoid shifting responsibility away from humans and suggest potential ethical implications for XAI research communication.

## Method Summary
The study analyzed 3,533 XAI articles from the S2ORC corpus, extracting 122,833 sentences containing AI-related target words that were subjects. Semantic shift detection using SciBERT embeddings and affinity propagation clustering identified three types of mind-attributing language: metaphorical ("to learn"), awareness ("to consider"), and agency ("to make decisions"). An experiment with 199 participants used vignette-based scenarios to test the impact of these descriptions on AI awareness and responsibility ratings. Ordinal regression models analyzed the effects of mind-attributing versus non-mind-attributing explanations.

## Key Results
- Mind-attributing explanations increased AI awareness ratings in the experimental condition
- Participants who read mind-attributing explanations gave slightly increased responsibility ratings, resisting shifts toward human experts
- Realistic AI depictions yielded lower awareness and responsibility ratings than anthropomorphic descriptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mind-attributing language in XAI research increases users' perception of AI system awareness.
- **Mechanism:** The use of mental state verbs (e.g., "to consider," "to make decisions") triggers anthropomorphic attribution of awareness, which in turn influences how users evaluate the AI's cognitive states.
- **Core assumption:** Semantic shift detection can isolate mind-attributing usage patterns, and these patterns causally affect downstream awareness ratings.
- **Evidence anchors:**
  - [abstract] "mind-attributing explanations increased AI awareness ratings"
  - [section 4.3.2] "Participants in the XAI Mind condition were more likely to rate the awareness of the AI system with the highest value (7)"
- **Break condition:** If the clustering fails to separate mind-attributing from non-mind-attributing contexts, the causal link to awareness ratings is weakened.

### Mechanism 2
- **Claim:** Mind-attributing explanations reduce the shift of responsibility from AI systems to human experts.
- **Mechanism:** When users read mind-attributing language, they attribute agency and awareness to the AI, making them less likely to reduce the AI's responsibility rating even after considering the experts' knowledge.
- **Core assumption:** Responsibility attribution is sensitive to perceived agency and awareness; explaining the AI's mental states interferes with the redistribution of blame toward humans.
- **Evidence anchors:**
  - [abstract] "mind-attributing explanation had a responsibility-concealing effect"
  - [section 4.3.3] "participants who read the mind-attributing explanation gave slightly increased responsibility ratings"
- **Break condition:** If participants do not perceive the AI as having agency, the effect on responsibility redistribution disappears.

### Mechanism 3
- **Claim:** Realistic AI depictions yield lower awareness and responsibility ratings than anthropomorphic ones.
- **Mechanism:** The framing of the AI (e.g., "the AI model" vs. "Jarvis, a robot") modulates the degree of anthropomorphism, thereby influencing ratings of awareness and responsibility.
- **Core assumption:** Vignette framing acts as a priming mechanism for anthropomorphism, which directly impacts mind perception.
- **Evidence anchors:**
  - [section 5.2] "When the AI system is depicted as a named human-like robot, the ratings of awareness were higher than in the more realistic descriptions"
  - [section 4.3.1] "The participants in the human-like no knowledge condition were most likely to rate the responsibility of the AI system with the highest value (7)"
- **Break condition:** If the anthropomorphic framing is neutralized by contextual cues, the difference in ratings disappears.

## Foundational Learning

- **Concept:** Semantic shift detection using SciBERT embeddings.
  - **Why needed here:** To automatically identify subtle differences in word usage that indicate mind attribution.
  - **Quick check question:** Does clustering on SciBERT embeddings separate metaphorical from literal usages of "to learn" in the corpus?

- **Concept:** Ordinal regression for Likert-scale ratings.
  - **Why needed here:** Ratings are ordinal, not interval; ordinal models avoid incorrect assumptions about metric spacing.
  - **Quick check question:** Does the cumulative link model produce the same conclusions as an ANOVA on the same data?

- **Concept:** Vignette-based experimental design with attention checks.
  - **Why needed here:** Ensures internal validity and mitigates low data quality in MTurk samples.
  - **Quick check question:** Do participants who fail attention checks differ systematically from those who pass?

## Architecture Onboarding

- **Component map:** Corpus preprocessing -> SciBERT embedding extraction -> Affinity propagation clustering -> Manual cluster classification -> Experimental design -> Data collection -> Ordinal regression analysis -> Visualization
- **Critical path:** Embedding extraction -> Clustering -> Classification -> Experiment execution
- **Design tradeoffs:** Using only subject-position sentences reduces syntactic noise but may miss non-subject mind attributions; affinity propagation is less parameter-sensitive but can yield many small clusters.
- **Failure signatures:** High silhouette scores but semantically incoherent clusters; manual classification disagreements; experiment attention check failures.
- **First 3 experiments:**
  1. Test clustering on a small annotated subset to validate semantic separation before full corpus processing.
  2. Run a pilot vignette experiment with a subset of participants to confirm the direction of the mind-attribution effect.
  3. Compare ordinal regression vs. ANOVA on simulated ordinal data to verify model choice.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the use of mind-attributing language in XAI research affect the perception of AI systems by non-expert users in real-world scenarios?
- **Basis in paper:** [explicit] The paper discusses the potential for mind-attributing language to strengthen the perception of AI systems as mindful, aware agents, and mentions the possibility of this language "trickling down" to educational materials, media reports, or regulators.
- **Why unresolved:** The study focuses on the prevalence of mind-attributing language in XAI research and its impact on awareness and responsibility ratings in a controlled vignette-based experiment. It does not investigate how this language affects real-world perceptions.
- **What evidence would resolve it:** A study that examines the perception of AI systems by non-expert users after exposure to explanations containing mind-attributing language in various real-world contexts, such as news articles, educational materials, or user manuals.

### Open Question 2
- **Question:** What are the long-term effects of mind-attributing language on the accountability and trustworthiness of AI systems?
- **Basis in paper:** [inferred] The paper highlights the need for careful language in XAI to avoid shifting responsibility away from humans and to maintain accountability. It suggests that mind-attributing explanations might be intentionally used by AI system designers to distance themselves from negative outcomes.
- **Why unresolved:** The study does not investigate the long-term effects of mind-attributing language on accountability and trustworthiness. It focuses on the immediate impact on awareness and responsibility ratings in a single experimental session.
- **What evidence would resolve it:** A longitudinal study that tracks the perceptions of AI systems and the assignment of responsibility over time, considering the use of mind-attributing language in explanations and other contexts.

### Open Question 3
- **Question:** How can XAI methods be designed to effectively surface the involvement of humans and maintain accountability while still providing clear and understandable explanations?
- **Basis in paper:** [explicit] The paper discusses the need for novel XAI methods that place a stronger focus on the socio-technical perspective by surfacing who interacts with AI systems. It mentions approaches like social transparency and datasheets for datasets as potential starting points.
- **Why unresolved:** The paper does not provide specific design guidelines or evaluate the effectiveness of existing or proposed methods in achieving this balance between human involvement, accountability, and explanation clarity.
- **What evidence would resolve it:** A comparative study that evaluates the effectiveness of different XAI methods in terms of their ability to surface human involvement, maintain accountability, and provide clear and understandable explanations to users.

## Limitations

- Semantic shift detection relies on unsupervised clustering, which may miss nuanced mind-attributing usages
- The experiment's between-subjects design limits causal inference about individual differences
- Synthetic vignettes may not generalize to real-world XAI explanations
- Study focuses on English-language research and participants, potentially limiting cross-cultural generalizability

## Confidence

- **High confidence:** Mind-attributing language increases AI awareness ratings and reduces responsibility shifts when considering human experts
- **Medium confidence:** Realistic AI depictions yield lower awareness and responsibility ratings than anthropomorphic ones
- **Low confidence:** The three types of mind attribution (metaphorical, awareness, agency) are distinct and separable in practice

## Next Checks

1. Replicate the semantic shift detection using alternative embedding models (e.g., BioBERT or domain-specific XAI embeddings) to verify cluster stability
2. Conduct a within-subjects experiment to test individual variation in response to mind-attributing language
3. Analyze real-world XAI explanations from deployed systems to assess ecological validity of laboratory findings