---
ver: rpa2
title: 'PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning'
arxiv_id: '2310.19958'
source_url: https://arxiv.org/abs/2310.19958
tags:
- pruning
- privacy
- attack
- information
- defense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper quantifies privacy leakage in federated learning with
  model pruning and introduces PriPrune, a defense mechanism that improves the privacy-accuracy
  tradeoff. Theoretical analysis derives upper bounds on information leakage showing
  that higher pruning rates and batch sizes reduce leakage.
---

# PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning

## Quick Facts
- arXiv ID: 2310.19958
- Source URL: https://arxiv.org/abs/2310.19958
- Reference count: 40
- Primary result: PriPrune improves privacy-accuracy tradeoff in pruned federated learning, achieving 36.3% privacy improvement over PruneFL on FEMNIST without accuracy loss.

## Executive Summary
This paper addresses privacy leakage in federated learning (FL) with model pruning by quantifying information leakage and introducing PriPrune, an adaptive defense mechanism. The authors derive theoretical bounds showing that pruning and batch size reduce privacy leakage, but demonstrate through experiments that pruning alone provides insufficient protection. PriPrune uses adaptive per-client defense masks with pseudo-pruning and Gumbel-Softmax sampling to jointly optimize privacy and accuracy. Empirical evaluation on FEMNIST and CIFAR-10 datasets shows PriPrune significantly outperforms baseline pruning methods in privacy protection while maintaining model accuracy.

## Method Summary
The paper combines theoretical analysis with empirical evaluation to quantify privacy leakage in pruned federated learning. The theoretical component derives upper bounds on information leakage using mutual information, showing privacy improves with higher pruning rates and batch sizes. The empirical component implements six pruning methods (Random, SNIP, SynFlow, GraSP, FedDST, PruneFL) and evaluates them against state-of-the-art attacks (DLG, GI, SGI). PriPrune is introduced as an adaptive defense that applies per-client masks with pseudo-pruning and Gumbel-Softmax sampling to jointly optimize privacy and accuracy. The method is evaluated over 20,000 FL rounds with 10 clients per round on FEMNIST and CIFAR-10 datasets.

## Key Results
- Theoretical analysis shows pruning rates and batch sizes directly reduce information leakage bounds
- Pruning alone provides insufficient privacy protection against SGI attacks (average NMI of 0.2)
- PriPrune achieves 36.3% privacy improvement over PruneFL on FEMNIST without accuracy loss
- PriPrune works universally across six different pruning methods
- Batch size of 20 provides optimal privacy-accuracy tradeoff for FEMNIST, while batch size of 1 is optimal for CIFAR-10

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model pruning reduces the number of parameters, thereby decreasing the information leakage in federated learning.
- Mechanism: By removing weights (via pruning), the pruned model exposes fewer parameters to the server, reducing the amount of sensitive information that can be exploited by privacy attacks like SGI.
- Core assumption: The privacy leakage is directly proportional to the number of model parameters.
- Evidence anchors:
  - [abstract]: "by making local models coarser, pruning is expected to also provide some protection against privacy attacks in the context of FL."
  - [section]: "Intuitively, model pruning results in fewer parameters thus exposing less information to the server."
  - [corpus]: Weak - related papers focus on communication efficiency and resource constraints, not privacy quantification.
- Break condition: If pruning methods prioritize retaining weights with large gradients for model accuracy, they may inadvertently preserve sensitive information, negating the privacy benefit.

### Mechanism 2
- Claim: Adaptive per-client defense masks with pseudo-pruning improve the privacy-accuracy tradeoff in pruned federated learning.
- Mechanism: PriPrune applies a personalized defense mask that selectively prunes weights before sending updates to the server (improving privacy), while retaining those weights locally for continued training (maintaining accuracy).
- Core assumption: The privacy benefit of pruning can be maximized without accuracy loss if the defense mask is adapted based on the local training dynamics.
- Evidence anchors:
  - [abstract]: "PriPrune uses adaptive per-client defense masks with pseudo-pruning and Gumbel-Softmax sampling to jointly optimize privacy and accuracy."
  - [section]: "We refer to this idea as Pseudo-Pruning...the parameters are not actually pruned, but retained for local training in next rounds, thus maintaining model accuracy."
  - [corpus]: Weak - related work focuses on pruning for efficiency, not privacy-aware adaptive mechanisms.
- Break condition: If the adaptive mechanism fails to properly balance privacy and accuracy, leading to either excessive pruning (accuracy loss) or insufficient pruning (privacy compromise).

### Mechanism 3
- Claim: Larger batch sizes in federated learning increase privacy protection by obscuring individual user data characteristics.
- Mechanism: Larger batch sizes aggregate more data points during local training, making it harder for the server to reconstruct individual user data from the gradients.
- Core assumption: The privacy leakage is inversely proportional to the batch size.
- Evidence anchors:
  - [section]: "larger batch sizes increase the privacy protection of the FL process as they enable more data points to be added during local training, which can help in obscuring the data characteristics of a particular user."
  - [section]: "The upper bounds on information leakage decrease with larger pruning rate ( p) and batch sizes ( B)."
  - [corpus]: Weak - related papers do not discuss the impact of batch size on privacy leakage.
- Break condition: If the batch size becomes too large, it may lead to computational inefficiencies or convergence issues, negating the privacy benefit.

## Foundational Learning

- Concept: Information Theory and Mutual Information
  - Why needed here: The paper uses mutual information to quantify the privacy leakage in pruned federated learning, providing a theoretical foundation for the privacy analysis.
  - Quick check question: How does mutual information measure the interdependence between two random variables, and why is it suitable for quantifying privacy leakage in this context?

- Concept: Federated Learning and Model Pruning
  - Why needed here: The paper combines federated learning with model pruning to optimize both communication efficiency and privacy protection in distributed machine learning scenarios.
  - Quick check question: What are the key challenges in federated learning, and how does model pruning address these challenges while potentially improving privacy?

- Concept: Privacy Attacks and Defenses in Machine Learning
  - Why needed here: The paper evaluates the effectiveness of privacy attacks (e.g., SGI) against pruned federated learning models and proposes a defense mechanism (PriPrune) to enhance privacy protection.
  - Quick check question: What are the common types of privacy attacks in federated learning, and how do defense mechanisms like differential privacy and secure aggregation work to mitigate these attacks?

## Architecture Onboarding

- Component map: Server -> Clients (Conv-2/VGG-11 models) -> PriPrune defense mechanism

- Critical path:
  1. Server broadcasts global model to clients
  2. Clients perform local training and apply base pruning
  3. Clients apply PriPrune defense (adaptive per-client mask with pseudo-pruning)
  4. Clients send pruned updates to server
  5. Server aggregates updates and launches privacy attacks

- Design tradeoffs:
  - Privacy vs. Accuracy: Higher pruning rates improve privacy but may reduce model accuracy
  - Communication vs. Privacy: Larger batch sizes improve privacy but increase communication overhead
  - Adaptiveness vs. Complexity: PriPrune's adaptive mechanism improves privacy-accuracy tradeoff but adds computational complexity

- Failure signatures:
  - Excessive privacy leakage despite pruning: Indicates ineffective pruning strategy or vulnerability to advanced attacks
  - Significant accuracy loss: Suggests over-pruning or ineffective defense mechanism
  - Convergence issues: May result from improper tuning of pruning rates or defense parameters

- First 3 experiments:
  1. Evaluate the impact of varying pruning rates on privacy leakage using NMI and PSNR metrics
  2. Assess the effectiveness of PriPrune's adaptive defense mechanism on different pruning methods and datasets
  3. Investigate the impact of batch size and model size on privacy leakage in pruned federated learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do other information-theoretic metrics like min-entropy or KL-divergence compare to mutual information for quantifying privacy leakage in pruned FL?
- Basis in paper: [explicit] The paper uses normalized mutual information (NMI) to quantify privacy leakage and shows it correlates with PSNR. It states "The use of mutual information is ideal for our privacy analysis, as it enables the measurement of privacy leakage considering non-linear relationships between variables, and thus offers a measure of true dependence."
- Why unresolved: The paper only validates NMI against PSNR and does not explore alternative information-theoretic metrics that might provide tighter bounds or better capture privacy leakage in the presence of pruning.
- What evidence would resolve it: Empirical comparison of NMI against min-entropy and KL-divergence bounds on the same pruned FL datasets, showing which metric provides the tightest theoretical upper bounds and best correlates with practical attack success rates.

### Open Question 2
- Question: What is the optimal adaptive strategy for the defense pruning rate that maximizes the privacy-accuracy tradeoff across different pruning methods and datasets?
- Basis in paper: [explicit] The paper introduces PriPrune with adaptive per-client defense masks and Gumbel-Softmax sampling, showing it improves privacy without accuracy loss. It states "The defense strategy should adapt to the changing dynamics of the training process, enabling a more effective equilibrium to be struck between the crucial trade-off of privacy and accuracy."
- Why unresolved: While PriPrune shows improvement over fixed-rate defenses, the paper does not explore whether the adaptive strategy could be further optimized through reinforcement learning or meta-learning approaches that learn optimal pruning rate schedules.
- What evidence would resolve it: Implementation of meta-learning approaches that learn optimal pruning rate schedules across multiple pruning methods and datasets, with ablation studies showing performance improvements over the current PriPrune adaptive strategy.

### Open Question 3
- Question: How does the privacy-accuracy tradeoff change when combining model pruning with other orthogonal privacy defenses like differential privacy or secure aggregation?
- Basis in paper: [explicit] The paper states "It is worth noting that we consider existing privacy defenses known in the literature, including such as secure aggregation (SA) and differential privacy (DP)... These ideas are orthogonal and can be complementary/combined with our pruning-based defense in the future."
- Why unresolved: The paper focuses solely on pruning-based defense and does not experimentally investigate how combining pruning with DP or SA affects the privacy-accuracy tradeoff compared to using each defense individually.
- What evidence would resolve it: Experimental evaluation comparing privacy-accuracy tradeoffs for pruning-only, DP-only, SA-only, and combined approaches (pruning+DP, pruning+SA, pruning+DP+SA) across multiple datasets and pruning methods, measuring both theoretical bounds and practical attack success rates.

## Limitations

- Theoretical privacy bounds rely on simplifying assumptions about gradient distributions and attack capabilities
- Adaptive mechanism's effectiveness depends heavily on hyperparameter tuning (λacc, λpri, λsha) that is not fully specified
- Evaluation focuses on specific datasets (FEMNIST, CIFAR-10) and pruning methods, limiting generalizability

## Confidence

- **High confidence**: The core claim that model pruning reduces privacy leakage is well-supported by theoretical analysis and empirical validation. The privacy-accuracy tradeoff improvement with PriPrune is demonstrated across multiple pruning methods.
- **Medium confidence**: The theoretical bounds on information leakage and their relationship to pruning rate and batch size are mathematically sound but may not capture all practical attack scenarios.
- **Medium confidence**: The universality claim for PriPrune across pruning methods is supported but requires further validation on diverse architectures and datasets.

## Next Checks

1. Test PriPrune's effectiveness against more advanced privacy attacks (beyond SGI, DLG, GI) to validate the claimed privacy improvements hold under stronger adversaries.

2. Evaluate the impact of different local training configurations (epochs per round, learning rates) on the privacy-accuracy tradeoff to identify optimal training settings.

3. Investigate the scalability of PriPrune to larger models and more complex tasks to assess its practical applicability beyond the tested convolutional architectures.