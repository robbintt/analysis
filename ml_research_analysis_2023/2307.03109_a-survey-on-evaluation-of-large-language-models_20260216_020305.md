---
ver: rpa2
title: A Survey on Evaluation of Large Language Models
arxiv_id: '2307.03109'
source_url: https://arxiv.org/abs/2307.03109
tags:
- llms
- arxiv
- evaluation
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first comprehensive survey on the evaluation
  of large language models (LLMs). It categorizes existing evaluation work into three
  dimensions: what to evaluate (tasks), where to evaluate (datasets and benchmarks),
  and how to evaluate (protocols).'
---

# A Survey on Evaluation of Large Language Models

## Quick Facts
- arXiv ID: 2307.03109
- Source URL: https://arxiv.org/abs/2307.03109
- Reference count: 7
- Primary result: Presents the first comprehensive survey categorizing LLM evaluation into three dimensions: tasks, datasets/benchmarks, and protocols.

## Executive Summary
This paper provides the first comprehensive survey on large language model evaluation, organizing the vast landscape of evaluation work into three orthogonal dimensions: what to evaluate (tasks), where to evaluate (datasets and benchmarks), and how to evaluate (protocols). The survey systematically covers evaluation across diverse domains including NLP, reasoning, medical applications, and others. It reveals that while LLMs excel in many natural language tasks, they still face significant challenges in reasoning, robustness, and ethical considerations. The paper identifies grand challenges for future research, emphasizing the need for more comprehensive, principled, and dynamic evaluation methods that can keep pace with rapidly evolving LLM capabilities.

## Method Summary
The survey systematically reviews existing evaluation work by mapping it onto three dimensions: task definition, dataset/benchmark selection, and evaluation protocol choice. The authors conducted a comprehensive literature review of evaluation papers, benchmarks, and protocols, categorizing them based on their application domain and evaluation approach. The methodology involves identifying key evaluation tasks across various domains, documenting the datasets and benchmarks used for each task, and analyzing the evaluation protocols employed. The survey synthesizes findings from multiple sources to provide insights into LLM strengths, weaknesses, and future challenges in evaluation.

## Key Results
- LLMs excel in natural language tasks like text generation, classification, and sentiment analysis
- LLMs struggle with complex reasoning, mathematical problem-solving, and maintaining robustness against adversarial prompts
- Static benchmarks are insufficient for modern LLMs; dynamic and evolving evaluation methods are needed
- The three-dimensional framework (what, where, how) provides a systematic way to organize LLM evaluation work
- Grand challenges include developing AGI benchmarks, improving robustness evaluation, and creating principled evaluation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey works by mapping LLM evaluation into three orthogonal dimensions (what, where, how), enabling systematic coverage of diverse evaluation approaches.
- Mechanism: Categorizing evaluation work along these dimensions allows the paper to capture the full evaluation lifecycle—from defining tasks to selecting datasets to choosing protocols—without overlap or omission.
- Core assumption: The three dimensions comprehensively span all aspects of LLM evaluation without missing critical areas.
- Evidence anchors:
  - [abstract] The paper presents a comprehensive review focusing on "what to evaluate, where to evaluate, and how to evaluate."
  - [section] The structure explicitly organizes content under "What to evaluate," "Where to evaluate," and "How to evaluate."
  - [corpus] Weak. Corpus neighbors focus on narrower aspects (e.g., code generation, medical evaluation) but don't challenge the three-dimensional framing.
- Break condition: If future evaluation work requires dimensions beyond these three, or if tasks/datasets/protocols are inherently interdependent in ways that break the orthogonality.

### Mechanism 2
- Claim: The paper's comprehensive task categorization reveals both LLM strengths and limitations, guiding future research priorities.
- Mechanism: By surveying tasks across NLP, reasoning, medical, social science, natural science, engineering, and agent applications, the paper identifies patterns of success (e.g., text generation) and failure (e.g., complex reasoning, robustness).
- Core assumption: Task categorization captures the full spectrum of LLM applications and their performance characteristics.
- Evidence anchors:
  - [section] The summary section explicitly lists "what can LLMs do well" and "when can LLMs fail" based on task evaluations.
  - [abstract] The paper aims to provide insights into LLM strengths and weaknesses across various domains.
  - [corpus] Missing. Corpus neighbors don't provide comparable task-level performance summaries to validate the paper's findings.
- Break condition: If new tasks emerge that don't fit existing categories, or if task performance changes significantly with newer models.

### Mechanism 3
- Claim: The survey identifies grand challenges that extend evaluation beyond static benchmarks, driving innovation in LLM assessment.
- Mechanism: By highlighting the need for dynamic, evolving, and principled evaluation methods, the paper motivates development of new evaluation paradigms like human-in-the-loop and adversarial testing.
- Core assumption: Static benchmarks are insufficient for modern LLMs and must evolve to capture emergent capabilities and risks.
- Evidence anchors:
  - [section] The "Grand challenges" section discusses dynamic evaluation, robustness, and principled approaches.
  - [abstract] The paper aims to shed light on future challenges in LLM evaluation.
  - [corpus] Weak. Corpus neighbors focus on specific evaluation aspects but don't address the broader challenge of evolving evaluation paradigms.
- Break condition: If static benchmarks prove sufficient for capturing LLM capabilities, or if dynamic evaluation proves impractical or unreliable.

## Foundational Learning

- Concept: Three-dimensional evaluation framework (what, where, how)
  - Why needed here: Provides a systematic way to organize and understand the vast landscape of LLM evaluation methods and benchmarks.
  - Quick check question: Can you explain how the "what," "where," and "how" dimensions map to specific sections in the paper?

- Concept: Task categorization and performance analysis
  - Why needed here: Enables identification of LLM strengths and weaknesses across different application domains, guiding future research.
  - Quick check question: What are the key differences between tasks where LLMs excel versus tasks where they struggle?

- Concept: Dynamic and evolving evaluation paradigms
  - Why needed here: Addresses the limitations of static benchmarks in capturing the rapidly evolving capabilities and risks of modern LLMs.
  - Quick check question: How do human-in-the-loop and adversarial testing methods differ from traditional static evaluation?

## Architecture Onboarding

- Component map:
  - Task definition → Dataset selection → Benchmark choice → Evaluation protocol → Result analysis
  - Three orthogonal dimensions: what (tasks), where (datasets/benchmarks), how (protocols)
  - Grand challenges: AGI benchmarks, behavioral evaluation, robustness, dynamic evaluation, principled evaluation

- Critical path:
  1. Define evaluation tasks based on application domain
  2. Select appropriate datasets and benchmarks for each task
  3. Choose evaluation protocols (automatic, human, dynamic)
  4. Analyze results to identify strengths, weaknesses, and grand challenges

- Design tradeoffs:
  - Comprehensive vs. focused: Covering all tasks vs. deep analysis of specific areas
  - Static vs. dynamic: Traditional benchmarks vs. evolving evaluation methods
  - Automatic vs. human: Efficiency vs. depth of evaluation
  - Task-specific vs. general: Specialized benchmarks vs. unified evaluation frameworks

- Failure signatures:
  - Incomplete task coverage: Missing critical evaluation areas
  - Outdated benchmarks: Static datasets no longer reflect current LLM capabilities
  - Biased evaluation: Protocols favoring certain model architectures or training approaches
  - Inconsistent metrics: Lack of standardized evaluation criteria across tasks

- First 3 experiments:
  1. Map existing evaluation work onto the three-dimensional framework to identify gaps
  2. Compare LLM performance across task categories to validate success/failure patterns
  3. Test dynamic evaluation methods (e.g., adversarial prompts) against static benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What evaluation metrics are most appropriate for assessing the reasoning capabilities of large language models, especially in domains like mathematics and logic?
- Basis in paper: [explicit] The paper highlights that LLMs show great potential in reasoning but still face many challenges and limitations, requiring more in-depth research and optimization.
- Why unresolved: Current evaluation metrics for reasoning tasks may not fully capture the nuances of human-like reasoning, especially in complex domains like mathematics and logic. There is a need for more comprehensive and principled evaluation methods.
- What evidence would resolve it: Development and validation of new evaluation metrics that can accurately assess reasoning abilities across various domains, including mathematical reasoning, common sense reasoning, and logical reasoning.

### Open Question 2
- Question: How can evaluation systems be designed to support all kinds of tasks, including value alignment, safety, verification, interdisciplinary research, fine-tuning, and others?
- Basis in paper: [explicit] The paper identifies the need for unified evaluation that supports all LLMs tasks as one of the grand challenges.
- Why unresolved: Existing evaluation systems are often task-specific and may not be able to comprehensively assess the performance of LLMs across diverse tasks. There is a need for more general and flexible evaluation systems.
- What evidence would resolve it: Development and implementation of a unified evaluation framework that can effectively assess LLMs across a wide range of tasks and domains.

### Open Question 3
- Question: What are the most effective methods for improving the robustness of large language models to adversarial prompts and out-of-distribution inputs?
- Basis in paper: [explicit] The paper discusses the importance of robustness evaluation and identifies it as a grand challenge, noting that current LLMs are vulnerable to adversarial prompts.
- Why unresolved: While some progress has been made in evaluating robustness, there is still a need for more effective methods to improve the robustness of LLMs to various types of adversarial inputs and out-of-distribution scenarios.
- What evidence would resolve it: Development and validation of novel techniques that can enhance the robustness of LLMs to adversarial prompts and out-of-distribution inputs, along with rigorous evaluation of their effectiveness.

## Limitations
- Relies heavily on reported results from original evaluation papers without independent verification of claimed performance metrics
- Three-dimensional framework may not capture all nuances of modern LLM evaluation, particularly emerging paradigms
- Task categorization may oversimplify complex relationships between different evaluation domains

## Confidence
- High confidence: Three-dimensional framework structure and systematic organization
- Medium confidence: Identified performance patterns across task categories
- Medium confidence: Grand challenges section relevance and completeness

## Next Checks
1. Cross-validate reported LLM performance metrics on major benchmarks by running current models through standardized evaluation protocols
2. Test the orthogonality of the three-dimensional framework by mapping recently published evaluation work to identify systematic gaps or overlaps
3. Implement and compare at least two dynamic evaluation methods against traditional static benchmarks to assess practical value