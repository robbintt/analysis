---
ver: rpa2
title: Online Algorithms with Uncertainty-Quantified Predictions
arxiv_id: '2310.11558'
source_url: https://arxiv.org/abs/2310.11558
tags:
- online
- algorithm
- problem
- predictions
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies how to optimally utilize uncertainty-quantified
  predictions (UQ) in online algorithms. The authors propose two approaches: (i) a
  single-instance distributionally-robust analysis that minimizes the worst-case competitive
  ratio over an ambiguity set defined by the UQ, and (ii) a multiple-instance regret
  analysis that learns to exploit the UQ across multiple problem instances via online
  learning.'
---

# Online Algorithms with Uncertainty-Quantified Predictions

## Quick Facts
- **arXiv ID**: 2310.11558
- **Source URL**: https://arxiv.org/abs/2310.11558
- **Reference count**: 40
- **Key outcome**: Proposes two approaches for optimally utilizing uncertainty-quantified predictions (UQ) in online algorithms: distributionally-robust analysis minimizing worst-case competitive ratio, and online learning framework with sublinear regret guarantees.

## Executive Summary
This paper bridges online algorithms and machine learning uncertainty quantification by developing frameworks that optimally exploit UQ to improve competitive ratios. The authors propose two complementary approaches: a single-instance distributionally-robust analysis that minimizes worst-case competitive ratios over ambiguity sets defined by UQ, and a multiple-instance regret analysis that learns to exploit UQ across instances via online learning. They demonstrate these approaches on classic problems like ski rental and online search, showing that their optimization-based algorithms can fully leverage UQ while their online learning framework achieves sublinear regret even with general forms of UQ.

## Method Summary
The paper introduces two distinct approaches for incorporating uncertainty-quantified predictions into online algorithms. The first approach uses distributionally-robust analysis to minimize the worst-case competitive ratio over ambiguity sets defined by probabilistic interval predictions (PIPs). This involves constructing hard instances consistent with the UQ and solving an optimization problem to find the optimal decision distribution. The second approach employs an online learning framework that treats predictions and UQ as context vectors, maintaining an ϵ-net over the UQ space and running separate master algorithms for each net point. This framework exploits Lipschitz continuity in the cost function with respect to UQ to achieve sublinear regret guarantees across multiple problem instances.

## Key Results
- Optimization-based algorithms can fully leverage probabilistic interval predictions to outperform simple meta-algorithms in distributionally-robust competitive ratio
- Online learning framework achieves sublinear policy regret by exploiting Lipschitz continuity in the cost function with respect to uncertainty quantification
- Experiments on ski rental demonstrate that the online learning approach can outperform optimal distributionally-robust algorithms in non-worst-case settings
- The proposed methods successfully handle both single-instance optimization and multi-instance learning scenarios with different forms of UQ

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The optimization-based approach minimizes DRCR by formulating an ancillary optimization problem over hard instances defined by the UQ.
- **Mechanism**: Given a PIP(ℓ, u; δ), the algorithm constructs hard instances where the prediction must be accurate within [ℓ, u] with probability at least 1-δ. It then solves a linear program with infinite variables/constraints to find the optimal decision distribution that minimizes the worst-case expected competitive ratio across these instances.
- **Core assumption**: The worst-case distribution for DRCR is a two-point distribution with probability 1-δ on the worst instance in the consistent set and probability δ on the worst instance overall.
- **Evidence anchors**:
  - [abstract]: "We particularly focus on a specific form of UQ, the probabilistic interval predictions, which match the output of conformal prediction methods and show the tractability of our approach for several online problems in this setting."
  - [section]: "For a given PIP(θ), let Iℓ,u ⊆ I denote a consistent set that contains all instances that confirm the interval prediction. Then the ambiguity set Dθ can include all instance distributions such that Pξθ(I ∈ I ℓ,u) ≥ 1 − δ, ∀ξθ ∈ D θ, i.e., under distribution ξθ, the probability of an instance I belongs to a set Iℓ,u is at least with probability 1 − δ."
  - [corpus]: Weak - no direct matching evidence in neighboring papers
- **Break condition**: If the UQ cannot be expressed as a probabilistic interval prediction, or if the resulting optimization problem becomes intractable due to problem structure.

### Mechanism 2
- **Claim**: The online learning approach learns to exploit general UQ across multiple instances by treating predictions and UQ as context vectors.
- **Mechanism**: The algorithm maintains an ϵ-net over the UQ space and runs separate master online learning algorithms for each net point. When a new instance arrives with UQ θt, the closest net point is selected and its corresponding algorithm is updated. This exploits Lipschitz continuity in the cost function with respect to UQ to achieve sublinear policy regret.
- **Core assumption**: The cost upper bound function Ut(w; θt) is Lipschitz in θt, meaning similar UQ vectors lead to similar instance costs.
- **Evidence anchors**:
  - [abstract]: "We consider how to utilize more general forms of uncertainty quantification, proposing an online learning framework that learns to exploit uncertainty quantification to make optimal decisions in multi-instance settings."
  - [section]: "Our approach builds on [19] and [27], and we focus on the novel setting of learning non-parametric policies for using UQ for general online problems. Specifically, we treat the prediction and UQ for a given instance as a 'context' vector."
  - [corpus]: Weak - no direct matching evidence in neighboring papers
- **Break condition**: If the cost function does not exhibit Lipschitz continuity in the UQ space, or if the UQ space is too high-dimensional for efficient ϵ-net coverage.

### Mechanism 3
- **Claim**: For ski rental with PPP, the optimal deterministic algorithm uses different decision regions based on whether the prediction favors buying or renting.
- **Mechanism**: The algorithm partitions the prediction space into three regions: pro-rent (P < B), pro-buy (P > (1+√5)/2 · B), and rent-or-buy (B ≤ P ≤ (1+√5)/2 · B). Within each region, it chooses the optimal day to buy based on minimizing the DRCR formula, which combines consistency and robustness weighted by prediction quality δ.
- **Core assumption**: The optimal decision structure for ski rental with PPP depends critically on which region the prediction falls into, with different trade-offs between early buying and waiting.
- **Evidence anchors**:
  - [section]: "This algorithm operates within distinct prediction regions: (i) in the pro-rent region, defined as P ∈ (0, B), the algorithm purchases on day B regardless of the specific prediction and prediction quality; (ii) in the pro-buy region, defined as P ∈ ((√5+1)/2 · B, +∞), the algorithm makes an early purchase within the initial B days..."
  - [section]: "Lemma 1. Given PPP(P ; δ), the DRCR of the meta-algorithm for ski rental is DRCR (META-SR) = CRPPP sr (δ) = (1 + 2·√(δ(1 − δ)) for δ ∈ [0, 1/2], 2 for δ ∈ (1/2, 1]."
  - [corpus]: Weak - no direct matching evidence in neighboring papers
- **Break condition**: If the cost structure changes (e.g., different rental/purchase costs), or if the prediction quality cannot be accurately quantified.

## Foundational Learning

- **Concept**: Competitive ratio and online algorithm design
  - Why needed here: The paper builds on classical competitive analysis to define new metrics (DRCR) that interpolate between worst-case and average-case performance
  - Quick check question: What is the competitive ratio of the classic ski rental algorithm, and how does it compare to the optimal randomized algorithm?

- **Concept**: Uncertainty quantification and conformal prediction
  - Why needed here: The paper uses UQ methods like conformal prediction to generate probabilistic interval predictions that capture prediction uncertainty
  - Quick check question: How does conformal prediction transform black-box predictions into prediction intervals with guaranteed coverage probability?

- **Concept**: Online learning with side information
  - Why needed here: The multi-instance learning framework treats UQ as side information and exploits its structure to achieve better regret guarantees
  - Quick check question: What is the key difference between standard online learning and online learning with side information, and how does Lipschitz continuity play a role?

## Architecture Onboarding

- **Component map**: UQ processor → Single-instance optimizer OR Online learning framework → Decision maker
- **Critical path**: For single-instance optimization: UQ → hard instance construction → LP formulation → solve → decision. For multi-instance learning: UQ → ϵ-net lookup → master algorithm update → decision.
- **Design tradeoffs**: Deterministic vs randomized algorithms (consistency vs robustness), exact vs approximate optimization (solution quality vs computational cost), ϵ-net granularity vs regret guarantees (learning accuracy vs computational overhead).
- **Failure signatures**: Suboptimal DRCR when UQ is inaccurate, slow convergence of online learning when instance distributions are adversarial, numerical instability in LP solvers for discrete approximations.
- **First 3 experiments**:
  1. Implement and test DSR-PPP on synthetic ski rental instances with varying PPP quality to verify the three-region decision structure.
  2. Implement RSR-PIP optimization solver and compare its DRCR against the theoretical bound on benchmark instances.
  3. Implement the online learning framework with ϵ-net for ski rental and measure regret convergence under different UQ quality distributions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimization-based approach be extended to other online problems beyond ski rental and online search?
- Basis in paper: The authors mention that the optimization-based approach is general and can be tuned to design algorithms for other online problems.
- Why unresolved: The paper only demonstrates the approach on two specific problems. No general framework or sufficient conditions for extending to other problems are provided.
- What evidence would resolve it: A formal theorem or framework showing how to construct the ancillary optimization problem for a broad class of online problems, along with examples beyond the two studied.

### Open Question 2
- Question: What is the optimal trade-off between the quality of the uncertainty quantification (UQ) and the complexity of the resulting optimization problem?
- Basis in paper: The paper shows that more precise UQ (PIP) leads to better performance but also more complex optimization. The online learning approach is proposed as an alternative when PIP is not available or tractable.
- Why unresolved: No systematic analysis of the trade-off between UQ quality and optimization complexity is provided. The paper only shows that the online learning approach can be useful when PIP is not available.
- What evidence would resolve it: A theoretical or empirical study comparing the performance of optimization-based algorithms with different levels of UQ precision to online learning approaches across a range of problems.

### Open Question 3
- Question: How does the performance of the online learning approach depend on the Lipschitzness assumptions?
- Basis in paper: The paper shows that the online learning approach achieves sublinear regret when the cost upper bound is Lipschitz in the UQ. It also mentions that the approach can work with Lipschitz policies even if the cost upper bound is not Lipschitz.
- Why unresolved: The paper does not provide a detailed analysis of how the regret bounds depend on the Lipschitz constant or how to verify the Lipschitzness assumption in practice.
- What evidence would resolve it: A theoretical analysis of the dependence of the regret bounds on the Lipschitz constant, along with empirical studies on the impact of Lipschitzness violations.

## Limitations
- The theoretical analysis relies heavily on probabilistic interval prediction format, which may not be available from all ML prediction systems
- Computational complexity of solving optimization problems with infinite variables/constraints could limit practical applicability for large-scale problems
- Experimental validation is limited to synthetic ski rental instances, leaving open questions about real-world performance

## Confidence
- **High confidence**: The theoretical framework for distributionally-robust competitive ratio and its relationship to standard competitive analysis is well-established and rigorously proven.
- **Medium confidence**: The online learning framework's regret guarantees depend on assumptions about Lipschitz continuity and ϵ-net coverage that may not hold in practice.
- **Low confidence**: The paper's experimental validation is limited to synthetic ski rental instances, leaving open questions about performance on real-world problems with different cost structures.

## Next Checks
1. **Lipschitz continuity verification**: Systematically test the cost function's sensitivity to UQ perturbations across different problem instances to empirically validate the Lipschitz continuity assumption underlying the online learning framework.

2. **Computational scalability analysis**: Evaluate the optimization solver's performance on increasingly complex problem instances to determine practical limits of the single-instance analysis approach, and benchmark against approximate methods.

3. **General UQ format testing**: Extend the experimental evaluation to include alternative UQ formats (e.g., prediction intervals without probability guarantees, full predictive distributions) to assess the framework's robustness to different UQ forms.