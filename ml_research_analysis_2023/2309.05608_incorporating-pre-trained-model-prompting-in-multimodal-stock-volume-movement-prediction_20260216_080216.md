---
ver: rpa2
title: Incorporating Pre-trained Model Prompting in Multimodal Stock Volume Movement
  Prediction
arxiv_id: '2309.05608'
source_url: https://arxiv.org/abs/2309.05608
tags:
- data
- news
- prediction
- stock
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses stock volume movement prediction using overnight
  news and historical trading data, tackling the problem of limited domain knowledge
  in small financial datasets. The core method, ProMUSE, incorporates pre-trained
  language models (specifically Financial-RoBERTa) with prompt learning to leverage
  universal and domain knowledge, and introduces cross-modality contrastive alignment
  alongside unimodal prediction heads to preserve representation quality during multimodal
  training.
---

# Incorporating Pre-trained Model Prompting in Multimodal Stock Volume Movement Prediction

## Quick Facts
- arXiv ID: 2309.05608
- Source URL: https://arxiv.org/abs/2309.05608
- Reference count: 40
- Primary result: ProMUSE achieves 73.56% accuracy on TOPIX500 dataset, outperforming traditional methods and baselines.

## Executive Summary
This paper addresses stock volume movement prediction by combining overnight news headlines and historical trading data, tackling the challenge of limited domain knowledge in small financial datasets. The proposed ProMUSE method leverages pre-trained language models with prompt learning (P-Tuning v2) to efficiently extract universal and domain knowledge without overfitting. A cross-modality contrastive alignment is introduced alongside separate unimodal prediction heads to preserve representation quality during multimodal training. Experimental results demonstrate that ProMUSE outperforms traditional statistical methods, unimodal models, and various fusion and ensemble baselines on the TOPIX500 dataset.

## Method Summary
ProMUSE uses a multimodal architecture with a frozen Financial-RoBERTa news encoder (with tunable prompts via P-Tuning v2) and a 6-layer transformer data encoder. The model incorporates separate prediction heads for each modality to maintain unimodal representation quality, while a fusion module combines information for multimodal predictions. Cross-modality contrastive alignment is implemented with an alignment loss between news and trading data embeddings. Training involves a weighted sum of unimodal losses, multimodal loss, and alignment loss, with inference combining unimodal and multimodal predictions equally.

## Key Results
- ProMUSE achieves 73.56% accuracy on the TOPIX500 dataset
- Outperforms traditional statistical methods, unimodal models, fusion-only, and ensemble-only baselines
- Ablation studies confirm effectiveness of each component
- Maintains superior performance under lower data resource conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ProMUSE mitigates lack of domain knowledge in small financial datasets by leveraging pre-trained language models with prompt learning
- Mechanism: Pre-trained models like Financial-RoBERTa provide universal and domain knowledge, while prompt learning efficiently extracts this knowledge without fine-tuning the entire model, reducing overfitting risk
- Core assumption: Pre-trained model's knowledge is relevant and transferable to stock movement prediction
- Evidence anchors: Abstract and section 2.3 mention prompt learning and avoiding overfitting, but no direct citation of prompt-based pre-training effectiveness in small financial datasets

### Mechanism 2
- Claim: Cross-modality contrastive alignment preserves unimodal representation quality during multimodal training
- Mechanism: Contrastive loss aligns embedding spaces of news and trading data without forcing direct fusion, preventing degradation of individual modality representations
- Core assumption: Contrastive alignment can improve representation learning without introducing harmful biases from weak modality connections
- Evidence anchors: Abstract and section 2.6 describe the alignment loss, but no direct citation of contrastive alignment effectiveness in multimodal financial tasks

### Mechanism 3
- Claim: Unimodal and multimodal supervision together improve robustness and performance
- Mechanism: Separate prediction heads for each modality ensure direct supervision, while fusion and contrastive losses integrate multimodal signals
- Core assumption: Combination of unimodal and multimodal supervision leads to better generalization than either alone
- Evidence anchors: Abstract and sections 2.3-2.5 describe the supervision design, but no direct citation of dual supervision effectiveness in multimodal stock prediction

## Foundational Learning

- Concept: Pre-trained language models and prompt learning
  - Why needed here: Small financial datasets lack sufficient data to train large models from scratch; pre-trained models provide knowledge base, and prompt learning allows efficient extraction without overfitting
  - Quick check question: What is the main advantage of using prompt learning over fine-tuning in small-data regimes?

- Concept: Multimodal learning and contrastive alignment
  - Why needed here: Stock movement depends on both news sentiment and historical trading patterns; contrastive alignment helps align these modalities without damaging individual representations
  - Quick check question: Why might direct fusion of modalities harm representation quality?

- Concept: Time series modeling and transformer architectures
  - Why needed here: Historical trading data is sequential; transformers can capture temporal dependencies and complex patterns better than traditional statistical models
  - Quick check question: What are the key differences between using a transformer and an LSTM for time series in this context?

## Architecture Onboarding

- Component map:
  - News Encoder: Financial-RoBERTa + P-Tuning v2 prompts (frozen backbone, tunable prompts)
  - Data Encoder: 6-layer transformer (pre-trained, fine-tuned)
  - Fusion Module: Linear projection + linear fusion + classification head
  - Contrastive Alignment: News-to-Data and Data-to-News loss
  - Loss Aggregation: Weighted sum of unimodal, multimodal, and alignment losses

- Critical path:
  1. Encode news and data separately
  2. Project to shared embedding space
  3. Compute unimodal, fusion, and alignment losses
  4. Aggregate losses and update model

- Design tradeoffs:
  - Prompt learning vs. fine-tuning: Prompting reduces overfitting but may limit adaptation; fine-tuning increases risk of overfitting on small data
  - Linear fusion vs. attention/transformer fusion: Linear is simpler and less prone to overfitting; more complex fusion may capture richer interactions but requires more data
  - Separate unimodal heads vs. single head: Separate heads preserve unimodal quality; single head may simplify but risk degradation

- Failure signatures:
  - Low unimodal accuracy but high fusion accuracy: Modalities may be misaligned or one modality is noisy
  - High unimodal accuracy but low fusion accuracy: Fusion may be harming representations or the alignment is not effective
  - Training instability: Loss weights or learning rates may be poorly tuned

- First 3 experiments:
  1. Train unimodal models separately to establish baseline performance
  2. Test simple fusion (averaging) without alignment to see if multimodal signal helps
  3. Add contrastive alignment and compare unimodal accuracy before and after

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of overnight news (e.g., earnings announcements vs. general market sentiment) differentially impact volume movement prediction accuracy?
- Basis in paper: The paper uses overnight news headlines from Reuters filtered with RIC labels but does not analyze the differential impact of news types on prediction accuracy
- Why unresolved: The paper aggregates all overnight news without distinguishing between types, missing potential nuances in how different news categories affect trading volume
- What evidence would resolve it: Conducting experiments that categorize overnight news and measure prediction accuracy for each category would provide insight into differential impacts

### Open Question 2
- Question: Does the performance of ProMUSE degrade significantly when applied to stocks outside the TOPIX500 index, particularly for less liquid or smaller-cap stocks?
- Basis in paper: The paper validates ProMUSE on the TOPIX500 dataset, but does not test its generalizability to other indices or stock types
- Why unresolved: The study's focus on TOPIX500 leaves open questions about the model's effectiveness on stocks with different liquidity profiles or market capitalizations
- What evidence would resolve it: Testing ProMUSE on datasets from different indices or including small-cap stocks would demonstrate its robustness and generalizability

### Open Question 3
- Question: How does the inclusion of additional modalities, such as macroeconomic indicators or social media sentiment, affect the predictive performance of ProMUSE?
- Basis in paper: The paper focuses on overnight news and historical trading data, but does not explore the impact of integrating other data sources
- Why unresolved: The study does not consider whether incorporating additional data modalities could enhance or complicate the model's predictions
- What evidence would resolve it: Experimenting with the inclusion of macroeconomic data or social media sentiment as additional inputs and comparing the results to the current model would clarify the potential benefits or drawbacks

## Limitations

- The theoretical justifications for prompt learning and contrastive alignment in this specific application are not well-supported by citations or experiments
- The paper does not demonstrate that P-Tuning v2 is superior to fine-tuning or other prompt strategies for this task
- The cross-modality contrastive alignment mechanism lacks evidence that it meaningfully improves representations rather than introducing spurious correlations

## Confidence

- High confidence: The overall experimental setup and dataset description are clear and reproducible
- Medium confidence: The ablation studies show that removing components hurts performance, but specific contributions of each mechanism are not independently validated
- Low confidence: The theoretical justifications for prompt learning and contrastive alignment in this specific application are not well-supported by citations or experiments

## Next Checks

1. Run an ablation study comparing P-Tuning v2 with full fine-tuning of Financial-RoBERTa on the TOPIX500 dataset to quantify overfitting reduction and performance trade-offs

2. Measure unimodal accuracy before and after adding the alignment loss to determine if representations are preserved or degraded during multimodal training

3. Replace the linear fusion with attention-based fusion and compare performance to isolate whether the choice of fusion mechanism significantly impacts results