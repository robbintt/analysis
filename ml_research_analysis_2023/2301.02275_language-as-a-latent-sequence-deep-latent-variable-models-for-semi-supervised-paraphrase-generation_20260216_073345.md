---
ver: rpa2
title: 'Language as a Latent Sequence: deep latent variable models for semi-supervised
  paraphrase generation'
arxiv_id: '2301.02275'
source_url: https://arxiv.org/abs/2301.02275
tags:
- learning
- conference
- generation
- source
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a semi-supervised deep latent variable model
  for paraphrase generation, addressing the problem of limited labelled data. The
  core method combines a novel unsupervised variational sequence auto-encoding reconstruction
  (VSAR) model with a supervised dual directional learning (DDL) model, parameterised
  with discrete latent language sequences.
---

# Language as a Latent Sequence: deep latent variable models for semi-supervised paraphrase generation

## Quick Facts
- **arXiv ID**: 2301.02275
- **Source URL**: https://arxiv.org/abs/2301.02275
- **Reference count**: 40
- **Primary result**: Introduces a semi-supervised deep latent variable model combining VSAR (unsupervised) and DDL (supervised) for paraphrase generation, achieving significant performance gains when labeled data is limited.

## Executive Summary
This paper addresses the challenge of paraphrase generation with limited labeled data by introducing a semi-supervised deep latent variable model. The approach combines a novel unsupervised variational sequence auto-encoding reconstruction (VSAR) model with a supervised dual directional learning (DDL) model, parameterizing discrete latent language sequences. To overcome training difficulties, the authors propose a knowledge-reinforced learning scheme with two-stage training that first pretrains DDL on labeled data before initializing VSAR with these parameters. Experiments on MSCOCO and Quora datasets demonstrate that this combined model achieves competitive performance against strong supervised baselines on full data and significantly outperforms them when only a fraction of labeled pairs are available.

## Method Summary
The proposed method integrates an unsupervised VSAR model with a supervised DDL model using a two-stage knowledge-reinforced learning approach. In stage one, DDL is trained on labeled paraphrase pairs to learn bidirectional mapping parameters. In stage two, VSAR is initialized with these pretrained parameters and combined with DDL for semi-supervised training on both labeled and unlabeled data. The VSAR model uses discrete latent language sequences sampled via Gumbel-TOPk, while DDL shares parameters between source→target and target→source mappings to enforce bidirectional consistency. The combined model is trained end-to-end using transformer base architecture with maximum token length of 20, and evaluated on BLEU, ROUGE, and i-BLEU scores.

## Key Results
- The combined DDL+VSAR model achieves competitive performance against strong supervised baselines on full datasets
- When only a fraction of labeled pairs are available, the semi-supervised model significantly outperforms supervised baselines (p < .05)
- Discrete latent language sequences improve model interpretability by aligning with natural language structure
- The two-stage knowledge-reinforced learning scheme effectively addresses the cold-start problem in training the combined model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Knowledge-reinforced learning resolves cold-start problems through two-stage training
- **Mechanism**: DDL is first pretrained on labeled pairs, then VSAR inherits these parameters for initialization, allowing unsupervised training to benefit from supervised signal even with unlabeled data
- **Core assumption**: Parameters learned by DDL on labeled data are transferable and useful for initializing VSAR
- **Evidence**: "We empirically demonstrate that semi-supervised learning benefits our combined model, given unlabelled data and a fraction of the paired data."
- **Break condition**: Poor parameter transfer due to mismatched data distribution

### Mechanism 2
- **Claim**: Discrete latent language sequences improve interpretability and model expressiveness
- **Mechanism**: VSAR samples from categorical distributions over discrete tokens using Gumbel-TOPk, representing actual language tokens rather than continuous vectors
- **Core assumption**: Language can be effectively modeled as discrete token sequences, improving model expressiveness
- **Evidence**: "This additionally can enhance the model interpretability, as language is naturally preserved as discrete variables."
- **Break condition**: Discrete space too coarse to capture subtle semantic variations

### Mechanism 3
- **Claim**: Dual directional learning enforces bidirectional consistency in paraphrases
- **Mechanism**: DDL trains two sequence-to-sequence models simultaneously with shared encoder/decoder parameters, enforcing mutual consistency
- **Core assumption**: Paraphrases should be consistent in both directions, regularizing the model
- **Evidence**: "Although sharing parameters is a very simple technique, as shown in Table 1 and Table 2, the DDL model significantly improves the performance"
- **Break condition**: Small datasets causing interference between shared tasks

## Foundational Learning

- **Concept**: Variational Autoencoder (VAE) framework
  - **Why needed**: VSAR builds on VAE principles to model joint distribution of source and latent target sequences for unsupervised learning
  - **Quick check**: What is the ELBO objective in VAEs, and how does it differ from a plain autoencoder?

- **Concept**: Gumbel-Softmax and reparameterization trick
  - **Why needed**: Standard continuous reparameterization doesn't work for discrete latent variables; Gumbel-Softmax enables differentiable sampling
  - **Quick check**: How does the temperature parameter τ in Gumbel-Softmax control discreteness of sampled tokens?

- **Concept**: Semi-supervised learning objectives
  - **Why needed**: Model combines unsupervised (VSAR) and supervised (DDL) losses; balancing them is crucial
  - **Quick check**: What role does KL divergence play in VSAR ELBO, and why might it be removed?

## Architecture Onboarding

- **Component map**: VSAR (source encoder, target encoder, source decoder, target decoder) + DDL (shared encoder/decoder for bidirectional mapping) + KRL (two-stage training)
- **Critical path**: 1) Pretrain DDL on labeled data, 2) Initialize VSAR with DDL parameters, 3) Fine-tune VSAR+DDL jointly on labeled and unlabeled data
- **Design tradeoffs**: Discrete vs continuous latent variables (better interpretability but harder optimization), parameter sharing in DDL (consistency vs interference), prior usage (helpful for small data but harmful for large labeled data)
- **Failure signatures**: Cold-start failure without KRL, posterior collapse in VSAR, mode collapse generating narrow paraphrase set
- **First 3 experiments**: 1) Train DDL on labeled data only to verify Transformer baseline improvement, 2) Train VSAR alone on unlabeled data to measure reconstruction quality, 3) Combine DDL+VSAR with KRL to evaluate on small labeled fraction with full unlabeled set

## Open Questions the Paper Calls Out

- **Open Question 1**: Impact of prior choice (source vs target text corpus) on semantic preservation in semi-supervised paraphrase generation
- **Open Question 2**: Performance comparison using different discrete latent variable sampling techniques beyond Gumbel-Softmax with top-k
- **Open Question 3**: Optimal balance between unsupervised and supervised components across different dataset sizes and characteristics

## Limitations

- Evaluation limited to MSCOCO image captions and Quora question pairs, unclear if discrete latent approach generalizes to other domains
- Computational complexity of discrete latent variables not discussed for scaling to longer sequences or larger vocabularies
- Sensitivity to initialization strategies and random seeds not thoroughly explored

## Confidence

- **High Confidence**: Performance gains on limited data (well-supported by statistical significance testing across metrics)
- **Medium Confidence**: Discrete latent variable benefits (conceptually sound but limited empirical interpretability evidence)
- **Medium Confidence**: Bidirectional consistency via DDL (demonstrated significance but mechanism could be more thoroughly analyzed)

## Next Checks

1. Conduct ablation study comparing KRL initialization against random initialization, pretraining only encoder, or pretraining only decoder
2. Perform qualitative analysis of discrete latent representations to assess interpretability and semantic capture
3. Test model scaling to longer sequences (50-100 tokens) to measure impact on training time, memory, and performance