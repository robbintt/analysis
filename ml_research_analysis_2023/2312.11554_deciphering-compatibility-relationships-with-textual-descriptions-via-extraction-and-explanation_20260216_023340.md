---
ver: rpa2
title: Deciphering Compatibility Relationships with Textual Descriptions via Extraction
  and Explanation
arxiv_id: '2312.11554'
source_url: https://arxiv.org/abs/2312.11554
tags:
- outfit
- items
- explanations
- dataset
- compatibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce a novel two-stage pipeline to generate natural
  language explanations for fashion item compatibility. First, a feature extraction
  model identifies important attributes from item descriptions using a large dataset
  of matched and mismatched pairs.
---

# Deciphering Compatibility Relationships with Textual Descriptions via Extraction and Explanation

## Quick Facts
- arXiv ID: 2312.11554
- Source URL: https://arxiv.org/abs/2312.11554
- Reference count: 27
- Primary result: Novel two-stage pipeline generates natural language explanations for fashion item compatibility that significantly outperform baselines on both automatic metrics and human evaluations

## Executive Summary
This paper introduces a two-stage pipeline for generating natural language explanations of fashion item compatibility relationships. The approach first extracts important features from item descriptions using a large dataset of matched and mismatched pairs, then fine-tunes a language model on a curated Pair Fashion Explanation dataset to generate explanations. The method demonstrates significant improvements over baseline approaches on multiple automatic metrics including BLEURT, BLEU, ROUGE, and FID, as well as human evaluations for conciseness, persuasiveness, and acceptability.

## Method Summary
The proposed method employs a two-stage pipeline where the first stage uses feature extraction models (Cross-Attn or Rationale Extraction) trained on positive/negative item pairs to identify key attributes contributing to compatibility. The second stage fine-tunes a language model (GPT-2, Flan-T5) on the curated PFE dataset, using extracted features and categories as prompts to generate explanations. The system combines data from Amazon Reviews and FashionVC for feature extraction training with the manually curated PFE dataset of 6,407 high-quality compatibility explanations for the explanation generation stage.

## Key Results
- Outperforms baseline methods on automatic metrics (BLEURT, BLEU, ROUGE) by significant margins
- Achieves competitive performance with ChatGPT on human evaluations while maintaining better alignment with ground-truth compatibility relationships
- Low FID scores indicate generated explanations closely match the distribution of ground-truth examples
- Demonstrates ability to produce knowledgeable, accurate, and informative explanations rather than generic associations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage pipeline improves alignment between generated explanations and ground-truth compatibility relationships
- Mechanism: Feature extraction identifies key attributes from item descriptions while fine-tuned language model generates explanations grounded in these specific features
- Core assumption: Compatibility relationships can be decomposed into feature identification and explanation articulation
- Evidence anchors:
  - [abstract] "Our experiments showcase the model's potential in crafting descriptions that are knowledgeable, aligned with ground-truth matching correlations..."
  - [section 3] "This fine-tuning allows the model to generate explanations that convey the compatibility relationships between items."
- Break condition: If feature extraction fails to identify relevant attributes, generation will produce vague or incorrect explanations

### Mechanism 2
- Claim: PFE dataset's curated nature enables more effective fine-tuning than larger, noisier datasets
- Mechanism: Manual filtering ensures high-density, task-relevant training signals focused on quality over quantity
- Core assumption: Domain-specific, high-quality explanations are more valuable than large volumes of generic text
- Evidence anchors:
  - [section 3] "By focusing on quality rather than quantity, our PFE dataset provides dense, high-quality data that can effectively support the intricate task of elucidating clothing compatibility relationships."
  - [section 4] "The results suggest that our models are generating explanations, not merely describing the two items, which is crucial for the task at hand."
- Break condition: Manual curation may introduce bias or miss important compatibility patterns, causing overfitting

### Mechanism 3
- Claim: Using extracted features as prompts ensures explanations are grounded in actual items
- Mechanism: Specific attributes are templated into prompts, forcing generation to focus on concrete, relevant details
- Core assumption: Language models generate more accurate explanations when provided concrete feature prompts versus just item categories
- Evidence anchors:
  - [section 3] "During inference, our proposed framework also employs a two-stage approach. We use the first-stage model to extract important features and make predictions... the extracted features are templated as prompts for the second-stage model to generate explanations."
  - [section 4] "Our model consistently outperforms PEPLER and maintains competitive performance with ChatGPT... suggests that it is not only producing linguistically competent explanations but is also effectively capturing and conveying the complex relationship between the pair of items."
- Break condition: If feature extraction misses important attributes or includes irrelevant ones, generated explanations will be incomplete or misleading

## Foundational Learning

- Concept: Feature extraction and importance ranking
  - Why needed here: First stage must identify which attributes from item descriptions actually contribute to compatibility, filtering out irrelevant details
  - Quick check question: If a shirt has attributes ["cotton", "striped", "button-down", "long-sleeve"], which ones would you prioritize for determining compatibility with a pair of jeans?

- Concept: Fine-tuning large language models on domain-specific datasets
  - Why needed here: General-purpose LLMs lack specific knowledge of fashion compatibility patterns required for this task
  - Quick check question: What's the difference between zero-shot prompting and fine-tuning when adapting an LLM to a new domain?

- Concept: Evaluation metrics for text generation (BLEU, ROUGE, BLEURT, FID)
  - Why needed here: Task requires measuring both surface-level similarity to ground truth and deeper semantic alignment
  - Quick check question: Why might a high BLEU score not necessarily indicate good explanations for compatibility relationships?

## Architecture Onboarding

- Component map: Item descriptions → Feature Extraction → Classification → Feature Templating → Explanation Generation → Output
- Critical path: Item descriptions → Feature Extraction → Classification (compatibility check) → Feature Templating → Explanation Generation → Output
- Design tradeoffs:
  - Two-stage vs. end-to-end: Two-stage allows specialized training for each subtask but requires careful coordination between stages
  - Dataset size vs. quality: Smaller curated dataset may generalize better than larger noisy dataset
  - Model size vs. efficiency: Larger language models (Flan-T5-xl) perform better but require more resources
- Failure signatures:
  - Feature extraction fails to identify key attributes → Explanations become generic or incorrect
  - Language model overfits to training examples → Explanations work for known patterns but fail on novel combinations
  - Classification threshold too high/low → Either too few explanations generated or many incorrect ones
- First 3 experiments:
  1. Run feature extraction on test set pairs and verify that extracted features align with human judgment of important attributes
  2. Generate explanations for a small set of known compatible pairs and check if they mention the actual compatibility factors
  3. Compare FID scores between explanations generated with and without the feature extraction stage to quantify the benefit of grounding

## Open Questions the Paper Calls Out

- Question: How does the proposed model handle multi-object compatibility relationships, and what are the limitations?
  - Basis in paper: [explicit] The authors mention that a current limitation is that the model may find multi-object compatibility relationships challenging to reveal, and they aim to extend the framework to explain multi-object compatibility relationships in the future
  - Why unresolved: The current model is designed to handle pairwise compatibility, and there is no detailed explanation or experimental results on how it would perform with multi-object compatibility
  - What evidence would resolve it: Detailed experiments and results showing the model's performance on multi-object compatibility tasks, along with any modifications made to the framework to handle such scenarios

- Question: What is the impact of using different feature extraction models (Cross-Attn vs. Rationale Extraction) on the overall performance of the pipeline?
  - Basis in paper: [explicit] The authors discuss two specific implementations of feature extraction models (Cross-Attn and Rationale Extraction) and provide performance metrics for each. However, they do not deeply analyze the impact of these models on the final explanation quality
  - Why unresolved: While performance metrics are provided, there is no comparative analysis on how the choice of feature extraction model affects the quality of the generated explanations
  - What evidence would resolve it: A comprehensive study comparing the generated explanations' quality when using different feature extraction models, possibly including qualitative assessments from human evaluators

- Question: How does the size of the training dataset (PFE-dataset) affect the model's ability to discern compatibility relationships?
  - Basis in paper: [explicit] The authors conduct experiments by finetuning the LLM with subsets of the PFE-dataset and discuss the effect of training sample size on performance
  - Why unresolved: The study indicates that larger models require fewer samples for finetuning, but the optimal size of the training dataset for achieving the best performance is not clearly established
  - What evidence would resolve it: A detailed analysis of the model's performance across various dataset sizes, identifying the point of diminishing returns and the optimal dataset size for different model architectures

## Limitations

- The PFE dataset is relatively small (6,407 examples), which may limit the model's ability to generalize to all fashion compatibility scenarios
- The paper lacks comparison with more recent large language models beyond GPT-3.5
- The human evaluation methodology, while thorough, uses a relatively small sample size (50 explanations) that may not capture full variability in generated outputs

## Confidence

- **High confidence**: Automatic metric improvements (BLEURT, BLEU, ROUGE) over baseline methods are well-supported by experimental results; two-stage pipeline architecture is clearly described and implemented
- **Medium confidence**: Claim that manual curation enables better fine-tuning is plausible but not directly validated through ablation studies; human evaluation results are convincing but based on limited sample
- **Low confidence**: Paper doesn't adequately address whether generated explanations capture deeper compatibility reasoning or merely surface-level feature matching

## Next Checks

1. **Feature Extraction Ablation**: Remove the feature extraction stage and directly prompt the language model with just item categories and descriptions. Compare FID scores and human evaluation ratings to quantify the contribution of grounded feature prompts.

2. **Dataset Size Scaling**: Train the same pipeline using progressively larger but less curated datasets (e.g., automatically filtered explanations) to empirically test the paper's claim about quality-over-quantity benefits.

3. **Cross-Compatibility Transfer**: Test the model on item pairs from fashion categories not well-represented in the PFE dataset (e.g., swimwear + outerwear) to assess generalization beyond training distribution.