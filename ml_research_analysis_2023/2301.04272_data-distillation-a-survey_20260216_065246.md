---
ver: rpa2
title: 'Data Distillation: A Survey'
arxiv_id: '2301.04272'
source_url: https://arxiv.org/abs/2301.04272
tags:
- data
- distillation
- learning
- neural
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey explores data distillation, a technique that synthesizes
  compact, high-fidelity data summaries to replace large datasets for efficient and
  accurate model training. The paper provides a formal framework and taxonomy of existing
  approaches, covering various data modalities like images, graphs, and recommender
  systems.
---

# Data Distillation: A Survey

## Quick Facts
- arXiv ID: 2301.04272
- Source URL: https://arxiv.org/abs/2301.04272
- Reference count: 15
- Key outcome: Data distillation synthesizes compact, high-fidelity data summaries to replace large datasets for efficient and accurate model training across multiple modalities

## Executive Summary
Data distillation is an emerging technique that creates compact data summaries to serve as effective drop-in replacements for large datasets, enabling more efficient and accurate model training. This survey provides a comprehensive framework and taxonomy of existing approaches, covering various data modalities including images, graphs, and recommender systems. The paper highlights data distillation's potential to accelerate training, reduce computational costs, and improve eco-sustainability, while also identifying key challenges and future research directions in the field.

## Method Summary
Data distillation addresses the fundamental problem of synthesizing compact, high-fidelity data summaries that can replace large datasets for model training. The core approach follows a bilevel optimization framework where an inner loop trains models on the distilled data, while an outer loop optimizes the distilled data itself to minimize the difference in model performance compared to training on the full dataset. Various techniques implement this framework differently, including gradient matching (one-step distance matching between models), trajectory matching (matching full training trajectories), distribution matching (directly matching data distributions using metrics like MMD), and factorization (parameterizing data summaries using bases and hallucinators).

## Key Results
- Data distillation can achieve similar model performance to full dataset training while using significantly fewer samples
- Different distillation approaches offer tradeoffs between computational efficiency and distilled data quality
- The technique shows promise across multiple data modalities including images, graphs, and recommender systems
- Factorized parameterization can reduce storage requirements while maintaining performance through knowledge sharing between synthesized data points

## Why This Works (Mechanism)

### Mechanism 1: Gradient Matching
- Claim: Gradient matching improves scalability by avoiding full unrolling of the inner loop
- Mechanism: Performs one-step gradient distance matching between models trained on original and distilled data, avoiding computationally expensive TBPTT
- Core assumption: First-order approximation of training trajectories captures sufficient model similarity
- Evidence anchors: Abstract discussion of gradient matching methods like DC, DSA, DCC; section on gradient matching efficiency
- Break condition: When first-order approximation breaks down for highly non-convex loss landscapes or requires longer training horizons

### Mechanism 2: Factorized Parameterization
- Claim: Factorized parameterization reduces storage requirements while maintaining performance
- Mechanism: Parameterizes data summary using bases (independent vectors) and hallucinators (mapping to joint data-label space)
- Core assumption: Knowledge sharing through common bases maintains data fidelity while reducing storage
- Evidence anchors: Abstract mentions "terse data summaries"; section on factorization-based techniques
- Break condition: When basis dimension becomes too low to capture original data complexity

### Mechanism 3: Distribution Matching
- Claim: Distribution matching enables single-level optimization instead of bilevel optimization
- Mechanism: Directly matches distributions of original vs. distilled data using metrics like Maximum Mean Discrepancy
- Core assumption: Datasets similar by distribution divergence metrics lead to similarly trained models
- Evidence anchors: Abstract on "high-fidelity data summaries"; section on distribution-matching techniques
- Break condition: When distribution similarity does not translate to model similarity for complex loss landscapes

## Foundational Learning

- **Bilevel optimization**: Why needed - Data distillation fundamentally solves a bilevel problem where inner loop trains on distilled data and outer loop optimizes the distilled data itself. Quick check: What is the relationship between inner and outer optimization problems in data distillation?

- **Neural Tangent Kernel (NTK)**: Why needed - NTK-based approaches can solve inner optimization in closed form, making data distillation more efficient by avoiding iterative SGD. Quick check: How does using NTK in inner loop change computational complexity of data distillation?

- **Maximum Mean Discrepancy (MMD)**: Why needed - MMD is used in distribution matching approaches to measure difference between distributions of original and distilled datasets. Quick check: Why is MMD suitable for comparing distributions in high-dimensional data spaces?

## Architecture Onboarding

- **Component map**: Data synthesizer module (optimizes distilled dataset) -> Model trainer (evaluates synthesized data) -> Loss function (measures similarity between models trained on original vs. distilled data) -> Optimization framework (meta-model matching, gradient matching, trajectory matching, distribution matching, or factorization)

- **Critical path**: Synthesize distilled dataset -> Train model on this dataset -> Evaluate model's performance on held-out test set from original dataset -> Iteratively improve distilled dataset based on evaluation

- **Design tradeoffs**: Main tradeoff between computational efficiency and distillation quality. Methods avoiding unrolling (like gradient matching) are faster but may miss longer-term training dynamics. Factorized methods save storage but may lose fidelity. Distribution matching is efficient but relies on strong assumptions about data distribution and model quality relationship.

- **Failure signatures**: Common failure modes include: (1) Distilled data overfitting to specific model architecture used during distillation; (2) Loss of diversity in distilled dataset leading to poor generalization; (3) Computational explosion when unrolling too many training steps; (4) Poor performance when scaling to larger dataset sizes.

- **First 3 experiments**: 1) Replicate basic MNIST experiment comparing random sampling vs. distilled data using simple ConvNet architecture; 2) Test gradient matching vs. distribution matching on CIFAR-10 with same computational budget; 3) Evaluate scaling behavior by comparing performance as number of distilled samples increases from 10 to 1000 per class.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does data distillation perform on datasets with extreme class imbalance, and can it be adapted to handle such cases effectively?
- Basis in paper: [inferred] Paper mentions power-law distribution in recommender systems leading to data scarcity, but does not explore extreme class imbalance in general datasets
- Why unresolved: Survey focuses on balanced or moderately imbalanced datasets, leaving performance on highly imbalanced datasets unexplored
- What evidence would resolve it: Empirical studies comparing data distillation methods on balanced vs. highly imbalanced datasets with metrics like accuracy, F1-score, and area under precision-recall curve

### Open Question 2
- Question: What are the theoretical guarantees for the quality of data summaries generated by different data distillation frameworks?
- Basis in paper: [inferred] Discusses various frameworks but does not provide theoretical guarantees for quality of synthesized data summaries
- Why unresolved: Most existing techniques are evaluated empirically, lacking theoretical analysis on quality and generalization of synthesized data summaries
- What evidence would resolve it: Mathematical proofs or bounds on performance of models trained on distilled data summaries compared to full dataset models

### Open Question 3
- Question: How can data distillation techniques be extended to handle non-image data modalities, such as time-series or multi-modal data?
- Basis in paper: [explicit] Discusses challenges in extending to text, graphs, and recommender systems, but not time-series or multi-modal data
- Why unresolved: Existing techniques primarily designed for image data; adapting to other modalities requires addressing unique challenges like temporal dependencies or heterogeneous data types
- What evidence would resolve it: Successful application to time-series or multi-modal datasets with quantitative comparisons to baseline methods and analysis of specific challenges encountered

## Limitations

- Relationship between distribution similarity and model quality remains an assumption rather than proven guarantee, particularly for complex loss landscapes
- Computational scalability of bilevel optimization across different dataset sizes and model architectures needs more empirical validation
- Survey does not fully address how data distillation performance varies across different learning algorithms beyond standard SGD-based approaches

## Confidence

- **High Confidence**: Basic framework of data distillation as bilevel optimization is well-established and consistently described across surveyed literature
- **Medium Confidence**: Taxonomy and categorization of different data distillation approaches accurately reflects current state of field
- **Low Confidence**: Claims about relationship between specific distribution divergence metrics and downstream model performance require more rigorous empirical validation across diverse datasets

## Next Checks

1. **Distribution-Model Quality Correlation**: Systematically test whether MMD or other distribution divergence metrics reliably predict model performance across at least three different dataset modalities and model architectures

2. **Scalability Analysis**: Measure computational complexity of different data distillation approaches as dataset size increases from 1K to 1M samples, tracking both wall-clock time and memory usage

3. **Cross-Algorithm Generalization**: Evaluate whether distilled datasets optimized for one learning algorithm (e.g., SGD) maintain effectiveness when used with fundamentally different optimization approaches (e.g., Adam, second-order methods, or NTK-based training)