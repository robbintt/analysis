---
ver: rpa2
title: 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal
  Large Language Models'
arxiv_id: '2309.16211'
source_url: https://arxiv.org/abs/2309.16211
tags:
- image
- samples
- questions
- label
- poisoned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the problem of detecting dirty samples\u2014\
  including poisoned samples from backdoor attacks, noisy labels from crowdsourcing,\
  \ and their hybrids\u2014that degrade the reliability of deep neural networks. The\
  \ authors propose VDC (Versatile Data Cleanser), which exploits the visual-linguistic\
  \ inconsistency between images and their labels."
---

# VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2309.16211
- Source URL: https://arxiv.org/abs/2309.16211
- Authors: 
- Reference count: 23
- Key outcome: VDC achieves ~99.9% TPR and ~2.8% FPR on CIFAR-10, outperforming existing dirty sample detection methods

## Executive Summary
This paper tackles the problem of detecting dirty samples—including poisoned samples from backdoor attacks, noisy labels from crowdsourcing, and their hybrids—that degrade the reliability of deep neural networks. The authors propose VDC (Versatile Data Cleanser), which exploits the visual-linguistic inconsistency between images and their labels. VDC leverages multimodal large language models through a three-stage pipeline: generating visual questions, answering them using MLLM, and evaluating answer correctness to compute a consistency score. Experiments show VDC achieves high true positive rates (e.g., ~99.9% on CIFAR-10) and low false positive rates (e.g., ~2.8%) across various dirty sample types, datasets, and poisoning ratios, outperforming existing detection methods. The method generalizes well even to hybrid dirty samples, and training on datasets cleaned by VDC significantly improves model accuracy and reduces attack success rates. Limitations include sensitivity to corrupted labels and occasional LLM errors. Overall, VDC offers a universal, effective framework for improving dataset quality in data-centric AI.

## Method Summary
VDC (Versatile Data Cleanser) detects dirty samples by measuring visual-linguistic inconsistency between images and labels using a three-stage pipeline. First, a visual question generation (VQG) module uses an LLM to generate diverse questions about the image based on its label. Second, a visual question answering (VQA) module employs an MLLM to answer these questions about the image content. Third, a visual answer evaluation (VAE) module assesses the correctness of answers by checking consistency with the label semantics, computing a matching score. The final detection decision is based on whether this score exceeds a threshold. VDC is evaluated across multiple datasets (CIFAR-10, ImageNet-100, ImageNet-Dog) and dirty sample types (poisoned, noisy labels, hybrids), showing superior performance compared to baseline methods.

## Key Results
- VDC achieves ~99.9% TPR and ~2.8% FPR on CIFAR-10 for poisoned sample detection
- VDC outperforms baseline methods on ImageNet-100, with ~99.5% TPR and ~4.6% FPR
- Training models on datasets cleaned by VDC improves clean accuracy (e.g., +1.14% on CIFAR-10) and reduces attack success rates (e.g., -15.14% on CIFAR-10)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VDC detects dirty samples by measuring visual-linguistic inconsistency between images and their labels using MLLM.
- Mechanism: The framework generates visual questions based on the label, uses MLLM to answer them, and evaluates consistency between answers and expected label semantics.
- Core assumption: MLLM can accurately interpret visual content and compare it with linguistic label semantics to identify mismatches.
- Evidence anchors:
  - [abstract]: "VDC leverages multimodal large language models through a three-stage pipeline: generating visual questions, answering them using MLLM, and evaluating answer correctness to compute a consistency score."
  - [section 4.3]: "The VAE module assesses visual-linguistic inconsistency by evaluating the matching score between the semantics of the image and label."
- Break condition: If MLLM fails to correctly interpret visual content or if the label itself is corrupted, the inconsistency measure may be unreliable.

### Mechanism 2
- Claim: VDC generalizes across different types of dirty samples because they share a common property of visual-linguistic inconsistency.
- Mechanism: By focusing on semantic mismatch rather than specific attack patterns, VDC can detect poisoned samples, noisy labels, and their hybrids using the same framework.
- Core assumption: Various dirty samples (poisoned, noisy, hybrid) exhibit detectable visual-linguistic inconsistency regardless of their specific generation method.
- Evidence anchors:
  - [section 1]: "We find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels."
  - [section 5.2.3]: "VDC still shows leading advantages compared with other methods, with average TPR reaching 99.41%."
- Break condition: If dirty samples are carefully crafted to maintain visual-linguistic consistency (e.g., clean-label attacks), VDC's detection capability may fail.

### Mechanism 3
- Claim: The vote-based ensemble approach improves detection reliability by aggregating multiple question-answer evaluations.
- Mechanism: Multiple visual questions are generated and answered, with the final decision based on the proportion of consistent answers exceeding a threshold.
- Core assumption: Aggregating multiple independent semantic consistency checks reduces the impact of individual MLLM errors or poorly designed questions.
- Evidence anchors:
  - [section 4.3]: "The matching score si of sample (xi, yi) is computed as the proportion of questions answered correctly... If the score is less than the threshold α, sample (xi, yi) is detected as a dirty sample."
  - [section 6]: "We observe that using only one type of question makes the model perform worse."
- Break condition: If most generated questions are irrelevant to the actual semantic content or if MLLM consistently fails on specific question types, ensemble voting may not improve reliability.

## Foundational Learning

- Concept: Multimodal large language models (MLLM) capabilities
  - Why needed here: VDC relies on MLLM to interpret visual content and answer semantic questions about images.
  - Quick check question: Can the MLLM accurately answer questions about both general image content and specific label-related features?

- Concept: Visual-linguistic semantic alignment
  - Why needed here: The detection framework measures inconsistency between visual semantics and linguistic label semantics.
  - Quick check question: How does the framework determine if an MLLM's answer matches the expected label semantics?

- Concept: Ensemble voting and threshold-based decision making
  - Why needed here: VDC uses multiple questions and a threshold to make final dirty sample detection decisions.
  - Quick check question: How does changing the threshold α affect the trade-off between true positive rate and false positive rate?

## Architecture Onboarding

- Component map: Image+Label → VQG → VQA (MLLM) → VAE → Detection Decision
- Critical path: Image+Label → VQG → VQA (MLLM) → VAE → Detection Decision
  - Each module must complete successfully for the framework to function.
- Design tradeoffs:
  - Question generation balance: General questions provide broad context but may be less discriminative; label-specific questions are more targeted but require LLM generation.
  - Number of questions: More questions improve reliability but increase computational cost and inference time.
  - Threshold selection: Higher thresholds reduce false positives but may miss some dirty samples.
- Failure signatures:
  - High false positive rate: Indicates threshold is too low or questions are not discriminative enough.
  - Low true positive rate: Suggests questions are not capturing relevant semantic information or MLLM is failing on the dataset.
  - Inconsistent performance across datasets: May indicate dataset-specific question generation needs adjustment.
- First 3 experiments:
  1. Baseline test: Run VDC on clean data to verify low false positive rate (should be < 5%).
  2. Poisoned sample test: Evaluate on a small dataset with known poisoned samples to verify true positive detection.
  3. Noisy label test: Test on dataset with artificially corrupted labels to validate detection of non-backdoor dirty samples.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- MLLM dependency: Framework performance heavily relies on MLLM accuracy, which isn't independently validated
- Threshold sensitivity: Fixed threshold may not generalize across datasets with different characteristics
- Clean-label attack vulnerability: Framework assumes dirty samples exhibit visual-linguistic inconsistency, but sophisticated clean-label attacks could maintain semantic consistency

## Confidence
- High Confidence: Framework architecture and detection pipeline are clearly described. Experimental methodology for evaluating TPR and FPR is well-defined and reproducible.
- Medium Confidence: Performance claims are supported by experiments but depend heavily on MLLM capabilities that aren't independently validated. Generalization to hybrid samples is demonstrated but limited to specific scenarios.
- Low Confidence: Claims about universal applicability across all dirty sample types lack sufficient stress-testing. Sensitivity analysis for threshold selection and question generation quality is incomplete.

## Next Checks
1. **MLLM Performance Audit**: Measure MLLM accuracy on a held-out validation set of clean images from each target dataset to establish baseline capabilities before applying the detection framework.
2. **Threshold Sensitivity Analysis**: Systematically vary α from 0.1 to 0.9 in increments of 0.1 and plot TPR-FPR curves for each dataset to identify optimal thresholds and understand performance trade-offs.
3. **Clean-Label Attack Testing**: Evaluate VDC on clean-label poisoning attacks (e.g., reflection backdoors or GAN-based methods) where dirty samples maintain visual-linguistic consistency to stress-test the fundamental detection assumption.