---
ver: rpa2
title: Max-affine regression via first-order methods
arxiv_id: '2308.08070'
source_url: https://arxiv.org/abs/2308.08070
tags:
- holds
- lemma
- page
- proof
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies max-affine regression, a piecewise linear model
  that combines affine models via the max function. The model arises in applications
  including multiclass classification, auction problems, and convex regression.
---

# Max-affine regression via first-order methods

## Quick Facts
- arXiv ID: 2308.08070
- Source URL: https://arxiv.org/abs/2308.08070
- Reference count: 40
- Key outcome: Gradient descent and mini-batch SGD converge linearly to a neighborhood of the ground truth for max-affine regression under sub-Gaussian covariates with anti-concentration and additive sub-Gaussian noise, with SGD outperforming AM and GD in runtime and sample efficiency.

## Executive Summary
This paper analyzes gradient descent (GD) and mini-batch stochastic gradient descent (SGD) for max-affine regression, a piecewise linear model that combines affine models via the max function. Under assumptions of sub-Gaussian covariates with anti-concentration and additive sub-Gaussian noise, the authors prove that suitably initialized GD and SGD converge linearly to a neighborhood of the ground truth parameters. The paper also provides a non-asymptotic error bound that improves upon alternating minimization by a factor that grows at least as k−1+2ζ−1, where ζ is the anti-concentration parameter. Numerical results demonstrate that SGD converges faster in runtime with fewer observations than AM and GD in the noiseless case, and also outperforms them in low-sample scenarios with noise.

## Method Summary
The paper studies max-affine regression, where the response is the maximum of k affine functions of covariates plus noise. The authors analyze gradient descent and mini-batch SGD for minimizing the mean squared error loss. They assume sub-Gaussian covariates with anti-concentration and additive sub-Gaussian noise. The analysis relies on local strong convexity induced by the geometric separation of the ground truth parameters and the anti-concentration of covariates. The authors provide non-asymptotic convergence guarantees and error bounds for GD and SGD, showing that SGD can achieve faster runtime convergence and better sample efficiency compared to alternating minimization and GD.

## Key Results
- GD and mini-batch SGD converge linearly to a neighborhood of the ground truth for max-affine regression under sub-Gaussian covariates with anti-concentration and additive sub-Gaussian noise.
- The error bound for GD improves upon alternating minimization by a factor that grows at least as k−1+2ζ−1.
- SGD converges faster in runtime with fewer observations than alternating minimization and GD in the noiseless scenario, and also outperforms them in low-sample scenarios with noise.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient descent converges linearly to a neighborhood of the ground truth when initialized in a local region defined by geometric separation and anti-concentration conditions.
- Mechanism: The descent uses local strong convexity induced by the geometric structure of the max-affine model. When β is close to β⋆, the gradient descent direction aligns with the direction toward β⋆ due to the separation parameter κ and the anti-concentration of covariates.
- Core assumption: The initial estimate β0 lies within the neighborhood N(β⋆) defined by the geometric parameter ρ, and the data satisfies sub-Gaussianity and anti-concentration.
- Evidence anchors:
  - [abstract]: "a suitably initialized GD and SGD converge linearly to a neighborhood of the ground truth specified by the corresponding error bound"
  - [section 2]: Theorem 2.1 provides the formal statement and proof under these assumptions
  - [corpus]: Missing explicit neighbor comparisons; weak anchor
- Break condition: If the initialization is outside N(β⋆), the linear convergence guarantee fails. If the covariate model does not satisfy anti-concentration, the local geometry breaks down.

### Mechanism 2
- Claim: Mini-batch SGD converges linearly in expectation to the ground truth under the same geometric conditions as GD, with error scaling depending on batch size m.
- Mechanism: The stochastic gradients provide unbiased estimates of the true gradients with variance controlled by the batch size. The same geometric separation ensures local strong convexity, and the noise is averaged out by the batch updates.
- Core assumption: The batch size m satisfies the condition in Theorem 3.1 and the step size is chosen adaptively with m.
- Evidence anchors:
  - [abstract]: "SGD not only converges faster in run time with fewer observations than alternating minimization and GD in the noiseless scenario but also outperforms them in low-sample scenarios with noise"
  - [section 3]: Theorem 3.1 formalizes the convergence with error bound ˜O(σ²k²(d/m ∨ kd/n))
  - [corpus]: Weak anchor; no direct neighbor comparisons on batch size effects
- Break condition: If m is too small relative to d and log(n/δ), the variance dominates and the method may not be consistent. If m is too large, the runtime advantage disappears.

### Mechanism 3
- Claim: The error bound for GD improves upon alternating minimization by a factor that grows at least as k−1+2ζ−1 due to better handling of the least squares update.
- Mechanism: GD updates use the full gradient, which implicitly regularizes the estimation by averaging over all partitions, while AM uses per-partition least squares that can amplify noise when partitions are small.
- Core assumption: The covariate distribution satisfies the sub-Gaussian and anti-concentration conditions uniformly.
- Evidence anchors:
  - [abstract]: "The error bound for AM is larger by a factor that grows at least as k−1+2ζ−1"
  - [section 2]: Theorem 2.1 and the comparison with [14, Theorem 1] explain the improvement
  - [corpus]: Missing explicit comparison to AM; weak anchor
- Break condition: If the anti-concentration parameter ζ is very small, the improvement factor becomes large, but if ζ approaches 1, the gap narrows. If k is small, the difference may be negligible.

## Foundational Learning

- Concept: Sub-Gaussianity and anti-concentration of covariate distributions
  - Why needed here: These conditions ensure that the outer products of covariates are well-behaved and that the partitions induced by the max-affine model have sufficient measure, which is critical for the concentration bounds used in the proofs.
  - Quick check question: What does the anti-concentration condition imply about the probability that a linear form of the covariate is close to a constant?

- Concept: VC dimension and growth function for polytopes
  - Why needed here: The partitions Cj are polytopes formed by intersections of half-spaces. Controlling their complexity via VC theory is essential for uniform concentration of the empirical measures over all possible partitions.
  - Quick check question: How does the VC dimension of the collection of k-fold intersections of half-spaces scale with k and d?

- Concept: Local strong convexity in non-convex optimization
  - Why needed here: The max-affine loss is non-convex globally, but near the ground truth it behaves like a strongly convex function due to the geometric separation, enabling linear convergence of first-order methods.
  - Quick check question: What geometric condition on the ground truth parameters ensures that the max-affine loss is locally strongly convex?

## Architecture Onboarding

- Component map:
  - Loss function ℓ(β): Quadratic loss over max-affine predictions
  - Gradient computation: Block-wise subgradients using partition sets Cj
  - Partition sets Cj: Defined by pairwise comparisons of linear models
  - Initialization: Spectral method from [15] under Gaussian covariates
  - Convergence analysis: Relies on local geometry (κ, πmin) and concentration bounds
- Critical path:
  1. Generate data from max-affine model with sub-Gaussian covariates and noise
  2. Apply spectral initialization to obtain β0 ∈ N(β⋆)
  3. Run GD or mini-batch SGD with constant or adaptive step size
  4. Monitor convergence via estimation error ∥βt − β⋆∥
- Design tradeoffs:
  - GD vs SGD: GD has lower per-iteration cost but may converge slower in wall-clock time; SGD trades variance for speed
  - Batch size m: Larger m reduces variance but increases per-iteration cost; must balance for consistency
  - Initialization: Spectral method works under Gaussian covariates; extension to general sub-Gaussian case is open
- Failure signatures:
  - If ∥β0 − β⋆∥ > κρ, linear convergence fails
  - If n < Cπ−2(1+ζ−1)min·(kd log(n/d) + log(k/δ)), concentration bounds do not hold
  - If m is too small, the error bound does not vanish with n (non-consistency)
- First 3 experiments:
  1. Verify linear convergence of GD from spectral initialization on synthetic Gaussian data with k=3, d=50
  2. Compare GD and mini-batch SGD (m=64) on noiseless data with varying n and d to observe phase transition
  3. Test SGD with different batch sizes on noisy data to observe the bias-variance tradeoff

## Open Questions the Paper Calls Out
- The paper mentions that extending the analysis to robust regression with outliers is an intriguing future direction.
- The paper does not provide a theoretical analysis of how the choice of initialization affects the convergence rate and final error for the relaxed covariate model with anti-concentration.

## Limitations
- The analysis assumes exact knowledge of parameters like πmin, πmax, and κ, which are typically unknown in practice.
- The error improvement over alternating minimization depends critically on the anti-concentration parameter ζ, which may be difficult to characterize for general covariate distributions.
- The theoretical framework is sound but the assumptions are strong (sub-Gaussian covariates with anti-concentration, bounded parameters).

## Confidence
- High: The theoretical framework is sound and the proofs appear rigorous.
- Medium: The numerical results are promising but limited in scope.
- Low: Some key parameters are not practically estimable and the assumptions are strong.

## Next Checks
1. Implement the spectral initialization method from [15] and test its performance under various sub-Gaussian covariate distributions beyond Gaussian.
2. Conduct a systematic study of SGD performance with varying batch sizes on noisy data to empirically verify the error bound scaling with m.
3. Extend the analysis to handle time-varying or adversarial noise models to assess robustness beyond the i.i.d. sub-Gaussian assumption.