---
ver: rpa2
title: Defense Against Model Extraction Attacks on Recommender Systems
arxiv_id: '2310.16335'
source_url: https://arxiv.org/abs/2310.16335
tags:
- target
- surrogate
- ndcg
- loss
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes GRO, the first defense method against model
  extraction attacks on recommender systems. GRO is a general framework that can protect
  any recommender systems by maximizing the loss of the attacker's surrogate model
  while minimizing the loss of the target model.
---

# Defense Against Model Extraction Attacks on Recommender Systems

## Quick Facts
- arXiv ID: 2310.16335
- Source URL: https://arxiv.org/abs/2310.16335
- Reference count: 40
- Key outcome: GRO is the first defense framework that protects recommender systems from model extraction attacks by maximizing the attacker's surrogate model loss while preserving target model utility

## Executive Summary
This paper introduces GRO (Gradient-based Ranking Optimization), the first defense method against model extraction attacks on recommender systems. The core innovation involves converting top-k rankings to differentiable swap matrices, enabling gradient-based optimization to maximize the attacker's surrogate model loss while preserving the target model's utility. GRO uses a student model to simulate the attacker's surrogate model and applies a swap loss function to guide the target model's learning. Extensive experiments on three benchmark datasets demonstrate GRO's effectiveness in reducing surrogate model utility while maintaining target model performance.

## Method Summary
GRO is a general framework that protects any recommender system by converting discrete ranking outputs into differentiable swap matrices. The method employs a student model that mimics the attacker's surrogate model architecture and loss function. During training, GRO computes gradients of the student model's loss with respect to swap matrices derived from the target model's rankings. These gradients are used to construct new swap matrices that maximize the student model's loss. A swap loss function then forces the target model to learn rankings that correspond to these new matrices while preserving original utility. The approach uses bi-level optimization to simultaneously minimize target model loss and maximize surrogate model loss.

## Key Results
- GRO significantly outperforms baseline defenses in reducing surrogate model utility across all tested datasets
- The framework maintains high target model performance (HR@k and NDCG@k) while degrading attacker effectiveness
- GRO demonstrates effectiveness against sequential recommendation models like Bert4Rec

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting top-k rankings to swap matrices enables gradient computation for discrete ranking outputs.
- Mechanism: Swap matrices are binary matrices where each row has a single 1 indicating the item ranked at that position. These matrices are differentiable and can be multiplied with item score vectors to compute losses.
- Core assumption: The target model's output can be treated as a differentiable proxy for ranking order when transformed into swap matrices.
- Evidence anchors:
  - [abstract]: "we transform them into swap matrices which are instead differentiable"
  - [section]: "By doing a matrix multiplication between the swap matrix and the item ID vector... we obtain the exact top-k ranking list"
- Break condition: If the ranking order changes drastically between A and A' due to gradient optimization, the swap loss may not preserve utility.

### Mechanism 2
- Claim: Using a student model to simulate the surrogate model allows optimization of defense without access to the actual attacker.
- Mechanism: The student model is trained with the same loss function as the attacker's surrogate model, using the target model's top-k rankings as supervision. Gradients from the student model are used to create a new swap matrix that maximizes the student's loss.
- Core assumption: The student model architecture matches or closely approximates the attacker's surrogate model architecture.
- Evidence anchors:
  - [abstract]: "We use a student model as a replacement of the attacker's surrogate model"
  - [section]: "we assume that the student model has the same architecture as the target model, which is the worst case for defense"
- Break condition: If the attacker uses a very different architecture, the student model may not accurately simulate the surrogate model's behavior.

### Mechanism 3
- Claim: The swap loss function preserves original ranking order while forcing the target model to maximize the student model's loss.
- Mechanism: The swap loss compares rows of the original swap matrix A with the new swap matrix A', pushing items with largest gradients to have higher scores than items ranked at corresponding positions.
- Core assumption: When the swap loss converges to zero, the target model will rank items at the same positions as in A' for items that appear only once in A'.
- Evidence anchors:
  - [section]: "According to Theorem 4.1 and its proof, the target model will rank an item at the same position as A' if the item first appears in A'"
  - [section]: Lemma 4.1 provides mathematical proof of this mechanism
- Break condition: If multiple items have similar gradients in the same row, the optimization may produce invalid swap matrices.

## Foundational Learning

- Concept: Swap matrices as differentiable representations of rankings
  - Why needed here: Rankings are discrete and non-differentiable, preventing gradient-based optimization for defense
  - Quick check question: How does a swap matrix preserve the information of a ranking list while being differentiable?

- Concept: Bi-level optimization for defense against model extraction
  - Why needed here: The defense must minimize the target model's loss while maximizing the surrogate model's loss, which requires solving two optimization problems simultaneously
  - Quick check question: Why can't we directly optimize the surrogate model's loss if we don't have access to it?

- Concept: Sequential recommendation model architecture (e.g., Bert4Rec)
  - Why needed here: The defense is designed for sequential recommendation systems, which require handling sequences of user interactions
  - Quick check question: What makes sequential recommendation models different from other recommender systems in terms of attack vulnerability?

## Architecture Onboarding

- Component map:
  - Target model -> Top-k ranking generator -> Swap matrix converter -> Student model loss calculator -> Gradient calculator -> New swap matrix generator -> Swap loss optimizer -> Target model update

- Critical path: Query → Target model ranking → Swap matrix conversion → Student model loss calculation → Gradient computation → New swap matrix → Swap loss → Target model update

- Design tradeoffs:
  - Student model architecture matching vs. performance overhead
  - λ hyperparameter tuning vs. generalization across datasets
  - Number of queries vs. defense effectiveness

- Failure signatures:
  - High swap loss but poor target model utility (λ too high)
  - Low swap loss but ineffective defense (λ too low)
  - Target model performance degradation over time

- First 3 experiments:
  1. Test swap matrix conversion with simple ranking lists to verify gradient computation
  2. Train student model with known rankings to validate it approximates surrogate behavior
  3. Test swap loss convergence with synthetic A and A' matrices to verify theoretical guarantees

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the effectiveness of GRO be evaluated on larger, more complex real-world datasets with diverse user behaviors?
- Basis in paper: [inferred] The paper mentions using benchmark datasets (MovieLens-1M, MovieLens-20M, Steam) and acknowledges the limitations of these datasets in representing real-world complexity.
- Why unresolved: The paper only uses benchmark datasets and does not explore the effectiveness of GRO on larger, more complex real-world datasets.
- What evidence would resolve it: Experiments on larger, more complex real-world datasets with diverse user behaviors would provide evidence for GRO's effectiveness in real-world scenarios.

### Open Question 2
- Question: How does GRO perform against more advanced model extraction attacks that use techniques like adversarial training or transfer learning?
- Basis in paper: [inferred] The paper only evaluates GRO against a single model extraction attack method and does not consider more advanced attacks.
- Why unresolved: The paper does not explore GRO's effectiveness against more advanced model extraction attacks.
- What evidence would resolve it: Experiments comparing GRO's performance against various advanced model extraction attacks would provide evidence for its robustness.

### Open Question 3
- Question: What are the potential side effects of using GRO on the target model's performance for benign users?
- Basis in paper: [explicit] The paper mentions that GRO aims to preserve the utility of the target model while defending against attacks.
- Why unresolved: The paper does not explicitly investigate the potential side effects of GRO on the target model's performance for benign users.
- What evidence would resolve it: Experiments evaluating GRO's impact on the target model's performance for benign users would provide evidence for its trade-offs.

## Limitations
- The mathematical proof assumes ideal conditions that may not hold in practice, particularly when multiple items have similar gradients
- The student model's ability to accurately simulate unknown attacker architectures remains unproven, as evaluation only tests against one extraction attack
- The paper doesn't address computational overhead implications for large-scale production systems

## Confidence

**High confidence in the theoretical framework**: The swap matrix conversion and gradient computation mechanisms are mathematically sound and well-defined. The dual optimization objective (maximizing surrogate loss while minimizing target loss) is a valid approach to defense.

**Medium confidence in practical effectiveness**: While experimental results show GRO outperforms baselines, the evaluation is limited to one type of extraction attack and three datasets. The paper doesn't test against adaptive attackers who might modify their strategies when GRO is detected.

**Low confidence in scalability and generalization**: The paper doesn't address computational overhead implications for large-scale production systems, and the hyperparameters (particularly λ) may require extensive tuning for different recommender architectures beyond the tested Bert4Rec model.

## Next Checks

1. Test GRO against multiple attack variants: Implement and evaluate GRO against at least three different model extraction attack strategies (beyond the single method used in the paper) to verify defense robustness across attack types.

2. Validate swap matrix stability under edge cases: Create synthetic ranking scenarios where multiple items have nearly identical scores and verify that swap matrix conversion produces valid, differentiable outputs without gradient explosion or vanishing issues.

3. Measure computational overhead at scale: Benchmark GRO's training time and memory requirements on progressively larger datasets (scaling from 1M to 100M+ interactions) to quantify real-world deployment feasibility.