---
ver: rpa2
title: 'Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs'
arxiv_id: '2307.13078'
source_url: https://arxiv.org/abs/2307.13078
tags:
- certified
- training
- rcert
- robust
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Adaptive Certified Training (ACERT), a method
  that addresses the trade-off between standard accuracy and certified robustness
  in neural networks. ACERT adapts certification intensity to each input sample, optimizing
  robustness across all levels.
---

# Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs

## Quick Facts
- arXiv ID: 2307.13078
- Source URL: https://arxiv.org/abs/2307.13078
- Reference count: 27
- Primary result: Up to 2× higher certified robustness at same accuracy compared to baseline certified training methods

## Executive Summary
This paper introduces Adaptive Certified Training (ACERT), a method that addresses the trade-off between standard accuracy and certified robustness in neural networks. ACERT adapts certification intensity to each input sample by computing individual certified robust radii, rather than using a shared perturbation bound. The key insight is that training with adaptive certified radii improves both accuracy and robustness by reducing overregularization. Experimental results on MNIST, CIFAR-10, and TinyImageNet demonstrate that ACERT yields models with significantly better accuracy-robustness tradeoffs compared to baseline approaches, achieving state-of-the-art performance.

## Method Summary
ACERT computes an individual certified robust radius Ecert(x) for each input sample using interval bound propagation (IBP) and BrentQ root finding. During training, ACERT optimizes the cross-entropy loss at these adaptive radii rather than a fixed perturbation bound. The method introduces a new metric called ART (Accuracy-Robustness Tradeoff) that combines standard accuracy and average certified radius using a geometric mean. ACERT uses a CNN7 architecture with batch normalization and ReLU activations, trained with a OneCycleLR scheduler and weight decay of 5e-4. The training process adapts the certified radius per sample while maintaining efficient computation through a limited number of root-finding iterations per batch.

## Key Results
- ACERT achieves up to 2× higher certified robustness at the same standard accuracy compared to FastIBP baseline
- ART scores improve from 0.68 to 0.84 on CIFAR-10 with 2 BrentQ iterations
- ACERT effectively eliminates overregularization present in certified training, maintaining high standard accuracy
- The effect is more prominent on challenging datasets like TinyImageNet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive certified radii reduce overregularization by tailoring perturbation bounds to each input's vulnerability level.
- Mechanism: Instead of using a shared certification radius εt across all samples, ACERT computes an individual certified robust radius Ecert(x) for each input. This allows stronger perturbation bounds for samples that can tolerate them and weaker bounds for fragile samples, preserving standard accuracy.
- Core assumption: The certified robust radius for each input exists and is uniquely defined under continuity and monotonicity of the certified margin function.
- Evidence anchors:
  - [abstract] "The key insight is that training with adaptive certified radii improves both accuracy and robustness."
  - [section 3.1] "Definition 3.1(Certified robust radius)... maximum radius of an ℓp ball B(x, ε)... where the classifier is certifiably robust."
  - [corpus] No direct empirical support for monotonicity/continuity; assumption relies on theoretical setup.
- Break condition: If the certified margin function Rcert(x, ε) is not monotonic or continuous, the adaptive radius may not be well-defined or unique, breaking the mechanism.

### Mechanism 2
- Claim: Maximizing the certified robust radius per sample via implicit differentiation is equivalent to minimizing the certified margin at the optimal radius.
- Mechanism: By the implicit function theorem, the gradient of Ecert(x) with respect to model parameters is proportional to the negative gradient of the certified margin Rcert at Ecert(x). Therefore, optimizing the cross-entropy loss at Ecert(x) drives the model to increase the certified radius for each input.
- Core assumption: Rcert(w, ε) is continuously differentiable and ∂εRcert(w, ε) ≠ 0 at the certified radius.
- Evidence anchors:
  - [section 3.2] "Theorem 3.4 (Implicit function)... ∂wEcert(w) = − ∂wRcert(w, Ecert(w)) / ∂εRcert(w, Ecert(w))"
  - [section 3.2] "Remark 3.5... gradient step in a direction of maximizing the certified robust radius... is a scaled gradient step in a direction of minimizing the certified margin"
  - [corpus] No empirical validation shown for the derivative condition; theoretical claim only.
- Break condition: If the certified margin function is not differentiable or its derivative with respect to ε is zero at Ecert(x), the gradient step becomes undefined or zero, halting adaptation.

### Mechanism 3
- Claim: Using ART (Accuracy-Robustness Tradeoff) as a composite metric ensures balanced optimization across accuracy and robustness.
- Mechanism: ART = √(Acc × ACR) is homogeneous in its arguments, meaning scaling one dimension scales the metric proportionally. This property ensures that tuning the proportion κ of standard loss does not distort the relative ranking of models across different perturbation scales.
- Core assumption: Standard accuracy and average certified radius are independent measures whose geometric mean meaningfully reflects overall model quality.
- Evidence anchors:
  - [section 3.3] "The main advantage of the geometric mean... is that it is homogeneous as a function of each argument."
  - [section 3.3] "Introducing the ART score helps to select models from a Pareto front of the accuracy-robustness curve."
  - [corpus] No empirical evidence provided that ART correlates with downstream utility; assumption untested.
- Break condition: If accuracy and robustness are inversely correlated in a way that geometric mean masks critical failure modes, ART may mislead selection toward suboptimal models.

## Foundational Learning

- Concept: Interval Bound Propagation (IBP)
  - Why needed here: ACERT relies on IBP to compute certified margins and outer bounds of the network output for any given perturbation radius.
  - Quick check question: What are the forward propagation rules for upper and lower bounds in IBP for an affine layer with ReLU activation?

- Concept: Implicit Function Theorem in Optimization
  - Why needed here: ACERT uses the implicit function theorem to derive the gradient of the certified robust radius with respect to model parameters.
  - Quick check question: Under what conditions can you express ∂Ecert/∂θ as a function of ∂Rcert/∂θ and ∂Rcert/∂ε?

- Concept: Certified Robustness and Adversarial Perturbations
  - Why needed here: The paper's core contribution is improving the tradeoff between standard accuracy and certified robustness against adversarial ℓ∞ perturbations.
  - Quick check question: How does the certified robust radius differ from the empirical adversarial radius, and why is certification important for safety-critical applications?

## Architecture Onboarding

- Component map: BrentQ root finder -> IBP bound propagation -> Loss combiner -> Model parameters
- Critical path:
  1. Forward pass with IBP to compute bounds at Ecert(x)
  2. Root finding to compute Ecert(x) for each sample
  3. Compute robust loss using cross-entropy on IBP bounds at Ecert(x)
  4. Combine with standard loss and backpropagate
- Design tradeoffs:
  - Precision vs speed in root finding (2 iterations ≈ 3.3× slower than baseline)
  - εmax scheduler vs no scheduler (impacts final ART score)
  - κ tuning vs fixed κ=0 (affects Pareto front placement)
- Failure signatures:
  - Root finder fails to converge → NaNs in loss, training crash
  - IBP bounds collapse to zero → all samples certified robust at ε=0
  - εmax too high early → overregularization, low standard accuracy
- First 3 experiments:
  1. Run ACERT with 0 BrentQ iterations (set Ecert=0 for misclassified, εmax for correct) to verify improvement over baseline.
  2. Compare ART scores with and without εmax scheduler on CIFAR-10.
  3. Sweep κ ∈ [0,1] and plot accuracy-robustness curves to confirm Pareto dominance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between the adaptive certified radii and the certified margin function's differentiability, particularly for non-smooth activation functions like ReLU?
- Basis in paper: [explicit] The paper states "We observe that in the regularly initialized classifiers fθ, interval bound propagation-based certified margin function Rcert(x, ·) in practice satisfies the requirements of Theorem 3.2" but does not provide rigorous proof for all activation functions.
- Why unresolved: The theoretical guarantees depend on the certified margin function being continuous and strictly monotonically increasing, which may not hold for all activation functions and network architectures.
- What evidence would resolve it: A formal proof showing that Rcert(x, ·) satisfies the required conditions for all commonly used activation functions and network architectures, or empirical validation demonstrating when these conditions break down.

### Open Question 2
- Question: How does the performance of ACERT scale with increasing network depth and width, and what are the computational bottlenecks in large-scale implementations?
- Basis in paper: [inferred] The experiments use CNN7 architecture, and the paper mentions "Further optimization of the root finding algorithm's efficiency" but doesn't explore scaling to larger models.
- Why unresolved: The current implementation and experimental results are limited to relatively small networks, and it's unclear how the method performs on state-of-the-art architectures.
- What evidence would resolve it: Scaling experiments with deeper and wider networks, analysis of computational complexity as a function of network size, and identification of specific bottlenecks in the root-finding and bound propagation steps.

### Open Question 3
- Question: What is the optimal strategy for balancing the trade-off between root-finding precision and computational efficiency in ACERT?
- Basis in paper: [explicit] The paper shows that 2 iterations of BrentQ achieve high ART scores but discusses "Further optimization of the root finding algorithm's efficiency."
- Why unresolved: While the paper demonstrates that 2 iterations are sufficient, it doesn't explore the full spectrum of possible trade-offs or provide guidance on adaptive precision strategies.
- What evidence would resolve it: A systematic study of different root-finding strategies, precision levels, and their impact on both accuracy-robustness trade-offs and computational efficiency across various datasets and architectures.

## Limitations
- The certified margin function's monotonicity and continuity assumptions lack empirical validation
- The method's performance on large-scale architectures and datasets remains untested
- The computational overhead from root finding (3.3× slower) may limit practical deployment

## Confidence

- **High confidence**: The mathematical derivation of the implicit gradient and the overall optimization framework are sound within their stated assumptions.
- **Medium confidence**: The experimental results showing ART improvements are convincing, though limited to three datasets and specific architectures.
- **Low confidence**: The foundational assumptions about Rcert continuity, uniqueness of Ecert, and the practical validity of ART as a selection metric lack empirical verification.

## Next Checks

1. **Empirical monotonicity test**: Measure Rcert(x, ε) values across a grid of ε values for 100 randomly selected samples to verify monotonicity and continuity assumptions.
2. **Gradient stability analysis**: Compute ∂εRcert(w, ε) at Ecert(x) across training epochs to verify the implicit function theorem conditions hold throughout training.
3. **ART correlation validation**: Compare ART-selected models against models selected by accuracy-only or robustness-only metrics on a held-out robustness benchmark to verify ART correlates with practical utility.