---
ver: rpa2
title: On the Representational Capacity of Recurrent Neural Language Models
arxiv_id: '2310.12942'
source_url: https://arxiv.org/abs/2310.12942
tags:
- symbol
- probability
- deterministic
- turing
- transitions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the computational expressivity of recurrent
  neural language models (RLMs) by analyzing their ability to represent semimeasures
  over strings. It extends previous work on RNNs to the probabilistic setting, showing
  that RLMs with rational weights and unbounded computation time can simulate probabilistic
  Turing machines.
---

# On the Representational Capacity of Recurrent Neural Language Models

## Quick Facts
- arXiv ID: 2310.12942
- Source URL: https://arxiv.org/abs/2310.12942
- Reference count: 31
- Key outcome: Establishes upper and lower bounds on RLM expressivity by relating them to probabilistic Turing machines

## Executive Summary
This paper investigates the computational expressivity of recurrent neural language models (RLMs) by analyzing their ability to represent semimeasures over strings. It extends previous work on RNNs to the probabilistic setting, showing that RLMs with rational weights and unbounded computation time can simulate probabilistic Turing machines. This result provides an upper bound on RLM expressivity. The paper also establishes a lower bound by showing that real-time RLMs can simulate deterministic real-time rational probabilistic Turing machines. These findings place RLMs within a hierarchy of formal computational models, offering insights into their theoretical capabilities and limitations.

## Method Summary
The paper provides a theoretical analysis of RLM expressivity by establishing connections between RLMs and formal computational models. The authors construct a correspondence between RLMs with rational weights and probabilistic Turing machines, proving that RLMs can simulate both deterministic and non-deterministic probabilistic Turing machines under certain conditions. The analysis distinguishes between RLMs with unbounded computation time (εRLMs) and those restricted to real-time operation. The proofs involve encoding the state and stack contents of pushdown automata into the hidden state of the RLM and using the output matrix to define transition probabilities.

## Key Results
- RLMs with rational weights and unbounded computation time can simulate probabilistic Turing machines, establishing an upper bound on their expressivity
- Real-time RLMs can simulate deterministic real-time rational probabilistic Turing machines, providing a lower bound
- These results place RLMs within a hierarchy of formal computational models and provide insights into their theoretical capabilities and limitations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Recurrent Neural Language Models (RLMs) with rational weights and unbounded computation time can simulate probabilistic Turing machines (PTMs), establishing an upper bound on their expressivity.
- **Mechanism:** The paper extends Siegelmann and Sontag's (1992) Turing completeness result to the probabilistic case by showing how a rationally weighted RLM with unbounded computation time can simulate any deterministic probabilistic Turing machine (PTM) with rationally weighted transitions. This is achieved by encoding the state and stack contents of a 2PDA (which is strongly equivalent to PTMs) into the hidden state of the RLM and using the output matrix to define transition probabilities.
- **Core assumption:** Rational-valued arithmetic is a reasonable approximation of real-world models, and unbounded computation time is allowed.
- **Evidence anchors:**
  - [abstract]: "We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM)."
  - [section 3.3]: "Theorem 3.1. An εRLM can simulate any Σ-deterministic probabilistic 2PDA."
- **Break condition:** If the assumption of rational-valued arithmetic is not valid for practical implementations, or if the assumption of unbounded computation time is not met (as in real-time RLMs).

### Mechanism 2
- **Claim:** Real-time RLMs (without ε-transitions) can simulate deterministic real-time rational probabilistic Turing machines (RD-QPTMs), establishing a lower bound on their expressivity.
- **Mechanism:** The paper restricts the RLM to operate in real-time, meaning it processes a symbol at every time step. This restriction results in a more fine-grained hierarchy of specific Turing machine-like models equivalent to an RLM. The RLM can simulate RD-QPTMs by encoding the state and stack contents and using the output matrix to define transition probabilities, but now without the ability to perform unbounded computations between symbols.
- **Core assumption:** Real-time operation is a more realistic model of how RLMs function in practice.
- **Evidence anchors:**
  - [abstract]: "We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs."
  - [section 4.2]: "Theorem 4.1. RLMs can simulate RD−2PDAs."
- **Break condition:** If the assumption of real-time operation is not met, or if the RLM cannot effectively encode the state and stack contents within the real-time constraint.

### Mechanism 3
- **Claim:** The equivalence between PTMs and εRLMs (RLMs with ε-transitions) provides insights into the theoretical capabilities and limitations of RLMs.
- **Mechanism:** By establishing the equivalence between PTMs and εRLMs, the paper shows that RLMs with rational weights and unbounded computation time can represent the same class of semimeasures over strings as PTMs. This equivalence allows us to understand the types of probability measures RLMs can represent and provides a framework for analyzing their computational expressivity.
- **Core assumption:** The connection between formal computational models and LMs can provide insights into the capabilities and limitations of RLMs.
- **Evidence anchors:**
  - [abstract]: "These findings place RLMs within a hierarchy of formal computational models, offering insights into their theoretical capabilities and limitations."
  - [section 5]: "This work establishes upper and lower bounds on the expressive power of RLMs."
- **Break condition:** If the connection between formal computational models and LMs does not accurately reflect the capabilities of practical RLMs, or if the hierarchy of models does not provide meaningful insights.

## Foundational Learning

- **Concept: Probabilistic Turing Machines (PTMs)**
  - **Why needed here:** PTMs are used as a formal model of computation to characterize the expressivity of RLMs. Understanding PTMs is crucial for grasping the theoretical foundations of the paper.
  - **Quick check question:** What is the key difference between a PTM and a deterministic Turing machine?

- **Concept: Semimeasures over strings**
  - **Why needed here:** LMs are defined as semimeasures over strings, and the paper investigates the computational expressivity of RLMs in terms of the classes of semimeasures they can represent. Understanding semimeasures is essential for understanding the paper's main contributions.
  - **Quick check question:** How does a semimeasure differ from a probability measure, and why is this distinction important for LMs?

- **Concept: Recurrent Neural Networks (RNNs) and Language Models (LMs)**
  - **Why needed here:** The paper focuses on RLMs, which are LMs based on RNNs. Understanding the basic principles of RNNs and LMs is necessary for following the paper's arguments and constructions.
  - **Quick check question:** How does an RNN maintain a hidden state, and how is this used to define a language model?

## Architecture Onboarding

- **Component map:** RLM (rational weights) -> Hidden state (encodes 2PDA/PTM configuration) -> Output matrix E (defines transition probabilities) -> Next symbol probabilities

- **Critical path:**
  1. Initialize the hidden state with the initial configuration of the 2PDA or PTM
  2. At each time step, compute the hidden state update based on the current input symbol and the previous hidden state
  3. Use the output matrix E to map the hidden state to a vector of probabilities over the next symbol
  4. Sample the next symbol according to the probabilities and update the hidden state accordingly
  5. Repeat steps 2-4 until the end-of-string symbol is generated or the simulated 2PDA or PTM halts

- **Design tradeoffs:**
  - Rational vs. real-valued weights: The paper assumes rational-valued weights for theoretical analysis, but practical RLMs use real-valued weights. This assumption may limit the applicability of the results to real-world scenarios
  - Unbounded vs. real-time computation: The upper bound result assumes unbounded computation time, while the lower bound result considers real-time RLMs. This distinction highlights the gap between theoretical capabilities and practical constraints

- **Failure signatures:**
  - If the RLM cannot effectively encode the state and stack contents of the simulated 2PDA or PTM within the given constraints (rational weights, real-time operation)
  - If the output matrix E cannot accurately represent the transition probabilities of the simulated 2PDA or PTM
  - If the saturated sigmoid function f is not suitable for implementing the required operations (popping, pushing, conjunction) in the RNN

- **First 3 experiments:**
  1. Implement a simple RLM with rational weights and test its ability to simulate a basic 2PDA or PTM on a small set of strings
  2. Vary the hidden state dimensionality and the output matrix size to study their impact on the RLM's ability to simulate more complex 2PDAs or PTMs
  3. Compare the performance of the RLM with rational weights to an RLM with real-valued weights on a set of benchmark language modeling tasks, to assess the practical implications of the rational-valued assumption

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the exact computational power of a rationally weighted recurrent language model (RLM)?
- **Basis in paper:** Explicit - The paper states this as an open question and discusses the gap between the upper and lower bounds established
- **Why unresolved:** The paper establishes upper and lower bounds but does not provide a precise characterization of the computational power of rationally weighted RLMs
- **What evidence would resolve it:** A proof showing either equivalence or strict containment between RLMs and specific classes of probabilistic Turing machines or pushdown automata

### Open Question 2
- **Question:** Can εRLMs simulate non-deterministic probabilistic Turing machines (PTMs) without introducing two types of ε symbols to store head movement direction?
- **Basis in paper:** Explicit - The paper mentions this as an open question in the discussion section
- **Why unresolved:** The current construction for simulating PTMs with εRLMs requires two types of ε symbols to encode head movement, which adds complexity
- **What evidence would resolve it:** A construction demonstrating that a single ε symbol is sufficient for εRLMs to simulate non-deterministic PTMs, or a proof that two ε symbols are necessary

### Open Question 3
- **Question:** How do practical considerations like bounded precision in floating-point arithmetic affect the theoretical capabilities of RLMs?
- **Basis in paper:** Explicit - The paper acknowledges this as a limitation in the discussion section
- **Why unresolved:** The theoretical analysis assumes rational arithmetic, which differs from the real-valued computations used in practice
- **What evidence would resolve it:** Empirical studies comparing the performance of RLMs with rational versus floating-point weights on tasks requiring precise computation, or theoretical bounds on the impact of finite precision on RLM expressivity

## Limitations

- The upper bound result assumes unbounded computation time and rational-valued weights, both unrealistic for practical RLMs
- No empirical validation of whether real RLMs achieve these theoretical bounds
- Paper does not use specific benchmark tasks or datasets to test theoretical claims

## Confidence

- **High confidence** in the mathematical proofs establishing theoretical connections between RLMs and probabilistic Turing machines
- **Medium confidence** that these theoretical bounds meaningfully constrain practical RLM performance
- **Low confidence** in direct practical implications without empirical validation on real language modeling tasks

## Next Checks

1. Implement a simple RLM with bounded computation time and real-valued weights, then empirically test whether its language modeling performance aligns with the theoretical lower bound predictions
2. Systematically vary the hidden state dimensionality and measure how this affects the RLM's ability to approximate semimeasures on synthetic probabilistic language tasks
3. Compare the theoretical upper bound semimeasure class with the actual semimeasures induced by trained RLMs on standard benchmarks (WikiText, PTB) to identify any gaps between theory and practice