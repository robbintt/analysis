---
ver: rpa2
title: 'Unified Classification and Rejection: A One-versus-All Framework'
arxiv_id: '2311.13355'
source_url: https://arxiv.org/abs/2311.13355
tags:
- detection
- learning
- classification
- training
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a unified one-versus-all (OVA) framework for
  open set recognition, combining multi-class classification and out-of-distribution
  (OOD) rejection. The core idea is to decompose the K-class problem into K binary
  OVA classifiers and combine their outputs using Dempster-Shafer theory of evidence
  to obtain (K+1)-class posterior probabilities, enabling unified classification and
  rejection decisions.
---

# Unified Classification and Rejection: A One-versus-All Framework

## Quick Facts
- arXiv ID: 2311.13355
- Source URL: https://arxiv.org/abs/2311.13355
- Reference count: 40
- Key outcome: Unified OVA framework for open set recognition combining classification and OOD rejection without requiring OOD samples in training

## Executive Summary
This paper proposes a unified one-versus-all (OVA) framework for open set recognition that simultaneously handles multi-class classification and out-of-distribution (OOD) rejection. The core innovation lies in decomposing the K-class problem into K binary OVA classifiers and combining their outputs using Dempster-Shafer theory of evidence to obtain (K+1)-class posterior probabilities. To maintain closed-set classification accuracy, a hybrid learning strategy combines OVA loss with multi-class cross-entropy loss. The framework is implemented using a convolutional prototype network and evaluated on popular OSR and OOD detection benchmarks, demonstrating competitive performance without requiring OOD samples during training.

## Method Summary
The method decomposes a K-class classification problem into K one-versus-all binary classification tasks, where each classifier distinguishes one class from all others. These binary outputs are combined using Dempster-Shafer theory of evidence to produce K+1 class probabilities (K known classes plus OOD). A hybrid learning strategy combines OVA loss with multi-class cross-entropy loss to maintain classification accuracy while enabling OOD rejection. The framework is implemented on a convolutional prototype network where each class is represented by a learnable prototype vector, and OVA classifiers are applied to distance-based features. The model is trained on in-distribution data only, yet can detect OOD samples implicitly through the OVA framework.

## Key Results
- Achieves competitive closed-set classification accuracy while enabling OOD detection without requiring OOD samples in training
- Outperforms many post-hoc techniques using a single multi-class classifier architecture
- Demonstrates strong performance on both OOD detection and misclassification detection tasks across multiple benchmark datasets
- Shows that the hybrid learning strategy effectively balances classification accuracy with OOD rejection capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: One-versus-all decomposition enables unified detection of both OOD and misclassification without requiring OOD samples in training
- Mechanism: By treating each known class as positive and the union of all other known classes as negative, the OOD class is implicitly included in the negative class. This allows OOD samples to be rejected by all binary classifiers simultaneously, even without explicit OOD training data
- Core assumption: The union of known classes in negative training data adequately represents the boundary between known and unknown classes
- Evidence anchors:
  - [abstract]: "By decomposing the K-class problem into K one-versus-all (OVA) binary classification tasks and binding some parameters, we show that combining the scores of OVA classifiers can give (K+1)-class posterior probabilities, which enables classification and OOD rejection in a unified framework"
  - [section 3.2]: "OVA classifier has the potential of rejecting OOD inputs because the OOD class is implied in the negative class of each binary classifier, even though there is no OOD samples in training"
- Break condition: If the negative class distribution in OVA training doesn't adequately capture the OOD boundary, OOD samples may be incorrectly classified to known classes

### Mechanism 2
- Claim: Dempster-Shafer theory of evidence enables combining K binary OVA classifiers into K+1 class posterior probabilities
- Mechanism: The DSTE framework combines evidence from multiple binary classifiers by calculating the combined probability for each known class and an additional OOD class probability, ensuring all probabilities sum to 1
- Core assumption: The binary OVA classifiers are conditionally independent given the true class label
- Evidence anchors:
  - [abstract]: "By decomposing the K-class problem into K one-versus-all (OVA) binary classification tasks and binding some parameters, we show that combining the scores of OVA classifiers can give (K+1)-class posterior probabilities"
  - [section 3.2]: "Using Dempster–Shafer theory of evidence [41, 27], the combined evidence of class i is pm i (x) = 1/A pb i (x) · Π j̸=i (1 − pb j (x))"
- Break condition: If binary classifiers are highly correlated or dependent, the DSTE combination may produce incorrect probability estimates

### Mechanism 3
- Claim: Hybrid learning strategy maintains closed-set classification accuracy while enabling OOD rejection
- Mechanism: Combining OVA loss with multi-class cross-entropy loss through a weighted objective function preserves multi-class decision boundaries while simultaneously training for OOD rejection capability
- Core assumption: The regularization from multi-class CE loss is sufficient to maintain classification accuracy when combined with OVA training
- Evidence anchors:
  - [abstract]: "To maintain the closed-set classification accuracy of the OVA trained classifier, we propose a hybrid training strategy combining OVA loss and multi-class cross-entropy loss"
  - [section 4]: "The objective of hybrid learning is: L = β · LOVA + (1 − β) · Lreg"
- Break condition: If β is poorly tuned, either OOD detection or classification accuracy may be significantly compromised

## Foundational Learning

- Concept: Dempster-Shafer Theory of Evidence
  - Why needed here: Provides mathematical framework to combine binary OVA classifier outputs into unified K+1 class probabilities
  - Quick check question: How does DSTE handle conflicting evidence from multiple binary classifiers?

- Concept: One-versus-All (OVA) Learning Strategy
  - Why needed here: Enables implicit OOD detection without requiring OOD samples in training
  - Quick check question: What is the negative class in OVA training composed of?

- Concept: Prototype-based Classification
  - Why needed here: Provides local density representation for each class, enabling distance-based OOD detection
  - Quick check question: How does CPN represent each class in feature space?

## Architecture Onboarding

- Component map:
  Feature extractor (CNN backbone like ResNet-18/50 or ViT) -> Prototype layer (K learnable prototype vectors) -> OVA classifiers (K binary discriminators with sigmoid outputs) -> DSTE combiner (transforms K binary outputs to K+1 class probabilities) -> Hybrid loss computation (OVA + CE regularization + prototype loss)

- Critical path:
  1. Input image → Feature extractor → Feature vector
  2. Feature vector → K prototype distances → K OVA discriminators → K binary probabilities
  3. K binary probabilities → DSTE combination → K+1 class probabilities
  4. K+1 probabilities → Decision (classification vs rejection)

- Design tradeoffs:
  - OVA vs CE: OVA enables OOD detection but may hurt classification accuracy
  - Shared vs individual thresholds: Shared thresholds improve multi-class accuracy but may reduce OOD detection sensitivity
  - Temperature parameter ξ: Higher values create sharper probability distributions but may cause gradient issues

- Failure signatures:
  - Poor OOD detection: High false positive rate, OOD samples classified as known classes
  - Degraded classification: Accuracy drops significantly compared to standard CE training
  - Training instability: Sigmoid saturation causing vanishing gradients

- First 3 experiments:
  1. CIFAR-10 baseline: Train CPN with standard CE loss, measure classification accuracy and OOD detection (AUROC)
  2. OVA ablation: Train CPN with OVA loss only, compare classification accuracy and OOD detection vs baseline
  3. Hybrid validation: Train CPN with hybrid loss, tune β parameter to balance classification vs OOD detection performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed OVA framework be effectively implemented with other classifier architectures beyond convolutional prototype networks, such as autoencoder-based models or CSSR?
- Basis in paper: [explicit] The paper mentions that the proposed framework and learning strategies can be implemented with other classifier architectures like autoencoder-based models and CSSR, but does not provide detailed experiments or analysis
- Why unresolved: Implementing the OVA framework with different architectures requires addressing architecture-specific challenges and may lead to different performance outcomes that are not yet explored
- What evidence would resolve it: Comparative experiments showing the performance of OVA framework implementations across various classifier architectures on OOD detection and misclassification detection tasks

### Open Question 2
- Question: What are the performance implications of training the OVA framework with OOD samples, and how does this affect the unified classification and rejection capabilities?
- Basis in paper: [explicit] The paper states that while OOD samples are not used in their experiments, the proposed OVA and hybrid learning strategies are easily applicable to cases where OOD samples are available for training
- Why unresolved: The impact of including OOD samples during training on the model's ability to classify known classes and reject OOD/misclassification inputs is not empirically validated in the paper
- What evidence would resolve it: Experimental results comparing the performance of the OVA framework with and without OOD samples in training, focusing on classification accuracy and rejection capabilities

### Open Question 3
- Question: How can the measurement of performance for joint classification and OOD/misclassification rejection be standardized to provide a comprehensive evaluation of the proposed framework?
- Basis in paper: [inferred] The paper evaluates OOD detection and misclassification detection separately, acknowledging that the proposed rejection rule allows for both rejection types simultaneously, but a standardized measurement for joint performance is not provided
- Why unresolved: Current evaluation metrics focus on separate tasks, and there is no established method to assess the combined performance of classification and rejection in a unified manner
- What evidence would resolve it: Development and validation of new metrics or adaptation of existing ones to accurately measure the joint performance of classification and rejection in open set recognition tasks

## Limitations
- The assumption that negative class distributions in OVA training adequately capture OOD boundaries without explicit OOD samples may not hold for all datasets
- The hybrid learning strategy requires careful tuning of the balance parameter β, which may not generalize across all datasets
- The DSTE combination assumes conditional independence of binary classifiers, which may not hold in practice

## Confidence
- High Confidence: The hybrid learning framework combining OVA and CE losses is mathematically sound and reproducible
- Medium Confidence: OVA's implicit OOD detection capability, as performance depends heavily on dataset characteristics and hyperparameter tuning
- Medium Confidence: DSTE combination method, as its effectiveness relies on classifier independence assumptions

## Next Checks
1. Ablation Study on β Parameter: Systematically evaluate how different β values affect the tradeoff between classification accuracy and OOD detection performance across multiple datasets

2. Correlation Analysis of Binary Classifiers: Measure pairwise correlations between OVA classifier outputs to validate the conditional independence assumption required for DSTE combination

3. OOD Boundary Visualization: Use t-SNE or UMAP to visualize feature distributions of in-distribution vs OOD samples in the prototype space to assess whether negative class training adequately captures OOD boundaries