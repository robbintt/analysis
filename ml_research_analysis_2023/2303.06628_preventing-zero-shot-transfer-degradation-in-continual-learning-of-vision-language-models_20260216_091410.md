---
ver: rpa2
title: Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language
  Models
arxiv_id: '2303.06628'
source_url: https://arxiv.org/abs/2303.06628
tags:
- learning
- task
- transfer
- dataset
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of catastrophic forgetting in
  continual learning of vision-language models, particularly the degradation of zero-shot
  transfer ability during fine-tuning. The authors propose a novel method called ZSCL
  that mitigates forgetting through two key components: distillation in feature space
  using a reference dataset and weight ensemble in parameter space.'
---

# Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models

## Quick Facts
- arXiv ID: 2303.06628
- Source URL: https://arxiv.org/abs/2303.06628
- Reference count: 40
- Key outcome: Novel ZSCL method achieves 9.7% higher average score on Multi-domain Task Incremental Learning benchmark compared to state-of-the-art methods

## Executive Summary
This paper addresses the critical problem of catastrophic forgetting in continual learning of vision-language models, specifically focusing on preserving zero-shot transfer ability during fine-tuning. The authors propose ZSCL, a method that combines feature-space distillation using a reference dataset with weight ensemble in parameter space. Their approach successfully mitigates forgetting while maintaining both downstream task performance and zero-shot transfer capability, achieving state-of-the-art results on both conventional class-incremental learning benchmarks and a challenging new Multi-domain Task Incremental Learning (MTIL) benchmark.

## Method Summary
ZSCL prevents zero-shot transfer degradation in continual learning through two complementary mechanisms: (1) feature-space distillation using a reference dataset with diverse semantics, which preserves the original CLIP model's similarity structure between images and texts without requiring matched pairs or labels; and (2) weight ensemble across training iterations, which prevents large parameter shifts by averaging model weights throughout training rather than just at initialization and final state. The method uses the initial pre-trained CLIP model as the teacher for all distillation, rather than using progressively fine-tuned models, to avoid amplifying deviations from the original feature space.

## Key Results
- ZSCL achieves 9.7% higher average score on the challenging MTIL benchmark compared to state-of-the-art methods
- Outperforms previous approaches on conventional class-incremental learning benchmarks (CIFAR100, TinyImageNet)
- Successfully preserves zero-shot transfer ability while maintaining strong downstream task performance
- Weight ensemble method is stable and not sensitive to hyper-parameters

## Why This Works (Mechanism)

### Mechanism 1: Feature-space distillation with reference dataset
Using a semantically diverse reference dataset for distillation between current and initial models preserves the original CLIP model's zero-shot transfer ability by maintaining the relative similarity structure between images and texts in feature space. This approach avoids the pitfalls of using previous task data or current task data for distillation, which can lead to deviations from the original pre-training distribution.

### Mechanism 2: Weight ensemble across training iterations
Averaging model weights throughout the training process prevents large parameter shifts that degrade zero-shot transfer while maintaining downstream task performance. This creates a weighted average of models with different learning-forgetting tradeoffs, providing a better balance than any single point in the training trajectory.

### Mechanism 3: Initial model as teacher
Using the initial pre-trained CLIP model as the teacher for all distillation (rather than the previous task's model) better preserves zero-shot transfer ability by preventing amplification of deviations from the original feature space that would occur with progressively fine-tuned teacher models.

## Foundational Learning

- **Catastrophic forgetting in neural networks**: Understanding why continual learning models lose performance on previous tasks is fundamental to grasping the problem ZSCL addresses. Quick check: What is the difference between catastrophic forgetting and graceful forgetting in continual learning?

- **Zero-shot transfer ability**: ZSCL specifically aims to preserve the model's ability to perform on unseen tasks without additional training. Quick check: How does zero-shot transfer differ from few-shot learning in vision-language models?

- **Contrastive learning and feature alignment**: CLIP's pre-training uses contrastive learning to align image and text embeddings, which is the foundation for its zero-shot capabilities. Quick check: What role does the temperature parameter play in CLIP's contrastive learning objective?

## Architecture Onboarding

- **Component map**: CLIP backbone (ViT-B/16 image encoder + text encoder) -> Distillation loss module (feature space regularization) -> Weight ensemble module (parameter space regularization) -> Reference dataset loader (ImageNet images + semantic text) -> MTIL benchmark evaluation

- **Critical path**: Training loop → Distillation loss computation → Weight ensemble update → Evaluation on current task and zero-shot transfer tasks

- **Design tradeoffs**: Reference dataset size vs. memory usage, distillation strength (λ) vs. new task learning rate, weight ensemble frequency (I) vs. training efficiency

- **Failure signatures**: Zero-shot transfer accuracy drops significantly while new task accuracy remains high, or vice versa; model performance degrades on both new tasks and zero-shot transfer

- **First 3 experiments**: 
  1. Baseline: Train CLIP on MTIL without any continual learning techniques, measure zero-shot transfer degradation
  2. Ablation: Implement feature-space distillation only, compare zero-shot transfer preservation vs. baseline
  3. Full system: Implement both distillation and weight ensemble, verify improved performance on both new tasks and zero-shot transfer

## Open Questions the Paper Calls Out

- **What is the optimal number of classes and images in the reference dataset for distillation?**: The paper mentions that fewer classes of images in the reference dataset have a worse impact on the performance compared with the image numbers, but does not provide a definitive optimal number.

- **Can the zero-shot transfer ability be preserved without an external reference dataset?**: The current method relies on a reference dataset for distillation, and the paper suggests that finding a way to preserve zero-shot transfer without it is an open problem.

- **How does the choice of text sources for replay affect the performance of the distillation loss?**: While the paper indicates that more semantic text sources can improve performance, it does not explore the full range of possible text sources or their comparative effectiveness.

- **What is the impact of different training orders on the performance of the ZSCL method in the MTIL benchmark?**: The paper mentions two training orders but does not provide a comprehensive comparison of their effects on the method's performance.

## Limitations

- Evaluation focuses on relatively small-scale datasets rather than full ImageNet-scale data that would truly test zero-shot transfer preservation
- Choice of reference dataset (ImageNet images with CC text) may introduce domain bias that could limit generalization to other vision domains
- Paper doesn't extensively explore how ZSCL performs under more challenging continual learning scenarios such as class-incremental learning with overlapping classes

## Confidence

- **High confidence**: The effectiveness of feature-space distillation using a semantically diverse reference dataset for preserving zero-shot transfer ability
- **Medium confidence**: The weight ensemble mechanism's contribution and the claim that using the initial pre-trained model as teacher is superior to using previous task models
- **Medium confidence**: Overall performance claims, as the paper shows quantitative improvements but lacks extensive ablation studies on different ensemble frequencies

## Next Checks

1. Test ZSCL on larger-scale datasets (full ImageNet or multi-dataset benchmarks) to verify zero-shot transfer preservation scales to real-world scenarios
2. Conduct ablation studies varying the reference dataset composition and size to determine sensitivity to dataset quality and diversity
3. Evaluate ZSCL under more challenging continual learning scenarios such as online learning with streaming data or class-incremental learning with class overlap