---
ver: rpa2
title: Narrowing the Gap between Supervised and Unsupervised Sentence Representation
  Learning with Large Language Model
arxiv_id: '2309.06453'
source_url: https://arxiv.org/abs/2309.06453
tags:
- sentence
- uni00000013
- dataset
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the significant performance gap between supervised
  and unsupervised sentence representation learning methods. The authors propose a
  new metric called Fitting Difficulty Increment (FDI) to quantify the relative fitting
  difficulty between training and evaluation datasets, and demonstrate that higher
  FDI correlates with better performance on semantic textual similarity (STS) tasks.
---

# Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model

## Quick Facts
- **arXiv ID**: 2309.06453
- **Source URL**: https://arxiv.org/abs/2309.06453
- **Reference count**: 40
- **Primary result**: Achieved state-of-the-art unsupervised results on STS tasks, with Spearman's correlation of 79.79% and 80.02% on BERT and RoBERTa backbones respectively

## Executive Summary
This paper addresses the significant performance gap between supervised and unsupervised sentence representation learning methods. The authors introduce a new metric called Fitting Difficulty Increment (FDI) to quantify the relative fitting difficulty between training and evaluation datasets, demonstrating that higher FDI correlates with better performance on semantic textual similarity (STS) tasks. By leveraging large language models' in-context learning capability to generate training data that simulates complex patterns from STS and NLI datasets, combined with a novel Hierarchical Triplet (HT) loss, the approach successfully narrows this performance gap and achieves state-of-the-art results among unsupervised methods on STS tasks.

## Method Summary
The method combines several innovations to improve unsupervised sentence representation learning. First, it introduces the FDI metric to quantify the fitting difficulty gap between training and evaluation data by measuring changes in alignment and uniformity metrics during training. Second, it uses large language models with in-context learning to generate synthetic training data that simulates complex patterns from STS and NLI datasets. Third, it employs a Hierarchical Triplet (HT) loss that leverages the graded similarity scores in STS data by ensuring that intermediate sentences are more similar to source sentences than negatives but less similar than positives. The approach trains BERT/RoBERTa encoders using contrastive learning with both InfoNCE and HT losses on the LLM-generated data, achieving significant improvements over existing unsupervised baselines.

## Key Results
- Achieved state-of-the-art unsupervised results on STS tasks with Spearman's correlation scores of 79.79% (BERT) and 80.02% (RoBERTa)
- Outperformed existing unsupervised baselines by a large margin, narrowing the performance gap with supervised methods
- Demonstrated that higher FDI values correlate with better STS task performance
- Successfully utilized LLM-generated data to simulate complex patterns from STS and NLI datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FDI captures the relative fitting difficulty between training and evaluation data, and higher FDI correlates with better STS task performance.
- Mechanism: During training, alignment and uniformity metrics change differently for supervised vs unsupervised data. When evaluation data is harder to fit than training data (negative FDI), performance drops. When evaluation data is easier to fit (positive FDI), performance improves.
- Core assumption: The change in alignment/uniformity during training reflects the underlying complexity of patterns in the data, and these patterns determine downstream STS performance.
- Evidence anchors:
  - [abstract] "We introduce a metric, called Fitting Difficulty Increment (FDI), to measure the fitting difficulty gap between the evaluation dataset and the held-out training dataset, and use the metric to answer the 'What' question."
  - [section] "When training on the supervised dataset, both alignment and uniformity on the training data drop to a lower value than those on the evaluation data. Conversely, under the unsupervised settings, both alignment and uniformity on the training data keep a higher value than those on the evaluation data."
  - [corpus] Weak: corpus mentions unsupervised sentence embedding methods but does not directly validate FDI metric. The evidence is indirect through related contrastive learning papers.
- Break condition: If the STS task fundamentally requires different features than what FDI measures, the correlation could break.

### Mechanism 2
- Claim: LLM pattern simulation introduces complex patterns from STS/NLI datasets into unsupervised training data, raising FDI and narrowing the performance gap.
- Mechanism: By using LLM's in-context learning to generate positive/negative sentence pairs that mimic STS/NLI patterns, the training data becomes more similar to supervised data in pattern complexity, raising FDI and improving STS performance.
- Core assumption: The LLM can accurately simulate the statistical patterns present in STS/NLI datasets, and these simulated patterns are sufficient to train effective sentence embeddings.
- Evidence anchors:
  - [abstract] "We achieve this by leveraging the In-Context Learning (ICL) capability of the Large Language Model (LLM) to generate data that simulates complex patterns."
  - [section] "We believe that the NLI datasets have more difficult patterns in both the (anchor, positive sentence) pair and (anchor, negative sentence) pair, compared to the Wiki dataset."
  - [corpus] Weak: corpus contains related contrastive learning papers but no direct evidence about LLM pattern simulation effectiveness. The evidence is indirect through related work on unsupervised sentence embedding.
- Break condition: If LLM-generated patterns don't capture the essential statistical properties of STS/NLI data, or if the patterns introduce artifacts that harm generalization.

### Mechanism 3
- Claim: Hierarchical Triplet (HT) loss effectively utilizes hierarchical STS patterns by ensuring that intermediate sentences are more similar to source than negatives but less similar than positives.
- Mechanism: The HT loss creates a hierarchical structure where sp_i (positive) > sm_i (intermediate) > sn_i (negative) in similarity to source sentence, forcing the model to learn graded similarity rather than binary similar/dissimilar distinctions.
- Core assumption: The STS task's graded scoring (0-5) represents a true hierarchy in semantic similarity, and models can learn this hierarchy through appropriate loss functions.
- Evidence anchors:
  - [abstract] "We notice the hierarchical nature of the STS dataset, where the semantic similarity between two sentences is measured with a score ranging from 0 to 5, rather than simply classified as similar or dissimilar."
  - [section] "The HT loss is defined as LHT = 1/2 ( max(f(si)⊤f(sm_i) − f(si)⊤f(sp_i) + m1, 0) + max(f(si)⊤f(sn_i) − f(si)⊤f(sm_i) + m2, 0))"
  - [corpus] Weak: corpus contains related contrastive learning papers but no direct evidence about hierarchical triplet loss effectiveness. The evidence is indirect through general contrastive learning literature.
- Break condition: If the graded STS scores don't represent a true hierarchy in semantic space, or if the margin parameters m1/m2 are poorly chosen.

## Foundational Learning

- Concept: Contrastive Learning of Sentence Embeddings (CSE)
  - Why needed here: The entire paper builds on CSE as the baseline technique, comparing supervised vs unsupervised variants and proposing improvements.
  - Quick check question: What is the key difference between supervised and unsupervised CSE in terms of training data?

- Concept: Alignment and Uniformity Metrics
  - Why needed here: These metrics are used to quantify the quality of sentence embeddings and form the basis of the FDI metric.
  - Quick check question: How do alignment and uniformity change differently during supervised vs unsupervised CSE training?

- Concept: In-Context Learning (ICL)
  - Why needed here: ICL is the mechanism by which LLM generates training data that simulates complex patterns from STS/NLI datasets.
  - Quick check question: What is the key difference between ICL and standard fine-tuning approaches?

## Architecture Onboarding

- Component map: Sentence encoder (BERT/RoBERTa) -> LLM pattern generator (gpt-3.5-turbo-0613 with ICL) -> Contrastive learning with InfoNCE + HT loss -> STS evaluation pipeline
- Critical path: LLM-generated data → model training → embedding extraction → STS evaluation. The LLM-generated data flows through the sentence encoder with contrastive and HT losses, producing embeddings that are evaluated on STS tasks.
- Design tradeoffs: Using LLM-generated data increases training complexity but reduces dependency on human-annotated data. The HT loss adds computational overhead but captures hierarchical patterns. Larger amounts of LLM data improve performance but increase cost.
- Failure signatures: Poor STS performance despite high training accuracy suggests patterns aren't being captured correctly. Large gaps between training and evaluation performance indicate overfitting to LLM patterns. Unstable training suggests margin parameters in HT loss are poorly chosen.
- First 3 experiments:
  1. Compare FDI values and STS performance for supervised vs unsupervised baselines to validate the core correlation.
  2. Generate LLM data with different pattern types (STS vs NLI) and measure impact on FDI and STS performance.
  3. Vary the amount of LLM-generated data and HT loss parameters to find optimal configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FDI correlate with different types of evaluation datasets beyond STS tasks?
- Basis in paper: [explicit] The paper demonstrates FDI's correlation with STS task performance but does not explore its applicability to other evaluation metrics or tasks.
- Why unresolved: The study focuses solely on STS tasks for validating FDI's effectiveness, leaving its generalizability to other NLP tasks untested.
- What evidence would resolve it: Empirical experiments showing FDI's correlation with performance on tasks like sentiment analysis, named entity recognition, or machine translation.

### Open Question 2
- Question: What is the impact of varying the number of examples in the LLM prompts on the quality of generated patterns?
- Basis in paper: [inferred] The paper uses three examples per prompt but does not investigate how changing this number affects pattern quality or FDI.
- Why unresolved: The study does not systematically explore the relationship between prompt complexity and generated data quality.
- What evidence would resolve it: Experiments comparing FDI and model performance across different numbers of examples (e.g., 1, 3, 5, 10) in LLM prompts.

### Open Question 3
- Question: How does the Hierarchical Triplet (HT) loss perform compared to other hierarchical loss functions in leveraging STS patterns?
- Basis in paper: [explicit] The paper introduces HT loss as a novel approach but does not compare it to other hierarchical loss functions like triplet loss with dynamic margins or metric learning approaches.
- Why unresolved: The study focuses on HT loss's effectiveness without benchmarking it against alternative hierarchical methods.
- What evidence would resolve it: Comparative experiments evaluating HT loss against other hierarchical loss functions on STS and transfer tasks.

### Open Question 4
- Question: Can the LLM-generated data be further optimized to reduce redundancy while maintaining or improving performance?
- Basis in paper: [inferred] The paper generates 20,000 instances but does not explore whether fewer, more diverse examples could achieve similar or better results.
- Why unresolved: The study does not investigate data efficiency or redundancy in the LLM-generated dataset.
- What evidence would resolve it: Experiments testing model performance with varying dataset sizes and diversity metrics (e.g., uniqueness, coverage) of LLM-generated data.

## Limitations

- The FDI metric correlation with STS performance relies on a single experimental observation and does not prove causation
- LLM pattern simulation depends heavily on prompt engineering quality and may not capture essential STS/NLI patterns
- The HT loss introduces sensitive hyperparameters (m1, m2) that require empirical tuning without theoretical guidance

## Confidence

- High confidence in core experimental results: The reported Spearman correlation improvements are well-documented with clear methodology
- Medium confidence in FDI metric: While mathematically well-defined, the theoretical justification for why alignment/uniformity changes reflect fitting difficulty remains somewhat hand-wavy
- Medium confidence in LLM pattern simulation: The approach appears effective but depends heavily on LLM capabilities and prompt quality

## Next Checks

1. **Ablation on FDI metric**: Systematically manipulate training data difficulty (e.g., by adding noise or filtering easy examples) while keeping the base algorithm constant, then measure whether FDI changes predict STS performance changes as claimed.

2. **Pattern simulation quality analysis**: Conduct human evaluation of LLM-generated positive/negative pairs to verify they capture the same semantic similarity patterns as NLI data, not just surface-level similarities.

3. **HT loss parameter sensitivity**: Run experiments varying m1 and m2 across a wider range to determine if the reported improvements are robust to hyperparameter choice or if they depend on specific tuning.