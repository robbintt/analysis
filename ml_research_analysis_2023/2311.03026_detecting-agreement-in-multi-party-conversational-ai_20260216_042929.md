---
ver: rpa2
title: Detecting Agreement in Multi-party Conversational AI
arxiv_id: '2311.03026'
source_url: https://arxiv.org/abs/2311.03026
tags:
- system
- answer
- users
- multi-party
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multi-party conversational system that invites
  two users to play a trivia quiz game and detects their agreement on a final answer.
  The system uses a combination of a neural network and state machine for dialogue
  management, and OpenAI's API for natural language generation.
---

# Detecting Agreement in Multi-party Conversational AI

## Quick Facts
- arXiv ID: 2311.03026
- Source URL: https://arxiv.org/abs/2311.03026
- Reference count: 21
- Primary result: 80% agreement detection accuracy (excluding microphone issues) with mean Likert user satisfaction score of 3.81

## Executive Summary
This paper presents a multi-party conversational system that invites two users to play a trivia quiz game and detects their agreement on final answers. The system uses a hybrid dialogue management approach combining a neural network (LSTM) and state machine, with OpenAI's GPT-3.5-turbo for natural language generation. The authors collected and annotated data from "Who Wants to Be a Millionaire" episodes to train the system. Evaluation results show promising performance in agreement detection and user satisfaction, though the study was limited to 10 sessions with 3 users.

## Method Summary
The system implements a hybrid dialogue manager combining LSTM neural network learning from annotated TV show transcripts with a state machine for rule enforcement. Two separate microphones capture individual user speech, bypassing the need for complex diarization. The LSTM learns to predict system responses based on intent and user ID sequences from the training data, while the state machine handles game logic and overrides when the neural component lacks coverage. OpenAI's GPT-3.5-turbo generates 50 candidate responses per intent, which are manually filtered for safety and accuracy before deployment.

## Key Results
- Achieved 80% agreement detection accuracy when excluding microphone-related failures
- Mean user satisfaction score of 3.81 on Likert scale across 10 evaluation sessions
- Successfully detected agreement in 80% of cases where users agreed on the same answer
- Demonstrated feasibility of hybrid neural-state machine dialogue management for game scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agreement detection accuracy improves when speaker diarization is bypassed via separate microphones.
- Mechanism: By using two distinct microphones (one per user), the system eliminates the need for complex diarization algorithms that are error-prone in overlapping speech scenarios. This separation directly reduces false positives caused by cross-talk.
- Core assumption: Each microphone captures only one speaker with high signal-to-noise ratio.
- Evidence anchors:
  - [abstract] states "In order to reliably record both parties within the conversation, we bypass diarisation [14], [15] by using two separate microphones."
  - [section IV-A] describes the setup and acknowledges that microphones still pick up both voices but mitigate by distance.
- Break condition: If environmental noise or speaker proximity causes significant cross-talk, the assumption fails and accuracy drops.

### Mechanism 2
- Claim: The hybrid dialogue manager (neural network + state machine) handles both predictable game flow and conversational variability.
- Mechanism: The LSTM neural network learns patterns from annotated transcripts for response selection, while the state machine enforces game rules (e.g., one round = 10 questions) and overrides erroneous neural outputs. This ensures compliance with rigid rules and adaptability to user utterances.
- Core assumption: Transcript data is sufficient to train the neural component for meaningful generalization.
- Evidence anchors:
  - [section IV-B1] explains training with intent and user ID one-hot encoding and discusses dataset imbalance handling.
  - [section IV-B2] describes state machine extending actions when transcript data is limited.
- Break condition: If the annotated corpus lacks coverage of edge cases, the neural component may not generalize, and the state machine must handle more cases.

### Mechanism 3
- Claim: OpenAI's GPT-3.5-turbo generation ensures natural and contextually appropriate system utterances.
- Mechanism: For each possible intent, the system generates 50 unique candidate responses via GPT-3.5-turbo, which are manually filtered for hallucinations, offense, and accuracy before deployment. This leverages large-scale language model generalization while maintaining control quality.
- Core assumption: Manual vetting of generated utterances is scalable and effective.
- Evidence anchors:
  - [section IV-C] states "we used OpenAI’s API, specifically 'gpt-3.5-turbo', in order to generate 50 unique options for each possible response from the game ‘host’. Each generated response was checked manually for hallucinations, offence, and accuracy."
- Break condition: If manual vetting becomes a bottleneck or if GPT-3.5-turbo generates unsafe content that slips through, user satisfaction and safety degrade.

## Foundational Learning

- Concept: Speaker diarization
  - Why needed here: Diarization identifies who spoke when in multi-party conversations, crucial for accurate turn-taking and intent attribution.
  - Quick check question: What is the main challenge of speaker diarization in overlapping speech scenarios?

- Concept: Intent recognition in dialogue systems
  - Why needed here: The system must classify user utterances (e.g., "offer-answer", "agreement") to decide the appropriate response and detect agreement.
  - Quick check question: How does intent recognition differ between dyadic and multi-party dialogues?

- Concept: Sequence-to-sequence modeling with LSTM
  - Why needed here: The LSTM learns to map sequences of (intent, user ID) pairs to system responses, enabling context-aware dialogue management.
  - Quick check question: Why is an LSTM chosen over a feed-forward network for this dialogue management task?

## Architecture Onboarding

- Component map: ASR (Two mics) → NLU → Dialogue Manager (LSTM + State Machine) → NLG (GPT-3.5-turbo) → Web app
- Critical path: ASR → NLU → Dialogue Manager → NLG → Output
- Design tradeoffs:
  - Bypassing diarization simplifies implementation but requires careful mic placement; adding diarization would improve robustness in noisy settings.
  - Hybrid DM balances rule compliance and flexibility but increases code complexity; pure ML would reduce engineering overhead but risk rule violations.
  - Manual vetting of GPT responses ensures safety but limits scalability; automated filtering would speed up deployment but risk unsafe content.
- Failure signatures:
  - False positives in agreement detection → likely microphone cross-talk or poor ASR transcription
  - System fails to progress to next question → state machine override not triggered or DM stuck in loop
  - Inappropriate or repetitive responses → NLG generation or filtering issue
- First 3 experiments:
  1. Test microphone separation: Place two mics at varying distances, measure cross-talk and agreement detection accuracy.
  2. Stress-test state machine: Feed edge-case intent sequences (e.g., "confirm-final-answer" before any answer offered) and verify overrides.
  3. Evaluate NLG filtering: Generate 50 responses per intent, manually score for relevance and safety, compute precision/recall of manual vetting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the system's agreement detection accuracy compare when using a social robot designed for speech interaction versus the current laptop-based setup?
- Basis in paper: [explicit] The paper mentions that the microphone issue, which affects agreement detection accuracy, could be mitigated by using social robots designed for speech interaction.
- Why unresolved: The current evaluation was conducted using laptop microphones, and the authors plan to implement the system on an ARI social robot in future work.
- What evidence would resolve it: Conducting a comparative study between the laptop-based setup and the social robot setup, measuring agreement detection accuracy in both scenarios.

### Open Question 2
- Question: What is the impact of the state machine's additional intents on the system's ability to handle unexpected user responses or conversation paths?
- Basis in paper: [explicit] The paper describes the use of a state machine to extend the system's actions beyond what was learned from the transcript data, adding ten additional intents.
- Why unresolved: While the paper mentions the state machine's role in handling logical errors, it does not provide detailed analysis of its effectiveness in handling unexpected scenarios.
- What evidence would resolve it: Evaluating the system's performance in handling unexpected user responses or conversation paths, comparing the state machine's effectiveness against a purely neural network-based approach.

### Open Question 3
- Question: How does the system's performance vary with different numbers of participants in the multi-party setting?
- Basis in paper: [inferred] The current system is designed for two participants, but the paper does not explore its performance with more or fewer participants.
- Why unresolved: The paper focuses on a two-party interaction and does not provide data or analysis for other group sizes.
- What evidence would resolve it: Testing the system with varying numbers of participants (e.g., one, three, or four) and measuring its performance metrics, such as agreement detection accuracy and user satisfaction, in each scenario.

## Limitations

- Small evaluation sample size (N=10) with only 3 users limits generalizability
- Agreement detection accuracy figure excludes microphone-related failures, suggesting environmental factors may significantly impact real-world performance
- Manual vetting process for GPT-generated responses presents scalability concerns for deployment

## Confidence

- Agreement detection accuracy (80%): Medium - results are promising but based on a small sample and exclude microphone-related failures
- User satisfaction (mean 3.81): Medium - limited sample size and self-reported metrics reduce reliability
- Microphone separation effectiveness: High - well-documented in the paper with clear experimental setup
- Hybrid dialogue manager performance: Medium - theoretical framework is sound but empirical validation is limited
- GPT-3.5-turbo generation quality: Medium - manual vetting ensures safety but scalability concerns exist

## Next Checks

1. Scale evaluation to 50+ participants across diverse environments to test microphone separation robustness and agreement detection accuracy under varying conditions

2. Implement automated filtering for GPT-generated responses and compare safety/precision metrics against manual vetting to assess scalability

3. Conduct A/B testing comparing the hybrid dialogue manager against pure ML and pure rule-based approaches to quantify the benefits of the hybrid architecture