---
ver: rpa2
title: Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication
  Cost under 18 Kilobytes
arxiv_id: '2312.06353'
source_url: https://arxiv.org/abs/2312.06353
tags:
- tuning
- federated
- fedkseed
- gradient
- communication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FedKSeed, a method for federated fine-tuning
  of billion-sized language models using zeroth-order optimization. The key innovation
  is the reuse of a finite set of random seeds to generate perturbations, enabling
  communication-efficient full-parameter tuning.
---

# Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes

## Quick Facts
- **arXiv ID**: 2312.06353
- **Source URL**: https://arxiv.org/abs/2312.06353
- **Reference count**: 12
- **Key outcome**: Achieves full-parameter tuning of billion-sized language models with communication under 18 KB per round using zeroth-order optimization and seed reuse

## Executive Summary
This paper introduces FedKSeed, a communication-efficient federated learning method for full-parameter tuning of billion-sized language models. The key innovation is reusing a finite set of random seeds to generate perturbations for zeroth-order optimization, reducing communication from gigabytes to under 18 KB per round. An enhanced version, FedKSeed-Pro, further improves efficiency by sampling seeds with non-uniform probabilities based on estimated importance. Experiments show significant improvements in accuracy (7.26% relative Rouge-L improvement) and communication efficiency (1000x reduction) compared to existing methods.

## Method Summary
FedKSeed uses zeroth-order optimization with two-point gradient estimation to enable full-parameter tuning without backpropagation. A finite set of K random seeds generates perturbations for gradient estimation, and scalar gradients are accumulated per seed rather than per parameter. The server transmits only K seed IDs and scalar gradients each round, drastically reducing communication. FedKSeed-Pro enhances this by sampling seeds with probabilities proportional to their estimated importance based on scalar gradient amplitudes. Both methods maintain memory usage at inference levels and require only forward passes for gradient estimation.

## Key Results
- Achieves 7.26% relative improvement in Rouge-L score compared to baselines
- Reduces communication costs by over 1000x (under 18 KB per round vs gigabytes)
- Maintains memory consumption at inference level without backpropagation
- Validated across 6 scenarios with different LLMs, datasets, and data partitions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Reuse of a finite set of random seeds enables full-parameter tuning with communication under 18 KB per round.
- **Mechanism**: Zeroth-order optimization updates are generated by scalar gradients multiplied by perturbations from random seeds. By limiting to K seeds and accumulating gradients per seed, the server only needs to transmit K seed IDs and K scalar gradients, reducing communication from O(d) to O(K).
- **Core assumption**: The diversity of perturbations is not relevant to convergence; a finite set of seeds can approximate the gradient well enough for effective tuning.
- **Evidence anchors**:
  - [abstract] "transmits only a few random seeds and scalar gradients, amounting to only a few thousand bytes"
  - [section 4.2] "ZOO-based FL can be effectively implemented with a finite set of seeds to generate perturbations"
  - [corpus] Weak evidence: no direct corpus citations support this specific claim about K seed reuse.
- **Break condition**: If K is too small (< ~1024 empirically), accuracy degrades due to insufficient perturbation diversity; if too large (> ~4096), computational overhead increases without accuracy gain.

### Mechanism 2
- **Claim**: Non-uniform seed sampling based on scalar gradient amplitude improves accuracy and synchronization speed.
- **Mechanism**: Seeds are sampled with probabilities proportional to the exponential of their normalized average scalar gradient amplitude. More important perturbations are updated more frequently, improving model convergence.
- **Core assumption**: The average amplitude of scalar gradients for a seed correlates with the seed's importance to model convergence.
- **Evidence anchors**:
  - [section 4.3] "average amplitude of scalar gradient ψj can characterize the importance of zj"
  - [section 5.2] "probabilities of each seed being sampled exhibit differences of several multiples" and FedKSeed-Pro achieves higher accuracy
  - [corpus] No direct corpus citations support the specific non-uniform sampling claim.
- **Break condition**: If the amplitude metric poorly correlates with importance, sampling may bias updates incorrectly, harming convergence.

### Mechanism 3
- **Claim**: Zeroth-order optimization with two-point gradient estimator is memory-efficient and suitable for full-parameter tuning on devices.
- **Mechanism**: Two forward passes per update estimate gradients without backpropagation, keeping memory usage low (inference-level) and avoiding large backward pass overhead.
- **Core assumption**: Two-point gradient estimator has lower variance than one-point and provides sufficient accuracy for LLM fine-tuning.
- **Evidence anchors**:
  - [section 4.1] "two-point gradient estimator proposed by Malladi et al. (2023)"
  - [section 5.3] "memory consumption to inference level" and "memory efficiency is attributed to the removal of BP"
  - [corpus] Weak evidence: no corpus citations directly support two-point estimator choice.
- **Break condition**: If gradient estimation variance is too high, convergence slows or fails; if memory constraints relaxed, BP-based methods might outperform.

## Foundational Learning

- **Concept**: Zeroth-order optimization (ZOO) gradient estimation
  - Why needed here: Enables gradient-based updates without backpropagation, critical for memory-constrained devices during full-parameter tuning.
  - Quick check question: How does a two-point ZOO estimator approximate the gradient using only forward passes?

- **Concept**: Federated learning with partial client participation
  - Why needed here: Only a subset of clients participate per round; methods must handle gradient accumulation and model synchronization efficiently.
  - Quick check question: What happens to the latest model calculation if a client skips several rounds under seed reuse?

- **Concept**: Parameter-efficient fine-tuning (PEFT) vs full-parameter tuning tradeoffs
  - Why needed here: PEFT reduces communication and memory but may limit accuracy; full-parameter tuning can achieve higher accuracy but is expensive without optimizations like FedKSeed.
  - Quick check question: Under what data distribution conditions does PEFT underperform full-parameter tuning?

## Architecture Onboarding

- **Component map**: Server maintains K seeds and accumulator → Clients maintain local model and scalar gradient history → Server aggregates gradients and updates seed probabilities
- **Critical path**:
  1. Server sends K seeds + accumulator to active clients
  2. Each client reconstructs latest global model via Equation (4)
  3. Clients sample seeds, compute scalar gradients, update local model
  4. Clients return scalar gradient history to server
  5. Server aggregates gradients into accumulator
  6. Server updates seed probabilities (FedKSeed-Pro)
- **Design tradeoffs**:
  - K too small → insufficient perturbation diversity → accuracy loss
  - K too large → longer local model reconstruction → slower synchronization
  - Non-uniform sampling → better accuracy but requires seed importance tracking
  - Two-point ZOO → memory efficient but higher variance than BP
- **Failure signatures**:
  - Accuracy stalls or degrades → K misconfigured or seed importance metric poor
  - Memory spikes → BP accidentally triggered or accumulator mis-sized
  - Communication overhead large → seeds not encoded efficiently or K too large
  - Clients slow to sync → excessive K or poor network conditions
- **First 3 experiments**:
  1. Verify convergence with K=1024, 2048, 4096 on small dataset; plot accuracy vs K
  2. Test non-uniform sampling: compare FedKSeed vs FedKSeed-Pro accuracy and sync time
  3. Measure memory usage vs BP baseline on same model size and batch size

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Limited empirical validation across diverse data distributions and non-IID scenarios
- Seed importance metric assumptions may not generalize to all model architectures
- Implementation complexity requires careful management of seed pools and gradient accumulators

## Confidence
- **High confidence**: Communication cost reduction claims (under 18 KB per round) - well-supported by mathematical framework and experimental results
- **Medium confidence**: Accuracy improvement claims (7.26% relative Rouge-L improvement) - supported by experiments but limited to specific models and datasets
- **Medium confidence**: Memory efficiency claims - reasonable given ZOO-based approach but lacks comprehensive BP comparison

## Next Checks
1. Cross-dataset generalization test: Evaluate FedKSeed on additional NLP tasks (e.g., summarization, question answering) with varying data distributions
2. Seed pool sensitivity analysis: Systematically vary K (256 to 8192) and measure accuracy degradation/gain, particularly for models >3B parameters
3. Non-IID data stress test: Create extreme non-IID scenarios where clients have disjoint label sets and evaluate seed importance metric effectiveness