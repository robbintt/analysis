---
ver: rpa2
title: Multi-dimensional data refining strategy for effective fine-tuning LLMs
arxiv_id: '2311.01049'
source_url: https://arxiv.org/abs/2311.01049
tags:
- data
- language
- vietnamese
- fine-tuning
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of acquiring suitable data for
  fine-tuning large language models (LLMs) for Vietnamese, a tonal language with unique
  linguistic characteristics. The authors propose a multi-dimensional strategy that
  includes leveraging existing English datasets and translating them into Vietnamese,
  as well as developing AI-assisted data crawling scripts to collect Vietnamese news
  articles from online sources.
---

# Multi-dimensional data refining strategy for effective fine-tuning LLMs

## Quick Facts
- arXiv ID: 2311.01049
- Source URL: https://arxiv.org/abs/2311.01049
- Reference count: 0
- One-line primary result: Fine-tuned Vietnamese BLOOM model (VN-BLOOM7B1-NEWS) achieves good performance generating Vietnamese news articles from prompts

## Executive Summary
This paper addresses the challenge of acquiring suitable data for fine-tuning large language models (LLMs) for Vietnamese, a tonal language with unique linguistic characteristics. The authors propose a multi-dimensional strategy that includes leveraging existing English datasets and translating them into Vietnamese, as well as developing AI-assisted data crawling scripts to collect Vietnamese news articles from online sources. The translated Alpaca 52k dataset and the crawled news articles were used to fine-tune a Vietnamese BLOOM model, resulting in VN-BLOOM7B1-NEWS. The model demonstrated good performance in generating Vietnamese news articles from prompts, as validated by human volunteers. The study offers practical solutions and guidance for future fine-tuning models in languages like Vietnamese.

## Method Summary
The authors employed a two-stage fine-tuning approach using BLOOM 7B1 as the base model. First, they translated the English Alpaca 52k dataset into Vietnamese (33k instructions retained), using a Python program to split and translate the dataset, followed by human curation. Second, they developed AI-assisted Python scripts (with ChatGPT) to crawl 17k Vietnamese news articles from VnExpress.net. The fine-tuning was performed using QLoRA with 4-bit quantization on Runpod cloud GPU service, with 5,000 steps per round at learning rate 2e-4 using 8-bit Adam optimizer.

## Key Results
- Successfully fine-tuned VN-BLOOM7B1-NEWS model for Vietnamese news generation
- Model demonstrated good performance generating Vietnamese news articles from prompts
- Validated through blind human evaluation focusing on writing styles and content quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating existing English datasets into Vietnamese provides a fast way to bootstrap high-quality fine-tuning data.
- Mechanism: Large, well-curated English datasets (e.g., Alpaca 52k) are segmented, machine-translated in bulk, and then human-curated to fix structural losses.
- Core assumption: Machine translation preserves instruction structure well enough that targeted human curation can recover accuracy without re-annotating from scratch.
- Evidence anchors:
  - [abstract] "leveraging existing datasets in the English language and developing customized data-crawling scripts with the assistance of generative AI tools"
  - [section] "Translating the dataset whose size is 40MB was a challenge itself... We developed a Python program to split the original file into 100 smaller files and then fed them into Microsoft Word for translation. A human volunteer helped with the process."
- Break condition: If translation tool quality degrades sharply or human curation time exceeds annotation time, the strategy loses its efficiency edge.

### Mechanism 2
- Claim: AI-assisted scripting (e.g., ChatGPT) enables rapid adaptation of data crawling tools to new website architectures.
- Mechanism: ChatGPT generates and iteratively refines Python scripts for crawling structured content (e.g., VnExpress), handling layout variations without manual reverse-engineering.
- Core assumption: The AI can produce syntactically correct, functionally relevant code for each site's HTML structure when given clear constraints.
- Evidence anchors:
  - [section] "We developed a Python program for data crawling with the assistance of ChatGPT... we were able to fine-tune the data crawling process, molding it to match the intricacies of our target sources."
- Break condition: If website access policies (rate limits, robots.txt) block crawling or if the AI cannot adapt to novel DOM structures, the approach fails.

### Mechanism 3
- Claim: Fine-tuning a multilingual base model (BLOOM 7B1) with both translated English data and native Vietnamese news data yields coherent, human-like Vietnamese text generation.
- Mechanism: Two-stage fine-tuning: first on translated Alpaca 52k, then on crawled Vietnamese news, using QLoRA to fit within limited GPU memory.
- Core assumption: BLOOM's multilingual pretraining provides sufficient cross-lingual transfer that fine-tuning on Vietnamese data improves coherence and cultural nuance.
- Evidence anchors:
  - [abstract] "A fine-tuned LLM model for the Vietnamese language, which was produced using resultant datasets, demonstrated good performance while generating Vietnamese news articles from prompts."
  - [section] "The generated articles were then evaluated blindly by human volunteers focusing on writing styles, and general contents against similar types of articles from newspaper sites."
- Break condition: If the model overfits to the fine-tuning data or the evaluation criteria are too subjective, performance gains may not generalize.

## Foundational Learning

- Concept: Cross-lingual transfer in LLMs
  - Why needed here: Understanding how a multilingual base model like BLOOM can be adapted to Vietnamese without full retraining.
  - Quick check question: Why does BLOOM work for Vietnamese fine-tuning despite being trained on multilingual data?

- Concept: Data augmentation and translation pipelines
  - Why needed here: Translating large English datasets into Vietnamese efficiently without losing instruction semantics.
  - Quick check question: What steps are taken to recover information lost during bulk machine translation?

- Concept: Evaluation metrics for generative models in low-resource languages
  - Why needed here: BLEU/ROUGE may not capture Vietnamese nuances; manual evaluation is used instead.
  - Quick check question: Why are traditional automated metrics insufficient for evaluating Vietnamese text generation?

## Architecture Onboarding

- Component map: Base multilingual LLM (BLOOM 7B1) → Fine-tuning pipeline (QLoRA, 4-bit quantization) → Dataset A (translated Alpaca 52k) → Dataset B (crawled Vietnamese news) → Evaluation (human blind test)
- Critical path: Data collection → Translation/curation → Fine-tuning (two rounds) → Model validation
- Design tradeoffs: Translation + curation vs. native annotation (cost vs. quality); AI-assisted crawling vs. manual scraping (speed vs. reliability)
- Failure signatures: Loss of instruction structure after translation; incomplete or biased crawled datasets; overfitting to training prompts
- First 3 experiments:
  1. Run a small-scale translation of Alpaca samples and evaluate structure preservation.
  2. Test ChatGPT-generated crawler on a subset of VnExpress articles for data extraction quality.
  3. Fine-tune BLOOM on a tiny combined dataset and evaluate output coherence manually.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are the proposed multi-dimensional data refining strategies in improving the performance of fine-tuned Vietnamese language models compared to traditional methods?
- Basis in paper: [explicit] The paper mentions that the fine-tuned VN-BLOOM7B1-NEWS model demonstrated good performance in generating Vietnamese news articles from prompts, as validated by human volunteers.
- Why unresolved: The paper does not provide a detailed comparison of the proposed strategies with traditional methods in terms of model performance metrics or qualitative assessments.
- What evidence would resolve it: Conducting a comprehensive comparison study with traditional methods using standard evaluation metrics (e.g., BLEU, ROUGE) and qualitative assessments to measure the effectiveness of the proposed strategies in improving model performance.

### Open Question 2
- Question: What are the potential limitations and challenges of using AI-assisted scripts for data crawling in languages with complex linguistic structures, such as Vietnamese?
- Basis in paper: [inferred] The paper mentions that data crawling from websites is challenging due to the inherent variability in website structures and access limitations, and that AI tools like ChatGPT were used to develop data crawling scripts for Vietnamese news websites.
- Why unresolved: The paper does not provide a detailed analysis of the limitations and challenges faced when using AI-assisted scripts for data crawling in languages with complex linguistic structures.
- What evidence would resolve it: Conducting a study to identify and analyze the specific limitations and challenges of using AI-assisted scripts for data crawling in languages with complex linguistic structures, and proposing solutions to address these issues.

### Open Question 3
- Question: How can the proposed multi-dimensional data refining strategy be adapted and applied to other low-resource languages with unique linguistic characteristics, such as tonal languages or languages with complex diacritics?
- Basis in paper: [explicit] The paper mentions that the study offers practical solutions and guidance for future fine-tuning models in languages like Vietnamese, which is a tonal language with unique linguistic characteristics.
- Why unresolved: The paper does not provide a detailed discussion on how the proposed strategy can be adapted and applied to other low-resource languages with unique linguistic characteristics.
- What evidence would resolve it: Conducting a study to explore the adaptability and applicability of the proposed strategy to other low-resource languages with unique linguistic characteristics, and providing guidelines for implementing the strategy in these languages.

## Limitations

- Translation quality not rigorously evaluated - human curation mentioned but no quantitative metrics on information loss
- Evaluation methodology lacks standardized metrics and inter-annotator agreement scores
- Crawling strategy vulnerable to website changes and may face legal/ethical challenges

## Confidence

- **High Confidence**: The feasibility of using BLOOM 7B1 as a base model for Vietnamese fine-tuning
- **Medium Confidence**: The effectiveness of the two-stage fine-tuning approach in improving Vietnamese generation quality
- **Low Confidence**: The scalability and replicability of the AI-assisted data crawling approach across different Vietnamese websites

## Next Checks

1. **Translation Quality Assessment**: Conduct a detailed comparison between English Alpaca instructions and their Vietnamese translations using established metrics like semantic similarity scores and instruction preservation rates to quantify information loss during translation.

2. **Automated Evaluation Framework**: Implement a comprehensive automated evaluation pipeline using Vietnamese-specific metrics (lexical diversity, grammatical correctness, cultural appropriateness) alongside the human evaluation to provide more objective performance measures.

3. **Generalization Testing**: Evaluate the fine-tuned VN-BLOOM7B1-NEWS model on held-out Vietnamese news domains not present in the training data to assess whether the model has generalized beyond its training distribution or simply memorized specific patterns.