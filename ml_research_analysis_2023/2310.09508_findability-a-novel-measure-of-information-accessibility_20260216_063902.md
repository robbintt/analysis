---
ver: rpa2
title: 'Findability: A Novel Measure of Information Accessibility'
arxiv_id: '2310.09508'
source_url: https://arxiv.org/abs/2310.09508
tags:
- findability
- document
- retrieval
- documents
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a new metric called \u201Cfindability\u201D\
  \ for quantifying document accessibility in information retrieval systems. The metric\
  \ measures the ease with which users can locate relevant documents for their queries."
---

# Findability: A Novel Measure of Information Accessibility

## Quick Facts
- arXiv ID: 2310.09508
- Source URL: https://arxiv.org/abs/2310.09508
- Reference count: 30
- Primary result: Introduces findability metric showing distinct query-aware accessibility measurement compared to retrievability, with experimental evidence that larger collections increase findability bias.

## Executive Summary
This paper proposes findability as a novel metric for quantifying document accessibility in information retrieval systems. Unlike existing retrievability measures that are query-agnostic, findability captures the expected likelihood of a document being found across all relevant queries, weighted by a convenience function modeling user click behavior. Experiments on three standard IR collections demonstrate that findability is orthogonal to retrievability and that different retrieval models exhibit varying levels of findability and bias, with larger collections showing decreased mean findability and increased bias due to heightened competition among documents.

## Method Summary
The findability metric is computed by generating known-item queries for each document using a popular+discriminative selection strategy that captures approximately 10% of distinct terms per document. Each query is then processed through the retrieval model to obtain ranked lists, and the convenience function (inverse rank-based with threshold c=100) is applied to calculate accessibility scores. The mean findability and Gini coefficient (findability bias) are then aggregated across all documents to provide overall system effectiveness and disparity metrics. The approach requires three standard IR collections (TREC Robust, WT10g, MS MARCO passage) and does not require manual relevance judgments for query generation.

## Key Results
- Findability and retrievability measures show near-zero correlation, confirming their orthogonality
- As collection size increases, mean findability decreases and findability bias (Gini coefficient) increases
- Different retrieval models exhibit varying levels of findability and findability bias
- The metric successfully quantifies document accessibility without requiring manual labor

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Findability provides a query-aware document accessibility metric, unlike retrievability which is query-agnostic.
- Mechanism: The measure computes the expected likelihood of finding a document across all queries for which the document is relevant, weighted by a convenience function that models user click behavior.
- Core assumption: Each document has a well-defined set of relevant queries, and these can be sampled or approximated without manual labeling.
- Evidence anchors:
  - [abstract] "findability of a document within a given IR system without resorting to manual labor."
  - [section] "the retrievability measure provides only a general approximation of the likelihood of a document being retrieved, regardless of which query is posed."
  - [corpus] No direct corpus evidence, but corpus titles reference accessibility and retrievability metrics.
- Break condition: If relevant query sets cannot be generated or approximated reliably, the findability measure becomes undefined or noisy.

### Mechanism 2
- Claim: Findability scores are independent of retrievability scores, as confirmed by near-zero correlation in experiments.
- Mechanism: By using known-item query generation and an inverse rank-based convenience function, findability captures user intent that retrievability does not, leading to orthogonal measurement.
- Core assumption: The known-item query generation method produces queries representative of real user behavior for known-item search tasks.
- Evidence anchors:
  - [section] "the obtained correlations reveal an almost negligible association between the two measures."
  - [section] "findability measure is well-suited for analyzing the findability of individual documents, whereas a single retrievability score alone may not adequately represent retrievability of a document."
  - [corpus] Related works reference similar bias and fairness metrics in retrieval.
- Break condition: If the known-item query set is unrepresentative, the independence assumption may not hold.

### Mechanism 3
- Claim: Larger collections inherently increase findability bias due to higher competition among documents.
- Mechanism: As the number of documents grows, the probability that any single document ranks highly for its relevant queries decreases, increasing the Gini coefficient.
- Core assumption: Retrieval model rank distributions follow the same decay patterns observed in CTR data.
- Evidence anchors:
  - [section] "as the Gini coefficient increases, there is a noticeable decrease in mean findability...larger number of documents results in heightened competition among them for higher rankings."
  - [section] "mean findability decreases and findability bias increases with collection size."
  - [corpus] No direct corpus evidence; assumption based on experimental observation.
- Break condition: If the collection structure or query distribution changes (e.g., fewer documents per topic), the competition effect may not hold.

## Foundational Learning

- Concept: Query-document relevance sets
  - Why needed here: Findability is defined over queries for which a document is relevant, so understanding how to model or approximate these sets is foundational.
  - Quick check question: How would you generate a relevant query set for a document without manual labeling?

- Concept: Click-through rate modeling and user behavior in search
  - Why needed here: The convenience function is derived from CTR observations, so understanding CTR distributions is critical to interpreting findability.
  - Quick check question: What does a steep drop in CTR after rank 10 imply for the convenience function?

- Concept: Gini coefficient as an inequality measure
  - Why needed here: Findability bias is quantified using the Gini coefficient, so understanding how it measures disparity is essential.
  - Quick check question: What does a Gini coefficient of 0.5 mean in terms of document findability distribution?

## Architecture Onboarding

- Component map: Query generation module -> Relevance query set construction -> Retrieval model execution -> Ranked list generation -> Convenience function -> Findability scorer -> Bias analyzer

- Critical path:
  1. Generate known-item queries for each document
  2. Run each query through the retrieval model
  3. Collect rank positions for target documents
  4. Apply convenience function to each rank
  5. Compute mean and Gini-based metrics

- Design tradeoffs:
  - Tradeoff between computational cost and query set size: larger sets give better estimates but are more expensive.
  - Tradeoff between convenience function fidelity and simplicity: more complex CTR models may be more accurate but harder to interpret.
  - Tradeoff between collection generality and task specificity: known-item queries are specific, while other query sets may be broader.

- Failure signatures:
  - Mean findability near zero: likely poor retrieval performance or incomplete query coverage.
  - Gini coefficient near one: retrieval model is highly biased, possibly dominated by a few documents.
  - High variance in findability scores: uneven distribution of relevant queries or retrieval instability.

- First 3 experiments:
  1. Run findability analysis on a small toy collection with known queries to verify computation logic.
  2. Compare findability and retrievability scores on the same query sets to validate independence.
  3. Vary the convenience function (exponential vs. inverse) and observe impact on findability scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the findability measure perform on specialized domains or non-standard document collections?
- Basis in paper: [explicit] The paper evaluates findability on three standard benchmark collections (news, web, web excerpts) but does not test specialized domains.
- Why unresolved: The experiments focus on general-purpose collections, leaving domain-specific performance unknown.
- What evidence would resolve it: Conducting findability experiments on domain-specific collections (medical, legal, technical) and comparing results with standard collections.

### Open Question 2
- Question: What is the optimal value of the rank threshold parameter c for different user populations and tasks?
- Basis in paper: [explicit] The paper acknowledges that c=100 is chosen for the study but notes that optimal values may vary by task and user preferences.
- Why unresolved: The study uses a fixed threshold without exploring how different values affect findability scores across user contexts.
- What evidence would resolve it: User studies testing multiple c values across different user groups, tasks, and information needs to determine optimal thresholds.

### Open Question 3
- Question: How does the findability measure correlate with actual user satisfaction and search success?
- Basis in paper: [inferred] The paper establishes findability as a theoretical metric but does not validate it against user satisfaction metrics or search success rates.
- Why unresolved: The study focuses on computational metrics without empirical validation through user studies.
- What evidence would resolve it: Controlled user studies measuring the correlation between findability scores and user-reported satisfaction, task completion rates, or search success metrics.

## Limitations

- The findability measure relies on known-item query generation without manual labeling, which may introduce approximation errors
- The inverse rank-based convenience function assumes user behavior patterns that may not generalize across all information needs
- The observed relationship between collection size and findability bias is based on experimental observation rather than theoretical proof

## Confidence

- **High confidence**: The orthogonality between findability and retrievability measures is well-supported by experimental correlation results
- **Medium confidence**: The claim that larger collections increase findability bias relies on observed patterns that may be context-dependent
- **Low confidence**: Without implementation details of the query generation strategy, exact reproduction is challenging

## Next Checks

1. **Query generation verification**: Implement and validate the popular+discriminative selection strategy on a small dataset, ensuring that generated queries capture the intended 10% of distinct terms per document while maintaining relevance to the target document.

2. **Convenience function sensitivity analysis**: Test alternative convenience functions (e.g., exponential decay, linear decay) and compare resulting findability distributions to assess robustness of findings to user behavior modeling assumptions.

3. **Cross-collection validation**: Apply the findability measure to a fourth, structurally different IR collection (e.g., ClueWeb or GOV2) to test whether the observed relationship between collection size and findability bias holds across diverse datasets.