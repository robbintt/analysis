---
ver: rpa2
title: AnglE-optimized Text Embeddings
arxiv_id: '2309.12871'
source_url: https://arxiv.org/abs/2309.12871
tags:
- angle
- text
- association
- cosine
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AnglE, a text embedding model designed to
  address the vanishing gradient problem caused by the cosine function's saturation
  zones in existing models. The core idea of AnglE is to optimize angles in a complex
  space, which mitigates the negative impact of cosine saturation zones on the learning
  process.
---

# AnglE-optimized Text Embeddings

## Quick Facts
- arXiv ID: 2309.12871
- Source URL: https://arxiv.org/abs/2309.12871
- Authors: 
- Reference count: 17
- Primary result: AnglE achieves 73.55% average Spearman correlation on non-transfer STS tasks, outperforming SBERT's 68.03%

## Executive Summary
AnglE introduces a novel text embedding model that addresses the vanishing gradient problem caused by cosine saturation zones through angle optimization in complex space. By decomposing embeddings into real and imaginary parts and optimizing normalized angle differences, AnglE mitigates the negative impact of cosine saturation on the learning process. The model combines cosine, in-batch negative, and angle objectives, demonstrating superior performance across short-text, long-text, and domain-specific semantic textual similarity tasks.

## Method Summary
AnglE uses a pre-trained BERT base model as backbone and introduces angle optimization in complex space to address cosine saturation zones. The model divides text embeddings into real and imaginary components, computes normalized angle differences between pairs, and optimizes these angles along with cosine similarity and in-batch negative samples. Training utilizes supervised STS datasets including MRPC, QQP, QNLI, STS 2012-2016, SICK-R, and STS-B. A new long-text dataset (GitHub Issues Similarity Dataset) is introduced. The model is evaluated on both transfer and non-transfer STS tasks using Spearman correlation as the primary metric.

## Key Results
- AnglE achieves 73.55% average Spearman correlation on non-transfer STS tasks versus SBERT's 68.03%
- Superior performance across short-text, long-text, and domain-specific STS benchmarks
- LLM-supervised learning method enhances applicability in low-resource domains
- Shows better generalization with in-batch negative sampling compared to unsupervised approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Angle optimization in complex space reduces vanishing gradients from cosine saturation zones
- Mechanism: Decomposes embeddings into real/imaginary parts, computes normalized angle differences, and optimizes angles instead of raw cosine similarity
- Core assumption: Normalized angle differences remain sensitive to similarity changes even when cosine saturates
- Evidence anchors:
  - [abstract] "optimize angles in a complex space, which mitigates the negative impact of cosine saturation zones"
  - [section 3.4] "If the normalized angle difference between two text embeddings is smaller, it means that the two text embeddings are closer to each other in the complex space"
- Break condition: If angle differences become equally saturated or normalization becomes unstable

### Mechanism 2
- Claim: Combined cosine, in-batch negative, and angle objectives yield higher STS performance
- Mechanism: Cosine pulls positive pairs closer, in-batch negatives strengthen generalization, angle mitigates saturation zones
- Core assumption: Different objectives capture complementary aspects without canceling each other
- Evidence anchors:
  - [abstract] "AnglE outperforms the state-of-the-art (SOTA) STS models that ignore the cosine saturation zone"
  - [section 3] Describes Lcos, Libn, and Langle objectives and their combination
- Break condition: If one objective dominates or weighting hyperparameters are poorly tuned

### Mechanism 3
- Claim: LLM-supervised learning compensates for limited domain-specific labeled data
- Mechanism: Uses LLMs as annotators to generate pseudo-supervised data for low-resource domains
- Core assumption: LLMs can generate high-quality positive and negative pairs reflecting domain semantics
- Evidence anchors:
  - [abstract] "propose an LLM-supervised learning method to address the scarcity of domain-supervised data"
  - [section 4.5] Details LLM-supervised learning procedure and shows better performance than unsupervised baselines
- Break condition: If LLMs generate noisy/bias labels or pseudo-data distribution diverges from real domain data

## Foundational Learning

- Concept: Complex number division and polar coordinate representation
  - Why needed here: To compute angle differences between embeddings while avoiding cosine saturation
  - Quick check question: Given z = 3 + 4i and w = 1 + 0i, what is the angle difference between z and w in radians?

- Concept: Contrastive learning objectives (cosine similarity, in-batch negatives)
  - Why needed here: To understand how positive pairs are pulled together and negatives pushed apart
  - Quick check question: In a batch of size 8, how many negative pairs are implicitly used for each anchor under in-batch negative sampling?

- Concept: Semantic Textual Similarity (STS) benchmarks and evaluation metrics
  - Why needed here: To interpret performance gains on standard STS datasets and understand Spearman correlation significance
  - Quick check question: If two sentences have ground truth similarity of 4.5/5, what Spearman correlation value indicates perfect ranking alignment for this pair?

## Architecture Onboarding

- Component map: Input layer (padded word embeddings → encoder) → Cosine objective → In-batch negative objective → Angle objective → Combined loss → Optimized embeddings
- Critical path: Encoder → Angle decomposition → Normalized angle difference computation → Loss aggregation → Backpropagation
- Design tradeoffs: Angle optimization adds computational overhead but mitigates saturation; in-batch negatives improve generalization but require duplicate detection; LLM-supervised learning expands applicability but depends on LLM quality
- Failure signatures: Gradient vanishing near saturation zones, poor STS correlation on long texts, unstable training when angle differences become numerically small
- First 3 experiments:
  1. Train AnglE with only cosine objective and evaluate on STS-B to confirm baseline performance
  2. Add angle objective alone and compare Spearman correlation to identify angle optimization contribution
  3. Enable in-batch negative sampling with duplicate detection and measure impact on generalization across datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal batch size for AnglE when using larger GPU memory like NVIDIA A100?
- Basis in paper: [inferred] Limited by GPU memory to batch size 50 in main experiment; suggests larger batch sizes yield better results with room for improvement
- Why unresolved: Experiments constrained by available GPU memory, preventing exploration of larger batch sizes
- What evidence would resolve it: Experiments with larger batch sizes using NVIDIA A100 and comparing AnglE performance across different batch sizes

### Open Question 2
- Question: How does AnglE performance compare to other SOTA models in real-world scenarios beyond STS tasks?
- Basis in paper: [inferred] Focuses on STS and text retrieval tasks without exploring other real-world applications
- Why unresolved: Paper concentrates on semantic textual similarity tasks, leaving other potential applications unexplored
- What evidence would resolve it: Experiments evaluating AnglE in sentiment analysis, named entity recognition, and question answering, comparing results with other SOTA models

### Open Question 3
- Question: How does angle optimization technique compare to other methods for addressing cosine saturation zones?
- Basis in paper: [explicit] Introduces angle optimization as novel approach but doesn't compare with other existing methods
- Why unresolved: Paper lacks comparative analysis with other techniques for addressing saturation zones
- What evidence would resolve it: Experiments comparing AnglE with models using different techniques for addressing cosine saturation zones (normalization techniques, alternative similarity measures)

## Limitations

- Implementation details for complex space division and angle computation are underspecified, potentially limiting reproducibility
- Computational overhead of angle optimization is mentioned but not quantified, raising scalability concerns
- LLM-supervised learning approach depends heavily on LLM quality and generation parameters, affecting reliability of pseudo-labeled data

## Confidence

- **High Confidence**: Empirical performance gains on standard STS benchmarks (STS-B, MRPC, QQP, etc.) with clear metrics and statistical significance
- **Medium Confidence**: Theoretical mechanism by which angle optimization mitigates cosine saturation zones lacks rigorous mathematical proof or isolated ablation studies
- **Medium Confidence**: LLM-supervised learning approach shows promise but depends heavily on underlying LLM quality and representativeness of generated pairs

## Next Checks

1. **Ablation Study on Objective Components**: Train AnglE variants with only cosine, only angle, and only in-batch negative objectives to quantify marginal contribution of each component to final performance

2. **Numerical Stability Analysis**: Implement and test various strategies for handling near-zero angle differences (epsilon clipping, alternative normalization schemes) to identify potential failure modes in edge cases

3. **LLM Supervision Quality Assessment**: Compare performance when using different LLMs (or no LLM) for pseudo-label generation to establish dependency of AnglE's domain adaptation capability on LLM quality