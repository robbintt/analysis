---
ver: rpa2
title: 'Tracing Influence at Scale: A Contrastive Learning Approach to Linking Public
  Comments and Regulator Responses'
arxiv_id: '2311.14871'
source_url: https://arxiv.org/abs/2311.14871
tags:
- comment
- comments
- text
- response
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of linking public comments to
  regulator responses in U.S. federal rulemaking.
---

# Tracing Influence at Scale: A Contrastive Learning Approach to Linking Public Comments and Regulator Responses

## Quick Facts
- arXiv ID: 2311.14871
- Source URL: https://arxiv.org/abs/2311.14871
- Reference count: 23
- Outperforms baselines and matches GPT-4 on comment-response linking task

## Executive Summary
This paper addresses the challenge of linking public comments to regulator responses in U.S. federal rulemaking. The authors propose an iterative contrastive learning approach that trains a neural model to match comment text to regulator responses without requiring labeled data. Their method achieves strong performance, with a Pearson correlation of 0.79 with human judgments on an annotated test set, approaching the performance of GPT-4 (0.82) while being more cost-effective at scale. The approach demonstrates model-agnostic behavior, working effectively with both SBERT and RoBERTa encoders.

## Method Summary
The method uses iterative contrastive learning to train a text matcher without labeled data. It begins with an initial SBERT or RoBERTa encoder to generate embeddings, then uses these to identify hard positive and negative training samples through similarity scoring. The model is updated using these challenging samples, and the process repeats for multiple iterations. The scoring layer computes cosine distance between embeddings and applies an exponential transformation with hyperparameter α=50. Hard positive samples are comments most similar to each response, while hard negative samples are comments least similar. This creates a self-improving loop that progressively refines the model's ability to distinguish relevant comment-response pairs.

## Key Results
- Iterative contrastive learning framework achieves Pearson correlation of 0.79 with human annotations
- Performance approaches GPT-4 (0.82 correlation) while being more cost-effective at scale
- Framework demonstrates model-agnostic behavior, improving both SBERT and RoBERTa performance
- Outperforms several text-matching baselines on the EPA rulemaking dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative contrastive learning improves semantic matching by progressively refining hard positive and negative samples
- Mechanism: The model starts with an initial SBERT encoder, uses it to identify hard positive pairs (most similar comments to each response) and hard negative pairs (least similar comments to each response), then retrains on these challenging samples. This process repeats, with each iteration using the improved model to generate better training samples.
- Core assumption: The initial SBERT encoder can generate meaningful embeddings that capture semantic similarity, even if imperfectly, to bootstrap the iterative process
- Evidence anchors:
  - [abstract] "we propose a simple yet effective iterative contrastive learning paradigm"
  - [section 2.2] "we optimize the text encoder in the matcher on selected hard positive and negative samples to effectively capture signals indicating the semantic relevance"
  - [corpus] Weak - the paper doesn't directly compare performance across iterations with different initial encoders
- Break condition: If the initial encoder produces random or meaningless embeddings, the hard mining step will fail and the process won't converge

### Mechanism 2
- Claim: The model-agnostic nature of the iterative framework allows transfer of performance gains across different text encoders
- Mechanism: The iterative contrastive learning framework creates a training methodology that improves matching performance regardless of the base encoder used. When applied to both RoBERTa and SBERT, both see significant correlation improvements with human judgments
- Core assumption: The contrastive learning approach captures generalizable patterns about comment-response relationships that aren't specific to one encoder architecture
- Evidence anchors:
  - [abstract] "demonstrates the model-agnostic behavior of our iterative contrastive learning framework when effectively interacting with different base encoders"
  - [section 3.2] "When our proposed contrastive learning framework is applied to RoBERTa and SBERT, the correlation of these two base text encoders with human judgments increases"
  - [corpus] Moderate - the paper shows performance gains for both encoders but doesn't test with other architectures
- Break condition: If the framework's gains are actually due to properties specific to the tested encoders, applying it to fundamentally different architectures might fail

### Mechanism 3
- Claim: The hard mining strategy effectively focuses training on challenging samples that improve generalization
- Mechanism: Instead of random sampling, the method identifies pairs the current model struggles with (hard positives it assigns low probability to, and hard negatives it assigns high probability to). This targeted approach forces the model to learn nuanced distinctions
- Core assumption: Samples that challenge the current model are the most informative for improving its decision boundaries
- Evidence anchors:
  - [section 2.2] "we first draw a batch of M comment/response strings and then extract hard positive and negative samples associated with strings in the batch"
  - [section 3.2] "it is very important to link the right comments to the right responses" suggesting the need for precise matching
  - [corpus] Moderate - the paper shows performance improvements but doesn't directly compare hard mining to random sampling strategies
- Break condition: If the "hard" samples are actually outliers or noise rather than challenging but valid examples, the model may overfit to spurious patterns

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: The task requires learning to distinguish between matched and unmatched comment-response pairs in an unsupervised setting
  - Quick check question: What is the fundamental difference between contrastive learning and standard supervised learning approaches?

- Concept: Text embedding similarity metrics
  - Why needed here: The model relies on cosine distance between embeddings to determine semantic similarity between comments and responses
  - Quick check question: Why might cosine similarity be preferred over Euclidean distance for comparing text embeddings?

- Concept: Iterative model refinement
  - Why needed here: The approach alternates between generating training samples and updating the model, creating a self-improving loop
  - Quick check question: What are the risks of using a model to generate its own training data in an iterative process?

## Architecture Onboarding

- Component map: Text encoder → Scoring layer → Hard mining module → Training loop → Improved text encoder
- Critical path: Text encoder → Hard mining → Model update → Improved encoder
- Design tradeoffs: The iterative approach trades computational cost (multiple training passes) for improved accuracy without requiring labeled data
- Failure signatures: If correlation with human judgment plateaus early, the iterative process may be stuck in a local optimum or the hard mining strategy may be ineffective
- First 3 experiments:
  1. Compare performance of different text encoders (BERT, RoBERTa, SBERT) with and without iterative training
  2. Test different numbers of training iterations to find the point of diminishing returns
  3. Evaluate the impact of batch size on the quality of hard positive/negative samples generated

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the iterative contrastive learning framework perform with different base encoders beyond SBERT and RoBERTa?
- Basis in paper: The authors state "we also test with the vanilla RoBERTa(base) (Liu et al., 2019) as an alternate backbone text encoder, aiming to discern if improvements brought by the iterative contrastive learning framework extend beyond just one particular text encoder."
- Why unresolved: While the paper shows that the framework improves performance for both SBERT and RoBERTa, it does not explore the performance with other potential base encoders like BERT or Legal-BERT.
- What evidence would resolve it: Testing the iterative contrastive learning framework with a variety of base encoders and comparing their performance on the task of matching public comments to regulator responses.

### Open Question 2
- Question: What is the impact of using different similarity metrics in the iterative contrastive learning framework?
- Basis in paper: The paper uses cosine distance as the similarity metric for the scoring layer. However, it does not explore the impact of using other similarity metrics like Euclidean distance or Manhattan distance.
- Why unresolved: The choice of similarity metric could potentially affect the performance of the model, but this is not explored in the paper.
- What evidence would resolve it: Comparing the performance of the model using different similarity metrics on the task of matching public comments to regulator responses.

### Open Question 3
- Question: How does the performance of the model change with different sizes of the training dataset?
- Basis in paper: The paper uses a dataset of 6,727 rules, 17,452 responses, 10,456 comments chopped into 193,143 comment chunks. However, it does not explore how the performance of the model changes with different sizes of the training dataset.
- Why unresolved: The size of the training dataset could potentially affect the performance of the model, but this is not explored in the paper.
- What evidence would resolve it: Training the model on datasets of different sizes and comparing their performance on the task of matching public comments to regulator responses.

### Open Question 4
- Question: How does the performance of the model change with different values of the hyperparameter α in the scoring layer?
- Basis in paper: The paper sets the hyperparameter α to 50 in the scoring layer, but does not explore how the performance of the model changes with different values of α.
- Why unresolved: The value of the hyperparameter α could potentially affect the performance of the model, but this is not explored in the paper.
- What evidence would resolve it: Training the model with different values of α and comparing their performance on the task of matching public comments to regulator responses.

## Limitations

- Limited to EPA rulemaking domain, may not generalize to other federal agencies
- Relies heavily on quality of initial text encoder embeddings without testing broader range of architectures
- Hard mining strategy assumes challenging samples are informative rather than noise or outliers

## Confidence

- High confidence: Iterative contrastive learning outperforms baselines on EPA dataset with strong quantitative metrics
- Medium confidence: Model-agnostic framework behavior since improvements shown for SBERT and RoBERTa but not broader architectures
- Medium confidence: Hard mining effectiveness since paper shows improvements but lacks direct comparison to random sampling

## Next Checks

1. Cross-domain validation: Test trained models on rulemaking datasets from other federal agencies (e.g., Department of Transportation, FDA) to assess generalizability beyond EPA regulations.

2. Ablation study on mining strategy: Compare iterative hard mining against random sampling and semi-hard negative mining to isolate hard mining's contribution to performance gains.

3. Encoder architecture robustness test: Apply iterative framework to additional text encoders (e.g., BERT, DeBERTa, or domain-specific encoders) to verify claimed model-agnostic behavior across wider range of architectures.