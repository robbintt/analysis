---
ver: rpa2
title: Adversarial Attacks Against Uncertainty Quantification
arxiv_id: '2309.10586'
source_url: https://arxiv.org/abs/2309.10586
tags:
- uncertainty
- attack
- attacks
- dropout
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive study on adversarial attacks
  against uncertainty quantification (UQ) techniques in machine learning. The authors
  propose a threat model where attackers aim to manipulate uncertainty estimates regardless
  of prediction correctness, focusing on integrity and availability of ML-based systems.
---

# Adversarial Attacks Against Uncertainty Quantification

## Quick Facts
- arXiv ID: 2309.10586
- Source URL: https://arxiv.org/abs/2309.10586
- Reference count: 31
- Key outcome: Adversarial attacks can significantly reduce uncertainty estimates in ML models while maintaining high accuracy

## Executive Summary
This paper presents a comprehensive study on adversarial attacks targeting uncertainty quantification (UQ) techniques in machine learning. The authors develop multiple attack strategies that manipulate uncertainty estimates regardless of prediction correctness, focusing on both probabilistic methods (MC dropout, Deep Ensembles) and deterministic approaches (DUQ). Their experiments reveal that all tested UQ methods are vulnerable to these attacks, with Deep Ensembles showing surprising resilience compared to other techniques.

## Method Summary
The authors implement four attack strategies: Maximum Variance Attack (MV A), Stochastic Targeted Attack (STAB), Aleatoric Targeted Attack (ATA), and Uniform Segmentation Target Attack (UST). They evaluate these attacks against MC dropout, Deep Ensembles, and DUQ on CIFAR-10, CIFAR-100, and PASCAL VOC datasets. The attacks are designed to minimize or maximize uncertainty measures while maintaining prediction accuracy, using PGD-based optimization with ℓ∞ norm constraints. Experiments cover both in-distribution and out-of-distribution scenarios.

## Key Results
- All tested UQ methods (MC dropout, Deep Ensembles, DUQ) are vulnerable to adversarial attacks targeting uncertainty
- Deep Ensembles surprisingly shows the least robustness against uncertainty-targeted attacks
- Uniform Segmentation Target Attack (UST) effectively reduces both epistemic and aleatoric uncertainty in semantic segmentation tasks
- Attacks can maintain high accuracy (>95%) while significantly reducing uncertainty measures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial attacks can specifically target uncertainty quantification (UQ) components by manipulating uncertainty estimates regardless of prediction correctness
- Mechanism: By crafting adversarial perturbations that minimize or maximize the uncertainty measure, attackers can break the statistical correlation between uncertainty values and prediction reliability
- Core assumption: UQ methods assume higher uncertainty correlates with higher misclassification probability, and this correlation can be exploited
- Evidence anchors:
  - [abstract] "adaptive attacks specifically aimed at reducing also the uncertainty estimate can easily bypass this defense mechanism"
  - [section 1] "an attacker may target an ML system to increase the uncertainty associated with its predictions, resulting in an unnecessary additional workload for the operator"
  - [corpus] Weak evidence - no direct neighbor papers discuss this specific attack scenario
- Break condition: If UQ methods become invariant to adversarial perturbations or if uncertainty measures incorporate robust statistical properties that cannot be easily manipulated

### Mechanism 2
- Claim: Different UQ techniques have varying vulnerabilities to adversarial attacks, with probabilistic methods being more susceptible than deterministic ones
- Mechanism: Probabilistic UQ methods (MC dropout, Deep Ensembles) rely on multiple forward passes and statistical aggregation, making them vulnerable to attacks that manipulate the variance or entropy of predictions. Deterministic methods (DUQ) are more directly vulnerable as they have a single prediction that can be manipulated to approach a centroid
- Core assumption: The architecture of UQ methods determines their attack surface and vulnerability profile
- Evidence anchors:
  - [section 3.2] "For a given sample x, the underlying idea is to craft an adversarial example x + δ pushing the prediction confidence for the target (wrong) class over 95% and simultaneously keeping the corresponding uncertainty not higher than the one of the original sample"
  - [section 4.2] "Deep Ensembles turned out to be the less robust UQ technique against adversarial attacks targeting uncertainty"
  - [corpus] Weak evidence - no direct neighbor papers discuss attack vulnerabilities of specific UQ methods
- Break condition: If UQ methods incorporate adversarial training or robust statistical aggregation techniques that make them invariant to adversarial perturbations

### Mechanism 3
- Claim: Semantic segmentation tasks present unique challenges for UQ attacks due to the pixel-wise nature of uncertainty and the inherent uncertainty along object edges
- Mechanism: Attacks targeting UQ in semantic segmentation must account for the high correlation between pixels and the spatial structure of the task. Uniform segmentation target attacks (UST) that force all pixels to the same class can effectively reduce both epistemic and aleatoric uncertainty
- Core assumption: The spatial structure and pixel-wise nature of semantic segmentation create unique attack opportunities and challenges
- Evidence anchors:
  - [section 3.4] "we find it challenging to decrease the uncertainty measure around the edges of the segmented objects... we hypothesize this is due to the fact that the network is 'forced' to abruptly change prediction around the edges"
  - [section 4.2] "Uniform Segmentation Target Attack (UST) turns out to be effective in reducing both the epistemic and the aleatoric uncertainty for each pixel"
  - [corpus] Weak evidence - no direct neighbor papers discuss UQ attacks in semantic segmentation
- Break condition: If semantic segmentation models incorporate edge-aware uncertainty estimation or spatial regularization that makes uniform attacks less effective

## Foundational Learning

- Concept: Uncertainty quantification in machine learning
  - Why needed here: Understanding UQ is fundamental to grasping how attackers can manipulate uncertainty estimates
  - Quick check question: What are the two main types of uncertainty in ML, and how do they differ?

- Concept: Adversarial machine learning and attack strategies
  - Why needed here: Knowledge of attack methodologies is essential for understanding how UQ can be targeted
  - Quick check question: What is the difference between white-box and black-box attacks, and which type is more relevant for UQ attacks?

- Concept: Probabilistic vs. deterministic modeling
  - Why needed here: Different UQ methods have different vulnerabilities, so understanding their architectures is crucial
  - Quick check question: How do probabilistic UQ methods (like MC dropout) differ from deterministic ones (like DUQ) in terms of their approach to uncertainty?

## Architecture Onboarding

- Component map:
  Input preprocessing and perturbation generation -> Model with uncertainty quantification (UQ) -> Attack algorithm (e.g., PGD, MV A, STAB, UST) -> Uncertainty estimation and evaluation metrics -> Data loaders for CIFAR-10, CIFAR-100, and PASCAL VOC

- Critical path:
  1. Load clean input and apply adversarial perturbation
  2. Pass through model with UQ
  3. Estimate uncertainty (variance, entropy, distance to centroid)
  4. Evaluate attack success (uncertainty reduction vs. perturbation budget)

- Design tradeoffs:
  - Probabilistic UQ: More accurate uncertainty estimates but computationally expensive and vulnerable to attacks
  - Deterministic UQ: Computationally efficient but potentially less accurate and still vulnerable to attacks
  - Attack strength vs. perceptibility: Stronger attacks reduce uncertainty more but may be more detectable

- Failure signatures:
  - Attack fails to reduce uncertainty despite high perturbation budget
  - Model becomes unstable or produces NaNs during attack iterations
  - Uncertainty estimates become negative or otherwise invalid

- First 3 experiments:
  1. Implement MV A attack on a simple MC dropout model with CIFAR-10, measure variance reduction vs. perturbation budget
  2. Compare STAB vs. ATA attacks on Deep Ensemble, evaluate which is more effective at reducing uncertainty
  3. Apply UST attack to semantic segmentation on PASCAL VOC, measure pixel-wise uncertainty reduction and segmentation accuracy

## Open Questions the Paper Calls Out
1. How effective are under-confidence attacks (U-attacks) that aim to increase uncertainty estimates compared to over-confidence attacks?
2. How do black-box attacks perform against uncertainty quantification methods compared to white-box attacks?
3. Can adversarial training effectively defend against attacks targeting uncertainty quantification, and if so, what are the optimal training strategies?

## Limitations
- Limited exploration of defense mechanisms against UQ-targeted attacks
- Absence of ablation studies on attack hyperparameters and their impact
- Findings may not generalize uniformly across all ML applications and architectures

## Confidence
- High Confidence: The vulnerability of MC dropout and Deep Ensembles to uncertainty-targeted attacks
- Medium Confidence: The relative robustness of Deep Ensembles compared to other methods
- Low Confidence: The effectiveness of attacks on semantic segmentation

## Next Checks
1. Test attack transferability across different model architectures (e.g., vision transformers) to validate method independence
2. Implement and evaluate simple uncertainty-aware defenses (like robust aggregation) to establish baseline protection levels
3. Conduct extensive ablation studies varying attack hyperparameters (iteration count, perturbation budgets) to identify critical attack parameters