---
ver: rpa2
title: Learning via Wasserstein-Based High Probability Generalisation Bounds
arxiv_id: '2306.04375'
source_url: https://arxiv.org/abs/2306.04375
tags:
- learning
- bounds
- online
- wasserstein
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PAC-Bayesian generalisation bounds based
  on the Wasserstein distance, offering a novel alternative to classical bounds involving
  KL divergence. The key contributions include: (1) new high-probability PAC-Bayes
  bounds for both batch learning with i.i.d.'
---

# Learning via Wasserstein-Based High Probability Generalisation Bounds

## Quick Facts
- arXiv ID: 2306.04375
- Source URL: https://arxiv.org/abs/2306.04375
- Reference count: 40
- Primary result: Wasserstein-based PAC-Bayesian bounds that replace KL divergence, enabling deterministic predictors and heavy-tailed loss handling with high-probability guarantees.

## Executive Summary
This paper introduces PAC-Bayesian generalisation bounds based on the Wasserstein distance, offering a novel alternative to classical bounds involving KL divergence. The key contributions include: (1) new high-probability PAC-Bayes bounds for both batch learning with i.i.d. data and online learning with potentially non-i.i.d. data, valid for heavy-tailed losses; (2) derivation of practical PAC-Bayes learning algorithms that optimise Wasserstein-based objectives, enabling deterministic predictors; (3) experimental validation on UCI, MNIST, and FashionMNIST datasets showing improved generalisation over empirical risk minimisation, especially for neural networks. The bounds hold with high probability, avoid restrictive assumptions like subgaussianity, and are amenable to optimisation. The work paves the way for further theoretical and practical developments in PAC-Bayes learning via optimal transport.

## Method Summary
The paper develops two main algorithms: a two-step batch learning algorithm (Algorithm 1) and an online learning algorithm (Algorithm 2). Both algorithms optimise a PAC-Bayes objective where the Kullback-Leibler divergence is replaced by the Wasserstein distance. The batch algorithm first trains K hypotheses on disjoint mini-batches using PRIORS LEARNING, then optimises a posterior distribution over hypotheses using POSTERIOR LEARNING, regularised by the Wasserstein distance to the priors. The online algorithm performs gradient updates at each step, incorporating a log-barrier constraint to maintain the Wasserstein distance bound. Both algorithms use Lipschitz continuous loss functions and operate on data split into K disjoint subsets to construct data-dependent priors. The Wasserstein distance is computed using the Kantorovich-Rubinstein duality, enabling efficient gradient-based optimisation.

## Key Results
- Introduces high-probability PAC-Bayes bounds based on Wasserstein distance that hold for heavy-tailed losses with bounded second moments.
- Develops practical learning algorithms (batch and online) that optimise Wasserstein-regularised objectives, demonstrating improved generalisation on UCI, MNIST, and FashionMNIST datasets.
- Enables the use of Dirac measures as both priors and posteriors, avoiding the absolute continuity requirement of KL divergence and allowing deterministic predictors.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wasserstein distance replaces KL divergence to avoid absolute continuity requirement.
- Mechanism: Wasserstein distance measures geometric discrepancy between distributions without requiring density ratios, enabling use of Dirac measures as both priors and posteriors.
- Core assumption: Predictor space H is Polish and loss function is Lipschitz continuous.
- Evidence anchors:
  - [abstract] "replace the KL divergence in the PAC-Bayesian bounds with the Wasserstein distance"
  - [section] "the Wasserstein distance does not require absolute continuity"
  - [corpus] No direct evidence; this is a standard result from optimal transport theory.
- Break condition: If loss function is not Lipschitz or predictor space lacks Polish structure, Kantorovich-Rubinstein duality fails.

### Mechanism 2
- Claim: Supermartingale techniques enable high-probability bounds for heavy-tailed losses.
- Mechanism: By constructing exponential supermartingales from conditional variance terms, the authors obtain concentration inequalities that hold with probability at least 1-δ, even for unbounded losses with bounded second moments.
- Core assumption: Losses admit bounded second moments and are Lipschitz.
- Evidence anchors:
  - [abstract] "heavy-tailed losses" and "high probability"
  - [section] "exploit the supermartingale toolbox introduced in [HG23a, CWR23]"
  - [corpus] Weak evidence; supermartingale techniques are referenced but not detailed in corpus.
- Break condition: If second moments are unbounded or supermartingale construction fails, high-probability guarantees collapse.

### Mechanism 3
- Claim: Data-dependent priors enable tighter bounds by controlling Wasserstein terms.
- Mechanism: Splitting data into K disjoint sets and using each subset to construct priors that depend only on the complementary data reduces the Wasserstein distance between posterior and prior, improving the bound's tightness.
- Core assumption: Data can be partitioned such that each prior is independent of its corresponding data subset.
- Evidence anchors:
  - [section] "we consider K stochastic kernels π1, ..., πK such that for any S, the distribution πi(S, ·) does not depend on Si"
  - [section] "the variance terms are considered with respect to the prior distributions πi,S"
  - [corpus] No direct evidence; this is a theoretical design choice.
- Break condition: If priors cannot be made data-independent in the required way, the variance control fails and the bound becomes vacuous.

## Foundational Learning

- Concept: Optimal transport and Wasserstein distance
  - Why needed here: Provides geometric notion of distance between distributions that doesn't require density ratios
  - Quick check question: Can you explain why Wasserstein distance can measure distance between Dirac measures while KL divergence cannot?

- Concept: Supermartingale concentration inequalities
  - Why needed here: Enables high-probability bounds for heavy-tailed losses without subgaussian assumptions
  - Quick check question: What is the key difference between using supermartingales versus classical concentration inequalities like Hoeffding's?

- Concept: PAC-Bayesian framework with randomized predictors
  - Why needed here: Provides the probabilistic framework where Wasserstein distance can replace KL divergence
  - Quick check question: In PAC-Bayes, what is the relationship between the prior, posterior, and the generalization gap?

## Architecture Onboarding

- Component map: Data → K-way split → Prior construction → Posterior optimization → Generalization bound verification
- Critical path: Data → K-way split → Prior construction → Posterior optimization → Generalization bound verification
- Design tradeoffs:
  - K parameter: Larger K gives more informative priors but increases computational cost
  - Wasserstein order: Paper uses order 1 (Earth Mover's), higher orders could capture different geometry but are harder to compute
  - Batch vs online: Different algorithms required; online needs constraint handling for stability
- Failure signatures:
  - Bounds becoming vacuous: Check if Wasserstein terms dominate or second moments are unbounded
  - Optimization divergence: Verify Lipschitz constants and check constraint satisfaction in online case
  - Poor empirical performance: Validate data-dependent prior construction and mini-batch implementation
- First 3 experiments:
  1. Verify Wasserstein distance computation on simple distributions (e.g., Gaussians, Diracs)
  2. Test PAC-Bayes bound tightness on synthetic data with known generalization gap
  3. Implement and benchmark the batch algorithm on a simple UCI dataset with linear models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of K (number of priors) affect the generalization performance across different datasets and model architectures?
- Basis in paper: [explicit] The paper discusses different values of K (1, √m, m) and their implications in Theorems 1 and 2, but empirical results in Appendix C.4 show performance varies with K.
- Why unresolved: While the paper provides theoretical bounds, the optimal choice of K appears to be dataset and model dependent, and the theoretical discussion doesn't prescribe a specific value.
- What evidence would resolve it: A comprehensive empirical study varying K across diverse datasets and architectures, coupled with theoretical analysis of how K affects the Wasserstein term and convergence rate, could identify guidelines for choosing K.

### Open Question 2
- Question: Can the Wasserstein-based PAC-Bayesian bounds be extended to cover losses with unbounded moments beyond order 2?
- Basis in paper: [inferred] The paper assumes heavy-tailed losses with bounded order 2 moments, but doesn't explore losses with higher or unbounded moments.
- Why unresolved: The supermartingale techniques used in the proofs rely on controlling the variance of the loss, which becomes more challenging for losses with higher moments.
- What evidence would resolve it: Developing new concentration inequalities or martingale techniques that can handle higher moments, and proving corresponding generalization bounds, would extend the applicability of the framework.

### Open Question 3
- Question: How does the choice of distance metric d in the Wasserstein distance affect the generalization bounds and learning performance?
- Basis in paper: [explicit] The paper assumes a Polish predictor space H equipped with a distance d, but doesn't explore the impact of different choices of d.
- Why unresolved: Different distance metrics can capture different geometric properties of the hypothesis space, which may influence the tightness of the bounds and the effectiveness of the regularization.
- What evidence would resolve it: Comparing the performance of the Wasserstein-based algorithms using different distance metrics (e.g., Euclidean, Mahalanobis, learned metrics) on various datasets would shed light on the impact of the choice of d.

## Limitations
- The paper does not provide direct empirical validation of the bound tightness, focusing instead on comparing learning algorithms.
- The online algorithm's constraint handling via log-barrier regularization is not thoroughly analyzed for convergence properties or sensitivity to hyperparameters.
- The supermartingale techniques used in the proofs are referenced but not detailed, making it difficult to assess their limitations for different loss functions.

## Confidence
- **High Confidence:** The theoretical derivation of Wasserstein-based PAC-Bayes bounds and their validity under the stated assumptions (Polish predictor space, Lipschitz losses, bounded second moments).
- **Medium Confidence:** The practical utility of data-dependent priors and their impact on bound tightness, as this relies on empirical performance rather than direct bound verification.
- **Low Confidence:** The robustness of the online algorithm to hyperparameter choices and its generalization to non-stationary data distributions, given limited analysis in the paper.

## Next Checks
1. **Bound Tightness Verification:** Construct synthetic datasets where the generalization gap is known and verify that the Wasserstein PAC-Bayes bound captures this gap accurately.
2. **Prior Sensitivity Analysis:** Systematically vary K and the prior construction method to quantify their impact on bound tightness and empirical performance.
3. **Online Algorithm Robustness:** Test the online algorithm on non-i.i.d. data streams (e.g., concept drift scenarios) to assess its stability and adaptation capabilities.