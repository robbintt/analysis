---
ver: rpa2
title: Multimodal Prompt Retrieval for Generative Visual Question Answering
arxiv_id: '2306.17675'
source_url: https://arxiv.org/abs/2306.17675
tags:
- question
- retrieval
- dataset
- answer
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multimodal prompt retrieval method for
  generative visual question answering (VQA) that enables rapid adaptation to new
  domains and open-ended answer generation. The core idea is to retrieve relevant
  image-question-answer examples and construct multimodal prompts to guide the generation
  of free-form answers.
---

# Multimodal Prompt Retrieval for Generative Visual Question Answering

## Quick Facts
- **arXiv ID:** 2306.17675
- **Source URL:** https://arxiv.org/abs/2306.17675
- **Reference count:** 35
- **Primary result:** MPR achieves up to 30% accuracy improvement over non-retrieval baselines in few-shot medical VQA adaptation

## Executive Summary
This paper introduces Multimodal Prompt Retrieval (MPR), a novel approach for generative visual question answering that leverages retrieval-based prompt construction to enable rapid adaptation to new domains and open-ended answer generation. The method retrieves relevant image-question-answer examples and constructs multimodal prompts to guide the generation of free-form answers, achieving significant performance gains on medical VQA tasks. By using a PubMedCLIP encoder for medical domain-specific features and a T5-based generative model, MPR demonstrates strong in-domain and cross-dataset performance while handling both closed and open-ended answers.

## Method Summary
MPR employs a retrieval-based approach where CLIP-based multimodal encoders retrieve top-k similar examples from a labeled mapping set for a given image-question pair. These retrieved examples are then combined with the input to construct prompts that guide a T5-based generative model in producing free-form answers. The model uses synthetic data augmentation from image-caption pairs and in-context prediction for domain adaptation. A string-matching heuristic maps generated text to answer labels, enabling flexibility across different VQA tasks and datasets.

## Key Results
- Achieves up to 30% accuracy improvement over non-retrieval counterparts in few-shot medical VQA adaptation
- PubMedCLIP initialization outperforms general CLIP by 5-7% on medical tasks
- Strong performance on both open and closed questions in SLAKE and VQA-RAD datasets
- Effective cross-dataset adaptation with in-context retrieval

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The model generalizes across datasets by retrieving relevant image-question pairs as multimodal prompts
- **Mechanism:** For new inputs, retrieves similar examples from a labeled retrieval set and uses them to guide answer generation
- **Core assumption:** Retrieved examples contain correct answers that can guide generation for new inputs
- **Evidence anchors:**
  - [abstract] "Our generative model enables rapid zero-shot dataset adaptation to unseen data distributions and open-set answer labels across datasets."
  - [section 3.2] "To answer a question x about an image v, we propose to retrieve its top-k most similar examples from the retrieval mapping set M."
- **Break condition:** Poor retrieval results from small or noisy retrieval sets lead to irrelevant prompts and incorrect answers

### Mechanism 2
- **Claim:** PubMedCLIP encoder improves medical VQA performance over general CLIP
- **Mechanism:** Fine-tuned on medical image-caption pairs, PubMedCLIP better captures domain-specific features for more accurate retrieval
- **Core assumption:** Medical domain features are better captured by models trained on medical data
- **Evidence anchors:**
  - [abstract] "Our experiments on medical VQA tasks show that MPR outperforms its non-retrieval counterpart by up to 30% accuracy points in a few-shot domain adaptation setting."
  - [section 5.1] "We also demonstrate that initializing our model with a PubMedCLIP pre-trained checkpoint results in higher accuracy than a general CLIP checkpoint."
- **Break condition:** Significant domain shifts or poor alignment between PubMedCLIP and target dataset reduce performance

### Mechanism 3
- **Claim:** Generative model handles both closed and open-ended answers with string-matching
- **Mechanism:** Generates free-form text answers that are matched to closest answer label using string-matching heuristic
- **Core assumption:** Generated text can be deterministically mapped to correct answer labels
- **Evidence anchors:**
  - [abstract] "Our generative model enables rapid zero-shot dataset adaptation to unseen data distributions and open-set answer labels across datasets."
  - [section 3.4] "As each answer label y has a corresponding text string z of varying length, we formulate the likelihood of an answer string z given an image v and a question x by the following conditional probability."
- **Break condition:** Generated text too dissimilar from answer labels causes string-matching to fail

## Foundational Learning

- **Concept:** Multimodal encoding and retrieval
  - Why needed here: To find relevant examples from retrieval set that guide answer generation
  - Quick check question: How does the model encode image-question pairs for retrieval?

- **Concept:** Prompt construction and integration
  - Why needed here: To combine retrieved examples with input to guide generative model
  - Quick check question: What are the three main components of the prompt used in the model?

- **Concept:** Generative modeling and string matching
  - Why needed here: To generate free-form answers and map them to correct labels
  - Quick check question: How does the model handle minor discrepancies between generated answers and answer labels?

## Architecture Onboarding

- **Component map:** CLIP encoder → Retrieval module → T5 generator → String-matching heuristic
- **Critical path:** Input → Multimodal encoding → Retrieval → Prompt construction → Answer generation → Label mapping
- **Design tradeoffs:**
  - Generative approach enables flexibility but may introduce answer noise
  - Large retrieval sets improve performance but increase computational cost
- **Failure signatures:**
  - Poor retrieval leads to irrelevant prompts and incorrect answers
  - String-matching limitations cause mismatch between generated text and labels
- **First 3 experiments:**
  1. Evaluate model on held-out test set to establish baseline performance
  2. Test adaptation to new dataset using in-context retrieval
  3. Compare PubMedCLIP vs general CLIP performance impact

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does MPR performance vary with different question templates for synthetic data augmentation?
- **Basis in paper:** [inferred] Paper mentions using different templates but doesn't explore their impact on performance
- **Why unresolved:** Template variations' effect on synthetic data quality and model performance remains unexplored
- **What evidence would resolve it:** Experiments with different template configurations comparing accuracy and generalization

### Open Question 2
- **Question:** Can retrieval-based approach extend to other knowledge-intensive domains beyond medical VQA?
- **Basis in paper:** [explicit] Focuses on medical VQA without exploring other domains
- **Why unresolved:** Applicability to other domains with different characteristics remains untested
- **What evidence would resolve it:** Application to domains like scientific literature or legal documents with performance evaluation

### Open Question 3
- **Question:** How does model performance change with different image encoders?
- **Basis in paper:** [explicit] Uses CLIP vision transformer but doesn't explore alternatives
- **Why unresolved:** Impact of alternative encoders like ResNet or EfficientNet on feature quality unexamined
- **What evidence would resolve it:** Experiments comparing different image encoders' impact on accuracy and retrieval

### Open Question 4
- **Question:** How does model performance change with different text encoders?
- **Basis in paper:** [explicit] Uses T5 text encoder without exploring alternatives
- **Why unresolved:** Impact of encoders like BERT or RoBERTa on question feature quality unexplored
- **What evidence would resolve it:** Experiments comparing different text encoders' impact on accuracy and retrieval

## Limitations

- 30% accuracy improvement claim requires careful interpretation across different VQA formulations
- Potential domain shift between medical training data and VQA target datasets not fully addressed
- String-matching heuristic introduces opaque dependencies that may bias results toward retrieval set patterns

## Confidence

- **High confidence:** Retrieval-based prompt construction mechanism is technically sound and well-supported
- **Medium confidence:** Domain adaptation benefits depend heavily on retrieval set quality and representativeness
- **Low confidence:** String-matching heuristic robustness across diverse answer vocabularies not thoroughly validated

## Next Checks

1. **Retrieval Quality Analysis:** Conduct systematic ablation study measuring retrieval precision/recall for medical domain, comparing PubMedCLIP vs general CLIP across medical sub-specialties

2. **String-Matching Robustness Test:** Evaluate answer mapping accuracy across diverse answer patterns including morphological variants, synonyms, and paraphrased answers to identify systematic failures

3. **Cross-Dataset Generalization:** Test adaptation capability on non-medical VQA datasets (GQA, VQA-v2) to determine if approach generalizes beyond medical domain