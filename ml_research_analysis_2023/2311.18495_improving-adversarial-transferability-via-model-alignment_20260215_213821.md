---
ver: rpa2
title: Improving Adversarial Transferability via Model Alignment
arxiv_id: '2311.18495'
source_url: https://arxiv.org/abs/2311.18495
tags:
- source
- alignment
- adversarial
- transferability
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel model alignment technique to improve
  the transferability of adversarial examples. The method fine-tunes a source model
  to minimize an alignment loss that measures the divergence in predictions between
  the source and an independently trained witness model.
---

# Improving Adversarial Transferability via Model Alignment

## Quick Facts
- arXiv ID: 2311.18495
- Source URL: https://arxiv.org/abs/2311.18495
- Reference count: 40
- Primary result: A model alignment technique improves adversarial transferability by fine-tuning source models to minimize divergence from witness models

## Executive Summary
This paper proposes a novel approach to improve adversarial transferability by aligning a source model with an independently trained witness model. The method fine-tunes the source model to minimize a divergence-based loss (such as KL divergence) between its predictions and those of the witness model. This alignment process encourages the source model to extract features similar to those of the witness model, leading to more transferable adversarial perturbations. The authors demonstrate that this technique not only improves transferability across various architectures but also leads to a smoother loss landscape, consistent with findings that flatter loss maxima produce more transferable attacks.

## Method Summary
The proposed method involves fine-tuning a source model to minimize an alignment loss that measures the divergence in predictions between the source and an independently trained witness model. This process encourages the source model to focus on shared, semantic features rather than model-specific, imperceptible ones. The alignment is achieved through KL divergence loss and can be applied to both output space and embedding space. The fine-tuning uses standard optimization techniques like SGD with momentum, and the method is compatible with multiple attack algorithms including PGD. The key insight is that by aligning the source model's predictions with those of the witness model, the resulting adversarial examples become more transferable to different target models.

## Key Results
- Model alignment significantly improves adversarial transferability across various CNN and Vision Transformer architectures on ImageNet
- The aligned model's loss landscape becomes notably smoother, consistent with prior findings about transferability
- The method is compatible with multiple attack algorithms and can be extended to embedding space alignment
- Using multiple witness models provides modest additional improvements in transferability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model alignment improves transferability by encouraging the source model to extract features more similar to those of the witness model.
- Mechanism: During alignment, the source model is fine-tuned to minimize a divergence-based loss (e.g., KL divergence) between its predictions and those of the witness model. This causes the source model to focus on shared, semantic features rather than model-specific, imperceptible ones.
- Core assumption: Transferability depends on the similarity of exploited features between source and target models.
- Evidence anchors:
  - [abstract] "This loss measures the divergence in the predictions between the source model and another, independently trained model, referred to as the witness model."
  - [section] "Through this alignment process, the source model learns to focus on a set of features that are similarly extracted by the witness model."
  - [corpus] Weak corpus evidence; no direct support for feature similarity claim.
- Break condition: If the witness model does not share semantic features with the target, alignment may not improve transferability.

### Mechanism 2
- Claim: Model alignment leads to a smoother loss landscape, which produces more transferable adversarial examples.
- Mechanism: Fine-tuning with soft labels (from the witness model) regularizes the input Jacobian and reduces sharpness in the loss surface around adversarial examples, making them flatter maxima.
- Core assumption: Adversarial examples from flatter maxima are more transferable than those from sharp maxima.
- Evidence anchors:
  - [abstract] "Results show that there is a notable smoothing effect on the loss surface of the model, which is consistent with prior findings that adversarial examples from flatter maxima are more transferable."
  - [section] "The analysis explores the relationship between model alignment and the use of soft labels... leading to smoother decision boundaries."
  - [corpus] No direct corpus support; relies on prior work cited in paper.
- Break condition: If alignment does not significantly smooth the loss landscape, transferability gains may be minimal.

### Mechanism 3
- Claim: Model alignment is compatible with various attack algorithms and can be extended to embedding space.
- Mechanism: The alignment process can be applied regardless of the specific attack method used to generate perturbations, and alignment can occur in hidden representation space as well as output space.
- Core assumption: Alignment changes model behavior in a way that is independent of the attack algorithm.
- Evidence anchors:
  - [abstract] "The method is also shown to be compatible with multiple attack algorithms."
  - [section] "We demonstrate that our alignment technique is compatible with a wide range of attack algorithms."
  - [corpus] Weak corpus evidence; no direct support for algorithm compatibility claim.
- Break condition: If an attack algorithm relies on specific model properties altered by alignment, compatibility may break down.

## Foundational Learning

- Concept: Adversarial transferability
  - Why needed here: Core phenomenon being improved; understanding why perturbations transfer between models is essential to grasp the problem.
  - Quick check question: What is the main reason adversarial examples generated from one model often fool other models?
- Concept: Loss landscape geometry
  - Why needed here: Key to understanding how alignment improves transferability; relates to sharpness/flatness of maxima.
  - Quick check question: Why are adversarial examples from flatter loss maxima generally more transferable?
- Concept: Knowledge distillation and feature alignment
  - Why needed here: Alignment process is conceptually similar; understanding embedding-space alignment helps with ablation studies.
  - Quick check question: What is the difference between output-space and embedding-space alignment?

## Architecture Onboarding

- Component map: Source model -> Witness model -> Alignment loss -> Fine-tuning process
- Critical path:
  1. Train source and witness models independently.
  2. Define alignment loss between source and witness outputs.
  3. Fine-tune source model on training data using alignment loss.
  4. Generate adversarial examples using aligned source model.
  5. Evaluate transferability to target models.
- Design tradeoffs:
  - Choice of witness model capacity: Smaller witness models may encourage focus on shared semantic features.
  - Output-space vs embedding-space alignment: Embedding alignment may capture richer feature similarity but requires architectural compatibility.
  - Number of witness models: More diverse witnesses may improve alignment but with diminishing returns.
- Failure signatures:
  - No improvement in transferability: Witness model may not share relevant features with targets.
  - Overfitting to witness: Alignment may cause source to mimic witness too closely, losing generalization.
  - Computational overhead: Fine-tuning adds training time and resources.
- First 3 experiments:
  1. Verify alignment loss decreases during fine-tuning on a small dataset.
  2. Compare adversarial example transferability before and after alignment on a validation set.
  3. Test alignment with different witness models (same vs different architecture) to observe impact on transferability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the alignment loss impact the model's performance on clean data versus adversarial examples?
- Basis in paper: [inferred] The paper focuses on transferability of adversarial examples but does not explicitly analyze the impact on clean data accuracy.
- Why unresolved: The study prioritizes transferability improvements and doesn't report clean accuracy metrics before and after alignment.
- What evidence would resolve it: Empirical comparison of clean data accuracy before and after model alignment across different architectures and datasets.

### Open Question 2
- Question: What is the theoretical relationship between the alignment loss and the smoothness of the loss landscape?
- Basis in paper: [explicit] The paper observes a correlation between alignment and smoother loss surfaces but does not provide theoretical justification.
- Why unresolved: The study is empirical and does not develop theoretical frameworks to explain the observed geometric changes.
- What evidence would resolve it: Mathematical proofs or theoretical analysis connecting the alignment loss formulation to properties of the loss landscape curvature.

### Open Question 3
- Question: How does the choice of witness model architecture affect the alignment process across different model families?
- Basis in paper: [explicit] The paper notes that CNN-based models benefit more from aligning with other CNN-based models, but the reasons are not fully explored.
- Why unresolved: The paper provides empirical observations but lacks detailed analysis of why certain architecture combinations work better.
- What evidence would resolve it: Systematic study comparing alignment effectiveness across different source-witness architecture pairs, potentially with ablation studies on layer-wise representations.

### Open Question 4
- Question: What is the optimal number and diversity of witness models needed for maximum transferability improvement?
- Basis in paper: [explicit] The paper shows that using more witness models provides modest improvements but does not explore the diversity aspect.
- Why unresolved: The study only varies the number of witness models while keeping them homogeneous (all ResNet18).
- What evidence would resolve it: Experiments comparing transferability when using multiple diverse witness models versus multiple similar ones, across different source architectures.

## Limitations
- The core mechanism linking alignment-induced loss landscape smoothing to improved transferability relies heavily on indirect evidence rather than direct experimental validation
- The claim that feature similarity between aligned source and witness models drives transferability improvements lacks empirical support from feature-space analysis
- Limited ablation studies on the choice of alignment loss (KL divergence vs other divergences) and its impact on transferability

## Confidence
- High confidence in empirical results showing improved transferability after alignment
- Medium confidence in the geometric analysis connecting loss landscape smoothing to transferability gains
- Low confidence in the proposed mechanism that feature similarity between models is the primary driver of improvements

## Next Checks
1. Conduct feature-space analysis comparing aligned source model representations with witness and target model representations to validate the feature similarity hypothesis
2. Perform ablation studies testing different alignment losses (beyond KL divergence) and their effects on transferability
3. Measure and report the actual sharpness of loss landscapes before and after alignment using quantitative sharpness metrics to strengthen the geometric analysis claims