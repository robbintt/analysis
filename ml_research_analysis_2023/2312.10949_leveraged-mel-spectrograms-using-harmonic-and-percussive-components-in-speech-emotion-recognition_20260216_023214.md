---
ver: rpa2
title: Leveraged Mel spectrograms using Harmonic and Percussive Components in Speech
  Emotion Recognition
arxiv_id: '2312.10949'
source_url: https://arxiv.org/abs/2312.10949
tags:
- feature
- speech
- emotion
- features
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to Speech Emotion Recognition
  (SER) by leveraging harmonic and percussive components of Mel spectrograms. The
  authors decompose the Mel spectrogram into harmonic and percussive components using
  median filtering, then combine these with the log Mel spectrogram to create enriched
  2D hybrid feature maps.
---

# Leveraged Mel spectrograms using Harmonic and Percussive Components in Speech Emotion Recognition

## Quick Facts
- arXiv ID: 2312.10949
- Source URL: https://arxiv.org/abs/2312.10949
- Authors: 
- Reference count: 33
- Key outcome: Achieves 92.79% test accuracy on Berlin EMO-DB using hybrid feature maps combining Mel spectrograms with harmonic and percussive components

## Executive Summary
This paper introduces a novel approach to Speech Emotion Recognition (SER) by leveraging harmonic and percussive components of Mel spectrograms. The authors decompose the Mel spectrogram into harmonic and percussive components using median filtering, then combine these with the log Mel spectrogram to create enriched 2D hybrid feature maps. These feature maps are input to a pre-trained CNN-VGG16 network for feature extraction, followed by an MLP classifier for emotion classification. The proposed method achieves a test accuracy of 92.79% on the Berlin EMO-DB database, outperforming previous works using CNN-VGG16. The study demonstrates that effective data augmentation through hybrid feature engineering significantly improves SER performance.

## Method Summary
The method processes raw speech signals (88.2kHz sampling) by computing Mel spectrograms (128 filterbanks, window size 2048, hop length 512), then decomposing them into harmonic and percussive components using median filtering along time and frequency axes respectively. These components are averaged and combined with the log Mel spectrogram to create 2D hybrid feature maps of size 128x128x2. A pre-trained VGG16 model extracts features from these maps, which are then classified by an MLP with four fully connected layers. The system uses ADAM optimization (learning rate 0.0001) for 128 epochs with batch size 128, trained on 80% of data with 10% validation and 10% test splits.

## Key Results
- Achieves 92.79% test accuracy on Berlin EMO-DB database
- Outperforms previous CNN-VGG16 based methods on the same dataset
- Demonstrates superior performance with 88.2kHz sampling rate and 2048 window size
- Shows effective data augmentation through hybrid feature engineering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Harmonic-percussive decomposition enables discriminative feature separation
- Mechanism: Median filtering separates harmonic (pitch-related) and percussive (timing/energy-related) components of the Mel spectrogram along horizontal and vertical axes respectively, capturing complementary emotional cues
- Core assumption: Harmonic and percussive components contain distinct emotional information that is more separable than the original Mel spectrogram
- Evidence anchors:
  - [abstract] "We attempt to leverage the Mel spectrogram by decomposing distinguishable acoustic features"
  - [section] "The harmonic and percussive components of the spectrum by applying a median filter in the horizontal (time-domain) and vertical (frequency-domain) direction"
  - [corpus] Weak - related papers focus on other architectures but don't specifically mention harmonic-percussive decomposition

### Mechanism 2
- Claim: Hybrid feature maps improve CNN-VGG16 generalization
- Mechanism: Combining log Mel spectrogram with averaged harmonic-percussive components creates richer 2D feature maps that provide complementary information to the CNN feature extractor
- Core assumption: CNN-VGG16 can effectively learn from combined harmonic, percussive, and log Mel features when presented as 2D images
- Evidence anchors:
  - [abstract] "This process results in a function that outputs a 2D image so that it can be used as input data for a pre-trained CNN-VGG16 feature extractor"
  - [section] "The proposed hybrid feature map method can be generalised with other supervised classifiers to obtain better prediction accuracy"
  - [corpus] Weak - related papers don't explicitly validate this specific hybrid approach

### Mechanism 3
- Claim: Higher sample rate and window size improve feature quality
- Mechanism: Larger window size (2048) with high sample rate (88.2kHz) provides better frequency resolution and more overlapping frames, creating richer subsamples
- Core assumption: Emotional information in speech signals benefits more from frequency resolution than time resolution
- Evidence anchors:
  - [section] "Based on the time-frequency trade-off, large frame size is chosen to obtain high-frequency resolution rather than time resolution"
  - [section] "the superior result is achieved on feature map dimensions of 128 x 128 with a sample rate of 88200"
  - [corpus] Weak - related papers don't specifically analyze the impact of sample rate and window size on this decomposition approach

## Foundational Learning

- Concept: Mel spectrogram construction and interpretation
  - Why needed here: Understanding how Mel spectrograms represent speech frequency content is essential for grasping the harmonic-percussive decomposition
  - Quick check question: How does the Mel scale differ from linear frequency scale in representing human perception of pitch?

- Concept: Median filtering for signal decomposition
  - Why needed here: The core technique for separating harmonic and percussive components relies on understanding how median filters operate in different dimensions
  - Quick check question: Why does median filtering along horizontal vs vertical axes separate harmonic vs percussive components?

- Concept: CNN-VGG16 feature extraction principles
  - Why needed here: The model architecture depends on understanding how pre-trained CNNs can extract features from 2D spectrograms
  - Quick check question: What advantages does transfer learning with CNN-VGG16 provide over training from scratch for spectrogram analysis?

## Architecture Onboarding

- Component map: Raw signal → Mel spectrogram → Harmonic-percussive decomposition → Feature combination → CNN-VGG16 → MLP → Emotion classification
- Critical path: Raw signal → Mel spectrogram → Harmonic-percussive decomposition → Feature combination → CNN-VGG16 → MLP → Emotion classification
- Design tradeoffs: High sample rate and window size improve accuracy but increase memory requirements (3GB for pkl files), limiting scalability
- Failure signatures: Poor performance on neutral/boredom emotions suggests decomposition may not capture all emotional dimensions equally
- First 3 experiments:
  1. Validate harmonic-percussive decomposition by visualizing separated components for different emotions
  2. Test accuracy sensitivity to window size and sample rate to confirm optimal configuration
  3. Compare performance with and without harmonic-percussive decomposition to isolate its contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different window sizes and sample rates affect the performance of the proposed hybrid feature map technique across various speech emotion recognition datasets?
- Basis in paper: [explicit] The paper discusses the effect of different sample rates and window sizes on the accuracy of the model, indicating that higher sample rates and larger window sizes generally improve accuracy.
- Why unresolved: The paper only provides empirical results for the Berlin EMO-DB database. The generalizability of these findings to other datasets or languages is not explored.
- What evidence would resolve it: Conducting experiments with the proposed method on multiple diverse speech emotion recognition datasets and comparing the results to understand the impact of window sizes and sample rates across different contexts.

### Open Question 2
- Question: What is the computational cost and efficiency of the proposed method compared to traditional feature extraction techniques in real-time speech emotion recognition applications?
- Basis in paper: [inferred] The paper mentions that higher sample rates and window sizes require more memory, which could limit application. However, it does not provide a detailed analysis of computational efficiency.
- Why unresolved: The paper does not discuss the real-time performance or computational efficiency of the proposed method compared to existing techniques.
- What evidence would resolve it: Benchmarking the proposed method against traditional techniques in terms of processing time, memory usage, and real-time applicability in various scenarios.

### Open Question 3
- Question: How does the proposed method perform in recognizing emotions in speech from speakers of different languages and cultural backgrounds?
- Basis in paper: [explicit] The paper mentions that emotions are universal but their understanding and interpretation can be culture-specific, suggesting a potential limitation of the acoustic-only method.
- Why unresolved: The experiments are conducted only on the Berlin EMO-DB dataset, which contains German speech. The performance on other languages or cultural contexts is not evaluated.
- What evidence would resolve it: Testing the proposed method on multilingual speech emotion recognition datasets and analyzing the performance across different languages and cultures to assess its robustness and generalizability.

## Limitations
- Poor performance on neutral and boredom emotions suggests the decomposition approach may not capture all emotional dimensions equally
- Memory-intensive preprocessing (3GB for feature maps) limits scalability to larger datasets or higher resolutions
- Reliance on a specific dataset (EMO-DB) raises questions about generalizability to other emotional speech corpora

## Confidence
- **High Confidence**: The overall methodology and reported test accuracy (92.79%) on EMO-DB are well-supported by the experimental results and comparison with baseline methods
- **Medium Confidence**: The mechanism by which harmonic-percussive decomposition improves emotion recognition is plausible but not exhaustively validated, particularly for all emotion categories
- **Medium Confidence**: The claim about optimal window size and sample rate being 2048 and 88.2kHz respectively is supported by experiments but could benefit from broader parameter exploration

## Next Checks
1. Conduct detailed error analysis to understand why neutral and boredom emotions show lower accuracy, and test whether alternative decomposition strategies or additional feature engineering can improve these specific classes
2. Evaluate the proposed method on additional speech emotion datasets (e.g., IEMOCAP, RAVDESS) to assess generalizability beyond EMO-DB and identify potential dataset-specific biases
3. Systematically vary the median filter sizes for harmonic and percussive decomposition, as well as test different combinations of window sizes and sample rates, to establish a more robust understanding of the optimal configuration and its sensitivity to these parameters