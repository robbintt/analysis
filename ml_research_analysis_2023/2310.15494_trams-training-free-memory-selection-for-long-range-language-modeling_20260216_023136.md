---
ver: rpa2
title: 'TRAMS: Training-free Memory Selection for Long-range Language Modeling'
arxiv_id: '2310.15494'
source_url: https://arxiv.org/abs/2310.15494
tags:
- memory
- attention
- selection
- trams
- transformer-xl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a training-free memory selection method (TRAMS)
  for Transformer-XL models to improve efficiency in long-range language modeling.
  The key idea is to select memory tokens based on a simple metric that estimates
  attention scores without needing queries, thus reducing computational complexity.
---

# TRAMS: Training-free Memory Selection for Long-range Language Modeling

## Quick Facts
- **arXiv ID**: 2310.15494
- **Source URL**: https://arxiv.org/abs/2310.15494
- **Reference count**: 11
- **Primary result**: Training-free memory selection that selects 50% of most relevant tokens, improving perplexity by 0.19 on WikiText-103 and bits-per-character by 0.017 on enwik8

## Executive Summary
This paper proposes TRAMS, a training-free method for memory selection in Transformer-XL models for long-range language modeling. The key innovation is selecting memory tokens based on a simple metric that estimates attention scores without requiring queries, thus reducing computational complexity. By selecting the top 50% of most relevant memory tokens using a combined metric of key norm and cosine similarity with the 1-vector, TRAMS achieves better memory utilization and improved performance metrics without additional training or parameters. The method addresses the issue of ineffective memories in Transformer-XL by focusing computational resources on tokens most likely to contribute to attention calculations.

## Method Summary
TRAMS reformulates the attention mechanism to enable memory selection without query dependency. The method computes attention scores using reformulated queries (Q') and keys (K') where all query-dependent parameters are absorbed into the key vectors. Memory tokens are then selected based on a combined metric s = cos(K',1)⋅||K'||, which ranks tokens by both their key norm and their orthogonality to the query space. This allows selection of the most relevant memory tokens (m=50% of M) before attention computation, reducing computational cost while maintaining or improving model performance. The approach is plug-and-play, requiring no additional training or parameters.

## Key Results
- 0.19 improvement in perplexity on WikiText-103
- 0.017 improvement in bits-per-character on enwik8
- Improves average attention probability by 24.25% for same memory size
- No additional training or parameters required

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Reformulated attention allows selection without query dependency
- **Mechanism**: By rewriting QK^T as (h)(mW^T_KWQ)^T, key vectors absorb all query-dependent parameters
- **Core assumption**: ||Q'|| is approximately constant across different queries
- **Evidence anchors**: [abstract] "selects tokens participating in attention calculation based on one simple metric"
- **Break condition**: If approximation ||Q'|| ≈ constant fails for certain query distributions

### Mechanism 2
- **Claim**: Memory tokens with high key norms are more likely to have high attention scores
- **Mechanism**: Since attention scores approximate ||Q'||⋅||K'||⋅cos(Q',K'), and ||Q'|| is constant, ranking by ||K'|| identifies important tokens
- **Core assumption**: Key norm distribution has sufficient variance to distinguish important tokens
- **Evidence anchors**: [section 2.4] "rankings based on key norms...produce close Spearman correlation scores"
- **Break condition**: If key norms become uniformly distributed across tokens

### Mechanism 3
- **Claim**: Combining key norm with cos(K',1) metric better identifies useful memory tokens
- **Mechanism**: The combined metric s = cos(K',1)⋅||K'|| selects tokens that are both high-norm and orthogonal to query space
- **Core assumption**: Tokens near 1 vector have lower relevance for general queries
- **Evidence anchors**: [section 2.4] "keys that are nearer to 1⃗ tend to yield a higher attention score"
- **Break condition**: If memory tokens consistently have similar angles with 1⃗ vector

## Foundational Learning

- **Concept**: Attention mechanism in transformers
  - **Why needed**: Understanding basic QK^T operation is crucial for grasping why reformulation works
  - **Quick check**: What is the time complexity of standard self-attention for sequence length N?

- **Concept**: Relative position embeddings
  - **Why needed**: TRAMS builds on Transformer-XL which uses relative position embeddings, affecting attention score calculation
  - **Quick check**: How do relative position embeddings differ from absolute positional encodings?

- **Concept**: Spearman correlation
  - **Why needed**: Used to validate that key norm ranking correlates with actual attention score ranking
  - **Quick check**: What does a Spearman correlation of 0.8 between two rankings indicate about their relationship?

## Architecture Onboarding

- **Component map**: Token generation → memory pool construction → token norm computation → sorting → top-m selection → attention computation
- **Critical path**: Token generation → memory pool construction → token norm computation → sorting → top-m selection → attention computation
- **Design tradeoffs**: Larger memory pool (M) increases selection quality but also memory overhead; smaller m reduces computation but may lose important context
- **Failure signatures**: Performance degradation when M is too large (selecting irrelevant tokens) or m is too small (missing important context)
- **First 3 experiments**:
  1. Validate that ||Q'|| is approximately constant across different query distributions
  2. Test Spearman correlation between key norm ranking and actual attention score ranking
  3. Measure performance impact of different (M,m) combinations on perplexity/bpc metrics

## Open Questions the Paper Calls Out

None explicitly stated in the paper.

## Limitations

- Limited evaluation scope: Only tested on two relatively clean datasets (WikiText-103 and enwik8) with perplexity and bits-per-character metrics
- Missing efficiency measurements: Actual FLOPs reduction and runtime measurements were not explicitly measured
- Memory pool selection ambiguity: The mechanism for selecting which tokens enter the memory pool M is not clearly specified

## Confidence

- **Medium Confidence**: Performance improvements (0.19 perplexity drop on WikiText-103 and 0.017 bpc reduction on enwik8) - well-documented but only on two datasets
- **Low Confidence**: Computational efficiency improvements - theoretically sound but actual runtime measurements not provided
- **Medium Confidence**: Reformulated attention mechanism working without query dependency - theoretical reformulation clear but empirical validation limited
- **Low Confidence**: Combined metric (cos(K',1)⋅||K'||) being superior to simple key norm ranking - shows correlation but not necessarily superior downstream performance

## Next Checks

1. **Query Norm Distribution Analysis**: Systematically measure the variance of ||Q'|| across different query distributions to empirically validate the key assumption that it's approximately constant

2. **Runtime Efficiency Benchmark**: Implement comprehensive runtime profiling comparing TRAMS against standard Transformer-XL, measuring actual FLOPs, memory bandwidth usage, and wall-clock time

3. **Robustness Testing**: Evaluate TRAMS on diverse datasets including noisy web text, code, and multilingual corpora, measuring both performance and efficiency metrics across various memory pool sizes M and selection ratios m/M