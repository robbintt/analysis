---
ver: rpa2
title: 'Spoken Word2Vec: Learning Skipgram Embeddings from Speech'
arxiv_id: '2311.09319'
source_url: https://arxiv.org/abs/2311.09319
tags:
- word
- embeddings
- features
- words
- acoustic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores the challenges of learning semantic word embeddings
  directly from speech signals, which is more difficult than from text due to the
  continuous and variable nature of speech. Previous approaches that mimic text-based
  Word2Vec algorithms fail to capture semantic information and instead encode phonetic
  features.
---

# Spoken Word2Vec: Learning Skipgram Embeddings from Speech

## Quick Facts
- arXiv ID: 2311.09319
- Source URL: https://arxiv.org/abs/2311.09319
- Reference count: 13
- Key outcome: Learning semantic word embeddings directly from speech is challenging due to phonetic correlations; end-to-end models with discretized units show promise but face segmentation/clustering hurdles

## Executive Summary
This paper investigates the feasibility of learning semantic word embeddings directly from speech signals, as opposed to text-based approaches like Word2Vec. The authors demonstrate that previous attempts to adapt text-based skipgram algorithms to speech fail to capture semantic information, instead encoding phonetic features. Through controlled experiments and theoretical analysis, they show that two-stage architectures cannot learn semantic embeddings due to phonetic correlations in input features, while end-to-end models with discretized acoustic units show potential. However, word segmentation and clustering errors significantly hamper semantic learning capabilities.

## Method Summary
The paper evaluates three main approaches to learning semantic embeddings from speech: two-stage architectures (phonetic feature learning followed by semantic training), end-to-end models, and a hypothetical perfect segmentation scenario. Methods use LibriSpeech dataset with forced alignments for word segmentation, extracting acoustic features including MFCCs, Wav2Vec2, and HuBERT. Models are trained using skipgram-like objectives and evaluated using Pearson correlation with semantic similarity datasets. Clustering of acoustic frames is performed using K-means to obtain discrete units for end-to-end training.

## Key Results
- Two-stage architectures fail to learn semantic embeddings because input phonetic embeddings are continuously correlated
- End-to-end models with discretized acoustic units can learn semantic features, with correlation improving as model scale increases
- Perfect word segmentation and clustering would yield trivial text-like embeddings, but realistic segmentation noise severely degrades semantic learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage architectures fail to learn semantic embeddings because input embeddings are phonetically correlated
- Mechanism: In the first stage, phonetic embeddings are learned that encode acoustic/phonetic content. In the second stage, these continuous, correlated embeddings are used as input for skipgram-style training. Since the input units already encode phonetic similarity, the model cannot separate semantic from phonetic information during the second stage.
- Core assumption: Phonetic embeddings are continuous vectors that inherently encode phonetic similarity between words
- Evidence anchors:
  - [abstract] "two-stage approach that first learns phonetic embeddings and then semantic embeddings cannot learn semantic features due to phonetic correlations in the input"
  - [section 4.1.2] "such a two-stage architecture is not suitable for learning semantic embeddings from subword units since the frozen embeddings are continuous and highly correlated by their orthographic/phonetic content"

### Mechanism 2
- Claim: End-to-end models with sufficient scale can learn semantic embeddings from discretized acoustic units
- Mechanism: When the model is trained end-to-end on discrete acoustic units (rather than continuous features), it can gradually learn to ignore low-level acoustic similarities and encode higher-level distributional semantics. Scaling the model increases its capacity to capture these semantic patterns.
- Core assumption: Discretization of acoustic units reduces phonetic correlations enough for semantic learning
- Evidence anchors:
  - [abstract] "end-to-end approach with sufficient scale can potentially learn semantic features from discretized acoustic units"
  - [section 4.1.2] "training the model end-to-end... results in much higher correlation with semantic features, and scaling the model results in higher correlation with the target embeddings"
  - [section 4.2.3] "After 50 epochs of training, The highest correlation with the target cosine distances was 0.38, achieved by the HuBERT-based model"

### Mechanism 3
- Claim: Perfect word segmentation and clustering assumptions lead to a trivial solution identical to text-based embeddings
- Mechanism: If word boundaries are perfectly known and words can be perfectly clustered by type, then each word occurrence can be replaced with a cluster ID. This creates a discrete corpus where each word type is a unique, identifiable unit. Standard text-based word embedding models can then be applied directly to this corpus.
- Core assumption: Perfect word segmentation and clustering are possible without supervision
- Evidence anchors:
  - [section 5.1] "Given speech utterances perfectly segmented at word boundaries... a trivial solution identical to text-based embeddings can be obtained"
  - [section 5.1] "Given oracle word boundaries and true word type clustering, the result is identical to text-based embeddings"

## Foundational Learning

- Concept: Skipgram with Negative Sampling (SGNS)
  - Why needed here: The paper's semantic embedding learning is based on the SGNS objective function, which predicts context words from target words
  - Quick check question: In SGNS, what is the difference between positive and negative examples in the training objective?

- Concept: Word segmentation and clustering
  - Why needed here: The paper emphasizes that imperfect word segmentation and clustering are the main obstacles to learning semantic embeddings from speech
  - Quick check question: What happens to semantic embedding quality when clustering noise increases from 10% to 40%?

- Concept: Acoustic feature discretization
  - Why needed here: Continuous acoustic features contain phonetic correlations that prevent semantic learning, while discretized units enable it
  - Quick check question: Why do Wav2Vec2 and HuBERT features perform better than MFCCs for semantic embedding learning?

## Architecture Onboarding

- Component map: Speech signal -> Acoustic feature extraction -> Frame clustering -> Discrete unit sequence -> End-to-end semantic model -> Semantic embeddings
- Critical path: Segment speech → Extract acoustic features → Cluster frames → Train end-to-end model → Evaluate semantic similarity
- Design tradeoffs: Two-stage vs. end-to-end architectures; continuous vs. discretized input; model scale vs. computational cost
- Failure signatures: High correlation with edit distance instead of semantic similarity; slow convergence; numerical instability in large models
- First 3 experiments:
  1. Train character-based two-stage model and measure correlation with edit distance vs. semantic similarity
  2. Train character-based end-to-end model with varying scales and measure semantic correlation improvement
  3. Train acoustic-based end-to-end model using discretized units and compare semantic correlation across different feature types (MFCC, Wav2Vec2, HuBERT)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can end-to-end architectures successfully learn semantic embeddings from speech if scaled sufficiently, and what is the minimal scale required?
- Basis in paper: [explicit] The paper shows that end-to-end models with sufficient scale and depth can potentially learn distributional semantic features from discretized acoustic units, but also notes that even with scaling, HuBERT-based models only achieved a 0.38 correlation with target embeddings after 50 epochs.
- Why unresolved: The paper does not provide conclusive evidence on whether scaling alone is sufficient, as the models did not converge to high semantic correlations even with large scales due to numerical instability and slow convergence.
- What evidence would resolve it: Training the end-to-end model with acoustic features until convergence, using even larger scales, and demonstrating high semantic correlations would resolve this question.

### Open Question 2
- Question: How do word segmentation and clustering errors impact the quality of semantic embeddings, and what is the threshold error rate beyond which semantic learning becomes infeasible?
- Basis in paper: [explicit] The paper shows that injecting 30-40% word substitution noise in text corpora drastically reduces semantic correlations, and that clustering errors in speech segmentation lead to noisy corpora that hamper semantic learning.
- Why unresolved: The paper does not quantify the exact impact of segmentation and clustering errors on semantic embedding quality or determine a specific threshold error rate.
- What evidence would resolve it: Systematically varying the error rates in word segmentation and clustering, and measuring the resulting semantic correlations, would provide insights into the impact and threshold of errors.

### Open Question 3
- Question: Are there alternative approaches to learning semantic embeddings from speech that do not rely on word segmentation and clustering, and how do they compare to the proposed methods?
- Basis in paper: [inferred] The paper highlights the challenges of word segmentation and clustering, suggesting that these steps are major bottlenecks in learning semantic embeddings from speech.
- Why unresolved: The paper does not explore alternative approaches that bypass word segmentation and clustering, such as using continuous acoustic features or different clustering strategies.
- What evidence would resolve it: Developing and evaluating alternative methods that do not rely on word segmentation and clustering, and comparing their performance to the proposed methods, would address this question.

## Limitations
- Perfect word segmentation and clustering assumptions are unrealistic and limit practical applicability
- Experimental focus on correlation metrics rather than downstream task performance
- Computational cost scaling of end-to-end approaches not thoroughly investigated

## Confidence
- High confidence: Two-stage architectures fail due to phonetic correlations in input embeddings
- Medium confidence: End-to-end models with discretized units can learn semantic features with sufficient scale
- Medium confidence: Perfect segmentation would yield text-like embeddings

## Next Checks
1. Quantify semantic embedding degradation as a function of realistic word segmentation error rates using state-of-the-art unsupervised segmentation algorithms
2. Evaluate cross-lingual transfer of semantic embeddings learned from English speech to other languages
3. Measure performance of learned semantic embeddings on downstream speech understanding tasks (intent classification, semantic parsing)