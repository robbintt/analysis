---
ver: rpa2
title: Overview of LiLAS 2020 -- Living Labs for Academic Search
arxiv_id: '2310.20387'
source_url: https://arxiv.org/abs/2310.20387
tags:
- search
- academic
- retrieval
- systems
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The LiLAS 2020 workshop introduced a Docker-based living lab infrastructure
  to enable reproducible online evaluation of academic search systems in real-world
  settings. The platform connects two academic search systems (LIVIVO for life sciences
  and GESIS Search for social sciences) with external research systems via the STELLA
  API.
---

# Overview of LiLAS 2020 -- Living Labs for Academic Search

## Quick Facts
- arXiv ID: 2310.20387
- Source URL: https://arxiv.org/abs/2310.20387
- Authors: 
- Reference count: 21
- Key outcome: Introduced Docker-based living lab infrastructure for reproducible online evaluation of academic search systems in real-world settings

## Executive Summary
The LiLAS 2020 workshop introduced a novel infrastructure for reproducible online evaluation of academic search systems through Docker-based containerization and A/B testing. The platform connects academic search systems LIVIVO (life sciences) and GESIS Search (social sciences) with external research systems via the STELLA API. Researchers can deploy experimental retrieval or recommendation algorithms as Docker containers, which are then evaluated through controlled experiments using actual user interactions. This approach addresses the challenge of moving beyond offline test collections to real-world academic search evaluation while maintaining reproducibility.

## Method Summary
The evaluation methodology centers on Docker containerization of experimental IR systems that connect to STELLA's central API infrastructure. Researchers package their retrieval or recommendation algorithms as Docker containers, which are deployed alongside production search systems. Users are randomly assigned to experimental or production versions when performing searches, with click patterns logged to generate evaluation profiles. The infrastructure supports multiple academic domains and document types through integration with LIVIVO and GESIS Search platforms, enabling cross-domain evaluation and recommendation testing.

## Key Results
- Docker-based containerization enables reproducible evaluation of experimental IR systems in production environments
- A/B testing with real user interactions provides more realistic evaluation than offline test collections
- Connecting multiple academic search platforms expands evaluation diversity and generalizability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Docker-based containerization enables reproducible evaluation of experimental IR systems in production environments
- Mechanism: Researchers package their retrieval algorithms as Docker containers that connect to STELLA's central API, which handles deployment, routing, and logging of user interactions across both experimental and production systems
- Core assumption: Containerization provides sufficient isolation while maintaining performance parity with production systems
- Evidence anchors:
  - [abstract] "Researchers can deploy their retrieval or recommendation algorithms as Docker containers"
  - [section] "STELLA's infrastructure relies on the container virtualization environment Docker [11], making it easier for STELLA to run multiple experimental systems"
  - [corpus] Weak evidence - corpus contains no direct mentions of Docker or containerization in related papers
- Break condition: If container startup overhead or resource contention degrades user experience below acceptable thresholds

### Mechanism 2
- Claim: A/B testing with real user interactions provides more realistic evaluation than offline test collections
- Mechanism: Users are randomly assigned to either production or experimental versions when performing searches, with click patterns logged to generate evaluation profiles comparing system performance
- Core assumption: User click behavior in controlled experiments reflects true relevance judgments and system preferences
- Evidence anchors:
  - [abstract] "Researchers can deploy their retrieval or recommendation algorithms as Docker containers, which are then evaluated through controlled A/B tests using actual user interactions"
  - [section] "An A/B testing, a controlled online experiment, allows to expose a percentage of real users and life-test those new or modified features"
  - [corpus] Weak evidence - corpus contains no direct mentions of A/B testing methodology in related papers
- Break condition: If user assignment randomization fails or if click data becomes sparse due to low experimental system usage

### Mechanism 3
- Claim: Connecting multiple academic search platforms expands evaluation diversity and generalizability
- Mechanism: STELLA supports integration with different academic search systems (LIVIVO for life sciences, GESIS Search for social sciences) that provide distinct document types and metadata schemas
- Core assumption: Evaluation results generalize across different academic domains and document types
- Evidence anchors:
  - [abstract] "The platform connects two academic search systems (LIVIVO for life sciences and GESIS Search for social sciences) with external research systems"
  - [section] "These systems are from the two disjunct scientific domains life sciences and social sciences and include different metadata on research articles, data sets, and many other entities"
  - [corpus] Weak evidence - corpus contains no direct mentions of multi-domain academic search evaluation
- Break condition: If domain-specific differences in user behavior or document characteristics invalidate cross-domain comparisons

## Foundational Learning

- Concept: Docker containerization and orchestration
  - Why needed here: Enables researchers to deploy experimental systems without modifying production infrastructure while maintaining reproducibility
  - Quick check question: What happens if a Docker container crashes during an active user session?

- Concept: A/B testing methodology and statistical significance
  - Why needed here: Required to properly interpret click-based evaluation results and determine if experimental systems outperform baselines
  - Quick check question: How many user interactions are needed to achieve statistical significance for a 5% performance difference?

- Concept: Academic search system architecture and metadata schemas
  - Why needed here: Understanding the structure of LIVIVO and GESIS Search is essential for developing compatible experimental systems
  - Quick check question: What are the key metadata fields that differentiate life sciences documents from social sciences documents?

## Architecture Onboarding

- Component map: STELLA central API (coordinator) -> Docker registry (container storage) -> production search systems (LIVIVO/GESIS) -> participant containers (experimental systems) -> logging infrastructure (click data collection)
- Critical path: User query -> STELLA API routing -> container selection -> experimental/production response -> click logging -> evaluation profile generation
- Design tradeoffs: Container isolation vs. performance overhead, randomization rate vs. statistical power, metadata standardization vs. domain specificity
- Failure signatures: High container startup latency, skewed user assignment distribution, missing or incomplete click logs, evaluation profile generation failures
- First 3 experiments:
  1. Deploy a simple BM25 baseline container and verify it receives queries and returns results
  2. Implement a modified ranking algorithm (e.g., add title boosting) and run an A/B test comparing it to baseline
  3. Test cross-domain recommendation by deploying a container that recommends social science datasets when users search life sciences literature

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more effective evaluation metrics for multi-layered relevance in academic search that go beyond traditional precision/recall measures?
- Basis in paper: [explicit] The paper mentions that "relevance in academic search is multi-layered" and references the Bibliometrics-enhanced Information Retrieval (BIR) workshops
- Why unresolved: Current evaluation campaigns still rely on traditional metrics while academic search requires capturing different dimensions of relevance (citation-based, topical, methodological fit, etc.)
- What evidence would resolve it: Empirical studies comparing multiple evaluation metrics on the same academic search tasks, showing which metrics better capture user satisfaction and research utility

### Open Question 2
- Question: What are the optimal strategies for implementing A/B testing in academic search platforms that balance experimental innovation with user experience?
- Basis in paper: [explicit] The paper discusses STELLA's A/B testing capabilities and mentions the need to "expose a percentage of real users" to experimental features
- Why unresolved: The paper describes the infrastructure but doesn't provide guidelines on how much traffic to divert, how long to run experiments, or how to handle cases where experimental systems perform worse than production
- What evidence would resolve it: Longitudinal studies measuring user engagement and satisfaction across different experimental designs and traffic allocation strategies

### Open Question 3
- Question: How can cross-domain recommendations (e.g., from publications to research data) be effectively implemented and evaluated in academic search systems?
- Basis in paper: [explicit] The paper describes GESIS Search's ability to provide "cross-domain recommendations" and mentions this is "still work in progress"
- Why unresolved: While the infrastructure exists, the paper acknowledges that implementing and evaluating cross-domain recommendations remains an open challenge
- What evidence would resolve it: Comparative studies showing the effectiveness of cross-domain versus same-domain recommendations in academic search contexts, with clear metrics for measuring their impact on research discovery

## Limitations
- Limited empirical validation of the evaluation infrastructure's effectiveness
- Unproven statistical framework for determining significance of performance differences
- Technical dependency risks on Docker container performance and STELLA API reliability

## Confidence

**High confidence** in the core infrastructure concept: The combination of Docker containerization, A/B testing methodology, and real-world user interaction logging represents a sound technical approach for reproducible evaluation of academic search systems.

**Medium confidence** in practical feasibility: While the technical architecture appears sound, the paper lacks detailed performance benchmarks, error handling procedures, and user experience metrics that would validate the approach in production environments.

**Low confidence** in cross-domain generalizability: The paper claims the infrastructure supports multiple academic domains but provides no empirical evidence that evaluation results transfer between life sciences and social sciences, or that the approach scales to additional domains.

## Next Checks
1. **API specification validation**: Request and test the complete STELLA API documentation, including authentication protocols, query/response formats, and rate limiting parameters to ensure experimental containers can properly integrate with production systems.

2. **Statistical power analysis**: Calculate required user interaction volumes for detecting 5% performance differences with 95% confidence across different A/B test configurations, and verify that typical academic search platforms can provide sufficient traffic.

3. **Cross-domain evaluation validation**: Deploy identical experimental algorithms on both LIVIVO and GESIS Search platforms and analyze whether performance rankings remain consistent, or if domain-specific factors create divergent results requiring domain-adapted evaluation criteria.