---
ver: rpa2
title: Improved Long-Form Speech Recognition by Jointly Modeling the Primary and Non-primary
  Speakers
arxiv_id: '2312.11123'
source_url: https://arxiv.org/abs/2312.11123
tags:
- data
- speaker
- long-form
- speech
- short
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel technique to address the long-form
  deletion problem in end-to-end ASR models. This problem occurs when models predict
  sequential blanks instead of words during transcription of lengthy audio.
---

# Improved Long-Form Speech Recognition by Jointly Modeling the Primary and Non-primary Speakers

## Quick Facts
- arXiv ID: 2312.11123
- Source URL: https://arxiv.org/abs/2312.11123
- Reference count: 0
- Primary result: Introduces <end-primary> and <end-others> tokens to significantly reduce long-form deletion errors in ASR models

## Executive Summary
This paper addresses the long-form deletion problem in end-to-end ASR models where models predict sequential blanks instead of words during transcription of lengthy audio. The proposed solution involves simultaneously modeling primary and non-primary speakers by introducing two new tokens to the ASR output vocabulary. This approach significantly alleviates the long-form deletion problem without requiring additional training data or incurring extra training or inference costs. The improved model demonstrates neutral WER and endpointing latency compared to the baseline while achieving a 55% improvement in the long-form deletion metric.

## Method Summary
The method introduces <end-primary> and <end-others> tokens to the ASR output vocabulary to model primary and non-primary speakers simultaneously. Training data is relabeled using two teacher models to add speaker-tag tokens at appropriate boundaries. The Conformer RNN-T model is trained with the relabeled data, incorporating speaker-tag tokens into the output vocabulary and prediction network conditioning. This unified approach allows a single model to serve all applications through post-processing without requiring domain-id input.

## Key Results
- 55% improvement in long-form deletion metric
- Neutral WER compared to baseline model
- Neutral endpointing latency compared to baseline model
- No additional training data or inference costs required

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing <end-primary> and <end-others> tokens reduces domain confusion during training
- Mechanism: By explicitly marking when primary vs non-primary speakers finish speaking, the model learns to distinguish between domains (Short/Dictation vs Caption) without requiring domain-id input
- Core assumption: Training-test data mismatch across domains causes the long-form deletion problem
- Evidence anchors: [abstract] "One of the culprits for long-form deletion is training-test data mismatch", [section 1] "We identify that a model trained on data from different applications can exhibit the long-form deletion problem for some domains more prominently than others"
- Break condition: If the model cannot reliably identify primary speakers, the token insertion fails and the benefit disappears

### Mechanism 2
- Claim: Speaker-tag tokens allow unified output format across all domains
- Mechanism: By transcribing all speakers and outputting speaker-tag tokens, a single model can serve all applications through post-processing
- Core assumption: Different applications require different speaker transcription policies
- Evidence anchors: [section 2.1] "Short and Dictation domains, the ASR output...can be processed to remove non-primary speakers transcript. And for the Caption domain, the ASR output can be processed to contain all speakers' transcript", [section 1] "Different application domains expect different types of speakers to be transcribed"
- Break condition: If post-processing becomes too complex or error-prone, the unified approach loses its advantage

### Mechanism 3
- Claim: Training with speaker-tags removes confusion about whose speech to transcribe
- Mechanism: Explicit modeling of primary vs non-primary speakers eliminates the conflicting training goals across domains
- Core assumption: Conflicting training goals from different domains cause confusion during model training
- Evidence anchors: [section 1] "These conflicting training goals ultimately limit the model from learning cross-domain capabilities, thus leading to long-form deletion errors", [section 4] "By explicitly modeling the primary and non-primary speakers in E1, we removed the confusion during the training"
- Break condition: If the relabeling process introduces errors, the benefit of removing confusion may be negated

## Foundational Learning

- Concept: End-to-End ASR with RNN-T architecture
  - Why needed here: The proposed method modifies the output vocabulary of an RNN-T model
  - Quick check question: What are the three main components of an RNN-T model and their roles?

- Concept: Speaker diarization and attribution
  - Why needed here: Understanding existing approaches helps position this work relative to prior art
  - Quick check question: How does speaker diarization differ from speaker attribution in ASR systems?

- Concept: Data relabeling techniques
  - Why needed here: The proposed method relies on automatically adding speaker-tags to training data
  - Quick check question: What challenges arise when automatically adding speaker-tags to existing transcripts?

## Architecture Onboarding

- Component map: Acoustic input → encoder → prediction network (conditioned on wordpieces + speaker-tags) → joint network → output tokens
- Critical path: Acoustic input → encoder → prediction network (conditioned on wordpieces + speaker-tags) → joint network → output tokens
- Design tradeoffs: Adding speaker-tags increases model complexity but eliminates need for domain-id input
- Failure signatures: Model emits same speaker-tag repeatedly, or fails to emit speaker-tags in alternating order
- First 3 experiments:
  1. Train baseline model without speaker-tags and measure long-form deletion metric
  2. Implement speaker-tag relabeling and train model with speaker-tags
  3. Compare WER and long-form deletion performance between baseline and speaker-tag models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for handling overlapping speech by both primary and non-primary speakers in the proposed model?
- Basis in paper: [explicit] The paper mentions that overlapping speech is a potential limitation and suggests it as an area of research for future work
- Why unresolved: The paper does not provide a concrete solution for handling overlapping speech, indicating that this remains an open challenge in the field
- What evidence would resolve it: Developing and testing a method that effectively models overlapping speech, possibly by introducing additional tokens or modifying the existing model architecture, and evaluating its impact on long-form deletion and overall ASR performance

### Open Question 2
- Question: How does the performance of the proposed speaker-tag model compare to other advanced techniques for long-form speech recognition, such as those using context-expanded transformers or monotonic segmental attention?
- Basis in paper: [inferred] The paper discusses the long-form deletion problem and introduces a novel technique, but does not compare its performance to other advanced techniques mentioned in the literature
- Why unresolved: There is a lack of direct comparison with other state-of-the-art methods, making it difficult to assess the relative effectiveness of the proposed approach
- What evidence would resolve it: Conducting experiments that compare the speaker-tag model's performance against other advanced techniques on the same benchmarks and metrics, such as WER, long-form deletion, and endpointer latency

### Open Question 3
- Question: Can the speaker-tag approach be extended to handle more than two groups of speakers, such as distinguishing between multiple non-primary speakers in a multi-party conversation?
- Basis in paper: [explicit] The paper introduces two tokens for primary and non-primary speakers but does not explore the possibility of handling more speaker groups
- Why unresolved: The current model is limited to two speaker groups, which may not be sufficient for complex multi-party conversations where distinguishing between multiple non-primary speakers could be beneficial
- What evidence would resolve it: Experimenting with models that introduce additional tokens for multiple non-primary speaker groups and evaluating their impact on ASR performance, particularly in scenarios with complex multi-party conversations

### Open Question 4
- Question: What is the impact of the speaker-tag model on the scalability and computational efficiency of the ASR system, especially when dealing with very long audio inputs?
- Basis in paper: [inferred] While the paper mentions that the model does not incur additional training or inference costs, it does not provide detailed analysis on scalability and computational efficiency with very long audio inputs
- Why unresolved: There is a lack of empirical data on how the model performs in terms of scalability and efficiency when processing extremely long audio files, which is crucial for real-world applications
- What evidence would resolve it: Conducting experiments to measure the computational efficiency and scalability of the speaker-tag model with varying lengths of audio inputs, comparing it to baseline models, and analyzing the trade-offs between performance and resource utilization

## Limitations

- The approach relies heavily on accurate speaker diarization and attribution for relabeling training data, with no addressing of potential errors in this critical preprocessing step
- The evaluation focuses primarily on WER and a specific long-form deletion metric without examining broader generalization capabilities or robustness to noise and different acoustic conditions
- The unified output format approach assumes that post-processing can cleanly separate speaker transcripts for different applications, which may not hold in real-world scenarios with overlapping speech or background noise

## Confidence

- High Confidence: The paper's core claim that introducing <end-primary> and <end-others> tokens to the ASR output vocabulary significantly reduces long-form deletion errors is well-supported by the experimental results
- Medium Confidence: The mechanism explaining that training-test data mismatch across domains causes the long-form deletion problem is plausible but lacks direct experimental validation
- Low Confidence: The assertion that the proposed method "does not require any additional training data" may be misleading, as the relabeling process effectively creates new training examples

## Next Checks

1. **Ablation Study on Domain Confusion**: Create controlled experiments where the model is trained on homogeneous domain data versus mixed domain data to quantify the specific contribution of domain confusion to the long-form deletion problem

2. **Error Analysis of Relabeling Process**: Conduct a detailed analysis of the relabeling errors introduced by the teacher models and speaker diarization system, measuring how these errors propagate to the final model performance

3. **Real-World Deployment Testing**: Evaluate the model on diverse real-world audio scenarios beyond the curated test sets, including noisy environments, overlapping speech, and spontaneous conversation to validate the unified output format approach maintains its advantages in practical scenarios