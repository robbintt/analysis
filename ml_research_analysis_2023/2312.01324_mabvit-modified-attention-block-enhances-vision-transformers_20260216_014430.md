---
ver: rpa2
title: MABViT -- Modified Attention Block Enhances Vision Transformers
arxiv_id: '2312.01324'
source_url: https://arxiv.org/abs/2312.01324
tags:
- standard
- attention
- parallel
- transformer
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the performance gap between parallel and standard
  transformer architectures in vision tasks, which arises due to representation collapse
  in deeper layers. The authors propose integrating a GLU-based activation function
  into the attention block's Value tensor, creating the MABViT architecture.
---

# MABViT -- Modified Attention Block Enhances Vision Transformers

## Quick Facts
- arXiv ID: 2312.01324
- Source URL: https://arxiv.org/abs/2312.01324
- Reference count: 3
- Key outcome: MABViT improves vision transformer accuracy while reducing parameters by applying GLU activation to the attention block's Value tensor

## Executive Summary
This paper addresses the performance gap between parallel and standard transformer architectures in vision tasks, which arises due to representation collapse in deeper layers. The authors propose integrating a GLU-based activation function into the attention block's Value tensor, creating the MABViT architecture. This modification partially resolves representation collapse by giving more significance to the attention output. Experiments on ImageNet-1K show that the parameter-reduced GLU variant (PR-GLU) of MABViT-S/16 achieves 79.44% accuracy with 20.4M parameters, outperforming the standard ViT-S/16 (78.832% with 22.2M parameters) by 0.6% while using fewer parameters.

## Method Summary
The authors modified the standard ViT attention block by applying GLU activation to the Value tensor, creating the MABViT architecture. To compensate for the additional parameters introduced by GLU, they reduced the MLP dimension from 4× to 3× the embedding size, creating parameter-reduced GLU (PR-GLU) variants. The models were trained on ImageNet-1K using AugReg methodology for 300 epochs with a batch size of 4096 (2048 for B/16). The approach addresses representation collapse in deeper layers by amplifying the attention output's contribution through non-linear transformation.

## Key Results
- MABViT-S/16 with PR-GLU achieves 79.44% accuracy with 20.4M parameters, outperforming standard ViT-S/16 (78.832% with 22.2M parameters) by 0.6% while using fewer parameters
- MABViT-M/16 variants surpass ViT-B/16 (80.174%) with only half the parameters (39.1M vs 86M)
- MABViT demonstrates faster convergence and better performance in deeper architectures compared to standard ViT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GLU activation on the Value tensor partially mitigates representation collapse by amplifying the contribution of attention outputs in deeper layers.
- Mechanism: In standard ViT, as layers deepen, the residual term dominates, making the attention output negligible. By applying GLU to the Value tensor, the attention output is non-linearly transformed, increasing its relative magnitude and importance in the residual sum.
- Core assumption: The representation collapse is primarily driven by the linear growth of the residual term overpowering the attention block output.
- Evidence anchors:
  - [abstract] "We hypothesize that representation collapse is the underlying cause for the comparable performance of parallel and standard transformer architectures at scale."
  - [section] "In Pre-LN Transformers, theory suggests that incorporating the Multi-Head Attention output into the residual before feeding it to the MLP block becomes redundant in deeper layers."

### Mechanism 2
- Claim: The parallel configuration alone causes performance degradation in vision tasks due to representation collapse, but combining it with GLU activation recovers and exceeds baseline performance.
- Mechanism: Parallel architectures process attention and MLP in parallel, which can worsen representation collapse in vision models due to their smaller scale compared to LLMs. Adding GLU activation to the Value tensor increases the nonlinearity of the attention output, partially counteracting this collapse.
- Core assumption: Vision models are more susceptible to representation collapse than LLMs due to their smaller parameter counts and architectural differences.
- Evidence anchors:
  - [abstract] "when the MLP and attention block were run in parallel for the image classification task, we observed a noticeable decline in performance."
  - [section] "We hypothesize that representation collapse is the underlying cause for the comparable performance of parallel and standard transformer architectures at scale."

### Mechanism 3
- Claim: Reducing the MLP dimension in GLU-based variants compensates for the extra parameters introduced by GLU, maintaining efficiency while improving accuracy.
- Mechanism: GLU activation increases the number of parameters by requiring additional weight matrices. By reducing the MLP dimension (e.g., from 4x to 3x embedding size), the total parameter count is kept lower than the standard ViT, while still benefiting from the GLU-induced performance gain.
- Core assumption: The performance gain from GLU activation outweighs the performance loss from reducing MLP capacity.
- Evidence anchors:
  - [section] "To counterbalance the additional parameters introduced by using the GLU activation, we reduced the number of parameters in the MLP block."
  - [section] "It also supersedes the B/16 variant while using only half the parameters."

## Foundational Learning

- Concept: Representation collapse in Pre-LN Transformers
  - Why needed here: Understanding why deeper layers in ViT suffer from diminishing attention output is critical to grasping why GLU activation helps.
  - Quick check question: What happens to the residual term as the number of layers increases in a Pre-LN Transformer?

- Concept: Gated Linear Units (GLUs) and their variants
  - Why needed here: GLU is the core activation function applied to the Value tensor; knowing its structure and behavior explains its effectiveness.
  - Quick check question: How does GLU differ from standard activations like ReLU or GELU in terms of gating behavior?

- Concept: Vision Transformer architecture and Patch Embedding
  - Why needed here: The ViT structure and how tokens are formed from image patches are foundational to understanding where and how MABViT modifies the attention block.
  - Quick check question: What is the role of the Patch Embedding layer in ViT, and how does it transform the input image?

## Architecture Onboarding

- Component map:
  Input image → Patch Embedding → Sequence of tokens → MABViT block (LN → Multi-Head Attention with GLU(Value) → Residual add → LN → MLP → Residual add) → Output class token or feature map

- Critical path:
  Tokenization → Multi-Head Attention with GLU(Value) → Residual addition → MLP → Final classification

- Design tradeoffs:
  GLU increases parameters but improves performance; reducing MLP size offsets this but risks underfitting. Parallel vs. sequential attention+MLP: Parallel is faster but can worsen collapse unless GLU is used. Depth vs. width: Deeper models suffer more from collapse; MABViT helps but doesn't fully solve it.

- Failure signatures:
  Overfitting on B/16 variants (as seen in experiments). No improvement or degradation if GLU is applied incorrectly or if MLP reduction is too aggressive. Instability if GELU is used instead of GLU in very deep models.

- First 3 experiments:
  1. Replace standard ViT attention block with MABViT attention (GLU on Value) on S/16, compare accuracy and parameter count.
  2. Train parallel ViT vs. parallel MABViT on Ti/16, measure convergence speed and final accuracy.
  3. Test M/16 architecture with PR-GLU and standard GLU variants, verify parameter efficiency and performance gain over B/16.

## Open Questions the Paper Calls Out
- What is the exact mechanism behind representation collapse in parallel transformer architectures, and how does the proposed GLU activation partially address it?
- How does the performance of MABViT variants scale with model depth beyond the tested 18 layers, and what are the limits of this approach?
- How does the MABViT architecture perform on tasks beyond image classification, such as object detection or semantic segmentation?

## Limitations
- The analysis focuses primarily on vision transformer architectures up to B/16 scale, leaving scaling behavior for larger architectures unverified.
- All experiments are conducted on ImageNet-1K, with no evidence for effectiveness on other vision tasks or datasets.
- The paper does not benchmark against other state-of-the-art attention modifications, limiting assessment of the approach's uniqueness.

## Confidence
- High Confidence: The experimental results on ImageNet-1K (accuracy and parameter efficiency comparisons) are well-supported by the data provided.
- Medium Confidence: The mechanism by which GLU activation mitigates representation collapse is plausible but not rigorously proven; alternative explanations are not ruled out.
- Low Confidence: Claims about the effectiveness of the approach at scales beyond B/16 or on tasks beyond image classification are not substantiated.

## Next Checks
1. Train MABViT variants on larger architectures (e.g., L/16 or higher) and compare performance with standard ViT to verify if the GLU-based mitigation of representation collapse scales effectively.
2. Evaluate MABViT on non-classification vision tasks (e.g., object detection on COCO, semantic segmentation on ADE20K) to assess the method's broader applicability.
3. Compare MABViT against other attention enhancement methods (e.g., FlashAttention, Performer, or other GLU variants) on the same ImageNet-1K benchmarks to determine the relative contribution of the proposed modifications.