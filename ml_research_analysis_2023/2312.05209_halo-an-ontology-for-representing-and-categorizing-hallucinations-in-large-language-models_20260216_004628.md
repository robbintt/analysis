---
ver: rpa2
title: 'HALO: An Ontology for Representing and Categorizing Hallucinations in Large
  Language Models'
arxiv_id: '2312.05209'
source_url: https://arxiv.org/abs/2312.05209
tags:
- halo
- hallucination
- hallucinations
- ontology
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents HALO, a formal ontology for representing and
  categorizing hallucinations in large language models (LLMs). The authors identify
  a gap in existing research: while hallucinations in LLMs are increasingly documented,
  there is no standardized vocabulary or model for describing them at a fine-grained
  level.'
---

# HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models

## Quick Facts
- arXiv ID: 2312.05209
- Source URL: https://arxiv.org/abs/2312.05209
- Reference count: 39
- Primary result: Presents HALO, an OWL-based ontology for systematically categorizing and analyzing hallucinations in LLMs

## Executive Summary
This paper introduces HALO, a formal ontology designed to address the lack of standardized vocabulary for describing hallucinations in large language models. The ontology provides a structured framework for categorizing six distinct types of hallucinations, along with comprehensive metadata tracking for provenance and experimental analysis. HALO is implemented as an OWL ontology with two main modules and is evaluated using a dataset of 40 hallucination instances, demonstrating its ability to support systematic analysis through SPARQL queries.

## Method Summary
The authors developed HALO as an OWL-based ontology with two modules: a Hallucination Module for categorizing hallucination types (Factuality and Faithfulness hallucinations with subcategories) and a Metadata Module for capturing experimental context. The ontology was evaluated by modeling a dataset of 40 hallucination instances collected from diverse web sources and demonstrating its utility in answering competency questions through SPARQL queries. The implementation follows established ontology design principles and integrates with external vocabularies like FOAF and schema.org.

## Key Results
- HALO successfully models a dataset of 40 hallucination instances with comprehensive metadata
- The ontology supports answering competency questions through SPARQL queries about LLM hallucination patterns
- The open-source implementation enables community contribution and extensibility for future research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HALO's ontology structure enables systematic classification of hallucinations into distinct types with hierarchical relationships.
- Mechanism: The ontology defines a clear taxonomy with two main branches (Factuality Hallucinations and Faithfulness Hallucinations) and further subcategories, allowing researchers to categorize hallucinations consistently across different studies and sources.
- Core assumption: Hallucinations can be meaningfully grouped into a finite set of types that capture the essential characteristics of the phenomenon.
- Evidence anchors:
  - [abstract] "formal, extensible ontology written in OWL that currently offers support for six different types of hallucinations"
  - [section] "two main hallucination categories (with subcategories) into HALO"
- Break condition: If new hallucination types emerge that don't fit into the existing taxonomy, the classification system would need significant revision.

### Mechanism 2
- Claim: HALO's metadata module enables comprehensive experimental tracking and provenance documentation.
- Mechanism: The Metadata Module captures detailed information about prompts, responses, LLMs used, sources, and dates, creating a complete experimental record that supports reproducibility and comparative analysis.
- Core assumption: Comprehensive metadata is essential for understanding the context and conditions under which hallucinations occur.
- Evidence anchors:
  - [abstract] "support for provenance and experimental metadata"
  - [section] "four primary concepts: GenerativeAI, LargeLanguageModel, LLMsPrompt, and LLMsAnswers"
- Break condition: If the metadata schema doesn't capture critical experimental variables, the analysis capability would be compromised.

### Mechanism 3
- Claim: HALO enables systematic analysis through competency questions expressed as SPARQL queries.
- Mechanism: The ontology provides a formal structure that allows researchers to translate analytical questions into executable SPARQL queries, enabling automated analysis of hallucination patterns across different LLMs and scenarios.
- Core assumption: The ontology structure maps naturally to queryable relationships that can answer meaningful research questions.
- Evidence anchors:
  - [abstract] "show that HALO can be successfully used to model this dataset and answer competency questions"
  - [section] "we devised a set of competency questions (CQs), shown in Table 3"
- Break condition: If the ontology doesn't capture relationships needed for key research questions, the SPARQL query capability would be limited.

## Foundational Learning

- Concept: Ontology design principles (OWL, taxonomy, metadata modeling)
  - Why needed here: Understanding how ontologies structure knowledge is essential for comprehending HALO's architecture and why it's structured the way it is
  - Quick check question: What's the difference between a class hierarchy and a property hierarchy in an OWL ontology?

- Concept: SPARQL query language and semantic web technologies
  - Why needed here: The evaluation section demonstrates how competency questions are translated into SPARQL queries, which is central to HALO's utility
  - Quick check question: How does a SPARQL query differ from a traditional SQL query when working with ontological data?

- Concept: Large language model evaluation methodologies
  - Why needed here: Understanding how hallucinations are identified and categorized is crucial for interpreting the ontology's structure and the dataset it models
  - Quick check question: What distinguishes a hallucination from a simple error or limitation in LLM performance?

## Architecture Onboarding

- Component map:
  - Hallucination Module: Contains the taxonomy of hallucination types (Factuality and Faithfulness with subcategories)
  - Metadata Module: Captures experimental context (GenerativeAI, LargeLanguageModel, LLMsPrompt, LLMsAnswer)
  - External Vocabulary Integration: Links to FOAF, schema.org, and other vocabularies
  - Competency Questions Framework: Defines analytical queries for SPARQL execution

- Critical path:
  1. Identify hallucination instance → 2. Classify using Hallucination Module → 3. Capture metadata using Metadata Module → 4. Link to external vocabularies → 5. Execute competency questions as SPARQL queries

- Design tradeoffs:
  - Generality vs. specificity: Broader categories make the ontology more applicable but may miss nuanced distinctions
  - Complexity vs. usability: More detailed taxonomy enables richer analysis but increases learning curve
  - Open vs. closed system: Public licensing enables community contribution but may lead to fragmentation

- Failure signatures:
  - Missing categories: New hallucination types don't fit existing taxonomy
  - Metadata gaps: Critical experimental variables aren't captured
  - Query limitations: SPARQL queries can't express important analytical questions
  - Integration issues: External vocabulary links break or don't align

- First 3 experiments:
  1. Model a simple hallucination instance using both modules and verify it can be queried
  2. Execute a basic competency question (e.g., "Which LLM had the most hallucinations?") using the sample dataset
  3. Extend the ontology with a new hallucination type and verify it integrates correctly with existing structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can HALO be extended to model hallucinations in non-language-based generative AI systems (e.g., image, audio, or video generation)?
- Basis in paper: [explicit] The authors mention this as a future direction, noting that HALO could be supplemented with competency queries and documented hallucinations from other GenAI models beyond LLMs.
- Why unresolved: The current ontology is specifically designed for language-based hallucinations, and the paper does not explore how it might adapt to other modalities.
- What evidence would resolve it: A demonstration of HALO modeling hallucination instances from non-text generative AI, along with an updated ontology schema accommodating new hallucination categories.

### Open Question 2
- Question: Can HALO effectively support longitudinal studies to track the evolution of hallucination patterns in LLMs over time?
- Basis in paper: [inferred] The authors suggest that longitudinal data could help determine if LLMs are improving on certain hallucination categories, but they lack such data due to the recency of the models.
- Why unresolved: The current evaluation uses a static dataset without temporal comparisons, so the ontology’s utility for tracking changes over time is untested.
- What evidence would resolve it: A study using HALO to analyze hallucination data collected at multiple time points, showing trends or improvements in specific hallucination types.

### Open Question 3
- Question: What are the most effective methods for automatically detecting and classifying hallucinations in LLMs, and how can HALO integrate these methods?
- Basis in paper: [inferred] The authors focus on modeling and categorizing hallucinations but do not address detection or classification methods, which are critical for populating HALO with accurate data.
- Why unresolved: The paper does not propose or evaluate automated detection mechanisms, leaving a gap between hallucination identification and ontology modeling.
- What evidence would resolve it: A validated automated detection system that outputs hallucination instances compatible with HALO’s schema, enabling large-scale data collection and analysis.

## Limitations

- The evaluation relies on a relatively small dataset of 40 hallucination instances, which may not capture the full diversity of hallucination types
- The paper does not address how the ontology would handle hallucinations that span multiple categories or exhibit characteristics of different types simultaneously
- While the ontology claims extensibility, there is no demonstration of how easily new hallucination types can be integrated or how conflicts between different classification schemes would be resolved

## Confidence

- **High Confidence**: The technical implementation of HALO as an OWL ontology with two distinct modules (Hallucination and Metadata) is well-specified and follows established ontology design principles. The basic structure and purpose are clearly defined.
- **Medium Confidence**: The claim that HALO can answer competency questions through SPARQL queries is supported by the evaluation, but the limited dataset size means the full utility for large-scale analysis remains unproven.
- **Low Confidence**: The assertion that HALO provides a comprehensive vocabulary for all hallucination types in LLMs is overstated given the limited evaluation scope and the rapidly evolving nature of LLM capabilities.

## Next Checks

1. **Scalability Test**: Apply HALO to a dataset of at least 500 hallucination instances from multiple LLM architectures to evaluate whether the ontology structure remains useful at scale and whether new hallucination types emerge that don't fit the existing taxonomy.

2. **Interoperability Assessment**: Test HALO's integration with existing ontologies (e.g., schema.org, FOAF) by attempting to map hallucination instances to real-world entities and verifying that the semantic relationships are preserved across vocabularies.

3. **Usability Study**: Conduct a user study with researchers unfamiliar with HALO to measure the time required to correctly classify new hallucination instances and identify any structural elements that cause confusion or misclassification.