---
ver: rpa2
title: Concrete Subspace Learning based Interference Elimination for Multi-task Model
  Fusion
arxiv_id: '2312.06173'
source_url: https://arxiv.org/abs/2312.06173
tags:
- concrete
- task
- tasks
- adamerging
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of interference among task-specific
  models when merging them into a multi-task model, which can negatively impact the
  performance of the unified model. The proposed method, CONtinuous relaxation of
  disCRETE (Concrete) subspace learning, identifies a common low-dimensional subspace
  across tasks and leverages shared information within it to mitigate interference.
---

# Concrete Subspace Learning based Interference Elimination for Multi-task Model Fusion

## Quick Facts
- arXiv ID: 2312.06173
- Source URL: https://arxiv.org/abs/2312.06173
- Reference count: 40
- Key outcome: Concrete subspace learning method identifies a common low-dimensional subspace across tasks to mitigate interference when merging task-specific models into a multi-task model, improving performance on both vision and language domains.

## Executive Summary
This paper addresses interference among task-specific models when merging them into a multi-task model, which can negatively impact the unified model's performance. The authors propose Concrete subspace learning, which identifies a common low-dimensional subspace across tasks and leverages shared information within it to mitigate interference. The method models the problem as a bi-level optimization problem and introduces a meta-learning framework to find the Concrete subspace mask through gradient-based techniques. Extensive experiments on both vision and language domains demonstrate the effectiveness of their approach in improving the performance of multi-task model fusion.

## Method Summary
The method fine-tunes pre-trained models (CLIP for vision, Flan-T5 for language) on individual downstream tasks, then computes task vectors as the difference between fine-tuned and pre-trained parameters. A shared Concrete mask is meta-learned to identify a common low-dimensional subspace across tasks. This mask filters and rescales task vectors before model merging using either Task Arithmetic or AdaMerging. The process involves upper-level optimization of mask logits to maximize merged model performance across all tasks, while the inner level finds optimal merging parameters. Test-time adaptation using entropy minimization on unlabeled data further refines the merged model for specific target tasks.

## Key Results
- Concrete Task Arithmetic and Concrete AdaMerging outperform baseline methods (Weight Averaging, Task Arithmetic, Ties-Merging, AdaMerging) on both vision and language tasks
- The method successfully identifies a common low-dimensional subspace that captures beneficial shared information across tasks
- Test-time adaptation using entropy minimization improves performance on unlabeled target data

## Why This Works (Mechanism)

### Mechanism 1
The Concrete mask identifies a common low-dimensional subspace across tasks, reducing interference by focusing model merging on shared information. By using a continuous relaxation of a Bernoulli distribution, the method learns a shared mask that filters task vectors to isolate parameters most beneficial across tasks while discarding conflicting ones. The rescaled task vectors preserve task-specific information within the shared subspace. This assumes task vectors from fine-tuned models share a common low-dimensional subspace where parameters contribute positively across tasks.

### Mechanism 2
Meta-learning the Concrete mask through a bi-level optimization framework finds masks that generalize across tasks. The upper-level objective maximizes merged model performance across all tasks by optimizing mask logits, while the inner level finds optimal merging parameters. This creates a shared mask beneficial for multiple tasks rather than task-specific masks. The method assumes a single shared mask can capture beneficial patterns across all tasks rather than requiring task-specific masks.

### Mechanism 3
Test-time adaptation using entropy minimization on unlabeled data refines the merged model for specific target tasks. After initial merging with the Concrete mask, the model uses entropy loss on unlabeled test data to make confident predictions, adapting to target distributions without requiring labeled data. This assumes entropy minimization on unlabeled data can effectively adapt the model to new tasks without overfitting.

## Foundational Learning

- Concept: Gumbel-Softmax relaxation for discrete random variables
  - Why needed here: Enables differentiable sampling of binary masks for gradient-based optimization
  - Quick check question: How does the Concrete distribution approximate a Bernoulli distribution as temperature λ approaches zero?

- Concept: Bi-level optimization and meta-learning
  - Why needed here: Allows learning parameters (mask) that are beneficial across multiple tasks rather than task-specific
  - Quick check question: What is the difference between the upper-level and inner-level objectives in the bi-level optimization?

- Concept: Test-time adaptation and entropy minimization
  - Why needed here: Enables adaptation to unlabeled target data without requiring labeled examples
  - Quick check question: Why does minimizing entropy on unlabeled data help with test-time adaptation?

## Architecture Onboarding

- Component map: Pre-trained model → Task-specific fine-tuning → Task vectors (τi = θi - θ0) → Concrete mask learning → Masked/rescaled task vectors → Model merging (Task Arithmetic/AdaMerging) → Test-time adaptation
- Critical path: Concrete mask learning → Model merging → Test-time adaptation
- Design tradeoffs: Shared mask vs task-specific masks (simplicity vs performance), temperature parameter λ (discreteness vs gradient flow), meta-learning steps vs adaptation steps (generalization vs specialization)
- Failure signatures: Mask becoming all zeros (no shared structure), performance worse than simple averaging (interference not resolved), convergence to local optima (insufficient exploration)
- First 3 experiments:
  1. Implement Concrete mask sampling and verify it approximates Bernoulli as λ→0
  2. Run meta-learning with synthetic task vectors to observe mask convergence
  3. Compare merged model performance with/without Concrete mask on a small task set

## Open Questions the Paper Calls Out

### Open Question 1
How does the Concrete Subspace Learning method perform when applied to models with different architectures (e.g., convolutional vs. transformer-based)? The paper demonstrates effectiveness on CLIP-ViT and Flan-T5 models but does not explore performance across different architectures. The experiments focus on vision transformers and language transformers, leaving the method's generalizability to other architectures untested.

### Open Question 2
What is the impact of the temperature parameter λ on the Concrete mask's performance, and is there an optimal range for different tasks or model sizes? The paper mentions that λ controls the "softness" of decisions in the Concrete distribution but does not explore its impact on performance. The paper uses a fixed λ but does not analyze how varying it affects the method's effectiveness.

### Open Question 3
How does the Concrete method scale with the number of tasks, and does its performance degrade when merging a large number of task-specific models? The paper tests on 8 tasks but does not explore scalability to larger task sets. The experiments are limited to a small number of tasks, leaving the method's scalability untested.

### Open Question 4
Can the Concrete Subspace Learning method be extended to handle heterogeneous tasks (e.g., combining vision and language tasks in a single model)? The paper focuses on homogeneous task sets (all vision or all language) and does not explore cross-modal fusion. The experiments are limited to single-domain tasks, leaving the method's applicability to heterogeneous tasks untested.

## Limitations

- The effectiveness relies heavily on the assumption that task vectors share common structure, which may not hold for orthogonal or highly dissimilar tasks
- The method's scalability to large numbers of tasks and heterogeneous task types remains untested
- Test-time adaptation effectiveness on truly unseen data distributions is not comprehensively evaluated

## Confidence

- **High confidence**: The bi-level optimization framework for mask learning and the basic Concrete distribution sampling are well-established techniques with clear implementation paths
- **Medium confidence**: The claim that a single shared mask can effectively capture beneficial patterns across all tasks, given the complexity of task relationships
- **Low confidence**: The effectiveness of entropy minimization for test-time adaptation on truly unseen data distributions

## Next Checks

1. **Temperature sensitivity analysis**: Systematically vary λ and analyze its impact on mask discreteness, gradient flow, and final merged model performance to identify optimal temperature settings

2. **Task relationship analysis**: Conduct ablation studies with task subsets of varying similarity to quantify how task relationships affect the Concrete mask's ability to identify beneficial shared subspaces

3. **Generalization stress test**: Evaluate test-time adaptation performance on data distributions deliberately shifted from training data to assess robustness beyond in-distribution adaptation