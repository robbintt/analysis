---
ver: rpa2
title: 'The Confidence-Competence Gap in Large Language Models: A Cognitive Study'
arxiv_id: '2309.16145'
source_url: https://arxiv.org/abs/2309.16145
tags:
- confidence
- llms
- language
- high
- levels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the alignment between Large Language Models'
  (LLMs) self-assessed confidence and actual performance, exploring potential parallels
  with the Dunning-Kruger effect. The research methodology involved testing seven
  different LLMs across diverse categories and difficulty levels, collecting both
  pre- and post-response confidence scores alongside correctness metrics.
---

# The Confidence-Competence Gap in Large Language Models: A Cognitive Study

## Quick Facts
- arXiv ID: 2309.16145
- Source URL: https://arxiv.org/abs/2309.16145
- Reference count: 40
- Primary result: Study reveals systematic miscalibration between LLM confidence and correctness, showing parallels with human Dunning-Kruger effect

## Executive Summary
This study investigates the alignment between Large Language Models' self-assessed confidence and actual performance across diverse problem categories and difficulty levels. Testing seven different LLMs, the research reveals intriguing patterns where models demonstrate high confidence even when providing incorrect answers, reminiscent of human cognitive biases. The findings suggest varying confidence calibration strategies across different model architectures, with some maintaining stable confidence while others show significant fluctuations based on problem complexity.

## Method Summary
The study tested seven LLMs across four categories (TruthfulQA, Mathematical Reasoning, LSAT Reasoning) with questions of varying difficulty levels (1-5). Using a three-tier prompting approach (Simple, Chain of Thought, Tree of Thoughts), researchers collected both pre- and post-response confidence scores alongside correctness metrics. The methodology involved collecting absolute (A1/A2) and relative (R1/R2) confidence scores, then analyzing calibration patterns through statistical metrics including the Closeness metric (|A1-A2| ≤ 1).

## Key Results
- GPT-4 shows high confidence even when incorrect (15 high-confidence wrong answers)
- LLaMA models display varying confidence patterns across categories, excelling in math/LSAT but lowering in Q&A
- Confidence stability varies by model architecture, with LLaMA-13B showing higher variance in pre-response confidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit systematic miscalibration between confidence and correctness that parallels human Dunning-Kruger effects.
- Mechanism: The study tracks pre-response (A1, R1) and post-response (A2, R2) confidence scores, comparing them against correctness outcomes to reveal whether high confidence correlates with accuracy.
- Core assumption: Confidence scores are meaningful self-assessment indicators and LLMs can introspect and report their own performance reliably.
- Evidence anchors: [abstract] "Our findings reveal intriguing instances where models demonstrate high confidence even when they answer incorrectly. This is reminiscent of the Dunning-Kruger effect observed in human psychology."

### Mechanism 2
- Claim: Confidence stability varies by LLM architecture and problem complexity, suggesting different calibration strategies.
- Mechanism: By segmenting data across problem levels and categories, the study observes whether confidence drops consistently as difficulty rises (A1, R1) or whether post-response recalibration (A2, R2) differs.
- Core assumption: Different model families have inherent differences in how they process difficulty cues and update confidence after seeing the answer.
- Evidence anchors: [section] "LLaMA-13B has a standard deviation of 2.06 for A1, which is higher, While series LLaMA-70B and LLaMA-7B are in the range of 1.12 and 1.21, respectively."

### Mechanism 3
- Claim: Category-specific performance and confidence patterns suggest LLMs may have specialized competence domains.
- Mechanism: Comparing average confidence across LSAT, Mathematical, and Truthful Q&A categories reveals whether models maintain high confidence uniformly or drop in unfamiliar domains.
- Core assumption: LLMs' pretraining data distributions bias their confidence in domain-specific tasks, creating "expertise illusions."
- Evidence anchors: [section] "Models like LLaMA-70B shows high confidence score for LSAT reasoning and Mathematical Reasoning; however, they possess lower confidence score in the Truthful Q&A category."

## Foundational Learning

- Concept: Dunning-Kruger effect
  - Why needed here: Provides the cognitive psychology framework for interpreting LLM confidence-miscalibration patterns.
  - Quick check question: What are the two key behavioral patterns that define the Dunning-Kruger effect in humans?

- Concept: Confidence calibration metrics
  - Why needed here: Enables quantitative comparison of self-assessed confidence against actual correctness across models.
  - Quick check question: How do A1/A2 (absolute) differ from R1/R2 (relative) confidence scores?

- Concept: Prompt engineering strategies (Simple, CoT, ToT)
  - Why needed here: Different prompting affects how models generate confidence and correctness; understanding these is key to interpreting results.
  - Quick check question: What is the primary purpose of using Chain of Thought (CoT) prompting in this study?

## Architecture Onboarding

- Component map: Data collection → Prompt generation (Simple/CoT/ToT) → Model inference → Confidence scoring (A1/A2, R1/R2) → Correctness labeling → Statistical analysis (calibration, closeness, category-level breakdown)
- Critical path: Prompt → Model response → Confidence scores → Correctness check → Calibration metrics
- Design tradeoffs: Balancing uniform prompts for cross-model fairness vs. model-specific prompting to maximize performance; capturing both pre- and post-response confidence adds overhead but enables dynamic calibration analysis.
- Failure signatures: Systematic overconfidence across all categories suggests calibration bug; large variance in A1/A2 within a model suggests unstable self-assessment; zero closeness scores indicate models never revise confidence after answering.
- First 3 experiments:
  1. Run identical prompt sets on two models (e.g., GPT-4 vs LLaMA-13B) and compare A1/A2 distributions to test calibration differences.
  2. Vary problem difficulty within one category and plot confidence vs. correctness to check for Dunning-Kruger-like patterns.
  3. Test post-response confidence update (A2 vs A1) on a subset to measure self-revision capability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs exhibit consistent patterns of overconfidence or underconfidence across different problem categories?
- Basis in paper: Explicit - The paper shows that different LLMs display varying confidence levels across categories like LSAT Reasoning, Mathematical Reasoning, and Truthful Q&A.
- Why unresolved: The study shows patterns but doesn't establish whether these are consistent traits of the models or context-dependent behaviors.
- What evidence would resolve it: Longitudinal studies testing the same LLMs across multiple problem categories and difficulty levels, measuring whether their confidence calibration patterns remain stable over time and across contexts.

### Open Question 2
- Question: Is there a relationship between an LLM's size (parameter count) and its confidence calibration accuracy?
- Basis in paper: Explicit - The paper compares multiple LLaMA models (7B, 13B, 70B parameters) and observes different confidence calibration patterns.
- Why unresolved: While the paper observes different behaviors across model sizes, it doesn't establish whether parameter count directly influences confidence calibration or if other factors (architecture, training data) are more significant.
- What evidence would resolve it: Controlled experiments comparing models of different sizes trained on identical datasets with the same architecture, measuring confidence calibration accuracy across various tasks.

### Open Question 3
- Question: How do LLMs' self-assessment mechanisms compare to human cognitive biases like the Dunning-Kruger effect?
- Basis in paper: Explicit - The paper draws parallels between LLM confidence patterns and the Dunning-Kruger effect, noting instances where models show high confidence despite incorrect answers.
- Why unresolved: The paper acknowledges these parallels but states that more research is needed to establish a definitive connection.
- What evidence would resolve it: Systematic comparison of LLM confidence calibration patterns with established human cognitive bias research, including statistical analysis to determine if LLM patterns meet criteria for established psychological phenomena.

## Limitations

- Confidence scores may reflect stochastic model outputs rather than genuine self-assessment capabilities, creating ambiguity about whether observed miscalibration represents meaningful cognitive processes
- Cross-platform API access differences (native chat interfaces vs POE.com) introduce uncontrolled variability in how confidence elicitation prompts are processed
- The causal mechanisms linking architectural differences to confidence patterns remain speculative without ablation studies isolating specific model components

## Confidence

**High confidence**: Empirical observations of confidence miscalibration (overconfidence on incorrect answers, underconfidence on correct answers) are well-supported by the collected A1/A2, R1/R2 metrics and correctness labels.

**Medium confidence**: Claims about architectural differences driving confidence patterns have reasonable support from variance comparisons but lack mechanistic explanations linking model internals to observed behavior.

**Low confidence**: The explicit parallel to human Dunning-Kruger effects, while conceptually appealing, rests on superficial similarity without experimental validation that LLMs exhibit the same underlying cognitive processes.

## Next Checks

1. **Ablation confidence analysis**: Run controlled experiments removing or modifying confidence elicitation prompts to determine whether confidence scores are genuine self-assessment outputs or prompt-dependent artifacts.

2. **Difficulty gradient validation**: Systematically vary problem difficulty within single categories and test whether confidence drops consistently with correctness, confirming the predicted miscalibration pattern rather than random variance.

3. **Cross-model consistency test**: Implement identical prompting strategies across all models and measure whether observed confidence calibration differences persist, isolating whether patterns reflect architectural differences or platform/API effects.