---
ver: rpa2
title: Reinforcement Learning Fine-tuning of Language Models is Biased Towards More
  Extractable Features
arxiv_id: '2311.04046'
source_url: https://arxiv.org/abs/2311.04046
tags:
- feature
- reward
- target
- features
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper examines whether inductive biases observed in supervised
  fine-tuning of language models (LLMs) also apply to RL fine-tuning. It tests two
  hypotheses: (1) features more extractable after pre-training are more likely to
  be utilized by the RL fine-tuned model, and (2) the evidence for/against a feature
  during RL fine-tuning predicts whether the model will rely on that feature.'
---

# Reinforcement Learning Fine-tuning of Language Models is Biased Towards More Extractable Features

## Quick Facts
- arXiv ID: 2311.04046
- Source URL: https://arxiv.org/abs/2311.04046
- Reference count: 22
- Key outcome: RL fine-tuning of LLMs shows biases toward features more extractable after pre-training and features with more training evidence

## Executive Summary
This paper investigates whether inductive biases observed in supervised fine-tuning of language models also apply to reinforcement learning fine-tuning. The authors test two hypotheses: (1) features more extractable after pre-training are more likely to be utilized by RL fine-tuned models, and (2) the amount of evidence for/against a feature during RL fine-tuning predicts whether the model will rely on that feature. Through controlled experiments on synthetic and natural language tasks, the paper finds statistically significant correlations supporting both hypotheses, demonstrating that RL fine-tuning exhibits similar biases to supervised fine-tuning.

## Method Summary
The study uses GPT-2 and GPT-2 large models fine-tuned with Proximal Policy Optimization (PPO) on both synthetic numerical sequence generation tasks and modified IMDb movie review datasets. The experiments introduce target-spurious feature pairs where models must learn to distinguish between the relevant feature (target) and irrelevant but correlated feature (spurious). Feature extractability is quantified using Minimum Description Length (MDL), and the evidence against spurious features is controlled by varying the proportion of training examples containing only the target feature. Models are evaluated on test sets with different feature combinations to measure how well they learn to rely on the correct features.

## Key Results
- Models trained on tasks with more extractable target features achieve higher reward
- The more evidence there is against spurious features, the more likely the model is to learn a policy that relies on the target feature
- The harder a target feature is to extract, the more evidence against the spurious feature is needed for the model to achieve high reward

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Models fine-tuned with RL rely more on features that are easier to extract from the pre-trained model.
- **Mechanism:** Extractability is quantified via Minimum Description Length (MDL), which measures how simply a feature can be represented by a probe classifier. Lower MDL indicates higher extractability, making the feature easier for the model to detect and utilize during fine-tuning.
- **Core assumption:** The RL fine-tuning process will favor features that are computationally simpler to extract, even if they are not the most predictive of the task.
- **Evidence anchors:**
  - [abstract] "features more extractable after pre-training are more likely to be utilized by the RL fine-tuned model"
  - [section 4] "The average reward in Ts-only and Tt-only (for p = 0) is positively correlated with the relative MDL of the studied tasks"
- **Break condition:** If the reward signal is strong enough to overcome the inductive bias toward simpler features, or if the model architecture fundamentally changes the way features are extracted.

### Mechanism 2
- **Claim:** The amount of evidence against a spurious feature during training influences whether the model learns to rely on the target feature.
- **Mechanism:** When there is high evidence against the spurious feature (i.e., many training examples where the spurious feature is absent), the model is more likely to learn that the spurious feature is irrelevant and instead focus on the target feature to maximize reward.
- **Core assumption:** The model can infer the irrelevance of spurious features from the training data distribution and adjust its policy accordingly.
- **Evidence anchors:**
  - [abstract] "the more evidence there is for/against a feature during RL fine-tuning, the more likely the model learns a policy that relies on that feature to get a high reward"
  - [section 4] "the more evidence p against the spurious feature, the more likely the RL fine-tuned GPT-2 is to learn a policy leading to higher reward for examples in Ts-only"
- **Break condition:** If the spurious feature is extremely easy to extract relative to the target feature, the model may continue to rely on it despite evidence against it.

### Mechanism 3
- **Claim:** The combination of feature extractability and evidence against spurious features determines the model's generalization performance.
- **Mechanism:** When the target feature is highly extractable, the model can learn an effective policy even with minimal evidence against the spurious feature. However, when the target feature is hard to extract, more evidence against the spurious feature is needed for the model to generalize well.
- **Core assumption:** The model's ability to generalize is a function of both the intrinsic difficulty of extracting the target feature and the amount of evidence provided during training.
- **Evidence anchors:**
  - [abstract] "the harder a target feature is to extract, the more evidence against the spurious feature is needed for the model to get high reward in Ts-only (and Tt-only)"
  - [section 5] "Combining the extractability hypothesis with the evidence hypothesis, we note that, the harder a target feature is to extract, the more evidence against the spurious feature is needed for the model to get high reward in Ts-only (and Tt-only)"
- **Break condition:** If the model is provided with an overwhelming amount of evidence against the spurious feature, it may overcome the bias toward easier-to-extract features.

## Foundational Learning

- **Concept:** Minimum Description Length (MDL)
  - **Why needed here:** MDL is used to quantify the extractability of features, which is a key component of the study's hypotheses.
  - **Quick check question:** What does a lower MDL value indicate about a feature's extractability?

- **Concept:** Proximal Policy Optimization (PPO)
  - **Why needed here:** PPO is the RL algorithm used to fine-tune the language models in the experiments.
  - **Quick check question:** What is the primary advantage of using PPO over other RL algorithms in this context?

## Architecture Onboarding

- **Component map:** Pre-trained model (GPT-2) -> Reward model -> PPO fine-tuning -> Probe classifier for MDL computation
- **Critical path:** Pre-trained model → Reward signal generation → RL fine-tuning → Policy evaluation
- **Design tradeoffs:**
  - Using a simpler pre-trained model (GPT-2) allows for clearer isolation of the effects of extractability and evidence, but may not generalize to larger, more capable models
  - The controlled synthetic tasks provide a clean experimental setup but may not capture the complexity of real-world NLP tasks
- **Failure signatures:**
  - If the RL fine tuned model fails to achieve high reward on the neither dataset, it indicates that the fine-tuning process did not converge to a good policy
  - If the model achieves high reward on the both dataset but low reward on the t-only and s-only datasets, it suggests that the model is relying on spurious features rather than the target feature
- **First 3 experiments:**
  1. Replicate the synthetic task experiments with the 4-layer transformer to validate the core findings in a simpler setting
  2. Extend the controlled sentiment experiments to larger pre-trained models (e.g., GPT-2 large) to assess the scalability of the results
  3. Design and test a new controlled task where the target and spurious features are equally extractable to isolate the effect of evidence on the model's policy

## Open Questions the Paper Calls Out
The paper acknowledges limitations including: testing only single target-spurious feature pairs at a time, using relatively small models (GPT-2), and the need to test these hypotheses on more complex real-world NLP tasks with multiple interacting features.

## Limitations
- Results are based on relatively small models (GPT-2) and may not generalize to larger frontier models
- Experiments focus on single target-spurious feature pairs, not capturing the complexity of real-world tasks with multiple interacting features
- The controlled synthetic tasks, while providing clean experimental conditions, may not fully represent the complexity of real NLP tasks

## Confidence
- **High Confidence:** The evidence hypothesis showing correlation between training evidence and feature reliance has strong statistical support (p < 0.01) across multiple experimental conditions
- **Medium Confidence:** The extractability hypothesis shows consistent directional effects but with weaker statistical significance in some conditions
- **Low Confidence:** The interaction between extractability and evidence is theoretically compelling but has limited empirical validation

## Next Checks
1. **Architecture Scaling Test:** Replicate the core experiments using GPT-2 large and LLaMA-7B models to assess whether extractability biases persist or diminish as model capacity increases
2. **Alternative Extractability Metrics:** Implement feature importance scoring using integrated gradients or attention-based methods as alternatives to MDL, to verify whether the same features emerge as most extractable
3. **Real-World Task Transfer:** Design a controlled experiment using a practical NLP task (e.g., medical text classification) where spurious correlations are known to exist, to test whether the observed biases affect real-world performance