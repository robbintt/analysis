---
ver: rpa2
title: 'MAEA: Multimodal Attribution for Embodied AI'
arxiv_id: '2307.13850'
source_url: https://arxiv.org/abs/2307.13850
tags:
- attribution
- action
- attributions
- language
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces MAEA, a framework for multimodal attribution\
  \ analysis in embodied AI policies. It computes global attributions per modality\
  \ (visual, language, previous action) using GRAD \u2299 INPUT at the fusion layer\
  \ of differentiable policies."
---

# MAEA: Multimodal Attribution for Embodied AI

## Quick Facts
- arXiv ID: 2307.13850
- Source URL: https://arxiv.org/abs/2307.13850
- Authors: 
- Reference count: 35
- Key outcome: MAEA framework computes global attributions per modality (visual, language, previous action) in embodied AI policies, revealing architectural and dataset biases.

## Executive Summary
MAEA introduces a framework for multimodal attribution analysis in embodied AI policies, enabling understanding of how different input modalities contribute to decision-making. The method computes global attributions using GRAD ⊙ INPUT at the fusion layer of differentiable policies, providing insights into both architectural biases (before training) and dataset-induced biases (after training). Experiments on ALFRED dataset policies demonstrate that transformer-based models show higher visual attributions while sequence-to-sequence models exhibit more balanced modality attributions.

## Method Summary
The MAEA framework computes multimodal attributions by applying GRAD ⊙ INPUT at the penultimate fusion layer of differentiable embodied AI policies. For each timestep, the method calculates gradients of the predicted action with respect to input modality embeddings, then pools these gradients using L2 norm to obtain scalar attribution values per modality. The framework analyzes four ALFRED policies (Baseline, MOCA, ET, HiTUT) on 100 validation-seen trajectories, comparing attributions before and after training to distinguish architectural from dataset biases. The method also tracks language attribution focus across episode progression and analyzes visual attributions relative to predicted interaction masks.

## Key Results
- Transformer-based models (ET, HiTUT) show higher visual attributions compared to sequence-to-sequence models
- Sequence-to-sequence models (Baseline, MOCA) exhibit more balanced modality attributions
- Architectural bias analysis reveals that before training, all models show high previous action attributions due to model design
- Language attribution tracking demonstrates how focus shifts throughout episode completion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal attribution at the fusion layer reveals how much each modality contributes to action prediction.
- Mechanism: GRAD ⊙ INPUT computes gradients of the predicted action with respect to modality inputs at the fusion layer, with L2 norm pooling providing scalar attributions invariant to embedding dimensionality.
- Core assumption: Gradients accurately reflect modality importance in decision-making.
- Evidence anchors: Abstract states global attributions per modality can be computed for differentiable policies.

### Mechanism 2
- Claim: Attribution patterns before/after training reveal architectural vs. dataset biases.
- Mechanism: Random initialization provides baseline architectural bias, while trained attributions show data-induced changes.
- Core assumption: Random initialization creates consistent baseline attribution pattern.
- Evidence anchors: Paper explicitly discusses decoupling architectural and dataset biases.

### Mechanism 3
- Claim: Modality-specific attribution analysis reveals attention evolution during task execution.
- Mechanism: Tracking individual word token attributions and visual region attributions shows how attention shifts.
- Core assumption: Attribution of tokens/regions corresponds to meaningful attention patterns.
- Evidence anchors: Paper analyzes language focus center shifts and visual attribution alignment with interaction masks.

## Foundational Learning

- **Gradient-based attribution methods**: Essential for quantifying input feature contributions; quick check: What's the difference between using gradients directly vs. gradient ⊙ input?
- **Modality fusion in neural networks**: Critical for understanding attribution at fusion layers; quick check: How might fusion layer choice affect attribution patterns?
- **Embodied AI task benchmarks**: Necessary for contextualizing findings; quick check: What challenges in ALFRED make multimodal attribution important?

## Architecture Onboarding

- **Component map**: Visual frames (V), previous action embeddings (a_t-1), language instructions (L) → modality encoders → fusion layer → action logits
- **Critical path**: Forward pass through encoders → fusion → action prediction; backpropagation for attributions; L2 norm pooling per modality
- **Design tradeoffs**: Attribution layer choice (earlier vs. later), pooling method selection, attribution target selection
- **Failure signatures**: Inconsistent attributions across seeds, extreme skewness (>90% to one modality), unchanging patterns over episode steps
- **First 3 experiments**: Compare random vs. trained model attributions; analyze language attribution shifts for poor language utilization; use XRAI on visual frames for interact actions

## Open Questions the Paper Calls Out

1. **Ideal target function for attributions**: Whether predicted action, loss, expert action, or other targets provide most interpretable results.
2. **Generalization to other task domains**: How attributions change across different embodied AI benchmarks beyond household navigation.
3. **Extension to modular/hierarchical policies**: Adapting attribution analysis for policies with multiple differentiable components.
4. **Impact of attribution method choice**: Comparing GRAD ⊙ INPUT with other methods like Integrated Gradients or SmoothGrad.

## Limitations

- Method relies on gradient-based attributions that may not capture implicit reasoning processes
- Initialization sensitivity could affect architectural bias analysis reliability
- Only tested on one benchmark (ALFRED) limiting generalizability claims

## Confidence

- Multimodal attribution at fusion layer accurately reveals modality importance: Low confidence
- Architectural vs. dataset bias decoupling: Medium confidence
- Language attribution tracking reveals reasoning evolution: Medium confidence

## Next Checks

1. Run Integrated Gradients analysis on 5 trajectories to compare with GRAD ⊙ INPUT results
2. Test initialization sensitivity across 10 different random seeds
3. Perform ablation studies by removing each modality and measuring performance drops