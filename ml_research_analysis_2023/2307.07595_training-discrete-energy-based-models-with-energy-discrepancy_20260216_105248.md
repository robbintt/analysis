---
ver: rpa2
title: Training Discrete Energy-Based Models with Energy Discrepancy
arxiv_id: '2307.07595'
source_url: https://arxiv.org/abs/2307.07595
tags:
- energy
- discrete
- pdata
- discrepancy
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Energy Discrepancy (ED), a novel contrastive
  loss function for training energy-based models (EBMs) on discrete spaces without
  requiring sampling from the model during training. The key idea is to perturb data
  points in various ways (Bernoulli noise, deterministic transforms, or neighborhood
  structures) and use the energy discrepancy between original and perturbed points
  as a loss function.
---

# Training Discrete Energy-Based Models with Energy Discrepancy

## Quick Facts
- arXiv ID: 2307.07595
- Source URL: https://arxiv.org/abs/2307.07595
- Reference count: 32
- Key outcome: Introduces Energy Discrepancy (ED), a novel contrastive loss for training EBMs on discrete spaces without MCMC sampling, achieving competitive performance on lattice Ising models and discrete image datasets

## Executive Summary
This work introduces Energy Discrepancy (ED), a novel contrastive loss function for training energy-based models (EBMs) on discrete spaces without requiring sampling from the model during training. The key idea is to perturb data points in various ways (Bernoulli noise, deterministic transforms, or neighborhood structures) and use the energy discrepancy between original and perturbed points as a loss function. The method is theoretically justified by data processing inequalities and is shown to have a unique global minimum. Experiments demonstrate competitive performance on lattice Ising models, discrete density estimation tasks, and high-dimensional discrete image modeling (MNIST, Omniglot, Caltech Silhouettes), with ED often outperforming or matching baseline methods while requiring fewer energy function evaluations than contrastive divergence approaches.

## Method Summary
Energy Discrepancy (ED) trains EBMs by measuring the energy difference between data points and their perturbed versions, eliminating the need for MCMC sampling. The method uses three perturbation types: Bernoulli noise, deterministic transforms (mean-pooling), and neighborhood structures. For each perturbation type, ED approximates a contrastive potential that enables tractable training. The loss is stabilized with an offset parameter to prevent early training instability. Theoretical guarantees are provided through data processing inequalities, ensuring the loss is minimized when the model matches the data distribution.

## Key Results
- ED achieves lower NLL than PCD and CD on synthetic 2D datasets (MMD: 0.04 vs 0.10 for PCD)
- Outperforms or matches CD and FVE on MNIST, Omniglot, and Caltech Silhouettes for discrete density estimation
- Recovers Ising model connectivity matrices more accurately than CD (RMSE: 0.42 vs 0.85)
- Requires fewer energy function evaluations than contrastive divergence methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Energy discrepancy (ED) works because it replaces sampling from the model with information-lossy perturbations, enabling contrastive training without MCMC.
- Mechanism: The loss measures the difference between energy at data points and at their perturbed counterparts, using the data processing inequality to ensure the loss is minimized when the model matches the data distribution.
- Core assumption: Perturbations q(y|x) must be information-lossy (Var(x|y) > 0) so that KL(Qpdata || Qpebm) â‰¤ KL(pdata || pebm) holds.
- Evidence anchors:
  - [abstract] "Energy discrepancy offers theoretical guarantees for a broad class of perturbation processes"
  - [section 2] "The definition of ED only requires the evaluation of the EBM on positive and contrasting, negative samples"
- Break condition: If perturbations are invertible (no information loss), the data processing inequality no longer provides a valid contrastive signal.

### Mechanism 2
- Claim: The three proposed perturbation methods (Bernoulli, deterministic transforms, neighborhood-based) are effective because they introduce controlled information loss while remaining computationally tractable.
- Mechanism: Each perturbation method defines a tractable approximation of the contrastive potential Uq(y) through sampling or enumeration over preimages.
- Core assumption: The preimage sets {x' : g(x') = y} or neighborhoods N(x) are small enough to sample efficiently while still being information-lossy.
- Evidence anchors:
  - [section 2.1] "Due to the symmetry of q, we can then write the contrastive potential as..." (Bernoulli)
  - [section 2.1] "The contrastive potential can be expressed in terms of the preimage of g..." (Deterministic)
  - [section 2.1] "We define the forward perturbation q(y|x) by selecting neighbours y ~ U(N(x)) uniformly at random" (Neighborhood)
- Break condition: If perturbation preimages become too large (high-dimensional spaces) or too small (insufficient information loss), the contrastive signal becomes either intractable or ineffective.

### Mechanism 3
- Claim: The stabilized loss formulation with offset w prevents training instability by avoiding negative infinity losses during early training.
- Mechanism: The log-sum-exp approximation introduces bias, but the offset w creates a lower bound on the loss, preventing the optimizer from seeking high-variance estimators.
- Core assumption: The bias introduced by the stabilization term goes to zero as M increases, preserving asymptotic consistency.
- Evidence anchors:
  - [section 2.1] "To stabilise training, we introduce an offset for the logarithm which introduces a deterministic lower bound for the

## Foundational Learning

### Perturbation Process
- Why needed: Core mechanism for creating contrastive samples without MCMC sampling
- Quick check: Verify that q(y|x) satisfies symmetry condition for tractable contrastive potential calculation

### Contrastive Potential
- Why needed: Enables tractable approximation of intractable expectations over perturbation preimages
- Quick check: Confirm that Uq(y) can be computed efficiently for chosen perturbation method

### Data Processing Inequality
- Why needed: Provides theoretical guarantee that ED loss is minimized when model matches data distribution
- Quick check: Ensure perturbations introduce sufficient information loss (Var(x|y) > 0)

## Architecture Onboarding

### Component Map
- Data points -> Perturbation method (Bernoulli/Deterministic/Neighborhood) -> Energy function -> Contrastive potential calculation -> ED loss -> Parameter update

### Critical Path
Data -> Perturbation -> Energy evaluation -> Contrastive potential -> Loss computation -> Gradient update

### Design Tradeoffs
- Perturbation choice: Information loss vs computational tractability
- Stabilization offset w: Training stability vs asymptotic consistency
- Number of samples M: Variance reduction vs computational cost

### Failure Signatures
- Loss divergence with w=0 and small M
- Mode collapse with ED-Pool on certain datasets
- Intractable contrastive potential with high-dimensional perturbations

### First Experiments
1. Implement ED loss with Bernoulli perturbation on synthetic 2D data
2. Compare ED vs CD training dynamics on small binary image dataset
3. Test different perturbation methods on Ising model connectivity recovery

## Open Questions the Paper Calls Out
- How does the choice of perturbation method affect the performance of Energy Discrepancy on different types of discrete data structures?
- What are the theoretical guarantees for Energy Discrepancy when applied to high-dimensional discrete spaces?
- How can Energy Discrepancy be adapted for structured data like graphs or text?

## Limitations
- Perturbation preimages can become computationally intractable in high-dimensional spaces
- Performance highly dependent on appropriate perturbation method selection for data structure
- Theoretical guarantees rely on information-lossy perturbations, limiting applicability

## Confidence

*High Confidence:* The core mechanism of using energy discrepancy with information-lossy perturbations is well-supported by both theory and experiments. The empirical results on synthetic data and standard benchmarks (MNIST, Omniglot) demonstrate consistent improvements over baseline methods.

*Medium Confidence:* The theoretical claims about global optimality and the relationship between ED and KL divergence are sound for the idealized case, but the practical implications in high-dimensional settings need further validation. The stabilization techniques (offset w) appear effective but their impact on asymptotic consistency requires more careful analysis.

## Next Checks
1. **Scalability Test:** Evaluate ED on larger discrete datasets (e.g., CIFAR-10 with binarized pixels) to determine the breaking point where perturbation preimages become computationally intractable.
2. **Perturbation Ablation:** Systematically vary perturbation strength and information loss (e.g., different Bernoulli noise probabilities, pooling window sizes) to quantify the trade-off between contrastive signal quality and computational cost.
3. **Theoretical Extension:** Formalize the conditions under which the log-sum-exp approximation bias becomes negligible, particularly for the neighborhood-based perturbation method where preimage sets can be large.