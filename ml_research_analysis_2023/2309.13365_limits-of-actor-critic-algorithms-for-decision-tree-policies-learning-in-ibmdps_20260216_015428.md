---
ver: rpa2
title: Limits of Actor-Critic Algorithms for Decision Tree Policies Learning in IBMDPs
arxiv_id: '2309.13365'
source_url: https://arxiv.org/abs/2309.13365
tags:
- policy
- learning
- ibmdp
- decision
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates learning interpretable decision trees (DTs)
  using reinforcement learning (RL) in the Iterative Bounding Markov Decision Process
  (IBMDP) framework. It shows that standard actor-critic algorithms like PPO and TRPO
  fail to learn optimal DTs even on simple supervised classification tasks.
---

# Limits of Actor-Critic Algorithms for Decision Tree Policies Learning in IBMDPs

## Quick Facts
- arXiv ID: 2309.13365
- Source URL: https://arxiv.org/abs/2309.13365
- Reference count: 40
- Primary result: Standard actor-critic algorithms fail to learn optimal decision trees in IBMDPs due to approximation errors in Q-function and policy representation

## Executive Summary
This paper investigates learning interpretable decision trees using reinforcement learning in the Iterative Bounding Markov Decision Process (IBMDP) framework. The authors demonstrate that standard actor-critic algorithms like PPO and TRPO fail to learn optimal decision trees even on simple supervised classification tasks. The key insight is that approximation errors in both the Q-function and policy representation cause these algorithms to ignore information gathering actions that are crucial for optimal decision trees. To address this, they propose an entropy regularized policy iteration (ERPI) algorithm that learns deterministic reactive policies and prove convergence to near-optimal performance.

## Method Summary
The authors reformulate the IBMDP into a fully observable MDP called the Observation-IBMDP, where states are defined by feature bounds rather than full states. They implement exact policy iteration with entropy regularization to learn reactive policies that map feature bounds to actions. The approach is evaluated on iris and wine datasets, comparing against CART and asymmetric PPO implementations. The method extracts decision trees by recursively querying the learned policy and controls interpretability through a trade-off parameter ζ that balances tree size against classification accuracy.

## Key Results
- Asymmetric PPO converges to zero-depth trees that ignore information gathering actions on both iris and wine datasets
- ERPI achieves similar accuracy to CART while producing interpretable DTs with controllable size
- The key failure of actor-critic methods is traced to approximation errors in Q-function and policy representation
- ERPI's entropy regularization mitigates these errors by preventing premature convergence to deterministic policies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Actor-critic algorithms fail to learn optimal DTs in IBMDPs because of approximation errors in both the Q-function and policy representation.
- Mechanism: When using neural networks to approximate Qπ and π in an IBMDP setting, the learned values deviate significantly from their true values, leading to poor policy updates. This is especially problematic when the optimal policy requires precise information gathering actions.
- Core assumption: The IBMDP can be solved exactly with tabular methods, and any deviation from exact solutions is due to approximation errors.
- Evidence anchors:
  - [abstract] "The authors identify the source of failure as approximation errors in the Q-function and policy representation."
  - [section] "We observe that approximating either Qπ or π is the main reason for failure."
  - [corpus] Weak evidence - no direct citation found in neighbor papers about approximation errors specifically.
- Break condition: When the state-action space becomes too large for tabular methods, forcing the use of function approximation again.

### Mechanism 2
- Claim: Entropy regularization mitigates approximation errors by preventing premature convergence to deterministic policies.
- Mechanism: Adding entropy to the policy objective encourages exploration and prevents the policy from getting stuck in local optima where information gathering actions are ignored. This is particularly important in IBMDPs where information gathering actions have negative immediate rewards.
- Core assumption: The base task is a supervised classification task where the Observation-IBMDP formulation is valid.
- Evidence anchors:
  - [abstract] "They propose an entropy regularized policy iteration (ERPI) algorithm that learns deterministic reactive policies and prove convergence to near-optimal performance."
  - [section] "To increase robustness to noisy Q estimates, a known remedy is entropy regularization that is known to average errors in Q instead of summing them."
  - [corpus] Weak evidence - neighbor papers don't discuss entropy regularization in the context of IBMDPs.
- Break condition: When the entropy regularization coefficient is too high, preventing the policy from converging to a deterministic solution.

### Mechanism 3
- Claim: Reformulating the IBMDP as a fully observable MDP (Observation-IBMDP) eliminates the need for reactive policies and enables exact dynamic programming.
- Mechanism: By exploiting the fact that the base task is supervised classification, the state can be reduced to just the feature bounds, eliminating partial observability. This allows using exact methods like policy iteration instead of approximate RL algorithms.
- Core assumption: The base MDP defines a supervised learning task where the base state distribution is independent of the base action.
- Evidence anchors:
  - [abstract] "The key insight is reformulating the IBMDP into a fully observable MDP, enabling efficient exact policy iteration."
  - [section] "Let π and π′ be two reactive policies of an IBMDP... When replacing the weighting of Aπθ(s, a) by p(s|o), in the gradient ∂J(πθ)/∂θ(o,a), the 'gradient' ascent of the logits becomes: θk+1(o, a) = θk(o, a) + αAπθk (o, a)"
  - [corpus] Weak evidence - no direct citation found about Observation-IBMDP reformulation.
- Break condition: When the base task is not a supervised classification task, making the Observation-IBMDP reformulation invalid.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and their formulation
  - Why needed here: Understanding the basic MDP framework is essential for grasping how IBMDPs extend this framework and why the reformulation to Observation-IBMDP works.
  - Quick check question: What are the five components of an MDP tuple ⟨S, A, R, T, γ⟩?

- Concept: Partially Observable MDPs (POMDPs) and reactive policies
  - Why needed here: IBMDPs require reactive policies that depend only on observations (feature bounds) rather than the full state, making POMDP theory directly relevant.
  - Quick check question: What is the key difference between a reactive policy and a history-dependent policy in a POMDP?

- Concept: Policy gradient methods and their convergence properties
  - Why needed here: Understanding why standard actor-critic methods fail requires knowledge of policy gradient convergence issues, especially for softmax policies.
  - Quick check question: Why is policy gradient convergence slow when the probability of the optimal action gets too low?

## Architecture Onboarding

- Component map:
  - IBMDP environment: Extends base MDP with feature bounds and information gathering actions
  - Policy network: Takes feature bounds as input, outputs action probabilities
  - Critic network: Takes full IBMDP state as input, outputs Q-values
  - ERPI algorithm: Tabular policy iteration with entropy regularization
  - Observation-IBMDP: Reformulated fully observable MDP for exact solutions

- Critical path:
  1. Initialize IBMDP with base MDP and feature bounds
  2. Collect trajectories using current policy
  3. Update Q-function estimates
  4. Update policy using entropy regularized policy iteration
  5. Extract DT from converged policy

- Design tradeoffs:
  - Tabular vs. neural network representations: Tabular methods guarantee exact solutions but don't scale; neural networks scale but introduce approximation errors
  - Entropy regularization coefficient: Higher values increase exploration but slow convergence
  - Maximum tree depth: Controls complexity but limits expressiveness

- Failure signatures:
  - Policy converges to zero-depth tree (no information gathering actions)
  - High variance in Q-function estimates across training runs
  - Policy gets stuck in local optima with suboptimal feature splits

- First 3 experiments:
  1. Implement exact AAC algorithm on toy classification task to verify it finds optimal solution
  2. Add neural network approximation to Q-function and observe performance degradation
  3. Implement ERPI algorithm and compare convergence to exact AAC method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do asymmetric actor-critic algorithms perform on IBMDPs with sequential decision-making base tasks beyond supervised learning?
- Basis in paper: The paper shows that asymmetric actor-critic algorithms like PPO and TRPO fail to learn optimal decision trees for simple supervised classification tasks in IBMDPs. However, it only briefly mentions that this remains an open problem for more general sequential decision-making tasks.
- Why unresolved: The paper focuses on the specific case of supervised learning tasks and does not provide experimental results or theoretical analysis for more complex decision-making scenarios.
- What evidence would resolve it: Experiments comparing asymmetric actor-critic algorithms on IBMDPs with sequential decision-making base tasks to algorithms like entropy regularized policy iteration (ERPI) or exact policy iteration. Analysis of the performance gap between these algorithms on various sequential decision-making tasks.

### Open Question 2
- Question: Can deep RL methods be combined with Monte Carlo Tree Search (MCTS) to scale decision tree learning to larger datasets with more features?
- Basis in paper: The paper discusses the scalability limitations of exact and entropy regularized policy iteration due to the exponential growth of the state space with the number of features and tree depth. It suggests MCTS as a potential solution but does not provide any experiments or analysis.
- Why unresolved: The paper does not explore the combination of deep RL and MCTS for decision tree learning. It only mentions MCTS as a future direction without any empirical evidence.
- What evidence would resolve it: Experiments comparing deep RL algorithms combined with MCTS to exact and entropy regularized policy iteration on larger datasets with more features. Analysis of the trade-off between scalability and optimality achieved by these hybrid approaches.

### Open Question 3
- Question: How does the choice of splitting thresholds in IBMDPs affect the performance and interpretability of learned decision trees?
- Basis in paper: The paper uses a fixed splitting threshold (p=1) in its experiments, which limits the granularity of the learned decision trees. It mentions that CART typically chooses finer splits, leading to improved accuracy, but does not explore the impact of different splitting thresholds in IBMDPs.
- Why unresolved: The paper does not investigate the effect of varying the splitting threshold parameter p on the quality of learned decision trees. It only provides results for a single value of p.
- What evidence would resolve it: Experiments comparing the performance and interpretability of decision trees learned by IBMDPs with different values of the splitting threshold parameter p. Analysis of the trade-off between tree complexity, accuracy, and interpretability as p varies.

## Limitations
- The approach doesn't scale to real-world problems with many features due to exponential state space growth in the Observation-IBMDP reformulation
- Focus on interpretability via tree size control is narrow, not addressing other interpretability aspects like feature importance or decision boundaries
- Asymmetric PPO implementation details are underspecified, making fair comparison difficult

## Confidence
- Actor-critic failure due to approximation errors: High confidence
- ERPI superiority over actor-critic methods: Medium confidence
- Theoretical guarantees for ERPI convergence: High confidence
- Practical significance of ERPI for real-world problems: Medium confidence

## Next Checks
1. Implement exact AAC on a simple 2D classification problem and verify it finds the optimal decision boundary tree
2. Test ERPI on a synthetic dataset with 10+ features to quantify the exponential scaling effects
3. Compare ERPI against modern decision tree learning methods (e.g., CART with pruning, XGBoost feature importance) on benchmark datasets beyond iris and wine