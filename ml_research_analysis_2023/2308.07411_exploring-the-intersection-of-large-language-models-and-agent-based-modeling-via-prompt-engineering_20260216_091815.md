---
ver: rpa2
title: Exploring the Intersection of Large Language Models and Agent-Based Modeling
  via Prompt Engineering
arxiv_id: '2308.07411'
source_url: https://arxiv.org/abs/2308.07411
tags:
- agent
- price
- simulation
- agents
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that large language models (LLMs) can\
  \ simulate human interactions in agent-based modeling through prompt engineering.\
  \ Two simulations are presented: a two-agent Pok\xE9mon card negotiation and a six-agent\
  \ murder mystery game."
---

# Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering

## Quick Facts
- arXiv ID: 2308.07411
- Source URL: https://arxiv.org/abs/2308.07411
- Reference count: 4
- Primary result: LLMs can simulate human interactions in agent-based modeling through prompt engineering, demonstrated via Pokémon card negotiations and murder mystery games.

## Executive Summary
This paper explores how large language models (LLMs) can be leveraged for agent-based modeling through prompt engineering techniques. The authors present two simulations demonstrating believable human-like interactions: a Pokémon card negotiation between two agents and a murder mystery game with six agents. By defining personas that dictate goals and personalities, the LLM agents engage in contextually appropriate dialogue that maintains coherence across multiple turns. The research reveals that LLMs can produce novel strategies while maintaining agent-specific behaviors, though the 4,096 token input limit of GPT-3.5-turbo constrains simulation complexity. The study suggests that larger context windows and improved information retrieval could enable more sophisticated human behavior simulations in the future.

## Method Summary
The research employs prompt engineering to define agent personas and control interactions within two simulation scenarios. The method uses OpenAI's gpt-3.5-turbo model with a Chat Completions API approach that incorporates three input types: System prompts (persona definitions), Assistant prompts (LLM agent responses), and User prompts (human or other agent inputs). Simulations are categorized as one-to-one, one-to-many, or many-to-many scenarios. The approach relies on context accumulation through conversation history and memory streams for multi-agent scenarios, where responses are concatenated to preserve ongoing context for subsequent agent interactions.

## Key Results
- LLMs successfully simulate believable human interactions in both Pokémon card negotiations and murder mystery scenarios
- Agents maintain coherent behavior across multiple turns by leveraging conversation history
- The seller agent in the negotiation scenario employed strategic price inflation, demonstrating emergent strategic behavior
- Memory streams enable context sharing among multiple agents, allowing the captain to correctly identify the killer in the murder mystery game

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt engineering with persona definitions enables LLMs to simulate human-like behavior in agent-based modeling.
- Mechanism: Personas act as behavioral constraints that guide the LLM's responses within defined goals and personality traits, allowing the model to generate contextually appropriate dialogue.
- Core assumption: LLMs can interpret freeform text personas and translate them into consistent behavioral patterns during interactions.
- Evidence anchors:
  - [abstract] "Through prompt engineering, inspired by Park et al. (2023), we present two simulations of believable proxies of human behavior"
  - [section 2.2.1] "System sets the persona of the Assistant (i.e. LLM agent). This is analogous to defining an agent's unique personality."
  - [corpus] Found 25 related papers with average neighbor FMR=0.472, indicating moderate relevance of this approach in the literature
- Break condition: Personas become too complex or contradictory for the LLM to maintain consistency across multiple turns of dialogue.

### Mechanism 2
- Claim: Context accumulation through conversation history enables LLMs to maintain coherent multi-turn interactions.
- Mechanism: Each response is conditioned on the cumulative conversation history, allowing agents to reference previous exchanges and maintain topic relevance.
- Core assumption: LLMs can effectively retrieve and utilize context from earlier parts of the conversation to inform current responses.
- Evidence anchors:
  - [section 2.2.2] "In this approach, each interaction's next response is conditioned on the cumulative conversation history thus far."
  - [section 3.1] "An intriguing observation is that the seller employed a strategy of inflating the sell price ($50) at the start of negotiations, which resulted in securing a higher final sell price."
  - [corpus] Liu et al. (2023) empirically measures the effectiveness of this mechanism, finding that LLMs are most proficient at retrieving context from the beginning and end of the prompt
- Break condition: The conversation history exceeds the 4,096 token limit, forcing truncation and loss of context.

### Mechanism 3
- Claim: Memory streams enable context sharing among multiple agents in one-to-many simulations.
- Mechanism: Responses from multiple agents are concatenated into a single memory stream that serves as the context for the coordinating agent's next response.
- Core assumption: LLMs can process and utilize concatenated text from multiple sources as coherent context for generating relevant responses.
- Evidence anchors:
  - [section 3.2] "The memory stream is created by concatenating the responses from each passenger agent into one string, helping to preserve the ongoing context of the conversation."
  - [section 3.2] "This context enables the captain to craft relevant responses, including any additional follow-up questions."
  - [corpus] Weak evidence - no direct corpus support found for memory stream technique specifically
- Break condition: Memory stream becomes too long to fit within the token limit, or becomes too complex for the LLM to effectively parse.

## Foundational Learning

- Concept: Agent-Based Modeling (ABM)
  - Why needed here: Provides the foundational framework for understanding how individual agents interact within larger systems, which this research extends with LLM technology
  - Quick check question: What distinguishes ABM from traditional system modeling approaches?
- Concept: Prompt Engineering
  - Why needed here: The core technique used to define agent behaviors and control their interactions through carefully crafted input prompts
  - Quick check question: How do system prompts differ from user prompts in the Chat Completions API?
- Concept: Context Window Limitations
  - Why needed here: Understanding the 4,096 token limit is crucial for designing simulations that work within these constraints
  - Quick check question: What happens when a conversation exceeds the maximum context window size?

## Architecture Onboarding

- Component map: User interface -> Conversation history buffer -> Persona definition module -> LLM API (gpt-3.5-turbo) -> Response parser -> Memory stream aggregator (for multi-agent) -> Output display
- Critical path: Persona definition -> Initial prompt construction -> API call -> Response parsing -> Context accumulation -> Next prompt construction -> API call (loop)
- Design tradeoffs: Using gpt-3.5-turbo provides fast responses but limits context to 4,096 tokens; larger models like Llama-2-7b-chat-hf offer more parameters but are computationally expensive
- Failure signatures: Agents rambling off-topic (context retrieval failure), inconsistent behavior across turns (persona interpretation issues), premature conversation termination (token limit reached)
- First 3 experiments:
  1. Test single-agent response consistency with varying persona complexity
  2. Test two-agent negotiation with different persona objectives
  3. Test memory stream effectiveness in three-agent conversation scenario

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different agent personas affect the believability and realism of LLM-driven simulations?
- Basis in paper: [explicit] The paper discusses defining personas for LLM agents to dictate their goals and personalities in simulations.
- Why unresolved: The paper does not provide a systematic evaluation of how different persona definitions impact the believability and realism of the simulations.
- What evidence would resolve it: Conducting a study with human evaluators to rate the believability and realism of simulations with varying agent personas.

### Open Question 2
- Question: What are the optimal strategies for maintaining context and coherence in multi-agent simulations with large numbers of participants?
- Basis in paper: [inferred] The paper mentions the challenge of sharing context across multiple agents in simulations and introduces the concept of a memory stream.
- Why unresolved: The paper does not explore alternative strategies for maintaining context and coherence in multi-agent simulations or evaluate the effectiveness of the memory stream approach.
- What evidence would resolve it: Comparing the performance of simulations using different context maintenance strategies, such as memory streams, attention mechanisms, or summarization techniques.

### Open Question 3
- Question: How can the limitations imposed by the 4,096 token input limit be overcome to enable more complex and realistic simulations?
- Basis in paper: [explicit] The paper discusses the 4,096 token input limit as a bottleneck for building large-scale, human-realistic simulations.
- Why unresolved: The paper does not propose concrete solutions for overcoming the token limit or explore the potential of larger context windows in future LLM models.
- What evidence would resolve it: Demonstrating the feasibility of running complex simulations using LLMs with larger context windows or implementing techniques to efficiently retrieve relevant information from large context windows.

## Limitations
- Token limit of 4,096 constrains simulation complexity and duration
- Memory stream technique lacks extensive validation and corpus support
- Persona complexity may exceed LLM's ability to maintain consistent behavior across turns

## Confidence
- High Confidence: LLMs can simulate human-like interactions in agent-based modeling through prompt engineering
- Medium Confidence: Context accumulation enables coherent multi-turn interactions
- Low Confidence: Memory streams effectively enable context sharing among multiple agents

## Next Checks
1. Systematically test simulations at varying lengths to identify the precise point where context truncation degrades agent behavior quality
2. Design a series of personas with increasing complexity and contradiction levels to determine the breaking point for consistent LLM behavior
3. Implement multi-agent simulations with increasing numbers of participants to evaluate the effectiveness and limitations of the memory stream approach as conversation complexity grows