---
ver: rpa2
title: Diversity of Thought Improves Reasoning Abilities of LLMs
arxiv_id: '2310.07088'
source_url: https://arxiv.org/abs/2310.07088
tags:
- block
- reasoning
- approaches
- prompt
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method to improve the reasoning abilities
  of large language models (LLMs) by leveraging diversity of thought. The approach
  involves automatically generating diverse prompts by soliciting feedback from the
  LLM to ideate problem-solving approaches.
---

# Diversity of Thought Improves Reasoning Abilities of LLMs

## Quick Facts
- **arXiv ID:** 2310.07088
- **Source URL:** https://arxiv.org/abs/2310.07088
- **Reference count:** 40
- **Primary result:** DIV-SE and IDIV-SE methods improve reasoning accuracy by leveraging diverse prompts, achieving 29.6 percentage points improvement on Blocksworld 4/5 planning benchmark

## Executive Summary
This paper introduces a method to improve large language model reasoning abilities by leveraging diversity of thought. The approach automatically generates diverse prompts by soliciting feedback from the LLM to ideate problem-solving approaches, then ensembles these diverse reasoning paths either across multiple inference calls (DIV-SE) or within a single call (IDIV-SE). The results demonstrate significant improvements over previous baselines on various reasoning benchmarks, with DIV-SE achieving 29.6 percentage points improvement on the challenging Blocksworld 4/5 planning benchmark while reducing inference costs.

## Method Summary
The method works by first generating diverse problem-solving approaches through LLM feedback solicitation, where the model acts as an autonomous ideation agent. These approaches are then incorporated into prompts either through multiple separate inference calls (DIV-SE) with majority voting aggregation, or combined within a single prompt (IDIV-SE) leveraging the autoregressive nature of LLMs. The approach diversity is designed to capture multiple valid reasoning paths for complex problems, improving accuracy through ensemble effects while maintaining reasonable computational costs.

## Key Results
- DIV-SE and IDIV-SE significantly outperform previous baselines on reasoning benchmarks
- DIV-SE achieves 29.6 percentage points improvement on Blocksworld 4/5 planning benchmark
- IDIV-SE provides comparable accuracy to DIV-SE with reduced inference costs
- Both methods demonstrate effective accuracy-cost trade-offs across multiple reasoning domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Soliciting LLM feedback generates diverse problem-solving approaches that improve reasoning accuracy
- **Mechanism:** The LLM acts as an autonomous ideation agent, producing multiple distinct reasoning strategies (e.g., direct calculation, visualization, algebra) for a given problem
- **Core assumption:** LLMs possess general expertise across domains and can self-generate valid reasoning strategies when prompted appropriately
- **Evidence anchors:** Abstract mentions automatic improvement of prompt diversity through LLM feedback; section 2.1 discusses LLMs encoding significant knowledge from multiple domains
- **Break condition:** If the LLM lacks sufficient domain knowledge or fails to generate valid approaches, diversity generation fails

### Mechanism 2
- **Claim:** Combining multiple reasoning approaches within a single prompt (IDIV-SE) achieves comparable accuracy to multiple inference calls while reducing cost
- **Mechanism:** Independent reasoning approaches can be chained in a single prompt, allowing the LLM to explore multiple solution paths without separate inference calls
- **Core assumption:** Different reasoning approaches are mutually independent and can be executed sequentially without interference
- **Evidence anchors:** Abstract describes IDIV-SE as combining approaches within a single inference call; section 2.2 notes approaches are often mutually independent
- **Break condition:** Error propagation occurs if earlier approach failures influence subsequent ones, or if approaches are not truly independent

### Mechanism 3
- **Claim:** Ensembling diverse reasoning paths through majority voting improves accuracy by capturing multiple valid solution strategies
- **Mechanism:** Different approaches represent different valid reasoning paths; aggregating their outputs through voting increases the probability of selecting the correct answer
- **Core assumption:** Multiple valid reasoning paths exist for complex problems, and different approaches will capture different subsets of these paths
- **Evidence anchors:** Abstract mentions ensembling diverse prompts via majority vote; section 2.2 describes majority voting aggregation
- **Break condition:** If all approaches converge on the same incorrect reasoning pattern, majority voting provides no benefit

## Foundational Learning

- **Concept:** Chain-of-Thought (CoT) prompting
  - **Why needed here:** DIV-SE and IDIV-SE build upon CoT by adding approach diversity; understanding CoT is essential for grasping the baseline comparison
  - **Quick check question:** What distinguishes zero-shot-CoT from few-shot-CoT prompting?

- **Concept:** Prompt engineering and template design
  - **Why needed here:** The paper relies heavily on carefully crafted prompts to extract approaches and structure reasoning; understanding prompt design is crucial
  - **Quick check question:** How does the template in Figure 6 guide the LLM to generate diverse approaches?

- **Concept:** Ensemble methods and aggregation strategies
  - **Why needed here:** The paper's core contribution involves ensembling diverse reasoning paths; understanding ensemble theory is important for evaluating effectiveness
  - **Quick check question:** Why might majority voting be suboptimal for aggregating diverse reasoning paths?

## Architecture Onboarding

- **Component map:** Approach generation module -> Prompt construction module -> Inference engine -> Aggregation module -> Evaluation pipeline
- **Critical path:** Generate diverse approaches via DIVERSE PROMPTING → Select top-performing approach-persona combinations → Construct prompts with augmented demonstrations → Execute inference (multiple calls or single combined prompt) → Aggregate responses via voting or meta-reasoning → Evaluate accuracy and cost trade-offs
- **Design tradeoffs:** DIV-SE offers higher accuracy but higher cost (multiple inference calls); IDIV-SE provides lower cost but potential error propagation from chained approaches; approach diversity vs. prompt complexity; persona selection influences reasoning style but adds optimization dimension
- **Failure signatures:** Low accuracy despite diverse approaches (approaches may not be truly diverse or valid); high cost with minimal improvement (approach generation may be inefficient); error propagation in IDIV-SE (approaches may not be independent); poor performance on simple tasks (diversity may not be necessary)
- **First 3 experiments:** Implement DIVERSE PROMPTING to extract approaches for a simple math benchmark (e.g., GSM8K); Test DIV-SE vs. baseline CoT on AQUA-RAT with varying ensemble sizes; Implement IDIV-SE and compare cost-accuracy trade-off against DIV-SE on Blocksworld 3

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the diversity of thought approach perform on tasks requiring non-numerical reasoning, such as creative writing or open-ended problem-solving?
- **Basis in paper:** The paper primarily focuses on numerical reasoning tasks and planning benchmarks, with limited exploration of non-numerical domains
- **Why unresolved:** Experiments were conducted on arithmetic reasoning, planning, and commonsense reasoning tasks, which may not fully capture performance on tasks requiring creative or abstract thinking
- **What evidence would resolve it:** Evaluating the approach on diverse tasks like creative writing prompts, open-ended problem-solving scenarios, or tasks requiring abstract reasoning would provide insights into its generalizability

### Open Question 2
- **Question:** How does the performance of diversity of thought approaches scale with larger language models, such as those with hundreds of billions of parameters?
- **Basis in paper:** The paper evaluates the approach on GPT-3.5, GPT-4, and LLaMA-2 70B, but does not explore the impact of scaling to larger models
- **Why unresolved:** Experiments were limited to models with up to 70 billion parameters, leaving uncertainty about how the approach would perform with significantly larger models
- **What evidence would resolve it:** Conducting experiments on models with hundreds of billions of parameters, such as GPT-3 175B or beyond, would clarify the scalability and potential limitations of the approach

### Open Question 3
- **Question:** Can the diversity of thought approach be extended to other modalities, such as image or audio data, to improve reasoning in multimodal tasks?
- **Basis in paper:** The paper focuses on text-based reasoning tasks and does not explore multimodal applications
- **Why unresolved:** The approach is designed for text-based reasoning, and its applicability to other modalities like images or audio remains unexplored
- **What evidence would resolve it:** Adapting the approach to multimodal tasks, such as visual reasoning or audio-based problem-solving, and evaluating its performance would demonstrate its potential for broader applications

## Limitations

- The paper assumes LLMs possess sufficient general expertise to generate diverse and valid reasoning approaches, but this capability may be domain-dependent and model-specific
- The independence assumption for IDIV-SE approaches lacks rigorous validation, potentially leading to error propagation
- No analysis of approach quality beyond simple majority voting aggregation

## Confidence

- **High confidence:** The core methodology of generating diverse approaches through LLM feedback is clearly specified and implementable
- **Medium confidence:** The effectiveness of diversity in improving reasoning accuracy is supported by empirical results, though the exact contribution of diversity versus other factors is unclear
- **Low confidence:** The theoretical guarantees for IDIV-SE's error independence and the general applicability of approach diversity across all reasoning domains

## Next Checks

1. **Error Propagation Analysis:** Systematically measure error rates when chaining approaches in IDIV-SE versus running approaches independently in DIV-SE on the same problems
2. **Approach Quality Evaluation:** Develop metrics to quantify the diversity and validity of generated approaches beyond simple success rates, examining whether approaches represent genuinely different reasoning paths
3. **Domain Transfer Test:** Apply DIV-SE and IDIV-SE to domains not included in training data (e.g., specialized scientific reasoning) to evaluate the robustness of the diversity generation mechanism