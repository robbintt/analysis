---
ver: rpa2
title: Measuring the Instability of Fine-Tuning
arxiv_id: '2302.07778'
source_url: https://arxiv.org/abs/2302.07778
tags:
- measures
- instability
- different
- ipwd
- ijsd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the instability of fine-tuning pre-trained
  language models on small datasets. While standard deviation of performance is commonly
  used to measure instability, the authors argue it is a narrow characterization.
---

# Measuring the Instability of Fine-Tuning

## Quick Facts
- arXiv ID: 2302.07778
- Source URL: https://arxiv.org/abs/2302.07778
- Reference count: 40
- Key outcome: Standard deviation alone is insufficient for characterizing fine-tuning instability; multiple measures at different granularity levels are needed for comprehensive assessment.

## Executive Summary
This paper examines the instability of fine-tuning pre-trained language models on small datasets, arguing that standard deviation of performance scores is a narrow characterization of instability. The authors propose six additional measures quantifying instability at different granularity levels: three prediction measures (pairwise disagreement, Fleiss' Kappa, pairwise Jensen-Shannon divergence) and three representation measures (SVCCA, OP, Linear-CKA). They develop a framework to evaluate the validity of these measures through convergent and concurrent validity tests. The study finds that measures at different granularity levels are not always consistent with each other and tend to differ more when models are more stable, suggesting the importance of using multiple measures for comprehensive instability assessment.

## Method Summary
The authors fine-tune BERT-large and RoBERTa-large models on RTE, MRPC, and CoLA tasks from the GLUE benchmark using 20 different random seeds. Models are trained for 5 epochs with AdamW optimizer, linear learning rate warmup and decay, batch size 16, dropout 0.1, and learning rate 2e-5. For each fine-tuned model, they compute seven instability measures: standard deviation, three prediction measures (pairwise disagreement, Fleiss' Kappa, Jensen-Shannon divergence), and three representation measures (SVCCA, orthogonal Procrustes distance, Linear-CKA). They then evaluate measure validity through convergent and concurrent validity tests and analyze consistency between measures using benchmarking and bootstrapping techniques.

## Key Results
- Standard deviation alone is insufficient for characterizing fine-tuning instability, as it cannot detect differences in prediction or representation when accuracy scores are identical.
- Measures at similar granularity levels show higher consistency with each other than measures at different granularity levels.
- Representation measures (Linear-CKA and OP) successfully distinguish successful from failed fine-tuning runs based on hidden state differences, while SVCCA fails to do so in bottom layers.
- Measures tend to differ more from each other when models are more stable, suggesting multiple measures are especially important for assessing stable models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using multiple instability measures at different granularity levels provides a more complete characterization of fine-tuning instability than standard deviation alone.
- Mechanism: Different measures capture distinct aspects of instability—prediction measures assess output variation while representation measures examine hidden state divergence. Coarse-grained measures like SD miss nuanced instabilities that fine-grained measures detect.
- Core assumption: Instability is multi-dimensional and cannot be fully characterized by a single scalar measure.
- Evidence anchors:
  - [abstract]: "However, most studies only used the standard deviation of performance scores (SD) as their measure, which is a narrow characterization of instability."
  - [section 3]: "SD can only offer very limited assessments. For example, classifiers can obtain the same accuracy score (i.e. zero SD) even when they neither make the same predictions on each example (prediction instability) nor have the same hidden representations (representation instability)."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.441, average citations=0.0. Top related titles include "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs" and "LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views."
- Break condition: When model instability is so extreme that all measures converge to maximum values, making granularity distinctions irrelevant.

### Mechanism 2
- Claim: Measures at similar granularity levels show higher consistency with each other than measures at different granularity levels.
- Mechanism: Fine-grained measures detect subtle differences between model runs that coarse-grained measures cannot capture, leading to lower correlation between different granularity types when models are stable.
- Core assumption: The consistency between measures depends on the magnitude of instability being measured.
- Evidence anchors:
  - [section 6.1]: "measures of similar granularity level tend to be consistent with each other... Ipwd and Iκ show better consistency with Ijsd (τ≥ 0.6) than with SD (τ =−0.2 for BERT on MRPC)."
  - [section 6.2]: "we observe that measures at closer granularity levels have higher correlations with each other."
  - [corpus]: Weak evidence - corpus neighbors focus on fine-tuning approaches but don't specifically address measure consistency relationships.
- Break condition: When all models are extremely unstable, making all measures saturated regardless of granularity.

### Mechanism 3
- Claim: Representation measures show good concurrent validity by distinguishing successful from failed fine-tuning runs based on hidden state differences.
- Mechanism: Failed runs exhibit vanishing gradients, causing less intensive weight updates and therefore lower representation instability in hidden states compared to successful runs.
- Core assumption: The degree of hidden state change during fine-tuning correlates with training success/failure.
- Evidence anchors:
  - [section 5.2]: "Mosbach et al. (2021) observe that failed runs suffer from vanishing gradients... compared with successful runs, failed runs bear lower representation instability."
  - [section 5.2]: "Linear-CKA and OP indeed indicate a lower instability in failed runs... SVCCA fails to distinguish successful and failed runs based on the representations in the bottom layers."
  - [corpus]: Weak evidence - corpus neighbors don't specifically address failed run characterization or validation methodology.
- Break condition: When gradient vanishing is not the primary cause of failure, or when other factors dominate the stability differences.

## Foundational Learning

- Concept: Measurement validity theory
  - Why needed here: The paper's framework for assessing instability measures relies on understanding convergent and concurrent validity concepts from measurement theory.
  - Quick check question: What are the two types of validity assessed in the paper, and what do they measure?

- Concept: Canonical correlation analysis (CCA)
  - Why needed here: SVCCA, one of the representation measures, is built upon CCA to compare neural representations across models.
  - Quick check question: How does CCA find meaningful subspaces in neural representations, and why is this important for measuring instability?

- Concept: Procrustes analysis
  - Why needed here: The orthogonal Procrustes distance (OP) measure uses this technique to align and compare hidden representations between models.
  - Quick check question: What is the mathematical objective of orthogonal Procrustes analysis, and how does it help measure representation instability?

## Architecture Onboarding

- Component map: Fine-tuning procedure -> Model outputs -> Prediction measures (SD, pairwise disagreement, Fleiss' Kappa, Jensen-Shannon divergence) + Hidden states -> Representation measures (SVCCA, OP, Linear-CKA) -> Validity framework (convergent and concurrent validity tests) + Consistency analysis (benchmarking, bootstrapping) -> Recommendations for measure selection

- Critical path: 1) Fine-tune models with varying random seeds, 2) Compute instability measures across all model pairs, 3) Assess measure validity through convergent and concurrent tests, 4) Analyze measure consistency via benchmarking and bootstrapping, 5) Generate recommendations for measure selection.

- Design tradeoffs: Using multiple measures increases computational cost but provides more comprehensive instability characterization. Fine-grained measures capture subtle instabilities but may be noisy when overall instability is high. Representation measures require access to hidden states, which may not be available in all deployment scenarios.

- Failure signatures: If all measures show perfect consistency regardless of instability level, this suggests the measures are redundant rather than complementary. If validity tests fail for most measures, the framework itself may be flawed. If bootstrapping shows inconsistent correlations across different datasets/IMMs, the measures may not generalize well.

- First 3 experiments:
  1. Replicate the standard deviation vs. multiple measures comparison on a new dataset to verify the narrow characterization claim.
  2. Test the validity framework on a generative model (e.g., BART) to check generalizability beyond classification.
  3. Perform an ablation study removing one measure type at a time to quantify the value added by each granularity level.

## Open Questions the Paper Calls Out
- The paper mentions extending the framework to larger datasets and generative models as future work, but doesn't explicitly call out specific open questions.

## Limitations
- The findings are based on a limited set of classification tasks (RTE, MRPC, CoLA) and two model architectures (BERT, RoBERTa), which may not generalize to other NLP tasks or model types.
- The validation framework depends on assumptions about gradient vanishing in failed runs that may not hold across all failure modes.
- The computational cost of representation measures limits their practical applicability in resource-constrained settings.

## Confidence
- High confidence: The core finding that standard deviation alone is insufficient for characterizing instability, and that different granularity levels capture distinct aspects of instability.
- Medium confidence: The consistency patterns between measures at different granularity levels, as this may vary with task difficulty and dataset size.
- Low confidence: The generalizability of the validity framework to other model architectures and tasks beyond the GLUE benchmark.

## Next Checks
1. Test the instability measure framework on a generative model (e.g., BART) on summarization tasks to verify generalizability beyond classification.
2. Conduct experiments with varying dataset sizes to determine at what point standard deviation becomes insufficient as an instability measure.
3. Perform cross-validation with different optimizers and learning rate schedules to assess robustness of the findings to training variations.