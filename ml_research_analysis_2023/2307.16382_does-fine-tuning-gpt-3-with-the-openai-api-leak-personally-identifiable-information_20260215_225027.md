---
ver: rpa2
title: Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable information?
arxiv_id: '2307.16382'
source_url: https://arxiv.org/abs/2307.16382
tags:
- fine-tuning
- gpt-3
- dataset
- data
- email
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study demonstrates that fine-tuning GPT-3 via OpenAI's API\
  \ can expose personally identifiable information (PII) from the training dataset.\
  \ Using two experimental setups\u2014a classification task and an email autocomplete\
  \ task\u2014the authors show that GPT-3 memorizes and generates PII, including names,\
  \ organizations, dates, and financial figures."
---

# Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable information?

## Quick Facts
- arXiv ID: 2307.16382
- Source URL: https://arxiv.org/abs/2307.16382
- Authors:
- Reference count: 7
- Key outcome: Fine-tuning GPT-3 via OpenAI's API can expose PII from training data, with precision/recall of 2.45-13.16% and 4.06-27.83%, respectively.

## Executive Summary
This study investigates whether fine-tuning GPT-3 through OpenAI's API can lead to the leakage of personally identifiable information (PII) from the training dataset. Using the Enron email corpus, the authors demonstrate that GPT-3 can memorize and reproduce sensitive information such as names, organizations, dates, and financial figures when prompted with appropriate contexts. The experiments reveal significant privacy risks, even with simple prompting strategies, highlighting the need for stronger privacy safeguards in fine-tuning practices.

## Method Summary
The authors fine-tune GPT-3 models using OpenAI's API on the Enron email dataset for two tasks: classification and email autocomplete. For the classification task, they fine-tune a model to categorize emails and then omit the separator token during inference to enable generation of arbitrary text. For the autocomplete task, they fine-tune a model to predict email body content given a subject line. Data extraction attacks are conducted by querying the fine-tuned models with prompts and analyzing the generated completions for PII, measuring precision and recall to quantify the extent of PII leakage.

## Key Results
- GPT-3 fine-tuning on sensitive datasets leads to memorization of PII that can be extracted through naive prompting.
- Omitting the separator token during inference enables generation of arbitrary text, exposing memorized PII.
- The autocomplete task exposes PII because the model associates subject lines with body content, allowing subject lines to cue generation of memorized PII-containing bodies.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning GPT-3 on sensitive datasets causes memorization of PII that can be extracted through naive prompting.
- Mechanism: During fine-tuning, the model updates its weights to fit the training data distribution, including rare and unique PII entries. The model's generative nature allows it to reproduce exact sequences from the training set when prompted with similar contexts.
- Core assumption: The fine-tuning process updates a sufficient number of model parameters to encode and later retrieve training data sequences, and the model lacks strong privacy-preserving regularization or differential privacy constraints.
- Evidence anchors:
  - [abstract] "Our findings reveal that fine-tuning GPT3 for both tasks led to the model memorizing and disclosing critical personally identifiable information (PII) obtained from the underlying fine-tuning dataset."
  - [section 2.4] "Previous studies have shown that training large language models without adequate privacy techniques can lead to severe risks of PII leakage"
  - [corpus] Weak: Related works cite leakage but not exact memorization mechanisms in OpenAI's fine-tuning API.
- Break condition: If the fine-tuning process uses differential privacy or strong regularization to suppress memorization of rare sequences, or if the model's capacity is insufficient to encode full PII entries.

### Mechanism 2
- Claim: Omitting the separator token during inference allows the classification model to generate arbitrary text, exposing memorized PII.
- Mechanism: During training, the separator token signals the model to output a classification label. When omitted during inference, the model continues generating as if in a completion task, producing longer sequences that may include memorized PII.
- Core assumption: The model's output is highly sensitive to the presence or absence of the separator token, and the fine-tuning process does not include safeguards against PII leakage in such edge cases.
- Evidence anchors:
  - [section 3.1] "To enable a generative model trained for classifications to produce text, we exclude the separator used during training from the end of prompts."
  - [corpus] Weak: No direct corpus evidence for separator-based PII leakage in GPT-3 fine-tuning.
- Break condition: If the model architecture or fine-tuning process explicitly handles separator omission to prevent generation of sensitive data.

### Mechanism 3
- Claim: The autocomplete task exposes PII because the model associates subject lines with body content, and subject lines can cue the model to generate memorized email bodies containing PII.
- Mechanism: During fine-tuning, the model learns the conditional distribution P(body|subject). If a subject line in a query matches or closely resembles a training subject, the model is likely to generate the corresponding memorized body, including any PII it contains.
- Core assumption: The model learns strong associations between subject lines and full email bodies, and the training set contains unique or rare subject lines that strongly cue specific PII-containing bodies.
- Evidence anchors:
  - [section 3.2] "The train set consists of the subjects of the emails in the training set, and the test set... are the subjects of the emails in a hold-out set... to determine if these PIIs can be unintentionally revealed."
  - [corpus] Weak: No direct corpus evidence for subject-to-body PII leakage in GPT-3 fine-tuning.
- Break condition: If the model uses attention mechanisms that de-emphasize exact subject-body matches or if the training process anonymizes PII in email bodies.

## Foundational Learning

- Concept: **Data extraction attacks** on language models
  - Why needed here: The paper's core contribution is demonstrating a data extraction attack on a fine-tuned GPT-3 model, showing how PII can be recovered from the training data.
  - Quick check question: What distinguishes a data extraction attack from a membership inference attack, and why is this distinction important for privacy in fine-tuning?

- Concept: **Differential privacy in machine learning**
  - Why needed here: The paper suggests the need for privacy-preserving fine-tuning techniques; understanding differential privacy is essential to evaluate and design such defenses.
  - Quick check question: How does adding calibrated noise during training reduce the risk of PII memorization, and what are the trade-offs for model utility?

- Concept: **Transformer model fine-tuning mechanics**
  - Why needed here: The attack exploits the fine-tuning process; understanding how GPT-3 is adapted for classification vs. generation tasks clarifies why PII can be leaked.
  - Quick check question: What changes in the model's weights during fine-tuning for a classification task, and how might this enable unintended generation of sensitive data?

## Architecture Onboarding

- Component map:
  OpenAI fine-tuning API -> GPT-3 (curie) model -> Custom training dataset (Enron emails) -> Prompted inference -> PII extraction pipeline

- Critical path:
  1. Prepare and upload fine-tuning dataset (prompts and completions)
  2. Submit fine-tuning job via OpenAI API
  3. Query fine-tuned model with test prompts (naive, blank, subject lines)
  4. Collect generated completions and filter for unique tokens
  5. Run PII extraction and evaluation (precision, recall)

- Design tradeoffs:
  - Using the Enron dataset provides realistic PII but may not generalize to all fine-tuning scenarios.
  - Excluding separator tokens enables generation but may not reflect typical API usage.
  - Manual PII filtering is precise but labor-intensive and may miss edge cases.

- Failure signatures:
  - Low precision/recall indicates either strong privacy safeguards or ineffective prompting.
  - No PII leakage despite known sensitive data in the dataset may indicate anonymization or differential privacy.
  - Inconsistent results across prompt types may signal sensitivity to prompt engineering.

- First 3 experiments:
  1. Replicate the classification task attack with varying prompt lengths and temperatures to map the attack surface.
  2. Test the autocomplete task with out-of-distribution subject lines to measure generalizability of PII leakage.
  3. Compare results with and without the separator token to confirm its role in enabling PII generation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective privacy-preserving fine-tuning techniques to mitigate PII leakage in large language models like GPT-3?
- Basis in paper: [inferred] The authors highlight the need to explore privacy-preserving fine-tuning models incorporating techniques like differential privacy and PII scrubbing to make language models resistant to extraction attacks.
- Why unresolved: The paper demonstrates the vulnerability of fine-tuned GPT-3 to PII extraction attacks but does not investigate or evaluate specific privacy-preserving techniques to mitigate this risk.
- What evidence would resolve it: Empirical studies comparing the effectiveness of various privacy-preserving fine-tuning techniques (e.g., differential privacy, PII scrubbing) in reducing PII leakage in large language models, with quantifiable metrics like precision and recall.

### Open Question 2
- Question: How does the size of the fine-tuning dataset impact the risk of PII leakage in large language models?
- Basis in paper: [inferred] The authors suggest that larger models are more prone to memorizing fine-tuning datasets and becoming vulnerable to extraction attacks, implying a potential relationship between dataset size and PII leakage risk.
- Why unresolved: The paper does not investigate the relationship between the size of the fine-tuning dataset and the extent of PII leakage in large language models.
- What evidence would resolve it: Empirical studies analyzing the correlation between fine-tuning dataset size and PII leakage risk, controlling for other factors like model size and fine-tuning techniques.

### Open Question 3
- Question: Are there specific types of PII that are more susceptible to extraction attacks in fine-tuned large language models?
- Basis in paper: [explicit] The authors provide a breakdown of leaked PIIs, showing that organization names and person names are memorized the most out of the categories examined.
- Why unresolved: While the paper identifies the types of PIIs most frequently leaked, it does not explore the underlying reasons for their susceptibility or compare their vulnerability across different fine-tuning tasks and model architectures.
- What evidence would resolve it: Empirical studies comparing the susceptibility of different PII types to extraction attacks across various fine-tuning tasks and model architectures, with explanations for the observed differences.

## Limitations
- The exact implementation details of OpenAI's fine-tuning API remain proprietary, limiting reproducibility and understanding of potential built-in privacy safeguards.
- The study relies on the Enron dataset, which may not represent all fine-tuning scenarios or data distributions.
- The manual PII extraction process introduces potential human error and may miss edge cases or subtle PII forms.

## Confidence
- **High confidence**: The core finding that GPT-3 fine-tuning can lead to PII memorization is well-supported by the experimental results across both classification and autocomplete tasks.
- **Medium confidence**: The mechanism explanations, particularly regarding separator token omission and subject-body associations, are plausible but not directly proven by the experimental results.
- **Low confidence**: The generalizability of these findings to other datasets, model sizes, or fine-tuning scenarios remains uncertain.

## Next Checks
1. Replicate the experiments using multiple diverse datasets (e.g., medical records, financial data) to assess whether PII leakage patterns hold across different domains and data types.
2. Conduct controlled experiments varying fine-tuning parameters (learning rate, batch size, training epochs) and compare results with and without differential privacy techniques to isolate the impact of specific implementation choices.
3. Test more sophisticated extraction techniques, such as gradient-based attacks or membership inference, to determine whether the observed PII leakage represents a fundamental vulnerability or is limited to naive prompting strategies.