---
ver: rpa2
title: Detecting Sexual Content at the Sentence Level in First Millennium Latin Texts
arxiv_id: '2309.14974'
source_url: https://arxiv.org/abs/2309.14974
tags:
- latin
- corpus
- sexual
- dataset
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of sentence-level semantic classification
  in Latin texts, specifically for detecting sexual content, which is challenging
  due to figurative language and lexical diversity across a 1200-year span of literature.
  The authors introduce a new dataset of ~2,500 manually annotated sentences and evaluate
  various deep learning architectures (LSTM, GRU, HAN, BERT pooling) with different
  embedding approaches (lexeme, lemma, character-level, contextual).
---

# Detecting Sexual Content at the Sentence Level in First Millennium Latin Texts

## Quick Facts
- **arXiv ID:** 2309.14974
- **Source URL:** https://arxiv.org/abs/2309.14974
- **Reference count:** 0
- **Primary result:** HAN with lemma embeddings achieves precision 85.33%, TPR 70.60%, outperforming token-based searches in Latin sexual content detection

## Executive Summary
This paper addresses sentence-level semantic classification for detecting sexual content in first millennium Latin texts, a challenging task due to figurative language and lexical diversity across 1200 years of literature. The authors introduce a new dataset of ~2,500 manually annotated sentences and evaluate various deep learning architectures (LSTM, GRU, HAN, BERT pooling) with different embedding approaches. HAN with lemma embeddings performs best, with attention mechanisms providing interpretable insights for human filtering. The approach remains effective even with smaller datasets, achieving precision 69% and TPR 51%.

## Method Summary
The authors tackle sentence-level semantic classification in Latin by creating a new dataset of ~2,513 manually annotated sentences containing sexual content. They evaluate multiple deep learning architectures including LSTM, GRU, Hierarchical Attention Network (HAN), and BERT pooling, using various embedding approaches: lexeme, lemma, character-level, and contextual (Latin BERT). The HAN model with lemma embeddings concatenates token-level and character-level embeddings, trained using AllenNLP with batch size 4, learning rate 1e-4, patience 5, and max epochs 50. The primary metrics are precision, True Positive Rate (TPR), True Negative Rate (TNR), and F1-score, with 10 runs per model using random seeds.

## Key Results
- HAN with lemma embeddings achieves the best performance: precision 85.33%, TPR 70.60%, outperforming token-based searches
- The stark contrast between token and lemma embeddings shows a 13-point TPR difference in HAN models
- Attention weights reveal interpretable patterns: 27% of exclamation marks, 35.6% of dots, 22.3% of semicolons, and 48.3% of question marks attract highest attention in negative samples
- The approach remains effective with smaller datasets: precision 69%, TPR 51%
- Adding sociolectal/idiolectal metadata leads to overfitting due to dataset biases, causing 10-20 point precision drops

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HAN with lemma embeddings captures semantic content more effectively than raw tokens or character-level approaches.
- Mechanism: Lemmatization reduces morphological noise while HAN's hierarchical attention focuses on key semantic tokens across the sentence.
- Core assumption: The morphological richness of Latin obscures semantic signals, and attention can identify the semantically important tokens even in figurative language.
- Evidence anchors:
  - [abstract] "We find that HAN with lemma embeddings performs best (precision 85.33%, TPR 70.60%), outperforming token-based searches."
  - [section 6.2] "The stark contrast between token and lemma embeddings is particularly striking, especially when examining TPR, where there is a notable difference of -13 points between the HAN models."
- Break condition: If the figurative language relies heavily on morphological forms that carry semantic weight, or if the attention mechanism fails to identify the correct tokens in novel figurative constructions.

### Mechanism 2
- Claim: HAN's attention mechanism can identify semantically relevant tokens in both positive and negative examples, making it interpretable for human filtering.
- Mechanism: The attention weights reveal which tokens the model considers important, allowing humans to verify model decisions and potentially identify false positives/negatives.
- Core assumption: Human experts can use attention weight patterns to distinguish between truly relevant and spurious correlations in the model's decisions.
- Evidence anchors:
  - [abstract] "The attention mechanism in HAN models provides interpretable insights for human filtering."
  - [section 6.2] "We discovered that in negative samples, punctuation significantly attract attention: 27% of exclamation marks, 35.6% of dots, 22.3% of semicolons, and 48.3% of question marks have the highest attention weights of their sentences."
- Break condition: If the attention patterns become too complex or rely on spurious correlations that humans cannot easily interpret.

### Mechanism 3
- Claim: Smaller datasets can still produce useful models for semantic classification, though with reduced performance.
- Mechanism: Even with limited training data, the HAN architecture can learn enough about semantic patterns to provide reasonable precision and recall.
- Core assumption: The semantic patterns in the training data are representative enough that a smaller sample still captures the essential features.
- Evidence anchors:
  - [abstract] "The approach remains effective even with smaller datasets (precision 69%, TPR 51%)."
  - [section 6.2] "While precision scores remain relatively consistent within the dataset, TPR scores reveal a noticeable 7-point difference between the top-ranked and second-ranked models."
- Break condition: If the training data is too small to capture the diversity of semantic patterns, or if the dataset is too biased toward specific authors or time periods.

## Foundational Learning

- Concept: Latin morphology and its impact on NLP
  - Why needed here: Latin's rich morphology creates challenges for semantic classification that must be addressed through techniques like lemmatization.
  - Quick check question: How does Latin's morphological complexity differ from English, and why does this matter for semantic classification?

- Concept: Hierarchical Attention Networks (HAN)
  - Why needed here: HAN's hierarchical structure and attention mechanism are key to the paper's success in identifying semantic content in Latin sentences.
  - Quick check question: What are the two levels of attention in HAN, and how do they work together to capture document-level semantics?

- Concept: Dataset bias and its effects on model performance
  - Why needed here: The paper shows how dataset biases (toward specific authors, time periods) can lead to overfitting when categorical features are included.
  - Quick check question: How can dataset biases affect model performance, and what strategies can be used to detect and mitigate them?

## Architecture Onboarding

- Component map: Input layer (lemma embeddings) → HAN encoder (hierarchical attention) → Linear classifier → Output
- Critical path: The attention mechanism in HAN is the critical path for capturing semantic content and providing interpretability
- Design tradeoffs: Lemmatization reduces noise but may lose some morphological information; HAN is more interpretable but potentially less powerful than transformers
- Failure signatures: Overfitting to specific authors, poor performance on figurative language, reliance on spurious correlations
- First 3 experiments:
  1. Compare HAN with lemma embeddings vs. HAN with token embeddings on a small validation set
  2. Test HAN with different levels of morphological preprocessing (lemmatization vs. stemming vs. raw tokens)
  3. Evaluate HAN's attention weights on a sample of positive and negative examples to check interpretability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the HAN model's attention mechanism specifically identify sexual semantics in sentences with complex figurative language?
- Basis in paper: [explicit] The authors note that HAN models consistently rank at the top and provide insights into the interpretability of the model's attention mechanism, particularly in identifying key attention weights related to sexual semantics.
- Why unresolved: While the paper demonstrates that HAN models are effective, it does not provide a detailed explanation of the specific features or patterns in the attention mechanism that lead to the identification of sexual semantics in complex figurative language.
- What evidence would resolve it: A detailed analysis of the attention weights in HAN models, showing how they correlate with specific linguistic features or patterns in sentences containing sexual semantics, would provide clarity.

### Open Question 2
- Question: What are the potential limitations of using lemma embeddings over lexeme embeddings in the context of sentence-level semantic classification for Latin texts?
- Basis in paper: [explicit] The authors observe a significant difference in performance between lemma and lexeme embeddings, particularly in True Positive Rate (TPR), and hypothesize that lemmatization mitigates noise in the dataset.
- Why unresolved: The paper does not explore the underlying reasons for the performance gap between lemma and lexeme embeddings or investigate whether this difference is consistent across other semantic fields or languages.
- What evidence would resolve it: Comparative studies using both lemma and lexeme embeddings across multiple semantic fields and languages would help determine the generalizability and limitations of each approach.

### Open Question 3
- Question: How does the inclusion of sociolectal and idiolectal metadata impact the model's performance in less biased datasets?
- Basis in paper: [inferred] The authors find that incorporating metadata leads to overfitting in their biased dataset but do not explore its potential benefits in a more balanced dataset.
- Why unresolved: The study focuses on a dataset with significant biases towards certain authors and time periods, leaving open the question of whether metadata embeddings could enhance performance in a less biased corpus.
- What evidence would resolve it: Experiments with a more balanced dataset, incorporating sociolectal and idiolectal metadata, would reveal whether these features can improve model performance without causing overfitting.

## Limitations

- Dataset bias: 42.1% of training examples come from a single author (Martial), creating potential overfitting to specific authorial styles
- Small dataset size: ~2,500 annotated sentences may not capture full diversity of sexual semantics across 1200-year timespan
- Limited generalizability: Findings may not extend to other semantic classification tasks or morphologically complex languages

## Confidence

**High Confidence:** HAN with lemma embeddings outperforming token-based approaches (precision 85.33% vs. 72.23%) is well-supported by experimental results and represents a clear, measurable improvement.

**Medium Confidence:** Smaller datasets producing useful models (precision 69%, TPR 51%) is supported but requires additional validation across different training set sizes.

**Low Confidence:** Generalizability to other semantic classification tasks in Latin or other morphologically complex languages remains untested.

## Next Checks

1. **Dataset Balance Validation:** Retrain the HAN model on a balanced dataset where each author contributes equally to training samples, then compare performance metrics to the original unbalanced results to quantify the impact of authorial bias.

2. **Temporal Generalization Test:** Evaluate model performance specifically on sentences from underrepresented time periods (pre-Augustan, post-4th century) versus overrepresented periods to measure temporal generalization capability.

3. **Cross-Linguistic Applicability:** Apply the lemma-based HAN approach to another morphologically rich language (e.g., Ancient Greek or Classical Arabic) with a similar semantic classification task to test whether the methodology generalizes beyond Latin.