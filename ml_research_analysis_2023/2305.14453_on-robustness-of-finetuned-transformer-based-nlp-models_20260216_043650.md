---
ver: rpa2
title: On Robustness of Finetuned Transformer-based NLP Models
arxiv_id: '2305.14453'
source_url: https://arxiv.org/abs/2305.14453
tags:
- stir
- pre-trained
- fine-tuned
- bert
- gpt-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares robustness across transformer models (BERT,
  GPT-2, T5) when finetuned for NLP tasks. It uses CKA and STIR to quantify similarity
  between pre-trained and finetuned representations, and measures robustness as change
  in performance under text perturbations.
---

# On Robustness of Finetuned Transformer-based NLP Models

## Quick Facts
- arXiv ID: 2305.14453
- Source URL: https://arxiv.org/abs/2305.14453
- Reference count: 40
- Key outcome: GPT-2 is most robust to text perturbations among BERT, GPT-2, and T5, with later layers showing greater finetuning impact than earlier ones

## Executive Summary
This paper systematically compares the robustness of three transformer architectures (BERT, GPT-2, T5) when finetuned for NLP tasks. Using Centered Kernel Alignment (CKA) and Shared Information Robustness (STIR) metrics, the study quantifies how finetuning changes model representations and how these representations withstand various text perturbations. The results reveal that later layers are more sensitive to finetuning than earlier ones, and that GPT-2 exhibits superior robustness compared to BERT and T5 across multiple perturbation types. The study also identifies that perturbations involving character changes, noun removal, and verb removal have the most significant impact on model performance.

## Method Summary
The study fine-tunes BERT, GPT-2, and T5 models on GLUE benchmark tasks, then extracts hidden states from both pre-trained and fine-tuned versions to compute CKA and STIR values layer by layer. Eight types of text perturbations (drop noun/verb, drop first/last, swap, change char, add text, bias) are applied to test sets with specified probabilities. Robustness is measured as the relative change in accuracy between perturbed and clean test sets. The analysis identifies which layers are most affected by each perturbation type and compares representation similarity across models and tasks.

## Key Results
- GPT-2 exhibits the highest robustness to text perturbations, followed by T5 and then BERT
- Later layers in all models show greater impact from finetuning than earlier layers
- Character changes, noun removal, and verb removal are the most impactful perturbation types
- Some layers consistently show high sensitivity across all models and perturbation types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Later layers in transformer models are more sensitive to finetuning than earlier layers.
- Mechanism: During finetuning, higher-level semantic representations (captured in later layers) are adapted more than low-level syntactic features (captured in earlier layers).
- Core assumption: Finetuning prioritizes task-specific features over general linguistic features.
- Evidence anchors:
  - [abstract] "Later layers are more affected by finetuning than earlier ones."
  - [section] "BERT and GPT-2 had the highest CKA values for the RTE and WNLI datasets, indicating that these models were least affected by finetuning."
  - [corpus] Weak evidence; related work focuses on bias analysis and model comparison but not layer-specific finetuning sensitivity.
- Break condition: If task requires preserving low-level linguistic features, earlier layers might show greater change.

### Mechanism 2
- Claim: GPT-2 exhibits greater robustness to text perturbations than BERT and T5.
- Mechanism: GPT-2's autoregressive pretraining and generative objectives create more generalizable representations that are less sensitive to input variations.
- Core assumption: Generative pretraining leads to more robust feature extraction.
- Evidence anchors:
  - [abstract] "GPT-2 representations are more robust than BERT and T5 across multiple types of input perturbation."
  - [section] "GPT-2 exhibited the highest robustness, with relatively high scores across seven perturbations."
  - [corpus] Weak evidence; related work mentions bias analysis but not comparative robustness across architectures.
- Break condition: If perturbation targets generative-specific patterns, GPT-2 might show decreased robustness.

### Mechanism 3
- Claim: Perturbations involving dropping nouns, verbs, or changing characters have the most significant impact on model performance.
- Mechanism: These perturbations directly affect the semantic content and lexical structure that models heavily rely on for prediction.
- Core assumption: Models depend heavily on parts-of-speech and character-level information for accurate predictions.
- Evidence anchors:
  - [abstract] "dropping nouns, verbs or changing characters are the most impactful."
  - [section] "all models showed significant sensitivity to perturbations except for bias" and "models were significantly affected by changing characters, followed by removing nouns and verbs."
  - [corpus] Weak evidence; related work focuses on bias analysis rather than perturbation impact studies.
- Break condition: If models are trained with data augmentation including these perturbations, impact might be reduced.

## Foundational Learning

- Concept: Representation similarity metrics (CKA and STIR)
  - Why needed here: To quantify changes between pre-trained and finetuned model representations across layers.
  - Quick check question: What does a CKA value of 1 indicate about two model representations?

- Concept: Text perturbation techniques
  - Why needed here: To systematically evaluate model robustness to various types of input variations.
  - Quick check question: Which perturbation type involves changing individual characters with a probability of 0.10?

- Concept: Layer-wise analysis
  - Why needed here: To understand how finetuning affects different levels of representation within transformer models.
  - Quick check question: Which layers (early or late) showed greater impact from finetuning according to the results?

## Architecture Onboarding

- Component map:
  Data pipeline: GLUE benchmark tasks → text perturbation generation → model inference
  Model components: Pre-trained transformer models (BERT, GPT-2, T5) → finetuned versions → hidden state extraction
  Analysis pipeline: Hidden state extraction → CKA/STIR calculation → robustness scoring

- Critical path:
  1. Extract hidden states from both pre-trained and finetuned models
  2. Compute CKA/STIR values layer by layer
  3. Apply perturbations to input text
  4. Calculate robustness scores based on performance degradation
  5. Identify most affected layers per perturbation type

- Design tradeoffs:
  - Layer-wise vs. whole-model comparison: Layer-wise provides more granular insights but requires more computation
  - Different perturbation types: Comprehensive coverage vs. computational cost
  - Using CKA vs. STIR: CKA is faster but STIR better captures shared invariances

- Failure signatures:
  - High CKA values with poor task performance: Model may have learned task-specific shortcuts
  - Low robustness scores across all perturbations: Model may be overfitting to training data
  - Inconsistent layer sensitivity patterns: Possible issues with model architecture or training procedure

- First 3 experiments:
  1. Compare CKA values between pre-trained and finetuned models on a single task (e.g., SST-2) to verify layer sensitivity patterns
  2. Apply one type of perturbation (e.g., drop nouns) to a small dataset and measure robustness score changes
  3. Identify most affected layers for a specific perturbation type and verify with logistic regression models trained on hidden states

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different combinations of text perturbations affect model robustness compared to individual perturbations?
- Basis in paper: [inferred] The paper mentions "Future directions include combining different perturbation types to validate robustness of NLP models" in the Future Work section, indicating this was not studied.
- Why unresolved: The study only examined individual perturbations, not their combinations.
- What evidence would resolve it: Experiments applying multiple perturbation types simultaneously and measuring their combined impact on model robustness across different architectures and tasks.

### Open Question 2
- Question: What specific mitigation strategies can be developed based on the perturbation analysis results?
- Basis in paper: [inferred] The paper states "Future directions include developing mitigation strategies based on our perturbation analysis results" in the Future Work section.
- Why unresolved: The study focused on analysis but did not implement or test any mitigation strategies.
- What evidence would resolve it: Implementation and evaluation of specific techniques (e.g., adversarial training, data augmentation, or architectural modifications) designed to improve robustness against the most impactful perturbations identified in the study.

### Open Question 3
- Question: How does model robustness transfer across different NLP tasks and datasets?
- Basis in paper: [inferred] The paper suggests exploring "the transferability of findings to other NLP tasks and datasets" in the Future Work section.
- Why unresolved: The study was limited to GLUE benchmark tasks and did not test robustness transfer.
- What evidence would resolve it: Testing finetuned models across multiple task families (e.g., dialogue, summarization, code generation) and measuring robustness consistency, or using domain adaptation techniques to assess transfer capabilities.

## Limitations

- Architecture comparison conflates pretraining objectives with architectural differences
- Results may not generalize beyond GLUE benchmark tasks
- Perturbation types examined represent only a subset of real-world text variations

## Confidence

**High Confidence:** The finding that later layers show greater sensitivity to finetuning is well-supported by the CKA analysis and aligns with established understanding of transformer layer functions.

**Medium Confidence:** The relative robustness ranking of GPT-2 > T5 > BERT is supported by experimental results but may be influenced by confounding architectural factors.

**Medium Confidence:** The identification of character changes, noun removal, and verb removal as most impactful perturbations is based on empirical results, but the study doesn't explore why these perturbations are most disruptive.

## Next Checks

1. **Architecture-controlled comparison:** Reproduce the robustness analysis using models with similar architectures but different pretraining objectives (e.g., BERT vs. RoBERTa, or GPT-2 vs. GPT-3) to isolate the effect of pretraining strategy on robustness.

2. **Cross-domain validation:** Apply the same analysis pipeline to non-GLUE datasets from different domains (biomedical text, legal documents, social media) to assess whether the observed robustness patterns generalize beyond the benchmark tasks.

3. **Adversarial perturbation extension:** Supplement the current perturbation set with adversarial examples specifically designed to target each model's weaknesses, testing whether the relative robustness ranking holds under more sophisticated attack patterns.