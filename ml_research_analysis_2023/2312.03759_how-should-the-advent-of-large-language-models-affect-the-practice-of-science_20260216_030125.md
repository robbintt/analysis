---
ver: rpa2
title: How should the advent of large language models affect the practice of science?
arxiv_id: '2312.03759'
source_url: https://arxiv.org/abs/2312.03759
tags:
- llms
- science
- scienti
- should
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents four perspectives on how large language models
  (LLMs) should affect the practice of science. Schulz et al.
---

# How should the advent of large language models affect the practice of science?

## Quick Facts
- arXiv ID: 2312.03759
- Source URL: https://arxiv.org/abs/2312.03759
- Reference count: 40
- Key outcome: The paper presents four perspectives on how large language models (LLMs) should affect the practice of science, emphasizing both their potential benefits and risks.

## Executive Summary
This paper presents four distinct perspectives on how the advent of large language models (LLMs) should affect scientific practice. Schulz et al. argue LLMs function more as human collaborators than mere software tools, with potential to enhance scientific writing, data analysis, and code generation. Bender et al. caution against overhyping LLMs, highlighting their limitations in tasks requiring genuine understanding and accountability. Marelli et al. emphasize the critical importance of transparency, accountability, and fairness when incorporating LLMs into scientific workflows. Botvinick and Gershman maintain that humans should retain responsibility for determining the scientific roadmap, particularly regarding normative and epistemic aspects. The paper ultimately calls for a balanced approach that considers both the benefits and risks of integrating LLMs into scientific practice.

## Method Summary
This paper presents an opinion piece featuring four distinct perspectives from different groups of scientists on how large language models should affect scientific practice. Rather than presenting empirical research, the authors facilitate a discussion by having each group articulate their viewpoint on LLM integration in science, followed by responses to each other's perspectives. The work synthesizes these viewpoints to highlight key considerations, challenges, and opportunities for the scientific community as LLMs become increasingly prevalent in research workflows.

## Key Results
- LLMs can potentially improve scientific writing, data analysis, and code generation when used appropriately
- The opacity of proprietary LLMs poses significant challenges to scientific reproducibility and transparency
- LLM-generated content can propagate errors and biases into scientific literature without proper verification
- Human researchers must maintain critical oversight and accountability when using LLMs in scientific work

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated content can propagate errors and biases into scientific literature when used without verification.
- Mechanism: LLMs learn statistical patterns from large text corpora, which include factual errors, outdated information, and societal biases. When scientists use LLM outputs directly in manuscripts, these flaws can be embedded in the literature without the authors' awareness, undermining scientific integrity.
- Core assumption: LLMs do not inherently verify factual accuracy or detect bias; they generate text that appears plausible based on training data patterns.
- Evidence anchors:
  - [abstract] "LLMs learn from web-sourced text data, acquiring inherent biases... and — in some cases — replicate excerpts from their training data"
  - [section] "When an LLM is used for data analysis, what happens when it hallucinates data? The content generated by LLMs can contain errors or fabricated information, presenting a potential threat to the integrity of scientific publishing"
  - [corpus] Corpus neighbors include discussions of LLMs' potential to introduce errors in scientific writing and the need for verification
- Break Condition: The mechanism breaks when robust verification systems are in place that can automatically detect factual errors, fabricated information, and biases in LLM-generated content before publication.

### Mechanism 2
- Claim: LLMs can serve as effective tools for accelerating scientific workflows when used as complements to human expertise rather than replacements.
- Mechanism: LLMs can handle routine tasks like literature summarization, code generation, and initial drafting, freeing researchers to focus on higher-level analysis and interpretation. This division of labor increases overall productivity without compromising scientific quality.
- Core assumption: Human researchers maintain critical oversight and verification of all LLM-generated outputs, ensuring that the final work meets scientific standards.
- Evidence anchors:
  - [abstract] "Schulz et al. argue LLMs are more like human collaborators than software tools, emphasizing their potential to improve scientific writing, data analysis, and code generation"
  - [section] "researchers might experiment with employing LLMs at certain stages of research with progressively reduced supervision, potentially leading to increased automation in some aspects of scientific exploration and discovery"
  - [corpus] Corpus neighbors discuss LLMs' potential to enhance productivity in scientific research when used appropriately
- Break Condition: The mechanism breaks when over-reliance on LLMs leads to reduced critical thinking and verification by human researchers, or when the quality of LLM outputs degrades due to training data contamination or model degradation.

### Mechanism 3
- Claim: The opacity of proprietary LLMs poses significant challenges to scientific reproducibility and transparency.
- Mechanism: When scientific work relies on closed-source LLMs whose internal workings and training data are not disclosed, other researchers cannot reproduce the methods or understand potential sources of bias or error, violating fundamental scientific principles.
- Core assumption: Scientific reproducibility requires full transparency about methods, including the specific models, parameters, and training data used in analysis.
- Evidence anchors:
  - [abstract] "when an LLM helps us to write text, who ensures that its output is not subject to plagiarism issues?" and "LLMs learn from web-sourced text data, acquiring inherent biases"
  - [section] "as many LLMs are proprietary, working with them poses a threat to this ideal. Nobody guarantees that OpenAI, Google, or other providers will not make changes to their models"
  - [corpus] Corpus neighbors include discussions of the need for transparency and reproducibility in scientific research using LLMs
- Break Condition: The mechanism breaks when open-source alternatives become widely available with transparent documentation of their architecture, training data, and implementation details, or when scientific communities establish acceptable standards for using proprietary models while maintaining reproducibility.

## Foundational Learning

- Concept: Statistical language modeling
  - Why needed here: Understanding that LLMs generate text based on statistical patterns rather than true comprehension is crucial for evaluating their limitations and appropriate use cases
  - Quick check question: If an LLM generates a factually incorrect statement, is it because the model "understands" incorrectly or because it's following statistical patterns that don't reflect reality?

- Concept: Scientific reproducibility and transparency
  - Why needed here: The discussion of LLMs in science raises fundamental questions about what constitutes reproducible research and how to maintain transparency when using complex AI systems
  - Quick check question: If a research paper uses an LLM for data analysis but doesn't disclose which model or version was used, can other researchers reproduce the exact same results?

- Concept: Human-AI collaboration dynamics
  - Why needed here: The perspectives presented frame LLMs as either tools or collaborators, requiring understanding of how humans can effectively work with AI systems while maintaining appropriate oversight
  - Quick check question: What are the key differences between treating an LLM as a tool versus a collaborator, and how do these differences affect accountability for errors?

## Architecture Onboarding

- Component map: LLM interfaces (API or local models) -> Verification pipelines (fact-checking, bias detection) -> Documentation systems (prompt tracking, output logging) -> Integration points with existing scientific workflows (manuscript preparation, data analysis)
- Critical path: Input generation -> LLM processing -> Output verification -> Documentation -> Integration -> Review
- Design tradeoffs: Proprietary vs. open-source models (performance vs. transparency), automation level vs. human oversight (efficiency vs. quality control), general vs. specialized models (versatility vs. task-specific accuracy)
- Failure signatures: Factual errors in published papers, inability to reproduce results, introduction of biases into scientific literature, reduced critical thinking among researchers
- First 3 experiments:
  1. Implement a simple verification pipeline that checks LLM-generated scientific claims against known databases and flags potential errors
  2. Develop documentation templates for tracking LLM usage in research workflows, including prompts, model versions, and verification steps
  3. Create a benchmark comparing the quality of LLM-assisted vs. traditional scientific writing across multiple disciplines, measuring both efficiency and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should the scientific community balance the benefits of using LLMs with the potential risks to scientific integrity and reproducibility?
- Basis in paper: [explicit] The paper discusses the potential benefits of LLMs, such as improving scientific writing and data analysis, but also highlights risks like plagiarism, data fabrication, and loss of transparency.
- Why unresolved: The paper presents multiple perspectives, some advocating for cautious use of LLMs while others emphasize their potential benefits. There is no consensus on the optimal balance.
- What evidence would resolve it: Studies quantifying the impact of LLM use on scientific output quality, reproducibility rates, and instances of misconduct would help determine the appropriate balance.

### Open Question 2
- Question: How can the scientific community ensure transparency and accountability when using LLMs, given their proprietary nature and potential for bias?
- Basis in paper: [explicit] The paper emphasizes the importance of transparency, accountability, and fairness when using LLMs. It discusses challenges like ensuring transparency in LLM-generated text and addressing biases.
- Why unresolved: The paper highlights the need for clear principles and guidelines, but there is no established framework for ensuring transparency and accountability in LLM use.
- What evidence would resolve it: Development and evaluation of best practices for LLM use, including guidelines for disclosure, bias mitigation, and quality control, would help address these concerns.

### Open Question 3
- Question: What is the optimal role of LLMs in the scientific process, and how can we ensure that they complement rather than replace human expertise?
- Basis in paper: [explicit] The paper presents different perspectives on the role of LLMs, with some arguing for their use as collaborators and others emphasizing the importance of human judgment and understanding.
- Why unresolved: The paper highlights the potential benefits of LLMs but also raises concerns about their limitations and the need for human oversight. There is no clear consensus on the optimal balance between LLM use and human expertise.
- What evidence would resolve it: Studies comparing the performance of LLMs and human scientists in various tasks, as well as evaluations of the impact of LLM use on scientific understanding and creativity, would help determine the optimal role of LLMs.

## Limitations

- The paper presents opinion-based perspectives rather than empirical research, limiting the evidence base for its conclusions
- No systematic studies are provided on error rates, bias propagation, or productivity gains when LLMs are used in actual scientific workflows
- The paper does not offer concrete guidelines or frameworks for implementing the various perspectives on LLM use in practice

## Confidence

- **High Confidence**: The assertion that LLMs can propagate errors and biases when used without verification (Mechanism 1). This is well-established from multiple studies on LLM outputs and their limitations.
- **Medium Confidence**: The claim that LLMs can accelerate scientific workflows when used as complements to human expertise (Mechanism 2). While plausible based on anecdotal evidence, systematic studies of productivity gains are limited.
- **Medium Confidence**: The concern about proprietary LLMs and scientific reproducibility (Mechanism 3). The theoretical argument is sound, but the practical impact depends on how scientific communities adapt their standards and whether open-source alternatives become viable.

## Next Checks

1. Conduct a controlled study comparing error rates in scientific papers written with and without LLM assistance, measuring both factual accuracy and bias introduction.
2. Implement and test verification pipelines for LLM-generated scientific content using multiple fact-checking databases and bias detection tools.
3. Survey practicing scientists about their experiences with LLMs in research workflows, focusing on both productivity gains and challenges with verification and reproducibility.