---
ver: rpa2
title: Semi-Autoregressive Streaming ASR With Label Context
arxiv_id: '2309.10926'
source_url: https://arxiv.org/abs/2309.10926
tags:
- streaming
- speech
- block
- accuracy
- proc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of improving streaming ASR accuracy
  for low-latency applications. The core method idea is to introduce a semi-autoregressive
  (SAR) ASR model that incorporates labels predicted from previous blocks as additional
  context using a Language Model (LM) subnetwork.
---

# Semi-Autoregressive Streaming ASR With Label Context

## Quick Facts
- **arXiv ID**: 2309.10926
- **Source URL**: https://arxiv.org/abs/2309.10926
- **Reference count**: 0
- **Primary result**: Streaming SAR model improves accuracy by 19% relative on Tedlium2, 16%/8% on Librispeech-100 clean/other, and 19%/8% on SWB/CH test sets while reducing latency by 2.5x

## Executive Summary
This paper addresses the challenge of improving streaming ASR accuracy while maintaining low latency by proposing a semi-autoregressive (SAR) model that incorporates previously predicted labels as additional context through a Language Model (LM) subnetwork. The model performs greedy NAR decoding within blocks while maintaining autoregressive properties across blocks, using an alignment greedy decoding algorithm to handle insertion and deletion errors near block boundaries. Experiments demonstrate significant accuracy improvements over streaming NAR models across multiple datasets while achieving substantial latency reduction.

## Method Summary
The streaming SAR model uses a Contextual Block Encoder (CBE) that processes overlapping blocks of speech features, incorporating label context embeddings from an LM subnetwork and previous acoustic context embeddings. The LM subnetwork, typically an LSTM pre-trained on transcripts, encodes previously emitted labels to provide additional context for the current block. Training uses frame-level cross-entropy loss with forced alignments as proxy ground truth, optionally augmented with intermediate CTC loss and random block regularization. Inference employs alignment greedy decoding that moves trailing non-blank frames from the current block to the next to handle token boundary errors. The approach achieves a better approximation of the original posterior distribution by relaxing the NAR conditional independence assumption within blocks while preserving autoregressive properties globally.

## Key Results
- Streaming SAR outperforms streaming NAR by 19% relative on Tedlium2 test sets
- Achieves 16%/8% improvement on Librispeech-100 clean/other test sets
- Reduces accuracy gap with streaming AR and non-streaming NAR models while achieving 2.5x lower latency

## Why This Works (Mechanism)

### Mechanism 1
The streaming SAR model reduces the accuracy gap with streaming AR and non-streaming NAR models while maintaining low latency. By incorporating label context embeddings through an LM subnetwork, the model conditions on previously emitted tokens while still outputting tokens concurrently within each block. This relaxes the NAR conditional independence assumption within blocks while preserving the autoregressive property globally, better approximating the original posterior distribution.

### Mechanism 2
The alignment greedy decoding algorithm improves ASR accuracy by addressing insertion and deletion errors near block boundaries. The decoding strategy removes the last non-blank frames from the current block and adds them to the next block if the current block ends with non-blank frames, effectively handling cases where segment boundaries appear in the middle of a token.

### Mechanism 3
Pre-training the LM subnetwork on external text data further improves streaming ASR accuracy. The LM subnetwork is pre-trained using a causal LM objective on training transcripts and external text data, allowing it to provide better label context embeddings by leveraging additional language information beyond the speech transcripts alone.

## Foundational Learning

- **Connectionist Temporal Classification (CTC) loss**
  - Why needed here: CTC loss is a non-autoregressive methodology that allows the model to output tokens concurrently while achieving good transcription accuracy.
  - Quick check question: How does CTC loss differ from cross-entropy loss in terms of modeling assumptions and output probabilities?

- **Transformer architecture**
  - Why needed here: The transformer architecture is used for the encoder and decoder components of the streaming SAR model, enabling efficient parallel processing and capturing long-range dependencies.
  - Quick check question: What are the key differences between the transformer architecture and recurrent neural networks (RNNs) in terms of their ability to handle long sequences and parallelization?

- **Language modeling**
  - Why needed here: The LM subnetwork is pre-trained using a causal LM objective on text data to improve its language modeling capabilities and provide better label context embeddings for the streaming SAR model.
  - Quick check question: How does the pre-training of the LM subnetwork on external text data contribute to the overall performance of the streaming SAR model?

## Architecture Onboarding

- **Component map**: Input speech features → Contextual Block Encoder → LM subnetwork → Label context embedding → Contextual Block Encoder (with label context) → Output transcription

- **Critical path**: Input speech features → Contextual Block Encoder → LM subnetwork → Label context embedding → Contextual Block Encoder (with label context) → Output transcription

- **Design tradeoffs**: The use of a streaming NAR model with blockwise attention allows for low latency but sacrifices some accuracy compared to non-streaming models. Incorporating the label context embedding through the LM subnetwork improves accuracy but adds computational overhead. The choice of block size and hop size affects the trade-off between latency and accuracy.

- **Failure signatures**: High WER may indicate the model is not effectively incorporating label context or the decoding algorithm is not handling block boundaries correctly. High latency may result from block size or hop size being too small, or the LM subnetwork being too computationally expensive. Poor generalization may occur if external text data pre-training is ineffective or forced alignments are inaccurate.

- **First 3 experiments**:
  1. Train the streaming SAR model with and without the LM subnetwork to quantify the impact of incorporating label context on accuracy.
  2. Compare the performance of the alignment greedy decoding algorithm with the overlap decoding algorithm from prior work to assess the effectiveness of the proposed decoding strategy.
  3. Train the LM subnetwork with and without external text data pre-training to evaluate the impact of leveraging additional language information on overall model performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of block size (Lblock) and hop size (Lhop) impact the trade-off between accuracy and latency in streaming SAR models?
- Basis in paper: [explicit] The paper mentions using Lblock = 40 and Lhop = 16 in experiments but doesn't explore the impact of varying these parameters.
- Why unresolved: The paper doesn't systematically study how different block and hop sizes affect the model's performance.
- What evidence would resolve it: Experiments varying Lblock and Lhop values while measuring both accuracy and latency metrics.

### Open Question 2
- Question: What is the impact of using different types of external text data for pre-training the LM subnetwork?
- Basis in paper: [explicit] The paper mentions using Librispeech and Fisher transcripts for pre-training but doesn't explore other types of text data or their relative effectiveness.
- Why unresolved: The paper only demonstrates the effectiveness of external text data without comparing different data sources.
- What evidence would resolve it: Experiments using various text data sources (e.g., domain-specific texts, social media data) and comparing their impact on streaming ASR accuracy.

### Open Question 3
- Question: How does the streaming SAR model perform on extremely long utterances compared to non-streaming models?
- Basis in paper: [inferred] The paper mentions that Transformer-based models have quadratic memory growth with input length, but doesn't test the SAR model's performance on very long utterances.
- Why unresolved: The paper focuses on datasets with relatively standard utterance lengths and doesn't explore edge cases with very long speech segments.
- What evidence would resolve it: Experiments using datasets with significantly longer utterances (e.g., audiobooks, lectures) comparing streaming SAR with non-streaming models.

## Limitations

- Dependency on accurate frame-level alignments as proxy ground truth for training, which may not perfectly align with block-based processing
- Computational overhead introduced by the LM subnetwork for label context encoding, particularly when pre-trained on external text data
- Effectiveness of the alignment greedy decoding algorithm relies heavily on the assumption that most errors occur at token boundaries

## Confidence

- **High Confidence**: The core mechanism of using previously predicted labels as additional context through an LM subnetwork is well-supported by architectural description and experimental results showing consistent WER improvements across multiple datasets
- **Medium Confidence**: The effectiveness of the alignment greedy decoding algorithm is supported by qualitative observations about error patterns, but lacks extensive quantitative ablation studies comparing it directly with other decoding strategies
- **Medium Confidence**: The claim about pre-training the LM subnetwork on external text data improving accuracy is demonstrated on Librispeech-100 and Switchboard datasets, but the extent of improvement may vary depending on the quality and relevance of external text data

## Next Checks

1. **Ablation Study on Alignment Quality**: Systematically evaluate the impact of alignment accuracy on model performance by training and testing with varying quality alignments (e.g., using different NAR baselines or different alignment methods) to quantify the sensitivity to this critical training component.

2. **Computational Efficiency Analysis**: Measure and compare the computational overhead (FLOPs, inference time per block, memory usage) of the streaming SAR model with and without the LM subnetwork, particularly when using external text data, to assess real-world deployment feasibility.

3. **Cross-Domain Generalization Test**: Evaluate the model on out-of-domain datasets (e.g., different accents, noisy environments, or multilingual scenarios) to verify the robustness of the label context mechanism and alignment greedy decoding across diverse acoustic conditions.