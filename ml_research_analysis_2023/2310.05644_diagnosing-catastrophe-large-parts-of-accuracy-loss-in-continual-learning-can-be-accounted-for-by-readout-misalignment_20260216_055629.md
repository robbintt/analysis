---
ver: rpa2
title: 'Diagnosing Catastrophe: Large parts of accuracy loss in continual learning
  can be accounted for by readout misalignment'
arxiv_id: '2310.05644'
source_url: https://arxiv.org/abs/2310.05644
tags:
- learning
- performance
- tasks
- representations
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates catastrophic forgetting in continual learning
  by analyzing representational changes in artificial neural networks. Using a four-layer
  convolutional network trained sequentially on CIFAR-100 tasks, the authors distinguish
  three components of performance loss: true forgetting (loss of task-relevant information),
  readout misalignment (shift in hidden representations), and geometric changes (rotation/translation/scaling
  of representational geometry).'
---

# Diagnosing Catastrophe: Large parts of accuracy loss in continual learning can be accounted for by readout misalignment

## Quick Facts
- arXiv ID: 2310.05644
- Source URL: https://arxiv.org/abs/2310.05644
- Reference count: 2
- Large parts of accuracy loss in continual learning can be accounted for by readout misalignment

## Executive Summary
This study investigates catastrophic forgetting in continual learning by analyzing representational changes in artificial neural networks. Using a four-layer convolutional network trained sequentially on CIFAR-100 tasks, the authors distinguish three components of performance loss: true forgetting (loss of task-relevant information), readout misalignment (shift in hidden representations), and geometric changes (rotation/translation/scaling of representational geometry). They find that readout misalignment accounts for the largest portion of accuracy loss, while actual forgetting is minimal. Procrustes alignment partially recovers performance, indicating that representational geometry is largely preserved. The analysis shows that network width modulates all three types of representational change, with wider networks experiencing less degradation.

## Method Summary
The authors use a four-layer convolutional network (architecture from Zenke et al. 2017) pretrained on CIFAR-10, then sequentially trained on CIFAR-100 tasks. After each training phase, they compute three performance metrics: standard classification accuracy, diagnostic readout accuracy using task-specific linear classifiers, and Procrustes-aligned accuracy. They vary the width of the final hidden layer (64, 128, 256, 512, 1024, 2048) and repeat the full experiment 5 times with different class-to-task assignments.

## Key Results
- Readout misalignment accounts for the largest portion of performance degradation in continual learning
- Representational geometry is largely preserved during catastrophic forgetting, with Procrustes alignment recovering approximately half of performance loss
- Network width modulates all three types of representational changes, with wider networks experiencing less degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Readout misalignment accounts for the largest portion of performance degradation in continual learning
- Mechanism: As networks learn new tasks sequentially, hidden representations shift relative to the fixed readout weights trained on the original task, causing performance to drop even though task-relevant information is preserved in the representations
- Core assumption: The network maintains task-relevant information in hidden layers even while learning new tasks
- Evidence anchors:
  - [abstract] "The largest component is a misalignment between hidden representations and readout layers"
  - [section] "performance of diagnostic readouts decreases much less, indicating that the discriminability of the old classes is indeed preserved" and "The primary cause of decreased performance is readout misalignment"
  - [corpus] Weak evidence - no directly relevant citations found
- Break condition: If hidden representations completely overwrite or corrupt task-relevant information during new task learning

### Mechanism 2
- Claim: Representational geometry is largely preserved during catastrophic forgetting
- Mechanism: The Procrustes alignment procedure can recover approximately half of the performance loss, indicating that the geometric structure of representations is maintained despite shifts in absolute position
- Core assumption: Geometric transformations (rotation, translation, scaling) account for a substantial portion of representational changes
- Evidence anchors:
  - [abstract] "Representational geometry is partially conserved under this misalignment and only a small part of the information is irrecoverably lost"
  - [section] "we observe that aligning representations accounts for approximately half of the performance difference between continual and diagnostic readouts"
  - [corpus] Weak evidence - no directly relevant citations found
- Break condition: If representational changes involve non-linear transformations that cannot be captured by Procrustes alignment

### Mechanism 3
- Claim: Network width modulates all three types of representational changes
- Mechanism: Wider networks experience less degradation in representational geometry and readout misalignment, suggesting that capacity provides redundancy that protects against catastrophic forgetting
- Core assumption: Increased representational capacity provides buffer space that reduces interference between tasks
- Evidence anchors:
  - [abstract] "All types of representational changes scale with the dimensionality of hidden representations"
  - [section] "we find that varying the width of the final hidden layer attenuates all three measures of representational change"
  - [corpus] Weak evidence - no directly relevant citations found
- Break condition: If network width effects are primarily due to regularization rather than capacity effects

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding the phenomenon being studied is essential to interpret the results about readout misalignment
  - Quick check question: What happens to performance on task A when a network trained on A is then trained on task B?

- Concept: Procrustes transformation and geometric alignment
  - Why needed here: The analysis relies on distinguishing between geometric changes (which can be aligned) and true forgetting (which cannot)
  - Quick check question: What type of transformations can Procrustes alignment correct?

- Concept: Diagnostic readouts and forgetting measurement
  - Why needed here: The experimental design uses diagnostic readouts to separate forgetting from misalignment
  - Quick check question: How does a diagnostic readout differ from the original readout in measuring performance?

## Architecture Onboarding

- Component map:
  Four-layer CNN (pretrained on CIFAR-10) -> Sequential training on CIFAR-100 tasks -> Diagnostic readout training -> Procrustes alignment analysis

- Critical path:
  1. Pretrain on CIFAR-10
  2. Sequentially train on CIFAR-100 task splits
  3. After each phase, train diagnostic readouts for all tasks
  4. Measure continual, diagnostic, and Procrustes-aligned performance
  5. Vary network width and repeat analysis

- Design tradeoffs:
  - Diagnostic readout training adds computational overhead but enables separation of forgetting types
  - Procrustes alignment assumes linear geometric transformations are sufficient
  - Task similarity is controlled by random class assignment to tasks

- Failure signatures:
  - If diagnostic readouts perform as poorly as continual readouts → true forgetting dominates
  - If Procrustes alignment provides no performance recovery → non-linear representational changes
  - If width variation shows no effect → capacity is not the limiting factor

- First 3 experiments:
  1. Replicate the basic continual learning setup with CIFAR-10 pretraining and sequential CIFAR-100 training
  2. Implement diagnostic readout training and measure the three performance types (continual, diagnostic, Procrustes)
  3. Vary the width of the final hidden layer and observe changes in the three performance components

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can readout misalignment be effectively solved without restricting network plasticity or constraining representations for new tasks?
- Basis in paper: [explicit] The authors hypothesize that catastrophic forgetting may be efficiently addressed by solving the problem of readout misalignment without influencing the learning of new tasks
- Why unresolved: While the paper identifies readout misalignment as the primary cause of performance degradation, it does not propose specific solutions for addressing this issue without restricting learning dynamics
- What evidence would resolve it: Successful implementation of algorithms that maintain diagnostic readout performance while allowing unconstrained learning of new tasks, demonstrating improved continual learning without sacrificing plasticity

### Open Question 2
- Question: How do the representational changes observed in artificial neural networks during continual learning compare to those in biological visual systems?
- Basis in paper: [explicit] The authors note that primate visual systems successfully learn new tasks without exhibiting forgetting, and suggest testing their analysis framework on biological data to understand differences
- Why unresolved: The paper only analyzes artificial neural networks and suggests future work should test the framework on biological data, but no such comparison has been conducted
- What evidence would resolve it: Direct comparison of representational geometry changes in primate neural recordings during sequential task learning versus artificial neural network behavior, revealing similarities and differences in how biological and artificial systems handle continual learning

### Open Question 3
- Question: What is the relationship between network width and the different components of representational change (forgetting, misalignment, and geometric changes) across a wider range of architectures?
- Basis in paper: [explicit] The authors find that varying the width of the final hidden layer attenuates all three measures of representational change, but note that small amounts of change still occur even in wider networks
- Why unresolved: The study only varies one dimension (width of final hidden layer) in a specific four-layer architecture, leaving open questions about how different architectural choices might affect these components
- What evidence would resolve it: Systematic investigation of how different architectural parameters (depth, width at multiple layers, attention mechanisms) affect each component of representational change across various network designs

## Limitations
- The use of CIFAR-100 with random class assignments may not reflect realistic task boundaries
- Procrustes alignment assumes linear geometric transformations are sufficient, potentially missing more complex representational changes
- Width variation analysis shows correlations but cannot definitively establish causation between capacity and forgetting mitigation

## Confidence
- High confidence in the diagnostic readout methodology and its ability to separate forgetting types
- Medium confidence in the Procrustes alignment interpretation due to potential non-linear effects
- Medium confidence in width variation results due to correlation-causation ambiguity

## Next Checks
1. Test the analysis on datasets with more naturalistic task boundaries (e.g., permuted MNIST, split CIFAR-10 with semantic groupings)
2. Implement non-linear alignment methods (e.g., optimal transport) to verify Procrustes limitations
3. Conduct ablation studies varying training hyperparameters to isolate the effect of network capacity versus regularization