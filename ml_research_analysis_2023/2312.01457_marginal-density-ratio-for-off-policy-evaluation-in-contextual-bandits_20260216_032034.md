---
ver: rpa2
title: Marginal Density Ratio for Off-Policy Evaluation in Contextual Bandits
arxiv_id: '2312.01457'
source_url: https://arxiv.org/abs/2312.01457
tags:
- estimator
- variance
- policy
- data
- shift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses high variance in existing off-policy evaluation
  (OPE) methods for contextual bandits, particularly when policy overlap is low or
  action/context spaces are large. It introduces the Marginal Ratio (MR) estimator,
  which focuses on shifts in the marginal distribution of outcomes rather than policy
  ratios.
---

# Marginal Density Ratio for Off-Policy Evaluation in Contextual Bandits

## Quick Facts
- **arXiv ID**: 2312.01457
- **Source URL**: https://arxiv.org/abs/2312.01457
- **Reference count**: 40
- **Primary result**: MR estimator achieves significantly lower MSE than IPW and DR methods across synthetic and real-world datasets

## Executive Summary
This paper addresses the high variance problem in off-policy evaluation (OPE) for contextual bandits by introducing the Marginal Ratio (MR) estimator. Traditional methods like Inverse Propensity Weighting (IPW) and Doubly Robust (DR) suffer from high variance when policy overlap is low or action/context spaces are large. The MR estimator instead focuses on shifts in the marginal distribution of outcomes rather than policy ratios, achieving lower variance by directly weighting outcomes based on changes in the marginal outcome distribution. The paper establishes a theoretical connection between MR and Marginalized Inverse Propensity Score (MIPS) estimators, proving MR achieves lower variance among this family. Experiments demonstrate MR's superior performance with up to an order of magnitude reduction in MSE compared to state-of-the-art methods.

## Method Summary
The Marginal Ratio (MR) estimator addresses high variance in OPE by focusing on the marginal density ratio w(y) = pπ∗(y)/pπb(y) instead of policy ratios π∗(a|x)/πb(a|x). The method estimates w(y) through regression on logged data, then computes the weighted average of outcomes using this ratio. Unlike IPW which requires estimating high-dimensional policy ratios, MR only needs to estimate a 1D function of the outcome. The estimator is defined as ˆθMR = (1/n)∑w(yi)yi where w(y) is learned by solving a regression problem that minimizes the difference between the estimated and true marginal ratios. This approach reduces variance by avoiding unnecessary weighting from changes in context-action distributions that don't affect the outcome.

## Key Results
- MR estimator achieves significantly lower MSE than IPW and DR methods across various settings
- Performance gains are most pronounced when policy overlap is low or action/context spaces are large
- MR provides more data-efficient estimation of Average Treatment Effects (ATE) in causal inference settings
- On real-world datasets, MR achieves up to an order of magnitude reduction in MSE compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The MR estimator achieves lower variance than IPW by focusing on the shift in the marginal distribution of outcomes Y rather than the joint distribution of (X, A, Y).
- **Mechanism:** Instead of weighting samples by policy ratios π∗(a|x)/πb(a|x), MR weights by the marginal density ratio pπ∗(y)/pπb(y). This captures only the change in outcome distribution, avoiding unnecessary variance from changes in context-action distributions that don't affect Y.
- **Core assumption:** The shift in the marginal distribution of Y is smaller than the shift in the joint distribution of (X, A, Y), and the marginal density ratio can be estimated accurately.
- **Evidence anchors:**
  - [abstract]: "which focuses on the shift in the marginal distribution of outcomes Y instead of the policies themselves."
  - [section 3.1]: "we prove that by only considering the shift in the marginal distribution of outcomes, the MR estimator achieves a lower variance than the standard OPE methods."
- **Break condition:** If the marginal distribution of Y changes drastically relative to the joint distribution, or if pπ∗(y)/pπb(y) is difficult to estimate accurately, the variance advantage diminishes.

### Mechanism 2
- **Claim:** MR estimator achieves lower variance than DR by reducing the variance term involving the conditional expectation over both X and A.
- **Mechanism:** DR variance includes a term Varπb[ρ(A,X)Y | Y] that involves both context and action. MR avoids this by directly modeling the marginal outcome shift, effectively reducing the conditioning set from (X,A) to just Y.
- **Core assumption:** Varπb[ρ(A,X)Y | Y] > Varπb[ρ(A,X)µ(A,X) | X] on average, which is more likely when the context space X is high-dimensional.
- **Evidence anchors:**
  - [section 3.1]: "if Varπb [ρ(A, X) Y | Y ] is greater than Varπb [ρ(A, X) µ(A, X) | X] on average, the variance of the MR estimator will be less than that of the DR estimator."
  - [section 3.1]: "This variance is likely to be large when the policy shift between πb and π∗ is large, or when the dimensions of contexts X and/or the actions A is large."
- **Break condition:** If the outcome model µ(A,X) is highly accurate and the context space is low-dimensional, DR might perform comparably or better.

### Mechanism 3
- **Claim:** MR estimator achieves lower variance than MIPS by considering the marginal shift in Y rather than a lower-dimensional embedding of (X,A).
- **Mechanism:** MIPS uses an embedding R of (X,A) that summarizes relevant information about Y. MR uses Y itself as the embedding, which contains the minimal necessary information and thus yields the smallest variance.
- **Core assumption:** There exists a representation R of (X,A) such that Y ⊥ ⊥(X,A) | R, and the embedding R=Y provides the tightest bound on variance.
- **Evidence anchors:**
  - [section 3.1.1]: "the MR estimator achieves lower variance than the MIPS estimator and doesn't require new assumptions."
  - [section D]: "the variance of MIPS estimator decreases as the representation gets closer to Y in terms of information content."
- **Break condition:** If the embedding E used in MIPS captures exactly the same information as Y (which is unlikely), the variance advantage would vanish.

## Foundational Learning

- **Concept:** Importance sampling and its variance properties
  - **Why needed here:** Understanding why IPW suffers from high variance when policy overlap is low is crucial for appreciating MR's approach.
  - **Quick check question:** Why does the variance of IPW increase when the target and behavior policies have low overlap?

- **Concept:** Doubly robust estimation and control variates
  - **Why needed here:** DR combines importance sampling with outcome modeling to reduce variance, providing context for how MR compares.
  - **Quick check question:** How does DR use an outcome model as a control variate to reduce variance compared to IPW?

- **Concept:** Conditional independence and causal inference
  - **Why needed here:** MR's connection to ATE estimation and MIPS's action embedding assumption both rely on understanding conditional independence relationships.
  - **Quick check question:** What does it mean for Y to be independent of A given some representation R, and why is this useful?

## Architecture Onboarding

- **Component map:** Logged data -> Behavior policy estimator -> Marginal ratio estimator -> MR estimator -> OPE estimate
- **Critical path:**
  1. Split logged data into training (for estimation) and evaluation (for OPE) sets
  2. Estimate behavior policy bπb(a|x) using training data
  3. Estimate marginal ratios ˆw(y) by solving regression problem Eπb[ˆρ(A,X)|Y=y]
  4. Apply MR estimator to evaluation data to estimate Eπ∗[Y]
  5. Compare results against baselines

- **Design tradeoffs:**
  - MR trades off potential bias from estimating ˆw(y) against variance reduction
  - MR requires estimating only a 1D function (w(y)) vs. high-dimensional policy ratios
  - MR uses all evaluation data vs. IPW/DR which use only data with A=a for ATE

- **Failure signatures:**
  - High bias in MR estimate indicates poor estimation of ˆw(y)
  - Performance similar to IPW suggests the marginal shift isn't smaller than joint shift
  - Poor performance in high-dimensional action spaces may indicate need for better embedding

- **First 3 experiments:**
  1. Synthetic data with known behavior policy and varying policy shift α* - verify MR achieves lower variance than IPW/DR
  2. Synthetic data with high-dimensional context and action spaces - test MR's robustness to dimensionality
  3. ATE estimation on Twins dataset - demonstrate MR's data efficiency compared to IPW/DR

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unresolved based on the content:
- The optimal architecture and training strategy for estimating marginal ratio weights w(y)
- Performance of MR in extremely high-dimensional action spaces
- Extensions to continuous or structured action spaces
- Comparison with other marginal importance sampling techniques in RL

## Limitations
- MR's performance depends on accurate estimation of the marginal ratio w(y), which may be challenging in practice
- The variance advantages assume the marginal distribution shift is smaller than the joint distribution shift
- The paper focuses on discrete action spaces and doesn't address continuous or structured action spaces
- Empirical validation is limited to specific synthetic and real-world settings

## Confidence
- **High confidence**: MR achieves lower variance than IPW in theory (proven via variance decomposition)
- **Medium confidence**: MR outperforms DR in practice (supported by experiments but depends on implementation details)
- **Medium confidence**: Connection to MIPS is theoretically sound (proven via conditional independence arguments)

## Next Checks
1. **Ablation study on embedding dimensionality**: Systematically test MR against MIPS variants using different embeddings of (X,A) to quantify the exact variance reduction from using Y directly.

2. **Bias-variance tradeoff analysis**: Measure both bias and variance of MR across different sample sizes to characterize the full MSE behavior, particularly when the marginal ratio estimator is imperfect.

3. **Cross-domain robustness test**: Evaluate MR on additional real-world datasets (e.g., recommendation systems, clinical trials) to verify performance gains generalize beyond the tested domains.