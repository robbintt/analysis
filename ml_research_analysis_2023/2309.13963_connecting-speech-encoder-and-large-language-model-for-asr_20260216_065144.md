---
ver: rpa2
title: Connecting Speech Encoder and Large Language Model for ASR
arxiv_id: '2309.13963'
source_url: https://arxiv.org/abs/2309.13963
tags:
- speech
- llms
- q-former
- were
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comparative study of three commonly used
  structures as connectors, including fully connected layers, multi-head cross-attention,
  and Q-Former, for connecting a speech encoder with a large language model (LLM)
  for automatic speech recognition (ASR). The study shows that LLMs with Q-Formers
  demonstrate consistent and considerable word error rate (WER) reductions over LLMs
  with other connector structures.
---

# Connecting Speech Encoder and Large Language Model for ASR

## Quick Facts
- arXiv ID: 2309.13963
- Source URL: https://arxiv.org/abs/2309.13963
- Authors:
- Reference count: 0
- Key outcome: Q-Former connectors achieve 12% relative WER reduction on Eval2000 and 17% on 90-second speech compared to Whisper baseline

## Executive Summary
This paper presents a comparative study of three connector structures for bridging speech encoders and large language models (LLMs) in automatic speech recognition systems. The study evaluates fully connected layers, multi-head cross-attention, and Q-Former connectors, finding that Q-Former consistently achieves superior word error rate (WER) performance. The authors also introduce a segment-level Q-Former to handle speech inputs longer than the encoder's duration limit, achieving significant improvements on long-form speech data.

## Method Summary
The study compares three connector structures: fully connected layers, multi-head cross-attention, and Q-Former, for connecting pre-trained speech encoders (Whisper series) with frozen LLMs (Vicuna series) in ASR systems. The Q-Former uses a transformer-based architecture with cross-attention between trainable query embeddings and input features, converting variable-length speech sequences into fixed-length output tokens. A segment-level Q-Former variant is proposed to handle speech inputs exceeding the encoder's 30-second duration limit by processing segments independently and concatenating outputs. The random concatenation training strategy is employed to reduce deletion errors for long speech inputs.

## Key Results
- Q-Former connectors demonstrate consistent WER reductions over fully connected layers and multi-head cross-attention across multiple test sets
- LLMs with Q-Formers achieve 12% relative WER reduction on Eval2000 test set without using in-domain training data
- Segment-level Q-Former results in 17% relative WER reductions over other connector structures on 90-second-long speech data
- Increasing output tokens from 40 to 80 in Q-Former considerably reduces WERs on LibriSpeech test-clean

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Q-Former connector enables LLMs to achieve lower WERs compared to fully connected layers and multi-head cross-attention
- Mechanism: The Q-Former uses transformer-based cross-attention between trainable query embeddings and input features, allowing the LLM to receive more informative representations from the speech encoder
- Core assumption: Transformer-based cross-attention can retain more relevant information from speech inputs than simpler compression methods
- Evidence anchors:
  - [abstract] "LLMs with Q-Formers demonstrated consistent and considerable word error rate (WER) reductions over LLMs with other connector structures."
  - [section] "The Q-Former uses a transformer-based module converting variable-length input sequences into fixed-length output query representations..."
- Break condition: If transformer-based cross-attention cannot effectively align speech and text feature spaces, or if information compression leads to loss of critical ASR-relevant information

### Mechanism 2
- Claim: Segment-level Q-Former allows LLMs to process speech inputs longer than the encoder's duration limit
- Mechanism: By splitting long speech sequences into segments and processing each segment independently with the same Q-Former, then concatenating the outputs, the seg-QF can handle variable-length speech inputs while maintaining fixed-length output per segment
- Core assumption: The speech encoder can effectively process shorter segments independently without losing crucial context information
- Evidence anchors:
  - [abstract] "Moreover, a novel segment-level Q-Former is proposed to enable LLMs to recognise speech segments with a duration exceeding the limitation of the encoders..."
  - [section] "To enable LLMs to process with longer speech inputs, the whole sequence can be split into several shorter segments to transform by the speech encoder separately..."
- Break condition: If splitting sequences causes loss of critical cross-segment dependencies, or if concatenated output fails to provide coherent input for the LLM

### Mechanism 3
- Claim: Random concatenation training strategy reduces deletion errors for long speech inputs
- Mechanism: By concatenating utterances during training to create longer sequences with varying lengths, the model learns to handle speech inputs that exceed the encoder's window size, reducing reliance on padding
- Core assumption: Training on varied sequence lengths helps the model generalize better to long-form speech inputs during inference
- Evidence anchors:
  - [abstract] "The random concatenation strategy is applied, which is similar to that used in [37]..."
  - [section] "To resolve this issue, a random concatenation strategy is applied... Each input utterance in a training mini-batch is concatenated with a number of utterances randomly selected from the whole training set."
- Break condition: If random concatenation introduces unrealistic speech patterns that don't generalize to real long-form speech inputs

## Foundational Learning

- Concept: Transformer-based cross-attention mechanisms
  - Why needed here: The Q-Former relies on cross-attention between speech encoder outputs and LLM input spaces, which is fundamental to its superior performance
  - Quick check question: How does cross-attention differ from self-attention, and why is it particularly useful for modality alignment?

- Concept: Sequence length limitations in speech encoders
  - Why needed here: Understanding the 30-second limitation of Whisper encoder is crucial for why segment-level Q-Former was necessary
  - Quick check question: What are the typical architectural reasons for sequence length limitations in transformer-based speech encoders?

- Concept: Information compression in ASR pipelines
  - Why needed here: The connector's ability to compress speech features while retaining information directly impacts ASR performance
  - Quick check question: How do different compression strategies (fully connected vs. attention-based) affect the preservation of phonetic and linguistic information?

## Architecture Onboarding

- Component map: Frozen speech encoder (Whisper series) → Connector (Q-Former or variants) → Frozen LLM (Vicuna series) → ASR output
- Critical path: Speech encoder output → Connector processing → LLM input → Transcription generation
- Design tradeoffs:
  - Q-Former vs. fully connected layers: Q-Former has better WER but higher parameter count (24.5M vs 23.6M for FC with 300 tokens)
  - Number of output tokens: More tokens (80 vs 60) improve WER but increase computation
  - Segment-level vs. standard Q-Former: Segment-level handles longer inputs but may lose cross-segment context
- Failure signatures:
  - High deletion rates: Often indicates connector failing to preserve speech information
  - Hallucinations in long sequences: Suggests LLM struggling with concatenated segment outputs
  - Degraded performance on out-of-domain data: May indicate overfitting to training domain
- First 3 experiments:
  1. Compare WER of Q-Former vs. fully connected layers on LibriSpeech test-clean with 80 output tokens
  2. Test segment-level Q-Former on concatenated 60-second sequences vs. standard Q-Former
  3. Vary the number of Q-Former output tokens (40, 60, 80, 160) and measure WER changes on test-clean

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Q-Former connectors compare to other connector types when scaling to even larger speech encoders and language models beyond those tested in this study?
- Basis in paper: [explicit] The paper states that increasing the sizes of both speech encoder and LLM can result in lower WERs, but notes that doubling the size of the speech encoder reduces the WERs more obviously than doubling the size of LLMs.
- Why unresolved: The study only tested up to Whisper large-v2 encoder and Vicuna 13B LLM, leaving uncertainty about performance at scales beyond these models.
- What evidence would resolve it: Empirical results comparing Q-Former performance with larger speech encoders and LLMs (e.g., Whisper large-v2 vs. future larger versions, Vicuna 13B vs. larger LLMs) on the same datasets.

### Open Question 2
- Question: What is the optimal number of trainable queries in the Q-Former for speech inputs significantly longer than 30 seconds, and how does this impact the overall ASR performance?
- Basis in paper: [explicit] The paper shows that increasing the number of trainable queries in the Q-Former from 40 to 80 considerably reduces WERs, but does not explore the impact of even more queries for longer speech inputs.
- Why unresolved: The study focused on inputs with less than 30 seconds and did not investigate the effects of varying the number of trainable queries for longer speech inputs.
- What evidence would resolve it: Comparative analysis of WERs with different numbers of trainable queries in the Q-Former for speech inputs exceeding 30 seconds, including inputs up to several minutes long.

### Open Question 3
- Question: How does the segment-level Q-Former perform on speech inputs with varying levels of background noise or in different acoustic environments compared to the standard Q-Former?
- Basis in paper: [inferred] The paper introduces the segment-level Q-Former to handle long-form speech inputs, but does not explore its robustness to different acoustic conditions or background noise levels.
- Why unresolved: The study focuses on the duration limitation of the pre-trained speech encoder and does not address the performance of the segment-level Q-Former in diverse acoustic environments.
- What evidence would resolve it: Experimental results comparing the segment-level Q-Former's WERs with the standard Q-Former on datasets with varying levels of background noise or in different acoustic environments.

## Limitations

- Primary experiments conducted on LibriSpeech, a relatively clean and controlled dataset, limiting generalizability to real-world scenarios
- Several critical implementation details are underspecified, including exact Q-Former architecture parameters and training hyperparameters
- Limited comparative analysis against other state-of-the-art ASR systems beyond the Whisper baseline

## Confidence

- **High Confidence**: The comparative performance ranking showing Q-Former > multi-head cross-attention > fully connected layers is well-supported by multiple experiments across different test sets
- **Medium Confidence**: The 12% relative WER improvement on Eval2000 and 17% improvement on 90-second speech data are promising but based on limited testing conditions
- **Low Confidence**: The exact architectural contributions of individual Q-Former components and optimal configuration for different use cases remain unclear due to limited ablation studies

## Next Checks

1. **Cross-Domain Robustness Test**: Evaluate the Q-Former connector on challenging real-world datasets like Common Voice, TED-LIUM, or spontaneous conversational speech to assess performance beyond clean read speech

2. **Ablation Architecture Analysis**: Conduct systematic ablation studies varying Q-Former depth, output token count, and attention mechanisms to quantify the contribution of each architectural component to observed WER improvements

3. **Real-Time Inference Benchmarking**: Measure and compare the computational efficiency (latency, memory usage) of Q-Former versus simpler connectors at inference time, particularly for the segment-level variant handling long sequences, to assess practical deployment viability