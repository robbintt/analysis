---
ver: rpa2
title: Even Small Correlation and Diversity Shifts Pose Dataset-Bias Issues
arxiv_id: '2305.05807'
source_url: https://arxiv.org/abs/2305.05807
tags:
- shifts
- diversity
- correlation
- training
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for analyzing the joint effects
  of diversity and correlation shifts in deep learning datasets. It introduces synthetic
  correlation shifts via colored squares and diversity shifts through subclass divisions
  in datasets like ImageNet.
---

# Even Small Correlation and Diversity Shifts Pose Dataset-Bias Issues

## Quick Facts
- arXiv ID: 2305.05807
- Source URL: https://arxiv.org/abs/2305.05807
- Reference count: 36
- Primary result: Even small correlation biases significantly affect model performance, but diversity shifts can reduce reliance on spurious correlations

## Executive Summary
This paper investigates how joint correlation and diversity shifts in deep learning datasets affect model performance and generalization. Through synthetic and real-world experiments, the authors demonstrate that even subtle spurious correlations can significantly impact model behavior, while diversity shifts can surprisingly reduce reliance on these correlations. The study introduces a framework for analyzing these shifts using colored squares as synthetic correlation features and subclass divisions for diversity shifts, revealing important insights about model robustness and bias propagation.

## Method Summary
The paper proposes a framework for analyzing joint effects of correlation and diversity shifts using three experimental scenarios: synthetic correlation/diversity shifts on ImageNet, synthetic correlation with real diversity shifts using HAM10000 and BCN20000 datasets, and real correlation/diversity shifts using ISIC 2019 trap sets. The method involves generating synthetic correlation shifts via colored squares, creating diversity shifts through subclass divisions, and evaluating model performance across four test scenarios (same-same, same-diff, no shortcuts, diff) using AUC metrics on fine-tuned ResNet models.

## Key Results
- Models learn and propagate correlation shifts even with low training bias, risking accumulation of weak biases
- Diversity shifts reduce reliance on spurious correlations, contrary to expectations
- Models learn robust features even in high-bias scenarios but use spurious features when available in test data
- Small correlation biases can significantly affect model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models learn and propagate correlation shifts even with low training bias, leading to reliance on spurious correlations.
- Mechanism: Deep neural networks memorize spurious correlations during training and apply them at inference, even when the correlation is weak (low training bias).
- Core assumption: The model's capacity allows it to detect and memorize subtle spurious correlations in the training data.
- Evidence anchors:
  - [abstract] "Models learn and propagate correlation shifts even with low-bias training; this poses a risk of accumulating and combining unaccountable weak biases"
  - [section] "Increasing training bias directly affect the performances for same-same and same-diff test sets. As expected, performances for the former increase, and decrease for the latter, showing models' reliance on the introduced spurious feature"

### Mechanism 2
- Claim: Diversity shifts can reduce reliance on spurious correlations.
- Mechanism: When test data contains previously unseen patterns (diversity shift), the model cannot fully rely on spurious features and must use more invariant features for classification.
- Core assumption: The presence of novel patterns in test data forces the model to utilize learned invariant features rather than spurious ones.
- Evidence anchors:
  - [abstract] "Diversity shift can reduce the reliance on spurious correlations; this is counter-intuitive since we expect biased models to depend more on biases when invariant features are missing"
  - [section] "When the test set does not present precisely the same shortcuts as training, the performance remains stable even when the training set is heavily biased"

### Mechanism 3
- Claim: Models learn robust features even in high-bias scenarios but use spurious features if available in test data.
- Mechanism: The model's architecture allows it to learn both robust and spurious features. It uses robust features when spurious ones are unavailable, but relies on spurious features when they are present.
- Core assumption: The model's capacity allows it to learn multiple types of features simultaneously.
- Evidence anchors:
  - [abstract] "Models learn robust features in high- and low-bias scenarios but use spurious ones if test samples have them"
  - [section] "Models fully learn robust features even in high-bias scenarios, but rely on spurious ones if test samples display the spurious feature"

## Foundational Learning

- Concept: Distribution shift
  - Why needed here: Understanding how data distribution changes between training and test sets is crucial for analyzing model performance.
  - Quick check question: What is the difference between correlation shift and diversity shift?

- Concept: Spurious correlation
  - Why needed here: Spurious correlations are the core focus of this paper, as they affect model performance and generalization.
  - Quick check question: How can spurious correlations be introduced in a dataset?

- Concept: Invariant features
  - Why needed here: Invariant features are the robust features that models should learn to generalize well to unseen data.
  - Quick check question: Why are invariant features important for model generalization?

## Architecture Onboarding

- Component map: Data generation -> Model training -> Evaluation -> Analysis of results
- Critical path: Synthetic data generation → Model training with controlled bias → Evaluation on four test scenarios → Analysis of correlation and diversity shift effects
- Design tradeoffs: Balancing between synthetic and real data, controlling correlation and diversity shifts, choosing appropriate model architectures
- Failure signatures: Over-reliance on spurious correlations, poor generalization to diverse test sets, inability to learn robust features
- First 3 experiments:
  1. Generate synthetic dataset with low training bias and evaluate on same-same, same-diff, no shortcuts, and diff test sets
  2. Introduce diversity shift in test sets and compare performance with non-shifted test sets
  3. Use real-world dataset (HAM10000) and evaluate on out-of-distribution test sets (BCN20000, Derm7pt-Clinical)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more effective methods to remove or reduce correlation shifts in deep learning datasets, considering the significant influence of subtle biases on model performance?
- Basis in paper: [explicit] The paper states that current methods to remove or reduce correlation shifts are ineffective or infeasible because they require human annotations of the sources of bias, which are impossible to provide for all the subtle correlations in data.

### Open Question 2
- Question: Can diversity shift attenuation of correlation shift be consistently observed across different datasets and problem domains, or is it specific to the datasets and scenarios studied in this paper?
- Basis in paper: [inferred] The paper mentions that diversity shift can reduce the reliance on spurious correlations, which is counter-intuitive. However, it is not clear whether this effect is consistent across different datasets and problem domains.

### Open Question 3
- Question: How can we develop methods to exploit the ability of models to learn robust features in high-bias scenarios to improve model performance and generalization?
- Basis in paper: [explicit] The paper mentions that models can learn robust features even in high-bias scenarios, but it does not provide specific methods for exploiting this ability.

## Limitations
- Synthetic data generation controls may not fully capture real-world complexity of correlation and diversity shifts
- Limited evaluation to binary classification tasks may not generalize to multi-class scenarios
- Focus on ResNet architectures may not represent broader model family behaviors

## Confidence
- **High confidence**: Core findings about correlation shifts affecting performance even at low bias levels, and models learning robust features in high-bias scenarios
- **Medium confidence**: The counter-intuitive finding that diversity shifts reduce spurious correlation reliance, requiring further validation across different domains
- **Low confidence**: Generalizability of results to non-medical domains and more complex classification tasks

## Next Checks
1. Replicate experiments with additional model architectures (Vision Transformers, ConvNeXt) to assess architectural dependencies
2. Validate findings on multi-class classification tasks beyond the binary setup
3. Test with alternative diversity shift mechanisms (e.g., domain adaptation, style transfer) to confirm the robustness of diversity shift effects