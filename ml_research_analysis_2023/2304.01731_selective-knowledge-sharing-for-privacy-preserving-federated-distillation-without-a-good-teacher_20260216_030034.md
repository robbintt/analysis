---
ver: rpa2
title: Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without
  A Good Teacher
arxiv_id: '2304.01731'
source_url: https://arxiv.org/abs/2304.01731
tags:
- proxy
- knowledge
- local
- client
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of effective knowledge sharing
  in federated distillation without a well-trained teacher model. The authors propose
  a selective knowledge sharing mechanism called Selective-FD that uses client-side
  selectors and a server-side selector to filter out misleading and ambiguous knowledge
  during the federated distillation process.
---

# Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without A Good Teacher

## Quick Facts
- arXiv ID: 2304.01731
- Source URL: https://arxiv.org/abs/2304.01731
- Authors: 
- Reference count: 40
- Primary result: Selective knowledge sharing mechanism improves federated distillation accuracy by up to 19.42% while reducing communication overhead by up to 900×

## Executive Summary
This paper addresses the challenge of effective knowledge sharing in federated distillation when teacher models are not well-trained. The authors propose Selective-FD, a selective knowledge sharing mechanism that uses client-side selectors with density-ratio estimation and a server-side selector to filter out misleading and ambiguous knowledge during federated distillation. The approach significantly improves accuracy while reducing communication overhead by filtering out-of-distribution samples and high-entropy ensemble predictions. The method is evaluated on a pneumonia detection task and three benchmark image classification tasks in non-IID settings.

## Method Summary
Selective-FD implements a dual-filtering mechanism for federated distillation. Client-side selectors use density-ratio estimation (specifically KuLSIF) to identify out-of-distribution samples from proxy datasets, preventing misleading predictions from being shared. The server-side selector computes ensemble predictions from received local predictions and filters those with high entropy based on ℓ1 distance from one-hot vectors. Only selected knowledge is used for distillation, improving generalization while reducing communication overhead. The method operates without requiring a well-trained teacher model, instead relying on local model predictions and proxy datasets.

## Key Results
- Achieves accuracy improvements of up to 19.42% on pneumonia detection task compared to baseline methods
- Reduces communication overhead by up to 900× compared to FedAvg
- Maintains strong performance in non-IID settings across multiple benchmark datasets
- AUROC scores for OOD detection exceed 0.8 in most cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Client-side selectors using density-ratio estimation can identify out-of-distribution (OOD) samples from proxy datasets, preventing misleading knowledge sharing.
- Mechanism: The client constructs a density-ratio estimator that compares the density of proxy samples against a uniform distribution. Samples with low density ratios are flagged as OOD and their predictions are not shared.
- Core assumption: The local model's distribution is well-represented by the density-ratio estimator, and OOD samples will have significantly lower density ratios than in-distribution samples.
- Evidence anchors:
  - [abstract]: "client-side selectors use density-ratio estimation to identify out-of-distribution samples from the proxy dataset"
  - [section]: "client-side selectors use density-ratio estimators to identify out-of-distribution (OOD) samples from the proxy dataset"
  - [corpus]: No direct evidence in corpus, but similar approaches exist in outlier detection literature.
- Break condition: If the proxy dataset contains samples that are OOD but still have high density ratios (e.g., due to similar feature distributions), the client-side selector will fail to filter them, leading to misleading knowledge sharing.

### Mechanism 2
- Claim: Server-side selector filters out ambiguous ensemble predictions with high entropy, ensuring only precise knowledge is shared back to clients.
- Mechanism: The server computes the average of received local predictions and calculates the ℓ1 distance between the ensemble prediction and its one-hot version. Predictions with distances exceeding a threshold are considered ambiguous and filtered out.
- Core assumption: High entropy in ensemble predictions indicates ambiguity and potential inconsistency among client predictions, which can degrade knowledge distillation.
- Evidence anchors:
  - [abstract]: "server-side selector filters out ensemble predictions with high entropy"
  - [section]: "server-side selector aims to eliminate ambiguous knowledge"
  - [corpus]: No direct evidence in corpus, but the concept of filtering high-entropy predictions is common in ensemble methods.
- Break condition: If the threshold is set too low, valid but slightly inconsistent predictions may be filtered out, reducing the diversity of knowledge shared. If set too high, ambiguous predictions may still be shared, degrading performance.

### Mechanism 3
- Claim: The combination of client-side and server-side selectors significantly improves generalization in federated distillation by reducing misleading and ambiguous knowledge.
- Mechanism: Client-side selectors prevent the sharing of incorrect predictions from OOD samples, while server-side selectors eliminate ambiguous ensemble predictions. This dual filtering ensures that only accurate and precise knowledge is used for distillation.
- Core assumption: The selective knowledge sharing mechanism effectively reduces the negative impact of misleading and ambiguous knowledge, leading to improved model performance.
- Evidence anchors:
  - [abstract]: "Empirical studies, backed by theoretical insights, demonstrate that our approach enhances the generalization capabilities of the FD framework"
  - [section]: "Selective-FD significantly outperforms baseline methods, achieving accuracy improvements of up to 19.42% on the pneumonia detection task"
  - [corpus]: Weak evidence in corpus; no direct comparisons of selective filtering vs. no filtering.
- Break condition: If the filtering thresholds are not properly tuned, the method may either filter out too much useful knowledge or fail to filter out enough misleading/ambiguous knowledge, negating the benefits.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Federated distillation is built upon knowledge distillation, which transfers knowledge from a teacher model to student models using a proxy dataset. Understanding this concept is crucial for grasping how Selective-FD works.
  - Quick check question: How does knowledge distillation differ from traditional supervised learning, and why is it useful in federated learning?

- Concept: Density-Ratio Estimation
  - Why needed here: Client-side selectors use density-ratio estimation to identify OOD samples. Understanding this technique is essential for implementing the selective knowledge sharing mechanism.
  - Quick check question: What is the intuition behind using density-ratio estimation for outlier detection, and how does it compare to other outlier detection methods?

- Concept: Ensemble Methods
  - Why needed here: The server aggregates local predictions to form ensemble predictions, which are then filtered by the server-side selector. Understanding ensemble methods is important for grasping how the server-side selector works.
  - Quick check question: How does averaging local predictions form an ensemble prediction, and why might this ensemble prediction be ambiguous?

## Architecture Onboarding

- Component map: Proxy samples → Client-side selectors → Server → Server-side selector → Selected ensemble predictions → Clients for knowledge distillation
- Critical path: Proxy samples are filtered by client-side selectors, aggregated at server, filtered by server-side selector, then used for knowledge distillation at clients
- Design tradeoffs: Balancing filtering thresholds to avoid removing too much useful knowledge while still filtering out misleading/ambiguous knowledge. Communication overhead vs. model performance.
- Failure signatures: Performance degradation if filtering thresholds are too strict or too lenient. Communication overhead increases if too many proxy samples are sent without filtering.
- First 3 experiments:
  1. Implement density-ratio estimation on a small proxy dataset and visualize the density ratios of in-distribution vs. OOD samples.
  2. Implement the server-side selector and test its ability to filter out high-entropy ensemble predictions on a synthetic dataset with known inconsistencies.
  3. Integrate both selectors into the full Selective-FD framework and evaluate its performance on a simple federated learning task with non-IID data.

## Open Questions the Paper Calls Out
None explicitly called out in the provided text.

## Limitations
- Density-ratio estimation method parameters (kernel choice, regularization) and filtering thresholds are not explicitly specified, limiting reproducibility
- COVIDx dataset evaluation lacks detailed baseline comparisons and ablation studies to isolate the impact of each selector component
- Communication efficiency comparison methodology may not be apples-to-apples given different communication patterns between Selective-FD and FedAvg

## Confidence

- High confidence: The core mechanism of using density-ratio estimation for OOD detection is theoretically sound and aligns with established outlier detection literature
- Medium confidence: The effectiveness of combining client-side and server-side selectors is supported by experimental results, though the ablation analysis could be stronger
- Medium confidence: The communication efficiency claims are plausible given the selective sharing approach, but the comparison methodology needs clarification

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary τclient and τserver values across a wide range to identify optimal thresholds and understand the robustness of the filtering mechanism to hyperparameter choices.

2. **Ablation Study**: Implement and evaluate each selector independently (client-side only, server-side only, both) on the same tasks to quantify the marginal contribution of each filtering component.

3. **OOD Detection Evaluation**: Test the density-ratio estimator on proxy samples with known contamination levels to verify the claimed AUROC > 0.8 performance and understand failure modes when proxy datasets contain significant OOD samples.