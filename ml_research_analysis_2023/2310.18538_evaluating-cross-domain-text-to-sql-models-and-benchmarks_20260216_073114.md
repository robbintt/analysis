---
ver: rpa2
title: Evaluating Cross-Domain Text-to-SQL Models and Benchmarks
arxiv_id: '2310.18538'
source_url: https://arxiv.org/abs/2310.18538
tags:
- queries
- query
- benchmarks
- these
- spider
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts an extensive study of prominent cross-domain
  text-to-SQL benchmarks and re-evaluates top-performing models within these benchmarks,
  by both manually evaluating the SQL queries and rewriting them in equivalent expressions.
  The evaluation reveals that attaining perfect performance on these benchmarks is
  unfeasible due to multiple interpretations derived from the provided samples.
---

# Evaluating Cross-Domain Text-to-SQL Models and Benchmarks

## Quick Facts
- arXiv ID: 2310.18538
- Source URL: https://arxiv.org/abs/2310.18538
- Reference count: 5
- Key outcome: Cross-domain text-to-SQL benchmarks significantly underestimate model performance due to ambiguous queries and execution issues; human evaluation reveals GPT-4 surpasses reference queries.

## Executive Summary
This paper conducts an extensive study of prominent cross-domain text-to-SQL benchmarks (Spider, Spider-DK, BIRD) and re-evaluates top-performing models by manually evaluating SQL queries and rewriting them in equivalent expressions. The evaluation reveals that attaining perfect performance on these benchmarks is unfeasible due to multiple interpretations derived from the provided samples. The true performance of the models is underestimated, and their relative performance changes after re-evaluation. Most notably, a recent GPT4-based model surpasses the gold standard reference queries in the Spider benchmark in human evaluation. This finding highlights the importance of interpreting benchmark evaluations cautiously, while also acknowledging the critical role of additional independent evaluations in driving advancements in the field.

## Method Summary
The authors manually evaluate SQL queries from top-performing models (DIN-SQL, T5+PICARD) on Spider development set and reference queries, identifying issues with underspecified natural language queries, tie handling in SQL results, and schema ambiguities. They rewrite problematic queries to resolve tie-related issues (e.g., replacing LIMIT 1 with aggregations) and conduct human evaluation to validate query equivalence. The study also migrates Spider databases to PostgreSQL for standardized SQL validation and categorizes errors by type (schema, condition, nested, GROUP BY, LIMIT).

## Key Results
- Cross-domain text-to-SQL benchmarks underestimate model performance due to multiple valid interpretations of ambiguous natural language queries
- Execution accuracy fails due to ties in SQL query results, causing valid queries to be marked incorrect
- A GPT-4-based model surpasses the gold standard reference queries in the Spider benchmark during human evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmarks overestimate model performance due to multiple correct interpretations of ambiguous natural language queries.
- Mechanism: Natural language questions often allow multiple valid SQL interpretations due to schema ambiguities and underspecification. The benchmark only provides one "correct" reference SQL, causing otherwise correct model outputs to be marked as incorrect.
- Core assumption: Natural language queries are inherently ambiguous and can be satisfied by multiple semantically equivalent SQL queries.
- Evidence anchors:
  - [abstract] "accurately matching a model-generated SQL query to a reference SQL query in a benchmark fails for various reasons, such as underspecified natural language queries"
  - [section] "Consider the example in Figure 1, which consists of a model-generated query...Both SQL queries return the id and name of makers that have more than 3 models...The natural language utterance is not specific about the type of name to be returned"
  - [corpus] Weak - related work focuses on robustness and evaluation metrics but not directly on interpretation ambiguity
- Break Condition: If natural language queries become fully specified with no ambiguity, or if benchmarks provide multiple reference queries for each question.

### Mechanism 2
- Claim: Execution accuracy fails due to ties in SQL query results, causing valid queries to be marked incorrect.
- Mechanism: SQL queries can produce ties (multiple rows with identical values for sorting or grouping), and the order of tied rows is non-deterministic. Different but equivalent queries may return tied rows in different orders, causing execution accuracy to mark them as incorrect.
- Core assumption: SQL execution is non-deterministic when handling ties without explicit ordering.
- Evidence anchors:
  - [abstract] "non-deterministic nature of SQL output under certain conditions"
  - [section] "Many text-to-SQL benchmarks encounter a different type of issue associated with ties...when there is a tie for the top position...the corresponding SQL query may return all ties or only one"
  - [section] "This means a model-generated query will be deemed incorrect if it does not return the same row, among tied rows, as the ground truth query"
  - [corpus] Weak - related work mentions evaluation reliability but not specifically tie-handling issues
- Break Condition: If benchmarks explicitly handle ties by either returning all tied rows or providing deterministic tie-breaking rules.

### Mechanism 3
- Claim: Models using prompting approaches (like GPT-4) outperform reference queries because they better handle benchmark limitations.
- Mechanism: Prompting approaches with few-shot demonstrations can implicitly learn to handle ambiguous cases and schema variations better than fine-tuned models that strictly learn from the training distribution.
- Core assumption: Large language models can generalize better to handle edge cases and ambiguities not well-represented in training data.
- Evidence anchors:
  - [abstract] "a recent GPT4-based model surpasses the gold standard reference queries in the Spider benchmark in our human evaluation"
  - [section] "Most notably, our evaluation reveals a surprising discovery: a recent GPT4-based model surpasses the gold standard reference queries in the Spider benchmark in our human evaluation"
  - [section] "Notably, the reference set shows the least number of errors, which is closely followed by DIN-SQL"
  - [corpus] Weak - related work focuses on evaluation methods but not comparative performance of prompting vs fine-tuning
- Break Condition: If prompting approaches are constrained by the same training data limitations, or if fine-tuned models receive more comprehensive training covering edge cases.

## Foundational Learning

- Concept: Schema linking and semantic understanding
  - Why needed here: Text-to-SQL systems must map natural language to database schema elements, which is complicated by synonyms, ambiguities, and underspecification.
  - Quick check question: What happens when multiple database columns could represent the same semantic concept mentioned in a question?

- Concept: SQL execution semantics and non-determinism
  - Why needed here: Understanding how SQL handles ties, ordering, and grouping is crucial for interpreting why valid queries might fail execution accuracy.
  - Quick check question: How does SQL determine which row to return when multiple rows have identical values for the ORDER BY clause?

- Concept: Evaluation metrics limitations
  - Why needed here: Exact set match and execution accuracy have inherent limitations that become apparent as model performance improves.
  - Quick check question: Why might two semantically equivalent SQL queries fail exact set match accuracy?

## Architecture Onboarding

- Component map: Data ingestion -> Query rewriting engine -> PostgreSQL migration layer -> Human evaluation interface -> Error analysis module

- Critical path:
  1. Load benchmark data and identify problematic query patterns
  2. Apply query rewriting to resolve tie-related issues
  3. Migrate databases to PostgreSQL for standard SQL validation
  4. Run execution accuracy tests on rewritten queries
  5. Conduct human evaluation on failed queries
  6. Analyze error patterns and generate insights

- Design tradeoffs:
  - Query rewriting vs manual correction: Automated rewriting handles common patterns but misses nuanced cases requiring human judgment
  - PostgreSQL vs SQLite: PostgreSQL enforces stricter SQL standards revealing more errors but requires query modifications
  - Execution vs human evaluation: Execution is scalable but misses semantic equivalence; human evaluation is accurate but labor-intensive

- Failure signatures:
  - High execution accuracy variance after query rewriting suggests tie-related issues
  - Consistent human evaluation agreement indicates benchmark limitations
  - PostgreSQL migration failures reveal syntax and semantic issues in reference queries

- First 3 experiments:
  1. Run query rewriting on all Spider development queries and measure execution accuracy change
  2. Select 100 failed queries from DIN-SQL and T5+PICARD, conduct human evaluation to identify correct queries
  3. Migrate Spider development set to PostgreSQL, run all queries, categorize failures by error type

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can benchmarks be improved to handle the ambiguity in schema matching and wrong assumptions on DB content?
- Basis in paper: [explicit] The paper identifies that ambiguities arise when there are multiple columns in the database that can represent the same semantic meaning, and wrong assumptions are made about database content and constraints.
- Why unresolved: The paper highlights the need for a solution but does not provide a concrete method to address these issues.
- What evidence would resolve it: A study that proposes and tests a method to incorporate multiple SQL queries as the ground truth, each representing a different interpretation, and evaluates its effectiveness in improving benchmark reliability.

### Open Question 2
- Question: How do the limitations identified in cross-domain text-to-SQL benchmarks affect domain-specific text-to-SQL benchmarks and models?
- Basis in paper: [inferred] The paper suggests that the failure cases identified in cross-domain benchmarks are likely to be present in domain-specific benchmarks as well, but further analysis is needed.
- Why unresolved: The paper does not provide specific evidence or analysis on domain-specific benchmarks.
- What evidence would resolve it: A comparative study that identifies and analyzes the specific failure cases within domain-specific benchmarks and models, and evaluates their impact on performance.

### Open Question 3
- Question: How can the evaluation process be further enhanced to achieve a more comprehensive and accurate assessment of Text-to-SQL models?
- Basis in paper: [explicit] The paper suggests that incorporating multiple SQL queries as the ground truth and representing different interpretations of queries offers a promising solution to enhance the evaluation process.
- Why unresolved: The paper does not provide a detailed method or evidence of the effectiveness of this approach.
- What evidence would resolve it: An experimental study that implements and evaluates the proposed solution, comparing its effectiveness with the current evaluation methods in terms of accuracy and reliability.

## Limitations
- Human evaluation covered only 240 queries across two models, which may not fully represent broader model performance
- Query rewriting approach may not capture all edge cases in SQL semantics
- PostgreSQL migration revealed syntax issues in reference queries, but the extent of impact on benchmark reliability remains unclear

## Confidence
**High Confidence**: The discovery that natural language questions allow multiple valid SQL interpretations is well-supported by manual evaluation data.

**Medium Confidence**: The extent to which tie-related issues affect benchmark accuracy is documented, but exact impact on model rankings requires further validation.

**Low Confidence**: Generalizability of findings to other text-to-SQL benchmarks beyond Spider, Spider-DK, and BIRD requires additional investigation.

## Next Checks
1. Expand human evaluation to include all development set queries from Spider and Spider-DK, focusing on systematically categorizing the types of ambiguities that lead to multiple valid interpretations.

2. Develop automated methods for detecting and handling SQL tie scenarios, then validate these methods across multiple SQL database systems to ensure consistent behavior.

3. Create a new evaluation framework that incorporates multiple reference queries per natural language question, measuring how this affects model performance rankings and reliability.