---
ver: rpa2
title: Data-efficient Active Learning for Structured Prediction with Partial Annotation
  and Self-Training
arxiv_id: '2305.12634'
source_url: https://arxiv.org/abs/2305.12634
tags:
- uni00000013
- uni00000011
- uni00000024
- uni0000001b
- uni00000051
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a data-efficient active learning framework
  for structured prediction that leverages partial annotation and self-training. The
  key idea is to use an adaptive error estimator to dynamically determine the ratio
  of partial annotation based on the current model's capability, combined with self-training
  to utilize model predictions on un-annotated sub-structures.
---

# Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training

## Quick Facts
- arXiv ID: 2305.12634
- Source URL: https://arxiv.org/abs/2305.12634
- Authors: [Authors not specified in source]
- Reference count: 40
- Key outcome: Adaptive partial annotation with self-training reduces annotation costs while maintaining performance across four structured prediction tasks

## Executive Summary
This paper introduces a data-efficient active learning framework that combines partial annotation with self-training for structured prediction tasks. The key innovation is an adaptive error estimator that dynamically determines the optimal ratio of partial annotation based on the current model's capability, coupled with self-training to utilize model predictions on un-annotated sub-structures. Experiments across named entity recognition, dependency parsing, event extraction, and relation extraction demonstrate that this approach achieves performance comparable to full annotation baselines while significantly reducing annotation costs when accounting for reading time.

## Method Summary
The method employs an adaptive partial annotation strategy where an error estimator predicts the model's error rate on candidate sub-structures to determine the selection ratio. This ensures annotation budget is allocated to the most uncertain sub-structures. Self-training is implemented through knowledge distillation, where the model is trained on both gold labels from partial annotation and soft pseudo-labels from its own predictions. The approach uses margin-based uncertainty for sentence selection, followed by substructure-level selection within chosen sentences. Marginalized likelihood training handles incomplete annotations, and the error estimator (a logistic regression model) is updated after each AL batch.

## Key Results
- Achieves similar performance to full annotation baselines while significantly reducing annotation costs
- Adaptive partial annotation ratio selection reduces annotation costs by targeting uncertain sub-structures
- Self-training with knowledge distillation effectively utilizes unlabeled data through soft pseudo-labels
- Combination of partial annotation and self-training provides complementary benefits

## Why This Works (Mechanism)

### Mechanism 1
- Adaptive partial annotation ratio selection reduces annotation costs by targeting uncertain sub-structures while trusting model predictions on confident ones.
- Uses error estimator to predict current model's error rate and sets selection ratio accordingly.
- Assumes model uncertainty correlates well with actual prediction errors.
- Break condition: If uncertainty estimates become unreliable, inappropriate ratios will waste resources or miss errors.

### Mechanism 2
- Self-training with knowledge distillation effectively utilizes unlabeled data through soft pseudo-labels.
- Trains model using gold labels and soft pseudo-labels generated by previous model via knowledge distillation.
- Assumes model predictions on un-annotated sub-structures are sufficiently accurate.
- Break condition: If model makes systematic errors, pseudo-labels will propagate errors and degrade performance.

### Mechanism 3
- Combining partial annotation with self-training provides complementary benefits.
- Partial annotation focuses budget on uncertain sub-structures while self-training leverages confident predictions.
- Assumes non-selected sub-structures are highly confident and correctly predicted.
- Break condition: If confidence estimates are unreliable, complementary benefits will diminish.

## Foundational Learning

- **Concept**: Structured prediction with conditional random fields (CRFs)
  - Why needed: CRF-based models used for sequence labeling requiring marginalized likelihood training with incomplete annotations
  - Quick check: How does marginalized likelihood differ from standard log-likelihood for partially annotated data?

- **Concept**: Uncertainty quantification in deep learning
  - Why needed: Method relies on uncertainty scores to select informative sub-structures and drive adaptive ratio selection
  - Quick check: What are differences between margin-based, least-confidence, and entropy-based uncertainty measures?

- **Concept**: Knowledge distillation and soft labels
  - Why needed: Self-training implemented using knowledge distillation with soft output distributions
  - Quick check: How does knowledge distillation differ from hard-label training, and when is it most beneficial?

## Architecture Onboarding

- **Component map**: BERT encoder -> task-specific structured prediction layers (CRF/parsers) -> uncertainty estimator -> error rate estimator -> self-training component with knowledge distillation
- **Critical path**: Data selection → Partial annotation with adaptive ratio → Model training with gold labels → Self-training with pseudo-labels → Repeat until convergence
- **Design tradeoffs**: Partial annotation reduces labeling costs but requires complex inference with marginalization; self-training adds training complexity but leverages unlabeled data; adaptive ratio selection adds computation but improves budget allocation
- **Failure signatures**: Inaccurate error estimator misallocates annotation budget; early self-training propagates errors; poor uncertainty calibration selects uninformative examples
- **First 3 experiments**:
  1. Implement partial annotation with fixed ratio (e.g., 30%) to establish baseline performance
  2. Add self-training with knowledge distillation to evaluate standalone benefit of pseudo-label utilization
  3. Implement adaptive ratio selection using logistic regression error estimator to test dynamic allocation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does adaptive selection ratio scheme compare to fixed ratio in annotation cost reduction across different structured prediction tasks?
- Basis: Paper discusses adaptive strategy contrasting with fixed-ratio schemes
- Why unresolved: Limited exploration across all tasks, especially complex ones like event/relation extraction
- Evidence needed: Comprehensive study comparing adaptive and fixed-ratio schemes across all tasks

### Open Question 2
- Question: What are specific impacts of domain transfer on AL effectiveness with partial annotation in different structured prediction tasks?
- Basis: Mentions domain-transfer experiments suggesting benefits of combining AL with transfer learning
- Why unresolved: Lacks detailed analysis of domain transfer impacts on each task individually
- Evidence needed: Detailed study analyzing performance in source and target domains across all tasks

### Open Question 3
- Question: How does performance of adaptive partial selection scheme change with different acquisition functions?
- Basis: Mentions using different acquisition functions but doesn't explore their interaction with adaptive partial selection
- Why unresolved: Tests different functions but doesn't analyze their specific effects on adaptive partial selection performance
- Evidence needed: Experiment comparing adaptive partial selection performance with various acquisition functions

## Limitations

- Limited empirical validation of adaptive error estimator accuracy across AL stages and tasks
- Computational overhead of error estimator and partial annotation machinery not evaluated
- Self-training risks of error propagation not thoroughly investigated, especially in early AL stages
- Simulated annotation settings may not reflect real human annotation behavior or constraints

## Confidence

**High confidence**: Overall experimental setup with clear baselines and metrics; establishes partial annotation with adaptive ratios can reduce costs compared to full annotation; multi-task evaluation lends credibility

**Medium confidence**: Specific mechanisms of adaptive error estimator and practical effectiveness; methodology described clearly but lacks detailed error estimator accuracy analysis

**Low confidence**: Generalization to real-world annotation scenarios; simplified cost model may not capture full complexity of actual annotation workflows

## Next Checks

1. **Error estimator accuracy validation**: Track error estimator predictions against actual error rates on held-out data across AL iterations; plot correlation between estimated and true error rates

2. **Ablation study on adaptive vs fixed ratios**: Compare adaptive ratio selection against multiple fixed ratios (20%, 40%, 60%) across all tasks to determine if complexity is justified

3. **Pseudo-label quality analysis**: Track accuracy of pseudo-labels generated in each AL iteration; create visualizations showing quality evolution and correlation with downstream performance improvements