---
ver: rpa2
title: Does mBERT understand Romansh? Evaluating word embeddings using word alignment
arxiv_id: '2306.08702'
source_url: https://arxiv.org/abs/2306.08702
tags:
- word
- romansh
- alignment
- language
- simalign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a trilingual corpus of German, Romansh, and
  Italian press releases from the Canton of Grisons. It evaluates word alignment models
  (SimAlign, awesome-align) using mBERT and XLM-R embeddings on German-Romansh sentence
  pairs in a zero-shot setting.
---

# Does mBERT understand Romansh? Evaluating word embeddings using word alignment

## Quick Facts
- arXiv ID: 2306.08702
- Source URL: https://arxiv.org/abs/2306.08702
- Reference count: 40
- Word alignment models achieve AER of 0.22 on German-Romansh pairs using mBERT embeddings, outperforming statistical baselines and improving to 0.09 with fine-tuning

## Executive Summary
This paper evaluates whether multilingual language models like mBERT can effectively process low-resource languages they haven't seen during pretraining, using Romansh as a case study. The authors create a trilingual corpus of German, Romansh, and Italian press releases from the Canton of Grisons and test word alignment models on German-Romansh sentence pairs. Both similarity-based models (SimAlign, awesome-align) and statistical models (fast_align, eflomal) are evaluated, with mBERT-based models achieving AER of 0.22 that improves to 0.09 after fine-tuning. The results suggest multilingual models contain transferable information applicable to unseen languages like Romansh.

## Method Summary
The authors collected a trilingual corpus (DERMIT) containing 106,091 German-Romansh sentence pairs from press releases. They manually annotated 600 sentence pairs to create a gold standard for evaluation. Word alignment models (SimAlign, awesome-align using mBERT/XLM-R embeddings, and statistical baselines fast_align/eflomal) were evaluated on this gold standard using alignment error rate (AER). The authors also fine-tuned mBERT using awesome-align's training objectives on the full parallel corpus and re-evaluated performance. The study employs a zero-shot setting where Romansh is an unseen language during mBERT pretraining.

## Key Results
- mBERT-based similarity models achieve AER of 0.22 on German-Romansh pairs, outperforming statistical baseline fast_align
- Fine-tuning mBERT with awesome-align reduces AER to 0.09 on the same pairs
- Performance on Romansh matches mBERT's performance on seen language pairs, suggesting effective cross-lingual transfer

## Why This Works (Mechanism)

### Mechanism 1
Similarity-based word alignment models (SimAlign and awesome-align) achieve AER comparable to statistical models when using mBERT embeddings on unseen language pairs. These models compute word alignments by extracting contextualized embeddings from mBERT, constructing a similarity matrix, and aligning source words to target words based on maximum similarity scores. The core assumption is that mBERT embeddings contain meaningful cross-lingual alignment information for unseen languages like Romansh.

### Mechanism 2
Fine-tuning mBERT with awesome-align on parallel data dramatically improves word alignment performance for unseen languages. Fine-tuning optimizes mBERT to better align source and target representations through masked language modeling on concatenated parallel sentences (TLM) and self-training objectives (SO) that bring aligned word representations closer together.

### Mechanism 3
Multilingual language models perform well on unseen languages when those languages share typological similarity and script with languages seen during pretraining. mBERT learns shared representations across languages with similar typological features and scripts during pretraining, enabling transfer to unseen but related languages.

## Foundational Learning

- **Word alignment and alignment error rate (AER)**: The paper's primary evaluation metric and the task being evaluated. Quick check: What is the formula for AER and what do the sets A, S, and P represent?

- **Zero-shot learning and cross-lingual transfer**: Romansh is an unseen language during mBERT pretraining, making this a zero-shot scenario. Quick check: How does zero-shot performance differ from few-shot performance in multilingual models?

- **Contextualized word embeddings and similarity-based alignment**: The core mechanism by which SimAlign and awesome-align operate. Quick check: What is the difference between static and contextualized word embeddings in the context of alignment tasks?

## Architecture Onboarding

- **Component map**: Data collection pipeline (web scraping + sentence alignment) -> Gold standard annotation tool (AlignMan) -> Word alignment models (fast_align, eflomal, SimAlign, awesome-align) -> Evaluation framework (precision, recall, AER computation) -> Fine-tuning module (awesome-align with TLM and SO objectives)

- **Critical path**: 1) Extract parallel sentences from corpus, 2) Compute word alignments using chosen model, 3) Evaluate against gold standard using AER, 4) (Optional) Fine-tune mBERT and repeat steps 2-3

- **Design tradeoffs**: Statistical models benefit from larger datasets but require substantial parallel data; similarity-based models are dataset-size independent but depend heavily on embedding quality; fine-tuning improves performance but requires computational resources and careful hyperparameter tuning

- **Failure signatures**: High AER persisting across models suggests issues with corpus quality or annotation consistency; performance gap between seen and unseen languages indicates limited cross-lingual transfer; fine-tuning overfitting indicated by large performance drop on validation data

- **First 3 experiments**: 1) Compare fast_align vs eflomal on varying dataset sizes to establish baseline trends, 2) Evaluate SimAlign with mBERT vs XLM-R on the gold standard to test embedding effectiveness, 3) Apply awesome-align fine-tuning and measure AER improvement to validate transfer capability

## Open Questions the Paper Calls Out

- **Open Question 1**: Does fine-tuning with parallel data work better for low-resource languages when the parallel data is with a closely related high-resource language? The paper only tested this with Romansh paired with German and doesn't establish whether this generalizes to other low-resource language pairs.

- **Open Question 2**: How does the quality of the parallel corpus (in terms of sentence alignment accuracy) affect the performance of fine-tuned word alignment models? The paper mentions using hunalign with 1.8% error rate but doesn't explore how this impacts word alignment performance after fine-tuning.

- **Open Question 3**: What is the impact of using different tokenization strategies (word vs. sub-word) on the performance of word alignment models for low-resource languages? The paper briefly touches on this but doesn't provide a comprehensive analysis for low-resource languages like Romansh.

## Limitations

- The AER of 0.22 for Romansh lacks comparison to other truly unseen languages, making it difficult to assess whether performance is due to genuine cross-lingual capabilities or Romansh's typological similarity to pretraining languages.

- The corpus composition is unclear regarding relative proportions and whether German-Romansh pairs are naturally occurring or created through translation, affecting zero-shot evaluation validity.

- Fine-tuning effectiveness claims are not supported by learning curves or ablation studies showing which components of the fine-tuning objective contribute most to performance gains.

## Confidence

- **High confidence**: Methodological framework for evaluating word alignment models and corpus collection pipeline implementation
- **Medium confidence**: Reported AER values for mBERT-based models, but interpretation of cross-lingual transfer capability needs additional control experiments
- **Low confidence**: Claims about mBERT containing "meaningful and applicable" information for Romansh are overstated without comparison to more distant language pairs

## Next Checks

1. Evaluate alignment models on German-Romanian and German-Slovene pairs to establish whether Romansh performance is due to genuine cross-lingual transfer or proximity to pretraining distribution.

2. Analyze DERMIT corpus for sentence length distributions, vocabulary overlap, and translationese indicators to determine if German-Romansh pairs are naturally occurring or created through translation.

3. Conduct fine-tuning ablation study of awesome-align objectives (TLM vs SO) with learning curves to verify AER improvement isn't due to overfitting and identify most effective components.