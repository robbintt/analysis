---
ver: rpa2
title: 'Exploring the Jungle of Bias: Political Bias Attribution in Language Models
  via Dependency Analysis'
arxiv_id: '2311.08605'
source_url: https://arxiv.org/abs/2311.08605
tags:
- speaker
- bias
- float
- does
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores bias in large language models (LLMs) by analyzing
  how they evaluate political arguments in US presidential debates. The authors extract
  variables representing normative values and use Activity Dependency Networks to
  understand the decision-making process of LLMs.
---

# Exploring the Jungle of Bias: Political Bias Attribution in Language Models via Dependency Analysis

## Quick Facts
- arXiv ID: 2311.08605
- Source URL: https://arxiv.org/abs/2311.08605
- Authors: 
- Reference count: 26
- One-line primary result: Bias in LLMs may stem from learned normative values rather than simple political alignment

## Executive Summary
This paper investigates political bias in large language models by analyzing how they evaluate arguments from US presidential debates. The authors propose that biases arise not just from party affiliation but from complex interactions between normative values like clarity, decorum, and relevance. Using Activity Dependency Networks (ADNs), they demonstrate that simple correlation-based bias estimates miss these nuanced relationships and may lead to ineffective mitigation strategies.

The study reveals that LLM bias is more complex than previously thought, with normative values acting as mediating factors between speaker characteristics and evaluation scores. This finding suggests that current approaches to bias detection and mitigation may be insufficient, requiring more sophisticated attribution methods that consider the full network of value interactions rather than isolated correlations.

## Method Summary
The authors analyze US presidential debate transcripts from 1988 to 2020, processing them into 2,500 BPE token slices with 10% overlap. They use the ChatGPT API to collect judgments on 103 speaker variables, 5 slice variables, and 21 contextual variables across 150 randomly sampled debate slices. These variables are organized into three categories: measured variables (speaker qualities like "decorum"), speaker variables (speaker attributes like "speaker party"), and contextual variables (debate metadata like "slice topic"). The collected judgments are then analyzed using Activity Dependency Networks to understand how changes in one variable influence perceptions of others, allowing for attribution of observed biases to underlying normative values.

## Key Results
- Correlation between speaker party and score varies dramatically (from -0.43 to 0.11) depending on how "score" is defined, demonstrating the inadequacy of simple bias metrics
- ADN analysis reveals complex interactions between normative values that mediate apparent political biases
- Different definitions of "score" produce substantially different bias estimates, with some showing bias against Democrats while others show bias against Republicans

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can learn and express normative values that are not directly observable but influence their judgments.
- Mechanism: The model learns latent value associations from training data (e.g., political debates) that shape how it scores argument quality. These associations are captured via Activity Dependency Networks (ADNs) that map variable interactions beyond simple correlations.
- Core assumption: Normative values are implicitly encoded in model weights and can be extracted via targeted prompting and measured variables.
- Evidence anchors:
  - [abstract]: "normative values influence these perceptions" and "normative value associations from a corpus of US presidential debates."
  - [section 5.2]: "LLMs are capable of learning normative values from data" and "certain biases arise from such normative values."
  - [corpus]: Weak - the corpus neighbors focus on political bias but not explicitly on normative values as latent drivers.
- Break condition: If extracted variables show no interaction beyond direct correlations, the normative value hypothesis fails.

### Mechanism 2
- Claim: Bias estimates based solely on correlation between speaker party and score are unreliable.
- Mechanism: Direct correlation overestimates bias because it ignores confounding normative variables (e.g., "clarity," "decorum") that covary with party affiliation. ADNs reveal these hidden dependencies.
- Core assumption: Political bias is mediated by multiple interacting variables rather than a single party-to-score link.
- Evidence anchors:
  - [section 6]: "naive bias estimates are unable to fully capture the complexity of LLM bias" and "bias is likely to originate from a cascade of normative values."
  - [Figure 5]: Shows high variability in correlation depending on how "score" is defined.
  - [corpus]: Weak - corpus papers discuss political bias but do not test mediation via normative variables.
- Break condition: If ADN edges show no significant mediation effects, correlation-based bias estimates may be sufficient.

### Mechanism 3
- Claim: Fine-tuning for bias mitigation without understanding ADN structure can cause unintended downstream effects.
- Mechanism: Blindly correcting for observed biases alters multiple value associations simultaneously, potentially degrading performance on unrelated tasks. Attribution-driven fine-tuning targets specific nodes/edges in the ADN.
- Core assumption: Model weights encode interdependent value associations that are not independently adjustable.
- Evidence anchors:
  - [section 6]: "downstream consequences of fine-tuning large models are unpredictable" and "debiasing efforts should be guided by careful attribution."
  - [Figure 1]: Illustrates how naive fine-tuning can change unrelated value associations.
  - [corpus]: Weak - corpus neighbors do not address fine-tuning side effects.
- Break condition: If fine-tuning experiments show no unexpected degradation in other tasks, the attribution-driven approach is unnecessary.

## Foundational Learning

- Concept: Activity Dependency Networks (ADNs)
  - Why needed here: ADNs quantify how changes in one variable (e.g., clarity) influence perceptions of others (e.g., score) beyond simple correlation.
  - Quick check question: What is the difference between a correlation coefficient and a partial correlation in ADN construction?

- Concept: Normative values vs. definition bias
  - Why needed here: Normative values are learned standards embedded in model weights; definition bias arises from prompt interpretation. Distinguishing them guides mitigation strategy.
  - Quick check question: If a model rates Democratic speakers higher on "empathy," is that a normative or definition bias?

- Concept: Variable ensembles and slicing
  - Why needed here: Ensembles reduce measurement noise; slicing ensures manageable context for LLM prompting without losing topical coherence.
  - Quick check question: Why does the paper use overlapping slices of 2,500 BPE tokens?

## Architecture Onboarding

- Component map: Data preprocessing -> Variable extraction -> LLM prompting -> ADN construction -> Bias attribution -> Mitigation guidance
- Critical path:
  1. Clean and slice debate corpus
  2. Design variable ensembles (measured, speaker, slice, contextual)
  3. Prompt LLM to score each variable per speaker
  4. Compute partial correlations and ADN edges
  5. Interpret bias origins and propose mitigation

- Design tradeoffs:
  - Prompt granularity vs. cost: Single-variable prompts are costlier but reduce inter-variable bias
  - Ensemble size vs. noise: Larger ensembles smooth variance but may blur distinct meanings
  - ADN sparsity vs. completeness: Showing only strongest edges aids readability but may miss subtle influences

- Failure signatures:
  - Low variance in LLM outputs -> prompts too vague or model saturation
  - ADN edges all weak -> variables poorly defined or dataset too small
  - High correlation between score and party with no mediating edges -> confounding variables missing

- First 3 experiments:
  1. Vary prompt temperature and compare ADN stability
  2. Add synthetic confounders to test ADN sensitivity
  3. Compare ADN-derived bias attribution with direct correlation bias estimates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more precise methods to attribute LLM biases to their underlying normative values, beyond simple correlation analysis?
- Basis in paper: [explicit] The paper argues that current methods for measuring LLM bias are insufficient as they ignore the complex interplay of normative values that may confound observed biases. It proposes using Activity Dependency Networks to analyze interactions between variables and better understand the origins of bias.
- Why unresolved: Current approaches rely on naive correlation analysis or prompting techniques that fail to capture the nuanced relationships between normative values and observed biases. The paper highlights the need for more sophisticated methods to disentangle these complex interactions.
- What evidence would resolve it: Development and validation of novel attribution methods that can reliably identify the specific normative values driving observed biases in LLM outputs, demonstrated through rigorous case studies across diverse domains.

### Open Question 2
- Question: How do fine-tuning and existing bias mitigation strategies affect the activity dependency networks of LLMs, and can we predict their downstream consequences?
- Basis in paper: [explicit] The paper cautions that the downstream consequences of fine-tuning large models are unpredictable, posing challenges for correction efforts. It suggests that debiasing efforts should be guided by careful attribution of bias origins to minimize undesirable effects.
- Why unresolved: While the paper introduces Activity Dependency Networks as a tool for understanding LLM decision processes, it does not explore how these networks change in response to fine-tuning or debiasing interventions. The complexity of foundation models makes it difficult to evaluate every potential downstream task.
- What evidence would resolve it: Empirical studies tracking changes in ADNs before and after various fine-tuning and debiasing techniques, coupled with comprehensive evaluations of downstream task performance to identify unintended consequences.

### Open Question 3
- Question: Can we create automated methods for generating comprehensive sets of variables that evenly populate the feature space of interest when analyzing LLM biases?
- Basis in paper: [inferred] The paper acknowledges that its choice of variables for analyzing political biases was somewhat arbitrary and iterative. It suggests that developing atomized ways of identifying gaps in variable coverage could improve the robustness of bias analysis.
- Why unresolved: The paper relies on manually curated sets of variables to analyze LLM biases, which may miss important factors or introduce sampling biases. Creating a more systematic approach to variable generation could lead to more comprehensive bias analysis.
- What evidence would resolve it: Development of algorithms that can automatically generate diverse and representative sets of variables from embeddings or other model representations, validated by demonstrating improved coverage and predictive power in bias attribution tasks.

## Limitations
- Normative Value Extraction Reliability: The paper's core claim depends heavily on the quality and comprehensiveness of the extracted variables, with uncertainty about whether 129 variables capture all relevant value dimensions
- ADN Interpretation Validity: While ADNs provide a sophisticated framework, the interpretation of edge weights and their causal implications remains challenging, particularly given the modest sample size
- Cross-LLM Generalizability: All analyses rely on ChatGPT, with no exploration of whether findings generalize to other LLMs with different training corpora

## Confidence
- Normative Values Drive LLM Bias: Medium
- Correlation-Based Bias Estimates Are Insufficient: High
- ADN-Guided Fine-Tuning Prevents Unintended Effects: Low

## Next Checks
1. **Prompt Robustness Test:** Systematically vary prompt templates, temperatures, and phrasings to assess stability of extracted normative values and resulting ADN structures
2. **Synthetic Confounder Experiment:** Inject known confounding variables into the analysis pipeline to validate ADN's ability to correctly identify and attribute bias sources
3. **Cross-Model Replication:** Repeat the entire analysis pipeline with at least two additional LLMs (e.g., Claude, LLaMA) to assess generalizability of normative value patterns and bias attribution findings