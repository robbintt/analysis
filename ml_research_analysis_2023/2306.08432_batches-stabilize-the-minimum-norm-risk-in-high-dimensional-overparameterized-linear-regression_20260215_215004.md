---
ver: rpa2
title: Batches Stabilize the Minimum Norm Risk in High Dimensional Overparameterized
  Linear Regression
arxiv_id: '2306.08432'
source_url: https://arxiv.org/abs/2306.08432
tags:
- batch
- then
- risk
- lemma
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the performance of batch minimum-norm (batch-min-norm)
  estimation in overparameterized linear regression. The method partitions data into
  small batches, computes per-batch min-norm estimators, pools them to form a modified
  feature matrix, and applies min-norm estimation again.
---

# Batches Stabilize the Minimum Norm Risk in High Dimensional Overparameterized Linear Regression

## Quick Facts
- arXiv ID: 2306.08432
- Source URL: https://arxiv.org/abs/2306.08432
- Authors: 
- Reference count: 40
- One-line primary result: Batch minimum-norm estimation stabilizes risk behavior in overparameterized linear regression, eliminating double-descent by introducing implicit regularization through feature overlap.

## Executive Summary
This paper analyzes batch minimum-norm (batch-min-norm) estimation for overparameterized linear regression, showing it provides stable risk behavior that is monotonically increasing in the overparameterization ratio. The method partitions data into small batches, computes per-batch min-norm estimators, pools them to form a modified feature matrix, and applies min-norm estimation again. Unlike standard min-norm, batch-min-norm exhibits stable risk behavior without the blowup at the interpolation point or double-descent phenomenon. The optimal batch size is inversely proportional to both the overparameterization ratio and the signal-to-noise ratio.

## Method Summary
The batch-min-norm estimator partitions n samples into n/b batches of size b, computes min-norm estimators for each batch, pools these estimators to form a modified feature matrix, and applies min-norm estimation to the pooled problem. The analysis uses Wasserstein distance to approximate projections onto random subspaces by Gaussian vectors, enabling tractable computation of bias and variance. The key insight is that feature overlap between batches creates implicit regularization that stabilizes the risk behavior, preventing the noise amplification that occurs in standard min-norm estimation.

## Key Results
- Batch-min-norm risk is inversely proportional to both the overparameterization ratio and noise level for optimal batch size
- Eliminates double-descent phenomenon by providing stable risk that is monotonically increasing in overparameterization ratio
- Optimal batch size is inversely proportional to both γ and SNR, with a threshold below which increasing batch size is always beneficial

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Batch-min-norm stabilizes risk by introducing implicit regularization through feature overlap between batches.
- Mechanism: When data is partitioned into small batches, each batch's min-norm estimator captures a portion of β. Pooling creates correlated rows in the modified feature matrix due to shared features across batches, providing regularization that prevents noise amplification.
- Core assumption: Feature vectors in different batches are slightly linearly dependent, creating beneficial feature overlap.
- Evidence anchors:
  - [abstract]: "Interestingly, we observe that this implicit regularization offered by the batch partition is partially explained by feature overlap between the batches."
  - [section 4]: "This suggests that the gain of batch-min-norm can be partially attributed to the linear dependence between the feature vectors in different batches."

### Mechanism 2
- Claim: Optimal batch size scales as 1/(γ·SNR), creating stable risk monotonically increasing in overparameterization ratio.
- Mechanism: As batch size increases, effective overparameterization ratio decreases, reducing variance but increasing bias. Optimal balance occurs when batch size scales as 1/(γ·SNR).
- Core assumption: Relationship between batch size, overparameterization ratio, and SNR follows derived bound in Theorem 1.
- Evidence anchors:
  - [section 5]: "The optimal batch size is inversely proportional to both γ and SNR; more specifically, there is a low-SNR threshold point below which increasing the batch size... is always beneficial."
  - [figure 1 caption]: "When ξ < 0.6478, the optimal batch size b → ∞, for any γ > 1."

### Mechanism 3
- Claim: Wasserstein distance approximation enables tractable computation by approximating projections onto random subspaces with Gaussian vectors.
- Mechanism: Projections onto random subspaces (each batch) are asymptotically close in Wasserstein distance to projections onto i.i.d. Gaussian vectors, enabling conversion of recursive bias calculation to differential equation and simplifying variance analysis.
- Core assumption: Projections onto random subspaces from Haar measure are close in Wasserstein distance to Gaussian projections.
- Evidence anchors:
  - [abstract]: "Our bound is derived via a novel combination of techniques, in particular normal approximation in the Wasserstein metric of noisy projections over random subspaces."
  - [section 6.1]: "We show that the statistics of these projections are asymptotically close in the Wasserstein metric to i.i.d. Gaussian vectors."

## Foundational Learning

- Concept: Overparameterized linear regression and minimum-norm estimation
  - Why needed here: The paper operates in regime where p > n, requiring regularization to select among infinitely many solutions.
  - Quick check question: In overparameterized linear regression, what is the standard regularization method to select a unique solution?

- Concept: Wasserstein distance and its application to random projections
  - Why needed here: Analysis relies on approximating projections onto random subspaces with Gaussian vectors using Wasserstein distance.
  - Quick check question: How does Wasserstein distance help in approximating the statistics of projections onto random subspaces?

- Concept: Double-descent phenomenon and its relationship to batch partitioning
  - Why needed here: Understanding why batch-min-norm eliminates double-descent requires knowledge of how standard min-norm exhibits this phenomenon.
  - Quick check question: What causes the double-descent phenomenon in standard minimum-norm estimation, and how does batch partitioning prevent it?

## Architecture Onboarding

- Component map: Data partitioning -> Per-batch min-norm estimation -> Pooling step -> Final min-norm estimation -> Risk analysis

- Critical path:
  1. Partition data into batches
  2. Compute per-batch min-norm estimators
  3. Pool estimators to form modified feature matrix
  4. Apply final min-norm estimation
  5. Analyze risk using derived bounds

- Design tradeoffs:
  - Small batch size: More regularization, higher computational cost, potentially unstable if too small
  - Large batch size: Less regularization, lower computational cost, approaches standard min-norm behavior
  - Batch size optimization: Requires knowledge of SNR or estimation from data

- Failure signatures:
  - Risk increases dramatically near interpolation point (batch size too small)
  - Risk plateaus at high overparameterization without improvement (batch size too large)
  - Computational cost becomes prohibitive (batch size too small)

- First 3 experiments:
  1. Implement batch-min-norm with varying batch sizes (b=1, b=2, b=10) and compare risk to standard min-norm
  2. Test Wasserstein distance approximation by comparing empirical projections to Gaussian approximations
  3. Validate optimal batch size relationship by varying γ and SNR and checking if b_opt ∝ 1/(γ·SNR)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the implicit regularization effect of batch partitioning and the linear dependence/overlapping subspaces between batches?
- Basis in paper: [explicit] The paper observes that the batch algorithm coincides with regular min-norm when feature matrix has orthogonal rows, and suggests that the gain comes from small overlap between subspaces spanned by batches. It states: "the gain of batch-min-norm can be partially attributed to the fact that feature vectors are slightly linearly dependent between batches, i.e., there is small overlap between the subspaces spanned by the batches."
- Why unresolved: The paper provides an intuitive explanation but does not provide a rigorous mathematical characterization of how this overlap exactly translates to the regularization effect and performance gains.
- What evidence would resolve it: A formal analysis quantifying the relationship between subspace overlap, the amount of implicit regularization, and the resulting risk reduction would resolve this question.

### Open Question 2
- Question: How does the optimal batch size behave in finite-sample regimes where b is not negligible compared to n?
- Basis in paper: [inferred] The paper derives the optimal batch size in the asymptotic limit n, p → ∞ with b ≪ n, but acknowledges this assumption may not hold in practice. The paper states: "In practice, one uses a finite number of data samples, in which case we need b ≪ n in order for UB(b, γ, ξ) to be reliable. Hence, there will be a finite (possibly very large) batch size b that will minimize the risk for any pair (γ, ξ)."
- Why unresolved: The theoretical analysis assumes b is sublinear in n, but real-world applications may involve batch sizes that are not negligible compared to sample size.
- What evidence would resolve it: Numerical experiments and theoretical analysis extending the results to finite-sample regimes where b is comparable to n would provide answers.

### Open Question 3
- Question: Can the batch-min-norm algorithm be generalized to more complex models beyond linear regression, such as deep neural networks?
- Basis in paper: [explicit] The paper mentions in the ramifications section: "It is interesting to explore whether this approach can be rigorously generalized to more complex models." The paper also notes that linear regression can serve as a proxy for more complex settings via linearization.
- Why unresolved: While the paper shows benefits in the linear regression setting, extending these results to non-linear models like deep networks would require significant theoretical work.
- What evidence would resolve it: Developing and analyzing a batch-min-norm-like algorithm for deep networks, along with theoretical guarantees on its performance and regularization effects, would address this question.

## Limitations
- Analysis relies on asymptotic approximations (n → ∞) which may not hold in finite-sample regimes
- Theoretical bound requires optimizing over batch size, but practical implementation needs SNR estimation from data
- Assumes isotropic Gaussian features; performance on correlated or non-Gaussian features remains unclear

## Confidence
**High Confidence**: The stabilization of risk behavior and elimination of double-descent phenomenon through batch partitioning is well-supported by both theoretical analysis and empirical observations.

**Medium Confidence**: The Wasserstein distance approximation for projecting onto random subspaces is theoretically justified but requires validation in finite-sample settings.

**Low Confidence**: The practical implications of the feature overlap mechanism in real-world applications with non-isotropic or correlated features are not fully explored.

## Next Checks
1. Test the batch-min-norm estimator with small to moderate sample sizes (n = 50-500) to quantify deviation from asymptotic predictions and assess Wasserstein approximation error.

2. Evaluate the estimator's performance when feature vectors follow non-Gaussian distributions (e.g., sub-Gaussian, heavy-tailed) to determine robustness of theoretical bounds.

3. Investigate behavior of batch-min-norm on datasets with correlated features, comparing performance to standard min-norm estimation and examining whether feature overlap mechanism remains beneficial.