---
ver: rpa2
title: 'Compilation as a Defense: Enhancing DL Model Attack Robustness via Tensor
  Optimization'
arxiv_id: '2309.16577'
source_url: https://arxiv.org/abs/2309.16577
tags:
- attacks
- optimization
- learning
- attack
- trials
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of side-channel attacks on deep
  learning models, specifically model extraction attacks that use kernel metrics during
  inference to predict model architecture. The proposed defense approach uses tensor
  optimization techniques from the TVM compiler to obfuscate the model architecture
  and increase attack robustness.
---

# Compilation as a Defense: Enhancing DL Model Attack Robustness via Tensor Optimization

## Quick Facts
- arXiv ID: 2309.16577
- Source URL: https://arxiv.org/abs/2309.16577
- Reference count: 19
- Primary result: Tensor optimization can decrease model extraction attack effectiveness by up to 43% through architectural obfuscation

## Executive Summary
This paper proposes using tensor optimization from the TVM compiler as a defense mechanism against side-channel attacks on deep learning models, specifically model extraction attacks that use kernel metrics during inference to predict model architecture. The core idea is to apply increasing levels of machine-generated tensor optimizations to obfuscate the model architecture, making it more difficult for attackers to recognize memory access patterns and extract the model architecture. The approach leverages AutoTVM's optimization capabilities to generate diverse tensor schedules that alter memory access patterns, thereby reducing the effectiveness of attacks like DeepSniffer that rely on recognizing predictable patterns in common DL operator implementations.

## Method Summary
The defense approach uses TVM's AutoTVM to apply increasing levels of tensor optimizations to deep learning models, with optimization and execution performed using CUDA 11.7 and Nvidia A100 accelerator. The experiments use four models (ResNet18, YoloV4, RoBERTa, DenseNet121) ranging from 8m-124m parameters, converted to ONNX format and optimized with 1 to 500 AutoTVM trials. Kernel metrics are collected using NSYS Profiler during inference, and attack robustness is measured using fidelity - a metric comparing predicted and actual model architecture (0 = no similarity, 1.0 = identical). The DeepSniffer attack framework analyzes the collected metrics to evaluate the defense effectiveness.

## Key Results
- Tensor optimization decreased DeepSniffer attack fidelity by up to 43% (YoloV4 from 0.2220 to 0.1259 after 500 trials)
- Attack fidelity decreased progressively with the number of optimization trials applied
- Heterogeneous operator sets in language models like RoBERTa showed inherent resistance to extraction due to unfamiliar memory patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tensor optimization obfuscates memory access patterns that model extraction attacks rely on to infer architecture
- Mechanism: AutoTVM generates optimized tensor schedules that alter the sequence and timing of memory operations during inference, making it difficult for attacks like DeepSniffer to correlate L2 cache patterns with specific architectural components
- Core assumption: Side-channel attacks depend on recognizing predictable memory access patterns in common DL operator implementations
- Evidence anchors: [abstract] "apply increasing levels of machine-generated tensor optimizations (TO) to obfuscate the model architecture", [section] "SCA attacks are designed to recognize model memory access patterns within these libraries, thus decreasing model architecture confidentiality"

### Mechanism 2
- Claim: Increasing optimization trials reduces attack fidelity by progressively disrupting architectural predictability
- Mechanism: Each AutoTVM trial generates a different tensor schedule, and with sufficient trials (500 shown), the cumulative effect creates enough architectural noise to prevent accurate model reconstruction
- Core assumption: More optimization trials create more architectural divergence from the baseline, increasing attack difficulty
- Evidence anchors: [section] "Figure 1 shows the impact of n trials of TO on DS fidelity for each model. We observe that DS attack fidelity decreased with the number of trials conducted", [section] "After 500 trials, XGB decreased DenseNet121 fidelity from 0.1678 → 0.1013 (-39.6%), and YoloV4 0.2220 → 0.1259 (-43.3%)"

### Mechanism 3
- Claim: Heterogeneous operator sets in language models resist extraction due to unfamiliar memory patterns
- Mechanism: Models like RoBERTa with diverse operators create complex, less predictable memory access patterns that DeepSniffer's architecture recognition fails to map effectively
- Core assumption: DeepSniffer is optimized for specific operator types and struggles with heterogeneous architectures
- Evidence anchors: [section] "The failure of DS to effectively profile RoBERTa is attributed to it being a language model with heterogeneous operators unfamiliar to DS", [section] "These models range between 8m - 124m parameters, representing a diverse suite of applications (object detection, generative text, image classification)"

## Foundational Learning

- Concept: Side-channel attacks on ML models
  - Why needed here: Understanding how attackers extract model architecture through kernel metrics is essential to appreciate the defense mechanism
  - Quick check question: What information can an attacker gather by monitoring L2 cache reads/writes during model inference?

- Concept: Tensor optimization and compilation
  - Why needed here: The defense relies on understanding how AutoTVM generates and applies tensor schedules to optimize DL models
  - Quick check question: How does AutoTVM use simulated annealing to search for optimal tensor schedules?

- Concept: Attack fidelity measurement
  - Why needed here: Evaluating defense effectiveness requires understanding how fidelity is calculated and what values indicate successful model extraction
  - Quick check question: What does a fidelity score of 0.15 indicate about the relationship between predicted and actual model architecture?

## Architecture Onboarding

- Component map: ONNX model loader -> TVM compiler with AutoTVM tuner -> CUDA runtime environment -> Nvidia A100 accelerator -> NSYS Profiler for kernel metrics collection -> DeepSniffer attack framework -> fidelity calculation

- Critical path: ONNX → TVM optimization → inference → kernel metrics collection → DeepSniffer analysis → fidelity calculation

- Design tradeoffs: Higher optimization trials improve defense but increase computational cost (83 GPU-hours total across all models/trials)

- Failure signatures: Minimal fidelity reduction despite optimization trials, suggesting either ineffective optimization or attack adaptation

- First 3 experiments:
  1. Run baseline experiment with 0 trials to establish initial DeepSniffer fidelity for each model
  2. Apply 1-10 AutoTVM trials incrementally and measure fidelity changes
  3. Compare heterogeneous vs homogeneous model architectures' resistance to extraction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of tensor optimization as a defense vary across different types of side-channel attacks beyond L2 cache monitoring?
- Basis in paper: [explicit] The paper focuses on DeepSniffer (DS), an SCA attack targeting kernel L2 cache reads/writes during inference. It would be valuable to understand how effective tensor optimization is against other types of side-channel attacks.
- Why unresolved: The paper only investigates one specific type of side-channel attack, so the generalizability of the findings to other attack types remains unknown.
- What evidence would resolve it: Conducting experiments using tensor optimization against a variety of side-channel attacks (e.g., power consumption, timing, memory access patterns) and comparing the effectiveness of the defense across these attacks.

### Open Question 2
- Question: What is the optimal balance between the number of optimization trials and the resource cost incurred, to maximize both inference time reduction and attack robustness?
- Basis in paper: [explicit] The paper mentions that maximizing optimization decreases inference time and improves robustness but incurs large resource costs. However, it does not provide a clear optimal point or trade-off analysis.
- Why unresolved: The paper only presents results for a fixed number of trials (up to 500) without exploring the relationship between resource cost and defense effectiveness.
- What evidence would resolve it: Analyzing the marginal benefit of additional optimization trials against the marginal cost in terms of computational resources, to determine the point of diminishing returns.

### Open Question 3
- Question: How effective is the proposed approach of optimizing only the operators found to be conductive to SCA success, compared to optimizing the entire model?
- Basis in paper: [explicit] The paper suggests this as a future direction, positing that focusing optimization on conductive operators could decrease resource requirements and attack effectiveness.
- Why unresolved: This focused optimization technique is only proposed as a future direction and has not been empirically tested.
- What evidence would resolve it: Implementing and evaluating a targeted optimization strategy that prioritizes the optimization of operators most susceptible to side-channel attacks, and comparing its effectiveness and resource usage to the current approach of optimizing the entire model.

## Limitations

- Evaluation Scope: The study evaluates only four models across limited architectures, making generalization claims about "any DL model" speculative
- Hardware Dependency: Results are demonstrated exclusively on Nvidia A100 with CUDA 11.7, potentially limiting cross-platform effectiveness
- Optimization Overhead: The practical deployment cost of this defense remains unclear as runtime overhead from optimization trials is not quantified

## Confidence

- **High Confidence**: Tensor optimization changes memory access patterns during inference, making architectural inference more difficult
- **Medium Confidence**: Increasing optimization trials provides diminishing returns in attack fidelity reduction
- **Low Confidence**: Heterogeneous operator sets inherently resist extraction due to unfamiliar memory patterns

## Next Checks

1. **Cross-Attack Evaluation**: Test the same defense against multiple model extraction attacks (not just DeepSniffer) to verify the robustness of tensor optimization across attack methodologies

2. **Hardware Portability**: Replicate the experiment on different GPU architectures (e.g., AMD, Intel) and compiler frameworks to assess whether the defense effectiveness transfers across hardware/compiler ecosystems

3. **Runtime Overhead Analysis**: Measure the actual inference latency and computational overhead introduced by the optimization trials to quantify the practical deployment cost of this defense mechanism