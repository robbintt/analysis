---
ver: rpa2
title: Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language
  Models
arxiv_id: '2303.16133'
source_url: https://arxiv.org/abs/2303.16133
tags:
- tasks
- contrast
- consistency
- sets
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COCO CON, a benchmark dataset for evaluating
  cross-task consistency in multimodal models. The dataset uses contrast sets, where
  test instances are perturbed in semantically meaningful ways to create pairs of
  original and modified inputs across multiple tasks.
---

# Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models

## Quick Facts
- arXiv ID: 2303.16133
- Source URL: https://arxiv.org/abs/2303.16133
- Reference count: 29
- One-line primary result: Unified vision-language models exhibit significant cross-task inconsistency, which can be improved through a rank correlation-based training objective.

## Executive Summary
This paper addresses a critical gap in multimodal AI evaluation by introducing COCO CON, a benchmark for measuring cross-task consistency in unified vision-language models. The authors find that state-of-the-art models like Unified-IO and OFA exhibit surprising inconsistencies when performing semantically equivalent tasks. To address this, they propose a rank correlation-based loss function that aligns output spaces across tasks during training. Experiments demonstrate that this approach improves consistency while maintaining or even enhancing task accuracy, particularly for heterogeneous task pairs.

## Method Summary
The authors introduce COCO CON, a benchmark dataset containing contrast sets that perturb semantically meaningful concepts in images to create paired inputs across multiple tasks. Consistency is measured by comparing model predictions for original and modified inputs using Spearman's rank correlation coefficient. A differentiable rank correlation loss function is proposed to improve consistency during training. The method is evaluated on pretrained Unified-IO and OFA models, with fine-tuning experiments demonstrating improved consistency without sacrificing task accuracy.

## Key Results
- State-of-the-art unified vision-language models exhibit high degrees of cross-task inconsistency, particularly for heterogeneous task pairs
- Larger multi-task models show more inconsistency due to wider semantic coverage
- The proposed rank correlation-based training objective improves consistency while maintaining or improving task accuracy
- Consistency improvements are most pronounced for task pairs with heterogeneous output modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrast sets expose systematic gaps in a model's decision boundaries across tasks
- Mechanism: By perturbing semantically meaningful concepts and comparing likelihood rankings across tasks, the method reveals whether the model maintains a consistent semantic understanding
- Core assumption: If a model's interpretation of an image varies significantly across tasks, this will manifest as inconsistent rankings of perturbed instances
- Evidence anchors:
  - [abstract] "We find that state-of-the-art vision-language models suffer from a surprisingly high degree of inconsistent behavior across tasks"
  - [section 3] "If a model is more likely to predict the contrast output for one task and the ground truth output for the other task or vice-versa, it implies that the model has contradicting interpretations of the same input for the two tasks"
  - [corpus] Weak evidence - related work focuses on knowledge graphs and ontology reasoning rather than multimodal task consistency
- Break condition: If the distance function d(.) is not meaningful or perturbations are too large, the contrast sets may not effectively probe the decision boundaries

### Mechanism 2
- Claim: Rank correlation loss encourages alignment of output spaces across tasks
- Mechanism: The soft ranking-based loss penalizes cases where equivalent semantic concepts receive different rankings across tasks, pushing the model toward consistent interpretations
- Core assumption: A model with aligned output spaces across tasks will assign similar rankings to semantically equivalent outputs
- Evidence anchors:
  - [section 5] "If an output for task t0 is ranked at k-2 while the equivalent output for task t1 is ranked at k+2, the gradients from this objective are designed to push the two misaligned outputs towards a common rank k"
  - [section 7.3] "We find that our proposed loss objective improves consistency along both metrics i.e. top-1 % consistency and rank correlation"
  - [corpus] Weak evidence - limited direct evidence in corpus for rank correlation methods in multimodal learning
- Break condition: If the weight coefficient λ is set too high, the consistency objective may dominate and harm task-specific accuracy

### Mechanism 3
- Claim: Larger multi-task models exhibit more cross-task inconsistency due to wider semantic coverage
- Mechanism: As models are trained on more diverse tasks, their output spaces become more complex and harder to keep aligned, leading to more opportunities for inconsistency
- Core assumption: Task heterogeneity and complexity increase the difficulty of maintaining consistent semantic representations
- Evidence anchors:
  - [section 7.1] "Models capable of performing more tasks are more inconsistent... Uniﬁed-IO models are trained on 90 diverse datasets... OFA models are pretrained on a subset of the tasks... we observe that OFA models are more consistent"
  - [section 7.1] "Inconsistency increases with the heterogeneity between output modalities within a pair of tasks as well as with the complexity of the tasks themselves"
  - [corpus] Moderate evidence - "Hierarchy-Aware Fine-Tuning of Vision-Language Models" suggests hierarchical structure affects consistency
- Break condition: If the additional tasks share significant semantic overlap, inconsistency may not increase proportionally with task count

## Foundational Learning

- Concept: Contrast sets as evaluation methodology
  - Why needed here: Traditional i.i.d. test sets cannot reveal how models handle semantically meaningful variations
  - Quick check question: How does a contrast set differ from adversarial examples in terms of perturbation strategy?

- Concept: Rank correlation as a consistency metric
  - Why needed here: Direct comparison of outputs across different modalities is ill-defined; ranking provides a common basis
  - Quick check question: Why is Spearman's rank correlation coefficient appropriate for measuring alignment between output spaces?

- Concept: Soft ranking for differentiable optimization
  - Why needed here: Standard ranking is non-differentiable, making it unsuitable for gradient-based training
  - Quick check question: What mathematical transformation makes ranking differentiable while preserving its properties?

## Architecture Onboarding

- Component map:
  Input -> Unified multimodal model (Unified-IO or OFA) with shared encoder and task-specific decoders -> Contrast set generation -> Consistency evaluation -> Rank correlation loss -> Model updates

- Critical path: Input → Unified encoder → Task decoders → Contrast set generation → Consistency evaluation → Rank correlation loss → Model updates
- Design tradeoffs: The unified architecture trades specialized task performance for flexibility, but this creates consistency challenges that must be explicitly addressed
- Failure signatures: Low consistency scores despite high task accuracy, particularly for heterogeneous task pairs
- First 3 experiments:
  1. Evaluate pretrained Unified-IO/OFA on COCO CON benchmark to establish baseline inconsistency
  2. Train with rank correlation loss (λ = 0.25) and evaluate consistency improvements
  3. Vary λ to find optimal balance between consistency and task accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does cross-task consistency generalize to models trained on different datasets or with different architectures?
- Basis in paper: [explicit] The paper states that "Models capable of performing more tasks are more inconsistent" and compares Uniﬁed-IO (trained on 90 diverse datasets) with OFA (trained on a subset of tasks).
- Why unresolved: The paper only evaluates two specific models (Uniﬁed-IO and OFA) and does not explore how consistency might vary across different model architectures or training datasets.
- What evidence would resolve it: Experiments evaluating cross-task consistency across a diverse set of models trained on different datasets and with different architectures (e.g., transformers, CNNs, etc.).

### Open Question 2
- Question: What is the relationship between cross-task consistency and robustness to adversarial attacks?
- Basis in paper: [inferred] The paper introduces contrast sets as a way to evaluate consistency, which is similar to adversarial examples. It also mentions that "contrast sets can be of varying difficulty, depending on the likelihood of the perturbations used to create the contrast sets."
- Why unresolved: The paper does not explicitly explore the connection between consistency and robustness to adversarial attacks, although the concepts are related.
- What evidence would resolve it: Experiments evaluating the robustness of models to adversarial attacks and comparing it with their cross-task consistency scores.

### Open Question 3
- Question: How does cross-task consistency impact real-world applications of multimodal models?
- Basis in paper: [explicit] The paper mentions that "Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs."
- Why unresolved: The paper focuses on the theoretical aspects of cross-task consistency and does not provide concrete examples of how inconsistency might impact real-world applications.
- What evidence would resolve it: Case studies or experiments evaluating the performance of multimodal models in real-world applications, such as image captioning for visually impaired users or visual question answering for customer service chatbots.

## Limitations
- The automated contrast set generation relies on LLM filtering whose quality control measures are not fully specified
- Rank correlation alignment may not capture the full complexity of semantic relationships across heterogeneous tasks
- Findings may not generalize beyond Unified-IO and OFA architectures to other multimodal model families

## Confidence

**High confidence** for claims about the existence of cross-task inconsistency in current unified vision-language models, supported by direct empirical measurements on the COCO CON benchmark.

**Medium confidence** for the effectiveness of the rank correlation-based training objective in improving consistency, as the improvements are demonstrated but the mechanism's general applicability to other architectures remains unproven.

**Low confidence** for claims about why larger models exhibit more inconsistency, as the correlation between model size and inconsistency could be influenced by multiple confounding factors including training data, architecture differences, and task heterogeneity.

## Next Checks
1. **Contrast set quality validation**: Manually verify a random sample of generated contrast sets to assess the quality and semantic validity of the perturbations
2. **Ablation study on loss components**: Systematically vary the weight coefficient λ in the rank correlation loss to determine the optimal balance between consistency improvement and task accuracy maintenance
3. **Cross-architecture generalization test**: Apply the same consistency evaluation methodology to other unified vision-language models (e.g., Flamingo, BLIP) to determine if the observed inconsistencies are architecture-specific or represent a broader challenge in the field