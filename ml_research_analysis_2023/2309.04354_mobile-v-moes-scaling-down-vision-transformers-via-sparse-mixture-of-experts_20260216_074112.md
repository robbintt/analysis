---
ver: rpa2
title: 'Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts'
arxiv_id: '2309.04354'
source_url: https://arxiv.org/abs/2309.04354
tags:
- experts
- sparse
- moes
- dense
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores using sparse Mixture-of-Experts (MoE) models
  to improve the efficiency of Vision Transformers (ViTs) for resource-constrained
  applications. The key idea is to simplify MoE design by routing entire images rather
  than individual patches to experts, and to use super-class information to guide
  expert specialization.
---

# Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts

## Quick Facts
- arXiv ID: 2309.04354
- Source URL: https://arxiv.org/abs/2309.04354
- Reference count: 21
- Key outcome: Mobile V-MoEs achieve up to 4.66% accuracy improvement for a 54M FLOP ViT variant on ImageNet-1k

## Executive Summary
This paper introduces Mobile V-MoEs, a novel approach to improve the efficiency of Vision Transformers (ViTs) for resource-constrained applications. The key innovation is a simplified MoE design that routes entire images rather than individual patches to experts, combined with a super-class-based routing mechanism that uses semantic groupings to guide expert specialization. This approach addresses training instability issues common in MoEs while achieving better performance-efficiency trade-offs than dense ViTs across different model scales.

## Method Summary
Mobile V-MoEs modify the ViT architecture by replacing some dense layers with MoE-ViT layers containing separate MLPs for each expert. The method employs per-image routing (activating k=1 expert per image from E=10 total experts) and uses super-class information derived from confusion matrix clustering to guide the router. The training procedure includes a cross-entropy loss for router supervision (λ=0.3) to ensure stable training. Models are trained from scratch on ImageNet-1k and evaluated on the validation set.

## Key Results
- Mobile V-MoEs outperform dense ViT baselines across all model sizes on ImageNet-1k
- Up to 4.66% accuracy improvement for a 54M FLOP ViT variant
- Better performance-efficiency trade-offs compared to per-patch routing and token-based routing methods
- Stable training achieved through super-class-based routing supervision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-image routing reduces computational overhead compared to per-patch routing
- Mechanism: By routing entire images to experts rather than individual patches, the model activates fewer experts per image, reducing the number of experts that need to be loaded and computed
- Core assumption: Images are processed as a single unit and share similar characteristics within an image
- Evidence anchors:
  - [abstract] "We propose a simplified and mobile-friendly MoE design where entire images rather than individual patches are routed"
  - [section 2.2] "We instead propose to use per-image routing (i.e., the inputs x are entire images) to reduce the number of activated experts per image"
  - [corpus] Weak evidence - no direct comparison of per-image vs per-patch routing efficiency in corpus
- Break condition: If images contain highly diverse patches that require different expert processing, per-image routing may lose the benefits of fine-grained specialization

### Mechanism 2
- Claim: Super-class-based routing provides stable training by avoiding expert collapse
- Mechanism: Using semantic super-classes to guide routing ensures each expert specializes on a distinct cluster of semantically similar images, preventing the model from routing most inputs to just a few experts
- Core assumption: The dataset has natural groupings of semantically similar classes that can be identified through confusion matrices
- Evidence anchors:
  - [abstract] "We also propose a stable MoE training procedure that uses super-class information to guide the router"
  - [section 2.2] "we propose to group the classes of the dataset into super-classes and explictly train the router to make each expert specialize on one super-class"
  - [section 3.2] "Our method (Fig. 2c) is better, except for learned per-token routing"
- Break condition: If the dataset lacks clear semantic groupings or the confusion matrix doesn't reveal meaningful clusters, the super-class approach may not provide stable routing

### Mechanism 3
- Claim: The MoE design achieves better performance-efficiency trade-offs by decoupling model capacity from inference cost
- Mechanism: The sparse MoE activates only k experts out of E total experts per image, allowing the model to scale capacity (determined by E) without increasing inference cost (determined by k)
- Core assumption: The conditional computation effectively captures the necessary information for accurate classification while maintaining efficiency
- Evidence anchors:
  - [abstract] "This allows us to scale-up the overall model capacity (determined by E) without increasing the inference cost (determined by k)"
  - [section 2.1] "In a sparse MoE, we have k ≪ E, s.t. we only need to load and compute the k experts with the largest routing weights"
  - [section 3.1] "Our Mobile V-MoEs outperform the corresponding dense ViT baselines across all model sizes"
- Break condition: If k needs to be increased significantly to maintain accuracy, the efficiency gains diminish

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding how MoE layers differ from dense layers and how they implement conditional computation
  - Quick check question: How does the routing function in an MoE layer determine which experts to activate?

- Concept: Vision Transformers (ViTs) and attention mechanisms
  - Why needed here: The work builds on ViT architecture, modifying it with MoE layers
  - Quick check question: What is the role of patch embeddings in ViTs and how do they relate to the routing mechanism?

- Concept: Training stability and load balancing in neural networks
  - Why needed here: The paper addresses training instability issues common in MoEs through its super-class routing approach
  - Quick check question: What causes expert collapse in MoE training and how do auxiliary losses typically address this?

## Architecture Onboarding

- Component map:
  Dense ViT backbone -> MoE-ViT layers with separate MLPs per expert -> Router layer -> Expert MLPs -> Aggregate expert outputs -> Remaining dense layers -> Classification heads

- Critical path:
  1. Input image → patch embedding → dense ViT layers
  2. MoE-ViT layer → router → activate k experts
  3. Expert MLPs process routed features
  4. Aggregate expert outputs → remaining dense layers
  5. Classification heads for both routing and main task

- Design tradeoffs:
  - Patch size (32x32) vs. FLOPs and parameter count
  - Number of experts E vs. routing complexity and super-class granularity
  - Number of MoE layers L vs. router information availability
  - Number of activated experts k vs. performance and efficiency

- Failure signatures:
  - Training instability or divergence (router collapse)
  - Performance plateaus or degrades with increasing model size
  - High variance in expert activation across inputs
  - FLOPs increase without corresponding accuracy gains

- First 3 experiments:
  1. Implement basic per-image routing with random expert assignment and compare FLOPs to dense baseline
  2. Add super-class routing using predefined class groupings and measure training stability
  3. Optimize k (experts per image) and E (total experts) to find the best performance-efficiency trade-off

## Open Questions the Paper Calls Out
None explicitly mentioned in the paper.

## Limitations
- The paper lacks detailed architectural specifications for the MoE-ViT layers, particularly expert MLP dimensions and router architecture
- The super-class clustering methodology is underspecified with no clear description of the clustering algorithm or implementation details
- The per-image routing assumption may break down for datasets with highly diverse content within individual images

## Confidence
- High confidence: Core premise that per-image routing reduces computational overhead compared to per-patch routing
- Medium confidence: Effectiveness of super-class routing for preventing expert collapse, given reliance on dataset-specific semantic groupings
- Medium confidence: Claimed performance-efficiency trade-offs, as paper lacks extensive ablation studies on all design choices

## Next Checks
1. Implement and test the per-image routing mechanism with random expert assignment on a small-scale dataset to verify claimed computational efficiency gains
2. Replicate the super-class clustering approach using the confusion matrix methodology and evaluate its impact on training stability across different dataset sizes
3. Conduct ablation studies varying k (experts per image) and E (total experts) to determine optimal configuration for different model scales and tasks