---
ver: rpa2
title: 'Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free
  Domain Adaptation'
arxiv_id: '2301.13428'
source_url: https://arxiv.org/abs/2301.13428
tags:
- domain
- data
- adaptation
- samples
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unsupervised domain adaptation
  without access to source data, known as source-free domain adaptation (SFDA). The
  authors propose a novel method called Contrast and Clustering (CaC) that leverages
  contrastive learning to learn domain-invariant features directly from unlabeled
  target data.
---

# Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation

## Quick Facts
- **arXiv ID**: 2301.13428
- **Source URL**: https://arxiv.org/abs/2301.13428
- **Reference count**: 11
- **Primary result**: Achieves 87.7% average accuracy on VisDA, surpassing previous SFDA methods

## Executive Summary
This paper introduces Contrast and Clustering (CaC), a novel method for source-free domain adaptation (SFDA) that leverages contrastive learning to align source and target domains without access to source data. The key innovation is using nearest neighbor relationships in the feature space to define positive pairs (same class) and extended neighbors for hard negative pairs. By clustering nearest neighbors while separating dissimilar samples, CaC learns domain-invariant features directly from unlabeled target data. The method demonstrates state-of-the-art performance across three standard benchmarks: VisDA, Office-Home, and Office-31.

## Method Summary
CaC addresses source-free domain adaptation by learning domain-invariant features through contrastive clustering. The method maintains three memory banks storing target features, predictions, and neighbor indexes. For each sample, it identifies K nearest neighbors as positive pairs and uses extended neighbors (neighbors of neighbors) to construct hard negative pairs. The CaC loss function maximizes agreement between positive pairs while minimizing similarity with negative pairs, weighted by a decay factor to handle class imbalance. Training proceeds by updating memory banks, computing nearest neighbors, generating extended neighbors, and optimizing the contrastive clustering loss through backpropagation.

## Key Results
- Achieves 87.7% average accuracy on VisDA benchmark
- Outperforms previous SFDA methods by significant margins across all three benchmarks
- Ablation studies confirm effectiveness of contrastive clustering loss and extended neighbor strategy
- Demonstrates robust performance on Office-Home and Office-31 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive clustering loss CaC improves feature alignment by pulling nearest neighbors (same class) closer while pushing dissimilar samples apart.
- Mechanism: CaC defines two probability functions—Psame for positive pairs (nearest neighbors) and Pdis for negative pairs (non-neighbors). The loss minimizes the negative log ratio of these probabilities, effectively clustering same-class samples while separating different classes.
- Core assumption: Nearest neighbors in the feature space belong to the same class, and the pretrained source model's features already form rough clusters.
- Evidence anchors:
  - [abstract] "The key idea of our work is to learn a domain-invariant feature by 1) performing clustering directly in the original feature space with nearest neighbors; 2) constructing truly hard negative pairs by extended neighbors"
  - [section 3.3] "Similar samples (e.g., husky and alaskan malamute) should have similar predictions, while dissimilar samples (e.g., husky and parrot) should have different predictions."
  - [corpus] Weak evidence - no direct corpus citations supporting this specific contrastive clustering mechanism
- Break condition: If nearest neighbors don't actually represent same-class samples (class overlap in feature space), the positive pair term will incorrectly pull dissimilar samples together.

### Mechanism 2
- Claim: Extended neighbors mined from memory banks create harder negative pairs without computational overhead.
- Mechanism: For each sample, extended neighbors are found by querying the nearest neighbor indexes of its nearest neighbors. These extended neighbors are more dissimilar than simple random negatives, providing better separation between classes.
- Core assumption: Extended neighbors are more likely to belong to different classes than the base nearest neighbors.
- Evidence anchors:
  - [abstract] "constructing truly hard negative pairs by extended neighbors without introducing additional computational complexity"
  - [section 3.5] "we utilize the expanded neighbors of each feature, i.e., the nearest neighbors of each feature and the nearest neighbors of these nearest neighbors"
  - [corpus] No direct evidence in corpus supporting the effectiveness of extended neighbor strategy
- Break condition: If extended neighbors still contain similar samples (e.g., in highly imbalanced datasets), the negative pair term becomes noisy and degrades performance.

### Mechanism 3
- Claim: Decaying the influence of negative pairs over training epochs mitigates class imbalance issues.
- Mechanism: A decay factor α = (max_iter/(max_iter+iter))^β is introduced to reduce the impact of negative pairs as training progresses, preventing the model from incorrectly separating large class clusters.
- Core assumption: Early training benefits from strong negative pairs, but later training requires more focus on positive pairs when class imbalance creates noisy negatives.
- Evidence anchors:
  - [section 4.2] "We find that CaC can maintain the accuracy improvement on Office-Home, but degrades at a later stage on VisDA... we utilized extended nearest neighbors to find more valuable negative samples; however, the contribution of the negative pair term to the loss may still be significant."
  - [section 4.2] "We introduce a factor α = (max_iter/(max_iter+iter))^β to control the impact of negative pairs."
  - [corpus] No corpus evidence supporting this specific decay strategy for contrastive learning
- Break condition: If the decay is too aggressive, the model loses the benefit of hard negative pairs too early; if too slow, class imbalance continues to degrade performance.

## Foundational Learning

- **Concept**: Contrastive learning and InfoNCE loss
  - Why needed here: The method builds upon InfoNCE to extend it for multiple positive pairs (nearest neighbors) rather than single augmented views
  - Quick check question: How does InfoNCE differ from the proposed CaC loss in terms of positive and negative pair definitions?

- **Concept**: Memory bank mechanism for efficient nearest neighbor retrieval
  - Why needed here: Memory banks store features and predictions to efficiently find nearest neighbors without recomputing similarities for all samples
  - Quick check question: What information is stored in each of the three memory banks (F, P, N) and why?

- **Concept**: Self-supervised learning without labels
  - Why needed here: The method relies on the pretrained source model's feature space structure to provide pseudo-supervision through nearest neighbor relationships
  - Quick check question: Why can we assume that nearest neighbors in the source-pretrained feature space belong to the same class?

## Architecture Onboarding

- **Component map**: Feature extractor f -> Classifier C -> Three memory banks (F for features, P for predictions, N for neighbor indexes) -> CaC loss function
- **Critical path**: Forward pass → memory bank update → nearest neighbor retrieval → extended neighbor computation → loss calculation → backward pass
- **Design tradeoffs**: Memory bank size vs. computational efficiency; number of neighbors K vs. noise in positive pairs; negative pair decay rate vs. class imbalance handling
- **Failure signatures**: Accuracy degradation on imbalanced classes; training instability when K is too large; poor performance when source model doesn't provide good feature clusters
- **First 3 experiments**:
  1. Implement basic InfoNCE with single positive pair and compare to CaC with nearest neighbor positives
  2. Test different K values (1, 3, 5, 8) on VisDA to find optimal neighbor count
  3. Compare performance with and without extended neighbor strategy on Office-Home dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CaC scale with increasingly large domain shifts between source and target domains?
- Basis in paper: [inferred] The paper demonstrates CaC's effectiveness on standard domain adaptation benchmarks but does not explore extreme domain shifts.
- Why unresolved: The experimental evaluation focuses on moderate domain adaptation tasks within established benchmark datasets, limiting insight into performance boundaries.
- What evidence would resolve it: Empirical testing of CaC across datasets with progressively larger domain discrepancies, including synthetic domain shifts and real-world scenarios with significant visual or semantic differences.

### Open Question 2
- Question: What is the theoretical relationship between the proposed CaC loss function and established domain adaptation theories such as theory of optimal transport or domain discrepancy measures?
- Basis in paper: [explicit] The paper proposes a contrastive clustering loss without establishing theoretical connections to existing domain adaptation frameworks.
- Why unresolved: The method is presented as empirically effective without formal theoretical justification or bounds on its domain adaptation capabilities.
- What evidence would resolve it: Mathematical analysis connecting CaC's objective to established domain adaptation theory, including proofs of convergence or bounds on domain discrepancy reduction.

### Open Question 3
- Question: How does CaC perform in the more challenging open-set domain adaptation scenario where target domain contains classes not present in the source domain?
- Basis in paper: [explicit] The paper explicitly states it focuses on closed-set domain adaptation where source and target share the same classes.
- Why unresolved: The proposed method relies on clustering based on shared class structure, which may fail when target data contains novel classes.
- What evidence would resolve it: Experimental evaluation of CaC on open-set domain adaptation benchmarks, along with potential modifications to handle unknown classes.

## Limitations
- Relies on the assumption that nearest neighbors in source-pretrained feature space belong to the same class, which may fail with significant class overlap
- Extended neighbor strategy lacks direct empirical validation and may introduce noisy negative pairs
- Decay mechanism for negative pairs is heuristic without theoretical grounding

## Confidence
- **High confidence**: The general framework of using contrastive learning for source-free domain adaptation is sound and builds on established methods
- **Medium confidence**: The specific implementation details (extended neighbors, decay factor) are reasonable but lack direct empirical validation
- **Low confidence**: The assumption that nearest neighbors always represent same-class samples is questionable and could fail in challenging scenarios

## Next Checks
1. Test CaC performance on datasets with known class overlap to verify the nearest neighbor assumption
2. Compare extended neighbor strategy against simpler hard negative mining methods (e.g., hardest negatives from memory bank)
3. Conduct ablation studies on the decay factor β to determine optimal values across different datasets