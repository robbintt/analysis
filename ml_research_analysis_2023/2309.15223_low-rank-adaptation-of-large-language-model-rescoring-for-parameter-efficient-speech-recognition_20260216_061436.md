---
ver: rpa2
title: Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient
  Speech Recognition
arxiv_id: '2309.15223'
source_url: https://arxiv.org/abs/2309.15223
tags:
- training
- lorb
- fine-tuning
- rescoring
- low-rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LoRB (Low-rank Rescoring for BERT), a parameter-efficient
  adaptation method for speech recognition rescoring using low-rank decomposition.
  The method inserts low-rank matrices into pretrained BERT layers, freezing all original
  parameters and updating only 0.08% of them.
---

# Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition

## Quick Facts
- arXiv ID: 2309.15223
- Source URL: https://arxiv.org/abs/2309.15223
- Reference count: 0
- Introduces LoRA-based rescoring method achieving comparable WER to full fine-tuning with 6x faster training and 32% less GPU memory

## Executive Summary
This paper presents LoRB (Low-rank Rescoring for BERT), a parameter-efficient adaptation method for speech recognition rescoring using low-rank decomposition. The method inserts low-rank matrices into pretrained BERT layers, freezing all original parameters and updating only 0.08% of them. A discriminative training objective (MWER) combined with correlation-based regularization addresses overfitting and generalization issues. Evaluated on LibriSpeech and internal datasets across domains, LoRB achieves comparable or better WER than full fine-tuning with up to 6x faster training and 32% less GPU memory usage. Scaling experiments show performance improvements plateau logarithmically with data size and improve with larger model scales.

## Method Summary
The paper introduces a low-rank adaptation approach for rescoring in speech recognition tasks. LoRA matrices are inserted additively into transformer layers of a pretrained BERT model, with all original parameters frozen and only the low-rank matrices updated during training. The model uses a discriminative training objective (MWER) that optimizes for word error rate rather than next-token prediction, combined with correlation-based regularization to prevent representation collapse. The method is evaluated on both public (LibriSpeech) and internal datasets across multiple domains, comparing performance against full fine-tuning baselines.

## Key Results
- LoRA achieves comparable WER to full fine-tuning on LibriSpeech and internal datasets while updating only 0.08% of parameters
- Training speed improves by up to 6x and GPU memory usage decreases by 32% compared to full fine-tuning
- Logarithmic scaling relationship observed between dataset size and WER improvement, with larger models showing better performance
- Correlation regularization helps prevent overfitting and improves generalization to non-target domains

## Why This Works (Mechanism)

### Mechanism 1
Freezing pretrained parameters while only updating low-rank matrices prevents catastrophic forgetting. The original BERT weights are frozen, so the model retains its general linguistic knowledge while the small LoRA matrices adapt to speech-specific patterns. Core assumption: The pretrained BERT model contains sufficient general language knowledge that doesn't need modification for ASR rescoring. Evidence: Abstract states "freeze all pretrained parameters in the LLM and inserts a trainable pair of matrices" and section 3.2 specifies "W0 is frozen and only WA and WB get updated." Break condition: If the domain shift between pretraining and ASR rescoring is too large, the frozen parameters may lack relevant knowledge.

### Mechanism 2
Low-rank decomposition captures essential adaptation directions while ignoring noise. The low-rank matrices WA and WB form a compressed representation of the necessary parameter changes, filtering out irrelevant or harmful fine-tuning updates. Core assumption: The adaptation needed for ASR rescoring lies in a low-dimensional subspace of the full parameter space. Evidence: Abstract mentions "low-rank decomposition to train a rescoring BERT model" and section 3.2 explains "perform a low-rank decomposition to the updates ∆W = WBWA." Break condition: If adaptation requires high-rank changes (complex domain shifts), the low-rank constraint may become a bottleneck.

### Mechanism 3
Correlation-based regularization prevents representation collapse during fine-tuning. The regularization loss forces the hidden representations to maintain their original variability structure, preventing them from becoming too focused on training data. Core assumption: Maintaining isotropy in the feature space preserves the model's ability to generalize to unseen data. Evidence: Abstract mentions "correlation-based regularization besides the minimum word error loss" and section 3.3 states "The correlation-based regularization [40] has been proposed to alleviate the representation degeneration [41] problem." Break condition: If the regularization strength is too high, it may prevent necessary adaptation to domain-specific patterns.

## Foundational Learning

- **Low-rank matrix decomposition**: Understanding how WA and WB combine to approximate the full parameter update ∆W is crucial for grasping LoRA's efficiency. Quick check: If original weight matrix W0 is 1024×1024 and LoRA rank r=8, how many parameters are in WA and WB versus the full update?

- **Discriminative training with MWER loss**: The training objective directly optimizes for word error rate rather than next-token prediction, making it more suitable for rescoring. Quick check: Why does the MWER loss use the expected word error over the N-best list rather than just picking the hypothesis with lowest score?

- **Transformer architecture components**: Understanding where LoRA matrices are inserted (Q, K, V, FFN weights) is essential for implementation and debugging. Quick check: Which transformer components (attention matrices, feed-forward weights) are typically targeted for LoRA insertion and why?

## Architecture Onboarding

- **Component map**: Input N-best list → BERT processing with frozen weights + LoRA updates → feed-forward scoring → MWER optimization
- **Critical path**: Input N-best list → BERT processing with frozen weights + LoRA updates → feed-forward scoring → MWER optimization
- **Design tradeoffs**: Memory efficiency (freezing weights) vs. potential adaptation limitations; regularization vs. domain specificity
- **Failure signatures**: Overfitting (good on target domain, poor on others); underfitting (poor on all domains); training instability
- **First 3 experiments**:
  1. Implement basic LoRA insertion in a single BERT layer and verify parameter count reduction
  2. Train on a small ASR dataset with MWER loss and compare to full fine-tuning
  3. Add correlation regularization and test generalization to out-of-domain data

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal rank value for LoRA matrices in ASR rescoring tasks, and how does it vary with model size and dataset characteristics? The paper mentions varying LoRA rank over values {4,8,16,32} but states "there is no obvious correlation between rank value and word error rate across different data scale settings." This remains unresolved because the paper shows no clear correlation between rank value and performance, and the optimal rank likely depends on multiple factors including model size, dataset size, and domain complexity. Systematic experiments varying rank values across different model scales (5M, 170M, 1B parameters) and dataset sizes (150K to 20M utterances) while measuring WER and computational efficiency would establish guidelines for optimal rank selection.

### Open Question 2
How do LoRA matrices learn domain-specific features compared to full fine-tuning, and what are the representational differences? While the paper shows LoRA achieves comparable performance with fewer parameters, it doesn't investigate the learned representations or feature importance. Analysis of LoRA matrix values across domains, comparison of attention patterns between LoRA and full fine-tuning, and ablation studies removing individual LoRA components would reveal the learned features and their importance.

### Open Question 3
Can the correlation-based regularization be further optimized or combined with other techniques to improve generalization across domains? The paper mentions correlation-based regularization helps but suggests it's not the complete solution for domain generalization. Experiments comparing correlation regularization with other regularization methods (dropout, weight decay, adversarial training), and combinations thereof, across multiple domains would identify optimal regularization strategies for ASR rescoring.

## Limitations
- Evaluation relies heavily on internal datasets whose data distribution and quality are not publicly available
- Logarithmic scaling relationship observed in scaling experiments is based on only 5 data points and may not hold for larger datasets
- Actual memory savings in real-world deployment scenarios with larger models or different hardware configurations is not quantified

## Confidence

**High confidence**: Claims about LoRA parameter efficiency (0.08% parameters updated) and training speed/memory improvements are supported by clear quantitative evidence and straightforward mathematical derivations. The freezing mechanism for preventing catastrophic forgetting is well-established in LoRA literature.

**Medium confidence**: Claims about MWER loss and correlation regularization improving generalization are supported by ablation studies on internal datasets, but the limited evaluation domains and absence of statistical significance testing reduce confidence. The scaling law observations (logarithmic relationship with data size) show promise but require more extensive validation.

**Low confidence**: Claims about LoRA being "optimal" for parameter-efficient rescoring are difficult to verify without comparisons to other adaptation methods (full fine-tuning, adapter tuning, prefix tuning) on identical datasets and training conditions.

## Next Checks

1. **Cross-domain generalization test**: Evaluate the trained models on additional public speech recognition datasets (e.g., Common Voice, TED-LIUM) to verify that the observed generalization benefits extend beyond the internal datasets used in the paper.

2. **Ablation of regularization strength**: Systematically vary the correlation regularization weight λ across multiple orders of magnitude to determine the optimal value and verify that the reported performance benefits are not sensitive to this hyperparameter.

3. **Direct comparison with full fine-tuning**: Conduct head-to-head comparisons on identical hardware configurations measuring not just WER but also actual wall-clock training time, GPU memory usage during training and inference, and parameter update efficiency.