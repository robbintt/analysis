---
ver: rpa2
title: 'Deciphering Diagnoses: How Large Language Models Explanations Influence Clinical
  Decision Making'
arxiv_id: '2310.01708'
source_url: https://arxiv.org/abs/2310.01708
tags:
- explanations
- explanation
- diagnoses
- doctors
- diagnosis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the quality and reliability of LLM-generated
  explanations for medical diagnoses based on patient complaints, and examines their
  impact on clinical decision-making. Three experienced doctors assessed LLM explanations
  across three experimental stages, measuring agreement rates with diagnoses both
  with and without explanations.
---

# Deciphering Diagnoses: How Large Language Models Explanations Influence Clinical Decision Making

## Quick Facts
- arXiv ID: 2310.01708
- Source URL: https://arxiv.org/abs/2310.01708
- Reference count: 24
- LLM explanations significantly increase doctors' agreement with diagnoses, but reveal 5-30% error rates in generated explanations

## Executive Summary
This study investigates how LLM-generated explanations for medical diagnoses affect clinical decision-making by evaluating three experienced doctors' responses to both ground truth and model-predicted diagnoses. The research demonstrates that LLM explanations significantly increase doctors' agreement rates with given diagnoses, even when the explanations contain inaccuracies. The findings reveal that while LLMs can produce high-quality explanations in most cases, they tend to invent connections between symptoms and diagnoses, leading to potential errors that could impact patient safety.

## Method Summary
The study employed a three-stage evaluation process using 500 patient complaint-diagnosis pairs from the RuMedBench dataset. First, doctors evaluated the quality of LLM-generated explanations for ground truth diagnoses. Second, they assessed how explanations affected their agreement with diagnoses. Third, they evaluated explanations for model-predicted diagnoses generated by a transformer classification model. Explanations were generated using GPT-3.5-turbo with a standardized prompt linking patient symptoms to diagnoses.

## Key Results
- LLM explanations increased doctors' agreement rates with diagnoses by 16.2% to 29.3% across all three doctors
- Doctors showed similar acceptance rates (76-98%) for both ground truth and model-predicted diagnoses
- LLM explanations contained errors in 5-30% of cases, primarily from attempting to connect all symptoms to the diagnosed condition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM explanations increase doctors' agreement with diagnostic hypotheses even when the explanations are not perfect.
- Mechanism: The LLM generates a narrative linking patient complaints to the diagnosis, which acts as a persuasive argument. Doctors are more likely to accept a diagnosis when provided with a coherent explanation, even if some details are inaccurate.
- Core assumption: Doctors' agreement rates are influenced by the presence of an explanation, regardless of its accuracy.
- Evidence anchors:
  - [abstract] "Experimental results demonstrated that LLM explanations significantly increased doctors' agreement rates with given diagnoses"
  - [section 4.1] "Overall, our data show that the provided explanation led to an increased agreement rate with the provided diagnosis for all three doctors, and this increase was statistically significant"

### Mechanism 2
- Claim: LLMs can generate explanations for model predictions that are as acceptable as those for ground truth diagnoses.
- Mechanism: The LLM uses the same prompt structure to generate explanations for both human-assigned and model-predicted diagnoses, focusing on linking symptoms to the diagnosis regardless of its source.
- Core assumption: The quality of explanations is independent of whether the diagnosis comes from a human or a model.
- Evidence anchors:
  - [section 4.3] "The results are presented in Table 3 and show that the acceptance ratio is similar across both settings and varies from 76% to 98%"

### Mechanism 3
- Claim: LLMs tend to invent connections between symptoms and diagnoses, leading to errors.
- Mechanism: The LLM attempts to explain every symptom mentioned in the patient's complaints, even when some symptoms are unrelated to the diagnosed condition. This results in the invention of non-existent connections or symptoms.
- Core assumption: LLMs prioritize providing a complete explanation over accuracy, leading to the inclusion of fabricated details.
- Evidence anchors:
  - [section 6.3] "The model, however, attempted to link all symptoms to the diagnosed condition, thereby causing most errors."

## Foundational Learning

- Concept: Understanding the difference between correlation and causation in medical diagnoses.
  - Why needed here: Doctors need to distinguish between symptoms that are causally related to a diagnosis and those that are merely correlated, which is crucial when evaluating LLM explanations.
  - Quick check question: Can a patient have a symptom that is not directly related to their diagnosed condition?

- Concept: Familiarity with the structure and content of Electronic Health Records (EHR).
  - Why needed here: The study uses patient complaints entered into the EHR system, so understanding this context is essential for interpreting the results.
  - Quick check question: What information is typically included in patient complaints entered into an EHR system?

- Concept: Knowledge of ICD (International Classification of Diseases) codes and their use in medical diagnoses.
  - Why needed here: The study refers to ICD codes as ground truth data for diagnoses, so understanding this coding system is important for interpreting the results.
  - Quick check question: What is the purpose of ICD codes in medical diagnoses and how are they used?

## Architecture Onboarding

- Component map:
  Patient complaints -> Transformer classification model -> GPT-3.5-turbo -> Medical experts

- Critical path:
  1. Input patient complaints into the transformer model
  2. Generate a diagnosis using the transformer model
  3. Use the GPT-3.5-turbo model to generate an explanation linking complaints to the diagnosis
  4. Have medical experts evaluate the explanation and diagnosis

- Design tradeoffs:
  - Using a pre-trained LLM (GPT-3.5-turbo) vs. training a custom model for explanations
  - Evaluating explanations based on agreement with diagnosis vs. evaluating the explanation's content independently
  - Using a single LLM for both human-assigned and model-predicted diagnoses vs. using separate models

- Failure signatures:
  - High agreement rates with explanations that contain fabricated connections or symptoms
  - Low inter-agreement among medical experts when evaluating explanations
  - Explanations that are accepted by some experts but not others, indicating inconsistency

- First 3 experiments:
  1. Evaluate the effect of LLM explanations on doctor agreement rates by comparing agreement with and without explanations.
  2. Compare doctor agreement rates with ground truth diagnoses vs. model-predicted diagnoses.
  3. Analyze the types of errors made by the LLM in generating explanations by having medical experts identify and categorize errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the inherent subjectivity in medical professionals' evaluations of LLM-generated explanations be standardized to ensure consistent quality assessments?
- Basis in paper: [explicit] The paper highlights the complexity of defining and evaluating the quality of explanations, noting the low inter-agreement among participating medical professionals.
- Why unresolved: The variability in doctors' requirements for explanations, rooted in their training, experiences, and possibly cultural contexts, makes it challenging to establish a universal standard.
- What evidence would resolve it: Developing a standardized framework or rubric for evaluating LLM-generated explanations, validated across diverse medical professionals and contexts, would help address this issue.

### Open Question 2
- Question: What specific mechanisms can be implemented to minimize the errors in LLM-generated explanations, particularly those arising from the model's attempt to link all symptoms to a diagnosed condition?
- Basis in paper: [explicit] The paper identifies that LLMs can err by attempting to connect all symptoms to the diagnosed condition, leading to potential errors ranging from 5% to 30%.
- Why unresolved: While the paper identifies the problem, it does not provide specific solutions or mechanisms to reduce these errors.
- What evidence would resolve it: Research demonstrating the effectiveness of targeted interventions, such as improved prompt engineering or additional validation checks, in reducing the error rate in LLM-generated explanations would be beneficial.

### Open Question 3
- Question: How does the inclusion of additional patient data beyond initial complaints impact the accuracy and reliability of LLM-generated diagnoses and explanations?
- Basis in paper: [inferred] The paper suggests that the ground truth diagnosis was made based on more information than is contained in the patient's complaints, which may lead to diagnoses not directly tied to the provided complaints.
- Why unresolved: The study primarily focuses on patient complaints as the input data, without exploring the impact of additional patient information on LLM performance.
- What evidence would resolve it: Comparative studies evaluating LLM performance with varying levels of patient data input would provide insights into the optimal data requirements for accurate and reliable diagnoses.

## Limitations
- Study relies on only three experienced doctors for evaluation, limiting generalizability
- Error rate of 5-30% in LLM explanations represents a significant safety concern for clinical applications
- Research focuses on Russian medical data, raising questions about cross-cultural applicability

## Confidence

**High Confidence**: The finding that LLM explanations increase doctors' agreement rates with diagnoses is well-supported by statistical analysis and multiple evaluation stages.

**Medium Confidence**: The characterization of LLM error types (inventing connections) is supported by qualitative analysis but would benefit from more systematic error categorization.

**Low Confidence**: The generalizability of findings to other medical contexts, languages, and healthcare systems remains uncertain.

## Next Checks

1. **Independent Replication**: Have a separate team of medical experts (ideally 10+ physicians) evaluate the same LLM-generated explanations to assess inter-rater reliability and identify potential bias in the original evaluation.

2. **Error Pattern Analysis**: Conduct a systematic categorization of LLM explanation errors by having medical experts identify whether errors stem from fabricated connections, misinterpretation of symptoms, or other sources, then calculate error rates for each category.

3. **Clinical Impact Study**: Design a prospective study where LLM explanations are used in actual clinical settings (with proper safeguards) to measure their impact on diagnostic accuracy, patient outcomes, and physician decision-making in real-world scenarios.