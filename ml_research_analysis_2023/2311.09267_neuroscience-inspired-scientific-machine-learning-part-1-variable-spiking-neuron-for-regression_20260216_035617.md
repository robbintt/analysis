---
ver: rpa2
title: 'Neuroscience inspired scientific machine learning (Part-1): Variable spiking
  neuron for regression'
arxiv_id: '2311.09267'
source_url: https://arxiv.org/abs/2311.09267
tags:
- spiking
- neuron
- networks
- energy
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a Variable Spiking Neuron (VSN) that combines
  integrate-and-fire behavior with continuous activation, enabling energy-efficient
  regression and classification. Unlike standard spiking neurons that only output
  discrete spikes, VSN activates continuous functions only when spiking occurs, preserving
  sparsity while allowing nonlinear outputs.
---

# Neuroscience inspired scientific machine learning (Part-1): Variable spiking neuron for regression

## Quick Facts
- arXiv ID: 2311.09267
- Source URL: https://arxiv.org/abs/2311.09267
- Reference count: 40
- Key outcome: VSN achieves ANN-level accuracy (≈98% MNIST) with 81% reduced synaptic energy via sparse spiking and continuous activation.

## Executive Summary
Variable Spiking Neurons (VSNs) bridge the gap between spiking neural networks (SNNs) and artificial neural networks (ANNs) by activating continuous functions only when spikes occur. This preserves energy efficiency while enabling accurate regression—a key weakness of traditional SNNs. Tested across classification and regression tasks, VSNs match ANN accuracy while drastically reducing synaptic energy, making them promising for low-power AI applications.

## Method Summary
VSNs combine integrate-and-fire memory updates with continuous activation functions (linear, ReLU, GELU) that only trigger when the memory state crosses a threshold. Surrogate backpropagation with fast sigmoid smoothing enables gradient flow through discontinuous spike thresholds. Networks are trained on MNIST, Fashion-MNIST, and two Feynman regression datasets using ADAM optimizer with learning rate 0.001 (classification) or 0.0001 (regression), weight decay 1e-4, and batch sizes of 200 or 1000. Energy efficiency is measured via average spiking activity (% spikes).

## Key Results
- VSN-1 achieves 98.4% MNIST accuracy with 19.7% average spikes vs. ANN's 12.1%.
- VSN-1 regression on 1-feature-dataset: NMSE = 0.0014, vs. SNN NMSE = 0.0145.
- VSN-1 on 9-feature-dataset: NMSE = 0.0014, S̃ = 22.6%, outperforming SNNs (NMSE > 0.01).

## Why This Works (Mechanism)

### Mechanism 1
VSNs achieve energy efficiency and regression capability by activating continuous functions only when a spike occurs, preserving sparsity while allowing nonlinear outputs. The continuous activation is applied only when the memory state crosses a threshold, enabling sparse yet expressive representations.

### Mechanism 2
Surrogate gradients enable backpropagation through the discontinuous spike threshold using a fast sigmoid approximation (slope 25). This allows computing gradients via chain rule through ∂ỹ(i)/∂M(i) and ∂M(i)/∂z(i), making VSNs trainable with standard optimizers.

### Mechanism 3
VSNs reduce synaptic energy by limiting computation to active spikes, quantified by average % spikes (S̃). Energy scales with spike count, so sparse spiking yields significant savings versus continuous ANN activations.

## Foundational Learning

- **Concept**: Leaky Integrate-and-Fire (LIF) neuron model
  - Why needed here: VSN inherits the memory update M(t) = βM(t-1) + z(t) and reset behavior from LIF, forming the core spiking mechanism.
  - Quick check: In the LIF model, what happens to M(t) when a spike is generated? (Answer: M(t) is reset to 0.)

- **Concept**: Surrogate gradient backpropagation
  - Why needed here: Enables training VSNs by approximating gradients through the discontinuous spike threshold using a smooth surrogate function.
  - Quick check: Why can't vanilla backpropagation be used with spiking neurons? (Answer: The spike function is discontinuous, making gradients undefined.)

- **Concept**: Energy-aware neural network design
  - Why needed here: VSNs trade off accuracy for energy by reducing spiking activity; understanding the energy model (E_VSN-Syn) guides threshold and leakage tuning.
  - Quick check: What metric quantifies the sparsity of spiking activity in VSNs? (Answer: Average % spikes, S̃.)

## Architecture Onboarding

- **Component map**: Input layer → Dense layer(s) → VSN activation layers (with M(t), threshold T, leakage β) → Output layer.

- **Critical path**:
  1. Forward pass: Compute z(t) from dense layer, update M(t), apply threshold to get ỹ(t), compute y(t) = σ(z(t)·ỹ(t)).
  2. Backward pass: Use surrogate gradients to propagate error through the threshold function and memory updates.
  3. Parameter update: Adjust weights, biases, β, and T via optimizer (e.g., ADAM).

- **Design tradeoffs**:
  - Lower T → more spikes → higher accuracy but higher energy.
  - Higher β → slower decay of M(t) → more persistent spiking, risk of saturation.
  - Single STS vs multiple STS: Single STS reduces energy but may limit accuracy for complex mappings; multiple STS increases representational capacity at energy cost.

- **Failure signatures**:
  - Network fails to converge: Likely T too high or β too low, causing insufficient spiking.
  - Poor regression accuracy: May need more STS or different activation (e.g., ReLU vs linear).
  - Training loss plateaus: Surrogate gradient slope may be too shallow; adjust slope parameter.

- **First 3 experiments**:
  1. Train VSNN-1 on MNIST with T=0.25, β=0.9, linear activation; verify accuracy ~98% and S̃ ~12%.
  2. Test VSNN-1 on 1-feature-dataset; check NMSE ~0.001 and S̃ ~22%.
  3. Sweep T from 0.01 to 0.05 on 9-feature-dataset; observe trade-off between S̃ and NMSE to find optimal threshold.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does VSN perform on neuromorphic hardware compared to traditional neural networks?
  - Basis: Authors note VSN's neuromorphic performance is yet to be explored.
  - Why unresolved: No experimental or theoretical analysis on VSN's performance when implemented on neuromorphic hardware.
  - What evidence would resolve it: Experimental results comparing VSN's performance, energy efficiency, and accuracy on neuromorphic hardware versus traditional neural networks.

- **Open Question 2**: What is the optimal way to tune the VSN's parameters (β and T) to minimize spiking activity while maximizing accuracy?
  - Basis: Authors suggest tuning β and T can minimize spiking activity and maximize accuracy, but do not provide a specific method for parameter optimization.
  - Why unresolved: No systematic approach or algorithm for finding optimal β and T for different datasets or network architectures.
  - What evidence would resolve it: A study demonstrating a method or algorithm to optimize β and T for various datasets and network architectures, showing improved energy efficiency and accuracy.

- **Open Question 3**: How does the VSN perform in more complex neural network architectures like neural operators?
  - Basis: Authors mention VSN's performance on complex neural network architectures like neural operators is yet to be explored.
  - Why unresolved: No experiments or analysis on VSN's performance in neural operator architectures, which are used for solving complex scientific problems.
  - What evidence would resolve it: Experimental results comparing VSN's performance in neural operator architectures against traditional neural networks for tasks such as solving partial differential equations.

## Limitations

- Surrogate gradient slope parameter (25) is asserted but not empirically validated for sensitivity across datasets; too low may cause vanishing gradients, too high may overshoot.
- Energy model assumes synaptic operations dominate, but real hardware energy depends on spike rate, fan-out, and implementation technology—scaling may not hold universally.
- Regression performance hinges on VSN's single spike time step (STS); multi-STS was not thoroughly benchmarked for accuracy-energy trade-offs.

## Confidence

- **High Confidence**: VSN achieves classification accuracy comparable to ANNs (≈98% MNIST) with reduced spiking activity (S̃ ~12-20%). This is directly measurable and replicated across datasets.
- **Medium Confidence**: Regression NMSE improvement over SNNs (0.001 vs >0.01) is significant but sensitive to threshold T and STS count. Single STS limits representational capacity for complex mappings.
- **Low Confidence**: Energy savings scaling (81% reduction) assumes linear relationship between spike count and synaptic energy, which may not generalize to diverse hardware or workloads.

## Next Checks

1. Sweep surrogate gradient slope from 10 to 50; measure impact on MNIST convergence speed and final accuracy.
2. Test multi-STS VSN (2-5 STS) on 9-feature-dataset; quantify accuracy gain vs. energy cost increase.
3. Benchmark VSN energy model on spiking hardware simulator (e.g., NEST) to validate spike-count-to-energy scaling assumptions.