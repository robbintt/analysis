---
ver: rpa2
title: 'Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series'
arxiv_id: '2310.14017'
source_url: https://arxiv.org/abs/2310.14017
tags:
- contrastive
- data
- learning
- time
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes COMET, a hierarchical contrastive learning
  framework for medical time-series data. The key idea is to leverage data consistencies
  at multiple levels (observation, sample, trial, and patient) to learn effective
  representations in a self-supervised manner.
---

# Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series

## Quick Facts
- arXiv ID: 2310.14017
- Source URL: https://arxiv.org/abs/2310.14017
- Authors: Multiple authors
- Reference count: 40
- Primary result: COMET outperforms six state-of-the-art baselines on medical time series datasets, especially with low label fractions (10% and 1%)

## Executive Summary
COMET introduces a hierarchical contrastive learning framework that leverages consistency across multiple data levels (observation, sample, trial, patient) to learn effective representations for medical time-series data. The method systematically captures data consistency through four contrastive blocks, each enforcing similarity within specific hierarchical levels. Evaluation on ECG and EEG datasets demonstrates consistent improvements over state-of-the-art baselines, particularly in scenarios with limited labeled data. The framework's flexibility allows adaptation to datasets with varying hierarchical structures by activating or deactivating specific contrastive blocks.

## Method Summary
COMET implements a four-level contrastive learning framework for medical time series. The method processes raw time series through an encoder to extract representations, then applies four independent contrastive blocks corresponding to observation, sample, trial, and patient levels. Each block uses timestamp masking augmentation and computes contrastive loss between positive pairs (augmented views of the same data unit) and negative pairs (views from different units at the same level). The losses are combined with equal weighting (λ = 0.25 for each level) and the resulting representation is pooled for downstream tasks. The framework supports both pre-training on unlabeled data and fine-tuning with limited labels.

## Key Results
- COMET consistently outperforms six state-of-the-art baselines across three medical time series datasets
- With 10% labeled data, COMET achieves 14% higher F1 score than baselines on EEG-based Alzheimer's detection
- With 1% labeled data, COMET achieves 13% higher F1 score than baselines on the same task
- The four-level design shows stable performance across datasets when all λ values are set equally

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: COMET learns more robust representations by leveraging hierarchical consistency across multiple data levels (observation, sample, trial, patient).
- **Mechanism**: Each contrastive block enforces similarity between positive pairs within a specific level while pushing negative pairs apart. The multi-level design captures both fine-grained and coarse-grained invariances in the data.
- **Core assumption**: Data consistency exists across all hierarchical levels in medical time series, and this consistency can be exploited via contrastive learning.
- **Evidence anchors**:
  - [abstract] "leverages data consistencies at multiple levels (observation, sample, trial, and patient) to learn effective representations"
  - [section 4.1] "we propose consistency across four data levels: observation, sample, trial, and patient"
  - [corpus] Weak correlation (0.4534 avg neighbor FMR); no direct evidence in neighbors supporting hierarchical contrastive learning for medical time series
- **Break condition**: If data consistency does not exist across levels (e.g., patient data comes from multiple distributions), trial-level and patient-level consistency assumptions break.

### Mechanism 2
- **Claim**: Hierarchical contrastive learning reduces reliance on labeled data by maximizing information utilization from unlabeled data.
- **Mechanism**: Self-supervised contrastive pre-training learns general representations from unlabeled data across all levels, making downstream tasks more sample-efficient.
- **Core assumption**: Representations learned through contrastive pre-training transfer effectively to downstream classification tasks.
- **Evidence anchors**:
  - [abstract] "consistently outperforms six state-of-the-art baselines, particularly with 10% and 1% labeled data fractions"
  - [section 5.2] "COMET outperforms SOTAs by 14% and 13% F1 score with label fractions of 10% and 1%, respectively, on EEG-based Alzheimer's detection"
  - [corpus] No direct evidence in neighbors about label efficiency; neighbors focus on anomaly detection or forecasting rather than representation learning
- **Break condition**: If contrastive pre-training fails to learn generalizable representations, transfer to downstream tasks will be poor regardless of label fraction.

### Mechanism 3
- **Claim**: The four-level design is more flexible and applicable than single-level methods.
- **Mechanism**: By allowing activation/deactivation of contrastive blocks through λ parameters, COMET adapts to datasets with varying availability of hierarchical information.
- **Core assumption**: Users can identify which hierarchical levels are present in their data and configure the model accordingly.
- **Evidence anchors**:
  - [section 4.1] "Although we present four levels here, our model can easily be adapted to accommodate specific datasets by adding or removing data levels"
  - [section 4.6] "Users can simply turn off specific data levels by setting λ of those levels to 0"
  - [corpus] No direct evidence in neighbors about model flexibility or configurable contrastive blocks
- **Break condition**: If users cannot correctly identify available hierarchical levels, inappropriate configuration will lead to suboptimal performance.

## Foundational Learning

- **Concept: Contrastive Learning**
  - Why needed here: Core technique for learning representations without labels by contrasting positive and negative pairs
  - Quick check question: In contrastive learning, what defines a positive pair versus a negative pair?

- **Concept: Hierarchical Data Structures**
  - Why needed here: Medical time series have inherent multi-level organization (patient → trial → sample → observation) that can be exploited
  - Quick check question: How does a trial differ from a sample in the medical time series context defined in this paper?

- **Concept: Self-Supervised Learning**
  - Why needed here: Enables learning from unlabeled data by creating supervisory signals from data structure itself
  - Quick check question: What is the key advantage of self-supervised learning for medical time series with scarce labels?

## Architecture Onboarding

- **Component map**: Raw sample → augmentation → encoder G → contrastive losses → representation pooling → downstream classifier
- **Critical path**: Raw sample → augmentation → encoder G → contrastive losses → representation pooling → downstream classifier
- **Design tradeoffs**:
  - Four contrastive blocks increase model capacity but risk overfitting
  - Equal λ weights (0.25 each) provide stability but may not be optimal for all datasets
  - Max-pooling before downstream tasks loses temporal information but simplifies architecture
- **Failure signatures**:
  - Poor downstream performance with high label fractions suggests overfitting during pre-training
  - Instability across random seeds indicates hyperparameter sensitivity
  - Performance degradation when removing contrastive blocks shows their necessity
- **First 3 experiments**:
  1. Run COMET with only observation-level and sample-level blocks (λ_trial=0, λ_patient=0) to verify basic contrastive learning works
  2. Test COMET with all four levels but vary λ weights systematically to find optimal configuration for each dataset
  3. Compare COMET pre-training with supervised baseline using 100% labels to quantify information utilization from unlabeled data

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Performance relies heavily on the existence of hierarchical structure in medical time series data
- Equal weighting of contrastive blocks (λ = 0.25) wasn't systematically optimized
- Evaluation focuses primarily on classification tasks with limited exploration of other downstream applications

## Confidence
- Hierarchical contrastive learning effectiveness: **High** (supported by consistent improvements across datasets and label fractions)
- Label efficiency gains: **High** (quantitative results show clear improvements with low label fractions)
- Four-level design flexibility: **Medium** (claimed but not thoroughly validated through ablation studies)

## Next Checks
1. Conduct systematic ablation studies varying λ weights across levels to identify optimal configurations for each dataset
2. Test COMET on medical time series without clear hierarchical structure to evaluate framework limitations
3. Evaluate representation quality through transfer learning to downstream tasks not seen during pre-training to assess generalization