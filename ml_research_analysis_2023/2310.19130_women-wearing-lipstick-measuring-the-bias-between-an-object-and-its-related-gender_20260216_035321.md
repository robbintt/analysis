---
ver: rpa2
title: 'Women Wearing Lipstick: Measuring the Bias Between an Object and Its Related
  Gender'
arxiv_id: '2310.19130'
source_url: https://arxiv.org/abs/2310.19130
tags:
- gender
- bias
- latexit
- score
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a visual semantic-based gender score to measure
  the degree of gender bias in image captioning systems. The approach investigates
  the impact of objects on gender bias by examining the semantic correlation between
  objects and gender at both word and sentence levels.
---

# Women Wearing Lipstick: Measuring the Bias Between an Object and Its Related Gender

## Quick Facts
- arXiv ID: 2310.19130
- Source URL: https://arxiv.org/abs/2310.19130
- Authors: 
- Reference count: 12
- Key outcome: Proposes a visual semantic-based gender score that measures gender bias by examining semantic correlation between objects and gender, showing that only gender-specific objects (e.g., women-lipstick) exhibit strong bias.

## Executive Summary
This paper introduces a novel approach to measure gender bias in image captioning systems by analyzing the semantic relationship between objects in images and gender terms. The method combines visual context from pre-trained object classifiers with semantic embeddings and a Belief Revision framework to estimate gender bias scores. Experiments demonstrate that the approach can detect bias amplification beyond simple object-gender co-occurrence counting, particularly for gender-specific objects like lipstick. The method is evaluated on widely used human-annotated datasets and shows improved balance in gender bias detection compared to existing methods.

## Method Summary
The approach extracts visual context from images using pre-trained object detection models (ResNet, CLIP, Inception-ResNet FRCNN), then computes semantic similarity between detected objects and gender terms using word and sentence embeddings (GloVe, GN-GloVe, Sentence-BERT). A Belief Revision formula transforms these similarity scores into calibrated probability estimates of gender bias, using GPT-2 to provide an initial gender hypothesis that gets updated with visual evidence. The method evaluates bias by comparing these scores against human-annotated captions from COCO and Flickr30K datasets, specifically using the Karpathy test split for benchmarking.

## Key Results
- Only gender-specific objects demonstrate strong gender bias correlations (e.g., lipstick strongly associated with women)
- The proposed Gender Score outperforms Object Gender Co-Occurrence approaches in balancing gender bias detection
- The method effectively measures bias relationships even when gender is not obvious or when classifier leakage occurs
- Visual semantic analysis provides more nuanced bias detection than simple frequency counting methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual context from pre-trained object classifiers can signal gender bias in captions
- Mechanism: Objects extracted using ResNet, CLIP, and Inception-ResNet FRCNN are analyzed for semantic similarity to gender terms using embeddings. Strong similarity indicates bias amplification.
- Core assumption: Certain objects have strong semantic association with specific gender (e.g., "lipstick" → "woman") detectable by standard embedding models.
- Evidence anchors: Results show only gender-specific objects have strong gender bias (e.g., women-lipstick). The approach proposes Gender Score via Belief Revision.
- Break condition: If objects are semantically neutral or classifiers fail to extract discriminative visual context, bias signal weakens.

### Mechanism 2
- Claim: Belief Revision transforms semantic similarity into calibrated probability of gender bias
- Mechanism: Initial hypothesis (caption gender via GPT-2) gets revised using visual evidence (object confidence) and similarity scores through a likelihood update formula.
- Core assumption: Initial bias estimate plus visual context provides better gender prediction than either alone.
- Evidence anchors: Gender Score works as plug-in for image captioning systems. Belief Revision converts cosine distance to probability measure.
- Break condition: If similarity scores are noisy or initial hypothesis highly incorrect, revision may not correct bias effectively.

### Mechanism 3
- Claim: Masked gender prediction reveals amplified bias not captured by simple object-gender counting
- Mechanism: <MASK> token replaces gender word in caption; model predicts gender based on object and caption context, exposing bias amplification.
- Core assumption: Fill-in-the-blank task surfaces bias more sensitively than frequency counts by testing model's internal gender associations.
- Evidence anchors: Gender Score measures bias relation between caption and related gender. Inspired by cloze probability completion task.
- Break condition: If model trained to avoid explicit gender terms, mask task may fail to reveal true bias.

## Foundational Learning

- Concept: Semantic similarity via embeddings (word2vec, GloVe, Sentence-BERT)
  - Why needed here: Core to measuring object-gender relatedness
  - Quick check question: How would you compute cosine similarity between "woman" and "lipstick" using GloVe embeddings?

- Concept: Probability revision (Bayesian updating, Belief Revision)
  - Why needed here: Converts similarity scores into calibrated gender likelihood estimates
  - Quick check question: What role does the "informativeness" parameter play in the revision formula?

- Concept: Visual grounding with object classifiers
  - Why needed here: Extracts visual context needed for bias estimation
  - Quick check question: Why filter out low-confidence objects (< 0.2) before computing bias?

## Architecture Onboarding

- Component map: Image + Caption -> Object Classifiers (ResNet, CLIP, FRCNN) -> Embedding Module (GloVe/Sentence-BERT) -> Revision Module (Belief Revision with GPT-2) -> Gender Bias Score

- Critical path:
  1. Extract objects from image
  2. Compute semantic similarity (object, gender)
  3. Estimate initial bias (GPT-2)
  4. Apply Belief Revision
  5. Output calibrated score

- Design tradeoffs:
  - Using pre-trained models speeds development but may inherit dataset biases
  - Sentence-level vs word-level embeddings trade context richness for computational efficiency
  - Confidence thresholding reduces noise but may miss subtle biases

- Failure signatures:
  - Score near zero for obviously gendered objects → similarity measure broken
  - High variance across similar images → visual context extraction unstable
  - Baseline models perform better → revision formula miscalibrated

- First 3 experiments:
  1. Compare Gender Score vs simple object-gender frequency counts on COCO Karpathy test split
  2. Test sensitivity of score to threshold changes in object confidence
  3. Validate gender predictions against human-annotated gender labels where available

## Open Questions the Paper Calls Out
- The paper mentions the need for future work to address gender as a spectrum, implying potential limitations in non-binary gender handling and cross-linguistic applicability.
- The effectiveness of the approach across different object detection models and varying numbers of top-k objects is not systematically explored.
- The paper assumes a binary gender system and focuses on single-person images, without addressing cases with multiple people, ambiguous gender, or non-binary individuals.

## Limitations
- The approach relies heavily on pre-trained models that may inherit biases from their training data, affecting reliability of measurements
- Comprehensive corpus validation of the Belief Revision formula's performance is lacking, making generalization assessment difficult
- Effectiveness of masked gender prediction mechanism is not thoroughly evaluated against simpler methods

## Confidence

- **High confidence**: The core observation that gender-specific objects (e.g., lipstick) show stronger gender bias associations, directly supported by experimental results
- **Medium confidence**: Effectiveness of Belief Revision formula in converting semantic similarity to calibrated gender likelihood estimates, limited by lack of extensive validation
- **Low confidence**: Superiority of masked gender prediction over simple object-gender counting methods for revealing bias amplification, insufficient quantitative comparison provided

## Next Checks

1. Conduct controlled experiments comparing Gender Score results against human-annotated gender labels to validate accuracy across diverse object-gender associations
2. Test robustness by varying object confidence thresholds and evaluating impact on bias detection sensitivity for subtle gender associations
3. Perform ablation studies removing Belief Revision component to assess whether simple semantic similarity scores provide comparable performance