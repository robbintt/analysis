---
ver: rpa2
title: Evaluating the Utility of Model Explanations for Model Development
arxiv_id: '2312.06032'
source_url: https://arxiv.org/abs/2312.06032
tags:
- image
- explanations
- images
- train
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether saliency-based explanations improve
  human decision-making on model selection and counterfactual simulation tasks. Using
  synthetic datasets with controlled mislabeling and visual perturbations, the authors
  compared human performance with and without saliency maps (SmoothGrad, GradCAM,
  and an oracle method).
---

# Evaluating the Utility of Model Explanations for Model Development

## Quick Facts
- arXiv ID: 2312.06032
- Source URL: https://arxiv.org/abs/2312.06032
- Reference count: 37
- Primary result: Saliency-based explanations (SmoothGrad, GradCAM) did not improve human accuracy on model selection or counterfactual simulation tasks

## Executive Summary
This study evaluates whether saliency-based explanations improve human decision-making on model selection and counterfactual simulation tasks. Using synthetic datasets with controlled mislabeling and visual perturbations, the authors compared human performance with and without saliency maps (SmoothGrad, GradCAM, and an oracle method). Despite expectations, explanations did not significantly improve task accuracy. Users with oracle explanations were better at describing model behavior, but this did not translate to better decisions. The findings suggest that saliency maps, even when designed to be highly informative, may not effectively convey model behavior to users and highlight the need for further research on explanation interpretability.

## Method Summary
The study used synthetic datasets created from Visual Genome images, with one baseline dataset (correctly labeled) and one mislabeled dataset (cars instead of buses/trains in positive class). Participants completed two tasks: model selection (choosing between models trained on baseline vs. mislabeled data) and counterfactual simulation (predicting model behavior on blurred images). Participants were randomly assigned to one of seven conditions: no explanation, or {SmoothGrad, GradCAM, oracle} Ã— {training time access only, training and evaluation time access}. A mixed-methods approach was used, combining quantitative accuracy measures with qualitative descriptions of user reasoning.

## Key Results
- Saliency-based explanations did not significantly improve accuracy on model selection or counterfactual simulation tasks
- Oracle explanations helped users more accurately describe model behavior but this did not translate to better decisions
- Users with explanations for all images focused on individual examples rather than learning general patterns
- Despite simple rules for generating oracle explanations, participants had widely varying interpretations

## Why This Works (Mechanism)

### Mechanism 1
Explanations did not improve model selection accuracy because users failed to extract actionable behavioral insights from saliency maps. Saliency maps highlight important pixels, but the highlighted regions did not map to distinct, interpretable model behaviors in the user's decision-making process.

### Mechanism 2
Explanations helped users describe model behavior more accurately but this did not translate to better decisions. Saliency maps provided visual cues that improved users' ability to articulate what the model focused on, but this awareness did not lead to improved task performance.

### Mechanism 3
Users with explanations for all images focused on individual examples rather than learning general patterns, hindering performance. When explanations were available for every image, users relied on reading individual explanations instead of identifying overarching patterns across the dataset.

## Foundational Learning

- Concept: Understanding of saliency-based explanation methods (SmoothGrad, GradCAM)
  - Why needed here: The study evaluates these specific explanation methods, so understanding how they work is crucial for interpreting the results.
  - Quick check question: What is the main difference between SmoothGrad and GradCAM in terms of how they generate saliency maps?

- Concept: Knowledge of model selection and counterfactual simulation tasks
  - Why needed here: The study evaluates these specific tasks, so understanding what they involve is essential for interpreting the results.
  - Quick check question: In the context of this study, what is the key difference between model selection and counterfactual simulation tasks?

- Concept: Familiarity with experimental design and statistical significance
  - Why needed here: The study involves human subjects and statistical analysis, so understanding these concepts is necessary for interpreting the results.
  - Quick check question: What is the null hypothesis being tested in this study, and what p-value threshold was used to determine statistical significance?

## Architecture Onboarding

- Component map: Model Selection Task (training phase -> evaluation phase -> accuracy assessment -> qualitative description) and Counterfactual Simulation Task (training phase -> evaluation phase -> accuracy assessment -> qualitative description)
- Critical path: For each task: (1) training phase with/without explanations, (2) evaluation phase with/without explanations, (3) quantitative assessment of accuracy, and (4) qualitative assessment of user descriptions
- Design tradeoffs: The study uses synthetic datasets with controlled mislabeling and visual perturbations to create clear behavioral differences between models. This allows for a more controlled evaluation but may limit generalizability to real-world scenarios.
- Failure signatures: If saliency maps fail to improve task accuracy, this could indicate that: (1) the maps do not effectively convey model behavior, (2) users are unable to extract actionable insights from the maps, or (3) the task design does not align with how users interact with explanations.
- First 3 experiments:
  1. Replicate the study with a larger sample size to detect smaller improvements in accuracy.
  2. Test additional explanation methods (e.g., LIME, Integrated Gradients) to see if other methods perform better.
  3. Evaluate the effect of providing users with explicit instructions on how to use explanations to identify patterns across examples.

## Open Questions the Paper Calls Out

### Open Question 1
What specific characteristics of saliency maps prevent them from effectively conveying model behavior to users, even when the explanations are synthetically designed to be highly informative? The authors note that even their oracle explanation, designed to be simple and highly indicative of the correct answer, did not significantly improve task performance.

### Open Question 2
How does the number of training examples provided with explanations affect users' ability to form accurate mental models of model behavior? The authors observed that users provided with explanations only for training examples versus all images had different usage patterns, and that more examples might be needed for stronger conclusions.

### Open Question 3
Under what conditions (if any) do saliency-based explanations improve human decision-making in model selection and counterfactual simulation tasks? The study found no significant improvement in task accuracy with any of the saliency maps tested, including the oracle explanation.

## Limitations
- Use of synthetic datasets with controlled mislabeling may limit generalizability to real-world scenarios
- Small sample size (n=48) may be insufficient to detect small improvements in accuracy
- Focus on saliency-based explanations may not generalize to other explanation methods like LIME or SHAP
- Assumption that users can extract actionable insights from saliency maps may not hold true for all user groups or task types

## Confidence

- **High Confidence:** The finding that explanations did not significantly improve task accuracy is well-supported by the experimental results and statistical analysis.
- **Medium Confidence:** The interpretation that users failed to extract actionable insights from saliency maps is plausible but could have alternative explanations (e.g., task design issues, explanation presentation).
- **Low Confidence:** The mechanism suggesting users focused on individual examples rather than patterns is speculative and not directly tested.

## Next Checks

1. Replicate the study with a larger sample size (n>100) to detect smaller improvements in accuracy and ensure statistical power.
2. Test additional explanation methods (LIME, Integrated Gradients) to determine if saliency-based methods are uniquely ineffective or if this is a broader issue with XAI explanations.
3. Conduct a follow-up study with explicit instructions on pattern recognition and compare performance to the original study to test whether users' focus on individual examples explains the lack of improvement.