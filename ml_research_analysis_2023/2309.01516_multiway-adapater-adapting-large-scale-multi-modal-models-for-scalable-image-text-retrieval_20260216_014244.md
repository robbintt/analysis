---
ver: rpa2
title: 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text
  retrieval'
arxiv_id: '2309.01516'
source_url: https://arxiv.org/abs/2309.01516
tags:
- multiway-adapter
- fine-tuning
- alignment
- learning
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MultiWay-Adapter introduces an efficient fine-tuning framework
  for large-scale multi-modal models, addressing the high computational cost and memory
  demands of traditional full-model fine-tuning. It uses a two-component design: a
  New Knowledge Extractor with a bottleneck structure to update task-specific features,
  and an Alignment Enhancer to maintain deep inter-modal alignment.'
---

# MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval

## Quick Facts
- arXiv ID: 2309.01516
- Source URL: https://arxiv.org/abs/2309.01516
- Reference count: 0
- MultiWay-Adapter achieves competitive or superior zero-shot performance to full fine-tuning while reducing trainable parameters to 1.25–2.68% and training time by up to 57%.

## Executive Summary
MultiWay-Adapter addresses the computational burden of fine-tuning large multi-modal models by introducing an efficient two-component adapter framework. It combines a New Knowledge Extractor with a bottleneck structure to capture task-specific features and an Alignment Enhancer to maintain deep inter-modal alignment. Experiments on image-text retrieval tasks with BEiT-3 models demonstrate that this approach achieves superior zero-shot performance compared to full fine-tuning while dramatically reducing GPU memory usage by up to 9GB and training time by up to 57%.

## Method Summary
MultiWay-Adapter is a parameter-efficient fine-tuning framework that adapts large-scale multi-modal models (LMMs) like BEiT-3 for image-text retrieval tasks. The method introduces two trainable components: a New Knowledge Extractor with a bottleneck structure to efficiently capture task-specific features, and an Alignment Enhancer to maintain and improve inter-modal alignment between vision and language representations. During fine-tuning, only these adapter modules are optimized while the pre-trained BEiT-3 weights remain frozen, preserving general knowledge while adapting to specific retrieval tasks.

## Key Results
- Achieves zero-shot performance on Flickr30K that matches or exceeds full model fine-tuning
- Reduces trainable parameters to just 1.25–2.68% of the original model
- Lowers GPU memory usage by up to 9GB and decreases training time by up to 57%
- Demonstrates effective scaling across BEiT-3 Base and Large model variants

## Why This Works (Mechanism)

### Mechanism 1
The bottleneck structure in the New Knowledge Extractor forces efficient task-specific feature extraction while preserving general knowledge from frozen layers. By replacing original FFNs with a down-projection followed by an up-projection layer (with middle dimension ˇd ≪ d), the model learns compressed task-specific representations. This bottleneck acts as a regularizer, preventing overfitting to the downstream dataset while maintaining transfer capability. The frozen BEiT-3 backbone retains sufficient general multi-modal knowledge that can be effectively adapted through small bottleneck modules.

### Mechanism 2
The Alignment Enhancer maintains and deepens inter-modal alignment between vision and language representations. The Alignment Enhancer module, placed atop the modality experts, uses a larger middle dimension to facilitate better feature fusion and alignment across modalities. This addresses the key weakness of prior adapter methods that focused only on unimodal knowledge extraction. Deep inter-modal alignment is critical for vision-language tasks and can be enhanced through dedicated alignment modules that bridge the modality gap more effectively than unimodal adapters alone.

### Mechanism 3
Freezing most parameters while tuning only adapter weights preserves zero-shot transfer capability. By keeping the pre-trained BEiT-3 weights frozen and only updating the adapter parameters, the model retains general knowledge learned from large-scale pre-training. This prevents catastrophic forgetting of the original model's capabilities. Large pre-trained models encode general knowledge that transfers well to downstream tasks when preserved through parameter freezing, allowing the model to maintain performance on unseen data while adapting to specific retrieval tasks.

## Foundational Learning

- Concept: Multi-modal contrastive learning and representation alignment
  - Why needed here: Understanding how vision and language representations are aligned in shared embedding spaces is crucial for designing effective alignment enhancers
  - Quick check question: What is the key difference between single-stream and dual-stream architectures in vision-language models?

- Concept: Parameter-efficient fine-tuning methods (LoRA, adapters)
  - Why needed here: MultiWay-Adapter builds on adapter methodology, so understanding how adapters work and their limitations is essential
  - Quick check question: How does the bottleneck structure in adapters differ from LoRA's low-rank decomposition approach?

- Concept: Bottleneck architectures and feature compression
  - Why needed here: The New Knowledge Extractor uses a bottleneck design, requiring understanding of how bottlenecks balance compression and information retention
  - Quick check question: What is the trade-off between bottleneck dimension size and parameter efficiency in adapter modules?

## Architecture Onboarding

- Component map: BEiT-3 backbone (frozen) → MultiWay-Transformer blocks → New Knowledge Extractor (trainable bottleneck) → Alignment Enhancer (trainable) → Output
- Critical path: Input → Shared self-attention → New Knowledge Extractor → Alignment Enhancer → Task-specific output
- Design tradeoffs: Parameter efficiency vs. model capacity, frozen weights vs. adaptability, alignment enhancement vs. task-specific optimization
- Failure signatures: Performance degradation on zero-shot tasks, overfitting to training data, poor cross-modal retrieval performance
- First 3 experiments:
  1. Ablation study removing the Alignment Enhancer to quantify its contribution
  2. Varying the bottleneck dimension (ˇd) to find optimal parameter-efficiency tradeoff
  3. Testing zero-shot performance on held-out datasets to verify knowledge preservation

## Open Questions the Paper Calls Out

### Open Question 1
How does MultiWay-Adapter's performance scale with model size beyond BEiT-3 Large? The paper states "we empirically demonstrate that our method scales effectively with model size" but only tests BEiT-3 Base and Large variants. The study only evaluated two model sizes, leaving uncertainty about performance on much larger LMMs. Experiments testing MultiWay-Adapter on significantly larger models (e.g., BEiT-3 XLarge or other billion-parameter LMMs) with performance metrics would resolve this.

### Open Question 2
What is the impact of different bottleneck dimension choices on modality alignment quality? The ablation study varies the New Knowledge Extractor's middle dimension but only measures retrieval performance, not alignment quality directly. While performance metrics improve with larger dimensions, there's no direct measurement of how well modalities are aligned at different bottleneck sizes. Quantitative metrics measuring inter-modal alignment quality (e.g., cross-modal attention patterns, alignment scores) across different bottleneck dimensions would resolve this.

### Open Question 3
How does MultiWay-Adapter compare to full fine-tuning when adapting to completely different domains? The experiments focus on image-text retrieval tasks within similar domains, but the paper claims generalizability without testing it. All experiments remain within vision-language tasks, leaving uncertainty about performance on non-retrieval tasks or very different domains. Experiments applying MultiWay-Adapter to diverse downstream tasks like VQA, image captioning, or completely different modalities (audio-text, etc.) with direct comparisons to full fine-tuning would resolve this.

## Limitations

- Architecture specificity limits generalizability beyond BEiT-3 models
- Dataset dependence on MSCOCO and Flickr30K may not represent diverse real-world scenarios
- Parameter efficiency vs. performance trade-off requires more thorough exploration across task types

## Confidence

- **High Confidence**: Claims about parameter efficiency (1.25-2.68% trainable parameters) and memory reduction (up to 9GB) are well-supported by ablation studies and direct measurements
- **Medium Confidence**: Zero-shot performance claims (competitive to full fine-tuning) are supported by Flickr30K results but lack broader validation across diverse datasets and tasks
- **Low Confidence**: The Alignment Enhancer's contribution is difficult to quantify precisely due to limited ablation studies and underspecified architecture details

## Next Checks

1. **Ablation Study**: Systematically remove the Alignment Enhancer component and measure performance degradation across multiple retrieval tasks to quantify its specific contribution

2. **Cross-Architecture Generalization**: Apply MultiWay-Adapter to alternative multi-modal backbones (e.g., CLIP, Flamingo) and evaluate whether parameter efficiency gains transfer across architectures

3. **Bottleneck Dimension Sensitivity**: Conduct a comprehensive study varying the bottleneck dimension (ˇd) across a wider range to identify optimal trade-offs between parameter efficiency and retrieval performance