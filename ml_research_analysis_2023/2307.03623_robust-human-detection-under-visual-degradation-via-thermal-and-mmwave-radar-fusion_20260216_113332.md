---
ver: rpa2
title: Robust Human Detection under Visual Degradation via Thermal and mmWave Radar
  Fusion
arxiv_id: '2307.03623'
source_url: https://arxiv.org/abs/2307.03623
tags:
- radar
- detection
- thermal
- feature
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces UTM, a multimodal human detection system that
  fuses thermal cameras and mmWave radars to address the limitations of RGB cameras
  in visually degraded environments. The proposed system consists of a Bayesian Feature
  Extractor (BFE) and an Uncertainty-Guided Fusion (UGF) module that systematically
  mitigate the challenges posed by low-contrast thermal images and noisy radar point
  clouds.
---

# Robust Human Detection under Visual Degradation via Thermal and mmWave Radar Fusion

## Quick Facts
- arXiv ID: 2307.03623
- Source URL: https://arxiv.org/abs/2307.03623
- Reference count: 0
- Primary result: Introduces UTM, a thermal-mmWave radar fusion system achieving 0.644 mAP50:95 and 0.672 mF150:95 on human detection.

## Executive Summary
This paper addresses the challenge of robust human detection in visually degraded environments by fusing thermal cameras and mmWave radars. The proposed UTM system introduces a Bayesian Feature Extractor (BFE) to model uncertainty in noisy sensor data and an Uncertainty-Guided Fusion (UGF) module that emphasizes uncertain features during training. The system outperforms state-of-the-art methods by 8.4% in mAP50:95 while maintaining real-time inference speeds on embedded devices.

## Method Summary
The UTM system fuses thermal images and mmWave radar point clouds through preprocessing (radar projection to depth images), Bayesian feature extraction with dropout-based uncertainty modeling, and uncertainty-guided fusion. The BFE uses dropout in final convolutional blocks to approximate Bayesian inference, producing mean and variance feature maps. The UGF module weights features by uncertainty before passing them to a multiscale detection network (MDN). The system is trained end-to-end with SGD for 100 epochs on synchronized thermal-radar datasets.

## Key Results
- Achieves mAP50:95 of 0.644 and mF150:95 of 0.672 on human detection
- Outperforms best competing method by 8.4% in mAP50:95
- Maintains real-time inference speeds on embedded devices
- Shows robust performance across diverse indoor environments

## Why This Works (Mechanism)

### Mechanism 1
The Bayesian Feature Extractor (BFE) reduces sensor uncertainty by modeling feature extraction weights as distributions rather than deterministic values, allowing the model to capture variance in sparse and noisy data. Dropout is applied only to the final two convolutional blocks in both feature extractors, approximating a Bayesian Neural Network. During training, the input data is forward propagated N times, producing a stack of N feature maps from which mean and variance maps are derived.

### Mechanism 2
The Uncertainty-Guided Fusion (UGF) module improves detection by emphasizing features with high uncertainty during training, forcing the model to learn from challenging regions. The variance map from the BFE is converted to weights via sigmoid and spatial softmax operations. The fused feature map is a weighted sum of the mean feature maps from both branches, with higher weights given to uncertain regions.

### Mechanism 3
Early fusion of thermal and radar data through preprocessing into a common image-like format enables more flexible and effective cross-modal feature learning. The mmWave radar point cloud is projected onto the 2D image plane using extrinsic and intrinsic parameters, creating a radar depth image with the same size and field of view as the thermal image. This allows for early fusion by stacking the two modalities.

## Foundational Learning

- **Variational Inference in Bayesian Neural Networks**: Understanding how dropout approximates posterior distributions is crucial for grasping how the BFE captures uncertainty.
  - Quick check: What is the key assumption behind using dropout to approximate a Bayesian Neural Network, and how does it relate to the feature extraction process?

- **Cross-Modal Sensor Fusion**: Recognizing the challenges and strategies for fusing heterogeneous sensor data (thermal images and radar point clouds) is essential for understanding the overall system design.
  - Quick check: What are the main differences between early fusion, feature fusion, and decision fusion, and why might early fusion be advantageous in this context?

- **Object Detection Metrics and Evaluation**: Interpreting the experimental results requires understanding metrics like mAP, mF1, and their relationship to IoU thresholds.
  - Quick check: How does the choice of IoU threshold affect the precision-recall trade-off in object detection, and why might higher IoU thresholds be more relevant for safety-critical applications?

## Architecture Onboarding

- **Component map**: Thermal camera → Preprocessor → BFE (main branch) / BFE (auxiliary branch) → UGF → MDN → Detections. Radar point cloud → Preprocessor → BFE (auxiliary branch) → UGF → MDN → Detections.
- **Critical path**: Preprocessor → BFE → UGF → MDN → NMS → Final detections.
- **Design tradeoffs**: Early fusion vs. late fusion (early fusion allows more flexible feature learning but requires careful preprocessing), dropout rate and layers in BFE (balancing uncertainty capture and representational capacity), UGF weighting scheme (emphasizing uncertainty vs. potentially amplifying noise).
- **Failure signatures**: Degraded performance on high IoU thresholds (indicates issues with precise localization), sensitivity to NMS IoU threshold changes (suggests instability in bounding box refinement), large performance gaps between modalities (points to preprocessing or fusion issues).
- **First 3 experiments**:
  1. Validate preprocessing: Compare detection performance using raw radar point clouds vs. preprocessed radar depth images.
  2. Ablate BFE: Replace BFE with deterministic FE and measure impact on mAP and mF1 scores.
  3. Test UGF variants: Compare UGF with simple feature addition and attention-based fusion to quantify the benefit of uncertainty-guided weighting.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed uncertainty-guided fusion (UGF) method perform when applied to other sensor combinations beyond thermal cameras and mmWave radars?
- **Basis in paper**: [explicit] The paper mentions that the UTM system could be generalized to other sensor combinations.
- **Why unresolved**: The current work only evaluates the UGF method on thermal cameras and mmWave radars, so its effectiveness on other sensor combinations is unknown.
- **What evidence would resolve it**: Testing the UGF method on different sensor combinations and comparing its performance to existing fusion methods.

### Open Question 2
- **Question**: What is the impact of the number of forward passes (N) in the Bayesian Feature Extractor (BFE) on the overall performance of the UTM system?
- **Basis in paper**: [explicit] The paper mentions that the BFE propagates the input data N times during training, but the impact of varying N is not explored.
- **Why unresolved**: The paper does not provide a detailed analysis of how changing the number of forward passes affects the system's performance.
- **What evidence would resolve it**: Conducting experiments with different values of N and evaluating the impact on detection accuracy, computational efficiency, and model size.

### Open Question 3
- **Question**: How does the UTM system perform in environments with more complex and dynamic lighting conditions, such as those with rapidly changing illumination or multiple light sources?
- **Basis in paper**: [inferred] The paper mentions that the system is designed to be robust under various illumination and visually degraded conditions, but it does not provide a detailed analysis of its performance in complex lighting scenarios.
- **Why unresolved**: The current evaluation focuses on a limited set of indoor environments, and the system's performance in more challenging lighting conditions is not explored.
- **What evidence would resolve it**: Collecting and evaluating the UTM system's performance on a dataset with diverse and dynamic lighting conditions, and comparing it to other state-of-the-art methods.

## Limitations

- Limited ablation analysis on the Bayesian feature extractor's impact, making it difficult to isolate the contribution of uncertainty modeling versus standard feature fusion
- Sparse comparison with non-thermal baselines, leaving questions about whether thermal-mmWave fusion is truly necessary versus simpler solutions
- Dataset collection details are insufficient for assessing potential biases or environmental limitations
- Performance gains on high IoU thresholds (mAP0.75) are modest, suggesting limitations in precise localization under challenging conditions

## Confidence

- **High confidence**: Core fusion architecture (BFE + UGF), real-time inference claims, dataset collection methodology
- **Medium confidence**: Generalization across environments, scalability to outdoor conditions, computational efficiency on embedded devices
- **Low confidence**: Comparative advantage over alternative fusion strategies, sensitivity to sensor calibration errors

## Next Checks

1. **Cross-dataset evaluation**: Test UTM on publicly available thermal-radar datasets (e.g., NUScenes, KAIST) to assess generalization beyond the curated collection environment.

2. **Sensor calibration sensitivity**: Systematically vary extrinsic parameter estimation errors (±5° rotation, ±0.5m translation) and measure degradation in detection performance to quantify robustness to calibration drift.

3. **Ablation of fusion strategy**: Implement and compare UTM variants using late fusion (separate feature extractors) and attention-based fusion against the current uncertainty-guided approach to isolate the specific benefits of the proposed method.