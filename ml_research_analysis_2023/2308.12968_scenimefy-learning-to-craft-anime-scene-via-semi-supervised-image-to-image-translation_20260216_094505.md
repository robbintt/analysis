---
ver: rpa2
title: 'Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image
  Translation'
arxiv_id: '2308.12968'
source_url: https://arxiv.org/abs/2308.12968
tags:
- anime
- loss
- data
- scene
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of converting real-world scene
  images into high-quality anime-style renderings. Existing methods struggle with
  preserving semantic content, achieving strong stylization, and capturing fine details.
---

# Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation

## Quick Facts
- **arXiv ID**: 2308.12968
- **Source URL**: https://arxiv.org/abs/2308.12968
- **Reference count**: 40
- **Primary result**: Achieves FID of 48.92 on high-resolution anime scene translation, outperforming state-of-the-art methods.

## Executive Summary
This paper tackles the challenge of converting real-world scene images into high-quality anime-style renderings. Existing methods struggle with preserving semantic content, achieving strong stylization, and capturing fine details. To address these issues, the authors propose Scenimefy, a semi-supervised image-to-image translation framework that leverages pseudo paired data generated through a novel semantic-constrained StyleGAN fine-tuning strategy. The method employs segmentation-guided data selection and introduces a patch-wise contrastive style loss to enhance stylization and detail preservation. Extensive experiments demonstrate that Scenimefy significantly outperforms state-of-the-art baselines, achieving a FID score of 48.92 and higher user preference scores across stylization, content preservation, and overall quality. The authors also contribute a high-resolution anime scene dataset to support future research.

## Method Summary
Scenimefy employs a semi-supervised image-to-image translation framework with two branches: supervised and unsupervised. The supervised branch uses pseudo paired data generated via semantic-constrained StyleGAN fine-tuning, guided by rich model priors like CLIP and VGG. Segmentation-guided data selection filters low-quality samples. A patch-wise contrastive style loss is introduced to enhance stylization and fine detail preservation. The unsupervised branch learns the true target domain distribution using semantic relation consistency and hard negative contrastive losses. The full framework balances faithfulness and fidelity of scene stylization, with the weight between branches decaying over time.

## Key Results
- Achieves FID score of 48.92 on high-resolution anime scene translation
- Outperforms state-of-the-art baselines in user preference studies for stylization, content preservation, and overall quality
- Demonstrates versatility by training on a different anime dataset (Hosoda Mamoru) with qualitative improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pseudo paired data generation via semantic-constrained StyleGAN fine-tuning bridges the domain gap and enables effective pixel-wise correspondence learning.
- **Mechanism**: StyleGAN is fine-tuned on the anime scene dataset while freezing early layers to preserve spatial structure. Rich pre-trained model priors (CLIP and VGG) guide the fine-tuning to capture complex scene features and maintain semantic consistency. Segmentation-guided data selection filters low-quality samples.
- **Core assumption**: Freezing early layers of StyleGAN maintains the spatial layout, while fine-tuning later layers adapts to the anime domain without overfitting.
- **Evidence anchors**:
  - [abstract]: "Our approach guides the learning with structure-consistent pseudo paired data... derived uniquely from a semantic-constrained StyleGAN leveraging rich model priors like CLIP."
  - [section 3.1]: "We leverage the desirable properties of StyleGAN... by fine-tuning it to generate coarse paired data between real and anime... We propose a novel semantic-constrained fine-tuning strategy that leverages rich pre-trained model priors, such as CLIP and VGG."
- **Break condition**: If the semantic consistency between real and anime domains is too large, the pseudo paired data may not provide sufficient guidance, leading to poor translation quality.

### Mechanism 2
- **Claim**: The patch-wise contrastive style loss enhances stylization and fine detail preservation by learning local anime textures.
- **Mechanism**: The patch-wise contrastive style loss brings corresponding patches from the translated image and pseudo ground truth closer in the embedded feature space, while pushing non-corresponding patches apart. This dense supervision helps the model learn robust local style similarity and focus on fine details.
- **Core assumption**: Local patches at the same location in the source and target images should have similar features, while patches at different locations should have dissimilar features.
- **Evidence anchors**:
  - [abstract]: "A patch-wise contrastive style loss is introduced to improve stylization and fine details."
  - [section 3.3.1]: "We introduce a novel patch-wise contrastive style loss for robust supervision... each patch within the translated image should be akin to the corresponding patch in the pseudo ground truth, rather than be identical to it."
- **Break condition**: If the patch-wise contrastive loss is too strong, it may lead to overfitting to the pseudo paired data and reduce generalization to the true target domain.

### Mechanism 3
- **Claim**: The semi-supervised framework with dual branches balances faithfulness and fidelity of scene stylization.
- **Mechanism**: The supervised branch uses pseudo paired data to establish pixel-wise correspondence and learn local anime textures, while the unsupervised branch learns the true target domain distribution using the original real and anime datasets. The total loss is a weighted combination of both branches, with the weight decaying over time.
- **Core assumption**: Combining supervised and unsupervised learning allows the model to benefit from the coarse guidance of pseudo paired data while still learning the true target distribution.
- **Evidence anchors**:
  - [abstract]: "Our semi-supervised framework seeks a desired trade-off between the faithfulness and fidelity of scene stylization."
  - [section 3.3]: "Our semi-supervised image-to-image translation framework... consists of two branches, supervised and unsupervised... The full framework of Scenimefy is thus semi-supervised, seeking a trade-off between faithfulness and fidelity of scene stylization."
- **Break condition**: If the weight decay for the supervised branch is too aggressive, the model may rely too heavily on the unsupervised branch and fail to learn the local anime textures effectively.

## Foundational Learning

- **Concept**: Generative Adversarial Networks (GANs)
  - **Why needed here**: GANs are the backbone of the StyleGAN fine-tuning and the semi-supervised image-to-image translation framework, enabling the generation of realistic anime-style images.
  - **Quick check question**: How does the adversarial loss in GANs help improve the quality of generated images?

- **Concept**: Contrastive Learning
  - **Why needed here**: Contrastive learning is used in the patch-wise contrastive style loss to bring corresponding patches closer and push non-corresponding patches apart in the embedded feature space, enhancing stylization and fine detail preservation.
  - **Quick check question**: How does the contrastive loss formulation in the patch-wise contrastive style loss differ from standard contrastive learning approaches?

- **Concept**: Semi-Supervised Learning
  - **Why needed here**: The semi-supervised framework combines supervised learning with pseudo paired data and unsupervised learning with the original datasets, allowing the model to benefit from both types of supervision and achieve better performance.
  - **Quick check question**: What are the advantages and disadvantages of using a semi-supervised approach compared to purely supervised or unsupervised approaches?

## Architecture Onboarding

- **Component map**: StyleGAN fine-tuning module -> Supervised branch (pseudo paired data + patch-wise contrastive style loss) -> Unsupervised branch (original datasets + semantic relation consistency loss) -> Combined output

- **Critical path**: The critical path for training Scenimefy involves first fine-tuning StyleGAN to generate pseudo paired data, then training the semi-supervised image-to-image translation framework using both the pseudo paired data and the original datasets.

- **Design tradeoffs**:
  - Freezing early layers of StyleGAN vs. training all layers: Freezing early layers preserves spatial structure but may limit the model's ability to adapt to the anime domain.
  - Using pseudo paired data vs. purely unsupervised learning: Pseudo paired data provides coarse guidance but may introduce biases or errors.
  - Patch-wise contrastive style loss vs. standard reconstruction loss: Patch-wise contrastive style loss enhances stylization and fine detail preservation but may lead to overfitting to the pseudo paired data.

- **Failure signatures**:
  - Poor stylization and lack of anime textures: May indicate issues with the StyleGAN fine-tuning or the patch-wise contrastive style loss.
  - Semantic inconsistencies or artifacts: May indicate issues with the pseudo paired data quality or the balance between the supervised and unsupervised branches.
  - Overfitting to the pseudo paired data: May indicate issues with the weight decay for the supervised branch or the patch-wise contrastive style loss being too strong.

- **First 3 experiments**:
  1. Ablation study of the StyleGAN fine-tuning module: Remove the semantic constraints or the segmentation-guided data selection and evaluate the impact on pseudo paired data quality and overall performance.
  2. Ablation study of the patch-wise contrastive style loss: Replace the patch-wise contrastive style loss with a standard reconstruction loss and evaluate the impact on stylization and fine detail preservation.
  3. Ablation study of the semi-supervised framework: Train the model using only the supervised branch or only the unsupervised branch and evaluate the impact on the balance between faithfulness and fidelity of scene stylization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Scenimefy's performance vary when using different anime style datasets, such as Hosoda Mamoru's style, compared to Shinkai's style?
- Basis in paper: [explicit] The paper mentions training the model on a different anime dataset, i.e., the Hosoda Mamoru dataset, to validate its versatility and showcases qualitative results.
- Why unresolved: The paper provides a qualitative comparison but lacks a quantitative evaluation of Scenimefy's performance across different anime styles.
- What evidence would resolve it: Quantitative metrics such as FID scores and user preference scores for each anime style dataset would provide a clear comparison of Scenimefy's performance across different styles.

### Open Question 2
- Question: What are the specific effects of the patch-wise contrastive style loss (StylePatchNCE) on the preservation of fine details and local textures in the generated anime scenes?
- Basis in paper: [explicit] The paper introduces the StylePatchNCE loss to improve stylization and fine details, and an ablation study is conducted to demonstrate its effectiveness.
- Why unresolved: While the ablation study shows the importance of StylePatchNCE, it does not quantify its specific impact on fine details and local textures compared to other loss functions.
- What evidence would resolve it: A detailed quantitative analysis comparing the quality of fine details and local textures in generated images with and without the StylePatchNCE loss would clarify its specific contributions.

### Open Question 3
- Question: How does the segmentation-guided data selection scheme impact the quality and diversity of the pseudo paired data, and what are the trade-offs involved?
- Basis in paper: [explicit] The paper describes the segmentation-guided data selection scheme to filter low-quality samples and enhance semantic abundance, and provides visualizations of retained and filtered images.
- Why unresolved: The paper does not provide a quantitative assessment of how this scheme affects the overall quality and diversity of the pseudo paired data, nor does it discuss potential trade-offs.
- What evidence would resolve it: Quantitative metrics evaluating the quality and diversity of the pseudo paired data before and after applying the segmentation-guided data selection scheme, along with an analysis of any trade-offs, would provide a comprehensive understanding of its impact.

## Limitations

- The reliance on pseudo paired data quality is fundamental yet only superficially validated - the segmentation-guided filtering criteria are not specified, and the 90kâ†’30k reduction ratio suggests significant quality issues.
- The patch-wise contrastive loss, while novel, lacks comparison to simpler alternatives like standard perceptual losses or style transfer objectives.
- The semantic consistency claims depend entirely on frozen CLIP/VGG features, which may not capture anime-specific aesthetics.

## Confidence

- **Method novelty**: Medium - The semi-supervised framework and patch-wise contrastive loss are novel, but the overall approach builds on existing techniques.
- **Empirical evaluation**: Medium - Strong results on curated datasets, but lacks head-to-head comparisons on established anime stylization benchmarks.
- **Generalization claims**: Low - Performance on different anime styles is only qualitatively evaluated, and true generalization to unseen domains is not tested.

## Next Checks

1. **Pseudo data quality audit**: Analyze the 30k filtered pseudo pairs for semantic consistency and style diversity. Report the exact filtering criteria and quantify the domain gap reduction achieved.

2. **Component ablation study**: Remove the patch-wise contrastive loss and StyleGAN fine-tuning constraints separately. Compare against a baseline with only supervised L1 reconstruction to isolate their contributions.

3. **Cross-dataset generalization**: Evaluate Scenimefy on unseen anime styles (different artists, lower resolution) and real scenes from domains not present in the training data to test true generalization beyond the curated datasets.