---
ver: rpa2
title: 'SQUARE: Automatic Question Answering Evaluation using Multiple Positive and
  Negative References'
arxiv_id: '2309.12250'
source_url: https://arxiv.org/abs/2309.12250
tags:
- square
- answer
- evaluation
- question
- references
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose SQuArE, a transformer encoder-based metric for evaluating
  QA systems. SQuArE leverages multiple positive and negative reference answers to
  capture diverse answer forms and refine evaluation accuracy.
---

# SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References

## Quick Facts
- arXiv ID: 2309.12250
- Source URL: https://arxiv.org/abs/2309.12250
- Authors: 
- Reference count: 40
- Primary result: SQuArE achieves highest correlation with human annotations across extractive and generative QA tasks using multiple reference answers

## Executive Summary
This paper introduces SQuArE, a transformer encoder-based metric for evaluating question answering (QA) systems that leverages multiple positive and negative reference answers. The method significantly outperforms previous metrics by capturing diverse answer forms and refining evaluation accuracy through contrastive learning. Extensive experiments on academic and industrial datasets demonstrate SQuArE's superiority in correlation with human judgments across both extractive and generative QA tasks.

## Method Summary
SQuArE uses a DeBERTaV3-Large transformer encoder fine-tuned on datasets containing question-answer pairs with multiple positive and negative reference answers. The model takes a formatted input string containing the question, target answer, and reference answers, then outputs a single score representing answer correctness. The training objective minimizes semantic distance between the target answer and positive references while maximizing distance from negative references, enabling the model to handle questions with diverse correct answers and leverage contrastive information.

## Key Results
- Achieves highest correlation with human annotations across extractive and generative QA tasks
- Significantly outperforms previous metrics on multiple academic and industrial datasets
- Using 5 reference answers consistently outperforms using 3 references across all datasets

## Why This Works (Mechanism)

### Mechanism 1
Using multiple reference answers improves QA evaluation accuracy by capturing diverse correct answers and combining information spread across multiple sources. The model learns to minimize semantic distance between the target answer and multiple positive references while maximizing distance from negative references. Core assumption: Reference answers cover the full semantic space needed to answer the question. Evidence: The paper demonstrates that SQuArE leverages multiple positive and negative reference answers to capture diverse answer forms and refine evaluation accuracy.

### Mechanism 2
Using negative references improves evaluation accuracy by providing contrastive information. The model learns to distinguish between correct and incorrect answers by comparing the target answer against both positive and negative references. Core assumption: Negative references are truly incorrect answers to the question. Evidence: The paper shows that an automatic QA evaluation system can use the information and semantics from an incorrect answer to help refine the accuracy of its prediction.

### Mechanism 3
The transformer encoder architecture enables semantic understanding beyond token-level matching. The model uses contextual embeddings from a transformer encoder to capture semantic similarity between questions, answers, and references, rather than relying on surface-level token matching. Core assumption: Transformer encoders can effectively capture semantic similarity for QA evaluation. Evidence: Recent works have shown that transformer LM encoder based similarity metrics transfer well for QA evaluation.

## Foundational Learning

- Concept: Semantic similarity in natural language
  - Why needed here: The model needs to understand when two answers are semantically equivalent even if they use different words
  - Quick check question: Can you explain why "The capital of France is Paris" and "Paris is the capital city of France" should be considered equivalent answers?

- Concept: Contrastive learning
  - Why needed here: The model needs to learn to distinguish between correct and incorrect answers by comparing them to both positive and negative references
  - Quick check question: How would you design a learning algorithm that pulls similar items together while pushing dissimilar items apart?

- Concept: Transformer encoder architecture
  - Why needed here: The model relies on contextual embeddings from transformers to capture semantic meaning
  - Quick check question: What is the key difference between contextual embeddings from transformers and static word embeddings like word2vec?

## Architecture Onboarding

- Component map: Input formatting -> DeBERTaV3-Large transformer encoding -> Score calculation -> Evaluation
- Critical path: Input formatting → Transformer encoding → Score calculation → Evaluation
- Design tradeoffs: Using multiple references improves accuracy but increases computational cost and requires more labeled data. The choice of reference answers can introduce bias.
- Failure signatures: Poor correlation with human annotations, degraded performance on questions with diverse correct answers, failure to distinguish between similar but incorrect answers.
- First 3 experiments:
  1. Test the model on a question with multiple correct answers using only positive references to verify it can handle answer diversity
  2. Test the model on a question with clearly incorrect negative references to verify it can use contrastive information
  3. Test the model on a question where the correct answer requires combining information from multiple references

## Open Questions the Paper Calls Out

### Open Question 1
How can automatic QA evaluation be extended to previously unseen questions without any reference answers? The paper states this is a challenging open problem in NLP QA. This remains unresolved because the paper focuses on using existing annotated datasets with reference answers and does not propose a solution for handling unseen questions. Evidence that would resolve it: A method that can evaluate the correctness of answers for questions without relying on any reference answers, perhaps by using knowledge graphs or other external knowledge sources.

### Open Question 2
How does the performance of SQuArE vary when using different numbers of reference answers per question? The paper mentions ablation studies showing SQuArE using 5 references outperforms SQuArE using 3 references, but suggests further exploration is needed. This remains unresolved because the paper only tests 3 and 5 references, leaving open the question of optimal number and the performance curve as number of references increases. Evidence that would resolve it: Experiments testing SQuArE with a wider range of reference numbers (e.g., 1, 2, 4, 6, 10) and plotting performance vs. number of references.

### Open Question 3
How well does SQuArE generalize to languages other than English? The paper only evaluates SQuArE on English datasets and mentions it as a limitation, suggesting potential for cross-lingual application. This remains unresolved because the paper does not test SQuArE on non-English datasets, leaving its performance in other languages unknown. Evidence that would resolve it: Experiments applying SQuArE to QA datasets in various languages and comparing its performance to English results.

## Limitations

- The paper's claims about superior performance rely heavily on the quality and diversity of reference answers, which is not fully addressed
- The selection criteria for positive and negative references across datasets remain underspecified, creating potential reproducibility issues
- The reliance on DeBERTaV3-Large as a backbone raises questions about whether similar performance could be achieved with smaller, more efficient models

## Confidence

**High Confidence**: The mechanism of using multiple positive references to capture diverse answer forms is well-supported by the literature on QA evaluation and the experimental results showing improved correlation with human judgments.

**Medium Confidence**: The effectiveness of contrastive learning with negative references is supported by the results but the selection process for negative references is not detailed enough to fully assess whether they truly represent incorrect answers.

**Medium Confidence**: The claim of achieving highest correlation with human annotations is supported by extensive experiments but could benefit from more detailed analysis of when and why the model succeeds or fails.

## Next Checks

1. **Reference Quality Analysis**: Conduct a systematic analysis of reference answer diversity and coverage across different datasets to verify that the positive references truly capture the full semantic space of correct answers.

2. **Negative Reference Validation**: Perform a detailed examination of negative reference selection criteria and their actual correctness to ensure the contrastive learning mechanism is functioning as intended.

3. **Ablation Study on Model Architecture**: Test whether the performance gains can be replicated with smaller transformer models or alternative architectures to isolate the contribution of the DeBERTaV3-Large choice from the overall methodology.