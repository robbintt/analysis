---
ver: rpa2
title: Poisoning Retrieval Corpora by Injecting Adversarial Passages
arxiv_id: '2310.19156'
source_url: https://arxiv.org/abs/2310.19156
tags:
- passages
- adversarial
- attack
- retrieval
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a corpus poisoning attack targeting dense retrieval
  models. The attack generates a small set of adversarial passages by iteratively
  perturbing tokens to maximize similarity with a training set of queries.
---

# Poisoning Retrieval Corpora by Injecting Adversarial Passages

## Quick Facts
- **arXiv ID:** 2310.19156
- **Source URL:** https://arxiv.org/abs/2310.19156
- **Authors:** [List of authors]
- **Reference count:** 17
- **Primary result:** 50 adversarial passages optimized on Natural Questions mislead >94% of questions posed in financial documents or online forums.

## Executive Summary
This paper presents a novel corpus poisoning attack targeting dense retrieval models by generating adversarial passages that maximize similarity to training queries. The attack uses gradient-based token perturbation to create passages that fool retrieval models into returning them for a broad set of queries, including out-of-domain ones. The method is effective across multiple state-of-the-art dense retrievers and even unsupervised models, raising significant security concerns for real-world deployment.

## Method Summary
The attack generates adversarial passages by iteratively perturbing tokens to maximize similarity with a training set of queries. It uses a gradient-based approach inspired by HotFlip, replacing tokens based on their impact on similarity scores. Queries are clustered using k-means, and one adversarial passage is generated per cluster to efficiently target different query subspaces. These adversarial passages are then inserted into the retrieval corpus, causing retrieval models to return them for both in-domain and out-of-domain queries.

## Key Results
- 50 adversarial passages optimized on Natural Questions mislead >94% of questions from financial documents or online forums.
- The attack is effective across multiple dense retrievers including single-vector (DPR, ANCE, Contriever) and multi-vector (ColBERT) architectures.
- Even unsupervised models like Contriever are highly vulnerable, with 10 adversarial passages fooling >90% of queries.
- Defenses like likelihood scoring and embedding norm clipping can mitigate but not fully prevent the attack.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The attack works by maximizing inner product similarity between query and passage embeddings through discrete token perturbation.
- **Mechanism:** The attack iteratively replaces tokens using gradient-based approximation (HotFlip) to maximize average similarity over a query set.
- **Core assumption:** The gradient of similarity with respect to token embeddings is a reliable signal for identifying beneficial token replacements.
- **Evidence anchors:** [abstract] "iteratively perturbs discrete tokens to maximize similarity with a provided set of training queries"; [section] "We use a gradient-based approach to solve the optimization problem, inspired by the HotFlip method (Ebrahimi et al., 2018)"
- **Break condition:** If gradients become unreliable due to training with adversarial examples, the optimization signal may fail.

### Mechanism 2
- **Claim:** Clustering queries enables parallel generation of multiple adversarial passages that target different query subspaces.
- **Mechanism:** Queries are embedded and clustered via k-means, with one adversarial passage generated per cluster by maximizing similarity to all queries in that cluster.
- **Core assumption:** Queries within the same cluster are semantically similar enough that a single passage can fool them all.
- **Evidence anchors:** [section] "we use the k-means clustering algorithm to cluster queries based on their embeddings Eq(qi)"; [section] "This allows us to generate multiple adversarial passages in parallel, each targeting a group of similar queries"
- **Break condition:** If query clusters are too diverse or overlapping, a single passage may not effectively target all queries in the cluster.

### Mechanism 3
- **Claim:** High ℓ2-norm passage embeddings amplify attack effectiveness by increasing inner product similarity.
- **Mechanism:** Adversarial passages have very high norm embeddings, directly increasing similarity scores since similarity is computed as inner product.
- **Core assumption:** Dense retrievers use inner product as similarity, so increasing embedding norm increases similarity.
- **Evidence anchors:** [section] "the interaction between queries and passages is strictly limited to the inner product, such that sim(p, q) ∝ ∥Ep(p)∥^2 cos θ"; [appendix] "Figure 4 shows that this is the case for the adversarial passages against Contriever"
- **Break condition:** If cosine similarity is used instead of inner product, or if embedding norms are clipped during inference, this amplification effect is nullified.

## Foundational Learning

- **Concept:** Gradient-based discrete token substitution (HotFlip)
  - **Why needed here:** Enables efficient search over token space to find replacements that improve similarity, avoiding full combinatorial search.
  - **Quick check question:** What is the computational complexity of evaluating all possible token replacements versus using the HotFlip gradient approximation?

- **Concept:** Contrastive learning objective for dense retrieval
  - **Why needed here:** Understanding how query and passage encoders are trained helps explain why the attack can generalize across domains (similar embedding geometry).
  - **Quick check question:** How does the choice between supervised vs. unsupervised contrastive learning affect vulnerability to corpus poisoning?

- **Concept:** k-means clustering for query grouping
  - **Why needed here:** Enables generation of multiple adversarial passages that each target a subset of queries, improving attack efficiency and coverage.
  - **Quick check question:** What is the effect of choosing a different number of clusters (k) on the attack success rate and computational cost?

## Architecture Onboarding

- **Component map:** Query encoder (Eq) -> Passage encoder (Ep) -> Similarity function (inner product) -> Corpus (collection of passages) -> Attacker (generates adversarial passages via gradient-based token perturbation)

- **Critical path:** Cluster queries → Generate adversarial passages → Insert into corpus → Evaluate attack success

- **Design tradeoffs:**
  - Token replacement steps vs. attack effectiveness
  - Number of adversarial passages vs. attack coverage
  - Passage length vs. vulnerability of different retriever architectures

- **Failure signatures:**
  - Low attack success rate → Poor gradient signal or ineffective clustering
  - High false positive rate → Embedding norm clipping or likelihood filtering
  - No transferability → Query embedding space mismatch between training and test sets

- **First 3 experiments:**
  1. Vary the number of adversarial passages (1, 10, 50) on a single retriever and measure top-k attack success rate.
  2. Test transferability by attacking one retriever and evaluating on another with same corpus.
  3. Apply embedding norm clipping as defense and measure reduction in attack success rate.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can attackers generate adversarial passages that appear more natural while still fooling dense retrievers?
- **Basis in paper:** The authors note that their adversarial passages contain unnatural sequences detectable by language models, and attackers might develop stronger attacks incorporating natural language priors.
- **Why unresolved:** The paper only demonstrates attacks with unnatural text and basic defenses using likelihood scoring. It does not explore whether more sophisticated generation methods could produce fluent adversarial text.
- **What evidence would resolve it:** Experiments comparing retrieval performance on attacks using different text generation approaches (e.g., GPT-based vs. HotFlip) while controlling for other factors like passage length and number of tokens.

### Open Question 2
- **Question:** How effective are current defenses against corpus poisoning attacks in real-world scenarios?
- **Basis in paper:** The authors propose two simple defenses (likelihood scoring and embedding norm clipping) but acknowledge they may not be fully robust against more sophisticated attacks.
- **Why unresolved:** The paper only evaluates basic defenses and does not test them against stronger attack variants or in realistic deployment scenarios with noisy data.
- **What evidence would resolve it:** Comprehensive benchmarking of various defense strategies against a spectrum of attack methods using real-world retrieval corpora and query distributions.

### Open Question 3
- **Question:** Can corpus poisoning attacks be adapted to target specific subsets of queries or user groups?
- **Basis in paper:** The authors mention the possibility of targeted attacks with misinformation in Section 4.5, showing that constrained adversarial passages can still achieve reasonable success rates.
- **Why unresolved:** The paper only demonstrates a basic form of targeted attack with a fixed prefix. It does not explore whether more sophisticated targeting strategies could be developed to influence specific user groups or demographic segments.
- **What evidence would resolve it:** Experiments measuring attack success rates when adversarial passages are optimized for specific query clusters or demographic features, comparing different targeting strategies.

## Limitations

- The attack requires access to the query encoder to generate adversarial passages, which may not be available in all real-world scenarios.
- The proposed defenses (likelihood scoring and embedding norm clipping) show some mitigation capability but their robustness against adaptive attackers is questionable.
- While the attack shows transferability across architectures, the extent and reliability of this transferability are not fully characterized.

## Confidence

**High Confidence Claims:**
- The gradient-based optimization approach using HotFlip effectively generates adversarial passages that increase similarity with training queries.
- Dense retrievers are vulnerable to corpus poisoning attacks that can manipulate retrieval results.
- High ℓ2-norm embeddings amplify attack effectiveness through increased inner product similarity.

**Medium Confidence Claims:**
- The attack generalizes across multiple state-of-the-art dense retriever architectures.
- Defenses like likelihood scoring and embedding norm clipping can partially mitigate the attack.
- Query clustering enables efficient generation of multiple adversarial passages targeting different query subspaces.

**Low Confidence Claims:**
- The exact effectiveness of defenses against adaptive attackers.
- The practical scalability of the attack in real-world scenarios with large corpora.
- The reliability of attack transferability across completely different domains and retriever training objectives.

## Next Checks

1. **Adaptive Attack Evaluation:** Implement an adaptive attacker that knows the specific defenses (likelihood scoring and embedding norm clipping) are in place, and evaluate whether the attack can bypass these defenses by adjusting the optimization objective or using alternative token replacement strategies.

2. **Large-Scale Corpus Testing:** Scale the attack to much larger retrieval corpora (e.g., 10M+ passages) and evaluate how the number of adversarial passages needed for successful attacks scales with corpus size. This would test the practical limitations of the attack approach.

3. **Cross-Architecture Transferability Study:** Systematically evaluate the attack's transferability across a wider range of retriever architectures with different training objectives (supervised vs. unsupervised) and embedding dimensions. Measure how well adversarial passages generated against one architecture perform against others, and identify patterns in successful transfers.