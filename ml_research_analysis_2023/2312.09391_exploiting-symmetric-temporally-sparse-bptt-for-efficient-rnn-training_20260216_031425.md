---
ver: rpa2
title: Exploiting Symmetric Temporally Sparse BPTT for Efficient RNN Training
arxiv_id: '2312.09391'
source_url: https://arxiv.org/abs/2312.09391
tags:
- delta
- training
- sparsity
- rnns
- lstm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a training algorithm for Delta RNNs that exploits
  temporal sparsity in the backward propagation phase to reduce computational requirements
  for training on the edge. By introducing a delta threshold on neuron activation
  changes, the update of slow-changing activations can be skipped, thus saving a large
  number of computes while achieving comparable accuracy.
---

# Exploiting Symmetric Temporally Sparse BPTT for Efficient RNN Training

## Quick Facts
- arXiv ID: 2312.09391
- Source URL: https://arxiv.org/abs/2312.09391
- Reference count: 27
- Key outcome: Reduces matrix operations by ~80% for training a 56k parameter Delta LSTM on Fluent Speech Commands dataset with negligible accuracy loss

## Executive Summary
This paper presents a training algorithm for Delta RNNs that exploits temporal sparsity in the backward propagation phase to reduce computational requirements for training on edge devices. By introducing a delta threshold on neuron activation changes, the algorithm skips updates of slow-changing activations, saving significant computation while maintaining accuracy. The approach leverages symmetric computation graphs between forward and backward propagation to skip gradient computations for inactivated neurons, enabling 2-10X speedup in matrix computations for activation sparsity ranges of 50%-90%.

## Method Summary
The paper introduces Delta RNNs that exploit temporal sparsity by skipping updates for neurons whose activation changes fall below a threshold. During forward propagation, neurons with delta changes below threshold Θ are marked as inactivated using a binary mask. In backward propagation, gradients for these inactivated neurons are zeroed out using the same mask, eliminating unnecessary matrix-vector multiplications. The method is implemented with a custom hardware accelerator designed for training Delta RNNs, which can skip DRAM weight column accesses for inactivated neurons when using batch size 1, reducing memory energy consumption by ~80%.

## Key Results
- Achieves ~80% reduction in matrix operations for training a 56k parameter Delta LSTM on Fluent Speech Commands dataset
- Demonstrates 2-10X speedup in matrix computations for activation sparsity ranges of 50%-90%
- Shows negligible accuracy loss compared to standard dense RNN training
- Enables online incremental learning on edge devices with limited computing resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse backward propagation in Delta RNNs can skip gradient computations for inactivated neurons without accuracy loss.
- Mechanism: During forward propagation, neurons with activation changes below threshold Θ are marked as inactivated using a binary mask. In backward propagation, gradients for these neurons are zeroed out using the same mask, eliminating unnecessary matrix-vector multiplications.
- Core assumption: The mask computed in forward propagation remains valid for backward propagation, and skipping gradients for inactivated neurons does not affect weight updates.
- Evidence anchors: The symmetric computation graphs of forward and backward propagation during training allow gradient computation of inactivated neurons to be skipped. The gradients of inactivated neurons are zeroed out during backward propagation due to the non-differentiability of the delta threshold function.

### Mechanism 2
- Claim: Temporal sparsity from forward propagation can be directly exploited in all three matrix-vector multiplications during backward propagation.
- Mechanism: The same delta mask computed during forward propagation is reused in backward propagation for weight gradient calculations, error propagation, and pre-activation gradient computation.
- Core assumption: The sparsity pattern created by delta thresholding is consistent between forward and backward propagation phases.
- Evidence anchors: Due to symmetric computation graphs, the reduction factor in computation and memory access equals (1 - oc), matching the sparsity of ∆xt.

### Mechanism 3
- Claim: Batch size 1 training in Delta RNNs can skip DRAM weight column accesses for inactivated neurons, reducing memory energy consumption.
- Mechanism: When training with batch size 1, the hardware accelerator only fetches weight columns corresponding to activated neurons using the delta mask, avoiding DRAM accesses for entire inactivated columns.
- Core assumption: The hardware architecture can efficiently implement sparse weight access patterns without significant overhead.
- Evidence anchors: The accelerator can skip both DRAM access of weight columns of inactivated neurons and computation for them, saving ~80% memory access in batch-1 training.

## Foundational Learning

- Concept: Backpropagation Through Time (BPTT) for RNNs
  - Why needed here: Understanding BPTT is essential to grasp how gradients are computed across time steps and how the delta mask can be applied to skip computations.
  - Quick check question: In standard BPTT, how are gradients for weights computed across multiple time steps?

- Concept: Sparsity exploitation in neural networks
  - Why needed here: The paper builds on existing sparsity techniques but applies them specifically to temporal dimensions in RNNs rather than spatial dimensions in CNNs.
  - Quick check question: What are the main differences between spatial sparsity (CNNs) and temporal sparsity (RNNs) in terms of implementation challenges?

- Concept: Delta networks and temporal stability
  - Why needed here: Delta networks exploit the temporal stability of RNN states to skip updates, which is the foundation for the entire approach.
  - Quick check question: Why do RNN internal states exhibit temporal stability, and how does this property enable delta-based sparsity?

## Architecture Onboarding

- Component map: Delta RNN layer -> Mask generation unit -> Sparse matrix-vector multiplier -> Memory controller -> Training loop

- Critical path:
  1. Forward pass with delta thresholding and mask generation
  2. Backward pass reusing masks for sparse gradient computation
  3. Weight update using sparse gradients
  4. Memory access optimization for batch size 1 training

- Design tradeoffs:
  - Threshold selection: Higher thresholds increase sparsity but may reduce accuracy
  - Mask storage: Requires additional memory but enables significant computation savings
  - Hardware specialization: Custom accelerators provide better speedup than general-purpose hardware

- Failure signatures:
  - Accuracy degradation: Indicates threshold too high or sparsity too aggressive
  - Unexpected memory access patterns: Suggests mask reuse not working correctly
  - Training instability: May indicate gradient skipping affecting weight convergence

- First 3 experiments:
  1. Implement basic Delta RNN with varying thresholds on a small speech dataset to verify sparsity-accuracy tradeoff
  2. Add mask generation and verify identical sparsity in forward and backward passes
  3. Implement sparse matrix-vector multiplication and measure speedup compared to dense implementation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical maximum speedup achievable for Delta RNN training, and how does it depend on the delta threshold value?
- Basis in paper: The paper shows that the speedup factor is inversely proportional to the occupancy of the delta vectors, which depends on the delta threshold. It mentions a speedup factor of 2-10X for activation sparsities of 50%-90%.
- Why unresolved: The paper does not provide a comprehensive analysis of the relationship between the delta threshold, activation sparsity, and speedup factor. It only presents a limited set of experimental results.
- What evidence would resolve it: A systematic study varying the delta threshold and measuring the resulting activation sparsity and speedup factor for different RNN architectures and datasets would provide a clearer understanding of the trade-off between accuracy and efficiency.

### Open Question 2
- Question: How does the proposed Delta RNN training method compare to other sparse training techniques in terms of accuracy, sparsity, and computational efficiency?
- Basis in paper: The paper mentions other methods for exploiting sparsity in RNNs, such as conditional computation and Skip RNN, but does not provide a direct comparison with the proposed Delta RNN training method.
- Why unresolved: The paper focuses on the specific advantages of the Delta RNN training method and does not evaluate its performance relative to other sparse training techniques.
- What evidence would resolve it: A comprehensive comparison of the proposed Delta RNN training method with other sparse training techniques on the same tasks and datasets would provide insights into its relative strengths and weaknesses.

### Open Question 3
- Question: What are the implications of the Delta RNN training method for online incremental learning on resource-constrained edge devices?
- Basis in paper: The paper mentions that the proposed Delta RNN training method is particularly useful for online incremental learning on edge devices with limited computing resources. It presents experimental results on incremental keyword learning.
- Why unresolved: The paper does not explore the full potential of the Delta RNN training method for online incremental learning, such as its impact on model adaptation, privacy, and energy efficiency.
- What evidence would resolve it: A detailed analysis of the Delta RNN training method's performance in online incremental learning scenarios, including its ability to adapt to new data, preserve privacy, and minimize energy consumption, would provide a better understanding of its practical implications.

## Limitations
- Results rely heavily on simulation rather than actual hardware measurements, which may not capture real-world energy and performance characteristics accurately
- Theoretical framework assumes perfect symmetry between forward and backward propagation, which may not hold for complex RNN architectures with gating mechanisms
- Delta threshold parameter requires careful tuning, and the ~80% reduction claim may not generalize to all RNN applications

## Confidence

**Major Limitations:**
- The paper relies heavily on simulation results rather than actual hardware measurements, which may not capture real-world energy and performance characteristics accurately. The 2-10X speedup claims are based on logic simulations in Vivado that don't account for practical implementation overheads.

- The theoretical framework assumes perfect symmetry between forward and backward propagation, but real-world RNN architectures with complex gating mechanisms (LSTM/GRU) may introduce asymmetries that reduce the effectiveness of mask reuse.

- The delta threshold parameter requires careful tuning, and the paper doesn't provide systematic guidelines for threshold selection across different tasks or datasets. The ~80% reduction claim may not generalize to all RNN applications.

**Confidence Levels:**
- High confidence: The basic mechanism of exploiting temporal sparsity in Delta RNNs is sound and mathematically well-founded. The theoretical analysis of how sparsity patterns transfer between forward and backward passes is rigorous.
- Medium confidence: The claimed computational reductions (80% matrix operations) are plausible given the theoretical framework, but actual hardware measurements would strengthen these claims.
- Low confidence: The specific speedup numbers (2-10X) and their dependency on activation sparsity ranges are difficult to verify without access to the actual hardware implementation.

## Next Checks
1. Implement the Delta RNN training on a different dataset (beyond Fluent Speech Commands) to verify generalization of the sparsity-accuracy tradeoff across tasks with varying temporal characteristics.

2. Conduct ablation studies by progressively removing components of the proposed method (e.g., test without mask reuse, without delta thresholding) to quantify the contribution of each optimization.

3. Build a small-scale FPGA prototype of the proposed hardware accelerator to measure actual performance and energy consumption, comparing against the simulated results to identify implementation gaps.