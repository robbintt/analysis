---
ver: rpa2
title: 'MMBench: Is Your Multi-modal Model an All-around Player?'
arxiv_id: '2307.06281'
source_url: https://arxiv.org/abs/2307.06281
tags:
- evaluation
- image
- mmbench
- reasoning
- abilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMBench, a multi-modality benchmark designed
  to robustly evaluate different abilities of large vision-language models (LVLMs).
  The paper argues that existing benchmarks either provide only task-specific evaluations
  or suffer from scalability and bias issues in human-involved subjective evaluations.
---

# MMBench: Is Your Multi-modal Model an All-around Player?

## Quick Facts
- arXiv ID: 2307.06281
- Source URL: https://arxiv.org/abs/2307.06281
- Reference count: 40
- Key outcome: Introduces MMBench, a multi-modal benchmark with 2,974 questions across 20 fine-grained abilities to evaluate vision-language models' comprehensive capabilities.

## Executive Summary
This paper presents MMBench, a comprehensive multi-modal benchmark designed to evaluate the diverse capabilities of large vision-language models (LVLMs). The benchmark addresses limitations in existing evaluations by providing a hierarchical taxonomy of 20 fine-grained abilities and employing novel strategies like CircularEval and ChatGPT-based choice extraction to handle models with limited instruction-following capabilities. The authors evaluate 14 well-known VLMs, revealing significant performance gaps in cross-instance understanding and logical reasoning, while demonstrating the benchmark's effectiveness in providing detailed diagnostic feedback.

## Method Summary
MMBench is a multi-modal benchmark consisting of 2,974 multiple-choice questions organized into a hierarchical taxonomy covering 20 fine-grained abilities. The evaluation employs a CircularEval strategy that rotates answer choices to ensure robustness, and uses ChatGPT to extract choice labels from free-form model predictions. The benchmark is evaluated in a zero-shot setting, requiring VLMs to output discrete answer labels (A, B, C, or D) without fine-tuning. ChatGPT (gpt-3.5-turbo-0613) is used for choice extraction when exact matching fails, enabling fair comparison across models with varying instruction-following abilities.

## Key Results
- VLMs show strong performance in basic perception tasks but struggle significantly with cross-instance understanding and logical reasoning.
- CircularEval improves evaluation robustness by requiring consistent predictions across rotated answer choices.
- ChatGPT-based choice extraction successfully aligns free-form predictions with correct answer labels in 87% of ambiguous cases.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CircularEval reduces false positives from model's tendency to over-predict a single answer choice.
- **Mechanism:** By evaluating each question multiple times with circularly shifted answer choices, the model must be consistent across all rotations to pass, mitigating bias toward any one option.
- **Core assumption:** The model's prediction is stable enough that if it is correct, it will be correct in all rotated forms; if incorrect, at least one rotation will fail.
- **Evidence anchors:**
  - [abstract] "CircularEval strategy and incorporates large language models to convert free-form predictions into pre-defined choices, which helps to yield accurate evaluation results for models with limited instruction-following capabilities."
  - [section] "CircularEval achieves a good trade-off between the robustness and the evaluation cost."
- **Break condition:** If the model's outputs are highly unstable or inconsistent across similar prompts, CircularEval could falsely penalize correct but unstable models.

### Mechanism 2
- **Claim:** ChatGPT-based choice extraction enables fair comparison of models with varying instruction-following abilities.
- **Mechanism:** For predictions not directly matching a label, ChatGPT is prompted to align the free-form output with one of the provided answer choices, standardizing evaluation across models.
- **Core assumption:** ChatGPT can reliably interpret and map free-form model outputs to the correct choice label.
- **Evidence anchors:**
  - [abstract] "incorporates large language models to convert free-form predictions into pre-defined choices"
  - [section] "ChatGPT can perfectly match human evaluations for 87% of ambiguous cases, demonstrating its good alignment and robustness as an evaluator."
- **Break condition:** If ChatGPT misaligns predictions due to ambiguous phrasing or model hallucinations, evaluation fairness degrades.

### Mechanism 3
- **Claim:** Hierarchical taxonomy of abilities allows fine-grained diagnostic feedback beyond single-task benchmarks.
- **Mechanism:** Organizing questions into three levels (L-1: Perception/Reasoning, L-2: coarse abilities, L-3: fine-grained abilities) lets evaluators pinpoint specific model weaknesses (e.g., cross-instance perception vs single-instance).
- **Core assumption:** Ability-specific data samples are representative and balanced across the taxonomy.
- **Evidence anchors:**
  - [abstract] "hierarchical taxonomy of 20 fine-grained abilities and 2,974 multiple-choice questions"
  - [section] "Currently, MMBench contains approximately 3000 single-choice questions covering 20 different ability dimensions, such as object localization and social reasoning"
- **Break condition:** If data samples for some abilities are sparse or biased, diagnostic granularity suffers.

## Foundational Learning

- **Concept:** Instruction-following capability in vision-language models
  - Why needed here: The benchmark requires models to output discrete answer labels (A, B, C, D), but many VLMs produce free-form text instead.
  - Quick check question: Can your VLM output only "A", "B", "C", or "D" when asked a multiple-choice question, or does it often generate full sentences?

- **Concept:** Multiple-choice question formatting and circular shifts
  - Why needed here: CircularEval relies on rotating the order of answer choices while keeping the correct answer aligned, so understanding how to construct and manipulate these prompts is essential.
  - Quick check question: If the original question has choices [A. 1, B. 2, C. 3, D. 4] with correct answer "C", what should the choices look like after one circular shift?

- **Concept:** ChatGPT prompt engineering for choice extraction
  - Why needed here: The benchmark uses a specific prompt template to instruct ChatGPT to map free-form predictions to the correct choice label.
  - Quick check question: Given the prediction "The answer is B: bicycle" and choices [A. car, B. bicycle, C. bus, D. train], what single character should ChatGPT output?

## Architecture Onboarding

- **Component map:**
  Data Collection Layer -> CircularEval Engine -> ChatGPT Extraction Module -> Evaluation Server -> Benchmarking Interface

- **Critical path:**
  1. Load question set with image and choices.
  2. For each question, run VLM inference N times (N = number of choices) with circular shifts.
  3. Collect free-form predictions.
  4. Apply ChatGPT-based extraction to map predictions to choice labels.
  5. Check consistency across all rotations; if all match ground truth, count as correct.
  6. Aggregate results by ability dimension.

- **Design tradeoffs:**
  - **Robustness vs. cost:** CircularEval increases reliability but multiplies inference cost by up to N.
  - **Generalizability vs. specificity:** Hierarchical taxonomy allows detailed diagnostics but requires more curated data.
  - **Automation vs. bias:** ChatGPT extraction reduces manual labor but introduces potential bias if ChatGPT misaligns predictions.

- **Failure signatures:**
  - High false-negative rate: Model consistently correct but fails CircularEval due to instability.
  - Low alignment rate: ChatGPT often outputs 'X' or mismatches predictions, indicating poor choice extraction.
  - Skewed ability scores: Certain abilities dominate due to imbalanced data distribution.

- **First 3 experiments:**
  1. **Baseline check:** Run VanillaEval (single-pass) on a small subset and compare to CircularEval results to measure robustness gain.
  2. **Choice extraction validation:** Manually annotate a sample of free-form predictions and compare ChatGPT alignments to human judgments.
  3. **Scalability test:** Measure time and cost for CircularEval on a medium-sized question set to identify bottlenecks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the instruction-following capabilities of vision-language models to enhance their usability in real-world applications?
- Basis in paper: [explicit] The paper notes that existing VLMs have limited instruction-following capabilities, which is the primary reason for employing ChatGPT for choice extraction in the MMBench evaluation.
- Why unresolved: While ChatGPT can help extract choices from free-form predictions, it does not directly address the underlying issue of VLMs' poor instruction-following capabilities. Improving this aspect of VLMs would require significant advancements in their training and architecture.
- What evidence would resolve it: Developing VLMs that can consistently and accurately follow instructions without the need for external tools like ChatGPT, and demonstrating their improved performance on benchmarks like MMBench.

### Open Question 2
- Question: What are the most effective strategies for improving cross-instance understanding and logical reasoning capabilities in vision-language models?
- Basis in paper: [explicit] The paper observes that cross-instance understanding and logical reasoning are particularly challenging for existing VLMs, with significantly lower accuracy compared to other abilities.
- Why unresolved: While the paper suggests that incorporating object localization data and more detailed object-specific information may help, the most effective strategies for improving these capabilities remain unclear. Further research is needed to identify and implement the best approaches.
- What evidence would resolve it: Developing VLMs that demonstrate significantly improved performance on cross-instance understanding and logical reasoning tasks, as measured by benchmarks like MMBench, and identifying the specific architectural or training modifications that led to these improvements.

### Open Question 3
- Question: How can we create a more comprehensive and balanced ability taxonomy for evaluating vision-language models?
- Basis in paper: [explicit] The paper presents a hierarchical ability taxonomy for MMBench, but notes that it is an initial version and further adjustments can be made to improve its comprehensiveness.
- Why unresolved: While the current taxonomy covers 20 fine-grained abilities, it may not fully capture the range of capabilities that vision-language models should possess. Creating a more comprehensive and balanced taxonomy would require input from experts in the field and extensive testing to ensure its effectiveness.
- What evidence would resolve it: Developing a revised ability taxonomy that is widely accepted by the research community, and demonstrating its effectiveness in evaluating the performance of vision-language models across a diverse set of tasks and applications.

## Limitations
- CircularEval's robustness gain is contingent on model prediction stability; highly unstable models may be unfairly penalized despite correct reasoning.
- ChatGPT-based choice extraction introduces a new evaluation bias source; alignment studies show 87% agreement but remaining 13% could significantly impact rankings.
- Hierarchical taxonomy requires balanced data across all 20 abilities; current 3,000 questions may be insufficient for truly representative coverage.

## Confidence
- **High confidence:** The core benchmark structure (2,974 questions, 20 abilities) and CircularEval mechanism are well-specified and reproducible.
- **Medium confidence:** ChatGPT extraction reliability claims are based on internal alignment studies; external validation needed for broader applicability.
- **Medium confidence:** Cross-model comparisons are meaningful but could be influenced by varying instruction-following capabilities and model-specific prediction patterns.

## Next Checks
1. Validate CircularEval robustness by testing on models with known prediction instability and measuring false-negative rates.
2. Conduct blind comparison of ChatGPT choice extraction against human annotators on a held-out sample to quantify remaining bias.
3. Analyze ability-specific performance gaps by expanding the dataset for underrepresented abilities to assess diagnostic utility.