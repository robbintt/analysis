---
ver: rpa2
title: 'Revisiting Adversarial Training for ImageNet: Architectures, Training and
  Generalization across Threat Models'
arxiv_id: '2303.01870'
source_url: https://arxiv.org/abs/2303.01870
tags:
- training
- convstem
- adversarial
- robust
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits adversarial training on ImageNet, comparing
  ViT and ConvNeXt architectures with focus on robustness and generalization to unseen
  attacks. It shows that replacing the patch stem with a convolutional stem improves
  both $\ell\infty$-robustness and generalization to $\ell1$/$\ell2$-threat models.
---

# Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models

## Quick Facts
- arXiv ID: 2303.01870
- Source URL: https://arxiv.org/abs/2303.01870
- Reference count: 35
- Key outcome: This paper revisits adversarial training on ImageNet, comparing ViT and ConvNeXt architectures with focus on robustness and generalization to unseen attacks. It shows that replacing the patch stem with a convolutional stem improves both $\ell_\infty$-robustness and generalization to $\ell_1$/$\ell_2$-threat models. The study also highlights the importance of strong pre-training and heavy data augmentation, leading to state-of-the-art $\ell_\infty$-robust accuracy (49.5% for small, 56.1% for large models) at resolution 224x224. Increasing test-time resolution further improves robustness. Fine-tuning to larger radii and other datasets shows consistent gains, making ConvNeXt + ConvStem a competitive architecture for adversarial robustness on ImageNet.

## Executive Summary
This paper revisits adversarial training on ImageNet with a focus on architectural choices and their impact on robustness and generalization. It demonstrates that replacing the patch stem with a convolutional stem in ViT and ConvNeXt models improves $\ell_\infty$-robustness and generalization to unseen $\ell_1$/$\ell_2$ threat models. The study highlights the importance of strong pre-training and heavy data augmentation, achieving state-of-the-art $\ell_\infty$-robust accuracy at resolution 224x224. The findings suggest that ConvNeXt + ConvStem is a competitive architecture for adversarial robustness on ImageNet.

## Method Summary
The paper evaluates adversarial robustness of ViT and ConvNeXt architectures on ImageNet, comparing patch vs convolutional stems. It uses ImageNet-1k training and validation sets, with objective metrics including $\ell_\infty$-robust accuracy at $\epsilon=4/255$ and generalization to unseen $\ell_1$ and $\ell_2$ attacks. The method involves $\ell_\infty$-adversarial training with 2-step APGD, standard models as warm initialization, heavy augmentations (RandAugment, MixUp, CutMix), and EMA. The evaluation uses AutoAttack on a 5k validation subset from RobustBench.

## Key Results
- Replacing PatchStem with ConvStem improves both $\ell_\infty$-robustness and generalization to $\ell_1$/$\ell_2$-threat models.
- Strong pre-training and heavy data augmentation lead to state-of-the-art $\ell_\infty$-robust accuracy (49.5% for small, 56.1% for large models) at resolution 224x224.
- Increasing test-time resolution further improves robustness.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing PatchStem with ConvStem improves adversarial robustness, especially generalization to unseen $\ell_1$/$\ell_2$ threat models.
- Mechanism: The ConvStem uses multiple small-kernel convolutional layers to progressively downsample the image, preserving more low-level spatial structure than the PatchStem's abrupt flattening. This smoother representation improves robustness to small, distributed perturbations like $\ell_1$/$\ell_2$ attacks.
- Core assumption: Adversarial examples in $\ell_1$/$\ell_2$ norms benefit from models that maintain spatial continuity in their early layers.
- Evidence anchors:
  - [abstract] "replacing PatchStem with ConvStem ... improve generalization to unseen $\ell_1$/$\ell_2$-attacks"
  - [section] "ConvStem ... improves robustness in particular regarding generalization to unseen threat models"
  - [corpus] Weak: No direct citations on ConvStem architecture, but "Is Certifying $\ell_p$ Robustness Still Worthwhile?" touches on broader robustness questions.
- Break condition: If $\ell_1$/$\ell_2$ attacks become highly localized or if patch-level representations are advantageous for a new threat model.

### Mechanism 2
- Claim: Strong pre-training on ImageNet-1k before adversarial training boosts both clean and robust accuracy.
- Mechanism: Initializing from a well-trained clean model places optimization in a region of lower loss, making the adversarial training landscape smoother and easier to traverse, enabling effective use of heavy data augmentations.
- Core assumption: Adversarial training on ImageNet is a difficult optimization problem where a good initialization reduces the risk of collapse.
- Evidence anchors:
  - [abstract] "using SOTA pre-trained clean models as initialization allows us to train using heavy augmentations"
  - [section] "starting from a point with low clean loss helps considerably in the first stages of training"
  - [corpus] Weak: No explicit corpus support, but "Revisiting Residual Connections" discusses initialization for stable training.
- Break condition: If the clean pre-trained model overfits or if the adversarial training objective diverges due to initialization mismatch.

### Mechanism 3
- Claim: Heavy data augmentation (RandAugment, MixUp, CutMix) combined with strong pre-training increases $\ell_\infty$ robustness.
- Mechanism: Data augmentation increases the effective diversity of the training set, forcing the model to learn invariances that make adversarial perturbations less effective. When combined with strong initialization, this diversity is incorporated without destabilizing training.
- Core assumption: The model's capacity is sufficient to learn from heavily augmented data without overfitting.
- Evidence anchors:
  - [abstract] "importance of strong pre-training and heavy data augmentation, leading to state-of-the-art $\ell_\infty$-robust accuracy"
  - [section] "adding heavy data augmentation on top of strong pre-training significantly boosts the performance ... in clean and $\ell_\infty$ robust accuracy"
  - [corpus] Weak: "Benchmarking Robustness to Adversarial Image Obfuscations" discusses augmentation effects but not in this context.
- Break condition: If augmentation introduces label noise that overwhelms the training signal or if model capacity is insufficient.

## Foundational Learning

- Concept: Adversarial training (AT) as a saddle-point optimization problem.
  - Why needed here: Understanding AT is essential to grasp why initialization, augmentation, and architecture choices matter.
  - Quick check question: What is the role of the inner maximization in adversarial training, and how does it differ from standard training?

- Concept: $\ell_p$ threat models and their geometric interpretation.
  - Why needed here: Different threat models ($\ell_\infty$, $\ell_1$, $\ell_2$) define different perturbation spaces; knowing this explains why generalization across them is nontrivial.
  - Quick check question: How does increasing image resolution affect the effective strength of an $\ell_\infty$-bounded adversary compared to an $\ell_2$-bounded one?

- Concept: Vision transformer architecture and patch embeddings.
  - Why needed here: ViT and ConvNeXt differ in their early-stage representations; understanding this helps explain the impact of ConvStem.
  - Quick check question: What is the main functional difference between a PatchStem and a ConvStem in a vision transformer?

## Architecture Onboarding

- Component map:
  - ViT backbone -> PatchStem (16x16 patches) -> Multi-head self-attention blocks -> Classifier
  - ConvNeXt backbone -> PatchStem (4x4 patches) -> Residual blocks (depthwise conv + 1x1 conv) -> Classifier
  - ConvStem variant: Replaces PatchStem with multiple conv layers + LayerNorm + GELU, progressively downsampling the image.

- Critical path:
  1. Load ImageNet-1k pre-trained weights for non-ConvStem part.
  2. Initialize ConvStem randomly.
  3. Fine-tune with standard training (100 epochs, heavy augmentations).
  4. Perform $\ell_\infty$ adversarial training (2-step APGD, 50-300 epochs).
  5. Evaluate with AutoAttack across $\ell_\infty$, $\ell_1$, $\ell_2$ radii.

- Design tradeoffs:
  - ConvStem increases FLOPs by ~2-9% and params by <1%, but improves robustness.
  - Heavy augmentation + strong init. requires more compute but yields higher robust accuracy.
  - Longer training (300 vs 50 epochs) improves robustness but increases cost.

- Failure signatures:
  - Collapse in adversarial training: sharp drop in both clean and robust accuracy.
  - Overfitting: clean accuracy improves but robust accuracy plateaus or degrades.
  - Poor generalization: high $\ell_\infty$ robustness but very low $\ell_1$/$\ell_2$ robustness.

- First 3 experiments:
  1. Train ConvNeXt + ConvStem with standard init. and basic augmentation; compare $\ell_\infty$ robustness to baseline.
  2. Replace PatchStem with ConvStem in ViT-S; measure clean and robust accuracy under $\ell_\infty$, $\ell_1$, $\ell_2$ attacks.
  3. Train ConvNeXt + ConvStem with strong pre-training + heavy augmentation; evaluate clean vs robust accuracy trade-off.

## Open Questions the Paper Calls Out

- **Question:** How does the ConvStem architecture affect adversarial robustness across different threat models and input resolutions?
  - **Basis in paper:** The paper demonstrates that ConvStem improves robustness to both seen ($\ell_\infty$) and unseen ($\ell_1$, $\ell_2$) threat models, and that robustness improves with increasing test-time image resolution.
  - **Why unresolved:** While the paper provides evidence of ConvStem's benefits, it does not fully explore the underlying mechanisms or provide a comprehensive theoretical explanation for its effectiveness across different threat models and resolutions.
  - **What evidence would resolve it:** Further experiments analyzing the feature representations learned by ConvStem models under various threat models and resolutions, coupled with theoretical analysis of the architecture's properties, would help explain its robustness benefits.

- **Question:** What is the optimal balance between clean accuracy and adversarial robustness, and how does it vary across different architectures and training schemes?
  - **Basis in paper:** The paper shows that its ConvNeXt + ConvStem models achieve state-of-the-art $\ell_\infty$-robust accuracy while maintaining high clean accuracy, but does not explore the trade-offs in detail or provide a framework for optimizing this balance.
  - **Why unresolved:** The paper focuses on improving robustness but does not investigate the relationship between clean accuracy and adversarial robustness, or provide guidelines for achieving the desired balance in different scenarios.
  - **What evidence would resolve it:** Systematic experiments varying the strength of adversarial training and data augmentation, and analyzing the resulting clean and robust accuracy across different architectures, would help establish a framework for optimizing the clean-robustness trade-off.

- **Question:** How does the choice of pre-training dataset and task affect adversarial robustness, and can we develop more effective pre-training strategies for robust models?
  - **Basis in paper:** The paper uses ImageNet-1k pre-trained models as initialization for adversarial training and observes that strong pre-training improves robustness. It also briefly explores the effect of using ImageNet-21k pre-trained models but finds no significant improvement.
  - **Why unresolved:** The paper does not investigate the impact of different pre-training datasets (e.g., ImageNet-21k, JFT-300M) or tasks (e.g., self-supervised learning, multi-task learning) on adversarial robustness, or propose new pre-training strategies tailored for robust models.
  - **What evidence would resolve it:** Experiments comparing the robustness of models pre-trained on various datasets and tasks, as well as developing and evaluating new pre-training strategies specifically designed for adversarial robustness, would shed light on the role of pre-training in achieving robust models.

## Limitations

- The ConvStem architecture is described but lacks exact kernel sizes, strides, and channel dimensions, making faithful reproduction challenging.
- The specific hyperparameters for RandAugment (depth, magnitude), MixUp, and CutMix are referenced to prior work but not fully specified in the paper.
- The evaluation setup uses a 5k subset from RobustBench without clear documentation of the exact split or preprocessing pipeline.

## Confidence

- High confidence in the architectural observation that ConvStem improves $\ell_\infty$ robustness (supported by controlled experiments).
- Medium confidence in the claim about generalization to unseen $\ell_1$/$\ell_2$ threat models (based on limited evaluation radii).
- Low confidence in the precise attribution of robustness gains to pre-training vs. augmentation (results are confounded).

## Next Checks

1. Re-implement ConvStem with exact specifications from timm repository and verify architectural similarity to the paper's description.
2. Reproduce the 50-epoch $\ell_\infty$ adversarial training baseline for ViT-S/16 and confirm ~49.5% robust accuracy on the 5k subset.
3. Evaluate the trained models on a held-out test set with $\ell_1$/$\ell_2$ radii matching the paper's protocol to verify generalization claims.