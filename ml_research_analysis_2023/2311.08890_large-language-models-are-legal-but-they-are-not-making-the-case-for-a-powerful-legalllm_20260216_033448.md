---
ver: rpa2
title: 'Large Language Models are legal but they are not: Making the case for a powerful
  LegalLLM'
arxiv_id: '2311.08890'
source_url: https://arxiv.org/abs/2311.08890
tags:
- legal
- llms
- language
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares zero-shot performance of general-purpose LLMs
  (ChatGPT-20b, LLaMA-2-70b, Falcon-180b) against smaller in-domain models on the
  LEDGAR contract provision classification task. While LLMs achieve respectable accuracy
  in theme identification, their micro-F1 scores are 19.2% lower and macro-F1 scores
  are 26.8% lower than LegalBERT's performance.
---

# Large Language Models are legal but they are not: Making the case for a powerful LegalLLM

## Quick Facts
- arXiv ID: 2311.08890
- Source URL: https://arxiv.org/abs/2311.08890
- Authors: 
- Reference count: 8
- Primary result: General-purpose LLMs (70.9% F1) underperform specialized legal models (88.2% F1) on contract classification by 19.2% micro-F1 and 26.8% macro-F1

## Executive Summary
This paper demonstrates that despite their large parameter counts and extended context windows, general-purpose LLMs cannot match the performance of smaller specialized legal-domain models on contract provision classification tasks. The study evaluates zero-shot performance of ChatGPT-20b, LLaMA-2-70b, and Falcon-180b against LegalBERT and LexGPT on the LEDGAR benchmark. The results show that legal practitioners can avoid using or training unnecessarily large models since domain-specific training data provides clear advantages over model size alone.

## Method Summary
The study evaluates zero-shot classification performance of three general-purpose LLMs (ChatGPT-20b, LLaMA-2-70b, Falcon-180b) on the LEDGAR contract provision classification task from LexGLUE benchmark. Researchers sampled 1,000 examples from the test set while maintaining the original label distribution, then created custom prompts listing all 100 EDGAR theme classes. The LLMs were evaluated using HuggingFace Chat for LLaMA and Falcon, with performance measured using micro-F1 and macro-F1 scores. Results were compared against established legal-domain models LegalBERT (88.2% F1) and LexGPT.

## Key Results
- General-purpose LLMs achieve 70.9% F1 (Falcon-Chat) on LEDGAR contract classification, 19.2% lower than LegalBERT's 88.2% F1
- Macro-F1 scores show even larger gaps, with LLMs performing 26.8% worse than specialized legal models
- Models struggle with semantically similar labels (e.g., "Indemnity" vs "Indemnifications") and fail to predict single-example classes
- Extended context windows (up to 32k tokens) provide no significant advantage for contract classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: General-purpose LLMs struggle with legal domain classification because they lack exposure to specialized legal vocabulary and context patterns.
- Mechanism: When trained on general web text, LLMs learn statistical patterns of common language but miss domain-specific relationships. Legal language contains unique terms, syntactic structures, and semantic relationships that don't appear frequently enough in general training data to be properly learned.
- Core assumption: The performance gap between LegalBERT (88.2% F1) and Falcon-Chat (70.9% F1) demonstrates that specialized training data is essential for capturing domain nuances.
- Evidence anchors:
  - [abstract] "Although the LLMs were not explicitly trained on legal data, we observe that they are still able to classify the theme correctly in most cases. However, we find that their mic-F1/mac-F1 performance is up to 19.2/26.8% lesser than smaller models fine-tuned on the legal domain"
  - [section] "Our findings echo that of (Chalkidis, 2023). Notably, for class labels with only one example in our sampled test set, the three chat-variants surprisingly show the same results: they fail to predict them correctly"

### Mechanism 2
- Claim: General LLMs cannot fully leverage their large context windows for legal classification because legal text often contains redundant or contextually similar content.
- Mechanism: While LLMs have large context windows (up to 32k tokens for GPT-4), legal documents contain extensive boilerplate text and repetitive legal phrasing. The additional context beyond key identifying features provides diminishing returns for classification tasks.
- Core assumption: The paper's finding that "LLMs' ability to process large context may not be necessary for classification" suggests that legal text's semantic similarity across different documents reduces the value of extended context.
- Evidence anchors:
  - [abstract] "The recent surge of Large Language Models (LLMs) has begun to provide new opportunities to apply NLP in the legal domain due to their ability to handle lengthy, complex sequences"
  - [section] "Our findings highlight that the perceived advantages LLMs have over BERT-based models (such as the sheer amount of large parameters, extended context length, and the amount of pre-training knowledge), cannot substitute for the obvious edge in-domain data gives to the much smaller models"

### Mechanism 3
- Claim: Zero-shot prompting is insufficient for legal classification because models cannot properly disambiguate semantically similar legal concepts without task-specific fine-tuning.
- Mechanism: Legal classification requires distinguishing between closely related concepts (e.g., "Indemnity" vs "Indemnifications", "Taxes" vs "Tax Withholdings"). Without fine-tuning on legal data, models rely on general semantic understanding which may not capture these subtle distinctions.
- Core assumption: The paper's observation that "labels that are semantically similar are frequently mislabeled by the models" demonstrates the need for domain-specific training to learn these distinctions.
- Evidence anchors:
  - [abstract] "Although the LLMs were not explicitly trained on legal data, we observe that they are still able to classify the theme correctly in most cases"
  - [section] "However, we find that their mic-F1/mac-F1 performance is up to 19.2/26.8% lesser than smaller models fine-tuned on the legal domain"

## Foundational Learning

- Concept: Imbalanced datasets and macro vs micro F1 metrics
  - Why needed here: The LEDGAR dataset has high label imbalance (Figure 1 shows frequency distributions), and understanding these metrics is crucial for interpreting why general LLMs underperform
  - Quick check question: Why do macro-F1 scores typically appear lower than micro-F1 scores in imbalanced datasets?

- Concept: Zero-shot vs few-shot vs fine-tuned learning paradigms
  - Why needed here: The paper compares zero-shot performance of general LLMs against fine-tuned legal models, highlighting the performance gap
  - Quick check question: What are the key differences in how zero-shot, few-shot, and fine-tuned models approach a new task?

- Concept: Long document processing in transformers
  - Why needed here: Legal documents are typically very long, and understanding transformer limitations (512-token limit for BERT) versus LLM capabilities is essential
  - Quick check question: What architectural modifications allow modern LLMs to process documents longer than 512 tokens?

## Architecture Onboarding

- Component map: LLM architecture -> prompt engineering -> evaluation metrics -> comparison baseline -> analysis pipeline
- Critical path: Data loading (LEDGAR) -> prompt formulation (zero-shot) -> model inference (ChatGPT/LLaMA/Falcon) -> F1 score calculation -> comparison with LegalBERT/LexGPT
- Design tradeoffs: Using zero-shot prompting (no task-specific training) versus fine-tuning (requires legal data but achieves higher performance)
- Failure signatures: Models consistently misclassifying semantically similar labels, poor performance on rare classes, inability to generate valid predictions for single-example classes
- First 3 experiments:
  1. Test zero-shot classification on a balanced subset of LEDGAR to isolate label imbalance effects
  2. Implement few-shot prompting with 5 examples per label to measure performance improvement
  3. Fine-tune a smaller general-purpose model (e.g., BERT) on LEDGAR to establish baseline for "how much legal data is needed"

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the specific threshold of legal-domain training data required for LLMs to achieve performance comparable to specialized legal-domain models?
- Basis in paper: [inferred] The paper demonstrates that even large LLMs (180B parameters) cannot match the performance of smaller legal-domain models, suggesting there's a minimum amount of legal-specific data needed for effective fine-tuning.
- Why unresolved: The paper compares general LLMs to legal-domain models but doesn't explore varying amounts of legal training data or determine the minimum required for competitive performance.
- What evidence would resolve it: Experiments systematically varying the amount of legal-domain training data used to fine-tune LLMs, measuring performance gains until they match specialized models.

### Open Question 2
- Question: How does the performance of LegalLLMs vary across different legal sub-domains (contracts, case law, regulations, etc.)?
- Basis in paper: [explicit] The paper uses LEDGAR contract provision classification but notes that models fine-tuned on one sub-domain don't necessarily improve on other sub-domains, highlighting the need for a general LegalLLM.
- Why unresolved: The study only evaluates on contract classification, leaving open how LegalLLMs would perform on other types of legal text like case law, statutes, or regulations.
- What evidence would resolve it: Comprehensive benchmarking of LegalLLMs across multiple legal sub-domains using appropriate datasets for each area.

## Limitations

- The study relies solely on zero-shot prompting without exploring intermediate approaches like few-shot learning or instruction tuning that could bridge the performance gap
- Evaluation focuses exclusively on classification accuracy without examining other potentially valuable capabilities like legal reasoning or document analysis
- The paper doesn't investigate whether fine-tuning the same LLMs on legal data would achieve superior results compared to smaller legal-domain models

## Confidence

**High Confidence**: The finding that general-purpose LLMs underperform specialized legal models on classification tasks is well-supported by the 19.2%/26.8% F1 gap and aligns with established principles of domain adaptation in machine learning.

**Medium Confidence**: The claim that LLMs cannot substitute for domain-specific training data is supported but potentially overstated, as the paper doesn't explore whether fine-tuning these same LLMs on legal data would achieve superior results.

**Low Confidence**: The assertion that "LLMs' ability to process large context may not be necessary for classification" lacks direct empirical evidence from the current study.

## Next Checks

1. Test whether providing 5-10 examples per label in the prompt improves LLM classification accuracy by 10-15% F1, potentially narrowing the gap with fine-tuned models without requiring extensive legal training data.

2. Systematically evaluate classification performance using different context lengths (512, 2048, 8192 tokens) on the same LEDGAR samples to empirically determine whether extended context provides measurable benefits for legal document classification.

3. Evaluate whether legal-domain models fine-tuned on LEDGAR maintain reasonable performance on other legal NLP tasks (contract review, legal question answering) compared to general LLMs, testing whether domain specialization creates task-specific limitations.