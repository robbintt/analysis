---
ver: rpa2
title: 'FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pretraining'
arxiv_id: '2309.09431'
source_url: https://arxiv.org/abs/2309.09431
tags:
- spectral
- spatial
- transformer
- hyperspectral
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FactoFormer introduces a factorized transformer architecture for
  hyperspectral image classification that separately processes spectral and spatial
  information using two dedicated transformers. The approach addresses limitations
  of existing methods that underutilize spatial information and struggle with limited
  annotated data.
---

# FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pretraining

## Quick Facts
- arXiv ID: 2309.09431
- Source URL: https://arxiv.org/abs/2309.09431
- Reference count: 40
- Primary result: Achieves state-of-the-art hyperspectral image classification with 7.15%-9.54% improvement in overall accuracy

## Executive Summary
FactoFormer introduces a factorized transformer architecture that separately processes spectral and spatial information in hyperspectral images using dedicated transformers. This approach addresses the limitations of existing methods that underutilize spatial information and struggle with limited annotated data. By factorizing the input into spectral and spatial tokens, the model learns rich feature representations through factorized self-attention mechanisms. A novel self-supervised pre-training strategy with spatially and spectrally consistent masking allows effective use of unlabeled data. Experiments on three datasets show FactoFormer achieves state-of-the-art performance while being more computationally efficient than previous methods.

## Method Summary
FactoFormer processes hyperspectral images using two dedicated transformers: a spectral transformer that handles spectral information and a spatial transformer that handles spatial information. The input hyperspectral cube is factorized into spectral tokens (each containing all spectral bands for a spatial patch) and spatial tokens (each containing all spatial pixels for a spectral band). Both transformers are pre-trained separately using self-supervised learning with masked image modeling, where the spectral transformer uses spatially consistent masking and the spatial transformer uses spectrally consistent masking. After pre-training, the classification tokens from both transformers are fused through concatenation and passed through an MLP for final classification. The model is fine-tuned end-to-end using the Adam optimizer.

## Key Results
- Achieves state-of-the-art performance on Indian Pines, Pavia University, and Houston 2013 datasets
- Improves overall accuracy by 7.15% to 9.54% compared to previous state-of-the-art methods
- Requires fewer parameters and achieves faster runtimes than competing transformer-based approaches
- Demonstrates effectiveness of factorized processing for hyperspectral image classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factorized self-attention allows separate, efficient processing of spectral and spatial dimensions
- Mechanism: By splitting the input into spectral tokens (each a full spatial patch across all bands) and spatial tokens (each a single pixel across all bands), the model computes attention in each dimension separately, reducing computational complexity from O((m+n)²) to O(m²)+O(n²)
- Core assumption: The spectral and spatial information in HSIs can be effectively processed independently before being fused
- Evidence anchors:
  - [abstract]: "The factorization of the inputs allows the spectral and spatial transformers to better capture the interactions within the hyperspectral data cubes"
  - [section]: "The factorization enables factorized self-attention where attention is computed in spatial and spectral dimensions simultaneously"
- Break condition: If spectral and spatial information are highly interdependent and cannot be meaningfully separated, the factorization would lose critical cross-dimension relationships

### Mechanism 2
- Claim: Factorized self-supervised pretraining leverages unlabeled data more effectively than joint pretraining
- Mechanism: Separate masking strategies (spatially consistent for spectral transformer, spectrally consistent for spatial transformer) allow each transformer to learn dimension-specific features before joint fine-tuning
- Core assumption: Dimension-specific pretraining tasks create better feature representations than attempting to learn both dimensions simultaneously from unlabeled data
- Evidence anchors:
  - [abstract]: "We propose novel self-supervised pre-training strategies for the factorized spatial and spectral transformers, which allows us to better utilize the large proportion of unlabeled data"
  - [section]: "Both transformers are pre-trained separately with spatially consistent and spectrally consistent masking strategies"
- Break condition: If the masking strategies don't capture meaningful structure in the data, the pretraining would learn irrelevant patterns

### Mechanism 3
- Claim: Fusion of classification tokens captures higher-order spectral-spatial correlations
- Mechanism: The classification tokens from each transformer, having learned dimension-specific context, are concatenated and passed through an MLP to capture interactions between spectral and spatial features
- Core assumption: The classification token in each transformer learns a meaningful representation of the entire sequence in its respective dimension
- Evidence anchors:
  - [section]: "Since the classification token learns to represent the entire input sequence, the classification token in each transformer captures an overall context in both spectral and spatial dimensions"
  - [section]: "Finally, learned embeddings from the factorized transformers are fused in order to perform classification"
- Break condition: If the classification tokens don't capture meaningful context, the fusion step would combine irrelevant information

## Foundational Learning

- Concept: Multi-head self-attention mechanism
  - Why needed here: Transformers rely on self-attention to capture long-range dependencies in spectral and spatial dimensions
  - Quick check question: How does multi-head attention differ from single-head attention in terms of feature extraction?

- Concept: Masked image modeling (MIM) pretraining
  - Why needed here: MIM allows learning from unlabeled data by reconstructing masked regions, addressing the limited labeled data problem in hyperspectral classification
  - Quick check question: What is the key difference between MIM and contrastive learning approaches for self-supervised learning?

- Concept: Factorized computation
  - Why needed here: Factorizing spectral and spatial processing reduces computational complexity and enables more efficient training on large hyperspectral datasets
  - Quick check question: What is the computational complexity of joint attention versus factorized attention in this architecture?

## Architecture Onboarding

- Component map:
  Input → Spectral Patching → Spectral Transformer → Classification Token
  Input → Spatial Patching → Spatial Transformer → Classification Token
  Classification Tokens → Concatenation → MLP → Output

- Critical path:
  Input → Spectral Patching → Spectral Transformer → Classification Token
  Input → Spatial Patching → Spatial Transformer → Classification Token
  Classification Tokens → Concatenation → MLP → Output

- Design tradeoffs:
  - Separate transformers increase parameter count but enable specialized processing
  - Simple linear decoder vs. heavy transformer decoder reduces parameters and training time
  - Factorized pretraining requires careful masking strategy design

- Failure signatures:
  - Low accuracy despite high training performance: Overfitting due to limited labeled data
  - Slow convergence: Pretraining masking ratio may be suboptimal
  - Poor performance on specific classes: Spatial or spectral resolution may be insufficient for those classes

- First 3 experiments:
  1. Ablation: Train without pretraining to measure pretraining contribution
  2. Masking ratio sweep: Test ratios from 0.5 to 0.8 to find optimal value
  3. Patch size variation: Test spatial patch sizes (3×3, 5×5, 7×7, 9×9) to find optimal balance between local detail and computational efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FactoFormer change when applied to larger-scale hyperspectral datasets with more diverse land cover types?
- Basis in paper: [inferred] The paper mentions that FactoFormer achieves state-of-the-art performance on three hyperspectral datasets but does not explore its scalability to larger datasets.
- Why unresolved: The experiments were limited to three specific datasets, and there is no mention of testing the model on larger or more diverse datasets.
- What evidence would resolve it: Conducting experiments on larger-scale hyperspectral datasets and comparing the performance metrics with current state-of-the-art methods.

### Open Question 2
- Question: Can the factorized architecture of FactoFormer be adapted for other remote sensing tasks such as object detection or semantic segmentation?
- Basis in paper: [inferred] The paper focuses on hyperspectral image classification and mentions that the FactoFormer framework can be applied to various other HSI applications that require representation learning.
- Why unresolved: The paper does not explore the application of FactoFormer to other remote sensing tasks beyond classification.
- What evidence would resolve it: Implementing FactoFormer for tasks like object detection or semantic segmentation in remote sensing and evaluating its performance against specialized methods.

### Open Question 3
- Question: How does the choice of patch size and spectral band grouping affect the model's performance in different hyperspectral datasets?
- Basis in paper: [explicit] The paper conducts an ablation study on the impact of spatial patch sizes and grouping of spectral bands, but only for the Indian Pines dataset.
- Why unresolved: The ablation study is limited to one dataset, and there is no mention of how these parameters might affect performance on other datasets.
- What evidence would resolve it: Performing similar ablation studies on different hyperspectral datasets to determine the optimal patch size and spectral band grouping for each dataset.

## Limitations
- Architecture complexity trade-off may not generalize to datasets with different spectral-spatial characteristics
- Pretraining generalizability depends heavily on masking strategies and ratios, which may not be optimal across different datasets
- Computational efficiency claims don't fully account for pretraining phase overhead

## Confidence
- High Confidence: Architectural design of factorized transformers is clearly specified and reproducible with well-documented performance improvements
- Medium Confidence: Self-supervised pretraining strategy contribution to performance gains, though implementation details could benefit from more ablation studies
- Low Confidence: Computational efficiency claims when considering full training pipeline including pretraining phase

## Next Checks
1. Cross-dataset generalization test: Evaluate FactoFormer on additional hyperspectral datasets with different spectral ranges and spatial resolutions
2. Ablation study on masking strategies: Systematically vary masking ratio (0.5-0.9) and masking pattern consistency
3. End-to-end efficiency analysis: Measure total training time including pretraining and evaluate trade-off between performance gains and computational overhead