---
ver: rpa2
title: Adaptive Anytime Multi-Agent Path Finding Using Bandit-Based Large Neighborhood
  Search
arxiv_id: '2312.16767'
source_url: https://arxiv.org/abs/2312.16767
tags:
- balance
- neighborhood
- search
- thompson
- mapf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limitations in state-of-the-art anytime multi-agent
  path finding (MAPF) using Large Neighborhood Search (LNS). The core issue is that
  fixed neighborhood sizes and greedy optimization can lead to low-quality solutions.
---

# Adaptive Anytime Multi-Agent Path Finding Using Bandit-Based Large Neighborhood Search

## Quick Facts
- arXiv ID: 2312.16767
- Source URL: https://arxiv.org/abs/2312.16767
- Reference count: 10
- Key outcome: Demonstrates cost improvements of at least 50% compared to state-of-the-art anytime MAPF in large-scale scenarios using adaptive bandit-based neighborhood selection

## Executive Summary
This paper addresses limitations in state-of-the-art anytime Multi-Agent Path Finding (MAPF) using Large Neighborhood Search (LNS). The core issue is that fixed neighborhood sizes and greedy optimization can lead to low-quality solutions. To address this, the authors propose Bandit-based Adaptive LArge Neighborhood search Combined with Exploration (BALANCE), which uses a bi-level multi-armed bandit scheme to adaptively select destroy heuristics and neighborhood sizes during search. BALANCE is evaluated on multiple maps from the MAPF benchmark set and demonstrates cost improvements of at least 50% compared to state-of-the-art anytime MAPF in large-scale scenarios.

## Method Summary
BALANCE implements a bi-level multi-armed bandit framework where a top-level H-Bandit selects from three destroy heuristics (random, agent-based, map-based), and for each selected heuristic, a bottom-level N-Bandit selects neighborhood size as powers of two. The framework maintains statistics for each arm and updates them incrementally after each iteration based on cost improvements. Thompson Sampling is used as the primary bandit algorithm, though comparisons with UCB1 and roulette wheel selection are provided. The method starts with an initial feasible solution via prioritized planning and iteratively destroys and repairs solution neighborhoods, adapting parameters on-the-fly.

## Key Results
- BALANCE demonstrates cost improvements of at least 50% compared to state-of-the-art anytime MAPF in large-scale scenarios
- Thompson Sampling outperforms alternative bandit algorithms (UCB1, roulette wheel selection) in exploration-exploitation balance
- Adaptive neighborhood sizing provides significant benefits over fixed-size LNS, especially when neighborhood size options (E) are increased to 3
- The method scales effectively to scenarios with 200-350 agents on benchmark maps including random-32-32-10 and warehouse-10-20-10-2-1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bi-level multi-armed bandit scheme improves MAPF-LNS by adaptively selecting both destroy heuristics and neighborhood sizes during search
- Mechanism: BALANCE uses a top-level H-Bandit to select destroy heuristics from a set of three (random, agent-based, map-based), and a bottom-level N-Bandit for each destroy heuristic to select neighborhood sizes as powers of two
- Core assumption: The destroy heuristic and neighborhood size choices significantly impact solution quality, and these impacts can be learned incrementally from cost improvements
- Evidence anchors:
  - [abstract] "BALANCE uses a bi-level multi-armed bandit scheme to adapt the selection of destroy heuristics and neighborhood sizes on the fly during search"
  - [section] "BALANCE first selects a destroy heuristic H with the top-level H-Bandit based on its current statistics ∆H. The selected destroy heuristic H determines the corresponding bottom-level N -Bandit..."
- Break condition: If the cost improvement signal is too noisy or sparse, the bandits may converge to suboptimal choices before sufficient exploration

### Mechanism 2
- Claim: Thompson Sampling provides superior exploration-exploitation balance compared to UCB1 and roulette wheel selection in the MAPF-LNS context
- Mechanism: Thompson Sampling maintains a Bayesian posterior for each arm (destroy heuristic or neighborhood size) and samples from this posterior to select actions, naturally balancing exploration and exploitation through uncertainty quantification
- Core assumption: The uncertainty inherent in randomized destroy heuristics makes Bayesian approaches like Thompson Sampling more effective than frequentist alternatives
- Evidence anchors:
  - [abstract] "We find that Thompson Sampling performs particularly well compared to alternative multi-armed bandit algorithms"
  - [section] "Thompson Sampling is a randomized algorithm whose initial exploration depends on prior parameters... previous works report that using prior distributions that are close to a uniform distribution is sufficient in most cases..."
- Break condition: If prior parameters are poorly chosen or computational overhead becomes significant, the benefits of Thompson Sampling may diminish

### Mechanism 3
- Claim: Adaptive neighborhood sizing prevents the fixed-size limitation of standard MAPF-LNS, improving solution quality especially for large-scale scenarios
- Mechanism: By treating neighborhood size as a learnable parameter with E options (powers of two), BALANCE can dynamically adjust the scope of destruction based on problem characteristics and search progress
- Core assumption: There exists an optimal neighborhood size that varies by problem instance, and this optimal size can be discovered through online learning rather than manual tuning
- Evidence anchors:
  - [abstract] "current LNS-based approaches lack exploration and flexibility due to greedy optimization with a fixed neighborhood size which can lead to low-quality solutions"
  - [section] "The neighborhood size is typically fixed, which limits the flexibility of the optimization process, thus possibly affecting the solution quality, especially for a large number of agents"
- Break condition: If replanning computational cost grows too quickly with neighborhood size, larger options may become impractical despite potential quality benefits

## Foundational Learning

- Concept: Multi-Armed Bandit Theory
  - Why needed here: BALANCE's core innovation relies on bandit algorithms to balance exploration and exploitation in selecting destroy heuristics and neighborhood sizes
  - Quick check question: What is the fundamental trade-off that bandit algorithms must manage during the learning process?

- Concept: Large Neighborhood Search (LNS) in MAPF
  - Why needed here: Understanding how LNS iteratively destroys and repairs solution neighborhoods is essential to grasp why adaptive selection matters
  - Quick check question: In standard MAPF-LNS, what two operations are performed iteratively to improve solutions?

- Concept: Thompson Sampling Bayesian Update Rules
  - Why needed here: Thompson Sampling requires maintaining and updating posterior distributions for each arm, which differs from other bandit approaches
  - Quick check question: In Thompson Sampling for this paper, what distribution family is assumed for the reward random variables?

## Architecture Onboarding

- Component map:
  - Top-level H-Bandit: Selects destroy heuristic from H = {random, agent-based, map-based}
  - Bottom-level N-Bandits: One per destroy heuristic, selects neighborhood size N = 2^e where e ∈ {1,...,E}
  - MAPF-LNS Core: Performs destruction of N paths using selected heuristic, repairs via prioritized planning
  - Statistics Tracker: Maintains ∆ for all bandits, updated incrementally after each iteration

- Critical path:
  1. Initial feasible solution via prioritized planning
  2. H-Bandit selects destroy heuristic H
  3. Corresponding N-Bandit selects neighborhood size exponent e
  4. N = 2^e paths destroyed using heuristic H
  5. Repaired via prioritized planning to generate new solution
  6. Cost improvement calculated and rewards assigned to selected arms
  7. Statistics updated incrementally
  8. Repeat until time budget exhausted

- Design tradeoffs:
  - Exploration vs exploitation: More exploration yields better long-term performance but slower initial convergence
  - Neighborhood size range: Larger E provides more flexibility but increases computational overhead
  - Bandit algorithm choice: Thompson Sampling offers better exploration but requires more complex statistics tracking

- Failure signatures:
  - Premature convergence: Bandits converge too quickly to suboptimal choices, visible as plateauing solution quality
  - Excessive exploration: Too much time spent trying poor options, visible as slow improvement rate
  - Computational bottleneck: Replanning becomes too expensive with large neighborhood sizes

- First 3 experiments:
  1. Run BALANCE with E=1 (fixed neighborhood size) to verify it matches standard MAPF-LNS performance
  2. Run with E=3 and Thompson Sampling to confirm adaptive neighborhood sizing provides benefit
  3. Compare Thompson Sampling vs UCB1 vs roulette wheel selection on the same problem instance to observe exploration differences

## Open Questions the Paper Calls Out
The paper identifies several directions for future research including investigation of non-stationary MAB approaches when no stationary optimum exists, exploration of online learnable destroy heuristics beyond the fixed set used, and analysis of how different neighborhood size option ranges affect performance across various MAPF instances.

## Limitations
- Evaluation focuses primarily on cost improvement metrics without comprehensive runtime analysis across all bandit algorithms
- The claim that Thompson Sampling "performs particularly well" lacks statistical significance testing against alternatives
- Computational overhead of maintaining Bayesian posteriors for each arm, especially with large neighborhood size spaces, is not quantified
- Results are based on specific benchmark maps that may not generalize to all MAPF domains

## Confidence
- High confidence: Adaptive neighborhood sizing improves solution quality over fixed-size LNS (supported by direct comparisons and ablation studies)
- Medium confidence: Thompson Sampling provides superior exploration-exploitation balance (supported by relative performance but lacks statistical testing)
- Low confidence: The 50% improvement claim represents typical performance across all scenarios (appears to be an upper bound from best-case results)

## Next Checks
1. Perform statistical significance testing (e.g., Wilcoxon signed-rank test) comparing Thompson Sampling against UCB1 and roulette wheel selection across all benchmark instances
2. Measure and compare the computational overhead of each bandit algorithm, including posterior updates and sampling operations
3. Test BALANCE on additional MAPF domains beyond warehouse and random grid maps, including those with different obstacle densities and agent distributions