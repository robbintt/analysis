---
ver: rpa2
title: Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language
  Models
arxiv_id: '2310.16570'
source_url: https://arxiv.org/abs/2310.16570
tags:
- knowledge
- association
- linguistics
- computational
- plms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of factual knowledge
  probing in pre-trained language models (PLMs), categorizing methods based on input,
  model, and output adaptations. It analyzes 94 relevant papers and 44 datasets, offering
  insights into knowledge retention, prompt optimization, and challenges in adopting
  PLMs as knowledge bases.
---

# Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models

## Quick Facts
- arXiv ID: 2310.16570
- Source URL: https://arxiv.org/abs/2310.16570
- Reference count: 40
- Provides comprehensive survey categorizing 94 papers on factual knowledge probing methods in PLMs

## Executive Summary
This survey systematically analyzes methods for probing factual knowledge in pre-trained language models (PLMs), categorizing approaches based on adaptations to inputs, models, and outputs. The authors examine 94 relevant papers and 44 datasets, revealing that model architecture and pre-training objectives are more critical for knowledge retention than model size. The survey identifies key challenges including PLMs' sensitivity to prompts, the need for unbiased datasets, and obstacles to using PLMs as reliable knowledge bases. It highlights promising directions for future research including interpretable prompt optimization and scalable knowledge editing methods.

## Method Summary
The survey conducted a systematic analysis of the literature through two annotation steps: first identifying relevant papers based on inclusion criteria (focusing on factual knowledge quantification, retention improvement, and PLM knowledge base challenges), then extracting detailed information about probing methods, datasets, and other aspects. The authors analyzed 94 papers from an initial corpus of 173, categorizing them according to their adaptations to inputs, models, and outputs. The study also examined 44 datasets, classifying them into general knowledge, domain-specific knowledge, and other aspects like consistency and knowledge updating.

## Key Results
- Model architecture and pre-training objectives are more decisive for knowledge retention than model size alone
- Optimized prompts can improve performance but may not reflect genuine knowledge enhancement
- PLMs face significant challenges as knowledge bases due to consistency, interpretability, and knowledge updating limitations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The categorization scheme based on input, model, and output adaptations enables systematic analysis of factual knowledge probing methods
- **Mechanism**: By decomposing methods into three orthogonal dimensions (input optimization, model adaptation, output modification), researchers can precisely identify which components contribute to performance gains and knowledge extraction effectiveness
- **Core assumption**: The three dimensions are independent and collectively exhaustive for characterizing factual knowledge probing approaches
- **Evidence anchors**:
  - [abstract] The paper explicitly states: "We propose a categorization scheme for factual probing methods that is based on how their inputs, outputs and the probed PLMs are adapted"
  - [section] The categorization is described in detail in Section 2, showing how each dimension is populated with specific methods
  - [corpus] The corpus analysis shows 94 papers distributed across these categories, validating the comprehensiveness of the scheme
- **Break condition**: If a novel probing method cannot be classified under any of the three dimensions, or if methods span multiple dimensions in ways that violate orthogonality assumptions

### Mechanism 2
- **Claim**: Knowledge retention in PLMs is primarily determined by model architecture and pre-training objectives rather than model size alone
- **Mechanism**: Architectural choices (like Salient Span Masking) and pre-training objectives that match downstream tasks create more effective knowledge encoding than simple parameter scaling
- **Core assumption**: The quality of knowledge encoding matters more than quantity of parameters for factual knowledge retention
- **Evidence anchors**:
  - [abstract] States that "model architecture and pre-training objectives are more decisive for knowledge retention than its size"
  - [section] Section 4.1 provides evidence that masking strategies and pre-training objectives have greater impact than model scale
  - [corpus] Multiple papers in the corpus demonstrate this through ablation studies comparing different architectures
- **Break condition**: If empirical evidence shows that for a specific knowledge domain or task, model size becomes the dominant factor regardless of architecture choices

### Mechanism 3
- **Claim**: Optimized prompts improve factual knowledge retrieval performance, but this improvement may not reflect genuine knowledge enhancement
- **Mechanism**: Prompt optimization can either make prompts more similar to pre-training data (improving retrieval) or cause overfitting to the training distribution (artificial performance gains)
- **Core assumption**: The performance improvement from optimized prompts is not necessarily due to increased knowledge but rather better alignment with the model's learned representations
- **Evidence anchors**:
  - [abstract] Notes that "it remains unclear whether this improvement is due to optimized prompts leaking new knowledge into the probed PLMs"
  - [section] Section 4.2 discusses how optimized prompts can be paraphrases of manual ones or find better prompts in discrete/continuous spaces
  - [corpus] Papers cited show that optimized prompts can retrieve facts from randomly initialized models, suggesting the improvement is not knowledge-based
- **Break condition**: If prompt optimization consistently leads to performance improvements across diverse knowledge domains and cannot be replicated by simple data augmentation or model architecture changes

## Foundational Learning

- **Concept**: Language model pretraining objectives and their impact on knowledge retention
  - **Why needed here**: Understanding how different pretraining objectives (MLM, span prediction, contrastive learning) affect factual knowledge encoding is crucial for designing effective probing methods
  - **Quick check question**: How would a pretraining objective that predicts whether entities have been replaced affect factual knowledge retention compared to standard MLM?

- **Concept**: Prompt sensitivity and its implications for knowledge extraction
  - **Why needed here**: PLMs' sensitivity to input phrasing affects the reliability of knowledge probing and the generalizability of findings
  - **Quick check question**: Why might a syntactic variation in a prompt (like changing word order) lead to different factual predictions from the same PLM?

- **Concept**: Knowledge base construction and evaluation metrics
  - **Why needed here**: Understanding KB construction principles helps in evaluating whether PLMs can serve as reliable knowledge sources and what metrics are appropriate
  - **Quick check question**: What makes a knowledge base reliable, and how do consistency and interpretability requirements apply to PLMs as knowledge sources?

## Architecture Onboarding

- **Component map**: Dataset selection -> Prompt creation/optimization -> PLM configuration (vanilla vs adapted) -> Inference -> Prediction evaluation -> Result analysis

- **Critical path**: For a new probing method, the critical path is: select dataset → create/optimize prompts → choose PLM configuration → run inference → evaluate predictions against ground truth → analyze results for knowledge extraction effectiveness

- **Design tradeoffs**:
  - Vanilla vs adapted PLMs: vanilla preserves original knowledge but may be less effective; adapted PLMs can be more effective but risk train-test overlap
  - Manual vs optimized prompts: manual prompts are interpretable but less effective; optimized prompts are more effective but potentially less trustworthy
  - Single vs multi-token entity handling: single-token is simpler but less comprehensive; multi-token is more complete but computationally heavier

- **Failure signatures**:
  - Low recall across diverse datasets suggests issues with prompt quality or model architecture choice
  - High variance in predictions for semantically equivalent prompts indicates sensitivity issues
  - Performance improvements only on specific datasets may indicate overfitting to particular knowledge distributions

- **First 3 experiments**:
  1. Implement basic LAMA-style probing with manually created cloze prompts on a standard dataset to establish baseline performance
  2. Apply prompt optimization techniques (like AUTOPROMPT) to the same setup to measure improvement and assess whether gains are knowledge-based
  3. Test the same prompts across different PLM architectures (BERT, RoBERTa, T5) to understand architecture-dependent knowledge retention patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can prompt optimization methods be adapted to produce more interpretable prompts while maintaining performance gains?
- Basis in paper: [explicit] The paper discusses the issue of optimized prompts being largely uninterpretable and potentially overfitting the facts distribution. It suggests that future work should consider adapting prompt optimization methods to produce more interpretable prompts.
- Why unresolved: While the paper acknowledges the problem of uninterpretable prompts in optimization methods, it does not provide a specific solution or framework for achieving interpretability without sacrificing performance.
- What evidence would resolve it: A successful adaptation of prompt optimization methods that demonstrably produces more interpretable prompts while maintaining or improving performance on factual knowledge probing tasks would resolve this question.

### Open Question 2
- Question: What factors contribute most significantly to the consistency of PLMs as knowledge bases across different languages?
- Basis in paper: [explicit] The paper highlights the challenge of PLMs' sensitivity to input queries and their high sensitivity to prompts, especially in languages other than English. It suggests that making PLMs more robust to prompts in non-English languages is a promising future work direction.
- Why unresolved: The paper identifies the problem of inconsistency across languages but does not delve into specific factors that contribute to this issue or propose methods to address it comprehensively.
- What evidence would resolve it: A comprehensive study identifying the key factors affecting PLMs' consistency across different languages and proposing effective methods to improve this consistency would resolve this question.

### Open Question 3
- Question: How can knowledge updating methods for PLMs be made more scalable and targeted to avoid affecting unintended facts?
- Basis in paper: [explicit] The paper discusses the challenge of updating knowledge in PLMs and mentions that current methods for facts editing still do not fulfill the requirements of only affecting targeted facts and making them retrievable using different paraphrases. It suggests that methods introducing additional parameters should be made more scalable.
- Why unresolved: While the paper acknowledges the limitations of current knowledge updating methods, it does not provide a detailed solution or framework for achieving scalability and targeted updates without unintended side effects.
- What evidence would resolve it: A successful development of knowledge updating methods that are both scalable and capable of making targeted updates without affecting unintended facts would resolve this question.

## Limitations

- Findings are primarily based on correlation analysis across literature rather than controlled experiments
- The survey cannot definitively establish causal relationships between design choices and knowledge retention
- Effectiveness of prompt optimization is assessed through reported performance improvements, but underlying mechanisms remain unclear

## Confidence

- **High confidence**: The categorization scheme's validity and comprehensiveness (based on systematic analysis of 94 papers)
- **Medium confidence**: The relative importance of model architecture and pre-training objectives over size for knowledge retention (based on multiple supporting studies)
- **Medium confidence**: The effectiveness of prompt optimization in improving probing performance (though mechanism remains unclear)

## Next Checks

1. Conduct controlled ablation studies comparing models with identical sizes but different architectures/objectives to quantify their relative contributions to knowledge retention
2. Design experiments using randomly initialized models with optimized prompts to distinguish between genuine knowledge retrieval and alignment-based performance gains
3. Implement a standardized benchmark suite testing prompt sensitivity across semantically equivalent formulations to measure reliability and generalizability of knowledge probing methods