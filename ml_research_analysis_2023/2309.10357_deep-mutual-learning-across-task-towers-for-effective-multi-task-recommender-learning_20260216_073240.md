---
ver: rpa2
title: Deep Mutual Learning across Task Towers for Effective Multi-Task Recommender
  Learning
arxiv_id: '2309.10357'
source_url: https://arxiv.org/abs/2309.10357
tags:
- task
- tasks
- learning
- multi-task
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses improving multi-task learning in recommender
  systems by enhancing the architecture of standalone task towers, which are argued
  to be suboptimal for promoting positive knowledge sharing. The proposed Deep Mutual
  Learning (DML) framework introduces Cross Task Feature Mining (CTFM) and Global
  Knowledge Distillation (GKD) components.
---

# Deep Mutual Learning across Task Towers for Effective Multi-Task Recommender Learning

## Quick Facts
- arXiv ID: 2309.10357
- Source URL: https://arxiv.org/abs/2309.10357
- Reference count: 29
- Primary result: The paper proposes a Deep Mutual Learning framework with Cross Task Feature Mining and Global Knowledge Distillation components that significantly improve multi-task learning performance in recommender systems across various architectures and datasets.

## Executive Summary
This paper addresses the limitations of standalone task towers in multi-task recommender systems by proposing a Deep Mutual Learning (DML) framework. The authors argue that current multi-task learning architectures fail to effectively promote positive knowledge sharing between tasks while preserving task-specific information. To address this, they introduce two key components: Cross Task Feature Mining (CTFM) for extracting relevant cross-task information while maintaining task-specific knowledge, and Global Knowledge Distillation (GKD) for ensuring prediction consistency across related tasks. Extensive experiments on MovieLens-1M and Amazon Electronics datasets demonstrate significant performance improvements in AUC and MSE metrics, with successful online A/B testing showing gains in effective PV and total watch time.

## Method Summary
The Deep Mutual Learning framework introduces two novel components to enhance multi-task learning in recommender systems. CTFM uses a modified attention mechanism to extract relevant information from other task towers while blocking gradients to preserve task-specific knowledge. GKD distills global knowledge from all task towers to ensure prediction consistency across related tasks. The framework is designed to be compatible with various backbone multi-task models including Shared-Bottom, MSSM, MMOE, and PLE. Training uses Adam optimizer with learning rate 0.001 and batch size 512, with equal task loss weights. The architecture processes lower-level shared features through CTFM, task-specific MLPs, and GKD modules before producing final predictions.

## Key Results
- Significant improvements in AUC and MSE metrics across multiple multi-task models and datasets
- Successful online A/B testing demonstrating gains in effective PV and total watch time
- Framework shows compatibility and consistent performance improvements across various backbone architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross Task Feature Mining (CTFM) solves the "task awareness missing" problem by selectively extracting relevant information from other task towers while preventing gradients from propagating back to their inputs.
- Mechanism: CTFM uses a modified attention mechanism where gradients from the key and value transformations are blocked, ensuring that each task tower's loss does not affect the inputs of other task towers. This preserves explicit task-specific knowledge while still allowing relevant cross-task information to be incorporated.
- Core assumption: The attention mechanism's design with gradient blocking maintains task-specific knowledge integrity while enabling effective cross-task information sharing.
- Evidence anchors:
  - [abstract] "CTFM uses attention mechanisms to extract relevant information from other tasks while preserving task-specific knowledge"
  - [section] "Our design can ensure appropriate information separation... reserving explicit task-specific knowledge has a positive effect on performance"
- Break condition: If gradient blocking is removed, the task towers lose task-specific knowledge preservation, leading to degraded performance.

### Mechanism 2
- Claim: Global Knowledge Distillation (GKD) ensures prediction consistency across related tasks by distilling global knowledge from all task towers.
- Mechanism: GKD processes hidden representations from all tasks, extracts global knowledge using an MLP with gradient blocking to prevent affecting other tasks' hidden representations, and then uses a weighted combination of this global knowledge with the task's own hidden representation to make predictions.
- Core assumption: Related tasks in recommender systems should have predictions that follow a joint distribution, and GKD helps ensure predictions don't fall into low-density areas of this distribution.
- Evidence anchors:
  - [abstract] "GKD distills global knowledge from upper layer task towers to enhance prediction consistency"
  - [section] "the training labels of multiple tasks should obey a joint distribution... the prediction results for these tasks should not densely fall into the low-density areas"
- Break condition: If task relationships are weak or uncorrelated, the benefit of GKD diminishes or becomes negative.

### Mechanism 3
- Claim: DML is compatible with various backbone multi-task models and provides consistent performance improvements across different architectures.
- Mechanism: DML's components (CTFM and GKD) are designed to be modular and can be added to existing multi-task learning architectures without requiring architectural changes to the base model.
- Core assumption: The improvements from CTFM and GKD are additive and beneficial regardless of the specific parameter sharing strategy used in the base model.
- Evidence anchors:
  - [abstract] "DML framework introduces CTFM and GKD components... Extensive offline experiments and online A/B tests show significant performance improvements across various multi-task models"
  - [section] "Extensive offline experiments and online A/B tests are conducted to evaluate and verify the proposed approach's effectiveness"
  - [corpus] "Found 25 related papers... Top related titles: Multi-Task Deep Recommender Systems: A Survey" - indicates active research area but limited direct evidence in corpus
- Break condition: If the base model already incorporates similar cross-task knowledge sharing mechanisms, the marginal benefit of DML may be reduced.

## Foundational Learning

- Concept: Multi-task learning parameter sharing strategies
  - Why needed here: Understanding the difference between hard sharing (like Shared-Bottom), soft sharing, customized routing, and dynamic gating is crucial to appreciate why standalone task towers are suboptimal and how DML improves upon them.
  - Quick check question: What is the main limitation of hard parameter sharing that DML addresses?

- Concept: Attention mechanisms and gradient flow
  - Why needed here: CTFM's effectiveness relies on understanding how attention mechanisms work and how gradient blocking can be used to preserve task-specific information while still enabling cross-task learning.
  - Quick check question: How does gradient blocking in CTFM differ from standard attention mechanisms?

- Concept: Knowledge distillation
  - Why needed here: GKD applies knowledge distillation principles to multi-task learning, requiring understanding of how knowledge can be distilled from multiple sources and weighted appropriately.
  - Quick check question: What is the purpose of using a weighted combination of global knowledge and task-specific knowledge in GKD?

## Architecture Onboarding

- Component map: Input features → Embedding layer → Lower-level shared networks (G) → CTFM module → Task-specific MLPs (H) → GKD modules → Final predictions
- Critical path: Lower-level networks → CTFM → Task-specific processing → GKD → Output
- Design tradeoffs:
  - CTFM vs. standard attention: Better task-specific knowledge preservation but potentially less cross-task information sharing
  - GKD complexity vs. benefit: Additional parameters and computation for improved consistency
  - Gradient blocking: Preserves task-specific knowledge but may limit some potential cross-task learning
- Failure signatures:
  - Performance degradation if gradient blocking is improperly implemented
  - Negative transfer if task relationships are incorrectly modeled
  - Increased training time and memory usage due to additional components
- First 3 experiments:
  1. Implement CTFM alone with a base model (e.g., Shared-Bottom) and compare to the base model without CTFM
  2. Implement GKD alone with a base model and compare to the base model without GKD
  3. Implement full DML with multiple base models (e.g., MMOE, PLE) and verify performance improvements across all models

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the proposed Deep Mutual Learning framework perform when applied to multi-task learning problems with a larger number of tasks (e.g., more than 5 tasks)?
  - Basis in paper: [inferred] The paper demonstrates the effectiveness of DML on datasets with 2 tasks (MovieLens-1M) and 2 tasks (Electronics). It is unclear how the framework scales with a larger number of tasks.
  - Why unresolved: The paper does not provide experiments or analysis on datasets with more than 2 tasks, making it difficult to assess the scalability of the framework.
  - What evidence would resolve it: Experiments on datasets with a larger number of tasks, comparing the performance of DML with other multi-task learning methods, would provide evidence for the scalability of the framework.

- **Open Question 2**: How does the performance of DML change when using different base models, such as transformer-based architectures or graph neural networks?
  - Basis in paper: [inferred] The paper evaluates DML on top of four base models (Shared-Bottom, MSSM, MMOE, and PLE). It is unclear how the framework performs with other base models, such as transformer-based architectures or graph neural networks.
  - Why unresolved: The paper does not provide experiments or analysis on different base models, making it difficult to assess the generalizability of the framework.
  - What evidence would resolve it: Experiments on different base models, comparing the performance of DML with other multi-task learning methods, would provide evidence for the generalizability of the framework.

- **Open Question 3**: How does the performance of DML change when applied to multi-task learning problems with tasks that have different data distributions or feature spaces?
  - Basis in paper: [inferred] The paper demonstrates the effectiveness of DML on datasets with tasks that have similar data distributions and feature spaces. It is unclear how the framework performs when the tasks have different data distributions or feature spaces.
  - Why unresolved: The paper does not provide experiments or analysis on tasks with different data distributions or feature spaces, making it difficult to assess the robustness of the framework.
  - What evidence would resolve it: Experiments on tasks with different data distributions or feature spaces, comparing the performance of DML with other multi-task learning methods, would provide evidence for the robustness of the framework.

## Limitations

- The precise implementation details of gradient-blocking mechanisms in CTFM and GKD are not fully specified, creating potential reproducibility challenges
- The framework's performance on tasks with weak or uncorrelated relationships may be limited, as GKD benefits depend on task correlations
- The paper lacks experiments on multi-task problems with more than 2 tasks, limiting scalability analysis

## Confidence

- **High Confidence**: The theoretical framework for improving cross-task information sharing through CTFM and prediction consistency through GKD is well-founded. The core hypothesis that standalone task towers are suboptimal for multi-task learning is supported by established literature.
- **Medium Confidence**: The empirical results showing performance improvements across multiple datasets and base models are convincing, though the magnitude of improvements may vary depending on implementation details and specific task relationships.
- **Low Confidence**: The claim of universal compatibility with all multi-task architectures without architectural modifications is the least certain, as different architectures may require different integration approaches.

## Next Checks

1. **Gradient Blocking Verification**: Implement and test different gradient blocking strategies in CTFM to verify which approach best preserves task-specific knowledge while enabling effective cross-task learning. Compare attention weight distributions and task performance metrics across implementations.

2. **Task Relationship Analysis**: Conduct experiments to quantify the strength of relationships between different tasks in recommender systems and measure how this affects the benefit derived from GKD. Use correlation analysis and ablation studies to understand when GKD provides the most value.

3. **Architectural Integration Testing**: Systematically test DML integration with a broader range of multi-task architectures beyond the four mentioned, including those with different parameter sharing strategies and dynamic routing mechanisms. Document any architectural modifications required for optimal performance.