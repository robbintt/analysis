---
ver: rpa2
title: Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic
arxiv_id: '2308.07336'
source_url: https://arxiv.org/abs/2308.07336
tags:
- deduction
- corpus
- corpora
- reasoning
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether language models can be trained
  to perform deductive reasoning using synthetic corpora based on formal logic. Prior
  work used limited or arbitrary deduction rules, restricting generalizability.
---

# Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic

## Quick Facts
- **arXiv ID**: 2308.07336
- **Source URL**: https://arxiv.org/abs/2308.07336
- **Reference count**: 40
- **Primary result**: Language models trained on synthetic corpora using first-order predicate logic axioms outperform baselines on both synthetic deduction tasks and EntailmentBank benchmark

## Executive Summary
This study investigates whether language models can be trained to perform deductive reasoning using synthetic corpora based on formal logic. Prior work used limited or arbitrary deduction rules, restricting generalizability. The authors propose a framework, FLD (Formal Logic Deduction), which generates synthetic corpora using the axioms of first-order predicate logic. These axioms can derive any valid deduction rule, making them more generalizable. Experiments show that language models trained on FLD corpora outperform baselines on both synthetic deduction tasks and the human-authored EntailmentBank benchmark. Analysis reveals that FLD corpora improve understanding of complex formulas and robustness to distractive facts, but struggle with very deep proof trees. The authors discuss future directions for improving deductive reasoning in language models, including incorporating more complex logical systems and developing better search strategies for long proofs.

## Method Summary
The FLD framework generates synthetic deductive reasoning corpora using the axioms of first-order predicate logic. The framework consists of four modules: Proof Tree Generator creates synthetic proof trees, Factual Distractor Generator adds relevant and irrelevant facts, Natural Language Assigner assigns diverse linguistic expressions to logical formulas, and Deduction Instance Converter assembles the final instances. The authors train T5-based provers on these corpora and evaluate their performance on both synthetic deduction tasks and the EntailmentBank benchmark using proof accuracy and answer accuracy metrics.

## Key Results
- Language models trained on FLD corpora show strong few-shot transfer to both synthetic deduction tasks and EntailmentBank
- FLD improves model understanding of complex formulas and robustness to distractive facts
- Provers struggle with very deep proof trees despite improved performance on shallower proofs
- FLD outperforms baseline approaches including RuleTaker and AACorpus on multiple evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: FLD's use of first-order predicate logic axioms enables better generalization across deduction rules than previous corpora.
- **Mechanism**: The completeness property of first-order predicate logic means any valid deduction rule can be derived through multistep deductions from the axioms. Training on FLD exposes the model to this foundational system, teaching it how to construct multistep deductions from atomic rules.
- **Core assumption**: The model learns to compose axioms into more complex deduction rules rather than memorizing specific rule patterns.
- **Evidence anchors**:
  - [abstract]: "The axioms of first-order predicate logic... can derive any other deduction rules when combined in a multistep way."
  - [section 2]: "Thanks to the completeness, all valid arguments can be derived in this way, and all (infinite) arguments derived in this way are valid."
  - [corpus]: Weak - the paper shows improved transfer but doesn't directly prove the model learned composition rather than pattern matching.
- **Break condition**: If the model simply memorizes specific axiom combinations without understanding how to compose them, it won't generalize to unseen deduction rules.

### Mechanism 2
- **Claim**: Random generation of diverse linguistic expressions helps models learn to understand varied ways of expressing logical statements.
- **Mechanism**: FLD assigns natural language statements to atomic components randomly from a large vocabulary (20k words), creating combinatorially diverse linguistic patterns that expose the model to multiple ways of expressing the same logical relationship.
- **Core assumption**: Exposure to diverse linguistic patterns during training enables better generalization to novel linguistic expressions during inference.
- **Evidence anchors**:
  - [abstract]: "These random and diverse statements constructed from a large vocabulary (about 20k words) are another major difference from the previous studies."
  - [section 3.3]: "We assign one natural language sequence to each formula of tree nodes and of distractors... random statement constructed (under a certain grammatical constraint) from a full vocabulary to each atomic component."
  - [corpus]: Moderate - the paper shows similar performance between linguistically diverse and less diverse corpora, suggesting LMs are self-sufficient on linguistic aspects.
- **Break condition**: If the model relies too heavily on specific linguistic patterns seen during training, it may fail to understand novel expressions.

### Mechanism 3
- **Claim**: Synthetic distractors improve model robustness to irrelevant information in real-world reasoning tasks.
- **Mechanism**: FLD adds distractor facts that are logically similar to gold facts, forcing the model to distinguish relevant from irrelevant information during proof construction.
- **Core assumption**: Training with synthetic distractors teaches the model to filter out irrelevant information, improving performance on tasks with real-world noise.
- **Evidence anchors**:
  - [abstract]: "We add distractor facts to each deduction instance... The distractor facts are formulas that are similar to the gold facts in their logical form."
  - [section 7.4]: "while the prover trained on the corpus without distractors (FLD.0) performed poorly on the corpus with distractors (FLD.2), the prover trained on FLD.2 performed well on both corpora."
  - [corpus]: Strong - clear evidence that training with distractors improves performance on tasks with distractors.
- **Break condition**: If distractors are too dissimilar from real-world noise, the learned robustness may not transfer effectively.

## Foundational Learning

- **Concept: First-order predicate logic and its axioms**
  - Why needed here: Understanding the axioms and their completeness property is essential for grasping why FLD's approach should generalize better than previous methods.
  - Quick check question: Can you explain why the axioms of first-order predicate logic can derive any other valid deduction rule?

- **Concept: Proof tree generation and depth**
  - Why needed here: Understanding how proof trees are constructed and how depth affects complexity is crucial for interpreting the experimental results and limitations.
  - Quick check question: How does the number of possible proof tree patterns grow with proof tree depth and the number of available arguments?

- **Concept: Multistep deduction and argument composition**
  - Why needed here: Understanding how individual arguments can be combined into multistep deductions is key to understanding FLD's mechanism for teaching generalization.
  - Quick check question: Can you describe how a syllogism can be derived through multistep deduction using modus ponens and conjunction elimination?

## Architecture Onboarding

- **Component map**: Proof Tree Generator -> Factual Distractor Generator -> Natural Language Assigner -> Deduction Instance Converter

- **Critical path**: The most critical components for generating effective training data are the Proof Tree Generator (which determines the logical structure) and the Factual Distractor Generator (which determines the difficulty and realism of the instances).

- **Design tradeoffs**: The framework trades off between generating highly realistic instances (closer to human-authored proofs) and maintaining the ability to generate large amounts of controlled training data. The use of random generation and template-based assignment prioritizes scalability over perfect realism.

- **Failure signatures**: If generated proofs contain contradictions, if distractors are too obvious or too subtle, or if the linguistic diversity is insufficient, the training data may not effectively teach the desired reasoning abilities.

- **First 3 experiments**:
  1. Generate a small corpus using only modus ponens and conjunction elimination axioms, then train a model and test its ability to solve simple proofs.
  2. Add synthetic distractors to the same corpus and measure whether this improves the model's ability to handle irrelevant information.
  3. Increase the proof tree depth and measure how this affects model performance, particularly on longer proofs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can language models be trained to effectively solve complex proof trees using the axioms of first-order predicate logic?
- **Basis in paper**: [explicit] The paper states that even after training on deduction corpora, LMs struggle with constructing many-step proofs using the axioms, as discussed in Sections 6.2 and 7.1.
- **Why unresolved**: The paper suggests that the challenge lies in choosing and generating the exact gold proof tree from a large number of possible trees, especially when using the axioms which provide a large number of argument choices.
- **What evidence would resolve it**: Experiments showing successful training of LMs to solve complex proof trees using the axioms, possibly by developing smarter and strategic search methods on the generation space.

### Open Question 2
- **Question**: What are the most effective approaches for improving LMs' understanding of the semantics of logical operators such as negation (¬), conjunction (∧), and disjunction (∨)?
- **Basis in paper**: [inferred] The paper shows that deduction corpora are beneficial for mastering complex formulas and understanding logical operators, but also mentions that LMs may need additional training to utilize deduction rules well in a realistic context, as discussed in Section 8.
- **Why unresolved**: While the paper suggests incorporating recent learning methodological approaches for making LMs understand negation into the learning on deduction corpora, it does not provide concrete evidence of the effectiveness of such approaches.
- **What evidence would resolve it**: Experiments demonstrating improved performance of LMs on tasks involving logical operators after incorporating specific learning methodological approaches.

### Open Question 3
- **Question**: How can language models be trained to effectively combine deduction rules with common sense knowledge and judge the validity of a proof step considering the overall context?
- **Basis in paper**: [inferred] The paper suggests that LMs may need additional training to utilize deduction rules well in a realistic context, including combining deduction rules with common sense knowledge and judging the validity of a proof step considering the overall context, as discussed in Section 8.
- **Why unresolved**: The paper does not provide concrete methods or evidence for training LMs to effectively combine deduction rules with common sense knowledge and judge the validity of proof steps.
- **What evidence would resolve it**: Experiments showing improved performance of LMs on real-world deductive reasoning tasks after training on datasets that include common sense knowledge and context-aware proof validation.

## Limitations

- **Depth Limitation**: The framework struggles with deep proof trees, as evidenced by significantly lower accuracy on 3-step proofs compared to 1-step proofs.
- **Generalization Gap**: While FLD shows improved performance on synthetic tasks and EntailmentBank, the paper acknowledges that synthetic proofs are "very simple and quite different from the real-world proofs" in EntailmentBank.
- **Linguistic Diversity Impact**: The paper notes that linguistically diverse corpora showed similar performance to less diverse versions, suggesting that the computational cost of generating diverse linguistic expressions may not be justified by performance gains.

## Confidence

- **High Confidence**: The core finding that FLD corpora outperform baselines on both synthetic and human-authored datasets is well-supported by experimental results across multiple metrics.
- **Medium Confidence**: The claim that first-order predicate logic axioms enable better generalization is supported by transfer learning results but lacks direct mechanistic evidence.
- **Low Confidence**: The assertion that synthetic distractors significantly improve real-world robustness is based primarily on in-domain performance differences rather than demonstrated transfer to truly noisy real-world data.

## Next Checks

1. **Ablation Study on Proof Depth**: Generate FLD corpora with controlled depth distributions and measure how performance scales with depth. Specifically, test whether performance on 3-step proofs can be improved through targeted training or architectural modifications.

2. **Cross-Domain Distractor Transfer**: Test whether models trained with FLD distractors show improved performance on datasets with naturally occurring irrelevant information (e.g., common sense reasoning benchmarks with distractor facts) versus models trained without synthetic distractors.

3. **Axiom Composition Analysis**: Design a diagnostic task where models must apply novel combinations of axioms not seen during training. Compare performance between models trained on FLD versus models trained on corpora with predefined deduction rules to test whether FLD truly teaches composition rather than pattern matching.