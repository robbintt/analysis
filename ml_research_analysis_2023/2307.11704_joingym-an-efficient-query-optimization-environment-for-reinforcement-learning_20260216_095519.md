---
ver: rpa2
title: 'JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning'
arxiv_id: '2307.11704'
source_url: https://arxiv.org/abs/2307.11704
tags:
- query
- join
- learning
- optimization
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: JoinGym is an offline RL environment for database query optimization
  that simulates join order selection using pre-computed cardinality traces. The environment
  supports both left-deep and bushy join plans, and includes a dataset of 3300 novel
  SQL queries based on real IMDb workloads.
---

# JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.11704
- Source URL: https://arxiv.org/abs/2307.11704
- Reference count: 20
- Primary result: RL algorithms achieve near-optimal performance on training queries but degrade by orders of magnitude on test queries in database join optimization

## Executive Summary
JoinGym is an offline reinforcement learning environment for database query optimization that simulates join order selection using pre-computed cardinality traces. By replacing live query execution with table lookups into a static dataset of intermediate result cardinalities, JoinGym enables rapid prototyping of RL algorithms without requiring live database setup. The environment supports both left-deep and bushy join plans and includes a dataset of 3300 novel SQL queries based on real IMDb workloads.

## Method Summary
JoinGym formulates join order selection as a Contextual Markov Decision Process (CMDP) where the agent selects join actions to minimize cumulative execution cost. The environment uses pre-computed cardinality traces to simulate query plan costs through offline lookup rather than live execution. State representations encode partial join plans compactly, and action masking ensures only valid joins are selectable at each step. Four RL algorithms (DQN, TD3, SAC, PPO) were benchmarked, revealing significant generalization gaps between training and test query performance.

## Key Results
- RL algorithms achieve near-optimal performance on training queries with cost multiples close to 1.0
- Performance degrades by several orders of magnitude on test queries, with heavy-tailed cost distributions
- JoinGym achieves 10x higher throughput than existing RL environments for database query optimization
- Offline RL approaches show significant generalization gaps between training and test query sets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Offline simulation via pre-computed cardinality traces enables orders-of-magnitude faster RL experimentation.
- **Mechanism:** The environment replaces live query execution with table lookups into a static dataset of all intermediate result cardinalities. Each trajectory step requires only a constant-time dictionary lookup rather than launching a DBMS and waiting for execution.
- **Core assumption:** Cardinality is a sufficiently accurate proxy for execution cost, so that optimizing for cardinality yields near-optimal query plans.
- **Evidence anchors:** [abstract] "simulates a query plan's cost by looking up intermediate result cardinalities from a pre-computed dataset." [section 4] "JOIN GYM is completely based on offline traces of all possible joins... simulates a query plan's cost by looking up intermediate result cardinalities."
- **Break condition:** If cardinality no longer correlates with runtime (e.g., due to I/O bottlenecks or non-uniform data distribution), the simulation loses fidelity and RL policies optimized here degrade in real deployment.

### Mechanism 2
- **Claim:** The CMDP formulation with partial plan encoding enables efficient RL training for join order selection.
- **Mechanism:** The state encodes only which columns have been joined/staged (via vpp), not the full tree structure, because future costs depend only on current IRs. This compact representation reduces the state space while preserving all information needed for optimal decision-making.
- **Core assumption:** The transition dynamics are deterministic given the query context and prior actions, so a policy need only track joined columns, not the join tree.
- **Evidence anchors:** [section 4.2] "the partial plan encoding only logs which columns have been joined/staged rather than the current join tree, because future costs only depend on the current IRs rather than the tree structure." [section 4] MDP components table shows deterministic transitions.
- **Break condition:** If the environment is extended to include non-deterministic costs (e.g., due to runtime data skew), the deterministic assumption fails and the partial plan encoding becomes insufficient.

### Mechanism 3
- **Claim:** Heavy-tailed cost distributions on test queries reveal generalization gaps that motivate risk-sensitive RL.
- **Mechanism:** While RL algorithms achieve near-optimal training performance, their cost multiples on unseen test queries are orders of magnitude worse, indicating that learned policies overfit to training query templates and fail to generalize to new selectivity patterns.
- **Core assumption:** Query templates share structural similarity but vary in optimal join orders due to different selectivities; thus, good generalization requires capturing selectivity-aware plan selection.
- **Evidence anchors:** [abstract] "While algorithms achieve near-optimal performance on training queries, their performance degrades significantly on test queries—often by several orders of magnitude." [section 5] Benchmark results show large train-test gaps, especially in the new dataset.
- **Break condition:** If the test set is not truly out-of-distribution (e.g., same selectivity patterns), the observed heavy tail may be due to other factors such as noise in cardinality estimation.

## Foundational Learning

- **Concept:** Markov Decision Process (MDP) formulation of sequential decision problems
  - Why needed here: Join order selection is naturally modeled as a sequence of join actions with cumulative costs, matching the MDP structure.
  - Quick check question: In the CMDP, what is the role of the context x, and how does it differ from the state sh?

- **Concept:** Action masking in RL for combinatorial spaces
  - Why needed here: At each step only a subset of joins are valid (e.g., left-deep requires joining the next table to the existing plan), so invalid actions must be excluded from the policy's output.
  - Quick check question: How does action masking prevent the agent from selecting already-joined tables or edges?

- **Concept:** Generalization in multi-task RL
  - Why needed here: Each query is a separate task; the environment tests whether a single policy can perform well across diverse query templates and selectivity settings.
  - Quick check question: Why might a policy that is optimal on training templates perform poorly on test templates with similar structure but different selectivities?

## Architecture Onboarding

- **Component map:** Query encoding module -> State encoder -> Simulator core -> RL interface -> Dataset loader
- **Critical path:**
  1. env.reset(query_id) → load context x and initialize partial plan encoding
  2. env.step(action) → lookup cardinality for action given current state, update encoding, return reward
  3. Episode ends when all tables joined; return cumulative cost multiple
- **Design tradeoffs:**
  - Offline vs online execution: offline trades fidelity for speed; relies on cardinality proxy assumption
  - Partial plan encoding vs full tree: compact state reduces learning complexity but assumes deterministic costs
  - Left-deep vs bushy: bushy increases search space and expressiveness but requires more complex action space
- **Failure signatures:**
  - Very slow step() calls: likely lookup table missing or corrupted
  - NaNs or infinite rewards: invalid action chosen despite masking, or cardinality overflow
  - Training collapse: state/action representation mismatch between env and policy network
- **First 3 experiments:**
  1. Verify environment step timing: measure queries per second on a small synthetic query set
  2. Test action masking: attempt to take an invalid action and confirm it is rejected or masked
  3. Check generalization: train on 10 queries from one template, evaluate on 10 from a different template, report cost multiple gap

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does incorporating table data distributions into the state representation significantly improve generalization across query templates?
- **Basis in paper:** [inferred] The authors note that "our CMDP is strictly speaking a partially observable MDP (POMDP) because the state contains no information about the data of each table" and suggest that "developing better table encodings...can resolve the partial observability issues."
- **Why unresolved:** The current environment does not include table data distributions in the state representation, and the authors did not experiment with such encodings.
- **What evidence would resolve it:** Experimental results comparing RL performance with and without table data distributions in the state representation across various query templates.

### Open Question 2
- **Question:** Which specific risk-sensitive RL methods are most effective for mitigating the heavy-tailed cost distributions observed in query optimization?
- **Basis in paper:** [explicit] The authors note that "practitioners may care more about the quantile or conditional value-at-risk (CVaR) of the cost multiple" and suggest "applying and developing risk-sensitive RL methods...to system applications can be a promising direction."
- **Why unresolved:** The paper only mentions risk-sensitive RL as a future direction without testing specific methods or comparing their effectiveness.
- **What evidence would resolve it:** Comparative experimental results of different risk-sensitive RL algorithms (e.g., CVaR-based, pessimistic value functions) on the JoinGym environment.

### Open Question 3
- **Question:** How does the performance gap between validation and test sets change when using offline RL algorithms versus online RL algorithms?
- **Basis in paper:** [explicit] The authors state that "we find that generalization is worse in our new dataset" and note a performance gap, but they only benchmark offline RL on JOB data in the appendix.
- **Why unresolved:** The paper does not provide direct comparisons of validation-test generalization gaps between online and offline RL methods on the new dataset.
- **What evidence would resolve it:** Direct experimental comparison of online vs. offline RL algorithms showing validation-test performance gaps on both JOB and the new dataset.

## Limitations
- The cardinality-as-cost proxy assumption remains unvalidated against real execution times, creating uncertainty about policy transferability to live systems
- Heavy-tailed generalization performance suggests fundamental limitations in current RL approaches for this problem
- Offline-only setting prevents evaluation of online learning benefits or adaptation to changing data distributions

## Confidence
- Mechanism 1 (Offline simulation speedup): High - direct evidence from implementation details
- Mechanism 2 (CMDP formulation): High - well-specified MDP components with deterministic transitions
- Mechanism 3 (Generalization gaps): Medium - empirical results shown but root causes not fully analyzed

## Next Checks
1. Benchmark JoinGym policies against actual DBMS execution times on a subset of queries to quantify the cardinality proxy fidelity gap
2. Analyze correlation between training query template similarity and test generalization performance to identify overfitting patterns
3. Implement and evaluate risk-sensitive RL variants (e.g., CVaR optimization) to address heavy-tailed cost distributions on test queries