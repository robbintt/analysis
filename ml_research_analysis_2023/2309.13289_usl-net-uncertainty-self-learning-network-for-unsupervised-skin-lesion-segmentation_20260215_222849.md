---
ver: rpa2
title: 'USL-Net: Uncertainty Self-Learning Network for Unsupervised Skin Lesion Segmentation'
arxiv_id: '2309.13289'
source_url: https://arxiv.org/abs/2309.13289
tags:
- segmentation
- learning
- lesion
- skin
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an uncertainty self-learning network (USL-Net)
  for unsupervised skin lesion segmentation. The key idea is to use contrastive learning
  to extract image features, then generate class activation maps (CAMs) as saliency
  maps to guide pseudo-label generation.
---

# USL-Net: Uncertainty Self-Learning Network for Unsupervised Skin Lesion Segmentation

## Quick Facts
- arXiv ID: 2309.13289
- Source URL: https://arxiv.org/abs/2309.13289
- Reference count: 40
- Key outcome: USL-Net achieves accuracy of 90.5%, Dice coefficient of 80.5%, Jaccard index of 68.5%, sensitivity of 88.6%, and specificity of 93.7% on ISIC-2017 dataset

## Executive Summary
This paper introduces USL-Net, an unsupervised skin lesion segmentation framework that leverages contrastive learning and uncertainty self-learning to achieve performance comparable to supervised methods. The key innovation is treating medium-saliency regions as uncertainty regions rather than forcing hard pseudo-labels, allowing the network to self-learn these regions while focusing on confident areas. Experimental results on ISIC-2017, ISIC-2018, and PH2 datasets demonstrate that USL-Net outperforms existing unsupervised methods and achieves competitive performance with weakly supervised approaches.

## Method Summary
USL-Net employs a multi-stage pipeline starting with contrastive learning (MoCov2) to extract semantic features from dermoscopic images without manual labels. These features are then used to generate class activation maps (CAMs) as saliency maps through the CCAM method. The uncertainty module treats low-saliency regions as background, high-saliency regions as foreground, and intermediate-saliency regions (0.35-0.65) as uncertainty regions. Connectivity and centrality detection refine foreground pseudo-labels while reducing noise-induced errors. A cycle-refining process iteratively improves pseudo-label quality over 5 cycles of 300 epochs each, with each cycle's segmentation results generating better pseudo-labels for the next iteration.

## Key Results
- Achieves 90.5% accuracy, 80.5% Dice coefficient, and 68.5% Jaccard index on ISIC-2017
- Outperforms existing unsupervised methods on all three tested datasets (ISIC-2017, ISIC-2018, PH2)
- Demonstrates comparable performance to weakly supervised methods while maintaining true unsupervised training
- Shows effectiveness of uncertainty handling and cycle-refining mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: USL-Net improves unsupervised segmentation by treating medium-saliency regions as "uncertainty regions" that the network learns to classify autonomously rather than forcing hard pseudo-labels.
- Mechanism: During pseudo-label generation, low-saliency regions are assigned as background, high-saliency regions as foreground, and intermediate-saliency regions are left unlabeled. The uncertainty loss function guides the network to focus on confident regions while self-learning the uncertain ones.
- Core assumption: The network can effectively learn to classify uncertain regions through context without explicit supervision, and avoiding forced pseudo-labels reduces learning confusion.
- Evidence anchors:
  - [abstract] "intermediate regions can be hard to classify... Rather than risk potential pseudo-labeling errors or learning confusion by forcefully classifying these regions, we consider them as uncertainty regions, exempting them from pseudo-labeling and allowing the network to self-learn."
  - [section] "Ratherthanriskpotentialpseudo-labelingerrorsorlearningconfusionbyforcefullyclassifyingthese regions,weconsiderthemasuncertaintyregions,exemptingthemfrompseudo-labelingandallowing thenetworktoself-learn."
- Break condition: If the network cannot learn meaningful classifications for uncertain regions through context alone, or if uncertainty regions are too prevalent to allow sufficient learning signal from confident regions.

### Mechanism 2
- Claim: Using contrastive learning for feature extraction provides robust semantic features that improve pseudo-label quality compared to direct image-based methods.
- Mechanism: Contrastive learning extracts image features that capture semantic similarity between positive and negative sample pairs without requiring manual labels. These features are then used to generate class activation maps (CAMs) that serve as saliency maps for pseudo-label generation.
- Core assumption: Contrastive learning features contain sufficient semantic information to distinguish foreground from background in dermoscopic images with artifacts like hair and blisters.
- Evidence anchors:
  - [abstract] "Initially, features are extracted using contrastive learning, followed by the generation of Class Activation Maps (CAMs) as saliency maps using these features."
  - [section] "In the context ofAI-assisteddermatology, skin lesion image segmentation is a fundamental technique... The progress in deep learning, particularly the evolution of convolutional neural networks (CNNs), has greatlyenhancedimagesegmentationcapabilities."
- Break condition: If contrastive learning features fail to capture sufficient semantic information for dermoscopic images, or if the feature space is too noisy due to image artifacts.

### Mechanism 3
- Claim: The cycle-refining process iteratively improves pseudo-label quality by using the current segmentation results as input for generating better pseudo-labels in subsequent cycles.
- Mechanism: After initial segmentation, the results are fed back through the uncertainty module to create new pseudo-labels. This process repeats for multiple cycles (typically 5), with each iteration producing better quality pseudo-labels that guide more accurate segmentation.
- Core assumption: Each cycle's segmentation results contain enough improvement over the previous cycle to generate meaningfully better pseudo-labels, and the improvement compounds over multiple cycles.
- Evidence anchors:
  - [section] "We introduceacycle-refiningmethodthatutilizes the current learning results to generate the next pseudo-label iteratively, further enhancing the overall quality of the pseudo-labels."
  - [section] "Cycle-Refining: We have designed a cycle-refining module. In this module, we defineùëô1, ùëô2, ùëôùëõ‚àí1 to refer to the pseudo-label generated by the 1st, 2nd, (n-1)th cycle, and lo to be the initial label defined earlier."
- Break condition: If segmentation results plateau or degrade after initial cycles, or if the uncertainty module cannot effectively filter improved pseudo-labels in later cycles.

## Foundational Learning

- Concept: Contrastive learning and feature extraction
  - Why needed here: USL-Net relies on contrastive learning to extract semantic features from dermoscopic images without manual labels. Understanding how contrastive learning works is essential to grasp how the network generates reliable pseudo-labels.
  - Quick check question: What is the key difference between contrastive learning and supervised learning in terms of label requirements?

- Concept: Class Activation Maps (CAMs) and saliency detection
  - Why needed here: The quality of pseudo-labels depends on generating accurate saliency maps using CAMs. Understanding how CAMs highlight important regions helps explain why USL-Net can effectively separate foreground, background, and uncertainty regions.
  - Quick check question: How does the CCAM method differ from traditional CAM approaches in terms of its focus on foreground-background separation?

- Concept: Uncertainty quantification and self-learning
  - Why needed here: The core innovation of USL-Net is treating uncertain regions as learnable rather than forcing hard labels. Understanding uncertainty quantification is crucial to grasp how the network avoids learning confusion from poor pseudo-labels.
  - Quick check question: What is the role of the uncertainty loss function in guiding the network to focus on confident regions while learning uncertain ones?

## Architecture Onboarding

- Component map: Contrastive learning module (MoCov1/2, SimCLR) ‚Üí Feature extraction ‚Üí CCAM module ‚Üí CAM/saliency map generation ‚Üí Uncertainty Module (UM) ‚Üí Pseudo-label generation with foreground/background/uncertainty regions ‚Üí ResNet50+FCN backbone ‚Üí Segmentation network ‚Üí Output

- Critical path: Image ‚Üí Contrastive learning ‚Üí CAM generation ‚Üí UM ‚Üí Pseudo-labels ‚Üí Segmentation network ‚Üí Output

- Design tradeoffs:
  - Using uncertainty regions vs. forcing hard pseudo-labels: More accurate segmentation but potentially slower convergence
  - Multiple contrastive learning methods vs. single method: Better feature robustness but increased computational cost
  - Cycle-refining vs. single-pass: Improved pseudo-label quality but increased training time

- Failure signatures:
  - Poor segmentation quality indicates issues with contrastive learning feature extraction
  - Excessive uncertainty regions suggest difficulty in distinguishing foreground from background
  - Plateau in cycle-refining indicates pseudo-label quality cannot be further improved

- First 3 experiments:
  1. Test baseline performance using only MoCov2 + Smooth Grad-CAM++ without uncertainty handling
  2. Evaluate the impact of switching from Smooth Grad-CAM++ to CCAM on pseudo-label quality
  3. Measure the improvement from adding connectivity and centrality detection in the uncertainty module

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of USL-Net change with different uncertainty thresholds for defining uncertain regions?
- Basis in paper: [explicit] The paper mentions using a saliency range of 0.35-0.65 for uncertain regions but suggests exploring the relationship between labeling ratios and performance.
- Why unresolved: The paper only briefly touches on this and doesn't provide detailed analysis of how varying the threshold impacts performance.
- What evidence would resolve it: A systematic study varying the uncertainty threshold (e.g., 0.3-0.4, 0.4-0.5, etc.) and measuring corresponding changes in accuracy, Dice coefficient, etc.

### Open Question 2
- Question: Can USL-Net's performance be further improved by incorporating additional medical image features beyond those extracted through contrastive learning?
- Basis in paper: [inferred] The paper focuses on contrastive learning for feature extraction but acknowledges that skin lesion images contain various artifacts like hair noise and blisters that might require additional processing.
- Why unresolved: The current approach relies solely on contrastive learning features, leaving room for potential improvement by integrating other feature types.
- What evidence would resolve it: Experiments comparing USL-Net's performance when augmented with additional features like texture, color, or superpixel-based features.

### Open Question 3
- Question: How does USL-Net generalize to datasets with significantly different lesion characteristics or image qualities compared to ISIC and PH2?
- Basis in paper: [explicit] The paper mentions cross-dataset validation but doesn't explore performance on datasets with substantially different characteristics.
- Why unresolved: The current evaluation is limited to datasets with similar characteristics, leaving questions about robustness to diverse real-world scenarios.
- What evidence would resolve it: Testing USL-Net on datasets with different imaging conditions, lesion types, or quality levels to assess generalization capabilities.

## Limitations
- Performance depends heavily on quality of pseudo-labels generated through uncertainty module, which may struggle with highly ambiguous dermoscopic images containing complex artifacts
- Requires 5 cycles of 300 epochs each, making it computationally intensive compared to single-pass approaches
- Uncertainty module's effectiveness relies on accurate identification of "medium-saliency" regions, which may vary across different dermoscopic image datasets

## Confidence
- **High confidence**: The contrastive learning + CAM generation pipeline is well-established and the reported performance metrics on ISIC datasets are verifiable
- **Medium confidence**: The uncertainty self-learning mechanism's effectiveness across diverse dermoscopic image qualities, as this depends on subjective threshold selection
- **Medium confidence**: The cycle-refining process compounding improvements, as the paper shows performance gains but doesn't provide detailed analysis of convergence behavior

## Next Checks
1. Conduct ablation studies to quantify the individual contribution of contrastive learning vs. uncertainty handling vs. cycle refining to overall performance
2. Test USL-Net on dermoscopic images with varying artifact densities (hair, bubbles, color variations) to assess robustness limits
3. Compare computational efficiency with weakly supervised methods using the same hardware and dataset splits to validate the claimed cost-effectiveness