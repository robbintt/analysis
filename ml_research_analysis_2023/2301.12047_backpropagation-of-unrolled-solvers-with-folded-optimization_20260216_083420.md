---
ver: rpa2
title: Backpropagation of Unrolled Solvers with Folded Optimization
arxiv_id: '2301.12047'
source_url: https://arxiv.org/abs/2301.12047
tags:
- optimization
- which
- pass
- learning
- unrolling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach called folded optimization
  for differentiable optimization in deep learning. The key idea is to convert unrolled
  optimization algorithms into analytically differentiable ones by leveraging the
  fixed-point conditions of the unrolled algorithm.
---

# Backpropagation of Unrolled Solvers with Folded Optimization

## Quick Facts
- arXiv ID: 2301.12047
- Source URL: https://arxiv.org/abs/2301.12047
- Reference count: 37
- This paper presents folded optimization, converting unrolled optimization algorithms into analytically differentiable ones by leveraging fixed-point conditions.

## Executive Summary
This paper introduces folded optimization, a novel approach that transforms unrolled optimization algorithms into analytically differentiable ones by recognizing that backpropagation through unrolled optimization is equivalent to solving a linear fixed-point iteration system. The key insight is that the backward pass of an iterative solver, when unfolded at a precomputed optimal solution, can be modeled as a differential fixed-point condition (DFP) system. This allows the forward and backward passes to be disentangled and solved separately using black-box solvers. Experiments on various tasks demonstrate that folded optimization achieves comparable or better performance than existing differentiable optimization frameworks while offering significant computational speedups.

## Method Summary
The approach converts unrolled optimization algorithms into analytically differentiable ones by leveraging the fixed-point conditions of the unrolled algorithm. The key idea is to recognize that backpropagation through unrolled optimization is equivalent to solving a linear fixed-point iteration system (DFP). This allows the forward and backward passes to be disentangled and solved separately using black-box solvers. The forward optimization step can be implemented as a blackbox software using any solver algorithm, while the backward pass is handled through the analytical model. This approach offers several advantages, including improved computational efficiency, the ability to handle non-convex optimization problems, and enhanced expressiveness.

## Key Results
- Folded optimization achieves computational equivalence to unrolled optimization while avoiding computational graph overhead
- The approach enables modular separation of forward and backward passes, allowing use of specialized solvers
- Folded optimization can handle nonconvex optimization problems in decision-focused learning, where traditional frameworks like cvxpy fail

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fixed-point folding achieves computational equivalence to unrolled optimization while avoiding the computational graph overhead.
- Mechanism: By recognizing that backpropagation through unrolled optimization is equivalent to solving a linear fixed-point iteration system (DFP), the approach replaces iterative unrolling with a direct solution of this system using black-box linear solvers.
- Core assumption: The Jacobian operator Φ of the unrolled algorithm is nonsingular with spectral radius less than 1.
- Evidence anchors:
  - [abstract] "This approach offers several advantages, including improved computational efficiency"
  - [section] "The backward pass of an iterative solver (U), unfolded at a precomputed optimal solution x⋆, is equivalent to solving the differential fixed-point conditions (DFP) using linear fixed-point iteration"
  - [corpus] Weak evidence - corpus neighbors don't discuss the specific linear system equivalence mechanism
- Break condition: If Φ is singular or has spectral radius ≥ 1, the linear fixed-point iteration won't converge, breaking the equivalence.

### Mechanism 2
- Claim: Folding optimization enables modular separation of forward and backward passes, allowing use of specialized solvers.
- Mechanism: The forward pass (finding x⋆) can use any black-box optimization solver, while the backward pass (computing gradients) is handled separately through the analytical model, decoupling solver choice from differentiation requirements.
- Core assumption: The forward optimization mapping x⋆(c) is differentiable and can be computed to sufficient accuracy by the black-box solver.
- Evidence anchors:
  - [abstract] "allows for the forward and backward passes of unrolled optimization to be disentangled and solved separately"
  - [section] "the forward optimization step of the mapping c→ x⋆(c) can be implemented as a blackbox software, using any solver algorithm"
  - [corpus] No direct evidence in corpus about modular solver separation
- Break condition: If the black-box solver cannot find x⋆ within required tolerance, or if the differentiability assumption fails, the approach breaks down.

### Mechanism 3
- Claim: Folding optimization enables handling of nonconvex optimization problems in decision-focused learning.
- Mechanism: By working with the differential fixed-point conditions rather than requiring convexity transformations, the approach can differentiate through nonconvex optimization mappings that traditional frameworks like cvxpy cannot handle.
- Core assumption: The optimization problem has a solution that can be found by some iterative algorithm, even if nonconvex.
- Evidence anchors:
  - [abstract] "Experiments over various model-based learning tasks demonstrate the advantages of the approach both computationally and in terms of enhanced expressiveness"
  - [section] "Importantly, we report the first demonstration of decision-focused learning with nonconvex decision models"
  - [corpus] Weak evidence - corpus neighbors discuss unrolled optimization but not specifically nonconvex differentiation
- Break condition: If the nonconvex problem has no solution or the iterative algorithm doesn't converge, the approach fails.

## Foundational Learning

- Concept: Automatic Differentiation (AD)
  - Why needed here: Understanding how AD works through computational graphs is essential for grasping why unrolling is computationally expensive
  - Quick check question: What is the main computational cost of using AD for unrolling optimization algorithms?

- Concept: Fixed-point iteration and convergence
  - Why needed here: The core mechanism relies on recognizing that backpropagation through unrolled optimization is equivalent to solving a linear fixed-point iteration system
  - Quick check question: Under what conditions does a fixed-point iteration converge?

- Concept: Implicit Function Theorem
  - Why needed here: The derivation of the differential fixed-point conditions (DFP) relies on implicit differentiation of the fixed-point equation
  - Quick check question: How does the Implicit Function Theorem allow us to differentiate implicitly defined functions?

## Architecture Onboarding

- Component map: Black-box forward optimization solver -> Fixed-point folding engine -> PyTorch computational graph
- Critical path: Forward pass → compute x⋆ using black-box solver → backward pass → solve DFP system → compute gradients → update parameters
- Design tradeoffs: The approach trades memory efficiency (avoiding unrolled computational graphs) for the requirement of solving linear systems, and gains expressiveness (handling nonconvex problems) at the cost of potentially slower linear solver performance.
- Failure signatures: (1) Forward pass solver fails to find x⋆, (2) Linear system solver fails to converge, (3) Gradients explode or vanish, (4) Training performance degrades compared to unrolling
- First 3 experiments:
  1. Implement a simple convex optimization layer (e.g., projection onto a ball) and compare folding vs unrolling performance
  2. Test folding on a nonconvex optimization problem where cvxpy fails
  3. Benchmark the memory usage difference between folded and unrolled implementations on a larger model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the starting point (x0) affect the convergence rate of the backward pass in fixed-point unfolding, beyond the two cases tested (precomputed optimal solution vs. random vector)?
- Basis in paper: [explicit] The paper discusses the effect of the starting point on backpropagation, showing that starting from the precomputed optimal solution leads to faster backward pass convergence compared to a random vector.
- Why unresolved: The paper only tests two specific starting points, and does not explore a broader range of initializations or their impact on convergence rates.
- What evidence would resolve it: Experiments testing various starting points (e.g., different random vectors, solutions from different algorithms) and measuring their effect on backward pass convergence rates.

### Open Question 2
- Question: Can the spectral radius (ρ(Φ)) of the Jacobian operator be analytically determined for specific classes of optimization problems, and how does this impact the choice of stepsize (α) in folded optimization?
- Basis in paper: [explicit] The paper mentions that the stepsize (α) should be chosen based on its effect on the spectral radius (ρ(Φ)) to improve efficiency, but does not provide a method for analytically determining ρ(Φ) or its optimal α.
- Why unresolved: The paper suggests that analyzing Φ could determine the optimal α, but does not pursue this analysis, leaving the relationship between ρ(Φ) and α unexplored.
- What evidence would resolve it: Analytical derivations or empirical studies showing the relationship between ρ(Φ) and α for different optimization problems, and how this relationship impacts convergence rates.

### Open Question 3
- Question: How does the performance of folded optimization compare to other differentiable optimization frameworks (e.g., cvxpy) when applied to large-scale optimization problems with millions of variables and constraints?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of folded optimization on various tasks, but the experiments focus on relatively small-scale problems. The scalability of the approach to large-scale problems is not addressed.
- Why unresolved: The paper does not test folded optimization on large-scale problems, leaving its performance and efficiency in such settings unknown.
- What evidence would resolve it: Experiments applying folded optimization to large-scale optimization problems and comparing its performance (e.g., runtime, memory usage, accuracy) to other differentiable optimization frameworks.

## Limitations
- The approach relies on assumptions about Jacobian nonsingularity and spectral radius < 1 that may not hold in practice
- The experimental section demonstrates performance benefits but doesn't provide detailed ablation studies on how the choice of linear solver affects convergence and accuracy
- While the approach claims to handle nonconvex problems, the specific nonconvex cases tested may not represent the full complexity of real-world nonconvex optimization

## Confidence

- High confidence: The core mathematical derivation of differential fixed-point conditions and the equivalence between unrolling and folding for convex problems
- Medium confidence: The computational efficiency claims, as these depend on specific problem characteristics and solver implementations
- Medium confidence: The nonconvex differentiation claims, as the paper provides limited empirical evidence beyond the bilinear programming example

## Next Checks

1. **Solver sensitivity analysis**: Systematically test how different linear solver algorithms (conjugate gradient, GMRES, direct solvers) affect the convergence and accuracy of folded optimization across the experimental tasks
2. **Spectral radius validation**: For each experimental task, explicitly compute or estimate the spectral radius of the Jacobian operator Φ to verify the theoretical convergence conditions hold in practice
3. **Nonconvex robustness testing**: Design additional nonconvex optimization problems with known properties (e.g., multiple local minima, saddle points) to evaluate how folding optimization handles challenging landscapes compared to unrolling