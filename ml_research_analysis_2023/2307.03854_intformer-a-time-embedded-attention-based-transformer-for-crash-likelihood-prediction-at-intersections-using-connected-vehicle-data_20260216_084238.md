---
ver: rpa2
title: 'inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood
  Prediction at Intersections Using Connected Vehicle Data'
arxiv_id: '2307.03854'
source_url: https://arxiv.org/abs/2307.03854
tags:
- crash
- data
- time
- intformer
- intersections
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces inTformer, a time-embedded attention-based
  Transformer model designed for real-time crash likelihood prediction at intersections
  using connected vehicle data. The model addresses the challenge of predicting intersection
  crashes by leveraging rich, infrastructure-free connected vehicle data from INRIX's
  Signal Analytics platform.
---

# inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data

## Quick Facts
- arXiv ID: 2307.03854
- Source URL: https://arxiv.org/abs/2307.03854
- Authors: 
- Reference count: 20
- One-line primary result: Achieved 73% sensitivity and 35% false alarm rate for intersection crash prediction using connected vehicle data

## Executive Summary
This paper introduces inTformer, a time-embedded attention-based Transformer model designed for real-time crash likelihood prediction at intersections using connected vehicle data. The model addresses the challenge of predicting intersection crashes by leveraging rich, infrastructure-free connected vehicle data from INRIX's Signal Analytics platform. The inTformer incorporates a novel Time Embedding layer to handle temporal dependencies in time series data, which is critical for accurate crash prediction. The study evaluates the model using data from eight intersections in Florida, achieving a sensitivity of 73% and a false alarm rate of 35%, outperforming established deep learning models like LSTM, CNN, and their combinations. The results demonstrate the viability of Transformers for real-time traffic safety applications and highlight the potential of connected vehicle data for proactive traffic management. The inTformer model offers a promising solution for enhancing intersection safety and reducing crash risks.

## Method Summary
The inTformer model uses connected vehicle data from INRIX's Signal Analytics platform, combined with crash data from S4A and weather data, to predict crash likelihood at intersections. The method involves labeling crash events as 1 if occurring within 15â€“30 minutes of traffic/weather observations, and 0 otherwise. SMOTE oversampling is applied to balance the training data, and timesteps are stacked to create 3D input arrays. The inTformer architecture includes a time embedding layer, multi-head attention, and position-wise feed-forward network. The model is trained on 80% of the data and tested on 20%, with hyperparameters tuned for optimal performance. Evaluation metrics include sensitivity and false alarm rate.

## Key Results
- Achieved 73% sensitivity and 35% false alarm rate for crash prediction
- Outperformed LSTM, CNN, and their combinations in crash prediction tasks
- Demonstrated the viability of Transformers for real-time traffic safety applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The time-embedded Transformer handles temporal dependencies in intersection crash prediction better than RNN-based models.
- Mechanism: The Time Embedding layer converts timestamps into a continuous representation that captures both periodic and non-periodic patterns, enabling the model to understand temporal order without sequential processing.
- Core assumption: Traffic dynamics at intersections exhibit time-dependent patterns that can be represented as periodic/non-periodic features.
- Evidence anchors:
  - [abstract] "The inTformer incorporates a novel Time Embedding layer to handle temporal dependencies in time series data"
  - [section] "Time Embedding... two ideas were implemented in the 'Time Embedding' layer: firstly, a realistic depiction of time must incorporate periodic and nonperiodic patterns"
  - [corpus] Weak evidence - no direct comparison of temporal handling in corpus
- Break condition: If intersection traffic patterns are truly non-temporal or if the periodic/non-periodic decomposition fails to capture critical timing patterns.

### Mechanism 2
- Claim: Multi-head attention enables the model to capture complex interactions between traffic features across different approaches and time steps.
- Mechanism: Each attention head learns different aspects of feature relationships, and concatenating these allows the model to simultaneously consider multiple interaction patterns between traffic flow, control delays, and weather conditions.
- Core assumption: Crash likelihood depends on complex, non-linear interactions between multiple traffic and environmental features.
- Evidence anchors:
  - [abstract] "Transformer has several functional benefits over extant deep learning models such as LSTM, CNN, etc. Firstly, Transformer can readily handle long-term dependencies"
  - [section] "Multi-Head Attention Mechanism... each single-head takes three inputs, namely query Q, key K, and value V, in total to calculate the attention weights that measure the relationship between elements/inputs in a sequence"
  - [corpus] Weak evidence - no specific attention mechanism validation in corpus
- Break condition: If the number of heads is insufficient to capture all relevant interaction patterns, or if attention weights become too uniform to be discriminative.

### Mechanism 3
- Claim: Zone-specific modeling improves prediction accuracy by accounting for different crash dynamics in within-intersection vs. approach zones.
- Mechanism: By training separate models for different spatial zones, the architecture can learn zone-specific patterns of crash likelihood without conflating distinct operational mechanisms.
- Core assumption: Crash occurrence mechanisms differ significantly between within-intersection and approach zones.
- Evidence anchors:
  - [section] "The traffic operation mechanism at intersections is very intricate... To analyze such crash complexity at intersections, this study segmented the intersection area into two zones"
  - [section] "The best inTformer models in 'within-intersection,' and 'approach' zone achieved a sensitivity of 73%, and 70%, respectively"
  - [corpus] Weak evidence - no direct validation of zone-specific benefit in corpus
- Break condition: If crash patterns are actually similar across zones, or if training data is too sparse to learn zone-specific patterns.

## Foundational Learning

- Concept: Time series data representation and temporal dependencies
  - Why needed here: The model must understand how traffic patterns evolve over time to predict future crash likelihood
  - Quick check question: How does the Time Embedding layer convert timestamps into features that capture both daily and weekly patterns?

- Concept: Attention mechanisms and multi-head attention
  - Why needed here: The model needs to identify which historical time steps and which features are most relevant for current prediction
  - Quick check question: What is the difference between the query, key, and value vectors in the attention mechanism?

- Concept: Handling imbalanced data with SMOTE
  - Why needed here: Crash events are rare compared to non-crash events, requiring synthetic minority oversampling
  - Quick check question: Why is it important to apply SMOTE only to the training data and not the test data?

## Architecture Onboarding

- Component map:
  Input -> Time Embedding layer -> Encoder stack (multi-head attention + position-wise feed-forward) -> Output layer (linear transformation to binary classification) -> SMOTE preprocessing

- Critical path:
  1. Data preparation and stacking
  2. SMOTE oversampling on training data
  3. Time Embedding layer transformation
  4. Multi-head attention computation
  5. Feed-forward network processing
  6. Binary classification output

- Design tradeoffs:
  - Fixed timestep stacking vs. variable length sequences
  - Number of attention heads vs. model complexity
  - Zone-specific models vs. unified model

- Failure signatures:
  - Vanishing gradients in deeper encoder stacks
  - Attention weights becoming uniform across time steps
  - Overfitting due to SMOTE-generated synthetic samples

- First 3 experiments:
  1. Compare single-head vs. multi-head attention performance on the same dataset
  2. Test different timestep stacking combinations (2, 3, 4 timesteps) to find optimal window
  3. Evaluate zone-specific vs. unified model performance on the same test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inTformer model's performance vary across different intersection types (e.g., four-legged vs. three-legged intersections)?
- Basis in paper: [explicit] The paper mentions that the study used six four-legged and two three-legged intersections but does not provide a detailed comparison of the model's performance across these types.
- Why unresolved: The paper does not analyze or report the model's sensitivity and false alarm rate for each intersection type separately, leaving a gap in understanding how well the model generalizes across different intersection configurations.
- What evidence would resolve it: A detailed analysis of the inTformer's performance metrics (sensitivity and false alarm rate) for each intersection type, along with a comparison of these metrics, would provide insights into the model's robustness and generalizability.

### Open Question 2
- Question: What is the impact of incorporating driver behavior data (e.g., hard braking, hard acceleration) on the inTformer model's predictive accuracy?
- Basis in paper: [inferred] The paper suggests that driver characteristics and driving behavior prior to crash occurrence could enhance the model's performance, but this aspect was not explored in the study.
- Why unresolved: The study did not include driver behavior data in the model, so the potential improvement in predictive accuracy from incorporating such data remains unknown.
- What evidence would resolve it: Conducting experiments with and without driver behavior data to compare the inTformer's sensitivity and false alarm rate would demonstrate the impact of this additional data on the model's performance.

### Open Question 3
- Question: How does the inTformer model's performance change with different resampling techniques or data augmentation methods?
- Basis in paper: [explicit] The paper mentions that SMOTE was used to address data imbalance, but it suggests exploring other resampling techniques or data augmentation methods to potentially enhance model performance.
- Why unresolved: The study only tested SMOTE for handling imbalanced data, leaving open the question of whether other methods could yield better results.
- What evidence would resolve it: Testing the inTformer model with various resampling techniques or data augmentation methods and comparing their effects on sensitivity and false alarm rate would provide insights into the optimal approach for handling data imbalance in this context.

## Limitations

- Data scope limited to eight intersections in Florida over one year, limiting generalizability
- Model performance (73% sensitivity, 35% false alarm rate) still leaves substantial room for error
- SMOTE oversampling may introduce synthetic samples that don't fully represent true crash dynamics

## Confidence

- **High Confidence**: The fundamental Transformer architecture and multi-head attention mechanism are well-established in literature and the implementation details are clearly specified.
- **Medium Confidence**: The time-embedded layer design and zone-specific modeling approach show promise but require further validation across diverse intersection types and conditions.
- **Low Confidence**: The generalizability of the model to intersections outside Florida and the long-term stability of performance metrics under varying operational conditions remain uncertain.

## Next Checks

1. **Cross-Regional Validation**: Test the inTformer model on intersection crash data from a different state/region with distinct traffic patterns, weather conditions, and intersection designs to assess generalizability.
2. **Temporal Stability Analysis**: Evaluate model performance across different seasons and time periods to identify any temporal drift or degradation in prediction accuracy.
3. **Ablation Study on Zone-Specific Modeling**: Conduct controlled experiments comparing zone-specific models against unified models on the same test set to quantify the actual benefit of the spatial segmentation approach.