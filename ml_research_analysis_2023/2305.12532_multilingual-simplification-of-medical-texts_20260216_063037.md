---
ver: rpa2
title: Multilingual Simplification of Medical Texts
arxiv_id: '2305.12532'
source_url: https://arxiv.org/abs/2305.12532
tags:
- simpli
- english
- sentence
- cation
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiCochrane, the first multilingual medical
  text simplification dataset spanning English, Spanish, French, and Farsi. It consists
  of 5K high-quality sentence pairs derived from Cochrane Library abstracts and plain-language
  summaries, plus a larger 100K-pair noisy set.
---

# Multilingual Simplification of Medical Texts

## Quick Facts
- arXiv ID: 2305.12532
- Source URL: https://arxiv.org/abs/2305.12532
- Authors:
- Reference count: 40
- Primary result: Introduces MultiCochrane, first multilingual medical text simplification dataset across 4 languages, with 5K high-quality pairs and 100K noisy pairs; finds GPT-3 performs best in English but struggles with extractiveness in other languages, while fine-tuned models have better simplicity but worse factuality.

## Executive Summary
This paper introduces MultiCochrane, the first multilingual medical text simplification dataset spanning English, Spanish, French, and Farsi. It consists of 5K high-quality sentence pairs derived from Cochrane Library abstracts and plain-language summaries, plus a larger 100K-pair noisy set. The authors evaluate zero-shot and fine-tuned models across these languages using both automatic metrics and human assessments. While GPT-3 generates fluent and factually accurate outputs in English, it tends to be overly extractive in other languages. Fine-tuned mT5 and Flan-T5 models perform better in terms of simplicity but struggle with factuality and fluency in non-English languages. A simplify-then-translate pipeline achieves more consistent results. Overall, the study highlights the need for improved multilingual medical simplification models.

## Method Summary
The authors create MultiCochrane by leveraging the structural alignment of Cochrane Library abstracts and plain-language summaries across four languages. English sentence pairs are manually aligned, then transferred to other languages using semi-automatic alignment with filtering based on overlapping token proportion (threshold r=0.5). The dataset includes both a high-quality 5K pair set and a larger 100K pair noisy set. Models evaluated include GPT-3 zero-shot, fine-tuned mT5 and Flan-T5, and a simplify-then-translate pipeline. Performance is assessed using automatic metrics (BLEU, BERTScore, SARI, LCS) and human evaluation of factuality, fluency, and simplicity.

## Key Results
- GPT-3 produces fluent and factually accurate outputs in English but is overly extractive in Spanish, French, and Farsi
- Fine-tuned mT5 and Flan-T5 models achieve better simplicity scores but struggle with factuality and fluency in non-English languages
- The simplify-then-translate pipeline delivers more consistent performance across languages compared to direct multilingual approaches
- Filtering the noisy dataset significantly improves model performance, indicating the importance of data quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Manual and semi-automatic alignment leverages structural alignment of Cochrane abstracts and plain-language summaries across languages
- Mechanism: Since Cochrane abstracts and PLS are produced in multiple languages with near 1:1 sentence correspondence, the English manual alignments can be transferred to other languages using the PLS as a bridge
- Core assumption: The sentence-level correspondence between abstract and PLS is consistent across all language versions
- Evidence anchors:
  - [abstract] "These translations have been produced through a combination of volunteer and professional work, as well as human-verified machine translation"
  - [section] "The PLS versions of other languages mostly correspond 1-1 sentence-wise to the English PLS"
  - [corpus] Weak - while the paper claims 1:1 alignment, the corpus evidence shows significant variation in the number of articles available per language (Figure 2)
- Break condition: If Cochrane's translation process changes or if language-specific formatting varies significantly

### Mechanism 2
- Claim: Filtering based on overlapping token proportion improves data quality by removing misalignments
- Mechanism: The filtering method calculates the proportion of overlapping tokens between aligned sentences and removes pairs where this proportion is below a threshold (r=0.5)
- Core assumption: Misalignments will have lower token overlap than correctly aligned pairs
- Evidence anchors:
  - [section] "This method can be described as follows: For each sentence pairing identified by the CRF model, we derive the set of lemmatized tokens in the complex and simple sentence denoted by Lc and Ls respectively. If the proportion of the overlapping tokens from the simplified sentence exceeds a threshold r, it is included"
  - [section] "Overall, as evident in Table 1, this filtering process reduced the dataset size by about half"
  - [corpus] Weak - the paper doesn't provide empirical validation that filtered data has fewer misalignments
- Break condition: If the threshold is too high and removes valid but paraphrased alignments

### Mechanism 3
- Claim: Fine-tuning multilingual models on the larger noisy dataset improves performance compared to zero-shot approaches
- Mechanism: Training mT5 and Flan-T5 models on the MC-NOISY dataset (100K pairs) provides exposure to diverse medical simplification patterns across languages
- Core assumption: The larger noisy dataset captures sufficient signal despite its noisiness to improve model generalization
- Evidence anchors:
  - [section] "Fine-tuned models (mT5, Flan-T5) were trained over a single epoch with a learning rate of 5e-5 and used the AdamW optimizer"
  - [section] "Results of these evaluations are presented in Table 6. The data in this table show a common trend in that filtering (Section 3.2.1) largely improves performance, indicating that filters improve data quality"
  - [corpus] Weak - the paper doesn't provide ablation studies comparing performance on MC-CLEAN vs MC-NOISY
- Break condition: If the noise in MC-NOISY is too high relative to the signal, causing models to learn incorrect patterns

## Foundational Learning

- Concept: Sentence alignment and evaluation metrics
  - Why needed here: The paper relies heavily on sentence alignment quality and various metrics (BLEU, BERTScore, SARI, LCS) to evaluate model performance
  - Quick check question: What is the difference between using the simplified sentence vs complex sentence as reference in BLEU scoring, and why does this matter for simplification tasks?

- Concept: Cross-lingual transfer and multilingual model limitations
  - Why needed here: The paper evaluates zero-shot and fine-tuned multilingual models, highlighting degradation in non-English languages
  - Quick check question: Why might GPT-3 perform better on English simplification than on other languages despite being a multilingual model?

- Concept: Factuality evaluation in text simplification
  - Why needed here: The paper emphasizes factuality as a key evaluation metric, finding that models introduce factual errors particularly in non-English languages
  - Quick check question: How does the paper's human evaluation framework assess factuality, and what scale is used?

## Architecture Onboarding

- Component map: Cochrane abstracts → manual alignment (English) → semi-automatic alignment (other languages) → filtering → training datasets → mT5/Flan-T5 → fine-tuning on MC-NOISY → evaluation on MC-CLEAN → automatic metrics (BLEU, BERTScore, SARI, LCS) + human evaluation (factuality, fluency, simplicity)

- Critical path: Data collection → alignment creation → filtering → model training → evaluation
  The most time-consuming and critical step is the manual alignment of English abstracts to PLS, which took significant human effort

- Design tradeoffs:
  - Data quality vs quantity: Using filtered MC-NOISY vs full MC-NOISY affects model performance
  - Direct multilingual vs simplify-then-translate: Each approach has different error profiles (extractiveness vs translation quality)
  - Model capacity vs training data: Fine-tuning vs zero-shot approaches

- Failure signatures:
  - Low BLEU scores when using simplified references indicate extractiveness issues
  - High LCS scores indicate the model is copying rather than simplifying
  - Poor human evaluation scores on factuality in non-English languages indicate cross-lingual generalization problems

- First 3 experiments:
  1. Compare filtered vs unfiltered MC-NOISY performance on a held-out validation set to quantify filtering benefits
  2. Test different alignment thresholds (r values) to find optimal filtering sensitivity
  3. Evaluate zero-shot performance on a smaller, cleaner validation set to establish baseline before fine-tuning

## Open Questions the Paper Calls Out
- Question: What is the optimal balance between filtering for quality and preserving diversity in multilingual medical simplification datasets?
  - Basis in paper: [inferred] The paper discusses the trade-off between filtering for quality (reducing misalignments) and extractiveness (preserving diverse information) in MC-NOISY. They used a threshold of r=0.5, but this is acknowledged as a compromise.
  - Why unresolved: The optimal threshold likely depends on the specific language pair and downstream task. The paper only evaluates one threshold, leaving open the question of whether other thresholds might yield better results for different languages or tasks.
  - What evidence would resolve it: A systematic evaluation of model performance across a range of filtering thresholds for each language pair, potentially using different thresholds for different tasks (e.g., higher threshold for factuality-critical applications).

- Question: How does the quality of automatically aligned data impact downstream model performance compared to manually aligned data in multilingual medical simplification?
  - Basis in paper: [explicit] The paper creates both MC-CLEAN (manually aligned) and MC-NOISY (automatically aligned) datasets, noting the trade-off between scale and quality. They evaluate models trained on the automatically aligned data.
  - Why unresolved: While the paper compares model performance on test sets derived from manually aligned data, it doesn't directly compare the impact of training on manually vs. automatically aligned data within the same model architecture.
  - What evidence would resolve it: A controlled experiment training the same model architecture on MC-CLEAN vs. MC-NOISY, comparing performance on held-out manually aligned test sets.

- Question: What are the most effective strategies for handling technical terminology and acronyms in multilingual medical text simplification?
  - Basis in paper: [explicit] The paper identifies acronym translation errors as a common issue across all systems, noting that medical acronyms often differ between languages and current models struggle with this.
  - Why unresolved: The paper identifies the problem but doesn't propose or evaluate specific solutions. It's unclear whether approaches like specialized terminology databases, acronym translation models, or targeted data augmentation would be most effective.
  - What evidence would resolve it: An evaluation comparing different strategies for handling technical terminology and acronyms (e.g., terminology databases, acronym translation models, data augmentation) on a multilingual medical simplification task.

## Limitations
- Small size of high-quality dataset (5K pairs) compared to noisy dataset (100K pairs) with no empirical validation of quality vs quantity trade-off
- Reliance on semi-automatic alignment methods without ground-truth validation of cross-lingual alignments
- Lack of systematic error analysis, particularly regarding sources of factual errors in non-English outputs
- No evaluation of domain-specific terminology handling, which is crucial for medical text simplification

## Confidence
**High Confidence**: The observation that GPT-3 performs better in English than other languages, and the general finding that simplify-then-translate pipelines achieve more consistent results than direct multilingual approaches. The methodology for creating the MultiCochrane dataset through alignment of Cochrane abstracts and plain-language summaries is also well-established.

**Medium Confidence**: The claim that fine-tuning on MC-NOISY improves performance compared to zero-shot approaches, as this lacks direct ablation studies. The evaluation that models struggle with factuality and fluency in non-English languages is supported but could benefit from more systematic analysis of error types.

**Low Confidence**: The assertion that the filtering method (threshold r=0.5) effectively removes misalignments, as there's no empirical validation of this claim. The paper also lacks confidence intervals or statistical significance testing for the reported metrics.

## Next Checks
1. **Ablation study on dataset quality**: Conduct controlled experiments comparing model performance when trained on filtered vs unfiltered MC-NOISY, and when trained on MC-CLEAN vs MC-NOISY, to quantify the actual benefit of the larger noisy dataset.

2. **Cross-lingual alignment validation**: Manually verify a sample of 100 sentence pairs in each non-English language to assess the accuracy of the semi-automatic alignment method and quantify the error rate introduced by this approach.

3. **Factuality error analysis**: Perform detailed error analysis on 200 generated outputs per language to categorize types of factual errors (omission, addition, modification) and correlate these with specific model architectures or training approaches.