---
ver: rpa2
title: 'Black Box Variational Inference with a Deterministic Objective: Faster, More
  Accurate, and Even More Black Box'
arxiv_id: '2304.05527'
source_url: https://arxiv.org/abs/2304.05527
tags:
- dadvi
- advi
- klvi
- posterior
- approximation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce DADVI, a method for black-box variational
  inference that addresses optimization and posterior uncertainty issues of mean-field
  ADVI. DADVI uses a fixed Monte Carlo approximation of the intractable objective
  (sample average approximation) to enable deterministic optimization, convergence
  assessment, and linear response covariance computation.
---

# Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box

## Quick Facts
- **arXiv ID**: 2304.05527
- **Source URL**: https://arxiv.org/abs/2304.05527
- **Reference count**: 40
- **Key outcome**: DADVI achieves faster, more accurate black-box variational inference by using fixed Monte Carlo approximations to enable deterministic optimization and linear response covariance computation.

## Executive Summary
DADVI addresses key limitations of mean-field ADVI by replacing the intractable variational objective with a fixed Monte Carlo approximation (sample average approximation). This enables deterministic second-order optimization, convergence assessment, and accurate linear response covariance estimates. Theoretical analysis shows DADVI's favorable scaling in high dimensions for problems with quadratic or global-local structure, while experiments demonstrate faster convergence and more accurate posterior approximations compared to standard ADVI methods.

## Method Summary
DADVI implements black-box variational inference by fixing Monte Carlo draws at the start and optimizing a deterministic objective function. This approach enables the use of second-order optimization methods like trust region algorithms, which are more stable and efficient than the first-order stochastic gradients used by standard ADVI. The deterministic objective also enables linear response covariance computation for accurate uncertainty quantification. The method is tested on 57 real-world models including 53 ARM models, Microcredit, Occupancy, Tennis, and POTUS datasets, comparing against mean-field ADVI, RAABBVI, full-rank ADVI, and NUTS MCMC.

## Key Results
- DADVI achieves faster convergence than standard ADVI by using deterministic second-order optimization on fixed Monte Carlo approximations
- DADVI provides more accurate posterior uncertainty estimates through linear response covariances, correcting mean-field ADVI's poor variance estimation
- DADVI scales well to high dimensions in problems with special structure (quadratic or global-local), requiring fewer samples than standard approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DADVI achieves faster and more reliable optimization than standard ADVI by using a fixed Monte Carlo approximation to the objective function.
- **Mechanism**: DADVI fixes the random draws ZN at the start and optimizes a deterministic objective. This enables the use of second-order optimization methods like trust region algorithms, which are more stable and efficient than the first-order stochastic gradients used by ADVI.
- **Core assumption**: The variational objective can be accurately approximated with a fixed Monte Carlo estimate, and the resulting optimization landscape is well-behaved enough for second-order methods to converge quickly.
- **Evidence anchors**: [abstract] "DADVI replaces the intractable MFVB objective with a fixed Monte Carlo approximation, a technique known in the stochastic optimization literature as the 'sample average approximation' (SAA). By optimizing an approximate but deterministic objective, DADVI can use off-the-shelf second-order optimization..." [section] "DADVI can use second-order optimization, such as trust region methods, to choose better search directions and verify improvement, e.g. using Frank-Wolfe step sizes."
- **Break condition**: If the variational approximation is too expressive (e.g., full-rank ADVI), the fixed Monte Carlo approximation becomes degenerate, leading to undefined or pathological solutions.

### Mechanism 2
- **Claim**: DADVI enables accurate posterior covariance estimates through linear response (LR) covariances, which are not available for standard ADVI.
- **Mechanism**: Because DADVI optimizes a deterministic objective, the optimum satisfies a computable first-order condition. This allows the use of the implicit function theorem to compute LR covariances, which correct the poor uncertainty estimates of mean-field approximations without fitting a more complex model.
- **Core assumption**: The variational approximation is close enough to the true posterior for LR covariances to provide accurate uncertainty estimates, and the fixed Monte Carlo approximation is sufficiently accurate for the sensitivity analysis to be valid.
- **Evidence anchors**: [abstract] "DADVI is amenable to more accurate posterior linear response (LR) covariance estimates." [section] "One well-documented failure of mean-field VB approximations... is the mis-estimation of posterior variance... 'Linear response' (LR) covariances are a technique for ameliorating the mis-estimation of posterior variances..."
- **Break condition**: If the number of Monte Carlo samples N is too small relative to the problem dimension, the fixed approximation may be too noisy for accurate LR covariance computation.

### Mechanism 3
- **Claim**: DADVI scales well to high dimensions in problems with special structure (approximately quadratic or global-local), where the number of samples needed does not grow linearly with dimension.
- **Mechanism**: In these structured problems, DADVI's fixed Monte Carlo approximation concentrates around the true optimum even with relatively few samples, because the problem decomposes into many low-dimensional subproblems or the posterior is close to Gaussian.
- **Core assumption**: The posterior has either an approximately quadratic form or a global-local structure with a low-dimensional global parameter shared across many local parameters.
- **Evidence anchors**: [abstract] "we show that, for problems that are approximately normal, or problems that are high dimensional due only to a having a large number of low-dimensional 'local' parameters, DADVI can be effective with a relatively small number of samples which, in particular, need not grow as the dimension of the problem grows." [section] "We argue that, for problems that are approximately normal, or problems that are high dimensional due only to a having a large number of low-dimensional 'local' parameters, DADVI can be effective with a relatively small number of samples which, in particular, need not grow as the dimension of the problem grows."
- **Break condition**: If the problem lacks these special structures (e.g., highly non-Gaussian or without global-local decomposition), DADVI requires many more samples, negating its advantages.

## Foundational Learning

- **Concept**: Sample Average Approximation (SAA)
  - **Why needed here**: DADVI is essentially SAA applied to the ADVI objective. Understanding SAA helps explain why fixing the Monte Carlo approximation enables deterministic optimization and what its theoretical guarantees are.
  - **Quick check question**: In SAA, how does fixing the Monte Carlo draws differ from using fresh draws at each optimization step, and what are the trade-offs?

- **Concept**: Linear Response (LR) Covariances
  - **Why needed here**: DADVI's ability to compute LR covariances is a key advantage over standard ADVI. LR covariances use sensitivity analysis to correct the poor uncertainty estimates of mean-field approximations.
  - **Quick check question**: Why can't standard ADVI compute LR covariances, and how does DADVI's deterministic objective enable their computation?

- **Concept**: Global-Local Model Structure
  - **Why needed here**: The paper shows DADVI works well in high dimensions when the model has a global-local structure. Understanding this structure explains when DADVI's sample efficiency holds.
  - **Quick check question**: In a global-local model, what is the role of the global parameters versus the local parameters, and why does this structure allow DADVI to scale?

## Architecture Onboarding

- **Component map**: Variational Objective -> Monte Carlo Approximation -> Optimization -> Postprocessing
- **Critical path**: 
  1. Draw N samples from standard normal
  2. Transform to constrained space using reparameterization
  3. Optimize fixed Monte Carlo objective using second-order method
  4. Compute LR covariances using implicit function theorem
  5. Estimate Monte Carlo error using delta method

- **Design tradeoffs**:
  - Fixed vs. fresh Monte Carlo draws: Fixed draws enable deterministic optimization but introduce approximation error; fresh draws provide unbiased gradients but require careful tuning.
  - Second-order vs. first-order optimization: Second-order is faster and more stable but requires computing and inverting Hessians; first-order is simpler but may converge slowly.
  - Mean-field vs. full-rank ADVI: Mean-field is scalable but poor at uncertainty; full-rank is better at uncertainty but computationally expensive and incompatible with DADVI in high dimensions.

- **Failure signatures**:
  - Optimization fails to converge: May indicate poor conditioning, need for better initialization, or that the fixed approximation is too noisy.
  - LR covariances are inaccurate: May indicate the variational approximation is too poor or too few Monte Carlo samples were used.
  - Posterior means are far from ground truth: May indicate the mean-field approximation is too restrictive for the problem.

- **First 3 experiments**:
  1. Compare DADVI vs. ADVI optimization convergence on a simple hierarchical model with known ground truth.
  2. Test DADVI's LR covariance accuracy on a multivariate normal posterior where ground truth is known.
  3. Evaluate DADVI's performance on a global-local model with increasing numbers of local parameters to verify the logarithmic scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does DADVI's performance scale with increasing dimensionality when the number of global parameters is fixed but the number of local parameters grows?
- **Basis in paper**: [explicit] The paper discusses "global-local" structure in section 3.2 and shows DADVI's favorable scaling with log P rather than linear P dependence, but doesn't provide comprehensive experimental validation across varying dimensionalities.
- **Why unresolved**: The theoretical analysis assumes certain uniform laws of large numbers and strict minimum conditions, but practical performance across different global-local model structures remains untested.
- **What evidence would resolve it**: Systematic experiments varying both the number of local parameters and global parameters across multiple real-world datasets would clarify DADVI's scaling behavior.

### Open Question 2
- **Question**: Can DADVI be effectively extended to more expressive variational families beyond mean-field, such as structured variational distributions or sparse approximations?
- **Basis in paper**: [inferred] Section 3.3 demonstrates DADVI's failure for full-rank ADVI due to over-fitting, but doesn't explore intermediate expressive families or structured approximations.
- **Why unresolved**: The paper establishes theoretical limitations for highly expressive families but doesn't investigate whether moderate expressiveness improvements over mean-field are feasible with DADVI.
- **What evidence would resolve it**: Empirical studies testing DADVI with structured variational families (e.g., low-rank approximations, sparse precision matrices) on high-dimensional datasets would reveal practical expressiveness limits.

### Open Question 3
- **Question**: What are the optimal strategies for dynamically allocating computation between the number of Monte Carlo samples (N) and the optimization iterations in DADVI?
- **Basis in paper**: [explicit] Section 2.3 mentions the need for balancing computation between samples and optimization but doesn't provide theoretical or empirical guidance on optimal allocation.
- **Why unresolved**: While the paper suggests assessing Monte Carlo error and re-running with more samples, it doesn't characterize the trade-off between sample size and optimization effort.
- **What evidence would resolve it**: Theoretical analysis deriving optimal sample-to-iteration ratios for different model classes, combined with empirical validation across diverse model families, would provide actionable guidance.

## Limitations

- The theoretical analysis relies on problem-specific structures (quadratic or global-local) that may not hold in general applications, with no clear guidance for diagnosing when these conditions are approximately met in practice.
- The experiments show DADVI performs well on the tested models, but the sample size (53 ARM models plus 4 additional models) is relatively small for establishing general superiority, potentially limiting generalizability.
- The paper claims DADVI enables accurate LR covariances, but only compares against mean-field ADVI without full-rank ADVI as a baseline for uncertainty quantification.

## Confidence

- **High confidence**: DADVI's mechanism for faster optimization through fixed Monte Carlo approximation is well-established in the optimization literature and the implementation appears sound.
- **Medium confidence**: DADVI's superior performance on structured problems is supported by both theory and experiments, but the generality of these results to arbitrary models remains uncertain.
- **Low confidence**: The claim that DADVI is "even more black box" than standard ADVI is subjective and depends on implementation details not fully specified in the paper.

## Next Checks

1. **Stress test the scaling claim**: Evaluate DADVI on increasingly high-dimensional problems without global-local structure to determine the conditions under which the logarithmic scaling breaks down.
2. **Compare uncertainty quantification**: Implement full-rank ADVI with LR covariances as a baseline to rigorously compare DADVI's uncertainty estimates against the best available alternatives.
3. **Analyze approximation error**: Systematically vary the number of Monte Carlo samples N and problem dimension D to quantify the trade-off between approximation accuracy and computational efficiency, particularly in the regime where N < D.