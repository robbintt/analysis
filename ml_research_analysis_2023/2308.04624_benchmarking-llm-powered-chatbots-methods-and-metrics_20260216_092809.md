---
ver: rpa2
title: 'Benchmarking LLM powered Chatbots: Methods and Metrics'
arxiv_id: '2308.04624'
source_url: https://arxiv.org/abs/2308.04624
tags:
- chatbot
- benchmark
- answers
- sentence
- golden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new benchmark called the E2E (End-to-End)
  benchmark for evaluating the performance of Large Language Model (LLM) powered chatbots.
  The E2E benchmark compares the chatbot's responses to expert human-generated "golden
  answers" using cosine similarity between their text embeddings, providing a user-centric
  evaluation that accounts for both accuracy and usefulness.
---

# Benchmarking LLM powered Chatbots: Methods and Metrics

## Quick Facts
- arXiv ID: 2308.04624
- Source URL: https://arxiv.org/abs/2308.04624
- Reference count: 0
- The E2E benchmark using cosine similarity of text embeddings outperforms ROUGE scores for evaluating LLM-powered chatbots, with Sentence Transformer embeddings showing higher sensitivity to prompt engineering.

## Executive Summary
This paper introduces the E2E (End-to-End) benchmark for evaluating Large Language Model (LLM) powered chatbots by comparing responses to expert human-generated "golden answers" using cosine similarity between text embeddings. The study demonstrates that this approach provides a more user-centric evaluation than traditional ROGUE scores by capturing both accuracy and usefulness. Results show that the E2E benchmark, particularly when using Sentence Transformer embeddings, is more sensitive to prompt engineering improvements and provides more reliable evaluation metrics than ROGUE scores.

## Method Summary
The E2E benchmark converts both chatbot responses and golden answers into n-dimensional vectors using embedding models (Universal Sentence Encoder or Sentence Transformer), then calculates cosine similarity between these vectors. The method compares chatbot answers to expert-generated golden answers across different prompt variations. The study evaluates both USE and ST embeddings against traditional ROGUE metrics (precision and recall at unigram, bigram, and LCS levels) to determine which provides more meaningful evaluation of chatbot performance.

## Key Results
- E2E benchmark using ST embeddings showed higher sensitivity to prompt engineering improvements compared to USE
- ROUGE scores were unpredictable and did not correlate well with improvements from prompt engineering
- ST embeddings outperformed USE, showing cosine similarity of 0.0 for random text versus USE's 0.5 floor

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cosine similarity between sentence embeddings captures semantic alignment between chatbot responses and expert "golden answers" more effectively than token-based metrics like ROUGE.
- Mechanism: Sentence embeddings encode semantic meaning into fixed-dimensional vectors. Cosine similarity measures the angular distance between these vectors, reflecting how closely the meaning of the chatbot's answer aligns with the expert's. This avoids penalizing synonym substitutions or minor wording changes that token-based metrics would penalize.
- Core assumption: Embedding models preserve semantic content sufficiently well to distinguish between correct and hallucinated responses.
- Evidence anchors:
  - [abstract] "The E2E benchmark compares the chatbot's responses to expert human-generated 'golden answers' using cosine similarity between their text embeddings"
  - [section] "For both the embeddings, we take a golden answer (G) and the chatbot generated answer (A), the we apply the embedding X = F (T ) where F can be either a Universal Sentence Encoder or a Sentence Transformer on our text T- which gives us XG and XA both being n-dimensional vectors generated as a result of applying the operations. Then we calculate the cosine similarities between XG and XA."
  - [corpus] Weak; no direct supporting papers found, but similar embedding-based benchmarks exist in the literature.
- Break condition: If the embedding model fails to capture domain-specific terminology or if the chatbot's responses are semantically correct but syntactically very different from the golden answers, cosine similarity may under-estimate alignment.

### Mechanism 2
- Claim: Prompt engineering significantly improves chatbot response quality, as measured by E2E cosine similarity but not by ROUGE scores.
- Mechanism: Enhanced prompts guide the LLM to produce more focused, context-aware responses. Since E2E measures semantic alignment, improvements in response relevance and coherence are captured better than by ROUGE, which is sensitive to exact word overlap.
- Core assumption: The chatbot's underlying LLM can interpret and act on more detailed prompts to generate better answers.
- Evidence anchors:
  - [section] "By providing enhanced prompts to the chatbot, we observed an increase in mean score for both libraries, however the ST score showed a sharper increase, indicating high sensitivity to the enhanced prompt."
  - [section] "ROUGE scores do not show improvement on applying prompt engineering. However the E2E benchmark metrics, i.e. cosine similarity of the USE and ST embeddings show considerable improvement on using engineered prompts."
  - [corpus] Weak; no corpus evidence directly supports this claim.
- Break condition: If the prompt changes do not alter the underlying semantic content or if the golden answers themselves are ambiguous, E2E may not reflect meaningful improvements.

### Mechanism 3
- Claim: Sentence Transformer embeddings are more sensitive to prompt engineering than Universal Sentence Encoder embeddings, as evidenced by larger score improvements.
- Mechanism: ST embeddings are fine-tuned on semantic textual similarity tasks and may better capture nuanced differences in meaning introduced by prompt changes, leading to greater sensitivity in cosine similarity scores.
- Core assumption: The fine-tuning process of ST makes it more responsive to subtle semantic shifts than the general-purpose USE.
- Evidence anchors:
  - [section] "We see a significant rise in Sentence Transformer outputs on engineered prompts, suggesting it is much more sensitive to prompt engineering tweaks on the query."
  - [section] "Sentence Transformer outperforms USE here as USE still gets a cosine similarity of about 0.5 whereas Sentence Transformers give a cosine similarity of 0.0."
  - [corpus] Weak; no corpus evidence directly supports this specific comparison.
- Break condition: If both embedding models saturate at high semantic similarity or if prompt changes are too subtle to be captured by either model, the difference in sensitivity may disappear.

## Foundational Learning

- Concept: Text embeddings and cosine similarity
  - Why needed here: The entire E2E benchmark relies on converting text to vectors and measuring semantic similarity.
  - Quick check question: What is the range of cosine similarity values, and what does a score of 0.7 indicate about two texts?

- Concept: Prompt engineering in LLMs
  - Why needed here: The paper demonstrates that better prompts lead to better semantic alignment, as measured by E2E.
  - Quick check question: How does adding a "system" role in a prompt influence the tone and focus of an LLM's response?

- Concept: Benchmarking metrics (ROUGE vs semantic similarity)
  - Why needed here: Understanding the limitations of ROUGE helps explain why E2E is a better choice for user-centric evaluation.
  - Quick check question: Why might a response be semantically correct but score poorly on ROUGE-1?

## Architecture Onboarding

- Component map:
  User query -> Chatbot (LLM) -> Chatbot response
  Chatbot response + Golden answer -> Embedding model (USE/ST) -> Cosine similarity score
  Chatbot response + Golden answer -> ROUGE metric -> Precision/Recall scores

- Critical path:
  1. Generate chatbot response
  2. Embed both chatbot and golden answers
  3. Compute cosine similarity
  4. Aggregate and compare across queries

- Design tradeoffs:
  - USE: Higher mean scores but systemic bias (0.5 floor)
  - ST: Lower mean scores but higher sensitivity to prompt changes
  - ROUGE: Unpredictable, does not correlate with improvements

- Failure signatures:
  - USE always returns ~0.5 for random text → systemic bias
  - ST returns ~0.0 for random text → no false positives
  - ROUGE scores uncorrelated with prompt changes → unreliable metric

- First 3 experiments:
  1. Compare cosine similarity scores for a known good vs bad chatbot response using both USE and ST.
  2. Apply a simple prompt engineering tweak (e.g., adding "be concise") and measure score changes.
  3. Replace golden answers with random text and verify ST returns ~0, USE returns ~0.5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the E2E benchmark results using Universal Sentence Encoder (USE) and Sentence Transformer (ST) embeddings compare to other semantic similarity metrics like BERTScore or MoverScore in evaluating LLM-powered chatbots?
- Basis in paper: [inferred] The paper compares the E2E benchmark using USE and ST embeddings to traditional ROGUE scores, but does not explore other semantic similarity metrics like BERTScore or MoverScore.
- Why unresolved: The paper focuses on comparing the E2E benchmark with ROGUE scores, but does not provide a comprehensive comparison with other semantic similarity metrics.
- What evidence would resolve it: Conducting a study that compares the E2E benchmark using USE and ST embeddings to other semantic similarity metrics like BERTScore or MoverScore in evaluating LLM-powered chatbots.

### Open Question 2
- Question: How does the E2E benchmark perform when evaluating chatbots that are not powered by Large Language Models (LLMs) but use other AI techniques like rule-based systems or decision trees?
- Basis in paper: [explicit] The paper introduces the E2E benchmark for evaluating LLM-powered chatbots, but does not discuss its applicability to chatbots using other AI techniques.
- Why unresolved: The paper focuses on the performance of the E2E benchmark for LLM-powered chatbots and does not explore its effectiveness for chatbots using other AI techniques.
- What evidence would resolve it: Conducting a study that evaluates the performance of the E2E benchmark for chatbots using different AI techniques like rule-based systems or decision trees.

### Open Question 3
- Question: How does the E2E benchmark handle cases where the chatbot's response is partially correct but contains some hallucination or irrelevant information?
- Basis in paper: [inferred] The paper discusses the importance of evaluating both accuracy and usefulness in chatbot benchmarking, but does not explicitly address how the E2E benchmark handles cases with partial correctness or hallucination.
- Why unresolved: The paper does not provide a detailed explanation of how the E2E benchmark handles cases where the chatbot's response is partially correct but contains some hallucination or irrelevant information.
- What evidence would resolve it: Conducting a study that evaluates the performance of the E2E benchmark in handling cases with partial correctness or hallucination in chatbot responses.

## Limitations
- The study tested only a single product support chatbot with a limited set of golden answers, limiting generalizability across domains and chatbot types.
- The paper doesn't address potential biases in golden answers or whether expert-generated responses represent optimal or consistent standards.
- No statistical power analysis was conducted to determine minimum sample sizes needed to detect meaningful differences between metrics.

## Confidence
- **High Confidence**: The core mechanism of using cosine similarity between embeddings is technically sound and the systemic bias observed in USE (mean score of 0.5 for random text) is well-documented in the literature.
- **Medium Confidence**: The claim that ST embeddings are more sensitive to prompt engineering is supported by the observed score increases, but could benefit from more rigorous statistical testing and ablation studies to rule out other factors.
- **Low Confidence**: The assertion that E2E is "more reliable and effective" than ROGUE for all chatbot evaluation scenarios requires broader validation across different domains and chatbot types.

## Next Checks
1. Test the E2E benchmark across multiple chatbot domains (e.g., customer service, technical support, medical advice) to verify generalizability of the findings.

2. Conduct a statistical power analysis to determine the minimum sample size needed to detect meaningful differences between E2E and ROUGE metrics across different prompt variations.

3. Implement a blind evaluation where human raters assess response quality independently of both golden answers and metric scores to establish ground truth correlation with automated metrics.