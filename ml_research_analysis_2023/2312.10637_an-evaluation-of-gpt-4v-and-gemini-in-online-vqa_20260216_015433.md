---
ver: rpa2
title: An Evaluation of GPT-4V and Gemini in Online VQA
arxiv_id: '2312.10637'
source_url: https://arxiv.org/abs/2312.10637
tags:
- gpt-4v
- gemini
- questions
- image
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive evaluation of two state-of-the-art
  large multimodal models (LMMs), GPT-4V and Gemini, using the VQAonline dataset of
  real-world visual questions. The authors analyze performance across 7 metadata dimensions:
  topic, super-topic, user intention, image processing capabilities, image types,
  difficulty, and required knowledge.'
---

# An Evaluation of GPT-4V and Gemini in Online VQA

## Quick Facts
- arXiv ID: 2312.10637
- Source URL: https://arxiv.org/abs/2312.10637
- Reference count: 23
- Key outcome: This paper provides a comprehensive evaluation of two state-of-the-art large multimodal models (LMMs), GPT-4V and Gemini, using the VQAonline dataset of real-world visual questions. The authors analyze performance across 7 metadata dimensions: topic, super-topic, user intention, image processing capabilities, image types, difficulty, and required knowledge. Key findings include: both models perform best on topics like ELL and Economics, but struggle with puzzling and LEGO-related questions; GPT-4V excels at non-visual questions and questions requiring object recognition, while Gemini performs better on scene understanding tasks; both models find questions requiring "expert knowledge" most challenging; and performance degrades on harder questions. The analysis identifies specific strengths and weaknesses of each model, providing insights to guide future LMM development.

## Executive Summary
This paper presents a comprehensive evaluation of GPT-4V and Gemini on the VQAonline dataset, analyzing their performance across multiple metadata dimensions. The study employs a zero-shot evaluation setup and leverages GPT-4 to generate metadata labels for each visual question, enabling granular performance analysis. Key findings reveal that both models perform best on topics like ELL and Economics, but struggle with puzzling and LEGO-related questions. The evaluation also highlights that GPT-4V outperforms Gemini on most topics and image processing capabilities, particularly excelling at non-visual questions and object recognition tasks. However, both models find questions requiring expert knowledge most challenging, indicating limitations in domain-specific reasoning.

## Method Summary
The study conducts a zero-shot evaluation of GPT-4V and Gemini on the VQAonline dataset, comprising nearly 2,000 visual questions sourced from Stack Exchange. The authors generate seven types of metadata for each question using GPT-4 and GPT-4V, including topic, super-topic, user intention, image processing capabilities, image types, difficulty, and required knowledge. Model predictions are obtained using the respective APIs (GPT-4V Turbo via Azure OpenAI and Gemini Pro Vision) with a straightforward prompt format. Answer correctness is evaluated by GPT-4, with scores rescaled to a 0-1 range. Performance is then analyzed across the metadata dimensions to identify strengths and weaknesses of each model.

## Key Results
- Both GPT-4V and Gemini perform best on topics like ELL and Economics, but struggle with puzzling and LEGO-related questions.
- GPT-4V outperforms Gemini on most topics and image processing capabilities, particularly excelling at non-visual questions and object recognition tasks.
- Both models find questions requiring expert knowledge most challenging, indicating limitations in domain-specific reasoning.
- Performance degrades on harder questions for both models, with accuracy dropping as difficulty increases.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4V and Gemini performance can be systematically characterized by metadata dimensions like topic, image type, and knowledge requirements.
- Mechanism: By generating rich metadata for each question, the evaluation captures multiple facets of model behavior, enabling granular performance analysis across different dimensions.
- Core assumption: The metadata labels generated by GPT-4 are reliable proxies for the underlying difficulty and capability requirements of each question.
- Evidence anchors:
  - [abstract] "We conduct fine-grained analysis by generating seven types of metadata for nearly 2,000 visual questions"
  - [section 3] "we leverage GPT-4 and GPT-4V to assign a set of metadata to each visual question"
- Break condition: If GPT-4-generated metadata labels are systematically biased or noisy, the performance patterns would reflect labeling artifacts rather than true model capabilities.

### Mechanism 2
- Claim: Both models perform better on questions requiring everyday life knowledge than on expert knowledge questions.
- Mechanism: Questions requiring expert knowledge involve domain-specific concepts and terminology that may fall outside the pretraining distribution of general-purpose models.
- Core assumption: Expert knowledge questions are inherently more difficult for models trained on broad web data.
- Evidence anchors:
  - [section 4.5] "Approximately half of the visual questions require 'Expert knowledge,' underscoring the significance of a model possessing domain-specific expertise"
  - [section 4.5] "GPT-4V performs slightly better on questions demanding expert and fine-grained knowledge compared to the other knowledge types"
- Break condition: If the model has strong domain-specific pretraining or the "expert knowledge" label is applied inconsistently, this performance gap could be explained by other factors.

### Mechanism 3
- Claim: GPT-4V outperforms Gemini on most topics and image processing capabilities.
- Mechanism: GPT-4V's superior performance stems from its stronger visual reasoning capabilities and more comprehensive training on multimodal data.
- Core assumption: GPT-4V's training included more diverse visual data or better visual reasoning architectures.
- Evidence anchors:
  - [section 4.1] "Across the majority of topics, GPT-4V exhibits superior performance compared to Gemini"
  - [section 4.4] "GPT-4V excels particularly in non-visual questions"
- Break condition: If the evaluation setup or prompt format systematically favors GPT-4V, the performance difference may not reflect true capability differences.

## Foundational Learning

- Concept: Zero-shot evaluation
  - Why needed here: The paper explicitly evaluates models in a zero-shot setting without prompt engineering to capture authentic model behavior.
  - Quick check question: What distinguishes zero-shot evaluation from few-shot or fine-tuned evaluation?

- Concept: Metadata generation for evaluation
  - Why needed here: The study generates multiple metadata types (topic, super-topic, user intention, etc.) to enable granular performance analysis across different dimensions.
  - Quick check question: Why is metadata generation important for understanding model strengths and weaknesses?

- Concept: Multimodal reasoning
  - Why needed here: The evaluation assesses models' ability to process both visual and textual information to answer questions.
  - Quick check question: What are the key differences between unimodal and multimodal reasoning tasks?

## Architecture Onboarding

- Component map: VQAonline dataset -> Metadata generation using GPT-4/GPT-4V -> Model inference using GPT-4V/Gemini APIs -> Correctness evaluation using GPT-4 -> Performance analysis across metadata dimensions
- Critical path: The most time-consuming and rate-limited steps are API calls for metadata generation and model inference, particularly for GPT-4V.
- Design tradeoffs: The paper chose a random subset of 20 questions per topic rather than using the full dataset to manage API costs and rate limits, potentially affecting statistical power for rare topics.
- Failure signatures: Poor performance on expert knowledge questions, puzzling topics, and sheet music images indicates limitations in domain-specific reasoning and specialized visual content processing.
- First 3 experiments:
  1. Replicate the metadata generation process for a small sample of questions to verify consistency and reliability.
  2. Test model performance on a held-out set of questions from the same topics to assess generalization.
  3. Analyze the correlation between metadata dimensions (e.g., topic vs. difficulty) to identify confounding factors in performance patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4V and Gemini vary when handling visual questions that require multiple image processing capabilities simultaneously?
- Basis in paper: [explicit] The paper notes that feature extraction and object recognition are frequently needed together, and that the models should possess multiple image processing capabilities concurrently.
- Why unresolved: The paper does not provide specific performance metrics for questions requiring multiple image processing capabilities. It only mentions the correlation between different capabilities without analyzing their combined effect on model performance.
- What evidence would resolve it: A detailed analysis comparing model accuracy on questions requiring one vs. multiple image processing capabilities, ideally with a breakdown by specific combinations of capabilities.

### Open Question 2
- Question: What specific aspects of "expert knowledge" make visual questions in VQAonline particularly challenging for GPT-4V and Gemini?
- Basis in paper: [explicit] The paper identifies that around half of the visual questions require "expert knowledge" and that both models perform worst on questions categorized as "Other" (which includes expert knowledge).
- Why unresolved: The paper defines expert knowledge as typically possessed by experts in a field but does not delve into what specific characteristics of this knowledge type pose challenges for the models.
- What evidence would resolve it: A qualitative analysis of expert knowledge questions where the models fail, identifying common themes or characteristics that make them difficult (e.g., highly specialized terminology, complex reasoning chains, or niche factual knowledge).

### Open Question 3
- Question: Is there a correlation between the difficulty level assigned by GPT-4 and the actual complexity of the visual questions in terms of required cognitive processes?
- Basis in paper: [inferred] The paper uses GPT-4 to assign difficulty levels and notes that both models perform worse on harder questions, but does not validate whether these difficulty levels align with actual cognitive complexity.
- Why unresolved: The difficulty levels are assigned by GPT-4 without external validation, and the paper does not explore whether these levels correspond to human perceptions of difficulty or specific cognitive demands.
- What evidence would resolve it: A comparison between GPT-4-assigned difficulty levels and human-rated difficulty levels, or an analysis linking difficulty levels to specific cognitive processes required (e.g., memory recall, logical reasoning, spatial reasoning).

## Limitations

- The evaluation relies heavily on GPT-4-generated metadata labels, which introduces potential bias and noise into the performance analysis.
- The zero-shot evaluation setup with minimal prompt engineering may not fully capture the models' capabilities, particularly for complex visual reasoning tasks.
- The random sampling of 20 questions per topic, while practical for API constraints, may not provide sufficient statistical power for rare or nuanced categories.

## Confidence

- **Performance patterns across metadata dimensions**: Medium confidence - The overall trends appear consistent with known model capabilities, but the reliability of GPT-4-generated metadata labels remains unverified.
- **GPT-4V vs Gemini capability differences**: Medium confidence - The performance gap is reported, but the evaluation setup (prompt format, API differences) may have introduced systematic biases.
- **Expert knowledge difficulty**: High confidence - This finding aligns with established limitations of general-purpose models on domain-specific tasks.
- **Image type-specific performance**: Low confidence - The analysis of image types (sheet music, tables, charts) shows clear patterns, but the small sample sizes for some categories make these conclusions tentative.

## Next Checks

1. **Metadata Quality Validation**: Independently verify the consistency and reliability of GPT-4-generated metadata by having human annotators label a random sample of 100 questions across all metadata dimensions, then compute inter-annotator agreement scores.

2. **Prompt Engineering Impact Test**: Re-run the evaluation with optimized prompts for both models, including few-shot examples and structured formatting, to establish the true performance ceiling and determine if current gaps reflect capability differences or prompt sensitivity.

3. **Statistical Power Analysis**: Calculate the statistical power for each metadata dimension by analyzing the variance in model performance across different sample sizes, identifying which conclusions are robust versus those that require larger datasets for validation.