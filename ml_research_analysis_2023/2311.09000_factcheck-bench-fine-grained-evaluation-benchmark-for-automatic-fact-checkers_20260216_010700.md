---
ver: rpa2
title: 'Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers'
arxiv_id: '2311.09000'
source_url: https://arxiv.org/abs/2311.09000
tags:
- claim
- claims
- evidence
- sentence
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a fine-grained, end-to-end framework for
  fact-checking and correcting factual errors in LLM-generated text. The approach
  involves decomposing long documents into atomic claims, decontextualizing sentences,
  identifying checkworthy claims, retrieving and evaluating evidence, and correcting
  false claims.
---

# Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers

## Quick Facts
- arXiv ID: 2311.09000
- Source URL: https://arxiv.org/abs/2311.09000
- Reference count: 22
- Fact-checkers struggle to identify false claims, with best F1-score of 0.53 on annotated benchmark

## Executive Summary
This paper introduces a fine-grained, end-to-end framework for fact-checking and correcting factual errors in LLM-generated text. The approach involves decomposing long documents into atomic claims, decontextualizing sentences, identifying checkworthy claims, retrieving and evaluating evidence, and correcting false claims. An annotation tool is developed to facilitate this process, enabling flexible incorporation of automatic results. The authors construct a document-level factuality benchmark with three levels of granularity: claim, sentence, and document. Preliminary experiments show that popular fact-checkers like FacTool, FactScore, and Perplexity.ai struggle to identify false claims, with the best F1-score of 0.53 on the annotated benchmark. The annotation tool, dataset, and code are publicly available.

## Method Summary
The paper presents a multi-stage annotation framework using a custom HTML tool to evaluate and correct factual errors in LLM-generated text. The method involves decomposing responses into atomic claims, identifying checkworthy statements, retrieving evidence via search engines, classifying stance relationships, and generating corrections. The framework uses semi-automatic components including claim decomposition prompts and evidence retrieval, with human annotators refining outputs. The benchmark dataset consists of 94 ChatGPT responses annotated at three granularity levels (claim, sentence, document) with stance classifications and corrections.

## Key Results
- Popular fact-checkers (FacTool, FactScore, Perplexity.ai) achieve low F1-scores (best 0.53) in detecting false claims
- Automatic evidence retrieval yields 62% relevant results (2057/3305 evidence items)
- Intrinsic evaluation metrics (edit distance, BERTScore) misalign with human preferences for correction quality
- The annotation tool reduces human workload by incorporating automatic intermediate results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing long documents into atomic claims enables fine-grained fact-checking.
- Mechanism: The system breaks down a response into single sentences, then further splits complex sentences into atomic claims, enabling context-independent verification.
- Core assumption: Atomic claims can be verified independently without losing or distorting information from the original context.
- Evidence anchors:
  - [abstract] "decomposing long documents into atomic claims"
  - [section] "The first step is to decompose R into a set of atomic statements that are context-independent and checkable"
  - [corpus] Weak - corpus mentions related work but not this specific mechanism
- Break condition: If decomposition leads to information loss or makes claims uncheckable, the approach fails.

### Mechanism 2
- Claim: Semi-automatic annotation with tool-assisted evidence retrieval reduces human workload.
- Mechanism: The annotation tool incorporates automatic methods for claim decomposition and evidence retrieval, allowing annotators to edit or label based on intermediate outputs rather than starting from scratch.
- Core assumption: Automatic methods can provide useful starting points for human annotation, even if not perfect.
- Evidence anchors:
  - [abstract] "design and build an annotation tool to speed up the labelling procedure and ease the workload of raters"
  - [section] "Annotators can edit or assign labels based on intermediate outputs and also can download annotated results by interfaces"
  - [corpus] Moderate - corpus shows related work on automated fact-checking tools
- Break condition: If automatic results are consistently poor quality, human annotators spend more time correcting than if they started fresh.

### Mechanism 3
- Claim: Fine-grained stance detection improves correction accuracy by distinguishing support levels.
- Mechanism: The system identifies four stance categories (completely support, partially support, refute, irrelevant) to better understand the relationship between claims and evidence.
- Core assumption: More granular stance categories provide better information for correction decisions than binary support/refute classification.
- Evidence anchors:
  - [abstract] "identify the stance of the claim towards the evidence"
  - [section] "we incorporate partially support in addition to support, refute and irrelevant"
  - [corpus] Moderate - corpus shows related work on stance detection in fact-checking
- Break condition: If stance distinctions don't improve correction accuracy or confuse annotators, the added complexity is not justified.

## Foundational Learning

- Concept: Context-independent claim verification
  - Why needed here: Ensures each claim can be verified without requiring the full document context
  - Quick check question: Can a claim be verified if it contains pronouns that reference earlier content?

- Concept: Check-worthiness determination
  - Why needed here: Identifies which claims require verification versus subjective opinions or obvious facts
  - Quick check question: How do you classify "The sky is blue" - check-worthy or not?

- Concept: Evidence stance classification
  - Why needed here: Determines how evidence relates to claims for accurate fact-checking
  - Quick check question: If evidence states "A is CEO of X" and claim states "A is founder of X", what stance applies?

## Architecture Onboarding

- Component map: Response → Decomposition → Check-worthiness → Evidence retrieval → Stance detection → Correction → Output
- Critical path: Response → Decomposition → Check-worthiness → Evidence retrieval → Stance detection → Correction → Output
- Design tradeoffs: Fine-grained vs. coarse-grained verification (precision vs. efficiency)
- Failure signatures: Poor decomposition quality, irrelevant evidence retrieval, stance detection errors
- First 3 experiments:
  1. Test decomposition quality on sample responses with varying complexity
  2. Evaluate evidence relevance scores from automated retrieval
  3. Measure stance detection accuracy with different model configurations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the fact-checking pipeline vary across different domains (e.g., science, history, current events)?
- Basis in paper: [inferred] The paper mentions that certain domains like gene and astronomy require domain knowledge, and that some claims involve expert-level knowledge or minor details known only to a small group of people.
- Why unresolved: The paper does not provide a detailed analysis of performance across different domains. It only mentions specific examples of challenging domains.
- What evidence would resolve it: A comprehensive evaluation of the fact-checking pipeline across a wide range of domains, comparing performance metrics (e.g., F1-score, accuracy) for each domain.

### Open Question 2
- Question: How can the fact-checking pipeline be improved to handle inter-claim dependencies, such as maintaining the overall logic of a procedure or handling implicit claims?
- Basis in paper: [explicit] The paper identifies three challenges related to inter-claim dependencies: (1) checking the overall logic correctness of a procedure, (2) deleting the entire text if the first claim is invalidated, and (3) decontextualizing implicit claims.
- Why unresolved: The paper acknowledges these challenges but does not provide solutions or further exploration of how to address them.
- What evidence would resolve it: Proposed methods or models for handling inter-claim dependencies, along with experimental results demonstrating improved performance on tasks involving procedures or implicit claims.

### Open Question 3
- Question: How can the fact-checking pipeline be improved to handle inter-claim dependencies, such as maintaining the overall logic of a procedure or handling implicit claims?
- Basis in paper: [explicit] The paper identifies three challenges related to inter-claim dependencies: (1) checking the overall logic correctness of a procedure, (2) deleting the entire text if the first claim is invalidated, and (3) decontextualizing implicit claims.
- Why unresolved: The paper acknowledges these challenges but does not provide solutions or further exploration of how to address them.
- What evidence would resolve it: Proposed methods or models for handling inter-claim dependencies, along with experimental results demonstrating improved performance on tasks involving procedures or implicit claims.

### Open Question 4
- Question: How effective are the current intrinsic metrics (e.g., edit distance, semantic similarity) in evaluating the quality of revised responses, and what alternative evaluation methods could be developed?
- Basis in paper: [explicit] The paper shows that intrinsic metrics based on edit distance and semantic similarity are ineffective in evaluating revised responses, misaligning with human preferences.
- Why unresolved: The paper does not propose alternative evaluation methods or explore why the current intrinsic metrics are ineffective.
- What evidence would resolve it: Development and validation of new evaluation metrics that better align with human preferences, along with experimental results demonstrating improved evaluation of revised responses.

## Limitations
- Small dataset size (94 examples) limits generalizability across diverse LLM-generated content
- Limited domain coverage (Covid-19, finance, legal) may not represent full range of factual errors
- Low F1-scores for false claim detection suggest significant challenges in current fact-checking approaches

## Confidence

- High confidence: The annotation framework and tool design are well-specified with clear implementation details provided.
- Medium confidence: The claim that fine-grained decomposition improves fact-checking accuracy is supported but requires validation on larger, more diverse datasets.
- Medium confidence: The effectiveness of semi-automatic annotation in reducing human workload is demonstrated but depends on the quality of automatic components.

## Next Checks

1. **Dataset Generalization**: Test the fact-checking framework on responses from multiple LLM models and diverse domains beyond the current Covid-19, finance, and legal topics to assess robustness.

2. **Annotation Tool Effectiveness**: Conduct a controlled experiment comparing annotation time and accuracy with and without the automatic components to quantify the claimed workload reduction.

3. **Correction Quality Evaluation**: Implement user studies to validate whether the corrected responses are preferred by human readers, as current metrics (edit distance, BERTScore) may not fully capture improvement in factual accuracy.