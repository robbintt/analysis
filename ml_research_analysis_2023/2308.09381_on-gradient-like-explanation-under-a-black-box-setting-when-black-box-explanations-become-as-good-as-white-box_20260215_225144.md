---
ver: rpa2
title: 'On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations
  Become as Good as White-box'
arxiv_id: '2308.09381'
source_url: https://arxiv.org/abs/2308.09381
tags:
- geex
- explanations
- igeex
- explanation
- gradients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GEEX, a black-box method that produces gradient-like
  explanations without internal access to the target model. GEEX estimates gradients
  via natural evolution strategies, sampling noise masks from a Gaussian distribution
  centered at the input and using cross-entropy loss to assess pixel contributions.
---

# On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box

## Quick Facts
- arXiv ID: 2308.09381
- Source URL: https://arxiv.org/abs/2308.09381
- Reference count: 31
- Primary result: GEEX achieves black-box gradient-like explanations close to white-box methods

## Executive Summary
This paper introduces GEEX, a novel black-box method for generating gradient-like explanations without internal access to the target model. By leveraging natural evolution strategies and mask resizing with bilinear interpolation, GEEX estimates gradients through query access only. The integrated variant, iGEEX, combines gradient estimation with integrated gradients to satisfy the four fundamental axioms of attribution. Experiments on MNIST, Fashion MNIST, and ImageNet demonstrate that GEEX and its variants outperform state-of-the-art black-box explainers and achieve performance approaching white-box methods, with mask resizing particularly beneficial for high-dimensional inputs.

## Method Summary
GEEX is a black-box explanation method that estimates gradients without internal model access. It uses natural evolution strategies to sample noise masks from a Gaussian distribution centered at the input, evaluating cross-entropy loss to assess pixel contributions. To address scaling limitations (noisy estimation, exponential sample growth, vanishing gradients), GEEX applies mask resizing with bilinear interpolation, smoothing masks and increasing the likelihood of fully masking local features. The integrated variant, iGEEX, combines estimated gradients with integrated gradients to satisfy sensitivity, insensitivity, implementation invariance, and linearity axioms. The method is evaluated on MNIST, Fashion MNIST, and ImageNet using AOPC scores against baseline methods including SmoothGrad, IG, LIME, and RISE.

## Key Results
- GEEX and GERS outperform state-of-the-art black-box explainers (LIME, RISE) on MNIST, Fashion MNIST, and ImageNet
- Mask resizing notably improves explanation quality for high-dimensional inputs, achieving AOPC scores close to white-box methods
- iGEEX satisfies the four fundamental axioms of attribution (sensitivity, insensitivity, implementation invariance, linearity)
- Performance approaches white-box methods (SmoothGrad, IG) as sample size increases, with GERS showing particular strength on ImageNet

## Why This Works (Mechanism)

### Mechanism 1
Natural Evolution Strategies (NES) can estimate gradients of a black-box model using only query access. NES approximates gradients by sampling noise masks from a Gaussian distribution centered at the input, evaluating cross-entropy loss to assess pixel contributions, and using the log-likelihood trick to compute expected gradients. Core assumption: The search distribution (Gaussian) can be differentiated and its expectation over losses approximates true gradients. Evidence anchors: [abstract] "delivers gradient-like explanations under a black-box setting"; [section] "Natural evolution strategies are algorithms designed for black-box optimization problems... The search gradients with respect to x can be then simplified with the log-likelihood trick". Break condition: If the search distribution is not differentiable or the loss function is too noisy, gradient estimation becomes unreliable.

### Mechanism 2
Mask resizing with bilinear interpolation addresses scaling-effect limitations by smoothing masks and increasing the likelihood of fully masking local features. Initial noise masks are sampled in reduced feature space, then upsampled to target size via bilinear interpolation, reducing search space and smoothing artifacts. Core assumption: Smaller initial masks preserve feature structure while interpolation smooths noise, and upsampling increases probability of full local feature masking. Evidence anchors: [abstract] "mask resizing notably improves explanation quality for high-dimensional inputs"; [section] "Mask resizing tackles all three challenges simultaneously... the smoothed masks (by interpolation) have a denoising effect". Break condition: If resizing ratio is too extreme, either precision is lost (large ratio) or noise is not sufficiently smoothed (small ratio).

### Mechanism 3
Integrating estimated gradients with a path method (iGEEX) satisfies the four axioms of attribution: sensitivity, insensitivity, implementation invariance, and linearity. iGEEX substitutes estimated gradients into the integrated gradients formula, integrating over a straight-line path from baseline to input, which mathematically satisfies the axioms when enough samples are drawn. Core assumption: Estimated gradients converge to true gradients as sample size increases, allowing path integration to recover axiom satisfaction. Evidence anchors: [abstract] "iGEEX satisfies the four fundamental axioms of attribution methods"; [section] "Theorem 1. Integrated GEEX... satisfies Sensitivity, Insensitivity, and Implementation Invariance". Break condition: If gradient estimation is too noisy or sample size too small, integration may amplify errors rather than satisfy axioms.

## Foundational Learning

- Concept: Natural Evolution Strategies for gradient estimation
  - Why needed here: Provides a method to approximate gradients without internal model access, enabling black-box explanations
  - Quick check question: How does the log-likelihood trick simplify gradient computation in NES?

- Concept: Mask resizing and bilinear interpolation
  - Why needed here: Addresses scaling-effect limitations (noisy estimation, exponential sample growth, vanishing gradients) in high-dimensional inputs
  - Quick check question: Why does upsampling small masks with bilinear interpolation reduce noise compared to pixel-wise sampling?

- Concept: Integrated gradients and attribution axioms
  - Why needed here: Combines gradient estimation with a path method to satisfy sensitivity, insensitivity, implementation invariance, and linearity axioms
  - Quick check question: What is the mathematical relationship between integrated gradients and the four attribution axioms?

## Architecture Onboarding

- Component map: Input → Gaussian noise sampling → Cross-entropy loss evaluation → Gradient estimation (NES) → Optional mask resizing → Integrated gradients (iGEEX) → Attribution map output
- Critical path: Noise sampling → Loss evaluation → Gradient estimation → (Optional) mask resizing → Integration path
- Design tradeoffs: Mask resizing trades precision for denoising and scalability; more samples improve accuracy but increase computation
- Failure signatures: Noisy attributions indicate insufficient samples or poor σ choice; vanishing gradients suggest mask resizing ratio too small or features not fully masked
- First 3 experiments:
  1. Test gradient estimation quality with varying sample sizes (m) and search range (σ) on MNIST
  2. Evaluate mask resizing impact by comparing AOPC scores with different resizing ratios (r) on ImageNet
  3. Validate axiom satisfaction by comparing iGEEX explanations to IG on simple datasets with known ground truth

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Limited theoretical validation of gradient estimation fidelity and whether estimated gradients truly approximate actual model gradients
- Mask resizing denoising mechanism lacks rigorous mathematical justification despite empirical improvements
- Axiom satisfaction in iGEEX depends on gradient estimate convergence, which is not thoroughly validated with finite sample sizes

## Confidence
- High Confidence: The implementation details of the GEEX algorithm (noise sampling, cross-entropy evaluation, mask resizing) are clearly specified and reproducible
- Medium Confidence: The empirical results showing GERS outperforming other black-box methods are convincing, though the absolute performance gap to white-box methods remains substantial
- Low Confidence: The theoretical claims about axiom satisfaction in iGEEX and the denoising mechanism of mask resizing require more rigorous mathematical proof and ablation studies

## Next Checks
1. Conduct controlled experiments comparing GEEX-estimated gradients against white-box gradients on a small, interpretable model (e.g., logistic regression) to quantify approximation error
2. Systematically vary the resizing ratio r and interpolation method (bilinear vs. bicubic) to isolate the contribution of each component to explanation quality improvements
3. Test iGEEX on adversarial examples designed to break specific attribution axioms, measuring the conditions under which theorem 1 fails in practice