---
ver: rpa2
title: Attention Mixtures for Time-Aware Sequential Recommendation
arxiv_id: '2304.08158'
source_url: https://arxiv.org/abs/2304.08158
tags:
- recommendation
- context
- item
- sequential
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MOJITO, a novel Transformer-based model for
  time-aware sequential recommendation. MOJITO addresses the limitations of existing
  Transformers by leveraging Gaussian mixtures of attention-based temporal context
  and item embedding representations.
---

# Attention Mixtures for Time-Aware Sequential Recommendation

## Quick Facts
- arXiv ID: 2304.08158
- Source URL: https://arxiv.org/abs/2304.08158
- Reference count: 40
- Primary result: MOJITO achieves state-of-the-art performance in time-aware sequential recommendation with significant improvements in NDCG and HR metrics

## Executive Summary
This paper presents MOJITO, a novel Transformer-based model for time-aware sequential recommendation that addresses the limitations of existing Transformers by leveraging Gaussian mixtures of attention-based temporal context and item embedding representations. The model effectively captures complex dependencies between user preferences and temporal context, enabling accurate predictions of the next items to recommend. Experimental results on real-world datasets from movies, books, and music domains demonstrate that MOJITO outperforms existing Transformers for sequential recommendation, particularly in modeling time-related factors.

## Method Summary
MOJITO extends Transformer architecture by introducing Gaussian mixture sampling for attention matrices, allowing different heads to share global attention patterns while maintaining diversity. The model jointly considers user-item interactions and temporal context to learn two attention matrices that are combined within Gaussian mixtures. It features a two-component design: short-term preferences captured through attention mixture blocks that process recent interaction sequences with temporal context, and long-term preferences captured using an attentive FISM approach that aggregates item representations based on user history. The components are combined with a weighting parameter λ to balance context-dependent and context-independent preferences.

## Key Results
- MOJITO achieves state-of-the-art performance on MovieLens, Amazon Book, and LFM-1b datasets
- Significant improvements in NDCG@10 and HR@10 metrics compared to existing Transformers
- Effective modeling of time-related factors in sequential recommendation
- Successful reduction of attention head redundancy through Gaussian mixture sampling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MOJITO addresses redundant attention heads in Transformers by introducing Gaussian mixtures of attention matrices with different inductive biases.
- Mechanism: Instead of each attention head independently computing its own attention matrix, MOJITO samples from a mixture model where different attention heads can share global attention matrices. This is achieved through Gaussian mixture sampling where heads are guided by different semantic biases (item vs context).
- Core assumption: Attention head redundancy occurs because different heads learn similar patterns when independently computing attention matrices, reducing model efficiency.
- Evidence anchors: The paper references recent efforts from Nguyen et al. [24] and explains how attention heads interact via a shared pool of global attention matrices.

### Mechanism 2
- Claim: MOJITO improves time-aware sequential recommendation by jointly modeling item embeddings and temporal context through mixed attention mechanisms.
- Mechanism: MOJITO uses a Mercer kernel to map temporal context information into compatible embedding space, then combines item and context embeddings before applying self-attention. The attention mechanism samples from Gaussian mixtures that can emphasize either item-item interactions or item-context relationships.
- Core assumption: User preferences for items vary significantly based on temporal context (time of day, day of week, etc.), and these temporal dependencies cannot be captured by relative positional encoding alone.
- Evidence anchors: The paper demonstrates that MOJITO leverages Gaussian mixtures of attention-based temporal context and item embedding representations for sequential modeling.

### Mechanism 3
- Claim: MOJITO captures both short-term and long-term user preferences through separate modeling pathways that are later combined.
- Mechanism: Short-term preferences are captured through the attention mixture mechanism that processes the recent interaction sequence with temporal context. Long-term preferences are captured using an attentive FISM approach that aggregates item representations based on user history, without temporal context. These two pathways are combined with a weighting parameter λ.
- Core assumption: User preferences can be decomposed into context-dependent short-term preferences and context-independent long-term preferences, and both components are necessary for accurate recommendations.
- Evidence anchors: The paper describes MOJITO as involving two components - one modeling short-term intents influenced by interactions and temporal context, and another capturing long-term preferences.

## Foundational Learning

- Concept: Attention mechanisms in Transformers
  - Why needed here: MOJITO is built upon Transformer architecture and extends its attention mechanism to handle temporal context
  - Quick check question: How does multi-head attention work in standard Transformers, and what problem does it solve?

- Concept: Gaussian mixture models
  - Why needed here: MOJITO uses Gaussian mixtures to sample attention matrices, allowing different heads to share global attention patterns while maintaining diversity
  - Quick check question: What is the purpose of using mixture models in probabilistic modeling, and how do they differ from single distribution models?

- Concept: Temporal context modeling in recommendation systems
  - Why needed here: MOJITO specifically addresses the limitation of existing Transformers that overlook temporal context in user interactions
  - Quick check question: What types of temporal information (hour, day, month) are typically relevant for recommendation systems, and how might they influence user preferences?

## Architecture Onboarding

- Component map: Embedding layer -> Attention mixture blocks -> Short-term predictor -> Long-term predictor -> Combination layer -> Loss calculation

- Critical path: Input sequence → Embedding layer → Attention mixture blocks → Short-term prediction → Long-term prediction → Combined prediction → Loss calculation

- Design tradeoffs:
  - Number of attention heads vs. redundancy: More heads increase capacity but risk redundancy without mixture sampling
  - Weight of short-term vs. long-term preferences (λ parameter): Domain-dependent balance between context sensitivity and stable preferences
  - Temporal context granularity: More granular temporal information (hours vs days) increases complexity but may improve accuracy

- Failure signatures:
  - Performance similar to non-time-aware baselines: Indicates temporal context is not being effectively captured
  - Training instability or slow convergence: May indicate issues with Gaussian mixture sampling or attention head interaction
  - One component (short-term or long-term) dominates predictions: May indicate imbalance in λ parameter or one pathway not learning effectively

- First 3 experiments:
  1. Ablation study: Remove temporal context from short-term pathway and compare performance to full model
  2. Head diversity analysis: Measure pairwise L2 distances between attention heads in output layer to verify reduced redundancy
  3. λ sensitivity analysis: Vary the weighting parameter between short-term and long-term predictions across different datasets to identify optimal settings

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the limitations and implications of the work.

## Limitations
- The effectiveness of Gaussian mixture sampling for reducing attention head redundancy lacks theoretical justification
- The Mercer kernel function for temporal context mapping is mentioned but not fully specified
- The assumption that long-term preferences are context-independent may not hold for all domains

## Confidence
- High confidence: MOJITO's architecture and implementation details are clearly described, and the experimental methodology follows standard practices
- Medium confidence: The performance improvements over baselines are significant, but the ablation studies could be more comprehensive to isolate the contribution of each component
- Low confidence: The claim that Gaussian mixtures specifically solve the attention head redundancy problem would benefit from more rigorous analysis of head diversity metrics

## Next Checks
1. Conduct a head diversity analysis by computing pairwise L2 distances between attention heads in output layers to verify that the mixture approach reduces redundancy compared to standard multi-head attention
2. Perform an ablation study removing the temporal context component entirely to quantify its specific contribution to performance gains
3. Test MOJITO on a dataset with minimal temporal variation to verify that the model doesn't degrade performance when temporal context is less relevant