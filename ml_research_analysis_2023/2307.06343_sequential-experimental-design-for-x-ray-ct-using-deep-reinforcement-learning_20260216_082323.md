---
ver: rpa2
title: Sequential Experimental Design for X-Ray CT Using Deep Reinforcement Learning
arxiv_id: '2307.06343'
source_url: https://arxiv.org/abs/2307.06343
tags:
- angles
- policy
- data
- reward
- phantoms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selecting optimal scan angles
  in X-ray CT for efficient 3D reconstruction with limited data. The authors formulate
  this as a sequential experimental design problem and solve it using deep reinforcement
  learning (DRL).
---

# Sequential Experimental Design for X-Ray CT Using Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.06343
- Source URL: https://arxiv.org/abs/2307.06343
- Authors: 
- Reference count: 27
- Key outcome: DRL-based sequential angle selection outperforms equidistant selection in 2D CT, especially for non-uniform informative angles, with generalization to unseen rotations but reduced gains with noise.

## Executive Summary
This paper addresses the problem of selecting optimal scan angles in X-ray CT for efficient 3D reconstruction with limited data. The authors formulate this as a sequential experimental design problem and solve it using deep reinforcement learning (DRL). They pose the problem as a partially observable Markov decision process (POMDP) in a Bayesian framework and train a DRL agent to learn adaptive policies for angle selection. The core method uses an Actor-Critic approach, where an encoder network extracts features from reconstructions, and separate actor and critic networks parameterize the policy and state-value function, respectively. The agent selects angles sequentially based on current reconstruction quality and receives rewards based on improvement in reconstruction (measured by PSNR). Numerical experiments on 2D tomography with synthetic data show that the learned policies outperform uninformed equidistant angle selection, especially for phantoms with non-uniform informative angles. The policies also generalize to unseen phantom rotations. However, measurement noise reduces the performance gain. The work demonstrates the potential of DRL for efficient adaptive angle selection in CT, with possible extensions to more complex geometries and noise handling.

## Method Summary
The method formulates sequential angle selection for X-ray CT as a partially observable Markov decision process (POMDP) in a Bayesian framework. A deep reinforcement learning agent, trained using an Actor-Critic approach, learns to select angles sequentially to maximize reconstruction quality. The agent uses an encoder network to extract features from current reconstructions, concatenates these with the history of selected angles, and passes them to separate actor and critic networks. The actor network outputs a probability distribution over angles for selection, while the critic network estimates the state-value function. The agent is trained using a reward function based on improvement in reconstruction quality (measured by PSNR), with the goal of finding optimal policies that outperform traditional equidistant angle selection, especially for phantoms with non-uniform informative angles.

## Key Results
- DRL-based sequential angle selection outperforms equidistant selection in 2D CT, especially for phantoms with non-uniform informative angles
- Learned policies generalize to unseen phantom rotations, demonstrating robustness
- Measurement noise reduces the performance gain of adaptive selection, but training on clean data improves performance on noisy measurements
- The Actor-Critic framework effectively balances exploration and exploitation in angle selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sequential angle selection problem is effectively modeled as a POMDP because the agent cannot directly observe the ground truth image.
- Mechanism: The agent operates on reconstructions as belief states, maintaining a history of selected angles and corresponding measurements. This allows the agent to implicitly track information gain without requiring direct access to the true underlying image.
- Core assumption: The reconstruction quality (belief state) contains sufficient information to guide angle selection decisions, and the history of measurements can be encoded in the reconstruction.
- Evidence anchors:
  - [abstract] "we pose the OED problem as a partially observable Markov decision process in a Bayesian framework"
  - [section III-A] "we adopt a Bayesian OED framework and model it as a POMDP"
- Break condition: If the reconstruction algorithm fails to capture essential information about the object, or if the belief state becomes too uncertain to guide decisions effectively.

### Mechanism 2
- Claim: Actor-Critic reinforcement learning enables non-greedy, adaptive angle selection by balancing exploration and exploitation.
- Mechanism: The actor network outputs a probability distribution over angles, allowing stochastic exploration, while the critic network estimates state values to guide policy updates. The TD error provides a learning signal that considers both immediate rewards and future expected rewards.
- Core assumption: The policy gradient computed via the Actor-Critic method converges to an optimal or near-optimal policy given sufficient training data and episodes.
- Evidence anchors:
  - [abstract] "We use a policy training method based on the Actor-Critic approach"
  - [section III-B] "The Actor-Critic method is a novel category in the field of reinforcement learning for computing the policy gradient"
- Break condition: If the reward signal is sparse or delayed, or if the state representation is insufficient for the critic to accurately estimate value functions.

### Mechanism 3
- Claim: The encoder network extracts relevant features from high-dimensional reconstructions, improving learning efficiency and policy generalization.
- Mechanism: The shared encoder network processes the current reconstruction through convolutional layers to produce a compact feature representation, which is then concatenated with the action history and fed into separate actor and critic networks.
- Core assumption: The bottleneck features from the encoder contain sufficient information about the reconstruction quality and object characteristics to guide angle selection.
- Evidence anchors:
  - [section III-C] "The proposed method requires the agent to extract relevant features from high-dimensional images to increase learning efficiency, which is accomplished using a deep neural encoder network"
  - [section III-C] "The proposed model adopts a shared encoder network between the actor and critic networks"
- Break condition: If the encoder architecture is too shallow to capture important features, or if the feature dimensionality is mismatched with the actor/critic network inputs.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The sequential angle selection problem is formulated as a POMDP because the agent cannot directly observe the ground truth image, only measurements and reconstructions.
  - Quick check question: What is the key difference between MDPs and POMDPs in terms of state observability?

- Concept: Bayesian Experimental Design (OED) and information gain metrics
  - Why needed here: The paper uses a Bayesian framework to quantify information gain from angle selection, with rewards based on reconstruction quality improvement.
  - Quick check question: How does A-optimality differ from D-optimality in Bayesian OED?

- Concept: Reinforcement Learning fundamentals: policy gradients, value functions, and Actor-Critic methods
  - Why needed here: The paper employs Actor-Critic RL to learn a policy for sequential angle selection, balancing immediate rewards with future expected rewards.
  - Quick check question: What role does the TD error play in the Actor-Critic method?

## Architecture Onboarding

- Component map:
  - Synthetic phantom generation -> Projection data creation (ASTRA Toolbox) -> SIRT reconstruction with box constraints -> Encoder network processing -> Actor network angle selection -> Measurement gathering -> Reconstruction update -> Reward computation and TD error calculation -> Policy and value function updates

- Critical path:
  1. Generate phantom and create measurement data
  2. Initialize reconstruction and angle selection history
  3. Pass reconstruction through encoder network
  4. Concatenate encoder output with angle history
  5. Actor network selects next angle
  6. Gather measurements and update reconstruction
  7. Compute reward and TD error
  8. Update policy and value function parameters

- Design tradeoffs:
  - Reconstruction method: SIRT chosen for simplicity vs. deep learning-based methods for potentially better performance
  - Reward function: End-to-end vs. incremental settings affecting learning stability and convergence speed
  - Network architecture: Shared encoder vs. separate encoders for actor and critic
  - Exploration strategy: Softmax policy vs. epsilon-greedy or other exploration methods

- Failure signatures:
  - Policy converges to always selecting the same angle (insufficient exploration)
  - Training loss plateaus early (learning rate too low or architecture too simple)
  - Test performance much worse than training performance (overfitting or poor generalization)
  - No improvement over equidistant baseline (reward signal not informative or state representation inadequate)

- First 3 experiments:
  1. Train and evaluate on circles dataset with 3 angles, comparing end-to-end vs. incremental reward settings
  2. Test generalization to unseen rotations using ellipses dataset with 4 angles
  3. Evaluate performance with noisy measurements on mixed phantoms dataset with 5 angles

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the presence of measurement noise specifically impact the performance gap between the equidistant and Actor-Critic policies in X-ray CT adaptive angle selection?
- Basis in paper: [explicit] The paper states that "adding noise to the measurements reduces the performance gap between the policies" and that "the modelâ€™s performance on noisy measurements improves when testing the model trained on clean measurements compared to the model trained on noisy measurements."
- Why unresolved: The paper identifies the impact of noise but does not provide a detailed quantitative analysis of how different noise levels affect the performance gap between the two policies.
- What evidence would resolve it: Systematic experiments varying noise levels (e.g., 1%, 3%, 5%, 10%) and measuring the performance gap between equidistant and Actor-Critic policies across these levels would clarify the relationship.

### Open Question 2
- Question: Can the Actor-Critic approach be effectively extended to 3D geometries with additional degrees of freedom like tilting and zooming in industrial CT applications?
- Basis in paper: [explicit] The authors mention in the discussion section that "in the future, we will extend the approach to more complex and realistic 3D geometries with additional degrees of freedom, such as tilting and zooming."
- Why unresolved: The paper only demonstrates the method on 2D parallel-beam geometry and does not validate its performance on more complex 3D scenarios.
- What evidence would resolve it: Implementing the Actor-Critic approach on 3D CT data with tilting and zooming capabilities, and comparing its performance against traditional methods, would demonstrate feasibility and effectiveness.

### Open Question 3
- Question: What is the optimal reward function design that combines the fast convergence of the incremental reward with the high average performance of the end-to-end reward in the Actor-Critic framework?
- Basis in paper: [explicit] The authors note that "the incremental reward function demonstrates faster convergence during training" while "the end-to-end reward function achieves the highest average performance on both the training and test datasets," and express interest in "designing reward functions that share both of these desirable properties."
- Why unresolved: The paper uses two separate reward functions but does not explore hybrid or alternative designs that might combine their benefits.
- What evidence would resolve it: Developing and testing hybrid reward functions (e.g., weighted combinations of incremental and end-to-end rewards, or multi-objective optimization approaches) and comparing their convergence speed and final performance against the existing reward functions would identify optimal designs.

## Limitations
- Performance degradation with measurement noise is significant but poorly characterized; the paper reports "reduced performance gain" without quantifying the relationship between noise level and effectiveness
- Generalization to 3D CT and real-world noise distributions remains unproven
- The specific encoder architecture details (layer sizes, filter counts) are not fully specified, potentially affecting reproducibility

## Confidence
- **High Confidence**: The POMDP formulation and Actor-Critic framework are theoretically sound and well-established in the literature
- **Medium Confidence**: The synthetic data experiments demonstrate the method works as intended in idealized conditions, but real-world applicability is uncertain
- **Low Confidence**: Claims about generalization to unseen phantom rotations are based on limited test cases and may not hold for more complex objects

## Next Checks
1. **Noise Sensitivity Analysis**: Systematically evaluate performance across multiple noise levels to quantify the trade-off between adaptive selection benefits and measurement noise
2. **3D Extension Test**: Implement a 3D version of the method using the same framework to assess scalability and identify new challenges
3. **Real Data Validation**: Test the approach on real CT measurement data from the ASTRA toolbox dataset to evaluate practical performance beyond synthetic phantoms