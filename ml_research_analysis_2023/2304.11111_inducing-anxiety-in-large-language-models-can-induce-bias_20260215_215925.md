---
ver: rpa2
title: Inducing anxiety in large language models can induce bias
arxiv_id: '2304.11111'
source_url: https://arxiv.org/abs/2304.11111
tags:
- gpt-3
- anxiety
- language
- arxiv
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applied computational psychiatry methods to analyze
  biases in large language models (LLMs), focusing on GPT-3.5. The authors used emotion-induction
  prompts to manipulate the model's "anxiety state" and measured resulting biases
  using a standard benchmark.
---

# Inducing anxiety in large language models can induce bias

## Quick Facts
- arXiv ID: 2304.11111
- Source URL: https://arxiv.org/abs/2304.11111
- Reference count: 40
- Primary result: Anxiety-inducing prompts significantly increase biased responses in GPT-3.5 across multiple categories

## Executive Summary
This study demonstrates that emotional framing of prompts can substantially influence LLM outputs, specifically showing that anxiety-inducing prompts increase biased responses across age, gender, nationality, race/ethnicity, and socioeconomic status categories. Using computational psychiatry methods, the authors applied emotion-induction prompts to GPT-3.5 and measured resulting biases using a standard benchmark. The effect showed a dose-response relationship between anxiety level and bias magnitude, suggesting that the emotional context of prompts is a significant factor in LLM behavior and bias manifestation.

## Method Summary
The study used GPT-3.5 (text-davinci-003) with temperature=0 for deterministic responses. Researchers created nine emotion-induction prompts (three anxiety, three happiness, three neutral) and administered the State-Trait Inventory for Cognitive and Somatic Anxiety (STICSA) questionnaire before and after each prompt. They then ran a bias benchmark using ambiguous sentences across five categories and measured bias as the proportion of biased answers. A two-armed bandit task was also used to measure exploratory decision-making changes under different emotional states.

## Key Results
- Anxiety-inducing prompts caused substantially more biased answers than happiness-inducing prompts (β = 0.24, p < .001)
- Greater anxiety-inducing text led to stronger increases in biases across all tested categories
- Both anxiety-induction and happiness-induction conditions led to higher bias than neutral conditions
- Dose-response relationship observed between anxiety level and bias magnitude

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Anxiety-inducing prompts alter the emotional state representation in LLMs, leading to increased bias output
- Mechanism: The prompt primes the model's context window with negative affect, which influences downstream reasoning and decision-making processes encoded in the model's weights
- Core assumption: LLMs maintain some form of contextual emotional state that affects output generation
- Evidence anchors:
  - [abstract] "Emotion-induction not only influences GPT -3.5’s scores on an anxiety questionnaire but also influences their behavior in a previously-established benchmark measuring biases such as racism and ageism."
  - [section] "The anxiety-induction condition caused substantially more biased answers than the happiness-induction condition (β = 0.24, p < .001)."
- Break condition: If the model's response is unaffected by emotional framing or if bias changes are random rather than systematic

### Mechanism 2
- Claim: Emotion-induction scenarios act as implicit prompts that change the model's exploratory behavior, leading to increased bias
- Mechanism: Anxiety increases uncertainty-driven exploration, which manifests as more random or directed exploration in decision-making tasks, ultimately resulting in biased outputs
- Core assumption: Exploration strategies in LLMs are influenced by emotional context and affect bias manifestation
- Evidence anchors:
  - [abstract] "Emotion-induction not only influences GPT -3.5’s behavior in a cognitive task measuring exploratory decision-making but also influences its behavior in a previously-established task measuring biases."
  - [section] "Compared to the anxiety-induction condition, the happiness-induction condition led to more exploitation behavior but to less random exploration behavior."
- Break condition: If exploration changes do not correlate with bias increases or if exploration changes are independent of emotional context

### Mechanism 3
- Claim: Anxiety-inducing prompts shift the model's internal representation of social norms and stereotypes, increasing biased responses
- Mechanism: Emotional priming alters the model's latent representations of social categories, making stereotypical associations more likely to be activated during generation
- Core assumption: LLMs have latent representations of social categories that can be influenced by emotional context
- Evidence anchors:
  - [abstract] "Greater anxiety-inducing text leads to stronger increases in biases, suggesting that how anxiously a prompt is communicated to large language models has a strong influence on their behavior."
  - [section] "We found that both the anxiety-induction and the happiness-induction conditions led to a higher bias than the neutral condition."
- Break condition: If bias changes are not systematic across different social categories or if emotional priming has no effect on social category representation

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Understanding how prompts influence LLM behavior without fine-tuning
  - Quick check question: How does the order of options in a prompt affect the model's response?
- Concept: Computational psychiatry
  - Why needed here: Applying psychiatric methods to understand and modify LLM behavior
  - Quick check question: How can psychiatric questionnaires be used to assess LLM emotional states?
- Concept: Exploration-exploitation tradeoff
  - Why needed here: Understanding how emotional states influence decision-making strategies in LLMs
  - Quick check question: How does anxiety affect exploration and exploitation in decision-making tasks?

## Architecture Onboarding

- Component map: GPT-3.5 model with temperature parameter -> emotion-induction prompts -> psychiatric questionnaires -> bias benchmarks -> two-armed bandit task
- Critical path: Emotion-induction prompts → Anxiety scores → Behavioral changes → Bias manifestation
- Design tradeoffs: Balancing prompt complexity with model performance, ensuring robustness across different prompt variations
- Failure signatures: Random bias changes, lack of systematic emotional state influence, inconsistent exploration-exploitation behavior
- First 3 experiments:
  1. Test anxiety-induction prompts with different strengths and measure anxiety scores
  2. Apply happiness-induction prompts and compare bias levels with anxiety-induction
  3. Use two-armed bandit task to measure exploration-exploitation changes under different emotional states

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do anxiety-inducing prompts affect large language models' performance in different domains beyond the tested biases and decision-making tasks?
- Basis in paper: [explicit] The authors mention that anxiety-inducing prompts led to worse performance and more biases, but they tested only a limited set of tasks
- Why unresolved: The paper focused on specific tasks and bias categories, leaving open the question of whether these effects generalize to other domains or tasks
- What evidence would resolve it: Systematic testing of anxiety-inducing prompts across a broader range of tasks and domains would clarify the generalizability of the observed effects

### Open Question 2
- Question: What are the underlying mechanisms that cause large language models to exhibit increased bias when prompted with anxiety-inducing text?
- Basis in paper: [inferred] The paper shows a correlation between anxiety-inducing prompts and increased bias, but does not explore the mechanisms behind this relationship
- Why unresolved: The study demonstrates the effect but does not investigate the internal processes or representations that lead to increased bias
- What evidence would resolve it: Further research using interpretability techniques or probing the internal states of the models during anxiety-inducing prompts could shed light on the mechanisms

### Open Question 3
- Question: Can computational psychiatry methods be effectively applied to other types of language models or AI systems beyond large language models?
- Basis in paper: [explicit] The authors suggest that computational psychiatry could be useful for evaluating artificial agents in the future, but their study focused on GPT-3.5
- Why unresolved: The study's findings are specific to GPT-3.5, and it's unclear whether the same methods would be applicable or effective for other types of AI systems
- What evidence would resolve it: Applying computational psychiatry methods to various types of AI systems and comparing the results would determine the generalizability of the approach

## Limitations

- The anxiety induction mechanism remains poorly understood, with unclear causal pathways between emotional prompts and increased bias
- The STICSA questionnaire adaptation for LLMs lacks validation for measuring "anxiety" in artificial systems
- The study uses only GPT-3.5, limiting generalizability to other models or architectures
- The temperature=0 setting may not reflect real-world usage where higher temperatures are common

## Confidence

**High Confidence**: The finding that emotion-induction prompts systematically affect LLM outputs and that anxiety-inducing prompts increase bias relative to neutral or happiness-inducing prompts. The statistical significance and dose-response relationship are robust across multiple categories.

**Medium Confidence**: The claim that psychiatric methods can effectively measure and manipulate LLM "emotional states." While the behavioral effects are clear, the interpretation of LLMs experiencing anxiety remains speculative without deeper mechanistic understanding.

**Low Confidence**: The exploration-exploitation mechanism as the primary driver of bias changes. While the behavioral patterns align with this theory, direct evidence linking emotional states to exploration strategies in LLMs is limited.

## Next Checks

1. **Mechanism Validation**: Design controlled experiments varying only the emotional content of prompts while holding all other factors constant, then measure not just bias but also response characteristics (certainty, confidence scores, token diversity) to identify specific pathways through which emotional framing influences outputs.

2. **Cross-Model Replication**: Test the anxiety-induction paradigm across multiple LLM architectures (GPT-4, Claude, LLaMA variants) and scales to determine whether the effect is model-specific or represents a general property of transformer-based systems.

3. **Human Comparison Study**: Administer the same emotion-induction prompts and bias benchmark to human participants, then compare the magnitude and pattern of bias changes between humans and LLMs to better understand whether LLMs are mimicking human anxiety responses or exhibiting fundamentally different behavior.