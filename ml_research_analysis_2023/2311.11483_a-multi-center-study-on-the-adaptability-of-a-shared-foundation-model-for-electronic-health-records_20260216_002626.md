---
ver: rpa2
title: A Multi-Center Study on the Adaptability of a Shared Foundation Model for Electronic
  Health Records
arxiv_id: '2311.11483'
source_url: https://arxiv.org/abs/2311.11483
tags:
- fmsm
- foundation
- mimic
- pretraining
- external
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This multi-center study evaluated the adaptability of a shared
  foundation model (FM) trained on Stanford Medicine EHR data across two diverse hospital
  datasets (SickKids and MIMIC-IV). The FMSM was compared against locally trained
  models for 8 clinical prediction tasks.
---

# A Multi-Center Study on the Adaptability of a Shared Foundation Model for Electronic Health Records

## Quick Facts
- arXiv ID: 2311.11483
- Source URL: https://arxiv.org/abs/2311.11483
- Reference count: 40
- A shared foundation model trained on Stanford data can match locally trained models while using far fewer labels, with 60-90% better sample efficiency than training from scratch.

## Executive Summary
This multi-center study evaluated whether a foundation model trained on Stanford Medicine EHR data could be effectively adapted to predict clinical outcomes at two diverse hospital datasets (SickKids and MIMIC-IV). The FMSM was compared against locally trained models for 8 clinical prediction tasks. Results showed that adapting the off-the-shelf FMSM matched locally trained GBM models while providing a 13% improvement in AUROC when training data was limited. With continued pretraining on local data, the FMSM required fewer than 1% of training examples to match fully trained GBMs. Continued pretraining was 60-90% more sample-efficient than training local FMs from scratch.

## Method Summary
The study evaluated a shared foundation model (FMSM) pretrained on Stanford Medicine EHR data across two diverse hospital datasets. FMSM was used as a frozen feature encoder with logistic regression task heads, compared against locally trained GBM models and foundation models. The evaluation included 8 clinical prediction tasks across SickKids (pediatric) and MIMIC-IV (adult ICU) datasets, measuring AUROC and ECE performance in few-shot settings (2-1024 examples) and testing sample efficiency of continued pretraining versus training from scratch.

## Key Results
- Adapting the off-the-shelf FMSM matched GBM performance while providing 13% AUROC improvement in few-shot settings
- With continued pretraining, FMSM required fewer than 1% of training examples to match fully trained GBMs
- Continued pretraining was 60-90% more sample-efficient than training local foundation models from scratch

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapting an externally trained EHR foundation model matches local GBM performance while using far fewer task-specific training labels.
- Mechanism: The external foundation model encodes general EHR patterns from a large, diverse patient population. When frozen and used as a feature encoder, it produces rich patient representations that a simple linear classifier can adapt to new tasks efficiently, reducing reliance on large labeled datasets.
- Core assumption: The general EHR representations learned on Stanford data are sufficiently transferable to different hospital coding systems and patient populations.
- Evidence anchors:
  - [abstract] "adapting the off-the-shelf FMSM matched the performance of GBM models locally trained on all data while providing a 13% improvement in settings with few task-specific training labels"
  - [section] "adapting an off-the-shelf external foundation model (FMSM) can yield comparable discrimination and better calibration compared to baseline GBM models locally trained on all available data at each site"
- Break condition: If the local hospital's coding system or patient demographics are so different that the general representations fail to capture meaningful local patterns.

### Mechanism 2
- Claim: Continued pretraining of the external foundation model on local hospital data dramatically improves label efficiency.
- Mechanism: Fine-tuning the frozen foundation model weights on local EHR data allows the model to adjust to site-specific patterns while retaining the general medical knowledge. This adaptation makes the representations more relevant for local tasks, reducing the number of task-specific examples needed for good performance.
- Core assumption: The external foundation model has sufficient capacity to absorb local patterns through continued pretraining without catastrophic forgetting.
- Evidence anchors:
  - [abstract] "With continued pretraining on local data, label efficiency substantially improved, such that FMSM required fewer than 1% of training examples to match the fully trained GBM's performance"
  - [section] "continued pretraining of FMSM requires 60 to 90% less patient data than local pretraining from scratch to achieve the same level of performance"
- Break condition: If the local data is too small or too different to meaningfully update the model, or if pretraining parameters are poorly tuned.

### Mechanism 3
- Claim: Continued pretraining is significantly more sample-efficient than training a local foundation model from scratch.
- Mechanism: Starting from a pre-trained model provides a strong prior that can be refined with local data, whereas training from scratch requires learning all patterns from limited local data. This results in better performance with fewer samples.
- Core assumption: The knowledge in the pre-trained model is valuable and can be effectively adapted to the local domain.
- Evidence anchors:
  - [abstract] "Continued pretraining was also 60 to 90% more sample-efficient than training local foundation models from scratch"
  - [section] "AUROCs for external foundation models (FMSM and FMSM+) were significantly better than local foundation models with very small subsamples (up to 1% for SickKids and 0.1% for MIMIC)"
- Break condition: If the local pretraining dataset is large enough to train a competitive model from scratch, or if the domain shift is minimal.

## Foundational Learning

- Concept: Transformer-based self-supervised learning on EHR sequences
  - Why needed here: The foundation model uses autoregressive pretraining on longitudinal medical records to learn rich patient representations that capture temporal dependencies and medical concepts.
  - Quick check question: Can you explain how masked language modeling or next-token prediction works on structured EHR data?

- Concept: Few-shot learning and label efficiency
  - Why needed here: The study focuses on adapting models with very few labeled examples, so understanding how pretraining enables learning from limited data is crucial.
  - Quick check question: Why does a pre-trained model require fewer labeled examples than training from scratch?

- Concept: Domain adaptation and distribution shift
  - Why needed here: The model must perform well across different hospitals with different coding systems and patient populations, requiring understanding of how models handle distribution shifts.
  - Quick check question: What are some techniques to measure and mitigate distribution shift in healthcare data?

## Architecture Onboarding

- Component map:
  - External foundation model (FMSM) - frozen transformer encoder
  - Linear task heads (logistic regression) for downstream tasks
  - Continued pretraining module for local adaptation
  - Baseline GBM models for comparison
  - Count-based featurization pipeline for GBM baselines

- Critical path:
  1. Load external foundation model weights
  2. Generate patient representations using the frozen encoder
  3. Train linear task heads on task-specific data
  4. For continued pretraining: fine-tune encoder on local data, then repeat step 2 and 3
  5. Evaluate on held-out test set

- Design tradeoffs:
  - Freezing the encoder vs. fine-tuning all layers (speed vs. adaptation)
  - Linear task heads vs. deeper task-specific networks (simplicity vs. performance)
  - Continued pretraining vs. training from scratch (data efficiency vs. maximum local performance)

- Failure signatures:
  - Poor performance despite large local datasets (model capacity bottleneck)
  - Performance degrades after continued pretraining (overfitting or catastrophic forgetting)
  - Significant calibration errors (model overconfident or underconfident)

- First 3 experiments:
  1. Adapt the frozen external foundation model to one simple task (e.g., in-hospital mortality) and compare against a GBM baseline
  2. Perform continued pretraining on a small subset of local data and evaluate label efficiency on the same task
  3. Vary the proportion of pretraining data used for continued pretraining and plot performance vs. sample size to confirm sample efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do EHR foundation models perform across different types of hospitals beyond the pediatric and adult ICU settings studied here?
- Basis in paper: [explicit] The paper notes that "a single external foundation model can perform well in a Canadian pediatric cohort and an American adult ICU-based cohort" but suggests this might not generalize to other hospital types.
- Why unresolved: The study only evaluated two specific hospital settings with different patient populations, and it's unclear if the results would hold for other hospital types like rural hospitals, specialty clinics, or hospitals in different countries.
- What evidence would resolve it: Additional multi-center studies across diverse hospital types and care settings, including different specialties, geographic locations, and resource levels.

### Open Question 2
- Question: What is the optimal strategy for deciding between adapting an existing EHR foundation model versus pretraining from scratch based on available data?
- Basis in paper: [explicit] The authors state "Our findings also provided insights into when it is beneficial to adapt an existing EHR foundation model vs. pretraining from scratch, depending on data availability" but don't provide clear decision criteria.
- Why unresolved: The paper shows that continued pretraining is more efficient but doesn't establish clear thresholds or guidelines for when each approach is optimal based on factors like dataset size, diversity, or computational resources.
- What evidence would resolve it: Systematic studies mapping specific data characteristics and computational constraints to optimal model development strategies, potentially including decision frameworks or cost-benefit analyses.

### Open Question 3
- Question: How can the privacy and ethical challenges of sharing EHR foundation models be effectively addressed?
- Basis in paper: [explicit] The authors acknowledge "privacy and ethical use of the underlying data" as a key concern and note that "issues like potential misuse (e.g., for surveillance) and the challenge of algorithmic biases remain open research questions."
- Why unresolved: While the paper mentions potential solutions like de-identification and consent, it doesn't provide concrete methods for ensuring privacy, preventing misuse, or addressing algorithmic bias in shared models.
- What evidence would resolve it: Development and validation of robust privacy-preserving techniques, bias detection and mitigation methods, and governance frameworks specifically designed for shared medical foundation models.

## Limitations
- The study only evaluated two hospital sites (pediatric and adult ICU), limiting generalizability to other hospital types
- Performance depends heavily on the diversity and representativeness of the Stanford Medicine training data
- The study does not address privacy concerns or regulatory requirements for sharing foundation models across institutions

## Confidence

- **High Confidence**: The finding that FMSM adaptation matches local GBM performance when sufficient training data is available, supported by consistent results across both hospital datasets and multiple clinical tasks.
- **Medium Confidence**: The 60-90% sample efficiency improvement for continued pretraining versus training from scratch, as this result depends on specific hyperparameter choices and the quality of the pretraining dataset.
- **Medium Confidence**: The 13% AUROC improvement in few-shot settings, as this finding requires careful control of random seeds and subsampling procedures to be reliably reproduced.

## Next Checks
1. Evaluate the FMSM's performance on additional hospital datasets with different coding systems and patient demographics to test generalizability beyond the two sites studied.

2. Conduct ablation studies on the pretraining dataset size and composition to determine the minimum requirements for achieving the reported sample efficiency improvements.

3. Test the model's performance on time-sensitive clinical tasks where rapid adaptation is critical, measuring both prediction accuracy and adaptation time compared to training from scratch.