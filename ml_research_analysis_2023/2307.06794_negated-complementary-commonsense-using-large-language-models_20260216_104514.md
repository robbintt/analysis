---
ver: rpa2
title: Negated Complementary Commonsense using Large Language Models
arxiv_id: '2307.06794'
source_url: https://arxiv.org/abs/2307.06794
tags:
- negated
- question
- complementary
- questions
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a limitation in large language models (LLMs)
  when answering negated complementary questions in commonsense scenarios. The authors
  propose a model-agnostic methodology using chain-of-thought prompting with negation
  logic and post-processing to improve LLM performance on such questions.
---

# Negated Complementary Commonsense using Large Language Models

## Quick Facts
- arXiv ID: 2307.06794
- Source URL: https://arxiv.org/abs/2307.06794
- Reference count: 8
- One-line primary result: Chain-of-thought prompting with negation logic and post-processing improves LLM performance on negated complementary questions by over 11 percentage points

## Executive Summary
This paper addresses a significant limitation in large language models (LLMs) when answering negated complementary questions in commonsense scenarios. The authors identify that LLMs struggle with questions like "What cannot be a curved yellow fruit?" despite being able to answer the standard version "What can be a curved yellow fruit?" They propose a model-agnostic methodology using chain-of-thought prompting with explicit negation logic and post-processing validation to improve performance on such questions.

Experiments on the ATOMIC-2020 dataset demonstrate that the proposed method outperforms the few-shot approach by more than 11 percentage points on negated complementary questions while maintaining similar performance on standard questions. The work highlights the importance of studying LLM responses to negated complementary questions and provides a practical solution to enhance performance on this task, contributing to a deeper understanding of LLM reasoning capabilities.

## Method Summary
The authors propose a methodology to improve LLM performance on negated complementary questions in commonsense knowledge. The approach consists of two main components: an input prompting technique and a post-processing module. The input prompt uses chain-of-thought prompting with explicit negation logic, guiding the LLM through a structured reasoning process that first answers the standard question, then applies negation. The post-processing module validates generated answers by feeding question-answer pairs back to the LLM for self-assessment. The methodology was evaluated on the ATOMIC-2020 dataset using GPT-3 (text-davinci-002), with human annotation via Amazon MTurk to assess answer correctness.

## Key Results
- The proposed method outperforms few-shot generation from GPT-3 by more than 11 percentage points on negated complementary questions
- Both the negation logic and post-processing steps contribute to improved results according to ablation studies
- The methodology maintains similar performance on standard questions while significantly improving performance on negated complementary questions
- Human evaluation with Krippendorff's alpha above 0.667 indicates reliable annotation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negation logic explicitly encoded in chain-of-thought prompts improves LLM performance on negated complementary questions.
- Mechanism: By structuring the prompt to first answer the standard question, then explicitly negate the answer using logical reasoning, the LLM is guided to produce correct responses for the negated version.
- Core assumption: LLMs have sufficient latent knowledge of commonsense facts but need explicit logical guidance to apply negation correctly.
- Evidence anchors:
  - [abstract] "Our method outperforms few-shot generation from GPT-3 (by more than 11 points) and, more importantly, highlights the significance of studying the response of large language models in negated complementary questions."
  - [section] "Our method can mainly be attributed to the specific chain-of-thought prompting with negation logic description"

### Mechanism 2
- Claim: Post-processing by validating question-answer pairs improves accuracy by filtering incorrect responses.
- Mechanism: The model is asked to evaluate whether a generated answer is correct for the given question. Only answers that pass this validation are retained.
- Core assumption: GPT-3 can accurately self-assess the correctness of its own outputs when prompted appropriately.
- Evidence anchors:
  - [abstract] "Ablation studies indicate that both the negation logic and post-processing steps contribute to the improved results."
  - [section] "Inspired by (Kadavath et al., 2022), we feed the question and answer pair back to the GPT-3 model and ask if it considers a question/answer pair correct."

### Mechanism 3
- Claim: The ATOMIC-2020 dataset's unbalanced representation of negated scenarios creates performance issues that the proposed methodology addresses.
- Mechanism: By explicitly constructing negated complementary questions and providing targeted prompting, the methodology compensates for the dataset's bias toward positive/normal scenarios.
- Core assumption: The underlying issue is data imbalance rather than fundamental LLM limitations.
- Evidence anchors:
  - [section] "Most of the text available on the web contains information supporting answers to 'positive' questions, like, how to do things or where to go, not to questions such as how things could not be done or where not to go. It results in an imbalance of the training datasets"
  - [section] "The worse-performer triples are intuitively more common in the normal format in written language than their negated complementary versions, which can result in unbalanced training data."

## Foundational Learning

- Concept: Set theory and logical negation
  - Why needed here: The paper's core mechanism relies on understanding complements and negation in answer sets (Equation 1: NC = V ∩ A′)
  - Quick check question: If A is the set of valid answers to "What can be a curved yellow fruit?", what is A′ in the universal set of all answers?

- Concept: Chain-of-thought prompting methodology
  - Why needed here: The proposed solution uses structured prompting with reasoning steps to guide the LLM's output
  - Quick check question: What are the five sequential steps in the proposed chain-of-thought prompt structure?

- Concept: Commonsense knowledge representation
  - Why needed here: The paper operates on commonsense knowledge graphs where relationships between concepts need to be understood
  - Quick check question: In the triple format ⟨head-relation-tail⟩, what would be the standard and negated complementary questions for ⟨banana-CanBe-fruit⟩?

## Architecture Onboarding

- Component map: Input processing (Triple-to-question verbalization) -> Prompt generation (Chain-of-thought with negation logic) -> LLM inference (GPT-3 text-davinci-002) -> Post-processing (Self-validation filtering) -> Evaluation (Human annotation with Amazon MTurk)

- Critical path: Prompt generation → LLM inference → Post-processing → Evaluation

- Design tradeoffs:
  - Complexity vs. performance: The chain-of-thought approach adds prompt complexity but significantly improves accuracy
  - Cost vs. benefit: GPT-3 API calls are expensive; the methodology must justify the additional computational expense
  - Generalization vs. specificity: The approach is model-agnostic but tuned for GPT-3's specific capabilities

- Failure signatures:
  - High variance in human evaluation scores (Krippendorff's alpha below 0.667)
  - Post-processing filter rejecting >50% of generated answers
  - Performance degradation on standard questions (as observed in the paper)

- First 3 experiments:
  1. Implement basic chain-of-thought prompting without negation logic and measure performance difference
  2. Add the self-validation post-processing step and evaluate its impact on accuracy
  3. Test the full methodology on a subset of relations and compare with few-shot baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on negated complementary questions vary across different domains or datasets beyond commonsense knowledge?
- Basis in paper: [explicit] The paper acknowledges the limitation of experiments to commonsense scenarios and English language, suggesting the need for further investigation in other contexts and languages.
- Why unresolved: The current study focuses on a specific domain (commonsense) and language (English), leaving open the question of generalizability to other domains and languages.
- What evidence would resolve it: Conducting experiments with LLMs on negated complementary questions in diverse domains (e.g., mathematical, scientific) and multiple languages to assess performance variations.

### Open Question 2
- Question: What is the impact of model size and architecture on the ability of LLMs to handle negated complementary questions?
- Basis in paper: [inferred] The paper uses GPT-3 with 175 billion parameters, but does not explore how different model sizes or architectures might affect performance on negated complementary questions.
- Why unresolved: The study uses a single, large-scale model, without comparing its performance to smaller models or different architectures.
- What evidence would resolve it: Comparing the performance of various LLM sizes and architectures on negated complementary questions to identify if model scale or architecture significantly impacts results.

### Open Question 3
- Question: How does the introduction of more diverse and complex negation structures in questions affect LLM performance on negated complementary tasks?
- Basis in paper: [inferred] The paper focuses on simple negation forms (e.g., "cannot be"), but does not explore the effect of more complex negation structures on LLM performance.
- Why unresolved: The study uses straightforward negation, without testing the robustness of LLMs to more intricate or varied negation forms.
- What evidence would resolve it: Designing experiments with questions containing diverse and complex negation structures to evaluate LLM performance and identify potential weaknesses or areas for improvement.

## Limitations

- The methodology was only validated on GPT-3 (text-davinci-002), raising questions about true model-agnosticism across different LLM architectures
- The approach relies on human evaluation for answer correctness, which may not scale well to larger datasets or diverse domains
- The study focuses on a limited set of 10 relation types from ATOMIC-2020, potentially limiting generalizability to other commonsense knowledge graphs

## Confidence

**High Confidence**: The core observation that LLMs struggle with negated complementary questions is well-supported by the experimental results showing 11+ percentage point improvement over few-shot approaches.

**Medium Confidence**: The assertion that both chain-of-thought prompting with negation logic and post-processing contribute to improved performance is supported by ablation studies, though the relative contribution of each component remains unclear.

**Low Confidence**: The claim that this represents a fundamental limitation in LLMs' understanding of negation versus a surface-level prompting issue requires further investigation with alternative models and prompting strategies.

## Next Checks

1. **Cross-model validation**: Test the proposed methodology on multiple LLM architectures (e.g., GPT-4, Claude, LLaMA) to verify true model-agnosticism and identify any architecture-specific limitations.

2. **Temporal stability assessment**: Evaluate the consistency of human annotations across multiple evaluation sessions and annotators to quantify the reliability of the human evaluation component and identify potential fatigue effects.

3. **Negation complexity scaling**: Extend experiments to test more complex negation scenarios beyond simple relation negation, including nested negations and negations involving multiple relations, to determine the methodology's limits.