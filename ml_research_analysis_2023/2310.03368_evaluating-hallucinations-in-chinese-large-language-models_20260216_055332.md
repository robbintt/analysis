---
ver: rpa2
title: Evaluating Hallucinations in Chinese Large Language Models
arxiv_id: '2310.03368'
source_url: https://arxiv.org/abs/2310.03368
tags:
- questions
- language
- hallucinations
- chinese
- laboratory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HalluQA, a Chinese hallucination evaluation
  benchmark for large language models. The benchmark contains 450 carefully designed
  adversarial questions across 30 domains, designed to measure both imitative falsehoods
  and factual errors.
---

# Evaluating Hallucinations in Chinese Large Language Models

## Quick Facts
- arXiv ID: 2310.03368
- Source URL: https://arxiv.org/abs/2310.03368
- Authors: 
- Reference count: 36
- Key outcome: HalluQA benchmark reveals 18 out of 24 tested Chinese LLMs have non-hallucination rates below 50%

## Executive Summary
This paper introduces HalluQA, a Chinese hallucination evaluation benchmark designed to assess the truthfulness of large language models through carefully crafted adversarial questions. The benchmark contains 450 questions across 30 domains, specifically constructed to trigger both imitative falsehoods (alignment issues) and factual errors (knowledge issues). The authors propose an automated evaluation method using GPT-4 to judge hallucinations, which shows high consistency with human evaluations. The results demonstrate that HalluQA is highly challenging, with most tested models achieving non-hallucination rates below 50%, and reveal distinct hallucination patterns across different model types.

## Method Summary
The authors constructed HalluQA by generating 1,500 questions using GLM-130B and ChatGPT, then filtering to 450 high-quality questions through expert review. Questions were classified into misleading (imitative falsehoods) and knowledge (factual errors) categories. For automated evaluation, GPT-4 was used with structured prompts to judge whether model outputs contained hallucinations, employing majority voting across 5 runs to reduce randomness. The benchmark was tested on 24 Chinese LLMs including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, and SparkDesk. Non-hallucination rate was used as the primary metric, measuring the percentage of answers without hallucinations.

## Key Results
- 18 out of 24 tested models achieved non-hallucination rates below 50%
- Pre-trained models hallucinate more on misleading questions while chat models improve on misleading but decline on knowledge questions
- GPT-4 automated evaluation shows high consistency with human expert judgments
- Retrieval-augmented models show improved performance on knowledge-based questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark differentiates hallucinations by two primary types: imitative falsehoods (alignment issues) and factual errors (knowledge issues).
- Mechanism: By constructing adversarial questions targeting both types, the evaluation isolates whether hallucinations stem from alignment gaps or knowledge gaps, allowing for targeted model improvements.
- Core assumption: Hallucinations in LLMs are fundamentally caused by either model's mimicry of incorrect patterns (alignment) or insufficient factual knowledge.
- Evidence anchors:
  - [abstract] "During the construction of HalluQA, we consider two types of hallucinations: imitative falsehoods and factual errors"
  - [section] "We have divided the test data into two parts: misleading and knowledge. The data in the misleading part is primarily used to detect the model's imitative falsehoods... The data in the knowledge part is primarily used to detect the model's factual errors."
- Break condition: If future models develop a unified internal representation that does not distinguish between learned patterns and factual knowledge, this two-type classification may become less actionable.

### Mechanism 2
- Claim: GPT-4 can serve as a reliable automated evaluator for hallucinations in Chinese LLMs with high consistency to human judgments.
- Mechanism: The authors use GPT-4 with structured prompts and majority voting over 5 runs to reduce randomness and align its judgments with human expert evaluations.
- Core assumption: GPT-4's reasoning capability is sufficient to understand hallucination criteria and apply them consistently across diverse question types.
- Evidence anchors:
  - [section] "We conducted experiments to assess the consistency between GPT-4's evaluation results and human evaluation results... the consistency rate between GPT-4's evaluations and human expert evaluations is relatively high."
  - [section] "Due to the inability of GPT-4 to access top logits and to produce deterministic outputs, we employ GPT-4 to generate five judgments for voting"
- Break condition: If GPT-4's own knowledge becomes outdated or it starts hallucinating in its evaluation judgments, the evaluation method will lose reliability.

### Mechanism 3
- Claim: Different model types (pre-trained, chat, retrieval-augmented) exhibit distinct hallucination profiles that can be addressed with targeted interventions.
- Mechanism: By analyzing non-hallucination rates across model types and question categories, the authors identify that pre-trained models hallucinate more on misleading questions, chat models improve on misleading but decline on knowledge, and retrieval-augmented models improve on knowledge.
- Evidence anchors:
  - [section] "pre-trained models exhibit a pronounced hallucination phenomenon when it comes to misleading questions... On the other hand, pre-trained models exhibit slightly fewer Hallucinations when dealing with knowledge-based questions"
  - [section] "Chat models show significant improvement in addressing misleading questions... However, the performance of chat models on knowledge-based questions has declined"
- Break condition: If model architectures converge such that all types develop similar strengths and weaknesses, the differentiated intervention strategy may become less relevant.

## Foundational Learning

- Concept: Adversarial question construction
  - Why needed here: The benchmark's effectiveness depends on crafting questions that reliably trigger hallucinations in different model types
  - Quick check question: How does the paper ensure that questions in the misleading part actually induce imitative falsehoods in pre-trained models?

- Concept: Automated evaluation consistency
  - Why needed here: The scalability of the benchmark depends on having an automated evaluation method that matches human judgment quality
  - Quick check question: What evidence does the paper provide that GPT-4's evaluation consistency is sufficient for benchmarking purposes?

- Concept: Domain coverage for cultural relevance
  - Why needed here: The benchmark needs to reflect Chinese cultural context to be meaningful for Chinese LLM evaluation
  - Quick check question: How many domains does HalluQA cover, and what types of cultural knowledge are included?

## Architecture Onboarding

- Component map: Data Collection -> Question Classification -> Human Annotation -> Automated Evaluation -> Model Testing -> Result Analysis
- Critical path: Data collection → Question classification → Expert annotation → Automated evaluation setup → Model testing → Result analysis
- Design tradeoffs: 
  - Using GPT-4 for evaluation trades some accuracy for scalability compared to full human evaluation
  - Including both pre-trained and chat models requires different prompting strategies
  - Chinese cultural specificity limits generalizability but increases relevance for target models
- Failure signatures:
  - Low consistency between GPT-4 and human evaluations indicates evaluation method problems
  - Uniform hallucination rates across all question types suggests questions aren't effectively differentiated
  - High hallucination rates on knowledge questions for retrieval-augmented models indicates retrieval system issues
- First 3 experiments:
  1. Test GPT-4 evaluation consistency on a small sample of 50 questions across 3 different models
  2. Run the full benchmark on a single representative model (e.g., Baichuan2-13B-chat) to validate the complete pipeline
  3. Compare non-hallucination rates between the misleading and knowledge sections to verify the two-type classification is meaningful

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important questions remain unanswered:

### Open Question 1
- Question: How does the performance of Chinese large language models on HalluQA compare to their performance on English hallucination benchmarks like TruthfulQA?
- Basis in paper: The paper mentions that HalluQA is a Chinese-specific hallucination evaluation benchmark, but does not directly compare model performance on HalluQA to English benchmarks.
- Why unresolved: The paper focuses on introducing and evaluating HalluQA, without conducting cross-linguistic comparisons to English benchmarks.
- What evidence would resolve it: Conducting experiments to evaluate the same Chinese models on both HalluQA and TruthfulQA, then comparing their performance across the two benchmarks.

### Open Question 2
- Question: What is the impact of different hallucination types (imitative falsehoods vs. factual errors) on downstream task performance for Chinese large language models?
- Basis in paper: The paper identifies two types of hallucinations and analyzes which types different models struggle with, but does not examine the downstream impact of these hallucination types.
- Why unresolved: The study focuses on measuring and categorizing hallucinations, without investigating their effects on practical applications.
- What evidence would resolve it: Designing experiments to evaluate model performance on downstream tasks (e.g., question answering, text generation) when hallucinations of different types are present or absent.

### Open Question 3
- Question: How does the hallucination behavior of Chinese large language models change as they are scaled up in size and trained on more diverse datasets?
- Basis in paper: The paper mentions that scaling up models can improve performance on some hallucination types, but does not provide a comprehensive analysis of how hallucination behavior evolves with model size and data diversity.
- Why unresolved: The study evaluates a range of models but does not systematically investigate the relationship between model scale, data diversity, and hallucination behavior.
- What evidence would resolve it: Conducting experiments to train and evaluate Chinese models of varying sizes and data diversities on HalluQA, then analyzing how their hallucination rates and types change.

## Limitations
- The benchmark is specifically designed for Chinese language and cultural context, limiting generalizability to other languages
- Dependence on GPT-4 as an evaluator may introduce its own biases and knowledge limitations
- The HalluQA dataset is not publicly available, preventing independent verification of results

## Confidence

High confidence in benchmark construction methodology:
- Clear two-type hallucination classification system
- Expert human annotation for quality control
- Comprehensive domain coverage across 30 areas

Medium confidence in automated evaluation consistency:
- GPT-4 evaluation shows high consistency with human judgments
- Majority voting helps reduce randomness
- Method requires further validation across diverse model architectures

Low confidence in generalizability beyond Chinese cultural contexts:
- Benchmark specifically targets Chinese knowledge and language patterns
- Results may not translate to non-Chinese LLMs or other cultural contexts

## Next Checks

1. Reproduce the GPT-4 evaluation consistency results using a held-out test set of 100 questions with independent human annotators
2. Test the benchmark on non-Chinese LLMs to verify that the evaluation method generalizes across languages and cultural contexts
3. Conduct ablation studies to determine the minimum number of GPT-4 evaluation runs needed to maintain high consistency while reducing computational costs