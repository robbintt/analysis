---
ver: rpa2
title: 'Beyond Human Data: Scaling Self-Training for Problem-Solving with Language
  Models'
arxiv_id: '2312.06585'
source_url: https://arxiv.org/abs/2312.06585
tags:
- rest
- performance
- data
- math
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores self-training to reduce reliance on human data
  for problem-solving tasks, specifically MATH reasoning and APPS coding. The proposed
  method, ReSTEM, uses expectation-maximization (EM) to iteratively generate model
  samples, filter them using binary rewards, and fine-tune on the filtered data.
---

# Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models

## Quick Facts
- arXiv ID: 2312.06585
- Source URL: https://arxiv.org/abs/2312.06585
- Authors: 
- Reference count: 8
- Key outcome: ReSTEM self-training outperforms fine-tuning on human data for MATH and APPS tasks

## Executive Summary
This paper introduces ReSTEM (Reinforcement Self-Training Expectation-Maximization), a self-training method that reduces dependence on human-labeled data for language model problem-solving. By iteratively generating model samples, filtering them with binary rewards, and fine-tuning on the filtered data, ReSTEM achieves superior performance compared to standard fine-tuning on human data. The approach shows particular effectiveness for larger models and demonstrates strong transfer to related tasks while maintaining general capabilities.

## Method Summary
ReSTEM implements an EM-based self-training framework that alternates between generating solutions (E-step) and fine-tuning on reward-weighted data (M-step). For each training problem, the model generates multiple candidate solutions using top-K sampling, which are then filtered using a binary correctness reward function. The model is fine-tuned from its base pretrained weights on the filtered solutions, with the process repeating for multiple iterations. The method uses reward-weighted negative log-likelihood loss and employs cutoff thresholds to balance the number of solutions per problem.

## Key Results
- ReSTEM significantly outperforms fine-tuning on human data for both MATH reasoning and APPS coding tasks
- Performance gains scale favorably with model size, particularly benefiting larger PaLM-2 models
- Multiple ReSTEM iterations improve performance but can lead to overfitting, with 2-3 iterations often optimal
- The method maintains general capabilities while improving task-specific performance and shows strong transfer to related benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReSTEM scales performance gains with model size by decoupling data collection from policy optimization
- Mechanism: The EM framework alternates between E-step (generating samples using a fixed policy) and M-step (fine-tuning on reward-weighted data), avoiding the high computational cost of online RL
- Core assumption: The sampling policy remains stable enough between iterations to provide useful data, and reward weighting focuses learning on correct solutions
- Evidence anchors:
  - [abstract] "ReSTEM scales favorably with model size and significantly surpasses fine-tuning only on human data"
  - [section] "Comparing the above equation with theLRL objective reveals the key distinction between standard RL and EM-based RL: how output data is sampled"
- Break condition: If the sampling policy diverges too much from the optimal policy, or if rewards become too sparse to provide meaningful gradients

### Mechanism 2
- Claim: Multiple correct solutions per problem provide richer training signals than single human annotations
- Mechanism: By generating many candidate solutions and filtering for correctness, the method creates a dataset with diverse correct reasoning paths, improving generalization
- Core assumption: The reward function (correct/incorrect) is sufficiently reliable and the model can generate diverse correct solutions
- Evidence anchors:
  - [section] "A key strength of ReSTEM is its ability to generate multiple correct solutions for each problem"
  - [section] "ReSTEM outperforms fine-tuning on human data even in this much more restricted setting" (comparison with single sample per problem)
- Break condition: If the model struggles to generate correct solutions, or if the filtering threshold is too strict, reducing effective training data

### Mechanism 3
- Claim: ReSTEM maintains general capabilities while improving task-specific performance
- Mechanism: By always fine-tuning from the base pretrained model (not the previous iteration), the method prevents task-specific drift while still benefiting from iterative improvement
- Core assumption: The base model's general capabilities are preserved during fine-tuning, and the task-specific improvements transfer to related benchmarks
- Evidence anchors:
  - [section] "we always fine tune the base pretrained language model to minimize task-specific over-fitting"
  - [section] "models fine-tuned using ReSTEM improve pass@k as well as majority voting performance" and show transfer to GSM8K, Hungarian HS finals, HumanEval, and BBH tasks
- Break condition: If fine-tuning from base model each time introduces too much noise, or if task-specific overfitting still occurs despite this design choice

## Foundational Learning

- Concept: Expectation-Maximization for Reinforcement Learning
  - Why needed here: The paper builds ReSTEM on this theoretical foundation, showing how alternating E-step and M-step relates to RL objectives
  - Quick check question: How does the EM framework in ReSTEM differ from standard online RL in terms of data collection?

- Concept: Binary reward functions and filtering
  - Why needed here: The method relies on generating many solutions and filtering them using binary correctness rewards to create training data
  - Quick check question: What are the advantages and limitations of using binary rewards compared to continuous reward signals in self-training?

- Concept: Overfitting in iterative self-training
  - Why needed here: The paper observes diminishing returns after 2-3 iterations, suggesting overfitting to the small training datasets
  - Quick check question: How does the size of the training dataset (MATH vs APPS) affect the optimal number of ReSTEM iterations?

## Architecture Onboarding

- Component map:
  Base language model (PaLM-2 variants) -> Prompt templates for few-shot generation -> Solution generation module (top-K sampling with temperature) -> Binary reward function (correctness checker) -> Data filtering pipeline (cutoff threshold per problem) -> Fine-tuning module (reward-weighted negative log-likelihood) -> Evaluation suite (MATH, APPS, GSM8K, HumanEval, BBH, Hungarian HS finals)

- Critical path:
  1. Generate solutions for all training problems using current model
  2. Filter solutions using binary reward function
  3. Fine-tune base model on filtered data
  4. Repeat for desired number of iterations
  5. Evaluate on test and transfer tasks

- Design tradeoffs:
  - Number of samples per problem vs. computational cost
  - Cutoff threshold per problem vs. dataset balance
  - Number of iterations vs. overfitting risk
  - Sampling temperature vs. diversity of generated solutions

- Failure signatures:
  - Training accuracy increases while test accuracy plateaus or decreases (overfitting)
  - No improvement after first iteration (insufficient diversity in generated solutions)
  - Very low pass@k despite high pass@1 (model generates few correct solutions but lacks diversity)
  - Performance degrades on transfer tasks (overfitting to specific training problems)

- First 3 experiments:
  1. Run ReSTEM for 1 iteration on MATH dataset with default parameters (32 samples/problem, cutoff 10) and compare to SFT on human data
  2. Test different numbers of samples per problem (16, 32, 64) on APPS to find optimal balance between performance and compute
  3. Evaluate transfer performance to GSM8K after ReSTEM fine-tuning on MATH to verify general capability preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ReSTEM scale with the size of the training dataset beyond the tested range?
- Basis in paper: [explicit] The paper conducts ablation studies on dataset size, showing performance improvements as the dataset size increases, but only up to 7000 questions
- Why unresolved: The study does not explore the effects of using datasets significantly larger than 7000 questions, leaving the scalability of ReSTEM in terms of dataset size unclear
- What evidence would resolve it: Experiments testing ReSTEM with datasets containing tens of thousands or more questions, comparing performance gains and computational costs

### Open Question 2
- Question: Can ReSTEM be effectively applied to tasks with non-binary rewards, and how does its performance compare to binary reward scenarios?
- Basis in paper: [inferred] The paper focuses on binary rewards and mentions that ReSTEM is based on expectation-maximization, which can handle non-binary rewards. However, it does not explore non-binary reward scenarios
- Why unresolved: The paper does not provide experimental results or theoretical analysis on the application of ReSTEM to tasks with non-binary rewards
- What evidence would resolve it: Experiments applying ReSTEM to tasks with non-binary rewards, comparing performance metrics such as accuracy, efficiency, and robustness to the binary reward case

### Open Question 3
- Question: What is the impact of using different sampling strategies (e.g., top-k, nucleus sampling) on the quality and diversity of the generated solutions in ReSTEM?
- Basis in paper: [explicit] The paper mentions using top-K sampling with K=40 and temperature of 0.7 for generating solutions, but does not explore other sampling strategies
- Why unresolved: The choice of sampling strategy could significantly affect the diversity and quality of generated solutions, which in turn impacts the effectiveness of ReSTEM
- What evidence would resolve it: Comparative experiments using different sampling strategies (e.g., top-k, nucleus sampling with various p values, greedy decoding) to generate solutions, analyzing their impact on solution quality, diversity, and overall performance of ReSTEM

## Limitations
- The method is limited to tasks where correctness can be easily verified through binary rewards
- Multiple iterations of sampling and fine-tuning increase computational requirements significantly
- Performance depends heavily on the accuracy and reliability of the binary reward function

## Confidence
- High Confidence: The core claim that ReSTEM outperforms fine-tuning on human data for both MATH and APPS tasks is well-supported by experimental results across multiple model sizes
- Medium Confidence: The claim about ReSTEM scaling better with model size than traditional fine-tuning has strong experimental support but relies on comparisons with specific baseline models
- Medium Confidence: The assertion that multiple iterations can lead to overfitting is supported by observed performance patterns, but optimal iteration numbers are not fully characterized

## Next Checks
1. Systematically evaluate how variations in reward function accuracy (simulated by injecting noise) affect ReSTEM performance to quantify sensitivity to reward reliability
2. Apply ReSTEM to a third problem-solving domain (e.g., scientific reasoning or logical deduction) to test generalizability beyond MATH and APPS
3. Conduct a detailed comparison of total computational resources (including all sampling iterations) required by ReSTEM versus supervised fine-tuning with varying amounts of human-annotated data