---
ver: rpa2
title: Combining Deep Learning and Street View Imagery to Map Smallholder Crop Types
arxiv_id: '2309.05930'
source_url: https://arxiv.org/abs/2309.05930
tags:
- crop
- images
- type
- field
- ground
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an automated pipeline to generate crop type
  ground references using street-view imagery and deep learning, enabling accurate
  crop type mapping in smallholder regions where ground truth data is scarce. The
  method extracts field points from OpenStreetMap, filters them using land cover maps,
  and classifies Google Street View images into crop types using a CNN trained on
  weakly-labeled web images.
---

# Combining Deep Learning and Street View Imagery to Map Smallholder Crop Types

## Quick Facts
- arXiv ID: 2309.05930
- Source URL: https://arxiv.org/abs/2309.05930
- Reference count: 15
- One-line primary result: Automated pipeline generates country-wide crop type maps with 93% accuracy using street-view imagery and deep learning

## Executive Summary
This paper presents an automated pipeline that combines street-view imagery with deep learning to generate crop type ground references for smallholder regions where traditional ground truth data is scarce. The method filters OpenStreetMap street points using land cover maps, downloads Google Street View images, and classifies them into crop types using CNNs trained on weakly-labeled web images. These automatically-generated labels are then combined with Sentinel-2 time series data to create wall-to-wall crop type maps. Applied to Thailand, the approach produced country-wide maps of rice, cassava, maize, and sugarcane with 93% overall accuracy using 81,000 automatically generated ground references.

## Method Summary
The pipeline begins by generating equidistant street points from OpenStreetMap data, which are filtered using land cover maps to identify crop fields while removing points near tree cover or non-crop areas. Google Street View images are downloaded at these filtered locations and classified using a CNN to distinguish field from non-field images. A second CNN, trained on weakly-labeled web images from sources like Google Images Creative Commons and iNaturalist, classifies field images into specific crop types. These crop type labels are combined with harmonic features extracted from Sentinel-2 time series and used to train random forest classifiers that generate wall-to-wall crop type maps.

## Key Results
- Achieved 93% overall accuracy and 0.92 F1 score for crop type classification in Thailand
- Generated 81,000 automatically-labeled ground references versus only 984 expert-labeled references
- Random forests trained on large CNN-generated ground references significantly outperformed those trained on small expert-labeled datasets
- Successfully mapped four major crops (rice, cassava, maize, sugarcane) across Thailand's smallholder landscape

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining street-view imagery with deep learning can automatically generate accurate crop type ground references in smallholder regions where expert-labeled data is scarce.
- Mechanism: The pipeline first filters OSM street points using land cover maps to identify crop fields, then downloads and classifies street-view images into crop types using CNNs trained on weakly-labeled web images. These labels are combined with Sentinel-2 time series to train a remote sensing crop type classifier.
- Core assumption: Street-view imagery captures enough visual features of crop fields to enable reliable classification by CNNs, even when trained on out-of-domain web images.
- Evidence anchors:
  - [abstract]: "The method efficiently curates a set of street view images containing crop fields, trains a model to predict crop type by utilizing weakly-labelled images from disparate out-of-domain sources..."
  - [section 2.3]: "We compiled training sets of crop type images from two sources: Google Images Creative Commons...and iNaturalist...Although we tried to compile training images that look similar to GSV, online images varied in quality..."
  - [corpus]: Weak, as related papers focus on different applications of street-view imagery rather than crop type mapping in smallholder regions.
- Break condition: If street-view images are occluded by vegetation or buildings, or if crop fields lack distinguishing visual features, the CNN classification accuracy will degrade significantly.

### Mechanism 2
- Claim: Using large volumes of automatically-labeled ground references is more effective than small expert-labeled datasets for training remote sensing crop type classifiers.
- Mechanism: The pipeline generates 81,000 ground references from street-view images, which are used to train random forests on Sentinel-2 time series. This outperforms training on only 984 expert-labeled references.
- Core assumption: The increased sample size from automated labeling compensates for the noise introduced by imperfect CNN classifications.
- Evidence anchors:
  - [section 3.3]: "We found that training a random forest on large CNN-generated GSV ground references (n = 81, 000) resulted in crop type maps that were significantly more accurate than training on small expert-labeled ground references (n = 984)..."
  - [section 2.4]: "We trained random forests (with 500 trees) on crop type classification, with the harmonic coefficients as input and the crop type labels generated by the street-view CNN as output."
  - [corpus]: Weak, as related papers focus on different applications of remote sensing data rather than the effectiveness of large automated datasets.
- Break condition: If the CNN's classification errors are systematic or biased, the large volume of incorrect labels could mislead the remote sensing classifier.

### Mechanism 3
- Claim: Weak supervision from noisy, out-of-domain web images can successfully train CNNs to classify crop types in street-view imagery.
- Mechanism: CNNs are trained on large, diverse datasets from Google Images and iNaturalist, which contain label noise and are not perfectly aligned with street-view domains. Despite this, they achieve high accuracy when classifying street-view images.
- Core assumption: CNNs are robust to noise and can learn relevant features from diverse datasets as long as enough signal is present.
- Evidence anchors:
  - [section 2.3]: "Although we tried to compile training images that look similar to GSV, online images varied in quality, size, and relevance...Fortunately, CNNs tend to be robust to noise and can perform well if enough signal from the desired task is present during training."
  - [section 3.2]: "Street-view crop type classification...WebCC (n = 2, 754) on its own achieved 82.9% overall accuracy, similar to iNaturalist (n = 8, 638) at 83.2%."
  - [corpus]: Weak, as related papers focus on different applications of web images rather than their effectiveness in training domain-specific classifiers.
- Break condition: If the noise in web images is too high or if the visual features of crops in street-view images are too different from web images, the CNN's performance will degrade.

## Foundational Learning

- Concept: Convolutional Neural Networks (CNNs) and their application to image classification
  - Why needed here: CNNs are used to classify crop types in street-view images and to filter out non-field images.
  - Quick check question: How do CNNs process spatial hierarchies in images, and why are they suitable for classifying crop types?

- Concept: Harmonic regression for time series feature extraction
  - Why needed here: Harmonic regression is used to extract frequency-domain features from Sentinel-2 time series for input to the random forest classifier.
  - Quick check question: What are the advantages of using harmonic regression over other time series analysis methods in this context?

- Concept: Random Forest classifiers and their hyperparameters
  - Why needed here: Random forests are used to classify crop types from the harmonic features extracted from Sentinel-2 time series.
  - Quick check question: How do hyperparameters like the number of trees and maximum depth affect the performance of a random forest classifier?

## Architecture Onboarding

- Component map: OSM data processing -> Land cover filtering -> GSV image retrieval -> Field classification CNN -> Crop type classification CNN -> Random forest training -> Crop type mapping
- Critical path: OSM → Land cover filter → GSV retrieval → Field CNN → Crop type CNN → Random forest → Crop map
- Design tradeoffs:
  - Using web images for weak supervision reduces manual labeling but introduces noise
  - Large automated datasets improve remote sensing accuracy but may contain systematic errors
  - Sentinel-2 data provides phenology information but has lower spatial resolution than GSV
- Failure signatures:
  - High false positives in field classification CNN: Many non-field images classified as fields
  - Low crop type classification accuracy: Incorrect labels propagating to remote sensing model
  - Overfitting in random forest: Poor generalization to unseen areas
- First 3 experiments:
  1. Evaluate field/not-field CNN performance on a held-out test set of GSV images
  2. Compare crop type classification accuracy using different web image sources (WebCC vs. iNaturalist)
  3. Assess the impact of training set size on remote sensing crop type map accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the method's accuracy vary with different land cover map sources or classification schemes?
- Basis in paper: [inferred] The paper uses ESA's WorldCover 10m v100 to filter street points near cropland and remove tree cover obstructions. It notes that 30% of remaining GSV images still had obstructed views due to small bushes not detected by satellite land cover maps.
- Why unresolved: The paper doesn't test alternative land cover maps or compare results using different classification schemes for filtering street points.
- What evidence would resolve it: Testing the pipeline with different land cover datasets (e.g., MODIS, Copernicus) and comparing filtering accuracy and subsequent crop type mapping results would show sensitivity to land cover map choice.

### Open Question 2
- Question: What is the optimal distance to extrapolate street points to crop field interior points in different agricultural contexts?
- Basis in paper: [explicit] The paper states they empirically determined 30m as the best distance for Thailand after inspecting 200 fields, but acknowledges that different regions may require different distances.
- Why unresolved: The 30m distance was determined for Thailand's smallholder farms and may not generalize to other regions with different field sizes or road layouts.
- What evidence would resolve it: Systematic testing of different extrapolation distances across multiple regions with varying agricultural systems would establish optimal distances for different contexts.

### Open Question 3
- Question: How does the quality of automatically-generated ground references compare to expert-labeled references for less common crop types?
- Basis in paper: [explicit] The paper shows that for major crops (rice, cassava, maize, sugarcane), CNN-generated ground references outperformed expert-labeled references due to sample size. However, cassava and maize had consistently lower F1 scores across all models.
- Why unresolved: The paper doesn't specifically analyze performance differences for rare crops or examine how noise in automatically-generated labels affects less common crop types.
- What evidence would resolve it: Detailed analysis of crop type mapping accuracy for rare crops using both automatically-generated and expert-labeled ground references would reveal performance differences and optimal approaches for different crop types.

## Limitations

- Street-view imagery availability is limited in many smallholder regions outside Thailand, potentially restricting the method's scalability.
- The approach assumes street-view images capture sufficient visual features of crop fields, which may not hold in regions with different cropping systems or field sizes.
- Weakly-labeled web images introduce noise that could propagate through the pipeline, potentially affecting the accuracy of final crop type maps.

## Confidence

- High Confidence: The overall effectiveness of combining street-view imagery with deep learning for generating crop type ground references, as demonstrated by the 93% accuracy achieved in Thailand.
- Medium Confidence: The scalability of this approach to other smallholder regions, given the variability in street-view coverage and crop field characteristics.
- Low Confidence: The long-term sustainability of this method, as it depends on the continued availability of street-view imagery and the stability of OSM data.

## Next Checks

1. Apply the method to another smallholder region with different crop types and field characteristics to assess its generalizability.
2. Conduct a detailed analysis of the noise introduced by weakly-labeled web images and its impact on the final crop type maps.
3. Evaluate the temporal consistency of the crop type maps by applying the method to multi-year street-view and satellite imagery.