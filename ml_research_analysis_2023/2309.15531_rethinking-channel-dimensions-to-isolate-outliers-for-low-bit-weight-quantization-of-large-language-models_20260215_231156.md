---
ver: rpa2
title: Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization
  of Large Language Models
arxiv_id: '2309.15531'
source_url: https://arxiv.org/abs/2309.15531
tags:
- quantization
- arxiv
- weight
- gptq
- per-ic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of sub-4-bit weight quantization
  for large language models, which is difficult due to activation outliers causing
  quantization errors. The authors propose per-input-channel (per-IC) quantization,
  which groups weights within each input channel instead of the conventional per-output-channel
  (per-OC) approach.
---

# Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models

## Quick Facts
- arXiv ID: 2309.15531
- Source URL: https://arxiv.org/abs/2309.15531
- Reference count: 17
- Primary result: Per-input-channel quantization with Adaptive Dimensions (AdaDim) improves sub-4-bit quantization of LLMs by isolating activation outliers, achieving up to 4.7% boost on MMLU and 10% on HumanEval.

## Executive Summary
This paper addresses the challenge of sub-4-bit weight quantization for large language models (LLMs), which is difficult due to activation outliers causing quantization errors. The authors propose per-input-channel (per-IC) quantization, which groups weights within each input channel instead of the conventional per-output-channel (per-OC) approach. This isolates the outlier effect within a group, as activation outliers affect specific input channels. They also introduce Adaptive Dimensions (AdaDim), a framework that adaptively chooses between per-IC and per-OC quantization for each layer based on reconstruction error. AdaDim is applied to augment methods like Round-To-Nearest and GPTQ, showing significant improvements in language modeling benchmarks for both base and instruction-tuned LLMs.

## Method Summary
The paper proposes a new quantization scheme called per-input-channel (per-IC) quantization to address the issue of activation outliers in low-bit weight quantization of LLMs. Per-IC quantization groups weights within each input channel, isolating the effect of activation outliers that affect specific input channels. The authors also introduce Adaptive Dimensions (AdaDim), a framework that adaptively selects between per-IC and per-OC quantization for each layer based on reconstruction error. AdaDim is applied to augment existing quantization methods like Round-To-Nearest and GPTQ, showing significant improvements in language modeling benchmarks.

## Key Results
- Per-IC quantization isolates activation outliers within quantization groups, preventing their effect from permeating across groups.
- AdaDim improves quantization accuracy by adaptively choosing the optimal quantization dimension (per-IC or per-OC) for each layer based on reconstruction error.
- Applied to LLaMA and LLaMA-V2 models, AdaDim with RTN and GPTQ achieves up to 4.7% boost on MMLU and 10% on HumanEval benchmarks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Per-input-channel (per-IC) quantization isolates the effect of activation outliers by grouping weights within each input channel rather than per-output-channel.
- **Mechanism:** Activation outliers affect specific input channels of the weight matrix. By grouping weights per input channel, the outlier effect is contained within a single quantization group, preventing it from influencing other groups.
- **Core assumption:** Activation outliers have a localized impact on input channels and do not affect all output channels uniformly.
- **Evidence anchors:**
  - [abstract]: "Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers within a group."
  - [section]: "When activation outliers emerge in certain hidden dimensions, the amplification effect is permeated across all quantization groups for per-OC quantization. In contrast, grouping within each IC yields a 1:1 mapping between hidden dimension to a quantization group which isolates the outlier effect to be within a group."
  - [corpus]: Weak evidence; corpus papers focus on different outlier mitigation techniques (e.g., dual transformation, spectral decomposition) rather than per-IC grouping.

### Mechanism 2
- **Claim:** Adaptive Dimensions (AdaDim) improves quantization by dynamically selecting between per-IC and per-OC quantization for each layer based on reconstruction error.
- **Mechanism:** AdaDim formulates channel quantization as a binary selection problem for each layer, choosing the dimension (IC or OC) that minimizes reconstruction error. This allows the method to adapt to different weight sensitivity patterns across layers.
- **Core assumption:** The optimal quantization dimension varies across layers and can be determined by minimizing reconstruction error.
- **Evidence anchors:**
  - [abstract]: "With per-IC quantization as a new outlier-friendly scheme, we then propose Adaptive Dimensions (AdaDim), a versatile quantization framework that can adapt to various weight sensitivity patterns by using both per-IC and per-OC quantization."
  - [section]: "To measure which dimension is more effective, we adopt the widely used reconstruction error metric... Since the search space of the dimension parameter is only two, we can inexpensively determine the optimal dimension for each layer of the network with a very small number of forward passes."
  - [corpus]: Weak evidence; corpus papers do not discuss adaptive dimension selection based on reconstruction error.

### Mechanism 3
- **Claim:** Per-IC quantization allows activation reordering for free, enabling hardware-efficient quantization without sacrificing accuracy.
- **Mechanism:** In per-IC quantization, quantization scales are placed in a row-major order, which remains contiguous in memory even after activation reordering. This allows sensitive channels to be quantized first without incurring memory access inefficiencies.
- **Core assumption:** The hardware constraint of contiguous quantization scales in memory is the primary barrier to using activation reordering with per-OC quantization.
- **Evidence anchors:**
  - [section]: "With per-IC quantization, quantization scales are placed in a row-major order, which allows scales to be contiguous even after reordering."
  - [abstract]: "By grouping sensitive weights together, AdaDim can use larger, localized updates that minimally perturb the original weight distribution."
  - [corpus]: Weak evidence; corpus papers do not discuss the interaction between quantization dimension and memory layout for activation reordering.

## Foundational Learning

- **Concept:** Quantization in neural networks
  - **Why needed here:** Understanding quantization is essential to grasp how per-IC and per-OC quantization differ and why activation outliers pose a challenge.
  - **Quick check question:** What is the difference between per-tensor, per-channel, and group-wise quantization in neural networks?

- **Concept:** Activation outliers in transformers
  - **Why needed here:** Activation outliers are central to the problem being addressed; knowing how they arise and affect quantization is crucial.
  - **Quick check question:** How do activation outliers in transformers differ from typical activation values, and why do they cause quantization errors?

- **Concept:** Reconstruction error as a metric
  - **Why needed here:** AdaDim uses reconstruction error to select the optimal quantization dimension; understanding this metric is key to the method.
  - **Quick check question:** How is reconstruction error calculated in the context of quantization, and why is it a good proxy for quantization quality?

## Architecture Onboarding

- **Component map:** Per-IC quantization -> Adaptive Dimensions (AdaDim) -> Activation reordering -> Improved quantization accuracy
- **Critical path:**
  1. Analyze weight sensitivity patterns to identify layers affected by activation outliers.
  2. Implement per-IC quantization for layers with high outlier impact.
  3. Integrate AdaDim to adaptively choose quantization dimensions per layer.
  4. Validate improvements on language modeling benchmarks.

- **Design tradeoffs:**
  - Per-IC vs. per-OC: Per-IC isolates outliers but may require custom kernels; per-OC is standard but less effective against outliers.
  - Adaptive selection: Increases computational overhead during quantization but improves accuracy.
  - Activation reordering: Improves quantization of sensitive channels but may cause memory access inefficiencies with per-OC.

- **Failure signatures:**
  - Poor accuracy gains: Indicates that activation outliers are not the primary bottleneck or that the per-IC grouping is not effective.
  - Increased memory usage: Suggests that the per-IC approach is not being optimized for memory layout.
  - Slow quantization: Implies that the adaptive selection process is too computationally intensive.

- **First 3 experiments:**
  1. Implement per-IC quantization on a small transformer layer and compare reconstruction error with per-OC.
  2. Apply AdaDim to select quantization dimensions for each layer in a medium-sized model and measure accuracy improvement.
  3. Test activation reordering with both per-IC and per-OC quantization to quantify memory access efficiency differences.

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the exact computational overhead of the Adaptive Dimensions (AdaDim) framework during both training and inference phases, and how does it scale with model size?
  - **Basis in paper:** [explicit] The paper mentions that AdaDim requires two partial forward passes per layer, compared to 40 partial forward passes for AWQ. However, the paper does not provide a detailed analysis of computational overhead during inference or how it scales with model size.
  - **Why unresolved:** The paper only briefly mentions the computational cost in comparison to AWQ, but does not provide a comprehensive analysis of the overhead during inference or its scaling behavior with model size.
  - **What evidence would resolve it:** A detailed analysis of the computational overhead during both training and inference phases, including benchmarks on various model sizes and configurations, would resolve this question.

- **Open Question 2:** How does the effectiveness of per-IC quantization change when applied to different transformer architectures beyond the LLaMA family, such as BERT or GPT-2?
  - **Basis in paper:** [inferred] The paper primarily focuses on the LLaMA family of models and mentions the potential effectiveness of AdaDim's versatile quantization scheme to other backbones such as GPT-2. However, it does not provide experimental results or analysis for other architectures.
  - **Why unresolved:** The paper does not explore the effectiveness of per-IC quantization on different transformer architectures beyond the LLaMA family, leaving uncertainty about its generalizability.
  - **What evidence would resolve it:** Experimental results and analysis of per-IC quantization applied to various transformer architectures, such as BERT or GPT-2, would resolve this question.

- **Open Question 3:** What is the impact of using task-specific calibration sets on the quantization performance for instruction-tuned models, and how does it compare to using generic text corpora?
  - **Basis in paper:** [explicit] The paper discusses the use of task-specific calibration sets for instruction-tuned models and provides results comparing them to generic text corpora. However, it does not provide a comprehensive analysis of the impact on quantization performance across different tasks and model scales.
  - **Why unresolved:** While the paper provides some results comparing task-specific calibration sets to generic text corpora, it does not offer a thorough analysis of the impact on quantization performance across various tasks and model scales.
  - **What evidence would resolve it:** A comprehensive analysis of the impact of task-specific calibration sets on quantization performance for instruction-tuned models across different tasks and model scales would resolve this question.

## Limitations

- **Implementation and Reproducibility Uncertainties:** Per-IC quantization may require custom kernels or hardware support that is not universally available, potentially limiting reproducibility across different hardware platforms.
- **Validation Scope Limitations:** The evaluation focuses on language modeling benchmarks and does not explore the impact of the proposed method on other domains or tasks. Additionally, the paper does not provide a comprehensive comparison with the latest state-of-the-art quantization techniques.
- **Generalizability Uncertainty:** The effectiveness of per-IC quantization and AdaDim is primarily demonstrated on the LLaMA family of models, leaving uncertainty about its performance on other transformer architectures.

## Confidence

- **High Confidence:** The observation that activation outliers affect input channels of the weight matrix and that per-IC quantization can isolate these outliers is well-supported by the paper's analysis and is a logical extension of existing quantization principles.
- **Medium Confidence:** The claim that per-IC quantization allows activation reordering for free due to memory layout is plausible but requires verification on specific hardware architectures.
- **Low Confidence:** The assertion that the computational overhead of evaluating both per-IC and per-OC quantization per layer is minimal may not hold for very large models or when applied at scale.

## Next Checks

1. **Hardware Compatibility and Efficiency Validation:** Implement the per-IC quantization and activation reordering on multiple hardware platforms (e.g., GPUs with different memory architectures) to verify the claimed memory layout advantages and assess any performance overhead.

2. **Comparative Analysis with State-of-the-Art Methods:** Conduct a comprehensive comparison of AdaDim against the latest quantization techniques (e.g., DuQuant, SpecQuant) on a diverse set of models and tasks to better understand its relative performance and generalizability.

3. **Scalability and Overhead Assessment:** Evaluate the computational overhead of the adaptive dimension selection process for very large models (e.g., 70B+ parameters) and assess whether the accuracy gains justify the additional complexity in practical deployment scenarios.