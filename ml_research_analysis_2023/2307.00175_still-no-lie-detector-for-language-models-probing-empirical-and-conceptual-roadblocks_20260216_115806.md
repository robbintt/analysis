---
ver: rpa2
title: 'Still No Lie Detector for Language Models: Probing Empirical and Conceptual
  Roadblocks'
arxiv_id: '2307.00175'
source_url: https://arxiv.org/abs/2307.00175
tags:
- beliefs
- probes
- language
- have
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We evaluated two probing approaches for detecting beliefs in large
  language models: supervised learning probes and unsupervised contrastive-consistency
  search (CCS) probes. Our empirical results show that supervised probes fail to generalize
  to negations, and CCS probes only achieve chance-level accuracy.'
---

# Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks

## Quick Facts
- **arXiv ID**: 2307.00175
- **Source URL**: https://arxiv.org/abs/2307.00175
- **Reference count**: 8
- **Primary result**: Current probing methods fail to detect beliefs in LLMs, with supervised probes failing on negations and CCS probes achieving only chance-level accuracy.

## Executive Summary
This paper evaluates two approaches for detecting beliefs in large language models: supervised learning probes and unsupervised contrastive-consistency search (CCS) probes. Empirical results show that supervised probes fail to generalize to negated statements, while CCS probes achieve only chance-level accuracy. The authors argue these failures stem from fundamental conceptual limitations - coherence criteria alone cannot isolate truth from other sentence properties. The paper concludes that detecting beliefs in LLMs remains an open empirical problem requiring new approaches, such as applying pressure for truth through prompting or investigating latent world models within LLMs.

## Method Summary
The study evaluates belief detection in LLMs through two probe architectures. First, supervised feedforward neural networks are trained on labeled true/false statements to predict truth values from LLM embeddings. Second, unsupervised CCS probes are trained using coherence criteria to find embeddings where positive and negative statements satisfy probabilistic consistency. The methods are tested on various datasets including Animals, Capitals, Cities, Companies, Elements, Facts, Inventions, and their negated counterparts. Evaluation measures include binary classification accuracy for supervised probes and both CCS loss and accuracy for unsupervised probes.

## Key Results
- Supervised probes achieve high accuracy on training data but fail to generalize to negated statements
- CCS probes only achieve chance-level accuracy despite satisfying coherence criteria
- Both methods fail to isolate truth representations from other coherent sentence properties
- Current probing techniques cannot reliably detect beliefs in LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised probes fail to generalize because they learn spurious correlations that correlate with truth in training data but not in broader contexts.
- Mechanism: When probes are trained on labeled true/false statements, they optimize to predict labels rather than detect truth itself. This leads them to learn properties that happen to align with truth in the specific dataset but break down when applied to negated statements or other variations.
- Core assumption: The probe's objective function drives it to minimize prediction error on training labels rather than capture genuine truth representations.
- Evidence anchors:
  - [abstract] "Our empirical results show that supervised probes fail to generalize to negations"
  - [section 4.5] "One of the common pitfalls involves learning spurious correlations that may be present in the training data, but do not consistently hold in more general contexts"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: When probes encounter statements where the spurious correlation no longer aligns with truth (e.g., negations), accuracy drops to chance levels.

### Mechanism 2
- Claim: CCS probes fail because coherence criteria alone cannot isolate truth from other sentence properties.
- Mechanism: The CCS method trains probes to find embeddings where positive and negative statements sum to approximately 1 in probability space. However, this coherence requirement is satisfied by any property that distinguishes positive from negative examples, not just truth.
- Core assumption: The mathematical property Pr(x∧P(x))+Pr(¬x∨¬P(x))=1 holds for any sentence property P, not just truth.
- Evidence anchors:
  - [abstract] "CCS probes only achieve chance-level accuracy"
  - [section 4.9] "CCS probes will either fail immediately or fail to generalize" due to inability to isolate truth
  - [corpus] Weak - corpus evidence doesn't directly address the mathematical properties
- Break condition: When probes encounter properties other than truth that satisfy coherence criteria (like negation presence), they achieve low loss but poor accuracy.

### Mechanism 3
- Claim: LLMs may have beliefs because tracking truth can be instrumentally useful for token prediction.
- Mechanism: Decision theory suggests that agents need beliefs (probability functions) combined with desires (utility functions) to make optimal choices. Since LLMs aim to maximize predictive accuracy, they may have evolved internal representations of truth as a means to this end.
- Core assumption: The instrumental utility of accurate beliefs applies selection pressure on agents to conform to epistemic norms.
- Evidence anchors:
  - [abstract] "We provide a more productive framing of questions surrounding the status of beliefs in LLMs"
  - [section 5.1] "It is a mistake to infer from the fact that the LLM outputs tokens that are likely to follow its inputs that the LLM must not have beliefs"
  - [corpus] Moderate - related work on "Honesty in LLMs" and "Truthful LLMs" suggests ongoing research into belief detection
- Break condition: When the cost of maintaining accurate beliefs exceeds the benefit for prediction, or when the environment doesn't reward truth-tracking.

## Foundational Learning

- Concept: Supervised vs. unsupervised learning
  - Why needed here: The paper compares two probe training approaches - one supervised (labeled data) and one unsupervised (coherence criteria)
  - Quick check question: What is the key difference between how supervised and unsupervised probes are trained?
- Concept: Generalization in machine learning
  - Why needed here: Both probe methods fail to generalize beyond their training distributions, which is central to the paper's conclusions
  - Quick check question: What is "out-of-distribution generalization" and why is it challenging for classifiers?
- Concept: Decision theory and expected utility maximization
  - Why needed here: The paper uses decision theory to argue that LLMs may have beliefs because they're useful for prediction
  - Quick check question: According to decision theory, what two components must an agent have to make rational choices?

## Architecture Onboarding

- Component map: Input text → LLM tokenization and embedding generation → Probe processing → Probability output → Evaluation against ground truth
- Critical path: Input text → LLM tokenization and embedding generation → Probe processing → Probability output → Evaluation against ground truth
- Design tradeoffs: Supervised probes can achieve high accuracy on training data but fail to generalize; unsupervised probes avoid label requirements but struggle to isolate truth from other coherent properties
- Failure signatures: Probe accuracy drops to chance levels on negated statements; CCS probes achieve low loss but poor accuracy; probes learn spurious correlations rather than truth
- First 3 experiments:
  1. Replicate the negation test by training a supervised probe on positive statements and testing on their negations
  2. Implement CCS with different normalization methods to see if it affects accuracy
  3. Test the hypothesis that applying pressure for truth (through prompting) makes representations more extractable by probes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs have beliefs?
- Basis in paper: [explicit] "We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them."
- Why unresolved: The paper argues that this is largely an empirical question, as our best theories of rational choice make it a live hypothesis that LLMs have beliefs, but there are also contexts in which there is little selection pressure for accurate beliefs.
- What evidence would resolve it: Further empirical work probing the latent variables that LLMs use to predict text, and evaluating whether these variables correspond to anything we might consider a world-model.

### Open Question 2
- Question: How can we measure the beliefs of LLMs if they exist?
- Basis in paper: [explicit] "Thus, there is still no lie-detector for LLMs."
- Why unresolved: Current probing techniques fail to generalize and there are conceptual problems with both supervised and unsupervised learning approaches.
- What evidence would resolve it: Development of new probing techniques that can accurately identify representations of truth within LLM embeddings, or evidence that such representations do not exist.

### Open Question 3
- Question: Can unsupervised learning methods like CCS successfully detect beliefs in LLMs?
- Basis in paper: [explicit] "The advantage of CCS and unsupervised approaches more generally over supervised approaches is that they do not restrict the training and testing data so severely."
- Why unresolved: While CCS avoids some limitations of supervised learning, the paper argues that coherence criteria alone cannot distinguish encodings of truth from other concepts.
- What evidence would resolve it: Successful application of CCS or similar methods to detect beliefs in LLMs, or demonstration that such methods are fundamentally limited in their ability to isolate truth.

### Open Question 4
- Question: What are the latent variables that LLMs use to predict text, and do they correspond to a world-model?
- Basis in paper: [explicit] "Using latent variables to compute probability distributions is commonplace in science and statistics... Thus, it would be fairly surprising if there weren't a useful way to think of LLMs as using some kinds of latent variables in order to make predictions about the next token."
- Why unresolved: While some preliminary work exists, the paper suggests this is an area requiring further empirical and conceptual investigation.
- What evidence would resolve it: Identification and characterization of the specific latent variables used by LLMs, and analysis of whether these correspond to anything resembling a world-model.

## Limitations
- Limited empirical breadth: Only tested one probe architecture (feedforward networks) and one unsupervised method (CCS)
- Theoretical assumptions: Relies on mathematical properties of coherence criteria without broader empirical validation
- Open question of LLM beliefs: Assumes LLMs have coherent belief representations to extract, which remains unproven

## Confidence
- **High confidence**: The empirical finding that supervised probes fail on negations is well-supported by the experimental results.
- **Medium confidence**: The theoretical argument that CCS probes cannot isolate truth from other coherent properties is mathematically sound but lacks empirical breadth.
- **Medium confidence**: The claim that belief detection is fundamentally limited by coherence requirements, while conceptually compelling, needs broader empirical validation.

## Next Checks
1. Test whether more sophisticated probe architectures (transformers, attention-based probes) can overcome the generalization failures observed with feedforward networks.
2. Evaluate alternative coherence formulations beyond CCS that might better isolate truth representations while maintaining mathematical consistency.
3. Investigate whether applying pressure for truthfulness through prompting or fine-tuning makes belief representations more extractable by probes, testing the hypothesis that LLMs can develop truth-tracking capabilities when rewarded for accuracy.