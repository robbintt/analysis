---
ver: rpa2
title: Few-Shot Adaptation for Parsing Contextual Utterances with LLMs
arxiv_id: '2309.10168'
source_url: https://arxiv.org/abs/2309.10168
tags:
- event
- contextual
- utterances
- utterance
- program
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles few-shot adaptation for parsing contextual
  utterances in conversational semantic parsing, where models must learn to handle
  contextual utterances with limited training data compared to non-contextual utterances.
  The authors propose four major paradigms for parsing contextual utterances: Parse-with-Utterance-History,
  Parse-with-Reference-Program, Parse-then-Resolve, and Rewrite-then-Parse.'
---

# Few-Shot Adaptation for Parsing Contextual Utterances with LLMs

## Quick Facts
- arXiv ID: 2309.10168
- Source URL: https://arxiv.org/abs/2309.10168
- Authors: 
- Reference count: 12
- Key outcome: Rewrite-then-Parse paradigm achieves similar accuracy to other approaches while requiring fewer annotated examples, making it the most promising approach for few-shot adaptation of contextual utterance parsing.

## Executive Summary
This paper addresses the challenge of few-shot adaptation for parsing contextual utterances in conversational semantic parsing. The authors propose four major paradigms for handling contextual utterances and construct a new dataset, SMCalFlow-EventQueries, to enable fair comparisons. Through extensive experiments with both in-context learning (ICL) using GPT-3.5 and fine-tuning using T5-base, they demonstrate that the Rewrite-then-Parse approach is most promising, achieving comparable accuracy while requiring minimal annotation effort. The work provides valuable insights into the tradeoffs between different adaptation strategies and releases code and data to support future research.

## Method Summary
The authors propose four paradigms for parsing contextual utterances: Parse-with-Utterance-History, Parse-with-Reference-Program, Parse-then-Resolve, and Rewrite-then-Parse. They construct SMCalFlow-EventQueries, a dataset derived from SMCalFlow with annotations for all four paradigms. Experiments use in-context learning with GPT-3.5 (with prompt templates) and fine-tuning with T5-base 220M using Adafactor optimizer, batch size 32, and slanted triangular learning rate scheduler. The evaluation measures exact match accuracy on contextual utterances using both few-shot and full-data settings.

## Key Results
- Rewrite-then-Parse achieves similar accuracy to other paradigms while requiring only a few annotated examples for the rewriter
- Fine-tuning with T5-base outperforms in-context learning by 7.9% to 29.4% absolute gain across all paradigms
- Parse-with-Reference-Program performs best for ICL, leveraging GPT-3.5's strong program editing capabilities
- All paradigms struggle with rare functions and missing constraints in contextual utterances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rewrite-then-Parse requires fewer annotated examples than other paradigms because the rewriting task is domain-independent and easier to annotate than full program parsing.
- Mechanism: The rewriter converts contextual utterances into non-contextual ones using history, then a base parser trained on non-contextual data can handle the rest without needing program annotations for contextual utterances.
- Core assumption: Rewriting contextual utterances into non-contextual form is easier and cheaper to annotate than directly annotating programs for contextual utterances.
- Evidence anchors:
  - [abstract]: "Rewrite-then-Parse is the most promising paradigm when holistically considering parsing accuracy, annotation cost, and error types."
  - [section 3.2]: "collecting annotations for the utterance rewriting task is relatively easy and domain independent compared to collecting annotations for parsers which often requires learning a domain-specific language."
  - [corpus]: Weak - corpus neighbors focus on log parsing and semantic parsing robustness, not annotation cost tradeoffs.
- Break condition: If rewriting contextual utterances requires complex reasoning about dialogue context that is as difficult to annotate as direct program annotation, the cost advantage disappears.

### Mechanism 2
- Claim: Parse-with-Reference-Program works well in few-shot settings because GPT-3.5 is effective at editing programs using natural language instructions.
- Mechanism: The model uses the previous turn's program as a reference and edits it based on the current contextual utterance, leveraging the model's strong code editing capabilities.
- Core assumption: GPT-3.5 has strong capabilities for program editing when given natural language instructions about what changes to make.
- Evidence anchors:
  - [abstract]: "For ICL, Parse-with-Reference-Program performs the best, suggesting it is easier for GPT-3.5 to softly edit a program than parsing directly from natural language."
  - [section 4.2]: "30% of the errors made by Parse-with-Reference-Program are due to incorrect function use. In particular, the model struggles with predicting rare functions..."
  - [corpus]: Weak - corpus neighbors don't address few-shot program editing capabilities specifically.
- Break condition: If the reference program requires substantial edits rather than minor modifications, or if the edits involve rare functions GPT-3.5 hasn't seen, performance degrades significantly.

### Mechanism 3
- Claim: Fine-tuning with T5-base outperforms in-context learning with GPT-3.5 because fine-tuning allows the model to learn domain-specific patterns from contextual utterances.
- Mechanism: Fine-tuning adapts the model parameters to the specific patterns and functions used in the domain, while in-context learning relies on the model's pre-existing knowledge.
- Core assumption: The target domain has specific patterns and functions that benefit from parameter adaptation rather than relying on pre-trained knowledge alone.
- Evidence anchors:
  - [abstract]: "Across all paradigms, FT achieves higher exact match than ICL by 7.9% to 29.4% absolute gain."
  - [section 4.2]: "For ICL, the most common error type is incorrect function calls. 30% of the errors made by Parse-with-Reference-Program are due to incorrect function use. In particular, the model struggles with predicting rare functions..."
  - [corpus]: Weak - corpus neighbors focus on log parsing and semantic parsing robustness, not the ICL vs FT tradeoff.
- Break condition: If the domain has many rare functions or patterns that aren't well-represented in the fine-tuning data, the model may overfit to the training examples and fail to generalize.

## Foundational Learning

- Concept: Constraint decoding with context-free grammars
  - Why needed here: Ensures that generated programs are syntactically valid according to the domain's grammar, preventing invalid outputs during semantic parsing.
  - Quick check question: How does Earley's parsing algorithm validate whether a partial program prefix is allowed by the grammar?

- Concept: Few-shot learning and in-context learning
  - Why needed here: The paper explicitly studies scenarios where only a limited number of annotated contextual utterances are available, requiring methods that can learn effectively from few examples.
  - Quick check question: What is the difference between in-context learning (ICL) and fine-tuning (FT) in terms of how they use demonstration examples?

- Concept: Dialogue state tracking and contextual understanding
  - Why needed here: Contextual utterances depend on dialogue history, so understanding how to track and use dialogue state is crucial for accurate parsing.
  - Quick check question: How does the Rewrite-then-Parse paradigm use dialogue history differently from the Parse-with-Utterance-History paradigm?

## Architecture Onboarding

- Component map: Base parser (trained on non-contextual utterances) → Adaptation module (ICL or FT with contextual utterances) → Constrained decoder (with CFG validation) → Output program
- Critical path: For Rewrite-then-Parse: Rewriter → Base parser → CFG validation → Output
- Design tradeoffs: ICL is cheaper (no model training) but less accurate; FT is more accurate but requires computational resources and time for fine-tuning
- Failure signatures: Low accuracy on contextual utterances, high frequency of specific error types (incorrect functions, missing constraints), poor generalization to new domains
- First 3 experiments:
  1. Test each paradigm (Parse-with-Utterance-History, Parse-with-Reference-Program, Parse-then-Resolve, Rewrite-then-Parse) with both ICL and FT on a small subset of SMCalFlow-EQ to establish baseline performance differences
  2. Analyze error types for each paradigm to understand which components are failing (rewriter, parser, resolver)
  3. Test the impact of varying the number of demonstration examples (N) in few-shot adaptation to find the minimum effective sample size for each paradigm

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different paradigms perform when adapting to languages other than English, given that the LLMs used are primarily pre-trained on English text?
- Basis in paper: [explicit] The authors note that LLMs used are pre-trained primarily on English and SMCalFlow-EQ only contains English utterances, suggesting future work to study the few-shot adaptation problem on other languages.
- Why unresolved: The paper only evaluates the paradigms on English data, leaving the question of how well these paradigms generalize to other languages unanswered.
- What evidence would resolve it: Conducting experiments with the same paradigms on multilingual datasets would provide evidence of their performance across different languages.

### Open Question 2
- Question: What is the impact of the size and diversity of the dataset on the performance of the different parsing paradigms?
- Basis in paper: [explicit] The authors mention that the size of the SMCalFlow-EQ test set is relatively small and only studies dialogues from SMCalFlow, suggesting future work to conduct a similar study on larger and diverse datasets.
- Why unresolved: The current study is limited by the size and diversity of the dataset, which may not fully capture the performance of the paradigms in more varied and extensive settings.
- What evidence would resolve it: Testing the paradigms on larger and more diverse datasets would provide insights into how dataset characteristics influence their performance.

### Open Question 3
- Question: How do different design choices in the resolver for the Parse-then-Resolve paradigm affect the quality of the annotations and the complexity of the resolver?
- Basis in paper: [inferred] The paper discusses that the Parse-then-Resolve paradigm involves specialized contextual symbols based on the domain, which can greatly affect the quality of the annotations and the complexity of the resolver.
- Why unresolved: The paper does not explore the specific design choices for the resolver or their impact on performance, leaving this aspect unexplored.
- What evidence would resolve it: Experimenting with different resolver designs and evaluating their impact on annotation quality and resolver complexity would provide clarity on this issue.

## Limitations

- Dataset generalization: Experiments are conducted on a single domain (calendar scheduling) derived from SMCalFlow, making it unclear whether results generalize to other conversational domains.
- Cost analysis gaps: While the paper claims Rewrite-then-Parse has lower annotation costs, it lacks quantitative analysis of annotation time, cost, or inter-annotator agreement for different paradigms.
- Error attribution: Error analysis relies on manual categorization without systematic investigation of how rare function frequency differs between contextual and non-contextual utterances.

## Confidence

**High Confidence**:
- Rewrite-then-Parse achieves similar accuracy to other paradigms while requiring fewer annotated examples
- Fine-tuning (FT) consistently outperforms in-context learning (ICL) across all paradigms
- Parse-with-Reference-Program shows particular strength in ICL settings due to GPT-3.5's code editing capabilities

**Medium Confidence**:
- Claim about lower annotation costs for Rewrite-then-Parse is plausible but lacks quantitative cost analysis
- Assertion that ICL struggles more with rare functions than FT is supported by error analysis but could benefit from more systematic investigation

**Low Confidence**:
- Generalization of results to domains beyond calendar scheduling
- Specific impact of varying N (number of demonstration examples) on paradigm performance across different domains

## Next Checks

1. **Cross-Domain Validation**: Implement and test all four paradigms on at least two additional conversational domains (e.g., weather queries and restaurant reservations) using the same experimental setup to validate whether Rewrite-then-Parse maintains its cost-accuracy tradeoff advantage across diverse contexts.

2. **Annotation Cost Quantification**: Conduct a controlled annotation study where annotators create examples for each paradigm (rewriting, direct program annotation, etc.) while measuring time per example, inter-annotator agreement, and learning curves to provide empirical validation of claimed cost advantages.

3. **Rare Function Analysis**: Systematically analyze the frequency of rare functions in contextual versus non-contextual utterances within SMCalFlow-EQ, then create synthetic datasets with controlled rare function distributions to test whether observed ICL vs FT differences are due to function rarity or other factors.