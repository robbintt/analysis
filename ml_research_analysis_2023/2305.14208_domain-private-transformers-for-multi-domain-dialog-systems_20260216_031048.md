---
ver: rpa2
title: Domain Private Transformers for Multi-Domain Dialog Systems
arxiv_id: '2305.14208'
source_url: https://arxiv.org/abs/2305.14208
tags:
- redacted
- airline
- domain
- redaction
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces domain privacy as a novel property for language
  models to prevent inter-domain leakage. The authors define domain privacy, propose
  fine-tuning algorithms that balance privacy with model performance, and conduct
  extensive experiments on a multi-domain dialog dataset.
---

# Domain Private Transformers for Multi-Domain Dialog Systems

## Quick Facts
- arXiv ID: 2305.14208
- Source URL: https://arxiv.org/abs/2305.14208
- Authors: 
- Reference count: 40
- Key outcome: Introduces domain privacy property to prevent inter-domain leakage in multi-domain language models, achieving comparable membership inference attack resilience to DP methods while maintaining better perplexity.

## Executive Summary
This paper introduces domain privacy as a novel property for language models to prevent inter-domain leakage. The authors define domain privacy, propose fine-tuning algorithms that balance privacy with model performance, and conduct extensive experiments on a multi-domain dialog dataset. They develop policy functions based on token-level domain classification and propose an efficient one-stage fine-tuning method that transitions from redacted to non-redacted data. Experiments show that their method achieves comparable resiliency to membership inference attacks as differentially private training methods while maintaining better perplexity.

## Method Summary
The authors propose domain privacy for multi-domain language models by introducing fine-tuning approaches that prevent cross-domain leakage. They use policy functions based on token-level domain classification to identify and redact sensitive tokens during training. The key innovation is a one-stage fine-tuning method with a redaction schedule that gradually transitions from redacted to non-redacted data, achieving better perplexity than traditional differentially private methods while maintaining comparable privacy guarantees against membership inference attacks.

## Key Results
- The redaction schedule approach achieves comparable membership inference attack success rates to differentially private methods
- Maintains better perplexity (21.65) compared to Private method (25.49) on AIRLINE domain
- One-stage fine-tuning with redaction schedule outperforms two-stage JFT method in both privacy and performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain privacy can be achieved by fine-tuning with redacted data to prevent inter-domain leakage.
- Mechanism: By removing domain-sensitive tokens during fine-tuning, the model learns to avoid generating these tokens when prompted with inputs from other domains.
- Core assumption: Redacting sensitive tokens during training effectively prevents the model from learning to generate them in inappropriate contexts.
- Evidence anchors:
  - [abstract] "Experiments on membership inference attacks show that our proposed method has comparable resiliency to methods adapted from recent literature on differentially private language models."
  - [section 5.2] "The second is fine-tuned on redacted data instead (Pub+Redacted)."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.282, average citations=0.0."

### Mechanism 2
- Claim: One-stage fine-tuning with a redaction schedule improves domain privacy while maintaining performance.
- Mechanism: Gradually transitioning from redacted to non-redacted data during fine-tuning allows the model to learn domain-specific language patterns without generating sensitive tokens from other domains.
- Core assumption: A gradual transition from redacted to non-redacted data provides the benefits of both domain adaptation and privacy preservation.
- Evidence anchors:
  - [section 5.2] "This one-stage process costs half in fine-tuning compared to JFT, but still matches many of its benefits."
  - [section 6.4] "We also test running each stage of JFT for half the number of steps, i.e. with total compute comparable to other fine-tuning methods."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.282, average citations=0.0."

### Mechanism 3
- Claim: Policy functions based on token-level domain classification enable effective redaction for domain privacy.
- Mechanism: Using contextual models like RoBERTa to classify tokens as sensitive or not allows for more accurate redaction compared to keyword-based approaches.
- Core assumption: Token-level domain classification can accurately identify sensitive tokens across different domains.
- Evidence anchors:
  - [section 5.1] "We use a specified threshold z to set F BERT i (τ ) = 1 if there exists token sequence t∗ ⊆ τ such that Pr[hBERT (t∗) = di] > z."
  - [section 6.4] "LiRa attacks are more successful w.r.t. the RoBERTa redaction policy compared to the keyword, because the former policy has higher recall and lower precision."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.282, average citations=0.0."

## Foundational Learning

- Concept: Differential Privacy
  - Why needed here: To understand the theoretical foundation for protecting sensitive information in language models.
  - Quick check question: What is the difference between standard differential privacy and selective differential privacy?

- Concept: Language Modeling
  - Why needed here: To understand how language models generate text and how domain privacy affects this process.
  - Quick check question: How does perplexity relate to the quality of language model outputs?

- Concept: Policy Functions
  - Why needed here: To understand how sensitive tokens are identified and redacted during fine-tuning.
  - Quick check question: What are the trade-offs between keyword-based and contextual token classification for redaction?

## Architecture Onboarding

- Component map: Pretrained model -> Redaction policy -> Fine-tuning with redaction schedule -> Domain privacy evaluation
- Critical path: Pretrained model → Redaction policy → Fine-tuning with redaction schedule → Domain privacy evaluation
- Design tradeoffs: Balancing between privacy preservation and model performance, choosing between different redaction policies, and determining the optimal redaction schedule.
- Failure signatures: High perplexity on test data, successful membership inference attacks, or failure to generate domain-appropriate responses.
- First 3 experiments:
  1. Compare perplexity and domain privacy metrics between models fine-tuned with and without redaction.
  2. Evaluate the effectiveness of different redaction policies (keyword-based vs. contextual) on domain privacy.
  3. Test various redaction schedules to find the optimal balance between privacy and performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the domain privacy property scale when the number of domains increases beyond the three tested in this paper?
- Basis in paper: [explicit] The paper mentions that the modular framework of domain privacy "scales well with the number of domains" but does not provide empirical evidence for scaling beyond 3 domains.
- Why unresolved: The experiments only tested domain privacy on 3 domains (AIRLINE, MEDIA, INSURANCE). Scaling to many more domains could reveal new challenges in maintaining privacy while preserving performance.
- What evidence would resolve it: Experiments testing domain privacy on datasets with 10+ domains, measuring LiRa attack success rates and perplexity as the number of domains increases.

### Open Question 2
- Question: Can domain privacy be theoretically guaranteed with the same rigor as differential privacy, or are empirical methods like LiRa attacks the best we can achieve?
- Basis in paper: [inferred] The paper notes that "domain privacy can be empirically tested, even though it does not come with theoretical guarantees alike differential privacy."
- Why unresolved: While the paper proposes domain privacy as a practical concept, it acknowledges the lack of theoretical guarantees that differential privacy provides. This leaves open the question of whether formal guarantees are possible.
- What evidence would resolve it: Development of theoretical bounds or proofs showing that domain privacy can provide formal guarantees similar to differential privacy, or a demonstration that such guarantees are impossible.

### Open Question 3
- Question: How does the choice of redaction schedule (linear, expconvex, expconcave) affect the trade-off between domain privacy and model performance?
- Basis in paper: [explicit] The paper mentions that "We found that expconcave schedule outperformed the other schedules" but does not provide a comprehensive comparison of all schedules across all metrics.
- Why unresolved: The paper only briefly mentions the relative performance of different redaction schedules without providing detailed comparisons. A full analysis could reveal optimal scheduling strategies.
- What evidence would resolve it: Experiments comparing all three redaction schedules (linear, expconvex, expconcave) across multiple metrics including LiRa success rate, perplexity, and computational cost, potentially leading to recommendations for different use cases.

## Limitations

- Evaluation relies entirely on the MultiDoGo dataset, which represents a relatively narrow domain space
- Membership inference attack evaluation tests only one specific attack methodology (LiRa)
- The definition of domain privacy is novel but not formally connected to established differential privacy frameworks
- Experiments do not explore failure modes where redaction policies might fail or where gradual schedule transitions could create unexpected vulnerabilities

## Confidence

**High Confidence**: The empirical results demonstrating that the redaction schedule approach achieves comparable membership inference resistance to differentially private methods while maintaining better perplexity. The experimental methodology is clearly specified and the results are reproducible given the described setup.

**Medium Confidence**: The core claim that domain privacy can be effectively achieved through token-level redaction policies. While the experiments support this, the evaluation is limited to a single dataset and attack methodology, and the definition of "domain privacy" itself requires further theoretical development.

**Low Confidence**: The scalability and generalizability claims for the approach across different domain types, model sizes, and attack methodologies. The paper provides limited evidence about performance beyond the specific experimental setup.

## Next Checks

1. **Cross-Dataset Validation**: Evaluate the domain privacy approach on additional multi-domain datasets (e.g., MultiWOZ, Schema-Guided Dialogue) to assess generalizability beyond MultiDoGo. Measure whether the same redaction policies and schedules maintain effectiveness across different domain types and distributions.

2. **Attack Surface Expansion**: Test the model against alternative membership inference attack methodologies beyond LiRa, including black-box attacks and attacks targeting different aspects of the model's behavior. This would validate whether the domain privacy guarantees hold against a broader threat model.

3. **Policy Function Robustness Analysis**: Systematically evaluate how variations in the redaction policy (different classification thresholds, alternative token classification models, keyword variations) affect both privacy guarantees and model performance. This would identify the sensitivity of the approach to policy design choices and establish guidelines for optimal policy configuration.