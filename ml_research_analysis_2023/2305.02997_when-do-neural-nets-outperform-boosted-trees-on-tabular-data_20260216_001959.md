---
ver: rpa2
title: When Do Neural Nets Outperform Boosted Trees on Tabular Data?
arxiv_id: '2305.02997'
source_url: https://arxiv.org/abs/2305.02997
tags: []
core_contribution: This study addresses the ongoing debate on whether neural networks
  (NNs) or gradient-boosted decision trees (GBDTs) perform better on tabular data.
  The authors conduct the largest tabular data analysis to date, comparing 19 algorithms
  across 176 datasets.
---

# When Do Neural Nets Outperform Boosted Trees on Tabular Data?

## Quick Facts
- arXiv ID: 2305.02997
- Source URL: https://arxiv.org/abs/2305.02997
- Reference count: 40
- Key outcome: The NN vs GBDT debate is overemphasized; dataset regularity is the strongest predictor of performance differences.

## Executive Summary
This study conducts the largest tabular data analysis to date, comparing 19 algorithms across 176 datasets to determine when neural networks (NNs) outperform gradient-boosted decision trees (GBDTs). The authors find that the performance difference between NNs and GBDTs is negligible for a high fraction of datasets, and that light hyperparameter tuning on a GBDT is often more important than choosing between NNs and GBDTs. The recently proposed TabPFN outperforms all other algorithms on average when training sets are limited to 3000 instances. The study analyzes 965 metafeatures to identify dataset properties that favor NNs or GBDTs, finding that GBDTs excel at handling skewed or heavy-tailed feature distributions and other forms of dataset irregularities.

## Method Summary
The study compares 19 algorithms (3 GBDTs, 11 neural networks, 5 baselines) on 176 OpenML classification datasets using 10 train/validation/test folds per dataset. Each algorithm is evaluated with up to 30 hyperparameter settings (1 default + 29 random via Optuna) within a 10-hour time limit per dataset split. Performance is measured using normalized accuracy, normalized log loss, and F1 score. The authors extract 965 metafeatures for each dataset split using PyMFE and analyze their correlation with algorithm performance differences. The TabZilla Benchmark Suite of the 36 'hardest' datasets is released along with the codebase and raw results.

## Key Results
- The NN vs GBDT debate is overemphasized; performance differences are negligible for a high fraction of datasets
- Light hyperparameter tuning on a GBDT is more important than choosing between NNs and GBDTs for about one-third of datasets
- TabPFN outperforms all other algorithms on average when training sets are limited to 3000 instances
- GBDTs are much better than NNs at handling skewed or heavy-tailed feature distributions and other forms of dataset irregularities
- Dataset regularity (skewness, heavy-tailedness) is the strongest predictor of NN vs GBDT performance differences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dataset regularity is the strongest predictor of whether neural networks outperform gradient-boosted decision trees (GBDTs).
- Mechanism: When feature distributions are less skewed and less heavy-tailed, neural networks benefit from their inductive biases and regularization capabilities, allowing them to capture complex patterns that GBDTs handle less efficiently on regular data. Conversely, GBDTs excel when feature distributions are highly irregular, as their decision-tree structure can naturally partition skewed and heavy-tailed data.
- Core assumption: The skewness and kurtosis of feature distributions directly impact the relative performance of NNs vs GBDTs.
- Evidence anchors:
  - [abstract] "GBDTs are much better than NNs at handling skewed or heavy-tailed feature distributions and other forms of dataset irregularities."
  - [section] "GBDTs tend to outperform NNs on irregular datasets, for example, datasets whose feature distributions are skewed or heavy tailed."
  - [corpus] No direct evidence found for this mechanism; requires domain-specific interpretation.
- Break condition: If feature engineering or transformation is applied to normalize distributions, the predictive power of dataset regularity may diminish.

### Mechanism 2
- Claim: Larger datasets with a high ratio of instances to features favor GBDTs over NNs.
- Mechanism: GBDTs can leverage the abundance of data to build more robust decision boundaries, while NNs may overfit or struggle with the increased complexity. The ratio of dataset size to number of features is critical because GBDTs split on features based on more datapoints, making them more effective when this ratio is high.
- Core assumption: The performance difference between NNs and GBDTs scales with dataset size and feature-to-instance ratio.
- Evidence anchors:
  - [abstract] "Furthermore, GBDTs tend to perform better on larger datasets."
  - [section] "GBDTs favor large datasets, and datasets for which the ratio of size to number of features is high."
  - [corpus] Weak evidence; the corpus mentions sample size effects but not the feature-to-instance ratio specifically.
- Break condition: If neural networks are pretrained or use strong regularization, the advantage of GBDTs on large datasets may be reduced.

### Mechanism 3
- Claim: Hyperparameter tuning on a strong algorithm like CatBoost or ResNet can yield greater performance improvements than choosing between NNs and GBDTs.
- Mechanism: Light hyperparameter tuning allows the algorithm to adapt to the specific characteristics of the dataset, potentially overcoming inherent limitations of the algorithm class. This suggests that algorithm selection is less critical than optimizing the chosen algorithm for the dataset at hand.
- Core assumption: Hyperparameter tuning is more impactful than algorithm selection for a significant fraction of datasets.
- Evidence anchors:
  - [abstract] "Light hyperparameter tuning on a GBDT is more important than choosing between NNs and GBDTs."
  - [section] "Light hyperparameter tuning yields a greater performance improvement than GBDT-vs-NN selection for about one-third of all datasets."
  - [corpus] No direct evidence found for this mechanism; requires interpretation from the paper's findings.
- Break condition: If the dataset is extremely small or has unique characteristics that require a specific algorithm class, hyperparameter tuning may not compensate for the wrong algorithm choice.

## Foundational Learning

- Concept: Metafeature extraction and analysis
  - Why needed here: Understanding dataset properties (metafeatures) is crucial for predicting algorithm performance and guiding algorithm selection.
  - Quick check question: What are the top 5 metafeatures that correlate most strongly with NN vs GBDT performance differences?

- Concept: Algorithm families and their strengths/weaknesses
  - Why needed here: Knowing the characteristics of GBDTs, NNs, and baselines is essential for interpreting the results and understanding when each family excels.
  - Quick check question: List the key differences between GBDTs and NNs in handling irregular data distributions.

- Concept: Statistical testing and significance
  - Why needed here: Determining whether performance differences between algorithms are statistically significant is critical for drawing valid conclusions.
  - Quick check question: Which statistical test is used to determine significant performance differences between algorithms?

## Architecture Onboarding

- Component map: 19 algorithms (CatBoost, XGBoost, LightGBM, ResNet, SAINT, FTTransformer, DANet, NODE, VIME, TabNet, AutoInt, FiLM, MLP, kNN, SVM, Decision Tree, Random Forest, AdaBoost, Naive Bayes) -> 176 datasets -> 965 metafeatures
- Critical path: For each dataset split, train and evaluate each algorithm with multiple hyperparameter settings, record test set performance, and extract 965 metafeatures for correlation analysis.
- Design tradeoffs: The choice between running many algorithms with default hyperparameters vs. tuning a few strong algorithms is a key tradeoff. Additionally, the decision to include time limits for experiments balances computational cost with result completeness.
- Failure signatures: If an algorithm consistently fails to complete within the time limit, it may indicate that the algorithm is not well-suited for the dataset characteristics. Also, if metafeature correlations are weak, the predictive power of the analysis may be limited.
- First 3 experiments:
  1. Run CatBoost and ResNet with default hyperparameters on a small, regular dataset to compare their performance and observe the impact of dataset regularity.
  2. Tune the hyperparameters of CatBoost on a large, irregular dataset to see if tuning can overcome the algorithm's inherent limitations.
  3. Analyze the metafeatures of a dataset to predict whether NNs or GBDTs will perform better, then verify the prediction by running both algorithm families.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions do neural networks outperform gradient-boosted decision trees on tabular data?
- Basis in paper: [explicit] The paper states "GBDTs outperform NNs on datasets that are irregular (such as having a wide range of skewness or kurtosis across the feature distributions), as well as large datasets, and datasets with a high ratio of size to number of features."
- Why unresolved: The paper provides general conditions favoring GBDTs but does not specify the exact thresholds or boundary conditions where NNs would outperform GBDTs.
- What evidence would resolve it: Empirical studies identifying precise dataset characteristics (e.g., exact kurtosis thresholds, dataset size breakpoints) where NN performance surpasses GBDT performance.

### Open Question 2
- Question: How much does hyperparameter tuning contribute to the performance gap between neural networks and gradient-boosted decision trees?
- Basis in paper: [explicit] "for roughly one-third of all datasets, light hyperparameter tuning on a GBDT or ResNet model increases performance more than choosing between GBDTs or NNs"
- Why unresolved: The paper quantifies the relative impact of hyperparameter tuning vs. algorithm selection but doesn't provide detailed analysis of tuning's absolute contribution to closing the performance gap.
- What evidence would resolve it: Systematic experiments measuring the performance improvement from extensive hyperparameter tuning of both NNs and GBDTs across diverse dataset types.

### Open Question 3
- Question: Can we develop a unified framework that automatically selects the optimal algorithm (NN, GBDT, or baseline) based on dataset characteristics?
- Basis in paper: [explicit] "Our insights act as a guide for the practitioner to follow when faced with a new dataset"
- Why unresolved: While the paper identifies dataset properties correlated with algorithm performance, it doesn't provide a practical decision-making framework for algorithm selection.
- What evidence would resolve it: Development and validation of a metalearning model that predicts the best-performing algorithm family based on dataset metafeatures, with demonstrated success on unseen datasets.

## Limitations

- The study's computational cost may have constrained the depth of hyperparameter exploration for each algorithm.
- The analysis of metafeatures, while comprehensive with 965 features, may not capture all relevant dataset characteristics.
- The findings are based on classification tasks and may not directly extend to regression problems.

## Confidence

- **High confidence**: The claim that dataset regularity (skewness, heavy-tailedness) is the strongest predictor of NN vs GBDT performance differences.
- **Medium confidence**: The claim that light hyperparameter tuning is more impactful than algorithm selection for a significant fraction of datasets.
- **Low confidence**: The specific ranking of metafeatures by their correlation with algorithm performance differences.

## Next Checks

1. **Replication on Regression Tasks**: Validate the findings on a diverse set of regression datasets to assess the generalizability of the NN vs GBDT performance insights beyond classification.
2. **Extended Hyperparameter Tuning**: Conduct a more extensive hyperparameter search for a subset of datasets to determine if deeper tuning could further reduce the performance gap between NNs and GBDTs.
3. **Impact of Feature Engineering**: Investigate how preprocessing techniques that normalize feature distributions (e.g., Box-Cox transformation) affect the relative performance of NNs and GBDTs on irregular datasets.