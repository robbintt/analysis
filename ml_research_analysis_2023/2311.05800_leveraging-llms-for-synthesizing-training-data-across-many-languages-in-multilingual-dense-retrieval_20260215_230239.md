---
ver: rpa2
title: Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual
  Dense Retrieval
arxiv_id: '2311.05800'
source_url: https://arxiv.org/abs/2311.05800
tags:
- training
- language
- retrieval
- swim-ir
- swim-x
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops SWIM-IR, a synthetic multilingual retrieval
  training dataset of 28 million query-passage pairs across 33 languages, generated
  using a summarize-then-ask prompting (SAP) method with PaLM 2. SAP first extracts
  an extractive summary from the input passage, then uses it as an intermediate step
  to generate informative queries in the target language.
---

# Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval

## Quick Facts
- arXiv ID: 2311.05800
- Source URL: https://arxiv.org/abs/2311.05800
- Reference count: 40
- This paper develops SWIM-IR, a synthetic multilingual retrieval training dataset of 28 million query-passage pairs across 33 languages, generated using a summarize-then-ask prompting (SAP) method with PaLM 2.

## Executive Summary
This paper presents SWIM-IR, a synthetic multilingual retrieval training dataset of 28 million query-passage pairs across 33 languages, generated using PaLM 2 with a novel summarize-then-ask prompting (SAP) method. SAP first extracts an extractive summary from input passages, then uses it as an intermediate step to generate informative queries in target languages. The dataset enables training of multilingual dense retrieval models (SWIM-X) without human supervision. When evaluated on three benchmarks—XOR-Retrieve (cross-lingual), MIRACL (monolingual), and XTREME-UP (cross-lingual)—SWIM-X models outperform supervised baselines on XOR-Retrieve (by 7.1 points Recall@5kt) and XTREME-UP (by 11.7 points MRR@10), and remain competitive on MIRACL. The approach demonstrates that synthetic data can effectively substitute for expensive human-labeled training data in multilingual retrieval.

## Method Summary
The method involves pretraining an mT5-base model on mC4 (101 languages) using contrastive loss, then generating synthetic training data using PaLM 2-S with SAP (summarize-then-ask prompting). SAP involves extractive summary generation followed by query generation in the target language. The synthetic dataset (SWIM-IR) contains 28 million query-passage pairs across 33 languages, generated from Wikipedia passages with content filtering. The mT5 model is then fine-tuned on SWIM-IR with contrastive loss using in-batch negatives, and evaluated on XOR-Retrieve, MIRACL, and XTREME-UP benchmarks.

## Key Results
- SWIM-X models outperform the best supervised baseline (mContriever-X) by 7.1 points Recall@5kt on XOR-Retrieve
- SWIM-X achieves 11.7 points MRR@10 improvement over mContriever-X on XTREME-UP
- Performance saturates after 500K synthetic pairs for XOR-Retrieve, suggesting optimal data efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAP improves multilingual query generation quality by breaking down the complex task into extractive summary and query generation stages
- Mechanism: The extractive summary identifies relevant information from passages, acting as a useful signal that guides query generation and reduces hallucinations
- Core assumption: Quality of extractive summary directly influences generated query quality
- Evidence anchors: Abstract mentions SAP identifies relevant sections using extractive summarization; Section 2.1 describes the two-stage process

### Mechanism 2
- Claim: Synthetic training data can effectively substitute for expensive human-labeled training data in multilingual dense retrieval
- Mechanism: Large amounts of synthetic data generated by LLMs at low cost can fine-tune dense retrieval models to achieve competitive performance
- Core assumption: Synthetic queries are of sufficient quality to enable effective model fine-tuning
- Evidence anchors: Abstract states SWIM-X is competitive with human-supervised models; Section 3.3 shows 7.1 point Recall@5kt improvement on XOR-Retrieve

### Mechanism 3
- Claim: Language-agnostic pre-training on mC4 improves multilingual dense retrieval performance
- Mechanism: Pre-training on large multilingual corpus provides general language understanding capabilities that transfer to dense retrieval tasks
- Core assumption: Language-agnostic pre-training captures beneficial general language understanding for retrieval tasks
- Evidence anchors: Section 3.3 shows only 1.6 point performance drop without mC4 pre-training; mentions break-even at 200K synthetic pairs

## Foundational Learning

- Concept: Dense retrieval
  - Why needed here: Understanding dense retrieval fundamentals is crucial for grasping the significance of synthetic data approach
  - Quick check question: What is the key difference between dense retrieval and traditional sparse retrieval methods like BM25?

- Concept: Cross-lingual retrieval
  - Why needed here: The paper addresses both cross-lingual and monolingual retrieval tasks, requiring understanding of cross-lingual challenges
  - Quick check question: What are the main challenges in cross-lingual retrieval, and how do they differ from monolingual retrieval?

- Concept: Large language models (LLMs)
  - Why needed here: LLMs are leveraged for generating synthetic training data, requiring understanding of their capabilities and limitations
  - Quick check question: What are the key strengths and weaknesses of using LLMs for generating synthetic training data in multilingual retrieval tasks?

## Architecture Onboarding

- Component map: Unlabeled passages (Wikipedia corpus) -> PaLM 2 with SAP -> Synthetic training data (SWIM-IR) -> mT5-based dense retrieval model (SWIM-X) -> Fine-tuning on synthetic data -> Retrieval benchmarks (XOR-Retrieve, MIRACL, XTREME-UP)

- Critical path: 1) Sample passages from Wikipedia corpus 2) Generate synthetic queries using LLM with SAP 3) Construct synthetic training dataset (SWIM-IR) 4) Fine-tune dense retrieval model (SWIM-X) on synthetic data 5) Evaluate model on retrieval benchmarks

- Design tradeoffs: LLM choice balancing performance and computational cost; prompt strategy optimizing for query quality and diversity; dataset size balancing coverage and efficiency; fine-tuning strategy optimizing for downstream performance

- Failure signatures: Low-quality synthetic queries leading to poor model performance; overfitting to synthetic data resulting in poor generalization; language-specific issues like code-switching and factual inconsistencies

- First 3 experiments: 1) Evaluate impact of different LLM sizes on query quality and retrieval performance 2) Investigate optimal number of prompt exemplars (K-shot) for SAP 3) Compare performance with and without language-agnostic pre-training

## Open Questions the Paper Calls Out
The paper identifies several open questions: (1) How does SAP compare to other synthetic data generation methods like InPars or Promptagator? (2) What is the optimal balance between synthetic data quantity and quality across different language resource levels? (3) How do different multilingual pre-trained models (mT5, ByT5, XLM-R, mBERT) affect synthetic fine-tuning performance when combined with SAP-generated data?

## Limitations
- Heavy reliance on LLM-generated synthetic data without human validation raises quality concerns
- Computational cost and environmental impact of generating 28 million synthetic pairs not fully addressed
- Performance gap between mT5 and ByT5 in low-resource languages suggests tokenizer limitations

## Confidence
- High confidence: Synthetic data can substitute for human-labeled data in multilingual retrieval (supported by strong benchmark results)
- Medium confidence: SAP method effectiveness (supported by ablation studies but exact component contributions unclear)
- Low confidence: Language-agnostic pre-training being essential (performance drops only 1.6 points without it)

## Next Checks
1. Conduct blind human evaluations comparing SWIM-IR queries against human-labeled queries from supervised baselines, measuring fluency, adequacy, and relevance across multiple languages
2. Evaluate SWIM-X models on retrieval tasks from completely different domains (e.g., medical or legal documents) not represented in Wikipedia to test generalization
3. Measure total computational cost of generating 28 million synthetic pairs versus human annotation costs, including both monetary costs and environmental impact to assess the "cheap" substitution claim quantitatively