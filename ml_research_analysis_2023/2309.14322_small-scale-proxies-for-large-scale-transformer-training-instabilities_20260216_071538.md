---
ver: rpa2
title: Small-scale proxies for large-scale Transformer training instabilities
arxiv_id: '2309.14322'
source_url: https://arxiv.org/abs/2309.14322
tags:
- learning
- loss
- rate
- figure
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies training instabilities in large Transformers
  by reproducing them at small scale. The authors examine two instabilities: attention
  logit growth and output logit divergence.'
---

# Small-scale proxies for large-scale Transformer training instabilities

## Quick Facts
- arXiv ID: 2309.14322
- Source URL: https://arxiv.org/abs/2309.14322
- Reference count: 40
- Key outcome: The paper demonstrates that training instabilities in large Transformers can be reproduced and studied in small models at high learning rates by examining the relationship between learning rate and loss across scales.

## Executive Summary
This paper addresses the challenge of training instabilities in large Transformer models by introducing a methodology to reproduce and study these instabilities at small scale. The authors focus on two specific instabilities: attention logit growth and output logit divergence. By measuring how loss varies with learning rate across different model scales, they show that these instabilities also manifest in small models when trained at high learning rates. This approach allows for efficient study and mitigation of instabilities without requiring massive computational resources. The paper also demonstrates that specific interventions like qk-layernorm and z-loss regularization, previously used at large scales, are equally effective in the small-scale regime.

## Method Summary
The authors develop a methodology to study large-scale Transformer training instabilities using small-scale proxies. They train small Transformer models with decoder-only architecture on the C4 dataset, systematically varying learning rates and model scales. The key innovation is measuring the relationship between learning rate and loss across scales to identify and reproduce instabilities. They examine scaling trends of model characteristics like gradient norms and attention logits to predict when instabilities will emerge in larger models. The methodology includes using AdamW optimizer with specific hyperparameters, warm-up and cosine decay learning rate schedules, and interventions like qk-layernorm and z-loss regularization. By analyzing how these characteristics scale with model size and learning rate, the authors can extrapolate to predict instability emergence in larger models.

## Key Results
- Attention logit growth and output logit divergence instabilities that occur in large models can be reproduced in small models at high learning rates.
- Learning rate sensitivity, defined as deviation from optimal performance when varying learning rate, increases with model scale.
- Specific interventions like qk-layernorm and z-loss regularization effectively mitigate instabilities across scales.
- Gradient norms decrease with both scale and learning rate, indicating that the default AdamW epsilon hyperparameter is too large for large-scale training.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The relationship between learning rate and loss across model scales can reveal and predict training instabilities.
- Mechanism: By measuring how loss varies with learning rate for different model sizes, instabilities that only appear at large scales can be reproduced in small models at high learning rates, enabling study without large compute.
- Core assumption: Instabilities scale predictably with model size and learning rate, and the shape of learning rate vs. loss curves is informative about underlying training dynamics.
- Evidence anchors:
  - [abstract] "By measuring the relationship between learning rate and loss across scales, we show that these instabilities also appear in small models when training at high learning rates..."
  - [section] "LR sensitivity measures the deviation from optimal performance when varying LR across orders of magnitude."
  - [corpus] Weak; no corpus neighbors directly address the specific learning rate vs. loss relationship across scales.
- Break condition: If the instability is not driven by a monotonic scaling effect (e.g., non-monotonic or threshold-dependent behaviors), the predictive relationship may fail.

### Mechanism 2
- Claim: Specific model characteristics like attention logit norms and gradient norms can predict when instabilities will occur as models scale.
- Mechanism: By examining how these characteristics scale with model size and learning rate, one can extrapolate to predict when an instability (e.g., attention logit growth) will emerge in larger models.
- Core assumption: The scaling trends of model characteristics are smooth and predictable, and critical thresholds for instability can be identified.
- Evidence anchors:
  - [section] "Using the attention logit growth instability as an example, we show that it is possible to predict an instability before it emerges by examining the scaling behavior of model activation and gradient norms."
  - [section] "Our investigation shows that gradient norms decrease with both scale and learning rate, such that the default AdamW epsilon hyperparameter is too large."
  - [corpus] Weak; corpus neighbors do not focus on predicting instabilities from scaling trends.
- Break condition: If the scaling behavior of model characteristics is non-smooth or if the instability is triggered by factors not captured by these characteristics.

### Mechanism 3
- Claim: Specific interventions like qk-layernorm and z-loss regularization can mitigate training instabilities and reduce learning rate sensitivity across scales.
- Mechanism: These interventions directly address the root causes of the instabilities (e.g., controlling attention logit growth, preventing output logit divergence), allowing stable training across a wider range of learning rates.
- Core assumption: The proposed interventions effectively target the underlying mechanisms causing the instabilities.
- Evidence anchors:
  - [abstract] "By measuring the relationship between learning rate and loss across scales, we show that these instabilities also appear in small models when training at high learning rates, and that mitigations previously employed at large scales are equally effective in this regime."
  - [section] "Qk-layernorm reduces LR sensitivity and enables successful training across three orders of magnitude of LR variation."
  - [section] "The mitigation proposed by Chowdhery et al. [6] is to encourage log Z to remain close to zero. They add an auxiliary loss log2 Z, referred to as z-loss, with coefficient 1e-4."
  - [corpus] Weak; corpus neighbors do not discuss these specific interventions.
- Break condition: If the underlying cause of the instability changes or if the intervention has unintended side effects that outweigh its benefits.

## Foundational Learning

- Concept: Learning rate sensitivity as a summary statistic for learning rate vs. loss curves.
  - Why needed here: To quantify and compare the stability of training across different model scales and hyperparameter settings.
  - Quick check question: How is learning rate sensitivity defined, and what does a high value indicate about training stability?

- Concept: Scaling laws for model characteristics (e.g., activation and gradient norms).
  - Why needed here: To predict when instabilities will emerge as models scale and to understand the underlying mechanisms driving these instabilities.
  - Quick check question: How can the scaling behavior of model characteristics be used to predict instabilities, and what are some examples of relevant characteristics?

- Concept: The role of specific interventions (e.g., qk-layernorm, z-loss, warm-up) in mitigating training instabilities.
  - Why needed here: To understand how to stabilize training across different model scales and hyperparameter settings.
  - Quick check question: What are the mechanisms by which interventions like qk-layernorm and z-loss regularization mitigate training instabilities?

## Architecture Onboarding

- Component map:
  Transformer model with decoder-only architecture -> AdamW optimizer with specific hyperparameters (β1, β2, ε, weight decay) -> Warm-up and cosine decay learning rate schedule -> QK-layernorm and z-loss regularization interventions -> C4 dataset with SentencePiece tokenizer

- Critical path:
  1. Data loading and preprocessing (SentencePiece tokenization, sequence packing)
  2. Model forward pass (self-attention with rotary positional embeddings, feed-forward MLP)
  3. Loss computation (auto-regressive loss)
  4. Backward pass and gradient computation
  5. Parameter update with AdamW (including weight decay and z-loss regularization)
  6. Learning rate scheduling (warm-up and cosine decay)

- Design tradeoffs:
  - Joint scaling of embedding size, depth, and number of heads vs. independent scaling
  - Choice of learning rate, weight decay, and warm-up length
  - Use of qk-layernorm and z-loss regularization vs. default settings
  - Batch size and sequence length

- Failure signatures:
  - Loss explosion at high learning rates (attention logit growth or output logit divergence)
  - Slow divergence or collapse in gradient norms
  - High learning rate sensitivity (large deviation from optimal loss across learning rate range)

- First 3 experiments:
  1. Reproduce attention logit growth instability in a small model at high learning rate, with and without qk-layernorm intervention.
  2. Measure learning rate sensitivity across different model scales and hyperparameter settings (e.g., warm-up length, weight decay).
  3. Examine scaling trends of model characteristics (e.g., gradient norms) to predict when instabilities will emerge in larger models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the attention logit growth instability manifest in larger models (e.g., > 10B parameters) at higher learning rates (e.g., > 0.1) when using qk-layernorm?
- Basis in paper: [explicit] The paper predicts that attention logit growth will occur in a 4.8B parameter model at LR 0.01 without qk-layernorm, and confirms this prediction experimentally. The paper also shows that qk-layernorm reduces LR sensitivity across scales, but LR sensitivity still increases with model scale.
- Why unresolved: The paper does not test models larger than 1.2B parameters at learning rates above 0.1 with qk-layernorm. It is unclear if qk-layernorm remains effective at preventing attention logit growth in much larger models at higher learning rates.
- What evidence would resolve it: Training experiments with models > 10B parameters at learning rates > 0.1 with and without qk-layernorm, measuring the maximum attention logit and final loss to determine if the instability occurs and if qk-layernorm mitigates it.

### Open Question 2
- Question: What is the precise relationship between parameter norm growth and the gradient RMS decreasing with scale and learning rate, leading to the AdamW epsilon instability?
- Basis in paper: [inferred] The paper observes that parameter RMS grows with learning rate and scale, which causes the Transformer block output RMS to also grow. This in turn causes the gradient received by the Transformer to shrink due to the layernorm gradient scaling. The paper also shows that gradient RMS decreases with scale and learning rate, approaching the default AdamW epsilon hyperparameter.
- Why unresolved: While the paper provides a qualitative explanation for how parameter norm growth leads to the AdamW epsilon instability, the precise quantitative relationship between these phenomena is not established. It is unclear how the growth of parameter norms, output RMS, and the decreasing gradient RMS interact to cause the instability.
- What evidence would resolve it: A detailed analysis quantifying the relationship between parameter norm growth, output RMS, and gradient RMS across different scales and learning rates. This could involve measuring these quantities during training and fitting mathematical models to characterize their scaling behavior.

### Open Question 3
- Question: Does the output logit divergence instability occur in larger models at higher learning rates when using z-loss regularization and weight decay?
- Basis in paper: [explicit] The paper shows that the output logit divergence instability occurs in small models at high learning rates without weight decay, and that z-loss regularization and weight decay mitigate this instability. The paper also shows that LR sensitivity increases with model scale, even with these mitigations.
- Why unresolved: The paper does not test models larger than 3e8 parameters at high learning rates with z-loss and weight decay. It is unclear if the output logit divergence instability persists in much larger models at higher learning rates, even with these mitigations.
- What evidence would resolve it: Training experiments with models > 1B parameters at learning rates > 0.1 with z-loss and weight decay, measuring the output logit mean and final loss to determine if the instability occurs and if the mitigations remain effective.

## Limitations

- The paper focuses on only two specific instabilities (attention logit growth and output logit divergence), and it's unclear whether the small-scale proxy approach generalizes to other types of instabilities.
- The corpus signals indicate limited related work, suggesting this is a relatively novel approach that needs broader validation across different model architectures and tasks.
- The paper doesn't extensively explore the potential interactions between multiple instabilities occurring simultaneously, which may be relevant in practice.

## Confidence

- High Confidence: The relationship between learning rate sensitivity and training stability is well-established. The experimental methodology for measuring this relationship is sound.
- Medium Confidence: The predictive power of scaling trends for model characteristics to forecast instabilities is promising but needs broader validation across different model architectures and tasks.
- Low Confidence: The generalizability of small-scale proxies to predict large-scale instabilities beyond the two specific cases studied remains to be fully established.

## Next Checks

1. Test the predictive power of scaling trends on a broader range of instabilities beyond attention logit growth and output logit divergence, including potential new instability types that may emerge at large scale.
2. Validate the small-scale proxy approach across different model architectures (e.g., encoder-decoder models, vision transformers) to assess generalizability.
3. Investigate the interaction effects between multiple instabilities occurring simultaneously and how the proposed interventions affect these complex scenarios.