---
ver: rpa2
title: 'Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge
  for Improved Language Model Reasoning'
arxiv_id: '2311.08505'
source_url: https://arxiv.org/abs/2311.08505
tags:
- knowledge
- reasoning
- answer
- question
- chain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a semi-structured prompting approach that
  integrates a model's parametric memory with external unstructured and structured
  knowledge for improved reasoning. The method parses multi-hop questions into masked
  reasoning chains, then fills in the masks using a combination of the model's parametric
  memory, retrieved documents, and knowledge graph queries.
---

# Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning

## Quick Facts
- **arXiv ID**: 2311.08505
- **Source URL**: https://arxiv.org/abs/2311.08505
- **Reference count**: 6
- **Key outcome**: Introduces a semi-structured prompting approach that integrates a model's parametric memory with external unstructured and structured knowledge for improved reasoning, showing significant performance improvements on multi-hop QA datasets.

## Executive Summary
This paper introduces a novel semi-structured chain-of-thought prompting approach that enhances language model reasoning by integrating multiple knowledge sources: parametric memory, external unstructured documents, and structured knowledge graphs. The method parses multi-hop questions into masked reasoning chains, then fills in the masks using a combination of the model's parametric memory, retrieved documents, and knowledge graph queries. Experiments on multi-hop QA datasets show significant performance improvements over existing prompting methods, even those requiring fine-tuning. The approach demonstrates the effectiveness of seamlessly combining multiple knowledge sources for enhanced reasoning capabilities.

## Method Summary
The method uses few-shot in-context learning to parse multi-hop questions into semi-structured reasoning chains with masked triplets. These masks are sequentially filled using knowledge sources in priority order: first querying a structured knowledge graph via SPARQL, then retrieving and reading relevant documents for unstructured knowledge, and finally using the model's parametric memory as a last resort. The approach filters syntactically incorrect chains and applies self-consistency across multiple samples. The entire process requires no model fine-tuning, relying instead on the LLM's ability to understand and replicate the semi-structured reasoning pattern from provided exemplars.

## Key Results
- Significant performance improvements on 2WikiMultiHopQA and MuSiQue-Ans datasets compared to existing prompting methods
- Effective integration of structured knowledge graphs, unstructured documents, and parametric memory without model fine-tuning
- Performance benefits from increased model size across all knowledge source combinations
- Semi-structured approach enables interpretable reasoning chains that can be filled with external knowledge

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Semi-structured chain-of-thought parsing enables the model to break down complex multi-hop questions into a series of interpretable reasoning steps that can be filled with external knowledge.
- **Mechanism**: The LLM first parses the question into a chain of (head, relation, tail) triplets, with masks as placeholders for missing entities. These masks are then sequentially filled by querying either structured knowledge (via KG SPARQL queries), unstructured knowledge (via document retrieval and QA), or parametric memory.
- **Core assumption**: The model can correctly parse multi-hop questions into valid logical chains without requiring fine-tuning on semantic parsing.
- **Evidence anchors**:
  - [abstract] "The method parses multi-hop questions into masked reasoning chains, then fills in the masks using a combination of the model's parametric memory, retrieved documents, and knowledge graph queries."
  - [section 3.1] "We use MLLM to parse masked reasoning chains from each input q with few-shot prompting."
  - [corpus] Weak — no direct evidence that the parsed chains actually improve reasoning performance without fine-tuning.
- **Break condition**: If the LLM fails to parse the question into a valid semi-structured chain (syntactic or semantic errors), the downstream knowledge-filling process cannot proceed correctly.

### Mechanism 2
- **Claim**: The staged filling of masks—first structured, then unstructured, then parametric memory—maximizes precision and minimizes hallucination.
- **Mechanism**: By prioritizing structured knowledge from the knowledge graph first, the system ensures high-precision answers for factual relations that are available. If the knowledge is not in the KG, it falls back to document retrieval. Only unresolved masks are filled using the LLM's parametric memory.
- **Core assumption**: Structured knowledge sources (knowledge graphs) are more reliable than unstructured text or parametric memory for factual recall.
- **Evidence anchors**:
  - [abstract] "Experimental results... demonstrate that our prompting method significantly surpasses existing techniques..."
  - [section 3.2] "We generally prioritize parametric memory last due to its lower reliability in knowledge retrieval."
  - [corpus] Moderate — Table 3 shows oracle KG + Text + Model achieves 0.89 F1 vs. 0.87 with Wikidata, but no direct ablation for order effects.
- **Break condition**: If the structured knowledge source lacks coverage for required facts, the system may over-rely on less accurate unstructured or parametric sources, degrading accuracy.

### Mechanism 3
- **Claim**: Few-shot in-context exemplars enable the LLM to generalize the semi-structured chain-of-thought parsing and mask-filling without model fine-tuning.
- **Mechanism**: The LLM is provided with manually annotated exemplars that show how to parse questions into reasoning chains and how to convert masked triplets into single-hop questions. This allows zero-training adaptation.
- **Core assumption**: In-context learning with a small number of exemplars is sufficient for the model to understand and replicate the semi-structured reasoning pattern.
- **Evidence anchors**:
  - [abstract] "The method parses multi-hop questions into masked reasoning chains, then fills in the masks using a combination of the model's parametric memory, retrieved documents, and knowledge graph queries."
  - [section 4.2] "We utilize few-shot in-context learning and do not perform any model training."
  - [corpus] Weak — no ablation on exemplar quantity or quality; assumes 25-shot is sufficient.
- **Break condition**: If the exemplars are insufficient or poorly annotated, the model may generate syntactically or semantically incorrect chains.

## Foundational Learning

- **Concept**: Semantic parsing into logical forms (text-to-SQL style)
  - Why needed here: The method relies on parsing natural language questions into structured triplets that can be mapped to knowledge sources.
  - Quick check question: Given "Who directed The Blue Umbrella?", what would the semi-structured reasoning chain look like?

- **Concept**: Knowledge graph querying (SPARQL, entity linking)
  - Why needed here: Structured knowledge is queried using entity and relation linking to ground heads and relations in the KG.
  - Quick check question: How would you convert the triplet ("The Blue Umbrella", "director", "?") into a SPARQL query?

- **Concept**: Document retrieval and reading comprehension
  - Why needed here: Unstructured knowledge is accessed by retrieving documents and answering single-hop questions over them.
  - Quick check question: If the triplet is ("Instincts", "performer", "?"), what single-hop question would you generate for retrieval?

## Architecture Onboarding

- **Component map**: Input (Multi-hop question) -> LLM (Parse into semi-structured chain) -> Filters (Syntactic error filtering + self-consistency) -> Knowledge sources (Structured KG SPARQL -> Unstructured document retrieval+QA -> Parametric memory) -> Output (Final answer)

- **Critical path**: Parse → Filter → Fill masks (Structured → Unstructured → Parametric) → Answer

- **Design tradeoffs**:
  - Structured KG usage gives precision but limited coverage; unstructured text gives coverage but lower precision; parametric memory is flexible but prone to hallucination.
  - Using more exemplars in few-shot prompting improves parsing quality but increases prompt size and cost.

- **Failure signatures**:
  - High syntactic error rate in parsed chains → Filter is too strict or exemplars are poor.
  - Low recall from structured KG → Entity linking model fails to ground entities correctly.
  - Answer quality drops when using only parametric memory → Over-reliance on parametric knowledge without external grounding.

- **First 3 experiments**:
  1. Run ablation: Structured KG + Model vs. Text + Model vs. KG + Text + Model on 2WikiMultiHopQA to confirm structured knowledge is most effective.
  2. Vary number of exemplars in few-shot prompting (5-shot, 15-shot, 25-shot) and measure parsing accuracy.
  3. Replace Wikidata KG with oracle KG (where ground-truth triples are known) to measure ceiling performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Semi-CoT scale with increasing model size across different knowledge source combinations?
- Basis in paper: Explicit - The paper states "Our approach benefits from increased model size, resulting in continuous performance improvements, regardless of the order in which sources of knowledge are utilized."
- Why unresolved: The paper only reports results for three specific model sizes (7B, 13B, 70B) and doesn't explore the full range of possible model sizes or provide a scaling analysis.
- What evidence would resolve it: A comprehensive study testing Semi-CoT across a wider range of model sizes (e.g., 1B to 175B) while varying the knowledge source combinations would provide insights into the scaling behavior.

### Open Question 2
- Question: What is the impact of using different entity linking and relation linking models on Semi-CoT's performance?
- Basis in paper: Explicit - The paper mentions using specific off-the-shelf models (BLINK for entity linking and all-MiniLM-L6-v2 for relation linking) but doesn't explore the effect of using alternative models.
- Why unresolved: The paper uses particular entity and relation linking models but doesn't investigate how the choice of these models affects overall performance.
- What evidence would resolve it: Experiments comparing Semi-CoT's performance using different entity linking and relation linking models (e.g., different BLINK variants, other sentence transformer models) would clarify the impact of these components.

### Open Question 3
- Question: How does Semi-CoT perform on other knowledge-intensive tasks beyond multi-hop question answering?
- Basis in paper: Inferred - The paper focuses on multi-hop QA datasets but discusses the potential of Semi-CoT for "knowledge-intensive tasks" more broadly.
- Why unresolved: The paper only evaluates Semi-CoT on two multi-hop QA datasets and doesn't explore its effectiveness on other knowledge-intensive tasks like fact checking, summarization, or open-domain dialogue.
- What evidence would resolve it: Applying Semi-CoT to a diverse set of knowledge-intensive tasks and comparing its performance to existing methods would demonstrate its versatility and generalizability.

## Limitations

- The exact prompt templates and exemplar quality for few-shot learning are not fully specified, making faithful reproduction challenging.
- The filtering rules for syntactic errors are described at a high level without specific thresholds, which could affect method robustness.
- No ablation studies are provided for the order of knowledge source usage, leaving the effectiveness of the staged filling mechanism partially unverified.

## Confidence

- **High Confidence**: The method's overall framework for integrating parametric, unstructured, and structured knowledge is well-supported by experimental results on 2WikiMultiHopQA and MuSiQue-Ans datasets.
- **Medium Confidence**: The staged filling of masks (structured → unstructured → parametric) is logically sound, but the lack of ablation studies on order effects introduces uncertainty.
- **Low Confidence**: The sufficiency of few-shot exemplars for parsing and mask-filling is assumed but not empirically validated, and the exact prompt templates are unspecified.

## Next Checks

1. **Ablation Study on Knowledge Source Order**: Run experiments to compare the performance of different sequences of knowledge source usage (e.g., unstructured → structured → parametric) to validate the claimed effectiveness of the current order.
2. **Exemplar Quality Analysis**: Vary the number and quality of exemplars in few-shot prompting (e.g., 5-shot, 15-shot, 25-shot) and measure the impact on parsing accuracy and overall performance.
3. **Scalability Test**: Apply the method to a different domain (e.g., science or medicine) or a knowledge graph with a different structure to assess its generalizability and robustness.