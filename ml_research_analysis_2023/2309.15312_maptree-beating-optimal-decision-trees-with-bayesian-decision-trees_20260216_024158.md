---
ver: rpa2
title: 'MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees'
arxiv_id: '2309.15312'
source_url: https://arxiv.org/abs/2309.15312
tags:
- maptree
- node
- tree
- leaf
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MAPTree, a Bayesian method for constructing
  optimal decision trees via maximum a posteriori inference. The authors formulate
  the search for the MAP tree as an AND/OR graph search problem and develop a best-first
  search algorithm guided by a consistent heuristic.
---

# MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees

## Quick Facts
- **arXiv ID:** 2309.15312
- **Source URL:** https://arxiv.org/abs/2309.15312
- **Reference count:** 40
- **Key outcome:** MAPTree achieves state-of-the-art accuracy on decision tree benchmarks while providing certificates of optimality, outperforming or matching baselines with smaller trees.

## Executive Summary
MAPTree introduces a Bayesian approach to constructing optimal decision trees via maximum a posteriori inference. The method formulates the search for the MAP tree as an AND/OR graph search problem and develops a best-first search algorithm guided by a consistent heuristic. Empirical evaluation shows that MAPTree either outperforms existing decision tree methods or achieves comparable accuracy with smaller trees across 16 real-world datasets. On synthetic data, MAPTree demonstrates better generalization and robustness to noise compared to baselines. The method also provides certificates of optimality, a key advantage over sampling-based approaches.

## Method Summary
MAPTree uses an AND/OR graph search algorithm to find the maximum a posteriori (MAP) decision tree from a Bayesian posterior over tree structures. The method constructs an AND/OR graph where each OR node represents a subproblem over a subset of samples at a given depth, and edges capture the cost of splitting or stopping. A best-first search guided by a consistent heuristic (Perfect Split Heuristic) explores the most promising subproblems first, pruning away large regions of the search space that cannot contain the MAP tree. The algorithm uses reversible sparse bitsets and subproblem caching to efficiently represent and identify equivalent subproblems. The output is the MAP tree and a certificate of optimality.

## Key Results
- MAPTree achieves state-of-the-art accuracy on 16 real-world binary classification datasets, either outperforming or matching baseline methods while producing smaller trees
- On synthetic data with controlled label noise, MAPTree demonstrates better generalization and robustness compared to CART, DL8.5, and GOSDT
- MAPTree provides certificates of optimality, a unique advantage over sampling-based approaches like SMC and MCMC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MAPTree uses a consistent AND/OR graph heuristic to guide the search toward the maximum a posteriori tree.
- Mechanism: The heuristic (Perfect Split Heuristic) is derived from the posterior probability of a perfectly classifying subtree, ensuring that the search avoids unnecessary depth and already poor splits.
- Core assumption: The heuristic provides a lower bound on the true value of each subproblem and is admissible for the AO* search.
- Evidence anchors:
  - [abstract] "we propose an AND/OR search algorithm, dubbed MAPTree, which is able to recover the maximum a posteriori tree."
  - [section] Theorem 8 proves the Perfect Split Heuristic is consistent, i.e., for any OR node o with children {t, a1, ..., aF}: h(o) ≤ minc∈{t,a1,...,aF} cost(o, c) + h(c).
  - [corpus] The cited work on Benders' decomposition also uses consistent heuristics for MAP inference in factor graphs, indicating this is a known valid strategy.
- Break condition: If the heuristic is not consistent or admissible, the AO* search may miss the optimal tree or fail to terminate.

### Mechanism 2
- Claim: The AND/OR graph GX,Y encodes the search for the MAP tree as a minimum-cost solution graph problem.
- Mechanism: Each OR node represents a subproblem over a subset of samples at a given depth, and edges capture the cost of splitting or stopping. The minimal cost solution corresponds to the maximum a posteriori tree.
- Core assumption: The bijection between solution graphs and decision trees is exact and preserves posterior probabilities.
- Evidence anchors:
  - [section] Theorem 5 states "There is natural bijection between solution graphs on GX,Y and binary decision trees."
  - [section] Theorem 6 shows "cost(S) = -log P(T,Y|X)" and the minimal cost solution corresponds to the MAP tree.
  - [corpus] The related work on Benders' decomposition uses a similar reduction from MAP inference to graph search, suggesting this encoding is valid.
- Break condition: If the bijection is not exact or the cost function does not reflect the posterior, the search will not find the MAP tree.

### Mechanism 3
- Claim: MAPTree is faster than sampling-based approaches (SMC, MCMC) because it deterministically searches the posterior space rather than relying on random sampling.
- Mechanism: MAPTree uses a best-first search guided by the heuristic to explore the most promising subproblems first, pruning away large regions of the search space that cannot contain the MAP tree.
- Core assumption: The heuristic is accurate enough to focus the search on high-probability subtrees and the search space is manageable in size for the datasets used.
- Evidence anchors:
  - [abstract] "MAPTree recovers the maximum a posteriori tree faster than existing sampling approaches."
  - [section] The Perfect Split Heuristic is designed to be admissible and consistent, which are properties that ensure efficient pruning in best-first search.
  - [corpus] The related work on MCMC for decision trees notes that mixing times are exponential in depth, supporting the claim that deterministic search can be faster.
- Break condition: If the search space is too large or the heuristic is not effective, MAPTree may be slower than sampling methods or fail to find the MAP tree in reasonable time.

## Foundational Learning

- Concept: Bayesian decision trees and the BCART posterior
  - Why needed here: MAPTree is designed to find the MAP tree from the BCART posterior over tree structures.
  - Quick check question: What is the role of the hyperparameters α and β in the BCART prior, and how do they affect the MAP tree?

- Concept: AND/OR graph search and the AO* algorithm
  - Why needed here: MAPTree uses an AND/OR graph search algorithm to explore the space of decision trees.
  - Quick check question: How does the consistency of the heuristic ensure that AO* will find the optimal solution?

- Concept: Reversible sparse bitsets and subproblem caching
  - Why needed here: MAPTree uses these data structures to efficiently represent and identify equivalent subproblems in the search.
  - Quick check question: How does the 128-bit hash function help identify equivalent subproblems, and what is the probability of a hash collision?

## Architecture Onboarding

- Component map: Input binary dataset -> AND/OR graph construction -> Best-first search with Perfect Split Heuristic -> MAP tree and certificate of optimality

- Critical path:
  1. Construct AND/OR graph GX,Y from input dataset
  2. Initialize MAPTree with root node and heuristic
  3. Expand nodes using findNodeToExpand until LB[root] = UB[root] or time limit
  4. Extract solution using getSolution

- Design tradeoffs:
  - Memory vs. speed: Caching subproblems takes O(1) memory per subproblem but may have hash collisions; explicit caching takes O(N) memory but is exact.
  - Heuristic quality vs. computation: A more accurate heuristic may take longer to compute but guide the search more effectively.

- Failure signatures:
  - Out of memory: Search space is too large for available memory; consider using explicit caching or a less accurate heuristic.
  - Timeout: Search is not converging; consider increasing the time limit or using a more aggressive heuristic.
  - Suboptimal solution: Heuristic is not effective; consider tuning hyperparameters or using a different heuristic.

- First 3 experiments:
  1. Run MAPTree on a small synthetic dataset with known MAP tree to verify correctness.
  2. Compare MAPTree's runtime and solution quality against SMC and MCMC on a benchmark dataset.
  3. Vary the hyperparameters α and β and observe their effect on the size and quality of the MAP tree.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MAPTree's performance scale with the number of features and samples in the dataset, and are there specific dataset characteristics that make it particularly effective or ineffective?
- Basis in paper: [inferred] The paper mentions that the AND/OR graph constructed by MAPTree contains F × 2^N OR nodes, F × 2^N terminal nodes, and F^2 × 2^N AND nodes, indicating potential scalability issues with large datasets. The authors also note that a limitation is the construction of a potentially large AND/OR graph that consumes significant memory.
- Why unresolved: The experiments were conducted on datasets from the CP4IM repository, which are relatively small (with thousands of datapoints and tens of features). The authors mention leaving optimizations for huge datasets to future work.
- What evidence would resolve it: Experiments on larger datasets with more features and samples, analysis of the algorithm's time and space complexity, and proposed optimizations for handling large datasets.

### Open Question 2
- Question: Can the Perfect Split Heuristic be further improved to guide the search more effectively and reduce the number of explored nodes in the AND/OR graph?
- Basis in paper: [explicit] The paper introduces the Perfect Split Heuristic as a key component of MAPTree, which guides the search away from subproblems that are too deep or for which the labels have already been poorly divided. The authors prove that this heuristic is a lower bound (admissible) and consistent.
- Why unresolved: While the heuristic is proven to be admissible and consistent, the paper does not explore potential improvements or alternatives to the heuristic. The effectiveness of the heuristic is demonstrated empirically but not theoretically analyzed in depth.
- What evidence would resolve it: Development and analysis of alternative heuristics, theoretical analysis of the heuristic's performance, and empirical comparison of different heuristics on various datasets.

### Open Question 3
- Question: How does MAPTree compare to ensemble methods like Random Forest and XGBoost in terms of accuracy, interpretability, and computational efficiency on real-world datasets?
- Basis in paper: [explicit] The paper mentions that decision trees form the backbone of ensemble methods such as Random Forest and XGBoost, which have been leading models in many machine learning competitions and often outperform neural networks on tabular data. The authors also emphasize the interpretability of decision trees as a key advantage.
- Why unresolved: The experiments in the paper focus on comparing MAPTree to other decision tree algorithms and do not include comparisons with ensemble methods. The paper does not discuss the potential of using MAPTree as a base learner for ensemble methods.
- What evidence would resolve it: Experiments comparing MAPTree to Random Forest and XGBoost on various real-world datasets, analysis of the interpretability of MAPTree trees compared to ensemble methods, and evaluation of the computational efficiency of MAPTree in an ensemble setting.

## Limitations
- The AND/OR graph construction and the bijection between solution graphs and decision trees may face practical challenges with large, complex datasets.
- The empirical evaluation is limited to binary classification tasks and may not generalize to multi-class or regression problems.
- The reproducibility of the results depends on the availability of the code and the exact implementation details, which are not fully specified in the paper.

## Confidence
- **High Confidence:** The theoretical foundations of MAPTree, including the consistency of the Perfect Split Heuristic and the bijection between solution graphs and decision trees, are well-established and supported by proofs.
- **Medium Confidence:** The empirical evaluation demonstrates strong performance of MAPTree on benchmark datasets, but the results may not be representative of all possible datasets and tasks.
- **Low Confidence:** The reproducibility of the results depends on the availability of the code and the exact implementation details, which are not fully specified in the paper.

## Next Checks
1. Verify the consistency of the Perfect Split Heuristic by testing its performance on a range of synthetic datasets with known MAP trees.
2. Investigate the scalability of MAPTree to larger, more complex datasets by evaluating its performance on benchmark datasets with hundreds of features and samples.
3. Assess the robustness of MAPTree to different prior specifications by varying the hyperparameters α and β and observing their effect on the size and quality of the MAP tree.