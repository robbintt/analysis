---
ver: rpa2
title: Multi Time Scale World Models
arxiv_id: '2310.18534'
source_url: https://arxiv.org/abs/2310.18534
tags:
- time
- latent
- observation
- mts3
- predictions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MTS3, a hierarchical latent variable model
  that can learn multi-time scale dynamics of partially observable systems. MTS3 combines
  multiple state-space models (SSMs) at different timescales to capture both slow-changing
  long-term trends and fast-changing short-term dynamics.
---

# Multi Time Scale World Models

## Quick Facts
- arXiv ID: 2310.18534
- Source URL: https://arxiv.org/abs/2310.18534
- Reference count: 40
- Key outcome: Hierarchical latent variable model MTS3 achieves superior long-horizon action-conditional predictions on robotics datasets compared to single-time scale models and transformers.

## Executive Summary
This paper introduces MTS3, a hierarchical latent variable model that learns multi-time scale dynamics in partially observable systems by combining state-space models (SSMs) at different timescales. The method captures both slow-changing long-term trends and fast-changing short-term dynamics through a two-level hierarchy, using closed-form Gaussian inference updates and imputation-based training for effective long-horizon predictions. Experiments on complex robotics datasets demonstrate that MTS3 outperforms single-time scale models like RKN and HiP-RSSM, as well as transformer variants, particularly on challenging tasks involving multi-object manipulation and navigation.

## Method Summary
MTS3 is a hierarchical latent variable model that uses two coupled state-space models operating at different timescales: a fast timescale SSM running at the original system time step and a slow timescale SSM updating every H steps. The slow timescale captures high-level task descriptors that condition the fast timescale dynamics, enabling the model to learn both long-term trends and precise short-term predictions. The method employs closed-form Gaussian inference updates across timescales using Kalman-like prediction and observation updates, along with imputation-based training where random future observations are masked during training to create a self-supervised learning signal for long-horizon prediction.

## Key Results
- MTS3 achieves lower RMSE and negative log-likelihood scores than single-time scale models (RKN, HiP-RSSM) and transformer variants on robotics datasets
- The method demonstrates superior performance particularly on complex tasks like multi-object manipulation and navigation
- Ablation studies confirm the importance of multi-time scale modeling, action abstractions, and imputation-based training for accurate long-term predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-time scale hierarchical modeling captures both slow-changing long-term trends and fast-changing short-term dynamics.
- Mechanism: The model uses two coupled state-space models at different timescales. The slow timescale SSM provides task descriptors that parametrize the fast timescale SSM dynamics, allowing learning of high-level abstractions for long-term trends while maintaining precise short-term predictions.
- Core assumption: Slow dynamics can be captured by coarser temporal abstraction and used to condition faster dynamics model.
- Evidence anchors: [abstract]: "MTS3 combines multiple state-space models (SSMs) at different timescales to capture both slow-changing long-term trends and fast-changing short-term dynamics."

### Mechanism 2
- Claim: Closed-form Gaussian inference enables computationally efficient and differentiable learning across multiple timescales.
- Mechanism: The paper derives closed-form inference updates for both timescales using Kalman-like prediction and observation updates that are fully differentiable, allowing end-to-end learning through backpropagation.
- Core assumption: Linear Gaussian assumptions for state-space models hold sufficiently well for effective inference.
- Evidence anchors: [abstract]: "The method uses closed-form inference updates across time scales and incorporates uncertainty propagation."

### Mechanism 3
- Claim: Imputation-based training enables effective learning for long-horizon predictions.
- Mechanism: Instead of one-step-ahead training, the model randomly masks observations and tasks the network to impute missing observations at both timescales, creating a strong self-supervised learning signal for long-term prediction with varying horizons.
- Core assumption: Ability to impute masked observations translates to improved long-horizon prediction accuracy.
- Evidence anchors: [section]: "To increase the long-term prediction performance, we can treat the long-term prediction problem as a case of the 'missing value' problem..."

## Foundational Learning

- Concept: State-Space Models (SSMs) and Kalman Filtering
  - Why needed here: MTS3 is built upon SSMs and uses Kalman-like inference updates for both timescales.
  - Quick check question: Can you derive the Kalman prediction and update equations for a linear Gaussian state-space model?

- Concept: Variational Inference and Approximate Bayesian Inference
  - Why needed here: Understanding approximate inference in probabilistic models when exact inference is intractable.
  - Quick check question: What is the evidence lower bound (ELBO) in variational inference, and how is it used to train latent variable models?

- Concept: Transformer Architectures and Attention Mechanisms
  - Why needed here: The paper compares MTS3 against transformer-based baselines.
  - Quick check question: How does the multi-head attention mechanism in transformers allow for capturing long-range dependencies in sequential data?

## Architecture Onboarding

- Component map: Raw observations/actions → Observation/Action Encoders → Slow Time-Scale SSM → Task Descriptors → Fast Time-Scale SSM → Output Decoder → Loss Computation

- Critical path: Raw observations and actions → observation/action encoders → slow timescale inference (task prediction and update) → task descriptors → fast timescale inference (state prediction and observation update) → output decoder → loss computation → backpropagation

- Design tradeoffs:
  - Linear vs. non-linear dynamics models: Linear models allow closed-form inference but may not capture complex dynamics as well as non-linear models
  - Number of timescales: More timescales could capture more complex hierarchical structures but increase model complexity and computational cost
  - Discretization step H: Larger H captures slower trends but may miss important intermediate dynamics

- Failure signatures:
  - Poor long-horizon predictions: Could indicate issues with coupling between timescales or imputation-based training
  - Unstable training: Could be due to improper initialization of transition matrices or issues with linear Gaussian assumptions
  - High uncertainty estimates: Could indicate model is not confident in predictions due to insufficient data or model capacity

- First 3 experiments:
  1. Implement and train MTS3 on a simple 1D linear dynamical system with known ground truth dynamics to verify recovery of true dynamics and accurate long-horizon predictions
  2. Train MTS3 on HalfCheetah from D4RL and compare against RKN baseline, analyzing learned task descriptors and their effect on fast timescale predictions
  3. Perform ablation study on discretization step H by training MTS3 with different values and evaluating performance on long-horizon predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would MTS3 perform on high-dimensional sensor inputs like images rather than proprioceptive data?
- Basis in paper: [explicit] The paper mentions MTS3 relies on "reconstruction loss" and has only been validated on proprioceptive sensors, with image-based experiments left for future work.
- Why unresolved: Current experiments are limited to low-dimensional proprioceptive data, and paper acknowledges this limitation without providing experimental results for image inputs.
- What evidence would resolve it: Experiments comparing MTS3 performance on image-based tasks versus proprioceptive data, including quantitative metrics and qualitative analysis.

### Open Question 2
- Question: What is the optimal number of temporal abstraction hierarchies for different types of tasks?
- Basis in paper: [inferred] The paper discusses that MTS3 was restricted to two levels of temporal abstractions and mentions more hierarchies could be beneficial for certain tasks like Maze2D.
- Why unresolved: Experiments only tested MTS3 with two hierarchies, and paper suggests this might not be optimal for all tasks without providing systematic analysis of different hierarchy depths.
- What evidence would resolve it: Comparative studies testing MTS3 with varying numbers of hierarchies (3, 4, 5+) across different task types, measuring performance and computational efficiency.

### Open Question 3
- Question: How does MTS3 handle hierarchical control and planning as an inference problem?
- Basis in paper: [explicit] The paper explicitly states that hierarchical control and planning using MTS3 is left for future work.
- Why unresolved: While MTS3 demonstrates strong predictive capabilities, paper acknowledges extending it to hierarchical control and planning remains an open research direction.
- What evidence would resolve it: Implementation and evaluation of hierarchical control algorithms built on MTS3, demonstrating improved planning efficiency and performance compared to non-hierarchical approaches.

## Limitations

- The evaluation focuses primarily on RMSE and NLL metrics without extensive ablation on the hierarchical structure itself
- The assumption that linear Gaussian dynamics are sufficient for complex robotics tasks represents a significant modeling constraint that may limit applicability to highly non-linear systems
- The specific masking strategy for imputation-based training and its effect on different prediction horizons is not thoroughly analyzed

## Confidence

- **High Confidence**: The multi-time scale architecture design and closed-form inference updates are well-specified and mathematically grounded. The comparison against single-time scale baselines provides clear evidence for benefits of hierarchical modeling.
- **Medium Confidence**: The claim that closed-form Gaussian inference is computationally efficient relies on factorization approximation, but computational benefits relative to other approaches are not explicitly quantified.
- **Medium Confidence**: The imputation-based training methodology is intuitively sound, but its superiority over alternative self-supervised approaches for long-horizon prediction is not rigorously established.

## Next Checks

1. **Hierarchical Structure Ablation**: Systematically vary the number of timescales (1, 2, and 3) and measure the impact on long-horizon prediction accuracy to isolate the contribution of hierarchical structure beyond simply having more parameters.

2. **Dynamics Model Validation**: Test MTS3 on datasets with known non-linear dynamics to quantify performance degradation when linear Gaussian assumptions break down, and compare against non-linear alternatives like neural network-based state-space models.

3. **Computational Efficiency Analysis**: Benchmark the actual training and inference time of MTS3 against transformer baselines and single-time scale models on identical hardware, including memory usage and wall-clock time for long sequence processing.