---
ver: rpa2
title: Quantifying Uncertainty in Answers from any Language Model and Enhancing their
  Trustworthiness
arxiv_id: '2308.16175'
source_url: https://arxiv.org/abs/2308.16175
tags:
- answer
- confidence
- uncertainty
- arxiv
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BSDetector, a method to estimate numeric
  confidence scores for responses generated by any large language model (LLM), including
  those accessed only via black-box APIs. The method combines intrinsic and extrinsic
  confidence assessments by sampling multiple responses from the LLM with varied prompts
  and asking the LLM to reflect on its own answers.
---

# Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness

## Quick Facts
- arXiv ID: 2308.16175
- Source URL: https://arxiv.org/abs/2308.16175
- Reference count: 6
- Key outcome: BSDetector achieves higher AUROC for identifying incorrect LLM responses and improves LLM accuracy without retraining

## Executive Summary
This paper introduces BSDetector, a method to estimate numeric confidence scores for responses generated by any large language model (LLM), including those accessed only via black-box APIs. The method combines intrinsic and extrinsic confidence assessments by sampling multiple responses from the LLM with varied prompts and asking the LLM to reflect on its own answers. Experiments on math word problems, commonsense reasoning, and trivia question-answering benchmarks show that BSDetector significantly outperforms alternative uncertainty estimation methods.

## Method Summary
BSDetector estimates confidence in LLM responses through a two-component approach. First, it generates k diverse responses using varied prompts (including Chain-of-Thought modifications) and computes an observed consistency score based on semantic contradictions detected via an NLI model. Second, it obtains self-reflection certainty by prompting the LLM to assess its own answer correctness via multiple-choice follow-up questions. These two confidence scores are then combined linearly to produce a final trustworthiness estimate. The method requires no additional training and works with any black-box LLM API.

## Key Results
- BSDetector achieves higher area under the ROC curve (AUROC) than alternative methods for identifying incorrect LLM responses
- The method improves LLM accuracy by selecting the highest-confidence response from sampled outputs
- Experiments show effectiveness across multiple benchmarks including GSM8K, CSQA, SVAMP, and TriviaQA
- The approach works with GPT-3, GPT-3.5 Turbo, and other black-box LLM APIs without requiring training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sampling multiple responses with varied prompts exposes contradictions that signal LLM uncertainty.
- Mechanism: The LLM is queried multiple times with slightly altered prompts (e.g., adding Chain-of-Thought instructions). Responses are compared using semantic similarity metrics that detect contradictions rather than mere textual differences. Low similarity between responses indicates the model is uncertain.
- Core assumption: A well-trained LLM will produce diverse, sometimes contradictory responses when uncertain about an answer, especially when prompts are varied.
- Evidence anchors:
  - [abstract] "By sampling multiple responses from the LLM with varied prompts and asking the LLM to reflect on its own answers."
  - [section 3.1] "Our first action runs the LLM multiple times to produce multiple varied responses... we consider semantic similarities. Not just overall similarities... but rather measuring whether the semantics of the two outputs contradict one another or not."
- Break condition: If the LLM always produces nearly identical outputs regardless of prompt variation, the contradiction-based metric will fail to detect uncertainty.

### Mechanism 2
- Claim: Intrinsic self-reflection allows the LLM to assess its own confidence in a calibrated way.
- Mechanism: After generating an answer, the LLM is prompted with follow-up questions asking it to rate the correctness of its own answer on a multiple-choice scale (e.g., Correct/Incorrect/Not Sure). These ratings are converted into a numerical confidence score.
- Core assumption: Modern LLMs have sufficient meta-reasoning ability to introspect about the correctness of their own outputs.
- Evidence anchors:
  - [abstract] "combining intrinsic and extrinsic confidence assessments by sampling multiple responses from the LLM with varied prompts and asking the LLM to reflect on its own answers."
  - [section 3.2] "Our Self-reflection certainty is an confidence estimate output by LLM itself when asked follow-up questions encouraging it to directly estimate the correctness of its original answer."
- Break condition: If the LLM systematically overestimates its confidence (a known issue with LLMs), the self-reflection score will be unreliable without external calibration.

### Mechanism 3
- Claim: Aggregating intrinsic and extrinsic confidence assessments yields a more robust overall confidence score.
- Mechanism: The observed consistency (extrinsic) and self-reflection certainty (intrinsic) scores are combined linearly with tunable weights to produce a final confidence estimate.
- Core assumption: The two confidence measures capture complementary aspects of uncertainty, so their combination is more informative than either alone.
- Evidence anchors:
  - [abstract] "Our uncertainty quantification technique works for any LLM accessible only via a black-box API, whose training data remains unknown... combines intrinsic and extrinsic assessments of confidence into a single trustworthiness estimate."
  - [section 3.3] "Considering the distinct characteristics of the Observed Consistency and Self-reflection Certainty, we anticipate they might complement each other. BSD ETECTOR thus aggregates the Observed Consistency and Self-reflection Certainty values into an overall confidence score."
- Break condition: If one source of confidence is consistently much noisier or biased than the other, simple linear combination may not optimally fuse them.

## Foundational Learning

- Concept: Semantic similarity and contradiction detection
  - Why needed here: The method relies on detecting when two LLM responses contradict each other, which requires understanding semantic equivalence/contradiction beyond surface form.
  - Quick check question: What is the difference between using a general semantic similarity metric (like embedding cosine similarity) and a contradiction-aware metric (like NLI) in this context?

- Concept: Calibration and overconfidence in LLMs
  - Why needed here: The method depends on the LLM being able to produce calibrated confidence estimates during self-reflection; understanding LLM calibration issues is key to interpreting results.
  - Quick check question: Why might an LLM that always outputs "I'm 95% sure" be problematic for this method?

- Concept: Prompt engineering and few-shot learning
  - Why needed here: The method uses carefully crafted prompts (e.g., with CoT) to elicit diverse and reflective responses from the LLM.
  - Quick check question: How does adding a Chain-of-Thought instruction to the prompt influence the diversity of LLM outputs?

## Architecture Onboarding

- Component map: Prompt → k diverse responses → NLI contradiction scoring → Self-reflection scoring → Weighted aggregation → Confidence output
- Critical path: Generate multiple LLM responses → Compute semantic contradictions → LLM self-assessment → Linear combination → Final confidence score
- Design tradeoffs: More responses (higher k) improve confidence estimate reliability but increase cost; heavier reliance on NLI vs indicator function trades generalization for robustness on closed-form tasks
- Failure signatures: Consistently high confidence scores regardless of answer correctness; low variance in observed consistency scores (indicating lack of diversity in sampled responses); self-reflection scores that do not correlate with ground truth
- First 3 experiments:
  1. Run the method on a small set of math word problems with k=2 and k=5 to observe how confidence scores vary with diversity
  2. Replace the NLI-based contradiction metric with simple embedding cosine similarity and compare calibration (AUROC) on a held-out set
  3. Remove the self-reflection component and use only observed consistency to see the degradation in uncertainty estimation performance

## Open Questions the Paper Calls Out

The paper acknowledges several open questions:

- What is the minimum number of sampled responses required from the LLM to achieve reliable uncertainty estimates without excessive computational cost? The paper states "Higher values of k lead to better uncertainty estimates, but require more computation (we found k = 5 works well enough in practice)" but doesn't explore a wider range of values.

- How well does BSDetector generalize to non-Q&A tasks like summarization, translation, or code generation? The paper primarily focuses on Question-Answering applications but acknowledges potential for broader application without validation on other generative tasks.

- Does BSDetector maintain its effectiveness when applied to smaller, less capable language models? While the method is claimed to be general, all experiments use GPT-3.5 and GPT-4 variants, leaving open whether effectiveness scales with model capability.

- Can BSDetector be adapted for streaming or real-time applications where computational overhead must be minimized? The current implementation requires multiple LLM calls plus NLI computation, making it computationally intensive for real-time use cases.

## Limitations

- The method assumes LLMs will produce diverse outputs when prompts are varied, which may not hold for highly deterministic models on factual recall tasks
- Self-reflection relies on LLM's meta-reasoning capabilities, which may be systematically overconfident without external calibration
- The effectiveness on black-box APIs is assumed rather than rigorously tested across different LLM architectures and rate-limited scenarios

## Confidence

- **High confidence**: The observed consistency mechanism through semantic contradiction detection is well-grounded in established NLP techniques (NLI models). The experimental design and evaluation metrics (AUROC, accuracy improvement) are standard and appropriately applied.
- **Medium confidence**: The aggregation of intrinsic and extrinsic confidence scores assumes linear combination is optimal. The paper doesn't explore whether different weighting schemes or nonlinear fusion methods might perform better.
- **Low confidence**: The method's effectiveness on black-box APIs is assumed rather than rigorously tested. While the paper claims the method works "without requiring additional training," it's unclear how robust the approach is across different LLM architectures, domains, or when API access is rate-limited.

## Next Checks

1. **Determinism test**: Evaluate BSDetector's performance on a benchmark where LLMs are known to produce highly deterministic outputs (e.g., simple factual questions) with temperature=0.0 to verify that the contradiction detection mechanism degrades as expected and to quantify the failure mode.

2. **Calibration analysis**: Conduct a post-hoc calibration study by binning confidence scores and computing expected calibration error (ECE) across different confidence ranges, particularly for the self-reflection component, to determine whether the LLM's self-assessed confidences are meaningfully calibrated.

3. **Cross-model generalization**: Apply BSDetector to a different LLM family (e.g., Claude, LLaMA) without any parameter tuning to assess whether the method's performance generalizes beyond the GPT models used in the original experiments, particularly focusing on whether the NLI-based contradiction detection remains effective.