---
ver: rpa2
title: Evaluating Large Language Models on Controlled Generation Tasks
arxiv_id: '2310.14542'
source_url: https://arxiv.org/abs/2310.14542
tags:
- generation
- llms
- language
- chatgpt
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates the controllability of large
  language models (LLMs) on five tasks across ten benchmarks, including a new Numerical
  Planning Benchmark (NPB) that tests fine-grained numerical constraints. After comparing
  LLMs against fine-tuned smaller models, the authors find that LLMs struggle with
  fine-grained hard constraints like numerical planning, despite performing well on
  tasks with coarse control signals such as sentiment, topic, and keyword incorporation.
---

# Evaluating Large Language Models on Controlled Generation Tasks

## Quick Facts
- arXiv ID: 2310.14542
- Source URL: https://arxiv.org/abs/2310.14542
- Reference count: 21
- Large language models struggle with fine-grained numerical constraints while performing well on coarse-grained semantic controls

## Executive Summary
This paper systematically evaluates the controllability of large language models (LLMs) across ten benchmarks, including a novel Numerical Planning Benchmark (NPB) that tests fine-grained numerical constraints. The authors compare LLMs against fine-tuned smaller models and find that while LLMs excel at tasks requiring coarse control signals like sentiment and topic, they struggle significantly with fine-grained hard constraints such as word count planning. Specifically, LLMs achieve low success rates (30-40%) on numerical planning tasks, whereas fine-tuned GPT-2 models achieve much higher success rates (60-90%). The study concludes that LLMs are better suited for tasks requiring coarse control rather than fine-grained hard constraints.

## Method Summary
The paper evaluates ten benchmarks including a new Numerical Planning Benchmark (NPB) and existing datasets for controlled content generation, story generation, rationale generation, and paraphrase generation. LLMs are compared against fine-tuned GPT-2 models using both zero-shot and few-shot in-context learning settings. The evaluation employs automatic metrics including success rates, mean squared error, BLEU, METEOR, ROUGE scores, and tree edit distances. The study tests multiple LLMs (ChatGPT, Alpaca, Vicuna, Falcon) across five task types: sentiment control, topic control, keyword incorporation, numerical planning, and story continuation.

## Key Results
- LLMs achieve 30-40% success rates on word and sentence count planning tasks
- Fine-tuned GPT-2 models achieve 60-90% success rates on the same numerical planning tasks
- LLMs perform well on coarse-grained controls (sentiment, topic, keywords) but struggle with fine-grained hard constraints
- Few-shot in-context learning deteriorates numerical planning performance compared to zero-shot approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can follow content constraints expressed in natural language when provided with in-context learning examples
- Mechanism: In-context learning allows LLMs to infer task-specific patterns from demonstration examples without gradient updates
- Core assumption: The model's internal representations can generalize from a small number of examples to new instances of the same task
- Evidence anchors: [abstract], [section 3], corpus
- Break condition: The model fails to understand the task definition or the examples are not representative of the task space

### Mechanism 2
- Claim: LLMs struggle with fine-grained hard constraints like numerical planning due to their autoregressive generation process
- Mechanism: Autoregressive generation lacks the ability to "look back" and adjust previous tokens to meet hard constraints
- Core assumption: The autoregressive nature of LLMs makes it difficult to plan ahead and satisfy numerical constraints
- Evidence anchors: [abstract], [section 2], corpus
- Break condition: The model can satisfy the constraint through chance or by using non-autoregressive methods

### Mechanism 3
- Claim: LLMs perform better on tasks with coarse control signals like sentiment, topic, and keyword incorporation
- Mechanism: LLMs can leverage their pre-trained knowledge to generate text that aligns with broad semantic categories
- Core assumption: The model's pre-training has captured semantic relationships that can be leveraged for coarse-grained control
- Evidence anchors: [abstract], [section 3], corpus
- Break condition: The model cannot generate text that aligns with the desired coarse-grained control signal

## Foundational Learning

- **Tokenization and subword units**
  - Why needed here: Understanding how LLMs process text at the subword level is crucial for tasks like numerical planning
  - Quick check question: What is the role of special tokens in BPE tokenization for indicating the start of a new word?

- **Autoregressive generation**
  - Why needed here: Autoregressive generation is the core mechanism behind LLMs and affects their ability to satisfy hard constraints
  - Quick check question: How does autoregressive generation differ from non-autoregressive methods in terms of constraint satisfaction?

- **In-context learning**
  - Why needed here: In-context learning is a key technique for adapting LLMs to new tasks without gradient updates
  - Quick check question: What are the limitations of in-context learning for numerical planning tasks?

## Architecture Onboarding

- **Component map**: LLM (e.g., ChatGPT, Alpaca-7B) -> Prompt template (zero-shot or few-shot) -> Evaluation metrics (success rate, MSE, BLEU, etc.) -> Task-specific datasets

- **Critical path**:
  1. Define the task and control constraints
  2. Prepare the prompt template (zero-shot or few-shot)
  3. Generate outputs using the LLM
  4. Evaluate the outputs using task-specific metrics
  5. Compare the performance with fine-tuned smaller models

- **Design tradeoffs**:
  - Zero-shot vs. few-shot prompting: Zero-shot is more general but may have lower performance, while few-shot can improve performance but requires task-specific examples
  - Sampling temperature: Higher temperature increases diversity but may decrease constraint satisfaction
  - Decoding strategy: Greedy decoding is deterministic but may lack diversity, while sampling can generate more diverse outputs but may not satisfy constraints

- **Failure signatures**:
  - Low success rate on numerical planning tasks
  - Inability to incorporate fine-grained control signals
  - Repetition or incoherence in generated text

- **First 3 experiments**:
  1. Evaluate ChatGPT on the word count planning task using zero-shot and few-shot prompting
  2. Compare the performance of ChatGPT and Alpaca-7B on the sentiment constrained text generation task
  3. Investigate the effect of sampling temperature on the success rate of numerical planning tasks

## Open Questions the Paper Calls Out

- **Open Question 1**: Why do LLMs show a sudden drop in numerical planning performance when the input number N increases from 5 to 6?
  - Basis in paper: explicit
  - Why unresolved: The paper identifies this as an observed phenomenon but does not provide a definitive explanation for why this specific threshold causes such a sharp decline in performance
  - What evidence would resolve it: Controlled experiments varying N around this threshold (e.g., 5, 5.5, 6, 6.5) while keeping other factors constant could reveal if this is a true discontinuity or gradual transition

- **Open Question 2**: What architectural modifications could enable LLMs to better handle fine-grained numerical constraints?
  - Basis in paper: explicit
  - Why unresolved: The paper suggests potential directions like chain-of-thought reasoning and non-autoregressive generation but does not implement or test these solutions
  - What evidence would resolve it: Empirical comparisons of modified LLM architectures against baseline models on numerical planning tasks would demonstrate whether these approaches improve performance

- **Open Question 3**: Why does few-shot in-context learning deteriorate performance on numerical planning tasks?
  - Basis in paper: explicit
  - Why unresolved: The paper observes this counterintuitive phenomenon but does not investigate the underlying mechanisms that cause few-shot examples to negatively impact numerical constraint adherence
  - What evidence would resolve it: Ablation studies varying the number, format, and content of in-context examples could identify which aspects of few-shot learning interfere with numerical planning

- **Open Question 4**: What is the relationship between tokenization schemes and numerical planning ability in LLMs?
  - Basis in paper: explicit
  - Why unresolved: While the paper discusses tokenization's potential impact, it does not systematically test different tokenization approaches to determine their effect on numerical planning performance
  - What evidence would resolve it: Experiments comparing LLM performance on numerical planning tasks using different tokenization schemes (BPE, WordPiece, SentencePiece) would reveal whether tokenization is a limiting factor

- **Open Question 5**: Why do LLMs consistently generate shorter continuations than required for numerical planning tasks?
  - Basis in paper: explicit
  - Why unresolved: The paper notes this as a general behavior but does not investigate whether this stems from training data biases, architectural limitations, or other factors
  - What evidence would resolve it: Analysis of training data distributions and attention patterns during generation could reveal whether LLMs are systematically biased toward shorter outputs or if this is task-specific

- **Open Question 6**: How does instruction tuning affect LLMs' ability to follow numerical constraints versus content constraints?
  - Basis in paper: explicit
  - Why unresolved: The paper observes different effects of instruction tuning on numerical versus content constraints but does not investigate the underlying reasons for this differential impact
  - What evidence would resolve it: Comparative studies of instruction-tuned versus non-instruction-tuned models on both constraint types with controlled prompts would reveal whether instruction tuning specifically benefits content constraints

- **Open Question 7**: What decoding strategies beyond sampling with temperature T=0.3 could improve numerical planning performance?
  - Basis in paper: explicit
  - Why unresolved: The paper tests several decoding strategies but does not exhaustively explore the parameter space or novel decoding approaches specifically designed for constraint satisfaction
  - What evidence would resolve it: Systematic exploration of decoding parameters and novel constraint-aware decoding algorithms would identify whether alternative strategies can improve numerical constraint adherence

- **Open Question 8**: Why does N=3 perform worse than expected in numerical planning tasks?
  - Basis in paper: explicit
  - Why unresolved: The paper identifies N=3 as an exception to the general trend but does not investigate whether this is due to specific linguistic properties or architectural biases
  - What evidence would resolve it: Linguistic analysis of English sentence structures around length 3 combined with model behavior analysis could reveal whether this is a language-specific phenomenon or a more general pattern

- **Open Question 9**: What is the relationship between model scale and numerical planning ability in LLMs?
  - Basis in paper: explicit
  - Why unresolved: The paper uses various model sizes but does not systematically investigate how scaling affects numerical constraint satisfaction
  - What evidence would resolve it: Experiments with models across a wide range of parameter counts (10M to 100B+) on numerical planning tasks would reveal whether larger models show improved performance on fine-grained constraints

- **Open Question 10**: How do different evaluation metrics capture different aspects of numerical planning performance?
  - Basis in paper: explicit
  - Why unresolved: The paper uses success rate and MSE but does not investigate whether these metrics capture all relevant aspects of numerical planning or if additional metrics would provide better insights
  - What evidence would resolve it: Correlation analysis between different evaluation metrics and human judgments on numerical planning tasks would reveal which metrics best capture true performance

## Limitations

- Benchmark Representativeness: The evaluation focuses on five specific control dimensions that may not capture the full spectrum of real-world use cases
- Evaluation Methodology Constraints: Reliance on automatic metrics may not fully capture the quality of generated text, with binary success rates potentially missing partial success
- Model Selection Bias: The comparison is primarily against fine-tuned GPT-2 models without including other contemporary fine-tuned models or newer LLM architectures

## Confidence

- **High Confidence**: The core finding that LLMs struggle with fine-grained numerical constraints is well-supported by experimental results across multiple models and datasets
- **Medium Confidence**: The broader claim about LLMs being better suited for coarse control is supported but may not be universally applicable
- **Low Confidence**: The absolute claim that LLMs "cannot" satisfy fine-grained constraints overstates the limitations, as some success is observed at lower constraint values

## Next Checks

1. **Expanded Constraint Range**: Replicate the numerical planning experiments with additional constraint types (e.g., character counts, syllable counts) and higher constraint values to map the precise boundary where LLMs fail

2. **Human Evaluation Validation**: Conduct systematic human evaluation studies to validate the automatic metrics, particularly for assessing whether LLM outputs with "failed" numerical constraints still maintain semantic quality and coherence

3. **Alternative Decoding Strategies**: Test whether alternative decoding methods (beam search, constrained decoding, iterative refinement) can improve LLM performance on fine-grained constraints, which would challenge the claimed limitations of autoregressive generation