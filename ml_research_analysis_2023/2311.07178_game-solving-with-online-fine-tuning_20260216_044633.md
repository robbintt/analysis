---
ver: rpa2
title: Game Solving with Online Fine-Tuning
arxiv_id: '2311.07178'
source_url: https://arxiv.org/abs/2311.07178
tags:
- online
- solving
- fine-tuning
- solver
- manager
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces online fine-tuning for game solving to improve
  AlphaZero-based heuristics on out-of-distribution positions encountered during proof
  search. It proposes an Online Fine-Tuning Trainer (OFT) that dynamically refines
  the Proof Cost Network (PCN) heuristic by training on solved and critical positions
  found during search.
---

# Game Solving with Online Fine-Tuning

## Quick Facts
- arXiv ID: 2311.07178
- Source URL: https://arxiv.org/abs/2311.07178
- Reference count: 40
- Primary result: Online fine-tuning reduces node visits by 78.31% and computation time by 76.46% on average for 7x7 Killall-Go solving

## Executive Summary
This paper introduces online fine-tuning for game solving to improve AlphaZero-based heuristics on out-of-distribution positions encountered during proof search. The proposed Online Fine-Tuning Trainer (OFT) dynamically refines the Proof Cost Network (PCN) heuristic by training on solved and critical positions found during search. Experiments on 16 challenging 7x7 Killall-Go openings show significant efficiency gains, particularly on larger problems, demonstrating the method's potential for scalable game solving.

## Method Summary
The paper proposes online fine-tuning for game solving by dynamically updating the Proof Cost Network (PCN) heuristic during search. The OFT maintains queues of solved and critical positions encountered during MCTS traversal, using these to generate self-play training data that updates the PCN weights. The fine-tuned model is immediately distributed to both manager and workers, improving accuracy for positions currently being explored. The approach uses a distributed architecture with job assignment strategies (virtual solving, top-k selection, AND-player job assignment) to reduce redundant computation and improve parallel efficiency.

## Key Results
- Online fine-tuning reduces node visits by 78.31% and computation time by 76.46% on average
- The method particularly excels on larger problems, solving one opening 6.35x faster
- Online fine-tuning outperforms baseline solver in most openings with 4.61x reduction in search space on average

## Why This Works (Mechanism)

### Mechanism 1: Dynamic PCN Refinement
The solver improves proof search efficiency by dynamically fine-tuning the Proof Cost Network (PCN) heuristic with positions encountered during solving. As the distributed solver searches, the manager selects solved and critical positions from its search tree. The Online Fine-Tuning Trainer (OFT) uses these positions to update the PCN weights, creating new models that are immediately distributed to both manager and workers. This updated heuristic improves accuracy for the current proof search context.

### Mechanism 2: Critical Position Focus
The critical position sampling strategy focuses fine-tuning on positions most relevant to current proof search, avoiding redundant training. The OFT maintains a queue of the 1,000 most recent critical positions (leaf nodes in recent MCTS selection paths that aren't already solved). During self-play training, it randomly selects from these positions to generate training samples, ensuring the PCN focuses on positions the manager is actively exploring.

### Mechanism 3: Efficient Job Assignment
The job assignment scheme (virtual solving + top-k selection + AND-player job assignment) improves parallel efficiency by reducing redundancy and enabling better load balancing. Virtual solving marks nodes as solved immediately when jobs are assigned, preventing duplicate work. Top-k selection at AND nodes randomly chooses among the top k children, enabling parallel job assignment. AND-player job assignment only sends AND nodes as jobs, allowing OR nodes to be expanded directly by the manager for faster lookahead.

## Foundational Learning

- Concept: Proof Cost Network (PCN) and its training via AlphaZero self-play
  - Why needed here: Understanding how PCN predicts proof costs rather than win rates is fundamental to grasping why online fine-tuning improves solving efficiency
  - Quick check question: What is the key difference between PCN and standard AlphaZero value networks, and why does this difference matter for game solving?

- Concept: Monte Carlo Tree Search (MCTS) and PUCT selection
  - Why needed here: The solver uses MCTS with PUCT selection, and understanding this is crucial for understanding how the manager traverses the search tree and selects critical positions
  - Quick check question: How does PUCT selection work in the context of AND-OR trees for game solving, and how does it differ from standard MCTS?

- Concept: Distributed computing and job-based parallelism
  - Why needed here: The solver uses a distributed architecture with managers and workers, and understanding this architecture is essential for implementing and debugging the system
  - Quick check question: How does the manager distribute jobs to workers, and what mechanisms ensure consistency between manager and worker heuristics?

## Architecture Onboarding

- Component map: Manager -> PCN evaluation -> Job assignment (if v ≤ vthr) -> Worker solving -> Result return -> Manager tree update -> OFT position selection -> PCN fine-tuning -> New model distribution

- Critical path: Manager MCTS selection → PCN evaluation → Job assignment (if v ≤ vthr) → Worker solving → Result return → Manager tree update → OFT position selection → PCN fine-tuning → New model distribution

- Design tradeoffs:
  - vthr setting: Higher values reduce job overhead but increase failure rate; lower values increase job count but improve success rate
  - Queue sizes: Larger queues provide more training data but increase memory usage and may include less relevant positions
  - Fine-tuning frequency: More frequent updates provide better heuristics but increase overhead

- Failure signatures:
  - Manager stuck in infinite loop: Check PCN evaluation or job assignment logic
  - Workers solving too slowly: Check GPU utilization, batch size, or network communication
  - OFT not improving PCN: Check training data quality, learning rate, or model architecture
  - Inconsistent results between manager and workers: Check PCN synchronization mechanism

- First 3 experiments:
  1. Run baseline solver on a simple opening (e.g., KA) to establish performance baseline and verify distributed architecture works
  2. Enable online fine-tuning with solved positions only, measure improvement on same opening, verify OFT produces new PCN models
  3. Enable full online fine-tuning (solved + critical positions), measure improvement, verify critical position queue management and fine-tuning focus

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the improvement from online fine-tuning scale predictably with problem size and difficulty?
- Basis in paper: [explicit] The paper states "Results suggest that the savings scale with problem size" and shows that online fine-tuning performs better on larger problems like JA (78.31% reduction in nodes) compared to smaller problems like SA (23.31% reduction in nodes)
- Why unresolved: The experiments only cover 7x7 Killall-Go openings of varying difficulty, but not a systematic progression of problem sizes. The paper doesn't establish a mathematical relationship between problem size/difficulty and improvement magnitude.
- What evidence would resolve it: Testing the solver on a series of progressively larger problems (e.g., 5x5, 7x7, 9x9, 11x11) with consistent difficulty scaling would show if the improvement follows a predictable pattern.

### Open Question 2
- Question: How does online fine-tuning perform when the underlying neural network architecture changes?
- Basis in paper: [inferred] The experiments use a specific PCN architecture with three residual blocks and 256 hidden channels. The paper doesn't test alternative architectures or depth of networks.
- Why unresolved: The paper focuses on one specific architecture and doesn't explore how architectural changes affect the benefits of online fine-tuning. Different architectures might show different learning dynamics during online fine-tuning.
- What evidence would resolve it: Testing online fine-tuning with various network depths (shallow vs deep), widths (narrow vs wide), or entirely different architectures (CNNs vs transformers) would reveal architecture dependencies.

### Open Question 3
- Question: What is the optimal frequency and timing for PCN updates during online fine-tuning?
- Basis in paper: [explicit] The paper states "the trainer typically generates a new PCN version about every 120 seconds" but doesn't systematically explore this parameter or justify why this frequency was chosen.
- Why unresolved: The experiments use a fixed update frequency that appears to work well, but the paper doesn't explore whether more frequent or less frequent updates would be better, or whether adaptive scheduling based on search progress would improve results.
- What evidence would resolve it: Systematic experiments varying the update frequency (e.g., every 30s, 60s, 120s, 240s, 480s) and testing whether update timing should depend on metrics like number of solved positions or search tree depth would identify optimal scheduling.

## Limitations

- The experimental validation is limited to 7x7 Killall-Go, raising questions about generalizability to other games or larger board sizes.
- The ablation study only tests one job assignment variant and one queue size, missing potential optimizations.
- The paper does not address computational overhead of online fine-tuning or analyze when the method fails.

## Confidence

- **High confidence**: The core mechanism of online fine-tuning improving PCN accuracy for current proof search contexts, supported by significant node visit and time reductions across all 16 test openings.
- **Medium confidence**: The claim that online fine-tuning particularly excels on larger problems, based on limited evidence from only one larger opening (A) showing 6.35x speedup.
- **Medium confidence**: The job assignment scheme's contribution to efficiency gains, as the ablation study shows variants outperforming baseline but doesn't clearly separate fine-tuning vs. job assignment effects.

## Next Checks

1. **Generalization Test**: Run the online fine-tuning solver on a different game (e.g., Hex or Othello) to verify the approach works beyond Killall-Go and test scalability to larger board sizes.

2. **Overhead Analysis**: Measure the computational cost of the OFT training process and job assignment scheme separately to quantify the net efficiency gain and identify bottlenecks.

3. **Failure Case Study**: Systematically identify and analyze openings where online fine-tuning performs worse than baseline to understand limitations and failure modes of the approach.