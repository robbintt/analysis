---
ver: rpa2
title: 'Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions'
arxiv_id: '2310.18780'
source_url: https://arxiv.org/abs/2310.18780
tags:
- layer
- error
- distillation
- function
- filters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of long convolution sequence
  models (LCSMs) during auto-regressive inference, where they require processing the
  entire input sequence for each generated token. To solve this, the authors propose
  Laughing Hyena, a distillation method that extracts low-dimensional linear state-space
  models from each convolution layer of a pre-trained LCSM.
---

# Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions

## Quick Facts
- arXiv ID: 2310.18780
- Source URL: https://arxiv.org/abs/2310.18780
- Reference count: 40
- Primary result: 10× higher throughput than Transformers and 1.5× higher than Hyena at 1.3B parameters, without quality loss after distillation

## Executive Summary
The paper addresses the inefficiency of long convolution sequence models (LCSMs) during auto-regressive inference, where they require processing the entire input sequence for each generated token. To solve this, the authors propose Laughing Hyena, a distillation method that extracts low-dimensional linear state-space models from each convolution layer of a pre-trained LCSM. This enables O(1) compute and memory cost per token during generation. The method involves identifying the target state dimension using the Hankel operator's spectrum, using a factorized modal form for parametrization, and minimizing the ℓ2 error between the original and distilled filters. The authors also introduce architectural improvements to LCSMs, such as weight-tying filters across channels into heads, which improves pre-training quality and reduces the number of filters to be distilled. The resulting model achieves 10× higher throughput than Transformers and 1.5× higher than Hyena at 1.3B parameters, without any loss in quality after distillation.

## Method Summary
The authors propose a distillation method that converts convolution filters from pre-trained LCSMs into state-space models (SSMs) using rational interpolation and model-order reduction. The key steps are: (1) identifying the target state dimension by analyzing the Hankel operator's spectrum, (2) applying factorized modal form parametrization to fit the transfer function, and (3) minimizing the ℓ2 error between original and distilled filters using gradient-based optimization. They also introduce a weight-tying mechanism across channels into heads to improve pre-training quality and reduce the number of filters to distill. During inference, the distilled SSMs enable O(1) compute and memory per token via companion canonical form recurrences.

## Key Results
- Distilled MultiHyena achieves 10× higher throughput than Transformers and 1.5× higher than Hyena at 1.3B parameters
- No loss in quality after distillation, maintaining perplexity on The Pile
- State dimension of 16 yields <0.2% quality degradation while enabling constant-memory generation
- Fast prefill via FFT convolution reduces initialization time from O(T) to O(T log T)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LCSMs can be converted to SSMs enabling O(1) compute and memory per generated token
- Mechanism: Convolution filter h is approximated by rational transfer function H(z) = Q(z)/P(z) and converted to companion canonical state-space (A,B,C,h0) with diagonal A for independent O(1) state updates
- Core assumption: Hankel singular values decay rapidly enough for low-order SSM approximation
- Evidence: [abstract] "extracting low-dimensional linear state-space models from each convolution layer"
- Break condition: Slow Hankel singular value decay requiring large state dimension

### Mechanism 2
- Claim: Weight-tying filters across channels into heads improves pre-training and distillation
- Mechanism: Multiple heads share same filter but process different channel chunks, reducing filters to distill and increasing effective filter dimension
- Core assumption: Filter sharing doesn't reduce capacity when combined with multi-head design
- Evidence: [abstract] "weight-tying the filters across channels into heads, we achieve higher pre-training quality"
- Break condition: Reduced model capacity degrading downstream performance

### Mechanism 3
- Claim: Hankel operator's spectrum predicts minimal state dimension for distillation
- Mechanism: Compute Hankel matrix S from convolution filter; its rank or singular value decay indicates minimal McMillan degree d*
- Core assumption: Hankel singular value decay pattern predicts approximation error
- Evidence: [section 3.3] "identify candidate state dimensions by analyzing the spectrum of the Hankel operator"
- Break condition: Unpredictable Hankel singular value decay or noise-dominated spectrum

## Foundational Learning

- Concept: Transfer functions and state-space realizations
  - Why needed: Distillation converts convolution filters to SSMs via transfer functions
  - Quick check: Given (A,B,C,h0), write H(z) = C(zI-A)^{-1}B + h0 and explain invariance under state transformations

- Concept: Hankel operators and singular value decomposition
  - Why needed: Hankel spectrum determines minimal state dimension and predicts approximation quality
  - Quick check: Hankel matrix rank from convolution filter tells you minimal state dimension for exact SSM

- Concept: Rational function approximation and modal forms
  - Why needed: Distillation fits rational transfer function using modal interpolation with complex conjugate pole-residue pairs
  - Quick check: Given H(z) = Σ R_i/(z-λ_i), convert to state-space and explain companion form computational advantages

## Architecture Onboarding

- Component map: qkv projections -> multi-head Hyena operator -> concatenation
- Critical path: Pre-trained MultiHyena with tied-filter heads -> modal interpolation of each filter -> state-space conversion to companion form -> fast FFT prefill -> O(d) auto-regressive generation
- Design tradeoffs: Higher distillation order = better quality but slower generation; more heads = better pre-training but more filters to distill; FFT prefill = faster init but O(T) memory vs O(d) for recurrence; companion form = O(d) recurrence but requires conversion
- Failure signatures: Distillation errors > 1% → quality degradation; slow Hankel singular value decay → high distillation orders losing efficiency; state dimension too large → memory benefits disappear; numerical instability → poor approximation
- First 3 experiments: (1) Distill single Hyena layer at d=4,8,16 and measure ℓ2 error vs Hankel singular values; (2) Compare MultiHyena vs standard Hyena perplexity on The Pile with identical architecture except filter tying; (3) Benchmark throughput of distilled MultiHyena vs Transformer at batch sizes 1,64,256 with prompt length 512 and generation length 256

## Open Questions the Paper Calls Out

- Question: How do different distillation objectives (ℓ2 vs H2 norm) impact quality and efficiency of distilled SSMs?
  - Basis: [explicit] Paper discusses ℓ2 and H2 norm minimization with different properties
  - Unresolved: No direct comparison of effectiveness on distillation quality or computational efficiency
  - Evidence needed: Comprehensive study comparing distillation results using both objectives across various models and tasks

- Question: What is the impact of distillation order on downstream performance, and how does it vary across tasks and architectures?
  - Basis: [explicit] Paper mentions optimal order varies across layers and higher orders introduce less degradation
  - Unresolved: No detailed analysis of how order affects performance on different downstream tasks
  - Evidence needed: Systematic evaluation of distilled models with varying orders on diverse tasks including language modeling, code generation, and question answering

- Question: How can distillation process be optimized to further reduce computational cost and memory footprint?
  - Basis: [inferred] Paper highlights benefits for constant-memory generation but doesn't explore advanced optimization techniques
  - Unresolved: No exploration of pruning, quantization, or knowledge distillation to improve efficiency
  - Evidence needed: Experiments comparing distilled models with techniques like structured pruning, quantization, or distillation from larger models

## Limitations

- The theoretical relationship between Hankel singular value decay and approximation quality needs more empirical validation across diverse filter types
- Weight-tying mechanism's impact on pre-training quality lacks ablation studies isolating its contribution from other improvements
- All experimental results focus on Hyena-based architectures; effectiveness on other LCSM variants remains untested

## Confidence

**High Confidence Claims:**
- Companion canonical form enables O(1) compute per state dimension during auto-regressive generation
- Fast prefill via FFT convolution is mathematically sound and implementable
- ℓ2 error minimization between original and distilled filters is a valid optimization objective

**Medium Confidence Claims:**
- Multi-head filter tying improves pre-training quality while reducing distillation burden
- Hankel singular value decay predicts minimal distillation orders accurately
- Combined architecture achieves 10× higher throughput than Transformers

**Low Confidence Claims:**
- Quality preservation across all downstream tasks after distillation
- Method generalizes to other LCSM architectures beyond Hyena
- Specific parameter counts (1.3B) and performance ratios are robust across training runs

## Next Checks

1. **Cross-Layer Distillation Validation:** Run controlled experiments distilling individual Hyena layers at various orders (d=4,8,16,32) and measure both ℓ2 approximation error and downstream perplexity on The Pile. Plot Hankel singular values against actual distillation errors to verify the predictive relationship claimed in section 5.2.

2. **Architecture Ablation Study:** Train identical parameter count models with: (a) standard Hyena without filter tying, (b) MultiHyena with filter tying, and (c) MultiHyena without distillation. Compare pre-training perplexity and FLOPs per token to isolate the contributions of each architectural innovation.

3. **Memory-Compute Tradeoff Analysis:** Benchmark the distilled MultiHyena model at batch sizes 1, 64, 256 with prompt lengths 512 and generation lengths 256. Measure wall-clock throughput, peak memory usage, and compare against both Transformers and undistilled Hyena models. Include GPU utilization metrics to verify the claimed O(1) efficiency during auto-regressive generation.