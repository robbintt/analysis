---
ver: rpa2
title: 'ChatGPT for GTFS: Benchmarking LLMs on GTFS Understanding and Retrieval'
arxiv_id: '2308.02618'
source_url: https://arxiv.org/abs/2308.02618
tags:
- gtfs
- data
- questions
- information
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores whether ChatGPT can understand and extract information
  from GTFS (General Transit Feed Specification) data, which is tabular and requires
  specialized tools for analysis. The study benchmarks OpenAI's GPT-3.5-Turbo and
  GPT-4 on GTFS understanding using multiple-choice questions, achieving 59.7% and
  73.3% accuracy respectively.
---

# ChatGPT for GTFS: Benchmarking LLMs on GTFS Understanding and Retrieval

## Quick Facts
- arXiv ID: 2308.02618
- Source URL: https://arxiv.org/abs/2308.02618
- Reference count: 29
- Primary result: GPT-4 achieved 73.3% accuracy on GTFS multiple-choice questions and up to 61% accuracy on complex information retrieval tasks

## Executive Summary
This paper investigates whether large language models (LLMs) can understand and extract information from GTFS (General Transit Feed Specification) data, which is tabular and typically requires specialized tools for analysis. The study benchmarks OpenAI's GPT-3.5-Turbo and GPT-4 on GTFS understanding tasks using multiple-choice questions and information retrieval tasks. Results show that while LLMs demonstrate reasonable GTFS comprehension, program synthesis techniques significantly outperform zero-shot approaches for data extraction, particularly for complex queries.

## Method Summary
The researchers used a filtered GTFS feed from the Chicago Transit Authority containing 4 bus routes and created a questionnaire with over 250 questions covering 10 categories including attribute identification, data retrieval, and categorical mapping. They tested zero-shot, chain-of-thought, and program synthesis prompting techniques using the OpenAI API. For information retrieval, they compared zero-shot approaches against program synthesis where the LLM generates Python code to query the GTFS data using pandas. The study used temperature=0 and top_p=1 settings, with max_tokens adjusted based on the task complexity.

## Key Results
- GPT-4 achieved 73.3% accuracy on multiple-choice questions about GTFS understanding, compared to 59.7% for GPT-3.5-Turbo
- Program synthesis outperformed zero-shot approaches for information retrieval, achieving up to 93% accuracy on simple queries and 61% on complex ones using GPT-4
- Zero-shot prompting alone achieved only 40% accuracy on information retrieval tasks, while program synthesis reached 67% accuracy
- Categorical mapping questions showed the worst performance, with LLMs struggling to correctly interpret GTFS-specific codes and conventions

## Why This Works (Mechanism)

### Mechanism 1
Program synthesis outperforms zero-shot prompting for GTFS information retrieval by leveraging structured reasoning through code generation. The LLM writes Python/pandas code to query tabular GTFS data, which breaks complex tasks into deterministic steps and bypasses context length limitations while reducing hallucination risk. This works because the LLM can correctly map natural language queries to appropriate data joins and transformations.

### Mechanism 2
Zero-shot prompting alone is insufficient for accurate GTFS attribute mapping because it relies entirely on the LLM's pre-trained knowledge without explicit examples. This approach struggles to resolve domain-specific categorical mappings (e.g., route_type values) and data type conventions in GTFS, as the LLM's pre-training corpus likely did not include detailed GTFS semantics.

### Mechanism 3
Chain-of-Thought prompting improves reasoning but does not eliminate categorical mapping errors by encouraging the LLM to articulate intermediate reasoning steps. This exposes gaps in domain knowledge, but without correct categorical mappings, even well-reasoned explanations lead to wrong answers, as the LLM cannot infer GTFS-specific conventions from reasoning alone.

## Foundational Learning

- Concept: GTFS file schema and required vs. optional files
  - Why needed here: Understanding which files are mandatory determines what data can be queried without error
  - Quick check question: Which GTFS file defines stop locations and is always required?

- Concept: Pandas DataFrame operations for relational joins
  - Why needed here: Program synthesis generates pandas code; understanding joins is essential to validate generated queries
  - Quick check question: How do you join trips and routes tables on route_id in pandas?

- Concept: Categorical variable encoding in GTFS
  - Why needed here: Many GTFS attributes use numeric codes (e.g., route_type) that must be correctly interpreted
  - Quick check question: What GTFS route_type value represents bus service?

## Architecture Onboarding

- Component map: Natural language query → Prompt builder (zero-shot/CoT/program synthesis) → LLM API (GPT-3.5/GPT-4) → Output (answer or code) → (If code) Python runtime → Final answer → Ground truth evaluator
- Critical path: Query → Prompt construction → LLM call → Result extraction → Validation
- Design tradeoffs:
  - Zero-shot: Simpler prompt, higher hallucination risk, lower accuracy
  - Program synthesis: More complex prompt, deterministic code execution, better accuracy but slower
  - Context length: Large GTFS feeds may exceed LLM context, necessitating sampling or file reduction
- Failure signatures:
  - Zero-shot: Nonsensical or inconsistent answers, missing data joins
  - Program synthesis: Syntax errors in generated code, incorrect column references, logical join errors
  - CoT: Well-articulated but wrong reasoning due to incorrect categorical mappings
- First 3 experiments:
  1. Run zero-shot query "What is the route_short_name for route_id 192?" on filtered CTA feed, measure accuracy vs ground truth
  2. Generate program synthesis code for same query, execute, compare accuracy and latency
  3. Apply CoT prompt to categorical mapping question "What route_type value represents a bus?" and analyze reasoning trace

## Open Questions the Paper Calls Out

### Open Question 1
Can fine-tuning GPT-4 with GTFS data significantly improve its accuracy on categorical mapping questions compared to zero-shot performance? The paper notes that GPT-3.5 struggles with categorical mapping and suggests that explicitly providing mapping information or fine-tuning the LLM could overcome this limitation. This remains unresolved as the study only tested zero-shot and few-shot approaches with GPT-3.5, not GPT-4, and did not explore fine-tuning. A controlled experiment comparing zero-shot, few-shot, and fine-tuned GPT-4 performance on categorical mapping questions, with and without explicit mapping information provided, would resolve this question.

### Open Question 2
How does the performance of open-source LLMs (e.g., BLOOM, LLaMA) on GTFS understanding and information retrieval tasks compare to closed-source models like GPT-3.5 and GPT-4? The paper mentions that open-source LLMs exist but only tested closed-source models, leaving their potential unexplored. This remains unresolved as the study did not include open-source models in its benchmarks. Benchmarking multiple open-source LLMs on the same GTFS understanding and information retrieval tasks used in the study, comparing their performance to GPT-3.5 and GPT-4, would resolve this question.

### Open Question 3
Can program synthesis approaches be further improved by incorporating GTFS-specific constraints or validation steps to reduce attribute mapping and logical errors? The paper identified that program synthesis errors were primarily due to attribute mapping issues and logical errors, suggesting room for improvement. This remains unresolved as the study used basic program synthesis without GTFS-specific optimizations or validation. Developing and testing an enhanced program synthesis approach that incorporates GTFS schema validation, attribute type checking, and logical consistency verification, then comparing its performance to the basic approach used in the study, would resolve this question.

## Limitations

- The study used a filtered GTFS feed containing only 4 bus routes, which may not represent the complexity of full transit systems
- The questionnaire used for evaluation is not publicly available, preventing independent validation of the claimed accuracy rates
- The study focused on a single transit agency (Chicago Transit Authority), limiting generalizability to other GTFS implementations

## Confidence

- **High confidence**: Program synthesis outperforms zero-shot for information retrieval (supported by direct experimental comparison showing 67% vs 40% accuracy)
- **Medium confidence**: LLMs demonstrate reasonable GTFS understanding (based on MCQ results, but dependent on unpublished questionnaire)
- **Low confidence**: CoT consistently improves reasoning accuracy (only anecdotal evidence from one categorical mapping failure case)

## Next Checks

1. Re-run the multiple-choice questionnaire with an independent GTFS expert-created test set to verify the 59.7% and 73.3% accuracy claims
2. Test program synthesis approach on a full GTFS feed (all routes) to evaluate context length handling and accuracy degradation
3. Implement and compare alternative chunking strategies for large GTFS feeds to determine if the data reduction approach was optimal