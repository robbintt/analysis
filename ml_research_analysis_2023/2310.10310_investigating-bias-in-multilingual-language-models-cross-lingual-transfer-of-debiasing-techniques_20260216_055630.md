---
ver: rpa2
title: 'Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer
  of Debiasing Techniques'
arxiv_id: '2310.10310'
source_url: https://arxiv.org/abs/2310.10310
tags:
- debiasing
- language
- bias
- techniques
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks cross-lingual debiasing of mBERT for English,
  French, German, and Dutch, finding that debiasing techniques (e.g., SentenceDebias)
  can reduce bias across languages with an average improvement of 13% in bias score,
  and that pretraining-based methods work best for low-resource languages.
---

# Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques

## Quick Facts
- arXiv ID: 2310.10310
- Source URL: https://arxiv.org/abs/2310.10310
- Reference count: 15
- Key outcome: Cross-lingual debiasing of mBERT reduces bias by an average of 13% across English, French, German, and Dutch, with pretraining-based methods most effective for low-resource languages.

## Executive Summary
This study benchmarks cross-lingual debiasing techniques on mBERT, demonstrating that bias mitigation in one language can transfer to others due to shared semantic representations. The research evaluates five debiasing methods across four languages, finding that SentenceDebias consistently outperforms others with an average 13% reduction in bias scores. Pretraining-based approaches show particular promise for low-resource languages, suggesting practical applications for multilingual models deployed in diverse linguistic contexts.

## Method Summary
The study applies debiasing techniques (CDA, DO, SentenceDebias, INLP, DR) to mBERT using English data and evaluates cross-lingual effectiveness on French, German, and Dutch. Researchers use the CrowS-Pairs dataset (English base, translated to target languages) and attribute word lists for gender, race, and religion in each language. The debiasing process involves additional pretraining on 2.5%-10% of Wikipedia corpus per language, with three random seeds for stability. Bias reduction is measured using the percentage of biased predictions versus unbiased predictions, with optimal performance at 50%.

## Key Results
- Cross-lingual debiasing reduces bias scores by an average of 13% across all evaluated languages
- SentenceDebias emerges as the most consistent technique, outperforming others across language combinations
- Pretraining-based debiasing methods show enhanced effectiveness for low-resource languages (German in this study)
- Projection-based techniques applied to one language yield similar bias reduction predictions across evaluation languages

## Why This Works (Mechanism)

### Mechanism 1
Cross-lingual debiasing is feasible because multilingual models like mBERT learn shared semantic representations across languages, allowing bias mitigation in one language to transfer to others. When debiasing techniques modify the embedding space in a source language, the aligned multilingual representations ensure that similar bias structures in target languages are also affected, reducing bias across the board. The multilingual model maintains strong cross-lingual alignment in the bias-relevant subspace, so projection-based debiasing generalizes across languages. If cross-lingual alignment is weak in bias subspaces, or if the bias is language-specific and not well-represented in the shared semantic space, transferability fails.

### Mechanism 2
Pretraining-based debiasing methods are especially effective for low-resource languages because they expose the model to unbiased data in the target language during fine-tuning. Additional pretraining on debiased data from a low-resource language helps the model internalize unbiased associations while adapting to the linguistic characteristics of that language. The model can learn unbiased associations in a low-resource language when exposed to targeted pretraining data, and this learning transfers to downstream tasks. If the pretraining corpus is too small or biased, the method cannot effectively reduce bias in low-resource languages.

### Mechanism 3
SentenceDebias is the most consistent cross-lingual debiasing technique because it uses PCA on contextualized embeddings to identify and remove bias directions in a stable manner. By projecting sentence representations onto a bias-subspace orthogonal complement, SentenceDebias systematically reduces bias across different languages without overfitting to language-specific idiosyncrasies. The first K principal components capture the dominant bias directions consistently across languages in the multilingual model. If the bias subspace is not consistent across languages or if PCA fails to capture the relevant bias directions, the method's effectiveness drops.

## Foundational Learning

- **Cross-lingual transfer in multilingual models**: Understanding how knowledge and modifications in one language affect others is key to evaluating the feasibility of cross-lingual debiasing. Quick check: If a multilingual model is debiased in English, why might the debiasing effect carry over to French or German?

- **Bias subspace identification and projection**: Debiasing techniques like SentenceDebias and INLP rely on identifying and removing bias directions in the embedding space; understanding this is essential to grasp how they work. Quick check: What does it mean to project sentence representations onto the orthogonal complement of a bias subspace?

- **Pretraining and fine-tuning in low-resource languages**: Some debiasing methods use additional pretraining steps, and their effectiveness can vary depending on the resource level of the target language. Quick check: Why might additional pretraining on debiased data be more effective for low-resource languages than for high-resource ones?

## Architecture Onboarding

- **Component map**: mBERT base model -> Debiasing techniques (CDA, DO, SentenceDebias, INLP, DR) -> Evaluation dataset (CrowS-Pairs translations) -> Bias metric (percentage of biased predictions vs. unbiased) -> Training/evaluation pipeline

- **Critical path**: 1) Prepare translated CrowS-Pairs dataset for target languages. 2) Select debiasing technique and source language. 3) Apply debiasing to mBERT (with or without additional pretraining). 4) Evaluate bias scores on target language dataset. 5) Compare results to baseline (unmodified mBERT).

- **Design tradeoffs**: Projection-based vs. pretraining-based debiasing: Projection is faster but may not generalize as well; pretraining is slower but can be more effective, especially for low-resource languages. Binary vs. multi-class bias handling: Some techniques (like DR) only work for binary bias, limiting their applicability. Dataset size and representativeness: Smaller or unrepresentative datasets can skew bias evaluation.

- **Failure signatures**: Bias scores increase after debiasing (overcompensation). Inconsistent performance across languages for the same technique. No improvement in bias scores after debiasing. High variance in results across random seeds.

- **First 3 experiments**: 1) Apply SentenceDebias to mBERT using English data, evaluate on French dataset; compare bias scores to baseline. 2) Apply CDA with additional pretraining to mBERT using Dutch data, evaluate on German dataset; compare bias scores to baseline. 3) Apply INLP to mBERT using French data, evaluate on Dutch dataset; compare bias scores to baseline.

## Open Questions the Paper Calls Out

- **Generalizability to diverse languages**: How does the cross-lingual transferability of debiasing techniques generalize to languages from different cultures and linguistic families? The study focused on four closely related languages from similar cultures, suggesting a limitation in generalizability to other languages. Conducting similar experiments with languages from different linguistic families would provide insights into the generalizability of the findings.

- **Performance across multilingual models**: How do the performance and effectiveness of debiasing techniques vary across different multilingual language models beyond mBERT? The research was conducted using mBERT, and extending the study to other multilingual language models would provide valuable insights. Implementing the debiasing techniques on other multilingual language models and comparing their performance with mBERT would reveal how the effectiveness of these techniques varies across different models.

- **Impact on downstream tasks**: How do debiasing techniques impact the performance of downstream tasks in multilingual models? The paper acknowledges the need to investigate the applicability of debiased models to downstream tasks, which was not addressed in the study. Conducting experiments to assess the performance of debiased models on various downstream tasks in different languages would provide insights into the impact of debiasing on task-specific performance.

## Limitations
- The study relies on CrowS-Pairs translations, but exact sampling methodology and translation quality across languages remain unclear, potentially affecting generalizability
- Implementation details for some debiasing techniques (e.g., DensRay for multiclass bias) and handling of attribute word lists in non-English languages are not fully specified
- While pretraining-based methods show promise for low-resource languages, the study only evaluates on German as a proxy, limiting evidence for truly low-resource languages

## Confidence
- **High Confidence**: Cross-lingual transfer of debiasing is feasible and yields measurable improvements (average 13% reduction in bias scores)
- **Medium Confidence**: Pretraining-based methods are most effective for low-resource languages, though evidence is limited to German
- **Low Confidence**: Exact implementation details for some techniques (e.g., DensRay) and their stability across languages are not fully specified

## Next Checks
1. Validate cross-lingual alignment by testing consistency of bias subspace identification across languages using English, French, and German
2. Replicate pretraining effectiveness by applying debiasing to a truly low-resource language (e.g., Swahili) and evaluating bias reduction
3. Test implementation stability by running multiple trials of projection-based debiasing (e.g., SentenceDebias, INLP) with different random seeds to assess variability in bias reduction across languages