---
ver: rpa2
title: Multilingual Previously Fact-Checked Claim Retrieval
arxiv_id: '2305.07991'
source_url: https://arxiv.org/abs/2305.07991
tags:
- dataset
- posts
- fact-checks
- methods
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MultiClaim, a new multilingual dataset for
  previously fact-checked claim retrieval (PFCR). It consists of 205,751 fact-checks
  in 39 languages, 28,092 social media posts in 27 languages, and 31,305 connections
  between them.
---

# Multilingual Previously Fact-Checked Claim Retrieval

## Quick Facts
- **arXiv ID**: 2305.07991
- **Source URL**: https://arxiv.org/abs/2305.07991
- **Reference count**: 18
- **Primary result**: MultiClaim dataset with 205,751 fact-checks in 39 languages; English TEMs outperform multilingual models for both monolingual and crosslingual retrieval

## Executive Summary
This paper introduces MultiClaim, a new multilingual dataset for previously fact-checked claim retrieval (PFCR) that significantly advances the state of the art in multilingual misinformation detection. The dataset contains 205,751 fact-checks across 39 languages, 28,092 social media posts in 27 languages, and 31,305 connections between them. The authors establish crosslingual PFCR as a new task and demonstrate that English text embedding models consistently outperform multilingual alternatives for both monolingual and crosslingual retrieval. They also show that machine translation substantially improves performance for both BM25 and text embedding methods, and that supervised fine-tuning with their dataset can significantly enhance text embedding performance.

## Method Summary
The paper evaluates unsupervised methods including BM25 and various text embedding models on the MultiClaim dataset, testing both monolingual and crosslingual retrieval scenarios. English text embedding models (TEMs) were found to be the best performing option, with machine translation significantly improving results for both BM25 and TEMs. The authors also explored supervised fine-tuning using contrastive or cosine loss with the dataset as positive samples and random pairs as negatives. The evaluation framework measures performance using success-at-K (S@K) metric and examines various data dimensions including post length and publication date.

## Key Results
- English text embedding models outperform multilingual models for both monolingual and crosslingual claim retrieval
- Machine translation significantly improves the performance of both BM25 and text embedding methods
- Supervised fine-tuning with the MultiClaim dataset can significantly improve text embedding method performance
- Same language bias is present and beneficial up to a certain point for meaningful text similarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: English TEMs outperform multilingual models for both monolingual and crosslingual claim retrieval.
- Mechanism: English TEMs benefit from higher quality training data and larger model sizes, leading to better semantic understanding and retrieval accuracy.
- Core assumption: The quality and quantity of training data for English TEMs is superior to that of multilingual models, resulting in better performance.
- Evidence anchors:
  - [abstract]: "We show that English TEMs perform best for both monolingual and crosslingual retrieval"
  - [section 4.1]: "English TEMs are the best performing option for both monolingual and crosslingual claim retrieval"
- Break condition: If the training data quality or quantity for multilingual models significantly improves, their performance may surpass English TEMs.

### Mechanism 2
- Claim: Machine translation significantly improves the performance of both BM25 and TEMs.
- Mechanism: Translating non-English content to English allows models to leverage their superior performance on English data, effectively bridging the language gap.
- Core assumption: Models perform better on English data than on other languages, and translation quality is sufficient to preserve semantic meaning.
- Evidence anchors:
  - [abstract]: "machine translation significantly improves the performance of both BM25 and TEMs"
  - [section 4.1]: "Machine translation signiﬁcantly improved the performance of both BM25 and TEMs"
- Break condition: If machine translation quality degrades significantly or if models are developed that perform equally well on multiple languages, this mechanism may break down.

### Mechanism 3
- Claim: Same language bias (SLB) - a tendency of methods to retrieve fact-checks that have the same language as the post - is present and can be beneficial up to a certain point.
- Mechanism: Models tend to prioritize fact-checks in the same language as the post due to lexical overlap and language-specific semantic understanding, which can be helpful for localizing claims.
- Core assumption: There is a correlation between language and the semantic content of fact-checks, and this correlation can be leveraged for better retrieval.
- Evidence anchors:
  - [section 4.1]: "We theorize that a certain amount of SLB is healthy, as long as the methods focus on meaningful similarities in texts written in the same language"
  - [section 4.1]: "BM25-Original has the highest SLB score of all the methods"
- Break condition: If the correlation between language and semantic content weakens or if models are developed that can effectively handle multilingual retrieval without relying on SLB, this mechanism may break down.

## Foundational Learning

- Concept: Text embedding models (TEMs)
  - Why needed here: TEMs are used to encode both social media posts and fact-checked claims into a common vector space, allowing for efficient retrieval based on semantic similarity.
  - Quick check question: What is the main advantage of using TEMs over traditional information retrieval methods like BM25?

- Concept: Machine translation
  - Why needed here: Machine translation is used to translate non-English content to English, enabling the use of models that perform better on English data.
  - Quick check question: What is a potential drawback of relying on machine translation for multilingual retrieval?

- Concept: Same language bias (SLB)
  - Why needed here: SLB is a phenomenon observed in the dataset where models tend to retrieve fact-checks in the same language as the post, which can be beneficial up to a certain point.
  - Quick check question: How can SLB be leveraged to improve the retrieval of localized claims?

## Architecture Onboarding

- Component map: Data collection pipeline -> Text encoding -> Retrieval -> Evaluation
- Critical path: Data collection → Text encoding → Retrieval → Evaluation
- Design tradeoffs:
  - Using English TEMs vs multilingual TEMs: English TEMs perform better but require machine translation
  - Using machine translation vs developing multilingual models: Machine translation is easier but may introduce errors
  - Focusing on monolingual vs crosslingual retrieval: Monolingual retrieval is simpler but may miss relevant fact-checks in other languages
- Failure signatures:
  - Low S@K scores: indicates poor retrieval performance
  - High same language bias: may indicate over-reliance on lexical overlap
  - Degradation in performance over time: may indicate issues with model aging or data quality
- First 3 experiments:
  1. Compare the performance of English TEMs and multilingual TEMs on a subset of the dataset to validate the superiority of English TEMs
  2. Evaluate the impact of machine translation on the performance of BM25 and TEMs by comparing results with and without translation
  3. Analyze the same language bias by measuring the percentage of top retrieved fact-checks that have the same language as the input post

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of text embedding models (TEMs) change when using a more sophisticated negative sampling strategy during supervised fine-tuning?
- Basis in paper: [explicit] The paper mentions that random selection of negative samples was too easy and that a more elaborate scheme for generating challenging negative samples could lead to further performance improvements.
- Why unresolved: The paper used a naive random selection of negative samples for supervised fine-tuning, which saturated the training set quickly but limited the amount of information the model could learn.
- What evidence would resolve it: Conducting experiments with different negative sampling strategies (e.g., hard negative mining, dynamic negative sampling) and comparing their performance on the test set would provide evidence of the impact on TEM performance.

### Open Question 2
- Question: To what extent does the dataset's linguistic diversity and potential selection bias affect the generalizability of the methods to other language pairs and time periods?
- Basis in paper: [inferred] The paper acknowledges the dataset's bias towards major languages and Indo-European language family, and the potential for selection bias towards social media posts that are already detectable by platforms or fact-checkers.
- Why unresolved: The paper's evaluation is limited to the specific languages and time periods represented in the dataset, and it is unclear how the methods would perform on other language pairs or with posts from different time periods.
- What evidence would resolve it: Testing the methods on additional datasets with different language pairs and time periods, or conducting a more thorough analysis of the dataset's composition and potential biases, would provide evidence of the methods' generalizability.

### Open Question 3
- Question: How do the methods perform on social media posts that require visual information (e.g., images or videos) to fully understand the claim being made?
- Basis in paper: [explicit] The paper mentions that 13% of the dataset pairs were not correct because the claim made in the social media post was based on visual information, and that the methods might still be able to retrieve correct fact-checks for some of these posts based on spurious correlations.
- Why unresolved: The paper's evaluation is based solely on text-based methods, and it does not explore how the methods would perform on posts that require visual information.
- What evidence would resolve it: Conducting experiments that incorporate visual information (e.g., using image or video analysis techniques) and comparing their performance to the text-based methods would provide evidence of their effectiveness on posts with visual content.

## Limitations
- The findings are based on current training data distributions and may not hold if multilingual model training data quality improves significantly
- The impact of machine translation quality on retrieval performance is not fully quantified
- The dataset's focus on fact-checks and posts from 2020-2021 may limit generalizability to other time periods

## Confidence

- **High confidence**: The superiority of English TEMs over multilingual models for retrieval tasks, supported by multiple evaluation dimensions
- **Medium confidence**: The significant improvement from machine translation, though the exact magnitude depends on translation quality
- **Medium confidence**: The beneficial role of same-language bias up to a certain threshold, based on observed correlations but requiring further validation

## Next Checks

1. Evaluate retrieval performance on out-of-distribution time periods (pre-2020 and post-2021) to test temporal generalizability of the findings
2. Conduct a controlled experiment comparing retrieval performance with human-translated vs machine-translated content to quantify the impact of translation quality
3. Test the models' ability to handle emerging claims by creating synthetic test cases that combine elements from different languages and domains not present in the training data