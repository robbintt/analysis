---
ver: rpa2
title: 'Big model only for hard audios: Sample dependent Whisper model selection for
  efficient inferences'
arxiv_id: '2309.12712'
source_url: https://arxiv.org/abs/2309.12712
tags:
- whisper
- speech
- small
- tiny
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores a decision module approach for selecting between
  Whisper Tiny and Whisper Small models based on input audio samples. The key idea
  is to train a lightweight model to predict which model will yield lower WER for
  a given sample, thus reducing computational costs while maintaining performance.
---

# Big model only for hard audios: Sample dependent Whisper model selection for efficient inferences

## Quick Facts
- arXiv ID: 2309.12712
- Source URL: https://arxiv.org/abs/2309.12712
- Reference count: 0
- Primary result: Decision module achieves 68.2% accuracy on CommonVoice test set, reducing MACs by 12-35% with minimal WER increase

## Executive Summary
This paper proposes a method to dynamically select between Whisper Tiny and Whisper Small models based on input audio characteristics to reduce computational costs while maintaining transcription quality. The approach uses a lightweight ResNet decider that predicts which model will yield lower WER for a given sample, effectively interpolating between model sizes without additional training. Results show the method achieves 68.2% accuracy in model selection on CommonVoice test set, reducing multiply-accumulate operations by 12-35% compared to using Whisper Small alone, with only a slight increase in word error rate.

## Method Summary
The method trains a lightweight ResNet decider to predict whether Whisper Tiny or Whisper Small will yield lower WER for a given audio sample. The decider uses high-level features extracted from a frozen Whisper Small encoder as input, processing them through 3 ResBlocks with learned layer weights and a sigmoid output. During inference, the decider's output is compared to a threshold (0.3 or 0.5) to select between the two ASR models. The system is trained using binary cross-entropy loss with Adam optimizer (lr=1e-5, cosine annealing) on features from LibriSpeech and CommonVoice datasets.

## Key Results
- Decision module achieves 68.2% accuracy on CommonVoice test set using Whisper Small encoder features
- MACs reduced by 12-35% compared to using Whisper Small alone
- WER increase is minimal, with interpolated performance close to intermediate Whisper Base model
- ResNet decider architecture outperforms transformer-based alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The decision module can predict which Whisper model will perform better for a given audio sample, allowing selection of the smaller model when sufficient.
- Mechanism: The decider learns to predict the outcome of a comparison between Whisper Tiny and Whisper Small WERs. It uses high-level features (Whisper Small encoder outputs) to capture relevant information about the sample's difficulty.
- Core assumption: High-level speech features contain enough information to predict relative WER performance between two models.
- Evidence anchors:
  - [abstract] The paper states the decision module allows selection of the smallest sufficient model for good transcription.
  - [section 2.1] Explains using Whisper Small encoder as feature extractor because it provides high-quality zero-shot phonetic encoding and additional information.
  - [corpus] Weak or missing - no corpus evidence directly supports this specific claim.
- Break condition: If high-level features do not capture the relevant characteristics that differentiate model performance, the prediction accuracy will be low.

### Mechanism 2
- Claim: Using a lightweight ResNet decider keeps computational costs low while still providing good accuracy.
- Mechanism: A small convolutional network processes the weighted sum of encoder layers to make a binary decision, avoiding the high cost of transformer-based approaches.
- Core assumption: A small convolutional network is sufficient to capture the relevant patterns for model selection.
- Evidence anchors:
  - [section 2.2] Explains the choice of a small ResNet over transformer stacks due to computational efficiency.
  - [section 4.1] Shows ResNet decider achieves 68.2% accuracy, better than transformer alternatives.
  - [corpus] Weak or missing - no corpus evidence directly supports this specific claim.
- Break condition: If the ResNet architecture is too simple to capture the necessary decision boundaries, accuracy will suffer.

### Mechanism 3
- Claim: The approach can interpolate between model sizes without training intermediate models.
- Mechanism: By dynamically selecting between two existing models based on sample difficulty, the system effectively creates a continuum of performance/compute trade-offs.
- Core assumption: The decision boundary learned between two models can generalize to represent the performance of intermediate-sized models.
- Evidence anchors:
  - [abstract] States the method allows interpolation between model sizes without additional training.
  - [section 4.2] Shows results where the selection approach yields performance close to an intermediate model (Whisper Base).
  - [corpus] Weak or missing - no corpus evidence directly supports this specific claim.
- Break condition: If the performance gap between the two models is not linear or if other factors affect intermediate models differently, the interpolation will be inaccurate.

## Foundational Learning

- Concept: ASR model architecture differences
  - Why needed here: Understanding why Whisper Tiny and Small perform differently on the same samples is crucial for building the decision module.
  - Quick check question: What architectural differences between Whisper Tiny and Small could explain why one performs better on certain samples?

- Concept: Speech feature representations
  - Why needed here: The decision module relies on high-level speech features, so understanding what information these contain is important.
  - Quick check question: What types of information do high-level speech features (like those from Whisper encoders) capture beyond basic acoustic properties?

- Concept: Model selection and ensemble methods
  - Why needed here: The core idea is selecting between models based on sample characteristics, which relates to broader concepts in model selection.
  - Quick check question: How does this approach differ from traditional ensemble methods in machine learning?

## Architecture Onboarding

- Component map: Audio → Whisper Small encoder (frozen) → ResNet decider → Threshold comparison → Selected model → Transcription

- Critical path: Audio → Feature extraction → Decision → Model selection → Transcription

- Design tradeoffs:
  - Computational cost vs accuracy: Using high-level features increases decider accuracy but adds encoding cost
  - Model complexity vs generalization: Simple ResNet may miss complex patterns but generalizes better
  - Threshold selection: Balancing WER improvement against MAC reduction

- Failure signatures:
  - Low decision accuracy: Check if high-level features capture relevant information
  - Poor WER improvement: Verify threshold selection and model performance correlation
  - High computational cost: Profile feature extraction and decision module separately

- First 3 experiments:
  1. Test baseline accuracy using only SNR or accent as decision features to establish minimum performance bar
  2. Compare different feature extractors (Mel, Wav2Vec2, Whisper encoder) to validate choice of Whisper Small encoder
  3. Sweep decision threshold values to find optimal WER/MAC tradeoff on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the decision module's accuracy be improved for selecting between Whisper Tiny and Whisper Small models?
- Basis in paper: [explicit] The paper mentions that the decision module achieves 68.2% accuracy on the CommonVoice test set, which is lower than the topline's 80.3% accuracy. The paper also notes that the failure of the baselines and the encoder layers being the best performing input, tend to show that the errors are very dependent of the ASR model.
- Why unresolved: The paper does not explore other methods or techniques to improve the decision module's accuracy beyond the ones presented.
- What evidence would resolve it: Experiments with different feature extractors, decider architectures, or training techniques that achieve higher accuracy on the CommonVoice test set.

### Open Question 2
- Question: How does the proposed method perform on other ASR models beyond Whisper?
- Basis in paper: [inferred] The paper only applies the proposed method to two Whisper models (Tiny and Small). It would be interesting to see how the method performs on other ASR models, such as Nvidia FastConformers or self-supervised models like Hubert or WavLM.
- Why unresolved: The paper does not provide any results or analysis on other ASR models.
- What evidence would resolve it: Experiments applying the proposed method to other ASR models and comparing the results with the ones obtained on Whisper models.

### Open Question 3
- Question: How does the proposed method scale to larger ASR models beyond Whisper Medium?
- Basis in paper: [inferred] The paper only considers Whisper models up to Medium size (759M parameters). It would be interesting to see how the proposed method scales to larger ASR models, such as Whisper Large (1.5B parameters) or other models with even more parameters.
- Why unresolved: The paper does not provide any results or analysis on larger ASR models.
- What evidence would resolve it: Experiments applying the proposed method to larger ASR models and comparing the results with the ones obtained on smaller models.

## Limitations
- Performance Gap Generalization: Interpolation claim lacks systematic validation across multiple model pairs
- Dataset Dependence: Primary evaluation on CommonVoice test set raises questions about generalizability to other domains
- Feature Extraction Overhead: Whisper Small encoder features are computed even when selecting Whisper Tiny, not fully accounted for in efficiency gains

## Confidence
- High Confidence: Core mechanism of using high-level features to predict model performance differences is well-supported
- Medium Confidence: ResNet decider architecture choice is justified but optimal architecture for different feature extractors remains unknown
- Low Confidence: Interpolation claim for intermediate models lacks systematic validation across multiple model sizes

## Next Checks
1. Cross-domain Evaluation: Test the decider on diverse datasets (TED talks, podcasts, noisy environments) to validate generalizability beyond CommonVoice
2. Intermediate Model Validation: Systematically evaluate the approach across multiple model pairs (Tiny→Base, Base→Small, Small→Large) to verify interpolation claim
3. Feature Extraction Cost Analysis: Measure actual runtime and energy consumption including feature extraction overhead, compare against naive post-hoc selection approach