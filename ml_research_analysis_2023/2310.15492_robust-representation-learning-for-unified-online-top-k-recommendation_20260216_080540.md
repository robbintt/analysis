---
ver: rpa2
title: Robust Representation Learning for Unified Online Top-K Recommendation
arxiv_id: '2310.15492'
source_url: https://arxiv.org/abs/2310.15492
tags:
- domain
- learning
- different
- matching
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified online top-k recommendation method
  designed for large-scale e-commerce systems with multi-entity and multi-domain advertising.
  The approach constructs unified modeling in entity space to ensure data fairness
  and employs robust representation learning to extract domain-invariant and domain-specific
  representations.
---

# Robust Representation Learning for Unified Online Top-K Recommendation

## Quick Facts
- arXiv ID: 2310.15492
- Source URL: https://arxiv.org/abs/2310.15492
- Reference count: 40
- Unified online top-k recommendation method improves recall metrics across multiple advertising domains

## Executive Summary
This paper introduces a unified online top-k recommendation method designed for large-scale e-commerce systems with multi-entity and multi-domain advertising. The approach constructs unified modeling in entity space to ensure data fairness and employs robust representation learning to extract domain-invariant and domain-specific representations. Robust representation learning leverages domain adversarial learning and multi-view Wasserstein distribution learning, balancing conflicting objectives through homoscedastic uncertainty weights and orthogonality constraints. Experiments on Alibaba's display advertising dataset show significant improvements over baselines in recall metrics across multiple domains.

## Method Summary
The method uses unified modeling to normalize multi-entity features into a common feature schema, enabling a single model to process different advertising entities without separate branches. The backbone framework employs shared-domain and specific-domain experts with gate units to adaptively combine domain-invariant and domain-specific representations. Robust representation learning includes domain adversarial learning (DAL) to extract domain-invariant features, multi-view Wasserstein distribution learning (MVWDL) to align distribution across domains, orthogonality constraints to reduce redundancy, and uncertainty weight learning (UWL) to dynamically balance multiple loss terms during training.

## Key Results
- Significant improvements in recall metrics (Recall-all@2000 and Recall-retrieval@2000) across all four advertising domains
- Online A/B testing demonstrates increased RPM (Revenue Per Mille) and cost efficiency
- Successfully deployed in real business scenarios, enhancing retrieval accuracy while alleviating unfair scoring across different ad entities

## Why This Works (Mechanism)

### Mechanism 1
Unified modeling in entity space resolves cross-entity feature inconsistency by normalizing item and content advertisement features through a single feature link, enabling the same model to process them without entity-specific branches. This works under the assumption that all entity types can be mapped into a common feature schema without significant information loss. Break condition: Entity-specific attributes cannot be meaningfully encoded into the unified schema, leading to information loss that harms recall.

### Mechanism 2
Domain adversarial learning extracts domain-invariant representations by training a shared-domain expert to learn features that a classifier cannot distinguish by domain, forcing the expert to generalize across domains. This relies on the assumption that the shared expert can learn representations useful for all domains while being domain-agnostic. Break condition: The adversarial training causes the shared expert to discard useful domain-specific signals, hurting domain performance.

### Mechanism 3
Multi-view Wasserstein distribution learning aligns domain-invariant representation distributions by mapping the shared representation into multiple views and minimizing the Wasserstein distance between these projected distributions. This assumes that aligning distributions across views improves the generalization of the shared representation without losing domain-specific signal in specific experts. Break condition: Excessive distribution alignment causes mode collapse, reducing the diversity of learned representations.

## Foundational Learning

- Concept: Domain adversarial learning
  - Why needed here: To force the shared expert to learn domain-agnostic features that generalize across multiple advertising domains
  - Quick check question: What happens to the shared expert if the domain classifier can still correctly predict the domain?

- Concept: Wasserstein distance
  - Why needed here: To measure and minimize the difference between distributions of domain-invariant representations across domains
  - Quick check question: Why is Wasserstein distance preferred over KL divergence for this alignment task?

- Concept: Homoscedastic uncertainty weighting
  - Why needed here: To dynamically balance the multiple loss terms based on their relative uncertainty during training
  - Quick check question: How does uncertainty weighting affect the learning dynamics compared to fixed loss coefficients?

## Architecture Onboarding

- Component map:
  Unified feature preprocessing -> Embedding -> Ad network/User transformer/Attention -> Backbone -> RRL -> Final logits

- Critical path:
  Unified feature preprocessing → Embedding → Ad network/User transformer/Attention → Backbone → RRL → Final logits

- Design tradeoffs:
  - Unified modeling vs. entity-specific models: reduces maintenance but may lose some entity-specific nuance
  - Domain adversarial vs. domain-specific learning: improves generalization but may hurt domain performance
  - Wasserstein alignment vs. adversarial alignment: smoother gradients but higher computational cost

- Failure signatures:
  - Recall drops across all domains: likely a backbone or data link issue
  - Domain-specific recall drops: likely over-alignment in RRL or insufficient specific expert capacity
  - Training instability: check adversarial loss weighting or gradient penalty

- First 3 experiments:
  1. Compare Backbone vs. Backbone + DAL to isolate the impact of domain adversarial learning
  2. Add MVWDL to Backbone + DAL and observe if domain-invariant alignment improves overall recall
  3. Evaluate the effect of UWL by comparing fixed loss coefficients vs. learned weights on domain-specific performance

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the limitations section, several unresolved issues remain:
- How does the performance change when new advertising entities are added incrementally?
- What is the optimal number of projection layers in the Multi-View Wasserstein Distribution Learning module?
- How does the model perform when domain distributions have significant overlap versus complete separation?

## Limitations

- Scalability concerns exist regarding computational complexity of multi-view Wasserstein alignment and adversarial training components at production scale
- Ablation studies don't fully isolate individual contributions of each RRL component across all domains
- Limited correlation between offline recall improvements and online metrics (only RPM and Cost reported)

## Confidence

High Confidence: The claim that unified modeling in entity space can process multiple entity types without separate branches is well-supported by methodology and offline recall improvements across all four domains.

Medium Confidence: The effectiveness of domain adversarial learning and multi-view Wasserstein distribution learning for extracting domain-invariant representations is theoretically sound but relies heavily on proper hyperparameter tuning.

Low Confidence: The claim that this approach "alleviates unfair scoring across different ad entities" lacks quantitative evidence and supporting metrics.

## Next Checks

1. Conduct a comprehensive ablation study that isolates each RRL component (DAL, MVWDL, orthogonality, uncertainty weighting) individually across all four domains to determine their relative contributions and identify which components are most critical for different entity types.

2. Measure and report fairness metrics (such as score distribution similarity across entities or domain-specific recall parity) to empirically validate the claim of reduced unfair scoring between item ads, shop ads, short video ads, and live broadcast ads.

3. Evaluate the computational overhead of the RRL components (particularly MVWDL and orthogonality constraints) as the number of domains and entities scales beyond four, measuring both training time per epoch and inference latency to assess production viability.