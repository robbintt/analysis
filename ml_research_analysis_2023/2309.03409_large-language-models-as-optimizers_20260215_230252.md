---
ver: rpa2
title: Large Language Models as Optimizers
arxiv_id: '2309.03409'
source_url: https://arxiv.org/abs/2309.03409
tags:
- optimization
- palm
- instructions
- step
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OPRO, a method to use large language models
  (LLMs) as optimizers for tasks described in natural language. In each optimization
  step, the LLM generates new solutions based on a prompt containing previously generated
  solutions with their values, then evaluates and adds the new solutions to the prompt
  for the next step.
---

# Large Language Models as Optimizers

## Quick Facts
- arXiv ID: 2309.03409
- Source URL: https://arxiv.org/abs/2309.03409
- Reference count: 40
- Key outcome: OPRO method uses LLMs as optimizers for tasks in natural language, achieving up to 8% improvement on GSM8K and 50% on Big-Bench Hard tasks

## Executive Summary
This paper introduces OPRO (Optimization by PROmpting), a method that treats large language models as optimizers for tasks described in natural language. The approach iteratively generates new solutions by prompting the LLM with previously generated solutions and their values, then evaluates and adds new solutions to the prompt for subsequent steps. OPRO demonstrates effectiveness on mathematical optimization problems (linear regression, traveling salesman) and prompt optimization for NLP tasks, where it outperforms human-designed prompts on zero-shot evaluation.

## Method Summary
OPRO is an iterative optimization framework where an LLM generates new solutions based on a meta-prompt containing previously generated solutions with their objective scores. In each step, the LLM optimizer produces multiple candidate solutions, which are evaluated and added to the optimization trajectory in the meta-prompt. The process continues until convergence criteria are met. For prompt optimization tasks, the solutions are instruction prompts, and the objective is task accuracy. The method balances exploration and exploitation through temperature-controlled sampling and improves stability by generating multiple solutions per step.

## Key Results
- OPRO-optimized prompts outperform human-designed prompts by up to 8% zero-shot on GSM8K
- On Big-Bench Hard tasks, OPRO achieves up to 50% improvement over human prompts
- The method successfully solves linear regression and traveling salesman problems through iterative improvement
- Temperature tuning (0.0, 0.5, 1.0, 1.5, 2.0) enables control over exploration-exploitation balance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OPRO leverages in-context learning and trajectory-based optimization to iteratively improve solutions
- Mechanism: The LLM generates new solutions based on a meta-prompt containing previously generated solutions with their scores, sorted in ascending order. This allows the LLM to identify patterns among high-scoring solutions and build upon them.
- Core assumption: LLMs can effectively recognize and exploit patterns from in-context demonstrations of solution-score pairs
- Evidence anchors:
  - [abstract] "In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step."
  - [section 2.2] "The optimization trajectory includes past solutions paired with their optimization scores, sorted in the ascending order. Including optimization trajectory in the meta-prompt allows the LLM to identify similarities of solutions with high scores"
- Break condition: The LLM fails to generate meaningfully different solutions or gets stuck proposing variations of already-explored solutions

### Mechanism 2
- Claim: Temperature-controlled sampling balances exploration and exploitation in the solution space
- Mechanism: Lower temperatures encourage exploitation of nearby solution space, while higher temperatures enable exploration of more distant, potentially better solutions
- Core assumption: LLMs can effectively navigate the exploration-exploitation tradeoff through temperature adjustment
- Evidence anchors:
  - [section 2.3] "We tune the LLM sampling temperature to balance between exploration and exploitation. A lower temperature encourages the LLM to exploit the solution space around the previously found solutions and make small adaptations, while a high temperature allows the LLM to more aggressively explore solutions that can be notably different."
  - [section 5.3] "Optimizations with smaller temperatures (0.0 and 0.5) lack exploration and thus creativity, and the optimizer LLM often gets stuck at the same instruction for tens of steps"
- Break condition: The optimal temperature range is too narrow or the LLM's output distribution doesn't meaningfully change with temperature adjustments

### Mechanism 3
- Claim: Multi-solution generation per step improves optimization stability and convergence speed
- Mechanism: Generating multiple candidate solutions at each step allows the LLM to explore diverse possibilities simultaneously and quickly discover promising directions
- Core assumption: LLMs can generate multiple diverse, high-quality solutions when prompted appropriately
- Evidence anchors:
  - [section 2.3] "To improve stability, we prompt the LLM to generate multiple solutions at each optimization step, allowing the LLM to simultaneously explore multiple possibilities and quickly discover promising directions to move forward."
  - [section 5.3] "Computing a mini-batch of gradients reduces the variance of a stochastic gradient descent procedure. Similarly, generating multiple instructions in each step improves the optimization stability with LLMs."
- Break condition: The LLM consistently generates redundant or low-quality solutions, or the evaluation cost of multiple solutions outweighs the benefit

## Foundational Learning

- Concept: In-context learning and few-shot prompting
  - Why needed here: OPRO fundamentally relies on LLMs' ability to understand and follow instructions based on examples in the prompt
  - Quick check question: If you remove all solution-score pairs from the meta-prompt, will the LLM still generate reasonable solutions?

- Concept: Exploration-exploitation tradeoff in optimization
  - Why needed here: The temperature parameter and multi-solution generation are explicit mechanisms for managing this tradeoff
  - Quick check question: What happens to optimization performance if you set temperature to 0.0 throughout?

- Concept: Prompt sensitivity and robustness
  - Why needed here: The paper demonstrates that semantically similar prompts can have drastically different accuracies, making robust optimization challenging
- Quick check question: If you generate 8 solutions at each step, what's the probability that at least one is meaningfully better than the current best?

## Architecture Onboarding

- Component map: Meta-prompt generator -> LLM optimizer -> Evaluator -> Solution manager -> Meta-prompt update -> termination check
- Critical path: Meta-prompt → LLM optimizer → Evaluator → Solution manager → Meta-prompt update → termination check
- Design tradeoffs:
  - Trajectory length vs. context window limits
  - Number of solutions per step vs. evaluation cost
  - Temperature setting vs. exploration-exploitation balance
  - Exemplar selection strategy vs. prompt relevance
- Failure signatures:
  - Optimization curve plateaus early (exploration issues)
  - Large variance in per-step improvements (stability issues)
  - Instructions drift away from task requirements (prompt sensitivity)
  - Context window overflow (trajectory management issues)
- First 3 experiments:
  1. Linear regression with 1D variables, test convergence from random starting points
  2. GSM8K prompt optimization with empty string starting point, test improvement vs. baselines
  3. BBH optimization with varying temperatures, identify optimal temperature range

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between exploration and exploitation in OPRO, and how does this balance vary across different problem domains and LLM sizes?
- Basis in paper: [explicit] The paper discusses exploration-exploitation trade-off and mentions that different LLM sampling temperatures affect this balance, but doesn't provide a systematic analysis of optimal settings.
- Why unresolved: The paper only briefly mentions temperature tuning (0.0, 0.5, 1.0, 1.5, 2.0) without providing a comprehensive study of how these settings affect convergence speed and final solution quality across different problem types.
- What evidence would resolve it: A systematic study varying temperature settings across multiple problem domains (mathematical optimization, prompt optimization, etc.) with different LLM sizes, measuring convergence speed, solution quality, and stability metrics.

### Open Question 2
- Question: How does the sensitivity of OPRO to initialization affect its practical utility, and can this sensitivity be reduced?
- Basis in paper: [explicit] The paper discusses different starting points for prompt optimization and shows that initialization affects early optimization performance, but doesn't fully explore why or how to mitigate this.
- Why unresolved: The paper shows that different initial instructions lead to different optimization trajectories but doesn't investigate the underlying causes of this sensitivity or propose methods to make OPRO more initialization-agnostic.
- What evidence would resolve it: Experiments comparing OPRO performance from diverse initializations, analysis of why certain initializations lead to better outcomes, and proposed techniques to make OPRO less sensitive to starting points.

### Open Question 3
- Question: What are the fundamental limitations of LLMs as optimizers for complex optimization landscapes, and how can these be overcome?
- Basis in paper: [explicit] The paper discusses several failure cases including getting stuck at non-optimal points, difficulty navigating bumpy loss landscapes, and hallucinating function values.
- Why unresolved: While the paper identifies these limitations, it doesn't provide a comprehensive framework for understanding when LLMs will struggle as optimizers or propose systematic solutions to these fundamental challenges.
- What evidence would resolve it: A theoretical framework characterizing which types of optimization landscapes are tractable for LLMs, empirical studies identifying failure patterns, and proposed architectural or algorithmic modifications to address these limitations.

## Limitations

- The method is fundamentally limited by LLM context windows, restricting trajectory length and potentially causing loss of valuable optimization history
- Each optimization step requires full evaluation of generated solutions, creating computational bottlenecks especially for complex tasks like prompt optimization
- The approach inherits and amplifies the fundamental challenge that semantically similar prompts can have vastly different performances, making optimization non-smooth and potentially trapping the method in local optima

## Confidence

- **High confidence**: The basic OPRO framework works as described and produces measurable improvements on tested tasks
- **Medium confidence**: The temperature-based exploration-exploitation mechanism provides meaningful control over optimization behavior
- **Medium confidence**: Multi-solution generation improves optimization stability, though cost-benefit trade-offs need clarification

## Next Checks

1. **Context window stress test**: Run OPRO on a task requiring long optimization trajectories (e.g., 100+ steps) and measure performance degradation as the prompt approaches context limits. Track whether early solutions get "forgotten" and if optimization regresses.

2. **Temperature sensitivity analysis**: Systematically test OPRO across a broader temperature range (0.0 to 2.0) on GSM8K prompt optimization, measuring not just final accuracy but also convergence speed and solution diversity at each temperature setting.

3. **Cross-task transferability test**: Take an optimized prompt from one task (e.g., GSM8K) and evaluate its performance on a semantically similar but distinct task (e.g., MATH problems). Measure both zero-shot transfer performance and the number of optimization steps needed to adapt the prompt to the new task.