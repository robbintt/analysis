---
ver: rpa2
title: 'LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack'
arxiv_id: '2308.00319'
source_url: https://arxiv.org/abs/2308.00319
tags:
- adversarial
- attack
- limeattack
- examples
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LimeAttack, a novel hard-label attack algorithm
  for textual adversarial examples. Unlike previous methods, LimeAttack uses a local
  explainable method (LIME) to approximate word importance ranking, then employs beam
  search to find optimal perturbations.
---

# LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack

## Quick Facts
- **arXiv ID:** 2308.00319
- **Source URL:** https://arxiv.org/abs/2308.00319
- **Reference count:** 40
- **Key outcome:** LimeAttack achieves up to 49.9% attack success rate on text classification tasks with 5.3% perturbation rate under 100 query budget.

## Executive Summary
LimeAttack introduces a novel hard-label adversarial attack method for textual data that uses LIME (Local Interpretable Model-agnostic Explanations) to estimate word importance in settings where traditional gradient-based methods are unavailable. The method combines LIME-based word ranking with beam search and semantic similarity guidance to find high-quality adversarial examples. Experiments demonstrate that LimeAttack outperforms existing hard-label attack methods, achieving higher success rates with fewer queries while producing more semantically similar adversarial examples.

## Method Summary
LimeAttack operates in a hard-label setting where the attacker can only query a model and receive discrete predictions, without access to gradients or confidence scores. The method consists of two main phases: first, LIME is used to approximate word importance by generating neighborhood samples through random word deletion and fitting a linear model to predict discrete labels. Second, beam search with semantic similarity guidance iteratively replaces important words with synonyms to find adversarial examples that maximize attack success while maintaining semantic similarity to the original text. The approach is specifically designed for textual data and addresses the challenges of discrete input spaces.

## Key Results
- LimeAttack achieves up to 49.9% attack success rate on text classification tasks with only 5.3% perturbation rate under a 100 query budget.
- The method outperforms baseline hard-label attacks (HLBB, TextHoaxer, LeapAttack, TextHacker) across multiple datasets including MR, SST-2, AG, Yahoo, SNLI, and MNLI.
- Adversarial examples generated by LimeAttack demonstrate strong transferability across different model architectures and improve model robustness when used in adversarial training.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LimeAttack uses LIME to estimate word importance in a hard-label setting where traditional deletion-based methods fail.
- Mechanism: LIME generates neighborhood samples by randomly removing words from the benign sample, then fits a linear model to predict discrete labels. The learned linear coefficients approximate word importance.
- Core assumption: The contribution of all words to the model's prediction is additive, allowing linear approximation to capture importance.
- Evidence anchors: [abstract] "We focus on a more realistic and challenging setting, named hard-label attack, in which the attacker can only query the model and obtain a discrete prediction label." [section] "LimeAttack follows two steps in score-based attack, i.e., word importance ranking and perturbation execution. Firstly, we use LIME to calculate the word importance ranking."
- Break condition: If word interactions are highly non-additive, linear approximation fails to capture true importance.

### Mechanism 2
- Claim: Beam search with semantic similarity guidance finds higher-quality adversarial examples than random initialization methods.
- Mechanism: After ranking words by importance, LimeAttack iteratively replaces words with synonyms, selecting perturbations that maximize semantic similarity to the original while attempting to change the model's prediction.
- Core assumption: Semantic similarity between adversarial and benign examples correlates with human perception of quality and fluency.
- Evidence anchors: [abstract] "LimeAttack substitutes origin word with its synonyms in the descending order of word importance ranking and adopts beam search to search the better adversarial example." [section] "We adopted beam search with a completely different motive: using semantic similarity between the adversarial and benign samples to guide the search direction."
- Break condition: If semantic similarity metric doesn't align with human perception, generated examples may be low-quality.

### Mechanism 3
- Claim: Hard-label attacks are more realistic than white-box or score-based attacks because real-world APIs don't expose model internals.
- Mechanism: By only querying discrete labels, LimeAttack mimics real-world attack scenarios where attackers have no access to gradients, confidence scores, or model parameters.
- Core assumption: Most deployed NLP models are accessed through APIs that only return predictions, not internal information.
- Evidence anchors: [abstract] "Previous textual adversarial attacks adopt gradients or confidence scores to calculate word importance ranking and generate adversarial examples. However, this information is unavailable in the real world." [section] "In a hard-label setting, the attacker can only query the victim model and get a discrete prediction label. Therefore, hard-label setting is more practical and challenging."
- Break condition: If API access changes to provide more information, this assumption becomes invalid.

## Foundational Learning

- Concept: LIME (Local Interpretable Model-agnostic Explanations)
  - Why needed here: LIME provides a way to estimate feature importance when model internals are unavailable, making it suitable for hard-label attacks.
  - Quick check question: How does LIME generate neighborhood samples, and why is this approach appropriate for discrete text data?

- Concept: Beam search
  - Why needed here: Beam search explores multiple perturbation paths simultaneously, balancing exploration and exploitation in the discrete text space.
  - Quick check question: What's the difference between greedy search and beam search in the context of generating adversarial examples?

- Concept: Semantic similarity metrics
  - Why needed here: Semantic similarity helps ensure generated adversarial examples remain fluent and preserve original meaning while being adversarial.
  - Quick check question: How does Universal Sentence Encoder (USE) calculate semantic similarity between sentences?

## Architecture Onboarding

- Component map:
  Input -> LIME module -> Candidate generation -> Beam search engine -> Output

- Critical path:
  1. Calculate word importance using LIME
  2. Generate candidate synonym sets
  3. Perform beam search with semantic similarity optimization
  4. Return successful adversarial example

- Design tradeoffs:
  - LIME vs deletion-based importance: LIME works in hard-label settings but requires more queries
  - Beam size vs query efficiency: Larger beam sizes improve quality but increase query count
  - Semantic similarity vs attack success: Optimizing for similarity may reduce attack success rate

- Failure signatures:
  - Low attack success rate: Word importance ranking may be inaccurate, or semantic similarity metric doesn't align with model's decision boundary
  - High perturbation rate: Beam search parameters may need tuning, or synonym candidates are poor quality
  - Excessive queries: LIME neighborhood generation may be inefficient, or beam search explores too many paths

- First 3 experiments:
  1. Test LIME importance ranking against random word selection on a small dataset
  2. Vary beam size (b=5, b=10, b=15) and measure impact on success rate and query count
  3. Compare semantic similarity optimization against random sampling in beam search

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LimeAttack scale with increasingly complex or domain-specific vocabulary, such as medical or legal texts?
- Basis in paper: [inferred] The paper evaluates on common NLP datasets but does not address domain-specific vocabulary complexity.
- Why unresolved: The experiments focus on general text classification and entailment datasets, leaving the effectiveness of LimeAttack on specialized domains untested.
- What evidence would resolve it: Experiments applying LimeAttack to domain-specific datasets (e.g., medical or legal corpora) with quantitative comparisons to baseline methods.

### Open Question 2
- Question: What is the impact of different distance metrics (e.g., Levenshtein vs. Jaccard) on the perturbation rate and semantic similarity of adversarial examples?
- Basis in paper: [explicit] The paper uses cosine similarity for semantic similarity and mentions edit distance for perturbation rate but does not explore alternative distance metrics.
- Why unresolved: The choice of distance metric could significantly affect the quality and efficiency of adversarial examples, but the paper does not investigate this.
- What evidence would resolve it: Comparative analysis of LimeAttack using different distance metrics, showing their effects on perturbation rate, semantic similarity, and attack success rate.

### Open Question 3
- Question: How does LimeAttack perform against ensemble models or models with defense mechanisms like adversarial training?
- Basis in paper: [explicit] The paper evaluates transferability and adversarial training but does not test against ensemble models or advanced defense mechanisms.
- Why unresolved: Real-world applications often use ensemble models or defenses, but the paper's scope is limited to single models and basic adversarial training.
- What evidence would resolve it: Experiments testing LimeAttack against ensemble models and models with state-of-the-art defense mechanisms, measuring attack success rates and transferability.

### Open Question 4
- Question: What is the trade-off between the number of queries and the quality of adversarial examples when using different beam sizes in LimeAttack?
- Basis in paper: [explicit] The paper discusses the effect of beam size on attack success rate and perturbation rate but does not provide a detailed trade-off analysis.
- Why unresolved: While the paper shows the impact of beam size, it does not quantify the relationship between query count and adversarial example quality across different beam sizes.
- What evidence would resolve it: A detailed analysis plotting query count against adversarial example quality (e.g., perturbation rate, semantic similarity) for varying beam sizes.

## Limitations
- The computational efficiency of LIME-based word importance ranking requires generating multiple neighborhood samples, with the paper reporting average 54 queries per example but lacking variance analysis.
- The quality of synonym candidates significantly impacts attack success, with the paper using WordNet and thesaurus but not evaluating candidate quality's effect on final results.
- The method's performance on domain-specific vocabulary (medical, legal, technical texts) remains untested, limiting applicability to specialized domains.

## Confidence
- **High confidence:** Hard-label attack setting is more realistic than white-box approaches
- **Medium confidence:** LIME provides effective word importance ranking in hard-label settings
- **Medium confidence:** Beam search with semantic similarity guidance produces high-quality adversarial examples

## Next Checks
1. Implement ablation study removing LIME component to quantify its contribution to attack success rate
2. Measure runtime efficiency and query distribution across different text lengths and dataset types
3. Test transferability of adversarial examples to models with different architectures (CNN, LSTM, BERT) to validate robustness claims