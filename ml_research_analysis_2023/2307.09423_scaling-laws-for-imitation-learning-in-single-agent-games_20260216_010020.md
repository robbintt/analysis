---
ver: rpa2
title: Scaling Laws for Imitation Learning in Single-Agent Games
arxiv_id: '2307.09423'
source_url: https://arxiv.org/abs/2307.09423
tags:
- flops
- figure
- loss
- scaling
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the scaling behavior of imitation learning
  (IL) and reinforcement learning (RL) in the challenging game of NetHack. The authors
  train a suite of neural NetHack agents with varying model sizes using behavioral
  cloning (BC) to imitate an expert policy, and analyze the loss and mean return as
  functions of the compute budget (FLOPs).
---

# Scaling Laws for Imitation Learning in Single-Agent Games

## Quick Facts
- arXiv ID: 2307.09423
- Source URL: https://arxiv.org/abs/2307.09423
- Authors: 
- Reference count: 40
- Key outcome: The paper investigates scaling behavior of imitation learning and reinforcement learning in NetHack, finding power-law relationships between model size, data size, compute budget, and performance metrics.

## Executive Summary
This paper investigates scaling laws for imitation learning (IL) and reinforcement learning (RL) in the challenging game of NetHack. The authors train neural NetHack agents of varying sizes using behavioral cloning to imitate expert policies, and analyze how loss and mean return scale with compute budget. They find smooth power-law relationships between model size, data size, compute, and performance. These scaling laws enable predicting optimal model sizes for a given compute budget and forecasting the performance of future, larger models. The authors also extend their analysis to RL using IMPALA, finding similar scaling behavior. Using their scaling laws, they forecast and train several NetHack agents, achieving performance that outperforms prior state-of-the-art by up to 2x.

## Method Summary
The authors train a suite of neural NetHack agents with varying model sizes (10k to 500M parameters) using behavioral cloning to imitate expert policies from the NLD-AA dataset. They analyze the loss and mean return as functions of the compute budget (FLOPs) and find power-law scaling relationships. To predict optimal model and data sizes from compute budget, they use isoFLOP profiles and parametric fits. They extend their analysis to RL using IMPALA with LSTM models of 100k to 50M parameters. Using their scaling laws, they forecast and train several NetHack agents with IL and RL, achieving performance that outperforms prior state-of-the-art.

## Key Results
- IL loss and mean return scale smoothly with compute budget following power laws.
- Optimal model and data size can be predicted from compute budget using isoFLOP profiles and parametric fits.
- Improvements in loss predictably translate to improvements in environment return.
- Scaling laws enable forecasting and training of high-performing NetHack agents that outperform prior state-of-the-art by up to 2x.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling laws in imitation learning arise from smooth, predictable relationships between model size, data size, and performance metrics.
- Mechanism: As model and data size increase, both the cross-entropy loss and the environment return follow power-law scaling with compute budget, modeled as Cobb-Douglas production functions.
- Core assumption: The relationship between inputs (model size, data size) and outputs (loss, return) is smooth and continuous.
- Evidence anchors:
  - [abstract] "IL loss and mean return scale smoothly with FLOPs and are strongly correlated, resulting in power laws for training compute-optimal IL agents."
  - [section] "We find all of them follow a clear power law trend."
- Break condition: If the relationship between model size, data size, and performance becomes non-smooth or exhibits sharp inflection points.

### Mechanism 2
- Claim: Optimal model and data size can be predicted from compute budget using isoFLOP profiles and parametric fits.
- Mechanism: By training models across a wide range of compute budgets and fitting parabolas to the loss-return curves at each budget, the optimal model and data sizes can be extracted, following power laws with respect to compute budget.
- Core assumption: The loss-return relationship for a given compute budget is well-approximated by a parabola, and the optima of these parabolas follow predictable scaling.
- Evidence anchors:
  - [section] "We observe clear parabolas with well-defined minima at the optimal model size for a given compute budget."
  - [section] "These regressions give rise to the following power laws..."
- Break condition: If the loss-return relationship deviates significantly from parabolic at certain compute budgets, or if the power laws do not hold across a wider range of budgets.

### Mechanism 3
- Claim: Improvements in loss predictably translate to improvements in environment return.
- Mechanism: The optimal cross-entropy loss and the mean return in the environment are strongly correlated, with a power-law relationship.
- Core assumption: The relationship between loss and return is monotonic and can be modeled as a power law.
- Evidence anchors:
  - [abstract] "We then relate the cross-entropy loss of our trained BC agents to their respective mean return when rolled out in the environment, and find that the mean return follows a power law with respect to the optimal cross-entropy loss."
  - [section] "We find a power law of the form R_opt ∝ L_opt^δ, as shown in Figure 3a, where δ = -2.42."
- Break condition: If the relationship between loss and return becomes non-monotonic or if the power-law relationship breaks down at certain performance levels.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: The paper models the NetHack environment as a POMDP, which accounts for the partial observability and stochasticity of the game.
  - Quick check question: What are the key components of a POMDP, and how do they differ from a regular MDP?

- Concept: Behavioral Cloning (BC)
  - Why needed here: The paper uses BC to train agents by imitating expert trajectories, and analyzes how scaling affects BC performance.
  - Quick check question: How does BC differ from other imitation learning methods like Inverse Reinforcement Learning (IRL)?

- Concept: Power Laws and Scaling
  - Why needed here: The paper's main contribution is showing that IL performance scales according to power laws with respect to model size, data size, and compute budget.
  - Quick check question: What is a power law, and why are they useful for characterizing scaling behavior in machine learning?

## Architecture Onboarding

- Component map: Data preprocessing -> Model architecture (LSTM + ResNet encoder + MLPs) -> Training loop (BC or RL) -> Evaluation (roll out in NetHack) -> Analysis (isoFLOP profiles + power law regressions)

- Critical path: 
  1. Preprocess data into training format
  2. Define model architecture and hyperparameters
  3. Train model using BC or RL
  4. Evaluate model in NetHack environment
  5. Analyze scaling behavior using isoFLOP profiles and power law regressions

- Design tradeoffs:
  - Model size vs. training time and memory usage
  - Data size vs. computational cost and potential for overfitting
  - Hyperparameter tuning vs. computational budget for running experiments

- Failure signatures:
  - Poor scaling behavior (deviation from power laws)
  - High variance in results, especially for RL experiments
  - Models not recovering expert behavior despite scaling up

- First 3 experiments:
  1. Train a small BC model (e.g., 10k parameters) on a subset of the data and evaluate its return in NetHack.
  2. Train a medium-sized BC model (e.g., 1M parameters) on a larger subset of the data and compare its return to the small model.
  3. Train a large BC model (e.g., 100M parameters) on the full dataset and analyze its scaling behavior compared to the smaller models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scaling behavior of imitation learning change when using human demonstrations instead of expert (AutoAscend) demonstrations?
- Basis in paper: [explicit] The authors mention that using human data from the NLD-NAO dataset would require handling the lack of actions, similar to BCO [27], and that investigating this could be interesting due to the diversity of the human dataset and the presence of successful trajectories.
- Why unresolved: The authors did not investigate this due to the additional complexity of handling missing actions in human data.
- What evidence would resolve it: Running experiments with human demonstrations and analyzing the scaling laws in terms of loss and return, comparing them to the expert demonstration results.

### Open Question 2
- Question: How does the scaling behavior of reinforcement learning with pretraining (kickstarted policies) differ from policies trained from scratch?
- Basis in paper: [explicit] The authors mention that promising neural methods for NetHack leverage pre-trained policies that are then fine-tuned with RL, and that it would be interesting to analyze the scaling behaviors for these kickstarted policies.
- Why unresolved: The authors only analyzed RL policies trained from scratch and did not investigate the scaling behavior of pretraining + RL approaches.
- What evidence would resolve it: Running experiments with RL policies initialized with pre-trained imitation learning models and analyzing the scaling laws in terms of return and interactions, comparing them to the from-scratch RL results.

### Open Question 3
- Question: What is the optimal number of samples and parameters required to achieve average human performance in NetHack using reinforcement learning, and how does this compare to the authors' forecasts?
- Basis in paper: [explicit] The authors forecast the compute requirements for training an RL agent from scratch to achieve human-level performance on NetHack, but due to computational constraints, they did not train such an agent.
- Why unresolved: The authors could not train an RL agent with the forecasted compute budget due to resource limitations.
- What evidence would resolve it: Training an RL agent with the forecasted model size and number of interactions, and evaluating its performance against the average human score in NetHack.

## Limitations

- Dataset Size Scaling: The relationship between dataset size and performance beyond 150B samples remains unexplored, and the scaling laws may not hold for significantly larger datasets.
- Hyperparameter Sensitivity: The study uses fixed hyperparameters across all experiments, which may limit the optimality of the scaling laws discovered.
- Single Environment Focus: All experiments are conducted in NetHack, and the scaling laws may not generalize to other single-agent games or imitation learning tasks.

## Confidence

- High Confidence: The observation that imitation learning loss and return scale smoothly with compute budget in NetHack.
- Medium Confidence: The applicability of these scaling laws to reinforcement learning settings.
- Medium Confidence: The prediction accuracy of optimal model and data sizes from compute budget.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Re-run a subset of experiments with varied hyperparameters (learning rates, batch sizes, etc.) to determine how sensitive the scaling laws are to these choices and whether optimal hyperparameters follow their own scaling relationships.

2. **Cross-Environment Generalization**: Apply the scaling law methodology to a different single-agent game or imitation learning task (e.g., Atari games with human expert demonstrations) to test whether the observed power-law relationships hold across domains.

3. **Dataset Size Extrapolation**: Design an experiment to test the scaling laws at dataset sizes significantly larger than 150B samples, potentially using synthetic data generation or dataset augmentation techniques to explore whether the power-law relationships continue to hold.