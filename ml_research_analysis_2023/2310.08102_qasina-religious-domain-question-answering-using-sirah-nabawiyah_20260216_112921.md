---
ver: rpa2
title: 'QASiNa: Religious Domain Question Answering using Sirah Nabawiyah'
arxiv_id: '2310.08102'
source_url: https://arxiv.org/abs/2310.08102
tags:
- question
- answering
- dataset
- context
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QASiNa, a new dataset for Question Answering
  on Sirah Nabawiyah in Indonesian. The dataset comprises 500 question-answer pairs
  across 66 contexts from nine Indonesian Sirah Nabawiyah literature sources.
---

# QASiNa: Religious Domain Question Answering using Sirah Nabawiyah

## Quick Facts
- arXiv ID: 2310.08102
- Source URL: https://arxiv.org/abs/2310.08102
- Reference count: 40
- Key outcome: XLM-R achieves best performance (61.20 EM, 75.94 F1-Score) on QASiNa dataset, while ChatGPT models show excessive interpretation unsuitable for religious domains

## Executive Summary
This paper introduces QASiNa, a novel dataset for Question Answering on Sirah Nabawiyah in Indonesian, comprising 500 question-answer pairs across 66 contexts from nine Indonesian literature sources. The authors evaluate three Indonesian language models (mBERT, XLM-R, IndoBERT) fine-tuned with SQuAD-ID, finding XLM-R performs best. They also compare these models with GPT-3.5 and GPT-4, discovering that both ChatGPT models produce lower EM and F1-Score but higher Substring Match, indicating excessive interpretation that is unsuitable for religious domains where interpretation must be limited to qualified experts. The results demonstrate the need for domain-specific QA datasets and models in religious contexts.

## Method Summary
The authors created QASiNa dataset from Sirah Nabawiyah literature sources and evaluated three Indonesian language models (mBERT, XLM-R, IndoBERT) using transfer learning from SQuAD-ID. Models were fine-tuned with learning rate 2e-5 or 2e-6, batch sizes 8 or 16, weight decay 0.01, and 5 epochs. Performance was measured using Exact Match, F1-Score, and Substring Match metrics. The same evaluation was applied to ChatGPT-3.5 and GPT-4 to compare extractive versus generative approaches in religious domain QA.

## Key Results
- XLM-R achieves best performance with 61.20 EM, 75.94 F1-Score, and 70.00 Substring Match on QASiNa dataset
- ChatGPT models show lower EM and F1-Score but higher Substring Match, indicating excessive interpretation
- Performance varies significantly by question type, with "who" questions performing best (EM 65.49) and "when" questions performing worst (EM 55.00)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using Sirah Nabawiyah literature enables domain-specific evaluation of LLM interpretations while preserving Islamic source integrity
- Mechanism: Sirah Nabawiyah provides rich historical narratives without direct Qur'anic interpretation authority, making it ideal for testing LLM's ability to retrieve factual historical content without overstepping interpretive bounds
- Core assumption: The corpus contains sufficient structured narrative data to construct valid QA pairs that test both factual recall and boundary adherence for LLM interpretation
- Evidence anchors: [abstract] "Sirah Nabawiyah contains comprehensive explanation about history of Islam from before the birth of Prophet Muhammad until his passing"
- Break condition: If texts lack sufficient factual granularity or if LLM consistently generates hallucinated historical details beyond provided context

### Mechanism 2
- Claim: Transfer learning from SQuAD-ID to QASiNa improves performance over training from scratch
- Mechanism: Pre-training on general Indonesian QA data provides linguistic foundations that can be fine-tuned on specialized religious domain
- Core assumption: Linguistic patterns and QA task structure from SQuAD-ID transfer effectively to Sirah Nabawiyah domain
- Evidence anchors: [section] "We employ transfer learning methods for model training. We utilize mBERT, XLM-R, and IndoBERT with transfer learning from Indonesian translation of SQuAD v2.0"
- Break condition: If domain-specific vocabulary differs too significantly from SQuAD-ID patterns, causing catastrophic forgetting

### Mechanism 3
- Claim: Comparing extractive models with generative LLMs reveals interpretation boundaries and hallucination tendencies
- Mechanism: Evaluating both model types on identical data measures how much each stays within context boundaries versus generating interpretive content
- Core assumption: Gap between Exact Match/F1-Score and Substring Match indicates degree of interpretation or hallucination
- Evidence anchors: [abstract] "Both Chat GPT version returned lower EM and F1-Score with higher Substring Match"
- Break condition: If both model types show similar performance gaps, suggesting metric itself is flawed

## Foundational Learning

- Concept: Domain-specific QA dataset creation and validation
  - Why needed here: Demonstrates necessity of specialized datasets when general QA datasets don't capture domain-specific requirements for religious content
  - Quick check question: What validation steps were taken to ensure QASiNa dataset quality and how do they differ from standard QA dataset creation?

- Concept: Transfer learning implementation for low-resource domains
  - Why needed here: Uses transfer learning from SQuAD-ID to address limited religious QA data challenge
  - Quick check question: Which language models were used and what specific hyperparameters were tuned during transfer learning process?

- Concept: Evaluation metrics for interpretation assessment
  - Why needed here: Introduces Substring Match to detect excessive interpretation crucial for religious domains
  - Quick check question: How does Substring Match differ from traditional Exact Match and F1-Score metrics?

## Architecture Onboarding

- Component map: Data acquisition → Context retrieval → Question/answer generation → Dataset validation → Model training (transfer learning) → Model evaluation (extractive vs generative) → Analysis
- Critical path: Dataset creation → Model fine-tuning → Comparative evaluation → Interpretation boundary analysis
- Design tradeoffs: Indonesian Sirah Nabawiyah literature provides domain specificity but limits dataset size to 500 pairs; transfer learning helps but may not fully capture religious nuances
- Failure signatures: High Substring Match with low EM/F1-Score indicates excessive interpretation; poor performance on "when" questions suggests temporal reasoning challenges
- First 3 experiments:
  1. Fine-tune XLM-R with different learning rates (2e-5 vs 2e-6) on QASiNa to identify optimal configuration
  2. Test ChatGPT with different prompt engineering approaches to reduce interpretation
  3. Evaluate model performance across different question types to identify challenging types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would larger-scale QASiNa datasets impact performance of domain-specific QA models compared to general-purpose LLMs?
- Basis in paper: [explicit] Authors note "Further studies to advance this research could involve increasing the dataset size"
- Why unresolved: Current dataset size of 500 questions may be too small to fully demonstrate potential
- What evidence would resolve it: Comparative evaluation on larger dataset (5000+ questions)

### Open Question 2
- Question: What specific methods could improve LLM performance on extractive QA tasks in religious domains while maintaining strict adherence to source texts?
- Basis in paper: [explicit] Authors conclude current ChatGPT "is not suitable for finding extractive answers for questions related to religious domain"
- Why unresolved: Paper identifies problem but doesn't explore potential solutions like constrained decoding
- What evidence would resolve it: Experimental results showing improved LLM performance using techniques like context-based token filtering

### Open Question 3
- Question: How do different question types vary in difficulty for both extractive models and LLMs in religious domain QA?
- Basis in paper: [explicit] Authors provide detailed breakdown showing XLM-R performance varies by question type
- Why unresolved: While paper shows performance differences, it doesn't investigate why certain types are more challenging
- What evidence would resolve it: Analysis of error patterns across question types for both model types

## Limitations

- Limited dataset size: Only 500 question-answer pairs created from Sirah Nabawiyah literature
- Domain specificity constraint: Results may not generalize to other religious domains or languages beyond Indonesian
- Transfer learning dependency: Model performance heavily relies on quality and relevance of SQuAD-ID pre-training data

## Confidence

**High Confidence Claims:**
- XLM-R outperforms mBERT and IndoBERT on QASiNa dataset
- ChatGPT models produce higher Substring Match scores compared to extractive models
- Need for domain-specific QA datasets in religious contexts is demonstrated

**Medium Confidence Claims:**
- Transfer learning from SQuAD-ID effectively improves performance on religious QA tasks
- Gap between EM/F1-Score and Substring Match indicates interpretation boundaries
- Sirah Nabawiyah literature is appropriate for testing LLM interpretation boundaries

**Low Confidence Claims:**
- Specific performance metrics (61.20 EM, 75.94 F1-Score) will generalize to other religious QA tasks
- Exact mechanisms by which transfer learning benefits religious domain adaptation
- Long-term reliability of models trained on this specific dataset

## Next Checks

1. Expand QASiNa dataset size to 1000+ question-answer pairs and evaluate whether model performance improves proportionally
2. Test the same models on other religious QA datasets (Quranic Arabic Corpus, Hadith collections) to assess cross-domain generalization
3. Implement human evaluation of model outputs to validate whether high Substring Match scores correspond to appropriate interpretation levels in religious contexts