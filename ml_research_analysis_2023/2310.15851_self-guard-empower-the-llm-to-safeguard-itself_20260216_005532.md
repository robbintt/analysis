---
ver: rpa2
title: 'Self-Guard: Empower the LLM to Safeguard Itself'
arxiv_id: '2310.15851'
source_url: https://arxiv.org/abs/2310.15851
tags:
- harmful
- training
- jailbreak
- safety
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Self-Guard, a method that trains large language
  models (LLMs) to self-assess and tag their own responses as either [harmful] or
  [harmless]. This approach aims to enhance LLM safety by combining safety training
  and external safeguards, addressing the limitations of existing methods in adapting
  to new attack types and potential performance degradation.
---

# Self-Guard: Empower the LLM to Safeguard Itself

## Quick Facts
- **arXiv ID**: 2310.15851
- **Source URL**: https://arxiv.org/abs/2310.15851
- **Reference count**: 39
- **Primary result**: Self-Guard trains LLMs to tag their own outputs as harmful or harmless, reducing jailbreak attack success rates from 26.5% to 3.8% without degrading general capabilities.

## Executive Summary
Self-Guard is a method that trains large language models to self-assess and tag their own responses as either [harmful] or [harmless], enhancing LLM safety by combining safety training with external safeguards. The approach addresses limitations of existing methods in adapting to new attack types and potential performance degradation. Self-Guard employs a two-stage fine-tuning process: the first stage enhances the model's ability to identify harmful content, while the second stage instructs the model to consistently tag its own responses. Experiments demonstrate that Self-Guard effectively defends against various jailbreak attacks without causing performance regression in the LLM's general capabilities.

## Method Summary
Self-Guard uses a two-stage fine-tuning approach on base LLMs like Vicuna or LLaMA-2-Chat. Stage 1 fine-tunes the model on 3,500 positive and 3,500 negative toxicity samples to enhance harmfulness identification abilities. Stage 2 continues training for 10 additional epochs using 447 harmful and 447 harmless Q&A pairs synthesized from HarmfulQ and Alpaca datasets, teaching the model to tag its own outputs. During inference, a simple filter inspects the generated tag and blocks harmful outputs before they reach the user. The training uses supervised fine-tuning with AdamW optimizer, learning rates from 1e-5 to 1e-6, batch size 32, 2048 token truncation, and DeepSpeed Stage 3 acceleration.

## Key Results
- Attack Success Rate reduced from 26.5% to 3.8% on Typical Jailbreak dataset
- Attack Success Rate reduced from 42.7% to 6.5% on Wild Jailbreak dataset
- Performance degradation limited to fluctuations within 1% on Open LLM Leaderboard tasks
- Over-sensitivity issues observed in LLaMA-2-Chat were alleviated after Self-Guard training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-Guard trains LLMs to tag their own outputs as harmful or harmless, making jailbreak attacks harder to succeed.
- Mechanism: The LLM is fine-tuned to evaluate the harmfulness of its responses and append an explicit tag. During inference, a simple filter inspects the tag and blocks harmful outputs before they reach the user.
- Core assumption: The LLM's own judgment of its outputs is more stable and less manipulable than the raw content, which attackers can craft carefully.
- Evidence anchors: [abstract] "The experiment has demonstrated that SELF-GUARD is robust against jailbreak attacks." [section] "Self-Guard effectively combines the advantages of safety training and safeguard and addresses the drawbacks of these approaches."
- Break condition: If the LLM can be induced to misclassify harmful content as harmless (e.g., via adversarial prompts that manipulate reasoning about harm), the defense fails.

### Mechanism 2
- Claim: Fine-tuning the LLM to detect harm in its outputs does not degrade general capabilities.
- Mechanism: Two-stage fine-tuning is used: Stage 1 improves the LLM's ability to judge harmfulness of arbitrary text; Stage 2 teaches the LLM to apply this judgment to its own outputs. Both stages use supervised fine-tuning with generated labels and reasons, keeping the task within the LLM's existing capability envelope.
- Core assumption: Generating reasons for harmfulness is a natural extension of the LLM's generation abilities and does not interfere with its performance on unrelated tasks.
- Evidence anchors: [abstract] "evaluated the general capabilities of the LLM before and after safety training, providing evidence that SELF-GUARD does not result in the LLM's performance degradation." [section] "Table 7 presents the results on the Open LLM Leaderboard... fluctuations within 1% of their original values."
- Break condition: If the harm detection task requires learning new concepts that interfere with general reasoning, performance could degrade.

### Mechanism 3
- Claim: Self-Guard reduces over-sensitivity compared to traditional safety training.
- Mechanism: By encouraging the LLM to provide helpful responses and only tag harmfulness, the training avoids forcing the model to refuse too many benign queries. This contrasts with methods that rely heavily on refusal.
- Core assumption: Traditional safety training causes over-sensitivity because it directly penalizes or refuses many borderline cases; tagging instead allows the LLM to respond while marking potential issues.
- Evidence anchors: [abstract] "In sensitivity tests, SELF-GUARD not only avoids inducing over-sensitivity in LLM but also can even mitigate this issue." [section] "Table 8 presents the results of sensitivity testing... The sensitivity issues observed in LLaMA-2-Chat are alleviated after SELF-GUARD training."
- Break condition: If tagging introduces its own bias toward false positives, over-sensitivity could still occur.

## Foundational Learning

- Concept: Adversarial prompting and jailbreak attacks
  - Why needed here: Understanding how attackers manipulate LLM inputs to bypass safety controls is essential to appreciate why Self-Guard's output-side approach is effective.
  - Quick check question: What are the four main types of jailbreak attacks mentioned in the paper?

- Concept: Supervised fine-tuning (SFT) and instruction tuning
  - Why needed here: Self-Guard relies on SFT to teach the LLM to judge harmfulness and to tag its own outputs. Knowing how SFT works is key to understanding the training pipeline.
  - Quick check question: In Self-Guard's Stage 1, what is the target sequence for fine-tuning the LLM?

- Concept: Harmfulness detection and classification
  - Why needed here: The core capability being trained is the LLM's ability to distinguish harmful from harmless content, which underpins the tagging mechanism.
  - Quick check question: How does Self-Guard generate training data for harmfulness detection in Stage 1?

## Architecture Onboarding

- Component map:
  User query -> LLM with Self-Guard fine-tuning -> Filter (extracts tag, blocks if harmful) -> Safe response or warning

- Critical path:
  1. Receive user query
  2. LLM generates response with tag
  3. Filter checks tag
  4. Deliver safe output or warning

- Design tradeoffs:
  - Fine-tuning vs. external guardrail: Self-Guard avoids deploying a second model but requires modifying the LLM
  - Tagging vs. refusal: Tagging allows more responses but risks false negatives if tagging fails
  - Harmfulness detection scope: Self-Guard is strong on clear harm but weaker on edge cases like financial or health advice

- Failure signatures:
  - LLM tags harmful content as harmless (false negative)
  - LLM refuses to tag or tag extraction fails
  - Over-sensitivity leads to excessive refusals
  - Performance degradation on general tasks

- First 3 experiments:
  1. Test robustness on Typical Jailbreak dataset: measure ASR before/after Self-Guard
  2. Evaluate performance on Open LLM Leaderboard: compare accuracy across ARC, HellaSwag, MMLU, TruthfulQA
  3. Run sensitivity tests on XSTest and Alpaca-AIM: measure refusal rates to detect over-sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SELF-GUARD vary across different LLM architectures and sizes?
- Basis in paper: Inferred from the evaluation on Vicuna, LLaMA-2-Chat, and GPT-3.5
- Why unresolved: The paper only tests SELF-GUARD on a limited set of LLM architectures and sizes. It is unclear how well SELF-GUARD generalizes to other architectures and sizes.
- What evidence would resolve it: Evaluating SELF-GUARD on a wider range of LLM architectures and sizes would provide insights into its generalizability.

### Open Question 2
- Question: How does the performance of SELF-GUARD change with the size of the training dataset?
- Basis in paper: Inferred from the discussion of training data in Section 3.1 and 3.2
- Why unresolved: The paper does not provide a detailed analysis of the relationship between training data size and SELF-GUARD performance. It is unclear how much training data is needed for optimal performance.
- What evidence would resolve it: Conducting experiments with varying sizes of training data and analyzing the impact on SELF-GUARD performance would provide insights into the data requirements.

### Open Question 3
- Question: How does SELF-GUARD handle more complex and nuanced forms of harmful content?
- Basis in paper: Inferred from the discussion of harmful content detection in Section 3.1 and 3.2
- Why unresolved: The paper does not provide a detailed analysis of SELF-GUARD's performance on complex and nuanced forms of harmful content. It is unclear how well SELF-GUARD can handle more subtle forms of harmfulness.
- What evidence would resolve it: Evaluating SELF-GUARD on a dataset with complex and nuanced forms of harmful content would provide insights into its ability to handle such cases.

## Limitations

- Self-Guard's effectiveness on non-content safety domains like financial advice, health information, and legal guidance remains unexplored
- The approach requires modifying the base LLM through fine-tuning, which may not be feasible for all deployment scenarios
- The paper focuses on LLaMA-2-Chat and Vicuna models, leaving uncertainty about performance on larger models or different architectures

## Confidence

**High Confidence**: The effectiveness of Self-Guard in reducing ASR on jailbreak attacks (Section 4.2 shows significant reductions from 26.5% to 3.8% on Typical Jailbreak and from 42.7% to 6.5% on Wild Jailbreak).

**Medium Confidence**: The claim that Self-Guard does not cause performance degradation. While the paper shows minimal fluctuations within 1% on Open LLM Leaderboard tasks, this evaluation is limited to four specific benchmarks and may not capture all aspects of LLM performance.

**Medium Confidence**: The assertion that Self-Guard mitigates over-sensitivity compared to traditional safety training. The paper provides evidence from XSTest and Alpaca-AIM datasets, but the mechanism by which tagging reduces over-sensitivity is not fully explained, and edge cases may still trigger false positives.

## Next Checks

1. **Cross-Model Validation**: Test Self-Guard on different LLM architectures (e.g., GPT-3.5, GPT-4, Claude) to assess generalizability beyond LLaMA-2-Chat and Vicuna.

2. **Domain-Specific Safety Testing**: Evaluate Self-Guard's effectiveness on non-content safety domains such as financial advice, health information, and legal guidance to identify potential gaps in coverage.

3. **Adversarial Prompt Analysis**: Conduct a systematic analysis of adversarial prompts that specifically target the tagging mechanism to identify potential vulnerabilities that could be exploited to bypass the self-guard system.