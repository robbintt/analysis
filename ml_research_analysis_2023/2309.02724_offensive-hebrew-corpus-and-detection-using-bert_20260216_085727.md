---
ver: rpa2
title: Offensive Hebrew Corpus and Detection using BERT
arxiv_id: '2309.02724'
source_url: https://arxiv.org/abs/2309.02724
tags:
- offensive
- dataset
- language
- data
- tweets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new Hebrew dataset for detecting offensive
  language in tweets. The dataset contains 15,881 tweets manually annotated by bilingual
  speakers with labels such as abusive, hate, violence, pornographic, or none offensive.
---

# Offensive Hebrew Corpus and Detection using BERT

## Quick Facts
- arXiv ID: 2309.02724
- Source URL: https://arxiv.org/abs/2309.02724
- Reference count: 40
- AlephBERT outperforms HeBERT on Hebrew offensive detection tasks

## Executive Summary
This paper introduces a new Hebrew dataset for detecting offensive language in tweets, containing 15,881 manually annotated tweets. The dataset was used to fine-tune two Hebrew BERT models, HeBERT and AlephBERT, with AlephBERT showing superior performance across multiple datasets. The study demonstrates that AlephBERT's larger vocabulary and multi-corpora training contribute to its better generalization capabilities for offensive language detection tasks.

## Method Summary
The researchers collected and annotated 15,881 Hebrew tweets with offensive language labels, then fine-tuned both HeBERT and AlephBERT models using this dataset. The fine-tuning process involved binary classification of offensive versus non-offensive content, with evaluation conducted on both the new dataset and an existing Hebrew offensive language dataset (D_OLaH). The models were trained with standard BERT fine-tuning procedures using a dense linear layer for classification.

## Key Results
- AlephBERT achieved 86% accuracy on the new dataset, outperforming HeBERT's 81%
- AlephBERT trained on the new dataset achieved 69% accuracy on D_OLaH, while D_OLaH-trained models achieved only 57% on the new dataset
- HeBERT performance increased by 2% when combined with D_OLaH data, while AlephBERT showed no improvement or slight degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AlephBERT's larger vocabulary and multi-corpora training yields better performance on Hebrew offensive detection tasks
- Mechanism: AlephBERT uses a vocabulary size of 52K compared to HeBERT's 30K, and is trained on three datasets (OSCAR, Twitter, Wikipedia) versus HeBERT's two (OSCAR, Wikipedia). This broader exposure allows better generalization to varied Hebrew text styles, especially colloquial Twitter data
- Core assumption: Vocabulary coverage and training corpus diversity directly improve downstream task performance
- Evidence anchors:
  - [abstract] AlephBERT is trained on OSCAR corpus, Twitter (~6.9 GB), and Hebrew Wikipedia
  - [section] HeBERT trained on OSCAR (~9.8 GB) and Wikipedia (~650 MB)

### Mechanism 2
- Claim: Combining the new dataset with existing data boosts HeBERT but not AlephBERT performance
- Mechanism: HeBERT benefits from additional training data because it has lower baseline performance and higher variance; AlephBERT is already near-optimal and cannot improve further with additional data
- Core assumption: AlephBERT's larger training corpus makes it less sensitive to data addition effects
- Evidence anchors:
  - [abstract] HeBERT performance increases by 2% when combined with D_OLaH
  - [section] AlephBERT does not improve and may degrade when combining datasets

### Mechanism 3
- Claim: Dataset generalizability is better when training on the new dataset and testing on D_OLaH than vice versa
- Mechanism: The new dataset captures a broader distribution of offensive language patterns, making models trained on it more adaptable to unseen datasets
- Core assumption: The new dataset covers more diverse contexts and targets than D_OLaH
- Evidence anchors:
  - [abstract] Model trained on new data and tested on D_OLaH achieves 69% accuracy, while the reverse yields 57%
  - [section] D_OLaH model achieves 79% on its own data but only 57% on new data

## Foundational Learning

- Concept: Understanding Hebrew morphology and orthography
  - Why needed here: Hebrew is nonconcatenative and uses right-to-left script; tokenization must handle roots and patterns correctly
  - Quick check question: Can you identify the root and pattern in the word "בָּנִים" (children)?

- Concept: Fine-tuning transformer models for binary classification
  - Why needed here: The task is converting multi-class offensive labels into binary classification, requiring appropriate loss functions and evaluation metrics
  - Quick check question: What loss function would you use for a binary classification head on top of BERT?

- Concept: Dataset balancing and class imbalance handling
  - Why needed here: The dataset is highly imbalanced (1,200 offensive vs. 14,681 non-offensive); proper sampling and metrics are essential
  - Quick check question: Which metric (accuracy, F1, precision, recall) is most appropriate for imbalanced datasets?

## Architecture Onboarding

- Component map: Pre-trained BERT (HeBERT/AlephBERT) -> Dense linear layer -> Binary classification output
- Critical path: Data preprocessing -> BERT tokenization -> Forward pass -> Dense layer -> Sigmoid activation -> Binary prediction
- Design tradeoffs: HeBERT has smaller vocab and less data exposure vs. AlephBERT's larger vocab and multi-source training; choosing affects performance and training time
- Failure signatures: Overfitting on training data (high train accuracy, low test accuracy), underfitting (low accuracy on both), or distribution mismatch (good on train/test split but poor on external dataset)
- First 3 experiments:
  1. Train HeBERT on D1 only, evaluate on test split; baseline performance check
  2. Train AlephBERT on D1 only, compare against HeBERT; assess model capacity
  3. Train HeBERT on D5 (combined datasets), evaluate on both test sets; measure data addition impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do differences in cultural and linguistic understanding between annotators impact the quality and consistency of offensive language annotations?
- Basis in paper: [explicit] The paper mentions that annotators were selected based on their familiarity with Israeli culture and politics, and that the annotation process was challenging due to the need for understanding cultural context. The review section also notes that changes were made to targets, suggesting potential differences in interpretation
- Why unresolved: The paper mentions an external reviewer made changes but doesn't provide a full inter-annotator agreement evaluation or quantify the impact of cultural understanding on annotation consistency
- What evidence would resolve it: A comprehensive inter-annotator agreement study with multiple bilingual annotators from different backgrounds, along with a correlation analysis between cultural familiarity and annotation consistency

### Open Question 2
- Question: What specific linguistic features or patterns in Hebrew contribute most to the difficulty of detecting offensive language compared to other languages?
- Basis in paper: [inferred] The paper highlights that offensive language detection in Hebrew is lagging behind other languages, and that understanding cultural context is crucial. This suggests there may be unique linguistic challenges in Hebrew
- Why unresolved: The paper doesn't analyze the specific linguistic features that make Hebrew offensive language detection challenging, nor does it compare these features to those in other languages
- What evidence would resolve it: A detailed linguistic analysis comparing offensive language patterns in Hebrew to those in other languages, including an examination of specific Hebrew linguistic features (e.g., morphology, syntax, or cultural references) that complicate detection

### Open Question 3
- Question: How does the performance of AlephBERT on Hebrew offensive language detection compare to its performance on other Semitic languages or languages with similar linguistic features?
- Basis in paper: [explicit] The paper shows that AlephBERT outperforms HeBERT on Hebrew offensive language detection tasks, but doesn't compare its performance to other languages or language families
- Why unresolved: The paper focuses solely on Hebrew and doesn't provide a comparative analysis with other languages, particularly Semitic languages or those with similar features
- What evidence would resolve it: Performance benchmarks of AlephBERT on offensive language detection tasks in other Semitic languages (e.g., Arabic) or languages with similar features, along with a comparative analysis of the results

## Limitations
- Small sample size of 15,881 tweets may limit generalizability
- Subjective nature of offensive language annotation introduces inter-annotator variability
- Exclusive focus on Hebrew limits cross-linguistic applicability

## Confidence

**High confidence**: Core findings about AlephBERT outperforming HeBERT on Hebrew offensive detection tasks. Experimental setup is straightforward and results are consistent.

**Medium confidence**: Generalizability findings showing asymmetric performance between datasets. Results are compelling but could reflect dataset-specific factors rather than universal generalization properties.

**Low confidence**: Claim that combining datasets helps HeBERT but not AlephBERT. Interpretation assumes AlephBERT is already near-optimal without exploring different training strategies.

## Next Checks

1. **Cross-dataset annotation consistency check**: Manually sample 100 tweets from both datasets and have bilingual annotators label them using both annotation schemes to determine whether performance differences stem from true generalizability or annotation guideline differences.

2. **Error analysis on false positives/negatives**: Analyze the specific types of errors made by HeBERT versus AlephBERT on the test set, categorizing errors by offensive category to identify whether AlephBERT's advantage comes from better handling specific offensive categories.

3. **Bias and fairness audit**: Test both models on subsets of the dataset stratified by gender, ethnicity, and political content to identify whether the models show systematic biases against particular demographic groups or viewpoints.