---
ver: rpa2
title: 'DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within
  Datasets'
arxiv_id: '2308.09878'
source_url: https://arxiv.org/abs/2308.09878
tags:
- dataset
- samples
- loss
- object
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DatasetEquity, a novel approach for addressing
  data imbalance in machine learning by computing sample likelihoods based on image
  appearance. The method uses deep perceptual embeddings and clustering to estimate
  sample occurrence probabilities without relying on class labels.
---

# DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets

## Quick Facts
- arXiv ID: 2308.09878
- Source URL: https://arxiv.org/abs/2308.09878
- Reference count: 40
- Key outcome: Achieves over 200% AP gains on underrepresented classes (Cyclist) in KITTI dataset

## Executive Summary
DatasetEquity introduces a novel approach to address data imbalance in machine learning by computing sample likelihoods based on image appearance rather than class labels. The method uses deep perceptual embeddings and clustering to estimate sample occurrence probabilities, which are then utilized to reweight training losses via a proposed Generalized Focal Loss function. Experiments on autonomous driving vision datasets demonstrate significant performance improvements on underrepresented classes, particularly achieving over 200% AP gains on cyclist detection in the KITTI dataset.

## Method Summary
DatasetEquity extracts deep perceptual embeddings from raw image pixels, clusters them to estimate sample likelihoods, and uses these likelihoods to reweight the Generalized Focal Loss during training. This approach captures nuanced dataset imbalances beyond what class labels provide, addressing issues like occlusion, resolution, and context that traditional class-based methods miss. The method is particularly effective for smaller datasets and rare classes, improving state-of-the-art 3D object detection methods without requiring class label information.

## Key Results
- Over 200% AP gains on under-represented classes (Cyclist) in KITTI dataset
- Generalizable approach that complements existing techniques
- Particularly beneficial for smaller datasets and rare classes
- Improves state-of-the-art 3D object detection methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reweighting training loss by image-based sample likelihoods improves model performance on rare and out-of-distribution (OOD) classes.
- Mechanism: The method extracts deep perceptual embeddings from raw image pixels, clusters them to estimate sample occurrence probabilities without relying on class labels, and uses these likelihoods to reweight the Generalized Focal Loss.
- Core assumption: The likelihood of a sample occurring in the dataset, as estimated by clustering perceptual embeddings, correlates with the difficulty of the sample for the model.
- Evidence anchors:
  - [abstract] "The loss function improves state-of-the-art 3D object detection methods, achieving over 200% AP gains on under-represented classes (Cyclist) in the KITTI dataset."
  - [section] "By first embedding frames into a perceptual feature space and clustering based on image semantics, we can estimate sample occurrence probabilities without relying on class labels."

### Mechanism 2
- Claim: Using image appearance-based likelihoods instead of class labels captures more nuanced dataset imbalances.
- Mechanism: Clustering perceptual embeddings based on image appearance groups samples with similar visual characteristics, revealing fine-grained details like occlusion, resolution, and context that class labels alone miss.
- Core assumption: Image appearance contains information about dataset imbalance that is not captured by class labels.
- Evidence anchors:
  - [abstract] "Compared to categorical distributions using class labels, image appearance reveals complex relationships between objects beyond what class labels provide."
  - [section] "Weighting by image likelihood captures these nuances missed by class information alone."

### Mechanism 3
- Claim: The Generalized Focal Loss function effectively reweights samples based on their likelihoods.
- Mechanism: The Generalized Focal Loss function is designed to increase the loss for less likely samples and decrease it for more likely samples, encouraging the model to focus more on rare and OOD samples during training.
- Core assumption: The Generalized Focal Loss function can effectively modulate the training loss based on sample likelihoods.
- Evidence anchors:
  - [section] "The main contributions of this work are: Introduction of novel Generalized Focal Loss that reweights by computed likelihoods, improving modeling of rare and OOD classes."
  - [section] "Wgf l(p, η, γ) = η + (1 − p)γ η + 1 (4)"

## Foundational Learning

- Concept: t-SNE (t-Distributed Stochastic Neighbor Embedding)
  - Why needed here: t-SNE is used to project high-dimensional image embeddings onto a lower-dimensional space for clustering.
  - Quick check question: What is the primary purpose of using t-SNE in this method?

- Concept: DBSCAN (Density-Based Spatial Clustering of Applications to Noise)
  - Why needed here: DBSCAN is used to cluster the lower-dimensional t-SNE embeddings to estimate sample likelihoods.
  - Quick check question: What property of DBSCAN makes it suitable for clustering image embeddings in this method?

- Concept: Focal Loss
  - Why needed here: The Generalized Focal Loss function is an extension of the Focal Loss function, which is used to address class imbalance in object detection.
  - Quick check question: How does the Generalized Focal Loss function differ from the original Focal Loss function?

## Architecture Onboarding

- Component map: Image embedding extraction -> t-SNE projection -> DBSCAN clustering -> Likelihood computation -> Generalized Focal Loss integration
- Critical path: Extract image embeddings -> Project with t-SNE -> Cluster with DBSCAN -> Compute likelihoods -> Integrate with Generalized Focal Loss -> Train model
- Design tradeoffs:
  - Using a pre-trained model for image embeddings vs. training a model from scratch
  - Choosing the parameters for t-SNE and DBSCAN (e.g., perplexity, epsilon, min_samples)
  - Selecting the parameters for the Generalized Focal Loss function (e.g., η, γ)
- Failure signatures:
  - Poor performance on rare or OOD classes
  - High variance in the performance across different runs
  - Inability to improve upon the baseline model
- First 3 experiments:
  1. Implement the image embedding extraction and t-SNE projection on a small subset of the dataset.
  2. Apply DBSCAN clustering to the t-SNE embeddings and visualize the clusters.
  3. Integrate the Generalized Focal Loss function with a simple object detection model and train on the clustered dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of DatasetEquity's clustering-based likelihood estimation compare to class-based resampling techniques in terms of model performance and computational efficiency?
- Basis in paper: [explicit] The paper introduces DatasetEquity as an alternative to class-based resampling, which involves oversampling under-represented classes or undersampling over-represented classes. However, the paper does not provide a direct comparison between DatasetEquity and class-based resampling techniques.
- Why unresolved: The paper does not include a comparative analysis of DatasetEquity against class-based resampling methods, making it difficult to determine if the proposed method is superior or more efficient in terms of model performance and computational resources.
- What evidence would resolve it: Conducting experiments that directly compare DatasetEquity to class-based resampling techniques on the same datasets and tasks, while measuring both model performance (e.g., mAP, NDS) and computational efficiency (e.g., training time, memory usage).

### Open Question 2
- Question: Can DatasetEquity be effectively applied to other computer vision tasks beyond 3D object detection, such as image classification or semantic segmentation?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of DatasetEquity on 3D object detection tasks using autonomous driving datasets. However, the method's applicability to other computer vision tasks is not explicitly discussed.
- Why unresolved: The paper focuses on a specific application domain (autonomous driving) and does not explore the generalizability of DatasetEquity to other computer vision tasks.
- What evidence would resolve it: Extending the experiments to include other computer vision tasks, such as image classification or semantic segmentation, and evaluating the performance of DatasetEquity on these tasks using appropriate datasets and metrics.

### Open Question 3
- Question: How sensitive is DatasetEquity to the choice of hyperparameters, such as the clustering algorithm, the number of clusters, and the parameters of the Generalized Focal Loss function?
- Basis in paper: [explicit] The paper mentions the use of specific hyperparameters for clustering (e.g., epsilon and minimum samples for DBSCAN) and the Generalized Focal Loss function (e.g., eta and gamma values). However, it does not provide a comprehensive analysis of the sensitivity of DatasetEquity to these hyperparameters.
- Why unresolved: The paper does not explore the impact of different hyperparameter settings on the performance of DatasetEquity, making it difficult to determine the robustness of the method to hyperparameter choices.
- What evidence would resolve it: Conducting extensive experiments that systematically vary the hyperparameters of DatasetEquity and evaluate their impact on model performance across different datasets and tasks.

## Limitations

- The method's effectiveness on datasets with different visual characteristics, object distributions, or environmental conditions remains to be fully explored
- Sensitivity to hyperparameter choices, particularly for t-SNE and DBSCAN clustering, is not thoroughly investigated
- Limited exploration of the method's applicability to computer vision tasks beyond 3D object detection

## Confidence

**High Confidence:** The core mechanism of using perceptual embeddings to estimate sample likelihoods and reweight training losses is technically sound and well-supported by the experimental results. The over 200% AP gains on underrepresented classes in KITTI provide strong empirical validation of this approach.

**Medium Confidence:** The generalizability of DatasetEquity across different domains and dataset sizes is reasonably supported but requires further validation. While the paper shows benefits for smaller datasets and complements existing techniques, the extent of these benefits across diverse applications remains to be fully explored.

**Low Confidence:** The optimal parameter settings for t-SNE and DBSCAN clustering across different datasets are not thoroughly investigated. The sensitivity of the method to these parameters and their impact on performance could be significant but remains unclear.

## Next Checks

1. **Cross-Dataset Validation:** Test DatasetEquity on datasets from different domains (medical imaging, satellite imagery, etc.) to assess its generalizability beyond autonomous driving scenarios.

2. **Parameter Sensitivity Analysis:** Systematically evaluate how variations in t-SNE perplexity, DBSCAN epsilon, and minimum samples affect clustering quality and downstream model performance across different dataset sizes.

3. **Comparison with Class-Label Approaches:** Conduct head-to-head comparisons between DatasetEquity and traditional class-label-based reweighting methods on datasets where both approaches are applicable, measuring not just detection performance but also training stability and convergence speed.