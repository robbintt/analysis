---
ver: rpa2
title: On the Variance of Neural Network Training with respect to Test Sets and Distributions
arxiv_id: '2304.01910'
source_url: https://arxiv.org/abs/2304.01910
tags:
- variance
- training
- test-set
- which
- runs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies variance in test-set performance between independent
  runs of neural network training. Despite significant variance on test-sets, the
  author demonstrates that standard CIFAR-10 and ImageNet trainings have little variance
  in performance on the underlying test-distributions.
---

# On the Variance of Neural Network Training with respect to Test Sets and Distributions

## Quick Facts
- arXiv ID: 2304.01910
- Source URL: https://arxiv.org/abs/2304.01910
- Reference count: 30
- Key outcome: Despite significant variance on test-sets, standard CIFAR-10 and ImageNet trainings have little variance in performance on underlying test-distributions due to examplewise independence and ensemble calibration properties.

## Executive Summary
This work studies variance in test-set performance between independent runs of neural network training. The author demonstrates that despite significant variance on test-sets, standard CIFAR-10 and ImageNet trainings have little variance in performance on the underlying test-distributions. Through large-scale experiments with 60,000+ runs, they show that trainings make approximately independent errors on their test-sets, and prove that variance is a downstream consequence of the class-calibration property. The analysis yields a simple formula which accurately predicts variance for the binary classification case.

## Method Summary
The study conducts large-scale experiments training 60,000+ neural networks on CIFAR-10 and ImageNet using standard training procedures with SGD-momentum, random initialization, and data augmentation. The methodology involves controlling and varying sources of randomness (initialization, data ordering, augmentation) to measure their impact on variance. Statistical analysis is performed to test examplewise independence between test examples across runs, and variance estimation formulas are derived and validated against empirical results. The study uses ResNet architectures (9-layer for CIFAR-10, ResNet-18 for ImageNet) with fixed hyperparameters across all runs.

## Key Results
- Test-set variance between runs overestimates true distribution-wise variance due to finite sampling effects
- Varying a single weight at initialization produces approximately the same variance as all three sources of randomness combined
- Data augmentation reduces variance between training runs, suggesting it improves consistency of learned representations
- The proposed variance estimation formula accurately predicts observed variance for the binary classification case

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Test-set variance between runs overestimates true distribution-wise variance due to finite sampling
- Mechanism: Test-set accuracy is a sample statistic from a binomial-like distribution where individual example predictions vary independently between runs when training converges. The observed variance includes excess noise from finite test-set size that doesn't reflect genuine differences in model quality
- Core assumption: Hypothesis 1 (examplewise independence) - individual test examples vary independently in terms of being correctly classified between different training runs
- Evidence anchors: [abstract] "Despite having significant variance on their test-sets, we demonstrate that standard CIFAR-10 and ImageNet trainings have little variance in performance on the underlying test-distributions" and [section] "we observe the relation Cx776,y776 ⊥ Cx796,y796 (up to statistical significance across 60,000 runs)"
- Break condition: If test examples are not independent (e.g., highly correlated near-duplicates), Hypothesis 1 fails and the variance estimation becomes inaccurate

### Mechanism 2
- Claim: Ensemble calibration property inevitably implies positive test-set variance between training runs
- Mechanism: Well-calibrated ensembles require some variation between individual models' predictions. If all runs produced identical predictions, the ensemble would be overconfident on misclassified examples. The calibration constraint forces positive variance in test-set accuracy
- Core assumption: Hypothesis 2 (Ensemble calibration) - for every class j and confidence p, P(y=j | Pθ∼A(fθ(x)=j)=p) = p
- Evidence anchors: [abstract] "we prove that variance is unavoidable given the observation that ensembles of trained networks are well-calibrated" and [section] "Lakshminarayanan et al. (2017) observe that neural networks have the following property: if we collect an ensemble of many independently trained networks, then the uncertainty estimates of this ensemble will be roughly calibrated"
- Break condition: If ensemble calibration fails (e.g., poorly calibrated models), the variance bound doesn't hold

### Mechanism 3
- Claim: Sensitivity to initial conditions dominates variance more than specific sources of randomness
- Mechanism: Small perturbations at initialization (like changing a single weight by 0.1%) propagate through training to produce large differences in final predictions. Any single source of randomness suffices to generate full variance when training long enough, suggesting the training process itself has high sensitivity to initial conditions
- Core assumption: Training dynamics are chaotic with respect to initial conditions
- Evidence anchors: [section] "varying just a single weight at initialization produces only 1% less churn than all three sources of randomness combined" and "For long trainings of 32 epochs or more, varying only one of the three random factors produces approximately the same variance as the baseline"
- Break condition: If training becomes less sensitive to initial conditions (e.g., with stronger regularization), variance would decrease

## Foundational Learning

- Concept: Statistical independence and correlation
  - Why needed here: The core analysis relies on testing whether test examples vary independently between training runs (Hypothesis 1) and measuring correlations between different test sets
  - Quick check question: If two test examples have probabilities 0.6 and 0.7 of being correctly classified, what's the expected joint probability under independence? (Answer: 0.42)

- Concept: Calibration and expected calibration error (ECE)
  - Why needed here: The theoretical lower bound on variance requires understanding ensemble calibration and how ECE relates to variance
  - Quick check question: If an ensemble is perfectly calibrated, what should P(y=j | Pθ∼A(fθ(x)=j)=0.7) equal? (Answer: 0.7)

- Concept: Binomial distribution and variance estimation
  - Why needed here: The analysis compares observed test-set variance to binomial approximations and derives unbiased estimators for distribution-wise variance
  - Quick check question: For n=10,000 trials with p=0.94 accuracy, what's the binomial standard deviation? (Answer: √(0.94×0.06×10000) ≈ 75, or 0.75%)

## Architecture Onboarding

- Component map: Data augmentation -> Training runs -> Prediction collection -> Independence testing -> Variance estimation
- Critical path: (1) Generate training runs with varied random seeds, (2) Collect predictions on test sets, (3) Test Hypothesis 1 (independence), (4) Estimate distribution-wise variance using Theorem 2, (5) Validate calibration properties
- Design tradeoffs: High variance in individual runs is acceptable because the analysis focuses on aggregate statistics; computational cost of 60,000 runs is justified by the insights gained
- Failure signatures: If Hypothesis 1 fails (dependent examples found), variance estimates become unreliable; if ensemble calibration fails, theoretical bounds don't apply
- First 3 experiments:
  1. Run 1,000 CIFAR-10 trainings with fixed random seed to establish baseline deterministic behavior
  2. Vary single random source (e.g., only initialization) across 1,000 runs and measure variance reduction vs full randomness
  3. Test independence hypothesis by checking correlation between random pairs of test examples across 10,000 runs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the variance-reducing effect of data augmentation observed on CIFAR-10 and ImageNet generalize to other datasets and architectures?
- Basis in paper: [inferred] The paper shows that removing data augmentation increases variance between runs for CIFAR-10 training, and speculates this might be a general property of data augmentation.
- Why unresolved: The study only examines CIFAR-10 and ImageNet with specific architectures. Other datasets may have different characteristics that affect how augmentation impacts variance.
- What evidence would resolve it: Systematic experiments measuring variance between runs with and without data augmentation across diverse datasets (e.g., medical imaging, natural language) and architectures (e.g., transformers, recurrent networks).

### Open Question 2
- Question: Can the neural posterior correlation kernel (NPCK) be used to improve model selection or ensemble methods?
- Basis in paper: [explicit] The paper introduces NPCK as a tool for revealing structure in datasets based on correlations between predicted logits across training runs, but doesn't explore its practical applications.
- Why unresolved: The paper demonstrates NPCK's ability to identify visually similar image pairs and contrasts it with other embedding methods, but doesn't investigate whether this information could be leveraged for model selection or ensemble methods.
- What evidence would resolve it: Experiments showing that models or ensembles selected using NPCK-based criteria outperform those selected using traditional validation metrics, or that NPCK-informed ensembles achieve better performance than random or validation-based ensembles.

### Open Question 3
- Question: What is the relationship between model initialization methods and variance in test-set performance?
- Basis in paper: [explicit] The paper shows that varying a single weight at initialization produces substantial variance, but doesn't compare different initialization schemes (e.g., Xavier, Kaiming, orthogonal).
- Why unresolved: While the paper demonstrates that neural networks are highly sensitive to initial conditions, it doesn't investigate whether certain initialization methods can reduce this sensitivity and consequently lower variance.
- What evidence would resolve it: Comparative studies measuring variance between runs using different initialization schemes while keeping all other factors constant, across multiple architectures and datasets.

## Limitations
- The theoretical claims about ensemble calibration bounds and their relationship to variance lack strong corpus evidence beyond the author's empirical observations
- The analysis assumes training runs are long enough to reach convergence, which may not hold for all architectures or datasets
- The variance estimation formula's accuracy depends on having large test-sets (n>10,000), limiting its applicability to smaller datasets

## Confidence
- High confidence: The empirical observation that test-set variance overestimates true distribution-wise variance (supported by 60,000+ runs)
- Medium confidence: The sensitivity to initial conditions finding (supported by controlled experiments but lacks broader validation)
- Low confidence: The theoretical claims about ensemble calibration bounds and their relationship to variance (limited corpus evidence)

## Next Checks
1. Test Hypothesis 1 across diverse datasets (MNIST, CIFAR-100, etc.) to verify examplewise independence holds beyond CIFAR-10/ImageNet
2. Conduct ablation studies on the sensitivity claim by systematically varying initialization scale and observing variance changes
3. Validate the variance estimation formula on synthetic data where the true distribution-wise variance is known through simulation