---
ver: rpa2
title: 'Lift-Attend-Splat: Bird''s-eye-view camera-lidar fusion using transformers'
arxiv_id: '2312.14919'
source_url: https://arxiv.org/abs/2312.14919
tags:
- depth
- camera
- lidar
- features
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the role of monocular depth estimation
  in camera-lidar fusion for autonomous driving. It shows that recent Lift-Splat methods
  do not effectively leverage depth and that removing depth estimation does not degrade
  performance.
---

# Lift-Attend-Splat: Bird's-eye-view camera-lidar fusion using transformers

## Quick Facts
- arXiv ID: 2312.14919
- Source URL: https://arxiv.org/abs/2312.14919
- Reference count: 40
- Key outcome: Novel camera-lidar fusion method achieves 71.5 mAP and 73.6 NDS on nuScenes 3D object detection test set

## Executive Summary
This paper investigates the role of monocular depth estimation in camera-lidar fusion for autonomous driving. It demonstrates that recent Lift-Splat methods do not effectively leverage depth information, showing that removing depth estimation does not degrade performance. The authors propose Lift-Attend-Splat, a novel fusion method that bypasses monocular depth entirely and uses attention to select and fuse camera and lidar features in bird's-eye-view space. The method outperforms Lift-Splat baselines on nuScenes 3D object detection, particularly for far-away and small objects.

## Method Summary
The method uses a Dual-Swin-Tiny backbone with feature pyramid network for camera features and VoxelNet for lidar BEV features. Camera features are projected into BEV space using a transformer-based attention mechanism that learns to select camera features based on lidar context, bypassing depth estimation. The projection uses column-wise encoding of camera features and cross-attention with lidar frustum rays. Features are then concatenated and processed through a fusion module before detection. The approach uses TransFusion-L detection head and is trained with AdamW optimizer, incorporating augmentations and temporal feature aggregation.

## Key Results
- Achieves 71.5 mAP and 73.6 NDS on nuScenes 3D object detection test set
- Removing depth estimation does not degrade performance compared to Lift-Splat baselines
- Significant improvements for far-away and small objects (10-25% mAP improvement)
- Camera features contribute complementary information beyond what lidar provides

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Depth estimation in Lift-Splat fusion is not leveraged as expected.
- Mechanism: The model can project camera features at wrong depth locations without harming detection performance because lidar provides accurate depth.
- Core assumption: Lidar features dominate the feature fusion process, allowing the model to suppress incorrectly projected camera features.
- Evidence anchors:
  - [abstract] "recent Lift-Splat methods do not effectively leverage depth and that removing depth estimation does not degrade performance."
  - [section] "methods based on Lift-Splat perform equally well when the monocular depth prediction is replaced by direct depth estimation from the lidar point cloud or removed completely."
  - [corpus] Weak - no direct citations found for this specific claim about depth not being leveraged.
- Break condition: If lidar density drops significantly (e.g., very far objects), incorrect camera projections could degrade performance.

### Mechanism 2
- Claim: Attention-based projection allows camera features to contribute to multiple BEV locations.
- Mechanism: Cross-attention between camera columns and lidar frustum rays enables selective projection without depth normalization constraints.
- Core assumption: The transformer's attention mechanism can learn optimal feature routing based on lidar context.
- Evidence anchors:
  - [abstract] "bypasses monocular depth estimation altogether and instead selects and fuses camera and lidar features in a bird's-eye-view grid using a simple attention mechanism."
  - [section] "our approach, the attention between camera and lidar is such that the same camera feature can contribute fully to multiple locations in the BEV grid."
  - [corpus] Weak - no direct citations found for this specific attention-based projection mechanism.
- Break condition: If the attention mechanism overfits to training lidar patterns and fails to generalize.

### Mechanism 3
- Claim: Camera features complement lidar information in specific regions.
- Mechanism: The model learns to select camera features that fill gaps in lidar coverage, particularly for far-away and small objects.
- Core assumption: Camera features contain unique semantic information not captured by sparse lidar points.
- Evidence anchors:
  - [abstract] "showcases the role of attention as a key contributor to these improvements."
  - [section] "the bulk of the improvements comes from objects located at large distances and of small sizes."
  - [corpus] Weak - no direct citations found for this specific claim about complementary feature selection.
- Break condition: If camera semantic information becomes redundant with lidar point cloud quality improvements.

## Foundational Learning

- Concept: Bird's-eye-view (BEV) representation
  - Why needed here: All fusion operations occur in BEV space where lidar features are naturally represented
  - Quick check question: Why is projecting camera features into BEV advantageous compared to frustum space fusion?

- Concept: Transformer cross-attention
  - Why needed here: Enables dynamic selection of camera features based on lidar context without depth estimation
  - Quick check question: How does the attention mechanism avoid the quadratic complexity of full self-attention?

- Concept: Multi-modal feature fusion
  - Why needed here: Camera and lidar provide complementary information that must be combined effectively
  - Quick check question: What are the key differences between concatenation, gating, and attention-based fusion approaches?

## Architecture Onboarding

- Component map:
  Camera backbone (Dual-Swin-Tiny) -> Camera features (100x56xC) -> Column-wise encoding -> Cross-attention with lidar frustum -> BEV projection -> Concatenation with lidar -> Fusion module -> Detection head

- Critical path:
  Camera features → Column-wise encoding → Cross-attention with lidar frustum → BEV projection → Concatenation with lidar → Detection

- Design tradeoffs:
  - Single vs multiple transformer blocks: Single block provides good balance between quality and performance
  - Depth supervision vs no supervision: Removing depth supervision simplifies architecture without hurting performance
  - Static vs dynamic fusion: Attention-based fusion adapts to lidar context availability

- Failure signatures:
  - Poor detection on far-away objects: May indicate attention mechanism not learning proper feature routing
  - Inconsistent performance across object sizes: Could suggest fusion module not handling scale variations well
  - Degraded performance with sparse lidar: May indicate over-reliance on lidar context

- First 3 experiments:
  1. Compare single vs two transformer decoder blocks on validation mAP
  2. Test different fusion modules (add, cat+conv, gated) with frozen backbones
  3. Evaluate performance on objects at different distances from ego vehicle

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does improving monocular depth estimation not improve 3D object detection performance in camera-lidar fusion models?
- Basis in paper: [explicit] The paper shows that BEVFusion's performance does not improve with better depth prediction, and in fact degrades with increased depth supervision. Removing depth estimation altogether does not affect performance.
- Why unresolved: The exact mechanism by which depth information is being underutilized or bypassed is unclear. The model may be learning to rely primarily on lidar depth and suppress camera features projected at wrong locations, but the specific architectural or training dynamics are not fully understood.
- What evidence would resolve it: Detailed ablation studies analyzing the gradient flow, feature activations, and attention weights with/without depth supervision. Experiments comparing different depth prediction architectures and supervision strategies.

### Open Question 2
- Question: How does the attention mechanism in Lift-Attend-Splat enable better camera feature utilization compared to Lift-Splat's depth-based projection?
- Basis in paper: [explicit] The paper states that attention allows camera features to contribute to multiple BEV locations and leverages lidar context for feature selection, unlike depth-based projection which normalizes features along their ray.
- Why unresolved: The specific ways in which attention modulates feature contributions and how this leads to better object detection performance are not fully characterized. The interplay between attention, lidar context, and camera feature selection needs further investigation.
- What evidence would resolve it: Detailed analysis of attention weights, feature distributions, and detection performance across different object distances, sizes, and occlusion levels. Comparison of attention patterns with/without lidar features.

### Open Question 3
- Question: How does temporal feature aggregation (TFA) improve camera-lidar fusion performance, and what are its limitations?
- Basis in paper: [explicit] The paper shows that TFA improves detection performance, but the analysis is limited to a simple autoregressive approach and specific model configurations.
- Why unresolved: The optimal TFA strategy for camera-lidar fusion, including the number of frames, feature alignment, and fusion mechanisms, is not fully explored. The trade-offs between computational cost and performance gains are also not clear.
- What evidence would resolve it: Extensive ablation studies varying TFA parameters, comparing different feature alignment and fusion methods, and analyzing the impact on detection performance and computational efficiency.

## Limitations

- Depth estimation role remains unclear despite empirical findings that it can be removed
- Implementation details for column-frustum attention mechanism and projected horizon geometry are not fully specified
- Performance analysis lacks comprehensive ablation studies on attention mechanism behavior

## Confidence

- **High confidence**: Empirical finding that removing monocular depth estimation doesn't degrade performance (supported by validation on nuScenes test set with 71.5 mAP/73.6 NDS)
- **Medium confidence**: Proposed mechanism that attention allows camera features to contribute to multiple BEV locations without depth constraints
- **Medium confidence**: Claim that camera features complement lidar for far-away and small objects, based on qualitative improvements
- **Low confidence**: Understanding the exact reasons why depth estimation is not leveraged in Lift-Splat methods

## Next Checks

1. Conduct ablation studies varying lidar density/sparsity to test whether performance remains stable when lidar coverage decreases
2. Visualize and analyze attention maps across different object distances and sizes to verify the proposed complementary feature selection mechanism
3. Test the model's robustness to depth estimation errors by injecting synthetic depth noise and measuring performance impact