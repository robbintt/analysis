---
ver: rpa2
title: Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization
arxiv_id: '2305.01951'
source_url: https://arxiv.org/abs/2305.01951
tags:
- summarization
- knowledge
- future
- data
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TEMPO SUM, a benchmark for evaluating the
  temporal generalization of text summarization models. The authors argue that existing
  benchmarks overlap in time with pre-training and fine-tuning datasets, leading to
  models relying on memorized parametric knowledge rather than source content.
---

# Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization

## Quick Facts
- arXiv ID: 2305.01951
- Source URL: https://arxiv.org/abs/2305.01951
- Reference count: 33
- Key outcome: Pre-trained models generate more outdated hallucinations on future data than non-pretrained models

## Executive Summary
This paper introduces TEMPO SUM, a benchmark designed to evaluate the temporal generalization of text summarization models. The authors argue that existing benchmarks overlap in time with pre-training and fine-tuning datasets, leading models to rely on memorized parametric knowledge rather than source content. Through extensive human evaluation, they demonstrate that pre-trained models like PEGASUS generate more outdated hallucinations than non-pretrained models on future data, while faithfulness enhancement methods show limited effectiveness. The study highlights the importance of using temporally split test sets with knowledge conflicts to properly assess model generalization to evolving information.

## Method Summary
The authors create TEMPO SUM benchmark using news articles from 2010-2022, with future test sets containing knowledge conflicts where source documents present information contradicting pre-training knowledge. They compare PEGASUS (pre-trained) and Transformer (non-pretrained) models, evaluating faithfulness using automatic metrics (FactCC, QAFactEval) and human annotation of hallucinations. Two faithfulness enhancement methods (CLIFF and ENT) are also tested on the future test sets to assess their effectiveness on knowledge-conflict data.

## Key Results
- PEGASUS generates more outdated hallucinations than Transformer on future test sets (39.81% vs 33.33% on BBC)
- Automatic faithfulness metrics show weak correlation with human judgments on future data (correlation coefficients 0.44-0.61)
- Faithfulness enhancement methods (CLIFF, ENT) show limited effectiveness on future data with knowledge conflicts
- Models fine-tuned on XSum generate more hallucinations on future data due to source-target divergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training improves faithfulness on future data by teaching models to generate more meaningful and grammatical summaries.
- Mechanism: Pre-training allows models to learn general linguistic features and time-invariant world knowledge applicable across different temporal periods.
- Core assumption: Time-invariant world knowledge learned during pre-training is useful for generating faithful summaries on future data containing knowledge conflicts.
- Evidence anchors: [abstract] "Although the pre-training process of PEGASUS helps the model generates more meaningful and grammatical summaries..."; [section] "The pre-training process helps generate meaningful and grammatical summaries. On the BBC dataset, PEGASUS generates much fewer non-verifiable errors than Transformer..."
- Break condition: If time-invariant knowledge becomes outdated or is not useful for the specific domain of future data.

### Mechanism 2
- Claim: Pre-training encourages models to hallucinate outdated information based on parametric knowledge, harming faithfulness on future data.
- Mechanism: Parametric world knowledge memorized during pre-training leads models to rely on outdated facts when summarizing entities with evolving information.
- Core assumption: Parametric knowledge stored in PLMs is outdated for entities that have changed after pre-training data cutoff.
- Evidence anchors: [abstract] "Through extensive human evaluation, we show that parametric knowledge stored in summarization models significantly affects the faithfulness..."; [section] "Pre-training encourages a summarization model to hallucinate outdated information based on its parametric knowledge. We observe that on the future test set of BBC, PEGASUS generates substantially more outdated hallucinations than Transformer (39.81% vs. 33.33%)."
- Break condition: If models can effectively distinguish between parametric knowledge and source document information.

### Mechanism 3
- Claim: Source-target divergence in fine-tuning dataset encourages models to generate summaries with less reliance on source, harming temporal generalization.
- Mechanism: Reference summaries containing entities not present in source text teach models to hallucinate contents not inferable from articles.
- Core assumption: Divergence between source documents and reference summaries in fine-tuning dataset teaches models to rely on parametric knowledge rather than source content.
- Evidence anchors: [abstract] "Moreover, the knowledge memorized by PLMs may quickly become outdated, which affects the generalization performance of PLMs on future data."; [section] "The source-target divergence in the fine-tuning dataset encourages the model to generate summaries with less reliance on the source. Our evaluation results show that models that are fine-tuned on XSum generate more hallucinations on future data."
- Break condition: If fine-tuning dataset has minimal divergence between source documents and reference summaries.

## Foundational Learning

- Concept: Temporal generalization in text summarization
  - Why needed here: Understanding how well summarization models can generate faithful summaries on data from different time periods is crucial for real-world deployment.
  - Quick check question: What is the difference between temporal generalization and domain adaptation in text summarization?

- Concept: Parametric world knowledge in PLMs
  - Why needed here: Parametric world knowledge stored in PLMs can affect the faithfulness of generated summaries, especially on future data containing knowledge conflicts.
  - Quick check question: How does the cutoff date for pre-training data affect the parametric knowledge stored in a PLM?

- Concept: Knowledge conflicts in text summarization
  - Why needed here: Knowledge conflicts, where source document presents information that contradicts PLM's parametric knowledge, are essential for evaluating temporal generalization.
  - Quick check question: How can we identify knowledge conflicts in text summarization datasets?

## Architecture Onboarding

- Component map: TEMPO SUM benchmark (BBC, CNN articles 2010-2022) -> Summarization models (PEGASUS, Transformer, CLIFF, ENT) -> Evaluation (FactCC, QAFactEval, human evaluation)

- Critical path:
  1. Collect news articles from 2010 to 2022
  2. Identify evolving facts using Wikidata
  3. Select articles with knowledge conflicts for future test sets
  4. Fine-tune summarization models on existing datasets (XSum, CNN/DM)
  5. Evaluate models on TEMPO SUM benchmark using automatic metrics and human evaluation

- Design tradeoffs:
  - Using news articles as data source allows broad temporal range but may limit generalizability to other domains
  - Focusing on evolving facts related to politicians as proxy for knowledge conflicts may not capture all types of temporal changes
  - Human evaluation is essential for understanding hallucination types but is time-consuming and may have inter-annotator variability

- Failure signatures:
  - High rates of outdated hallucinations on future test sets indicate over-reliance on parametric knowledge
  - Low human correlations for faithfulness evaluation metrics suggest metrics are not effective at detecting hallucinations on future data
  - Inconsistent performance of faithfulness enhancement methods across datasets may indicate overfitting to training data

- First 3 experiments:
  1. Compare PEGASUS and Transformer performance on in-distribution vs future test sets of TEMPO SUM to understand pre-training impact on temporal generalization
  2. Evaluate CLIFF and ENT effectiveness on future test sets of TEMPO SUM to assess faithfulness improvement on knowledge-conflict data
  3. Measure human correlations of FactCC and QAFactEval on future test sets of TEMPO SUM to determine reliability of automatic faithfulness evaluation metrics on future data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively prevent summarization models from learning spurious correlations among entities during pre-training and fine-tuning?
- Basis in paper: [explicit] The paper discusses how spurious correlations learned from training data can affect temporal generalization ability of summarization models
- Why unresolved: The paper identifies this as key factor affecting temporal generalization but does not provide concrete solution to prevent learning of spurious correlations
- What evidence would resolve it: Evidence would include experimental results showing specific techniques or modifications to pre-training or fine-tuning process successfully reduce generation of outdated or incorrect information based on spurious correlations

### Open Question 2
- Question: Can we develop automatic faithfulness evaluation metrics that generalize well to future data?
- Basis in paper: [explicit] The paper shows existing faithfulness evaluation metrics like FactCC and QAFactEval have weak correlations with human judgments on future data
- Why unresolved: While paper highlights need for better metrics, it does not propose or test new metrics that could effectively evaluate faithfulness on future data
- What evidence would resolve it: Evidence would include development and validation of new automatic metrics that show strong correlations with human judgments on both current and future data

### Open Question 3
- Question: How does temporal generalization ability of summarization models vary across different domains beyond news?
- Basis in paper: [explicit] The paper focuses on news domain for its benchmark and analysis, but acknowledges need for empirical studies in other domains like dialogue and scientific domains
- Why unresolved: Study is limited to news domain, leaving open questions about how well findings apply to other types of text
- What evidence would resolve it: Evidence would include experiments applying same methodology to benchmarks from other domains, comparing performance and challenges across different types of text

### Open Question 4
- Question: What are the impacts of using more recent pre-training data on temporal generalization ability of summarization models?
- Basis in paper: [explicit] The paper notes that most existing pre-trained language models are trained on data up to 2019, and TEMPO SUM is designed to evaluate models trained on this data
- Why unresolved: The paper does not explore how using more recent pre-training data affects models' ability to handle future data with evolving facts
- What evidence would resolve it: Evidence would include experiments comparing performance of models pre-trained on different time ranges of data, analyzing their ability to generate accurate summaries on future data with knowledge conflicts

## Limitations

- Small sample size: Analysis based on only 6-7 evolving entities across both BBC and CNN datasets
- Annotation reliability: Human evaluation methodology relies on Mechanical Turk annotations without reporting inter-annotator agreement scores
- Limited faithfulness methods: Only two faithfulness enhancement methods (CLIFF and ENT) were tested on future test sets

## Confidence

**High Confidence**: Automatic faithfulness metrics show weak correlation with human judgments on future test sets (correlation coefficients 0.44-0.61)

**Medium Confidence**: Pre-training encourages outdated hallucinations (modest difference: PEGASUS 39.81% vs Transformer 33.33% on future BBC data, small sample size)

**Low Confidence**: Recommendation that TEMPO SUM should be used as default benchmark for all summarization research (covers only two news domains and specific type of temporal shift)

## Next Checks

1. Replicate with expanded entity types: Conduct same analysis using additional entity categories beyond politicians (e.g., companies, sports teams, technology products) to verify whether pre-training harm effect generalizes across different types of evolving facts

2. Test inter-annotator reliability: Have multiple annotators independently label hallucinations in same sample of summaries to calculate Cohen's kappa or similar agreement metrics, establishing reliability of human evaluation methodology

3. Compare additional faithfulness methods: Evaluate 2-3 additional faithfulness enhancement techniques (such as fact-aware decoding or retrieval-augmented generation) on TEMPO SUM future test sets to determine if CLIFF and ENT are representative of field's capabilities