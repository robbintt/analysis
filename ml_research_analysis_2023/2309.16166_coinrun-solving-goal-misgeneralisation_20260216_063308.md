---
ver: rpa2
title: 'CoinRun: Solving Goal Misgeneralisation'
arxiv_id: '2309.16166'
source_url: https://arxiv.org/abs/2309.16166
tags:
- agent
- goal
- reward
- coin
- misgeneralisation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of goal misgeneralisation in
  AI alignment, specifically the CoinRun challenge where an agent learns to pursue
  a proxy goal (reaching the right side of the level) instead of the true goal (collecting
  the coin). The authors demonstrate that the ACE (Algorithm for Concept Extrapolation)
  agent can solve this problem by learning multiple reward hypotheses and selecting
  the correct one without receiving any new reward information in the test environment.
---

# CoinRun: Solving Goal Misgeneralisation

## Quick Facts
- arXiv ID: 2309.16166
- Source URL: https://arxiv.org/abs/2309.16166
- Authors: Adam Gleave, Evan Hubinger, Neel Nanda, et al.
- Reference count: 26
- One-line primary result: ACE agent achieves 71.70% coin collection success rate vs 59.13% for standard agent when coin positions are randomized

## Executive Summary
This paper addresses the challenge of goal misgeneralisation in AI alignment, specifically the CoinRun challenge where agents learn to pursue proxy goals rather than true objectives. The authors demonstrate that their ACE (Algorithm for Concept Extrapolation) agent can successfully distinguish between the true reward function (collecting coins) and spurious reward functions (reaching the right side of the level) without receiving any new reward information in the test environment. The ACE approach achieves a 12.57 percentage point improvement over standard reinforcement learning agents and a 16.20 percentage point improvement over baseline agents that simply move right.

## Method Summary
The method involves training an agent on environments where coins are always positioned on the right side, then testing on environments with randomized coin positions. The ACE agent first undergoes standard reinforcement learning training on the initial environment, then uses concept extrapolation to analyze unlabeled data from test environments to generate multiple reward hypotheses. It selects the most likely reward function and retrains on the test environment using this hypothesis, achieving superior performance in collecting coins despite never receiving explicit reward signals during the test phase.

## Key Results
- ACE agent achieves 71.70% success rate in collecting randomized coins vs 59.13% for standard agent
- ACE outperforms baseline "move right" agent by 16.20 percentage points (55.50% vs 71.70%)
- The agent successfully identifies R1 (collecting coin) as the true reward rather than R0 (reaching right)
- ACE demonstrates ability to "know" monsters are irrelevant once the coin is collected

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ACE agent avoids goal misgeneralization by learning multiple reward hypotheses and selecting the correct one through concept extrapolation.
- Mechanism: ACE uses unlabeled data from the test environment to generate multiple reward hypotheses (R0 and R1). It then compares these hypotheses against the known reward and non-reward patterns from the training environment to identify which hypothesis corresponds to the true reward function R (collecting the coin) rather than the spurious reward R' (reaching the right side).
- Core assumption: The true reward function can be distinguished from spurious reward functions by analyzing patterns in the training data without requiring any reward signals in the test environment.

### Mechanism 2
- Claim: The ACE agent achieves superior performance by actively exploring the test environment to gather evidence about reward function structure.
- Mechanism: Instead of passively observing, the ACE agent explores 50 different testing environments, moving mainly right and saving image histories of the first 50 time-steps. This exploration provides the necessary data for the ACE algorithm to distinguish between R and R' based on how the agent's actions correlate with potential reward patterns.
- Core assumption: Active exploration in the test environment can reveal structural differences between the true reward function and spurious reward functions, even without explicit reward signals.

### Mechanism 3
- Claim: The ACE agent's ability to "know" that monsters are irrelevant once the coin is collected demonstrates successful concept extrapolation beyond simple reward maximization.
- Mechanism: By correctly identifying R as the true reward function, the ACE agent can reason about the irrelevance of certain obstacles (monsters) once the primary goal is achieved. This shows that the agent has learned to extrapolate the concept of "coin collection" to include understanding of the environment's structure and how different elements relate to the goal.

## Foundational Learning

- Concept: Goal Misgeneralization
  - Why needed here: Understanding goal misgeneralization is crucial because the paper addresses this specific problem where agents learn proxy goals that work in training but fail in testing environments.
  - Quick check question: What is the difference between the true reward R and the behavioral objective R' in the CoinRun example?

- Concept: Concept Extrapolation
  - Why needed here: The ACE algorithm relies on concept extrapolation to extend learned concepts from training to new environments without additional reward information.
  - Quick check question: How does concept extrapolation help the ACE agent distinguish between R and R' in the test environment?

- Concept: Distribution Shift
  - Why needed here: The paper deals with out-of-distribution (OOD) generalization, where the test environment differs from the training environment in ways that break standard agent performance.
  - Quick check question: Why does the standard agent fail to collect coins when their positions are randomized, even though it performs well when coins are always on the right?

## Architecture Onboarding

- Component map: Training on E → Exploration of E' → ACE analysis → Hypothesis selection → Retraining on E' → Testing
- Critical path: Training → Exploration → Analysis → Selection → Retraining → Testing
- Design tradeoffs: The ACE approach trades computational complexity (generating and comparing multiple hypotheses) for robustness to goal misgeneralization. It also requires exploration time in the test environment before achieving good performance.
- Failure signatures: The agent gets stuck following the spurious reward R' (always moving right), fails to explore the test environment sufficiently, incorrectly selects R0 instead of R1, or cannot distinguish between reward functions due to insufficient training data.
- First 3 experiments:
  1. Implement the baseline agent that always moves right and verify it achieves ~55.50% coin collection rate.
  2. Train the standard RL agent on the CoinRun environment with coins always on the right, then test on randomized coin positions to confirm ~59.13% success rate.
  3. Implement the ACE algorithm to generate multiple reward hypotheses from unlabeled test data and verify it can correctly select R1 (collecting coin) over R0 (reaching right).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ACE algorithm's performance scale when applied to more complex environments with multiple competing reward functions beyond the simple CoinRun scenario?
- Basis in paper: [explicit] The paper demonstrates ACE's success on a simplified CoinRun problem with only two reward functions (R and R'). The authors mention "Beyond the goal misgeneralisation problem is the overall generalisation problem: how to get an AI to robustly extend its goals and capabilities to any new environment" suggesting interest in scaling to more complex scenarios.
- Why unresolved: The paper only tests ACE on the specific CoinRun environment with two reward functions. No evidence is provided about performance in environments with more complex reward structures, multiple competing objectives, or more sophisticated state spaces.
- What evidence would resolve it: Empirical testing of ACE on environments with multiple reward functions (e.g., 3+ competing objectives), more complex state-action spaces, and varying levels of reward correlation would demonstrate its scalability and limitations.

### Open Question 2
- Question: What is the theoretical bound on the number of reward hypotheses ACE can reliably distinguish in the absence of reward information in the test environment?
- Basis in paper: [explicit] The authors note that ACE "produces two hypotheses, R0 and R1, for what the 'true' reward function could be" but do not discuss theoretical limitations on the number of hypotheses it can handle or the conditions under which it might fail to identify the correct reward function.
- Why unresolved: The paper demonstrates ACE working with exactly two hypotheses but provides no theoretical framework for understanding how the algorithm performs when faced with multiple competing hypotheses or when the true reward function lies outside the hypothesis space.
- What evidence would resolve it: A mathematical analysis proving convergence bounds for ACE when distinguishing between n reward hypotheses, or empirical studies testing ACE with varying numbers of hypotheses (3, 5, 10+) to identify failure modes and performance degradation.

### Open Question 3
- Question: How does ACE handle reward functions that are not perfectly separable in the feature space, where multiple reward functions produce similar behavior patterns in the training environment?
- Basis in paper: [inferred] The paper assumes reward functions R and R' are distinguishable in the training environment (getting to the right equals getting the coin), but doesn't address scenarios where different reward functions produce nearly identical behavior patterns, making them difficult to disambiguate.
- Why unresolved: The CoinRun example presents a clear case where R and R' are easily separable, but real-world reward functions often have complex, overlapping behaviors that make them difficult to distinguish without additional information or constraints.
- What evidence would resolve it: Experiments demonstrating ACE's performance on environments where reward functions are intentionally designed to be difficult to distinguish (e.g., functions that differ only in edge cases or produce similar average rewards), along with analysis of the algorithm's behavior in these ambiguous scenarios.

## Limitations
- The ACE algorithm's performance in environments with multiple competing reward functions beyond the two-hypothesis case remains untested
- The paper doesn't address scenarios where spurious rewards and true rewards have overlapping patterns that make them difficult to distinguish
- Active exploration in test environments may become computationally prohibitive as environment complexity increases

## Confidence

**High Confidence** (Level 1): The 12.57 percentage point improvement over the standard agent and 16.20 percentage point improvement over the baseline agent are directly measured and reproducible.

**Medium Confidence** (Level 2): The claim that ACE can extrapolate concepts to understand environmental structure is supported by behavioral observations but lacks quantitative analysis of the agent's internal reasoning processes.

**Low Confidence** (Level 3): The paper doesn't address how ACE performs when the spurious reward R' and true reward R have more complex or overlapping patterns.

## Next Checks

1. Test ACE on CoinRun variants where both coin position and obstacle configuration vary randomly, to assess robustness to multiple distribution shifts simultaneously.

2. Implement ablation studies removing the exploration component to quantify how much of ACE's performance gain comes from active data collection versus the concept extrapolation algorithm itself.

3. Scale the environment complexity by increasing platform heights, adding more obstacles, or introducing multiple potential goals to test whether ACE maintains its advantage over standard training approaches.