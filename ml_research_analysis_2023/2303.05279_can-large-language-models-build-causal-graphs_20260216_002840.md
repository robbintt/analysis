---
ver: rpa2
title: Can large language models build causal graphs?
arxiv_id: '2303.05279'
source_url: https://arxiv.org/abs/2303.05279
tags:
- causal
- medical
- variables
- dags
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential of large language models
  (LLMs) to assist in building causal graphs in the medical domain. The authors used
  GPT-3 to score edges in directed acyclic graphs (DAGs) representing known exposure-outcome
  relationships in medicine.
---

# Can large language models build causal graphs?

## Quick Facts
- arXiv ID: 2303.05279
- Source URL: https://arxiv.org/abs/2303.05279
- Reference count: 4
- Primary result: GPT-3 can score causal edges in medical DAGs above random guessing, but performance varies significantly with prompt choice and linking verb.

## Executive Summary
This paper investigates whether large language models (LLMs) like GPT-3 can assist in building causal graphs for medical applications. The authors tested GPT-3's ability to score edges in directed acyclic graphs (DAGs) representing known exposure-outcome relationships in medicine. They evaluated different prompting strategies and linking verbs to determine which yielded the most accurate predictions. Results showed that while GPT-3's accuracy varied depending on the prompt and linking verb used, it consistently performed above random guessing for all DAGs in at least one setting. The study demonstrates the potential of LLMs to complement expert knowledge in DAG construction, while also revealing significant limitations due to the sensitivity of LLMs to language variations.

## Method Summary
The study used four DAGs representing well-known exposure-outcome relationships in medicine (Alcohol, Cancer, Diabetes, Obesity). For each DAG, the authors looped through every ordered variable pair and asked GPT-3 to score two statements per pair: one implying the presence of a directed edge from variable 1 to variable 2, and one implying its absence. They tested various prompts (including citations to medical authorities) and different linking verbs between variables. Accuracy was measured as the proportion of correct predictions in scoring the presence or absence of edges between variables in the DAGs.

## Key Results
- GPT-3 achieved above-random guessing accuracy for all DAGs in at least one prompting setting
- No single prompting strategy or linking verb consistently outperformed others across all DAGs
- Using more specific medical terminology sometimes decreased accuracy compared to simpler terms
- Performance varied significantly based on the choice of linking verbs between variables

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs encode enough domain knowledge to score causal edges when prompted correctly.
- Mechanism: GPT-3 processes natural language statements about variable relationships and assigns higher scores to those consistent with its training corpus, enabling binary classification of edge presence.
- Core assumption: Training corpus contains sufficient causal language describing the specific medical relationships being tested.
- Evidence anchors:
  - [abstract] "By encoding common and medical knowledge, large language models (LLMs) represent an opportunity to ease this process by automatically scoring edges (i.e., connections between two variables) in potential graphs."
  - [section] "Large language models capture non-trivial relationships and knowledge about the datasets they have been trained upon."
  - [corpus] Weak evidence - corpus shows related work on LLMs and causality but no direct validation of medical knowledge encoding.
- Break condition: Training data lacks sufficient coverage of specific causal relationships, or medical terminology is too domain-specific for general LLMs.

### Mechanism 2
- Claim: Prompt engineering significantly affects GPT-3's accuracy in scoring causal edges.
- Mechanism: Different prompts (e.g., citing medical authorities) activate different knowledge representations in GPT-3, leading to variation in scoring accuracy.
- Core assumption: GPT-3's knowledge is sensitive to prompt context and framing.
- Evidence anchors:
  - [abstract] "LLMs however have been shown to be brittle to the choice of probing words, context, and prompts that the user employs."
  - [section] "Our results demonstrated that while no verb consistently improved classification accuracy, the choice of verb linking the two variables of interest influenced accuracy."
  - [corpus] Moderate evidence - corpus includes related work on prompt sensitivity in LLMs.
- Break condition: Prompts fail to activate relevant knowledge, or GPT-3's architecture prevents effective context switching.

### Mechanism 3
- Claim: Specificity of language impacts GPT-3's ability to correctly score causal edges.
- Mechanism: More detailed descriptions of variables provide clearer context for GPT-3, improving its ability to match patterns in its training data.
- Core assumption: GPT-3's training data contains detailed causal descriptions that match specific variable descriptions.
- Evidence anchors:
  - [section] "Unsurprisingly, rephrasing the 'alcohol' variable to 'excessive alcohol consumption' increased the accuracy of GPT-3 on the Alcohol DAG."
  - [section] "Overall, in this analysis, more specific statements did not increase the accuracy and often resulted in worse accuracy for different linking verbs."
  - [corpus] Weak evidence - corpus shows related work on language specificity but no direct validation.
- Break condition: Training data lacks detailed causal descriptions matching the specific variable descriptions used.

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs)
  - Why needed here: DAGs are the target structure being evaluated - understanding their properties is essential for interpreting results.
  - Quick check question: What is the defining property of a DAG that prevents feedback loops?

- Concept: Causal inference principles
  - Why needed here: The paper evaluates whether LLMs can identify causal relationships, requiring understanding of what constitutes valid causal inference.
  - Quick check question: Why can't observational data alone answer causal questions without external knowledge?

- Concept: Prompt engineering techniques
  - Why needed here: The study tests different prompts to improve LLM performance, requiring understanding of how prompts affect model behavior.
  - Quick check question: How might citing different authorities (e.g., "medical doctors" vs "Big Pharma") affect an LLM's response?

## Architecture Onboarding

- Component map: GPT-3 (language model) -> medical DAGs (ground truth) -> prompting system (generates input statements) -> scoring mechanism (compares GPT-3 outputs to ground truth)
- Critical path: For each variable pair in DAG → generate two statements (edge present/absent) → prompt GPT-3 → compare scores → determine accuracy
- Design tradeoffs: Using general LLMs vs. domain-specific models (BioBert) - general models have broader knowledge but may lack medical specificity
- Failure signatures: Inconsistent accuracy across different prompts, poor performance on complex DAGs, sensitivity to minor wording changes
- First 3 experiments:
  1. Test GPT-3 on simple DAG with baseline prompt to establish baseline accuracy
  2. Test effect of different linking verbs on same DAG to measure sensitivity to language
  3. Test effect of medical authority prompts on complex DAG to measure impact of credibility framing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can context-specific LLMs like BioBert outperform GPT-3 in building medical causal graphs?
- Basis in paper: [explicit] The paper mentions that context-specific LLMs like BioBert have shown promise in medical NLP tasks and suggests future work should explore medical language-specific models.
- Why unresolved: The study only tested GPT-3 and did not compare its performance against domain-specific medical LLMs.
- What evidence would resolve it: Direct comparison of GPT-3 versus medical-specific LLMs (e.g., BioBert, PubMedGPT) on the same causal graph tasks with identical prompts and evaluation metrics.

### Open Question 2
- Question: How sensitive are LLMs to changes in medical terminology and clinical language?
- Basis in paper: [explicit] The study found that using more specific medical terms (like "excessive alcohol consumption" or "excessive fat accumulation") sometimes decreased accuracy compared to simpler terms, suggesting sensitivity to language variations.
- Why unresolved: The experiments only tested a limited set of terminology variations, and the inconsistent results suggest a need for more systematic exploration of how different medical language styles affect LLM performance.
- What evidence would resolve it: Systematic testing of LLM performance across a broad range of medical terminology complexity, clinical jargon, and language styles on standardized causal graph tasks.

### Open Question 3
- Question: Can LLMs reliably identify and respect acyclicity constraints in causal graph construction?
- Basis in paper: [explicit] The authors note that future work should focus on controlling for acyclicity, as their preliminary evaluations only examined the presence/absence of arrows and their direction.
- Why unresolved: The current study did not test whether LLMs could detect or prevent cycles in graphs, which is a fundamental requirement for valid causal DAGs.
- What evidence would resolve it: Testing whether LLMs can accurately identify and prevent cycles when constructing or evaluating causal graphs, possibly through specialized prompts or constraints that explicitly require acyclic structures.

### Open Question 4
- Question: How does the recency of training data affect LLM performance on emerging medical knowledge and novel diseases?
- Basis in paper: [explicit] The authors note that LLM updates lag behind new medical literature, potentially limiting usefulness for novel diseases, and that GPT-3 was trained on general internet text rather than current medical literature.
- Why unresolved: The study used well-established medical relationships where training data would likely be sufficient, but didn't test LLMs on newer or less-documented causal relationships.
- What evidence would resolve it: Comparative testing of LLMs with different training data recency on causal relationships from both established and emerging medical conditions, measuring performance differences.

## Limitations
- GPT-3's accuracy varies significantly with prompt choice and linking verb, with no consistent optimal strategy identified
- Study tested only four DAGs from the medical domain, limiting generalizability to other domains or more complex causal structures
- Binary classification task (edge present vs absent) oversimplifies the actual DAG construction problem, which involves determining edge directions and handling cycles

## Confidence
- High confidence: LLMs encode sufficient domain knowledge to score some causal edges when prompted appropriately
- Medium confidence: Prompt engineering significantly affects accuracy, given observed variation but lack of clear patterns
- Low confidence: Claims about specificity of language improving performance, as results showed more specific statements often decreased accuracy

## Next Checks
1. Cross-domain validation: Test the same prompting strategies on DAGs from non-medical domains (e.g., social science or economics) to determine whether observed sensitivity to language is domain-specific or a general LLM characteristic.

2. Model comparison study: Compare GPT-3 performance against domain-specific language models (BioBert, ClinicalBERT) and smaller causal inference models on identical DAG construction tasks to quantify the trade-off between general knowledge breadth and domain specificity.

3. Prompt sensitivity analysis: Systematically vary prompt components (authority framing, verb choice, specificity level) across a larger set of variable pairs to identify whether performance patterns emerge with sufficient sample size, or if results remain fundamentally unpredictable.