---
ver: rpa2
title: Fooling Contrastive Language-Image Pre-trained Models with CLIPMasterPrints
arxiv_id: '2307.03798'
source_url: https://arxiv.org/abs/2307.03798
tags:
- fooling
- image
- images
- clip
- master
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors show that Contrastive Language-Image Pre-training (CLIP)
  models can be fooled by "fooling master images" that appear random or unrelated
  to humans but maximize the model's confidence for many diverse text prompts. These
  images are generated by optimizing latent vectors in a generative model's latent
  space using stochastic gradient descent or an evolution strategy.
---

# Fooling Contrastive Language-Image Pre-trained Models with CLIPMasterPrints

## Quick Facts
- arXiv ID: 2307.03798
- Source URL: https://arxiv.org/abs/2307.03798
- Reference count: 36
- Primary result: CLIP models can be fooled by "fooling master images" that appear random but maximize model confidence for diverse text prompts

## Executive Summary
This paper demonstrates a novel attack on Contrastive Language-Image Pre-training (CLIP) models by generating "fooling master images" that achieve high confidence scores for diverse text prompts while being unrecognizable to humans. The attack exploits a fundamental modality gap in CLIP's embedding space, using evolutionary optimization to search off-manifold in the latent space of a generative model. The resulting images appear random or unrelated to humans but can fool CLIP models into assigning high confidence to targeted text prompts, even generalizing to semantically related prompts. The authors propose mitigation strategies including retraining to map off-manifold images to a special token or reducing the modality gap through embedding centroid shifting, with the latter being more effective.

## Method Summary
The authors generate fooling master images by optimizing latent vectors in a generative model's latent space (specifically Stable Diffusion's autoencoder) using stochastic gradient descent or an evolution strategy (CMA-ES). The optimization maximizes CLIP's cosine similarity between the generated image and multiple targeted text prompts simultaneously. The attack searches off-manifold in the latent space to find images whose embeddings achieve higher cosine similarity scores than any on-manifold image. For mitigation, the authors explore two approaches: retraining the model to map off-manifold images to a special token, and reducing the modality gap by shifting the centroids of image and text embeddings closer together in the latent space.

## Key Results
- CLIP models can be fooled by images that appear random or unrelated to humans but achieve high confidence scores for targeted text prompts
- The attack generalizes to semantically related prompts beyond the explicitly targeted ones
- Mitigation through embedding centroid shifting is more effective than mapping off-manifold images to a special token
- The vulnerability is closely related to the modality gap in CLIP's embedding space between image and text representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP models are vulnerable to fooling master images because of a modality gap between image and text embeddings
- Mechanism: The modality gap means that image and text embeddings in CLIP's latent space can only align to a certain degree. This creates a ceiling for how well matching text-image pairs can align (s(x,c) â‰ˆ 0.3). By searching off-manifold in the latent space, the attack finds images whose embeddings can achieve higher cosine similarity scores than any on-manifold image
- Core assumption: The CLIP model cannot achieve perfect alignment between image and text embeddings even for matching pairs
- Evidence anchors:
  - [abstract] "vulnerability to fooling master examples is closely related to a modality gap in contrastive pre-trained multi-modal networks"
  - [section] "the fact that image and text embeddings can only be aligned to a certain degree in CLIP latent space plays a central role with respect to a model's vulnerability"
  - [corpus] Weak evidence - corpus focuses on CLIP applications in medical imaging but doesn't directly address modality gap vulnerabilities
- Break condition: If the modality gap is eliminated through architectural changes or training procedures that allow perfect alignment between image and text embeddings

### Mechanism 2
- Claim: The attack generalizes to semantically related prompts beyond the explicitly targeted ones
- Mechanism: Since class labels in ImageNet are derived from WordNet hierarchies where many labels share superclasses, the latent space encoding of semantically related prompts is close to each other. A fooling image that aligns well with targeted prompts will also align well with related prompts due to this proximity
- Core assumption: Related prompts have embeddings that are spatially close in CLIP's latent space
- Evidence anchors:
  - [section] "we find that our mined fooling master images seem to generalize not only to the prompts they target, but also to semantically related prompts"
  - [section] "a possible explanation is that the classes of the ImageNet dataset have been derived as a subset from tree-like structures in WordNet"
  - [corpus] No direct evidence in corpus - it focuses on CLIP applications but not generalization properties
- Break condition: If prompt embeddings are distributed uniformly in latent space without semantic clustering, or if the model uses non-semantic prompt encodings

### Mechanism 3
- Claim: Information in fooling master images is distributed throughout the entire image for all targeted prompts
- Mechanism: The optimization process searches for latent vectors with maximum alignment to all targeted captions simultaneously. Since the model needs to achieve high scores for all prompts, the relevant information cannot be localized but must be spread across the image. This makes the images vulnerable to occlusion
- Core assumption: The optimization process requires simultaneous alignment to all targeted prompts, forcing information distribution across the entire image
- Evidence anchors:
  - [section] "we conclude that information from fooling examples resulting in high CLIP confidence scores is spread throughout the image for all targeted prompts"
  - [section] "the relevant information for all optimized prompts is spread throughout the image"
  - [corpus] No evidence in corpus - it focuses on CLIP applications rather than attack analysis
- Break condition: If the optimization could find solutions where information is localized for each prompt, or if the model's scoring mechanism doesn't require global image features

## Foundational Learning

- Concept: Contrastive learning and CLIP architecture
  - Why needed here: Understanding how CLIP learns joint representations and the nature of its embedding spaces is crucial for grasping why the attack works
  - Quick check question: How does CLIP create image and text embeddings, and what does it mean for them to be "contrastive"?

- Concept: Latent variable evolution and CMA-ES optimization
  - Why needed here: The attack uses evolutionary strategies to search the latent space of a generative model, so understanding this optimization approach is key
  - Quick check question: How does CMA-ES differ from gradient-based optimization, and why might it be more effective for this attack?

- Concept: Modality gap and embedding alignment
  - Why needed here: The core vulnerability exploited by the attack stems from limitations in how well image and text embeddings can align in CLIP's space
  - Quick check question: What factors contribute to the modality gap in multi-modal models like CLIP?

## Architecture Onboarding

- Component map:
  CLIP model (ViT-L/14 or similar) -> Stable Diffusion autoencoder -> CMA-ES optimizer -> Loss function (minimum cosine similarity)

- Critical path:
  1. Initialize CMA-ES with random latent vector
  2. Generate candidate latent vectors through mutation
  3. Decode candidates to images using autoencoder
  4. Compute CLIP cosine similarities for all targeted prompts
  5. Return minimum similarity as loss
  6. Update CMA-ES statistics and repeat

- Design tradeoffs:
  - Evolutionary vs. gradient-based optimization: Evolution is more robust to non-differentiable loss but slower; gradient-based is faster but requires differentiable loss
  - Image resolution: Higher resolution gives better quality but requires more computational resources
  - Number of targeted prompts: More prompts increase attack power but make optimization harder

- Failure signatures:
  - Images don't achieve high cosine similarity scores for targeted prompts
  - Images appear recognizable rather than random/unrelated
  - Optimization converges too slowly or gets stuck in local minima
  - Attack doesn't generalize to semantically related prompts

- First 3 experiments:
  1. Reproduce the basic attack on CLIP with 5 famous artwork titles to verify the core vulnerability
  2. Test the generalization effect by measuring cosine similarity on 10 non-targeted but semantically related prompts
  3. Implement and test the embedding shift mitigation by moving image and text centroids closer together and verifying reduced attack effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are other contrastively pre-trained multimodal models besides CLIP also vulnerable to similar fooling master image attacks?
- Basis in paper: [explicit] The authors conclude that "it seems worthwhile to investigate whether other contrastively pre-trained multimodal beyond CLIP are vulnerable to CLIPMasterPrints as well" and argue that devising effective counter-strategies against the introduced model vulnerability is necessary to ensure this powerful class of models can be applied in a safe and robust way.
- Why unresolved: The study focused solely on CLIP models. While the authors speculate that the modality gap issue may apply to other contrastively pre-trained multimodal models, empirical testing across different model architectures has not been conducted.
- What evidence would resolve it: Testing various contrastively pre-trained multimodal models (e.g., ALIGN, FILIP) with the same or adapted fooling master image generation techniques would provide empirical evidence of their vulnerability or robustness.

### Open Question 2
- Question: What are the most effective mitigation strategies against fooling master image attacks on CLIP and similar models?
- Basis in paper: [explicit] The authors attempted two mitigation strategies: refining the model to map off-manifold images to a special token, and bridging the modality gap by shifting centroids of image and text embeddings. They found that the latter was more effective, but still left the model vulnerable to newly mined fooling images.
- Why unresolved: The mitigation strategies tested showed partial success but did not completely eliminate the vulnerability. The authors call for further research on effective mitigation strategies, suggesting that more work is needed to fully address the issue.
- What evidence would resolve it: Developing and rigorously testing new mitigation techniques, potentially combining multiple approaches or exploring entirely different methods (e.g., adversarial training with fooling master images), would provide evidence of their effectiveness in preventing such attacks.

### Open Question 3
- Question: How does the semantic generalization of fooling master images to related prompts affect the overall security of CLIP models in real-world applications?
- Basis in paper: [explicit] The authors observed that fooling master images trained on a small number of prompts seem to generalize to a larger number of semantically related prompts, potentially increasing their impact. They note that this generalization effect could be due to semantically related prompts being close to each other in CLIP latent space.
- Why unresolved: While the generalization effect is observed, its extent and implications for real-world security are not fully explored. The paper does not quantify how many additional prompts or classes could be affected by this generalization, nor does it assess the practical impact on specific application scenarios.
- What evidence would resolve it: Conducting extensive tests to map the semantic relationships between prompts and their susceptibility to fooling master images, combined with case studies of potential real-world attack scenarios, would provide a clearer picture of the security implications and help in developing more targeted defenses.

## Limitations

- The attack requires access to CLIP's embedding space and cannot be performed in a black-box setting
- The evolutionary optimization approach is computationally intensive and may not scale efficiently to very large numbers of targeted prompts
- The effectiveness of mitigation strategies is demonstrated on specific CLIP models but their generalizability to other contrastive language-image pre-trained models remains uncertain

## Confidence

**High Confidence**: The demonstration that CLIP models can be fooled by off-manifold images achieving high cosine similarity scores for targeted text prompts. This is directly observable through the cosine similarity metrics and reproducible with the provided methodology.

**Medium Confidence**: The claim that the vulnerability is fundamentally tied to the modality gap in CLIP's embedding space. While the paper provides strong evidence linking the two, the causal relationship between the modality gap and attack success could benefit from additional ablation studies.

**Medium Confidence**: The effectiveness of mitigation strategies, particularly the embedding centroid shifting approach. The paper shows quantitative improvements, but the long-term robustness of these mitigations against adaptive attacks is not fully explored.

## Next Checks

1. **Cross-Model Validation**: Test the attack against multiple CLIP variants (including more recent versions) and other contrastive language-image pre-trained models to assess the generality of the vulnerability across different architectures and training approaches.

2. **Adaptive Attack Testing**: After applying mitigation strategies, attempt to develop adaptive attacks that specifically target the modified embedding spaces to evaluate the long-term robustness of the proposed defenses.

3. **Latent Space Analysis**: Conduct detailed analysis of the distribution of targeted prompt embeddings in CLIP's latent space before and after mitigation to better understand how the modality gap manifests and whether it can be completely eliminated.