---
ver: rpa2
title: 'Enhancing Predictive Capabilities in Data-Driven Dynamical Modeling with Automatic
  Differentiation: Koopman and Neural ODE Approaches'
arxiv_id: '2310.06790'
source_url: https://arxiv.org/abs/2310.06790
tags:
- koopman
- operator
- state
- space
- observables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a modification of extended dynamic mode decomposition
  with dictionary learning (EDMD-DL) that simultaneously determines both the dictionary
  of observables and the Koopman operator approximation using automatic differentiation
  through the pseudoinverse. The authors address the limitation of the original EDMD-DL
  approach, which uses an iterative optimization procedure that can lead to suboptimal
  solutions.
---

# Enhancing Predictive Capabilities in Data-Driven Dynamical Modeling with Automatic Differentiation: Koopman and Neural ODE Approaches

## Quick Facts
- arXiv ID: 2310.06790
- Source URL: https://arxiv.org/abs/2310.06790
- Reference count: 0
- Key outcome: KDLAoo significantly outperforms original EDMD-DL by using automatic differentiation through pseudoinverse to directly optimize neural network weights for Koopman operator approximation.

## Executive Summary
This paper presents a novel modification to extended dynamic mode decomposition with dictionary learning (EDMD-DL) that simultaneously determines both the dictionary of observables and the Koopman operator approximation using automatic differentiation through the pseudoinverse. The authors address the limitation of the original EDMD-DL approach, which uses an iterative optimization procedure that can lead to suboptimal solutions. By leveraging automatic differentiation, their method directly optimizes the neural network weights to minimize the loss function without the need for alternating optimization steps. The proposed approach is evaluated on a range of dynamical systems, including low-dimensional systems with steady, oscillatory, and chaotic attractors, as well as partial differential equations exhibiting complex behaviors. The results demonstrate that the new method significantly outperforms the original EDMD-DL approach.

## Method Summary
The method modifies EDMD-DL by replacing the alternating optimization scheme with automatic differentiation through the Moore-Penrose pseudoinverse. A neural network maps states to observables (dictionary), and the Koopman operator is computed via pseudoinverse of snapshot matrices. Gradients are computed through this pseudoinverse operation to directly update the network weights, eliminating the need for iterative updates of dictionary and operator. The framework supports both pure Koopman approaches (evolution entirely in observable space) and state-observable alternation approaches.

## Key Results
- KDLAoo achieves significantly lower prediction error than original KDLoo on Duffing oscillator, Rossler attractor, and cylinder flow model
- NODE and KDLso achieve comparable performance for short-term tracking of chaotic systems
- For viscous Burgers equation, KDLAoo captures eigenvalues and power spectrum more accurately than polynomial EDMD
- Pure Koopman approach (KDLAoo) outperforms state-observable alternation approach (KDLoo) for most systems tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed method improves over EDMD-DL by avoiding the iterative alternating optimization, instead using automatic differentiation through the Moore-Penrose pseudoinverse to directly optimize the neural network weights.
- Mechanism: In the original EDMD-DL, the dictionary and Koopman operator are updated alternately, leading to a two-step optimization that can cause convergence to suboptimal solutions. The new method combines these steps by computing gradients through the pseudoinverse, so the neural network weights are updated directly with respect to the full objective.
- Core assumption: The pseudoinverse computation can be differentiated through using automatic differentiation without introducing numerical instability or prohibitive computational cost.
- Evidence anchors:
  - [abstract] "This innovation leverages automatic differentiation to facilitate gradient descent computations through the pseudoinverse."
  - [section II B 1] "Instead, we optimize the NN parameters by using automatic differentiation to compute the gradient through the pseudoinverse directly."

### Mechanism 2
- Claim: The performance gain from KDLAoo over KDLoo is because the former integrates the evolution entirely within the observable space using the Koopman operator, whereas the latter alternates between observable and state spaces, losing linearity.
- Mechanism: KDLAoo keeps the linear Koopman operator intact during time evolution, so the predicted trajectory remains within the linear subspace defined by the observables. KDLoo introduces nonlinear mappings back and forth between spaces at every step, which breaks the linear structure of the Koopman framework and can accumulate errors.
- Core assumption: The linear Koopman operator approximation is sufficiently accurate in the observable space to capture the system's dynamics over the time horizon of interest.
- Evidence anchors:
  - [abstract] "We assess a 'pure' Koopman approach, which involves the direct time-integration of a linear, high-dimensional system governing the dynamics within the space of observables."
  - [section III A] "This highlights that predictions are enhanced when m is smaller," and "KDLso and NODE lead to small errors."

### Mechanism 3
- Claim: State-space methods (NODE) outperform pure Koopman approaches for short-term tracking but are comparable when the Koopman approach alternates between spaces.
- Mechanism: NODE directly learns the nonlinear state evolution without the need to map to a high-dimensional observable space, so it can capture complex short-term dynamics more flexibly. When Koopman alternates spaces, the nonlinear mapping can partially correct for errors, bringing its performance closer to NODE.
- Core assumption: The training data is sufficient and noise-free so NODE can learn the nonlinear mapping accurately.
- Evidence anchors:
  - [abstract] "the state space approach offers superior performance compared to the 'pure' Koopman approach where the entire time evolution occurs in the space of observables."
  - [section III A] "For KDLso and NODE, after t > 4, there is a rise in the error until t = 7, after which KDLAoo dips below KDLoo, suggesting that our approach lands closer to the fixed points."

## Foundational Learning

- Concept: Moore-Penrose pseudoinverse and its role in least-squares fitting.
  - Why needed here: The Koopman operator is approximated by solving a least-squares problem, whose closed-form solution uses the pseudoinverse of the snapshot matrix.
  - Quick check question: What property does the pseudoinverse provide that a simple inverse does not in the context of non-square matrices?

- Concept: Koopman operator theory and the function space approach.
  - Why needed here: The paper's entire framework is built on lifting state-space dynamics into a space of observables where the evolution is linear.
  - Quick check question: Why is the Koopman operator infinite-dimensional in general, and how does finite approximation affect predictions?

- Concept: Automatic differentiation through linear algebra operations.
  - Why needed here: To enable end-to-end training of the dictionary network with gradients flowing through the pseudoinverse computation.
  - Quick check question: Which PyTorch/Autograd functions can be used to differentiate through matrix pseudo-inverses?

## Architecture Onboarding

- Component map:
  Neural network (NN) mapping states to observables -> Pseudoinverse computation of snapshot matrices -> Automatic differentiation engine -> Time integration module -> Evaluation and prediction pipeline

- Critical path:
  1. Feed batch of state snapshots into NN to produce observables.
  2. Assemble snapshot matrices ψ(t) and ψ(t+δt).
  3. Compute Koopman operator K via pseudoinverse.
  4. Backpropagate loss through K to NN weights using automatic differentiation.
  5. Update NN weights via optimizer.
  6. Iterate until convergence.

- Design tradeoffs:
  - Using automatic differentiation through pseudoinverse trades off numerical stability for end-to-end optimization capability.
  - Choosing the number of observables vs. snapshot size: too many observables can cause ψψ^T to be ill-conditioned; too few may underfit.
  - Batch size vs. training dynamics: full batch ensures K is stable but removes stochasticity; small batch introduces noise.

- Failure signatures:
  - KDLAoo predictions collapse to trivial dynamics (e.g., all trajectories converge to fixed points) -> pseudoinverse poorly conditioned or NN collapsed.
  - Training loss plateaus early -> learning rate too high or insufficient expressivity in NN architecture.
  - NODE outperforms KDLAoo on chaotic systems -> observable space not rich enough to capture continuous spectrum.

- First 3 experiments:
  1. Run KDLAoo on the Duffing oscillator and compare loss curves with KDLoo to confirm training efficiency gain.
  2. Test KDLAoo with varying numbers of observables to identify the point where performance saturates or degrades.
  3. Apply KDLAoo to the Stuart-Landau equation and verify the crossover point at R = 1/√2 is captured, unlike polynomial EDMD.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the KDLAoo approach scale with increasing dimensionality of the state space and observables?
- Basis in paper: [explicit] The authors mention that the framework presented in the paper offers new avenues and challenges for data-driven dynamical systems, including the natural extension to further enhance its accuracy and physical relevance by incorporating physical laws into the methodology.
- Why unresolved: The paper focuses on systems of increasing complexity but does not provide a comprehensive analysis of how the performance scales with dimensionality.
- What evidence would resolve it: Systematic studies varying the state space and observable dimensions, along with corresponding performance metrics.

### Open Question 2
- Question: Can the KDLAoo approach be effectively applied to high-dimensional chaotic systems, such as the Kuramoto-Sivashinsky equation with a large number of modes?
- Basis in paper: [explicit] The authors state that the primary goal of their future studies is to develop a Koopman-based approach for accurately predicting high-dimensional chaotic systems, such as the Kuramoto-Sivashinsky equation with a large number of modes.
- Why unresolved: The paper demonstrates the approach on systems of increasing complexity but does not specifically address high-dimensional chaotic systems with a large number of modes.
- What evidence would resolve it: Successful application and performance evaluation of the KDLAoo approach on high-dimensional chaotic systems with a large number of modes.

### Open Question 3
- Question: How does the incorporation of physical laws, such as symmetries or conservation properties, impact the predictive power and accuracy of the KDLAoo approach?
- Basis in paper: [explicit] The authors mention that the natural extension of their framework is to further enhance its accuracy and physical relevance by incorporating physical laws into their methodology, such as symmetries or conservation of properties (energy).
- Why unresolved: The paper focuses on the development and evaluation of the KDLAoo approach without explicitly incorporating physical laws.
- What evidence would resolve it: Comparative studies evaluating the performance of the KDLAoo approach with and without the incorporation of physical laws, along with corresponding performance metrics.

## Limitations
- Performance claims are based on comparisons with only one baseline (EDMD-DL), lacking comparison with other state-of-the-art data-driven modeling approaches
- The method's generalization to highly nonlinear, high-dimensional, or noisy real-world systems remains unproven
- Reliance on automatic differentiation through pseudoinverse may introduce numerical instability for ill-conditioned snapshot matrices

## Confidence
**High Confidence:** The core mechanism of using automatic differentiation through the pseudoinverse to enable end-to-end training of the dictionary network is well-supported by the mathematical formulation and implementation details provided.

**Medium Confidence:** The claim that the pure Koopman approach (KDLAoo) outperforms the alternating approach (KDLoo) is supported by the results but may be problem-dependent.

**Low Confidence:** The assertion that NODE and KDLso achieve comparable performance across all tested systems is based on limited examples.

## Next Checks
1. Evaluate KDLAoo on noisy versions of the test systems and with reduced training data to assess its generalization capabilities compared to NODE and other baselines.

2. Benchmark KDLAoo against SINDy, LSTM-based forecasting, and physics-informed neural networks on systems where these methods are known to perform well, to establish its relative strengths and weaknesses.

3. Systematically vary the number of observables, neural network architecture, and learning rate schedule to identify the optimal configuration for each system type and to understand the method's sensitivity to hyperparameters.