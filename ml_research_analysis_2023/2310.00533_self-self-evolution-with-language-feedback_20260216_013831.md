---
ver: rpa2
title: 'SELF: Self-Evolution with Language Feedback'
arxiv_id: '2310.00533'
source_url: https://arxiv.org/abs/2310.00533
tags:
- self
- training
- data
- self-refinement
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SELF is a self-evolution framework for LLMs that enables autonomous
  improvement through self-feedback and self-refinement. The framework consists of
  two stages: meta-skill learning, where the model learns to provide feedback and
  refine responses, and self-evolution, where the model iteratively generates and
  refines responses on unlabeled data to improve itself.'
---

# SELF: Self-Evolution with Language Feedback

## Quick Facts
- arXiv ID: 2310.00533
- Source URL: https://arxiv.org/abs/2310.00533
- Authors: 
- Reference count: 18
- Primary result: SELF framework achieves up to 5.15% accuracy improvement on GSM8K and 4.5% on SVAMP through self-evolution

## Executive Summary
SELF is a self-evolution framework that enables large language models to autonomously improve through iterative self-feedback and self-refinement. The framework operates in two stages: first, meta-skill learning where the model learns to generate feedback and refine responses from curated training data; second, self-evolution where the model iteratively generates, refines, and trains on its own outputs. Experiments demonstrate that SELF progressively enhances model capabilities, with smaller models acquiring advanced self-refinement abilities and achieving significant performance gains on math benchmarks.

## Method Summary
The SELF framework consists of meta-skill learning followed by iterative self-evolution. In meta-skill learning, an initial LLM is fine-tuned on a corpus containing question-answer pairs with feedback-driven refinements to learn self-feedback and self-refinement patterns. During self-evolution, the model generates responses to unlabeled prompts, applies self-feedback and self-refinement, filters low-quality data, and fine-tunes on the remaining high-quality synthetic data. This process iterates multiple times, with the model progressively improving its capabilities. At inference, the model can apply online self-refinement to further enhance response quality.

## Key Results
- 5.15% accuracy improvement on GSM8K math benchmark
- 4.5% accuracy improvement on SVAMP math benchmark  
- Demonstrates progressive capability enhancement through iterative self-evolution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Meta-skill learning provides a foundation for self-evolution by teaching the model to generate feedback and refine responses.
- Mechanism: The model is fine-tuned on a dataset where strong LLMs (or human labelers) provide feedback and refined answers for unlabeled prompts. This teaches the model the patterns of effective self-feedback and self-refinement.
- Core assumption: A model can learn to provide useful feedback and refine its own responses by observing examples of this behavior from stronger models.
- Evidence anchors:
  - [abstract] "Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement."
  - [section] "The acquisition of these meta-skills is realized through a fine-tuning process. The LLMs undergo fine-tuning on a specially curated Meta-Skill Training Corpus."
  - [corpus] Weak - the paper doesn't provide quantitative evidence of the meta-skill training corpus effectiveness before self-evolution.
- Break condition: If the meta-skill training data is of low quality or the model cannot generalize the feedback/refinement patterns to its own outputs.

### Mechanism 2
- Claim: Iterative self-evolution progressively improves the model by generating, refining, and training on its own outputs.
- Mechanism: In each iteration, the model generates responses to unlabeled prompts, applies self-feedback and self-refinement, filters low-quality data, and fine-tunes on the remaining high-quality synthetic data.
- Core assumption: The model's ability to generate and refine outputs improves over iterations, creating a positive feedback loop of increasing data quality and model capability.
- Evidence anchors:
  - [abstract] "The model undergoes progressive improvement through this iterative self-evolution process."
  - [section] "Experimental results on representative benchmarks substantiate that SELF can progressively advance its inherent abilities with the requirement of human intervention."
  - [corpus] Weak - while the paper shows performance improvements over iterations, it doesn't provide direct evidence of improving data quality.
- Break condition: If the model's self-refinement capability plateaus or degrades, leading to no improvement or quality degradation in synthetic training data.

### Mechanism 3
- Claim: Online self-refinement during inference further improves response quality by applying learned meta-skills.
- Mechanism: At inference time, the model generates an initial response, then applies the self-feedback and self-refinement process to produce a higher-quality final response.
- Core assumption: The meta-skills learned during training can be directly applied to improve responses on unseen prompts during inference.
- Evidence anchors:
  - [abstract] "Additionally, the SELF framework enables the model to apply self-refinement during inference, which further improves response quality."
  - [section] "During inference, the acquired meta-skills facilitate LLMs in elevating response quality through response self-refinement."
  - [corpus] Weak - the paper shows improved performance with self-refinement, but doesn't isolate this effect from the self-evolution training.
- Break condition: If the model's self-refinement capability is not robust enough to handle diverse inference prompts, leading to inconsistent or degraded output quality.

## Foundational Learning

- Concept: Supervised fine-tuning on instruction-following datasets.
  - Why needed here: Provides the base capability for the model to understand and respond to instructions before meta-skill learning.
  - Quick check question: Can the base model follow simple instructions and generate coherent responses?

- Concept: Chain-of-thought reasoning.
  - Why needed here: Enables the model to break down complex problems and provide step-by-step solutions, which is crucial for math tasks.
  - Quick check question: Can the model solve simple math word problems with clear reasoning steps?

- Concept: Data filtering and quality assessment.
  - Why needed here: Ensures that only high-quality synthetic data is used for self-evolution training, preventing degradation from noisy examples.
  - Quick check question: Can the model accurately distinguish between correct and incorrect responses?

## Architecture Onboarding

- Component map: Meta-skill training corpus generation -> Meta-skill learning fine-tuning -> Self-evolution data generation and filtering -> Self-evolution fine-tuning -> Inference-time self-refinement
- Critical path: Meta-skill learning → Self-evolution iterations → Inference-time refinement
- Design tradeoffs:
  - Using stronger models for meta-skill training data vs. human labelers (quality vs. cost)
  - Filtering vs. using all generated data (quality vs. quantity)
  - Restart vs. continual training for self-evolution (forgetting vs. efficiency)
- Failure signatures:
  - No improvement over self-evolution iterations
  - Degradation in performance on held-out validation data
  - Inconsistent or low-quality self-refined responses during inference
- First 3 experiments:
  1. Run meta-skill learning and evaluate on a held-out set of prompts to ensure the model has acquired feedback/refinement capabilities.
  2. Run one iteration of self-evolution and compare performance to the meta-skill trained model to verify the self-evolution process is working.
  3. Implement inference-time self-refinement and test on a sample of prompts to ensure the learned meta-skills are being properly applied.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SELF framework compare to other methods of LLM self-improvement, such as online self-improvement or human preference alignment?
- Basis in paper: [explicit] The paper mentions online self-improvement and human preference alignment as related works and highlights key differences.
- Why unresolved: The paper doesn't provide a direct comparison of performance or efficiency between SELF and these other methods.
- What evidence would resolve it: Experiments comparing the performance and efficiency of SELF against online self-improvement and human preference alignment methods on the same benchmarks.

### Open Question 2
- Question: What is the impact of the number of self-evolution iterations on the performance of the model?
- Basis in paper: [inferred] The paper mentions that the self-evolution process is iterative, but doesn't provide a detailed analysis of how the number of iterations affects performance.
- Why unresolved: The paper doesn't provide a study on the effect of varying the number of self-evolution iterations.
- What evidence would resolve it: Experiments showing the performance of the model after different numbers of self-evolution iterations.

### Open Question 3
- Question: How does the quality of the initial LLM (Vicuna) affect the performance of the SELF framework?
- Basis in paper: [inferred] The paper uses Vicuna as the initial LLM, but doesn't explore how the quality of the initial model affects the final performance.
- Why unresolved: The paper doesn't provide experiments with different initial LLMs.
- What evidence would resolve it: Experiments comparing the performance of SELF when initialized with LLMs of varying quality or sizes.

### Open Question 4
- Question: How does the choice of evaluation metric affect the assessment of the model's performance?
- Basis in paper: [explicit] The paper mentions using accuracy as the evaluation metric for GSM8K and SVAMP, but doesn't discuss the choice of metric or explore other metrics.
- Why unresolved: The paper doesn't provide a discussion on the suitability of accuracy as the sole metric or explore other metrics like F1-score or AUC.
- What evidence would resolve it: Experiments using different evaluation metrics and a discussion on the strengths and weaknesses of each metric for the given tasks.

### Open Question 5
- Question: How does the SELF framework handle out-of-distribution prompts or prompts that are significantly different from the training data?
- Basis in paper: [inferred] The paper doesn't discuss the model's performance on out-of-distribution prompts or its ability to generalize to unseen domains.
- Why unresolved: The paper doesn't provide experiments or analysis on the model's robustness to out-of-distribution data.
- What evidence would resolve it: Experiments testing the model's performance on prompts from unseen domains or with different characteristics than the training data.

## Limitations

- Meta-skill training corpus quality is critical but not quantitatively characterized
- Filtering criteria for self-evolving data are abstract without specific evaluation prompts
- Evaluation focuses primarily on math benchmarks with limited general language task validation
- Computational overhead of multiple iterations and inference-time refinement is not discussed

## Confidence

**High confidence**: The core mechanism of meta-skill learning followed by iterative self-evolution is technically sound and well-described.

**Medium confidence**: Experimental results showing performance improvements are convincing but may not generalize beyond math tasks.

**Low confidence**: Claims about inference-time self-refinement effectiveness are weakly supported as the effect isn't isolated from self-evolution training.

## Next Checks

1. **Meta-skill corpus quality audit**: Manually sample and evaluate 100 examples from the Meta-Skill Learning Corpus to assess feedback comprehensiveness and refinement quality.

2. **Cross-domain generalization test**: Evaluate the self-evolved model on non-math benchmarks including commonsense reasoning (HellaSwag), reading comprehension (NaturalQuestions), and creative writing tasks.

3. **Ablation study on self-evolution iterations**: Compare models trained with 1, 3, and 5 self-evolution iterations on held-out validation data to determine if performance improvements plateau or degrade.