---
ver: rpa2
title: 'Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional,
  and Extensional Learning for Faithful Natural Language Generation'
arxiv_id: '2310.15355'
source_url: https://arxiv.org/abs/2310.15355
tags:
- world
- evidence
- output
- learning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that large language models (LLMs) hallucinate
  because their output is not constrained to be consistent with their evidence about
  the world. The authors propose a framework called Learn-Babble-Prune to build factual
  or faithful LLMs that constrain output to be synonymous with claims for which the
  model has evidence.
---

# Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation

## Quick Facts
- arXiv ID: 2310.15355
- Source URL: https://arxiv.org/abs/2310.15355
- Reference count: 15
- Primary result: Proposes a framework (Learn-Babble-Prune) to build factual/faithful LLMs that constrain output to be synonymous with claims for which the model has evidence

## Executive Summary
This paper identifies why large language models hallucinate: their outputs are not constrained to be consistent with their evidence about the world. The authors propose a framework called Learn-Babble-Prune to build factual or faithful LLMs that ensure output is synonymous with verified claims. The key insight is that LLMs need three types of learning - perceptual (learning about the world), extensional (mapping strings to states of the world), and intensional (learning which sentences have the same meaning) - to achieve evidential closure. The framework uses rejection sampling to cross-check generated output against an evidence set, rejecting any output that is not a paraphrase of a verified claim.

## Method Summary
The Learn-Babble-Prune framework consists of three phases: First, the model learns about the world (perceptual learning) and acquires an evidence set of verified claims. Second, it learns paraphrase mappings for all sentences in the evidence set (intensional learning). Third, when generating output, the model uses rejection sampling - it generates candidate sentences and only accepts those that are synonymous with claims in the evidence set. This ensures all output is consistent with the model's evidence, preventing hallucinations. The approach applies to both text-to-text and image-to-text applications.

## Key Results
- Shows that LLMs hallucinate because their outputs are not constrained to be synonymous with claims for which they have evidence
- Proposes Learn-Babble-Prune framework as a solution to achieve evidential closure
- Demonstrates that multimodal LLMs require three types of learning (perceptual, extensional, intensional) to be faithful
- Identifies synonymy-based rejection sampling as a method to ensure factual output

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs hallucinate because they lack evidential closure - their outputs are not constrained to be synonymous with claims for which they have evidence.
- Mechanism: The LLM generates text based on conditional probability distributions learned from training data, but these distributions do not incorporate semantic information about truth or falsity of sentences. Without explicit learning of states of the world, reference mappings, and synonymy relations, the model can generate strings that refer to false states of the world.
- Core assumption: Information about truth or falsity of sentences is not statistically identified in standard neural probabilistic language models and cannot be conditioned on during generation.
- Evidence anchors:
  - [abstract]: "We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence: a condition that we call evidential closure."
  - [section]: "The model solution is invariant to assignments of truth-values to states of the world. To put it another way, V0 is an unidentified nuisance parameter. This entails that: arg max x ˆf(x|C, P) = ˆy ̸=⇒ V0[R(ˆy)] = 1"
  - [corpus]: Found 25 related papers with average neighbor FMR=0.441, but none directly address the theoretical identification problem of truth-values in LLM training.
- Break condition: If the LLM incorporates explicit learning of states of the world, reference mappings, and synonymy relations, then the mechanism fails as the model would have evidential closure.

### Mechanism 2
- Claim: Evidential closure can be achieved by constraining LLM output to be synonymous with claims in a validated evidence set.
- Mechanism: The Learn-Babble-Prune framework ensures that generated output is cross-checked against an evidence set. If the output is not synonymous with a claim in the evidence set, it is rejected. This guarantees that all output is consistent with the model's evidence.
- Core assumption: Synonymy is truth-preserving - if we know a particular sentence is true, then any sentence synonymous with that sentence is also true.
- Evidence anchors:
  - [abstract]: "The output of the LLM is then cross-checked against its evidence and rejected if it is not a paraphrase of a verified claim."
  - [section]: "Proposition 3.5 (Closure under synonymy) . ∀ℓ, ℓ′ ∈ L : V0[R(ℓ)] = 1 and R(ℓ) = R(ℓ′) = ⇒ V0[R(ℓ′)] = 1"
  - [corpus]: None of the 25 related papers explicitly discuss synonymy-based rejection sampling for ensuring factual output.
- Break condition: If the evidence set contains false claims or if the synonymy detection is inaccurate, the mechanism fails as false output could be accepted.

### Mechanism 3
- Claim: Multimodal LLMs require perceptual, extensional, and intensional learning to achieve evidential closure.
- Mechanism: Perceptual learning acquires information about the world through sensors. Extensional learning maps perceptual information to strings. Intensional learning learns paraphrases of sentences. Together, these enable the model to generate output that is synonymous with its perceptual evidence.
- Core assumption: Learning about the world, learning which sentences refer to which states of the world, and learning which sentences have the same meaning together allow a speaker to attain fluency.
- Evidence anchors:
  - [abstract]: "A multimodal LLM must learn about the external world (perceptual learning); it must learn a mapping from strings to states of the world (extensional learning); and, to achieve fluency when generalizing beyond a body of evidence, it must learn mappings from strings to their synonyms (intensional learning)."
  - [section]: "We say that a faithful speaker must perform three tasks: they must learn about the world (perceptual learning ); they must learn which sentences map onto which states of the world (extensional learning); and they must learn which sentences have the same meaning ( intensional learning)."
  - [corpus]: None of the 25 related papers discuss the three types of learning required for multimodal evidential closure.
- Break condition: If any of the three learning components is missing or inaccurate, the mechanism fails as the model cannot ensure evidential closure.

## Foundational Learning

- Concept: Semantics and reference
  - Why needed here: Understanding the distinction between meaning/intension, reference/extension, and truth/facts is crucial for formalizing why LLMs hallucinate and how to build faithful/factual LLMs.
  - Quick check question: Can you explain the difference between a sentence's meaning and its reference?

- Concept: Causal inference and identifiability
  - Why needed here: The paper uses causal inference concepts to show that truth-values of states of the world are unidentified nuisance parameters in standard LLM training, leading to hallucinations.
  - Quick check question: Why does the invariance of the LLM solution to assignments of truth-values to states of the world lead to hallucinations?

- Concept: Symmetry groups and orbits
  - Why needed here: The paper uses symmetry group theory to formalize paraphrases and semantic orbits, which are key to the Learn-Babble-Prune framework.
  - Quick check question: How does the set of paraphrase maps form a symmetry group of the set of strings?

## Architecture Onboarding

- Component map: Evidence acquisition -> Paraphrase learner -> LLM generator -> Synonymy checker -> Rejection module
- Critical path: Evidence acquisition → Paraphrase learning → LLM generation → Synonymy checking → Output/rejection
- Design tradeoffs:
  - Evidence set size vs. computational cost: Larger evidence sets enable more diverse output but increase computation
  - Synonymy detection accuracy vs. false rejections: More accurate synonymy detection reduces false rejections but may be computationally expensive
  - Generation diversity vs. factual accuracy: Allowing more diverse generation may increase hallucinations
- Failure signatures:
  - High rejection rate: Evidence set may be too small or synonymy detection inaccurate
  - Factual errors in output: Evidence set may contain false claims or synonymy detection may be flawed
  - Lack of output diversity: Evidence set may be too small or synonymy detection too strict
- First 3 experiments:
  1. Train on a small evidence set (e.g. 100 verified claims) and measure rejection rate and output accuracy
  2. Vary synonymy detection method (e.g. paraphrase corpus vs. contrastive learning) and measure impact on rejection rate and output diversity
  3. Scale evidence set size (e.g. 100 → 1000 → 10000 claims) and measure impact on output diversity and computational cost

## Open Questions the Paper Calls Out
The paper explicitly identifies several open questions, including how to efficiently enforce evidential closure beyond rejection sampling (suggesting reinforcement learning approaches), how to collect and verify large-scale evidence bases especially in contested domains, and how to extend the framework to handle compositional semantics and sentences that refer to multiple states of the world.

## Limitations
- The theoretical proof of the identification problem relies on specific assumptions about training objective and data distribution that may not fully apply to real-world LLM training
- Synonymy detection methods have error rates that could compromise evidential closure, but the paper doesn't quantify this impact
- The rejection sampling approach becomes computationally prohibitive as evidence sets grow, but the paper doesn't provide concrete scalability bounds

## Confidence
- Evidential Closure Framework: High confidence - the theoretical foundation connecting lack of evidential closure to hallucination is well-established through causal inference arguments
- Learn-Babble-Prune Architecture: Medium confidence - the conceptual framework is sound, but implementation details critical for practical deployment are underspecified
- Three Types of Learning Requirement: Medium confidence - while the distinction between perceptual, extensional, and intensional learning is theoretically grounded, empirical validation of this tripartite decomposition is limited

## Next Checks
1. **Empirical Identification Analysis**: Run controlled experiments varying the proportion of true/false claims in training data to quantify how the LLM's output faithfulness degrades as the ratio of true to false claims in the evidence set changes. This would validate the theoretical identification problem.

2. **Synonymy Detection Stress Test**: Systematically evaluate paraphrase detection accuracy across different domains, sentence complexities, and semantic distances. Measure how synonymy detection error rates translate to factual hallucination rates in generated output.

3. **Scalability Benchmarking**: Implement Learn-Babble-Prune with evidence sets of varying sizes (100, 1000, 10000, 100000 claims) and measure: (a) computational cost per generated token, (b) rejection rate, and (c) output diversity metrics. Identify the practical limits of the approach.