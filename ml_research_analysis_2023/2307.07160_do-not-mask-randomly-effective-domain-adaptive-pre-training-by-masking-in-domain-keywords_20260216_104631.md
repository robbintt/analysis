---
ver: rpa2
title: 'Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain
  Keywords'
arxiv_id: '2307.07160'
source_url: https://arxiv.org/abs/2307.07160
tags:
- masking
- pre-training
- keywords
- in-domain
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of domain adaptation in pre-trained
  language models (PLMs) by proposing a novel task-agnostic in-domain pre-training
  method that selectively masks in-domain keywords. The method identifies domain-specific
  keywords using KeyBERT and masks them during pre-training, enhancing the model's
  focus on relevant domain information.
---

# Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords

## Quick Facts
- arXiv ID: 2307.07160
- Source URL: https://arxiv.org/abs/2307.07160
- Authors: 
- Reference count: 13
- Outperforms random masking and no adaptation baselines on text classification tasks with statistically significant improvements

## Executive Summary
This paper addresses domain adaptation challenges in pre-trained language models by proposing a task-agnostic method that selectively masks in-domain keywords during pre-training. The approach uses KeyBERT to identify domain-specific keywords and masks them with 75% probability, improving the model's focus on relevant domain information. Evaluated across three datasets (IMDB, Amazon pet product reviews, and PUBHEALTH) with BERT Base and Large, the method achieves significant accuracy and F1 score improvements over traditional random masking approaches. The keyword extraction overhead is reasonable at 7-15% of pre-training time.

## Method Summary
The method extracts domain-specific keywords from unlabeled target domain documents using KeyBERT, then filters them by frequency to remove noise. During in-domain pre-training, only these identified keywords are masked with 75% probability, while the rest of the tokens follow standard MLM masking. After pre-training, the model is fine-tuned on labeled data using standard hyperparameters. This approach preserves task-agnostic applicability while improving domain fit, offering a simple alternative to random masking for domain adaptation.

## Key Results
- Statistically significant improvements in accuracy and F1 scores across all three datasets compared to random masking baselines
- BERT Large with keyword masking achieves 0.939 accuracy on PUBHEALTH versus 0.920 with random masking
- Keyword extraction overhead is reasonable at 7-15% of pre-training time
- The approach is task-agnostic and applicable to any text classification task within a domain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masking domain-specific keywords improves domain adaptation compared to random masking.
- Mechanism: Keyword masking focuses the model's attention on information-dense domain-specific tokens during pre-training, leading to better capture of domain-specific patterns.
- Core assumption: Domain-specific keywords identified by KeyBERT are truly representative of the target domain's information.
- Evidence anchors:
  - [abstract] "Our approach selectively masks in-domain keywords, i.e., words that provide a compact representation of the target domain."
  - [section] "By applying token masking only to in-domain keywords, the meaningful information in the target domain is more directly captured by the PLM."

### Mechanism 2
- Claim: Selective masking preserves task-agnostic applicability while improving domain fit.
- Mechanism: By masking only in-domain keywords without using downstream task information, the method adapts the model to the domain while remaining applicable to any task within that domain.
- Core assumption: Domain-specific information is broadly useful across tasks within a domain, even without task-specific guidance.
- Evidence anchors:
  - [abstract] "Our approach selectively masks in-domain keywords, i.e., words that provide a compact representation of the target domain."
  - [section] "Our results reveal that the fine-tuned PLMs adapted using our in-domain pre-training strategy outperform PLMs that used in-domain pre-training with random masking."

### Mechanism 3
- Claim: The overhead of keyword extraction is reasonable relative to performance gains.
- Mechanism: The computational cost of keyword extraction (7-15% of pre-training time) is offset by the improved performance in downstream tasks.
- Core assumption: The performance improvement from better domain adaptation outweighs the additional computational cost of keyword extraction.
- Evidence anchors:
  - [abstract] "Further, the overhead of identifying in-domain keywords is reasonable, e.g., 7–15% of the pre-training time (for two epochs) for BERT Large."

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: The proposed method builds upon MLM by selectively masking tokens rather than random ones, requiring understanding of how MLM works.
  - Quick check question: What is the standard masking probability used in MLM, and how does it affect model learning?

- Concept: Domain Adaptation in NLP
  - Why needed here: The paper addresses domain adaptation, which requires understanding why models trained on general corpora may underperform on domain-specific tasks.
  - Quick check question: Why might a model trained on general text struggle with biomedical or legal documents compared to a model adapted to those domains?

- Concept: Keyword Extraction and Representation
  - Why needed here: The method relies on extracting domain-specific keywords using KeyBERT, requiring understanding of how keywords represent document content.
  - Quick check question: How does KeyBERT use BERT's contextualized embeddings to identify keywords that best represent a document?

## Architecture Onboarding

- Component map:
  Input documents → KeyBERT keyword extraction → Frequency-based filtering → Keyword list → Masking during pre-training → Adapted PLM → Fine-tuning → Downstream task performance
  KeyBERT (unmodified BERT Base) → Custom data collator (subclasses Hugging Face's whole word masking collator) → Pre-training pipeline → Fine-tuning pipeline

- Critical path:
  1. Extract keywords from unlabeled in-domain documents using KeyBERT
  2. Filter keywords by frequency to remove noise
  3. Mask identified keywords during pre-training with 75% probability
  4. Fine-tune the adapted model on labeled data
  5. Evaluate on test set

- Design tradeoffs:
  - Keyword vs. random masking: Better domain adaptation vs. simplicity and computational efficiency
  - Frequency threshold selection: Removing noise vs. preserving relevant rare keywords
  - Masking probability: Strong domain focus vs. maintaining context learning
  - Number of epochs: Sufficient adaptation vs. computational cost

- Failure signatures:
  - Poor downstream performance despite keyword masking: Keywords may not be truly representative or may be too noisy
  - High computational overhead: Keyword extraction may be taking too long relative to dataset size
  - Overfitting to domain: Model may perform well on in-domain test data but poorly on out-of-domain data
  - Inconsistent improvements: Method may not work well for all domains or tasks

- First 3 experiments:
  1. Run keyword extraction on a small sample of your target domain documents and manually verify if the extracted keywords are truly domain-representative
  2. Compare pre-training with keyword masking vs. random masking on a small labeled dataset to confirm performance improvements
  3. Test different frequency thresholds for keyword filtering to find the optimal balance between noise removal and information preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed keyword masking approach scale with domain size and keyword extraction complexity?
- Basis in paper: [explicit] The paper mentions keyword extraction adds 7-15% overhead for BERT Large, but does not explore scalability across larger datasets or more complex domains.
- Why unresolved: The paper only evaluates on three datasets, which may not represent extreme cases of domain size or keyword density.
- What evidence would resolve it: Empirical results showing performance and overhead across datasets varying in size by orders of magnitude and domains with varying keyword density.

### Open Question 2
- Question: What is the impact of using different keyword extraction models besides KeyBERT?
- Basis in paper: [inferred] The paper uses KeyBERT for keyword extraction but does not compare its performance against other keyword extraction methods.
- Why unresolved: The effectiveness of the approach might be tied to the specific keyword extraction method used, but this is not tested.
- What evidence would resolve it: Comparative experiments using alternative keyword extraction methods (e.g., TF-IDF, TextRank) and their impact on downstream task performance.

### Open Question 3
- Question: How does the proposed approach perform on non-text classification tasks?
- Basis in paper: [explicit] The paper explicitly states that further experiments are required to determine if findings translate to other NLP applications beyond text classification.
- Why unresolved: The paper only evaluates the approach on text classification tasks, leaving its applicability to other NLP tasks untested.
- What evidence would resolve it: Experiments applying the keyword masking approach to other NLP tasks (e.g., named entity recognition, question answering) and comparing performance against baselines.

## Limitations
- Domain generalization remains uncertain across diverse domains beyond the three tested datasets
- Frequency threshold selection for keyword filtering lacks a systematic methodology
- Computational overhead may scale differently with larger datasets or different hardware configurations

## Confidence
- High Confidence: The claim that keyword masking outperforms random masking for domain adaptation is well-supported by statistical significance tests (p-values < 0.05) across multiple datasets and PLM sizes.
- Medium Confidence: The assertion that the overhead is "reasonable" (7-15%) is supported by the reported measurements, but generalizability to other domains and hardware remains uncertain.
- Low Confidence: The paper's claim about task-agnostic applicability is theoretical rather than empirically validated across diverse downstream tasks.

## Next Checks
1. **Cross-domain Robustness Test**: Apply the keyword masking method to at least three additional domains (e.g., legal documents, financial news, and technical manuals) to verify whether the performance improvements generalize beyond the original three datasets.
2. **Threshold Sensitivity Analysis**: Systematically vary the frequency threshold parameter across multiple orders of magnitude (e.g., 2, 5, 10, 20, 50, 100) on a new domain and measure the impact on downstream performance.
3. **Computational Scaling Study**: Measure keyword extraction time and pre-training performance on progressively larger datasets (e.g., 10K, 100K, 1M documents) and different hardware configurations to establish how the 7-15% overhead ratio changes with scale.