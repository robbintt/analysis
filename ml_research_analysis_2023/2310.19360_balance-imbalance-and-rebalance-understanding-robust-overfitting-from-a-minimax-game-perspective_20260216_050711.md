---
ver: rpa2
title: 'Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a
  Minimax Game Perspective'
arxiv_id: '2310.19360'
source_url: https://arxiv.org/abs/2310.19360
tags:
- training
- adversarial
- features
- non-robust
- decay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the cause of robust overfitting in adversarial
  training through a dynamic minimax game perspective. The authors argue that robust
  overfitting occurs when the learning rate decay breaks the balance between the model
  trainer and the attacker in the adversarial training game, allowing the trainer
  to overfit non-robust features.
---

# Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective

## Quick Facts
- arXiv ID: 2310.19360
- Source URL: https://arxiv.org/abs/2310.19360
- Reference count: 40
- Primary result: ReBAT achieves superior robustness on various benchmark datasets with negligible robust overfitting even after long training

## Executive Summary
This paper investigates robust overfitting in adversarial training through a dynamic minimax game perspective. The authors argue that learning rate decay breaks the balance between the model trainer and attacker, enabling the trainer to memorize non-robust features that create shortcuts for test-time attacks. They propose three rebalancing strategies and introduce ReBAT, which combines bootstrapping objective with smaller learning rate decay factor. Experiments show ReBAT achieves superior robustness across CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets while maintaining negligible overfitting.

## Method Summary
ReBAT (ReBalanced Adversarial Training) addresses robust overfitting by rebalancing the adversarial training minimax game. The method combines a bootstrapping objective with a smaller learning rate decay factor to prevent the trainer from memorizing non-robust features. The approach maintains the equilibrium between attacker and trainer by either restricting the trainer's capacity through regularization, adjusting the learning rate decay factor, or strengthening the attacker's perturbation budget. ReBAT is evaluated on standard benchmark datasets using PreActResNet-18 and WideResNet-34-10 architectures.

## Key Results
- ReBAT achieves superior robustness on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets
- The method shows negligible robust overfitting even after long training periods
- ReBAT is effective when combined with other techniques like CutMix and knowledge distillation
- The approach maintains strong performance across different network architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LR decay breaks the balance in the adversarial training minimax game by giving the trainer a stronger local fitting ability, enabling it to memorize non-robust features.
- Mechanism: In the balanced state, the attacker and trainer reach an equilibrium where the trainer cannot fit non-robust features. When LR decay occurs, the smaller learning rate enhances the trainer's local fitting power, allowing it to memorize adversarial non-robust features that the attacker previously exploited.
- Core assumption: Adversarial training can be modeled as a dynamic minimax game between the trainer (minimizing loss) and attacker (maximizing loss).
- Break condition: When LR decay is applied, shifting the equilibrium toward trainer dominance

### Mechanism 2
- Claim: Memorizing non-robust features creates shortcuts for test-time adversarial attack, causing robust overfitting.
- Mechanism: After LR decay, the trainer learns false mappings of non-robust training features. These false mappings open shortcuts for test-time attackers because they no longer need to find non-robust features from different classes - features from the same class suffice due to the incorrect learned mapping.
- Core assumption: Non-robust features learned during training don't generalize to test data but create exploitable patterns.
- Break condition: When memorized non-robust features create test-time attack shortcuts

### Mechanism 3
- Claim: Rebalancing the minimax game by either restricting the trainer's capacity or strengthening the attacker mitigates robust overfitting.
- Mechanism: By either regularizing the trainer's fitting ability (e.g., through weight averaging, BoAT loss, or smaller LR decay) or strengthening the attacker (using larger perturbation budgets), the game balance is restored, preventing the trainer from memorizing non-robust features.
- Core assumption: The imbalance caused by LR decay can be counteracted by adjusting either player's strength
- Break condition: When either player's strength is adjusted to restore equilibrium

## Foundational Learning

- Concept: Minimax optimization
  - Why needed here: The paper models adversarial training as a minimax game between the trainer and attacker
  - Quick check question: In a minimax game, which player tries to maximize the loss and which tries to minimize it?

- Concept: Robust and non-robust features
  - Why needed here: Understanding how models use robust vs non-robust features is central to explaining why adversarial training overfits
  - Quick check question: What is the key difference between robust and non-robust features in terms of their behavior under adversarial perturbations?

- Concept: Learning rate decay effects
  - Why needed here: LR decay is identified as the trigger for breaking the minimax game balance
  - Quick check question: How does decreasing the learning rate typically affect a model's ability to fit training data?

## Architecture Onboarding

- Component map: Attacker → Generate adversarial examples → Trainer → Update weights → Balance monitor → Adjust rebalancing if needed
- Critical path: Attacker generates adversarial examples → Trainer updates model parameters using standard or modified loss functions → Balance monitor tracks training vs test robustness → Rebalancing controller adjusts LR decay factor, attacker strength, or applies regularization
- Design tradeoffs:
  - Stronger attacker improves robustness but may harm natural accuracy
  - Smaller LR decay factor reduces overfitting but may slow convergence
  - Weight averaging improves flatness but adds memory overhead
- Failure signatures:
  - Training robustness continues to increase while test robustness decreases
  - Confusion matrix becomes symmetric (indicating bilateral class correlation)
  - Target-class non-robust features in test adversarial examples decrease over time
- First 3 experiments:
  1. Run vanilla PGD-AT with standard LR decay and observe robust overfitting pattern
  2. Test ReBAT with BoAT loss and smaller LR decay factor on CIFAR-10
  3. Vary attacker strength (ε values) in ReBAT to find optimal balance between robustness and natural accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed ReBAT method perform when combined with other data augmentation techniques beyond CutMix, such as Mixup or RandAugment?
- Basis in paper: [inferred] The paper mentions that ReBAT can be combined with CutMix to achieve further improvements in robustness, but does not explore other data augmentation techniques.
- Why unresolved: The paper focuses on demonstrating the effectiveness of ReBAT in combination with CutMix, but does not provide a comprehensive evaluation of its performance with other data augmentation methods.
- What evidence would resolve it: Experiments comparing the performance of ReBAT with various data augmentation techniques, including CutMix, Mixup, and RandAugment, on multiple benchmark datasets and network architectures.

### Open Question 2
- Question: Can the ReBAT method be extended to handle other types of adversarial attacks, such as universal adversarial perturbations or black-box attacks?
- Basis in paper: [explicit] The paper primarily focuses on defending against white-box adversarial attacks using PGD and AutoAttack.
- Why unresolved: The paper does not address the performance of ReBAT against other types of adversarial attacks, which are also important in real-world scenarios.
- What evidence would resolve it: Experiments evaluating the robustness of ReBAT against various types of adversarial attacks, including universal adversarial perturbations and black-box attacks, on multiple benchmark datasets and network architectures.

### Open Question 3
- Question: How does the proposed ReBAT method scale to larger datasets and more complex models, such as those used in large-scale image classification tasks?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of ReBAT on benchmark datasets like CIFAR-10, CIFAR-100, and Tiny-ImageNet using PreActResNet-18 and WideResNet-34-10 architectures.
- Why unresolved: The paper does not provide insights into the scalability of ReBAT to larger datasets and more complex models, which are crucial for real-world applications.
- What evidence would resolve it: Experiments evaluating the performance of ReBAT on larger datasets and more complex models, such as ImageNet and ResNeXt architectures, to assess its scalability and effectiveness in large-scale image classification tasks.

## Limitations
- The central claim about LR decay specifically breaking minimax game balance relies heavily on theoretical reasoning rather than comprehensive empirical validation
- The mechanism linking LR decay to memorization of non-robust features would benefit from more direct causal evidence like feature visualization studies
- Claims about negligible robust overfitting need validation across more diverse settings and longer training durations

## Confidence

- **High confidence**: The observation that robust overfitting occurs when training robustness increases while test robustness decreases is well-documented and experimentally validated throughout the paper
- **Medium confidence**: The specific mechanism linking LR decay to memorization of non-robust features is plausible but would benefit from more direct causal evidence
- **Low confidence**: The claim that ReBAT achieves negligible robust overfitting even after long training needs validation across more diverse settings

## Next Checks

1. Test ReBAT on additional architectures (e.g., ResNet-50, EfficientNet) and datasets (e.g., SVHN, ImageNet) to verify generalization of the rebalancing effect
2. Conduct ablation studies specifically isolating the impact of each rebalancing strategy (LR decay factor, attacker strength, regularization) to quantify their individual contributions
3. Perform feature visualization experiments comparing non-robust feature learning trajectories between standard PGD-AT and ReBAT to directly validate the memorization hypothesis