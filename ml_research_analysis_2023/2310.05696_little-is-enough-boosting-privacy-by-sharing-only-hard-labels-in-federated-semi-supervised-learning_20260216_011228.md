---
ver: rpa2
title: 'Little is Enough: Boosting Privacy by Sharing Only Hard Labels in Federated
  Semi-Supervised Learning'
arxiv_id: '2310.05696'
source_url: https://arxiv.org/abs/2310.05696
tags:
- learning
- dataset
- local
- fedct
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes federated co-training (FedCT), a privacy-preserving
  federated learning method that shares only hard labels on a public unlabeled dataset
  instead of model parameters or soft predictions. The server forms a consensus of
  these labels, which clients use as pseudo-labels for local training.
---

# Little is Enough: Boosting Privacy by Sharing Only Hard Labels in Federated Semi-Supervised Learning

## Quick Facts
- arXiv ID: 2310.05696
- Source URL: https://arxiv.org/abs/2310.05696
- Reference count: 39
- Primary result: FedCT achieves model quality comparable to federated learning while substantially improving privacy through hard label sharing

## Executive Summary
The paper introduces Federated Co-Training (FedCT), a privacy-preserving federated learning method that shares only hard labels on a public unlabeled dataset rather than model parameters or soft predictions. The server aggregates these hard labels via majority vote to create consensus pseudo-labels, which clients use for local training. This approach achieves model quality comparable to standard federated learning while significantly improving privacy, as demonstrated by membership inference attack vulnerability scores around 0.5 (random guessing) versus 0.7+ for baselines. FedCT also enables training of interpretable models like decision trees and XGBoost in federated settings, which is not possible with traditional gradient-based methods.

## Method Summary
FedCT operates by having clients train local models on their private data and a public unlabeled dataset with pseudo-labels. Instead of sharing model parameters or gradients, clients generate hard predictions (class labels) on the public unlabeled dataset and send these to the server. The server aggregates these predictions via majority vote to form consensus labels, which are then broadcast back to all clients. Clients use these consensus labels as pseudo-labels for training on the public dataset in the next round. The method converges under a linear training accuracy assumption and can achieve differential privacy using an XOR mechanism suitable for binary prediction data.

## Key Results
- Test accuracy comparable to FedAVG and Distributed Distillation across multiple datasets (CIFAR10, FashionMNIST, SVHN)
- Privacy vulnerability (ROC AUC of membership inference attacks) around 0.5 for FedCT versus 0.7+ for baselines
- Enables training of non-differentiable models (decision trees, random forests, XGBoost) in federated settings
- Communication efficiency improvements by transmitting prediction vectors rather than model parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hard label sharing improves privacy by reducing information leakage compared to soft predictions or model parameters
- Mechanism: Clients share only hard labels (class predictions) on a public unlabeled dataset rather than gradients, parameters, or soft predictions. The server aggregates these labels via majority vote to create pseudo-labels for local training.
- Core assumption: Majority voting on hard labels leaks less information about local training data than sharing soft predictions or model parameters
- Evidence anchors: [abstract] "sharing only hard labels on a public unlabeled dataset instead of model parameters or soft predictions"; [section] "sharing hard labels not only improves privacy over both federated averaging and distributed distillation, but also allows us to use any supervised learning method for local training"

### Mechanism 2
- Claim: FedCT enables training of non-differentiable models in federated settings
- Mechanism: By sharing only hard labels rather than model parameters or gradients, FedCT removes the requirement for gradient-based local training methods. This allows clients to use any supervised learning algorithm including decision trees, random forests, and XGBoost.
- Core assumption: Hard label sharing contains sufficient information for effective local model training without requiring gradient compatibility
- Evidence anchors: [abstract] "allows the use of local models that are not suitable for parameter aggregation in traditional federated learning, such as gradient-boosted decision trees, rule ensembles, and random forests"; [section] "sharing hard labels not only improves privacy over both federated averaging and distributed distillation, but also allows us to use any supervised learning method for local training"

### Mechanism 3
- Claim: Differentially private FedCT can achieve good utility-privacy tradeoff through XOR mechanism
- Mechanism: The XOR mechanism applies randomized noise to binary prediction matrices through XOR operations with Bernoulli-distributed random matrices, providing differential privacy for the binary data while maintaining model utility.
- Core assumption: The XOR mechanism is effective for binary data privacy while preserving enough signal for model convergence
- Evidence anchors: [section] "DP-F EDCT achieves a high utility in terms of test accuracy even for moderate-to-high privacy levels ϵ with an accuracy of 0.8 for ϵ = 0.1"

## Foundational Learning

- Concept: Semi-supervised learning and co-training
  - Why needed here: FedCT fundamentally relies on using unlabeled data with pseudo-labels generated through consensus, which is a semi-supervised approach
  - Quick check question: How does the consensus mechanism ensure that pseudo-labels improve over time rather than degrading?

- Concept: Differential privacy and sensitivity analysis
  - Why needed here: The paper uses differential privacy with the XOR mechanism for binary data, requiring understanding of privacy guarantees and sensitivity bounds
  - Quick check question: What is the relationship between the sensitivity bound and the epsilon parameter in the XOR mechanism?

- Concept: Federated learning convergence analysis
  - Why needed here: The paper provides theoretical convergence analysis for FedCT, which requires understanding of federated learning convergence conditions
  - Quick check question: Under what conditions does the convergence proof guarantee that consensus labels will stabilize?

## Architecture Onboarding

- Component map: Clients -> Server -> Clients (via hard label sharing and consensus aggregation)

- Critical path:
  1. Clients train local models on private data + pseudo-labeled public data
  2. Clients generate predictions on public unlabeled dataset
  3. Clients send predictions to server
  4. Server aggregates predictions via majority vote
  5. Server sends consensus labels back to clients
  6. Clients use consensus as pseudo-labels for next training round

- Design tradeoffs:
  - Communication vs privacy: Hard labels reduce privacy leakage but require transmitting entire prediction vectors
  - Model flexibility vs convergence: Allowing heterogeneous models increases flexibility but may affect consensus quality
  - Privacy budget vs utility: Higher privacy (lower epsilon) reduces model accuracy through increased noise

- Failure signatures:
  - Poor consensus quality: High variance in local predictions indicates either too little private data or model heterogeneity issues
  - Slow convergence: Linear increase in training accuracy assumption may not hold for certain model types
  - Privacy degradation: If unlabeled dataset is too small or class distribution is skewed, majority voting may leak information

- First 3 experiments:
  1. Run FedCT without privacy mechanisms on a simple dataset (e.g., FashionMNIST) to verify convergence and consensus formation
  2. Compare test accuracy with FedAVG and Distributed Distillation to establish baseline performance
  3. Test interpretable models (decision tree, random forest) to verify the non-differentiable model capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the XOR mechanism's performance compare to other differential privacy mechanisms when applied to federated co-training with binary data?
- Basis in paper: [explicit] The paper mentions using an XOR mechanism for binary data and compares it to standard DP mechanisms like Gaussian and Laplacian.
- Why unresolved: The paper does not provide a direct comparison of the XOR mechanism's performance against other DP mechanisms in the context of federated co-training.
- What evidence would resolve it: A study comparing the XOR mechanism with other DP mechanisms on federated co-training tasks, focusing on privacy-utility trade-offs and convergence behavior.

### Open Question 2
- Question: Can federated co-training be effectively applied to regression tasks, and if so, what consensus mechanisms would be most appropriate?
- Basis in paper: [inferred] The paper focuses on classification problems and mentions that different consensus mechanisms could be explored for regression.
- Why unresolved: The paper does not explore federated co-training for regression tasks or evaluate different consensus mechanisms for such scenarios.
- What evidence would resolve it: Experiments applying federated co-training to regression tasks with various consensus mechanisms, measuring performance and privacy benefits.

### Open Question 3
- Question: How does federated co-training perform when clients use heterogeneous model architectures, and what impact does this have on convergence and privacy?
- Basis in paper: [explicit] The paper mentions that federated co-training allows clients to use different models but does not explore this aspect in detail.
- Why unresolved: The paper does not provide empirical results on the effects of heterogeneous model architectures on federated co-training's performance and privacy.
- What evidence would resolve it: Experiments with federated co-training using diverse model architectures across clients, analyzing convergence rates and privacy vulnerabilities.

## Limitations
- Convergence proof relies on linear training accuracy assumption that may not hold for all model types
- XOR mechanism for differential privacy is novel but lacks extensive validation on diverse binary data distributions
- Membership inference attack methodology uses a relatively simple shadow model approach that may not capture all attack vectors

## Confidence

- **High confidence**: The core mechanism of hard label sharing and consensus-based pseudo-labeling is well-established and empirically validated across multiple datasets
- **Medium confidence**: The privacy benefits over soft predictions are demonstrated but could be further strengthened with additional attack models
- **Medium confidence**: The convergence analysis provides theoretical guarantees but relies on assumptions that need broader validation
- **Medium confidence**: The XOR mechanism for DP is promising but requires more extensive testing across different data distributions

## Next Checks

1. **Convergence validation across model types**: Test FedCT with multiple model architectures (neural networks, decision trees, random forests) on the same dataset to verify the linear training accuracy assumption holds across heterogeneous models.

2. **Comprehensive privacy analysis**: Evaluate FedCT against more sophisticated membership inference attacks beyond the shadow model approach, including reconstruction attacks and attribute inference attacks, to validate the claimed privacy improvements.

3. **Communication efficiency measurement**: Implement the actual communication protocol and measure bandwidth usage across different unlabeled dataset sizes and model complexities to verify the theoretical communication bounds.