---
ver: rpa2
title: The Potential and Pitfalls of using a Large Language Model such as ChatGPT
  or GPT-4 as a Clinical Assistant
arxiv_id: '2307.08152'
source_url: https://arxiv.org/abs/2307.08152
tags:
- gpt-4
- clinical
- chatgpt
- extraction
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared ChatGPT and GPT-4 to classical machine learning
  workflows in identifying patients with specific diseases (COPD, CKD, PBC, HSV, and
  cancer cachexia) from real-world EHR data, and in providing clinical diagnostic
  assistance for hypothetical patients. Using MIMIC-III data and clinical vignettes,
  GPT-4 with chain-of-thought and few-shot prompting achieved up to 96% F1 scores
  in disease classification, outperforming or matching ML models.
---

# The Potential and Pitfalls of using a Large Language Model such as ChatGPT or GPT-4 as a Clinical Assistant

## Quick Facts
- arXiv ID: 2307.08152
- Source URL: https://arxiv.org/abs/2307.08152
- Authors: 
- Reference count: 2
- Key outcome: GPT-4 with chain-of-thought and few-shot prompting achieved up to 96% F1 scores in disease classification from EHR data and correctly diagnosed patients 3 out of 4 times in clinical vignettes.

## Executive Summary
This study evaluates GPT-4's performance in identifying patients with specific diseases from real-world EHR data and providing clinical diagnostic assistance. Using MIMIC-III data and clinical vignettes, GPT-4 with chain-of-thought and few-shot prompting demonstrated high accuracy in disease classification (up to 96% F1 scores), outperforming or matching classical machine learning workflows. For clinical assistance, GPT-4 arrived at correct diagnoses three out of four times when evaluating hypothetical patients. However, the model occasionally provided factually incorrect statements, overlooked key findings, and recommended unnecessary investigations. Despite these limitations, the study highlights GPT-4's potential for scalable healthcare applications due to reduced data and time requirements compared to traditional ML approaches.

## Method Summary
The study compared GPT-4 to classical ML workflows using MIMIC-III EHR data for disease identification and clinical vignettes for diagnostic assistance. For disease classification, GPT-4 was prompted with discharge summaries, clinical guidelines, and evaluation instructions using chain-of-thought and few-shot prompting techniques. Baseline ML models used extraction+rules/prediction workflows. For clinical assistance, GPT-4 was evaluated on 31 COPD case scenarios across eight steps of a generic clinical care pathway, with responses assessed by physicians on scientific correctness, comprehension, content, and bias using 4-point Likert scales.

## Key Results
- GPT-4 achieved up to 96% F1 scores in disease classification, outperforming or matching ML models
- GPT-4 correctly diagnosed patients 3 out of 4 times in clinical vignettes
- Model performance varied significantly across different clinical assistance tasks, with perfect scores in 57-82% of questions depending on the evaluation domain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 achieves high disease classification accuracy by combining chain-of-thought and few-shot prompting with elaborate clinical guidelines.
- Mechanism: The elaborate guideline provides both task structure (chain-of-thought) and exemplar mappings between clinical features and disease labels (few-shot), which guide the model to follow a step-by-step reasoning process and reduce reliance on implicit knowledge.
- Core assumption: GPT-4 can reliably follow the reasoning steps laid out in the guideline and use provided examples to generalize to new patient records.
- Evidence anchors:
  - [abstract] "GPT-4 across disease classification tasks with chain of thought and few-shot prompting can achieve performance as high as 96% F1 scores."
  - [section] "When using the same input clinical guideline for the same task, GPT-4 compared to ChatGPT could be 10-18% higher (in terms of absolute F1 scores) in classification performance. For a given model, simply improving the level of detail in the guideline using best prompting practices such as breaking down the process of assigning a class into subtasks (i.e., chain-of-thought) and providing examples for each class (few-shot) can improve classification performance by up to nearly 30% (absolute % increase in F1)."
  - [corpus] Weak: Related papers focus on general LLM utility in medicine, not on prompting strategies specifically.
- Break condition: If the input guideline omits key disease features or if the examples provided are not representative, the model may produce inaccurate classifications or hallucinate missing information.

### Mechanism 2
- Claim: GPT-4 generates interpretable rationales for its predictions, which can reveal the model's decision-making process.
- Mechanism: The model is prompted to provide a natural language explanation for each prediction, drawing from the input clinical guideline and patient EHR features. These rationales can be analyzed to assess adherence to the guideline and detect errors.
- Core assumption: The model's generated rationales accurately reflect the reasoning behind its predictions and are not fabricated.
- Evidence anchors:
  - [abstract] "Among the clinical case scenarios for diagnosis, the GPT-4 model demonstrated a strong understanding of the given patient information and generated rationales that were coherent and well-supported."
  - [section] "For interpretability of GPT-4 predicted disease class, the input prompt instructed the model to provide a rationale for the assigned prediction... Qualitatively, it appears that for correct predictions (true positives and true negatives), the rationales provided for assigning a particular disease class were usually scientifically correct, exhaustive in context with minimal bias, demonstrating appropriate task comprehension, correct information retrieval, and good adherence to the provided clinical guideline."
  - [corpus] Weak: No direct evidence from corpus on rationale generation quality.
- Break condition: If the model hallucinates or fabricates information in its rationale, or if the rationale does not align with the actual features used for classification, the interpretability is compromised.

### Mechanism 3
- Claim: GPT-4 can provide clinical assistance across multiple steps of patient evaluation, but performance varies by task domain and step complexity.
- Mechanism: The model is prompted with specific questions aligned to each step of a generic clinical care pathway, and its responses are evaluated by physicians on scientific correctness, comprehension, content, and bias.
- Core assumption: GPT-4 can accurately retrieve and reason with relevant clinical information from the patient scenario and generate appropriate recommendations.
- Evidence anchors:
  - [abstract] "For patient assessment, GPT-4 was accurate in arriving at a diagnosis three out of four times."
  - [section] "Our results show that there is some variability in scores assigned by different human evaluators when using x-point likert scales for assessment. Overall, the performance of ChatGPT in providing clinical assistance across the eight-steps was good but varied greatly between the domains assessed, achieving a perfect score of 3 (mean across evaluators), in 229 (57.25%) questions in scientific correctness, 299 (74.75%) in the comprehension, retrieval, and reasoning domain, 109 (27.25%) in the content domain, and 329 (82.25%) in the bias domain."
  - [corpus] Weak: Related papers focus on general LLM utility in medicine, not on specific clinical care pathway tasks.
- Break condition: If the model fails to retrieve crucial information, provides factually incorrect statements, or recommends unnecessary investigations, its utility in clinical assistance is limited.

## Foundational Learning

- Concept: Prompt engineering techniques (chain-of-thought, few-shot prompting)
  - Why needed here: These techniques significantly improve GPT-4's performance on disease classification and clinical assistance tasks by providing task structure and exemplars.
  - Quick check question: How does providing a step-by-step reasoning process (chain-of-thought) in the prompt affect GPT-4's ability to classify patients with specific diseases?

- Concept: Interpretability and explainability of AI models
  - Why needed here: Understanding GPT-4's decision-making process through generated rationales is crucial for assessing its reliability and trustworthiness in clinical applications.
  - Quick check question: What are the key elements to look for when evaluating the quality and reliability of GPT-4's generated rationales for its predictions?

- Concept: Clinical care pathways and patient evaluation steps
  - Why needed here: The study evaluates GPT-4's performance across multiple steps of a generic clinical care pathway, requiring an understanding of the typical tasks and information needs at each step.
  - Quick check question: What are the key differences between the types of questions and information needs at early steps (e.g., history-taking) versus later steps (e.g., diagnosis and management) of a clinical care pathway?

## Architecture Onboarding

- Component map: Patient EHR data -> GPT-4 model with prompt engineering -> Disease classifications/clinical assistance responses -> Physician evaluation
- Critical path: 1. Prepare patient EHR data and clinical guidelines 2. Design and implement prompt engineering techniques 3. Run GPT-4 model on patient identification and clinical assistance tasks 4. Collect and analyze model outputs 5. Evaluate model performance through physician assessment
- Design tradeoffs:
  - Model choice: GPT-4 vs. ChatGPT - GPT-4 generally performs better but may have higher costs and latency
  - Prompt design: More detailed guidelines improve performance but increase prompt complexity and token usage
  - Token limits: Restricting input to discharge summaries may limit model performance if crucial information is in other notes
- Failure signatures:
  - Inaccurate disease classifications due to missing or incorrect information in the input guideline
  - Fabricated or hallucinated information in generated rationales
  - Biased or inappropriate recommendations in clinical assistance responses
- First 3 experiments:
  1. Compare GPT-4 performance on disease classification using baseline vs. elaborate clinical guidelines with chain-of-thought and few-shot prompting
  2. Evaluate the reliability of GPT-4's generated rationales for correct and incorrect predictions by having physicians assess their scientific accuracy and adherence to the guideline
  3. Assess GPT-4's performance on clinical assistance tasks across different steps of the care pathway, identifying areas of strength and weakness for targeted improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would GPT-4's performance compare to ML models when provided with all relevant clinical notes (not just discharge summaries) for patient identification tasks?
- Basis in paper: [explicit] The paper notes that GPT-4's performance was limited because token constraints restricted input to only discharge summaries, while extraction + rules models used all relevant patient notes.
- Why unresolved: The study intentionally limited GPT-4 input to discharge summaries due to token constraints, preventing direct comparison with full clinical note access.
- What evidence would resolve it: A study providing GPT-4 with all relevant clinical notes (not just discharge summaries) for the same patient identification tasks, with performance metrics compared to extraction + rules models.

### Open Question 2
- Question: Can local deployment of LLM models like GPT-4 address privacy concerns while maintaining performance comparable to commercial APIs?
- Basis in paper: [explicit] The paper mentions privacy concerns with commercial LLM APIs and suggests local deployment as a potential solution, but notes uncertainty about performance and infrastructure costs.
- Why unresolved: The paper only discusses local deployment as a theoretical possibility without empirical validation of performance or cost-effectiveness.
- What evidence would resolve it: Empirical studies comparing the performance of locally deployed LLM models versus commercial APIs on the same healthcare tasks, including cost analysis and privacy assessments.

### Open Question 3
- Question: How can word network analyses and other quantitative NLP metrics be developed to provide unbiased evaluation of AI-generated clinical text?
- Basis in paper: [explicit] The paper suggests that current human adjudication is labor-intensive and subjective, and calls for future studies to explore quantitative NLP metrics for evaluating AI-generated text quality.
- Why unresolved: While the paper identifies this need, it doesn't propose specific methodologies or metrics for such evaluation.
- What evidence would resolve it: Development and validation of quantitative NLP metrics specifically designed to evaluate clinical accuracy, completeness, and appropriateness of AI-generated medical text, with comparison to human evaluation.

## Limitations
- The study's reliance on physician-provided gold-standard labels for EHR data introduces potential bias
- Evaluation of clinical assistance performance is limited to a small set of 31 hypothetical COPD case scenarios
- The study does not address potential model drift or performance degradation over time as clinical guidelines and patient populations evolve

## Confidence

- **High Confidence**: GPT-4's ability to achieve high F1 scores (up to 96%) on disease classification tasks using chain-of-thought and few-shot prompting techniques.
- **Medium Confidence**: GPT-4's performance in providing clinical assistance across multiple steps of a patient evaluation, with variability observed between different task domains.
- **Low Confidence**: The generalizability of GPT-4's clinical assistance performance to real-world patient data and other disease domains beyond the evaluated COPD case scenarios.

## Next Checks
1. Conduct a larger-scale evaluation of GPT-4's clinical assistance performance using real patient data from multiple disease domains to assess generalizability and robustness.
2. Implement a continuous monitoring system to track GPT-4's performance over time and detect potential model drift or degradation in accuracy as clinical guidelines and patient populations evolve.
3. Investigate the impact of using different prompt engineering techniques and varying levels of detail in clinical guidelines on GPT-4's performance and interpretability across a wider range of clinical tasks.