---
ver: rpa2
title: 'Crossed-IoT device portability of Electromagnetic Side Channel Analysis: Challenges
  and Dataset'
arxiv_id: '2310.03119'
source_url: https://arxiv.org/abs/2310.03119
tags:
- data
- device
- accuracy
- devices
- em-sca
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines device variability in electromagnetic side-channel
  analysis for IoT forensics, focusing on multi-core processor effects. The authors
  collect EM datasets from Dragonboard and Echo Show devices, running different software
  activities while capturing EM traces.
---

# Crossed-IoT device portability of Electromagnetic Side Channel Analysis: Challenges and Dataset

## Quick Facts
- arXiv ID: 2310.03119
- Source URL: https://arxiv.org/abs/2310.03119
- Reference count: 21
- Multi-core processors cause device variability that degrades ML model accuracy across IoT devices, but transfer learning can mitigate this effect.

## Executive Summary
This study investigates device variability in electromagnetic side-channel analysis for IoT forensics, focusing on how multi-core processor architectures affect ML model portability. The authors collected EM datasets from Dragonboard and Echo Show devices running different software activities while capturing EM traces. They found that ML models trained on one device showed poor performance when tested on similar devices, with accuracy dropping significantly. Using transfer learning techniques improved cross-device performance substantially, achieving up to 87% accuracy. The study highlights the challenges of device variability and demonstrates transfer learning as an effective solution for improving ML model portability in EM-SCA applications.

## Method Summary
The authors collected EM datasets from Dragonboard 410c and Echo Show 5 devices using a HackRF One SDR with H-loop antenna, sampling at 20 MHz with 1.2 GHz center frequency. Activities included Print, Math, Memory, and I/O tasks on Dragonboard, and time, music, weather, and radio on Echo Show. EM traces were preprocessed using STFT (4096 FFT size, 512 overlap) and normalized with MinMaxScaler. A Keras sequential model with 6 hidden layers was trained for 30 epochs with 10% validation split. Transfer learning was applied by freezing pre-trained layers and adding new dense/output layers for cross-device adaptation.

## Key Results
- Cross-device accuracy dropped from 91% to 25% when testing models trained on one device against another without transfer learning
- Transfer learning improved cross-device accuracy from 25% to 87% for Dragonboard devices
- Reducing activities from 4 to 2 improved accuracy from 38% to 87%, showing activity correlation impacts classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-core SoC architectures introduce device variability that degrades ML model accuracy across devices.
- Mechanism: When multiple cores operate simultaneously, their electromagnetic emissions overlap and interfere, making it difficult to isolate the emissions associated with a specific activity. This interference pattern varies between devices due to differences in core scheduling, thermal conditions, and hardware implementation.
- Core assumption: EM emissions from a single-core activity are sufficiently distinct and stable to enable ML classification, but multi-core interference breaks this stability.
- Evidence anchors:
  - [abstract] "Our study then addresses one of the most important limitation, which is caused by the multi-core architecture of the processors (aka. System-On-Chip)."
  - [section] "Our hypothesis is that using multiple cores in SoC architecture can affect EM emissions and make it more difficult to identify the right source of emissions for a particular activity."

### Mechanism 2
- Claim: Transfer learning effectively mitigates cross-device variability by adapting pre-trained models to new device characteristics.
- Mechanism: Pre-trained models learn general features from one device's EM patterns. When fine-tuned on a small dataset from a new device, the model adapts these features to account for device-specific variations while retaining the core activity-recognition capability.
- Core assumption: EM patterns contain common features across devices that transfer learning can exploit, even when absolute values differ.
- Evidence anchors:
  - [abstract] "Using transfer learning techniques improves cross-device performance substantially, achieving up to 87% accuracy."
  - [section] "We utilized a pre-trained model as the starting point and fine-tuned it on a new dataset or task, which allowed the model to benefit from the features learned by the pre-trained model."

### Mechanism 3
- Claim: Reducing the number of activities improves classification accuracy by reducing correlation between classes.
- Mechanism: Each additional activity increases the similarity between classes' EM patterns, making them harder to distinguish. Fewer activities create more separable feature spaces, improving model discrimination.
- Core assumption: EM emissions from different activities have overlapping frequency components that increase with the number of activities.
- Evidence anchors:
  - [section] "Another hypothesis is the number of activities, which is also a factor that affects the ML model's accuracy" and results showing accuracy improving from 38% (4 activities) to 87% (2 activities).
  - [section] "We notices that the device variability is directly affected on the model performance" and correlation analysis showing higher activity correlation with more activities.

## Foundational Learning

- Concept: Electromagnetic Side-Channel Analysis (EM-SCA)
  - Why needed here: Understanding how EM emissions correlate with device operations is fundamental to designing and interpreting ML models for device activity classification.
  - Quick check question: What physical phenomenon allows EM-SCA to reveal information about device operations without direct access?

- Concept: Transfer Learning in Deep Learning
  - Why needed here: Transfer learning is the primary method proposed to overcome cross-device accuracy degradation, requiring understanding of how pre-trained models can be adapted to new domains.
  - Quick check question: What is the key difference between transfer learning and training a model from scratch on new data?

- Concept: Signal Processing (STFT, FFT)
  - Why needed here: EM traces require preprocessing to extract features suitable for ML, and understanding these transforms is crucial for interpreting results and troubleshooting.
  - Quick check question: Why is STFT preferred over simple FFT for analyzing time-varying EM emissions from IoT devices?

## Architecture Onboarding

- Component map: HackRF One SDR + H-loop antenna -> Raw I/Q samples -> STFT preprocessing -> MinMaxScaler normalization -> Keras sequential model -> Transfer learning fine-tuning
- Critical path: EM trace collection -> Feature extraction -> Model training/testing -> Cross-device validation -> Transfer learning adaptation
- Design tradeoffs: Single-core vs. multi-core data collection (accuracy vs. real-world representativeness), dataset size vs. training time, model complexity vs. generalization
- Failure signatures: Large gap between training and testing accuracy (overfitting), consistently low accuracy across devices (fundamental feature mismatch), high correlation between activity classes (feature ambiguity)
- First 3 experiments:
  1. Reproduce baseline: Train on DragonBoard 1, test on same device to establish performance ceiling (~96% accuracy)
  2. Cross-device test: Train on DragonBoard 1, test on DragonBoard 2 without transfer learning to measure baseline degradation
  3. Transfer learning validation: Fine-tune pre-trained DragonBoard 1 model on DragonBoard 2 data and measure accuracy improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which multi-core processors create device variability in EM-SCA data, and can this be mathematically modeled or quantified?
- Basis in paper: [explicit] The paper states "Using multiple cores can affect electromagnetic (EM) emissions" and hypothesizes that multi-core SoC architecture affects EM emissions, but doesn't provide a detailed mechanistic explanation.
- Why unresolved: The paper identifies this as a key research question (R1) but only provides experimental evidence of the problem rather than explaining the underlying physical or signal processing reasons for why multiple cores create variability.
- What evidence would resolve it: Detailed signal analysis showing how emissions from different cores interfere/interact, or mathematical models quantifying how core-switching patterns affect EM signatures.

### Open Question 2
- Question: How does the number of concurrent activities running on an IoT device affect the correlation between EM traces, and is there a threshold beyond which classification becomes impractical?
- Basis in paper: [explicit] The paper notes "As the number of activities increases, the correlation between them also increases, making distinguishing between activities based on EM emissions more challenging."
- Why unresolved: While the paper shows experimental results with different numbers of activities, it doesn't establish a theoretical framework for understanding when activity correlation becomes too high for reliable classification, or what factors determine this threshold.
- What evidence would resolve it: Systematic experiments varying both the number and similarity of activities, combined with correlation analysis to identify practical limits.

### Open Question 3
- Question: What are the optimal transfer learning architectures and fine-tuning strategies specifically for EM-SCA data across different IoT device models?
- Basis in paper: [explicit] The paper demonstrates that transfer learning improves cross-device performance but doesn't explore which architectures work best or provide guidelines for fine-tuning.
- Why unresolved: The paper uses a generic transfer learning approach but doesn't compare different architectures, layer-freezing strategies, or fine-tuning protocols specifically for EM-SCA applications.
- What evidence would resolve it: Comparative studies testing different pre-trained models, layer-freezing schemes, and fine-tuning strategies on the same cross-device datasets.

## Limitations
- Limited device diversity: Only two device types tested (Dragonboard 410c and Echo Show 5), limiting generalizability to broader IoT ecosystem
- No physical mechanism explanation: The study identifies multi-core variability as a problem but doesn't explain the underlying physical or signal processing reasons
- Transfer learning data requirement: The approach still requires some target device data, which may not always be available in forensic scenarios

## Confidence
- Confidence: High in the fundamental finding that device variability degrades cross-device EM-SCA performance
- Confidence: Medium in the transfer learning solution's effectiveness, though residual challenges remain
- Confidence: Low in the generalizability of findings beyond the specific devices and activities tested

## Next Checks
1. **Physical mechanism validation**: Conduct controlled experiments varying specific hardware parameters (clock speeds, voltage levels, thermal conditions) to isolate which factors contribute most to device variability.

2. **Alternative device adaptation methods**: Compare transfer learning against other domain adaptation techniques like domain adversarial neural networks or meta-learning approaches to establish if transfer learning is optimal.

3. **Longitudinal stability testing**: Monitor model performance over extended periods with the same devices to assess whether device aging, thermal cycling, or environmental changes affect the learned patterns.