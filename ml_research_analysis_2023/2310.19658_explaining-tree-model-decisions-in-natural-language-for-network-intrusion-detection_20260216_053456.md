---
ver: rpa2
title: Explaining Tree Model Decisions in Natural Language for Network Intrusion Detection
arxiv_id: '2310.19658'
source_url: https://arxiv.org/abs/2310.19658
tags:
- decision
- tree
- explanations
- explanation
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach for generating explanations
  for decision tree-based network intrusion detection (NID) systems using large language
  models (LLMs). The key idea is to use LLMs to generate natural language explanations
  that abstract away the underlying mechanics of the decision tree while incorporating
  background knowledge to explain features and classes.
---

# Explaining Tree Model Decisions in Natural Language for Network Intrusion Detection

## Quick Facts
- arXiv ID: 2310.19658
- Source URL: https://arxiv.org/abs/2310.19658
- Reference count: 23
- Key outcome: LLM-based explanations for decision tree-based NID systems significantly outperform rule-based explanations in human comprehension and preference metrics

## Executive Summary
This paper proposes a novel approach for generating natural language explanations for decision tree-based network intrusion detection (NID) systems using large language models (LLMs). The key innovation is using LLMs to abstract away the mechanics of decision trees while incorporating background knowledge to explain features and classes. A new human evaluation framework with counterfactual quiz questions measures understanding. Experiments on the NF-BoT NID dataset demonstrate that LLM-based explanations significantly outperform rule-based explanations, leading to higher quiz scores and better human ratings of readability, quality, and use of background knowledge.

## Method Summary
The approach involves training a decision tree classifier (max depth 4) on the NF-BoT NID dataset, then generating explanations using either rule-based templates or LLM prompts. The LLM (GPT-4) receives prompts containing the decision tree structure, path information, and instructions to incorporate background knowledge. For evaluation, counterfactual quiz questions are automatically generated from decision tree paths and administered to human evaluators who are also asked to rate explanation quality. The framework measures both objective understanding (quiz scores) and subjective preferences (readability, quality, background knowledge usage).

## Key Results
- LLM-generated decision tree explanations correlate highly with human ratings of readability, quality, and use of background knowledge
- Human evaluators achieve significantly higher quiz scores when using LLM-based explanations compared to rule-based ones
- Evaluators strongly preferred LLM-based explanations for readability, quality, and background knowledge incorporation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based explanations outperform rule-based ones by incorporating background knowledge and abstracting decision tree mechanics
- Mechanism: LLMs leverage their pre-trained knowledge to generate contextually rich explanations that explain not just what the decision tree did, but why certain features matter
- Core assumption: LLMs have sufficient pre-training knowledge relevant to network intrusion detection concepts
- Evidence anchors:
  - [abstract] "LLM generated decision tree explanations correlate highly with human ratings of readability, quality, and use of background knowledge"
  - [section] "LLM-based decision tree explanations (LLM-DTE) seek to remedy the issues with Rule-based explanations by abstracting away the underlying mechanics of the decision tree as well as providing background knowledge"
  - [corpus] Weak evidence - related papers focus on ML/NID performance but don't address explanation quality
- Break condition: If LLM lacks relevant pre-training knowledge about network security concepts, generated explanations would be superficial or inaccurate

### Mechanism 2
- Claim: Human evaluators understand decision tree inference better when given LLM-based explanations compared to rule-based ones
- Mechanism: The counterfactual quiz framework measures understanding by testing whether evaluators can predict how changes in input features would affect outcomes
- Core assumption: Correctly answering counterfactual questions indicates genuine understanding of decision boundaries
- Evidence anchors:
  - [abstract] "LLM generated decision tree explanations...leading to higher quiz scores and better human ratings"
  - [section] "we construct a set of counterfactual quiz questions based on T. Human annotators are then given a natural language explanation and quizzed with the questions to evaluate their understanding"
  - [corpus] No direct evidence - related work focuses on detection performance, not explanation comprehension
- Break condition: If quiz questions don't accurately capture the nuances of decision tree inference, the evaluation framework would misrepresent understanding

### Mechanism 3
- Claim: The counterfactual quiz framework correlates with human preferences for explanation quality
- Mechanism: Evaluators rate explanations on readability, quality, and background knowledge usage, and these ratings align with their quiz performance
- Core assumption: Human subjective preferences for explanation quality reflect actual understanding of the decision process
- Evidence anchors:
  - [abstract] "LLM generated decision tree explanations correlate highly with human ratings of readability, quality, and use of background knowledge"
  - [section] "Evaluators preferred LLM-based explanations to Rule-based ones for readability, quality, and background knowledge"
  - [corpus] No evidence - related papers don't discuss human evaluation frameworks for explanations
- Break condition: If human evaluators' subjective preferences don't align with their actual understanding (measured by quiz scores), the framework would be misleading

## Foundational Learning

- Concept: Decision tree inference mechanics
  - Why needed here: Understanding how decision trees traverse nodes based on feature thresholds is essential for both generating and evaluating explanations
  - Quick check question: If a decision tree node checks "protocol == 17" (UDP), what happens when the input protocol is 6 (TCP)?

- Concept: Network intrusion detection feature semantics
  - Why needed here: Features like "flow duration," "source port," and "protocol" have specific meanings in NID that must be conveyed in explanations
  - Quick check question: Why would "source port: 22" (SSH) be significant in distinguishing benign from threat traffic?

- Concept: Counterfactual reasoning
  - Why needed here: The evaluation framework tests understanding by asking how changing feature values would affect predictions
  - Quick check question: If a decision tree classified traffic as threat because "bytes > 1000," would it still be threat if bytes were 500?

## Architecture Onboarding

- Component map: Network traffic data -> Decision tree classifier -> Classification -> Explanation template -> LLM prompt -> Natural language explanation -> Counterfactual quiz questions -> Human evaluation -> Understanding score

- Critical path:
  1. Network data → Decision tree → Classification
  2. Classification + tree structure → Explanation template
  3. Template → LLM prompt → Natural language explanation
  4. Explanation + counterfactual questions → Human evaluation
  5. Quiz answers → Understanding score + preference ratings

- Design tradeoffs:
  - LLM-based vs rule-based: Quality vs speed/cost (LLM explanations are slower and more expensive)
  - Tree depth: Simplicity vs completeness (depth limited to 4 for manageable explanations)
  - Evaluation method: Objective comprehension vs subjective preference (quiz scores vs ratings)

- Failure signatures:
  - Low quiz scores despite high ratings: Evaluation framework may not measure actual understanding
  - High quiz scores but poor ratings: Explanations may be technically accurate but poorly communicated
  - Degradation with deeper trees: LLM explanations become less coherent as prompt length increases

- First 3 experiments:
  1. Compare LLM vs rule-based explanations on a small decision tree (depth 2) with simple quiz questions
  2. Test whether adding feature importance information to LLM prompts improves quiz scores
  3. Evaluate whether domain-specific prompt engineering (network security terms) improves explanation quality

## Open Questions the Paper Calls Out
- How does the quality of LLM-based decision tree explanations compare to rule-based explanations for more complex decision trees with greater depth?
- Can retrieving relevant documents similar to dense passage retrieval (DPR) further improve the quality of LLM-based decision tree explanations?
- How do human evaluators' background knowledge and expertise impact their ability to understand and prefer LLM-based decision tree explanations?

## Limitations
- The evaluation framework relies entirely on human subjective ratings and quiz performance, which may not capture all aspects of explanation quality
- The decision tree depth limitation (max 4) may not generalize to deeper, more complex trees where LLM-based explanations could degrade
- The NF-BoT dataset characteristics may influence the results; performance on other NID datasets remains untested

## Confidence
- **High Confidence**: LLM explanations correlate with human ratings for readability and quality
- **Medium Confidence**: Counterfactual quiz framework effectively measures understanding of decision tree inference
- **Low Confidence**: Results generalize to deeper decision trees or different NID datasets

## Next Checks
1. Test explanation quality and comprehension scores on deeper decision trees (depth 6-8) to assess scalability limits
2. Validate the counterfactual quiz framework by correlating quiz scores with actual task performance on unseen NID data
3. Evaluate the approach on alternative NID datasets to verify generalization across different network traffic distributions