---
ver: rpa2
title: Revisiting LARS for Large Batch Training Generalization of Neural Networks
arxiv_id: '2309.14053'
source_url: https://arxiv.org/abs/2309.14053
tags:
- base
- learning
- lars
- batch
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits the Layer-wise Adaptive Rate Scaling (LARS)
  technique for large batch training of neural networks. The authors identify that
  LARS, when used with warm-up, can get trapped in sharp minimizers early on due to
  redundant ratio scaling.
---

# Revisiting LARS for Large Batch Training Generalization of Neural Networks

## Quick Facts
- arXiv ID: 2309.14053
- Source URL: https://arxiv.org/abs/2309.14053
- Reference count: 39
- Key outcome: TVLARS consistently outperforms LARS and LAMB in most cases, with up to 2% improvement in classification scenarios and up to 10% in self-supervised learning cases.

## Executive Summary
This paper revisits the Layer-wise Adaptive Rate Scaling (LARS) technique for large batch training of neural networks and identifies limitations in its warm-up strategy that can lead to getting trapped in sharp minimizers early on. To address these issues, the authors propose Time Varying LARS (TVLARS), a novel algorithm that replaces warm-up with a configurable sigmoid-like function for robust training in the initial phase. TVLARS promotes gradient exploration early on, surpassing sharp optimizers and gradually transitioning to LARS for robustness in later phases.

## Method Summary
TVLARS introduces a time-varying scaling factor ϕ(t) based on a sigmoid function to replace the warm-up strategy in LARS. This scaling factor starts with high learning rates to promote exploration and gradually transitions to LARS for stability in later phases. The algorithm is configurable through parameters like the decay coefficient λ and the minimum learning rate γ_min, allowing it to adapt to different datasets and batch sizes without heuristic tuning.

## Key Results
- TVLARS consistently outperforms LARS and LAMB in most cases, with up to 2% improvement in classification scenarios.
- In all self-supervised learning cases, TVLARS dominates LARS and LAMB with performance improvements of up to 10%.
- The algorithm is effective across various batch sizes and datasets, including CIFAR-10 and Tiny ImageNet.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TVLARS avoids sharp minimizers early by starting with high scaling ratios instead of low warm-up rates.
- Mechanism: TVLARS uses a sigmoid-like time-varying factor that begins with high learning rates, allowing the model to explore the loss landscape broadly and escape sharp minima before settling into stable convergence.
- Core assumption: High initial learning rates prevent the model from getting trapped in sharp minimizers near initialization, which are associated with poor generalization.
- Evidence anchors:
  - [abstract] "TVLARS promotes gradient exploration early on, surpassing sharp optimizers and gradually transitioning to LARS for robustness in later phases."
  - [section 4] "We then construct TVLARS as an optimizer that uses a high initial learning rate owing to its ability of local sharp minima evasion..."
  - [corpus] No direct evidence, but the cited paper on "Edge of Stochastic Stability" supports the idea that step size affects convergence stability.
- Break condition: If the initial learning rate is too high, the training may become unstable and diverge before the decay phase takes effect.

### Mechanism 2
- Claim: The time-varying component in TVLARS smoothly transitions from exploration to exploitation.
- Mechanism: The sigmoid decay function controlled by a soft-temperature parameter λ ensures that the learning rate decreases gradually after a delay period, maintaining stability while allowing exploration.
- Core assumption: A smooth decay avoids abrupt changes in learning rate that could destabilize training, especially in later phases.
- Evidence anchors:
  - [section 4] "The proposed time-varying component ϕt is constructed based on the sigmoid function whose curve is smooth to keep the model away from unstable learning..."
  - [abstract] "TVLARS... gradually transitioning to LARS for robustness in later phases."
  - [corpus] No direct evidence, but the related work on "MERIT" and "GSNR" suggests that learning rate scheduling impacts generalization.
- Break condition: If λ is too large, the decay may be too steep, causing the model to converge too quickly and miss better minima.

### Mechanism 3
- Claim: TVLARS is configurable to adapt to different datasets and batch sizes without heuristic tuning.
- Mechanism: By adjusting the decay coefficient λ and the minimum learning rate γ_min, TVLARS can be tuned to balance exploration and stability for various training scenarios.
- Core assumption: Different datasets and batch sizes require different exploration-to-exploitation balances, which can be captured by these parameters.
- Evidence anchors:
  - [section 5.2.1] "Decay coefficient λ... is a simple regularized parameter, which can be used to anneal the learning rate to enhance the model performance."
  - [abstract] "TVLARS... replaces warm-up with a configurable sigmoid-like function..."
  - [corpus] No direct evidence, but the cited paper on "Initialization Matters" supports the idea that initialization and learning rate impact generalization.
- Break condition: If the parameters are not tuned appropriately, TVLARS may underperform compared to standard LARS or LAMB.

## Foundational Learning

- Concept: Layer-wise Adaptive Rate Scaling (LARS)
  - Why needed here: LARS adjusts the learning rate per layer based on the ratio of weight norm to gradient norm, which helps stabilize training with large batches.
  - Quick check question: What is the purpose of the scaling ratio ∥w∥/∥∇L(w)∥ in LARS?

- Concept: Sharp minimizers and generalization
  - Why needed here: Sharp minimizers are local minima with high curvature that can lead to poor generalization, especially in large batch training.
  - Quick check question: How do sharp minimizers affect the generalization performance of neural networks?

- Concept: Warm-up strategy in large batch training
  - Why needed here: Warm-up gradually increases the learning rate from a small value to avoid instability at the start of training, but it may not be optimal for escaping sharp minima.
  - Quick check question: Why might a warm-up strategy be insufficient for avoiding sharp minimizers in large batch training?

## Architecture Onboarding

- Component map:
  - Time-varying scaling factor ϕ(t) based on sigmoid function
  - Decay coefficient λ controlling the steepness of the decay
  - Minimum learning rate γ_min to bound the scaling factor
  - Delayed epochs de to specify when the decay starts
  - Layer-wise learning rate adjustment using the scaling factor

- Critical path:
  1. Initialize model parameters and set hyperparameters (λ, γ_min, de)
  2. For each epoch, compute the time-varying scaling factor ϕ(t)
  3. Adjust the layer-wise learning rates using ϕ(t) and the weight/gradient norms
  4. Update model parameters using the adjusted learning rates
  5. Monitor training stability and generalization performance

- Design tradeoffs:
  - Higher initial learning rates improve exploration but risk instability
  - Steeper decay (larger λ) leads to faster convergence but may miss better minima
  - Smaller γ_min ensures stability but may limit exploration in later phases

- Failure signatures:
  - Training diverges if the initial learning rate is too high
  - Poor generalization if the decay is too steep or γ_min is too large
  - Slow convergence if the initial learning rate is too low or the decay is too gradual

- First 3 experiments:
  1. Compare TVLARS with LARS and LAMB on CIFAR-10 with batch size 512, varying λ
  2. Test TVLARS on Tiny ImageNet with batch size 16384, comparing different γ_min values
  3. Evaluate TVLARS on a self-supervised learning task (e.g., Barlow Twins) with varying de

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Time Varying LARS (TVLARS) algorithm compare to other large batch training techniques such as CLARS and AGVM in terms of performance and convergence speed?
- Basis in paper: [inferred] The paper compares TVLARS to LARS and LAMB but does not provide a comprehensive comparison with other large batch training techniques.
- Why unresolved: The paper focuses on the comparison between TVLARS, LARS, and LAMB, leaving a gap in understanding how TVLARS performs relative to other state-of-the-art large batch training methods.
- What evidence would resolve it: Conducting experiments to compare TVLARS with CLARS and AGVM on the same datasets and tasks would provide insights into its relative performance and convergence speed.

### Open Question 2
- Question: Can the TVLARS algorithm be extended to handle other types of optimization problems beyond classification and self-supervised learning tasks?
- Basis in paper: [inferred] The paper primarily focuses on classification and self-supervised learning tasks, leaving open the question of TVLARS's applicability to other optimization problems.
- Why unresolved: The paper does not explore the potential of TVLARS in solving other types of optimization problems, such as regression or reinforcement learning.
- What evidence would resolve it: Applying TVLARS to different optimization problems and evaluating its performance would provide evidence of its versatility and effectiveness in handling various tasks.

### Open Question 3
- Question: How does the choice of hyperparameters, such as the soft-temperature factor λ and the number of delayed epochs de, affect the performance of the TVLARS algorithm?
- Basis in paper: [explicit] The paper mentions the impact of the soft-temperature factor λ on the steepness of the time-variant component and the number of delayed epochs de on the learning rate decay, but does not provide a comprehensive analysis of their effects on TVLARS's performance.
- Why unresolved: The paper does not provide a detailed analysis of how different hyperparameter settings affect TVLARS's performance, leaving room for further exploration.
- What evidence would resolve it: Conducting experiments with varying hyperparameter settings and analyzing their impact on TVLARS's performance would provide insights into the optimal configuration for different tasks and datasets.

## Limitations
- The paper primarily focuses on ResNet architectures and classification/self-supervised learning tasks, leaving the generalizability to other architectures and tasks unexplored.
- The effectiveness of TVLARS is primarily demonstrated through empirical results, with limited theoretical analysis of its convergence properties and generalization guarantees.
- The paper does not provide clear guidelines for tuning the hyperparameters of TVLARS, which may limit its practical applicability without extensive experimentation.

## Confidence
- Confidence Level: Medium
  The paper presents a novel approach to address limitations in LARS for large batch training, but there are several areas of uncertainty. The effectiveness of TVLARS is primarily demonstrated through empirical results, with limited theoretical analysis of why the proposed mechanism works.
- Confidence Level: Medium
  The experimental setup, while comprehensive, may not fully capture the generalizability of TVLARS across different architectures and tasks. The results are primarily shown for ResNet18/34 on CIFAR10 and Tiny ImageNet, with additional results on self-supervised learning tasks.
- Confidence Level: Low
  The paper mentions that TVLARS is configurable through parameters like the decay coefficient λ and the minimum learning rate γ_min, but it does not provide clear guidelines on how to tune these parameters for different scenarios.

## Next Checks
1. Conduct a theoretical analysis to prove the convergence properties and generalization guarantees of TVLARS compared to LARS.
2. Evaluate TVLARS on a wider range of architectures (e.g., Transformers, RNNs) and tasks (e.g., object detection, segmentation) to assess its generalizability.
3. Perform a sensitivity analysis of the TVLARS hyperparameters (λ, γ_min, de) to understand their impact on performance and provide guidelines for tuning these parameters in different scenarios.