---
ver: rpa2
title: Insights From Insurance for Fair Machine Learning
arxiv_id: '2306.14624'
source_url: https://arxiv.org/abs/2306.14624
tags:
- zero
- nine
- three
- insurance
- five
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues that insurance serves as an insightful analogon\
  \ for understanding the social situatedness of machine learning systems. By examining\
  \ the historical evolution of insurance practices\u2014from welfare state solidarity\
  \ to neoclassical actuarial fairness and personalized behavior-based pricing\u2014\
  the authors draw parallels with current debates in fair machine learning."
---

# Insights From Insurance for Fair Machine Learning

## Quick Facts
- arXiv ID: 2306.14624
- Source URL: https://arxiv.org/abs/2306.14624
- Reference count: 1
- This paper argues that insurance serves as an insightful analogon for understanding the social situatedness of machine learning systems.

## Executive Summary
This paper draws parallels between historical insurance practices and current machine learning fairness debates to illuminate how technical fairness choices embed normative assumptions. By examining the evolution from welfare state solidarity to personalized actuarial fairness, the authors show how concepts of responsibility, performativity, and the tension between aggregate and individual dimensions play central roles in both domains. The analysis reveals that actuarial fairness cannot be genuinely individual and encourages reflection on the implicit stories of causality, control, and responsibility embedded in fairness measures.

## Method Summary
The paper employs conceptual mapping and theoretical analysis to establish correspondences between insurance fairness concepts and machine learning definitions. It maps calibration to actuarial fairness and responsibility, while independence corresponds to non-responsibilization. The analysis distinguishes four dimensions of responsibility (causal, control-based, moral, material) and examines how ML predictions, like insurance categories, are not neutral representations but actively construct social categories through feedback loops.

## Key Results
- Actuarial fairness and calibration are in close correspondence, with calibration mapping to responsibilization and independence to non-responsibilization
- The myth of actuarial fairness as objective and neutral is critiqued, emphasizing its normative and performative dimensions
- Actuarial fairness cannot be genuinely individual, creating a probability paradox when applied to individual risk assessment
- Responsibility dimensions from insurance provide a framework for understanding fairness choices in ML systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Insurance history provides a conceptual framework to understand fairness tensions in machine learning
- Mechanism: The paper maps historical insurance fairness concepts (actuarial fairness, solidarity, responsibility) to ML fairness definitions (calibration, independence) to reveal how technical fairness choices embed normative assumptions
- Core assumption: Fairness concepts in insurance and ML share structural parallels despite different domains
- Evidence anchors: [abstract] "We link insurance fairness conceptions to their machine learning relatives"; [section 4] "Actuarial fairness and calibration are in close correspondence"
- Break condition: If insurance fairness concepts cannot be meaningfully mapped to ML definitions, the analogical bridge collapses

### Mechanism 2
- Claim: Responsibility dimensions in insurance help structure fairness debates in ML
- Mechanism: By distinguishing responsibility from responsibilization, the paper shows how ML fairness choices implicitly assign or withhold responsibility, affecting downstream effects
- Core assumption: Responsibility concepts from insurance are transferable to ML contexts
- Evidence anchors: [section 3] "We distinguish four dimensions of responsibility"; [section 4] "Calibration maps to responsibilization, while independence maps to non-responsibilization"
- Break condition: If responsibility dimensions do not map meaningfully to ML feature selection or fairness metric choices

### Mechanism 3
- Claim: Performativity of insurance categories illuminates how ML predictions shape social realities
- Mechanism: The paper argues that ML predictions, like insurance categories, are not neutral representations but actively construct social categories and behaviors through feedback loops
- Core assumption: Performativity concepts from insurance apply to ML systems
- Evidence anchors: [section 5] "The performativity of insurance and machine learning becomes especially relevant"; [section 5] "The predictions may act as interventions and through responsibilization shape the behaviour of people"
- Break condition: If ML predictions do not exhibit feedback effects on the populations they classify

## Foundational Learning

- Concept: Statistical fairness definitions (calibration, independence)
  - Why needed here: The paper's core argument relies on mapping these ML concepts to insurance fairness concepts
  - Quick check question: What is the mathematical difference between calibration and independence in ML?

- Concept: Historical evolution of insurance fairness
  - Why needed here: Understanding the three historical modes (welfare state, neoclassical, personalized) is essential to grasp the paper's argument about responsibility and performativity
  - Quick check question: How does the welfare state mode of insurance differ from personalized insurance in terms of fairness principles?

- Concept: Performativity in social systems
  - Why needed here: The paper uses performativity to explain how ML systems shape rather than just represent social realities
  - Quick check question: What is the difference between constitutive and causal dimensions of performativity?

## Architecture Onboarding

- Component map: Insurance history → ML fairness mapping → Responsibility/performativity analysis → Aggregate-individual tensions
- Critical path: Understanding insurance history → Mapping to ML concepts → Applying insights to ML fairness debates
- Design tradeoffs: The paper sacrifices comprehensive coverage of ML literature to focus deeply on insurance parallels; uses historical analysis rather than empirical ML studies
- Failure signatures: If readers cannot see the structural parallels between insurance and ML, the analogical bridge fails; if performativity concept is not well understood, downstream analysis becomes unclear
- First 3 experiments:
  1. Map one ML fairness definition (e.g., demographic parity) to an insurance fairness concept and analyze the implications
  2. Apply the four responsibility dimensions to a specific ML deployment case study
  3. Analyze the performativity of a real ML system by examining feedback effects on the population it serves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we empirically test whether actuarial fairness in machine learning systems reproduces past injustices embedded in training data?
- Basis in paper: [explicit] The paper argues that actuarial fairness may reproduce past injustice implicit in training data through performativity
- Why unresolved: While the paper makes theoretical arguments about performativity and injustice reproduction, no empirical methodology is provided for testing this claim in real machine learning systems
- What evidence would resolve it: Controlled experiments deploying machine learning systems with different fairness metrics while tracking long-term societal impacts

### Open Question 2
- Question: What specific features should be designated as "accountability" versus "irrelevant" in Heidari et al.'s framework, and how should these designations be determined?
- Basis in paper: [explicit] The paper discusses Heidari et al.'s framework of splitting features into "accountability" and "irrelevant" categories but notes that lessons from insurance about causality, control, and responsibility could guide this selection
- Why unresolved: The paper provides theoretical guidance about using insurance lessons to inform feature selection but doesn't specify concrete criteria or decision procedures for making these determinations in practice
- What evidence would resolve it: Case studies or experimental results showing how different feature classification choices affect system outcomes

### Open Question 3
- Question: How can we measure and model the performative effects of machine learning systems on group formation and social categories?
- Basis in paper: [inferred] The paper discusses the performativity of groups and how categories can contribute to constituting what they purport to describe
- Why unresolved: While the paper highlights this as an important consideration, it doesn't provide methodologies for measuring or modeling these effects
- What evidence would resolve it: Longitudinal studies tracking changes in social categories before and after machine learning deployment

## Limitations
- The analogical bridge between insurance and ML fairness remains largely untested in broader literature
- Responsibility dimensions from insurance may not fully transfer to ML contexts where causal chains are more complex
- Performativity claims about ML systems lack direct empirical evidence in the corpus

## Confidence
- Core claim about insurance-ML conceptual mapping: Medium confidence
- Critique of actuarial fairness as a myth: High confidence (given mathematical proof)
- Performativity arguments: Low confidence (limited empirical evidence)

## Next Checks
1. Test whether calibration's aggregate nature creates practical fairness issues in real ML deployments, replicating the probability paradox argument
2. Map specific ML fairness metrics (e.g., demographic parity, equalized odds) to the four responsibility dimensions to validate the insurance-ML correspondence
3. Examine whether ML predictions exhibit measurable feedback effects on populations, providing empirical evidence for performativity claims