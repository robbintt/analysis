---
ver: rpa2
title: 'The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected
  vs Proxy Features'
arxiv_id: '2310.08617'
source_url: https://arxiv.org/abs/2310.08617
tags:
- explanations
- bias
- disclosure
- fairness
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Explanations and disclosures help people recognize direct bias
  but may increase reliance on biased AI decisions. Disclosures alone do not improve
  fairness perceptions, but disclosures combined with explanations can help participants
  correct for indirect bias.
---

# The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features

## Quick Facts
- arXiv ID: 2310.08617
- Source URL: https://arxiv.org/abs/2310.08617
- Authors: 
- Reference count: 40
- Key outcome: Explanations and disclosures help people recognize direct bias but may increase reliance on biased AI decisions. Disclosures alone do not improve fairness perceptions, but disclosures combined with explanations can help participants correct for indirect bias.

## Executive Summary
This study investigates how explanations and disclosures affect human fairness perceptions and decisions when working with biased AI models. Using semi-synthetic micro-lending data, researchers manipulated whether bias was direct (via protected gender feature) or indirect (via proxy university feature). The experiments reveal that while explanations help detect direct bias, they paradoxically increase acceptance of biased predictions overall. Disclosing both model bias and the proxy-protected feature correlation, especially when combined with explanations, improves fairness in indirect bias scenarios.

## Method Summary
The study used semi-synthetic data from the Prosper lending platform with synthetic binary gender and university features. Logistic regression models were trained with controlled bias conditions (direct via gender, indirect via university). Participants completed loan repayment prediction tasks with profiles showing feature values, model predictions, and either explanations, disclosures, or both. The study measured demographic parity, fairness perceptions, and decision quality across different experimental conditions.

## Key Results
- Explanations help detect direct bias but not indirect bias through proxies
- Explanations increase agreement with biased AI decisions regardless of bias type
- Disclosing both model bias and proxy-protected correlation with explanations improves fairness in indirect bias scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanations help participants recognize direct biases but not indirect biases.
- Mechanism: When bias is directly tied to a visible protected feature (e.g., gender), explanations reveal which features contribute to predictions, allowing humans to identify unfair feature usage. For indirect bias through proxies (e.g., university), the feature's connection to the protected attribute is less obvious, so explanations do not help participants notice unfairness.
- Core assumption: Humans can interpret feature importance explanations correctly when the link between feature and bias is explicit.
- Evidence anchors:
  - [abstract]: "explanations help people detect direct but not indirect biases"
  - [section]: "explanations significantly improve participants' ability to recognize unfairness (Figure 5a). ... we find that explanations can help people recognize unfairness in the case of direct bias but not indirect."
  - [corpus]: "Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods for Fairness Exploration" (FMR 0.60) supports the premise that explanation methods are being evaluated for fairness detection.
- Break condition: If explanations are too complex or if humans misinterpret the relative importance of proxy features, recognition fails even for direct bias.

### Mechanism 2
- Claim: Explanations increase reliance on biased AI decisions regardless of whether bias is recognized.
- Mechanism: By making the AI's reasoning visible, explanations give the appearance of transparency and correctness, leading participants to trust and accept the AI's biased predictions even when they can see the unfairness.
- Core assumption: Visibility of reasoning increases trust, even if the reasoning is flawed.
- Evidence anchors:
  - [abstract]: "regardless of bias type, explanations tend to increase agreement with model biases"
  - [section]: "explanations lead people to accept model biases leading to less fair decisions."
  - [corpus]: "Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making" (Wang & Yin, 2021) documents how explanations can increase overreliance.
- Break condition: If explanations are paired with strong bias disclosures, the trust-boosting effect can be neutralized or reversed.

### Mechanism 3
- Claim: Disclosing both model bias and the proxy-protected feature correlation helps participants correct indirect bias.
- Mechanism: Providing explicit information about the statistical link between the proxy and protected features makes the indirect bias visible, enabling participants to override biased predictions. This works best when explanations are also shown to highlight how the proxy features are being used.
- Core assumption: Humans can integrate disclosed statistical relationships into their decision-making when given concrete examples.
- Evidence anchors:
  - [abstract]: "Disclosures can help mitigate this effect for indirect biases, improving both unfairness recognition and decision-making fairness."
  - [section]: "disclosing both the model bias and the relationship between the protected and proxy feature... significantly decreases perceived model fairness... significantly increases gender parity in decision-making."
  - [corpus]: "Auditing Predictive Models for Intersectional Biases" (FR 0.0) indicates the need for explicit bias auditing tools, analogous to correlation disclosures.
- Break condition: If the correlation is weak or if participants misinterpret the disclosed relationship, corrections may not occur.

## Foundational Learning

- Concept: Demographic parity as a fairness metric.
  - Why needed here: The study uses demographic parity to measure whether the AI treats different groups equally in loan acceptance rates.
  - Quick check question: If a model accepts 60% of men and 40% of women, what is the demographic parity score, and is it below the 80% rule threshold?

- Concept: Direct vs. indirect bias.
  - Why needed here: The experimental design manipulates whether bias comes from a protected feature (gender) or a proxy feature (university) correlated with gender.
  - Quick check question: In a dataset where zip code is correlated with race, would using zip code as a feature introduce direct or indirect bias?

- Concept: Input-influence explanations in logistic regression.
  - Why needed here: The study uses logistic regression models and explains predictions by showing feature weights multiplied by feature values.
  - Quick check question: If a feature has weight +2.0 and value 0.5 (normalized), what is its contribution to the prediction score?

## Architecture Onboarding

- Component map: Synthetic data generator → Logistic regression model trainer → Explanation renderer → Human decision interface → Survey module → Statistical analysis pipeline
- Critical path:
  1. Generate synthetic gender/university features with controlled correlation
  2. Train biased logistic regression model
  3. Produce input-influence explanations
  4. Present profiles with/without explanations and disclosures
  5. Collect decisions and survey responses
  6. Compute fairness and trust metrics
- Design tradeoffs:
  - Synthetic data allows controlled bias but may lack real-world complexity; using partial synthetic data balances control and realism
  - Input-influence explanations are simple and interpretable but may not capture non-linear model behavior
- Failure signatures:
  - If demographic parity does not change between phases despite disclosures, the disclosure may be ineffective
  - If accuracy drops significantly after interventions, participants may be overcorrecting or misunderstanding
- First 3 experiments:
  1. Compare fairness perception with and without explanations in direct bias condition (test Mechanism 1)
  2. Test whether bias disclosure alone changes decision-making in indirect bias condition (test Mechanism 3 baseline)
  3. Measure effect of full bias+correlation disclosure with explanations on acceptance rates for disadvantaged group (test Mechanism 3 with explanations)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design explanations that help humans recognize and correct for indirect biases revealed through proxy features?
- Basis in paper: [explicit] The paper finds that explanations help people detect direct biases but not indirect biases revealed through proxy features. It also finds that explanations lead people to accept model biases regardless of fairness perceptions.
- Why unresolved: The paper identifies this as a key challenge but does not provide a clear solution. Designing explanations that can effectively communicate indirect biases is an open problem.
- What evidence would resolve it: A study comparing different explanation techniques for indirect biases, measuring their effectiveness in helping humans recognize and correct for these biases in decision-making.

### Open Question 2
- Question: What are the optimal disclosure strategies to improve fairness perceptions and decision-making when working with biased AI models?
- Basis in paper: [explicit] The paper finds that disclosing both model bias and the correlation between proxy and protected features, especially with explanations, improves fairness perceptions and decision-making for indirect biases. However, it does not identify the optimal disclosure strategy.
- Why unresolved: The paper explores different disclosure strategies but does not determine the most effective one. The optimal strategy may depend on factors like the type of bias, user characteristics, and task context.
- What evidence would resolve it: A comparative study of different disclosure strategies, varying factors like the level of detail, timing, and format of disclosure, to identify the most effective approach for improving fairness.

### Open Question 3
- Question: How does dispositional trust in AI affect human reliance on and correction of biased AI predictions?
- Basis in paper: [explicit] The paper finds that higher dispositional trust in AI leads to more unfair decisions by relying more on the biased model, but this effect is alleviated after including disclosures.
- Why unresolved: While the paper identifies a relationship between dispositional trust and reliance on biased AI, it does not fully explore the underlying mechanisms or how to mitigate the negative effects of high dispositional trust.
- What evidence would resolve it: A study investigating the relationship between dispositional trust, reliance on AI, and fairness perceptions in different decision-making contexts, as well as interventions to mitigate the negative effects of high dispositional trust.

## Limitations

- The synthetic data approach may not capture real-world complexity of proxy discrimination scenarios
- Binary gender simplification may not reflect nuanced intersectional biases in actual lending decisions
- Online MTurk participants may have different bias recognition patterns compared to domain experts or real loan officers

## Confidence

- **High Confidence**: The finding that explanations increase agreement with biased AI decisions (Mechanism 2) is well-supported by multiple studies in the corpus and consistent behavioral patterns
- **Medium Confidence**: The differential impact of explanations on direct versus indirect bias recognition (Mechanism 1) is supported by the experimental results but may be sensitive to the specific proxy feature choice and correlation strength
- **Medium Confidence**: The effectiveness of combined bias and correlation disclosures for correcting indirect bias (Mechanism 3) shows positive results in the study but may require stronger correlations or different disclosure framings in other contexts

## Next Checks

1. Replicate the study with a continuous gender proxy (e.g., income proxy) to test whether the binary simplification affects bias recognition patterns
2. Conduct a follow-up experiment measuring long-term learning effects - do participants improve their bias detection over repeated exposures to the same explanation formats?
3. Test the robustness of disclosure effectiveness across different correlation strengths between proxy and protected features (e.g., 0.3 vs 0.7 correlation) to identify the threshold where disclosures become effective