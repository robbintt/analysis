---
ver: rpa2
title: Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection
arxiv_id: '2304.04688'
source_url: https://arxiv.org/abs/2304.04688
tags:
- action
- interaction
- feature
- prompting
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the zero-shot spatio-temporal action detection
  (ZSSTAD) problem and proposes the Interaction-CLIP (iCLIP) method to solve it. iCLIP
  uses CLIP encoders to extract person, object, and context features, then models
  their interactions via interaction blocks to obtain an "interaction feature." It
  also uses this feature to prompt CLIP text embeddings of action labels, creating
  interaction-aware prompting.
---

# Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection

## Quick Facts
- arXiv ID: 2304.04688
- Source URL: https://arxiv.org/abs/2304.04688
- Reference count: 40
- Primary result: Proposes iCLIP method for ZSSTAD using CLIP encoders, interaction modeling, and interaction-aware prompting to achieve state-of-the-art performance on J-HMDB and UCF101-24 datasets.

## Executive Summary
This paper introduces the zero-shot spatio-temporal action detection (ZSSTAD) problem and proposes the Interaction-CLIP (iCLIP) method to solve it. iCLIP uses CLIP encoders to extract person, object, and context features, then models their interactions via interaction blocks to obtain an "interaction feature." It also uses this feature to prompt CLIP text embeddings of action labels, creating interaction-aware prompting. The model is trained to align interaction features with text embeddings. Experiments on J-HMDB and UCF101-24 datasets show that iCLIP outperforms a CLIP-based baseline in ZSSTAD, demonstrating the effectiveness of interaction modeling and interaction-aware prompting for zero-shot action detection.

## Method Summary
The iCLIP method extracts features for person, object, and context using CLIP image encoders, then models their relationships through Person, Object, Context, and Memory interaction blocks using self-attention and cross-attention layers. The enhanced person feature is combined with the context feature to generate an interaction feature, which is then used to prompt CLIP text embeddings of action labels via multi-head self-attention. The model calculates cosine similarity between interaction features and prompted text embeddings for action classification, trained with cross-entropy loss. The method is evaluated on J-HMDB and UCF101-24 datasets using frame mAP with 0.5 IoU threshold.

## Key Results
- iCLIP achieves superior performance on zero-shot spatio-temporal action detection compared to CLIP baseline
- Interaction modeling between person, object, and context features improves action detection accuracy
- Interaction-aware prompting of text embeddings using interaction features enhances zero-shot detection capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modeling person-object and person-context interactions through dedicated attention blocks produces a richer "interaction feature" than using raw image features alone.
- Mechanism: Interaction blocks use cross-attention between person features and object/context features to aggregate relevant information from the environment. This enriched feature captures spatial-temporal relationships that are critical for action recognition.
- Core assumption: The action class of a person can be better determined by modeling how they interact with objects and their surrounding context, rather than just examining the person in isolation.
- Evidence anchors:
  - [abstract] "model the relationship between these features through different interaction modules to obtain the interaction feature"
  - [section] "person-object interaction will also be considered by the Object interaction block. In addition, we use the Context interaction block to observe the target person's action from the perspective of the whole image"
  - [corpus] Weak - no direct evidence from corpus about interaction blocks improving action detection

### Mechanism 2
- Claim: Interaction-aware prompting generates label-specific text embeddings that are better aligned with the interaction feature for each detected person.
- Mechanism: The interaction feature is used as key-value pairs in a multi-head self-attention mechanism over the original CLIP text embedding of each action label. This process conditions the label representation on the specific interaction context of the person being classified.
- Core assumption: The same action label can have different optimal text representations depending on the specific interaction context in which it appears.
- Evidence anchors:
  - [abstract] "we use this feature to prompt each label to obtain more appropriate text feature"
  - [section] "we use our interaction feature for prompting because the interaction feature can describe the action of each target person in more details"
  - [corpus] Weak - corpus doesn't contain evidence about prompting improving action detection specifically

### Mechanism 3
- Claim: The combination of interaction feature generation and interaction-aware prompting creates a better alignment between visual and text modalities in CLIP's embedding space, enabling zero-shot action detection.
- Mechanism: The interaction feature captures person-object-context relationships through attention mechanisms, while interaction-aware prompting conditions text embeddings on this feature. The cosine similarity between these aligned features determines action classification, allowing detection of unseen action classes.
- Core assumption: CLIP's embedding space can effectively represent the relationship between interaction features and action labels when properly aligned through the proposed mechanisms.
- Evidence anchors:
  - [abstract] "the proposed interaction module and prompting make the visual-language features better aligned, thus achieving excellent accuracy for zero-shot spatio-temporal action detection"
  - [section] "calculate the similarity between the interaction feature and the text feature for each label to determine the action category"
  - [corpus] Weak - corpus doesn't provide evidence about modality alignment improving zero-shot detection

## Foundational Learning

- Concept: Zero-shot learning through visual-language alignment
  - Why needed here: The task requires detecting actions that weren't seen during training, so the model must generalize from seen to unseen classes using semantic relationships
  - Quick check question: How does CLIP enable zero-shot classification without seeing examples of each class?

- Concept: Cross-attention mechanisms for feature interaction
  - Why needed here: Person actions are inherently interactive with objects and context, requiring mechanisms that can model these relationships rather than treating features independently
  - Quick check question: What's the difference between self-attention and cross-attention in the context of person-object interaction modeling?

- Concept: Prompt engineering in vision-language models
  - Why needed here: Standard CLIP text embeddings may not capture the specific context needed for accurate action detection, requiring adaptation through prompting
  - Quick check question: How does interaction-aware prompting differ from traditional prompt engineering approaches?

## Architecture Onboarding

- Component map: CLIP image encoder → Person/Object/Context feature extraction → Interaction blocks (Person, Object, Context, Memory) → Interaction feature → CLIP text encoder → Original text embeddings → Interaction-aware prompting → Final text embeddings → Cosine similarity computation → Action classification
- Critical path: Image feature extraction → Interaction feature generation → Text embedding prompting → Similarity computation → Classification
- Design tradeoffs: Using frozen CLIP encoders provides strong pre-trained representations but limits fine-tuning flexibility; the interaction blocks add complexity but capture richer context; prompting adds computational overhead but improves specificity
- Failure signatures: Poor performance on object-interaction-heavy actions suggests interaction blocks aren't capturing relationships; inconsistent results across similar actions suggests prompting isn't effectively conditioning on context
- First 3 experiments:
  1. Compare baseline CLIP image features vs. interaction features on a subset of J-HMDB to validate interaction modeling
  2. Test different orders of interaction blocks to determine optimal feature aggregation sequence
  3. Evaluate prompting with random interaction features vs. learned interaction features to validate the prompting mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the resolution of input videos affect the performance of iCLIP's interaction feature extraction?
- Basis in paper: [inferred] The paper mentions that UCF101-24 videos have lower resolution, leading to noisy interaction features and reduced performance in the 50% vs 50% experiment.
- Why unresolved: The paper only briefly mentions this observation without providing detailed analysis or experiments to quantify the impact of resolution on performance.
- What evidence would resolve it: Systematic experiments varying video resolution (e.g., HD, SD, compressed versions) on UCF101-24 and J-HMDB datasets, measuring interaction feature quality and detection accuracy across resolutions.

### Open Question 2
- Question: What is the optimal number of neighboring frames to include in the memory block for capturing temporal context?
- Basis in paper: [explicit] The paper mentions using a "certain number of neighboring frames" for the memory block but doesn't specify what this number should be or how it affects performance.
- Why unresolved: The authors only mention the memory block conceptually without providing ablation studies on different temporal window sizes.
- What evidence would resolve it: Experiments testing different temporal window sizes (e.g., 1, 3, 5, 7 frames before and after) and measuring the impact on detection accuracy for various action types.

### Open Question 3
- Question: How does iCLIP's performance scale when applied to datasets with significantly more action categories than J-HMDB and UCF101-24?
- Basis in paper: [explicit] The paper demonstrates effectiveness on datasets with 21 and 24 action categories but doesn't test on larger-scale datasets.
- Why unresolved: The experiments are limited to relatively small datasets, and there's no discussion of performance degradation or adaptation needs for larger label spaces.
- What evidence would resolve it: Testing iCLIP on larger datasets like Something-Something v2 (174 classes) or Kinetics-700, measuring performance and identifying bottlenecks in scaling to larger label spaces.

## Limitations

- The method relies heavily on CLIP's pre-trained embeddings, inheriting its limitations and biases
- Performance may degrade with lower video resolution due to noisier interaction features
- The approach is primarily validated on relatively small datasets with limited action categories

## Confidence

**High confidence**: The core claim that interaction modeling improves action detection accuracy is well-supported by the experimental results showing consistent improvements over the baseline across both datasets.

**Medium confidence**: The specific contribution of each interaction block to overall performance is not clearly isolated in the experiments, and the claim that interaction-aware prompting is essential for good zero-shot performance would benefit from ablation studies.

**Low confidence**: The scalability of this approach to larger, more diverse action datasets and its robustness to varying object detection quality are not thoroughly investigated.

## Next Checks

1. **Ablation study on interaction blocks**: Systematically remove each interaction block (Person, Object, Context, Memory) individually to quantify their individual contributions to detection accuracy and identify which components are most critical for performance.

2. **Robustness to object detection quality**: Evaluate model performance across different object detection confidence thresholds and with simulated object detection errors to understand how sensitive the interaction modeling is to object detection quality.

3. **Cross-dataset generalization**: Train the model on one dataset (e.g., J-HMDB) and evaluate on the other (UCF101-24) to assess whether the interaction modeling and prompting generalize across different action distributions and video characteristics.