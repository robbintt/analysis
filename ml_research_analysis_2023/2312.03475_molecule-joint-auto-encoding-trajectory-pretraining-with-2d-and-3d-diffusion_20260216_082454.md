---
ver: rpa2
title: 'Molecule Joint Auto-Encoding: Trajectory Pretraining with 2D and 3D Diffusion'
arxiv_id: '2312.03475'
source_url: https://arxiv.org/abs/2312.03475
tags:
- diffusion
- learning
- arxiv
- data
- molecule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoleculeJAE, a novel pretraining framework
  that learns joint representations of 2D molecular graphs and 3D conformers using
  diffusion models. It formulates molecule pretraining as trajectory distribution
  modeling, where augmented trajectories are generated via diffusion processes and
  the model learns to reconstruct and contrast these trajectories.
---

# Molecule Joint Auto-Encoding: Trajectory Pretraining with 2D and 3D Diffusion

## Quick Facts
- arXiv ID: 2312.03475
- Source URL: https://arxiv.org/abs/2312.03475
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance on 15 out of 20 downstream geometry-related tasks

## Executive Summary
This paper introduces MoleculeJAE, a novel pretraining framework that learns joint representations of 2D molecular graphs and 3D conformers using diffusion models. It formulates molecule pretraining as trajectory distribution modeling, where augmented trajectories are generated via diffusion processes and the model learns to reconstruct and contrast these trajectories. The method achieves state-of-the-art performance on 15 out of 20 downstream geometry-related tasks, including quantum property prediction and molecular dynamics prediction, by comparing against 12 competitive baselines. Ablation studies confirm the effectiveness of the contrastive learning component.

## Method Summary
MoleculeJAE pretrains on 2D molecular graphs and 3D conformers simultaneously using a diffusion-based trajectory modeling approach. The framework models the joint distribution of augmented trajectories that include both 2D bond structure and 3D conformer structure. It uses SE(3)-equivariant diffusion processes to preserve molecular geometry symmetries during trajectory augmentation. The learning objective decomposes into marginal distribution estimation (reconstruction) and contrastive regularization, capturing both the data distribution and correlations between noisy and clean data points. The model is trained on the PCQM4Mv2 dataset (3.4M molecules) and evaluated on QM9 and MD17 datasets for quantum property prediction and molecular dynamics prediction tasks.

## Key Results
- Achieves state-of-the-art performance on 15 out of 20 downstream geometry-related tasks
- Outperforms 12 competitive baselines across quantum property prediction and molecular dynamics prediction
- Ablation studies show contrastive component with λ2=0.01 provides optimal performance, while λ2=1 degrades results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint trajectory modeling of 2D topology and 3D geometry enables better molecular representation learning than modeling either modality alone
- Mechanism: The paper proposes modeling the joint distribution of augmented trajectories that include both 2D bond structure (H, E) and 3D conformer structure (P). This trajectory distribution modeling decomposes into marginal distribution estimation (reconstruction) and contrastive regularization, capturing both the data distribution and correlations between noisy and clean data points.
- Core assumption: The augmented trajectories generated by diffusion processes preserve chemically meaningful information that can be learned through trajectory distribution modeling
- Evidence anchors:
  - [abstract] "MoleculeJAE can learn both the 2D bond (topology) and 3D conformation (geometry) information, and a diffusion process model is applied to mimic the augmented trajectories of such two modalities"
  - [section 3.1] "our goal is to jointly estimate the distribution of a molecule's 2D topology (including atom types and chemical bonds) and its 3D geometries (conformers)"
  - [corpus] Weak evidence - neighboring papers mention joint 2D+3D generation but lack detailed mechanism discussion
- Break condition: If the augmented trajectories lose chemical meaning through excessive noise, the learned representations would not capture useful molecular structure information

### Mechanism 2
- Claim: The SE(3) equivariant diffusion process preserves molecular geometry symmetries during trajectory augmentation
- Mechanism: The paper constrains the diffusion process for 3D conformers to maintain SE(3) equivariance, meaning RP(x3D) = P(Rx3D) for any rotation R. This is achieved by using Gaussian noise that respects the symmetry properties of the original data.
- Core assumption: Maintaining geometric symmetries during diffusion is crucial for learning chemically meaningful 3D representations
- Evidence anchors:
  - [section 3.1] "the augmented trajectory P(0) → P(t) should also be SE(3)-equivariant" and provides mathematical formulation
  - [section 3.3] "the SE(3)-invariant density p3D defined by Eq. 5 implies that the corresponding 3D score function ∇xp3D(x) is equivariant under SE(3)"
  - [corpus] Weak evidence - neighboring papers mention SE(3) equivariance but don't explain why it's critical for trajectory learning
- Break condition: If the equivariance constraint is too restrictive, it might prevent the model from learning important conformational variations

### Mechanism 3
- Claim: The contrastive regularization component improves molecular representation quality by aligning augmented views while contrasting different molecules
- Mechanism: The paper decomposes the trajectory distribution modeling into reconstruction and contrastive tasks. The contrastive component maximizes the similarity between representations of the same molecule at different time points while contrasting with other molecules.
- Core assumption: The contrastive loss provides regularization that helps the model learn invariant features across different views of the same molecule
- Evidence anchors:
  - [section 3.3] "maximizing the second term of Eq. 16 as a contrastive task" and describes the contrastive surrogate formulation
  - [section 4.4] Provides ablation study showing λ2 = 0.01 performs best, with λ2 = 1 degrading performance significantly
  - [corpus] Weak evidence - neighboring papers mention contrastive learning but don't discuss its role in trajectory-based pretraining
- Break condition: If λ2 is set too high, the contrastive component dominates and harms performance (as shown in ablation studies)

## Foundational Learning

- Concept: Stochastic differential equations (SDEs) and their solutions
  - Why needed here: The paper uses SDEs to model the continuous diffusion process for generating augmented molecular trajectories
  - Quick check question: Can you explain the difference between Variance Exploding (VE) and Variance Preserving (VP) SDEs and when each might be appropriate?

- Concept: SE(3) equivariance and its implications for neural networks
  - Why needed here: The paper requires the model to respect 3D molecular symmetries, which is fundamental to the architecture design
  - Quick check question: How does SE(3) equivariance constrain the form of neural network layers used for 3D molecular representations?

- Concept: Contrastive learning and mutual information maximization
  - Why needed here: The contrastive component of the framework relies on contrasting augmented views of the same molecule with views of different molecules
  - Quick check question: What's the relationship between contrastive learning objectives and mutual information estimation in the context of molecular representations?

## Architecture Onboarding

- Component map: Equivariant encoder for ground truth → Score network → Reconstruction loss; Equivariant encoder for noised molecule → Projection head → Contrastive loss
- Critical path: Noised molecule → Equivariant encoder → Score network → Reconstruction loss; Ground truth molecule → Equivariant encoder → Projection head → Contrastive loss
- Design tradeoffs: The paper uses separate heads for score functions and contrastive learning, which adds complexity but allows specialized processing. An alternative