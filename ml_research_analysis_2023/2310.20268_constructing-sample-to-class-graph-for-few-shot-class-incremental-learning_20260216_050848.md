---
ver: rpa2
title: Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning
arxiv_id: '2310.20268'
source_url: https://arxiv.org/abs/2310.20268
tags:
- learning
- graph
- few-shot
- samples
- fscil
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles few-shot class-incremental learning (FSCIL),
  where models must learn new classes from few samples without forgetting prior knowledge.
  The key idea is to build a Sample-to-Class (S2C) graph that captures relationships
  at both sample and class levels.
---

# Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning

## Quick Facts
- arXiv ID: 2310.20268
- Source URL: https://arxiv.org/abs/2310.20268
- Reference count: 40
- Primary result: Sample-to-Class (S2C) graph method improves FSCIL performance by up to 0.82 accuracy on MiniImageNet and reduces forgetting rates by over 0.5

## Executive Summary
This paper introduces a novel Sample-to-Class (S2C) graph approach for few-shot class-incremental learning (FSCIL), where models must learn new classes from few samples without forgetting prior knowledge. The method constructs a Sample-level Graph Network (SGN) to refine features by aggregating similar samples within sessions, and a Class-level Graph Network (CGN) to establish cross-session connections between old and new class prototypes. A three-stage training strategy enables effective graph construction despite limited few-shot samples. Experiments on CIFAR100, MiniImageNet, and CUB200 demonstrate significant performance gains over state-of-the-art methods.

## Method Summary
S2C builds two complementary graph networks: SGN operates at the sample level to reduce overfitting through similarity-based feature aggregation, while CGN operates at the class level to prevent forgetting by connecting old and new class prototypes across sessions. The method uses a multi-stage training approach - first pre-training on a base session with abundant data, then adapting to few-shot scenarios through pseudo-incremental tasks, and finally fine-tuning on real FSCIL tasks. The approach uses a ResNet backbone with SGN and CGN modules, trained with a combination of classification, triplet, and S2C losses.

## Key Results
- Achieves performance gains of up to 0.82 accuracy on MiniImageNet compared to state-of-the-art methods
- Reduces forgetting rates by over 0.5 on all tested datasets (CIFAR100, MiniImageNet, CUB200)
- Demonstrates effectiveness of sample-level feature refinement through SGN and cross-session class connections through CGN
- Shows consistent improvements across 5-way 5-shot FSCIL setup with varying number of incremental sessions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SGN reduces overfitting in few-shot sessions by refining class-level features through sample similarity aggregation
- Mechanism: SGN constructs a fully-connected graph where sample features are nodes and sample similarity defines edges. Iterative message passing aggregates neighbor features to produce more discriminative class-level representations
- Core assumption: Sample-level relationships within a session contain discriminative information that can be leveraged to reduce overfitting
- Evidence anchors: [abstract] "SGN helps aggregate similar samples, ultimately leading to the extraction of more refined class-level features" [section] "By applying iterative aggregation operations of the GNN on both node information and edge information, the features of the samples are continuously updated"
- Break condition: If sample relationships within a session are uninformative (e.g., highly noisy data), SGN's aggregation may amplify noise rather than refine features

### Mechanism 2
- Claim: CGN prevents catastrophic forgetting by establishing cross-session semantic connections between old and new class prototypes
- Mechanism: CGN takes refined class-level features from SGN and uses transformer-based attention to create edges between old and new class prototypes, allowing the model to leverage semantic relationships across sessions
- Core assumption: Class-level features from different sessions contain meaningful semantic relationships that can be exploited to maintain old knowledge while learning new classes
- Evidence anchors: [abstract] "CGN establishes connections across class-level features of both new and old classes" [section] "CGN forges connections between old and novel classes, thereby augmenting the capacity to differentiate classes across sessions"
- Break condition: If semantic gap between old and new classes is too large (e.g., disjoint domains), attention-based connections may not effectively transfer knowledge

### Mechanism 3
- Claim: Multi-stage training enables effective graph construction despite limited few-shot samples
- Mechanism: Three-stage training (base graph pre-construction → pseudo-incremental adaptation → real FSCIL training) progressively builds graph capabilities, starting from abundant base data and progressing to few-shot scenarios
- Core assumption: Graph construction capabilities can be learned incrementally, starting from abundant data and progressing to few-shot scenarios
- Evidence anchors: [abstract] "multi-stage training strategy initializes the model with a base session, adapts it to few-shot scenarios via pseudo-incremental tasks, and fine-tunes it on real FSCIL tasks" [section] "We design a multi-stage strategy for training S2C model, which mitigates the training challenges posed by limited data in the incremental process"
- Break condition: If base session is too small or unrepresentative, initial graph construction may be poor, limiting subsequent stages' effectiveness

## Foundational Learning

- Graph Neural Networks (GNN)
  - Why needed here: GNNs enable the model to capture complex relationships between samples and classes that traditional neural networks cannot easily represent
  - Quick check question: Can you explain how message passing in GNNs differs from standard convolution operations?

- Meta-learning and few-shot learning principles
  - Why needed here: Understanding how models learn to learn from limited examples is crucial for grasping why S2C's pseudo-incremental stage works
  - Quick check question: What is the key difference between metric-based and optimization-based few-shot learning approaches?

- Catastrophic forgetting and knowledge distillation
  - Why needed here: The CGN component specifically addresses forgetting by maintaining relationships between old and new classes
  - Quick check question: How does knowledge distillation typically help prevent catastrophic forgetting in incremental learning?

## Architecture Onboarding

- Component map:
  Feature extractor -> SGN (sample-level graph) -> CGN (class-level graph) -> Joint classifier

- Critical path:
  1. Extract features from input samples
  2. Build SGN with sample features as nodes and similarity as edges
  3. Aggregate SGN to obtain refined class-level features
  4. Build CGN connecting old and new class features
  5. Calibrate class features through CGN
  6. Make predictions using joint classifier

- Design tradeoffs:
  - GNN depth vs. overfitting: Deeper SGN/CGN may capture more complex relationships but risk overfitting on few-shot data
  - Attention mechanism complexity: Full attention in CGN captures rich relationships but increases computational cost
  - Pseudo-incremental task diversity: More diverse synthetic tasks improve adaptation but require careful sampling

- Failure signatures:
  - Overfitting in early sessions: SGN may be too powerful relative to available samples
  - Forgetting in later sessions: CGN may not effectively maintain old class relationships
  - Poor adaptation: Pseudo-incremental stage may not generalize to real FSCIL tasks

- First 3 experiments:
  1. Baseline vs. SGN-only: Compare performance with and without SGN to verify its overfitting reduction
  2. CGN impact test: Train with SGN only, then add CGN to measure forgetting reduction
  3. Multi-stage ablation: Test different training stage combinations (base only, base+pseudo, all stages) to validate staged learning benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of S2C change when using different types of graph neural networks (e.g., GCN, GAT, GIN) for the Sample-level Graph Network (SGN)?
- Basis in paper: [inferred] The paper mentions using GNN for SGN but does not specify the exact type
- Why unresolved: The choice of GNN architecture could significantly impact the model's ability to capture sample relationships and affect overall performance
- What evidence would resolve it: Experiments comparing S2C's performance using different GNN architectures for SGN on benchmark datasets

### Open Question 2
- Question: What is the impact of the pseudo-incremental learning stage on the final performance of S2C, and can it be replaced or improved by other meta-learning techniques?
- Basis in paper: [explicit] The paper introduces a pseudo-incremental learning stage but does not explore alternatives or analyze its specific impact
- Why unresolved: Understanding the contribution of this stage and potential alternatives could lead to more efficient training strategies
- What evidence would resolve it: Ablation studies removing the pseudo-incremental stage and replacing it with other meta-learning approaches, comparing results

### Open Question 3
- Question: How does S2C perform when the base session is not available, or when it contains very few samples?
- Basis in paper: [inferred] The paper assumes a base session with ample samples, but real-world scenarios might not always provide this
- Why unresolved: This would test the robustness of S2C in more challenging scenarios and its ability to learn from limited initial data
- What evidence would resolve it: Experiments on datasets where the base session is reduced or removed entirely, comparing S2C's performance to other methods

### Open Question 4
- Question: Can the Class-level Graph Network (CGN) be extended to capture more complex relationships between classes, such as hierarchical or semantic relationships?
- Basis in paper: [explicit] The paper mentions that CGN establishes connections between old and new classes but does not explore more complex relationship types
- Why unresolved: Incorporating richer semantic information could further improve the model's ability to transfer knowledge between classes and reduce forgetting
- What evidence would resolve it: Experiments incorporating hierarchical or semantic information into CGN and measuring the impact on FSCIL performance

## Limitations

- The paper's claims about SGN reducing overfitting and CGN preventing forgetting are primarily supported by internal experimental results rather than external validation
- Architecture details for the encoding network ϕ and specific hyperparameters (triplet loss margin, S2C loss weight) are not fully specified
- The multi-stage training strategy lacks ablation studies isolating the contribution of each stage

## Confidence

- **High Confidence**: The paper's core methodology is technically sound, with clear descriptions of the S2C graph construction and multi-stage training approach
- **Medium Confidence**: Claims about performance improvements over state-of-the-art methods are supported by experimental results, but the specific contributions of individual components could benefit from more detailed ablation studies
- **Low Confidence**: The assertion that SGN specifically reduces overfitting and CGN effectively prevents forgetting relies heavily on comparative results without providing mechanistic insights

## Next Checks

1. **Component Ablation Study**: Implement and test SGN-only and CGN-only versions to quantify the individual contributions of each network to overall performance improvements

2. **Cross-Dataset Generalization**: Evaluate S2C on additional datasets beyond CIFAR100, MiniImageNet, and CUB200 to assess the robustness of the approach across different data distributions and domain shifts

3. **Hyperparameter Sensitivity Analysis**: Systematically vary key hyperparameters (triplet loss margin, S2C loss weight, GNN depth) to determine the stability of performance improvements and identify potential overfitting to specific parameter settings