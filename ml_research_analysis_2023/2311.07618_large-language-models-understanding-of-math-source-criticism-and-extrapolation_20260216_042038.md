---
ver: rpa2
title: 'Large Language Models'' Understanding of Math: Source Criticism and Extrapolation'
arxiv_id: '2311.07618'
source_url: https://arxiv.org/abs/2311.07618
tags:
- gpt-4
- mathematical
- proof
- extrapolation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study critically examines claims about GPT-4's mathematical
  understanding through source criticism and controlled evaluation. The authors craft
  simple mathematical problems whose formal proofs are not readily available online,
  testing whether GPT-4 can solve them through genuine understanding or merely reproduces
  known proofs.
---

# Large Language Models' Understanding of Math: Source Criticism and Extrapolation

## Quick Facts
- arXiv ID: 2311.07618
- Source URL: https://arxiv.org/abs/2311.07618
- Reference count: 19
- Primary result: GPT-4 fails on novel mathematical problems despite their simplicity, revealing limitations in genuine mathematical understanding

## Executive Summary
This study critically examines claims about GPT-4's mathematical understanding by testing it on formal mathematical problems whose proofs are not readily available online. The authors demonstrate that GPT-4 fails on these novel problems, providing incorrect proofs with obvious errors and using irrelevant lemmas from the mathlib library. The study reveals that GPT-4's capabilities evolve over time despite claiming to be a fixed model, and argues that mathematical theorem proving is more akin to a retrieval task like Google's search engine rather than next-word prediction. The authors conclude that GPT-4's strength lies in reproducing, rephrasing, and polishing existing proofs rather than genuine mathematical understanding.

## Method Summary
The authors craft mathematical problems in Lean formal language where formal proofs are not available online, verified through Google search. They present these problems to GPT-4 and analyze its responses, comparing them to known proofs and identifying errors or irrelevant lemmas used. The study also tracks GPT-4's performance over time to test the claim that it's a fixed model, observing improvements between June and October 2023 on problems it previously couldn't solve.

## Key Results
- GPT-4 fails on simple mathematical problems whose formal proofs are not available online, despite being able to solve similar problems with known proofs
- The model uses irrelevant lemmas from the mathlib library and makes false claims about mathematical properties in its proofs
- GPT-4's capabilities evolve over time, contradicting its assertion of being a fixed model with training data only up to January 2022

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's mathematical capabilities are limited to reproducing, rephrasing, and polishing proofs it has already seen, not to genuine understanding.
- Mechanism: The model matches new prompts to similar patterns in its training data, interpolating to the closest match rather than reasoning from first principles.
- Core assumption: The training set contains formal mathematical proofs that are not publicly disclosed, making it impossible to distinguish between genuine understanding and memorized patterns.
- Evidence anchors:
  - [abstract] The authors craft problems whose formal proofs are not readily available online and observe GPT-4 failing on these despite their simplicity.
  - [section 3] Case studies show GPT-4 using irrelevant lemmas from the mathlib library and making false claims about mathematical properties.
  - [corpus] No direct corpus evidence that GPT-4 has seen the specific proofs tested, but the paper notes that any formal proof available online may have been used in training.
- Break condition: If GPT-4 could successfully solve novel mathematical problems that have no formal proof available online, this mechanism would break.

### Mechanism 2
- Claim: GPT-4's capabilities evolve over time despite claiming to be a fixed model.
- Mechanism: The model incorporates new information sources and capabilities through modules that search additional sources and update outputs based on prompt history.
- Core assumption: GPT-4 is not a single-module model but has a sophisticated system of modules that preprocess prompts and regulate outputs.
- Evidence anchors:
  - [section 2.2] Examples show GPT-4 solving problems in October 2023 that it couldn't solve in June 2023, despite claiming to be a fixed model with training data only up to January 2022.
  - [abstract] The study reveals that GPT-4's capabilities evolve over time, contradicting its assertion of being a fixed model.
  - [corpus] Weak corpus evidence as the paper focuses on GPT-4's interactions rather than third-party validation of capability evolution.
- Break condition: If OpenAI provides transparent documentation showing GPT-4's capabilities remain fixed as claimed, this mechanism would break.

### Mechanism 3
- Claim: Mathematical theorem proving is more akin to a retrieval task like Google's search engine rather than next-word prediction.
- Mechanism: Solving theorems requires identifying applicable lemmas and tactics from a large library, which is best accomplished through search and retrieval methods rather than language model interpolation.
- Core assumption: The task of finding relevant mathematical premises is fundamentally different from predicting the next word in a sentence.
- Evidence anchors:
  - [section 5] The paper argues that using a search engine to retrieve applicable lemmas is more efficient than training a language model on all available formal proofs.
  - [abstract] The authors suggest that mathematical theorem proving is comparable to methods used in search engines rather than next-word prediction.
  - [corpus] No direct corpus evidence supporting this mechanism, but the paper references existing premise selection methods like Sledgehammer.
- Break condition: If language models could consistently solve novel mathematical theorems without relying on retrieval of existing proofs, this mechanism would break.

## Foundational Learning

- Concept: Source criticism
  - Why needed here: To evaluate claims about GPT-4's mathematical understanding by examining the sources it has likely seen during training.
  - Quick check question: What is the primary method used in the paper to determine whether GPT-4 has genuinely understood mathematical concepts or simply reproduced known proofs?

- Concept: Extrapolation in language models
  - Why needed here: To understand how GPT-4 generates responses beyond its training data and why this leads to failures in novel mathematical problems.
  - Quick check question: According to the paper, what is the difference between the type of extrapolation GPT-4 performs when rephrasing text versus when attempting to prove novel theorems?

- Concept: Kolmogorov complexity
  - Why needed here: To provide context for discussions about compression and generalization in deep learning models, though the paper argues it's overemphasized.
  - Quick check question: Why does the paper argue that Kolmogorov complexity alone cannot explain the generalization capabilities of GPT-4?

## Architecture Onboarding

- Component map: User prompt -> preprocessing modules -> language model generation -> post-processing modules -> output
- Critical path: User prompt → preprocessing modules → language model generation → post-processing modules → output
- Design tradeoffs: The model trades off between generating novel responses through extrapolation and staying grounded in its training data, with the latter being more reliable for mathematical proofs.
- Failure signatures: Using irrelevant lemmas, making false claims about mathematical properties, and failing to solve simple problems whose proofs are not available online.
- First 3 experiments:
  1. Test GPT-4 on simple mathematical problems whose formal proofs are not available online to verify it cannot solve them.
  2. Compare GPT-4's responses to the same problem at different time points to check for capability evolution.
  3. Analyze GPT-4's use of lemmas from the mathlib library in its proofs to identify patterns of irrelevant lemma selection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What percentage of mathematical theorems can GPT-4 solve correctly without prior exposure to their formal proofs?
- Basis in paper: [explicit] The paper states that GPT-4's correct answers appear to be based on proofs it has seen before, suggesting it cannot solve theorems it hasn't encountered.
- Why unresolved: The authors only tested GPT-4 on a small set of problems with unavailable formal proofs, making it unclear how GPT-4 would perform on a broader range of unseen theorems.
- What evidence would resolve it: OpenAI should disclose GPT-4's training data and evaluate its performance on a large, diverse set of formal theorems, comparing results between seen and unseen proofs.

### Open Question 2
- Question: Does GPT-4's performance on mathematical theorems improve over time due to model updates or expanded training data?
- Basis in paper: [explicit] The authors observed that GPT-4's ability to prove theorems expanded between June and October 2023, contradicting its claim of being a fixed model.
- Why unresolved: The authors only have anecdotal evidence of improvement on a few specific problems, and the mechanism behind GPT-4's evolving capabilities remains unclear.
- What evidence would resolve it: OpenAI should clarify whether GPT-4 undergoes continuous training or data updates and provide a detailed analysis of how its mathematical reasoning capabilities change over time.

### Open Question 3
- Question: Is the task of proving mathematical theorems more similar to a retrieval task (like Google search) or a next-word prediction task?
- Basis in paper: [explicit] The authors argue that theorem proving is more akin to a retrieval task, as it involves finding and applying relevant lemmas from a large library, rather than predicting the next word in a sentence.
- Why unresolved: While the authors provide reasoning for their claim, they do not present empirical evidence directly comparing the performance of GPT-4 and retrieval-based methods on theorem proving tasks.
- What evidence would resolve it: A comparative study should be conducted, evaluating GPT-4's performance on theorem proving tasks against a retrieval-based approach that searches and ranks relevant lemmas from a formal library.

## Limitations
- The paper cannot definitively exclude that GPT-4 has seen the specific proofs tested, as training data is undisclosed
- The evolving capabilities claim relies on observed behavior rather than transparent model architecture disclosure
- The retrieval vs prediction claim lacks direct comparison experiments between search-based and language-model-only approaches

## Confidence
- Medium: Claims about GPT-4's reliance on reproduction vs understanding - supported by concrete examples but limited by inability to verify training data
- Medium: Claims about evolving capabilities contradicting fixed model assertion - well-documented but potentially explainable by external search modules
- Medium: Claims about retrieval vs next-word prediction for theorem proving - theoretically sound but lacking direct empirical validation

## Next Checks
1. **Blind Validation**: Have independent researchers create and verify novel mathematical problems whose formal proofs are truly unavailable online, then test multiple GPT-4 instances across different time periods to confirm consistent failure patterns.

2. **Module Isolation Test**: Design experiments to determine whether GPT-4's mathematical responses come from its core language model or from external search modules, by testing under conditions that would disable web access if present.

3. **Cross-Model Comparison**: Compare GPT-4's performance on novel mathematical problems against specialized theorem provers and search-based systems to empirically validate whether retrieval-based approaches outperform language model extrapolation for mathematical reasoning.