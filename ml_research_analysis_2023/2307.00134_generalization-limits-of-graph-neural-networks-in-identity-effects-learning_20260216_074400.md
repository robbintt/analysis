---
ver: rpa2
title: Generalization Limits of Graph Neural Networks in Identity Effects Learning
arxiv_id: '2307.00134'
source_url: https://arxiv.org/abs/2307.00134
tags:
- graph
- graphs
- neural
- test
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates the generalization capabilities of Graph
  Neural Networks (GNNs) when learning identity effects, i.e., the task of determining
  whether an object is composed of two identical components or not. The authors analyze
  two case studies: (i) two-letter words, where they show that GNNs trained via stochastic
  gradient descent are unable to generalize to unseen letters when utilizing orthogonal
  encodings like one-hot representations; (ii) dicyclic graphs, i.e., graphs composed
  of two cycles, where they present positive existence results leveraging the connection
  between GNNs and the Weisfeiler-Lehman test.'
---

# Generalization Limits of Graph Neural Networks in Identity Effects Learning

## Quick Facts
- **arXiv ID**: 2307.00134
- **Source URL**: https://arxiv.org/abs/2307.00134
- **Reference count**: 40
- **Primary result**: GNNs trained via SGD cannot generalize to unseen letters in two-letter word identity tasks when using orthogonal encodings like one-hot, but can classify symmetric vs asymmetric dicyclic graphs by leveraging 1-WL test connection.

## Executive Summary
This work investigates when Graph Neural Networks (GNNs) can or cannot learn to identify whether an object is composed of two identical components. The authors analyze two case studies: two-letter words where orthogonal encodings prevent generalization to unseen letters, and dicyclic graphs where GNNs succeed by leveraging the expressive power of the Weisfeiler-Lehman test. The theoretical analysis reveals fundamental limitations arising from encoding choices and dataset symmetries, supported by extensive numerical experiments comparing different architectures and encodings.

## Method Summary
The paper analyzes GNN generalization limits in identity effects learning through theoretical analysis and experiments. GNN models are implemented with message-passing layers using UPDATE and AGGREGATE functions, followed by READOUT operations (sum pooling or difference). The models are trained using Adam optimizer with binary cross-entropy loss on two tasks: classifying two-letter words as identical or not, and distinguishing symmetric from asymmetric dicyclic graphs. Different node encodings (one-hot, Haar, distributed, Gaussian) are tested to evaluate their impact on generalization capabilities.

## Key Results
- GNNs trained via SGD fail to generalize to unseen letters in two-letter word tasks when using orthogonal encodings like one-hot
- GNNs can successfully classify symmetric vs asymmetric dicyclic graphs by leveraging the connection to the 1-WL test
- Non-orthogonal encodings (Gaussian, distributed) achieve better generalization in identity tasks compared to orthogonal encodings
- Architecture choice (sum pooling vs difference pooling) significantly impacts generalization performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GNNs cannot generalize to unseen letters in two-letter word identity tasks when using orthogonal encodings like one-hot.
- Mechanism: The transformation τ defined in the paper swaps two unseen letters (e.g., Y and Z). If the encoding is orthogonal and the dataset is invariant under τ, the GNN parameters remain symmetric under τ, leading to identical output distributions for τ(x) and x, preventing correct classification of unseen identity effects.
- Core assumption: The training data excludes the transformed letters, and the encoding is orthogonal so that the transformation matrix T₂ is orthogonal and symmetric.
- Evidence anchors:
  - [abstract] "...GNNs trained via stochastic gradient descent are unable to generalize to unseen letters when utilizing orthogonal encodings like one-hot representations..."
  - [section] Theorem III.1 provides conditions for invariance under transformation τ.
  - [corpus] Corpus signals mention "orthogonal encodings" and "generalization" but no direct support for this specific transformation argument.
- Break condition: If the encoding is not orthogonal, or if the dataset contains the transformed letters, the invariance breaks and generalization may succeed.

### Mechanism 2
- Claim: GNNs can classify symmetric vs asymmetric dicyclic graphs because the 1-WL test distinguishes them.
- Mechanism: In a dicyclic graph [m,n], after one WL iteration, 3-degree nodes have a unique color. The 1-WL test then propagates colors differently depending on whether m = n, leading to distinct stable colorings for symmetric and asymmetric cases. GNNs can learn these colorings, enabling correct classification.
- Core assumption: The GNN architecture matches the expressive power of the 1-WL test, and the graph is uniformly colored initially.
- Evidence anchors:
  - [abstract] "...dicyclic graphs...for which we present positive existence results leveraging the connection between GNNs and the WL test."
  - [section] Theorem III.5 proves 1-WL distinguishes symmetric dicyclic graphs; Corollary III.6 shows GNNs can learn this.
  - [corpus] Corpus signals include "Weisfeiler-Lehman test" and "dicyclic graphs" but no detailed mechanism support.
- Break condition: If the graph is not uniformly colored or if the GNN architecture cannot simulate the required WL iterations, classification fails.

### Mechanism 3
- Claim: Training procedure and initialization affect generalization in GNNs for identity effects.
- Mechanism: If the initial weights G₀ and H₀ are independently sampled from distributions invariant under right-multiplication by T₂ (e.g., isotropic Gaussian), then SGD updates preserve this invariance, leading to inability to distinguish τ(x) and x. Architecture choices (e.g., sum pooling vs difference pooling) influence whether learned representations respect or break this symmetry.
- Core assumption: SGD updates with invariant initializations maintain invariance of the learned function.
- Evidence anchors:
  - [section] Theorem III.1 assumes independent initialization and SGD with invariant distributions.
  - [section] Experimental section shows that non-orthogonal encodings (Gaussian, distributed) achieve better generalization, suggesting invariance breaking helps.
  - [corpus] Corpus mentions "SGD" and "initialization" but lacks detailed support for invariance preservation.
- Break condition: If initialization is not invariant, or if the optimization algorithm does not preserve symmetry (e.g., using ReLU nonlinearities), invariance may break and generalization may improve.

## Foundational Learning

- Concept: Graph Neural Networks and Message Passing
  - Why needed here: The paper analyzes GNN generalization limits; understanding how GNNs aggregate neighbor information is essential to grasp the theoretical results.
  - Quick check question: In a two-node graph, what does a GNN layer compute in terms of node features?
- Concept: Weisfeiler-Lehman Test and Graph Isomorphism
  - Why needed here: The paper links GNN expressive power to the WL test; knowing how WL colors nodes is crucial for understanding why GNNs can or cannot distinguish certain graphs.
  - Quick check question: After one WL iteration on a dicyclic graph, which nodes get a different color from the rest?
- Concept: Invariance and Symmetry in Learning
  - Why needed here: The rating impossibility theorem relies on dataset and model invariance under a transformation; understanding when and why invariance leads to generalization failure is key.
  - Quick check question: If a transformation τ leaves the dataset unchanged and the model is invariant under τ, what must be true about the model's output on x and τ(x)?

## Architecture Onboarding

- Component map:
  Input -> Message Passing Layers -> READOUT -> Output
- Critical path:
  1. Encode nodes (e.g., one-hot or distributed)
  2. Run T message passing iterations
  3. Apply READOUT (sum pooling or difference)
  4. Final linear layer + sigmoid
- Design tradeoffs:
  - Orthogonal vs non-orthogonal encodings: Orthogonal may cause invariance issues; non-orthogonal may allow better generalization.
  - Sum pooling vs difference pooling: Sum pooling may average out key asymmetries; difference pooling may preserve them.
  - Number of layers: Too few may not propagate distinguishing information; too many may overfit or cause optimization issues.
- Failure signatures:
  - Equal outputs for τ(x) and x in identity tasks with orthogonal encodings.
  - Poor accuracy on symmetric vs asymmetric dicyclic graphs despite correct training loss.
  - High variance in training loss across trials with different random seeds.
- First 3 experiments:
  1. Train Gconv-glob on two-letter words with one-hot encoding; check if AA and BB get same rating as XY and XZ.
  2. Train Gconv-diff on dicyclic graphs with nmax=8; evaluate accuracy on [7,9] vs [8,8].
  3. Compare Gconv-glob vs Gconv-diff on dicyclic graphs; observe difference in training vs test accuracy for extrapolation tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are the conditions for rating impossibility in Theorem III.1 also necessary?
- Basis in paper: [explicit] The authors state that Theorem III.1 identifies sufficient conditions for rating impossibility, but do not explore whether these conditions are also necessary.
- Why unresolved: The paper focuses on establishing sufficient conditions for rating impossibility, but does not investigate the necessity of these conditions. Further research is needed to determine if the identified conditions are also necessary for rating impossibility to hold.
- What evidence would resolve it: To resolve this question, researchers could conduct experiments with variations in the assumptions of Theorem III.1 to determine if the theorem still holds when certain conditions are relaxed or removed.

### Open Question 2
- Question: Why do non-orthogonal encodings allow generalization outside the training set in the two-letter word task?
- Basis in paper: [inferred] The numerical experiments in §IV-B show that non-orthogonal encodings, such as the 6-bit distributed encoding and the Gaussian encoding, exhibit superior performance compared to orthogonal encodings in the two-letter word classification task. However, the paper does not provide a theoretical explanation for this phenomenon.
- Why unresolved: While the experimental results demonstrate the superior performance of non-orthogonal encodings, the underlying reasons for this behavior are not explored in the paper. Further theoretical analysis is needed to understand why non-orthogonal encodings allow for better generalization in this specific task.
- What evidence would resolve it: To resolve this question, researchers could conduct a theoretical analysis of the impact of encoding orthogonality on the generalization capabilities of GNNs in the two-letter word classification task. This could involve studying the properties of non-orthogonal encodings and their influence on the learning process.

### Open Question 3
- Question: What conditions on the GNN architecture lead to rating impossibility in the dicyclic graph classification task?
- Basis in paper: [inferred] The numerical experiments in §IV-C show that the choice of GNN architecture significantly affects the ability to generalize in the dicyclic graph classification task. However, the paper does not provide a theoretical analysis of the relationship between GNN architecture and rating impossibility in this setting.
- Why unresolved: While the experimental results highlight the importance of architecture choice, the paper does not explore the underlying reasons for this behavior. Further research is needed to identify the specific conditions on the GNN architecture that lead to rating impossibility in the dicyclic graph classification task.
- What evidence would resolve it: To resolve this question, researchers could conduct a theoretical analysis of the impact of different GNN architectures on the generalization capabilities in the dicyclic graph classification task. This could involve studying the properties of different architectures and their influence on the learning process.

## Limitations
- The theoretical impossibility results rely on strong assumptions about dataset invariance and orthogonal encodings that may not hold in practical scenarios
- The analysis focuses on specific architectures (Gconv-glob and Gconv-diff) without exploring whether alternative GNN designs could overcome the identified limitations
- The paper assumes uniform node coloring in dicyclic graphs, which may not be realistic for practical graph datasets

## Confidence
- **High confidence**: Claims about 1-WL test distinguishing symmetric vs asymmetric dicyclic graphs (Theorem III.5, Corollary III.6) - supported by rigorous mathematical proof
- **Medium confidence**: Generalization impossibility results for orthogonal encodings in two-letter word tasks (Theorem III.1) - relies on strong assumptions about dataset and initialization invariance
- **Low confidence**: Claims about SGD preserving invariance under transformations - depends on unstated assumptions about initialization distributions and optimization dynamics

## Next Checks
1. Test whether SGD with typical initializations (e.g., Glorot uniform) actually preserves invariance under the transformation τ in the two-letter word task, or if practical training breaks the theoretical failure mode.

2. Verify the 1-WL distinction between symmetric and asymmetric dicyclic graphs experimentally by implementing the WL test and comparing its accuracy directly against GNN performance across varying cycle lengths.

3. Investigate whether alternative readout functions (e.g., attention-based pooling, hierarchical pooling) or architectural modifications can achieve generalization in identity effects learning where the proposed Gconv-glob and Gconv-diff architectures fail.