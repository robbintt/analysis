---
ver: rpa2
title: 'LLM Factoscope: Uncovering LLMs'' Factual Discernment through Inner States
  Analysis'
arxiv_id: '2312.16374'
source_url: https://arxiv.org/abs/2312.16374
tags:
- factual
- output
- data
- llms
- inner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM Factoscope, a Siamese network-based model
  for factual detection in large language models (LLMs) using inner states analysis.
  The key idea is to identify distinguishable patterns in LLMs' inner states when
  generating factual versus non-factual content.
---

# LLM Factoscope: Uncovering LLMs' Factual Discernment through Inner States Analysis

## Quick Facts
- arXiv ID: 2312.16374
- Source URL: https://arxiv.org/abs/2312.16374
- Reference count: 40
- Primary result: Achieves over 96% accuracy in factual detection across various LLM architectures using inner states analysis

## Executive Summary
This paper introduces LLM Factoscope, a novel Siamese network-based model for detecting factual versus non-factual content generated by large language models (LLMs). The approach leverages distinctive patterns in LLMs' inner states—specifically activation maps, final output ranks, top-k output indices, and top-k output probabilities—when generating factual versus non-factual content. Through comprehensive experiments across six different LLM architectures (GPT2-XL-1.5B, Llama2-7B, Vicuna-7B, StableLM-7B, Llama2-13B, and Vicuna-13B), the model demonstrates over 96% accuracy in factual detection, opening new avenues for enhancing LLM reliability and transparency.

## Method Summary
LLM Factoscope employs a Siamese network framework with four specialized sub-models (ResNet18 for activation maps, GRU for final output ranks, and two additional ResNet18 models for top-k output indices and probabilities) to analyze LLMs' inner states during text generation. The model uses triplet margin loss to learn embeddings that maximize similarity between same-class samples while minimizing similarity between different-class samples. During inference, a support set of labeled examples enables few-shot classification by comparing test samples to this reference set, allowing the model to classify outputs as factual or non-factual based on their proximity to known examples in the embedding space.

## Key Results
- Achieves over 96% accuracy in factual detection across six different LLM architectures
- Multi-modal inner state analysis (activation maps, ranks, indices, probabilities) proves more effective than single-feature approaches
- Siamese network with triplet margin loss enables effective few-shot learning for factual detection
- Ablation studies demonstrate that each sub-model contributes meaningfully to overall performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM Factoscope can detect factual vs. non-factual outputs by analyzing distinct patterns in inner states like activation maps, final output ranks, top-k indices, and probabilities.
- Mechanism: The model uses a Siamese network to learn embeddings from these inner states, then compares test samples to a support set of labeled examples to classify outputs.
- Core assumption: Factual and non-factual content generate distinguishable activation and decision-making patterns within the LLM.
- Evidence anchors:
  - [abstract] "Our investigation reveals distinguishable patterns in LLMs' inner states when generating factual versus non-factual content."
  - [section] "We hypothesize that LLMs...might exhibit distinguishable patterns in their inner states when generating outputs that are either factual or non-factual."
  - [corpus] Weak - neighboring papers focus on hallucination detection but don't directly confirm the inner state patterns described.
- Break condition: If LLMs generate factual and non-factual content using similar inner state patterns, the detection accuracy would drop significantly.

### Mechanism 2
- Claim: Different inner state types (activation maps, ranks, indices, probabilities) provide complementary information for factual detection.
- Mechanism: Each inner state type is processed by a specialized sub-model (CNN or GRU), then embeddings are combined to capture spatial and temporal aspects of LLM decision-making.
- Core assumption: No single inner state type captures all the relevant information needed for accurate factual detection.
- Evidence anchors:
  - [abstract] "The LLM factoscope analyzes the inner states from LLMs, including activation maps, final output ranks, top-k output indices, and top-k output probabilities, each offering a unique perspective on the model's internal decision-making process."
  - [section] "By integrating this multi-dimensional analysis of inner states within LLMs, LLM Factoscope effectively discerns factual from non-factual outputs."
  - [corpus] Weak - neighboring papers mention inner state analysis but don't specifically address the multi-modal approach described.
- Break condition: If one inner state type becomes significantly more predictive than others, the multi-model approach might be unnecessary.

### Mechanism 3
- Claim: The Siamese network framework with triplet margin loss enables effective few-shot learning for factual detection.
- Mechanism: The model learns to maximize similarity between embeddings of same-class (factual or non-factual) samples while minimizing similarity between different-class samples.
- Core assumption: The embedding space can be structured such that factual and non-factual outputs form separable clusters.
- Evidence anchors:
  - [abstract] "Our model comprises several sub-models...The training process maximizes the loss, refining the model's ability to accurately differentiate between factual and non-factual content."
  - [section] "The triplet margin loss aims to ensure that the similarity between the anchor and the positive instance...is larger than the similarity between the anchor and the negative instance...by at least a margin α."
  - [corpus] Weak - neighboring papers mention Siamese networks but don't provide direct evidence for their effectiveness in this specific application.
- Break condition: If the embedding space cannot effectively separate factual from non-factual content, classification accuracy will suffer.

## Foundational Learning

- Concept: Inner states of LLMs (activation maps, hidden states, logits)
  - Why needed here: Understanding what inner states are and how they're captured is essential for grasping how Factoscope works
  - Quick check question: What are the four types of inner states collected by Factoscope, and what does each represent about the LLM's processing?

- Concept: Siamese networks and triplet margin loss
  - Why needed here: The model architecture relies on Siamese networks with triplet margin loss for few-shot learning
  - Quick check question: How does the triplet margin loss function work to separate embeddings of different classes?

- Concept: Few-shot learning and support set classification
  - Why needed here: Factoscope uses a support set during testing rather than traditional classification
  - Quick check question: How does the support set approach differ from traditional classification, and why might it be advantageous for factual detection?

## Architecture Onboarding

- Component map:
  Data collection pipeline (CSV datasets → prompts → LLM queries) → Inner state capture (activation maps, ranks, indices, probabilities) → Preprocessing (normalization, transformation, distance calculation) → Four sub-models (ResNet18 for activation/indices/probabilities, GRU for ranks) → Embedding combination and classification via support set comparison

- Critical path: Data collection → Inner state capture → Preprocessing → Sub-model processing → Embedding combination → Support set classification

- Design tradeoffs:
  - Multi-modal inner state analysis vs. simpler single-feature approaches
  - Siamese network with support set vs. traditional classification
  - Computational cost of capturing multiple inner states vs. accuracy gains

- Failure signatures:
  - Low accuracy across architectures suggests fundamental issues with inner state patterns or model design
  - Inconsistent performance across different relations suggests generalization problems
  - High false negative rate with large support sets suggests class imbalance in non-factual content

- First 3 experiments:
  1. Verify inner state patterns: Run LLM on factual vs. non-factual prompts and visualize activation patterns to confirm distinguishable differences
  2. Test sub-model effectiveness: Train each sub-model independently and measure individual accuracy to understand their contributions
  3. Evaluate support set size: Test classification accuracy with varying support set sizes to find optimal balance between accuracy and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of LLM Factoscope vary when applied to real-time, dynamic data sources versus static datasets?
- Basis in paper: [inferred] The paper evaluates the model using static datasets from Kaggle, but real-time applications might introduce variability.
- Why unresolved: The study does not explore the performance of LLM Factoscope in dynamic, real-world scenarios where data is constantly changing.
- What evidence would resolve it: Testing LLM Factoscope on live data streams and comparing its performance with static datasets to assess adaptability and accuracy in real-time environments.

### Open Question 2
- Question: What are the computational trade-offs when scaling LLM Factoscope to handle multiple languages or dialects simultaneously?
- Basis in paper: [inferred] The paper focuses on English datasets and does not address multilingual capabilities or the computational costs associated with them.
- Why unresolved: The impact of multilingual support on computational efficiency and accuracy is not explored, leaving uncertainty about its feasibility.
- What evidence would resolve it: Conducting experiments to measure performance and resource usage when LLM Factoscope is extended to multiple languages, identifying bottlenecks and potential optimizations.

### Open Question 3
- Question: How does the integration of external knowledge bases affect the accuracy and reliability of LLM Factoscope in factual detection?
- Basis in paper: [explicit] The paper mentions that current approaches rely on external databases, but it does not explore integrating such resources into LLM Factoscope.
- Why unresolved: The potential benefits or drawbacks of incorporating external knowledge bases into the model's architecture are not investigated.
- What evidence would resolve it: Implementing a hybrid model that combines LLM Factoscope's inner states analysis with external knowledge verification, then comparing its performance to the standalone model.

## Limitations
- The specific architectural details of the four sub-models (particularly ResNet18 and GRU configurations) are not fully specified, making exact reproduction challenging
- The evaluation focuses on the same domains used in training without testing the model's ability to detect factual errors in novel contexts or domains
- The preprocessing pipeline for converting raw inner states into model inputs requires additional clarification, especially for normalization and transformation steps

## Confidence
- **High Confidence** in the core hypothesis that LLMs exhibit distinguishable inner state patterns for factual vs. non-factual content, supported by the reported >96% accuracy across six different architectures
- **Medium Confidence** in the effectiveness of the multi-modal Siamese network approach, as ablation studies show individual sub-model contributions but don't fully characterize redundancy or complementarity
- **Low Confidence** in the generalizability claims without additional validation on truly out-of-distribution data, as the evaluation focuses on the same domains used in training

## Next Checks
1. **Inner State Pattern Verification**: Generate activation maps for a controlled set of factual and non-factual prompts across multiple LLMs, then apply t-SNE or UMAP visualization to confirm that the two classes form distinguishable clusters in the activation space.

2. **Architecture Generalization Test**: Evaluate Factoscope on a held-out LLM architecture (e.g., GPT-3 or Claude) that wasn't part of the training/evaluation set to assess true cross-architecture generalization capabilities.

3. **Minimal Inner State Analysis**: Systematically test which single inner state type achieves the highest accuracy independently, then gradually add other types to quantify the marginal benefit of each additional modality.