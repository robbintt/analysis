---
ver: rpa2
title: 'Causal Reasoning and Large Language Models: Opening a New Frontier for Causality'
arxiv_id: '2305.00050'
source_url: https://arxiv.org/abs/2305.00050
tags:
- causal
- llms
- answer
- have
- would
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can perform knowledge-based causal discovery
  and achieve high accuracy on pairwise causal tasks, outperforming traditional covariance-based
  methods. GPT-4 achieves 97% accuracy on a pairwise causal benchmark and 96% on a
  medical pain diagnosis task.
---

# Causal Reasoning and Large Language Models: Opening a New Frontier for Causality

## Quick Facts
- arXiv ID: 2305.00050
- Source URL: https://arxiv.org/abs/2305.00050
- Reference count: 40
- Primary result: GPT-4 achieves 97% accuracy on pairwise causal discovery and 92% on counterfactual reasoning benchmarks

## Executive Summary
Large language models (LLMs) like GPT-4 demonstrate remarkable capabilities in causal reasoning tasks, outperforming traditional methods on benchmarks for causal discovery, counterfactual reasoning, and necessary/sufficient cause identification. These models leverage their vast training data to capture domain knowledge and reason about causal relationships using natural language prompts rather than raw data analysis. While showing impressive accuracy rates (up to 97% on certain tasks), LLMs exhibit unpredictable failure modes and their performance depends heavily on the relevance of training data to specific domains. The research opens new frontiers for causality by enabling natural language interfaces to causal reasoning, though human oversight remains essential for high-stakes applications.

## Method Summary
The study evaluates LLMs (GPT-3.5 and GPT-4) on multiple causal reasoning tasks using carefully designed prompts and established benchmarks. For causal discovery, the researchers use the Tübingen cause-effect pairs dataset and apply LLMs to determine causal direction between variable pairs. For counterfactual reasoning, they use the CRASS benchmark testing physical laws, logic, and common sense scenarios. The study also evaluates necessary/sufficient cause identification using vignettes from the Actual Causality dataset. LLMs are prompted with variable names, context descriptions, and specific questions, with responses evaluated against ground truth labels. The research compares LLM performance to traditional covariance-based methods and previous benchmarks.

## Key Results
- GPT-4 achieves 97% accuracy on pairwise causal discovery, substantially outperforming traditional covariance-based methods
- LLMs achieve 92% accuracy on counterfactual reasoning benchmarks spanning physics, logic, and common sense
- GPT-4 correctly identifies necessary and sufficient causes with 86% accuracy on standard vignettes
- LLMs excel at knowledge-based causal discovery by reasoning over metadata rather than raw data values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs achieve high accuracy in causal discovery by leveraging memorized causal relationships and reasoning over metadata (variable names) rather than raw data values.
- Mechanism: LLMs trained on vast text corpora have internalized common causal patterns and relationships. When given variable names and context, they retrieve and apply this knowledge to infer likely causal directions without analyzing the underlying data distributions.
- Core assumption: The training data contains sufficient causal relationships across domains that the LLM can generalize from, and variable names provide enough semantic context for correct inference.
- Evidence anchors:
  - [abstract] "LLMs can infer causal structure by reasoning on metadata associated with the variables, for example, the name of the variable and the problem context expressed in natural language"
  - [section 3.1] "With GPT-4, the weighted accuracy shoots up to 97%. This accuracy is substantially higher than that of the best-known covariance-based causal discovery method"
  - [corpus] Weak evidence - corpus only shows related work on LLMs and causality but doesn't directly support the specific mechanism of metadata-based reasoning
- Break condition: If variable names are ambiguous, domain-specific knowledge is missing from training data, or the causal relationship contradicts common patterns learned by the LLM.

### Mechanism 2
- Claim: LLMs excel at counterfactual reasoning by simulating alternative scenarios based on their understanding of physical laws, common sense, and domain knowledge encoded in their training data.
- Mechanism: LLMs can parse premise statements, understand the context, and generate plausible alternative outcomes by applying their learned knowledge of how the world works. This allows them to answer "what if" questions with high accuracy.
- Core assumption: The training data contains sufficient examples of cause-and-effect relationships and counterfactual scenarios that the LLM can learn to generalize from.
- Evidence anchors:
  - [abstract] "LLMs like GPT-4 are capable of answering natural language questions...On a benchmark of counterfactual queries spanning basic physics, logic, and common sense, gpt-4 obtains 92% accuracy"
  - [section 4.1] "GPT-4 improves it further to 92.44%, which is 20 percentage points higher than the previous best accuracy"
  - [corpus] Moderate evidence - corpus shows related work on counterfactual reasoning but doesn't directly validate the specific mechanism
- Break condition: If the counterfactual scenario is highly unusual, requires specialized domain knowledge not in training data, or the premise contains ambiguous language.

### Mechanism 3
- Claim: LLMs can identify necessary and sufficient causes by applying background knowledge and reasoning about counterfactual dependencies in natural language scenarios.
- Mechanism: LLMs parse event descriptions, identify relevant causal events, and reason about whether those events are necessary (the outcome wouldn't occur without them) or sufficient (the outcome would occur if they happened alone) based on their understanding of causal mechanisms.
- Core assumption: The LLM has learned sufficient background knowledge about common causal mechanisms to reason about necessity and sufficiency, and can parse complex natural language descriptions accurately.
- Evidence anchors:
  - [abstract] "LLMs...are capable of answering natural language questions...These counterfactual capabilities also help LLMs to isolate the necessary and sufficient causes given any event"
  - [section 4.2] "Using this prompt, we evaluate all 15 example scenarios on two LLMs: gpt-3.5-turbo and GPT-4. For example, on this vignette, both LLMs correctly answer that Alice is not a necessary cause"
  - [corpus] Weak evidence - corpus shows related work on actual causality but doesn't directly validate the specific mechanism
- Break condition: If the vignette requires highly specialized knowledge, involves complex interactions between multiple causes, or the natural language description is ambiguous.

## Foundational Learning

- Concept: Causal discovery vs. effect inference
  - Why needed here: Understanding the distinction between discovering causal structures (graphs) and estimating the strength of specific causal relationships is crucial for interpreting LLM performance
  - Quick check question: If an LLM correctly identifies that A causes B, does this mean it can also tell you how much A affects B?

- Concept: Counterfactual reasoning fundamentals
  - Why needed here: LLMs' ability to answer counterfactual questions is a key mechanism for their causal capabilities, so understanding what counterfactual reasoning means is essential
  - Quick check question: What is the difference between asking "What would happen if X occurred?" versus "What would have happened if X had occurred?"

- Concept: Necessary vs. sufficient causes
  - Why needed here: These are fundamental concepts in actual causality that LLMs must reason about, so understanding the distinction is crucial for interpreting their performance
  - Quick check question: Can an event be both necessary and sufficient for an outcome? Give an example.

## Architecture Onboarding

- Component map: Prompt → LLM → Parse output → Validate against ground truth/benchmark → Analyze failure modes
- Critical path: Prompt → LLM → Parse output → Validate against ground truth/benchmark → Analyze failure modes
- Design tradeoffs: High accuracy vs. unpredictability, knowledge-based vs. data-based approaches, general vs. domain-specific performance
- Failure signatures: Inconsistent answers to similar questions, correct reasoning with wrong final answers, failure on specialized knowledge domains
- First 3 experiments:
  1. Reproduce pairwise causal discovery results on Tübingen dataset with different prompt variations
  2. Test counterfactual reasoning on CRASS benchmark with systematic perturbation of premises
  3. Evaluate necessary/sufficient cause identification on standard vignettes with and without additional context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can large language models (LLMs) be effectively integrated into existing causal discovery algorithms to improve their accuracy on real-world datasets?
- Basis in paper: [explicit] The paper discusses the potential of using LLMs as a prior or critic during learning in causal discovery algorithms.
- Why unresolved: The paper does not provide a concrete method or framework for integrating LLMs into causal discovery algorithms.
- What evidence would resolve it: A proposed framework or method for integrating LLMs into causal discovery algorithms, along with experimental results demonstrating improved accuracy.

### Open Question 2
- Question: Can LLMs be used to suggest potential instrumental variables for causal effect inference, given metadata about observed variables?
- Basis in paper: [explicit] The paper mentions the potential of using LLMs to suggest potential instrumental variables for a causal task.
- Why unresolved: The paper does not provide any experimental results or evidence to support this claim.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of LLMs in suggesting instrumental variables for causal effect inference.

### Open Question 3
- Question: How can LLMs be used to build robustness and validation checks for a given causal analysis, including suggesting appropriate variables to use for negative controls?
- Basis in paper: [explicit] The paper discusses the potential of using LLMs to suggest robustness checks and variables for negative controls in causal analysis.
- Why unresolved: The paper does not provide a concrete method or framework for using LLMs in this context.
- What evidence would resolve it: A proposed method or framework for using LLMs to suggest robustness checks and variables for negative controls, along with experimental results demonstrating improved validation of causal analyses.

## Limitations

- LLM performance heavily depends on whether relevant causal relationships exist in their training data, creating unpredictable blind spots in specialized domains
- LLMs exhibit inconsistent reasoning patterns and can produce high-accuracy answers while exhibiting incoherent or contradictory outputs
- The study doesn't definitively resolve whether LLM performance stems from genuine causal reasoning or memorization of training data patterns

## Confidence

- High Confidence: The observation that GPT-4 achieves 97% accuracy on pairwise causal discovery tasks, as this is directly measurable and benchmarked against established methods.
- Medium Confidence: The claim that LLMs can perform knowledge-based causal discovery, as this mechanism is plausible but not definitively proven to be the primary driver of observed performance.
- Low Confidence: The assertion that LLMs can reliably replace traditional causal discovery methods, given their unpredictable failure modes and dependency on training data coverage.

## Next Checks

1. **Controlled Memorization Test**: Systematically evaluate LLM performance on datasets with varying degrees of similarity to likely training data to quantify the memorization effect.

2. **Cross-Domain Transfer Validation**: Test LLM causal reasoning capabilities on domains with minimal overlap to their training data to assess genuine generalization ability.

3. **Reasoning Coherence Analysis**: Implement automated analysis of LLM reasoning chains to quantify the frequency and nature of incoherent or contradictory outputs across multiple runs.