---
ver: rpa2
title: 'Divorce Prediction with Machine Learning: Insights and LIME Interpretability'
arxiv_id: '2310.08620'
source_url: https://arxiv.org/abs/2310.08620
tags:
- divorce
- wife
- dataset
- have
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses divorce prediction using machine learning
  on a dataset of 170 couples with 54 attributes. Six algorithms were evaluated: Logistic
  Regression, LDA, KNN, CART, Naive Bayes, and SVM.'
---

# Divorce Prediction with Machine Learning: Insights and LIME Interpretability

## Quick Facts
- arXiv ID: 2310.08620
- Source URL: https://arxiv.org/abs/2310.08620
- Reference count: 40
- Primary result: SVM achieved 98.57% accuracy on divorce prediction with LIME interpretability

## Executive Summary
This study applies machine learning to predict divorce likelihood using a dataset of 170 couples with 54 psychological attributes. Six algorithms were evaluated, with SVM achieving the highest accuracy of 98.57%. The authors introduce LIME interpretability analysis to identify the top 10 features influencing predictions and develop a GUI-based application for individual use. While results are promising, the balanced dataset may inflate accuracy metrics, and the small sample size raises concerns about generalizability.

## Method Summary
The study uses the Divorce Predictor Dataset from UCI ML Repository (170 instances, 54 integer attributes, 2 classes). An 80/20 train-test split was applied with 10-fold cross-validation. Six algorithms were trained: Logistic Regression, LDA, KNN, CART, Naive Bayes, and SVM. LIME was used for feature interpretability, and a GUI application was developed. The dataset is balanced (49% divorced, 51% married), which may affect accuracy metrics.

## Key Results
- SVM achieved the highest accuracy at 98.57%, followed by KNN and LDA
- LIME interpretability identified the top 10 predictive features from 54 attributes
- A GUI-based application was developed for individual divorce prediction use
- The balanced dataset may inflate accuracy metrics and increase overfitting risk

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SVM with RBF kernel achieves 98.57% accuracy because the dataset is perfectly linearly separable in the transformed feature space.
- Mechanism: SVM finds the maximum-margin hyperplane in a high-dimensional kernel space, which separates the classes with minimal misclassification.
- Core assumption: The 54 psychological attributes create a feature space where divorce vs. married couples are linearly separable after kernel transformation.
- Evidence anchors:
  - [abstract] "Preliminary computational results show that algorithms such as SVM, KNN, and LDA, can perform that task with an accuracy of 98.57%"
  - [section] "SVM outperformed all algorithms in terms of higher accuracy and lower error rate"
- Break condition: If the data contains overlapping class distributions or noise, the margin maximization fails and accuracy drops sharply.

### Mechanism 2
- Claim: LIME explanations are reliable because the model is highly accurate, so local linear approximations closely match the true decision boundary.
- Mechanism: LIME perturbs the input and fits a linear model locally; high global accuracy means local neighborhoods are also well-separated.
- Core assumption: A globally accurate model implies accurate local explanations for individual predictions.
- Evidence anchors:
  - [abstract] "This work's additional novel contribution is the detailed and comprehensive explanation of prediction probabilities using Local Interpretable Model-Agnostic Explanations (LIME)"
- Break condition: If the model overfits (due to small, balanced dataset), local explanations may be misleading despite high accuracy.

### Mechanism 3
- Claim: Balanced dataset (49% divorced, 51% married) inflates accuracy metrics because both classes are equally represented.
- Mechanism: Classification accuracy is maximized when the model correctly predicts the majority class, and here both classes are nearly equal.
- Core assumption: Accuracy alone is insufficient for imbalanced or balanced binary classification without precision/recall.
- Evidence anchors:
  - [abstract] "However, the dataset is highly balanced, potentially inflating accuracy"
- Break condition: If applied to imbalanced real-world data, the same model would show much lower accuracy and poor recall for the minority class.

## Foundational Learning

- Concept: Kernel trick in SVMs
  - Why needed here: The paper achieves 98.57% accuracy using SVM, implying effective kernel transformation.
  - Quick check question: What kernel (linear, polynomial, RBF) would you choose if the classes overlap in the original feature space?

- Concept: Local Interpretable Model-Agnostic Explanations (LIME)
  - Why needed here: The study uses LIME to identify the top 10 predictive features from 54 attributes.
  - Quick check question: How does LIME ensure that its local linear model is faithful to the black-box model's predictions?

- Concept: Feature selection bias in small datasets
  - Why needed here: The study selects top 10 features from 54, but the dataset only has 170 instances.
  - Quick check question: What statistical test would you use to confirm that the selected features are not artifacts of overfitting?

## Architecture Onboarding

- Component map: Data ingestion → preprocessing (scaling, train-test split) → model training (SVM, KNN, LDA, etc.) → evaluation (accuracy, precision, recall) → LIME interpretation → GUI deployment
- Critical path: Load dataset → 80/20 split → train SVM → evaluate → apply LIME to test set → extract top features → build GUI
- Design tradeoffs:
  - High accuracy vs. generalizability: 98.57% on 170 samples may not generalize
  - Interpretability vs. complexity: LIME adds explainability but assumes local linearity
  - GUI simplicity vs. feature richness: Only top 10 features are exposed to users
- Failure signatures:
  - Accuracy drops dramatically on new data (overfitting)
  - LIME explanations contradict domain knowledge (local linearity assumption fails)
  - GUI crashes when input values are outside training range
- First 3 experiments:
  1. Train SVM on 80% of data, test on 20%, report confusion matrix and F1-score
  2. Apply LIME to 5 test instances, compare top features with domain expert expectations
  3. Build minimal GUI with 3 input fields (top features), test end-to-end prediction flow

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the model's performance change when applied to a dataset with a different class balance (e.g., 70% married, 30% divorced)?
- Basis in paper: [explicit] The authors note that the dataset used is highly balanced (49% divorced, 51% married), which they acknowledge may inflate accuracy and increase overfitting risk.
- Why unresolved: The study does not test the model on imbalanced datasets or explore how class imbalance affects accuracy, precision, recall, or F1-score.
- What evidence would resolve it: Retraining and evaluating the model on datasets with varying class distributions (e.g., 70/30, 60/40) to compare performance metrics and assess robustness to imbalance.

### Open Question 2
- Question: Which specific features from the top 10 identified by LIME and SelectKBest are most critical for divorce prediction, and how do they interact?
- Basis in paper: [explicit] The authors use LIME and SelectKBest to identify the top 10 features but do not provide a detailed analysis of their individual or combined impact on predictions.
- Why unresolved: The study does not explore feature interactions, relative importance, or how changes in specific features influence prediction outcomes.
- What evidence would resolve it: Conducting feature ablation studies or SHAP (SHapley Additive exPlanations) analysis to quantify the contribution of each feature and their interactions.

### Open Question 3
- Question: How would the inclusion of additional demographic or contextual variables (e.g., socioeconomic status, duration of marriage, or cultural background) affect the model's accuracy?
- Basis in paper: [inferred] The current dataset includes 54 attributes based on the Gottman couples therapy scale but lacks broader demographic or contextual factors that could influence divorce outcomes.
- Why unresolved: The study does not explore the impact of adding external variables beyond the Gottman-based attributes.
- What evidence would resolve it: Collecting and incorporating additional demographic or contextual data into the dataset and retraining the model to evaluate changes in performance metrics.

## Limitations
- Small dataset size (170 instances) with 54 features raises overfitting concerns
- Balanced dataset (49% divorced, 51% married) may artificially inflate accuracy metrics
- Limited generalizability to real-world divorce prediction scenarios with different class distributions

## Confidence
- High confidence: Experimental methodology (80/20 split, 10-fold cross-validation) is properly specified and reproducible
- Medium confidence: LIME interpretability analysis is valid but may be unreliable given small sample size
- Low confidence: Generalization to real-world divorce prediction scenarios given controlled dataset characteristics

## Next Checks
1. Test model performance on an independent, real-world divorce dataset with different class distributions to assess true generalization capability
2. Apply cross-validation with different train-test splits (e.g., 70/30, 60/40) to verify stability of the 98.57% accuracy claim
3. Compare LIME feature importance rankings against statistical feature selection methods (e.g., SelectKBest) to validate interpretability results