---
ver: rpa2
title: Estimating Deep Learning energy consumption based on model architecture and
  training environment
arxiv_id: '2307.05520'
source_url: https://arxiv.org/abs/2307.05520
tags:
- energy
- training
- uni00000013
- consumption
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes how deep learning model architecture and training
  environment affect energy consumption in computer vision systems. The researchers
  trained five computer vision models across three different environments and collected
  energy consumption and accuracy metrics.
---

# Estimating Deep Learning energy consumption based on model architecture and training environment

## Quick Facts
- **arXiv ID**: 2307.05520
- **Source URL**: https://arxiv.org/abs/2307.05520
- **Reference count**: 25
- **Primary result**: Model architecture and training environment selection can reduce DL training energy consumption by up to 80.68% with minimal accuracy loss.

## Executive Summary
This study investigates how deep learning model architecture and training environment impact energy consumption in computer vision systems. Through systematic experimentation with five computer vision models across three different environments, the researchers demonstrate that energy consumption is not solely determined by model complexity (FLOPs). They reveal that GPU efficiency scales with model complexity and that training environment selection can significantly reduce energy consumption while maintaining model accuracy. The study proposes two novel estimation methods - Stable Training Epoch Projection (STEP) and Pre-training Regression-based Estimation (PRE) - that outperform existing tools by a factor of two or more in accuracy.

## Method Summary
The researchers trained five computer vision models (MobileNet V2, NASNet Mobile, Xception, ResNet50, VGG16) across three environments: two local setups (GTX 750 Ti-2GB, RTX 3070 8GB) and one cloud environment (RTX 3090-24GB). Using transfer learning on a binary classification dataset of 34,010 50×50 color images, they executed 30 training runs per configuration with 5-minute pauses. Energy consumption was measured using nvidia-smi, normalized by images processed, and analyzed using non-parametric statistical tests. The study evaluated model accuracy (F1-score), FLOPs, GPU usage, and proposed novel estimation methods for predicting energy consumption.

## Key Results
- Selecting optimal model-training environment combinations can reduce energy consumption by up to 80.68% with less than 2% loss in F1 score
- FLOPs and GPU TDP are poor estimators of energy consumption due to significant variation across different training environments
- GPU efficiency scales with model complexity, with more complex models showing better energy efficiency on more powerful GPUs
- The proposed STEP and PRE estimation methods outperform existing tools by 2x or more in prediction accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Energy consumption of deep learning models is not determined solely by model complexity (FLOPs).
- Mechanism: GPU efficiency and training environment have significant interaction effects on energy consumption. Models with similar FLOPs can have drastically different energy profiles depending on hardware utilization patterns.
- Core assumption: Energy consumption during training is influenced by factors beyond computational complexity, including hardware utilization patterns and training environment characteristics.
- Evidence anchors: The study demonstrates that FLOPs might be a better estimator for GPU usage rather than energy consumption, with huge variations depending on the environment.
- Break condition: If GPU utilization becomes consistent across different model architectures and training environments.

### Mechanism 2
- Claim: GPU efficiency scales with model complexity, creating an optimal pairing between hardware and model architecture.
- Mechanism: Energy efficiency improves when GPU computational power scales with model complexity. Mismatched pairings (overly powerful GPUs for simple models or underpowered GPUs for complex models) lead to suboptimal energy efficiency.
- Core assumption: There exists an optimal relationship between model computational requirements and GPU capabilities that maximizes energy efficiency.
- Evidence anchors: The research shows that energy efficiency improves when GPU computational power scales with model complexity, and selecting energy-efficient architectures can significantly reduce DL training energy consumption.
- Break condition: If model architectures become more uniform in their computational patterns, or if GPU architectures become more adaptive to different computational loads.

### Mechanism 3
- Claim: Training environment selection can reduce energy consumption by up to 80.68% while maintaining model accuracy.
- Mechanism: Different training environments (local vs cloud) have distinct energy consumption profiles. Cloud environments with more powerful GPUs may have lower GPU utilization, while local environments may have higher utilization but different power efficiency characteristics.
- Core assumption: Training environment characteristics (hardware specifications, utilization patterns, power efficiency) significantly impact overall energy consumption beyond just the model architecture itself.
- Evidence anchors: The study finds that selecting the right model-training environment combination can reduce training energy consumption by up to 80.68% with less than 2% loss in F1 score.
- Break condition: If cloud providers standardize GPU utilization patterns or if local hardware becomes more power-efficient across all configurations.

## Foundational Learning

- Concept: Energy measurement and profiling techniques
  - Why needed here: Understanding how to accurately measure energy consumption during training is crucial for comparing different models and environments
  - Quick check question: What tools and methods are used to measure GPU energy consumption during deep learning training?

- Concept: Transfer learning and model architecture fundamentals
  - Why needed here: The study uses transfer learning with five different base models, requiring understanding of how architectural choices affect both performance and energy consumption
  - Quick check question: How does transfer learning affect the energy consumption of training compared to training from scratch?

- Concept: Statistical analysis of non-normal data
  - Why needed here: The study uses non-parametric tests due to non-normal distribution of measurements, requiring understanding of appropriate statistical methods
  - Quick check question: When should non-parametric tests be used instead of parametric tests in experimental analysis?

## Architecture Onboarding

- Component map: Model architecture selection → Training environment selection → Energy measurement setup → Training execution → Statistical analysis → Results interpretation

- Critical path: Model selection → Environment selection → Training execution → Energy measurement → Statistical analysis → Results interpretation

- Design tradeoffs: Balancing model complexity with energy efficiency requires considering both computational requirements and hardware capabilities. Local environments offer better utilization but may have hardware limitations, while cloud environments provide more powerful hardware but potentially lower utilization.

- Failure signatures: Unexpected energy consumption patterns may indicate hardware issues, software conflicts, or measurement errors. Significant deviations from expected GPU utilization patterns suggest problems with the training configuration.

- First 3 experiments:
  1. Run baseline measurements with a single model architecture across all three environments to establish energy consumption patterns
  2. Test different model architectures on the same environment to isolate architecture effects
  3. Vary only the training environment while keeping model architecture constant to understand environment impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the relationship between GPU computational power and model complexity be quantified to optimize energy efficiency in DL training?
- Basis in paper: The paper states that "energy efficiency improves when GPU computational power scales with model complexity" and suggests that the training environment should factor in the selection of model architecture.
- Why unresolved: The paper identifies the importance of this relationship but does not provide a concrete method for quantifying it.
- What evidence would resolve it: Development and validation of a predictive model or tool that can estimate the optimal GPU requirements for efficiently training a given DL model with respect to energy consumption.

### Open Question 2
- Question: Can FLOPs be used as a reliable estimator for energy consumption in DL models across different training environments?
- Basis in paper: The paper finds that "FLOPs might be a better estimator for GPU usage rather than energy consumption" and that "there are huge variations depending on the environment" when considering FLOPs as an estimator for energy consumption.
- Why unresolved: The paper demonstrates that FLOPs are not a good estimator for energy consumption due to variability in training environments, but it does not propose an alternative method or model that could serve as a reliable estimator.
- What evidence would resolve it: Research that develops and validates an alternative metric or set of metrics that can more accurately predict energy consumption across various training environments.

### Open Question 3
- Question: What is the optimal strategy for selecting between Cloud and Local environments for training DL models to minimize energy consumption and carbon footprint?
- Basis in paper: The paper discusses the trade-offs between Cloud and Local environments, noting that "Local environments can help to reduce [the overall carbon footprint] given their lower energy consumption" but also acknowledging the embodied footprint of GPUs and the benefits of Cloud environments for training complex models.
- Why unresolved: The paper provides insights into the energy consumption differences between Cloud and Local environments but does not offer a clear decision-making framework for selecting the optimal environment based on model complexity, usage patterns, and carbon footprint considerations.
- What evidence would resolve it: A comprehensive study that includes a decision-making framework for selecting training environments, taking into account energy consumption, carbon footprint, cost, and the specific needs of the DL model being trained.

## Limitations

- Study focuses exclusively on binary classification tasks in computer vision using a specific dataset composition, limiting applicability to multi-class problems or different domains
- The three GPU configurations tested represent a narrow range of hardware options, and results may differ significantly with other GPU architectures
- Energy measurement methodology relies on nvidia-smi, which captures GPU-specific consumption but may not account for total system power draw including CPU, memory, and cooling overhead

## Confidence

**High confidence** in findings regarding GPU efficiency scaling with model complexity and the effectiveness of the proposed STEP and PRE estimation methods, supported by extensive statistical analysis and consistent patterns across multiple runs.

**Medium confidence** in environment selection recommendations due to the limited hardware configurations tested and potential variations in cloud provider energy efficiency across different regions and instance types.

**Low confidence** in the generalizability of FLOPs-based estimation failures to all deep learning tasks, as the study focuses specifically on transfer learning for computer vision with relatively small image sizes.

## Next Checks

1. **Dataset generalization test**: Validate findings across multiple computer vision datasets with varying complexity (from simple binary to complex multi-class problems) to assess whether energy consumption patterns hold across different classification tasks.

2. **Hardware diversity validation**: Test the proposed estimation methods and efficiency relationships across a broader range of GPU architectures (including AMD GPUs and different NVIDIA generations) and CPU-based training to verify hardware independence.

3. **Domain transfer assessment**: Apply the same experimental framework to non-computer vision tasks (natural language processing, reinforcement learning) to determine if the identified relationships between model complexity, GPU efficiency, and energy consumption extend beyond the studied domain.