---
ver: rpa2
title: Robust Learning via Conditional Prevalence Adjustment
arxiv_id: '2310.15766'
source_url: https://arxiv.org/abs/2310.15766
tags:
- sites
- data
- site
- copa
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoPA is a method for learning domain-invariant representations
  in anti-causal learning problems, where label Y and confounding variables Z cause
  input X. The approach assumes stable data generation (P(X|Y,Z)) and observable confounders,
  leveraging conditional prevalence estimates P(Y|Z,E) at each site to adjust for
  unstable correlations.
---

# Robust Learning via Conditional Prevalence Adjustment

## Quick Facts
- arXiv ID: 2310.15766
- Source URL: https://arxiv.org/abs/2310.15766
- Reference count: 40
- Key outcome: CoPA achieves 0.84-0.87 F1-scores on ISIC test sites vs 0.75-0.81 for baselines

## Executive Summary
CoPA is a domain generalization method that addresses anti-causal learning problems where label Y and confounding variables Z cause input X. Unlike traditional approaches requiring multiple training sites, CoPA works with a single site by leveraging conditional prevalence estimates P(Y|Z,E) to adjust for unstable correlations between Y and Z across sites. The method assumes stable data generation P(X|Y,Z) and observable confounders, making it particularly effective in healthcare settings where such variables are routinely recorded.

## Method Summary
CoPA learns domain-invariant representations by combining input features X with confounding variables Z through a backbone network, producing a domain-invariant ratio that is adjusted by site-specific prevalence estimates. The model architecture concatenates X representations with Z features, applies a linear layer to compute the ratio, and multiplies by prevalence estimates to produce final predictions. Unlike methods like IRM or DANN that require multiple training domains, CoPA can work with single-site training while maintaining strong out-of-domain generalization performance.

## Key Results
- On ISIC skin cancer data, CoPA achieved F1-scores of 0.84-0.87 at test sites compared to baseline scores of 0.75-0.81
- Works with single training site, unlike methods requiring multiple domains
- Robust to label-shift and handles both spurious and causal relationships between Y and Z

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prevalence adjustment enables stable prediction by absorbing site-specific instability through conditional prevalence estimates P(Y|Z,E).
- Mechanism: The model learns a stable ratio R(X,Z) that captures stable associations between X and Y given Z. By multiplying this ratio with site-specific prevalence estimates bP(Y|Z,E), the prediction becomes invariant to site-specific changes in correlations.
- Core assumption: The generation mechanism P(X|Y,Z) is stable across sites, meaning X ⊥⊥E|Y,Z.
- Evidence anchors:
  - [abstract] "CoPA assumes that (1) generation mechanism is stable, i.e. label Y and confounding variable(s) Z generate X, and (2) the unstable conditional prevalence in each site E fully accounts for the unstable correlations between X and Y"
  - [section] "Since P (X|Y, Z) is assumed to be stable (i.e. invariant across sites), X ⊥ ⊥E|Y, Z"
  - [corpus] Weak - no direct evidence found
- Break condition: If the generation mechanism P(X|Y,Z) varies between sites, the core assumption fails and the method breaks down.

### Mechanism 2
- Claim: CoPA is robust to label-shift because prevalence adjustment absorbs changes in prior P(Y|E).
- Mechanism: The conditional prevalence estimate bP(Y|Z,E) captures both spurious correlations and label-shift effects. By adjusting predictions with these estimates, the model remains stable even when P(Y|E) varies between sites.
- Core assumption: Confounding variables Z are routinely recorded and observable at training and test sites.
- Evidence anchors:
  - [abstract] "Our crucial observation is that confounding variables are routinely recorded in healthcare settings and the prevalence can be readily estimated"
  - [section] "CoPA has several advantages over baselines. Since the conditional prevalence estimate absorbs the effect of label-shift, CoPA is less susceptible to this change which is quite common in healthcare data"
  - [corpus] Weak - no direct evidence found
- Break condition: If confounding variables Z are not observable at test sites, prevalence estimates cannot be obtained and the method fails.

### Mechanism 3
- Claim: CoPA works regardless of causal relationship between Y and Z (spurious or causal).
- Mechanism: The prevalence adjustment procedure does not assume any specific causal ordering between Y and Z. Whether Y causes Z, Z causes Y, or they share a common cause, the adjustment absorbs all site-specific instability through bP(Y|Z,E).
- Core assumption: The conditional prevalence P(Y|Z,E) fully captures all unstable correlations between X and Y.
- Evidence anchors:
  - [abstract] "CoPA can deal with not only spurious correlations (Figure 1, E1 to E4) but also changing causal correlations (Figure 1, E5 and E6) because the prevalence-adjustment procedure of CoPA does not assume any specific causal ordering between Y and Z"
  - [section] "Equation (4) implies that R(X, Z) is invariant across sites and all the site-specific instability can be absorbed by the conditional prevalence P (Y |Z, ej)"
  - [corpus] Weak - no direct evidence found
- Break condition: If there are unstable correlations between X and Y that are not captured by P(Y|Z,E), the method cannot adjust for them.

## Foundational Learning

- Concept: Causal data generation assumptions
  - Why needed here: CoPA relies on understanding whether X is caused by Y and Z (anti-causal) versus Y being caused by X (causal). This determines whether invariant risk minimization or prevalence adjustment is appropriate.
  - Quick check question: Given the causal graph Y → X ← Z, which learning framework should be used - IRM or CoPA?

- Concept: Domain generalization vs domain adaptation
  - Why needed here: CoPA operates in a domain generalization setting where no test data is available during training, unlike domain adaptation which has access to target domain data.
  - Quick check question: What's the key difference between CoPA's approach and traditional domain adaptation methods like DANN?

- Concept: Prevalence estimation from (Y,Z) pairs
  - Why needed here: CoPA requires estimating bP(Y|Z,E) at each site using only (Y,Z) samples without needing corresponding X samples.
  - Quick check question: How can you estimate the prevalence of melanoma given age and anatomical site without having the actual skin images?

## Architecture Onboarding

- Component map: Backbone network (ResNet50 for real data, simple FC/CNN for synthetic) -> Z feature extraction and concatenation with X representation -> Linear layer producing domain-invariant ratio fθ(X,Z) -> Element-wise multiplication with prevalence estimate bP(Y|Z,E) -> Softmax activation producing final probability distribution

- Critical path: Backbone → Concatenation → Linear layer → Ratio computation → Prevalence adjustment → Prediction

- Design tradeoffs:
  - Late fusion (concatenation) vs early fusion for combining X and Z features
  - Simple prevalence estimation vs complex neural network for bP(Y|Z,E)
  - Single training site capability vs multiple site performance

- Failure signatures:
  - High variance in prevalence estimates indicates insufficient (Y,Z) samples
  - Poor performance when Z variables are not truly confounding
  - Degradation when generation mechanism P(X|Y,Z) varies between sites

- First 3 experiments:
  1. Train on synthetic 2-dim data with strong spurious correlation (β=0.9) and test with weak correlation (β=0.3) - verify CoPA outperforms ERM
  2. Train on single site synthetic data and test on held-out site - confirm CoPA works without multiple training sites
  3. Use real ISIC data with age as confounder - compare CoPA performance across sites with different age distributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CoPA's performance degrade when confounding variables Z are only partially observed or have measurement errors?
- Basis in paper: [explicit] The paper discusses that CoPA assumes observability of confounders Z at training and test sites, but notes that its ablation study shows "tolerable performance when Z is not observed" and calls for "more rigorous treatment of this case."
- Why unresolved: The ablation study only provides preliminary results using Monte Carlo approximations when Z is unobserved, but doesn't systematically quantify the performance degradation under realistic measurement errors or partial observability scenarios.
- What evidence would resolve it: A comprehensive study varying the completeness and accuracy of Z observations (e.g., missing values, noise levels, reduced dimensionality) with corresponding performance metrics across multiple real-world datasets.

### Open Question 2
- Question: What is the computational complexity of CoPA compared to baseline methods, particularly regarding the estimation of conditional prevalence P(Y|Z,E)?
- Basis in paper: [explicit] The paper mentions that CoPA requires training auxiliary models to estimate P(Y|Z,E) when Z is continuous or multi-dimensional, but doesn't provide computational complexity analysis or runtime comparisons with baselines.
- Why unresolved: While the method is described, no empirical or theoretical analysis of computational efficiency is provided, making it difficult to assess scalability to large datasets or real-time applications.
- What evidence would resolve it: Detailed runtime analysis showing training and inference times for CoPA versus baselines on datasets of varying sizes, including memory requirements for the prevalence estimation step.

### Open Question 3
- Question: How does CoPA handle scenarios where the stable data generation assumption P(X|Y,Z) is violated due to domain-specific transformations or preprocessing?
- Basis in paper: [inferred] The paper assumes stable P(X|Y,Z) across sites but doesn't address what happens when this assumption is violated through site-specific imaging protocols, preprocessing steps, or domain-specific augmentations that systematically alter the input space.
- Why unresolved: The theoretical framework relies on this stability assumption, but no experiments or analysis explore robustness to violations of this assumption, which is common in healthcare settings with different imaging devices and protocols.
- What evidence would resolve it: Experiments systematically introducing domain-specific transformations (e.g., different normalization, resolution changes, noise patterns) and measuring CoPA's performance degradation relative to baselines.

## Limitations

- The method's performance heavily depends on the accuracy of prevalence estimates bP(Y|Z,E), which requires sufficient (Y,Z) samples at test sites
- The assumption that P(X|Y,Z) is stable across sites may not hold in domains where data collection protocols vary significantly
- Limited empirical validation of CoPA's performance with single training site beyond synthetic data

## Confidence

- **High Confidence**: The core mechanism of prevalence adjustment for domain generalization, and the empirical results showing CoPA outperforming baselines on ISIC and CXR datasets.
- **Medium Confidence**: The claim that CoPA works regardless of causal relationship between Y and Z, and the robustness to label-shift, as these are supported by theoretical arguments but limited empirical validation.
- **Low Confidence**: The assertion that CoPA can work with a single training site, as the experiments only demonstrate this on synthetic data with specific correlation structures.

## Next Checks

1. **Prevalence Estimation Robustness**: Systematically vary the amount of (Y,Z) data available at test sites and measure the impact on CoPA's OOD performance to establish minimum sample requirements.
2. **Cross-Domain Transfer**: Test CoPA on non-medical datasets (e.g., natural images with domain shifts) to evaluate generalizability beyond healthcare settings where confounders are routinely recorded.
3. **Mechanism Stability Verification**: Design experiments where the generation mechanism P(X|Y,Z) is deliberately varied between sites to quantify the impact on CoPA's performance and validate the core assumption's importance.