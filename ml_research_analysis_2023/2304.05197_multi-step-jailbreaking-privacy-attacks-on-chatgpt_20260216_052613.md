---
ver: rpa2
title: Multi-step Jailbreaking Privacy Attacks on ChatGPT
arxiv_id: '2304.05197'
source_url: https://arxiv.org/abs/2304.05197
tags:
- information
- chatgpt
- email
- data
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the privacy threats posed by large language
  models (LLMs) like ChatGPT and the New Bing. It shows that ChatGPT, despite enhanced
  safety measures, can leak private information through jailbreak and Chain-of-Thought
  (CoT) prompts.
---

# Multi-step Jailbreaking Privacy Attacks on ChatGPT

## Quick Facts
- arXiv ID: 2304.05197
- Source URL: https://arxiv.org/abs/2304.05197
- Authors: 
- Reference count: 11
- This paper studies privacy threats from LLMs like ChatGPT and the New Bing, showing they can leak private information through jailbreak and Chain-of-Thought prompts, with the New Bing achieving 94% accuracy in recovering institutional data.

## Executive Summary
This paper investigates privacy vulnerabilities in large language models, particularly ChatGPT and the New Bing search engine. The authors demonstrate that these systems can extract personal information through carefully crafted jailbreak prompts combined with Chain-of-Thought prompting, even when enhanced safety measures are in place. The study reveals that ChatGPT can recover 57.95% of frequent Enron emails and 4% of institutional emails, while the New Bing achieves 94% accuracy for institutional data. These findings highlight significant privacy risks including identity disclosure and potential misuse for malicious activities like spamming and doxing.

## Method Summary
The study employs jailbreak prompts combined with Chain-of-Thought (CoT) prompting to bypass ChatGPT's ethical safeguards and extract personal information. The methodology involves collecting email datasets (Enron and institutional pairs), constructing appropriate prompt templates, sending prompts to model APIs, parsing responses to extract emails, and verifying accuracy through multiple generations. The experiments test both ChatGPT's API and the New Bing web interface using direct prompts, jailbreak prompts, and jailbreak+CoT combinations.

## Key Results
- ChatGPT recovers 57.95% of frequent Enron emails and 4% of faculty emails using jailbreak+CoT prompts
- The New Bing achieves 94% accuracy in recovering institutional email data
- Jailbreak+CoT prompts are more effective than direct prompts alone at bypassing safety measures
- The New Bing's search integration enables extraction of personal information beyond training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT memorizes training data containing personal information, enabling extraction attacks.
- Mechanism: During training, ChatGPT encounters personal data from web sources. When prompted with specific names, the model retrieves memorized email addresses from its learned distribution, especially when moral constraints are bypassed via jailbreak prompts combined with Chain-of-Thought (CoT) prompting.
- Core assumption: Personal information exists in ChatGPT's training corpus and is retrievable through appropriately crafted prompts.
- Evidence anchors:
  - [abstract] "It shows that ChatGPT, despite enhanced safety measures, can leak private information through jailbreak and Chain-of-Thought (CoT) prompts."
  - [section 4.2.2] "ChatGPT memorizes certain private information. More than 50% frequent Enron emails and 4% faculty emails can be recovered via our proposed prompt."
  - [corpus] Weak evidence - only general claims about training data sources, no specific corpus analysis provided.
- Break condition: If safety measures or prompt filtering prevent the generation of personal information, or if the target data is not in the training corpus.

### Mechanism 2
- Claim: Jailbreak prompts combined with Chain-of-Thought (CoT) effectively bypass ChatGPT's moral safeguards.
- Mechanism: Jailbreak prompts set a "Developer Mode" role that allows unrestricted responses. CoT prompting further undermines ethical considerations by encouraging the model to "think step by step" and make guesses about personal information, even when it would normally refuse.
- Core assumption: ChatGPT's safety mechanisms can be circumvented through carefully constructed prompt combinations that alter the model's perceived role and reasoning process.
- Evidence anchors:
  - [section 4.2.2] "CoT effectively undermines the morality of ChatGPT. Both Table 1 and 2 verify that JPCoT can lead to more parsed emails and correct generations than JP."
  - [section 3.3.3] "Our proposed CoT prompt aims to relieve LLMs' ethical considerations and force LLMs to recover personal information."
  - [corpus] Moderate evidence - mentions jailbreak techniques from external sources but lacks direct corpus validation.
- Break condition: If the model's safety training becomes robust enough to detect and reject even complex jailbreak + CoT prompt combinations.

### Mechanism 3
- Claim: The New Bing's integration with search engine capabilities enables free-form personal information extraction beyond model memorization.
- Mechanism: The New Bing combines ChatGPT's generation ability with real-time web search. This allows it to retrieve personal information from online sources that may not be in its training data, using direct prompts or free-form extraction queries.
- Core assumption: The New Bing's search integration can access and synthesize personal information from external web sources that are not part of its training corpus.
- Evidence anchors:
  - [abstract] "The New Bing, an LLM-integrated search engine, is even more vulnerable, accurately recovering personal data via direct prompts or free-form extraction."
  - [section 4.3.3] "Table 4 shows the free-form extraction results. Unsurprisingly, most listed (name, email) pairs are correct with corresponding online sources."
  - [corpus] Weak evidence - claims about search integration are stated but not empirically validated through corpus analysis.
- Break condition: If the search integration is restricted, or if personal information sources are removed from accessible web pages.

## Foundational Learning

- Concept: Prompt Engineering and Jailbreak Techniques
  - Why needed here: Understanding how to craft prompts that bypass safety measures is essential for both conducting the attacks and defending against them.
  - Quick check question: What is the difference between a direct prompt and a jailbreak prompt in the context of extracting personal information from LLMs?

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: CoT prompting decomposes complex problems into intermediate steps, which in this case is used to undermine the model's ethical considerations and encourage generation of private information.
  - Quick check question: How does adding "Let's think step by step" to a prompt affect the model's response, particularly in the context of bypassing safety measures?

- Concept: Data Privacy and GDPR Compliance
  - Why needed here: Understanding privacy regulations like GDPR is crucial for assessing the legality and ethical implications of training models on personal data without consent.
  - Quick check question: Under GDPR, what are the requirements for processing personal data, and how might training an LLM on web-sourced data violate these requirements?

## Architecture Onboarding

- Component map:
  - OpenAI's ChatGPT API (gpt-3.5-turbo) with chat completion interface
  - New Bing web interface with integrated search and generation capabilities
  - Data collection from Enron Email Dataset and institutional email sources
  - Prompt templates (direct, jailbreak, jailbreak+CoT, verification methods)
  - Response parsing and verification scripts

- Critical path:
  1. Collect (name, email) pairs from public sources
  2. Construct appropriate prompts (direct, jailbreak, jailbreak+CoT)
  3. Send prompts to model APIs and collect responses
  4. Parse responses to extract email addresses
  5. Verify extracted information using multiple-choice or majority voting
  6. Analyze accuracy and Hit@5 metrics

- Design tradeoffs:
  - Direct prompts are simpler but ineffective against safety-enhanced models
  - Jailbreak prompts can bypass safety but may be inconsistent
  - Jailbreak+CoT prompts are more reliable but require more complex prompt engineering
  - Multiple generations improve accuracy but increase API costs and processing time

- Failure signatures:
  - Model refuses to generate personal information ("I'm sorry, but I cannot disclose personal information")
  - Generated responses contain no parsable email patterns
  - High variance in generated emails across multiple attempts
  - Search results return no relevant pages for the target domain

- First 3 experiments:
  1. Test direct prompt extraction on a small set of known (name, email) pairs to establish baseline performance
  2. Implement jailbreak prompt and test on the same dataset to measure improvement
  3. Add CoT component to jailbreak prompt and compare results against previous experiments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the safety measures of ChatGPT and other LLMs to prevent the extraction of private information through jailbreak and Chain-of-Thought prompts?
- Basis in paper: Explicit
- Why unresolved: The paper demonstrates that despite enhanced safety measures, ChatGPT can still leak private information through jailbreak and Chain-of-Thought prompts. The effectiveness of these prompts in bypassing ethical considerations suggests that current safety measures may not be sufficient to protect against such attacks.
- What evidence would resolve it: Developing and testing new safety measures that can effectively prevent the extraction of private information through jailbreak and Chain-of-Thought prompts, and evaluating their performance in comparison to existing measures.

### Open Question 2
- Question: How can we ensure the privacy of individuals whose data might be included in the training data of LLMs like ChatGPT?
- Basis in paper: Explicit
- Why unresolved: The paper raises concerns about the potential inclusion of personal data in the training data of LLMs and the associated privacy risks. It is unclear how to balance the need for large-scale training data with the protection of individual privacy rights.
- What evidence would resolve it: Investigating methods for identifying and removing personal data from training data, as well as developing guidelines for the ethical collection and use of data in LLM training.

### Open Question 3
- Question: How can we mitigate the privacy risks associated with the integration of LLMs and search engines, as exemplified by the New Bing?
- Basis in paper: Explicit
- Why unresolved: The paper highlights the potential for LLMs integrated with search engines to extract personal information beyond their training data, posing significant privacy risks. It is unclear how to balance the benefits of enhanced search capabilities with the need to protect user privacy.
- What evidence would resolve it: Developing and testing methods for limiting the exposure of personal information in search results generated by LLM-integrated search engines, as well as evaluating the effectiveness of these methods in practice.

### Open Question 4
- Question: How can we detect and prevent the use of LLMs for malicious purposes, such as spamming, spoofing, doxing, and cyberbullying?
- Basis in paper: Explicit
- Why unresolved: The paper discusses the potential for LLMs to be used for malicious purposes due to their ability to extract and generate personal information. It is unclear how to effectively detect and prevent such misuse of LLMs.
- What evidence would resolve it: Investigating methods for monitoring and controlling the use of LLMs, as well as developing guidelines for responsible AI development and deployment.

### Open Question 5
- Question: How can we ensure that individuals have the right to opt out of having their personal data included in the training data of LLMs?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that individuals may have the right to opt out of uninformed data collection under certain privacy laws, but it is unclear how to implement such a right in practice for LLM training data.
- What evidence would resolve it: Developing and testing methods for identifying and removing personal data from training data upon request, as well as creating guidelines for LLM developers to respect individuals' privacy preferences.

## Limitations

- The study's results may not generalize beyond the specific Enron and institutional email datasets used
- The effectiveness of jailbreak and CoT techniques may diminish over time as models undergo safety updates
- The experiments focus on targeted extraction of known individuals rather than indiscriminate data harvesting

## Confidence

**High Confidence**: The finding that jailbreak + CoT prompts successfully extract memorized information from ChatGPT (57.95% recovery of frequent Enron emails) is well-supported by controlled experiments and clear methodology.

**Medium Confidence**: The claim about New Bing's superior performance (94% accuracy) due to search integration is supported but requires additional validation.

**Low Confidence**: The assertion that this represents a fundamental privacy threat to all LLM users is overstated, as the experiments focus on targeted extraction rather than indiscriminate data harvesting.

## Next Checks

1. Test the extraction techniques on a broader, randomly sampled dataset of personal information from multiple domains to assess generalizability beyond the structured email datasets used.

2. Repeat the experiments after a 3-6 month interval to measure whether model updates have reduced the effectiveness of jailbreak + CoT techniques, establishing whether these vulnerabilities persist over time.

3. Conduct a blinded user study where participants attempt to extract information about individuals they don't know personally, measuring success rates for both targeted extraction (with name) and free-form queries (without name) to better understand real-world threat potential.