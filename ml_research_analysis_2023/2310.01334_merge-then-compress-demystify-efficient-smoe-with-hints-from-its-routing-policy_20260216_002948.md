---
ver: rpa2
title: 'Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing
  Policy'
arxiv_id: '2310.01334'
source_url: https://arxiv.org/abs/2310.01334
tags:
- experts
- smoe
- expert
- merging
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses memory inefficiency in sparse mixture-of-experts
  (SMoE) models due to redundant experts. The core method, MC-SMoE, merges experts
  guided by routing statistics, starting with neuron permutation alignment, grouping
  similar experts based on activation frequency, and merging them with frequency-weighted
  averaging.
---

# Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy

## Quick Facts
- **arXiv ID:** 2310.01334
- **Source URL:** https://arxiv.org/abs/2310.01334
- **Reference count:** 20
- **Primary result:** Achieves up to 80% memory and 20% FLOPs reduction in SMoE models with minimal performance loss

## Executive Summary
This paper addresses memory inefficiency in sparse mixture-of-experts (SMoE) models caused by redundant experts. The proposed MC-SMoE method merges experts guided by routing statistics, starting with neuron permutation alignment to align experts, grouping similar experts based on activation frequency, and merging them with frequency-weighted averaging. The merged experts exhibit lower dimensionality, enabling further compression. Experiments across 8 NLP benchmarks demonstrate significant efficiency gains while maintaining model performance.

## Method Summary
The MC-SMoE method operates in two stages: First, M-SMoE merges experts by aligning them using permutation matrices to minimize ℓ2 distance, then groups experts based on routing activation frequency and similarity, and merges groups using frequency-weighted averaging. Second, MC-SMoE applies low-rank decomposition (UV + sparse S) to the merged experts to further reduce memory and FLOPs. The approach is fine-tuned with knowledge distillation from the full SMoE model to recover performance.

## Key Results
- Achieves up to 80% memory reduction on COPA task while maintaining accuracy
- Reduces FLOPs by up to 20% across multiple benchmarks
- Maintains less than 1% accuracy drop compared to unmerged models
- Demonstrates effectiveness across 8 diverse NLP tasks including SST-2, MRPC, MultiRC, WinoGrande, SQuAD, WikiQA, and HotpotQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert merging guided by routing statistics reduces memory while preserving performance.
- Mechanism: Uses routing activation frequencies to weight expert averaging, emphasizing dominant experts and down-weighting redundant ones.
- Core assumption: Routing frequency reflects expert importance and redundancy.
- Evidence anchors:
  - [abstract] "Our pilot investigation reveals that conventional model merging methods fail to be effective in such expert merging for SMoE. The potential reasons are: (1) redundant information overshadows critical experts; (2) appropriate neuron permutation for each expert is missing to bring all of them in alignment."
  - [section 3.1] "The activation frequency of experts indicates its utilization and can be regarded as a great proxy for its importance."
  - [corpus] Weak evidence: corpus shows related work on SMoE compression but no direct evidence for routing frequency as proxy.
- Break condition: If routing becomes uniform (all experts activated equally), frequency weighting loses discriminative power.

### Mechanism 2
- Claim: Neuron permutation alignment improves merging quality.
- Mechanism: Aligns experts by finding optimal permutation matrices to minimize ℓ2 distance between permuted weights before merging.
- Core assumption: Permutation invariance allows function-preserving weight rearrangement.
- Evidence anchors:
  - [abstract] "appropriate neuron permutation for each expert is missing to bring all of them in alignment."
  - [section 3.1] "Ainsworth et al. (2022) tells us that for any arbitrary permutation matrix P, the following equation Wout(act(Winx)) = WoutPT(act(PWinx)) always holds."
  - [corpus] No direct corpus evidence for permutation alignment in SMoE.
- Break condition: If expert functions are too divergent, optimal permutations may not exist or be computationally expensive.

### Mechanism 3
- Claim: Merging induces low-rank structure enabling further compression.
- Mechanism: Post-merging decomposition into low-rank UV + sparse S components exploits reduced dimensionality.
- Core assumption: Merging reduces effective weight dimensionality.
- Evidence anchors:
  - [abstract] "we observed that our proposed merging promotes a low dimensionality in the merged expert's weight space, naturally paving the way for additional compression."
  - [section 3.2] "Figure 4 showcases several stable-rank change ratio instances of SMoEs fine-tuned on various tasks... These mostly negative values throughout the SMoE layers emphasize a lower dimensionality achieved through the merging process."
  - [corpus] Weak evidence: corpus mentions low-rank decomposition but not specifically linked to merging.
- Break condition: If merged weights don't exhibit low-rank structure, decomposition provides no benefit.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding how experts are routed and activated is crucial for merging strategy.
  - Quick check question: In a 32-expert SMoE layer, how many experts are typically activated per token?
- Concept: Permutation invariance in neural networks
  - Why needed here: Enables neuron alignment without changing functional behavior.
  - Quick check question: What mathematical property allows permuting neurons in a feed-forward layer without changing output?
- Concept: Low-rank matrix decomposition
  - Why needed here: Enables post-merging compression by exploiting reduced dimensionality.
  - Quick check question: How does decomposing W = UV + S reduce parameter count compared to full W?

## Architecture Onboarding

- Component map: Router layer → Expert layers → Merging module → Compression module → Fine-tuning
- Critical path: Router → Expert activation → Merging → Compression → Fine-tuning
- Design tradeoffs:
  - Merging ratio: More merging → greater efficiency but potential performance loss
  - Compression level: Higher compression → smaller model but possible accuracy degradation
  - Router pruning: Could improve latency but requires careful implementation
- Failure signatures:
  - Performance collapse after merging: Indicates over-aggressive merging or poor alignment
  - Memory savings without performance gain: Suggests ineffective grouping or weighting
  - Compression fails to reduce parameters: Indicates weights not low-rank after merging
- First 3 experiments:
  1. Apply merging to first SMoE layer only, compare performance vs full model
  2. Vary merging ratio (keep 16, 8, 4 experts per layer), measure efficiency/accuracy tradeoff
  3. Test router-logits vs router-weights for grouping similarity, compare results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does expert merging affect the long-term training dynamics of sparse mixture-of-experts models?
- Basis in paper: [explicit] The paper demonstrates that merged experts exhibit lower dimensionality and that M-SMoE achieves significant memory and FLOPs reduction, but it does not explore how merging impacts training dynamics beyond the initial merging phase.
- Why unresolved: The study focuses on the downstream efficiency of merged models rather than on their training behavior. It remains unclear whether merged experts might lead to different convergence patterns or affect the overall learning capacity during training.
- What evidence would resolve it: Experiments tracking training loss, gradient flow, and expert activation patterns during training of merged versus unmerged models would clarify the long-term effects of merging on model dynamics.

### Open Question 2
- Question: Can routing policy-based merging be generalized to non-text modalities, such as vision or multimodal data?
- Basis in paper: [inferred] The paper applies expert merging to text-based models (e.g., T5-based and GPT-based SMoEs) and relies on routing statistics specific to language tasks, but it does not explore whether similar strategies work for other modalities.
- Why unresolved: The methodology depends on the structure of routing decisions in text models, and it is unclear whether routing statistics from vision or multimodal transformers would provide similar benefits for expert merging.
- What evidence would resolve it: Applying M-SMoE to vision transformers or multimodal models and comparing merging effectiveness across modalities would determine the generalizability of the approach.

### Open Question 3
- Question: What is the impact of merging on model robustness to adversarial attacks or distribution shifts?
- Basis in paper: [inferred] The paper focuses on efficiency gains and performance preservation but does not evaluate robustness under adversarial conditions or data distribution shifts.
- Why unresolved: Merging experts reduces model redundancy, which could either improve robustness by eliminating unstable experts or degrade it by reducing model capacity. The trade-off between efficiency and robustness is not explored.
- What evidence would resolve it: Testing merged models against adversarial examples and out-of-distribution data compared to unmerged baselines would reveal whether merging affects robustness.

## Limitations

- The method assumes routing activation frequencies reliably indicate expert importance, which may fail when routing patterns are uniform or experts serve specialized functions
- Neuron permutation alignment effectiveness for SMoE models lacks rigorous empirical validation beyond the proposed method
- The paper lacks detailed ablation studies showing individual contributions of merging versus compression components to overall efficiency gains

## Confidence

**High Confidence Claims:**
- SMoE models exhibit significant memory redundancy due to duplicate experts
- Merging experts based on routing statistics can reduce memory usage

**Medium Confidence Claims:**
- Frequency-weighted averaging preserves performance while reducing parameters
- Post-merging low-rank decomposition is effective

**Low Confidence Claims:**
- Neuron permutation alignment significantly improves merging quality
- The specific grouping strategy based on router logits similarity is optimal

## Next Checks

1. **Routing Uniformity Test**: Evaluate the method on tasks with near-uniform expert activation patterns to test the robustness of frequency-based weighting when the core assumption about routing frequency as importance proxy breaks down.

2. **Permutation Alignment Ablation**: Compare merging quality with and without neuron permutation alignment on diverse SMoE architectures to quantify the contribution of this component and test its effectiveness across different model configurations.

3. **Expert Diversity Analysis**: Measure the functional similarity between merged experts using techniques like centered kernel alignment (CKA) to verify that merged experts are truly redundant rather than complementary, validating the fundamental premise of the merging approach.