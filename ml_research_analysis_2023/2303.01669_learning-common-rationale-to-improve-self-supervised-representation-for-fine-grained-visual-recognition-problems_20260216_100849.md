---
ver: rpa2
title: Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained
  Visual Recognition Problems
arxiv_id: '2303.01669'
source_url: https://arxiv.org/abs/2303.01669
tags:
- learning
- moco
- feature
- gradcam
- projections
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of self-supervised learning (SSL)
  for fine-grained visual recognition (FGVR), where existing SSL methods may learn
  features not suitable for characterizing subtle differences in FGVR. To overcome
  this, the authors propose learning an additional screening mechanism, termed common
  rationales, to identify discriminative patterns commonly seen across instances and
  classes.
---

# Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems

## Quick Facts
- arXiv ID: 2303.01669
- Source URL: https://arxiv.org/abs/2303.01669
- Authors: 
- Reference count: 40
- Key outcome: 49.69% rank-1 accuracy on CUB-200-2011 retrieval task, 32.62% higher than MoCo v2 baseline

## Executive Summary
This paper addresses the challenge of self-supervised learning (SSL) for fine-grained visual recognition (FGVR) by proposing a method to learn "common rationales" - discriminative patterns that occur across multiple instances and classes. The approach introduces a GradCAM fitting branch (GFB) with limited capacity that learns to identify and focus on these common patterns rather than instance-specific features. The method significantly improves performance on FGVR tasks, achieving 49.69% rank-1 accuracy on CUB-200-2011 retrieval compared to 32.62% for the MoCo v2 baseline.

## Method Summary
The method builds upon MoCo v2 by adding a GradCAM fitting branch (GFB) with limited fitting capacity (K filters with max-out operation) to identify common rationales. During training, the GFB learns to fit GradCAM maps derived from the SSL objective using KL divergence loss, forcing it to capture patterns common across samples. At inference, the GFB generates attention masks that are used for weighted average pooling over the convolutional feature map, replacing standard global average pooling. The combined loss includes both the contrastive loss (LCL) and KL divergence loss (LKL) between normalized GradCAM and GFB outputs.

## Key Results
- 49.69% rank-1 accuracy on CUB-200-2011 retrieval task (32.62% improvement over MoCo v2 baseline)
- 76.6% Top-1 accuracy on Stanford Cars linear classification (3.8% improvement over MoCo v2)
- Significant improvements across four FGVR datasets including CUB-200-2011, Stanford Cars, FGVC Aircraft, and iNaturalist2019

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The GradCAM fitting branch with limited fitting capacity captures discriminative patterns that are common across instances and classes (common rationales), which are more likely to correspond to key object parts or foreground regions.
- Mechanism: The GFB uses a convolutional layer with K filters followed by max-out operation to fit the GradCAM derived from the SSL objective. The limited fitting capacity forces the branch to focus on common patterns rather than overfitting to instance-specific features.
- Core assumption: Discriminative patterns for fine-grained visual recognition are more likely to be common across samples rather than instance-specific, and these common patterns correspond to key object parts or foreground regions.
- Evidence anchors: [abstract]: "We propose learning an additional screening mechanism to identify discriminative clues commonly seen across instances and classes, dubbed as common rationales in this paper. Intuitively, common rationales tend to correspond to the discriminative patterns from the key parts of foreground objects."

### Mechanism 2
- Claim: The attention mask generated by the GFB at inference time selectively aggregates features representing an instance by performing weighted average pooling over the convolutional feature map.
- Mechanism: During inference, the GFB predicts a normalized attention mask that is used to perform weighted average pooling over the last-layer convolutional feature map, replacing standard global average pooling.
- Core assumption: The attention mask learned during training effectively identifies which spatial regions contain discriminative information for FGVR tasks.
- Evidence anchors: [abstract]: "At the test stage, the branch generates a set of spatial weights to selectively aggregate features representing an instance."

### Mechanism 3
- Claim: The KL divergence loss between the normalized GradCAM and the GFB output ensures the attention mask learns to match the important regions identified by the SSL objective.
- Mechanism: The method uses KL divergence loss between the normalized GradCAM from the SSL objective and the normalized attention mask predicted by the GFB to train the branch.
- Core assumption: The regions important for the SSL objective (as identified by GradCAM) are good proxies for regions important for FGVR tasks.
- Evidence anchors: [section]: "we require the GFB to produce a similar attention map as the one produced from the GradCAM of LCL. We follow [33] to normalize the GradCAM into a probability distribution and adopt KL divergence as the loss function."

## Foundational Learning

- Concept: Self-supervised learning (SSL) and contrastive learning objectives
  - Why needed here: The method builds upon MoCo v2, a contrastive learning framework, and uses its GradCAM to identify important regions. Understanding how contrastive learning works is essential to grasp why the method targets these specific regions.
  - Quick check question: What is the difference between instance discrimination in contrastive learning and fine-grained visual recognition tasks?

- Concept: Grad-CAM (Gradient-weighted Class Activation Mapping)
  - Why needed here: The method uses Grad-CAM derived from the SSL objective to identify important spatial regions, then fits these with a limited-capacity branch. Understanding how Grad-CAM works is crucial for understanding the core mechanism.
  - Quick check question: How does Grad-CAM calculate the importance of spatial regions, and why is it applicable even without ground truth class labels in SSL?

- Concept: Weighted average pooling vs. global average pooling
  - Why needed here: The method replaces standard global average pooling with weighted average pooling using the attention mask. Understanding the difference and implications is important for understanding the architectural change.
  - Quick check question: What are the advantages and disadvantages of using weighted average pooling compared to global average pooling in convolutional neural networks?

## Architecture Onboarding

- Component map: Encoder (MoCo v2-based) -> GradCAM Fitting Branch (GFB) -> Loss components (LCL + LKL) -> Inference pipeline (GFB attention mask -> weighted average pooling)
- Critical path: 1) During training: SSL objective generates GradCAM → GFB fits GradCAM using KL loss; 2) During inference: GFB generates attention mask → weighted average pooling of feature map
- Design tradeoffs: Limited fitting capacity (K filters) vs. expressiveness: Fewer filters may miss some discriminative patterns but prevents overfitting to instance-specific features
- Failure signatures: Performance similar to baseline MoCo v2: GFB not learning meaningful attention masks; Dramatic performance drop when using weighted average pooling: Attention masks not aligned with discriminative regions
- First 3 experiments: 1) Ablation study: Remove GFB and use standard MoCo v2 to establish baseline performance; 2) Hyperparameter sweep: Test different values of K (number of projections) to find optimal balance; 3) Visualization: Examine attention masks produced by GFB to verify they focus on object parts rather than background

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of projections (K) for the GradCAM fitting branch (GFB) in the proposed method, and how does it vary across different fine-grained datasets?
- Basis in paper: [explicit] The paper mentions that the rank-1 accuracy peaks at K around 32 for the CUB-200-2011 dataset, but further increasing K beyond 64 leads to a decrease in performance.
- Why unresolved: The optimal value of K may depend on the specific characteristics of the dataset, such as the number of classes, the complexity of the objects, and the diversity of the images. The paper only provides results for one fine-grained dataset (CUB-200-2011), and it is unclear whether the same optimal K would apply to other fine-grained datasets like Stanford Cars or FGVC Aircraft.
- What evidence would resolve it: Conducting experiments with different values of K on multiple fine-grained datasets and analyzing the relationship between K and performance would help determine the optimal number of projections for each dataset.

### Open Question 2
- Question: How does the proposed method compare to other state-of-the-art self-supervised learning methods on non-fine-grained datasets?
- Basis in paper: [explicit] The paper mentions that the proposed method also works for non-fine-grained cases, but it does not provide a detailed comparison with other methods on non-fine-grained datasets.
- Why unresolved: The paper primarily focuses on evaluating the proposed method on fine-grained datasets, and it is unclear how it would perform on non-fine-grained datasets compared to other state-of-the-art self-supervised learning methods.
- What evidence would resolve it: Conducting experiments on non-fine-grained datasets and comparing the proposed method's performance with other state-of-the-art self-supervised learning methods would provide insights into its effectiveness in a broader range of scenarios.

### Open Question 3
- Question: How does the proposed method's performance vary with different backbone architectures and pre-training strategies?
- Basis in paper: [explicit] The paper mentions that the proposed method is implemented based on MoCo v2 and uses a ResNet-50 backbone initialized with ImageNet-trained weights, but it does not explore the impact of using different backbone architectures or pre-training strategies.
- Why unresolved: The choice of backbone architecture and pre-training strategy can significantly impact the performance of self-supervised learning methods. The paper only uses one specific backbone architecture and pre-training strategy, and it is unclear how the proposed method would perform with other options.
- What evidence would resolve it: Conducting experiments with different backbone architectures (e.g., ResNet-101, EfficientNet) and pre-training strategies (e.g., supervised pre-training, unsupervised pre-training) would provide insights into the proposed method's performance under various configurations.

## Limitations

- The method's reliance on specific SSL framework (MoCo v2) raises questions about generalizability to other SSL approaches
- Core hypothesis that common rationales improve FGVR performance is supported empirically but the mechanism connecting GradCAM from SSL to FGVR-relevant regions remains indirect
- Claims about optimal number of projections (K=32) and specific types of patterns captured versus discarded lack supporting analysis or ablation studies

## Confidence

**High Confidence**: The architectural design of adding a GradCAM fitting branch is technically sound and the weighted average pooling mechanism is clearly specified. The experimental methodology follows established practices.

**Medium Confidence**: The core hypothesis that common rationales improve FGVR performance is supported by significant empirical improvements, but the mechanism connecting GradCAM from SSL to FGVR-relevant regions remains indirect. The assumption that discriminative patterns for FGVR are predominantly common rather than instance-specific needs more direct validation.

**Low Confidence**: Claims about the specific number of projections (K=32) being optimal, and assertions about which types of discriminative patterns are captured versus discarded, lack supporting analysis or ablation studies.

## Next Checks

1. **Ablation study on K values**: Systematically vary the number of projections (K) in the GradCAM fitting branch from 8 to 128, measuring both FGVR performance and the diversity of captured patterns through visualization, to verify that limited capacity is crucial for the method's success.

2. **Cross-dataset generalization test**: Evaluate whether attention masks learned on one FGVR dataset (e.g., CUB-200-2011) transfer to another (e.g., Stanford Cars), to confirm that the GFB is indeed capturing common rather than dataset-specific rationales.

3. **Direct comparison with ground truth**: For datasets with part annotations (like CUB-200-2011), compare the attention masks generated by GFB with human-labeled object parts to quantitatively assess whether the method focuses on key discriminative regions as claimed.