---
ver: rpa2
title: Fully Automatic Neural Network Reduction for Formal Verification
arxiv_id: '2305.01932'
source_url: https://arxiv.org/abs/2305.01932
tags:
- network
- neurons
- neural
- networks
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a fully automatic, sound neural network reduction
  method for formal verification. The core idea is to merge similar neurons in nonlinear
  layers based on their output bounds, reducing network size while preserving verifiability.
---

# Fully Automatic Neural Network Reduction for Formal Verification

## Quick Facts
- arXiv ID: 2305.01932
- Source URL: https://arxiv.org/abs/2305.01932
- Reference count: 28
- One-line primary result: Neural network reduction method that merges neurons based on output bounds, achieving up to 95% reduction while preserving verifiability

## Executive Summary
This paper introduces a fully automatic method for reducing neural network size specifically for formal verification. The core innovation is an on-the-fly neuron merging technique that identifies and merges neurons with similar output bounds during verification. By using interval arithmetic to look ahead and compute bounds dynamically, the method avoids expensive zonotope computations while maintaining soundness guarantees. The approach is shown to reduce networks to less than 5% of their original neuron count on MNIST and CIFAR-10 benchmarks, significantly accelerating verification times without compromising correctness.

## Method Summary
The method works by merging neurons in nonlinear layers that have similar output bounds within a specified tolerance δ. During verification, interval arithmetic is used to compute over-approximative bounds on neuron outputs without constructing high-dimensional zonotopes. Merge buckets are created dynamically based on these output bounds, and neurons within the same bucket are merged into a single neuron with constant output. This reduction is performed on-the-fly during the verification process, making it computationally cheap while preserving the soundness guarantee that the reduced network's verification entails the original network's verification.

## Key Results
- Networks can be reduced to less than 5% of their original neuron count
- Verification time is significantly decreased while maintaining soundness
- The method works with various activation functions (ReLU, sigmoid, tanh)
- Applicable to both fully connected and convolutional neural networks
- Computational overhead of reduction is minimal compared to verification gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neuron merging reduces network size while preserving verifiability.
- Mechanism: The method merges neurons in nonlinear layers that have similar output bounds within a tolerance δ, replacing them with a single neuron with constant output y. This is done on-the-fly during verification using interval arithmetic to compute bounds without constructing high-dimensional zonotopes.
- Core assumption: Neurons with similar output bounds behave similarly enough that replacing them preserves the property that the reduced network's verification entails the original network's verification.
- Evidence anchors:
  - [abstract] "The core idea is to merge similar neurons in nonlinear layers based on their output bounds, reducing network size while preserving verifiability."
  - [section] "Proposition 4 (Neuron Merging)... The construction is sound."
- Break condition: If the tolerance δ is too large, the approximation error becomes significant enough to violate the soundness guarantee.

### Mechanism 2
- Claim: Interval arithmetic look-ahead enables efficient on-the-fly reduction.
- Mechanism: Instead of propagating full zonotopes through each layer, the algorithm propagates interval bounds to determine merge buckets, then constructs the reduced network only when needed. This avoids expensive zonotope computations while maintaining soundness.
- Core assumption: Interval bounds provide sufficient information to identify neurons that can be merged without computing exact output sets.
- Evidence anchors:
  - [section] "We deploy a look-ahead algorithm using interval arithmetic [26] to avoid these expensive computations and reduce the network on-the-fly."
  - [section] "The overhead is computationally cheap and the bound computation is over-approximative."
- Break condition: If the interval bounds are too loose, fewer neurons will be identified as mergeable, reducing the effectiveness of the reduction.

### Mechanism 3
- Claim: Dynamic bucket creation adapts to specific input specifications.
- Mechanism: Merge buckets are created dynamically based on the output bounds of neurons for a given input set, rather than using static thresholds. This allows the method to adapt to the specific behavior of the network for the verification task at hand.
- Core assumption: The output bounds of neurons vary significantly depending on the input, making static thresholds suboptimal.
- Evidence anchors:
  - [section] "Dynamic buckets. The merge buckets are dynamically created based on the output boundsIk = [lk, u k]⊂ Rvk."
  - [section] "Dynamic merge bucket creation reduces the number of neurons to 25% while still verifying all images" (from evaluation).
- Break condition: If the input set is too large or varied, the dynamic buckets may become too numerous or too specific.

## Foundational Learning

- Concept: Interval arithmetic for set-based computation
  - Why needed here: Used to compute over-approximative bounds on neuron outputs without expensive zonotope computations
  - Quick check question: How does interval arithmetic differ from exact set representations like zonotopes in terms of computational cost and precision?

- Concept: Zonotopes and their operations
  - Why needed here: Used as the primary set representation for formal verification, with operations like interval enclosure and interval addition
  - Quick check question: What are the key operations on zonotopes that are used in the neural network verification process?

- Concept: Neural network verification using reachability analysis
  - Why needed here: The foundation for the formal verification that the reduction method aims to accelerate
  - Quick check question: What are the main challenges in scaling reachability-based neural network verification to large networks?

## Architecture Onboarding

- Component map:
  Input -> Interval Arithmetic Computation -> Merge Bucket Creation -> Neuron Merging -> Reduced Network -> Verification

- Critical path:
  1. Compute interval bounds for current layer using look-ahead
  2. Create merge buckets based on bounds and tolerance
  3. Reduce network by merging neurons in buckets
  4. Verify reduced network using underlying verifier
  5. Return result (safe/unsafe)

- Design tradeoffs:
  - Tight vs. loose tolerance δ: Tighter tolerance preserves more accuracy but reduces reduction effectiveness
  - Static vs. dynamic buckets: Static is simpler but less adaptive; dynamic is more effective but more complex
  - Look-ahead depth: Deeper look-ahead could improve merging but increases computational cost

- Failure signatures:
  - No reduction achieved: Likely due to loose interval bounds or inappropriate tolerance
  - Soundness violation: Tolerance too large, causing the reduced network to no longer entail the original's verification
  - Performance degradation: Overhead of reduction outweighing benefits, possibly due to small networks or very tight specifications

- First 3 experiments:
  1. Run on a small ReLU network with known properties, compare reduction rate and verification time with/without reduction
  2. Vary tolerance δ on a benchmark network, plot reduction rate vs. verification success rate
  3. Compare static vs. dynamic bucket creation on a convolutional network, measure reduction effectiveness and computational overhead

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section suggests several areas for future work including scalability to larger networks and extending the approach to other architectures.

## Limitations
- Evaluation is limited to relatively small networks (6×200 ReLU/sigmoid from ERAN benchmark)
- Effectiveness on larger, more complex networks remains unverified
- The impact of approximation errors on long-term verification accuracy is not thoroughly explored

## Confidence

**High Confidence**: The core mechanism of neuron merging based on output bounds is well-founded and the soundness proof appears rigorous. The interval arithmetic look-ahead approach is a reasonable and computationally efficient method for on-the-fly reduction.

**Medium Confidence**: The claim that dynamic bucket creation significantly improves reduction effectiveness is supported by evaluation results, but the comparison with static thresholds could be more comprehensive. The computational overhead characterization as "computationally cheap" is based on limited benchmarks.

**Low Confidence**: The scalability of the method to very large networks and complex architectures (beyond the tested fully connected and simple convolutional networks) is uncertain. The long-term stability of the approximation errors across multiple verification runs is not addressed.

## Next Checks

1. **Scalability Test**: Apply the reduction method to larger, more complex networks (e.g., ResNet or VGG architectures) and measure the reduction rate and verification time compared to the baseline.

2. **Approximation Error Analysis**: Conduct a detailed study of the accumulation of approximation errors during interval arithmetic propagation through multiple layers, and its impact on verification accuracy over time.

3. **Robustness Verification**: Test the method's effectiveness under adversarial conditions, such as networks with higher adversarial robustness requirements or those trained with different objectives (e.g., certified defense).