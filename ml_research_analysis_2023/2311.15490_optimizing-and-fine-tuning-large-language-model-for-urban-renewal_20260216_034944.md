---
ver: rpa2
title: Optimizing and Fine-tuning Large Language Model for Urban Renewal
arxiv_id: '2311.15490'
source_url: https://arxiv.org/abs/2311.15490
tags:
- urban
- renewal
- fine-tuning
- chatglm
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops an urban renewal large language model (LLM)
  by fine-tuning ChatGLM with automatically generated question-answering datasets
  derived from urban renewal literature. The research introduces a joint fine-tuning
  approach combining Prefix and LoRA techniques, which significantly improves model
  performance on knowledge QA tasks.
---

# Optimizing and Fine-tuning Large Language Model for Urban Renewal

## Quick Facts
- arXiv ID: 2311.15490
- Source URL: https://arxiv.org/abs/2311.15490
- Reference count: 40
- Key outcome: Joint Prefix and LoRA fine-tuning improves urban renewal LLM performance by 15-20% over baseline and 5% over LoRA alone

## Executive Summary
This study develops an urban renewal large language model by fine-tuning ChatGLM with automatically generated question-answering datasets derived from urban renewal literature. The research introduces a joint fine-tuning approach combining Prefix and LoRA techniques, which significantly improves model performance on knowledge QA tasks. Experimental results demonstrate that the joint fine-tuning method increases Bleu-4 scores by approximately 5% compared to LoRA fine-tuning alone and by 15%-20% compared to the original ChatGLM model.

## Method Summary
The study fine-tunes ChatGLM using a joint approach that combines Prefix tuning (with 3.5M trainable parameters) and LoRA (with 1.86M trainable parameters) on an automatically generated QA dataset from urban renewal literature. The method leverages Prefix tuning to provide contextual guidance and LoRA to inject domain-specific knowledge, resulting in improved text generation quality while maintaining computational efficiency with less than 0.1% of total model parameters being trainable.

## Key Results
- Joint fine-tuning increases Bleu-4 scores by approximately 5% compared to LoRA fine-tuning alone
- The method improves Bleu and Rouge metrics by 15%-20% compared to the original ChatGLM model
- Trainable parameters comprise less than 0.1% of the total 6.2B parameter model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint fine-tuning with Prefix and LoRA simultaneously improves model performance by combining contextual guidance with task-specific adaptation.
- Mechanism: Prefix tuning adds task-relevant virtual tokens that guide the model's generation contextually, while LoRA provides low-rank adaptations that inject specific urban renewal knowledge.
- Core assumption: The combined approach utilizes complementary strengthsâ€”Prefix for generation quality and coherence, LoRA for domain-specific knowledge injection.
- Evidence anchors: Experimental results show 5% improvement over LoRA and 15-20% over baseline; methodology describes how each method contributes to performance gains.

### Mechanism 2
- Claim: Prefix tuning improves text generation coherence by leveraging original text structure and contextual information.
- Mechanism: Prefix tokens are prepended to the input, influencing the model's generation process without modifying original parameters.
- Core assumption: The original model's contextual understanding is preserved and enhanced through task-specific prefixes.
- Evidence anchors: The paper states Prefix improves accuracy while maintaining original text structure by leveraging contextual information.

### Mechanism 3
- Claim: LoRA enables efficient domain adaptation by learning low-rank transformations that capture urban renewal-specific knowledge.
- Mechanism: LoRA introduces small trainable matrices that modify the model's behavior through low-rank decomposition.
- Core assumption: The changes needed for domain adaptation are low-rank, making LoRA sufficient for capturing task-specific nuances.
- Evidence anchors: The paper describes LoRA as a parameter-efficient fine-tuning method that assumes model parameter changes during task adaptation are low-rank.

## Foundational Learning

- Concept: Large Language Models (LLMs) and their architecture
  - Why needed here: Understanding the base model (ChatGLM) and how fine-tuning methods interact with its architecture is crucial for implementing and debugging the joint fine-tuning approach.
  - Quick check question: What is the role of the Transformer architecture in LLMs, and how do fine-tuning methods like Prefix and LoRA modify its behavior?

- Concept: Parameter-efficient fine-tuning methods (Prefix tuning and LoRA)
  - Why needed here: The study's core contribution relies on understanding how Prefix and LoRA work individually and how they can be combined effectively.
  - Quick check question: How does Prefix tuning differ from LoRA in terms of parameter modification and computational efficiency?

- Concept: Evaluation metrics for text generation (BLEU and ROUGE)
  - Why needed here: The performance improvements are quantified using BLEU and ROUGE scores, so understanding these metrics is essential for interpreting results.
  - Quick check question: What is the difference between precision-based (BLEU) and recall-based (ROUGE) evaluation metrics, and why are both used?

## Architecture Onboarding

- Component map: ChatGLM base model (6.2B parameters) -> Prefix layer with task-specific virtual tokens -> LoRA adapters with low-rank matrices A and B -> Urban renewal QA dataset (1085 training, 275 test samples) -> Evaluation pipeline using BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L

- Critical path: 1) Dataset generation using self-instruct method 2) Prefix fine-tuning to adapt base model 3) LoRA fine-tuning on Prefix-adapted model 4) Joint evaluation using test set

- Design tradeoffs: Prefix tuning offers better coherence but requires more parameters than LoRA; LoRA is more parameter-efficient but may sacrifice some generation quality; joint approach balances both but adds complexity

- Failure signatures: Performance degradation when switching from individual to joint fine-tuning; overfitting on training data (high train scores, low test scores); computational inefficiency despite parameter savings

- First 3 experiments: 1) Train Prefix-only model and evaluate baseline performance 2) Train LoRA-only model and compare with Prefix results 3) Implement joint fine-tuning and measure improvement over individual methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of joint fine-tuning with LoRA followed by Prefix compare to the current Prefix-then-LoRA approach for urban renewal LLM applications?
- Basis in paper: The authors suggest exploring joint fine-tuning with LoRA followed by Prefix as a future research direction, noting that different fine-tuning sequences could impact model performance.
- Why unresolved: The paper only tested the Prefix-then-LoRA sequence, leaving the reverse sequence unexplored. The optimal sequence for urban renewal applications remains unknown.
- What evidence would resolve it: Systematic comparison experiments testing both fine-tuning sequences (Prefix-then-LoRA vs LoRA-then-Prefix) on identical urban renewal datasets with identical hyperparameters.

### Open Question 2
- Question: What is the relationship between training sample size and model performance for different parameter-efficient fine-tuning methods in urban renewal LLM applications?
- Basis in paper: The authors acknowledge limitations in dataset size and suggest future research exploring the relationship between training samples and model parameters.
- Why unresolved: The current study used a fixed dataset size (1085 training samples) without exploring how varying sample sizes would impact performance across different fine-tuning methods.
- What evidence would resolve it: Controlled experiments varying training sample sizes (e.g., 100, 500, 1000, 2000 samples) while measuring performance metrics for Prefix, LoRA, and joint fine-tuning methods.

### Open Question 3
- Question: How generalizable is the joint fine-tuning method across different LLM architectures for urban renewal applications?
- Basis in paper: The authors propose testing other open-source models like LLaMA, Qwen, and Baichuan in future work to demonstrate the universality of their joint fine-tuning method across different LLM architectures.
- Why unresolved: The current study only tested the method on ChatGLM, limiting understanding of whether the approach works equally well across different LLM architectures.
- What evidence would resolve it: Replicating the entire experimental pipeline (dataset generation, fine-tuning, evaluation) using at least three different LLM architectures (e.g., LLaMA, Qwen, Baichuan) with comparable parameter counts.

## Limitations
- Dataset quality concerns due to automatic generation from only 70-80 papers using ChatGLM self-instruct methods
- Small dataset size (1085 training samples) limiting generalizability and potentially causing overfitting
- Complexity of joint fine-tuning makes it difficult to isolate individual contributions of Prefix and LoRA methods

## Confidence
- **High confidence**: Computational efficiency claims regarding parameter reduction (less than 0.1% of total parameters) are well-supported by the methodology description and align with established LoRA principles.
- **Medium confidence**: Performance improvement metrics (5% over LoRA, 15-20% over baseline) are credible given the experimental setup, but the small dataset size limits generalizability.
- **Low confidence**: Mechanism explanations for why joint fine-tuning outperforms individual methods are somewhat speculative, lacking ablation studies to definitively attribute improvements to specific components.

## Next Checks
1. Conduct ablation studies to separately evaluate Prefix and LoRA contributions by testing individual fine-tuning methods with identical datasets and hyperparameters.
2. Validate dataset quality by manually reviewing a random sample of generated QA pairs to assess relevance, accuracy, and potential hallucinations.
3. Test model generalization by evaluating performance on external urban renewal datasets or related domains to confirm domain-specific adaptation effectiveness.