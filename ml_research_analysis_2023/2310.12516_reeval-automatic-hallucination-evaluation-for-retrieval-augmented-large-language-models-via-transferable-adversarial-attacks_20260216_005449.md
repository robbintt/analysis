---
ver: rpa2
title: 'ReEval: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language
  Models via Transferable Adversarial Attacks'
arxiv_id: '2310.12516'
source_url: https://arxiv.org/abs/2310.12516
tags:
- answer
- evidence
- question
- llms
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AutoDebug, a framework for generating transferable
  adversarial attacks to assess the hallucination of large language models (LLMs).
  The core idea is to automatically generate evaluation data by editing existing data
  where LLMs behave faithfully.
---

# ReEval: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks

## Quick Facts
- arXiv ID: 2310.12516
- Source URL: https://arxiv.org/abs/2310.12516
- Authors: Multiple
- Reference count: 26
- One-line primary result: A framework that automatically generates transferable adversarial examples to evaluate LLM hallucination, achieving significant accuracy drops across models including GPT-4.

## Executive Summary
This paper introduces AutoDebug, a framework for evaluating hallucination in large language models through automatically generated adversarial attacks. The method identifies cases where LLMs answer correctly with context but fail without it, then creates challenging test cases by either swapping answers or enriching context. The generated examples are transferable across different LLM architectures, enabling cost-effective evaluation. Experiments show substantial accuracy drops when models are tested on these adversarial examples, with the attacks remaining effective even on advanced models like GPT-4.

## Method Summary
AutoDebug generates evaluation data by first identifying QA examples where LLMs behave faithfully with context but hallucinate without it. Two attack strategies are then applied: answer swapping (replacing the correct answer with another valid but incorrect alternative while preserving context) and context enriching (adding more relevant information to the evidence). These attacks are implemented using ChatGPT to generate human-readable test cases that maintain contextual support while introducing confusion. The framework is evaluated on popular open-domain QA datasets, demonstrating transferability across multiple LLMs and effectiveness in triggering hallucination.

## Key Results
- Adversarial examples generated by AutoDebug cause significant accuracy drops across all tested LLMs
- Attacks are transferable between models, making the approach cost-effective
- Human evaluation confirms the generated evidence is readable (93%) and supportive (91%)
- Even GPT-4 shows vulnerability to these adversarial examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Answer swapping triggers hallucination by introducing conflicting factual information while keeping supporting context intact.
- Mechanism: The framework replaces correct answers with plausible but incorrect alternatives, exploiting LLMs' tendency to prioritize contextual consistency over explicit verification.
- Core assumption: LLMs rely on contextual patterns rather than rigorous fact-checking when answering questions.
- Break condition: If LLMs explicitly verify answers against context before responding, they may avoid hallucination in these cases.

### Mechanism 2
- Claim: Context enriching triggers hallucination by overwhelming models with additional relevant information.
- Mechanism: Adding more relevant information to original evidence creates complex contexts that confuse LLMs' reasoning processes.
- Core assumption: LLMs struggle to filter and prioritize information when presented with dense, multi-faceted contexts.
- Break condition: If LLMs can effectively parse and prioritize information, they may still identify correct answers despite enriched context.

### Mechanism 3
- Claim: Transferability of adversarial examples across LLMs enables cost-effective evaluation.
- Mechanism: Adversarial examples generated by smaller, cost-effective models can evaluate larger, more capable models due to shared vulnerabilities.
- Core assumption: Different LLMs share similar reasoning patterns and vulnerabilities.
- Break condition: If LLMs have unique defensive mechanisms making them resistant to specific attack types.

## Foundational Learning

- Concept: Adversarial Machine Learning
  - Why needed here: Framework uses adversarial techniques to generate evaluation data that triggers hallucination in LLMs.
  - Quick check question: What is the primary goal of adversarial machine learning in LLM evaluation?

- Concept: Transferability of Adversarial Examples
  - Why needed here: Understanding why attacks generated by one LLM work against others is crucial for cost-effectiveness.
  - Quick check question: What does transferability imply about vulnerabilities shared across different LLMs?

- Concept: Question Answering (QA) Systems
  - Why needed here: Framework focuses on QA scenarios where LLMs answer open-domain questions using supporting evidence.
  - Quick check question: What are key components of QA systems and how do they relate to this framework's approach?

## Architecture Onboarding

- Component map: Seed Case Identification -> Answer Swapping/Context Enriching -> Evaluation -> Analysis
- Critical path: Identify faithful QA examples → Generate adversarial examples → Evaluate on target LLMs → Analyze performance drops
- Design tradeoffs: Using smaller pivot LLM vs. larger one for attack generation; balancing enriched context complexity with hallucination likelihood; ensuring naturalness while maintaining difficulty
- Failure signatures: Low transferability across LLMs; inability to generate human-readable evidence; failure to trigger hallucination even in smaller models
- First 3 experiments:
  1. Implement Seed Case Identification module and verify correct categorization of QA examples
  2. Implement Answer Swapping module and test effectiveness in generating hallucinatory adversarial examples
  3. Implement Context Enriching module and evaluate impact on evidence complexity and hallucination likelihood

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different backbone LLMs on the effectiveness of generated adversarial attacks in terms of transferability?
- Basis in paper: [explicit] Paper compares effectiveness of adversarial examples generated by different LLMs (ChatGPT, GPT-4, Alpaca-7B) but doesn't provide comprehensive analysis across broader model ranges.
- Why unresolved: Paper lacks analysis of how backbone LLM choice affects transferability across wider range of models.
- What evidence would resolve it: Experiments with broader set of backbone LLMs evaluating transferability across various target models.

### Open Question 2
- Question: How do different prompting techniques influence LLM robustness against AutoDebug-generated adversarial attacks?
- Basis in paper: [explicit] Paper evaluates opinion-based prompts and in-context learning with mixed results but doesn't explore wide range of techniques.
- Why unresolved: Paper doesn't systematically test various prompting techniques or analyze their impact on model robustness.
- What evidence would resolve it: Systematically testing various prompting techniques and combinations with AutoDebug examples.

### Open Question 3
- Question: What are limitations of AutoDebug for tasks beyond question-answering and how can these be addressed?
- Basis in paper: [inferred] Paper focuses on QA scenario without exploring applicability to other tasks or discussing limitations in different contexts.
- Why unresolved: Paper doesn't provide insights into generalizability to other NLP tasks or identify challenges in extending use.
- What evidence would resolve it: Applying AutoDebug to various NLP tasks and analyzing effectiveness and limitations.

## Limitations

- Evaluation relies heavily on single pivot LLM (ChatGPT), potentially introducing bias in attack patterns
- Transfer success tested only on five models with varying architectures, limiting generalizability
- Human evaluation conducted with limited workers on subset of examples, affecting representativeness

## Confidence

- **High Confidence**: Core finding that adversarial examples trigger hallucination is well-supported by substantial, consistent accuracy drops across multiple models
- **Medium Confidence**: Transferability across different LLM architectures demonstrated but may not generalize to all model types
- **Low Confidence**: Human evaluation results (93% readability, 91% supportiveness) based on limited sample size may not represent diverse perspectives

## Next Checks

1. **Cross-Model Transferability Test**: Evaluate adversarial examples on additional LLM architectures, particularly sparse models and mixture-of-experts, to assess breadth of transferability

2. **Adversarial Robustness Analysis**: Test whether fine-tuning LLMs on generated adversarial examples improves their robustness, distinguishing between fundamental limitations and training artifacts

3. **Human Evaluation Scaling**: Expand human evaluation to larger, more diverse set of examples and evaluators, including domain experts for specialized question types, to validate readability and supportiveness across broader contexts