---
ver: rpa2
title: Counterfactual Image Generation for adversarially robust and interpretable
  Classifiers
arxiv_id: '2310.00761'
source_url: https://arxiv.org/abs/2310.00761
tags:
- images
- adversarial
- counterfactual
- image
- unet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a unified framework that addresses both interpretability
  and robustness in neural image classifiers by leveraging image-to-image translation
  GANs to generate counterfactual and adversarial examples. The framework integrates
  the classifier and discriminator into a single model capable of both class attribution
  and identifying generated images as "fake".
---

# Counterfactual Image Generation for adversarially robust and interpretable Classifiers

## Quick Facts
- arXiv ID: 2310.00761
- Source URL: https://arxiv.org/abs/2310.00761
- Authors: 
- Reference count: 40
- Key outcome: The paper introduces a unified framework that addresses both interpretability and robustness in neural image classifiers by leveraging image-to-image translation GANs to generate counterfactual and adversarial examples.

## Executive Summary
This paper presents a novel framework that simultaneously addresses two critical challenges in neural image classification: interpretability and robustness against adversarial attacks. The approach integrates a classifier and discriminator into a single model that generates counterfactual images representing what an image would look like if it belonged to the opposite class. By training the generator to create realistic counterfactuals that fool the discriminator, the model learns to produce both robust predictions and interpretable saliency masks that highlight important regions for classification decisions.

The framework demonstrates effectiveness on two domains: detecting internal feeding worm damage in apples and concrete crack segmentation. It achieves competitive F1-scores exceeding 0.98 and shows improved robustness against PGD attacks compared to conventional classifiers. Additionally, the discriminator's "fakeness" value serves as a reliable uncertainty measure, and the generated saliency masks achieve reasonable IoU scores despite being trained without pixel-level supervision.

## Method Summary
The method reformulates binary classification as a three-class problem: real undamaged, real damaged, and generated ("fake"). A generator network creates counterfactual images representing what an input would look like in the opposite class, while a discriminator evaluates both real and generated images, outputting class probabilities and a "fakeness" score. The model is trained using adversarial learning with cycle-consistent loss to stabilize training and improve gradient flow. Saliency masks are generated as the absolute difference between real and counterfactual images. The approach leverages image-to-image translation GANs, specifically UNet and Swin UNet architectures, to transform images between classes while maintaining realism.

## Key Results
- Achieves F1-scores of 0.980 for Swin UNet G on CASC IFW dataset
- Improves robustness against PGD attacks compared to conventional classifiers
- Generates saliency masks with IoU scores of 0.720 (Swin UNet G) on concrete crack segmentation, only 12% lower than models trained with pixel-level labels
- Discriminator's "fakeness" value shows positive correlation (0.081-0.100) with prediction uncertainty

## Why This Works (Mechanism)

### Mechanism 1
The discriminator-classifier (D) serves dual roles: distinguishing real from generated images and classifying images into undamaged/damaged classes, which enables simultaneous interpretability and robustness training. By extending the discriminator to output three values (p0, p1, pfake), the model learns to identify adversarial perturbations as "fake" while still performing binary classification. This joint training forces the generator to create realistic counterfactuals that are misclassified as the opposite class, improving both robustness and interpretability. Core assumption: The discriminator can effectively learn to detect subtle perturbations while maintaining classification accuracy when trained with the proposed loss function.

### Mechanism 2
The cycle-consistent loss (LGc) stabilizes GAN training by providing stronger gradient signals to the generator through nested counterfactuals. By generating counterfactuals of counterfactuals over multiple cycles, the generator receives more informative gradients that help it produce more realistic and subtle perturbations, addressing the instability inherent in adversarial training. Core assumption: Multiple cycles of counterfactual generation provide meaningful gradient information that improves generator performance without causing gradient explosion or vanishing.

### Mechanism 3
The discriminator's "fakeness" value (pfake) serves as a reliable uncertainty measure for model predictions. Since pfake indicates how likely an image is to be generated (adversarially perturbed), it correlates with prediction confidence - images with high pfake values are likely to be uncertain predictions or adversarial examples. Core assumption: The discriminator learns to associate generated images with uncertainty, and this learned behavior transfers to real images with uncertain predictions.

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: The framework fundamentally relies on GAN architecture where a generator creates counterfactuals and a discriminator evaluates them.
  - Quick check question: What is the primary training objective for the generator in a standard GAN setup?

- Concept: Adversarial training and robustness
  - Why needed here: The method uses adversarial examples to improve model robustness against perturbations.
  - Quick check question: How does adversarial training typically improve model robustness?

- Concept: Image-to-image translation
  - Why needed here: The generator must transform images from one class to another while maintaining realism.
  - Quick check question: What is the key difference between image-to-image translation and standard image generation?

## Architecture Onboarding

- Component map:
  Generator (G) -> Discriminator (D) -> Combined loss computation -> Backpropagation to both G and D

- Critical path:
  1. Input image passes through generator to create counterfactual
  2. Both real and counterfactual images pass through discriminator
  3. Discriminator outputs class probabilities and fakeness score
  4. Losses are computed and gradients flow back to update both generator and discriminator

- Design tradeoffs:
  - Using a single discriminator for both classification and fake detection vs separate models
  - Cycle-consistent loss for stability vs increased computational cost
  - L1 regularization for sparsity vs potential loss of detail in counterfactuals

- Failure signatures:
  - Mode collapse: Generator produces limited variety of counterfactuals
  - Discriminator overpowers generator: Generator fails to create realistic counterfactuals
  - Poor correlation between pfake and uncertainty: Uncertainty estimation fails

- First 3 experiments:
  1. Train G and D without cycle consistency on a small dataset to verify basic functionality
  2. Add cycle consistency with 1 cycle to test stability improvements
  3. Evaluate pfake correlation with prediction loss on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed unified framework outperform specialized models in terms of both interpretability and robustness on more diverse datasets?
- Basis in paper: [explicit] The authors demonstrate the method's efficacy on two datasets (CASC IFW and Concrete Crack Segmentation) and note that their method achieves competitive results compared to specialized models.
- Why unresolved: The evaluation is limited to two specific datasets. The generalizability of the framework to other domains or more complex image classification tasks remains untested.
- What evidence would resolve it: Testing the framework on a broader range of datasets with varying characteristics (e.g., medical imaging, natural scenes) and comparing its performance against state-of-the-art specialized models for interpretability and robustness.

### Open Question 2
- Question: How does the framework's performance scale with increasing dataset size and complexity?
- Basis in paper: [inferred] The authors mention the potential of assembling larger and more diverse datasets to narrow the gap between segmentation and attribution methods. However, they do not explore the impact of dataset size on the framework's performance.
- Why unresolved: The experiments are conducted on relatively small datasets. The framework's ability to handle large-scale, complex datasets and maintain its performance is unknown.
- What evidence would resolve it: Evaluating the framework on progressively larger datasets with increasing complexity and analyzing its performance trends. Investigating the computational resources and training time required for larger datasets.

### Open Question 3
- Question: Can the framework be extended to handle multi-class classification tasks effectively?
- Basis in paper: [explicit] The authors acknowledge the limitation of the current implementation to binary classification tasks and suggest breaking multi-class problems into several binary classification objectives.
- Why unresolved: The effectiveness of this approach for multi-class tasks is not explored. The potential challenges and limitations of extending the framework to handle more complex classification scenarios are not addressed.
- What evidence would resolve it: Implementing and evaluating the framework on multi-class datasets using the proposed approach of breaking them into binary tasks. Comparing the performance and interpretability of the results with other multi-class explanation methods.

## Limitations
- Limited evaluation to small, domain-specific datasets (apples and concrete cracks)
- Moderate correlation (0.081-0.100) between discriminator's "fakeness" value and prediction uncertainty
- Lack of extensive ablation studies to quantify individual component contributions

## Confidence

**High Confidence:**
- The framework successfully generates counterfactual images that can serve as saliency masks
- The discriminator's pfake value correlates positively with prediction uncertainty
- The method achieves competitive classification performance (F1-scores >0.98)

**Medium Confidence:**
- The cycle-consistent loss meaningfully improves training stability
- The method provides better robustness against PGD attacks compared to conventional classifiers
- The generated saliency masks achieve reasonable IoU scores despite not using pixel-level supervision

**Low Confidence:**
- The framework generalizes well to domains beyond the tested datasets
- The uncertainty estimation via pfake is practically useful for real-world applications
- The improvements in robustness and interpretability justify the increased model complexity

## Next Checks

1. **Generalization Testing:** Evaluate the framework on a larger, more diverse dataset (e.g., CIFAR-10 or ImageNet) to assess whether the observed benefits hold for more complex image classification tasks.

2. **Ablation Studies:** Systematically remove or modify key components (cycle consistency, noise injection, auxiliary classification) to quantify their individual contributions to performance, robustness, and interpretability.

3. **Uncertainty Calibration:** Test the discriminator's pfake value as an uncertainty measure by measuring calibration error (e.g., Expected Calibration Error) and evaluating its effectiveness in selective prediction or active learning scenarios.