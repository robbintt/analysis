---
ver: rpa2
title: Investigating semantic subspaces of Transformer sentence embeddings through
  linear structural probing
arxiv_id: '2310.11923'
source_url: https://arxiv.org/abs/2310.11923
tags:
- language
- computational
- linguistics
- association
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies structural semantic probing to analyze sentence
  embeddings from different Transformer-based models, including encoder-only, decoder-only,
  and encoder-decoder architectures. The method involves finding low-dimensional subspaces
  in the embedding space that preserve task-specific distances between sentence pairs,
  enabling analysis of semantic information across model layers.
---

# Investigating semantic subspaces of Transformer sentence embeddings through linear structural probing

## Quick Facts
- arXiv ID: 2310.11923
- Source URL: https://arxiv.org/abs/2310.11923
- Reference count: 20
- Primary result: Structural probing reveals how different Transformer architectures encode semantic information across layers, with encoder-only models excelling at STS and decoder-only models at NLI

## Executive Summary
This paper introduces structural semantic probing as a method to analyze sentence embeddings from Transformer-based models, focusing on identifying low-dimensional subspaces that preserve task-specific semantic relationships. The approach learns linear projection matrices that map high-dimensional embeddings to subspaces where distances reflect semantic similarity or dissimilarity. Experiments across seven model families (BERT, RoBERTa, ELECTRA, T5, GPT-2, Llama, OPT) on semantic textual similarity and natural language inference tasks reveal substantial differences in how architectures structure semantic information across layers, with most results showing remarkable model-size invariance.

## Method Summary
The method involves learning a projection matrix M that maps sentence embeddings to a lower-dimensional subspace where Euclidean distances correspond to semantic relationships. For STS tasks, the probe minimizes the difference between projected distances and similarity scores using a loss function that penalizes deviations from human-annotated similarities. For NLI tasks, a triplet loss formulation encourages positive sentence pairs to be closer in the projected space than negative pairs. The probe is trained with AdamW optimizer (learning rate 1e-5) for 300 epochs with early stopping on STS and 5 epochs on NLI. Performance is evaluated by computing Spearman correlation between projected distances and gold labels for STS, and accuracy for NLI across different projection dimensions and model layers.

## Key Results
- Encoder-only models (BERT, RoBERTa, ELECTRA) achieve best STS performance using middle-to-upper layers, while decoder-only models (GPT-2, Llama, OPT) excel at NLI using the last layer
- T5 models show best performance in the very last layer with some informativity loss in middle layers
- Model size has minimal effect on layer-wise performance patterns, though larger models achieve slightly better absolute performance on NLI
- ELECTRA uniquely shows performance degradation in upper layers across all tasks, unlike other architectures
- Decoder-only models exhibit cyclic performance patterns across layers, with fluctuations that suggest alternating information processing stages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structural semantic probing identifies low-dimensional subspaces in sentence embeddings that preserve task-specific distances between sentence pairs.
- Mechanism: A projection matrix M is learned to minimize the difference between Euclidean distances in the projected space and task-specific similarity/dissimilarity labels.
- Core assumption: Semantic information is linearly separable in the embedding space when projected onto certain subspaces.
- Evidence anchors:
  - [abstract] "finding a subspace of the embedding space that provides suitable task-specific pairwise distances between data-points"
  - [section 2] "we aim to find a projection matrix M to a lower-dimensional space, such that we can directly 'read off' the answer from the application of the matrix to elements of R"
  - [corpus] Weak - corpus only provides related papers, not direct evidence for this mechanism
- Break condition: If semantic information is not linearly separable or if the embedding space lacks sufficient structure for the task.

### Mechanism 2
- Claim: Different model architectures and training regimes structure semantic information differently across layers.
- Mechanism: The performance of structural probing varies by layer and architecture, revealing how each model type encodes semantic information.
- Core assumption: The training objective and architecture influence how semantic information is distributed across layers.
- Evidence anchors:
  - [abstract] "model families differ substantially in their performance and layer dynamics"
  - [section 4.2.1] "encoder-only MLMs... best results are achieved using the same setup: extracting representations from the layers close to the last one" vs. "causal models... most informative in the last layer"
  - [corpus] Weak - corpus only provides related papers, not direct evidence for this mechanism
- Break condition: If all architectures encoded semantic information identically across layers, or if layer position had no effect on probing performance.

### Mechanism 3
- Claim: The informativeness of sentence embeddings for semantic tasks is largely model-size invariant.
- Mechanism: Models of different sizes within the same architecture family show similar patterns of semantic information distribution across layers.
- Core assumption: Scaling model parameters doesn't fundamentally change how semantic information is structured within the architecture.
- Evidence anchors:
  - [abstract] "the results are largely model-size invariant"
  - [section 4.2.1] "While bigger models perform better, the distribution of the semantic information across layers is very similar across model sizes"
  - [section 4.2.1] "all T5-efficient models attain the best performance in the very last layer and show some loss of informativity in middle layers"
  - [corpus] Weak - corpus only provides related papers, not direct evidence for this mechanism
- Break condition: If performance patterns across layers changed dramatically with model size, or if larger models consistently showed fundamentally different information structures.

## Foundational Learning

- Concept: Linear algebra and projection matrices
  - Why needed here: The method relies on learning a projection matrix M that maps high-dimensional embeddings to lower-dimensional subspaces where distances are meaningful for semantic tasks
  - Quick check question: Can you explain why multiplying an embedding by a projection matrix reduces its dimensionality?

- Concept: Distance metrics and similarity measures
  - Why needed here: The method uses Euclidean distance and cosine similarity to measure semantic relationships between sentences in the projected space
  - Quick check question: What's the difference between Euclidean distance and cosine similarity, and when would you use each?

- Concept: Optimization and loss functions
  - Why needed here: The projection matrix is learned by minimizing a loss function that measures the discrepancy between projected distances and task labels
  - Quick check question: Can you write the loss function for the STS task and explain what each term represents?

## Architecture Onboarding

- Component map:
  Input sentence pairs with semantic labels -> Pre-trained transformer models -> Linear projection layer -> Task-specific loss function -> Optimizer -> Performance metrics

- Critical path:
  1. Extract sentence representations from pre-trained models
  2. Project representations to lower-dimensional subspaces
  3. Compute distances in projected space
  4. Compare distances to task labels
  5. Optimize projection matrix to minimize discrepancy

- Design tradeoffs:
  - Dimensionality of projection space: Higher dimensions capture more information but risk overfitting and computational cost
  - Layer selection: Different layers capture different types of information; optimal layer varies by model and task
  - Loss function choice: Triplet loss vs. cosine similarity affects how well the probe captures semantic relationships

- Failure signatures:
  - Low performance across all dimensions and layers suggests embeddings lack semantic structure
  - Performance peaking at very high dimensions suggests overfitting or that the probe is learning task-specific noise
  - Inconsistent results across runs suggest unstable optimization or insufficient training

- First 3 experiments:
  1. Apply structural probing to BERT base on STS benchmark with projection dimensions 32, 64, 128
  2. Compare performance across BERT, RoBERTa, and ELECTRA on STS benchmark
  3. Analyze how performance changes across layers for a single model on STS benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ELECTRA's unique training objective affect its layer-wise semantic representation patterns compared to traditional masked language models?
- Basis in paper: [explicit] The paper notes that ELECTRA shows degradation in performance in upper layers across all tasks, unlike BERT and RoBERTa which peak in higher layers.
- Why unresolved: The paper identifies this unusual pattern but does not investigate the underlying reasons for why ELECTRA's semantic representations degrade in upper layers.
- What evidence would resolve it: Analysis of how ELECTRA's discriminator-based training objective affects information flow and representation quality across layers, particularly comparing it to masked language model training.

### Open Question 2
- Question: What causes the cyclic performance patterns observed in decoder-only language models across layers?
- Basis in paper: [explicit] The paper observes clear cyclic development in performance across layers for GPT-2 and similar patterns in Llama and OPT models, but does not explain this phenomenon.
- Why unresolved: While the paper documents the cyclic pattern, it does not investigate the architectural or training-related reasons behind this layer-wise oscillation.
- What evidence would resolve it: Investigation into how attention patterns, feed-forward network operations, or residual connections in decoder-only architectures contribute to cyclic information processing across layers.

### Open Question 3
- Question: Why do extremely small causal language models like OPT 125m outperform larger models on semantic textual similarity tasks?
- Basis in paper: [explicit] The paper observes that OPT 125m shows steady performance increase and outperforms all larger models on STS, nearly reaching T5 results.
- Why unresolved: The paper notes this surprising result but does not explore why minimal parameter counts lead to better semantic representation learning in this context.
- What evidence would resolve it: Comparative analysis of how model size affects the capacity to learn semantic representations versus overfitting to superficial patterns in the training data.

## Limitations
- The method assumes linear separability of semantic information, which may not hold for all semantic relationships
- Analysis focuses exclusively on English language tasks, limiting cross-linguistic generalizability
- The paper doesn't investigate whether learned subspaces capture interpretable linguistic phenomena versus task-specific correlations

## Confidence
- High confidence: Encoder-only vs decoder-only performance differences on STS/NLI tasks
- Medium confidence: Model-size invariance of semantic information distribution
- Medium confidence: Layer-specific optimization patterns varying by architecture

## Next Checks
1. Apply structural probing to additional semantic tasks (e.g., semantic role labeling, coreference resolution) to test generalizability beyond STS and NLI.
2. Test the probing method on emerging model architectures like hybrid encoder-decoder models (e.g., GLM, UL2) and vision-language models to assess framework extensibility.
3. Conduct qualitative analysis of the learned subspaces to determine if they capture linguistically meaningful dimensions (e.g., tense, negation, quantification) rather than just task-specific correlations.