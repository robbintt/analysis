---
ver: rpa2
title: Predicting Group Choices from Group Profiles
arxiv_id: '2308.03083'
source_url: https://arxiv.org/abs/2308.03083
tags:
- group
- choice
- data
- profile
- profiles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the problem of predicting group choices in\
  \ recommender systems by aggregating individual preferences into group profiles\
  \ and leveraging machine learning to predict the group\u2019s final choice. The\
  \ core idea is to use a machine learning approach, specifically Multinomial Logistic\
  \ Regression, to predict group choices from group profiles constructed using preference\
  \ aggregation strategies like Average, Multiplicative, Least Misery, SDS1, SDS3,\
  \ and Copeland."
---

# Predicting Group Choices from Group Profiles

## Quick Facts
- arXiv ID: 2308.03083
- Source URL: https://arxiv.org/abs/2308.03083
- Reference count: 34
- One-line primary result: A machine learning method (LCP) predicts group choices more accurately than classical aggregation strategies by learning inter-group interaction patterns from observed data.

## Executive Summary
This paper introduces a machine learning approach to predict group choices in recommender systems by aggregating individual preferences into group profiles and training a classifier to map profiles to choices. The method, Learning-based Choice Prediction (LCP), uses Multinomial Logistic Regression and outperforms classical preference aggregation strategies (PACP) in accuracy. LCP is validated on a dataset of 79 groups with 282 participants and shows robustness to missing data while achieving superior performance to human predictions. The study also introduces data augmentation techniques with synthetic group profiles to improve prediction accuracy and choice distribution similarity.

## Method Summary
The method constructs group profiles by aggregating individual preferences using strategies like Average, Multiplicative, Least Misery, SDS1, SDS3, and Copeland. These profiles are paired with actual group choices and used to train a Multinomial Logistic Regression classifier. The trained model predicts group choices for new groups. Data augmentation with synthetic profiles (Winners and Permutations) is applied to address data scarcity and improve accuracy. The method is evaluated using four-fold cross-validation and compared against baseline PACP methods.

## Key Results
- LCP achieves nearly 50% accuracy in predicting group choices, outperforming baseline PACP methods
- LCP is robust to missing preference data and outperforms human predictions on the group choice prediction task
- Data augmentation with synthetic group profiles improves prediction accuracy and makes predicted choice distribution more similar to observed distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Group choice can be predicted more accurately than classical aggregation strategies by learning the effect of inter-group interactions from observed data.
- Mechanism: Individual preferences are aggregated into group profiles using various preference aggregation strategies (e.g., Average, Multiplicative, Least Misery, SDS variants). These profiles, paired with actual group choices, are used to train a machine learning classifier (Multinomial Logistic Regression) that learns a mapping from profile patterns to group choice.
- Core assumption: The group choice depends on the relative pattern of aggregated preferences rather than the specific items themselves, and this relationship can be learned from observational data.
- Evidence anchors:
  - [abstract] "we search for a group profile definition that, in conjunction with a machine learning model, can be used to accurately predict a group choice"
  - [section] "SDS theory states that individual preferences are the ingredients of the group profile... and consensus processes act on such a group profile to yield a collective response (group choice)"
  - [corpus] Found 25 related papers, but only 8 have measurable similarity scores; corpus evidence is weak for direct confirmation of learned inter-group interaction patterns.
- Break condition: If the learned model fails to generalize beyond training data, or if group choice is primarily determined by factors not captured in individual preference profiles (e.g., social dynamics, time constraints).

### Mechanism 2
- Claim: Data augmentation with synthetic group profiles can improve prediction accuracy and make the predicted choice distribution more similar to the observed one.
- Mechanism: Two augmentation strategies are used: (1) "Winners" - synthetic groups where all members prefer the same option, and (2) "Permutations" - synthetic groups created by permuting the order of options in existing profiles, preserving relative preference patterns. These synthetic groups are added to the training set to increase data diversity and coverage.
- Core assumption: Synthetic groups represent realistic decision-making scenarios that, although not observed, should follow the same choice function as actual groups.
- Evidence anchors:
  - [abstract] "we propose two data augmentation methods, which add synthetic group profiles to the training data, and we hypothesise they can further improve the choice prediction accuracy"
  - [section] "we make some assumptions about the functional relationship between the group profile and the group choice. These assumptions are then translated into synthetic groups that were not actually observed but should be observed, given the assumptions"
  - [corpus] Weak direct evidence; corpus neighbors focus on LLM-generated recommendations rather than synthetic data augmentation.
- Break condition: If the synthetic profiles introduce noise that misleads the model, or if the assumptions about choice function do not hold for the specific domain.

### Mechanism 3
- Claim: The proposed method is robust to missing preference data and outperforms human prediction accuracy.
- Mechanism: The model can handle sparse user-option rating matrices by computing group scores only from available ratings, and it uses machine learning to learn patterns even when some individual preferences are missing. The method's accuracy is validated against human predictions on the same task.
- Core assumption: Missing data patterns do not fundamentally alter the underlying choice function, and the model can learn robust patterns even with incomplete information.
- Evidence anchors:
  - [abstract] "The method we propose is robust with the presence of missing preference data and achieves a performance superior to what human can achieve on the group choice prediction task"
  - [section] "we also aim to examine the validity of this hypothesis when the user-option matrix is sparse"
  - [corpus] No direct corpus evidence; this is a novel claim supported by the paper's experiments.
- Break condition: If missing data is not missing at random and correlates strongly with choice, or if human predictions capture non-preference factors not available to the model.

## Foundational Learning

- Concept: Preference aggregation strategies
  - Why needed here: The group profile is constructed by aggregating individual preferences using these strategies; understanding their differences is key to interpreting model inputs and outputs.
  - Quick check question: What is the difference between Average and Least Misery aggregation strategies?

- Concept: Social Decision Scheme (SDS) theory
  - Why needed here: The paper builds on SDS theory, which models how individual preferences are transformed into group choices through social influence patterns; this motivates the learning approach.
  - Quick check question: How does SDS theory differ from classical preference aggregation in predicting group choice?

- Concept: Data augmentation and synthetic data generation
  - Why needed here: The paper introduces synthetic group profiles (Winners and Permutations) to address data scarcity; understanding how these are generated and why is crucial for model training and evaluation.
  - Quick check question: What is the rationale behind adding "Winners" and "Permutations" synthetic group profiles to the training set?

## Architecture Onboarding

- Component map: User-option rating matrix -> Preference aggregation strategies (Average, Multiplicative, Least Misery, SDS1/SDS3, Copeland) -> Multinomial Logistic Regression classifier -> Predicted group choice

- Critical path:
  1. Aggregate individual preferences into group profiles using selected strategy
  2. Train ML model on profiles and actual group choices
  3. Optionally augment training data with synthetic profiles
  4. Predict group choice for new groups using trained model

- Design tradeoffs:
  - Profile strategy choice: Average aggregation yields best accuracy but may oversimplify; other strategies capture different group dynamics
  - Data augmentation: Winners improve accuracy but add few samples; Permutations add many but may introduce noise if assumptions are wrong
  - Model simplicity: Multinomial Logistic Regression is interpretable and fast but may miss complex patterns; more complex models could overfit given limited data

- Failure signatures:
  - Accuracy drops significantly when applied to new datasets or domains
  - Model performs well on training data but poorly on test data (overfitting)
  - Augmentation leads to worse performance, indicating synthetic profiles do not reflect real group behavior

- First 3 experiments:
  1. Compare LCP-AVE accuracy with PACP-AVE on the original dense dataset
  2. Test LCP robustness by gradually removing ratings and measuring accuracy drop
  3. Evaluate the impact of Winners vs Permutations augmentation on LCP-AVE accuracy and choice distribution similarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the LCP method be adapted to handle larger choice sets (more than 10 options) effectively?
- Basis in paper: [inferred] The paper notes that LCP is currently designed for small choice sets and mentions the need to design methods that can scale to larger numbers of options.
- Why unresolved: The paper does not provide solutions or experiments for scaling LCP to larger choice sets. It only acknowledges this as a limitation and suggests exploring different learning algorithms or modeling approaches.
- What evidence would resolve it: Experiments demonstrating LCP's performance on datasets with larger choice sets, or theoretical analysis showing how LCP could be modified to handle scalability issues.

### Open Question 2
- Question: What is the impact of using predicted ratings (instead of actual ratings) to handle extremely sparse user-option matrices in LCP?
- Basis in paper: [explicit] The paper mentions that an extension to deal with sparser ratings data is needed and suggests testing the effect of replacing missing ratings with ratings obtained from a rating prediction method.
- Why unresolved: The paper does not provide experiments or analysis on using predicted ratings to handle sparse data in LCP. It only proposes this as a potential direction for future work.
- What evidence would resolve it: Experiments comparing LCP's performance when using actual ratings versus predicted ratings in sparse matrices, or a study showing how rating prediction methods affect group choice prediction accuracy.

### Open Question 3
- Question: How does LCP perform on datasets from different application domains (e.g., music, video) compared to the tourism domain used in this study?
- Basis in paper: [inferred] The paper acknowledges the need to test LCP on datasets from other domains to increase the generality of the results. It suggests that data sets with information about individual preferences and corresponding group choices are needed from various domains.
- Why unresolved: The paper only uses a single dataset from the tourism domain. It does not provide any evidence of LCP's performance on other types of choice tasks or domains.
- What evidence would resolve it: Experiments applying LCP to datasets from different domains (e.g., music, video) and comparing its performance across these domains to the tourism domain results.

## Limitations

- Limited dataset size: The method is validated on a single dataset of 79 groups, which may not capture the full diversity of group decision-making scenarios
- Synthetic profile assumptions: The effectiveness of data augmentation relies on assumptions about choice functions that may not generalize across domains
- Single domain validation: The method is only tested on tourism domain data, raising questions about performance in other application areas

## Confidence

High confidence in the specific methodology and experimental results presented, Medium confidence in generalizability across domains and datasets, Low confidence in scalability to larger choice sets.

## Next Checks

1. Test the LCP method on multiple datasets with varying group sizes and composition patterns to assess generalizability
2. Conduct ablation studies to quantify the specific contribution of each data augmentation strategy (Winners vs Permutations)
3. Evaluate human prediction accuracy using the same group profile information available to the model to verify the claimed superiority