---
ver: rpa2
title: Language Models Represent Space and Time
arxiv_id: '2310.02207'
source_url: https://arxiv.org/abs/2310.02207
tags:
- arxiv
- preprint
- data
- time
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides evidence that large language models (LLMs)
  learn rich spatiotemporal representations of the real world, beyond just memorizing
  superficial statistics. The authors analyze the learned representations of three
  spatial datasets (world, US, NYC places) and three temporal datasets (historical
  figures, artworks, news headlines) in the Llama-2 family of models.
---

# Language Models Represent Space and Time

## Quick Facts
- arXiv ID: 2310.02207
- Source URL: https://arxiv.org/abs/2310.02207
- Reference count: 22
- One-line primary result: Large language models learn rich linear spatiotemporal representations that are robust, unified across entity types, and include individual space/time neurons

## Executive Summary
This paper provides evidence that large language models (LLMs) learn rich spatiotemporal representations of the real world beyond mere memorization. The authors analyze Llama-2 family models across three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines). They discover that LLMs learn linear representations of space and time across multiple scales that are robust to prompting variations and unified across different entity types. The study identifies individual "space neurons" and "time neurons" that reliably encode spatial and temporal coordinates, suggesting that modern LLMs acquire structured knowledge about fundamental dimensions such as space and time.

## Method Summary
The authors constructed six datasets containing entities with known spatial or temporal coordinates and trained linear regression probes on the internal activations of Llama-2 models (7B, 13B, 70B parameters). For each entity, they ran the name through the model and saved the hidden state on the last entity token at each layer. Linear probes were trained to predict latitude/longitude coordinates for spatial data or years for temporal data. The authors compared linear probe performance against nonlinear MLP probes, tested robustness across different prompts, and examined whether representations were unified across entity types by holding out entire classes during training.

## Key Results
- LLMs learn linear representations of space and time that achieve high R² values (0.5-0.8) across multiple scales and datasets
- These representations are robust to prompting variations and show strong generalization across different entity types
- Individual neurons with high cosine similarity to probe directions reliably encode spatial and temporal coordinates
- Dimensionality reduction experiments show that most spatial/temporal information is concentrated in the first few principal components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs learn linear spatiotemporal representations that are structured across multiple scales
- Mechanism: During next-token prediction training, the model compresses correlated spatial/temporal information into low-dimensional linear features distributed across early-to-mid layers, enabling robust decoding via simple linear probes
- Core assumption: The model treats space and time as continuous dimensions that can be represented in a low-dimensional linear subspace
- Evidence anchors:
  - [abstract] "We discover that LLMs learn linear representations of space and time across multiple scales"
  - [section 3.2] "Table 2 reports our results and shows that using nonlinear probes results in minimal improvement to R2 for any dataset or model"
  - [corpus] Weak - corpus papers discuss spatial-temporal learning but focus on trajectory modeling rather than LLM internal representations
- Break condition: If spatial/temporal information is encoded nonlinearly or in high-dimensional manifolds, linear probes would fail to capture the structure

### Mechanism 2
- Claim: Individual neurons act as interpretable feature detectors for space and time coordinates
- Mechanism: The model distributes spatial/temporal information across multiple neurons, with some neurons having high cosine similarity to the probe directions, enabling direct decoding without probe supervision
- Core assumption: Neurons can encode interpretable, distributed features even without explicit supervision
- Evidence anchors:
  - [section 5] "we search for individual neurons with input or output weights that have high cosine similarity with the learned probe direction"
  - [section 5] "these neurons are indeed highly sensitive to the true location of entities in space or time"
  - [corpus] Weak - corpus papers focus on spatial-temporal graph networks rather than individual neuron analysis in LLMs
- Break condition: If features are distributed in superposition across many neurons rather than concentrated in individual detectors

### Mechanism 3
- Claim: Spatial and temporal representations are unified across different entity types through shared coordinate systems
- Mechanism: The model learns a common geometric space where different entities (cities, landmarks, historical figures) map to consistent spatial/temporal coordinates regardless of their type
- Core assumption: The model can represent diverse entities in a unified geometric framework without type-specific features
- Evidence anchors:
  - [abstract] "These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks)"
  - [section 4.1.2] "We distinguish these hypotheses by training a series of probes where the train-test split is performed to hold out all points of a particular entity class"
  - [corpus] Weak - corpus papers discuss heterogeneous graphs but not unified representations across entity types in LLMs
- Break condition: If the model uses separate coordinate systems for different entity types or if type-specific features dominate representation

## Foundational Learning

- Concept: Linear regression and ridge regularization
  - Why needed here: The probing methodology relies on fitting linear models to predict spatial/temporal coordinates from model activations
  - Quick check question: What happens to the probe weights when λ→0 versus λ→∞ in ridge regression?

- Concept: Principal component analysis (PCA)
  - Why needed here: The dimensionality reduction experiments project activations onto principal components to test how many dimensions are needed for good spatial/temporal decoding
  - Quick check question: If the first k principal components capture most spatial variance, what does this imply about the structure of the representation?

- Concept: Spearman rank correlation
  - Why needed here: Used to evaluate the quality of ordinal predictions (ordering of locations/times) rather than absolute values, capturing whether the model learns the correct geometry
  - Quick check question: Why might Spearman correlation be more appropriate than R² for evaluating spatial/temporal representations?

## Architecture Onboarding

- Component map: Token → Embedding → Attention layers → MLP layers → Residual connections → Final activation → Probe prediction
- Critical path: Token → Embedding → Attention layers → MLP layers → Final activation → Probe prediction
- Design tradeoffs: Linear vs nonlinear probes (simplicity vs expressivity), number of principal components vs probe accuracy, prompt engineering vs representation robustness
- Failure signatures: Poor probe performance indicates missing spatial/temporal information; entity-specific probe failures suggest lack of unified representation; sensitivity to prompting indicates fragile representations
- First 3 experiments:
  1. Train linear probes on world dataset at different layers to find where spatial representations emerge
  2. Compare linear vs MLP probe performance to test linearity hypothesis
  3. Test probe generalization across entity types to verify unified representation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do spatial and temporal representations in LLMs evolve during training, and at what point do they become structured?
- Basis in paper: [inferred] The authors suggest investigating training checkpoints to identify when models organize features into coherent geometries, but this analysis was not conducted.
- Why unresolved: The paper only analyzes a trained model's representations, not the developmental trajectory during training.
- What evidence would resolve it: Analyzing spatial/temporal probe performance across multiple training checkpoints to identify when and how these representations emerge.

### Open Question 2
- Question: Do spatial and temporal representations in LLMs follow a discretized hierarchical mesh structure as conjectured by the authors?
- Basis in paper: [explicit] The authors conjecture that the most canonical form of spatial/temporal structure is a discretized hierarchical mesh but did not test this hypothesis.
- Why unresolved: The authors only demonstrated linear decodability of absolute positions, not the underlying representation structure.
- What evidence would resolve it: Extracting and analyzing the basis points and their organization across different scales of granularity in the model's internal representations.

### Open Question 3
- Question: How do LLMs use spatial and temporal representations during inference for downstream tasks?
- Basis in paper: [explicit] The authors note their analysis doesn't show if models actually use these features, and preliminary experiments showed difficulty answering spatial/temporal relation questions.
- Why unresolved: The paper focuses on representation existence rather than usage patterns during generation or reasoning.
- What evidence would resolve it: Conducting causal intervention studies to measure how manipulating spatial/temporal representations affects model outputs on relevant tasks.

## Limitations

- Linear probe methodology may miss nonlinear or distributed representations that are equally important
- Datasets rely on Wikipedia-derived statistics that may not represent true spatial/temporal information
- Probing methodology cannot distinguish between explicit coordinate representations versus learned statistical associations

## Confidence

- Confidence is Medium for the claim that LLMs learn linear spatiotemporal representations
- Confidence is Medium for the claim about unified representations across entity types
- Confidence is High for the existence of space and time neurons

## Next Checks

1. **Nonlinear probe comparison**: Systematically compare linear probes against more complex architectures (deep MLPs, attention-based probes) across all six datasets to determine if linear representations are truly optimal or if more complex structures exist.

2. **Cross-model consistency**: Apply the same probing methodology to other LLM families (GPT, Claude, Mistral) to test whether spatiotemporal representation learning is a universal phenomenon or specific to Llama-2's training approach.

3. **Ablation on training data**: Conduct controlled experiments where models are trained with and without explicit spatial/temporal context in their pretraining data to determine whether these representations emerge spontaneously or require specific training signals.