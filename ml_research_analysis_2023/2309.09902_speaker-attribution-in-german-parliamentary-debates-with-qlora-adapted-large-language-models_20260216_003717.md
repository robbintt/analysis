---
ver: rpa2
title: Speaker attribution in German parliamentary debates with QLoRA-adapted large
  language models
arxiv_id: '2309.09902'
source_url: https://arxiv.org/abs/2309.09902
tags:
- cues
- roles
- role
- language
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We investigated the potential of large language models (LLMs) to
  automate speaker attribution in German parliamentary debates. We fine-tuned the
  Llama 2 70B model using the QLoRA parameter-efficient training strategy.
---

# Speaker attribution in German parliamentary debates with QLoRA-adapted large language models

## Quick Facts
- arXiv ID: 2309.09902
- Source URL: https://arxiv.org/abs/2309.09902
- Reference count: 7
- We fine-tuned Llama 2 with QLoRA, an efficient training strategy, and achieved competitive performance on the GermEval 2023 Shared Task.

## Executive Summary
This paper investigates the use of large language models (LLMs) for automated speaker attribution in German parliamentary debates. The authors fine-tune the Llama 2 70B model using QLoRA, a parameter-efficient training strategy, to detect cues and predict roles in speech events. Their approach achieves competitive performance on the GermEval 2023 Shared Task, with F1 scores of 0.889 for cue detection and 0.804 for role prediction. The results demonstrate the feasibility of using LLMs for speaker attribution and suggest promising avenues for semantic role labeling tasks.

## Method Summary
The authors fine-tuned the Llama 2 70B model using QLoRA, an efficient training strategy that achieves similar performance to full fine-tuning while using only a fraction of the memory. They used separate models for cue detection and role prediction, with specific prompt templates in English to leverage the multilingual capabilities of Llama 2. The models were trained on the GermEval 2023 Shared Task dataset, which consists of German parliamentary speeches from 2017-2021. The authors employed a preprocessing pipeline to split the speeches into samples and elements using spaCy, and postprocessing to handle model outputs.

## Key Results
- Achieved F1 scores of 0.889 for cue detection and 0.804 for role prediction on the GermEval 2023 Shared Task
- Demonstrated the feasibility of using LLMs for speaker attribution in German parliamentary debates
- Showed that QLoRA is an effective parameter-efficient training strategy for fine-tuning large language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning Llama 2 with QLoRA preserves general language understanding while adapting to speaker attribution task
- Mechanism: QLoRA quantizes model weights to 4 bits and freezes them, training only low-rank adapters (LoRA layers). This reduces memory usage significantly while maintaining performance similar to full fine-tuning
- Core assumption: The pretrained Llama 2 weights contain sufficient general language understanding that can be adapted to speaker attribution through parameter-efficient fine-tuning
- Evidence anchors:
  - [abstract]: "We fine-tune Llama 2 with QLoRA, an efficient training strategy"
  - [section]: "QLoRA is a highly efficient fine-tuning technique for large language models that achieves similar performance to full fine-tuning while using only a fraction of the memory"
  - [corpus]: Weak - the paper doesn't provide comparative evidence between QLoRA and full fine-tuning
- Break condition: If the task requires significant architectural changes not captured by adapter weights, or if the pretrained knowledge is insufficient for the specific domain

### Mechanism 2
- Claim: Using English instructions with German text leverages multilingual capabilities of Llama 2
- Mechanism: The authors use English instructions in their prompt templates while processing German parliamentary speeches, following observed performance improvements when using English prompts with multilingual models
- Core assumption: Llama 2's multilingual capabilities are enhanced when provided with English prompts, even when processing non-English text
- Evidence anchors:
  - [section]: "We wrote the instructions in our prompt templates in English, because it was observed that the performance of multilingual models such as Llama 2 is improved when English prompts are used"
  - [section]: References Fu et al. (2022) and Huang et al. (2023) for this observation
  - [corpus]: Weak - the paper doesn't provide ablation studies comparing English vs German instructions
- Break condition: If the model's performance significantly degrades with English instructions, or if German-specific linguistic features are critical and lost through English instructions

### Mechanism 3
- Claim: The task structure (cue and role detection) aligns with semantic role labeling capabilities of LLMs
- Mechanism: The authors frame speaker attribution as detecting cue words that invoke speech events and identifying roles (source, message, addressee) associated with those events, which is closely related to semantic role labeling
- Core assumption: LLMs pretrained on large text corpora have learned semantic relationships that can be leveraged for role labeling tasks
- Evidence anchors:
  - [abstract]: "automated speaker attribution, which detects who said what to whom in a speech event and is closely related to semantic role labeling"
  - [section]: "Semantic role labeling is considered a key component for natural language understanding and has been demonstrated to enhance systems for various applications"
  - [corpus]: Weak - the paper doesn't provide evidence that the pretrained Llama 2 model had learned semantic role labeling capabilities before fine-tuning
- Break condition: If the semantic relationships required for speaker attribution differ significantly from those captured during pretraining, or if the parliamentary debate domain contains specialized language not well-represented in pretraining data

## Foundational Learning

- Concept: QLoRA parameter-efficient fine-tuning
  - Why needed here: The Llama 2 70B model has approximately 70 billion parameters, making traditional fine-tuning computationally prohibitive for most hardware setups
  - Quick check question: How does QLoRA achieve memory efficiency compared to full fine-tuning, and what are the trade-offs?

- Concept: Semantic role labeling and its relationship to speaker attribution
  - Why needed here: The task involves identifying who said what to whom, which maps directly to semantic role labeling where predicates have arguments like agent, patient, etc.
  - Quick check question: What are the key differences between general semantic role labeling and the specific roles (Addr, Evidence, Medium, Message, Source, Topic, PTC) used in this speaker attribution task?

- Concept: Prompt engineering for instruction-tuned models
  - Why needed here: The authors use specific prompt templates with English instructions to guide the model's output format for cues and roles
  - Quick check question: Why might English instructions work better than German instructions for a German language task when using multilingual LLMs?

## Architecture Onboarding

- Component map: Text preprocessing → Cue model inference → Role model inference (per cue) → Postprocessing → Evaluation
- Critical path: Text preprocessing → Cue model inference → Role model inference (per cue) → Postprocessing → Evaluation
- Design tradeoffs: Using English instructions with German text trades potential language-specific nuance for improved multilingual model performance; QLoRA trades some fine-tuning capacity for memory efficiency and faster training
- Failure signatures: Poor cue detection leading to cascading errors in role prediction; model generating words not in the input text; model ignoring punctuation that should be included in roles
- First 3 experiments:
  1. Train cue model only on a small subset of data with default hyperparameters to verify training pipeline works
  2. Compare English vs German instructions on a validation set to verify the English instruction hypothesis
  3. Test different context window sizes (concatenating 0, 1, or 2 additional samples) to find optimal balance between context and computational efficiency

## Open Questions the Paper Calls Out
- How do different prompt strategies (e.g., English vs. German instructions) affect the performance of multilingual LLMs like Llama 2 in speaker attribution tasks?
- What are the potential risks and ethical considerations associated with using fine-tuned LLMs for automated speaker attribution in political discourse analysis?
- How does the performance of fine-tuned LLMs for speaker attribution compare to traditional rule-based or feature-engineered approaches, and what are the trade-offs between these methods?

## Limitations
- No ablation study comparing QLoRA against full fine-tuning to validate claimed efficiency benefits
- No comparison between English and German instruction prompts to verify multilingual prompt engineering hypothesis
- Postprocessing rules described at high level without implementation details, impacting reproducibility

## Confidence
- High Confidence: The technical feasibility of using LLMs for speaker attribution tasks
- Medium Confidence: The effectiveness of QLoRA as a parameter-efficient training strategy
- Medium Confidence: The benefit of using English instructions with German text for multilingual LLMs
- Medium Confidence: The alignment between speaker attribution and semantic role labeling capabilities

## Next Checks
1. Conduct a controlled experiment comparing English versus German instructions on a held-out validation set to empirically verify the multilingual prompt engineering hypothesis
2. Implement both QLoRA and full fine-tuning on a subset of the dataset, measuring not only final performance metrics but also training time, memory usage, and convergence behavior
3. Perform a detailed analysis of model errors on the development set, categorizing mistakes by type and examining whether errors correlate with specific linguistic features or structural patterns in the parliamentary debates