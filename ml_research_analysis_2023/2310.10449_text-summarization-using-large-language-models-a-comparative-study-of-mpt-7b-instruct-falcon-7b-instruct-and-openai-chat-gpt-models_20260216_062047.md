---
ver: rpa2
title: 'Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct,
  Falcon-7b-instruct, and OpenAI Chat-GPT Models'
arxiv_id: '2310.10449'
source_url: https://arxiv.org/abs/2310.10449
tags:
- summarization
- text
- llms
- datasets
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares three large language models (MPT-7b-instruct,
  Falcon-7b-instruct, and text-davinci-003) for text summarization using CNN/Daily
  Mail and XSum datasets. Experiments involved varying hyperparameters and evaluating
  outputs with BLEU, ROUGE, and BERT Scores.
---

# Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models

## Quick Facts
- arXiv ID: 2310.10449
- Source URL: https://arxiv.org/abs/2310.10449
- Reference count: 15
- Text-davinci-003 achieved the highest performance across all metrics, notably BLEU 0.49 and BERT F1 0.87 on CNN data

## Executive Summary
This study compares three large language models (MPT-7b-instruct, Falcon-7b-instruct, and text-davinci-003) for text summarization using CNN/Daily Mail and XSum datasets. Experiments involved varying hyperparameters and evaluating outputs with BLEU, ROUGE, and BERT Scores. Text-davinci-003 achieved the highest performance across all metrics, notably BLEU 0.49 and BERT F1 0.87 on CNN data, while Falcon and MPT showed lower scores. The work demonstrates the effectiveness of larger, more capable models for summarization and highlights their potential for business-oriented generative AI applications. Future work may explore higher-parameter models and domain-specific fine-tuning.

## Method Summary
The study employed three large language models (MPT-7b-instruct, Falcon-7b-instruct, and text-davinci-003) to generate summaries from CNN/Daily Mail and XSum datasets. Using LangChain and Hugging Face libraries, the models were configured with temperature 0.1 and max_tokens 100 to produce 25 summaries per model. Generated summaries were evaluated using BLEU, ROUGE (N and L), and BERT Scores to assess quality across multiple dimensions.

## Key Results
- Text-davinci-003 achieved BLEU score of 0.49 and BERT F1 of 0.87 on CNN/Daily Mail dataset
- MPT-7b-instruct and Falcon-7b-instruct showed lower performance across all metrics
- Model size and training data volume correlate with summarization quality
- Temperature and token length significantly influence summary creativity and coherence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger parameter models like text-davinci-003 outperform smaller fine-tuned models in summarization tasks due to richer contextual understanding and generation capabilities.
- Mechanism: The model's 175 billion parameters and extensive pretraining on 45TB of text data allow it to generate more fluent, coherent, and semantically accurate summaries by capturing deeper linguistic patterns and producing longer, more consistent outputs.
- Core assumption: Parameter count and training data volume directly correlate with summarization quality, especially in abstractive summarization.
- Evidence anchors:
  - [abstract]: "text-davinci-003 outperformed the others. This exceptional performance can be attributed to davinci being the largest and most powerful model, with 175 billion parameters and 45TB of text data."
  - [section]: "These findings underscore the significance of model architecture and size in text summarization tasks, as well as the potential of OpenAIâ€™s model for achieving state-of-the-art results in diverse NLP applications."
- Break condition: If fine-tuning on domain-specific data or using retrieval augmentation significantly closes the performance gap between smaller and larger models.

### Mechanism 2
- Claim: The choice of evaluation metrics (BLEU, ROUGE, BERT Score) provides a multi-dimensional view of summarization quality, capturing different aspects of fluency, content overlap, and semantic similarity.
- Mechanism: BLEU captures n-gram overlap, ROUGE evaluates content overlap and longest common subsequence, and BERT Score measures contextual embedding similarity, together providing a robust assessment of summary quality.
- Core assumption: No single metric fully captures summary quality; a combination is needed for comprehensive evaluation.
- Evidence anchors:
  - [abstract]: "To assess the quality and effectiveness of the generated summaries, we employed a set of widely accepted evaluation metrics: BLEU Score, ROUGE Score, and BERT Score."
  - [section]: "By calculating these metrics for summaries generated with different LLMs, we aim to provide a comprehensive assessment of their performance."
- Break condition: If newer evaluation methods (e.g., reference-free or human preference-based metrics) show better correlation with human judgment than this metric combination.

### Mechanism 3
- Claim: Temperature and token length hyperparameters significantly influence the balance between creativity and coherence in generated summaries.
- Mechanism: Lower temperature (0.1) reduces randomness and produces more deterministic, focused summaries, while the 100-token limit ensures concise outputs that meet summarization requirements without excessive verbosity.
- Core assumption: Consistent hyperparameter settings across models enable fair comparison of their inherent capabilities.
- Evidence anchors:
  - [section]: "For each LLM, experiments were conducted using a temperature value of 0.1 and a maximum token length of 100."
  - [corpus]: No direct corpus evidence found for this specific hyperparameter choice; assumption based on standard summarization practices.
- Break condition: If optimal hyperparameters vary significantly across models or datasets, making direct comparison less meaningful.

## Foundational Learning

- Concept: Large Language Models and their architecture
  - Why needed here: Understanding how LLMs generate text through transformer architectures is essential to interpret why different models perform differently in summarization tasks.
  - Quick check question: What is the key architectural difference between decoder-only transformers (used in summarization models) and encoder-decoder transformers?

- Concept: Text summarization techniques (abstractive vs extractive)
  - Why needed here: The study focuses on abstractive summarization, which requires understanding how models generate new text versus selecting existing sentences.
  - Quick check question: How does abstractive summarization differ from extractive summarization in terms of the source text usage?

- Concept: Evaluation metrics for text generation
  - Why needed here: Interpreting the BLEU, ROUGE, and BERT Score results requires understanding what each metric measures and their limitations.
  - Quick check question: What is the main difference between ROUGE-N and ROUGE-L metrics?

## Architecture Onboarding

- Component map:
  - Datasets (CNN/Daily Mail, XSum) -> LLMs (MPT-7b-instruct, Falcon-7b-instruct, text-davinci-003) -> Evaluation (BLEU, ROUGE, BERT Scores) -> Infrastructure (Google Compute Engine with NVIDIA T4 GPUs)

- Critical path:
  1. Load dataset and preprocess text
  2. Configure LLM with temperature=0.1 and max_tokens=100
  3. Generate summaries using LangChain/Hugging Face pipeline
  4. Calculate evaluation metrics
  5. Compare results across models and datasets

- Design tradeoffs:
  - Model choice: Larger models provide better quality but higher cost and latency
  - Dataset selection: CNN/Daily Mail offers more context while XSum requires extreme conciseness
  - Metric selection: Traditional metrics may not fully capture human judgment of summary quality

- Failure signatures:
  - Low BLEU/ROUGE scores with high BERT scores: Model captures meaning but uses different wording
  - Consistently low scores across all metrics: Model may not understand the task or dataset
  - High variance in scores across samples: Temperature or prompt engineering issues

- First 3 experiments:
  1. Generate summaries with default parameters to establish baseline performance
  2. Vary temperature (0.1, 0.5, 1.0) to observe creativity vs coherence tradeoff
  3. Test different max_token lengths (50, 100, 150) to find optimal summary length

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does increasing the number of parameters in LLMs (e.g., moving from 7B to 30B or 40B models) affect text summarization performance on CNN/Daily Mail and XSum datasets?
- Basis in paper: [explicit] The paper suggests exploring larger-parameter models like mosaicml/mpt-30b-instruct and tiiuae/falcon-40b-instruct for future enhancements.
- Why unresolved: The study only evaluated 7B-parameter models (MPT-7b-instruct, Falcon-7b-instruct) and did not test higher-parameter models.
- What evidence would resolve it: Experimental results comparing summarization metrics (BLEU, ROUGE, BERT Scores) between 7B and higher-parameter models on the same datasets.

### Open Question 2
- Question: How does domain-specific fine-tuning of LLMs impact summarization performance for specialized content types?
- Basis in paper: [explicit] The paper mentions that fine-tuning LLMs on specific domains and datasets could unlock potential for domain-specific summarization models.
- Why unresolved: The study used general-purpose LLMs without domain-specific fine-tuning.
- What evidence would resolve it: Comparative evaluation of general LLMs vs. domain-specific fine-tuned models on specialized datasets, measuring improvements in summarization quality metrics.

### Open Question 3
- Question: What is the optimal balance between abstractive and extractive summarization approaches when using LLMs?
- Basis in paper: [inferred] The paper discusses both abstractive and extractive summarization methods but does not compare their effectiveness when implemented with LLMs.
- Why unresolved: The study did not explicitly compare these two approaches or investigate their hybrid potential.
- What evidence would resolve it: Direct comparison of abstractive vs. extractive vs. hybrid summarization approaches using LLMs, evaluated across multiple metrics and datasets.

## Limitations
- Study uses only two datasets (CNN/Daily Mail and XSum), limiting generalizability to other summarization tasks
- Evaluation relies exclusively on automated metrics without human judgment validation
- Hyperparameter configuration was kept constant across models, which may not represent optimal settings for each model-dataset combination

## Confidence
- High confidence: The core finding that text-davinci-003 outperforms the 7B parameter models on the tested datasets using the specified metrics
- Medium confidence: The attribution of performance differences primarily to model size and training data volume
- Low confidence: The broader claims about "potential for achieving state-of-the-art results in diverse NLP applications" extending beyond the specific summarization experiments

## Next Checks
1. Conduct blind human assessments comparing summaries from all three models across multiple quality dimensions (informativeness, coherence, fluency, and factual accuracy) to validate whether automated metrics align with human preferences
2. Systematically vary temperature and max_tokens parameters for each model-dataset combination to identify optimal configurations, then reassess performance rankings under these optimized settings
3. Test model performance on out-of-domain summarization tasks (e.g., scientific articles, legal documents, medical literature) to assess generalizability beyond the news-focused CNN/Daily Mail and XSum datasets