---
ver: rpa2
title: 'Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning'
arxiv_id: '2312.11420'
source_url: https://arxiv.org/abs/2312.11420
tags:
- layernorm
- tuning
- finetuning
- mllms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a parameter-efficient method for adapting Large
  Language Models (LLMs) to Multi-Modal Large Language Models (MLLMs) by tuning only
  the LayerNorm parameters within each attention block. The approach treats the transformation
  from text-only to multi-modal understanding as a domain adaptation process, where
  adjusting normalization layers proves highly effective.
---

# Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning

## Quick Facts
- arXiv ID: 2312.11420
- Source URL: https://arxiv.org/abs/2312.11420
- Reference count: 17
- One-line primary result: LayerNorm tuning achieves 20% average performance improvement across five multi-modal tasks while reducing trainable parameters by 41.9% and GPU memory usage by 17.6% compared to LoRA on a 13B model

## Executive Summary
This paper presents a parameter-efficient method for adapting Large Language Models (LLMs) to Multi-Modal Large Language Models (MLLMs) by tuning only the LayerNorm parameters within each attention block. The approach treats the transformation from text-only to multi-modal understanding as a domain adaptation process, where adjusting normalization layers proves highly effective. When compared to full parameter fine-tuning or LoRA on a 13B model, LayerNorm tuning achieves an average performance improvement of over 20% across five multi-modal tasks while reducing trainable parameters by 41.9% and GPU memory usage by 17.6%. Further simplification to tune only LayerNorm (LayerNorm-simp.) with just 0.004% of trainable parameters still outperforms LoRA by 4.3% on average. The method also demonstrates that selectively fine-tuning on conversational data can improve efficiency further. Analysis reveals that LayerNorm-tuned models exhibit lower cross-layer similarity, indicating enhanced expressive power and adaptability to novel multi-modal datasets. These findings suggest LayerNorm tuning as a simple yet effective approach for efficient MLLM fine-tuning.

## Method Summary
The method involves fine-tuning only LayerNorm parameters within attention blocks of pre-trained LLMs (Vicuna/LLaMA2) while keeping all other parameters frozen. The architecture consists of a vision encoder (CLIP ViT-L), vision-language connector (linear projector), LLM, and output head. Training uses an 80K filtered image-text dataset with 3 epochs pre-training on CC3M, learning rate search from [2e-3, 1e-3, 6e-4, 3e-4, 1e-4, 5e-5, 2e-5, 1e-5, 6e-6, 1e-6, 1e-7], weight decay=0, warmup ratio=0.03, cosine learning rate scheduler, gradient checkpointing, DeepSpeed, and TensorFloat32 precision. The approach achieves significant parameter efficiency and performance improvements compared to full fine-tuning and LoRA methods.

## Key Results
- LayerNorm tuning achieves 20% average performance improvement across five multi-modal benchmarks (MME, VQAv2, MSCOCO, Flickr30k, POPE) compared to LoRA
- Reduces trainable parameters by 41.9% and GPU memory usage by 17.6% versus full fine-tuning on 13B models
- LayerNorm-simp. configuration with 0.004% trainable parameters still outperforms LoRA by 4.3% on average
- LayerNorm-tuned models exhibit lower cross-layer similarity, indicating enhanced expressive power and adaptability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LayerNorm tuning acts as a domain adaptation mechanism by normalizing internal representations during the shift from text-only to multi-modal understanding.
- **Mechanism:** LayerNorm adjusts the mean and variance of activations within each attention block, effectively aligning the model's internal feature distributions to the new multi-modal input space without requiring full parameter updates.
- **Core assumption:** The shift from text-only to multi-modal tasks can be characterized as a domain adaptation problem where normalization layers are sufficient to bridge the distributional gap.
- **Evidence anchors:** [abstract] "conceptualizing this transformation as a domain adaptation process, i.e., transitioning from text understanding to embracing multiple modalities"; [section] "Adjusting normalization layers, as suggested by prior research, emerges as a particularly effective technique in such domain shifts (Li et al., 2016)"; [corpus] Weak evidence - no corpus papers directly discuss LayerNorm as a domain adaptor; evidence is primarily from internal experiments.
- **Break condition:** If the multi-modal task requires learning entirely new representational primitives not expressible via normalization shifts, LayerNorm alone would be insufficient.

### Mechanism 2
- **Claim:** LayerNorm tuning improves model expressiveness by reducing cross-layer similarity in learned representations.
- **Mechanism:** By tuning only LayerNorm parameters, the model learns more anisotropic (directionally diverse) layer representations, preventing different layers from converging to similar representations and thus capturing a wider range of learning patterns.
- **Core assumption:** Lower cross-layer similarity correlates with higher expressive power and better generalization, as supported by Pires et al. (2023).
- **Evidence anchors:** [section] "LayerNorm-tuned MLLMs exhibit lower cross-layer similarity compared to models all of which parameters are finetuned. This lowered similarity is indicative of a more expressive model"; [section] "a Transformer model incorporating anisotropic layer representation can capture a wider range of learning patterns"; [corpus] Weak evidence - no corpus papers discuss LayerNorm's role in reducing cross-layer similarity specifically.
- **Break condition:** If LayerNorm tuning causes excessive divergence between layers that harms task performance, the assumption about optimal anisotropy would break.

### Mechanism 3
- **Claim:** LayerNorm tuning maintains smaller gradient variance, leading to better optimization stability and generalization.
- **Mechanism:** LayerNorm layers naturally constrain the variance of gradients as network depth increases, resulting in more stable training dynamics and preventing overfitting to the training distribution.
- **Core assumption:** Smaller gradient variance in LayerNorm-tuned models leads to better generalization, as proven theoretically for deep networks (Xu et al., 2019).
- **Evidence anchors:** [section] "MLLM with LayerNorm tuning method has a more concentrated LayerNorm gradients than fine-tuning during the training process"; [section] "As we presented in fig. 4, MLLM with LayerNorm tuning method has a more concentrated LayerNorm gradients than fine-tuning during the training process"; [corpus] Weak evidence - no corpus papers directly address gradient variance in LayerNorm-tuned models.
- **Break condition:** If the smaller gradient variance becomes too restrictive and prevents the model from learning necessary complex patterns, optimization would stall.

## Foundational Learning

- **Concept:** Domain adaptation in neural networks
  - **Why needed here:** The paper frames multi-modal adaptation as a domain shift problem where normalization layers can bridge the gap between text and multi-modal distributions
  - **Quick check question:** Can you explain why adjusting normalization statistics might be sufficient to adapt a model to a new domain without changing all parameters?

- **Concept:** LayerNorm normalization mechanics
  - **Why needed here:** Understanding how LayerNorm computes mean and variance, and how these statistics affect gradient flow and representation learning
  - **Quick check question:** What happens to the gradients of a LayerNorm layer when the input variance becomes very small?

- **Concept:** Parameter-efficient fine-tuning (PEFT) methods
  - **Why needed here:** Context for comparing LayerNorm tuning against LoRA, adapters, and full fine-tuning in terms of parameter efficiency and performance tradeoffs
  - **Quick check question:** How does the parameter count of LayerNorm tuning compare to LoRA with rank 32 on a 13B model?

## Architecture Onboarding

- **Component map:** Vision encoder (CLIP ViT-L) → Vision-Language connector (linear projector) → LLM (Vicuna/LLaMA2) → Output head

- **Critical path:** 1) Pre-train vision-language connector on image-text pairs; 2) Fine-tune on multi-modal instruction data with LayerNorm parameters active; 3) Evaluate on multi-modal benchmarks (MME, VQAv2, MSCOCO, Flickr30k, POPE)

- **Design tradeoffs:** Parameter efficiency vs. performance: LayerNorm tuning uses 41.9% fewer trainable parameters than LoRA while achieving 20% better average performance; Memory usage: LayerNorm-simp. reduces GPU memory by 17.6% compared to full fine-tuning; Data efficiency: Conversational data proves most effective for multi-modal tuning

- **Failure signatures:** Out-of-memory errors on standard fine-tuning or LoRA methods for 13B models; Degraded performance on cognition tasks in MME when using LayerNorm-simp. instead of full tuning; Sensitivity to learning rate hyperparameter selection

- **First 3 experiments:** 1) Compare LayerNorm tuning vs. LoRA on a 7B model using the same multi-modal instruction dataset, measuring performance on VQAv2 and MSCOCO; 2) Test LayerNorm-simp. configuration on a 13B model to verify the 0.004% parameter efficiency claim and measure memory usage; 3) Conduct ablation study activating only vision-language connector vs. only LayerNorm to isolate their individual contributions to multi-modal performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal number of trainable LayerNorm parameters for balancing performance and efficiency across different model scales?
- **Basis in paper:** [explicit] The paper discusses tuning LayerNorm parameters versus full parameter fine-tuning, showing LayerNorm tuning achieves significant performance improvements with much fewer trainable parameters.
- **Why unresolved:** The paper does not explore the effects of tuning different subsets of LayerNorm parameters or finding the optimal number for various model sizes.
- **What evidence would resolve it:** Systematic experiments varying the proportion of LayerNorm parameters tuned across different model scales, measuring both performance and parameter efficiency.

### Open Question 2
- **Question:** How does LayerNorm tuning compare to other parameter-efficient fine-tuning methods like prefix tuning or adapters in terms of performance and computational efficiency?
- **Basis in paper:** [inferred] The paper compares LayerNorm tuning to full fine-tuning and LoRA but does not include other PEFT methods like prefix tuning or adapters.
- **Why unresolved:** The comparative analysis is limited to LoRA and full fine-tuning, leaving uncertainty about LayerNorm's relative performance against other PEFT techniques.
- **What evidence would resolve it:** Direct comparisons of LayerNorm tuning with prefix tuning and adapters on the same benchmarks and model scales.

### Open Question 3
- **Question:** What is the impact of LayerNorm tuning on model generalization to unseen tasks and domains beyond the multi-modal benchmarks tested?
- **Basis in paper:** [explicit] The paper shows LayerNorm-tuned models have lower cross-layer similarity, indicating enhanced expressive power and adaptability.
- **Why unresolved:** The study focuses on specific multi-modal tasks, and it's unclear if these findings generalize to other types of tasks or domains.
- **What evidence would resolve it:** Evaluations of LayerNorm-tuned models on diverse task sets, including non-vision tasks, to assess generalization capabilities.

## Limitations
- Limited empirical evidence for LayerNorm as a domain adaptation mechanism - primarily relies on internal experiments rather than established corpus research
- Theoretical claims about cross-layer similarity and expressive power are hypothesized rather than empirically proven
- Claims about gradient variance optimization lack theoretical backing or comparison with other PEFT methods

## Confidence
- **High Confidence:** Parameter efficiency claims (41.9% reduction vs. LoRA, 17.6% memory reduction) and raw performance improvements (20% average improvement across five benchmarks)
- **Medium Confidence:** The LayerNorm-simp. variant achieving 4.3% improvement over LoRA with only 0.004% trainable parameters
- **Low Confidence:** The theoretical mechanisms explaining why LayerNorm tuning works (domain adaptation framing, cross-layer similarity reduction, gradient variance optimization)

## Next Checks
1. **Cross-model validation:** Replicate the LayerNorm tuning experiments on a different LLM architecture (e.g., Mistral or Gemma) to verify that the 20% performance improvement is not architecture-specific to Vicuna/LLaMA2

2. **Mechanism isolation ablation:** Conduct controlled experiments comparing LayerNorm tuning against tuning only the vision-language connector and only the word embeddings to isolate which component contributes most to the observed performance gains

3. **Gradient dynamics comparison:** Track and compare gradient norms, variances, and distributions across different PEFT methods (LayerNorm, LoRA, adapters) during training to empirically validate the claim about superior optimization stability in LayerNorm-tuned models