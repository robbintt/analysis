---
ver: rpa2
title: Normalization Layers Are All That Sharpness-Aware Minimization Needs
arxiv_id: '2306.04226'
source_url: https://arxiv.org/abs/2306.04226
tags:
- sam-on
- normalization
- parameters
- sharpness
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the effectiveness of sharpness-aware minimization
  (SAM) in deep learning by focusing on the role of normalization layers. The authors
  propose SAM-ON, a method that applies SAM only to the affine parameters of normalization
  layers, which comprise less than 0.1% of the total parameters.
---

# Normalization Layers Are All That Sharpness-Aware Minimization Needs

## Quick Facts
- arXiv ID: 2306.04226
- Source URL: https://arxiv.org/abs/2306.04226
- Authors: 
- Reference count: 40
- The paper shows that applying sharpness-aware minimization (SAM) only to normalization layer parameters (SAM-ON) outperforms traditional SAM applied to all parameters, despite comprising less than 0.1% of total parameters.

## Executive Summary
This paper investigates why sharpness-aware minimization (SAM) works by proposing SAM-ON, a variant that perturbs only the affine parameters of normalization layers (BatchNorm/LayerNorm). The authors demonstrate that SAM-ON achieves better generalization than standard SAM on CIFAR and ImageNet datasets across multiple architectures including ResNet and Vision Transformers. Surprisingly, they find that SAM-ON models are actually sharper than SAM-all models, challenging the conventional wisdom that flatness of minima is the key to SAM's success. The study reveals that normalization layers play a unique and critical role in SAM's effectiveness, with the benefits stemming from targeting these specific parameters rather than from sparsity alone.

## Method Summary
The authors propose SAM-ON, which applies SAM perturbations only to the γ (scale) and β (shift) parameters of normalization layers, which typically comprise less than 0.1% of total network parameters. They compare SAM-ON against standard SAM (perturbing all parameters) and various sparse perturbation methods across CIFAR-100 and ImageNet datasets using ResNet and Vision Transformer architectures. The experiments use multiple SAM variants including elementwise ℓ2, elementwise ℓ∞, layerwise ℓ2, and Fisher-SAM perturbations. Sharpness is measured using AutoPGD with adaptive ℓ∞ m-sharpness metrics, and the analysis includes weight distribution studies of normalization parameters throughout training.

## Key Results
- SAM-ON achieves 0.1-0.5% higher test accuracy than SAM-all on CIFAR-100 across multiple SAM variants and batch sizes
- SAM-ON models are computationally more efficient than SAM-all while maintaining or improving generalization
- SAM-ON models are actually sharper than SAM-all models, challenging the notion that flatness drives SAM's success
- The effectiveness of SAM-ON cannot be attributed to sparsity alone, as other sparse methods do not achieve comparable results

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Perturbing only normalization layer parameters (SAM-ON) achieves better generalization than perturbing all parameters (SAM-all).
- **Mechanism:** The normalization layers (BatchNorm/LayerNorm) are critical for model performance, and perturbing them with SAM improves generalization more effectively than perturbing all parameters.
- **Core assumption:** Normalization layers play a unique and disproportionate role in SAM's effectiveness.
- **Evidence anchors:**
  - [abstract] "applying SAM only to the normalization layers of a network (less than 0.1% of the total parameters) outperforms perturbing all parameters."
  - [section 4.1] "SAM-ON obtains enhanced generalization performance compared to conventional SAM (SAM-all) for ResNet architectures with BatchNorm... and Vision Transformers with LayerNorm (Section 4.2) on CIFAR data and performs competitively on ImageNet."
  - [corpus] Weak correlation - normalization-specific findings not well covered in neighboring papers.
- **Break condition:** If other sparse perturbation methods targeting non-normalization parameters achieve similar performance at the same sparsity level.

### Mechanism 2
- **Claim:** The success of SAM-ON is not solely due to sparsity benefits.
- **Mechanism:** The effectiveness comes from targeting normalization layers specifically, not just from reducing the number of perturbed parameters.
- **Core assumption:** Normalization layers have unique properties that make them especially effective targets for SAM perturbations.
- **Evidence anchors:**
  - [section 5.1] "we show that the success of SAM-ON cannot be attributed to sparsity alone: targeting the normalization layers clearly improves over other masked sparsity approaches."
  - [section 4.3] "Although the use of sparsified perturbations was recently shown to benefit generalization [36], we show that the success of SAM-ON cannot be attributed to sparsity alone."
  - [corpus] Limited - sparse perturbation approaches discussed but not normalization-specific mechanisms.
- **Break condition:** If alternative sparse methods (SSAM-F, SSAM-D) achieve comparable results at similar sparsity levels.

### Mechanism 3
- **Claim:** SAM-ON can find sharper minima while achieving better generalization.
- **Mechanism:** The relationship between sharpness and generalization is not as straightforward as previously thought; SAM-ON demonstrates that improved generalization can occur with increased sharpness.
- **Core assumption:** Sharpness reduction is not the sole mechanism for SAM's generalization benefits.
- **Evidence anchors:**
  - [section 5.2] "Training with SAM-ON can enhance generalization while actually increasing sharpness, rather than decreasing it."
  - [section 5.2] "Although SAM-ON improves generalization, it actually increases sharpness. Although perhaps surprising given SAM's original motivation, this finding relates naturally to the literature."
  - [corpus] Moderate - recent papers questioning sharpness-generalization correlation are cited.
- **Break condition:** If subsequent work demonstrates that SAM-ON consistently finds flatter minima than SAM-all.

## Foundational Learning

- **Concept: Sharpness-aware minimization (SAM)**
  - Why needed here: The paper builds on SAM and proposes a variant that only perturbs normalization parameters. Understanding SAM's core mechanism is essential.
  - Quick check question: What is the key difference between SAM and standard SGD in terms of the optimization objective?

- **Concept: Normalization layers (BatchNorm/LayerNorm)**
  - Why needed here: The paper's central finding is that perturbing only normalization parameters is effective. Understanding what these layers do is crucial.
  - Quick check question: What are the trainable parameters in BatchNorm and LayerNorm, and what do they control?

- **Concept: Sparsity in optimization**
  - Why needed here: The paper compares SAM-ON to other sparse perturbation methods and argues that sparsity alone doesn't explain the results.
  - Quick check question: What is the primary motivation for using sparse perturbations in optimization algorithms?

## Architecture Onboarding

- **Component map:**
  - Base optimizer (SGD with momentum/AdamW)
  - SAM perturbation mechanism
  - Normalization layers with affine parameters (γ, β)
  - Sparse mask application (SAM-ON variant)

- **Critical path:**
  1. Compute gradients on current parameters
  2. Apply perturbation to normalization parameters only
  3. Recompute gradients at perturbed point
  4. Update all parameters using perturbed gradients

- **Design tradeoffs:**
  - Full SAM: Higher computational cost but potentially better sharpness reduction
  - SAM-ON: Lower computational cost, better generalization in many cases, but potentially higher sharpness
  - Alternative sparse methods: May not achieve SAM-ON's performance even with similar sparsity levels

- **Failure signatures:**
  - If SAM-ON underperforms significantly on architectures without normalization layers
  - If alternative sparse methods achieve similar results at the same sparsity level
  - If removing normalization layers entirely doesn't significantly impact SAM-ON's effectiveness

- **First 3 experiments:**
  1. Train a ResNet-50 on CIFAR-10 with SAM-all vs SAM-ON, comparing test accuracy and wall-clock time
  2. Train a Vision Transformer on CIFAR-100 with various SAM variants (elementwise ℓ2, layerwise ℓ2, elementwise ℓ∞) comparing all vs ON variants
  3. Apply SAM-ON to a ResNet without normalization layers to verify the necessity of normalization parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the underlying mechanism that allows SAM-ON to outperform SAM-all despite inducing higher sharpness in the loss landscape?
- Basis in paper: [explicit] The paper explicitly states that SAM-ON models are sharper than SAM-all models yet generalize better, challenging the notion that flatness of minima is the sole reason for SAM's success.
- Why unresolved: The study highlights this counterintuitive finding but does not provide a definitive explanation for why increased sharpness does not negatively impact generalization in the case of SAM-ON.
- What evidence would resolve it: Further investigation into the relationship between sharpness, normalization layers, and generalization could provide insights. This could include analyzing the dynamics of SAM-ON training and its effects on the loss landscape.

### Open Question 2
- Question: How does the perturbation model of ASAM elementwise-ℓ2 implicitly focus on perturbing the normalization layers, leading to a performance decrease when these layers are excluded?
- Basis in paper: [explicit] The paper mentions that the elementwise ℓ2 variant shows a strong performance decrease in no-norm, indicating that it implicitly focuses on perturbing the BatchNorm layers already.
- Why unresolved: The paper does not provide a detailed explanation of how the ASAM elementwise-ℓ2 perturbation model achieves this implicit focus on normalization layers.
- What evidence would resolve it: A thorough analysis of the ASAM elementwise-ℓ2 perturbation model and its interaction with normalization layers could shed light on this phenomenon. This could involve comparing the perturbation patterns of different SAM variants and their effects on the loss landscape.

### Open Question 3
- Question: What is the role of the stage of training in the effectiveness of SAM-ON compared to SAM-all?
- Basis in paper: [explicit] The paper investigates the effect of applying SAM-ON and SAM-all at different stages of training, finding that the advantage of SAM-ON shrinks when applied later in training.
- Why unresolved: While the paper observes this phenomenon, it does not provide a comprehensive explanation for why the timing of SAM-ON application affects its effectiveness.
- What evidence would resolve it: A more detailed analysis of the training dynamics of SAM-ON and SAM-all at different stages could reveal the factors that contribute to the observed differences. This could involve studying the evolution of the loss landscape and the optimization trajectory throughout training.

## Limitations
- The computational efficiency claims assume specific hardware/software implementations that aren't fully specified
- The sharpness measurements rely on AutoPGD with parameters that could influence results
- The ablation studies focus primarily on CIFAR-100 and ImageNet, with limited exploration of other domains

## Confidence
- **High confidence:** SAM-ON achieves better generalization than SAM-all on CIFAR datasets (well-supported by multiple experiments)
- **Medium confidence:** The superiority of SAM-ON is due to normalization layers specifically rather than sparsity alone (supported but could benefit from more alternative sparse methods)
- **Medium confidence:** SAM-ON can find sharper minima while improving generalization (challenged by recent literature questioning sharpness-generalization correlation)

## Next Checks
1. Replicate the CIFAR-100 experiments comparing SAM-all vs SAM-ON across all SAM variants to verify the 0.1-0.5% accuracy improvements
2. Implement and test at least two additional sparse perturbation methods (e.g., targeting different parameter subsets) to confirm SAM-ON's unique effectiveness
3. Measure actual wall-clock time for SAM-all vs SAM-ON on ImageNet-scale training to validate the computational efficiency claims