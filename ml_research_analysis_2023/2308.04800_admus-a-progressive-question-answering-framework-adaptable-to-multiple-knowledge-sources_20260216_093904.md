---
ver: rpa2
title: 'ADMUS: A Progressive Question Answering Framework Adaptable to Multiple Knowledge
  Sources'
arxiv_id: '2308.04800'
source_url: https://arxiv.org/abs/2308.04800
tags:
- query
- knowledge
- kbqa
- graph
- admus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents ADMUS, a progressive knowledge base question
  answering (KBQA) framework designed to handle multiple knowledge sources and datasets.
  ADMUS decouples dataset-related components from dataset-independent modules in the
  semantic parsing pipeline, enabling seamless integration of new datasets with minimal
  effort.
---

# ADMUS: A Progressive Question Answering Framework Adaptable to Multiple Knowledge Sources

## Quick Facts
- arXiv ID: 2308.04800
- Source URL: https://arxiv.org/abs/2308.04800
- Reference count: 30
- Primary result: ADMUS decouples KBQA components to enable seamless integration of new datasets with minimal effort while maintaining reliability through progressive query stages

## Executive Summary
ADMUS is a progressive knowledge base question answering framework designed to handle multiple knowledge sources and datasets. The framework addresses the high costs of adapting KBQA systems to disparate datasets by decoupling dataset-related components from dataset-independent modules in the semantic parsing pipeline. This modular architecture, combined with a three-stage progressive approach (exact queries, approximate queries, and LLM-based queries as fallback), enables seamless integration of new datasets while balancing reliability and user experience. The authors demonstrate ADMUS using three datasets of varying scales and languages, showcasing its ability to switch between different knowledge base sources.

## Method Summary
ADMUS employs a microservice architecture that decomposes the KBQA pipeline into dataset-related components (Node Extraction, Relation Extraction) and a dataset-independent backbone (Query Graph Generation, Answer Collection). The framework routes incoming questions to appropriate dataset-specific services based on knowledge base selection, allowing the same backbone to process questions from multiple sources. The three-stage progressive framework attempts exact SPARQL queries first, falls back to approximate queries if exact fails, and finally uses LLM prompting as a last resort. This staged approach provides traceable answers when possible and user-friendly responses otherwise, addressing the reliability-usability tradeoff in KBQA systems.

## Key Results
- Successfully demonstrates integration of three datasets with varying scales (birdDB: 17K triples, filmDB: 4.5M triples, DBpedia2016: 198M triples) and languages (Chinese and English)
- Shows that decoupling dataset-related components from the backbone enables minimal-effort adaptation to new knowledge bases
- Achieves progressive query generation that balances precision, reliability, and responsiveness through staged fallback mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling dataset-related components from dataset-independent modules reduces system retraining cost.
- Mechanism: By splitting the KBQA pipeline into microservices for dataset-specific tasks (Node Extraction, Relation Extraction) and a shared backbone (Query Graph Generation, Answer Collection), new datasets can be added by only implementing the dataset-specific services without retraining the entire system.
- Core assumption: Dataset-specific components (NE, RE) are independent from the semantic parsing and query construction logic.
- Evidence anchors:
  - [abstract] "decouple the architecture of conventional KBQA systems and propose this dataset-independent framework"
  - [section] "decomposes their workflow into two parts: dataset-related components... and dataset-independent backbone"
  - [corpus] Weak: corpus mentions in-context learning but does not directly address modular decoupling.

### Mechanism 2
- Claim: Progressive three-stage query generation balances reliability and usability.
- Mechanism: The system attempts exact SPARQL queries first, falls back to approximate queries (e.g., FILTER CONTAINS) if exact fails, and finally uses LLM prompting as a last resort. This staged approach provides traceable answers when possible and user-friendly responses otherwise.
- Core assumption: Exact queries are most reliable, approximate queries are less reliable but more forgiving, and LLM queries are least reliable but most responsive.
- Evidence anchors:
  - [abstract] "progressive framework consisting of three stages, ranges from executing exact queries, generating approximate queries and retrieving open-domain knowledge"
  - [section] "three-stage progressive framework... (1) Exact Query... (2) Approximate Query... (3) LLM Query"
  - [corpus] Weak: corpus papers focus on in-context learning but do not discuss staged reliability tradeoffs.

### Mechanism 3
- Claim: Microservice architecture enables dynamic routing between dataset-specific components.
- Mechanism: ADMUS routes incoming questions to the appropriate NE and RE services based on dataset selection, allowing the same backbone to process questions from multiple knowledge bases with different schemas and languages.
- Core assumption: NE and RE services can be independently implemented for each dataset while the backbone remains unchanged.
- Evidence anchors:
  - [abstract] "microservice architecture, which allows for adaptive routing between different microservice providers for the various dataset-related components"
  - [section] "ADMUS adopts the microservice architecture, which allows for adaptive routing between different microservice providers"
  - [corpus] Weak: corpus discusses in-context learning for KBQA but not microservice routing.

## Foundational Learning

- Concept: Semantic Parsing in KBQA
  - Why needed here: ADMUS relies on converting natural language questions into structured queries (e.g., SPARQL) to retrieve answers from knowledge bases.
  - Quick check question: What is the difference between syntactic parsing and semantic parsing in the context of question answering?

- Concept: Entity Linking
  - Why needed here: NE services must map entity mentions in questions to actual entities in the knowledge base using similarity scores.
  - Quick check question: How does entity linking differ from named entity recognition (NER)?

- Concept: Query Graph Construction
  - Why needed here: QGG module builds a semantic query graph from entity mentions and dependency tree structure, which is essential for generating valid SPARQL queries.
  - Quick check question: What role does the dependency tree play in constructing the query graph?

## Architecture Onboarding

- Component map:
  - Node Extraction (NE) service: extracts entity mentions from questions
  - Query Graph Generation (QGG) module: builds semantic query graph from dependency tree
  - Relation Extraction (RE) service: identifies predicates between node pairs
  - SPARQL Generation: converts query graph to executable SPARQL
  - Answer Collection (AC) module: executes SPARQL and implements progressive fallback

- Critical path: NLQ → NE → QGG → RE → SPARQL Generation → AC (Exact → Approximate → LLM)

- Design tradeoffs:
  - Modularity vs. performance: Decoupling increases flexibility but may add routing overhead
  - Reliability vs. usability: Progressive stages improve UX but may reduce answer precision
  - Dataset independence vs. optimization: Shared backbone reduces retraining but may miss dataset-specific optimizations

- Failure signatures:
  - NE service returns empty entity set → no query graph construction
  - RE service fails to find predicates → SPARQL generation fails
  - All SPARQL queries fail → LLM fallback triggered (may indicate out-of-domain question)
  - Dynamic routing selects wrong dataset → incorrect answers

- First 3 experiments:
  1. Deploy ADMUS with a single small knowledge base and test exact query execution
  2. Add a second dataset with different language and test dynamic routing and NE/RE service selection
  3. Test the progressive fallback by crafting questions that fail exact and approximate stages to trigger LLM query

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the progressive framework's three-stage approach compare to other potential architectures (e.g., two-stage, four-stage) in terms of usability and reliability trade-offs?
- Basis in paper: [explicit] The paper explicitly discusses a three-stage progressive framework (exact queries, approximate queries, LLM-based queries) designed to balance usability and reliability.
- Why unresolved: The paper presents the three-stage framework as a solution but does not explore or compare alternative architectures or the optimal number of stages.
- What evidence would resolve it: Comparative studies evaluating different progressive framework architectures (two-stage, three-stage, four-stage) on usability metrics (user satisfaction, response time) and reliability metrics (accuracy, precision) across diverse datasets and KB types.

### Open Question 2
- Question: What is the optimal balance between exact query execution and approximate query expansion in terms of performance and user experience across different KB scales and complexities?
- Basis in paper: [inferred] The paper describes approximate queries as a fallback when exact queries fail, but does not quantify or optimize the balance between these approaches.
- Why unresolved: The paper acknowledges approximate queries as a fallback but does not provide empirical analysis of when and how to optimally transition between exact and approximate queries based on KB characteristics.
- What evidence would resolve it: Empirical studies measuring query success rates, execution times, and user satisfaction across different KB scales (small, medium, large) when varying the threshold for transitioning from exact to approximate queries.

### Open Question 3
- Question: How can the LLM fallback component be optimized to reduce hallucination while maintaining its utility as an open-domain knowledge source?
- Basis in paper: [explicit] The paper mentions that LLM-based queries serve as a fallback option but acknowledges the hallucination problem and notes that LLM answers "may not be entirely reliable."
- Why unresolved: While the paper recognizes the hallucination issue with LLMs, it does not propose specific strategies to mitigate this problem or evaluate different prompting techniques and fine-tuning approaches.
- What evidence would resolve it: Comparative evaluation of different LLM prompting strategies (Chain-of-Thought, few-shot examples, domain-specific fine-tuning) measuring hallucination rates and answer quality across diverse question types and domains.

## Limitations
- The framework's modular design assumes complete independence between dataset-specific and dataset-independent components, which may not hold for complex semantic parsing tasks where cross-dataset knowledge transfer is beneficial
- LLM fallback queries lack traceability and may introduce hallucination, potentially degrading user trust despite improving response coverage
- Dynamic routing overhead and service discovery complexity could impact real-time performance in production environments

## Confidence
- High confidence: Decoupling architecture reduces adaptation costs for new datasets
- Medium confidence: Progressive three-stage approach effectively balances reliability and usability
- Medium confidence: Microservice architecture enables flexible knowledge base integration

## Next Checks
1. Benchmark ADMUS with a fourth, unseen knowledge base to verify true dataset independence and minimal adaptation requirements
2. Measure latency impact of microservice routing and progressive fallback stages in production-scale deployments
3. Conduct user studies comparing answer satisfaction and trust between progressive stages, particularly LLM-generated responses vs. traceable SPARQL answers