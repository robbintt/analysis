---
ver: rpa2
title: Contrastive Learning of Temporal Distinctiveness for Survival Analysis in Electronic
  Health Records
arxiv_id: '2308.13104'
source_url: https://arxiv.org/abs/2308.13104
tags:
- survival
- which
- time
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses survival analysis in electronic health records
  (EHRs) using a novel contrastive learning approach. The proposed Ontology-aware
  Temporality-based Contrastive Survival (OTCSurv) framework incorporates medical
  ontologies to enhance feature representation and employs a Supervised Weighted Contrastive
  (SupWCon) loss to learn temporal distinctiveness.
---

# Contrastive Learning of Temporal Distinctiveness for Survival Analysis in Electronic Health Records

## Quick Facts
- arXiv ID: 2308.13104
- Source URL: https://arxiv.org/abs/2308.13104
- Reference count: 40
- Primary result: Achieves Ctd of 0.6990 and MAE of 1.890 on AKI prediction task

## Executive Summary
This paper introduces OTCSurv, a novel framework for survival analysis in electronic health records that leverages contrastive learning to capture temporal distinctiveness. The method combines ontology-aware encoding with supervised weighted contrastive loss to handle both observed and censored data effectively. Through extensive experiments on a large EHR dataset for acute kidney injury prediction, OTCSurv demonstrates superior performance compared to existing methods, achieving state-of-the-art results in both discrimination and prediction accuracy.

## Method Summary
OTCSurv is a survival analysis framework that integrates contrastive learning with medical ontologies to improve temporal distinctiveness in EHR data. The method uses an ontological encoder and sequential self-attention encoder to represent longitudinal EHR data, incorporating the GRAM approach for medical code embeddings. A Supervised Weighted Contrastive (SupWCon) loss is employed to learn temporal distinctiveness, with temperature parameters adjusted based on survival duration differences. The model is trained using multiple loss components including loglikelihood, pairwise ranking, SupWCon, and mean squared error losses, optimized through a combination of negative log-likelihood and concordance loss functions.

## Key Results
- Achieves Ctd of 0.6990 and MAE of 1.890 on AKI prediction task
- Outperforms existing methods on both time-dependent discrimination and mean absolute error metrics
- Demonstrates effectiveness through comprehensive quantitative and qualitative studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised Weighted Contrastive (SupWCon) loss uses survival duration differences to define negative pair hardness, enabling more effective temporal distinctiveness learning.
- Mechanism: For each negative pair, the temperature parameter is set to the inverse of their survival duration difference (ùúèùëñùëó = |ùëáùëñ ‚àí ùëáùëó|‚Åª¬π). This makes the model pull apart representations more strongly for pairs with larger temporal differences.
- Core assumption: Survival duration differences directly reflect meaningful clinical dissimilarity that should be captured in the embedding space.
- Evidence anchors:
  - [abstract] "construct negative sample pairs with adjustable hardness for contrastive learning"
  - [section] "For example, if patient ùëñ and patient ùëó make a negative pair, the adjusted temperature parameter for this negative pair is calculated as follows: ùúèùëñùëó = |ùëáùëñ ‚àí ùëáùëó|‚Åª¬π"
- Break condition: If survival duration differences don't correlate with clinically meaningful distinctions, the hardness weighting becomes arbitrary and may harm representation quality.

### Mechanism 2
- Claim: Ontology-aware encoding improves data efficiency by leveraging hierarchical medical knowledge when observations are sparse.
- Mechanism: Medical codes are embedded using attention over themselves and their ancestors in the medical ontology DAG, creating representations that benefit from broader clinical context.
- Core assumption: Higher-level medical concepts provide useful semantic information that can compensate for limited individual code observations.
- Evidence anchors:
  - [abstract] "uses an ontological encoder and a sequential self-attention encoder to represent the longitudinal EHR data with rich contexts"
  - [section] "In order to address the challenge of data limitation in the healthcare domain, acquire comprehensive representations of medical codes, and increase predictability, we utilize the attention-based graph representation approach known as GRAM"
- Break condition: If the ontology hierarchy is noisy or doesn't align with clinical reality, the attention mechanism may propagate incorrect information.

### Mechanism 3
- Claim: Multi-loss training balances temporal ordering, survival probability prediction, and exact time estimation objectives.
- Mechanism: Four loss components (loglikelihood, ranking, SupWCon, MSE) are combined with weights to simultaneously optimize concordance, risk ranking, and time prediction accuracy.
- Core assumption: Survival analysis requires both ranking ability and precise time estimation, which cannot be achieved by a single objective.
- Evidence anchors:
  - [abstract] "we incorporate the contrastive task into the time-to-event predictive task with multiple loss components"
  - [section] "To train this model, combined with SupWCon, three more loss functions of Loglikelihood Loss, Pairwise Ranking Loss, Mean Squared Error loss are implemented"
- Break condition: If the loss weights are poorly calibrated, one objective may dominate and degrade overall performance.

## Foundational Learning

- Concept: Survival analysis with censoring
  - Why needed here: The method explicitly handles both observed and censored data, using the pre-censoring interval as partial labels
  - Quick check question: What's the difference between observed and censored survival data, and why can't we simply treat censored cases as non-events?

- Concept: Contrastive learning fundamentals
  - Why needed here: The core innovation uses temporal distinctiveness as a contrastive signal rather than semantic class labels
  - Quick check question: How does adjusting temperature parameters based on survival duration differences change the contrastive objective?

- Concept: Medical ontologies and hierarchical encoding
  - Why needed here: The model uses ICD-9 hierarchy to create richer code representations when individual codes are rarely observed
  - Quick check question: Why might representing a rare diagnosis using its higher-level category be more useful than using a learned embedding based on few observations?

## Architecture Onboarding

- Component map: Ontological encoder ‚Üí Visit-level attention-pooling ‚Üí Transformer encoder ‚Üí Instance-level attention-pooling ‚Üí (SupWCon projection + survival prediction)
- Critical path: Code ‚Üí ontological embedding ‚Üí visit attention ‚Üí transformer ‚Üí instance attention ‚Üí dual-task output
- Design tradeoffs: Hierarchical ontology encoding vs learned embeddings; multiple loss functions vs single objective; attention-based compression vs fixed pooling
- Failure signatures: Poor Ctd/MAE indicates contrastive temperature scaling issues; low performance on rare codes suggests ontology encoding problems; convergence issues may indicate loss weight imbalance
- First 3 experiments:
  1. Remove SupWCon and verify baseline performance drops in Ctd while improving in exact time prediction
  2. Replace ontology encoder with learned embeddings and measure impact on rare code performance
  3. Vary the positive window size (T parameter) and observe effects on contrastive learning quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OTCSurv change when applied to different medical conditions beyond acute kidney injury?
- Basis in paper: [explicit] The paper evaluates OTCSurv on a large EHR dataset for acute kidney injury (AKI) prediction and demonstrates its effectiveness.
- Why unresolved: The study focuses on a single medical condition, limiting generalizability.
- What evidence would resolve it: Testing OTCSurv on diverse medical conditions and comparing its performance across different datasets.

### Open Question 2
- Question: What is the impact of using different medical ontologies on the performance of OTCSurv?
- Basis in paper: [explicit] The paper mentions the use of medical ontologies to enhance feature representation but does not explore the impact of different ontologies.
- Why unresolved: The study uses a specific ontology without comparing alternative ontologies.
- What evidence would resolve it: Evaluating OTCSurv with various medical ontologies and analyzing performance differences.

### Open Question 3
- Question: How does the model's performance scale with larger and more diverse datasets?
- Basis in paper: [inferred] The paper uses a large EHR dataset but does not discuss scalability or performance on even larger datasets.
- Why unresolved: The scalability and performance on significantly larger datasets are not addressed.
- What evidence would resolve it: Testing OTCSurv on progressively larger and more diverse datasets to assess scalability and performance trends.

## Limitations

- The model's effectiveness is demonstrated primarily on a single medical condition (AKI), limiting generalizability across different survival analysis tasks
- The adaptive temperature scaling relies on the assumption that survival duration differences correlate with clinical dissimilarity, which may not hold universally
- The multi-loss optimization approach requires careful hyperparameter tuning, with limited ablation studies on loss weight selection

## Confidence

- **Confidence Level: Medium** - Strong performance on KUMC EHR dataset for AKI prediction, but results may not generalize across medical conditions
- **Confidence Level: Medium** - Temperature scaling based on survival duration differences is theoretically sound but depends on domain-specific assumptions
- **Confidence Level: Low** - Multi-loss optimization is effective but sensitive to hyperparameter choices with limited sensitivity analysis

## Next Checks

1. **Cross-Domain Validation**: Test OTCSurv on multiple survival analysis tasks beyond AKI prediction (e.g., cardiovascular disease, cancer survival) to assess generalizability across different medical conditions.

2. **Loss Component Sensitivity**: Systematically vary the weights of the four loss components through ablation studies to determine the robustness of the multi-loss optimization approach and identify optimal configurations.

3. **Temporal Assumption Validation**: Conduct controlled experiments to verify whether survival duration differences genuinely correlate with clinically meaningful distinctions across different medical conditions, or if the temperature scaling assumption breaks down in certain domains.