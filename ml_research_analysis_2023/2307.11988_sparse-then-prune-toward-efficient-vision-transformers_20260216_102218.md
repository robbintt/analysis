---
ver: rpa2
title: 'Sparse then Prune: Toward Efficient Vision Transformers'
arxiv_id: '2307.11988'
source_url: https://arxiv.org/abs/2307.11988
tags:
- sparse
- regularization
- transformer
- vision
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies sparse regularization and pruning techniques
  to Vision Transformer models for image classification on CIFAR and ImageNet datasets.
  Models are pre-trained on ImageNet21K then fine-tuned for 20 epochs.
---

# Sparse then Prune: Toward Efficient Vision Transformers

## Quick Facts
- arXiv ID: 2307.11988
- Source URL: https://arxiv.org/abs/2307.11988
- Reference count: 27
- Primary result: Sparse regularization improves ViT accuracy by 0.12% on CIFAR-100 and ImageNet-100; pruning with sparse regularization increases accuracy by 0.568% on CIFAR-10, 1.764% on CIFAR-100, and 0.256% on ImageNet-100

## Executive Summary
This paper proposes combining sparse regularization and pruning techniques to improve the efficiency of Vision Transformer models for image classification. The authors apply sparse regularization to Vision Transformers after calculating attention weights in the self-attention layer, followed by global pruning using the L1-norm method. Models are pre-trained on ImageNet-21K and fine-tuned on CIFAR and ImageNet-100 datasets for 20 epochs. Results show that sparse regularization alone improves accuracy by 0.12% on CIFAR-100 and ImageNet-100, while combining sparse regularization with pruning yields larger improvements of 0.568% on CIFAR-10, 1.764% on CIFAR-100, and 0.256% on ImageNet-100 compared to pruning without sparse regularization.

## Method Summary
The approach involves two-stage training: pre-training on ImageNet-21K followed by fine-tuning on target datasets (CIFAR-10/100, ImageNet-100) for 20 epochs with batch size 64 and learning rate 0.03. Sparse regularization is applied after attention weight calculation in the self-attention layer using a specific function S(hk) = log(1 + h²k). After fine-tuning, global unstructured pruning is performed using the L1-norm method to identify and remove unimportant weights. The ViT-B/16 architecture is used throughout experiments.

## Key Results
- Sparse regularization improves accuracy by 0.12% on CIFAR-100 and ImageNet-100 datasets
- Pruning with sparse regularization increases accuracy by 0.568% on CIFAR-10, 1.764% on CIFAR-100, and 0.256% on ImageNet-100 compared to pruning alone
- Sparse regularization is most effective when placed after attention weight calculation in the self-attention layer
- L1-norm global pruning maintains accuracy with up to 30% pruning ratio when combined with sparse regularization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse regularization applied after attention weight calculation improves pruning efficiency by preserving more important weights.
- Mechanism: By introducing sparsity before pruning, the model naturally pushes less important weights toward zero, allowing pruning to remove them with minimal accuracy loss.
- Core assumption: The attention weight layer contains the most critical information for distinguishing important versus unimportant parameters.
- Evidence anchors:
  - [abstract] "Sparse regularization is most effective when placed after the attention weight calculation in the self-attention layer."
  - [section] "As a result, the model achieved the highest accuracy of 92.52% when sparse regularization was applied to the Self-Attention layer after calculating the Attention Weight."
  - [corpus] No direct corpus evidence found for this specific mechanism placement.
- Break Condition: If sparse regularization is applied at other locations (similarity score, weighted value, output layer, or MLP input), the accuracy improvement diminishes or becomes negative.

### Mechanism 2
- Claim: Transfer learning from ImageNet21K provides a strong initialization that enhances the effectiveness of sparse regularization and pruning.
- Mechanism: Pre-training on large, diverse datasets creates a more stable weight distribution, making sparsity-inducing regularization more effective at identifying truly unimportant parameters.
- Core assumption: The benefits of sparse regularization and pruning compound when applied to models with transfer learning versus randomly initialized models.
- Evidence anchors:
  - [section] "The Vision Transformer performs remarkably when complemented by a pre-training process known as transfer learning [3]."
  - [section] "The Vision Transformer model with sparse regularization can improve accuracy by 0.12% on CIFAR-100 and 0.12% on ImageNet-100."
  - [corpus] No direct corpus evidence for this specific transfer learning + sparse regularization interaction.
- Break Condition: If models are trained from scratch without transfer learning, the accuracy gains from sparse regularization and pruning may be significantly reduced or eliminated.

### Mechanism 3
- Claim: Global pruning with L1-norm efficiently identifies and removes unimportant weights while maintaining accuracy.
- Mechanism: The L1-norm method provides a differentiable way to measure weight importance across the entire model, allowing uniform pruning that preserves critical parameters.
- Core assumption: L1-norm provides a reliable metric for weight importance across different layers and attention heads in Vision Transformers.
- Evidence anchors:
  - [section] "The weights were determined to be pruned using the l1-norm method."
  - [section] "Pruning performed on models with sparse regularization produces higher accuracy than those without sparse regularization."
  - [corpus] No direct corpus evidence for this specific pruning methodology.
- Break Condition: If pruning percentages exceed 30% or if unstructured pruning targets the wrong weights, accuracy degrades significantly regardless of sparse regularization.

## Foundational Learning

- Concept: Vision Transformer Architecture
  - Why needed here: Understanding the self-attention mechanism and where sparse regularization can be inserted is critical for implementing this approach correctly.
  - Quick check question: What is the difference between the similarity score, attention weight, and weighted value stages in self-attention?

- Concept: Transfer Learning and Pre-training
  - Why needed here: The effectiveness of sparse regularization and pruning depends heavily on the quality of pre-training, which establishes the initial weight distribution.
  - Quick check question: Why is ImageNet21K specifically chosen for pre-training rather than ImageNet1K?

- Concept: Regularization Techniques
  - Why needed here: Sparse regularization is distinct from other regularization methods like L1/L2 regularization and batch normalization, and understanding these differences is crucial.
  - Quick check question: How does sparse regularization differ from traditional L1 regularization in terms of the objective function?

## Architecture Onboarding

- Component map:
  - Input patches → Patch and Embedding → Class Token Addition → Transformer Encoder (N layers) → MLP Head
  - Sparse regularization can be inserted at 5 locations: similarity score, attention weight, weighted value, output layer, or MLP input
  - Pruning occurs globally across all weight layers after training

- Critical path:
  - Pre-train on ImageNet21K → Fine-tune on target dataset (CIFAR/ImageNet-100) → Apply sparse regularization at attention weight stage → Global pruning with L1-norm → Evaluate accuracy

- Design tradeoffs:
  - Sparse regularization placement vs accuracy: attention weight stage provides best results but requires precise implementation
  - Pruning percentage vs accuracy: 10-20% provides good balance, 30%+ causes significant accuracy degradation
  - Pre-training dataset size vs effectiveness: larger datasets like ImageNet21K provide better initialization than smaller datasets

- Failure signatures:
  - Accuracy drops immediately after sparse regularization indicates incorrect placement
  - No accuracy improvement from pruning suggests poor weight importance estimation
  - High variance in results across runs may indicate instability in sparse regularization implementation

- First 3 experiments:
  1. Implement Vision Transformer with pre-training on ImageNet21K, fine-tune on CIFAR-10 without any regularization, establish baseline accuracy
  2. Add sparse regularization at attention weight stage, fine-tune on CIFAR-10, compare accuracy to baseline
  3. Apply global pruning (10%) to both baseline and sparse regularization models, compare accuracy degradation patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal placement of sparse regularization within the Vision Transformer architecture for maximizing accuracy improvements?
- Basis in paper: [explicit] The paper investigates five different placements of sparse regularization and finds that applying it after the attention weight calculation in the self-attention layer yields the highest accuracy.
- Why unresolved: While the paper identifies the best placement among the tested options, it does not explore other potential placements or provide a theoretical explanation for why this specific placement is optimal.
- What evidence would resolve it: Further experiments testing additional placements of sparse regularization, coupled with theoretical analysis of the impact on the model's learning dynamics, would provide a more comprehensive understanding of optimal placement strategies.

### Open Question 2
- Question: How does the effectiveness of sparse regularization and pruning vary across different Vision Transformer architectures and datasets?
- Basis in paper: [explicit] The paper primarily focuses on the ViT-B/16 architecture and CIFAR/ImageNet datasets, observing varying degrees of accuracy improvement.
- Why unresolved: The paper's experiments are limited to specific architectures and datasets, leaving open the question of generalizability to other ViT variants and diverse data domains.
- What evidence would resolve it: Extensive experiments applying the proposed techniques to a wide range of ViT architectures and datasets would reveal patterns of effectiveness and potential limitations across different scenarios.

### Open Question 3
- Question: What are the long-term effects of sparse regularization and pruning on model performance and generalization capabilities?
- Basis in paper: [inferred] The paper focuses on short-term accuracy improvements observed during fine-tuning, without investigating the potential impact on the model's ability to generalize to unseen data over extended periods.
- Why unresolved: The experiments conducted in the paper do not address the long-term stability and generalization performance of the pruned models with sparse regularization.
- What evidence would resolve it: Longitudinal studies tracking the performance of pruned models with sparse regularization on diverse datasets and tasks over extended periods would provide insights into their long-term effectiveness and potential limitations.

## Limitations
- The paper lacks detailed implementation specifications for the sparse regularization function and precise architectural parameters
- Effectiveness of sparse regularization and pruning is primarily demonstrated on ViT-B/16 architecture and specific datasets (CIFAR, ImageNet-100)
- The interaction between transfer learning effectiveness and sparse regularization benefits is not empirically validated across different pre-training dataset sizes

## Confidence

**High Confidence**: The claim that sparse regularization improves accuracy by 0.12% on CIFAR-100 and ImageNet-100, and that pruning with sparse regularization increases accuracy by 0.568% on CIFAR-10, 1.764% on CIFAR-100, and 0.256% on ImageNet-100. These results are directly reported from experimental measurements with specific percentages.

**Medium Confidence**: The mechanism that sparse regularization is most effective when placed after attention weight calculation. While the paper reports this finding, it lacks ablation studies comparing all five proposed positions with statistical significance testing.

**Low Confidence**: The claim that transfer learning from ImageNet21K specifically enhances sparse regularization effectiveness. This is asserted but not experimentally validated by comparing against models trained from scratch or with different pre-training datasets.

## Next Checks

1. Implement ablation study testing all five proposed positions for sparse regularization (similarity score, attention weight, weighted value, output layer, MLP input) to verify the claimed optimal placement with statistical significance testing.

2. Conduct controlled experiments comparing sparse regularization effectiveness on models with and without ImageNet21K pre-training to validate the transfer learning interaction claim.

3. Test pruning effectiveness across different pruning percentages (5%, 15%, 25%, 35%) to establish the optimal pruning ratio range and identify the breaking point where accuracy degradation becomes unacceptable.