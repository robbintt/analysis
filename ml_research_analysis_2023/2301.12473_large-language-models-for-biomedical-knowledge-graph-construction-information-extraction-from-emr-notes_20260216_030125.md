---
ver: rpa2
title: 'Large Language Models for Biomedical Knowledge Graph Construction: Information
  extraction from EMR notes'
arxiv_id: '2301.12473'
source_url: https://arxiv.org/abs/2301.12473
tags:
- causal
- which
- notes
- data
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents an end-to-end machine learning solution using
  large language models (LLMs) to construct biomedical knowledge graphs from electronic
  medical record notes. The approach identifies relationships between diseases, treatments,
  factors, and manifestations.
---

# Large Language Models for Biomedical Knowledge Graph Construction: Information extraction from EMR notes

## Quick Facts
- arXiv ID: 2301.12473
- Source URL: https://arxiv.org/abs/2301.12473
- Authors: 
- Reference count: 22
- Key outcome: LLM-based approach extracts biomedical relationships from EMR notes with decoder-only models outperforming encoder-only and encoder-decoder architectures

## Executive Summary
This study presents an end-to-end machine learning solution using large language models (LLMs) to construct biomedical knowledge graphs from electronic medical record notes. The approach identifies relationships between diseases, treatments, factors, and manifestations through two methods: semantic role labeling and open-book question answering. The system is evaluated on age-related macular degeneration, cataract, and branch retinal vein occlusion, measuring precision through manual annotation.

## Method Summary
The method uses LLMs to extract biomedical relationships from clinical notes through two approaches: semantic role labeling (SRL) using RoBERTa and open-book question answering using BERT-based models. Clinical notes are first preprocessed to remove template parts and normalize text. Entities are identified using BioBERT NER, then relations between diseases, treatments, factors, and manifestations are extracted. The extracted relationships are scored and categorized to construct knowledge graphs, with optional refinement using billing data integration.

## Key Results
- Decoder-only LLMs outperform encoder-only and encoder-decoder architectures for biomedical knowledge graph construction
- Open-book QA with clinical note context improves extraction quality compared to prompt-only methods
- Domain-specific LLMs (like BioBERT) show better performance than generic LLMs for medical entity recognition and relation extraction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoder-only LLMs outperform encoder-only and encoder-decoder architectures for biomedical knowledge graph construction from EMR notes.
- Mechanism: Decoder-only models are better at generating structured outputs and handling open-book question answering tasks, which are central to extracting relationships from clinical notes.
- Core assumption: The task requires generation of structured relationships rather than pure classification, making decoder-only models more suitable.
- Evidence anchors:
  - [abstract] "Results show that decoder-only LLMs perform better than encoder-only or encoder-decoder architectures for this task."
  - [section] "Both on our annotated data and during the experiments, open-book QA with the effects questionnaire performed noticeably better."
- Break condition: If the task shifts to primarily classification-based relation extraction, encoder-only models might perform comparably or better.

### Mechanism 2
- Claim: Providing context from clinical notes (open-book QA) improves extraction quality compared to prompt-only methods.
- Mechanism: By giving LLMs access to relevant clinical notes as context, they can ground their answers in actual patient data rather than relying solely on pre-trained knowledge.
- Core assumption: Clinical notes contain sufficient information to answer relationship extraction questions when properly contextualized.
- Evidence anchors:
  - [section] "Unlike [5] which only relies on information contained within a prompt and LM, our methods have access to clinical notes as a context so they have a hint from where to look up the answer."
  - [section] "We offer SRL and open-book QA methods for extraction. On top of it, we emphasize the importance of the context provided to LLMs."
- Break condition: If clinical notes are too sparse or noisy, the context may not provide meaningful improvement over prompt-only methods.

### Mechanism 3
- Claim: Domain-specific LLMs (like BioBERT) perform better than generic LLMs for biomedical knowledge extraction.
- Mechanism: Domain-specific models have been pre-trained on biomedical literature, giving them better understanding of medical terminology and relationships.
- Core assumption: The biomedical domain has sufficient specialized vocabulary and concepts that general-purpose models struggle with.
- Evidence anchors:
  - [section] "Though at the moment we do not have enough annotated data and necessary conducted experiments, we observe comparably weaker results by using generic LMs compared to domain-specific ones."
  - [section] "Motivated by [6] we also assess the adequacy of using billing data to improve the final graph obtained via LMs."
- Break condition: If the biomedical domain becomes saturated with general-purpose models through continued training, the performance gap may narrow.

## Foundational Learning

- Concept: Semantic Role Labeling (SRL)
  - Why needed here: SRL helps identify the semantic roles of entities in sentences, crucial for extracting relationships like "disease treats condition" or "factor causes disease."
  - Quick check question: How does SRL differentiate between the subject and object in a sentence like "Vitamin A treats night blindness"?

- Concept: Open-Book Question Answering
  - Why needed here: This approach allows the model to reference clinical notes when answering questions about relationships, providing grounded answers rather than relying on pre-trained knowledge alone.
  - Quick check question: What's the difference between open-book QA and traditional QA in the context of extracting medical relationships?

- Concept: Knowledge Graph Construction
  - Why needed here: The ultimate goal is to create a structured representation of medical relationships that can be used for applications like drug discovery and clinical trial design.
  - Quick check question: What are the key components needed to construct a knowledge graph from unstructured clinical notes?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline (template removal, normalization) -> Entity recognition (BioBERT NER for disease identification) -> Relation extraction (SRL and open-book QA modules) -> Scoring system (two-side scoring for cause-effect determination) -> Graph construction and refinement (billing data integration)

- Critical path:
  Preprocess clinical notes -> Identify entities -> Extract relations via SRL/QA -> Score and categorize relations -> Construct knowledge graph -> Refine with billing data

- Design tradeoffs:
  Model choice (decoder-only vs encoder-only vs encoder-decoder) vs context length vs computational efficiency vs precision vs recall (influenced by annotation requirements) vs domain-specific vs general-purpose models

- Failure signatures:
  High false positive rate in relation extraction, inability to handle medical terminology variations, poor performance on rare diseases or treatments, over-reliance on billing data leading to incomplete graphs

- First 3 experiments:
  1. Compare decoder-only vs encoder-only performance on a small annotated dataset
  2. Test open-book QA with varying context lengths on relation extraction accuracy
  3. Evaluate domain-specific vs general-purpose models on medical entity recognition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do decoder-only LLMs compare to encoder-only and encoder-decoder architectures for biomedical knowledge graph construction?
- Basis in paper: [explicit] "The results illustrate that in contrast to encoder-only and encoder-decoder, decoder-only LLMs require further investigation."
- Why unresolved: The paper only compares decoder-only LLMs to encoder-only and encoder-decoder architectures without directly comparing their performance.
- What evidence would resolve it: A direct comparison of decoder-only, encoder-only, and encoder-decoder LLMs on the same dataset and task.

### Open Question 2
- Question: How does the size of the questionnaire impact the quality of the knowledge graph?
- Basis in paper: [explicit] "It is noteworthy to say that the evaluation is done on a private dataset containing clinical notes about patient conditions and developments."
- Why unresolved: The paper does not explore the impact of the number of questions per category of relationships on the quality of the KG.
- What evidence would resolve it: An experiment varying the number of questions per category and measuring the resulting KG quality.

### Open Question 3
- Question: How transferable is the proposed method to other datasets and domains?
- Basis in paper: [explicit] "The transferability of the method is yet to be analyzed as data is collected from only one source."
- Why unresolved: The paper only tests the method on a single dataset from a single source.
- What evidence would resolve it: Testing the method on multiple datasets from different sources and domains.

## Limitations

- Evaluation relies entirely on precision metrics without reporting recall, making it impossible to assess completeness
- Sample size limited to only three eye-related diseases with 1430 notes, raising questions about generalizability
- Claims about domain-specific model superiority made without sufficient controlled experiments with annotated data

## Confidence

**High Confidence**: Decoder-only LLMs outperform encoder-only and encoder-decoder architectures for this specific task has strong empirical support through direct experimental comparison.

**Medium Confidence**: Open-book QA with clinical note context improves extraction quality is reasonably supported, though lacks ablation experiments isolating the context effect.

**Medium Confidence**: Domain-specific models perform better than generic LLMs is based on observational comparisons rather than controlled experiments with sufficient annotated data.

**Low Confidence**: Claims about effectiveness of billing data integration for graph refinement are not empirically tested within this study, only referenced from external work.

## Next Checks

1. Conduct a recall assessment using a fully annotated gold standard dataset to determine what percentage of actual relationships are being captured by the LLM-based extraction methods.

2. Apply the same methodology to a completely different medical domain (e.g., cardiovascular or neurological conditions) with comparable note volumes to assess whether the decoder-only advantage and context benefits generalize beyond eye diseases.

3. Design a controlled experiment comparing BioBERT versus general-purpose BERT on the same dataset with sufficient annotation, varying only the pre-training domain while holding all other factors constant to definitively assess domain-specific advantages.