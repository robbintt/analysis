---
ver: rpa2
title: 'Are Large Language Models Good Fact Checkers: A Preliminary Study'
arxiv_id: '2311.17355'
source_url: https://arxiv.org/abs/2311.17355
tags:
- llms
- performance
- evidence
- fact-checking
- fact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a preliminary study on the potential of large
  language models (LLMs) for fact-checking tasks. The authors systematically evaluate
  various LLMs on four subtasks: check-worthiness detection, evidence retrieval, fact
  verification, and explanation generation.'
---

# Are Large Language Models Good Fact Checkers: A Preliminary Study

## Quick Facts
- arXiv ID: 2311.17355
- Source URL: https://arxiv.org/abs/2311.17355
- Reference count: 19
- Primary result: LLMs achieve competitive performance compared to small models in most fact-checking scenarios, particularly excelling in English fact verification and explanation generation tasks.

## Executive Summary
This paper presents a comprehensive evaluation of large language models (LLMs) on fact-checking tasks, examining their performance across four subtasks: check-worthiness detection, evidence retrieval, fact verification, and explanation generation. The study tests multiple LLMs including GPT-3.5-turbo, GLM-6b, GLM-130b, and LLaMa2-7b on three datasets covering both English and Chinese fact-checking scenarios using various prompt engineering techniques. Results demonstrate that while LLMs show promise, particularly in English fact verification and explanation generation, they struggle with Chinese fact verification and the complete fact-checking pipeline due to language inconsistencies and hallucination issues.

## Method Summary
The study systematically evaluates LLMs on three datasets: CheckThat! Lab (195 English tweets), A VeriTeC (English fact verification), and CHEF (Chinese fact verification). Experiments employ 0-shot, 1-shot, and 3-shot settings with multiple prompt types including standard prompts, task-definition-enhanced prompts, and Chain-of-Thought-enhanced prompts. Four LLMs are tested: GPT-3.5-turbo, GLM-6b, GLM-130b, and LLaMa2-7b. Evaluation metrics include F1 score for check-worthiness detection, accuracy for English fact verification, micro- and macro-F1 for Chinese fact verification, and Meteor Score and BLEU-4 for explanation generation tasks.

## Key Results
- GPT-3.5-turbo achieves the best performance in 1-shot and 3-shot settings for English fact verification
- LLMs outperform small fine-tuned models on English fact verification when given 1-3 examples
- Performance significantly degrades on Chinese fact verification due to language inconsistencies between prompts and data
- LLMs can generate relevant evidence for fact verification but suffer from hallucination issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs outperform small fine-tuned models on English fact verification when given a small number of examples (1-3 shots)
- Mechanism: Prompt tuning with few examples allows LLMs to leverage their large-scale pretraining knowledge to generalize better than domain-specific small models
- Core assumption: The pretraining corpus of LLMs includes sufficient domain-relevant knowledge for fact verification
- Evidence anchors:
  - [abstract] "Experiments demonstrate that LLMs achieve competitive performance compared to other small models in most scenarios."
  - [section] "It is shown that GPT-3.5-turbo gets the best performance in 1-shot and 3-shot settings, but there is still a huge gap between LLMs and the SOTA model."

### Mechanism 2
- Claim: Language inconsistency between prompts and data degrades LLM performance on non-English tasks
- Mechanism: LLMs are primarily trained on English data, so when prompts and data use different languages, the model struggles to align semantic understanding
- Core assumption: LLMs have strong language-specific representations but weak cross-language transfer without explicit multilingual training
- Evidence anchors:
  - [abstract] "they encounter challenges in effectively handling Chinese fact verification and the entirety of the fact-checking pipeline due to language inconsistencies and hallucinations."
  - [section] "When the input context is consistent in the same language between prompt and data, the performance will be better."

### Mechanism 3
- Claim: LLMs can generate relevant evidence for fact verification but suffer from hallucination issues
- Mechanism: LLMs use their knowledge base to retrieve or generate evidence, but without grounding in verified sources, they may fabricate plausible-sounding but false information
- Core assumption: The LLM's internal knowledge is not perfectly aligned with factual truth and can generate convincing falsehoods
- Evidence anchors:
  - [abstract] "there is still a long way to go to eliminate hallucination and false information when LLMs are utilized as a knowledge base."
  - [section] "They can flexibly use their knowledge to retrieve relevant evidence to predict the label as well. Nevertheless, there is still a long way to go to eliminate hallucination and false information."

## Foundational Learning

- Concept: Prompt engineering and few-shot learning
  - Why needed here: The study relies on prompt tuning with 0-shot, 1-shot, and 3-shot settings to evaluate LLM performance
  - Quick check question: How does adding examples to a prompt change an LLM's behavior compared to zero-shot prompting?

- Concept: Multilingual model limitations
  - Why needed here: The paper highlights significant performance degradation when using English prompts for Chinese fact verification
  - Quick check question: What architectural differences between monolingual and multilingual LLMs might explain this performance gap?

- Concept: Hallucination in generative models
  - Why needed here: The study identifies hallucination as a key limitation when LLMs generate evidence for fact verification
  - Quick check question: What are the primary causes of hallucination in LLMs, and how might they be mitigated in fact-checking applications?

## Architecture Onboarding

- Component map: Data preprocessing → Prompt construction (standard, CoT-enhanced, task-definition-enhanced) → Model inference (GPT-3.5-turbo, GLM-6b, GLM-130b, LLaMa2-7b) → Evaluation metrics (F1, Accuracy, Meteor, BLEU-4)
- Critical path: Data → Prompt → LLM → Output → Evaluation
- Design tradeoffs: Using few-shot prompting trades precision for flexibility; using larger models trades computational cost for potentially better performance; using different prompt enhancements trades prompt complexity for improved reasoning
- Failure signatures: Performance degradation when language consistency is broken; hallucination in generated evidence; poor few-shot learning when domain knowledge is insufficient in pretraining data
- First 3 experiments:
  1. Run 0-shot fact verification on English data with all four LLMs to establish baseline performance
  2. Test 1-shot and 3-shot performance on English fact verification to measure few-shot learning effectiveness
  3. Evaluate Chinese fact verification with both English and Chinese prompts to confirm language consistency impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be effectively trained or fine-tuned to handle multilingual fact-checking tasks, especially for languages other than English?
- Basis in paper: [explicit] The paper highlights that LLMs struggle with Chinese fact verification and suggests that this is because LLMs are mainly trained on English data
- Why unresolved: The paper does not provide specific methods or techniques for improving LLMs' performance on multilingual fact-checking tasks
- What evidence would resolve it: Experimental results demonstrating the effectiveness of various training or fine-tuning approaches on multilingual fact-checking datasets

### Open Question 2
- Question: What are the most effective methods for mitigating hallucination issues in LLMs when they are used as knowledge bases for evidence retrieval in fact-checking?
- Basis in paper: [explicit] The paper mentions that LLMs can retrieve relevant evidence but still have problems with hallucination, leading to the generation of false information as evidence
- Why unresolved: The paper does not propose specific solutions or techniques to address the hallucination problem in LLMs
- What evidence would resolve it: Comparative studies evaluating the performance of different hallucination mitigation techniques on fact-checking tasks

### Open Question 3
- Question: How can the outputs of LLMs be standardized and formatted to improve their usability and evaluation in fact-checking tasks?
- Basis in paper: [inferred] The paper notes that LLM outputs are not always in the desired form, which poses challenges for evaluation and analysis, especially in classification tasks
- Why unresolved: The paper does not provide concrete suggestions for standardizing or formatting LLM outputs
- What evidence would resolve it: Development and evaluation of standardized output formats and evaluation metrics for LLM-based fact-checking systems

## Limitations
- Small dataset sizes (195 English tweets, 13 English claim-evidence pairs) may not provide sufficient statistical power
- Evaluation relies heavily on few-shot learning settings without exploring larger shot settings
- Paper does not provide detailed prompt templates, making exact replication challenging
- Performance gaps between English and Chinese tasks suggest unexplored cultural and linguistic biases

## Confidence
- **High Confidence**: The observation that LLMs outperform small fine-tuned models on English fact verification tasks with few-shot examples is well-supported by experimental results
- **Medium Confidence**: The claim about language inconsistency causing performance degradation is supported by experimental observations but lacks extensive comparative analysis
- **Low Confidence**: The assertion that LLMs can generate relevant evidence but suffer from hallucinations lacks quantitative measurement of hallucination frequency

## Next Checks
1. Scale Validation: Replicate experiments using larger few-shot settings (5-shot, 10-shot) to determine if the performance gap between LLMs and SOTA models narrows with more examples
2. Language Transfer Analysis: Systematically test cross-lingual transfer by evaluating Chinese fact verification using Chinese prompts, English prompts, and bilingual prompts to isolate the impact of language consistency on performance
3. Hallucination Quantification: Implement a human evaluation protocol to measure the frequency and severity of hallucinations in generated evidence, comparing LLM outputs against verified ground truth to quantify the hallucination problem beyond task accuracy metrics