---
ver: rpa2
title: Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering
arxiv_id: '2305.13691'
source_url: https://arxiv.org/abs/2305.13691
tags:
- document
- question
- data
- answer
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a data synthesis framework for multi-hop question
  answering that requires fewer than 10 human-annotated examples. The method uses
  large language models to generate synthetic training data by pairing related Wikipedia
  documents, generating questions, answering them, and creating queries.
---

# Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering

## Quick Facts
- **arXiv ID:** 2305.13691
- **Source URL:** https://arxiv.org/abs/2305.13691
- **Reference count:** 17
- **Primary result:** Finetuning smaller models (7B, 65B) on synthetic multi-hop QA data achieves performance competitive with much larger models (GPT-3.5).

## Executive Summary
This paper introduces a data synthesis framework that enables few-shot multi-hop question answering by generating synthetic training data using large language models. The approach requires fewer than 10 human-annotated examples and can produce models that rival much larger models in performance. By pairing related Wikipedia documents, generating questions, answering them, and creating queries, the method creates high-quality synthetic datasets that improve both multi-hop QA and fact verification tasks. The framework demonstrates that finetuning smaller models on synthetic data can significantly reduce the performance gap between different model sizes.

## Method Summary
The method uses a large language model (LLaMA 65B) to generate synthetic training data through a multi-step process: question generation from document pairs, question answering, query generation, and query verification using retrievers. The synthetic data is then used to finetune smaller models (7B and 65B LLaMA). The approach includes filtering steps to ensure data quality and can be extended to generate fact verification data by pairing questions with contradictory statements. The synthetic data is mixed with plain Wikipedia text during finetuning.

## Key Results
- Finetuning on synthetic data dramatically improves performance, allowing finetuned LLaMA 65B to achieve results similar to much larger models
- Finetuning effectively reduces the performance gap between 7B- and 65B-parameter LLaMA models
- Automatic filtering steps are crucial for improving model performance
- Synthetic data improves retrieval accuracy compared to prior methods

## Why This Works (Mechanism)

### Mechanism 1
Few-shot data synthesis using LLMs can produce training data that allows smaller models to match performance of much larger ones. The LLMs generate high-quality synthetic question-answer-query triples that capture multi-hop reasoning patterns, enabling effective finetuning of smaller models. Core assumption: The synthetic data distribution is sufficiently similar to real data distribution that finetuning generalizes well.

### Mechanism 2
The multi-step generation process (question generation → answering → query generation → verification) creates higher quality synthetic data than single-step approaches. Each generation step filters and refines the output, with query verification ensuring retrieved documents match intended answer paths. Core assumption: Each generation step produces outputs that maintain coherence with previous steps and correct answers.

### Mechanism 3
Finetuning on synthetic data effectively reduces the performance gap between different model sizes. Smaller models can learn multi-hop reasoning patterns from high-quality synthetic data that they cannot easily learn through in-context learning alone. Core assumption: The synthetic data contains sufficient multi-hop reasoning examples to teach smaller models the required patterns.

## Foundational Learning

- **Multi-hop reasoning in question answering**: The entire framework is built around generating and answering questions that require multiple reasoning steps. Quick check: What distinguishes a multi-hop question from a single-hop question in terms of required information sources?

- **Retrieval-augmented generation (RAG)**: The approach relies on retrievers to verify generated queries against Wikipedia documents. Quick check: How does query verification using retrievers ensure the quality of synthetic training data?

- **Prompt engineering and in-context learning**: The data generation process depends on carefully crafted prompts to guide LLM outputs. Quick check: What are the key differences between the prompts used for question generation versus query generation?

## Architecture Onboarding

- **Component map**: LLM (65B LLaMA) → Question Generation → Question Answering → Query Generation → Query Verification → Synthetic Dataset → Finetuning (7B/65B LLaMA) → Evaluation
- **Critical path**: Data synthesis pipeline → Finetuning → Inference on downstream tasks
- **Design tradeoffs**: Large generation model (65B) vs. smaller finetuned models (7B) - the approach trades upfront generation cost for efficient inference
- **Failure signatures**: Poor downstream performance typically indicates issues in synthetic data quality, insufficient filtering, or mismatch between synthetic and real data distributions
- **First 3 experiments**: 1) Generate 100 questions with basic prompt and evaluate question quality and answerability; 2) Run full pipeline with small dataset (10k examples) and finetune 7B model to verify training pipeline works; 3) Compare synthetic vs. real data finetuning performance on a single benchmark task to establish baseline effectiveness

## Open Questions the Paper Calls Out

1. What is the optimal amount of synthetic data needed for finetuning to achieve the best performance across different model sizes? The paper notes that the most significant gains are from the initial 100k examples, after which improvements plateau, but leaves finding the exact optimal amount for future work.

2. How does the quality of synthetic data change when using smaller LLMs (e.g., 7B instead of 65B) for generation, and does this impact downstream finetuning performance? The paper uses LLaMA 65B for data synthesis but doesn't explore the trade-off between generation cost and data quality when using smaller models.

3. Can the synthetic data generation framework be extended to create multi-hop questions with more than two hops while maintaining answerability and quality? The paper focuses on single- and two-hop questions as prior work found questions with more than two hops can be difficult to understand even for human readers.

## Limitations
- The entire approach depends heavily on the quality of synthetic data generation, with limited analysis of generation failure modes
- Limited human evaluation of synthetic data quality beyond basic answerability checks
- Experiments focus on Wikipedia-based tasks without addressing domain generalization to other document types

## Confidence
- **High Confidence**: The core finding that finetuning on synthetic data significantly improves performance for smaller models is well-supported by experimental results
- **Medium Confidence**: The claim that smaller models can rival much larger ones when trained on well-synthesized data is supported but requires careful interpretation of comparisons to GPT-3.5 baselines
- **Low Confidence**: The assertion that this approach is particularly effective for "few-shot" settings lacks rigorous validation with systematic variation of human-annotated examples

## Next Checks
1. **Synthetic Data Quality Audit**: Randomly sample 100 synthetic QA pairs and conduct detailed human evaluation on logical soundness, required reasoning steps, and document retrieval relevance to quantify generation pipeline quality.

2. **Few-Shot Scaling Study**: Systematically vary the number of human-annotated examples (0, 5, 10, 25, 50) and measure performance degradation/gains to establish actual few-shot limits and minimum viable annotation budget.

3. **Domain Transfer Experiment**: Apply the same pipeline to a non-Wikipedia domain (e.g., scientific literature or medical documents) and measure synthetic data quality changes, finetuning effectiveness, and required modifications to prompts or filtering.