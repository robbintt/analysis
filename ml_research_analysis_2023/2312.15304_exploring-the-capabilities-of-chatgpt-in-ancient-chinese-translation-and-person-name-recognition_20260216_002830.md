---
ver: rpa2
title: Exploring the Capabilities of ChatGPT in Ancient Chinese Translation and Person
  Name Recognition
arxiv_id: '2312.15304'
source_url: https://arxiv.org/abs/2312.15304
tags:
- chinese
- chatgpt
- ancient
- translation
- sent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates ChatGPT's performance on ancient Chinese translation
  and name recognition tasks using the Shi Shuo Xin Yu text. For translation, ChatGPT
  achieved up to 20.19% BLEU-2 and 78.5% F1 score when given three context sentences.
---

# Exploring the Capabilities of ChatGPT in Ancient Chinese Translation and Person Name Recognition

## Quick Facts
- arXiv ID: 2312.15304
- Source URL: https://arxiv.org/abs/2312.15304
- Reference count: 5
- Key outcome: ChatGPT achieved up to 20.19% BLEU-2 and 78.5% F1 score for translation with 3-sentence context, and 77.58% F1 score for name recognition, outperforming Jieba baseline

## Executive Summary
This paper evaluates ChatGPT's performance on ancient Chinese translation and name recognition tasks using the Shi Shuo Xin Yu text. The study finds that ChatGPT performs best on translation tasks when provided with three context sentences, achieving BLEU-2 scores up to 20.19% and F1 scores of 78.5%. For name recognition, ChatGPT significantly outperforms the Jieba baseline with an F1 score of 77.58%. However, the results indicate that ChatGPT's proficiency in ancient Chinese tasks remains limited, suggesting the need for further research and potential fine-tuning on specialized ancient Chinese corpora.

## Method Summary
The study uses API calls to ChatGPT with varying context lengths (1, 3, 5, 8 sentences) to evaluate translation and name recognition performance on the Shi Shuo Xin Yu text. For translation, BLEU and BERTScore metrics are calculated against human references, while name recognition uses F1 score comparing ChatGPT's outputs to ground truth annotations. A baseline comparison is conducted using Jieba for name recognition on 300 randomly selected sentences. The experiments test different prompt formulations and context window sizes to determine optimal performance conditions.

## Key Results
- ChatGPT achieved optimal translation performance with 3-sentence context (20.19% BLEU-2, 78.5% F1)
- Name recognition performance reached 77.58% F1 score, significantly outperforming Jieba baseline (48.43%)
- Performance degrades with longer context windows (5 or 8 sentences) for translation tasks
- ChatGPT's proficiency in ancient Chinese remains below satisfactory levels despite contextual advantages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT's translation performance improves with moderate context length (3 sentences) due to better disambiguation of polysemous ancient Chinese terms.
- Mechanism: Providing additional sentences helps the model resolve ambiguity by giving broader semantic context, reducing errors in translation.
- Core assumption: Ancient Chinese has high polysemy and context-dependent meaning that requires multi-sentence understanding.
- Evidence anchors:
  - [abstract] states "ChatGPT performs the best on ancient-to-modern translation when feeding with three context sentences."
  - [section] shows "ChatGPT renders the most proficient translations... when it is engaged with three sequential ancient Chinese sentences (3-sent.)"
  - [corpus] indicates related works focus on BERT-based models for ancient Chinese, suggesting context is crucial.
- Break condition: If context exceeds optimal length (3 sentences), translation quality degrades as shown in Table 2 where 5-sent. and 8-sent. perform worse than 3-sent.

### Mechanism 2
- Claim: ChatGPT achieves better name recognition than Jieba baseline due to its contextual understanding capabilities.
- Mechanism: Large language models can leverage surrounding context to disambiguate names versus common words, while rule-based systems like Jieba rely on static dictionaries.
- Core assumption: Ancient Chinese texts often use abbreviated or context-dependent name references that require understanding of the full sentence.
- Evidence anchors:
  - [abstract] shows ChatGPT achieved 77.58% F1 score compared to Jieba's 48.43%.
  - [section] states "ChatGPT performs reasonably well on the personal name recognition task" and "much better than Jieba."
  - [corpus] evidence is limited as no direct comparison studies are cited.
- Break condition: When sentences omit names or use pronouns, even ChatGPT struggles as "understanding ancient Chinese sentences rely heavily on the context of the sentence."

### Mechanism 3
- Claim: ChatGPT's limitations in ancient Chinese stem from training data bias toward modern languages.
- Mechanism: The model's pre-training corpus contains limited ancient Chinese text, resulting in poor understanding of historical language patterns.
- Core assumption: Pre-training data distribution determines performance on specialized domains.
- Evidence anchors:
  - [abstract] concludes "the proficiency of ancient Chinese by ChatGPT is yet to reach a satisfactory level."
  - [section] states "One possible explanation might be that ChatGPT is mainly pre-trained on English corpora with limited amount of Chinese corpora."
  - [corpus] shows related works use specialized models like SikuBERT and GujiBERT trained on ancient text corpora.
- Break condition: If fine-tuning on ancient Chinese corpora occurs, performance should improve beyond current limitations.

## Foundational Learning

- Concept: BLEU score calculation and interpretation
  - Why needed here: Used to evaluate translation quality against human references
  - Quick check question: What does a BLEU-4 score of 0 indicate about translation quality?

- Concept: F1 score calculation for named entity recognition
  - Why needed here: Used to measure name recognition performance balancing precision and recall
  - Quick check question: How does F1 score differ from simply averaging precision and recall?

- Concept: Context window effects on transformer performance
  - Why needed here: Explains why different input lengths yield varying results
  - Quick check question: Why might longer context windows sometimes degrade performance in language models?

## Architecture Onboarding

- Component map: Sentence → Prompt construction → ChatGPT API call → Result extraction → Evaluation metric calculation
- Critical path: Ancient Chinese text → Preprocessing → Prompt engineering → ChatGPT API → Evaluation (BLEU/BERTScore/F1)
- Design tradeoffs: Using GPT-3.5-Turbo vs GPT-4 for cost vs performance, sentence-level vs multi-sentence context, manual vs automatic evaluation
- Failure signatures: BLEU scores of 0 across all n-grams, inconsistent translations for similar inputs, API rate limiting errors
- First 3 experiments:
  1. Test single-sentence translation with varying prompt formulations
  2. Compare 1-sentence vs 3-sentence context for the same ancient Chinese text
  3. Run Jieba baseline on the same 300-sentence name recognition dataset for comparison

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ChatGPT's performance on ancient Chinese translation improve with additional fine-tuning on domain-specific ancient Chinese corpora?
- Basis in paper: [explicit] The paper notes that ChatGPT is "mainly pre-trained on English corpora with limited amount of Chinese corpora" and suggests that "engagements of deeper research and enhancements - coupled with an abundant corpus of ancient Chinese and specialized knowledge - can ameliorate its proficiency in comprehending and translating ancient Chinese text."
- Why unresolved: The paper only evaluated ChatGPT's base capabilities without any domain-specific fine-tuning or adaptation to ancient Chinese.
- What evidence would resolve it: Comparative experiments showing ChatGPT's performance on ancient Chinese translation before and after fine-tuning on a large ancient Chinese corpus.

### Open Question 2
- Question: How does ChatGPT's name recognition performance compare to other specialized named entity recognition models trained specifically on ancient Chinese texts?
- Basis in paper: [explicit] The paper compares ChatGPT to Jieba for name recognition but notes that "ChatGPT also faces many challenges in recognizing personal names from ancient Chinese texts, because understanding ancient Chinese sentences rely heavily on the context of the sentence."
- Why unresolved: Only a basic baseline (Jieba) was used for comparison, and no specialized ancient Chinese NER models were evaluated.
- What evidence would resolve it: Head-to-head comparison of ChatGPT with state-of-the-art ancient Chinese NER models on the same dataset.

### Open Question 3
- Question: What is the optimal context length for ancient Chinese translation tasks, and does this vary based on text complexity or genre?
- Basis in paper: [explicit] The paper found that ChatGPT performed best with three context sentences but did not systematically explore the relationship between context length, text complexity, and translation quality.
- Why unresolved: The study only tested fixed context lengths (1, 3, 5, 8 sentences) without varying text complexity or analyzing the relationship between these factors.
- What evidence would resolve it: Experiments varying both context length and text complexity across different genres of ancient Chinese literature.

## Limitations
- Limited dataset scope: Only 300 sentences evaluated for name recognition may not capture full complexity of ancient Chinese naming conventions
- Low absolute performance metrics: BLEU-2 score of 20.19% indicates fundamental challenges with phrase-level matching in translation
- Unproven training data hypothesis: Claim about ChatGPT's poor performance stemming from limited ancient Chinese training data lacks quantitative analysis

## Confidence
- Translation performance claims (Medium confidence): While the trend of optimal performance at 3-sentence context is supported by data, the absolute BLEU scores remain low. The 78.5% F1 score for single-sentence translation may be inflated due to evaluation artifacts or prompt engineering effects.
- Name recognition superiority (High confidence): The 77.58% F1 score compared to Jieba's 48.43% provides strong evidence for ChatGPT's advantage in contextual understanding, though this assumes accurate manual annotation of the 300-sentence test set.
- Training data limitation hypothesis (Low confidence): The claim that ChatGPT's poor performance stems from limited ancient Chinese training data is plausible but unproven. No quantitative analysis of the model's pre-training corpus distribution is provided.

## Next Checks
1. **Statistical significance testing**: Run multiple iterations of the same prompts to ChatGPT to determine the variance in outputs and calculate confidence intervals for the BLEU and F1 scores.

2. **Fine-tuning experiment**: Test whether fine-tuning ChatGPT on a small ancient Chinese corpus improves performance metrics by at least 10% absolute improvement on both tasks.

3. **Error analysis**: Manually categorize translation and name recognition errors into types (polysemy, historical context, character ambiguity) to identify whether specific linguistic phenomena consistently challenge the model.