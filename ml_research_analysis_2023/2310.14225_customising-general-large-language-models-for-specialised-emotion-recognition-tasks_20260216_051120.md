---
ver: rpa2
title: Customising General Large Language Models for Specialised Emotion Recognition
  Tasks
arxiv_id: '2310.14225'
source_url: https://arxiv.org/abs/2310.14225
tags:
- emotion
- recognition
- language
- adaptation
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the effectiveness of adapting general
  large language models (LLMs) for specialized emotion recognition tasks. The research
  focuses on customizing the ChatGLM2 model using two model adaptation techniques:
  deep prompt tuning (P-Tuning v2) and low-rank adaptation (LoRA).'
---

# Customising General Large Language Models for Specialised Emotion Recognition Tasks

## Quick Facts
- arXiv ID: 2310.14225
- Source URL: https://arxiv.org/abs/2310.14225
- Authors: 
- Reference count: 0
- Key outcome: This study investigates the effectiveness of adapting general large language models (LLMs) for specialized emotion recognition tasks.

## Executive Summary
This research explores the adaptation of general large language models for specialized emotion recognition tasks using two model adaptation techniques: deep prompt tuning (P-Tuning v2) and low-rank adaptation (LoRA). The study customizes the ChatGLM2 model and evaluates its performance across six publicly available datasets in both English and Chinese languages. Results demonstrate that both adaptation methods significantly improve the LLM's performance in emotion recognition, with adapted models outperforming non-adapted versions and matching or surpassing state-of-the-art specialized models in many cases. The findings suggest that LLMs can effectively transfer their knowledge to emotion recognition tasks with minimal training data and computational resources, offering a promising approach for future emotion recognition systems.

## Method Summary
The study customizes the ChatGLM2 model using two model adaptation techniques: deep prompt tuning (P-Tuning v2) and low-rank adaptation (LoRA). Experiments were conducted on six publicly available datasets spanning English and Chinese languages, with varying numbers of emotion classes and contextual requirements. The research evaluates the effectiveness of these adaptation methods by comparing the performance of adapted models against non-adapted versions and state-of-the-art specialized models. The study uses accuracy and macro-F1 scores as primary evaluation metrics, with weighted average F1 score used for the M3ED dataset.

## Key Results
- Both P-Tuning v2 and LoRA significantly improve LLM performance in emotion recognition tasks
- Adapted models outperform non-adapted ChatGLM2 and match or surpass state-of-the-art specialized models
- P-Tuning v2 shows better performance on multi-class classification tasks, while LoRA offers memory efficiency without inference latency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep prompt tuning (P-Tuning v2) improves emotion recognition by adding trainable continuous prompts at every transformer layer, increasing the number of task-specific parameters.
- Mechanism: The model freezes all original weights and injects continuous prompts into each layer. This gives the model more direct influence over predictions and expands the adaptation capacity without full fine-tuning.
- Core assumption: The intrinsic rank of the weight updates during adaptation is low, so low-rank decomposition can still represent the needed changes effectively.
- Evidence anchors:
  - [abstract] "deep prompt tuning (P-Tuning v2) and low-rank adaptation (LoRA)"
  - [section] "P-Tuning v2 technique incorporates continuous prompts into every layer of the model"
  - [corpus] No direct corpus evidence; claims based on internal citations.
- Break condition: If the model is too large for the GPU to handle the extra prompt parameters, or if the emotion task requires full parameter updates rather than low-rank adjustments.

### Mechanism 2
- Claim: Low-rank adaptation (LoRA) reduces the number of trainable parameters by decomposing weight updates into low-rank matrices.
- Mechanism: LoRA introduces small trainable matrices (A and B) into each transformer layer, keeping original weights frozen. This enables efficient fine-tuning with minimal memory usage.
- Core assumption: The pretrained weights are in a low intrinsic dimension space, so random projections into smaller subspaces can still preserve learning capacity.
- Evidence anchors:
  - [abstract] "low-rank adaptation (LoRA)"
  - [section] "LoRA freezes the pre-trained model weights and injects trainable low-rank decomposition matrices into each layer"
  - [corpus] No direct corpus evidence; references [8] for the claim.
- Break condition: If the target task has very high complexity or requires nuanced weight changes that low-rank matrices cannot capture.

### Mechanism 3
- Claim: Adapted LLMs can outperform specialized emotion recognition models on simple classification tasks even without context modeling.
- Mechanism: The broad pretraining of LLMs provides rich linguistic knowledge that transfers to emotion recognition when adapted with prompt tuning or LoRA, especially for tasks with fewer classes.
- Core assumption: Emotion recognition benefits from general language understanding, and simple classification tasks do not require contextual dialogue modeling.
- Evidence anchors:
  - [abstract] "adapted models outperforming non-adapted versions and matching or surpassing state-of-the-art specialized models"
  - [section] "ChatGLM2 performs competitively with these specialised models in many datasets"
  - [corpus] No direct corpus evidence; inferred from tables showing performance comparisons.
- Break condition: If emotion recognition requires long context or multi-turn dialogue understanding, the LLM adaptation may underperform specialized models.

## Foundational Learning

- Concept: Prompt tuning vs. full fine-tuning
  - Why needed here: Understanding the efficiency and trade-offs of parameter-efficient fine-tuning methods is key to grasping why P-Tuning v2 and LoRA work.
  - Quick check question: What is the main difference between prompt tuning and full fine-tuning in terms of trainable parameters?

- Concept: Low-rank decomposition in LoRA
  - Why needed here: Knowing how low-rank matrices approximate weight updates helps explain the computational savings and limitations.
  - Quick check question: Why does LoRA assume that weight updates have low intrinsic rank?

- Concept: Emotion classification metrics
  - Why needed here: Accuracy and macro-F1 are used to evaluate performance; understanding their meaning is essential for interpreting results.
  - Quick check question: How does macro-F1 differ from accuracy in imbalanced datasets?

## Architecture Onboarding

- Component map: LLM backbone (ChatGLM2-6B) → Prompt tuning module (P-Tuning v2) or LoRA adapters → Emotion classification head
- Critical path: Input text → Embedding → Transformer layers (with prompts/adapters) → Classification logits → Softmax → Predicted emotion label
- Design tradeoffs: P-Tuning v2 uses more parameters than LoRA but may perform better on multi-class tasks; LoRA is more memory-efficient and introduces no inference latency.
- Failure signatures: Poor performance on context-heavy datasets; overfitting on small datasets; GPU memory overflow when prompt length or rank is too high.
- First 3 experiments:
  1. Run baseline ChatGLM2 without adaptation on a binary emotion dataset and record accuracy/F1.
  2. Apply P-Tuning v2 with rank 8 and prompt length 32, train on same dataset, compare results.
  3. Replace P-Tuning v2 with LoRA (rank 8), train, and compare performance and memory usage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different model adaptation techniques (P-Tuning v2 vs LoRA) perform across various emotion recognition tasks with different levels of contextual dependency?
- Basis in paper: [explicit] The paper compares P-Tuning v2 and LoRA performance across six datasets with varying contextual requirements
- Why unresolved: The paper shows performance differences but doesn't systematically analyze how contextual dependency affects each adaptation method's effectiveness
- What evidence would resolve it: A comprehensive study varying context availability and task complexity while keeping other factors constant, measuring adaptation method performance

### Open Question 2
- Question: What is the optimal balance between model size and adaptation technique for emotion recognition tasks in terms of computational efficiency and accuracy?
- Basis in paper: [inferred] The paper uses ChatGLM2-6B and mentions computational constraints but doesn't explore different model sizes or adaptation efficiency trade-offs
- Why unresolved: The study focuses on one model size and doesn't compare computational costs across different adaptation methods or model scales
- What evidence would resolve it: Systematic experiments varying model sizes and measuring both accuracy and computational efficiency across adaptation techniques

### Open Question 3
- Question: How do adaptation techniques perform when dealing with domain-specific emotional expressions and cultural nuances in different languages?
- Basis in paper: [explicit] The paper uses both English and Chinese datasets but doesn't analyze cross-linguistic or cross-cultural adaptation effectiveness
- Why unresolved: The multilingual aspect is mentioned but not thoroughly investigated for cultural or domain-specific emotional expressions
- What evidence would resolve it: Comparative studies across diverse languages and cultural contexts, measuring adaptation effectiveness for domain-specific emotional expressions

## Limitations
- Limited evaluation diversity: The study evaluates only six datasets, which may not represent the full range of emotion recognition challenges.
- No ablation of adaptation depth: The paper does not explore how many layers to adapt or the impact of varying prompt lengths and LoRA ranks systematically.
- Single backbone model: Only ChatGLM2-6B is tested, limiting generalizability to other LLMs like LLaMA, GPT, or PaLM.

## Confidence
- High confidence: The core claim that prompt tuning and LoRA improve emotion recognition accuracy is well-supported by direct experimental results across multiple datasets.
- Medium confidence: The assertion that adapted LLMs can match or surpass state-of-the-art specialized models is supported but may not generalize to all emotion recognition scenarios.
- Low confidence: The claim about computational efficiency (minimal training data and resources) lacks quantitative comparison with full fine-tuning baselines.

## Next Checks
1. **Ablation study**: Systematically vary LoRA rank and prompt length to determine optimal adaptation configurations for different dataset sizes and emotion class counts.
2. **Cross-model generalization**: Apply the same adaptation pipeline to a different LLM (e.g., LLaMA-7B) and evaluate performance consistency across all six datasets.
3. **Robustness evaluation**: Test adapted models on out-of-distribution emotion datasets or adversarial examples to assess real-world reliability.