---
ver: rpa2
title: 'Constrained Bayesian Optimization Under Partial Observations: Balanced Improvements
  and Provable Convergence'
arxiv_id: '2312.03212'
source_url: https://arxiv.org/abs/2312.03212
tags:
- optimization
- function
- exploration
- design
- feasible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a new constrained Bayesian optimization (CBO)
  method, CBOB, for partially observable constrained optimization problems (POCOPs).
  CBOB introduces two key innovations: (1) a balanced exploration strategy via a novel
  acquisition function, EICB, which mitigates over-exploitation on known feasible
  regions by incorporating an exploration function; (2) a Gaussian process model with
  heterogeneous likelihoods (HLGP) that leverages mixed observations (feasible and
  infeasible) to better represent unknown constraints compared to traditional classification-based
  models.'
---

# Constrained Bayesian Optimization Under Partial Observations: Balanced Improvements and Provable Convergence

## Quick Facts
- arXiv ID: 2312.03212
- Source URL: https://arxiv.org/abs/2312.03212
- Reference count: 40
- One-line primary result: CBOB introduces balanced exploration and HLGP models for partially observable constrained optimization, achieving better global search and efficiency than state-of-the-art CBO methods.

## Executive Summary
This paper addresses the challenge of constrained Bayesian optimization (CBO) under partial observability, where constraints and objectives are only observable within feasible regions. The authors propose CBOB, a method that introduces two key innovations: a balanced exploration strategy via the EICB acquisition function and a Gaussian process model with heterogeneous likelihoods (HLGP) that leverages mixed observations for better constraint modeling. Theoretical convergence guarantees are provided, and empirical results on synthetic and real-world problems demonstrate CBOB's competitiveness against state-of-the-art CBO methods, with improved optimization efficiency and global search capability.

## Method Summary
CBOB tackles partially observable constrained optimization problems (POCOPs) by combining a balanced exploration acquisition function (EICB) with a heterogeneous likelihood Gaussian process (HLGP) model. EICB augments expected improvement (EI) with a dynamic exploration function (DPOF) that encourages exploration of uncertain or boundary regions, reducing over-exploitation on known feasible areas. HLGP uses expectation propagation to handle mixed observations (feasible and infeasible) by attaching Gaussian likelihoods to feasible values and truncated distributions to infeasible ones, providing a more accurate surrogate for constraints than classification-based models. The method is validated on synthetic benchmarks and real-world applications, showing improved optimization efficiency and global search.

## Key Results
- CBOB demonstrates better optimization efficiency and global search capability compared to state-of-the-art CBO methods.
- The method achieves higher ratios of feasible evaluations (ROF) and better best observed values (BOV) on benchmark problems.
- CBOB's balanced exploration strategy effectively mitigates over-exploitation on known feasible regions, even with a lower ratio of feasible evaluations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The balanced exploration strategy (DPOF) reduces over-exploitation on known feasible regions by dynamically weighting exploration potential and feasibility.
- Mechanism: DPOF modifies the traditional POF by multiplying it with an exploration function ρ_i that assigns higher weights to uncertain or boundary regions. This prevents the optimizer from clustering evaluations in already-known feasible areas and encourages global search.
- Core assumption: Partial observability makes infeasible regions uninformative; therefore, guiding exploration toward uncertain boundaries improves global search without sacrificing feasibility.
- Evidence anchors:
  - [abstract] "introduces balanced exploration during optimization" and "augments the EI with a more balanced constraint handling technique."
  - [section] "we posit that prioritizing search towards less explored regions can enhance exploratory capability, thus promoting more global search behaviors in a CBO method."
  - [corpus] Weak; no corpus neighbors directly discuss balanced exploration in CBO, suggesting this is a novel contribution.
- Break condition: If the exploration function ρ_i is poorly tuned (e.g., too aggressive), it may lead to excessive exploration of infeasible regions, reducing optimization efficiency.

### Mechanism 2
- Claim: HLGP models leverage mixed observations (feasible and infeasible) to build a more accurate surrogate of unknown constraints than classification-based models like GPC.
- Mechanism: HLGP attaches heterogeneous likelihood distributions to each observation—Gaussian for feasible values, truncated for infeasible ones—allowing the model to incorporate all available data, not just binary feasibility labels.
- Core assumption: Infeasible observations still carry information about constraint boundaries, and a regression-based model can extract this information better than a classifier.
- Evidence anchors:
  - [abstract] "Gaussian process embedding different likelihoods as the surrogate model for a partially observable constraint."
  - [section] "observations are composed of two distinct aspects: i) the actual values associated with feasible solutions, and ii) the truncated distribution of possible values for all solutions."
  - [corpus] Weak; no corpus neighbors discuss HLGP or mixed-observation surrogate modeling.
- Break condition: If the EP approximation in HLGP fails to converge or the likelihood model is misspecified, the surrogate accuracy may degrade.

### Mechanism 3
- Claim: The EICB acquisition function preserves the asymptotic convergence properties of EI-based methods while introducing balanced exploration.
- Mechanism: EICB is a product of EI and DPOF, inheriting EI's stepwise uncertainty reduction (SUR) structure and convergence proof, while DPOF adjusts the feasibility weighting to encourage exploration.
- Core assumption: The convergence proof for EI-based methods can be extended to EICB because DPOF does not break the supermartingale property of the underlying SUR process.
- Evidence anchors:
  - [abstract] "rigorously study the convergence properties of this design to demonstrate its effectiveness."
  - [section] "Theorem 1... suggests that the incorporation of ρi as designed in equation (6) does not undermine the asymptotic convergence capability of EI-based acquisition functions."
  - [corpus] Weak; no corpus neighbors discuss theoretical convergence of CBO methods with partial observations.
- Break condition: If the smoothness or boundedness assumptions on ρ_i are violated, the convergence proof may not hold.

## Foundational Learning

- Concept: Gaussian Process regression and classification
  - Why needed here: CBOB uses GP models for both objective and constraint surrogates; understanding GPR and GPC is essential to grasp HLGP and model differences.
  - Quick check question: What is the difference between a Gaussian Process classifier and a Gaussian Process regressor in terms of output type and likelihood function?

- Concept: Bayesian Optimization acquisition functions
  - Why needed here: EICB extends EI with constraints; knowing how EI, EIC, and entropy-based methods work is key to understanding the balanced exploration contribution.
  - Quick check question: How does the expected improvement (EI) acquisition function balance exploration and exploitation?

- Concept: Expectation Propagation for non-Gaussian likelihoods
  - Why needed here: HLGP uses EP to handle truncated likelihoods; understanding EP is necessary to follow the inference process and potential computational trade-offs.
  - Quick check question: In what way does Expectation Propagation approximate a non-Gaussian likelihood with a Gaussian site?

## Architecture Onboarding

- Component map:
  - Acquisition function layer: EICB = EI × DPOF
  - Surrogate modeling layer: GPR for objective, HLGP for constraints
  - Inference layer: EP for HLGP site parameter updates
  - Optimization loop: Sobol initialization → sequential candidate selection → model update

- Critical path:
  1. Build GPR model for objective f(x).
  2. For each constraint g_i, update observations with EP and build HLGP.
  3. Compute EICB(˜x) for candidate ˜x.
  4. Select arg max EICB → evaluate → update data.

- Design tradeoffs:
  - Exploration vs. exploitation: DPOF trades off aggressive exploration (like level-set methods) for balanced exploration to maintain feasibility.
  - Model complexity: HLGP uses EP to keep inference tractable but may be less accurate than exact non-Gaussian inference.
  - Computational cost: EICB preserves EI's efficiency but HLGP inference adds overhead per constraint.

- Failure signatures:
  - Poor exploration: If EICB gets stuck evaluating only known feasible points, check ρ_i values and β tuning.
  - Model misspecification: If HLGP predictions are inaccurate, verify EP convergence and likelihood parameterization.
  - Numerical instability: Watch for singular K matrices or EP site parameter divergence; consider adding jitter or reinitializing.

- First 3 experiments:
  1. Run CBOB on a synthetic 1D problem (e.g., Ackley with one constraint) with full observability to verify basic functionality.
  2. Run CBOB on the same problem with partial observability to test HLGP and DPOF behavior.
  3. Compare EICB vs. EIC and EICB vs. MESCh on a benchmark (e.g., XGB-H) to measure exploration benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of CBOB compare to other constrained Bayesian optimization methods in theory?
- Basis in paper: [explicit] The paper studies the asymptotic convergence of EICB, but does not provide a definitive answer on whether it outperforms other methods at the convergence rate.
- Why unresolved: The paper focuses on proving asymptotic convergence but does not analyze the rate of convergence compared to other methods.
- What evidence would resolve it: A theoretical analysis comparing the convergence rates of CBOB and other constrained Bayesian optimization methods would provide a definitive answer.

### Open Question 2
- Question: How does the performance of CBOB change with different exploration function designs for individual optimization problems?
- Basis in paper: [explicit] The paper mentions that CBOB preserves flexibility in designing exploration functions, making it adaptable to individual optimization problems.
- Why unresolved: The paper does not provide empirical evidence on how different exploration function designs affect CBOB's performance on specific problems.
- What evidence would resolve it: Empirical studies comparing CBOB's performance using different exploration function designs on various optimization problems would provide insights into its adaptability.

### Open Question 3
- Question: How does the uncertainty handling of HLGP models compare to other surrogate models for constraints in partially observable scenarios?
- Basis in paper: [explicit] The paper introduces HLGP models and compares them to GPC models, highlighting their better representation of unknown constraints.
- Why unresolved: The paper does not provide a comprehensive comparison of HLGP models with other surrogate models for constraints in partially observable scenarios.
- What evidence would resolve it: A thorough comparison of HLGP models with other surrogate models for constraints in partially observable scenarios would provide insights into their relative performance.

## Limitations
- The practical robustness of the expectation propagation inference in HLGP under real-world conditions remains unverified.
- The experimental validation is limited to synthetic benchmarks and two real-world applications, which may not fully capture the method's scalability or generalization to higher-dimensional problems.
- The computational efficiency and numerical stability of the HLGP model with EP inference in high-dimensional or highly constrained scenarios are not thoroughly evaluated.

## Confidence
- **High**: The theoretical convergence analysis of EICB, supported by rigorous proof and the preservation of EI's stepwise uncertainty reduction property.
- **Medium**: The empirical performance improvements over state-of-the-art CBO methods, given the controlled experimental setup and benchmark selection.
- **Low**: The computational efficiency and numerical stability of the HLGP model with EP inference in high-dimensional or highly constrained scenarios.

## Next Checks
1. **EP Inference Robustness**: Test the HLGP model's expectation propagation inference on a diverse set of synthetic POCOPs with varying constraint complexities and observation noise levels to assess numerical stability and convergence.
2. **Scalability to Higher Dimensions**: Evaluate CBOB's performance on high-dimensional benchmark problems (e.g., from the COCO platform) to verify its scalability and identify potential computational bottlenecks.
3. **Real-World Applicability**: Apply CBOB to a real-world constrained optimization problem from engineering or scientific domains (e.g., structural design or chemical process optimization) to assess its practical utility and robustness to model misspecification.