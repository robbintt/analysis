---
ver: rpa2
title: 'Analogical Proportions and Creativity: A Preliminary Study'
arxiv_id: '2310.13500'
source_url: https://arxiv.org/abs/2310.13500
tags:
- analogical
- proportions
- animals
- creativity
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the creative power of analogical proportions
  to generate novel concepts from existing ones. Using a zoo dataset, the authors
  test Boolean feature vectors and word embeddings to predict new animal descriptions
  based on analogical reasoning.
---

# Analogical Proportions and Creativity: A Preliminary Study

## Quick Facts
- arXiv ID: 2310.13500
- Source URL: https://arxiv.org/abs/2310.13500
- Reference count: 8
- Key outcome: Analogical proportions can generate plausible novel concepts, with GloVe embeddings achieving 83.44% precision at k=8 compared to 59.67% for Boolean vectors with 5 key features

## Executive Summary
This study investigates the creative power of analogical proportions to generate novel concepts from existing ones using a zoo dataset. The authors test both Boolean feature vectors and word embeddings to predict new animal descriptions based on analogical reasoning. When applying analogical proportions to Boolean vectors using five key features, 59.67% of generated animals already exist in the dataset. Using GloVe embeddings on the same task, the method achieves up to 83.44% precision at k=8 in proposing existing animals. The results demonstrate that analogical proportions can effectively "create" plausible novel concepts, with embeddings outperforming Boolean representations.

## Method Summary
The study applies analogical proportions to generate new concepts from existing ones using two representation methods. For Boolean vectors, each animal is represented by a 16-feature vector, and analogical proportions are solved using six valid patterns that make the proportion true. The researchers first use all 16 features, then select the 5 most important features (hair, eggs, milk, venomous, domestic) and solve for the fourth element. For word embeddings, GloVe vectors represent each animal, and the fourth element is calculated using vector arithmetic (⃗d = ◌c + ⃗b - ⃗a), followed by nearest-neighbor search. The creative output is evaluated by measuring how many generated animals exist in the original dataset.

## Key Results
- Boolean vector approach with 5 key features achieved 59.67% precision (173 out of 290 generated animals existed in dataset)
- GloVe embedding approach achieved 83.44% precision at k=8 in proposing existing animals
- Generated animals included plausible combinations like platypus and scorpions
- Embeddings outperformed Boolean representations in creative generation tasks
- Nested analogical proportions enabled compositionality across different attributes

## Why This Works (Mechanism)

### Mechanism 1: Vector Arithmetic for Analogical Proportions
Analogical proportions enable creative generation by computing a missing fourth element from three given items using Boolean vector arithmetic or embedding vector arithmetic. For Boolean vectors, each component satisfies ai : bi :: ci : di using the six patterns that make an AP true. For embeddings, the same holds if ⃗a - ⃗b = ⃗c - ⃗d, which directly yields ⃗d = ⃗c + ⃗b - ⃗a. The core assumption is that items can be represented as vectors (Boolean or real-valued embeddings) such that the analogical proportion condition holds component-wise or vector-wise. If any component equation has no solution under the six valid Boolean patterns, or if the embedding vectors are not sufficiently aligned to satisfy the parallelogram condition, the analogical proportion fails and no valid fourth item is produced.

### Mechanism 2: Nested Analogical Proportions
The creative power of analogical proportions arises from nested structures, where a higher-level AP is composed of lower-level APs across different attributes or feature subsets. When some attributes themselves do not satisfy AP directly (e.g., subject names), their Boolean feature encodings can satisfy AP, enabling the higher-level analogical relation to hold. The core assumption is that items can be decomposed into features or sub-representations that individually satisfy AP, allowing compositionality of analogical reasoning. If the feature decomposition is too coarse or if key attributes cannot be encoded to satisfy lower-level APs, the nested structure collapses and the creative output becomes implausible or fails.

### Mechanism 3: Semantic Richness of Embeddings
Word embeddings improve creative generation performance over Boolean vectors because embeddings capture richer semantic relationships that align better with analogical proportion geometry. Embeddings encode similarity and relational structure in continuous space, so the vector arithmetic ⃗d = ⃗c + ⃗b - ⃗a naturally yields plausible nearest-neighbor words that satisfy the intended analogical relation. The core assumption is that the embedding space preserves analogical proportions such that vector differences correspond to meaningful relational transformations. If embeddings do not preserve the analogical proportion geometry (e.g., in out-of-domain concepts or low-quality embeddings), the computed ⃗d will not correspond to meaningful or existent items, reducing precision.

## Foundational Learning

- **Analogical proportion as quaternary relation**: Understanding "a is to b as c is to d" as the core mathematical construct enabling creative generation by enforcing structural similarity between two pairs of items. *Quick check*: Given a : b :: c : d in Boolean terms, which of the six patterns in the truth table must hold for each component?
- **Vector representation of items**: Recognizing that analogical proportions must be defined over a representation space (Boolean vs embedding), with different representations yielding different computational properties and creative outcomes. *Quick check*: For embeddings, what vector equation must hold for ⃗a : ⃗b :: ⃗c : ⃗d to be true?
- **Precision@k evaluation**: Understanding precision at k as a retrieval quality metric used to measure how often generated items exist in the dataset, serving as a proxy for creative success. *Quick check*: If 5 out of 10 top-k predictions exist in the dataset, what is P@10?

## Architecture Onboarding

- **Component map**: Dataset loader -> Feature selector -> Triplet generator -> Analogical solver (Boolean or embedding) -> Nearest-neighbor search (for embeddings) -> Precision calculator -> Results logger
- **Critical path**: Triplet generation -> Analogical solving -> Existence checking (Boolean) or nearest-neighbor lookup (embedding) -> Precision calculation
- **Design tradeoffs**: Boolean vectors guarantee exact logical consistency but may be too restrictive; embeddings allow richer semantics but depend on embedding quality and may introduce noise; nested APs increase expressiveness but require careful feature decomposition
- **Failure signatures**: Zero solvable triplets (Boolean), low precision despite many predictions, nearest-neighbor words not mapping to valid animals, high computational cost for full triplet enumeration
- **First 3 experiments**:
  1. Run the Boolean AP solver on the full 16-feature set and record precision; verify that triplets with differing classes are selected
  2. Repeat with the 5 most important features; compare precision and inspect specific generated animals (e.g., platypus)
  3. Implement the embedding-based solver using GloVe; for each Boolean-solved triplet, compute ⃗d = ⃗c + ⃗b - ⃗a and find nearest animal embeddings; compute P@8 and compare to Boolean results

## Open Questions the Paper Calls Out

- **Better creativity evaluation measures**: The authors explicitly state that much remains to be done for understanding computational creativity using APs for the generation of novel creative instances and that we need better measures of the evaluation of creativity which do not take into account only the existence or not of the proposed instances. This is unresolved because current evaluation methods rely on binary existence checks, which don't capture the novelty or interestingness of generated concepts. Development and validation of new metrics that assess both the plausibility and novelty of generated concepts, potentially incorporating human evaluation or measures of concept rarity, would resolve this.

- **Embedding method comparison**: The authors state their intention to explore other embedding methods including contextual vectors from BERT and other Transformer-based architectures such as GPT-3 and GPT-4 in future work. This is unresolved because the current study only uses GloVe embeddings, and there's no comparison with more advanced contextual embeddings. Systematic experiments comparing the performance of different embedding methods on the same analogical creativity tasks would resolve this.

- **Feature complexity and novelty relationship**: The authors observe that using different numbers of features (16 vs 5) affects precision rates, suggesting a relationship between feature complexity and output quality. This is unresolved because the study only tests two specific feature subset sizes and doesn't explore the full range of possible configurations. Systematic experiments varying the number and combination of features to identify optimal configurations for generating novel concepts would resolve this.

## Limitations
- Limited evaluation scope: The study uses a single dataset (zoo animals) and does not test the method on other domains or creative tasks, making generalization unclear
- Evaluation measure limitations: Precision-based metrics may not fully capture creative novelty, as they only measure retrieval of existing concepts rather than truly novel generation
- Feature selection ambiguity: The exact methodology for selecting "5 most important features" is not specified, making exact replication difficult

## Confidence
- **High confidence**: The mathematical framework of analogical proportions and vector arithmetic operations is correctly described and implemented
- **Medium confidence**: The experimental results showing embeddings outperforming Boolean vectors are reliable within the zoo dataset context
- **Low confidence**: The claim that this approach can generate truly novel concepts is not fully supported, as most generated items already exist in the dataset

## Next Checks
1. **Cross-domain validation**: Test the analogical proportion method on a different creative task (e.g., inventing new product designs or fictional creatures) to assess generalization beyond zoo animals
2. **Novelty measurement**: Implement evaluation metrics that can distinguish between retrieving existing concepts and generating genuinely novel combinations not present in the training data
3. **Baseline comparison**: Compare performance against simpler methods like random vector generation or basic vector arithmetic without analogical proportion constraints to establish whether the specific structure adds value