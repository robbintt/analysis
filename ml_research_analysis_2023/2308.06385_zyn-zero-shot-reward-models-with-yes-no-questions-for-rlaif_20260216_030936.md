---
ver: rpa2
title: 'ZYN: Zero-Shot Reward Models with Yes-No Questions for RLAIF'
arxiv_id: '2308.06385'
source_url: https://arxiv.org/abs/2308.06385
tags:
- reward
- rlaif
- language
- movie
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ZYN, a zero-shot reward model framework that
  uses yes-no questions to guide language model behavior. ZYN prompts an instruction-tuned
  model with a text generation and a yes-no question reflecting desired attributes,
  using the model's responses as a reward signal.
---

# ZYN: Zero-Shot Reward Models with Yes-No Questions for RLAIF

## Quick Facts
- arXiv ID: 2308.06385
- Source URL: https://arxiv.org/abs/2308.06385
- Reference count: 40
- Primary result: ZYN enables zero-shot reward modeling using yes-no questions, eliminating need for labeled data while effectively steering language models toward desired attributes.

## Executive Summary
ZYN introduces a zero-shot reward modeling framework that leverages instruction-tuned models to evaluate text generations through yes-no question prompts. By computing rewards from the model's response probabilities for "Yes" and "No" tokens, ZYN eliminates the need for labeled data or pairwise comparisons while effectively steering language models toward desired attributes. The framework demonstrates strong performance across diverse applications including sentiment steering, toxicity reduction, and text-to-image prompt personalization.

## Method Summary
ZYN prompts an instruction-tuned model with a text generation and a yes-no question reflecting desired attributes. The critic model's response probabilities for "Yes" and "No" tokens are used to compute a scalar reward score. This reward can be integrated into various optimization frameworks including reinforcement learning pipelines, quality-diversity search, and best-of-N sampling. The approach requires only evaluating token probabilities, making it computationally efficient and easy to implement without requiring custom reward model training.

## Key Results
- Effectively steers language models toward positive sentiment and reduced toxicity without labeled data
- Successfully personalizes text-to-image prompts with desired aesthetic attributes
- Integrates seamlessly with reinforcement learning pipelines and quality-diversity search methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ZYN enables zero-shot reward modeling by leveraging instruction-tuned models to evaluate text generations through yes-no question prompts
- Mechanism: The critic model is prompted with a text generation and a yes-no question reflecting desired attributes. The model's response probabilities for "Yes" and "No" tokens are used to compute a reward score, which guides the student language model during fine-tuning
- Core assumption: Instruction-tuned models can effectively evaluate text generations based on natural language prompts without requiring labeled data
- Evidence anchors:
  - [abstract] "ZYN prompts an instruction-tuned model with a text generation and a yes-no question reflecting desired attributes, using the model's responses as a reward signal"
  - [section 3.1] "To construct a zero-shot reward model, ZYN begins with an instruction-tuned model... This model will act as the critic with respect the text outputs of the unaligned LM, computing a scalar reward r according to desired attribute of the text"
- Break condition: The approach fails if the instruction-tuned model cannot accurately evaluate the text generations based on the yes-no questions, or if the model's responses are not well-calibrated

### Mechanism 2
- Claim: ZYN can be used for various applications beyond RLAIF, such as quality-diversity search and prompt personalization
- Mechanism: The computed rewards from the yes-no questions can be integrated into different optimization frameworks. For quality-diversity search, the rewards guide the exploration of diverse text generations. For prompt personalization, the rewards steer the language model towards generating prompts with desired aesthetic attributes
- Core assumption: The reward signals derived from yes-no questions are compatible with different optimization methods and can effectively guide the language model's behavior
- Evidence anchors:
  - [abstract] "The framework integrates seamlessly into reinforcement learning pipelines and other optimization methods, demonstrating strong performance across diverse applications without requiring custom reward model training"
  - [section 3.1] "Once a reward function f (o, q) has been chosen, it can be integrated into any RL pipeline, such as PPO-based fine-tuning or any other training algorithm, to optimize the rewards by steering the output distribution of the base LM, as in RLHF"
- Break condition: The approach fails if the reward signals are not effective in guiding the language model's behavior for the specific application or if the optimization method is not compatible with the reward computation

### Mechanism 3
- Claim: ZYN is computationally efficient and easy to implement, as it only requires evaluating the probabilities of "Yes" and "No" tokens
- Mechanism: The reward computation involves prompting the critic model with the text generation and yes-no question, and then extracting the logits for the "Yes" and "No" tokens. This process is computationally cheap compared to generating multiple text samples or training custom reward models
- Core assumption: The instruction-tuned model can provide reliable probabilities for the "Yes" and "No" tokens based on the given prompt, and these probabilities are indicative of the text generation's quality or alignment with the desired attributes
- Evidence anchors:
  - [section 3.1] "ZYN is straightforward to implement, as it only requires access to the value of the tokens of the zero-shot, critic reward model"
  - [section 3.2] "c l a s s ZeroShotRewardModel : ... d e f r e w a r d _ f n ( s e l f , o : s t r , q : s t r ) âˆ’> f l o a t : ... reward = v_yes_exp / ( v_yes_exp + v_no_exp )"
- Break condition: The approach fails if the instruction-tuned model's response probabilities are not reliable or if the reward computation process is computationally expensive for the given model size or hardware constraints

## Foundational Learning

- Concept: Instruction tuning
  - Why needed here: ZYN relies on instruction-tuned models as the critic reward models. Understanding instruction tuning is crucial for comprehending how these models can effectively evaluate text generations based on natural language prompts
  - Quick check question: What is the main difference between instruction tuning and standard pre-training or fine-tuning of language models?

- Concept: Reinforcement Learning from AI Feedback (RLAIF)
  - Why needed here: ZYN is presented as a zero-shot reward modeling approach that can be integrated into RLAIF pipelines. Familiarity with RLAIF is necessary to understand how ZYN's rewards can guide the fine-tuning of language models
  - Quick check question: How does RLAIF differ from Reinforcement Learning from Human Feedback (RLHF) in terms of the source of the reward signals?

- Concept: Quality-diversity search
  - Why needed here: The paper demonstrates ZYN's applicability to quality-diversity search, where the goal is to explore diverse text generations while maintaining high quality. Understanding quality-diversity search helps in grasping how ZYN's rewards can guide the exploration process
  - Quick check question: What is the main objective of quality-diversity search, and how does it differ from traditional reinforcement learning approaches?

## Architecture Onboarding

- Component map: Base language model (student) -> Instruction-tuned model (critic) -> Optimization method (PPO, quality-diversity search, etc.)

- Critical path:
  1. Define the desired attributes and corresponding yes-no questions for the critic model
  2. Implement the ZYN reward computation function, which prompts the critic model and extracts the "Yes" and "No" token probabilities
  3. Integrate the ZYN rewards into the chosen optimization method (e.g., RLAIF, quality-diversity search)
  4. Fine-tune or guide the base language model using the optimization method and the ZYN rewards
  5. Evaluate the performance of the fine-tuned or guided language model on the target task

- Design tradeoffs:
  - Choosing the right instruction-tuned model as the critic: Different models may have varying levels of effectiveness in evaluating text generations based on yes-no questions
  - Designing the yes-no questions: The questions should be clear, specific, and relevant to the desired attributes of the text generations
  - Selecting the reward computation formula: Different formulas (e.g., probability ratio, log-odds ratio) may have varying effects on the stability and effectiveness of the fine-tuning process

- Failure signatures:
  - The base language model's generations do not improve or diverge from the desired attributes despite using ZYN rewards
  - The critic model's responses are inconsistent or unreliable across different text generations and yes-no questions
  - The optimization process fails to converge or produces unstable results due to the reward signals

- First 3 experiments:
  1. Evaluate the effectiveness of ZYN rewards in guiding a language model to generate text with positive sentiment
  2. Assess ZYN's ability to steer a language model towards producing text with specific attributes (e.g., professional tone, focus on certain topics)
  3. Test ZYN's compatibility with different optimization methods, such as RLAIF and quality-diversity search, on various text generation tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of scaling and centering hyperparameters (ks, kc) in Eq. (3) affect the stability and performance of ZYN during PPO training?
- Basis in paper: [explicit] The paper discusses using Eq. (3) with hyperparameters ks and kc to re-scale and center the rewards, but notes that the choice between reward function alternatives should be treated as a hyperparameter
- Why unresolved: The paper does not provide empirical results comparing different values of ks and kc or their impact on training stability and performance
- What evidence would resolve it: Experiments comparing ZYN performance with different ks and kc values during PPO training, measuring training stability, convergence speed, and final reward scores

### Open Question 2
- Question: How does expanding the set of possible answers beyond "Yes" and "No" in ZYN affect the quality and diversity of generated text?
- Basis in paper: [inferred] The paper mentions that ZYN only looks at tokens for "Yes" and "No" but suggests expanding to more answers could be beneficial
- Why unresolved: The paper does not explore alternative answer sets or their effects on reward model performance
- What evidence would resolve it: Experiments comparing ZYN performance using different answer sets (e.g., "Yes", "No", "Unsure") on various text generation tasks, measuring reward scores and text quality metrics

### Open Question 3
- Question: How does the performance of ZYN compare to other RLAIF methods like RLCD and CLAIF across different domains and task complexities?
- Basis in paper: [explicit] The paper compares ZYN to vanilla RLAIF and RLCD but does not provide direct comparisons with CLAIF or across diverse domains
- Why unresolved: The paper focuses on demonstrating ZYN's capabilities but lacks comprehensive benchmarking against other RLAIF approaches
- What evidence would resolve it: Head-to-head comparisons of ZYN, RLCD, and CLAIF on a wide range of text generation tasks, measuring reward scores, text quality, and computational efficiency

### Open Question 4
- Question: What is the impact of using an ensemble of question prompts versus a single question prompt in ZYN on the robustness and quality of generated text?
- Basis in paper: [explicit] The paper introduces ensemble formulation in Eq. (4) and shows it improves quality-diversity search results but doesn't systematically compare it to single-prompt ZYN
- Why unresolved: The paper doesn't provide quantitative comparisons of single-prompt vs ensemble ZYN across different tasks
- What evidence would resolve it: Experiments comparing ZYN performance using single question prompts versus ensembles on various text generation tasks, measuring reward scores, text quality, and robustness to adversarial inputs

## Limitations
- Zero-shot nature relies heavily on the quality and alignment of instruction-tuned critic models, which may not capture nuanced or complex attributes
- Effectiveness depends on stability of reward computation, as small variations in token probabilities could lead to significant reward fluctuations
- Demonstrated primarily on straightforward binary attributes (sentiment, toxicity) rather than more complex behavioral goals

## Confidence
- **High confidence**: The core zero-shot reward computation mechanism is technically sound and the implementation approach is clearly specified. The computational efficiency claim is well-supported by the simple reward calculation
- **Medium confidence**: The effectiveness of ZYN for steering language models toward desired attributes is demonstrated empirically, but results are primarily shown on relatively straightforward binary attributes
- **Low confidence**: The claim that ZYN "integrates seamlessly into reinforcement learning pipelines and other optimization methods" lacks evidence about edge cases or failure modes when used with different optimization algorithms

## Next Checks
1. Test ZYN's robustness to prompt variations by systematically modifying yes-no questions and measuring stability of reward scores across different phrasings of the same attribute
2. Evaluate whether ZYN can effectively steer models toward more complex, multi-dimensional attributes that require balancing multiple criteria
3. Compare ZYN's performance against supervised reward models on tasks where labeled data is available, to quantify the trade-off between convenience and optimality