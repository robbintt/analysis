---
ver: rpa2
title: Leveraging Temporal Graph Networks Using Module Decoupling
arxiv_id: '2310.02721'
source_url: https://arxiv.org/abs/2310.02721
tags:
- edge
- prediction
- dynamic
- batch
- updates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a decoupling strategy for temporal graph
  networks to handle the trade-off between batch size and frequency of updates in
  streaming scenarios. The key idea is to split the standard TGN into two separate
  modules: a memory module that uses smaller batches for frequent updates, and a prediction
  module that uses larger batches for efficiency.'
---

# Leveraging Temporal Graph Networks Using Module Decoupling

## Quick Facts
- arXiv ID: 2310.02721
- Source URL: https://arxiv.org/abs/2310.02721
- Authors: 
- Reference count: 21
- Primary result: LDTGN achieves 20%+ improvement on benchmarks requiring rapid model update rates

## Executive Summary
This paper addresses the trade-off between batch size and update frequency in temporal graph networks (TGNs) for streaming scenarios. The authors propose a decoupling strategy that splits the standard TGN into separate memory and prediction modules, allowing frequent memory updates with smaller batches while maintaining efficient predictions with larger batches. They introduce Lightweight Decoupled Temporal Graph Network (LDTGN), which achieves state-of-the-art performance on various dynamic graph benchmarks with significantly higher throughput than previous methods.

## Method Summary
LDTGN implements a two-module architecture where the memory module processes small mini-batches to frequently update node and edge states, while the prediction module uses larger batches for efficient predictions. The memory module updates states more frequently by dividing batches into mini-batches, reducing the likelihood of missing critical updates. The prediction module receives extracted states after all memory updates are complete. Additionally, LDTGN parameterizes EdgeBank's static threshold with a learnable parameter and incorporates node information into the decision rule. The model is trained in an online setting with backpropagation at inference time.

## Key Results
- LDTGN outperforms previous approaches by more than 20% on benchmarks requiring rapid model update rates
- Achieves state-of-the-art performance on 13 dynamic graph benchmarks with significantly higher throughput
- Effectively handles the trade-off between frequent updates and batch processing efficiency in streaming scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling memory and prediction modules allows more frequent memory updates without sacrificing prediction efficiency.
- Mechanism: By splitting the TGN into memory and prediction modules with different batch sizes, the memory module can process smaller mini-batches more frequently, reducing the likelihood of missing critical updates while the prediction module handles larger batches for throughput.
- Core assumption: Temporal locality in dynamic graphs means recent updates are more important for accurate predictions.
- Evidence anchors:
  - [abstract]: "The key idea is to split the standard TGN into two separate modules: a memory module that uses smaller batches for frequent updates, and a prediction module that uses larger batches for efficiency."
  - [section]: "We suggest a decoupling strategy that enables the models to update frequently while using batches."
  - [corpus]: Weak. No direct mention of batch size decoupling in neighbor papers.

### Mechanism 2
- Claim: Using smaller mini-batches in the memory module reduces the occurrence of missing updates.
- Mechanism: When the memory module processes updates in smaller consecutive mini-batches, it can update node states more frequently, ensuring that recent interactions are incorporated before predictions are made.
- Core assumption: Missing updates occur when crucial interactions are not reflected in the node states used for prediction.
- Evidence anchors:
  - [abstract]: "Using batches, however, forces the models to update infrequently, which results in the degradation of their performance."
  - [section]: "In such scenarios, the model will give predictions based on its current state, which does not include the updates in the batch."
  - [corpus]: Weak. No direct mention of missing updates or mini-batch strategies in neighbor papers.

### Mechanism 3
- Claim: Parameterizing EdgeBank's threshold allows the model to learn the optimal cutoff for considering edges as positive.
- Mechanism: By replacing EdgeBank's static threshold with a learnable parameter, LDTGN can adapt to different datasets and tasks, improving prediction accuracy.
- Core assumption: The optimal threshold for considering edges as positive varies across different dynamic graph datasets.
- Evidence anchors:
  - [section]: "We can parameterize the threshold of 1000 suggested by Poursafaei et al. (2022) and receive Eq. (7)."
  - [section]: "Using Eq. (7) we can learn the right threshold for each task."
  - [corpus]: Weak. No direct mention of threshold parameterization in neighbor papers.

## Foundational Learning

- Concept: Temporal Graph Networks (TGNs)
  - Why needed here: Understanding the base architecture is crucial for grasping how decoupling improves performance.
  - Quick check question: What are the two core modules of a TGN, and what are their primary responsibilities?

- Concept: Batch Processing in Streaming Scenarios
  - Why needed here: The trade-off between batch size and update frequency is central to the paper's contribution.
  - Quick check question: How does increasing batch size affect the frequency of model updates and the likelihood of missing updates?

- Concept: EdgeBank Model
  - Why needed here: LDTGN builds upon EdgeBank by parameterizing its decision rule and adding node information.
  - Quick check question: What is the basic decision rule used by EdgeBank to predict whether an edge is positive?

## Architecture Onboarding

- Component map:
  - Input Batch -> Memory Module (small mini-batches) -> Prediction Module (large batch) -> Output Predictions

- Critical path:
  1. Receive a batch of updates and prediction inputs.
  2. Divide the batch into mini-batches for the memory module.
  3. Process each mini-batch, updating node and edge states.
  4. Extract states relevant to the next mini-batch to prevent override.
  5. After processing all mini-batches, pass extracted states to the prediction module.
  6. Make predictions for the input edges based on the updated states.

- Design tradeoffs:
  - Mini-batch size vs. update frequency: Smaller mini-batches allow more frequent updates but increase overhead.
  - Batch size for prediction module vs. throughput: Larger batches improve efficiency but may delay state updates.
  - Parameterization vs. simplicity: Adding learnable parameters improves adaptability but increases complexity.

- Failure signatures:
  - High rate of missing updates: Indicates mini-batch size is too large or temporal locality assumption is violated.
  - Low throughput: Suggests batch size for prediction module is too small or overhead is excessive.
  - Poor prediction accuracy: Could indicate issues with state extraction, parameterization, or insufficient training.

- First 3 experiments:
  1. Vary mini-batch size and measure impact on missing updates and throughput.
  2. Compare performance of parameterized vs. static threshold in EdgeBank-like decision rule.
  3. Test model on datasets with different temporal characteristics to assess adaptability.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The temporal locality assumption may not hold for all dynamic graph types, particularly those with bursty or non-local update patterns
- The optimal mini-batch size for the memory module is dataset-dependent and not fully characterized
- The paper does not address heterogeneous graphs with multiple node/edge types

## Confidence
- **High Confidence**: The decoupling strategy's effectiveness in improving throughput (Mechanism 1) is well-supported by quantitative results showing 20%+ improvement
- **Medium Confidence**: The claim about reducing missing updates (Mechanism 2) is plausible but lacks direct ablation studies showing the impact of different mini-batch sizes
- **Low Confidence**: The benefit of parameterizing EdgeBank's threshold (Mechanism 3) is demonstrated but the specific contribution of this parameterization versus other architectural choices is unclear

## Next Checks
1. Conduct ablation studies varying mini-batch sizes across different temporal patterns to identify the relationship between batch size, update frequency, and performance
2. Test LDTGN on heterogeneous graph datasets to evaluate its generalizability beyond homogeneous dynamic graphs
3. Implement a version without EdgeBank parameterization to quantify the specific contribution of the learnable threshold to overall performance