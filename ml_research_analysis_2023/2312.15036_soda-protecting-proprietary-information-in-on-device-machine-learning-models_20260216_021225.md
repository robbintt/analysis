---
ver: rpa2
title: 'SODA: Protecting Proprietary Information in On-Device Machine Learning Models'
arxiv_id: '2312.15036'
source_url: https://arxiv.org/abs/2312.15036
tags:
- queries
- adversarial
- soda
- attacks
- service
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines how machine learning models deployed on users\u2019\
  \ devices can leak proprietary information about the service provider. The authors\
  \ show that simple query-based attacks can recover up to 100% of model outputs and\
  \ exploit decision boundaries with high accuracy, enabling adversaries to steal\
  \ sensitive business rules or offer distributions."
---

# SODA: Protecting Proprietary Information in On-Device Machine Learning Models

## Quick Facts
- arXiv ID: 2312.15036
- Source URL: https://arxiv.org/abs/2312.15036
- Reference count: 37
- Key outcome: Achieves 89% accuracy in detecting adversarial queries within 50 queries using autoencoder-based anomaly detection

## Executive Summary
This paper addresses the vulnerability of on-device machine learning models to query-based attacks that can leak proprietary information about service providers. The authors demonstrate that simple adversarial queries can recover up to 100% of model outputs and exploit decision boundaries with high accuracy, enabling theft of sensitive business rules. They propose SODA, a system that combines an autoencoder-based anomaly detector with three leakage rate metrics (reconstruction error, query distance, output entropy) to distinguish benign from adversarial usage. SODA achieves 89% detection accuracy within 50 queries while maintaining minimal impact on latency and storage, highlighting the critical need for proactive privacy protections in on-device ML deployments.

## Method Summary
SODA uses an autoencoder trained on benign data to detect adversarial queries through reconstruction error, query distance patterns, and output entropy. The system trains both the service model and autoencoder simultaneously on the same benign dataset, then deploys them as a black-box service where the model weights are encrypted and inaccessible. During inference, SODA computes three leakage rates: (1) reconstruction error between original and autoencoder-reconstructed queries, (2) cumulative median Euclidean distance between encoded queries over time, and (3) entropy of model predictions. These metrics are combined with threshold-based classification to identify adversarial usage. The framework was evaluated on HAR and MNIST datasets using random seed queries to generate both random and perturbed adversarial queries.

## Key Results
- Achieves 89% accuracy in detecting adversarial queries within 50 queries
- Successfully prevents up to 100% recovery of model outputs through query-based attacks
- Maintains minimal impact on service latency and storage requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Autoencoder reconstruction error can distinguish adversarial queries from benign ones
- Mechanism: Autoencoders trained on benign data fail to accurately reconstruct anomalous inputs, producing higher reconstruction error
- Core assumption: Adversarial queries are sufficiently different from the training distribution
- Evidence anchors:
  - [abstract]: "SODA achieves 89% accuracy in detecting adversarial queries within 50 queries, with minimal impact on latency and storage"
  - [section]: "We use this aspect of autoencoders during the design of the defense layer... The reconstruction error between the original input and reconstructed input is expected to be low for ID queries and high for OOD queries"
  - [corpus]: Weak - no direct corpus support found
- Break condition: If adversarial queries are crafted to be in-distribution (ID) with respect to training data, reconstruction error won't effectively distinguish them

### Mechanism 2
- Claim: Query distance over time can identify adversarial behavior
- Mechanism: Benign users have consistent query patterns, while adversaries show either very similar (decision boundary exploitation) or very different (output diversity exploitation) queries
- Core assumption: Adversarial queries exhibit distinct distance patterns compared to normal usage
- Evidence anchors:
  - [section]: "We use the cumulative median Euclidean distance between encoded query at time t and all the previous encoded queries by the same user to identify the distance at time t"
  - [section]: "Intuitively, we expect queries from adversaries exploiting decision boundaries to be very similar with minute perturbations whereas adversaries exploiting output diversity will have fairly different queries"
  - [corpus]: Weak - no direct corpus support found
- Break condition: If an adversary mimics normal query patterns or uses adaptive strategies that maintain natural distance distributions

### Mechanism 3
- Claim: Output entropy can detect adversarial behavior patterns
- Mechanism: Adversarial queries either produce highly diverse predictions (output diversity exploitation) or repetitive predictions (decision boundary exploitation), both resulting in abnormal entropy
- Core assumption: Adversarial goals lead to entropy values that deviate from benign usage patterns
- Evidence anchors:
  - [section]: "We use this as a measure to explore the diversity of the predictions made by the model... substantially high entropy would indicate diverse and varied predictions, whereas substantially low entropy would indicate similar or repetitive predictions, both of which can be suggestive towards adversarial usage"
  - [abstract]: "SODA can detect adversarial usage with 89% accuracy in less than 50 queries"
  - [corpus]: Weak - no direct corpus support found
- Break condition: If adversarial queries are crafted to maintain normal entropy values

## Foundational Learning

- Concept: Autoencoder architecture and training
  - Why needed here: SODA uses an autoencoder to detect anomalous queries based on reconstruction error
  - Quick check question: How does an autoencoder's inability to reconstruct anomalous inputs enable anomaly detection?

- Concept: Distance metrics in high-dimensional spaces
  - Why needed here: SODA uses Euclidean distance to measure query similarity over time for adversarial detection
  - Quick check question: Why is Euclidean distance chosen for measuring query similarity in SODA's design?

- Concept: Information entropy and its applications
  - Why needed here: SODA uses output entropy to detect abnormal prediction diversity from adversarial queries
  - Quick check question: How does entropy help distinguish between normal and adversarial usage patterns?

## Architecture Onboarding

- Component map: Autoencoder (encoder/decoder) → Service model → Adversarial detector (reconstruction error + query distance + output entropy) → Classification
- Critical path: Query → Autoencoder encoding → Service model prediction → All three leakage rate components computed → Threshold comparison → Benign/Adversarial classification
- Design tradeoffs: Black-box deployment limits visibility but increases security; using multiple detection components improves robustness but adds complexity; in-memory encryption protects data but requires careful key management
- Failure signatures: High false positive rate suggests threshold too sensitive; low detection accuracy suggests leakage rate components not well-calibrated; excessive latency indicates optimization needed
- First 3 experiments:
  1. Test autoencoder reconstruction error on benign vs. random adversarial queries
  2. Verify query distance patterns differ between benign users and adversaries
  3. Validate output entropy distinguishes between decision boundary exploitation and output diversity exploitation scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SODA perform against more sophisticated attack methods that employ gradient-based optimization or adaptive query strategies?
- Basis in paper: [explicit] The paper acknowledges that SODA has been evaluated against simpler attack methods and suggests exploring more intricate attack scenarios as future work.
- Why unresolved: The paper's evaluation focused on basic random and perturbation-based attacks, leaving the effectiveness of SODA against more advanced techniques unexplored.
- What evidence would resolve it: Empirical results demonstrating SODA's detection accuracy and robustness when subjected to gradient-based attacks, adaptive query strategies, or attacks that leverage model gradients to craft adversarial queries.

### Open Question 2
- Question: Can SODA be effectively extended to generative AI models, which operate fundamentally differently from classification models?
- Basis in paper: [explicit] The paper concludes by noting that extending SODA to generative AI models is an interesting direction for future work, as these models are inherently different from the classification models studied.
- Why unresolved: The paper's evaluation and design are tailored for classification tasks, and generative models have distinct architectures and output characteristics that may require different detection strategies.
- What evidence would resolve it: A modified SODA framework designed for generative models, along with experimental results showing its effectiveness in detecting adversarial usage in generative AI systems.

### Open Question 3
- Question: How does SODA handle scenarios involving colluding adversaries who coordinate their queries across multiple devices?
- Basis in paper: [explicit] The paper mentions examining the effect of colluding adversaries as an interesting scope for future work.
- Why unresolved: The current SODA framework focuses on detecting adversarial usage at the individual device level and does not account for coordinated attacks across multiple devices.
- What evidence would resolve it: Experimental results showing SODA's performance when faced with colluding adversaries, along with proposed modifications to the framework to detect and mitigate such coordinated attacks.

## Limitations

- Evaluation limited to only two simple datasets (HAR and MNIST) rather than complex real-world models
- Significant runtime overhead (up to 199× slower) for black-box attacks when feature space is large
- Threshold parameters (δ) and weight parameters (α, β, γ) require manual tuning without clear specification

## Confidence

**High Confidence**: The core claim that proprietary information can be leaked through query-based attacks on on-device ML models is well-supported by experimental results showing successful recovery of up to 100% of model outputs.

**Medium Confidence**: The effectiveness of SODA in real-world scenarios is supported by experimental results but limited by the narrow scope of evaluated datasets and attack types.

**Low Confidence**: The practical deployment claims regarding minimal impact on latency and storage are not thoroughly validated with limited quantitative evidence.

## Next Checks

1. **Dataset Generalization Test**: Evaluate SODA on more complex, real-world datasets with higher dimensionality and multiple model architectures (CNNs, transformers) to assess robustness beyond the simple HAR and MNIST cases used in the paper.

2. **Dynamic Threshold Calibration**: Implement an automated method for determining optimal threshold values (δ) and weight parameters (α, β, γ) based on the specific deployment context, rather than requiring manual tuning, and validate the impact on detection accuracy across different scenarios.

3. **Large-Scale Feature Space Performance**: Conduct experiments specifically designed to measure SODA's performance when the feature space significantly exceeds the model's input features, quantifying the exact runtime overhead and detection accuracy degradation to better understand practical deployment limitations.