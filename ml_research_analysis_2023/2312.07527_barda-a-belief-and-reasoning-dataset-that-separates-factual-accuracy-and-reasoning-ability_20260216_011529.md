---
ver: rpa2
title: 'BaRDa: A Belief and Reasoning Dataset that Separates Factual Accuracy and
  Reasoning Ability'
arxiv_id: '2312.07527'
source_url: https://arxiv.org/abs/2312.07527
tags:
- reasoning
- 'true'
- facts
- entailment
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents BARDA, a new Belief and Reasoning Dataset designed
  to separate factual accuracy and reasoning ability in language models. The dataset
  contains 3000 entailments, using 6681 true and 2319 false statements, and is engineered
  to express both good and bad chains of reasoning, as well as using a mixture of
  true and false facts to avoid belief bias.
---

# BaRDa: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability

## Quick Facts
- arXiv ID: 2312.07527
- Source URL: https://arxiv.org/abs/2312.07527
- Reference count: 1
- Primary result: BARDA dataset enables separate measurement of language models' factual accuracy (74.1%-87.1%) and reasoning ability (63.1%-79.2%) through counterfactual examples

## Executive Summary
BARDA is a novel dataset designed to evaluate language models on two distinct capabilities: factual accuracy and reasoning ability. The dataset contains 3000 entailments constructed to separate these two aspects by including counterfactual examples where premises are false but reasoning is valid. Testing on four GPT-series models reveals that newer models show clear progression in both factual and reasoning capabilities, with reasoning accuracy ranging from 63.1% to 79.2%. The dataset provides a new benchmark for measuring these capabilities independently.

## Method Summary
BARDA uses entailments from EntailmentBank, labeled for factual correctness and reasoning validity. The dataset includes counterfactual examples to avoid belief bias. Models are evaluated using few-shot prompting with consistent templates to elicit responses about factual correctness and reasoning validity. Performance is measured as factual accuracy (percentage of correctly predicted statement truth values), reasoning accuracy (percentage of correctly predicted entailment validity), and reasoning consistency (internal coherence of model beliefs).

## Key Results
- Factual accuracy scores for GPT-series models range from 74.1% to 87.1%
- Reasoning accuracy scores range from 63.1% to 79.2%
- GPT3 (text-davinci-003) shows stronger entailment reasoning than GPT3.5 (gpt-3.5-turbo), an unexpected result
- All models show clear progression in both factual and reasoning capabilities for newer models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: BARDA's inclusion of counterfactual examples effectively separates factual accuracy from reasoning ability by preventing belief bias in model evaluations.
- **Mechanism**: The dataset contains entailments with both true and false premises/hypotheses, including counterfactual examples where the premises are false but the reasoning is valid. This design forces models to evaluate the logical structure of arguments independently of their truth value, measuring pure reasoning ability.
- **Core assumption**: Models will attempt to evaluate entailments based on logical validity rather than factual correctness when presented with counterfactual examples.
- **Evidence anchors**:
  - [abstract]: "Our approach is to leverage and extend a collection of human-annotated entailment trees, engineered to express both good and bad chains of reasoning, and using a mixture of true and false facts, in particular including counterfactual examples, to avoid belief bias (also known as the 'content effect')."
  - [section]: "To address this limitation, our goal is a dataset that more clearly separates factual accuracy and reasoning ability. Our approach is to use a mixture of both good and bad reasoning chains, constructed using a mixture of correct and incorrect (counterfactual) statements about the world."
- **Break condition**: If models exhibit strong belief bias and fail to evaluate counterfactual entailments logically, conflating factual belief with reasoning validity.

### Mechanism 2
- **Claim**: BARDA's four-category classification system (TT, TF, FT, FF) provides granular measurement of model capabilities in both factual accuracy and reasoning ability.
- **Mechanism**: By categorizing entailments into four types based on the truth values of premises/hypotheses and the validity of the reasoning, BARDA enables precise measurement of how well models handle different combinations of factual and reasoning challenges. This granular breakdown reveals specific strengths and weaknesses.
- **Core assumption**: The four-category system provides meaningful distinctions in model performance that would be obscured in binary classifications.
- **Evidence anchors**:
  - [section]: "Given a gold-labeled entailment, along with gold labels on the correctness of the premises and hypothesis statements, we can define four classes of entailments... Having examples in these different classes is useful, as it allows us to separate factual accuracy from reasoning accuracy."
  - [section]: "Table 6) shows reasoning accuracies broken down by inference type (true/false facts, valid/invalid entailments)"
- **Break condition**: If the distinctions between categories prove not to be meaningful or if models perform similarly across all categories.

### Mechanism 3
- **Claim**: BARDA's use of few-shot prompting with consistent templates across all models ensures fair and comparable evaluation of factual and reasoning accuracy.
- **Mechanism**: The standardized prompts for eliciting model responses about factual correctness and reasoning validity create a controlled evaluation environment. By using the same prompts for all models, the dataset enables valid comparisons of model capabilities.
- **Core assumption**: The few-shot prompts effectively communicate the evaluation task to models in a consistent manner.
- **Evidence anchors**:
  - [section]: "To elicit GPT*'s answers about whether a statement is true (factual accuracy), and whether an entailment is valid (reasoning accuracy), we use few-shot prompting to pose the statement/entailment to the model."
  - [section]: "Note, though, that we use the same prompts for all models, helping to keep comparative performances valid"
- **Break condition**: If models interpret the prompts differently or if prompt variations significantly affect performance across models.

## Foundational Learning

- **Concept**: Belief bias (content effect) in reasoning tasks
  - Why needed here: Understanding belief bias is crucial for appreciating why BARDA's counterfactual approach is necessary and innovative
  - Quick check question: What is belief bias and how does it typically affect human reasoning performance on syllogisms?

- **Concept**: Textual entailment and its evaluation
  - Why needed here: The core evaluation mechanism in BARDA relies on understanding what constitutes valid entailment
  - Quick check question: How does the definition of textual entailment differ from simple logical implication?

- **Concept**: Few-shot prompting and prompt engineering
  - Why needed here: BARDA's evaluation methodology depends on effective prompt design to elicit model responses
  - Quick check question: What are the key considerations when designing few-shot prompts for evaluating model reasoning?

## Architecture Onboarding

- **Component map**: Dataset construction -> Evaluation framework -> Model interface -> Analysis pipeline
- **Critical path**: Load entailment dataset with gold labels -> For each model, apply factual accuracy prompt -> For each model, apply reasoning validity prompt -> Calculate performance metrics -> Analyze results by entailment type (TT, TF, FT, FF)
- **Design tradeoffs**: Counterfactual examples provide cleaner separation of reasoning from factual knowledge but may be less representative of real-world usage; crowdsourced gold labels provide ground truth but introduce noise and potential inconsistencies; few-shot prompting is lightweight but may not fully capture model capabilities compared to fine-tuning approaches
- **Failure signatures**: High factual accuracy but low reasoning accuracy suggests models rely on knowledge recall rather than logical reasoning; low consistency indicates internal contradictions in model beliefs; uniform performance across all entailment types suggests failure to distinguish between factual and reasoning challenges
- **First 3 experiments**: 1) Replicate the main results on a subset of BARDA to verify the factual/reasoning accuracy measurements; 2) Test model performance on just the counterfactual entailments (FT and TF types) to isolate reasoning ability; 3) Compare performance across different prompt variations to assess prompt sensitivity

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- The evaluation methodology using few-shot prompting assumes all models interpret prompts identically
- BARDA's counterfactual examples may be less representative of real-world usage where belief and reasoning are intertwined
- Generalizability of findings to other types of statements and reasoning tasks not covered in the dataset

## Confidence

- **Confidence: Medium** The claim that BARDA effectively separates factual accuracy from reasoning ability relies heavily on the assumption that models will process counterfactual entailments without belief bias.
- **Confidence: Medium** The evaluation methodology using few-shot prompting with consistent templates across models assumes that all models interpret these prompts identically.
- **Confidence: Low** The generalizability of BARDA's findings to real-world applications remains uncertain.

## Next Checks

1. **Cross-domain validation**: Test BARDA's effectiveness on a broader range of language models beyond the GPT-series, including open-source models with varying architectures, to verify the dataset's ability to separate factual and reasoning capabilities across different model types.

2. **Prompt sensitivity analysis**: Systematically vary the few-shot prompts used in BARDA's evaluation to determine how sensitive model performance is to prompt engineering, identifying whether current prompts represent a stable evaluation methodology.

3. **Real-world applicability test**: Design a complementary evaluation that mixes factual and reasoning tasks in naturalistic scenarios to assess how well BARDA's measured capabilities translate to practical language model applications where belief and reasoning are not artificially separated.