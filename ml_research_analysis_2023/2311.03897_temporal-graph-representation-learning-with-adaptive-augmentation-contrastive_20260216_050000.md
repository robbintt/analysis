---
ver: rpa2
title: Temporal Graph Representation Learning with Adaptive Augmentation Contrastive
arxiv_id: '2311.03897'
source_url: https://arxiv.org/abs/2311.03897
tags:
- temporal
- graph
- node
- learning
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel Temporal Graph representation learning
  with Adaptive augmentation Contrastive (TGAC) model for temporal graph representation
  learning. TGAC addresses the challenge of time-varying noise in temporal graphs
  by combining prior knowledge with temporal information to perform adaptive augmentation.
---

# Temporal Graph Representation Learning with Adaptive Augmentation Contrastive

## Quick Facts
- **arXiv ID**: 2311.03897
- **Source URL**: https://arxiv.org/abs/2311.03897
- **Reference count**: 39
- **Primary result**: Novel model TGAC achieves up to 98.86% AUC and 98.98% AP on temporal link prediction, and 90.13% AUC on dynamic node classification.

## Executive Summary
This paper introduces TGAC, a temporal graph representation learning model that addresses time-varying noise through adaptive augmentation. By combining prior knowledge with temporal information, TGAC employs three adaptive augmentation strategies to modify topological features and reduce network noise. The model demonstrates superior performance on both temporal link prediction and dynamic node classification tasks across multiple real-world datasets, outperforming existing temporal graph representation learning methods.

## Method Summary
TGAC works by first pruning temporal graphs using edge centrality measures (degree, eigenvector, or PageRank) weighted by timestamps to remove noise. Two augmented views are then generated via importance-weighted edge perturbations. A shared Temporal Graph Network encoder with memory updates processes these views, and a combined task loss (link prediction) and contrastive loss (aligning views) trains the model. The approach balances noise reduction with information preservation while leveraging contrastive learning to improve representation consistency across time.

## Key Results
- Achieves up to 98.86% AUC and 98.98% AP on temporal link prediction tasks
- Achieves up to 90.13% AUC on dynamic node classification
- Outperforms other temporal graph representation learning methods across various real networks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Temporal graph pruning removes redundant edges based on centrality measures, reducing noise while preserving critical temporal information.
- **Mechanism**: Edges are scored by combining node centrality (DE, EV, PR) with temporal weighting; top-k edges are retained to form a pruned graph.
- **Core assumption**: Edge centrality reflects its importance for downstream link prediction and node classification.
- **Evidence anchors**:
  - [abstract] "adaptive augmentation on the temporal graph is made by combining prior knowledge with temporal information"
  - [section] "We propose a temporal graph augmentation method that leverages both the structural and temporal information of neighborhoods"
- **Break condition**: If pruned edges include critical temporal interactions, model performance drops.

### Mechanism 2
- **Claim**: Contrastive learning on augmented temporal views improves node representation consistency across time.
- **Mechanism**: Two augmented views are generated by random edge perturbations weighted by edge importance; contrastive loss maximizes agreement between corresponding node embeddings.
- **Core assumption**: Perturbing less important edges preserves temporal graph structure while providing effective negative samples.
- **Evidence anchors**:
  - [abstract] "the contrastive objective function is constructed by defining the augmented inter-view contrast and intra-view contrast"
  - [section] "Two views of the pruned graph are generated using random augmentation operations"
- **Break condition**: If perturbation removes too many important edges, contrastive signal degrades.

### Mechanism 3
- **Claim**: Temporal graph encoder updates node memory with message passing over pruned graph, capturing dynamic node states.
- **Mechanism**: Memory vectors for each node are updated via message passing from neighbors, with temporal embeddings incorporated; node representations combine memory and current features.
- **Core assumption**: Memory-based updates better encode temporal dependencies than static embeddings.
- **Evidence anchors**:
  - [section] "The temporal graph encoder is based on TGN... Each node in the model has a memory vector that represents its past interactions"
  - [section] "To address the issue of stale information, the embedding module calculates node embeddings at each time step by using their neighborhood and memory state"
- **Break condition**: If memory updates fail to capture temporal patterns, node representations become stale.

## Foundational Learning

- **Concept**: Centrality measures (degree, eigenvector, PageRank)
  - **Why needed here**: Used to score edge importance for pruning and perturbation.
  - **Quick check question**: How does eigenvector centrality differ from degree centrality in edge scoring?

- **Concept**: Contrastive learning (InfoNCE, multi-view training)
  - **Why needed here**: Enables learning of node representations that are consistent across augmented temporal views.
  - **Quick check question**: What is the difference between inter-view and intra-view negative samples?

- **Concept**: Temporal graph neural networks (TGN, memory updates)
  - **Why needed here**: Encoder module that updates node states with temporal information.
  - **Quick check question**: How does a memory vector in TGN differ from a static node embedding?

## Architecture Onboarding

- **Component map**: Input temporal graph → Pruning module → Augmentation module → TGNN encoder → Contrastive loss + task loss → Output embeddings
- **Critical path**: Temporal graph → Pruning → Augmentation → Encoder → Contrastive + Task Loss → Embeddings
- **Design tradeoffs**: Pruning ratio (c) balances noise removal vs information loss; augmentation strength (pe) balances perturbation vs topology preservation.
- **Failure signatures**: Low AUC/AP in link prediction suggests pruning removes critical edges; poor node classification suggests contrastive learning fails to align views.
- **First 3 experiments**:
  1. Vary pruning ratio (c) on small dataset, measure link prediction AUC.
  2. Disable contrastive loss, compare with full model on node classification.
  3. Test different centrality measures (DE, EV, PR) in pruning on same dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of TGAC vary when different centrality measures (degree, eigenvector, PageRank) are used for temporal graph pruning, and which centrality measure is optimal for different types of temporal networks?
- **Basis in paper**: [explicit] The paper discusses three centrality measures (degree, eigenvector, and PageRank) and their application in temporal graph pruning, but does not provide a comparative analysis of their effectiveness.
- **Why unresolved**: The paper mentions the use of different centrality measures but does not compare their performance or determine which is optimal for specific types of temporal networks.
- **What evidence would resolve it**: Conducting experiments to compare the performance of TGAC using different centrality measures across various temporal network types and analyzing the results to identify the optimal centrality measure for each type.

### Open Question 2
- **Question**: How does the balance between the task loss and contrastive loss (λ) affect the performance of TGAC, and what is the optimal value of λ for different temporal graph datasets?
- **Basis in paper**: [explicit] The paper mentions the balance parameter λ in the total loss function but does not provide a detailed analysis of its impact on TGAC's performance or determine the optimal value for different datasets.
- **Why unresolved**: The paper does not explore the sensitivity of TGAC's performance to the balance parameter λ or identify the optimal value for various temporal graph datasets.
- **What evidence would resolve it**: Conducting experiments to analyze the impact of different λ values on TGAC's performance across various temporal graph datasets and determining the optimal λ for each dataset.

### Open Question 3
- **Question**: How does the temporal graph pruning ratio (c) affect the performance of TGAC, and what is the optimal pruning ratio for different types of temporal networks?
- **Basis in paper**: [explicit] The paper mentions the temporal graph pruning ratio c but does not provide a detailed analysis of its impact on TGAC's performance or determine the optimal pruning ratio for different types of temporal networks.
- **Why unresolved**: The paper does not explore the sensitivity of TGAC's performance to the pruning ratio c or identify the optimal pruning ratio for various types of temporal networks.
- **What evidence would resolve it**: Conducting experiments to analyze the impact of different pruning ratios c on TGAC's performance across various types of temporal networks and determining the optimal pruning ratio for each type.

## Limitations
- Lack of precise mathematical formulas for edge pruning function and augmentation probabilities
- No detailed analysis of sensitivity to key hyperparameters (λ, c, α)
- Missing implementation details for TGN encoder, contrastive projection, and negative sampling

## Confidence
- **Mechanism 1**: Medium - Limited experimental ablation on different centrality measures
- **Mechanism 2**: Medium - Contrastive loss formulation described but not fully specified
- **Mechanism 3**: Low - Absence of memory update details in the paper

## Next Checks
1. Implement edge pruning with varying centrality measures (DE, EV, PR) on a small dataset to verify impact on link prediction AUC.
2. Conduct ablation study comparing models with and without the contrastive loss component on dynamic node classification.
3. Test the sensitivity of model performance to the temporal weighting parameter α and pruning ratio c across multiple datasets.