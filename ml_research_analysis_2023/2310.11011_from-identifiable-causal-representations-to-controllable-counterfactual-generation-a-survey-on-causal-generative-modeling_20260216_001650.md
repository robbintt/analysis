---
ver: rpa2
title: 'From Identifiable Causal Representations to Controllable Counterfactual Generation:
  A Survey on Causal Generative Modeling'
arxiv_id: '2310.11011'
source_url: https://arxiv.org/abs/2310.11011
tags:
- causal
- learning
- data
- generative
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically categorizes and analyzes causal generative
  modeling methods into two main tasks: identifiable causal representation learning
  and controllable counterfactual generation. The paper provides a comprehensive technical
  overview of methods based on their data-generating assumptions from Pearl''s causal
  hierarchy (observational, interventional, and counterfactual) and discusses key
  concepts like structural causal models, variational autoencoders, and diffusion
  models.'
---

# From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling

## Quick Facts
- arXiv ID: 2310.11011
- Source URL: https://arxiv.org/abs/2310.11011
- Reference count: 21
- Primary result: Systematic survey categorizing causal generative modeling methods into identifiable causal representation learning and controllable counterfactual generation, organized by Pearl's causal hierarchy

## Executive Summary
This survey provides a comprehensive technical overview of causal generative modeling methods, organizing them into two main tasks: identifiable causal representation learning and controllable counterfactual generation. The paper systematically categorizes approaches based on their data-generating assumptions (observational, interventional, and counterfactual) within Pearl's causal hierarchy, covering VAE-based, GAN-based, flow-based, and diffusion-based frameworks. It presents theoretical identifiability results, discusses key evaluation metrics like DCI, MIG, and causal disentanglement scores, and explores applications in fairness, privacy, distribution shift modeling, and precision medicine while identifying critical open challenges in the field.

## Method Summary
The survey systematically categorizes causal generative modeling methods based on their data-generating assumptions from Pearl's causal hierarchy. For identifiable causal representation learning, it covers methods that assume observational data with labels (CausalVAE, DEAR), interventional data (Ahuja et al., 2023), and counterfactual data pairs (Brehmer et al., 2022). For controllable counterfactual generation, it examines methods using VAEs with causal decoders (CausalVAE), GANs with label-based interventions (CausalGAN), and diffusion models (Preechakul et al., 2022; Mittal et al., 2023). The survey evaluates methods using metrics like DCI (Disentanglement, Completeness, Informativeness), MIG (Mutual Information Gap), and CLD (Counterfactual Latent Divergence), and discusses applications in fairness, privacy, distribution shift modeling, and precision medicine.

## Key Results
- Identifiable causal representations can be achieved through latent conditioning with exponential family priors when auxiliary information renders causal factors conditionally independent
- Interventional data enables identifiability of causal factors up to permutation and scaling when perfect interventions isolate individual causal variables
- Paired counterfactual data enables identifiability up to permutation and elementwise reparameterization through contrastively paired samples

## Why This Works (Mechanism)

### Mechanism 1
Identifiable causal representations are achievable through latent conditioning with exponential family priors. The approach assumes causal variables are conditionally independent given auxiliary information (labels or intervention targets), allowing a VAE to learn a unique factorization of the joint distribution. This satisfies permutation-equivalent identifiability up to scaling and reparameterization. Core assumption: auxiliary variables provide sufficient information to render causal factors conditionally independent. Break condition: if auxiliary information is noisy, incomplete, or does not capture true causal structure, conditional independence assumption fails and identifiability breaks down.

### Mechanism 2
Interventional data enables identifiability of causal factors up to permutation and scaling. By observing data from multiple interventional distributions, each corresponding to a perfect intervention on a single causal variable, the model can uniquely identify causal factors. Key is that each intervention isolates the effect of one causal variable while preserving others. Core assumption: access to at least one interventional distribution per causal variable, with perfect interventions that remove parental dependencies. Break condition: if interventions are imperfect or some causal variables are never intervened upon, model cannot distinguish between different causal structures and identifiability fails.

### Mechanism 3
Paired counterfactual data enables identifiability up to permutation and elementwise reparameterization. The model observes pairs of data points representing a system before and after a random, perfect, sparse intervention on a causal variable. By learning to map these pairs to their corresponding noise encodings and identifying intervened dimensions, the model can recover true causal factors. Core assumption: counterfactual pairs are available and interventions are perfect (removing all causal dependencies of intervened variable). Break condition: if interventions are not perfect or counterfactual pairs do not represent sparse interventions, model cannot isolate effects of individual causal variables and identifiability fails.

## Foundational Learning

- **Structural Causal Models (SCMs) and Pearl's Causal Hierarchy**
  - Why needed here: The survey is built entirely on SCM framework and three levels of causal reasoning. Understanding these concepts is essential to grasp different data-generating assumptions and identifiability results.
  - Quick check question: What are the three levels of Pearl's Causal Hierarchy, and how do they differ in terms of the type of queries they can answer?

- **Variational Autoencoders (VAEs) and their ELBO objective**
  - Why needed here: Majority of causal representation learning methods surveyed use VAE-based frameworks. Understanding ELBO objective, encoder-decoder architecture, and role of prior and posterior distributions is crucial for understanding how these methods learn causal representations.
  - Quick check question: What are the two main terms in the ELBO objective, and what do they encourage the model to do?

- **Identifiability and disentanglement in representation learning**
  - Why needed here: Survey focuses on methods that aim to learn identifiable and disentangled causal representations. Understanding identifiability (learning unique solution) and disentanglement (axis-aligned factors) is essential for evaluating strengths and limitations of different approaches.
  - Quick check question: What is the difference between affine equivalence and permutation equivalence in context of identifiability, and why is permutation equivalence considered stronger result?

## Architecture Onboarding

- **Component map**: The survey covers a complex landscape of methods, but core components are: (1) data-generating assumptions (observational, interventional, counterfactual), (2) model architectures (VAEs, GANs, flows, diffusion models), (3) identifiability results and assumptions, and (4) evaluation metrics and applications.
- **Critical path**: To understand the survey, one should first grasp foundational concepts of SCMs and Pearl's hierarchy, then explore different data-generating assumptions and their implications for identifiability, and finally examine various model architectures and their strengths/weaknesses.
- **Design tradeoffs**: The survey highlights key tradeoff between strength of supervision (labels, interventions, counterfactuals) and identifiability of learned representations. Methods that rely on weaker supervision often require stronger assumptions about data-generating process, while methods that assume stronger supervision can achieve better identifiability but may be less practical in real-world scenarios.
- **Failure signatures**: Identifiability can fail if assumptions about data-generating process are violated (e.g., hidden confounders, non-linear mixing functions), if supervision signal is noisy or incomplete, or if model architecture is not expressive enough to capture true causal structure.
- **First 3 experiments**:
  1. Implement simple VAE with Gaussian prior and examine learned representations on toy dataset with known generative factors. Evaluate disentanglement using metrics like MIG and DCI.
  2. Extend VAE to condition on auxiliary labels and observe how this affects identifiability and disentanglement of learned representations.
  3. Simulate interventional data by applying perfect interventions to toy dataset and train VAE to learn causal representations from this data. Compare results to observational setting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can diffusion-based models effectively learn causal representations compared to VAE-based approaches?
- Basis in paper: [explicit] Paper mentions Preechakul et al. (2022) and Mittal et al. (2023) explore learning semantically meaningful representations in diffusion framework, but notes this area is underexplored.
- Why unresolved: While some initial work exists, there's limited research comparing diffusion-based causal representation learning to established VAE-based methods, and no systematic evaluation of their relative strengths and weaknesses.
- What evidence would resolve it: Comprehensive study comparing diffusion-based and VAE-based methods on standard causal representation learning benchmarks, evaluating both disentanglement quality and downstream task performance.

### Open Question 2
- Question: How can we evaluate counterfactual quality without access to underlying data-generating process?
- Basis in paper: [explicit] Paper notes obtaining counterfactual data for evaluation is impossible in real-world scenarios since they represent hypothetical samples, and mentions Monteiro et al. (2023) propose general framework for evaluating image counterfactuals.
- Why unresolved: Current evaluation methods often require ground-truth counterfactuals or knowledge of data-generating process, which is typically unavailable in real applications.
- What evidence would resolve it: Development and validation of proxy metrics that can reliably assess counterfactual quality using only factual observations and domain knowledge.

### Open Question 3
- Question: What are the minimal supervision requirements for learning identifiable causal representations?
- Basis in paper: [explicit] Paper discusses various supervision levels from labels (CausalVAE, DEAR) to interventional data (Ahuja et al., 2023) to counterfactual data (Brehmer et al., 2022), and notes Ahuja et al. (2023) show interventions on multi-node causal latents can generalize to arbitrary causal graphs.
- Why unresolved: While different levels of supervision are explored, fundamental question of how much supervision is truly necessary remains open, and whether weaker forms of supervision could be sufficient.
- What evidence would resolve it: Theoretical bounds on minimum supervision required for identifiability, coupled with empirical validation across different supervision regimes and causal graph structures.

## Limitations
- Theoretical identifiability results may not translate to practical algorithms with real-world data due to violated assumptions about perfect interventions or complete counterfactual pairs
- Many methods assume perfect interventions or complete counterfactual pairs, which are rarely available in practice
- Survey does not extensively discuss computational complexity or scalability of proposed methods, which could be significant barriers to real-world deployment

## Confidence
- **High**: Survey accurately categorizes existing methods and correctly represents their stated assumptions and contributions. Taxonomy based on Pearl's causal hierarchy is well-established.
- **Medium**: Technical descriptions of VAE-based frameworks and their identifiability results are consistent with cited literature, but some implementation details may vary between studies.
- **Low**: Claims about real-world applicability and performance in domains like precision medicine and fairness lack empirical validation within the survey itself.

## Next Checks
1. Implement and evaluate one causal representation learning method (e.g., Conditional VAE with auxiliary labels) on controlled synthetic dataset to verify claimed identifiability properties.
2. Compare performance of interventional versus observational methods on datasets with known causal structure to quantify practical benefit of stronger supervision.
3. Test scalability of these methods on high-dimensional real-world datasets (e.g., CelebA or ImageNet) to assess computational feasibility and identify potential bottlenecks.