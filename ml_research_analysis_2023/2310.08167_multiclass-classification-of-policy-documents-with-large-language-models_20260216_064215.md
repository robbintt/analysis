---
ver: rpa2
title: Multiclass Classification of Policy Documents with Large Language Models
arxiv_id: '2310.08167'
source_url: https://arxiv.org/abs/2310.08167
tags:
- other
- accuracy
- bill
- which
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of GPT 3.5 and GPT 4 models
  for multiclass classification of policy documents into 21 major policy issue topics
  from the Comparative Agendas Project. Three use-case scenarios are proposed, varying
  in human intervention levels.
---

# Multiclass Classification of Policy Documents with Large Language Models

## Quick Facts
- arXiv ID: 2310.08167
- Source URL: https://arxiv.org/abs/2310.08167
- Reference count: 40
- Primary result: GPT 4 achieves 83% accuracy on 65% of data where it agrees with GPT 3.5

## Executive Summary
This study evaluates GPT 3.5 and GPT 4 models for multiclass classification of congressional bills and hearings into 21 Comparative Agendas Project policy topics using zero-shot learning. Three scenarios with varying human intervention levels show that model agreement filtering achieves the highest accuracy (83%) on the majority of documents. However, complete reliance on GPT predictions without human oversight yields suboptimal results, suggesting hybrid approaches combining automated classification with human review are optimal for policy document coding.

## Method Summary
The researchers used zero-shot learning with GPT 3.5 and GPT 4 to classify congressional bills (468,438 records) and hearings (102,151 records) into 21 CAP policy topics. They implemented three scenarios: (1) using untouched LLM predictions, (2) omitting non-matching predicted labels, and (3) trusting only mutual predictions of both GPT models. The study evaluated performance using accuracy, F1 score, and weighted F1 score metrics, comparing results across scenarios and model versions.

## Key Results
- GPT 4 achieves 83% accuracy when its predictions agree with GPT 3.5, covering 65% of documents
- Overall accuracy ranges from 58-83% depending on scenario and model version
- Complete reliance on GPT with minimal human intervention is suboptimal for policy document classification
- Model agreement filtering significantly improves classification reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuned LLMs can perform multiclass classification without task-specific training
- Mechanism: Zero-shot learning leverages pre-trained knowledge and instruction-following capability to map text to predefined categories based on semantic understanding
- Core assumption: Training corpus included sufficient examples of policy topics and their semantic boundaries
- Evidence anchors: Abstract mentions using pre-trained instruction-tuned LLMs for CAP topic classification; section explains instruction-tuned LLMs use next token prediction for user prompts

### Mechanism 2
- Claim: Combining predictions from multiple LLM versions improves accuracy through model disagreement filtering
- Mechanism: Consensus predictions between GPT 3.5 and GPT 4 are more reliable due to complementary strengths and reduced individual biases
- Core assumption: Models have different strengths and weaknesses that are not perfectly correlated
- Evidence anchors: Abstract shows 83% accuracy when models agree; section states overall accuracy exceeds 80% when models agree

### Mechanism 3
- Claim: Prompt engineering significantly affects LLM classification performance, especially for earlier model versions
- Mechanism: Carefully crafted prompts with category descriptions and examples help models understand the classification task and reduce ambiguity
- Core assumption: Models can extract relevant information from prompt structure to improve decisions
- Evidence anchors: Section describes using prompts with CAP topic descriptions and example titles; mentions including information to help with ambiguous issues

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: Study uses pre-trained LLMs without fine-tuning, requiring understanding of how models generalize to new tasks
  - Quick check question: How does zero-shot learning differ from few-shot learning in terms of required input and expected performance?

- Concept: Classification accuracy metrics (precision, recall, F1-score)
  - Why needed here: Study evaluates performance using multiple metrics that capture different aspects of classification quality
  - Quick check question: When would a model have high accuracy but poor F1-score, and what does that indicate about its performance?

- Concept: Model agreement and consensus filtering
  - Why needed here: Third scenario relies on using only predictions where two models agree
  - Quick check question: What statistical relationship between model errors would make consensus filtering most effective?

## Architecture Onboarding

- Component map: Data sources (congressional bills and hearings) -> LLM models (GPT 3.5 and GPT 4) -> Processing pipeline (three scenarios) -> Evaluation (accuracy and F1 metrics)
- Critical path: Prepare text data and prompts -> Generate predictions using both models -> Apply scenario-specific filtering rules -> Calculate performance metrics -> Compare across scenarios and models
- Design tradeoffs: Cost vs. accuracy (GPT 4 costs ~20x more but achieves higher accuracy), context length vs. prompt complexity, human effort vs. automation
- Failure signatures: Low precision on specific classes (e.g., immigration at 0.28 for GPT 4), high variance in accuracy across scenarios, poor performance on out-of-domain text
- First 3 experiments: Compare zero-shot vs. few-shot learning on small subset, test model agreement filtering on balanced dataset, evaluate cross-dataset generalization on different time periods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance vary when classifying documents from different political contexts or time periods not in training data?
- Basis in paper: [inferred] Paper notes ability to generalize depends on whether data was used in training GPT, but doesn't test different contexts
- Why unresolved: Only tests US congressional data which may be in training data
- What evidence would resolve it: Systematic testing on policy documents from different countries, time periods, or political systems not in training data

### Open Question 2
- Question: What is the impact of prompt engineering on GPT 4 vs GPT 3.5 classification accuracy, and how does this change with future model updates?
- Basis in paper: [explicit] Paper notes GPT 4 is less susceptible to prompt engineering than GPT 3.5, but doesn't explore full range of techniques
- Why unresolved: Brief mention without comprehensive analysis of various prompt engineering strategies
- What evidence would resolve it: Systematic study testing various prompt engineering techniques on both models, including future versions

### Open Question 3
- Question: How does GPT accuracy compare to state-of-the-art custom-tailored approaches for specific policy topics with lower performance?
- Basis in paper: [explicit] Paper identifies topics like immigration with significantly lower accuracy but doesn't benchmark against specialized models
- Why unresolved: Doesn't benchmark GPT performance on problematic topics against custom-tailored approaches
- What evidence would resolve it: Direct comparison of GPT performance on problematic topics against state-of-the-art custom approaches

## Limitations
- Zero-shot learning without task-specific fine-tuning constrains performance on nuanced policy classification
- ~83% accuracy only covers 65% of cases where models agree, leaving 35% requiring human review
- Lacks ablation studies to quantify impact of prompt engineering versus model capabilities

## Confidence

**High Confidence**: GPT 4 outperforms GPT 3.5 (accuracy ~80% vs ~58-70%) is well-supported by direct comparison and aligns with known model capabilities. Consensus filtering improving accuracy through model disagreement is robust, demonstrated by 83% accuracy when models agree.

**Medium Confidence**: Effectiveness of prompt engineering for GPT 3.5 and its reduced importance for GPT 4 is plausible but lacks quantitative validation. Cost-effectiveness tradeoffs between models are reasonable but depend on unmeasured factors.

**Low Confidence**: Generalizability to other policy domains or document types is uncertain, as study only evaluates congressional bills and hearings. Long-term reliability of consensus filtering as model versions evolve is unknown.

## Next Checks

1. Conduct ablation studies comparing zero-shot vs. few-shot learning on identical datasets to quantify impact of prompt engineering versus model capabilities.

2. Perform category-level error analysis to identify systematic misclassification patterns and test whether specific CAP topics consistently challenge the models.

3. Evaluate model performance on out-of-domain policy documents (e.g., state legislation or international policy texts) to assess generalizability beyond congressional data.