---
ver: rpa2
title: 'Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical
  Overlap in Train and Test Reference Summaries'
arxiv_id: '2311.09458'
source_url: https://arxiv.org/abs/2311.09458
tags:
- training
- summaries
- data
- test
- xsum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a key weakness in summarization model evaluation:
  average scores mask large performance differences between test subsets with low
  vs high lexical overlap with training summaries. The authors propose a fine-grained
  evaluation protocol that partitions test data by lexical similarity to training
  summaries and shows models perform significantly worse on low-overlap subsets.'
---

# Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries

## Quick Facts
- **arXiv ID**: 2311.09458
- **Source URL**: https://arxiv.org/abs/2311.09458
- **Reference count**: 25
- **Primary result**: Average summarization scores mask large performance gaps between test subsets with low vs high lexical overlap with training summaries

## Executive Summary
This paper identifies a critical weakness in current summarization model evaluation: models perform significantly worse on test subsets with low lexical overlap with training summaries, yet average scores hide this disparity. Through controlled experiments, the authors demonstrate that increasing lexical repetitions in training summaries boosts average performance but harms generalization to novel content and increases factual errors. They propose a two-stage training approach that calibrates models on diverse training subsets after initial fine-tuning, which improves performance on low-overlap test subsets while maintaining overall scores. Human evaluation confirms this approach generates more factually consistent summaries on temporally distant news articles.

## Method Summary
The authors partition test data into subsets based on 4-gram overlap with training summaries, then train models on training data with controlled lexical diversity (varying θ4G thresholds). They evaluate performance across these partitions using ROUGE-2 and entity-based metrics. To address the generalization problem, they propose a two-stage training approach: initial full-data fine-tuning followed by BRIO calibration on diverse training subsets. This calibration teaches models to rank diverse generated summaries higher, reducing reliance on memorized patterns.

## Key Results
- Test subsets with lowest lexical overlap show up to 5x lower ROUGE-2 scores compared to high-overlap subsets
- Increasing lexical repetitions (higher θ4G) in training improves average performance but reduces entity precision and increases factual hallucinations
- BRIO calibration on diverse training subsets improves performance on low-overlap test cases while maintaining overall scores
- Human evaluation confirms improved factual consistency and relevance for temporally distant articles

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Lexical overlap between training and test reference summaries drives systematic performance differences in summarization models.
- **Mechanism**: The model's ability to generate summaries correlates with the lexical similarity between reference test summaries and training summaries. High overlap allows models to rely on pattern matching and memorization rather than true comprehension and generalization.
- **Core assumption**: Test subsets with low lexical overlap represent novel summary-worthy content that requires genuine summarization capabilities rather than memorization.
- **Evidence anchors**:
  - [abstract]: "We observe up to a 5x (1.2x) difference in ROUGE-2 (entity recall) scores between the subsets with the lowest and highest similarity."
  - [section 4]: "As shown in Fig. 1a and 1d, all models obtain significantly lower R2 scores on the most novel test subset (Tnov) for both XSUM and CNN."
  - [corpus]: Weak evidence - related works focus on lexical quality but not the specific overlap-performance correlation described here.
- **Break condition**: If test data partitioning does not correlate with semantic novelty (e.g., if high-overlap subsets contain truly novel content), the mechanism fails.

### Mechanism 2
- **Claim**: Increased lexical repetitions in training summaries lead to rote learning and factual hallucinations.
- **Mechanism**: When training summaries contain repeated factual information, models learn to reproduce these facts regardless of source document content, treating them as generic summarization artifacts rather than context-specific information.
- **Core assumption**: Factual errors in training summaries act as data artifacts that models learn to reproduce when test summaries have high lexical overlap with training data.
- **Evidence anchors**:
  - [abstract]: "We show that such training repetitions also make a model vulnerable to rote learning, reproducing data artifacts such as factual errors, especially when reference test summaries are lexically close to training summaries."
  - [section 5.2]: "As expected, repeated 4-grams in training summaries (i.e., higher θ4G) lead to more hallucinated facts (high remembered entities) and more factual errors (low entity precision)."
  - [corpus]: Weak evidence - related works study repetition within generated summaries but not the specific effect of training summary repetitions.
- **Break condition**: If training data is curated to eliminate factual errors, this mechanism loses its explanatory power.

### Mechanism 3
- **Claim**: Calibrating models on diverse training subsets after initial fine-tuning improves generalization while maintaining overall performance.
- **Mechanism**: Initial full-data fine-tuning allows models to learn general summarization capabilities and assign high probabilities to reference summaries. Subsequent calibration on diverse subsets teaches the model to rank diverse generated summaries higher, reducing reliance on memorized patterns.
- **Core assumption**: The two-stage training process (full-data fine-tuning followed by diverse-data calibration) can balance between utilizing all training information and preventing rote learning.
- **Evidence anchors**:
  - [abstract]: "We propose to limit lexical repetitions in training summaries during both supervised fine-tuning and likelihood calibration stages to improve the performance on novel test cases while retaining average performance."
  - [section 5.3]: "BRIO learns to assign high probability mass to a generated summary if it obtains a higher ROUGE score compared to the reference training summary. So, by controlling the diversity of training summaries, we can teach the model to assign relatively higher probabilities to diverse generated summaries."
  - [corpus]: Weak evidence - related calibration approaches exist but not specifically targeting lexical diversity.
- **Break condition**: If diverse subsets are too small to provide meaningful calibration signals, performance gains disappear.

## Foundational Learning

- **Concept**: Lexical similarity measurement using n-gram overlap
  - Why needed here: The entire evaluation protocol and training data selection depend on quantifying lexical similarity between summaries.
  - Quick check question: If a reference test summary has 10 four-grams and 4 of them appear in training summaries, what is its lexical overlap percentage?

- **Concept**: ROUGE and entity-based evaluation metrics
  - Why needed here: The paper uses multiple metrics (ROUGE-2, entity recall, entity precision, remembered entities) to assess different aspects of summarization quality.
  - Quick check question: If a generated summary contains 5 entities, 3 of which are in both the reference summary and source document, what is the entity recall?

- **Concept**: Contrastive learning and likelihood calibration
  - Why needed here: The BRIO calibration approach uses contrastive loss to rank generated summaries, which is central to the proposed solution.
  - Quick check question: In contrastive summarization training, what is the model trying to optimize when comparing multiple generated summaries?

## Architecture Onboarding

- **Component map**: Data pipeline (XSUM, CNN, WikiHow, SamSum datasets → 4-gram extraction → lexical overlap calculation → test partitioning) → Model pipeline (BART/PEGASUS/BRIO/REINA → supervised fine-tuning → likelihood calibration → evaluation) → Evaluation pipeline (ROUGE metrics + entity-based metrics → partitioned test sets → performance comparison)

- **Critical path**: Data partitioning (4-gram overlap calculation) → Model training with controlled lexical diversity → Evaluation on partitioned test sets → Analysis of performance patterns

- **Design tradeoffs**: 
  - Using 4-grams vs other n-gram sizes: 4-grams balance between sufficient sample size in partitions and meaningful lexical similarity measurement
  - Two-stage training vs single-stage: Two-stage allows maximizing training data usage while preventing rote learning
  - Entity-based vs token-based evaluation: Entity metrics better capture factual consistency but are more computationally expensive

- **Failure signatures**:
  - Uniform performance across all test partitions suggests partitioning method doesn't capture meaningful differences
  - High entity precision but low ROUGE scores indicates models may be generating correct but non-overlapping content
  - Calibration on diverse subsets degrades overall performance suggests insufficient diverse training data

- **First 3 experiments**:
  1. Reproduce the 4-gram overlap calculation and test partitioning for XSUM dataset to verify the performance gap between Tnov and Tsim
  2. Train BART models with different θ4G values on XSUM and measure ROUGE-2 and entity metrics on partitioned test sets
  3. Implement BRIO calibration on a model trained with θ4G=25 and compare performance against model calibrated on all data

## Open Questions the Paper Calls Out
- The paper acknowledges that all experiments use English datasets and that the findings may not generalize to other languages with different morphological complexity.

## Limitations
- The core assumption that lexical overlap partitions perfectly align with semantic novelty is not empirically validated.
- Human evaluation was conducted on a limited subset of temporally distant news articles, raising questions about generalizability across different domains.
- The proposed calibration approach requires careful balance between diverse and non-diverse training data, with potential performance degradation if diverse subsets are too small.

## Confidence

**High confidence**: The observation that test subsets with different lexical overlap show performance gaps (up to 5x difference in ROUGE-2 scores) is well-supported by empirical evidence across multiple datasets and models. The mechanism linking training repetitions to factual errors is also strongly supported by the correlation between higher θ4G values and increased hallucinated entities.

**Medium confidence**: The proposed two-stage training solution (full-data fine-tuning followed by diverse-data calibration) shows promise, but the sample size for human evaluation and the limited scope of test cases (temporally distant news articles) reduce confidence in the robustness of these findings across different domains and evaluation scenarios.

**Low confidence**: The assumption that lexical overlap partitions perfectly align with semantic novelty is not empirically validated. Without this validation, the interpretation of performance differences as reflecting true generalization capabilities versus coincidental correlations remains uncertain.

## Next Checks
1. **Validate semantic-novelty correlation**: Conduct semantic similarity analysis (using sentence embeddings) between test partitions to verify that low-overlap subsets indeed contain more semantically novel content compared to high-overlap subsets.

2. **Cross-domain generalization test**: Apply the evaluation protocol to non-news domains (e.g., scientific papers or dialogue summaries) to test whether the lexical overlap-performance relationship holds across different content types.

3. **Ablation study on calibration subset size**: Systematically vary the size of diverse training subsets used for BRIO calibration to determine the minimum effective size and identify potential performance degradation thresholds.