---
ver: rpa2
title: Hallucination Reduction in Long Input Text Summarization
arxiv_id: '2309.16781'
source_url: https://arxiv.org/abs/2309.16781
tags:
- summary
- dataset
- text
- summarization
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reducing hallucinations in
  long-form text summarization. The authors propose a method that fine-tunes the Longformer
  Encoder-Decoder (LED) model using data filtering and the Joint sAlient ENtity and
  Summary generation (JAENS) approach.
---

# Hallucination Reduction in Long Input Text Summarization

## Quick Facts
- arXiv ID: 2309.16781
- Source URL: https://arxiv.org/abs/2309.16781
- Reference count: 24
- Primary result: Data filtering improves entity-level precision but reduces traditional metrics; JAENS approach does not improve F1 scores.

## Executive Summary
This paper addresses the problem of reducing hallucinations in long-form text summarization by fine-tuning the Longformer Encoder-Decoder (LED) model using data filtering and the Joint sAlient ENtity and Summary generation (JAENS) approach on the PubMed dataset. The authors propose filtering training data to remove examples where named entities in the summary cannot be matched to the source document, and using JAENS to explicitly train the model to generate salient named entities before the summary. While data filtering improves entity-level precision, it reduces traditional metrics like ROUGE, METEOR, and BERTScore. The JAENS approach does not improve F1 scores and may reduce them further, prompting plans for future investigation.

## Method Summary
The method involves fine-tuning the LED model on the PubMed dataset using three variants: standard fine-tuning, fine-tuning with a filtered dataset, and fine-tuning with the filtered dataset plus JAENS. The filtering process removes article-abstract pairs where named entities in the abstract cannot be matched to the source document using n-gram matching via scispaCy NER. JAENS trains the model to generate a sequence containing summary-worthy named entities, a special separator token, and the summary itself. The models are evaluated using both traditional metrics (ROUGE, METEOR, BERTScore) and entity-level factual consistency metrics (precision-source, F1-target).

## Key Results
- Data filtering improves entity-level precision but reduces traditional metrics like ROUGE, METEOR, and BERTScore.
- JAENS approach does not improve F1 scores and may reduce them further.
- The authors plan to investigate the reasons behind these findings in future work.

## Why This Works (Mechanism)

### Mechanism 1
Data filtering reduces entity-level hallucinations by removing training examples where named entities in the summary cannot be matched to the source document using scispaCy NER and n-gram matching. The core assumption is that removing training examples with mismatched entities will prevent the model from learning to hallucinate those entities during inference.

### Mechanism 2
JAENS improves entity-level precision by explicitly training the model to generate salient named entities before the summary, using a multitask learning setup where the model produces entities, a special token, and the summary in sequence. The core assumption is that by prioritizing entity generation, the model will be more likely to include correct, salient entities in the final summary.

### Mechanism 3
Fine-tuning on a filtered dataset improves precision-source scores because the model is trained only on examples where every named entity in the summary has a corresponding n-gram match in the source. The core assumption is that training data quality is more important than quantity for reducing hallucinations; removing noisy examples improves factual consistency.

## Foundational Learning

- **Named Entity Recognition (NER) with scispaCy**: Why needed here - The paper relies on scispaCy to identify scientific named entities in both source documents and summaries for filtering and evaluation. Quick check - How does scispaCy handle domain-specific terminology in biomedical texts compared to general-purpose NER tools?

- **Longformer Encoder-Decoder (LED) architecture**: Why needed here - LED is chosen for its ability to handle long input sequences (up to 8192 tokens) via sliding window and dilated attention mechanisms, which is essential for scientific paper summarization. Quick check - What is the difference between sliding window and dilated sliding window attention in LED, and why is it important for long documents?

- **Entity-level factual consistency metrics**: Why needed here - Traditional summarization metrics like ROUGE do not capture factual consistency; the paper introduces precision-source, precision-target, recall-target, and F1-target to measure entity hallucination. Quick check - How do precision-source and precision-target differ in their evaluation of hallucinations, and why are both needed?

## Architecture Onboarding

- **Component map**: Input (long scientific document) -> Preprocessor (text cleaning, scispaCy NER, entity-based filtering) -> Model (LED-base-16384) -> JAENS module (concatenates entities + special token + summary for training) -> Output (generated summary + entity-level evaluation metrics)

- **Critical path**: 1. Load and preprocess PubMed dataset (cleaning, NER, filtering) 2. Fine-tune LED on original dataset 3. Fine-tune LED on filtered dataset 4. Fine-tune LED with JAENS on filtered dataset 5. Evaluate all models using ROUGE, BERTScore, METEOR, and entity-level metrics

- **Design tradeoffs**: Using LED-base-16384 instead of larger variants limits maximum context and capacity but reduces training cost. Entity-based filtering improves factual consistency but reduces dataset size and may hurt traditional metric scores. JAENS adds complexity to decoding but aims to improve entity precision; however, it may not improve F1 scores.

- **Failure signatures**: Precision-source remains low despite filtering → NER or n-gram matching is not working correctly. F1-target drops significantly → Model is generating fewer entities overall, possibly due to over-filtering or JAENS complexity. ROUGE scores drop after filtering → Too much data removed or filtering is too aggressive.

- **First 3 experiments**: 1. Train LED on original dataset, evaluate with all metrics, confirm baseline performance. 2. Apply entity-based filtering, train on filtered dataset, compare precision-source and traditional metrics to baseline. 3. Apply JAENS approach on filtered dataset, evaluate entity-level metrics and check if F1-target improves.

## Open Questions the Paper Calls Out

### Open Question 1
Why does the data filtering technique improve entity-level precision (precs) but not F1-target scores in the generated summaries? The paper states that data filtering improves precs scores but F1-target scores remain unchanged or may even decrease. The authors plan to investigate this behavior in future work.

### Open Question 2
How does the accuracy of entity recognition impact the effectiveness of the JAENS approach in reducing hallucinations? The authors note that entity recognition accuracy may be an issue, as many phrases detected as entities do not appear to be very important, but their match/mismatch between the generated and golden summary do impact the F1t scores.

### Open Question 3
What specific factors contribute to the reduction in ROUGE, METEOR, and BERTScore values observed in all the hallucination-mitigating designs? The authors mention that they observed a reduction in these traditional metrics in all the hallucination-mitigating designs and plan to study this issue in detail.

## Limitations
- The exact relationship between filtering criteria and observed metric changes is not fully explained, particularly why traditional metrics decrease despite improved entity-level precision.
- JAENS effectiveness is not validated; the approach does not improve F1 scores and may reduce them, with no detailed error analysis provided.
- The paper does not report NER performance or error rates, nor does it discuss how NER failures might impact filtering and downstream metrics.

## Confidence
- Data filtering improves entity-level precision: Medium confidence
- JAENS improves entity-level precision: Low confidence
- LED-base-16384 is suitable for long scientific document summarization: High confidence

## Next Checks
1. Analyze filtering impact on dataset diversity by varying the filtering threshold and comparing entity-level precision, traditional metrics, and dataset size at each threshold.
2. Validate JAENS mechanism with error analysis by performing a detailed qualitative analysis of generated summaries from JAENS vs. non-JAENS models.
3. Benchmark NER accuracy on biomedical texts by evaluating scispaCy's NER performance on a held-out set of PubMed abstracts with manual entity annotation.