---
ver: rpa2
title: On Conditional and Compositional Language Model Differentiable Prompting
arxiv_id: '2307.01446'
source_url: https://arxiv.org/abs/2307.01446
tags:
- props
- task
- tasks
- rules
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PROPS is a differentiable prompt generator that learns to transform
  task instructions or input metadata into continuous prompts, conditioned on task
  and input-specific text. It uses a modular network structure based on Production
  Systems, allowing it to learn discrete rules and specialize in transforming particular
  prompt input patterns.
---

# On Conditional and Compositional Language Model Differentiable Prompting

## Quick Facts
- arXiv ID: 2307.01446
- Source URL: https://arxiv.org/abs/2307.01446
- Authors: 
- Reference count: 40
- Key outcome: PROPS is a differentiable prompt generator that learns to transform task instructions or input metadata into continuous prompts, conditioned on task and input-specific text. It uses a modular network structure based on Production Systems, allowing it to learn discrete rules and specialize in transforming particular prompt input patterns. PROPS consistently surpasses other PLM adaptation techniques and often improves upon fully fine-tuned models on compositional generalization tasks, controllable summarization, and multilingual translation, while needing fewer trainable parameters.

## Executive Summary
PROPS introduces a novel approach to differentiable prompt generation for language models, addressing the limitations of manual prompt engineering and simple continuous prompt methods. The model leverages a modular network structure inspired by Production Systems to learn discrete rules that specialize in transforming specific prompt input patterns. This conditional and compositional approach enables PROPS to effectively adapt frozen PLMs to downstream tasks while requiring fewer trainable parameters than traditional fine-tuning methods.

## Method Summary
PROPS is a conditional differentiable prompting method that transforms task instructions or input metadata into continuous prompts using a modular network structure based on Production Systems. The model consists of N separately differentiable rules, each represented by an attention head that specializes in transforming particular prompt input patterns. PROPS uses a Gumbel top-k mechanism to sparsely select a subset of rules based on the conditioning text, allowing for efficient composition of knowledge from different tasks and inputs. The generated prompts are prepended to the PLM's hidden states, enabling effective adaptation to downstream tasks without modifying the PLM's parameters.

## Key Results
- PROPS consistently outperforms other PLM adaptation techniques on compositional generalization, controllable summarization, and multilingual translation tasks.
- The model often improves upon fully fine-tuned models while requiring fewer trainable parameters.
- PROPS demonstrates strong compositional transfer learning and few-shot learning capabilities due to its modular structure and sparse rule selection mechanism.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditional prompting with task-specific instructions improves performance by providing explicit guidance for the model.
- Mechanism: PROPS uses task instructions or metadata as conditioning text, which is encoded and mapped to a set of learnable attention heads. These heads then transform the input text to generate task-specific prompts that are prepended to the PLM's hidden states.
- Core assumption: The model can effectively learn to map textual instructions to continuous prompts that elicit the desired task-specific outputs from the PLM.
- Evidence anchors:
  - [abstract]: "Our model uses a modular network structure based on our neural formulation of Production Systems, which allows the model to learn discrete rules – neural functions that learn to specialize in transforming particular prompt input patterns, making it suitable for compositional transfer learning and few-shot learning."
  - [section 3.2]: "PROPS consists of N separately differentiable rules, {R1, R2, .., RN } that are each represented by Ri = ( ⃗Ri,eHi), where ⃗Ri is a rule embedding vector andeHi is an attention head."
  - [corpus]: Weak. The corpus does not contain evidence directly supporting this mechanism. The related papers focus on different aspects of prompting, such as vision-language models and sentence embeddings.
- Break condition: If the model fails to learn meaningful mappings between instructions and prompts, or if the learned prompts do not effectively guide the PLM for the desired tasks.

### Mechanism 2
- Claim: Modular network structure with sparse rule selection improves generalization and compositionality.
- Mechanism: PROPS uses a modular network structure based on Production Systems, where each module (rule) is represented by an attention head. The model sparsely selects a subset of rules based on the conditioning text, allowing it to compose knowledge from different tasks and inputs.
- Core assumption: The model can effectively learn to select and combine relevant rules for different tasks and inputs, leading to improved generalization and compositionality.
- Evidence anchors:
  - [abstract]: "Our model uses a modular network structure based on our neural formulation of Production Systems, which allows the model to learn discrete rules – neural functions that learn to specialize in transforming particular prompt input patterns, making it suitable for compositional transfer learning and few-shot learning."
  - [section 3.2]: "Each condition sequence SC = ⟨ct|t ∈ {1, . . . , TC}⟩C, where TC is the sequence length of condition C ∈ C and C is the set of conditions, is first encoded by a Condition Encoder f(·). We use a light-weight attentive max pooling layer[Wu et al., 2020] to encode condition sequences into a fix sized representation. Each condition vector ⃗C is then concatenated together to create a Condition Embedding matrix [E] ∈ R|C|×d, where d is the embedding dimension."
  - [corpus]: Missing. The corpus does not contain evidence directly supporting this mechanism.
- Break condition: If the model fails to learn meaningful rule selection or if the selected rules do not effectively combine knowledge for improved generalization and compositionality.

### Mechanism 3
- Claim: Prompt generation as a differentiable process allows for efficient adaptation of PLMs to downstream tasks.
- Mechanism: PROPS generates continuous prompts that are differentiable and can be optimized through gradient descent. These prompts are prepended to the PLM's hidden states, allowing the model to adapt to downstream tasks without modifying the PLM's parameters.
- Core assumption: The differentiable prompt generation process can effectively adapt the PLM to downstream tasks, leading to improved performance compared to other adaptation methods.
- Evidence anchors:
  - [abstract]: "Prompts can be represented by a human-engineered word sequence or by a learned continuous embedding. In this work, we investigate conditional and compositional differentiable prompting."
  - [section 3.1]: "Conditional Prompt Generation, as in PROPS, is achieved by plugging condition-dependent prompt vectors [Pk, Pv] = [Pk(ct), Pv(ct)] = P (ct) in Equation 1, where Pk, Pv is created by splitting the last dimension of P (ct) ∈ RTP ×2d in two and ct is the word embedding at the tth condition token."
  - [corpus]: Weak. The corpus contains papers on differentiable prompting for vision-language models and sentence embeddings, but does not directly support the claim for PLM adaptation.
- Break condition: If the differentiable prompt generation process fails to effectively adapt the PLM to downstream tasks, or if other adaptation methods (e.g., fine-tuning) outperform PROPS.

## Foundational Learning

- Concept: Production Systems
  - Why needed here: PROPS is based on a neural formulation of Production Systems, which allows the model to learn discrete rules and specialize in transforming particular prompt input patterns.
  - Quick check question: Can you explain how Production Systems work and how they relate to the modular structure of PROPS?

- Concept: Attention mechanisms in Transformers
  - Why needed here: PROPS uses attention heads as the basic building blocks for its rules. Understanding how attention mechanisms work in Transformers is crucial for understanding how PROPS selects and applies rules.
  - Quick check question: Can you describe how multi-head attention works in Transformers and how it is used in PROPS for rule selection?

- Concept: Prompt engineering and continuous prompts
  - Why needed here: PROPS generates continuous prompts that are prepended to the PLM's hidden states. Understanding the concept of prompts and how they can be engineered or learned is essential for grasping how PROPS adapts PLMs to downstream tasks.
  - Quick check question: Can you explain the difference between discrete and continuous prompts, and how continuous prompts can be optimized through gradient descent?

## Architecture Onboarding

- Component map: Condition Encoder -> Rule Embedding -> Rule-Condition Matrix -> Gumbel Top-k -> Prompt Generator

- Critical path:
  1. Encode conditioning text using the Condition Encoder.
  2. Create the Rule-Condition Matrix by multiplying the Condition Embedding and Rule Embedding matrices.
  3. Apply Gumbel Top-k to select a sparse subset of rules for each condition.
  4. Apply the selected rules to the conditioning text using attention mechanisms.
  5. Generate the final prompts by concatenating the outputs of the selected rules.

- Design tradeoffs:
  - Number of rules (N) vs. model complexity: Increasing the number of rules allows for more specialized transformations but also increases the model's complexity and computational cost.
  - Top-k selection vs. rule coverage: Choosing a larger k allows for more rule combinations but may also lead to redundancy and over-smoothing of task-specific information.

- Failure signatures:
  - Poor performance on downstream tasks: Indicates that the learned prompts are not effectively guiding the PLM or that the rule selection mechanism is not capturing the relevant information from the conditioning text.
  - Overfitting to the training data: Suggests that the model is not generalizing well to unseen tasks or inputs, possibly due to an insufficient number of rules or an overly complex rule selection mechanism.

- First 3 experiments:
  1. Ablation study on the number of rules (N) and top-k selection (k) to find the optimal balance between model complexity and performance.
  2. Comparison of PROPS with other prompt generation methods (e.g., Prefix-Tuning, Prompt-Tuning) on a diverse set of downstream tasks to assess its effectiveness.
  3. Analysis of the learned rule embeddings and their relationship to the conditioning text to gain insights into how PROPS captures task-specific information.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of k (number of selected rules) and N (total number of available rules) affect PROPS performance and generalization?
- Basis in paper: [explicit] The paper mentions that PROPS performance is dependent on k and N, and provides theoretical bounds and experimental results showing that using more rules does not always translate into better performance. It also notes that PROPS works best when T k = 1/2 N.
- Why unresolved: While the paper provides some insights, the relationship between k, N, and performance is complex and may depend on the specific task and dataset. The paper conjectures that too much rule overlap may oversmooth task-specific information, but this needs further investigation.
- What evidence would resolve it: Additional experiments varying k and N across different tasks and datasets, along with theoretical analysis, could help clarify the optimal choice of k and N for PROPS.

### Open Question 2
- Question: How does PROPS compare to other PLM adaptation methods in terms of parameter efficiency and performance on different tasks and datasets?
- Basis in paper: [explicit] The paper states that PROPS often improves upon fully fine-tuned models and consistently surpasses other PLM adaptation techniques, while needing fewer trainable parameters. However, it also mentions that fully fine-tuned models have an edge over adaptation methods in the full data multitask regime.
- Why unresolved: The paper provides results on specific tasks and datasets, but it is unclear how PROPS would perform on other tasks and datasets, especially in comparison to other adaptation methods like LoRA or Prefix-Tuning.
- What evidence would resolve it: Additional experiments comparing PROPS to other adaptation methods on a wider range of tasks and datasets, along with an analysis of parameter efficiency, would help determine its relative strengths and weaknesses.

### Open Question 3
- Question: How does the composition of task instructions from support tasks affect the performance of PROPS on target tasks?
- Basis in paper: [explicit] The paper mentions that PROPS can compose knowledge from bridge tasks to enhance transfer learning and improve unseen tasks in few-shot settings. It also shows that using detailed task instructions with greater overlap between support and target tasks can improve performance.
- Why unresolved: The paper provides some evidence for the benefits of instruction composition, but it is unclear how much of the improvement is due to the composition of instructions versus the composition of modules (rules). It is also unclear how to best construct task instructions for optimal performance.
- What evidence would resolve it: Further experiments ablating the effects of instruction composition and module composition, along with an analysis of the optimal structure of task instructions, could help clarify the role of instruction composition in PROPS performance.

## Limitations

- The paper's claims about PROPS' effectiveness are based on experiments on specific datasets (SCAN, XSum, Europarl), and it is unclear how well the model generalizes to other domains or with larger PLMs.
- While the modular Production System approach is theoretically sound, it may face scaling challenges with more complex tasks or longer prompts.
- The training efficiency claims (fewer parameters than fine-tuning) need verification across different PLM sizes and task complexities.

## Confidence

- **High Confidence**: The PROPS architecture is technically sound and the modular approach using Production Systems is well-defined. The paper clearly explains how condition encoding, rule selection via Gumbel top-k, and prompt generation work together. The implementation details for SCAN experiments are sufficiently specified for reproduction.

- **Medium Confidence**: The experimental results on SCAN show consistent improvements across compositional splits, suggesting PROPS effectively handles compositional generalization. However, the XSum and Europarl results, while promising, involve fewer comparisons and less detailed hyperparameter analysis. The claim of fewer trainable parameters than fine-tuning is supported but could benefit from more comprehensive ablation studies.

- **Low Confidence**: The paper's claims about PROPS' advantages over other differentiable prompting methods (like Prefix-Tuning) are not thoroughly validated. The comparison focuses mainly on end-task performance rather than examining the quality of generated prompts or the interpretability of learned rules. The analysis of why PROPS works better than alternatives is somewhat superficial.

## Next Checks

1. **Rule Interpretability Analysis**: Conduct a detailed analysis of the learned rule embeddings to verify they capture meaningful task-specific patterns. This should include visualization of rule activations across different condition types and correlation analysis between rule selection patterns and task performance.

2. **Cross-Domain Generalization Test**: Evaluate PROPS on a broader range of tasks beyond the three presented, particularly in domains with different linguistic characteristics (e.g., legal, medical, or technical domains). This would test the claim that PROPS generalizes well across diverse compositional tasks.

3. **Scaling Analysis**: Test PROPS with different PLM sizes (e.g., T5-small, T5-base, T5-large) to verify the claimed parameter efficiency holds across scales. Additionally, analyze how performance scales with the number of rules (N) and top-k selection (k) to identify optimal configurations for different task complexities.