---
ver: rpa2
title: 'VeCLIP: Improving CLIP Training via Visual-enriched Captions'
arxiv_id: '2310.07699'
source_url: https://arxiv.org/abs/2310.07699
tags:
- clip
- data
- veclip
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a scalable pipeline to improve the quality
  and diversity of large-scale pre-training data for vision-language models (VLMs)
  by leveraging visual concepts extracted from images. Specifically, it introduces
  a method to rewrite captions using large language models (LLMs) to incorporate visual-enriched
  concepts (VeC) and proposes a mixed training scheme that alternates between original
  AltTexts and rewritten captions.
---

# VeCLIP: Improving CLIP Training via Visual-enriched Captions

## Quick Facts
- arXiv ID: 2310.07699
- Source URL: https://arxiv.org/abs/2310.07699
- Reference count: 40
- Key outcome: VeCLIP achieves up to +25.2% gain in COCO and Flickr30k retrieval tasks under the 12M setting, demonstrating data efficiency by using only 14% of the data employed in vanilla CLIP and 11% in ALIGN.

## Executive Summary
This paper proposes a scalable pipeline to improve the quality and diversity of large-scale pre-training data for vision-language models (VLMs) by leveraging visual concepts extracted from images. Specifically, it introduces a method to rewrite captions using large language models (LLMs) to incorporate visual-enriched concepts (VeC) and proposes a mixed training scheme that alternates between original AltTexts and rewritten captions. This approach, named VeCLIP, is evaluated on various scales of web-crawled datasets, achieving significant improvements in image-text alignment and overall model performance.

## Method Summary
VeCLIP improves CLIP training by rewriting captions to include visual concepts extracted from images. The method uses LLaVA to generate visual-enriched captions (VeC) and an LLM (Vicuna-1.1-13B) to fuse these concepts with original AltText, creating LLM-VeC. Training alternates between AltText and LLM-VeC using a mixed training scheme, enhancing data diversity and preventing overfitting.

## Key Results
- Up to +25.2% gain in COCO and Flickr30k retrieval tasks under the 12M setting.
- Achieves +3% gain while using only 14% of the data employed in vanilla CLIP and 11% in ALIGN.
- Demonstrates significant improvements in image-text alignment and overall model performance across various scales of web-crawled datasets.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Visual-enriched captions (VeC) improve image-text alignment by injecting visual concepts that are missing from noisy AltTexts.
- **Mechanism:** The pipeline uses LLaVA to extract image-centric visual entities and then fuses them with AltText via Vicuna to generate LLM-VeC. This dual-source fusion ensures both fidelity to the original caption and coverage of visual details.
- **Core assumption:** Visual concepts from LLaVA can be reliably extracted and remain faithful to the image content, and LLMs can merge them without introducing hallucinations.
- **Evidence anchors:**
  - [abstract]: "we emphasize exploiting visual concepts and their integration into the captions to improve data quality"
  - [section 3.3]: "we employ the prowess of LLMs to refine the caption, amalgamating insights derived from both the knowledge from AltText and the novel visual concepts from xTv"
  - [corpus]: Weak; no direct citations, but related works like "Parrot Captions Teach CLIP to Spot Text" imply text spotting bias in CLIP models that VeC could mitigate.
- **Break condition:** If LLaVA's visual extraction is noisy or biased, or if the LLM consistently hallucinates entities not present in the image.

### Mechanism 2
- **Claim:** Alternating between AltText and LLM-VeC during training (mixed training) improves data diversity and mitigates overfitting.
- **Mechanism:** Instead of training solely on rewritten captions, the model sees both original AltTexts and LLM-VeC in a uniform random mix. This provides both high-quality, concept-rich captions and the broader, unfiltered distribution of AltTexts.
- **Core assumption:** Diversity in training data improves generalization, and alternating prevents the model from overfitting to the uniform style of LLM rewrites.
- **Evidence anchors:**
  - [abstract]: "we propose a novel mixed training scheme that optimally leverages AltTexts alongside newly generated Visual-enriched Captions (VeC)"
  - [section 3.4]: "interchanging between AltText and LLM-VeC proves to be advantageous, not only in retaining substantial performance gains in retrieval tasks but also in markedly elevating zero-shot results on ImageNet"
  - [corpus]: Weak; no direct evidence, but "Multilingual Diversity Improves Vision-Language Representations" suggests diversity is broadly beneficial.
- **Break condition:** If the model's learning rate is too high, causing instability when switching between distributions, or if one source dominates the other due to imbalance in the sampling.

### Mechanism 3
- **Claim:** Scaling the rewriting pipeline to large datasets is feasible and effective without significant quality degradation.
- **Mechanism:** The authors avoid expensive API calls by using open-source Vicuna-1.1-13B for rewriting, truncating long AltTexts, and handling edge cases (e.g., ethical concerns) with fallbacks to VeC alone.
- **Core assumption:** Open-source LLMs can match the rewriting quality of closed models like ChatGPT at scale, and the fallback strategy preserves data utility.
- **Evidence anchors:**
  - [section 3.3]: "To facilitate the rewriting tasks on a large-scale dataset, we turn to open-source state-of-the-art LLMs. Due to the license issue, we select Vicuna-1.1 (Zheng et al., 2023)"
  - [section 3.3]: "we mitigate this issue by preserving VeC but truncating the AltText to conform to the maximum allowable length"
  - [corpus]: Weak; no direct citations, but "CLIP with Quality Captions: A Strong Pretraining for Vision Tasks" suggests quality captions are crucial, implying open-source rewrites could suffice if well-curated.
- **Break condition:** If Vicuna's rewriting quality degrades with scale or if truncation removes critical context, leading to poor alignment.

## Foundational Learning

- **Concept: Contrastive learning with image-text pairs**
  - Why needed here: CLIP training relies on aligning image and text embeddings via contrastive loss; understanding this loss is essential to grasp how VeCLIP modifies training data.
  - Quick check question: What is the role of the temperature parameter Ï„ in the contrastive loss formulation?

- **Concept: Multimodal model fusion (vision encoder + LLM)**
  - Why needed here: LLaVA fuses a vision encoder with an LLM to generate visual concepts; knowing how this fusion works clarifies how VeC is produced.
  - Quick check question: How does LLaVA project vision features into the LLM's token space?

- **Concept: Data augmentation and diversity in pre-training**
  - Why needed here: The mixed training scheme is a form of text augmentation; understanding data augmentation principles explains why alternating captions helps.
  - Quick check question: Why might training on only rewritten captions lead to overfitting?

## Architecture Onboarding

- **Component map:**
  - Vision encoder (CLIP's ViT-B/16 or ViT-L/14) -> Text encoder (CLIP's text encoder) -> LLaVA model (for VeC extraction) -> Vicuna-1.1-13B model (for LLM-VeC generation) -> JAX-based training pipeline with TPU scaling

- **Critical path:**
  1. Load image and AltText from dataset.
  2. Run image through LLaVA to generate VeC.
  3. Use Vicuna to fuse AltText and VeC into LLM-VeC.
  4. During training, sample either AltText or LLM-VeC uniformly.
  5. Compute contrastive loss and update encoders.

- **Design tradeoffs:**
  - Using Vicuna vs. ChatGPT: Vicuna is open-source and cost-effective but may have lower rewriting quality.
  - Truncating long AltTexts vs. full processing: Truncation is faster but may lose context.
  - Uniform mixing vs. weighted sampling: Uniform is simpler but may not account for quality differences between AltText and LLM-VeC.

- **Failure signatures:**
  - Degradation in retrieval tasks: Could indicate poor alignment in LLM-VeC.
  - Overfitting to LLM-VeC style: Could indicate insufficient diversity from AltText.
  - High variance in training loss: Could indicate instability from switching between distributions.

- **First 3 experiments:**
  1. Run a small-scale test (e.g., 1k samples) comparing retrieval performance with and without VeC generation.
  2. Test the mixing ratio effect (e.g., 25/75 vs. 50/50 vs. 75/25 AltText/LLM-VeC) on ImageNet zero-shot accuracy.
  3. Evaluate the effect of truncation length on retrieval performance and LLM-VeC generation time.

## Open Questions the Paper Calls Out
None explicitly stated in the provided text.

## Limitations
- Lack of direct comparisons to alternative caption rewriting strategies or ablation studies on the optimal mixing ratio.
- The truncation strategy for long AltTexts introduces a potential trade-off between computational efficiency and information retention that is not fully quantified.
- The assumption that open-source Vicuna can match the rewriting quality of proprietary models like ChatGPT at scale is plausible but unverified.

## Confidence
- **Medium**: The retrieval task improvements (e.g., +25.2% on COCO/Flickr30k) are well-documented, but the ImageNet zero-shot gains and data efficiency claims rely heavily on the specific dataset and model scale used.

## Next Checks
1. **Mixing ratio ablation**: Systematically test different AltText/LLM-VeC mixing ratios (e.g., 25/75, 50/50, 75/25) on ImageNet zero-shot accuracy to determine the optimal balance and confirm the benefits of alternating training.
2. **Rewriting quality comparison**: Compare the retrieval and zero-shot performance of models trained on Vicuna-generated LLM-VeC versus those trained on proprietary LLM-generated captions (e.g., ChatGPT) to quantify any quality gap.
3. **Truncation impact analysis**: Evaluate the effect of different truncation lengths on retrieval performance and LLM-VeC generation time to find the optimal trade-off between efficiency and information retention.