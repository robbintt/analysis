---
ver: rpa2
title: 'SUBP: Soft Uniform Block Pruning for 1xN Sparse CNNs Multithreading Acceleration'
arxiv_id: '2310.06218'
source_url: https://arxiv.org/abs/2310.06218
tags:
- pruning
- block
- subp
- conference
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SUBP proposes a novel approach to train 1\xD7N sparse CNNs from\
  \ scratch with balanced workload across threads. The method combines block angular\
  \ redundancy pruning (BPAR) with a uniform block pruning strategy, allowing pruned\
  \ blocks to regrow via importance sampling during training."
---

# SUBP: Soft Uniform Block Pruning for 1xN Sparse CNNs Multithreading Acceleration

## Quick Facts
- arXiv ID: 2310.06218
- Source URL: https://arxiv.org/abs/2310.06218
- Reference count: 40
- Key outcome: SUBP achieves up to 4× FLOPs reduction with 76.0% top-1 accuracy on ResNet50(1×32) ImageNet, outperforming state-of-the-art pruning methods.

## Executive Summary
SUBP introduces a novel approach for training 1xN sparse CNNs from scratch using soft uniform block pruning combined with block angular redundancy pruning (BPAR). The method periodically allows pruned blocks to regrow during training via importance sampling, eliminating the need for pre-training and fine-tuning while achieving better accuracy than existing methods. Experiments demonstrate competitive performance on ImageNet classification, object detection, and instance segmentation tasks.

## Method Summary
SUBP trains uniform 1xN sparse CNNs from scratch using a combined approach of periodic block pruning and regrowing via importance sampling with BPAR criterion. The method dynamically explores optimal sparse architectures while learning weights simultaneously, avoiding information loss from one-shot pruning of pre-trained models. The uniform sparsity pattern ensures balanced workload across threads, enabling efficient multithreading acceleration on CPUs.

## Key Results
- Achieves 4× FLOPs reduction with 76.0% top-1 accuracy on ResNet50(1×32) ImageNet
- Outperforms state-of-the-art pruning methods while training from scratch
- Demonstrates 2.1× speedup over ARM Cortex-A76 CPU with 1×N sparsity
- Shows competitive performance on object detection and instance segmentation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SUBP enables training 1xN sparse CNNs from scratch while maintaining accuracy by periodically allowing pruned blocks to regrow during training.
- Mechanism: Combines periodic block pruning and regrowing via importance sampling with uniform sparsity pattern to maintain model capacity and achieve balanced workload across threads.
- Core assumption: Model capacity can be maintained through periodic regrowing of pruned blocks, and balanced workload improves multithreading efficiency.
- Evidence anchors: Abstract states pruned blocks regrow based on block angular redundancy and importance sampling; section confirms compressed network has large model capacity.

### Mechanism 2
- Claim: BPAR improves pruning decisions by considering angular relationships between blocks rather than just magnitude-based importance.
- Mechanism: Evaluates blocks based on both norm and angular redundancy with other blocks, retaining blocks with different orientations even if they have smaller norms.
- Core assumption: Angular redundancy between blocks captures meaningful redundancy in feature space, allowing better preservation of diverse feature representations.
- Evidence anchors: Section introduces BPAR as new pruning criterion taking block angular redundancy into account; section explains blocks with similar orientation angles have angular redundancy.

### Mechanism 3
- Claim: Training from scratch with SUBP eliminates need for expensive pre-training and fine-tuning cycles while achieving better accuracy than methods dependent on dense pre-trained models.
- Mechanism: Incorporates pruning and regrowing directly into training process from random initialization, dynamically exploring optimal sparse architectures while learning weights simultaneously.
- Core assumption: Network can learn to adapt its sparse structure effectively during training without inductive bias from pre-trained weights.
- Evidence anchors: Abstract mentions approach trains uniform 1×N sparse CNNs from scratch effectively reducing training cost; section states compressed network achieves higher accuracy than others.

## Foundational Learning

- Concept: 1xN sparse pattern and Block Sparse Row (BSR) matrix
  - Why needed here: Understanding how 1xN sparsity partitions weights into blocks and how BSR matrix stores them continuously in memory is essential for grasping memory efficiency and multithreading acceleration claims
  - Quick check question: How does the 1xN pruning pattern differ from N:M pruning and unstructured pruning in terms of block structure and memory layout?

- Concept: Importance sampling and multinomial distribution
  - Why needed here: The regrowing mechanism uses importance sampling to select which pruned blocks to recover, requiring understanding how probabilities are assigned and sampled
  - Quick check question: In the context of SUBP, how is the importance sampling probability for block regrowing computed, and why is the temperature parameter τ used?

- Concept: Angular similarity and cosine distance
  - Why needed here: BPAR uses angular redundancy measured via cosine distance between vectorized blocks, fundamental to understanding the pruning criterion
  - Quick check question: What is the mathematical relationship between the angle between two vectors and their cosine similarity, and how does this relate to the angular redundancy measure in BPAR?

## Architecture Onboarding

- Component map:
  BPAR pruning criterion -> Block pruning stage -> Block regrowing stage -> Uniform sparsity enforcement -> Training scheduler

- Critical path:
  1. Initialize model with random weights
  2. For each training epoch:
     - Compute BPAR importance scores for all blocks
     - Prune blocks below threshold while maintaining uniform sparsity
     - Sample and regrow blocks based on importance probabilities
     - Update weights through standard backpropagation
  3. Export final pruned model with optimized block weights

- Design tradeoffs:
  - Periodic regrowing vs. one-shot pruning: Regrowing maintains capacity but adds complexity and computational overhead
  - Angular vs. norm-based importance: Angular considers redundancy but may be computationally more expensive
  - Uniform sparsity vs. adaptive sparsity: Uniform ensures balanced workload but may be suboptimal for some architectures

- Failure signatures:
  - Training instability or divergence: May indicate improper balance between pruning and regrowing rates
  - Underperformance compared to dense models: Could suggest regrowth mechanism isn't recovering important blocks
  - Load imbalance on CPUs: Would indicate uniform sparsity constraint isn't being properly enforced

- First 3 experiments:
  1. Reproduce MobileNetV1(1×16) ImageNet experiment from Table 1 to verify BPAR improves over ℓ1 norm pruning
  2. Test ResNet50(1×32) on ImageNet to verify training from scratch achieves comparable or better accuracy than PPF methods
  3. Measure multithreading inference latency on CPU platform to verify uniform sparsity achieves balanced workload

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SUBP compare when applied to other types of DNNs like RNNs or transformers, beyond CNNs?
- Basis in paper: The paper acknowledges missing experiments including applying 1×N sparsity on other types of DNNs like RNN, transformer, and other tasks including object detection, natural language processing, etc.
- Why unresolved: The paper primarily focuses on CNNs and does not provide comparative results for other types of neural networks.
- What evidence would resolve it: Conducting experiments applying SUBP to RNNs, transformers, and other types of DNNs, and comparing results with those obtained from CNNs.

### Open Question 2
- Question: What is the optimal value of N for 1×N pruning in different hardware platforms to achieve best trade-off between accuracy and inference latency?
- Basis in paper: The paper mentions optimizing multithreading inference with 1×N pruning and selecting appropriate N based on suitable platforms are directions that can be further explored. It also notes N=16 achieves faster inference speed than other N on M1 Pro platform.
- Why unresolved: The paper does not provide comprehensive study on optimal N values for different hardware platforms.
- What evidence would resolve it: Conducting experiments to determine optimal N values for 1×N pruning on various hardware platforms, and analyzing trade-off between accuracy and inference latency.

### Open Question 3
- Question: How can the training efficiency of SUBP be improved to reduce computational cost during training?
- Basis in paper: The paper mentions SUBP masks weights throughout training and still needs dense computations, indicating room for improving training efficiency.
- Why unresolved: The paper does not propose or explore methods to improve training efficiency of SUBP.
- What evidence would resolve it: Developing and testing techniques to reduce computational cost during training, such as more efficient weight masking strategies or alternative training algorithms.

## Limitations
- Generalizability to other neural network architectures beyond CNNs remains untested
- Training efficiency could be improved as SUBP still requires dense computations during training
- Optimal N value selection for different hardware platforms requires further exploration

## Confidence
The confidence in SUBP's claims is Medium. While the paper presents a novel approach to 1xN sparse CNN training from scratch with multithreading acceleration, several aspects require further validation.

## Next Checks
1. Reproduce MobileNetV1(1×16) ImageNet results: Implement SUBP with BPAR criterion and train from scratch, comparing accuracy against both dense baselines and existing pruning methods like AMC and CP.

2. Cross-architecture generalization test: Apply SUBP to a different architecture family (e.g., EfficientNet or ConvNext) to evaluate whether the angular redundancy pruning criterion generalizes beyond tested ResNet and MobileNet architectures.

3. Multithreading performance validation: Measure actual CPU inference latency on commodity processor with varying thread counts, comparing SUBP's uniform sparsity against baseline pruning methods to verify claimed 2.1× speedup over ARM Cortex-A76.