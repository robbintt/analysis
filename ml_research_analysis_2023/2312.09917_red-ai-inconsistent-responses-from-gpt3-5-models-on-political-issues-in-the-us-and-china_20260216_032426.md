---
ver: rpa2
title: Red AI? Inconsistent Responses from GPT3.5 Models on Political Issues in the
  US and China
arxiv_id: '2312.09917'
source_url: https://arxiv.org/abs/2312.09917
tags:
- questions
- chinese
- political
- answers
- issues
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines political bias in GPT's multilingual models
  by posing identical questions about US and China political issues in both English
  and simplified Chinese. Results show GPT's bilingual models display significant
  inconsistency and sentiment bias, with Chinese GPT models providing more pro-China
  information and less negative sentiment towards China's problems compared to English
  GPT.
---

# Red AI? Inconsistent Responses from GPT3.5 Models on Political Issues in the US and China

## Quick Facts
- arXiv ID: 2312.09917
- Source URL: https://arxiv.org/abs/2312.09917
- Reference count: 18
- Key outcome: Chinese and English GPT-3.5 models provide significantly inconsistent and sentiment-biased responses to identical political questions about the US and China, with Chinese models showing more pro-China bias and English models more anti-China sentiment.

## Executive Summary
This study investigates political bias in GPT's multilingual models by posing identical questions about US and China political issues in both English and simplified Chinese. The researchers found that GPT models display significant inconsistency and sentiment bias, with Chinese models providing more pro-China information and less negative sentiment towards China's problems compared to English models. China-related questions received the least consistent answers, likely influenced by Chinese state censorship and US-China geopolitical tensions. The study also found that both Chinese and English models tend to be less critical towards issues of "their own" country than those of "the other," suggesting GPT models develop a "political identity" based on training language.

## Method Summary
The study compiled 717 bilingual questions (184 natural science, 533 political - 267 US-related, 266 China-related) in English and simplified Chinese. Each question was submitted to GPT-3.5-turbo in both languages, obtaining responses that were translated using Google Translate. Researchers hand-coded content consistency (binary agreement) and sentiment (positive/neutral/negative) for all response pairs, then trained consistency and sentiment classifiers on the translated data. Metrics were compared across topics and languages to identify patterns of bias and inconsistency.

## Key Results
- Chinese GPT models showed more pro-China bias and less negative sentiment towards China's problems compared to English models
- China-related questions received the least consistent answers across language pairs
- Both Chinese and English models were less critical towards issues of "their own" country than those of "the other"
- Fact-based questions tended to be less consistent than opinion-based questions across the board

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT bilingual models develop distinct "political identities" based on training language, leading to asymmetric sentiment biases toward "self" vs. "other" political issues.
- Mechanism: Language-specific training corpora embed culturally distinct political values and censorship norms. When queried in a given language, the model's responses reflect the political attitudes prevalent in that language's training data, creating a systematic bias where the model is more lenient toward issues associated with its training language and more critical toward issues associated with the other language.
- Core assumption: The model's outputs are shaped by the political and cultural context embedded in its training corpus, not by an inherent understanding of geopolitical realities.
- Evidence anchors:
  - [abstract] "This suggests that GPT multilingual models could potentially develop a 'political identity' and an associated sentiment bias based on their training language."
  - [section] "Presumably shaped by the training corpora, GPTs in different languages seem to possess distinct political or even 'national' identities, if they can have one, that bias toward their own 'country' while being more hostile toward others."
  - [corpus] Weak evidence; corpus search found related studies on geopolitical bias but no direct evidence of training data composition influencing political identity formation.
- Break condition: If training corpora are balanced across languages or if the model applies neutral, context-independent reasoning, the asymmetric sentiment bias would not manifest.

### Mechanism 2
- Claim: State censorship in China leads to pro-China bias in Chinese GPT models by suppressing critical content in the training data.
- Mechanism: Chinese state censorship filters out or downweights politically sensitive or critical content about China in the training corpus. This results in a training distribution that is less critical of China, which the model then reflects in its outputs. Consequently, Chinese GPT models are less negative toward China-related issues compared to English GPT models.
- Core assumption: Censorship effectively shapes the distribution of information available in the Chinese training corpus, and the model learns from this biased distribution.
- Evidence anchors:
  - [abstract] "The simplified Chinese GPT models not only tended to provide pro-China information but also presented the least negative sentiment towards China's problems."
  - [section] "Given that the simplified Chinese Internet is subject to strict state control, potentially leading to bias in Chinese GPT models."
  - [corpus] Weak evidence; corpus search found related studies but no direct evidence of censorship's impact on GPT training data.
- Break condition: If Chinese training data includes uncensored sources or if the model applies cross-linguistic reasoning to counteract censorship bias, the pro-China bias would diminish.

### Mechanism 3
- Claim: Anti-China rhetoric in English-language sources leads to heightened negativity toward China in English GPT models.
- Mechanism: English-language training data contains prevalent anti-China narratives and "China threat" rhetoric, which the model learns and reproduces. This results in English GPT models being more critical of China-related issues compared to Chinese GPT models.
- Core assumption: The prevalence of anti-China sentiment in English-language sources is reflected in the training corpus and influences the model's outputs.
- Evidence anchors:
  - [abstract] "The English GPT was significantly more negative towards China."
  - [section] "It may also speak to the impact of the longstanding 'China threat' rhetoric in American and Western political discourse, which makes the English GPT model highly negative towards China-related issues."
  - [corpus] Weak evidence; corpus search found related studies but no direct evidence of anti-China rhetoric's impact on GPT training data.
- Break condition: If English-language training data is balanced or if the model applies neutral reasoning, the heightened negativity toward China would not manifest.

## Foundational Learning

- Concept: Bias in AI models
  - Why needed here: Understanding how biases arise in AI models is crucial for interpreting the results of this study, which found significant political biases in GPT models.
  - Quick check question: What are some common sources of bias in AI models, and how can they impact model outputs?

- Concept: Censorship and its impact on information flow
  - Why needed here: The study suggests that Chinese state censorship may contribute to pro-China bias in Chinese GPT models. Understanding how censorship affects information availability is key to interpreting this finding.
  - Quick check question: How does state censorship influence the information available in a given language or region, and what are the potential implications for AI models trained on that data?

- Concept: Cross-cultural communication and its challenges
  - Why needed here: The study's findings have implications for cross-cultural communication, as different language models may provide inconsistent or biased information about political issues. Understanding these challenges is crucial for interpreting the study's results.
  - Quick check question: What are some common challenges in cross-cultural communication, and how can AI models potentially exacerbate or mitigate these challenges?

## Architecture Onboarding

- Component map: Question generation -> Data collection from GPT models -> Content consistency analysis -> Sentiment analysis -> Robustness checks
- Critical path: The critical path involves generating a comprehensive set of political questions, collecting bilingual responses from GPT models, analyzing content consistency and sentiment, and performing robustness checks to validate the findings.
- Design tradeoffs: The study balances the need for comprehensive data collection with the limitations of manual coding and the potential biases of automated classifiers. The use of both hand-coding and classifiers helps mitigate these tradeoffs.
- Failure signatures: Potential failures include insufficient question coverage, biased classifier training data, or inadequate robustness checks. These failures could lead to incorrect conclusions about GPT models' political biases.
- First 3 experiments:
  1. Generate a diverse set of political questions covering various topics and question types (fact/opinion, close/open-ended).
  2. Collect bilingual responses from GPT models using a controlled experimental setup (e.g., fixed temperature and token limits).
  3. Analyze content consistency and sentiment using both hand-coding and trained classifiers, and perform robustness checks to validate the findings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of Chinese state censorship on the training corpus of GPT models and how does it affect their political responses?
- Basis in paper: Explicit. The paper discusses the role of Chinese state censorship in shaping the Chinese GPT model's responses, making it more lenient towards China-related issues.
- Why unresolved: The paper acknowledges the potential influence of censorship but does not provide a comprehensive analysis of its extent or specific mechanisms.
- What evidence would resolve it: A detailed analysis of the training data used for the Chinese GPT model, including a comparison with uncensored data, would provide insights into the impact of censorship.

### Open Question 2
- Question: How do GPT models trained on different languages and cultural contexts develop distinct "political identities" and associated sentiment biases?
- Basis in paper: Explicit. The paper suggests that GPT models trained on different languages may develop distinct political identities and sentiment biases, as evidenced by the different responses to US and China-related issues.
- Why unresolved: The paper does not explore the underlying mechanisms of how language and cultural context shape these biases or provide a comparative analysis across multiple languages.
- What evidence would resolve it: A comparative study of GPT models trained on multiple languages and cultural contexts, analyzing their responses to a wide range of political issues, would provide insights into the development of these biases.

### Open Question 3
- Question: To what extent do fact-based questions elicit more inconsistent answers compared to opinion-based questions in GPT models?
- Basis in paper: Explicit. The paper finds that fact-based questions tend to be less consistent than opinion-based questions across the board, challenging the assumption that objective facts would elicit more consistent responses.
- Why unresolved: The paper does not provide a detailed analysis of the factors contributing to this inconsistency or explore the implications for knowledge acquisition.
- What evidence would resolve it: A systematic analysis of the types of fact-based questions that elicit inconsistent answers and the underlying reasons for this inconsistency would provide insights into the limitations of GPT models in handling factual information.

## Limitations

- The study cannot directly access or analyze GPT's training data, limiting the ability to definitively prove that training corpus composition causes the observed biases
- Findings may be influenced by translation artifacts or subtle prompt variations, though robustness checks were conducted
- The study's focus on GPT-3.5-turbo limits generalizability to other models or versions

## Confidence

- Core finding (inconsistent responses across languages): Medium-High
- Attribution mechanisms (censorship, anti-China rhetoric, political identity): Medium
- Generalizability to other models/versions: Low

## Next Checks

1. Replicate the analysis using GPT-4 or other multilingual models to assess whether similar patterns emerge across model architectures.
2. Conduct a controlled experiment varying only the prompt language while holding all other parameters constant to isolate language effects.
3. Analyze the actual training corpus composition for Chinese and English subsets to empirically verify the proposed censorship and anti-China rhetoric mechanisms.