---
ver: rpa2
title: 'Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the
  Avalon Game'
arxiv_id: '2312.17515'
source_url: https://arxiv.org/abs/2312.17515
tags:
- player
- agents
- agent
- players
- team
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores multi-agent collaboration using large language
  models (LLMs) in the Avalon game, a complex setting requiring ad hoc teamwork without
  pre-established coordination protocols. The authors introduce the AvalonPlay benchmark,
  where LLM agents must infer teammates' roles and collaborate to achieve shared goals.
---

# Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game

## Quick Facts
- **arXiv ID**: 2312.17515
- **Source URL**: https://arxiv.org/abs/2312.17515
- **Reference count**: 5
- **Primary result**: CodeAct achieves 0.830 team selection accuracy in Avalon game ad hoc teamwork

## Executive Summary
This paper addresses the challenge of ad hoc teamwork using large language models in the complex Avalon game environment. The authors introduce CodeAct, an agent that combines memory retrieval and code-driven reasoning to overcome limitations like hallucinations and information forgetting in LLM agents. Through structured Python code reasoning and self-debugging capabilities, CodeAct demonstrates superior performance compared to semantic reasoning methods in inferring teammate roles and making collaborative decisions.

## Method Summary
The study develops CodeAct, a general agent that enhances LLM capabilities with memory retrieval and code-driven reasoning for ad hoc teamwork in the Avalon game. The method involves a memory system that collects and retrieves information from past interactions, a communication protocol for structured information exchange, and a Python-based reasoning process that converts semantic steps into reliable code execution. The agent is evaluated against Chain of Thought and ReAct methods using metrics including Game Win Rate, Quest Win Rate, and Team Selection Accuracy.

## Key Results
- CodeAct achieves 0.830 team selection accuracy, outperforming semantic reasoning methods
- The code-driven reasoning approach demonstrates better reliability in role inference compared to Chain of Thought and ReAct
- Memory retrieval system effectively prevents forgetting of early game information

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CodeAct leverages structured Python code to transform complex reasoning into reliable inference
- **Mechanism**: By converting semantic reasoning steps into code, the agent can use formal computational tools and debugging capabilities to reduce hallucinations
- **Core assumption**: LLMs can reliably generate correct code when prompted in a zero-shot manner
- **Evidence anchors**: CodeAct's development and the use of Python_REPL interpreter for reasoning
- **Break condition**: If the LLM generates incorrect code, the reasoning process fails and requires self-debugging

### Mechanism 2
- **Claim**: Memory retrieval system enables agents to access factual information from past interactions
- **Mechanism**: By maintaining global memory of key information and leader-specific memory for team selection, agents can avoid forgetting early game information
- **Core assumption**: Past interactions contain reliable information that can be extracted and used for future reasoning
- **Evidence anchors**: Memory retrieval system design and implementation details
- **Break condition**: If the memory retrieval system fails to capture relevant information, the agent cannot use past knowledge effectively

### Mechanism 3
- **Claim**: Communication protocol, while not always beneficial, provides structured interaction framework
- **Mechanism**: The designed communication protocol allows agents to acquire information while accounting for potential misinformation
- **Core assumption**: Structured communication can enhance collaboration even if not always leading to improved efficiency
- **Evidence anchors**: Communication protocol design and experimental observations
- **Break condition**: If communication leads to more hallucinations than useful information, the protocol may hinder rather than help

## Foundational Learning

- **Ad Hoc Teamwork**: Why needed here - The entire framework is built around the challenge of cooperating with previously unseen teammates without pre-established coordination protocols. Quick check: Can you explain the difference between ad hoc teamwork and traditional multi-agent collaboration?
- **Memory Management in LLMs**: Why needed here - The study specifically addresses the issue of LLMs forgetting early information, making memory management crucial. Quick check: What are the key differences between global memory and leader-specific memory in this context?
- **Code-Driven Reasoning**: Why needed here - The CodeAct agent relies on converting reasoning into code for more reliable inference. Quick check: How does code-driven reasoning differ from semantic reasoning approaches like Chain of Thought?

## Architecture Onboarding

- **Component map**: Memory Retrieval System -> Code Generation Module (Python_REPL) -> Self-Debugging Mechanism -> Communication Protocol Handler -> Observation Understanding Module
- **Critical path**: 1. Receive observation and game state 2. Retrieve relevant memory information 3. Generate code for reasoning about teammate roles 4. Execute code and interpret results 5. Make team selection decision 6. Participate in communication and voting
- **Design tradeoffs**: Memory vs. token limits, Code generation reliability vs. flexibility, Communication protocol complexity vs. practical benefits
- **Failure signatures**: Incorrect team selection despite sufficient information, Hallucinations in communication despite memory retrieval, Code generation failures leading to reasoning breakdowns, Memory retrieval missing crucial information for decision-making
- **First 3 experiments**: 1. Test CodeAct with varying memory window sizes (k) to find optimal balance 2. Compare code-driven reasoning with semantic approaches on simple scenarios 3. Evaluate the impact of communication protocol on agent performance in controlled settings

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the CodeAct agent's performance compare to human players in the Avalon game? The paper focuses on comparing different LLM agents and their prompting strategies, without benchmarking against human performance.
- **Open Question 2**: How do the LLM agents handle dynamic changes in team composition or game rules? The study focuses on agents' ability to reason within established game rules, without examining flexibility in response to unexpected changes.
- **Open Question 3**: How does the CodeAct agent's performance scale with an increasing number of players or rounds? The paper describes the Avalon game with seven players and five rounds, but does not explore scenarios with larger groups or extended gameplay.

## Limitations

- Experimental scope is limited to Avalon game with GPT-4 backend
- Communication protocol showed inconsistent benefits in practice
- Memory retrieval effectiveness is bounded by information extraction quality and LLM context limitations

## Confidence

- CodeAct mechanism effectiveness: Medium confidence - promising results in controlled experiments but untested across diverse scenarios
- Memory retrieval system utility: Low confidence - lacks ablation studies to quantify specific contribution
- Communication protocol design: Low confidence - sparse implementation details and mixed performance raise questions about practical value

## Next Checks

1. Conduct cross-LLM evaluation by testing CodeAct with different backend models (e.g., Claude, Llama) to assess architecture independence
2. Perform ablation studies isolating memory retrieval, code generation, and communication components to determine their individual contributions to overall performance
3. Test the framework in alternative ad hoc teamwork environments beyond Avalon to evaluate generalizability and robustness to different game mechanics