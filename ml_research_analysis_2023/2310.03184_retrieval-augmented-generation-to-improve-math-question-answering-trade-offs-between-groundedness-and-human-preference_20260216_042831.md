---
ver: rpa2
title: 'Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs
  Between Groundedness and Human Preference'
arxiv_id: '2310.03184'
source_url: https://arxiv.org/abs/2310.03184
tags:
- responses
- math
- guidance
- groundedness
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a retrieval-augmented generation (RAG) system
  for conceptual math question-answering, addressing the problem of hallucinations
  in large language model (LLM) responses. The authors implement a system that retrieves
  relevant sections from an open-source Prealgebra textbook and uses them to guide
  LLM responses.
---

# Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference

## Quick Facts
- arXiv ID: 2310.03184
- Source URL: https://arxiv.org/abs/2310.03184
- Authors: 
- Reference count: 40
- This paper presents a retrieval-augmented generation (RAG) system for conceptual math question-answering, addressing the problem of hallucinations in large language model (LLM) responses.

## Executive Summary
This paper explores the use of retrieval-augmented generation (RAG) to improve math question-answering systems by grounding responses in vetted educational content. The authors implement a system that retrieves relevant sections from an open-source Prealgebra textbook and uses them to guide LLM responses, addressing the common problem of hallucinations in AI-generated answers. Through three studies, they examine the trade-off between groundedness (how closely responses align with the retrieved document) and response quality, finding that prompt engineering can significantly influence this balance. The research demonstrates that while RAG can improve response quality, there exists a Goldilocks principle where moderate guidance is preferred over both no guidance and excessive guidance by human evaluators.

## Method Summary
The study implements a RAG system using the OpenStax Prealgebra textbook as the retrieval corpus, segmented by subsection. The system uses OpenAI's text-embedding-ada-002 model for document retrieval through cosine similarity calculations. Three prompt guidance conditions are tested: None (no document citation), Low (loose document citation), and High (strict document citation). The authors use 51 real student questions from Math Nation discussion boards and generate responses using OpenAI's gpt-3.5-turbo-0613 API. The system is evaluated using automated metrics (K-F1++, BERTScore, BLEURT) and human preference surveys, where respondents rank responses and assess groundedness.

## Key Results
- Different prompt guidance conditions significantly impact groundedness, with high guidance producing more grounded responses
- Human respondents prefer low guidance responses over no guidance or high guidance responses
- Document relevance positively correlates with groundedness but does not predict human preference
- Automated metrics show only modest correlation with human judgments of groundedness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation improves response quality by grounding answers in vetted educational content
- Mechanism: The system retrieves relevant sections from a Prealgebra textbook and uses them to guide LLM responses, ensuring factual accuracy and curriculum alignment
- Core assumption: Retrieved documents contain accurate, relevant information that can effectively guide LLM responses
- Evidence anchors:
  - [abstract] "retrieval-augmented generation (RAG) system for conceptual math question-answering, addressing the problem of hallucinations in large language model (LLM) responses"
  - [section] "RAG in an educational context aims to bolster the correctness of LLM-based QA by drawing from external knowledge sources such as syllabi, workbooks, and handouts"
- Break condition: Retrieved documents are irrelevant or contain incorrect information, leading to poor guidance for the LLM

### Mechanism 2
- Claim: Prompt engineering can influence the balance between groundedness and response quality
- Mechanism: Different prompt guidance conditions (None, Low, High) manipulate how much the LLM relies on retrieved content versus its own knowledge
- Core assumption: The LLM can effectively use retrieved content when properly prompted
- Evidence anchors:
  - [section] "We adopted a commercially-realistic chatbot context as the underlying LLM, generating all responses with the OpenAI API"
  - [section] "Each guidance condition was selected by iterative, qualitative exploration of prompts given 1-3 sample student questions"
- Break condition: The LLM fails to effectively use retrieved content even with guidance, or the guidance becomes counterproductive

### Mechanism 3
- Claim: Human preferences for responses follow a Goldilocks principle - preferring moderate guidance over no guidance or excessive guidance
- Mechanism: Survey respondents rank responses, showing preference for low guidance over no guidance or high guidance conditions
- Core assumption: Human evaluators can accurately assess response quality and groundedness
- Evidence anchors:
  - [section] "Survey respondents ranked responses by preference and assessed groundedness in the underlying math textbook used as a retrieval corpus"
  - [section] "Responses in the low guidance condition are preferred over responses in the no guidance and high guidance conditions"
- Break condition: Human evaluators consistently disagree on preferences, or preferences change significantly with different user groups

## Foundational Learning

- Concept: Information Retrieval (IR) and Relevance
  - Why needed here: Understanding how retrieved documents are matched to queries is crucial for the RAG system's effectiveness
  - Quick check question: What metric is used to determine the relevance of retrieved documents in this study?
- Concept: Prompt Engineering and Guidance
  - Why needed here: Different levels of prompt guidance significantly impact response quality and groundedness
  - Quick check question: What are the three guidance conditions used in the study?
- Concept: Human Evaluation Methods
  - Why needed here: Human preferences and groundedness assessments are central to evaluating the system's effectiveness
  - Quick check question: What scale was used to assess groundedness in the study?

## Architecture Onboarding

- Component map: Student query → Document retrieval (cosine similarity on embeddings) → Prompt construction with guidance level → LLM response generation
- Critical path: Student query → Document retrieval (cosine similarity on embeddings) → Prompt construction with guidance level → LLM response generation
- Design tradeoffs: Balancing response quality against groundedness, choosing between different levels of prompt guidance, and selecting appropriate evaluation metrics
- Failure signatures: Irrelevant document retrieval, LLM ignoring retrieved content, or human evaluators disagreeing on preferences
- First 3 experiments:
  1. Test document retrieval accuracy by manually checking retrieved documents against sample queries
  2. Compare LLM responses across different guidance conditions for a set of sample queries
  3. Conduct a small-scale human evaluation to validate response preferences and groundedness assessments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between groundedness and student preference in RAG systems for math QA?
- Basis in paper: [explicit] The paper concludes that designers of math QA systems should consider trade-offs between generating responses preferred by students and responses closely matched to specific educational resources.
- Why unresolved: The study shows that low guidance responses are preferred over no guidance and high guidance, but it's unclear what specific balance would be optimal across different educational contexts and student populations.
- What evidence would resolve it: A larger-scale study across multiple educational contexts with diverse student populations, measuring both preference and learning outcomes, could identify optimal guidance levels.

### Open Question 2
- Question: How does the relevance of retrieved documents impact the faithfulness of LLM responses in math QA?
- Basis in paper: [explicit] Study 3 found no significant relationship between document relevance and preference, despite relevance leading to more grounded responses.
- Why unresolved: The study had limited inter-rater reliability and sample size, leaving open questions about the true relationship between relevance and faithfulness.
- What evidence would resolve it: A larger study with improved annotation reliability and more queries could clarify the relationship between relevance and faithfulness.

### Open Question 3
- Question: Can automated metrics reliably measure groundedness and faithfulness in math QA responses?
- Basis in paper: [explicit] The paper tested three automated metrics (K-F1++, BERTScore, BLEURT) against human annotations, finding only modest correlations.
- Why unresolved: The study showed that automated metrics have limitations, particularly as guidance levels change, but did not explore alternative metrics or approaches.
- What evidence would resolve it: Development and testing of new automated metrics specifically designed for educational contexts, validated against human judgments across diverse prompt guidance conditions.

## Limitations
- The evaluation was conducted using only 51 student questions, which may not be representative of the full diversity of middle-school math queries
- The human preference study used a relatively small sample of survey respondents, limiting the generalizability of the preference findings
- The study focused on a single domain (prealgebra) and a specific textbook corpus, which may not generalize to other educational contexts or subjects

## Confidence

**High Confidence:** The finding that different prompt guidance levels significantly impact groundedness (Study 1) is well-supported by the data and methodology. The experimental design is clear and the results are statistically robust.

**Medium Confidence:** The claim that human preferences follow a Goldilocks principle (Study 2) is reasonably supported but could benefit from a larger sample size and more diverse respondent pool. The preference patterns are clear but the sample size limits generalizability.

**Medium Confidence:** The relationship between document relevance and groundedness (Study 3) shows a clear positive correlation, but the lack of preference prediction suggests more complex dynamics at play. The methodology is sound but the interpretation could be refined.

## Next Checks

1. **Replicate with expanded dataset:** Test the RAG system with a larger and more diverse set of student questions (minimum 200 questions) to validate the generalizability of the findings across different math topics and question types.

2. **Conduct larger-scale human evaluation:** Implement a preference study with at least 100 survey respondents across different demographic groups to strengthen the human preference findings and identify any demographic variations in response preferences.

3. **Cross-domain validation:** Apply the same RAG system and evaluation methodology to a different educational domain (such as high school algebra or physics) using a different textbook corpus to assess the generalizability of the prompt engineering findings and preference patterns.