---
ver: rpa2
title: 'Split-and-Denoise: Protect large language model inference with local differential
  privacy'
arxiv_id: '2310.09130'
source_url: https://arxiv.org/abs/2310.09130
tags:
- privacy
- user
- language
- embeddings
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes Split-N-Denoise (SnD), a framework that employs\
  \ split inference and denoising techniques to protect LLM inference with LDP. The\
  \ framework splits the language model to deploy the token representation layer on\
  \ user side, where users add calibrated noise to the embeddings to guarantee d\u03C7\
  -privacy before transmitting them to the server."
---

# Split-and-Denoise: Protect large language model inference with local differential privacy

## Quick Facts
- arXiv ID: 2310.09130
- Source URL: https://arxiv.org/abs/2310.09130
- Reference count: 13
- Key outcome: Split-N-Denoise (SnD) framework improves embedding utility compared to baseline while maintaining local differential privacy guarantees.

## Executive Summary
This paper proposes Split-N-Denoise (SnD), a framework that employs split inference and denoising techniques to protect LLM inference with LDP. The framework splits the language model to deploy the token representation layer on user side, where users add calibrated noise to the embeddings to guarantee dχ-privacy before transmitting them to the server. To improve the utility of embeddings, users conduct local denoising with a pre-trained model leveraging the raw token representations and specific noises. Empirical studies show that SnD performs better in maintaining the utility of embeddings compared with baseline methods.

## Method Summary
The SnD framework splits LLM inference between client and server, placing only the token embedding layer on the user side. Users add calibrated Laplacian noise to embeddings to ensure dχ-privacy before transmission. A pre-trained denoising model on the server uses public data with synthetic noise to learn inverse mappings. After server processing, users apply the denoising model locally to improve embedding quality before downstream tasks.

## Key Results
- SnD maintains better embedding utility compared to baseline method
- Denoise model significantly reduces noise as shown by improvements in cosine similarity (COS) and mean squared error (MSE)
- Framework achieves local differential privacy guarantees while preserving reasonable downstream task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Split inference at the token representation layer allows privacy protection with minimal computational overhead on the user side.
- Mechanism: The language model is divided into a local encoder (token embedding layer) on the client side and a cloud encoder (subsequent layers) on the server side. The client adds calibrated noise to the embeddings before transmission, ensuring local differential privacy.
- Core assumption: Adding noise at the token representation layer sufficiently masks sensitive information while preserving enough utility for downstream tasks.
- Evidence anchors:
  - [abstract] "Our approach is designed for the inference stage of LLMs and requires no modifications to the model parameters."
  - [section 2.2] "To minimize the computational overhead on users, we deploy only the token representation layer on client sides."
  - [corpus] Weak evidence - no direct references to split inference at token representation layer in neighboring papers.

### Mechanism 2
- Claim: Local denoising using a pre-trained model improves the utility of noisy embeddings.
- Mechanism: After receiving the noisy embeddings from the server, the client uses a pre-trained denoising model that leverages the raw token representations and specific noise levels to enhance the embeddings.
- Core assumption: The denoising model can effectively learn the inverse mapping from noisy embeddings back to clean embeddings using the raw inputs and noise information.
- Evidence anchors:
  - [abstract] "To improve the utility of embeddings, users conduct local denoising with a pre-trained model leveraging the raw token representations and specific noises."
  - [section 2.4] "Once receiving the output from server, users input their private data into the denoise model to improve the utility of embeddings."
  - [corpus] No direct evidence - denoising in the context of split inference is not covered by neighboring papers.

### Mechanism 3
- Claim: dχ-privacy provides a stronger privacy guarantee for textual data compared to traditional differential privacy.
- Mechanism: dχ-privacy allows high probability of observing the same output for inputs with similar semantics, making it suitable for text data where semantic similarity matters.
- Core assumption: Semantic similarity in text can be effectively captured by the dχ metric, providing appropriate privacy protection.
- Evidence anchors:
  - [section 2.1.2] "dχ-privacy allows to impose high probability of observing the same output for inputs with similar semantics."
  - [corpus] Weak evidence - while dχ-privacy is mentioned, its application to text data in the context of split inference is not directly supported by neighboring papers.

## Foundational Learning

- Concept: Differential Privacy (DP) and Local Differential Privacy (LDP)
  - Why needed here: DP and LDP are the theoretical foundations for ensuring privacy in the framework. Understanding these concepts is crucial for grasping how the noise addition and privacy guarantees work.
  - Quick check question: What is the difference between DP and LDP, and why is LDP particularly relevant for this framework?

- Concept: Split Learning
  - Why needed here: Split learning is the architectural approach used to divide the model between client and server, enabling privacy protection with minimal computational overhead on the client side.
  - Quick check question: How does split learning differ from traditional federated learning, and what are its advantages in the context of LLM inference?

- Concept: Denoising Autoencoders
  - Why needed here: Denoising autoencoders are the basis for the denoising model used to improve the utility of noisy embeddings. Understanding their principles is essential for implementing and optimizing the denoising process.
  - Quick check question: How do denoising autoencoders learn to reconstruct clean data from noisy inputs, and what are the key considerations when applying them to embeddings?

## Architecture Onboarding

- Component map: Local Encoder -> Privatization Module -> Cloud Encoder -> Denoise Module -> Downstream Tasks

- Critical path:
  1. User inputs text into the local encoder module.
  2. Privatization module adds noise to the embeddings.
  3. Noisy embeddings are transmitted to the server.
  4. Server processes the embeddings and returns the output.
  5. Client receives the output and applies the denoising model.

- Design tradeoffs:
  - Privacy vs. Utility: Increasing noise levels for stronger privacy may degrade embedding quality.
  - Computation on Client vs. Server: More computation on the client side reduces server load but increases client overhead.
  - Complexity of Denoising Model: A more complex model may better denoise but requires more resources.

- Failure signatures:
  - Privacy Leakage: Attack success rates are non-negligible, indicating insufficient noise.
  - Poor Utility: Downstream task performance significantly drops compared to non-private inference.
  - High Computation Overhead: Client-side computation becomes prohibitive.

- First 3 experiments:
  1. Test the privacy-utility tradeoff with different noise levels on a small dataset.
  2. Evaluate the effectiveness of the denoising model by comparing denoised vs. non-denoised embeddings.
  3. Assess the impact of split inference on computation time and resource usage compared to full model inference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SnD framework's performance scale when applied to larger language models beyond 1GB in size?
- Basis in paper: [inferred] The paper mentions testing SnD on larger models like LLaMa and OPT-6.7B, observing improvements in MSE and COS but noting that accuracy on downstream tasks still requires enhancement.
- Why unresolved: The paper suggests that larger models undergo more intricate transformations, requiring more sophisticated noise and denoising mechanisms, but does not provide a clear solution or detailed analysis.
- What evidence would resolve it: Empirical results showing the performance of SnD on a range of larger language models, including detailed analysis of the noise and denoising mechanisms' effectiveness, would clarify scalability issues.

### Open Question 2
- Question: What strategies could be employed to reduce the user computation cost associated with local denoising in the SnD framework?
- Basis in paper: [inferred] The paper identifies local denoising as a major component of user computation overhead and notes that the size of the denoise model scales with the underlying LLM.
- Why unresolved: While the paper acknowledges the computational cost issue, it does not propose specific strategies or solutions to reduce the size or complexity of the denoise model.
- What evidence would resolve it: Development and testing of lightweight denoise mechanisms or optimizations that significantly reduce computation cost without compromising performance would address this issue.

### Open Question 3
- Question: How can the SnD framework be extended to handle sequence-to-sequence (S2S) inference models effectively?
- Basis in paper: [inferred] The paper mentions the interest in extending EaaS to S2S models but highlights challenges such as noise amplification due to the auto-regressive mechanism.
- Why unresolved: The paper does not provide a detailed approach or solution for adapting the SnD framework to S2S models, given the complexity of error correction in generated sequences.
- What evidence would resolve it: A proposed method for adapting SnD to S2S models, along with empirical results demonstrating improved performance and error correction, would resolve this question.

## Limitations
- Privacy guarantees rely on dχ-privacy which lacks empirical validation against real-world attacks
- Denoising model implementation details are underspecified, making full replication difficult
- Scaling to larger models remains challenging with unresolved performance issues

## Confidence
- **High confidence**: The split inference architecture and basic privacy mechanism are well-established concepts
- **Medium confidence**: The denoising approach shows promise based on reported metrics, but implementation details are insufficient for full validation
- **Low confidence**: The dχ-privacy guarantees and their practical effectiveness against real-world attacks

## Next Checks
1. **Empirical privacy validation**: Conduct inference attacks on the SnD framework to measure actual privacy leakage and verify that the dχ-privacy guarantees hold in practice, particularly for sensitive text data.

2. **Ablation study of denoising**: Systematically evaluate the contribution of the denoising model by comparing downstream task performance with and without denoising across different noise levels and model architectures.

3. **Computation overhead analysis**: Measure and compare the actual computation overhead on both client and server sides for different model sizes (BERT, GPT-2, T5) to verify the claimed efficiency of the split approach.