---
ver: rpa2
title: MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs
arxiv_id: '2312.03731'
source_url: https://arxiv.org/abs/2312.03731
tags:
- graph
- pretext
- pre-training
- tasks
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MultiGPrompt, a novel framework for multi-task
  pre-training and prompting on graphs. The key idea is to design pretext tokens to
  synergize multiple pretext tasks during pre-training, reducing task interference.
---

# MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs

## Quick Facts
- arXiv ID: 2312.03731
- Source URL: https://arxiv.org/abs/2312.03731
- Reference count: 40
- Key outcome: MultiGPrompt achieves 57.72% accuracy on one-shot node classification for Cora, outperforming best baseline by 3.47%

## Executive Summary
MultiGPrompt is a novel framework for multi-task pre-training and prompting on graphs that addresses the challenge of task interference in multi-task learning. The framework introduces pretext tokens that synergize multiple pretext tasks during pre-training, and employs a dual-prompt mechanism with composed and open prompts for effective downstream adaptation. Extensive experiments on six public datasets demonstrate significant performance improvements over state-of-the-art methods on both node classification and graph classification tasks, particularly in few-shot settings.

## Method Summary
MultiGPrompt employs a two-stage approach: multi-task pre-training followed by downstream adaptation. During pre-training, three pretext tasks (DGI, GraphCL, and link prediction) are combined using pretext tokens that modify different layers of the graph encoder to reduce task interference. The dual-prompt mechanism consists of composed prompts (learned composition of pretext tokens) and open prompts (directly tuned vectors) that reformulate downstream inputs. During adaptation, only the prompt parameters are updated while the pre-trained model weights remain frozen, enabling efficient few-shot learning.

## Key Results
- MultiGPrompt achieves 57.72% accuracy on one-shot node classification for Cora, outperforming the best baseline by 3.47%
- The framework demonstrates strong performance across six public datasets including Cora, Citeseer, PROTEINS, ENZYMES, BZR, and COX2
- Ablation studies confirm the effectiveness of both the pretext token mechanism for reducing task interference and the dual-prompt design for downstream adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretext tokens synergize multiple pretext tasks by reducing task interference during pre-training
- Mechanism: Pretext tokens are associated with each pretext task and applied to modify input, hidden, and output layers of the graph encoder, acting as task-specific reformulations
- Core assumption: Different pretext tasks capture different aspects of graph knowledge, and task interference occurs when directly summing losses from diverse tasks
- Evidence anchors: [abstract] states pretext tokens "synergize multiple pretext tasks during pre-training, reducing task interference"; [section 4.2] describes token application to GNN layers
- Break condition: If pretext tokens fail to effectively reduce interference, multi-task pre-training could degrade performance compared to single-task approaches

### Mechanism 2
- Claim: Dual-prompt mechanism effectively transfers both task-specific and global pre-trained knowledge to downstream tasks
- Mechanism: Composed prompts aggregate pretext tokens for task-specific knowledge, while open prompts are directly tuned vectors for global inter-task knowledge
- Core assumption: Both task-specific knowledge (from composed prompts) and global inter-task knowledge (from open prompts) are necessary for optimal downstream performance
- Evidence anchors: [abstract] mentions dual-prompt mechanism "leverages task-specific and global pre-training knowledge"; [section 5.3] ablation shows removing either degrades performance
- Break condition: If one type of knowledge becomes significantly more important than the other for certain downstream tasks, dual-prompt approach might be suboptimal

### Mechanism 3
- Claim: Multi-task pre-training with multiple pretext tasks provides more comprehensive pre-trained knowledge than single-task approaches
- Mechanism: Incorporates DGI, GraphCL, and link prediction tasks that capture node connectivity, node/edge features, and local/global patterns
- Core assumption: Different pretext tasks capture different aspects of graph knowledge, and combining them provides more comprehensive coverage
- Evidence anchors: [abstract] states framework "exploit multiple pretext tasks for more comprehensive pre-trained knowledge"; [section 5.3] ablation shows MultiGPrompt outperforms single-task variants
- Break condition: If certain pretext tasks are redundant or conflicting for particular downstream tasks, multi-task approach could be less effective than carefully selected single tasks

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message-passing framework
  - Why needed here: MultiGPrompt builds upon GNNs as the base encoder, with pretext tokens and prompts modifying GNN layers
  - Quick check question: How does a typical GNN layer aggregate messages from neighboring nodes?

- Concept: Prompt-based learning and parameter-efficient adaptation
  - Why needed here: The paper's dual-prompt mechanism is built on the concept of prompts as lightweight task adapters that don't require updating pre-trained parameters
  - Quick check question: What is the key advantage of prompt-based learning compared to full fine-tuning in few-shot settings?

- Concept: Multi-task learning and task interference
  - Why needed here: The paper addresses task interference as a core challenge in multi-task pre-training, and pretext tokens are specifically designed to mitigate this
  - Quick check question: What is task interference in multi-task learning, and why does it occur?

## Architecture Onboarding

- Component map: Graph encoder (GNN with L layers) -> K pretext token sets (each with L+1 tokens) -> Pretext tasks (DGI, GraphCL, Link Prediction) with losses -> Composed prompts (learnable aggregation) -> Open prompts (directly tuned vectors) -> Downstream classifier

- Critical path:
  1. Multi-task pre-training: Generate pretext tokens ‚Üí Modify graph encoder layers ‚Üí Compute pretext task losses ‚Üí Update pretext tokens and encoder
  2. Downstream adaptation: Generate composed and open prompts ‚Üí Modify pre-trained encoder layers ‚Üí Combine embeddings ‚Üí Compute downstream loss ‚Üí Update prompts only

- Design tradeoffs:
  - Number of pretext tasks vs. task interference
  - Complexity of prompt aggregation functions vs. parameter efficiency
  - Layer-wise vs. global pretext token application

- Failure signatures:
  - Poor performance on downstream tasks despite good pre-training loss
  - High variance across different random seeds
  - Degraded performance compared to single-task baselines

- First 3 experiments:
  1. Verify pretext tokens reduce interference: Compare single-task vs. multi-task pre-training with and without pretext tokens
  2. Validate dual-prompt effectiveness: Compare composed-only, open-only, and dual-prompt variants on downstream tasks
  3. Test parameter efficiency: Measure tunable parameters during downstream adaptation vs. full fine-tuning

## Open Questions the Paper Calls Out

- Question: How do different combinations of pretext tasks affect the performance of MultiGPrompt, and are there certain combinations that yield better results than others?
- Question: How does the choice of hyperparameters, such as the values of ùõºùëô's and ùõΩùëò's, impact the performance of MultiGPrompt, and is there an optimal set of hyperparameters for different datasets or tasks?
- Question: How does MultiGPrompt perform on larger and more complex graph datasets, such as social networks or biological networks, and what are the limitations of the current approach in handling such datasets?

## Limitations

- Implementation details of the composed prompt aggregation function are underspecified, which could affect reproducibility
- Ablation studies are limited to three variants without exploring the full design space of pretext task combinations or aggregation functions
- Performance generalizability to larger-scale graphs or different domain types (e.g., temporal or multimodal graphs) remains unproven

## Confidence

- High Confidence: Pretext token mechanism for reducing task interference is well-supported by ablation showing MultiGPrompt outperforms single-task variants
- Medium Confidence: Dual-prompt mechanism's effectiveness is demonstrated through ablation, but design choices could be further validated
- Medium Confidence: Multi-task pre-training provides comprehensive knowledge, but analysis doesn't explore potential redundancy between pretext tasks

## Next Checks

1. Design an experiment that directly measures task interference by comparing gradient cosine similarity between pretext tasks during multi-task pre-training with and without pretext tokens
2. Implement and compare multiple variants of the composed prompt aggregation function (simple linear combination, attention-based aggregation, learned neural networks)
3. Apply MultiGPrompt to graph datasets from different domains (social networks, biological interaction networks, knowledge graphs) to evaluate cross-domain generalization