---
ver: rpa2
title: Unsupervised Prototype Adapter for Vision-Language Models
arxiv_id: '2308.11507'
source_url: https://arxiv.org/abs/2308.11507
tags:
- clip
- prototype
- recognition
- image
- unsupervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of adapting vision-language models
  (VLMs) like CLIP to downstream tasks without requiring annotated data, which is
  labor-intensive and limits scalability. The proposed Unsupervised Prototype Adapter
  (UP-Adapter) leverages CLIP's text-image alignment capability to automatically generate
  pseudo-labels for unlabeled target datasets.
---

# Unsupervised Prototype Adapter for Vision-Language Models

## Quick Facts
- **arXiv ID**: 2308.11507
- **Source URL**: https://arxiv.org/abs/2308.11507
- **Reference count**: 40
- **Key outcome**: UP-Adapter achieves 63.58% accuracy on ImageNet, outperforming 8-shot CoOp (61.45%) and 8-shot Tip-Adapter (61.45%) without requiring annotated data

## Executive Summary
This paper addresses the challenge of adapting vision-language models (VLMs) like CLIP to downstream tasks without requiring annotated data, which is labor-intensive and limits scalability. The proposed Unsupervised Prototype Adapter (UP-Adapter) leverages CLIP's text-image alignment capability to automatically generate pseudo-labels for unlabeled target datasets. It selects the most confident samples per class, generates class prototypes, and initializes a learnable prototype model. The method combines the prototype model's prediction with CLIP's original prediction using a residual connection. Experiments on image recognition and domain generalization tasks show that UP-Adapter outperforms state-of-the-art methods, including 8-shot CoOp, 8-shot Tip-Adapter, and the unsupervised UPL method, by significant margins.

## Method Summary
UP-Adapter works by first using CLIP's text-image alignment to generate pseudo-labels for unlabeled target datasets. It computes cosine similarity scores between image features and class-specific text descriptions, then selects the top-K most confident samples per class. These samples are used to generate class prototypes by averaging their L2-normalized features. A learnable prototype model is initialized with these prototypes and combined with CLIP's original predictions via a residual connection. The prototype model is then fine-tuned using cross-entropy loss. This approach allows the model to adapt to new tasks while preserving CLIP's general knowledge through the residual connection.

## Key Results
- Achieves 63.58% accuracy on ImageNet, outperforming 8-shot CoOp (61.45%) and 8-shot Tip-Adapter (61.45%)
- Demonstrates strong robustness to distribution shifts in domain generalization tasks
- Outperforms state-of-the-art methods including unsupervised UPL on multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The unsupervised prototype adapter leverages CLIP's text-image alignment capability to generate reliable pseudo-labels for unlabeled samples.
- Mechanism: CLIP's pre-trained model computes cosine similarity scores between test image features and class-specific text descriptions. The most confident top-K samples per class are selected based on these scores, ensuring high-confidence pseudo-labels for prototype estimation.
- Core assumption: CLIP's zero-shot performance is sufficiently accurate to produce reliable pseudo-labels without any labeled data.
- Evidence anchors:
  - [abstract] "leverage the text-image aligning capability of CLIP to automatically select the most confident samples for each class"
  - [section] "we can leverage Equation (2) and Equation (3) to derive pseudo-labels for unlabeled samples in the target dataset"
- Break condition: If CLIP's zero-shot accuracy drops significantly on the target domain, the pseudo-label selection becomes unreliable and the prototype initialization degrades.

### Mechanism 2
- Claim: Class prototypes generated from selected samples provide an effective initialization point for the learnable prototype model.
- Mechanism: For each class, L2-normalized features of the top-K confident samples are averaged to create a class prototype. This prototype is used to initialize the linear layer weights in the prototype model, placing it at a high starting point in the optimization landscape.
- Core assumption: Averaging features of high-confidence samples per class produces a representative prototype that captures the class distribution.
- Evidence anchors:
  - [section] "We can obtain the prototype by averaging the L2 normalized features of these K images"
  - [section] "we initialize the network with prototype P, which sets the model at a high starting point"
- Break condition: If the selected samples are not representative of the class distribution (e.g., biased selection), the prototype initialization will be poor and require extensive fine-tuning to recover.

### Mechanism 3
- Claim: Combining prototype model predictions with CLIP's original predictions via residual connection preserves prior knowledge while adapting to new tasks.
- Mechanism: The final logits are computed as a weighted sum of the prototype model output and CLIP's original text-image similarity score. This allows the model to leverage both the adapted knowledge from the prototype and the general knowledge from CLIP.
- Core assumption: CLIP's pre-trained knowledge remains valuable and should be preserved during adaptation rather than being completely overwritten.
- Evidence anchors:
  - [section] "the prototype model prediction is integrated with the original CLIP's prediction to predict the final labels of images"
  - [section] "logits = β exp(-η(1 - vW⊤)) + vf⊤t"
- Break condition: If the residual weight β is set too high or too low, the model may either fail to adapt or lose valuable prior knowledge.

## Foundational Learning

- Concept: Contrastive learning and embedding space alignment
  - Why needed here: Understanding how CLIP aligns text and image features in a shared embedding space is crucial for grasping the pseudo-label generation mechanism
  - Quick check question: How does CLIP's contrastive loss function ensure that related image-text pairs are closer in the embedding space than unrelated pairs?

- Concept: Prototype-based classification
  - Why needed here: The method relies on generating class prototypes from selected samples and using them for classification, which requires understanding prototype-based approaches
  - Quick check question: What is the mathematical operation used to compute the affinity between a test image and class prototypes in this method?

- Concept: Residual connections in neural networks
  - Why needed here: The method combines predictions from the prototype model and CLIP using a residual connection, which requires understanding how residual connections work
  - Quick check question: In what way does the residual connection help preserve information from the original CLIP model during fine-tuning?

## Architecture Onboarding

- Component map: CLIP image encoder → Feature extraction → Prototype adapter → Combined logits → Classification
- Critical path: Image → CLIP image encoder → Feature extraction → Prototype adapter → Combined logits → Classification
  The most critical path is the feature extraction and prototype adapter stages, as they directly impact the quality of the final predictions.
- Design tradeoffs:
  - Number of selected samples K per class: Higher K provides more representative prototypes but increases computational cost and may include less confident samples
  - Residual weight β: Higher β preserves more CLIP knowledge but may limit adaptation; lower β allows more adaptation but risks losing prior knowledge
  - Fine-tuning epochs: More epochs allow better adaptation but increase computational cost and risk overfitting to pseudo-labels
- Failure signatures:
  - Poor pseudo-label quality manifests as unstable training and degraded performance compared to zero-shot CLIP
  - Insufficient K value results in underfitting and poor prototype representation
  - Excessive fine-tuning leads to overfitting to pseudo-labels and loss of generalization
- First 3 experiments:
  1. Run zero-shot CLIP on the target dataset to establish baseline performance and verify the text-image alignment quality
  2. Implement pseudo-label generation with K=16 and visualize the selected samples to verify confidence-based selection works as expected
  3. Test the prototype adapter with fixed prototypes (no fine-tuning) to verify the initialization quality before adding the learnable component

## Open Questions the Paper Calls Out
- How does the UP-Adapter method perform on datasets with highly imbalanced class distributions compared to existing supervised fine-tuning methods?
- Can the UP-Adapter method be extended to multi-modal tasks beyond image recognition, such as visual question answering or image captioning?
- How does the UP-Adapter method handle datasets with complex class hierarchies or fine-grained categories compared to traditional supervised methods?

## Limitations
- Performance heavily depends on CLIP's zero-shot accuracy for pseudo-label generation, which can vary significantly across domains
- The method requires careful tuning of hyperparameters like K and β, with no clear guidance on optimal values for different dataset sizes
- Limited evaluation on imbalanced datasets and complex class hierarchies, which are common challenges in real-world applications

## Confidence
- **High**: Prototype initialization and residual connection mechanisms
- **Medium**: Pseudo-label generation reliability and hyperparameter sensitivity
- **Low**: Performance on imbalanced datasets and complex class hierarchies

## Next Checks
1. Evaluate pseudo-label quality by comparing CLIP's zero-shot accuracy on each target dataset before adaptation
2. Test the method's sensitivity to the K parameter by varying it across a wider range (e.g., K=8, 16, 32) to find optimal values for different dataset sizes
3. Conduct ablation studies to isolate the contribution of the prototype model versus CLIP's original predictions in the final classification accuracy