---
ver: rpa2
title: Parallel Diffusion Model-based Sparse-view Cone-beam Breast CT
arxiv_id: '2303.12861'
source_url: https://arxiv.org/abs/2303.12861
tags:
- reconstruction
- image
- data
- sinogram
- ddpm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a novel parallel diffusion model-based method
  for sparse-view cone-beam breast CT reconstruction. The method uses a cube-based
  3D denoising diffusion probabilistic model (DDPM) to process overlapping cubes of
  the sinogram in parallel, overcoming memory limitations for high-resolution data.
---

# Parallel Diffusion Model-based Sparse-view Cone-beam Breast CT

## Quick Facts
- arXiv ID: 2303.12861
- Source URL: https://arxiv.org/abs/2303.12861
- Reference count: 40
- Primary result: Novel parallel diffusion model approach for sparse-view cone-beam breast CT reconstruction, achieving competitive performance at 1/2 to 1/3 of standard radiation dose

## Executive Summary
This paper presents a novel cube-based 3D denoising diffusion probabilistic model (DDPM) framework for sparse-view cone-beam breast CT reconstruction. The method addresses memory limitations of processing high-resolution CBCT sinograms by dividing them into overlapping cubes processed in parallel across multiple GPUs. The approach trains two DDPM models concurrently on projection and image domains, using pseudo-complete sinograms as conditioning information. Experimental results demonstrate effective artifact suppression and textural detail preservation while significantly reducing radiation dose requirements.

## Method Summary
The method implements a cube-based 3D DDPM framework that processes overlapping cubes of the sinogram in parallel to overcome memory limitations for high-resolution CBCT data. Two DDPM models are trained concurrently on projection and image data in dual domains. The approach generates pseudo-complete sinograms by combining sparse-view data with FDK-reconstructed sinograms, which serve as conditioning information during DDPM denoising. The framework uses 1000 time steps with AdamW optimizer (learning rate 1e-4) and processes cubes of 16³ or 32³ size with 0.5 overlap coefficient.

## Key Results
- Achieves competitive reconstruction performance at 1/2 to 1/3 of standard radiation dose
- Effectively removes artifacts while preserving textural details in breast CT images
- Demonstrates cube-based approach successfully addresses memory limitations for high-resolution sinograms
- Shows preference for smaller cube sizes (16³) for model stability, though this increases computational redundancy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cube-based 3D DDPM processing reduces memory requirements for ultra-high-resolution CBCT sinograms.
- Mechanism: By dividing the 3D sinogram into overlapping cubes and processing them in parallel across multiple GPUs, the method avoids loading the entire sinogram into memory.
- Core assumption: Individual cube sizes are small enough to fit within GPU memory limits while maintaining sufficient context for effective denoising.
- Evidence anchors:
  - [abstract]: "transform the cutting-edge Denoising Diffusion Probabilistic Model (DDPM) into a parallel framework for sub-volume-based sparse-view breast CT image reconstruction"
  - [section]: "This presents a memory issue for high-resolution CBCT sinograms, limiting further research and application. In this paper, we propose a cube-based 3D denoising diffusion probabilistic model (DDPM) for CBCT reconstruction using down-sampled data."
  - [corpus]: Weak - corpus neighbors focus on different architectures, no direct evidence for cube-based memory reduction.
- Break condition: If cube size is too large to fit in GPU memory, or if the overlap creates excessive redundancy that overwhelms available computational resources.

### Mechanism 2
- Claim: Dual-domain training (projection and image domains) improves reconstruction quality and artifact removal.
- Mechanism: The method trains two DDPM models concurrently - one processing sinogram data and another processing image data, allowing synergistic information exchange between domains.
- Core assumption: Information from both domains is complementary and combining them provides better guidance than single-domain approaches.
- Evidence anchors:
  - [abstract]: "This novel approach involves the concurrent training of two distinct DDPM models dedicated to processing projection and image data synergistically in the dual domains."
  - [section]: "Incorporating sinogram data and utilizing dual-domain information can enhance anti-artifact performance"
  - [corpus]: Weak - corpus papers mention dual-domain approaches but don't specifically validate the synergistic training aspect.
- Break condition: If the dual-domain training becomes unstable or if the two models learn conflicting representations that degrade overall performance.

### Mechanism 3
- Claim: Conditional DDPM with pseudo-complete sinogram guidance improves inference accuracy.
- Mechanism: The method generates pseudo-complete sinograms by combining sparse-view data with FDK-reconstructed sinograms, then uses these as conditioning information during DDPM denoising.
- Core assumption: The pseudo-complete sinogram provides meaningful structural information that guides the denoising process even if it contains some artifacts.
- Evidence anchors:
  - [section]: "To provide prior for the missing information and obtain better conditioning, we use the incomplete sinogram to generate the pseudo-complete sinogram."
  - [section]: "The conditional DDPM incorporates the pseudo-complete cube ˜z as a condition, which provides additional information to guide the denoising process"
  - [corpus]: Weak - corpus papers don't discuss pseudo-complete sinogram conditioning in DDPM frameworks.
- Break condition: If the pseudo-complete sinogram introduces misleading information that the DDPM model cannot distinguish from true signal.

## Foundational Learning

- Concept: Denoising Diffusion Probabilistic Models (DDPM)
  - Why needed here: DDPM forms the core reconstruction framework, requiring understanding of forward/reverse processes and noise schedule
  - Quick check question: What is the relationship between the noise schedule β and the signal-to-noise ratio at each diffusion step?

- Concept: Cone-beam geometry and Radon transform
  - Why needed here: The method operates directly on sinogram data, requiring understanding of projection geometry and back-projection
  - Quick check question: How does the FDK algorithm approximate the inverse Radon transform for cone-beam data?

- Concept: Memory management in deep learning
  - Why needed here: The cube-based approach is fundamentally about memory optimization for large 3D datasets
  - Quick check question: What are the memory implications of processing 32³ vs 64³ cubes on a 32GB GPU?

## Architecture Onboarding

- Component map: Data preprocessing -> Cube partitioning -> DDPM inference (sinogram) -> DDPM inference (image) -> Cube assembly -> Final reconstruction

- Critical path:
  1. Generate pseudo-complete sinograms from sparse data
  2. Partition sinograms into overlapping cubes
  3. Parallel DDPM inference on each cube
  4. Assemble denoised cubes into complete sinogram
  5. Perform final reconstruction using assembled sinogram

- Design tradeoffs:
  - Cube size vs. model stability (smaller cubes more stable but increase redundancy)
  - Number of diffusion steps vs. inference speed (more steps = better quality but slower)
  - Overlap percentage vs. memory usage (more overlap = better continuity but higher memory)

- Failure signatures:
  - Visible seams between adjacent cubes in final reconstruction
  - Persistent streak artifacts despite denoising
  - GPU memory errors during cube processing
  - Slow inference due to excessive diffusion steps

- First 3 experiments:
  1. Test cube sizes (16³, 32³, 64³) on a small sinogram subset to identify stability limits
  2. Measure inference time vs. number of diffusion steps (500, 1000, 2000) to find quality-speed tradeoff
  3. Compare reconstruction quality with and without pseudo-complete sinogram conditioning to validate its contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the efficiency of the parallel diffusion model be improved for clinical applications without compromising reconstruction quality?
- Basis in paper: [explicit] The paper discusses the low efficiency of the DDPM processing due to thousands of iterations and the need for 128 GPUs for inference. It suggests focusing on accelerating the method based on previous work.
- Why unresolved: The paper acknowledges the need for acceleration but does not provide specific solutions or experimental results for improving efficiency.
- What evidence would resolve it: Experimental results showing improved processing speed while maintaining or enhancing reconstruction quality, possibly through algorithmic optimizations or hardware acceleration techniques.

### Open Question 2
- Question: What are the optimal cube sizes for different types of medical imaging data to balance model stability and reconstruction accuracy?
- Basis in paper: [explicit] The paper notes that 3D DDPM is sensitive to input data size and that smaller data sizes (e.g., 16^3 vs 32^3) provide more stable training, but does not explore optimal cube sizes for various applications.
- Why unresolved: The paper only tests two cube sizes and does not provide a comprehensive analysis of how cube size affects performance across different medical imaging scenarios.
- What evidence would resolve it: Systematic experiments comparing reconstruction quality and model stability across a range of cube sizes for various types of medical imaging data.

### Open Question 3
- Question: How can dual-domain diffusion models be effectively integrated to further enhance breast CT image reconstruction?
- Basis in paper: [explicit] The paper suggests that introducing a dual-domain DDPM approach could further enhance breast CT images, as current results still have some missing details due to errors in the sinogram spreading to the image domain.
- Why unresolved: The paper proposes this direction for future work but does not implement or test a dual-domain approach.
- What evidence would resolve it: Experimental results comparing reconstruction quality of a dual-domain DDPM approach against the current single-domain approach, demonstrating improvements in detail preservation and artifact reduction.

## Limitations

- Low processing efficiency due to thousands of diffusion iterations required for inference
- Preference for smaller cube sizes increases computational redundancy and memory overhead
- Limited validation primarily on breast CT data, generalization to other CT applications remains untested
- Incomplete specification of architectural details and hyperparameters affecting reproducibility

## Confidence

- Memory optimization claims: Medium confidence - cube-based approach logically addresses memory constraints, but specific efficiency metrics are limited
- Dual-domain training benefits: Medium confidence - synergistic training concept is supported but lacks direct experimental validation
- Practical efficiency claims: Low confidence - acknowledges significant computational burden without providing solutions

## Next Checks

1. Measure actual GPU memory usage and processing time for different cube sizes (16³, 32³, 64³) to quantify the memory-efficiency tradeoff
2. Compare reconstruction quality metrics (PSNR, SSIM) across different numbers of diffusion steps to identify the optimal quality-speed balance
3. Test the method's performance on non-breast CT datasets to evaluate generalization beyond the training domain