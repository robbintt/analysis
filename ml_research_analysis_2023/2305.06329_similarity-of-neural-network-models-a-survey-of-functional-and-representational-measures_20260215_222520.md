---
ver: rpa2
title: 'Similarity of Neural Network Models: A Survey of Functional and Representational
  Measures'
arxiv_id: '2305.06329'
source_url: https://arxiv.org/abs/2305.06329
tags:
- similarity
- measures
- representations
- neural
- representational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive overview of two main perspectives
  on measuring neural network similarity: representational similarity, which compares
  intermediate layer activations, and functional similarity, which compares model
  outputs. We introduce and formally define 46 similarity measures, categorizing them
  into methodological groups such as CCA-based, alignment-based, and neighborhood-based
  measures.'
---

# Similarity of Neural Network Models: A Survey of Functional and Representational Measures

## Quick Facts
- arXiv ID: 2305.06329
- Source URL: https://arxiv.org/abs/2305.06329
- Reference count: 40
- Key outcome: Comprehensive survey of 46 similarity measures for neural networks, categorized into representational and functional perspectives

## Executive Summary
This survey provides a systematic overview of methods for measuring neural network similarity, examining both functional measures (comparing model outputs) and representational measures (comparing intermediate layer activations). The authors introduce a formal framework for understanding these measures, categorizing them into methodological groups such as CCA-based, alignment-based, and neighborhood-based approaches. The work identifies key properties like transformation invariance and robustness, while highlighting open research problems and offering practical guidance for measure selection in different contexts.

## Method Summary
The paper conducts a comprehensive literature review to identify and formalize 46 similarity measures, organizing them around two main perspectives: representational similarity (comparing activations) and functional similarity (comparing outputs). Each measure is characterized by its transformation invariances, preprocessing requirements, and mathematical properties. The authors perform a meta-analysis examining relationships between measures, their robustness to various factors, and correlations with functional behavior, synthesizing findings from the broader literature.

## Key Results
- Identified 46 similarity measures across representational and functional categories
- Formalized transformation invariance properties for different measure classes
- Found that representational measures are sensitive to input similarity while functional measures are more robust
- Demonstrated that combining both perspectives provides more comprehensive insights than either alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representational similarity measures are fundamentally about finding invariant mappings between neural network activations under transformations.
- Mechanism: The paper formalizes equivalence classes of transformations (permutations, orthogonal transformations, scaling, etc.) and defines invariance of measures to these classes. This allows meaningful comparison of representations even when the geometry differs.
- Core assumption: Neural network representations can be meaningfully compared when mapped through transformation-invariant metrics.
- Evidence anchors:
  - [abstract]: "Representational similarity measures assess how activations of intermediate neural layers differ"
  - [section]: "Two broad and complementary perspectives: representational and functional similarity measures"
  - [corpus]: Weak - corpus papers focus on benchmarking but don't validate transformation invariance claims
- Break condition: If transformations fundamentally alter the semantic meaning of representations (e.g., interpretable embeddings where dimensions have fixed meanings)

### Mechanism 2
- Claim: Functional similarity measures work because classification outputs have universal semantics across models.
- Mechanism: Hard predictions and soft probabilities are directly comparable across different models and architectures since they represent the same underlying task semantics.
- Core assumption: The output space of classification tasks has consistent interpretation regardless of model architecture.
- Evidence anchors:
  - [abstract]: "Functional similarity measures compare the output behavior of neural networks with respect to the given (classiﬁcation) task"
  - [section]: "outputs have clear and universal semantics, so that they can be compared in a straightforward manner"
  - [corpus]: Strong - multiple corpus papers cite disagreement measures and their effectiveness for model comparison
- Break condition: When models optimize for different objectives or when outputs are generated rather than classified

### Mechanism 3
- Claim: Combining representational and functional measures provides complementary insights that neither can provide alone.
- Mechanism: Functional measures capture task-specific behavior while representational measures reveal architectural and learning dynamics differences.
- Core assumption: The two perspectives capture orthogonal aspects of model similarity.
- Evidence anchors:
  - [abstract]: "Both perspectives on their own are not sufﬁcient to gain detailed insights into similarity of neural network models"
  - [section]: "combining these two perspectives provides a more comprehensive approach to analyze similarity between neural networks at all layers"
  - [corpus]: Moderate - corpus papers discuss benchmarking but limited discussion of complementary nature
- Break condition: If one perspective is dominated by noise or if measures from both perspectives are highly correlated

## Foundational Learning

- Concept: Matrix transformations and equivalence classes
  - Why needed here: The paper's core contribution is formalizing how different similarity measures handle various matrix transformations. Understanding permutation groups, orthogonal transformations, and scaling is essential to grasp why different measures exist.
  - Quick check question: What transformation class includes rotations, reflections, and permutations but excludes scaling?

- Concept: Distance metrics and their properties
  - Why needed here: Many similarity measures are actually distance metrics. Understanding triangle inequality, symmetry, and the zero property is crucial for interpreting results and comparing measures.
  - Quick check question: If a measure satisfies all metric properties, what value indicates perfect similarity?

- Concept: Canonical correlation analysis
  - Why needed here: CCA-based measures are a major category in the paper. Understanding how canonical correlations find maximally correlated projections is key to understanding SVCCA and PWCCA.
  - Quick check question: What do the canonical correlation vectors represent in the context of neural representations?

## Architecture Onboarding

- Component map: The paper is organized around two main axes: (1) representational vs functional similarity, and (2) methodological approaches within each category. Each measure is characterized by its transformation invariances, preprocessing requirements, and metric properties.

- Critical path: Start with understanding the basic definitions (representations R, outputs O), then move to transformation invariance concepts, then explore each measure category, and finally understand the meta-analysis results.

- Design tradeoffs:
  - Preprocessing vs invariance: More preprocessing can increase invariance but may alter representation structure
  - Computational cost vs accuracy: Some measures (like SVD-based SVCCA) are computationally expensive but more robust
  - Black-box vs white-box access: Some measures require full model access while others only need outputs

- Failure signatures:
  - High representational similarity but low functional similarity suggests the measure isn't capturing task-relevant information
  - Low representational similarity but high functional similarity suggests equivalence under non-linear transformations
  - Inconsistent results across different input subsets suggest confounding by input similarity

- First 3 experiments:
  1. Compute disagreement between two randomly initialized models on the same dataset to establish baseline functional similarity
  2. Apply orthogonal Procrustes to representations from models trained with different seeds to test invariance properties
  3. Compare CKA scores before and after adding noise to a single instance representation to test sensitivity to perturbations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of input similarity on representational similarity scores across different measures?
- Basis in paper: [explicit] Section 5.3 discusses confounding effects of input similarity on representational similarity measures like CKA and RSA.
- Why unresolved: While the paper mentions that increasing input similarity leads to higher representational similarity scores, it doesn't quantify this relationship or identify thresholds beyond which input similarity no longer meaningfully affects the scores.
- What evidence would resolve it: Experiments systematically varying input similarity (e.g., using different levels of class overlap, feature correlation, or semantic similarity) and measuring how representational similarity scores change across multiple measures.

### Open Question 2
- Question: How robust are representational similarity measures to perturbations in individual instance representations?
- Basis in paper: [explicit] Section 5.2 mentions Davari et al.'s finding that CKA is sensitive to translating a single instance representation, while also being robust to removing principal components.
- Why unresolved: The paper only provides one example of CKA's sensitivity to single-instance perturbations. It's unclear how other measures behave under similar conditions or what magnitude of perturbation causes meaningful score changes.
- What evidence would resolve it: Controlled experiments applying various types and magnitudes of perturbations (translations, rotations, noise addition) to individual instance representations and measuring how similarity scores change across different measures.

### Open Question 3
- Question: How does dimensionality affect the range and interpretability of representational similarity scores?
- Basis in paper: [explicit] Section 5.3 mentions Williams et al.'s finding that invariance type affects the N/D ratio needed for consistent similarity scores, and Figure 3 shows Orthogonal Procrustes scores increasing with dimensionality.
- Why unresolved: While the paper shows one example of how Orthogonal Procrustes scores change with dimensionality, it doesn't establish baseline ranges for different measures or provide guidance on interpreting scores at different dimensionalities.
- What evidence would resolve it: Systematic experiments varying dimensionality across multiple measures and establishing baseline score ranges, potentially using techniques like random matrix comparisons or establishing theoretical bounds.

## Limitations
- Meta-analysis relies heavily on theoretical derivations rather than empirical validation across diverse model architectures and tasks
- Transformation invariance claims may not hold uniformly in practice, particularly for measures sensitive to batch normalization effects
- Relationships between representational and functional similarity measures are primarily correlational rather than causal

## Confidence
- Transformation invariance properties: Medium confidence (theoretically sound but not empirically validated)
- Measure relationships and correlations: Medium confidence (based on literature review but limited empirical testing)
- Claims about complementary nature of perspectives: Low confidence (requires more extensive empirical validation)

## Next Checks
1. Conduct systematic ablation studies removing preprocessing steps to quantify their impact on measure invariance properties
2. Test measure correlations across diverse model families (CNNs, transformers, MLPs) to validate generalization claims
3. Design controlled experiments where ground truth similarity is known to evaluate measure accuracy against baselines