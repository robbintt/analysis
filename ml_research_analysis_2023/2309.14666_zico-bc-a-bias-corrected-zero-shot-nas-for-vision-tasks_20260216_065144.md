---
ver: rpa2
title: 'ZiCo-BC: A Bias Corrected Zero-Shot NAS for Vision Tasks'
arxiv_id: '2309.14666'
source_url: https://arxiv.org/abs/2309.14666
tags:
- search
- zico
- bias
- zero-shot
- zico-bc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates zero-shot NAS (Z-NAS) on complex vision
  tasks (segmentation and detection) and identifies a bias in the state-of-the-art
  metric ZiCo, which favors thinner and deeper networks. A bias-corrected version
  (ZiCo-BC) is proposed, adding a penalty term for depth and width imbalance.
---

# ZiCo-BC: A Bias Corrected Zero-Shot NAS for Vision Tasks

## Quick Facts
- arXiv ID: 2309.14666
- Source URL: https://arxiv.org/abs/2309.14666
- Reference count: 21
- Key outcome: ZiCo-BC corrects depth-width bias in zero-shot NAS, improving ranking accuracy and enabling up to 30% latency reduction in vision tasks.

## Executive Summary
This paper investigates bias in the state-of-the-art zero-shot NAS metric ZiCo, which favors thinner and deeper networks, leading to sub-optimal architectures. The authors propose ZiCo-BC, a bias-corrected version that adds a penalty term for depth-width imbalance during evolutionary search. This correction improves ranking accuracy on NATS-Bench-SSS and enables more effective micro-architecture search across ImageNet, COCO, and Cityscapes tasks, finding models with up to 30% latency reduction while maintaining or improving accuracy compared to original ZiCo-based searches.

## Method Summary
The method proposes ZiCo-BC by adding a depth-width penalty term to the original ZiCo metric, which subtracts a penalty proportional to the product of spatial resolution and channel count per layer. The evolutionary search uses NSGA-2 with hardware latency in the optimization loop. The bias correction is applied during the search process to discourage excessively deep, thin architectures. The approach is validated across multiple vision tasks including image classification, object detection, and semantic segmentation using various search spaces and datasets.

## Key Results
- ZiCo-BC improves Kendall's τ and Spearman's ρ correlation scores on NATS-Bench-SSS across CIFAR-10, CIFAR-100, and ImageNet-16-120
- Searched architectures achieve up to 30% latency reduction on Samsung Galaxy S10 while maintaining or improving accuracy
- For semantic segmentation on Cityscapes, the searched architecture achieves a 29% latency reduction compared to EfficientDet-D0 while maintaining better accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ZiCo-BC reduces bias toward thinner and deeper networks by penalizing depth-width imbalance during evolutionary search.
- Mechanism: The bias correction term subtracts a penalty proportional to the product of spatial resolution and channel count per layer. Deeper networks with fewer channels accumulate a larger penalty, shifting the search toward models with better width-depth balance.
- Core assumption: The original ZiCo score grows linearly with depth but logarithmically with gradient statistics, so deeper models can achieve higher scores even if they are suboptimal in accuracy.
- Evidence anchors:
  - [abstract] "We empirically study the bias of state-of-the-art (SOTA) zero-shot proxy ZiCo... observe that ZiCo is biased towards thinner and deeper networks, leading to sub-optimal architectures."
  - [section] "Due to the bias, ZiCo can favor deeper and thinner models over potentially more optimal ones during the evolutionary search."
- Break condition: If the penalty coefficient β is set too high, the search may favor overly wide but shallow models, hurting accuracy.

### Mechanism 2
- Claim: ZiCo-BC improves correlation between zero-shot proxy and test accuracy on NATS-Bench-SSS.
- Mechanism: By correcting depth-width bias, the revised metric better reflects true model performance, leading to higher Kendall's τ and Spearman's ρ values.
- Core assumption: A training-free proxy that ranks models more accurately will lead to better architectures after full training.
- Evidence anchors:
  - [section] "The bias correction improves the correlation score of ZiCo across all three datasets, indicating that the ZiCo-BC score can be a more representative proxy of test accuracy for ranking candidates during a micro-architecture search."
- Break condition: If the search space changes dramatically (e.g., macro search), the same penalty may no longer correct bias effectively.

### Mechanism 3
- Claim: Bias correction enables lower-latency, higher-accuracy models in downstream tasks.
- Mechanism: By discouraging excessive depth, the search explores architectures with fewer parameters and MACs while maintaining or improving accuracy, yielding models that run faster on mobile hardware.
- Core assumption: Latency reductions are possible without accuracy loss if the original metric's bias led to unnecessarily deep models.
- Evidence anchors:
  - [abstract] "The corrected metric finds models with up to 30% latency reduction and higher accuracy than the original ZiCo-based searches."
  - [section] "Our searched architecture achieves a remarkable 29% latency reduction while maintaining even better accuracy compared to EfficientDet-D0."
- Break condition: If the hardware platform changes significantly, the optimal width-depth trade-off may shift, requiring a different β value.

## Foundational Learning

- Concept: Zero-shot NAS and training-free proxies
  - Why needed here: The paper builds on ZiCo, a gradient-based zero-shot metric that avoids full training to rank architectures.
  - Quick check question: What is the key advantage of zero-shot NAS compared to traditional training-based NAS?

- Concept: Correlation metrics for NAS benchmarks (Kendall's τ, Spearman's ρ)
  - Why needed here: These metrics quantify how well the zero-shot proxy ranks architectures compared to their true accuracy.
  - Quick check question: How do Kendall's τ and Spearman's ρ differ in measuring ranking correlation?

- Concept: Depth-width trade-off in CNNs
  - Why needed here: Understanding how depth and width affect model accuracy and efficiency is essential to grasp why bias correction matters.
  - Quick check question: Why might a deeper but thinner network have lower accuracy than a shallower but wider one?

## Architecture Onboarding

- Component map: Zero-shot proxy computation (ZiCo/BC) -> Evolutionary search (NSGA-2) -> Latency estimation on target hardware -> Training pipeline for final architectures
- Critical path: Proxy computation → Evolutionary search → Latency-aware selection → Final training
- Design tradeoffs:
  - β tuning: Too low → bias remains; too high → search favors overly wide models
  - Fixed input size assumption: May miss accuracy gains from larger images
  - Micro vs macro search: Bias correction applies only to repeated-block architectures
- Failure signatures:
  - Evolutionary search converges to maximum depth models
  - Latency improvement without accuracy gain
  - Correlation with true accuracy drops on new datasets
- First 3 experiments:
  1. Run ZiCo-BC on NATS-Bench-SSS and verify improved Kendall's τ/Spearman's ρ.
  2. Perform micro-architecture search on ImageNet with β=1; compare latency/accuracy vs original ZiCo.
  3. Apply bias correction to a macro-architecture search space; observe if correlation improves or breaks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can bias correction methods be generalized to macro-architecture search where backbones have different depths?
- Basis in paper: [explicit] The paper explicitly states that the current bias correction assumes repeated blocks and may unfairly treat shallower backbones in macro-architecture search.
- Why unresolved: Different topologies in macro search create varying layer counts, making a common penalty problematic.
- What evidence would resolve it: Empirical studies comparing bias-corrected and uncorrected macro searches across diverse backbone families, showing whether correction improves or harms search outcomes.

### Open Question 2
- Question: How can bias correction methods account for varying input image sizes while maintaining consistent scoring?
- Basis in paper: [explicit] The paper notes that the current bias correction ignores potential accuracy gains from larger input sizes, which may vary across tasks.
- Why unresolved: No existing method adjusts for input size effects on both latency and accuracy in zero-shot proxies.
- What evidence would resolve it: A modified bias correction formula that normalizes scores based on input size, validated across tasks like detection (larger inputs) and segmentation.

### Open Question 3
- Question: What are the theoretical bounds on the improvement achievable by bias correction in zero-shot NAS metrics?
- Basis in paper: [inferred] The paper shows empirical improvements but does not analyze the theoretical limits or optimality of the correction.
- Why unresolved: No analysis of how much bias exists in ZiCo or how close the corrected version gets to the optimal ranking.
- What evidence would resolve it: A theoretical framework linking gradient-based metrics to ground-truth accuracy, quantifying the maximum possible ranking improvement via correction.

## Limitations
- The bias correction assumes repeated blocks and may not generalize to macro-architecture search with diverse backbone topologies
- The correction relies on fixed input sizes, potentially missing accuracy gains from larger input resolutions
- The optimal β coefficient may require task-specific tuning rather than a universal value

## Confidence

- **High Confidence**: The empirical observation that ZiCo favors deeper/thinner networks and the effectiveness of the bias correction term in improving correlation metrics on NATS-Bench-SSS.
- **Medium Confidence**: The generalizability of β=1 and β=2 coefficients across different tasks and search spaces, as these values were determined empirically and may require tuning for new scenarios.
- **Medium Confidence**: The claim of "up to 30% latency reduction" since this represents the best-case scenario from searched architectures rather than an average improvement across all searches.

## Next Checks

1. **Cross-task coefficient validation**: Test whether β=1 (image classification) and β=2 (segmentation) remain optimal when applied to each other's tasks, or if task-specific tuning is required.
2. **Macro-architecture generalization**: Apply ZiCo-BC to a macro search space with non-repeated blocks and evaluate whether correlation improvements persist or if new bias patterns emerge.
3. **Input resolution sensitivity**: Evaluate a small set of architectures at multiple input resolutions (e.g., 224, 288, 384) to quantify potential accuracy gains missed by the fixed-size assumption in the current bias correction.