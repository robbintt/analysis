---
ver: rpa2
title: Inferring Latent Class Statistics from Text for Robust Visual Few-Shot Learning
arxiv_id: '2311.14544'
source_url: https://arxiv.org/abs/2311.14544
tags:
- text
- class
- covariance
- mean
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve few-shot learning by inferring
  class statistics from text. The approach leverages CLIP's text encoder to map class
  labels or descriptions to visual feature distributions, predicting both the mean
  and covariance of each class.
---

# Inferring Latent Class Statistics from Text for Robust Visual Few-Shot Learning

## Quick Facts
- arXiv ID: 2311.14544
- Source URL: https://arxiv.org/abs/2311.14544
- Reference count: 33
- Primary result: Proposed method improves few-shot learning by predicting class statistics (mean and covariance) from text using CLIP's text encoder

## Executive Summary
This paper introduces a novel approach to few-shot learning that leverages text-derived statistics to improve classification performance. The method uses CLIP's text encoder to extract semantic features from class labels or descriptions, then trains mapping networks to predict the mean and covariance of visual feature distributions for each class. These predictions are incorporated into a Mahalanobis distance-based classifier, enabling more robust few-shot learning, particularly in one-class and fine-grained classification tasks. Experiments on multiple datasets demonstrate consistent improvements, especially when combining mean and covariance predictions.

## Method Summary
The proposed method works by first extracting semantic features from class labels using CLIP's text encoder. Two mapping networks are then trained to predict the mean (gμ) and covariance (gΣ) of visual feature distributions from these semantic features, using L2 loss against empirical statistics computed from a base dataset. During few-shot learning, the predicted statistics are adapted through interpolation with empirical estimates from the few available samples and shrinkage techniques. Classification is performed using a Mahalanobis distance classifier that incorporates both the predicted mean and covariance, allowing the model to account for the distribution of features within each class.

## Key Results
- Consistent 2% improvement in few-shot classification when including covariance predictions
- Over 8% AUROC improvement in one-class classification when combining mean and covariance predictions
- Effective performance on both base datasets (ImageNet, iNaturalist) and cross-domain test datasets (Caltech, EuroSAT, Food, Flowers, SUN397, DTD, Pets, Cars, UCF101)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-derived statistics (mean and covariance) improve few-shot classification by enriching the latent space with distributional information
- Mechanism: CLIP's text encoder captures semantic information about classes, which is used to predict the mean and covariance of visual feature distributions
- Core assumption: Semantic information captured by text encoder is sufficient to infer meaningful statistics about visual feature distribution
- Evidence anchors: [abstract] "Our primary focus is to examine whether statistics such as the covariance of the distribution of visual features for a class can be inferred from text"; [section 3.1] "Given some semantic features si of a class ci, our target is to accurately estimate the mean and covariance of the visual features..."
- Break condition: If text encoder fails to capture relevant semantic information or mapping networks cannot learn relationship between text and visual statistics

### Mechanism 2
- Claim: Using both mean and covariance predictions from text provides more robust and generalizable few-shot learning models compared to using only the mean
- Mechanism: Covariance matrix captures spread and orientation of visual feature distribution, enabling Mahalanobis distance classifier to better distinguish between classes
- Core assumption: Covariance matrix contains useful information for distinguishing between classes in visual feature space
- Evidence anchors: [abstract] "Our method shows that we can use text to predict the mean and covariance of the distribution offering promising improvements in few-shot learning scenarios"; [section 4.3.1] "While including the covariance brings a consistent improvement of ≃ 2% for all data regimes"
- Break condition: If covariance predictions from text are inaccurate or Mahalanobis distance classifier is not well-suited for given dataset

### Mechanism 3
- Claim: The proposed method is particularly effective in one-class and fine-grained classification tasks
- Mechanism: In one-class classification, covariance matrix defines class boundaries; in fine-grained classification, covariance captures subtle differences in visual feature distributions
- Core assumption: Text encoder can capture subtle semantic differences between fine-grained classes reflected in visual feature distributions
- Evidence anchors: [abstract] "Our method is especially effective in one-class and fine-grained classification tasks, demonstrating the potential of using text-derived statistics to improve cross-domain robustness in few-shot learning"; [section 4.3.1] "Particularly in a very low shot regime, it's interesting to combine both mean and covariance predictions from text which brings over 8% AUROC improvement"
- Break condition: If text encoder fails to capture semantic differences between fine-grained classes or visual feature distributions are not well-represented by predicted mean and covariance

## Foundational Learning

- Concept: CLIP's text encoder
  - Why needed here: Extracts semantic features from class labels or descriptions used to predict visual feature distributions
  - Quick check question: What is the output dimension of CLIP's text encoder?

- Concept: Mahalanobis distance
  - Why needed here: Distance metric that accounts for covariance of visual feature distribution for each class
  - Quick check question: How does the Mahalanobis distance differ from the Euclidean distance?

- Concept: Few-shot learning
  - Why needed here: The proposed method aims to improve performance when very few examples are available for each class
  - Quick check question: What is the main challenge in few-shot learning, and how does the proposed method address it?

## Architecture Onboarding

- Component map: Text → CLIP's text encoder → Mapping networks (gμ, gΣ) → Mahalanobis distance-based classifier → Classification
- Critical path: Text features extracted from CLIP's text encoder are passed through mapping networks to predict class statistics, which are then used by Mahalanobis classifier
- Design tradeoffs:
  - Using diagonal covariance matrix reduces computational complexity but may not capture full structure of visual feature distribution
  - Interpolating mean prediction with empirical mean from few shots helps account for domain shifts but may introduce noise
- Failure signatures:
  - Poor cross-domain performance may indicate text encoder not capturing relevant semantic information or mapping networks not learning text-visual relationship
  - Overfitting on base dataset may occur if mapping networks are too complex or regularization is insufficient
- First 3 experiments:
  1. Train mapping networks (gμ and gΣ) on base dataset and evaluate on held-out validation set
  2. Implement Mahalanobis distance-based classifier and test on few-shot learning benchmark dataset
  3. Experiment with different interpolation coefficients (α and β) to find optimal values for given dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method change when using more sophisticated covariance estimation techniques beyond the diagonal assumption, such as low-rank approximations or structured covariances?
- Basis in paper: [inferred] The paper uses diagonal covariance matrices due to high dimensionality and limited samples, but does not explore alternative covariance estimation methods
- Why unresolved: The paper only considers diagonal covariance matrices and does not investigate impact of using more complex covariance structures
- What evidence would resolve it: Experiments comparing proposed method with different covariance estimation techniques (e.g., low-rank approximations, structured covariances) on various datasets and few-shot learning tasks

### Open Question 2
- Question: Can the proposed method be extended to handle higher-order moments beyond mean and covariance, such as skewness or kurtosis, and how would this impact performance?
- Basis in paper: [inferred] The paper focuses on mean and covariance predictions from text but does not explore potential benefits of incorporating higher-order moments
- Why unresolved: The paper only considers mean and covariance predictions, leaving open question of whether higher-order moments could provide additional useful information for few-shot learning
- What evidence would resolve it: Experiments incorporating higher-order moments into proposed framework and evaluating impact on few-shot learning performance across different datasets and tasks

### Open Question 3
- Question: How does the proposed method perform when applied to tasks beyond classification, such as object detection or segmentation, in few-shot learning scenarios?
- Basis in paper: [explicit] The paper concludes by suggesting that investigating applicability of approach in tasks other than classification could offer further insights
- Why unresolved: The paper only evaluates proposed method on classification tasks
- What evidence would resolve it: Experiments applying proposed method to few-shot object detection or segmentation tasks and comparing performance to existing approaches

### Open Question 4
- Question: How sensitive is the proposed method to the choice of text encoder and visual backbone, and can it benefit from using more recent or task-specific models?
- Basis in paper: [inferred] The paper uses CLIP's text encoder and visual backbone but does not explore impact of using alternative models
- Why unresolved: The paper only considers CLIP's text encoder and visual backbone
- What evidence would resolve it: Experiments comparing proposed method with different combinations of text encoders and visual backbones, including more recent or task-specific models

## Limitations
- Effectiveness heavily depends on quality of semantic features extracted by CLIP's text encoder, which may not hold for abstract concepts or weak visual-textual correlations
- Mapping networks' performance may degrade when transferring between domains with significantly different visual-textual relationships
- Assumes diagonal covariance matrix due to high dimensionality and limited samples, potentially missing important structural information

## Confidence
- **High Confidence**: Core mathematical framework (Mahalanobis distance with predicted statistics) is sound and well-established
- **Medium Confidence**: Empirical improvements shown across multiple datasets, particularly consistent 2% improvement with covariance predictions
- **Medium Confidence**: Effectiveness in one-class and fine-grained classification tasks, though demonstrated on limited experiments

## Next Checks
1. **Cross-Domain Robustness Test**: Evaluate method's performance when transferring from ImageNet to domains with significantly different visual-textual relationships (e.g., medical imaging or satellite imagery) to test limits of text-derived statistics

2. **Ablation on Text Encoder Choice**: Replace CLIP's text encoder with other pre-trained language models (BERT, T5) to assess whether improvements are specifically due to CLIP's architecture or more general to text-to-vision mappings

3. **Statistical Significance Analysis**: Conduct paired t-tests or bootstrap confidence intervals on few-shot classification results to determine whether reported improvements are statistically significant, especially for covariance predictions which show smaller gains