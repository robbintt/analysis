---
ver: rpa2
title: Cross-Axis Transformer with 3D Rotary Positional Embeddings
arxiv_id: '2311.07184'
source_url: https://arxiv.org/abs/2311.07184
tags:
- attention
- transformer
- embeddings
- image
- cross-axis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Cross-Axis Transformer (CAT), a vision
  transformer architecture designed to address computational inefficiency and inadequate
  spatial dimension handling in existing models. CAT draws inspiration from Axial
  Transformers and Microsoft's Retentive Network, reducing floating-point operations
  while achieving faster convergence and higher accuracy.
---

# Cross-Axis Transformer with 3D Rotary Positional Embeddings

## Quick Facts
- arXiv ID: 2311.07184
- Source URL: https://arxiv.org/abs/2311.07184
- Authors: 
- Reference count: 9
- Key outcome: Achieves 2.8x higher validation accuracy than baseline ViT with 2/3 the FLOPs and half the training time

## Executive Summary
The Cross-Axis Transformer (CAT) introduces a novel vision transformer architecture that addresses computational inefficiency in existing models through Cross-Axis Attention, reducing complexity from O(N²) to O(N) by splitting attention across image axes. The model incorporates 2D Rotary Positional Embeddings for effective spatial encoding and a residual imprint technique that reduces overfitting. Experimental results on ImageNet 1k demonstrate significant performance improvements over baseline Vision Transformers while requiring substantially less computational resources.

## Method Summary
CAT implements cross-axis attention by computing attention along height and width axes separately rather than over all patches simultaneously, reducing computational complexity. The model uses 2D rotary positional embeddings with rotation matrices applied to spatial coordinates, providing scale-invariant spatial encoding. A residual imprint technique adds convolutional embeddings to attention inputs, improving feature expressiveness. The architecture is trained on ImageNet 1k with patch size 8, 8 attention heads, 5 hidden layers, and hidden size 1024, using AdamW optimizer with cosine annealing learning rate schedule for 10 epochs.

## Key Results
- Achieves 2.8x higher validation accuracy than baseline Vision Transformer
- Requires only 2/3 the number of floating point operations (FLOPs)
- Trains in half the time of baseline models
- Exhibits reduced overfitting through convolutional embeddings and imprint technique

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-Axis Attention reduces computational complexity from O(N²) to O(N) by splitting attention across image axes separately
- Mechanism: Computes attention along height and width axes independently, then combines results, transforming O(S⁴) to O(S²) for square images
- Core assumption: Attention along individual axes preserves sufficient global context while dramatically reducing computation
- Evidence anchors: [abstract] complexity reduction claim; [section 2.1] 2.8x accuracy improvement with reduced FLOPs; [corpus] weak evidence from neighboring papers

### Mechanism 2
- Claim: 2D Rotary Positional Embeddings provide scale-invariant spatial encoding superior to traditional learned embeddings
- Mechanism: Uses rotation matrix formulation with frequency modulation along hidden dimensions, creating multi-scale representation for any image size
- Core assumption: Sinusoidal-like decay along hidden dimension better captures spatial relationships than learned embeddings
- Evidence anchors: [abstract] effective spatial encoding claim; [section 2.2] detailed mathematical formulation; [corpus] moderate evidence from rotary embedding discussions

### Mechanism 3
- Claim: Residual Imprint reduces overfitting by providing expressive convolutional embeddings to later layers
- Mechanism: Adds copy of image embedding from Conv2d layer to attention input, giving BatchNorm layers more diverse data
- Core assumption: Convolutional embeddings provide richer feature representations than linear projections, improving generalization
- Evidence anchors: [abstract] reduced overfitting observation; [section 2.3] 0.6% accuracy improvement from imprint; [corpus] no direct evidence in neighboring papers

## Foundational Learning

- Concept: Self-attention mechanism and quadratic complexity
  - Why needed here: Understanding why standard attention is computationally prohibitive for images (O(N²) where N is number of patches)
  - Quick check question: For a 224x224 image with 16x16 patches, how many patches total, and what's the complexity of standard attention?

- Concept: Axial attention and its limitations
  - Why needed here: Recognizing that splitting attention across axes reduces complexity but may miss cross-axis relationships
  - Quick check question: What's the complexity reduction from O(N²) to O(N√N) in axial attention, and what spatial information might be lost?

- Concept: Rotary positional embeddings and their mathematical basis
  - Why needed here: Understanding how rotation matrices encode relative positions and why frequency modulation matters
  - Quick check question: How does the rotation matrix R = [[cosθ, -sinθ], [sinθ, cosθ]] encode positional information when applied to feature vectors?

## Architecture Onboarding

- Component map: Image patches (8x8) → Conv2d embedding → Positional embeddings → Cross-Axis Attention blocks (5 layers) → Feed-forward network → Classification token (with Residual Imprint connection from embedding to attention input)

- Critical path: Conv2d → Positional Embeddings → Cross-Axis Attention → FFN → Classification

- Design tradeoffs:
  - Complexity reduction vs. potential loss of cross-axis spatial correlations
  - Fixed rotary embeddings vs. learned embeddings flexibility
  - Imprint technique potentially reduces overfitting but adds parameter dependencies

- Failure signatures:
  - Training instability: Likely from improper normalization or gamma scaling
  - Poor convergence: May indicate issues with rotary embedding formulation or axis separation
  - Overfitting despite imprint: Could mean imprint is applied too early or parameters need tuning

- First 3 experiments:
  1. Baseline comparison: Run CAT vs standard ViT on same hardware with identical hyperparameters to verify claimed 2.8x accuracy improvement
  2. Ablation study: Remove residual imprint to confirm its 0.6% accuracy contribution and overfitting reduction
  3. Complexity validation: Measure actual FLOPs during training to verify O(N) scaling claims vs theoretical expectations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Cross-Axis Transformer's improved performance primarily stem from the cross-axis attention mechanism or from the 2D rotary positional embeddings?
- Basis in paper: [explicit] The paper presents both innovations but doesn't isolate their individual contributions through ablation studies
- Why unresolved: The authors mention "we attempted a few novel ideas" and present ablation results, but don't specifically test the contribution of each component independently
- What evidence would resolve it: A controlled experiment comparing CAT with and without cross-axis attention but with standard positional embeddings, and vice versa

### Open Question 2
- Question: How does the Cross-Axis Transformer scale to larger datasets and longer training schedules?
- Basis in paper: [explicit] The authors explicitly state they trained for only 10 epochs on consumer hardware and note this is a limitation
- Why unresolved: The paper's results are based on limited compute resources and short training times, making it unclear if the advantages persist at scale
- What evidence would resolve it: Training CAT on larger datasets (e.g., ImageNet-21k) for extended periods (100+ epochs) on professional compute infrastructure

### Open Question 3
- Question: What is the theoretical explanation for why the image imprint technique reduces overfitting in CAT?
- Basis in paper: [inferred] The authors observe reduced overfitting with the imprint technique but only offer speculation about more expressive convolutions
- Why unresolved: The paper presents empirical evidence of reduced overfitting but lacks a rigorous theoretical explanation for the mechanism
- What evidence would resolve it: A formal analysis of the gradient flow and feature representation changes when using convolutional vs linear embeddings with the imprint technique

## Limitations

- Critical implementation details of the residual imprint mechanism remain underspecified
- Computational complexity claims require empirical validation beyond theoretical analysis
- Limited training duration (10 epochs) raises questions about scalability to larger datasets

## Confidence

**High Confidence**: The core architectural concept of splitting attention across image axes is well-established from related work on axial transformers. The mathematical formulation of cross-axis attention is sound and the complexity reduction from O(N²) to O(N) follows directly from the algorithmic structure.

**Medium Confidence**: The 2D rotary positional embeddings represent a reasonable extension of existing rotary embedding techniques. However, the claim of "scale-invariant" encoding requires empirical validation, as the effectiveness of this approach for varying image sizes is not demonstrated in the paper.

**Low Confidence**: The residual imprint technique is presented with minimal theoretical justification. The paper provides only empirical evidence (0.6% accuracy improvement) without explaining the mechanism by which convolutional embeddings reduce overfitting. The interaction between imprint and BatchNorm layers is not theoretically grounded.

## Next Checks

1. **Ablation Study Validation**: Implement and test a version of CAT without the residual imprint mechanism to independently verify the claimed 0.6% accuracy improvement and overfitting reduction. This requires careful control of all other hyperparameters.

2. **Cross-Axis Attention Implementation Review**: Verify the actual FLOPs during training on different image sizes (224x224, 384x384) to confirm the claimed O(N) scaling. Measure wall-clock time and memory usage to validate practical efficiency gains.

3. **Positional Embedding Robustness Test**: Evaluate CAT's performance on varying image resolutions and aspect ratios to test the scale-invariance claim of 2D rotary positional embeddings. Compare against standard learned positional embeddings across this distribution.