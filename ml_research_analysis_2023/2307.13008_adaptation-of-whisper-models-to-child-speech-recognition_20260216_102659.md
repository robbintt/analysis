---
ver: rpa2
title: Adaptation of Whisper models to child speech recognition
arxiv_id: '2307.13008'
source_url: https://arxiv.org/abs/2307.13008
tags:
- speech
- whisper
- child
- finetuning
- wav2vec2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates the adaptation of Whisper models to child speech
  recognition by finetuning them on child speech datasets and comparing their performance
  with wav2vec2 self-supervised models. The study shows that finetuning Whisper on
  child speech significantly improves ASR performance compared to non-finetuned Whisper
  models.
---

# Adaptation of Whisper models to child speech recognition

## Quick Facts
- arXiv ID: 2307.13008
- Source URL: https://arxiv.org/abs/2307.13008
- Reference count: 0
- Primary result: Finetuning Whisper on child speech improves ASR performance, but wav2vec2 models finetuned on child speech outperform Whisper finetuning.

## Executive Summary
This paper evaluates the effectiveness of finetuning Whisper and wav2vec2 models for child speech recognition. The authors demonstrate that while finetuning Whisper on child speech significantly improves performance compared to non-finetuned models, wav2vec2 models finetuned on child speech achieve even better results. Specifically, wav2vec2-large finetuned on MyST and PFSTAR datasets achieved the lowest word error rates across all child speech datasets tested. The study suggests that wav2vec2 is better suited for task-specific applications while Whisper may be more appropriate for unseen datasets due to its extensive multilingual training.

## Method Summary
The authors finetuned Whisper (Medium and Large-V2) and wav2vec2 (Base and Large) models on child speech datasets (MyST and PFSTAR) while freezing all layers except the final one. They evaluated the models on child speech test sets (MyST_test, PFS_test, CMU_test) and an adult speech test set (dev-clean). The finetuning used learning rates of 1e-5 for Whisper and 1e-4 (wav2vec2-base) or 2.5e-5 (wav2vec2-large). Word error rate (WER) was used as the primary evaluation metric.

## Key Results
- Finetuning Whisper on child speech significantly improves ASR performance compared to non-finetuned Whisper models
- Wav2vec2 models finetuned on child speech outperform Whisper finetuning on all child speech datasets
- wav2vec2-large finetuned on MyST and PFSTAR achieved the lowest WER across all child speech datasets
- Whisper models showed better generalization to unseen datasets compared to wav2vec2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finetuning Whisper on child speech significantly improves ASR performance compared to non-finetuned Whisper models.
- Mechanism: Whisper models are trained on large amounts of adult speech data, which differ acoustically from child speech due to differences in pitch, vocal tract length, and pronunciation ability. Finetuning adjusts the model parameters to the specific acoustic and linguistic characteristics of child speech, improving recognition accuracy.
- Core assumption: The pre-trained Whisper model has learned general speech recognition capabilities that can be adapted to child speech with relatively small amounts of finetuned data.
- Evidence anchors:
  - [abstract] "We demonstrate that finetuning Whisper on child speech yields significant improvements in ASR performance on child speech, compared to non-finetuned Whisper models."
  - [section] "Finetuning with MyST_55h showed a significant improvement in the WER of MyST_test and PFS_test."
- Break condition: If the child speech dataset is too small or too different from the Whisper pretraining data, finetuning may not provide significant improvement or could lead to overfitting.

### Mechanism 2
- Claim: Wav2vec2 models finetuned on child speech outperform Whisper finetuning.
- Mechanism: Wav2vec2 uses self-supervised learning to learn speech representations from large amounts of unlabeled speech data. When finetuned on child speech, these representations are adapted to the specific characteristics of child speech, leading to better performance than Whisper finetuning.
- Core assumption: The self-supervised pretraining of wav2vec2 allows it to learn more robust and generalizable speech representations than Whisper's supervised pretraining.
- Evidence anchors:
  - [abstract] "Additionally, utilizing self-supervised Wav2vec2 models that have been finetuned on child speech outperforms Whisper finetuning."
  - [section] "When both MyST_55h and PFS_10h were used for finetuning, the lowest WER was observed with wav2vec2 finetuning across all child speech datasets as compared to Whisper finetuning."
- Break condition: If the wav2vec2 pretraining data does not sufficiently cover the acoustic characteristics of child speech, finetuning may not lead to significant improvements.

### Mechanism 3
- Claim: Whisper models generalize better to unseen datasets compared to wav2vec2.
- Mechanism: Whisper models are trained on a much larger dataset (680k hours) compared to wav2vec2 (60k hours), including multilingual and low-resource language data. This extensive training allows Whisper to learn more robust and generalizable speech representations, leading to better performance on unseen datasets.
- Core assumption: The larger and more diverse training data of Whisper leads to better generalization capabilities compared to wav2vec2.
- Evidence anchors:
  - [section] "Since Whisper is trained with an order of magnitude more data than wav2vec2 (680k vs 60k) and contains a lot of multilingual and low resource languages during training, we believe that this multilingual data can be utilized to provide child speech recognition tasks via finetuning."
  - [section] "Although Whisper may be more appropriate for unseen datasets, wav2vec2 is a better choice for real-time, task-specific applications."
- Break condition: If the unseen dataset has characteristics that are very different from the Whisper pretraining data, generalization may not be effective.

## Foundational Learning

- Concept: Automatic Speech Recognition (ASR)
  - Why needed here: ASR is the core technology being evaluated in this paper. Understanding its principles and challenges is crucial for interpreting the results.
  - Quick check question: What are the main challenges in ASR for child speech compared to adult speech?

- Concept: Self-supervised learning
  - Why needed here: Wav2vec2 uses self-supervised learning to pretrain speech representations. Understanding this approach is essential for comparing it with Whisper's supervised learning.
  - Quick check question: How does self-supervised learning differ from supervised learning in the context of speech recognition?

- Concept: Finetuning
  - Why needed here: Finetuning is the key method used in this paper to adapt pre-trained models to child speech. Understanding its principles and limitations is crucial for interpreting the results.
  - Quick check question: What are the advantages and disadvantages of finetuning compared to training a model from scratch?

## Architecture Onboarding

- Component map: Child speech datasets (MyST, PFSTAR, CMU Kids) -> Whisper and wav2vec2 models -> Finetuning process -> WER evaluation on test sets
- Critical path:
  1. Preprocess child speech datasets (MyST, PFSTAR, CMU Kids, dev-clean)
  2. Finetune Whisper models on child speech datasets
  3. Evaluate finetuned Whisper models on test datasets
  4. Finetune wav2vec2 models on child speech datasets
  5. Evaluate finetuned wav2vec2 models on test datasets
  6. Compare results and draw conclusions
- Design tradeoffs:
  - Model size vs. performance: Larger models generally perform better but require more computational resources
  - Pretraining data vs. finetuning data: Models trained on more diverse pretraining data may generalize better but require more finetuning data for specific tasks
  - Supervised vs. self-supervised learning: Supervised learning may lead to better performance on specific tasks but requires more labeled data, while self-supervised learning can leverage large amounts of unlabeled data
- Failure signatures:
  - High WER on test datasets: Indicates poor model performance
  - Large discrepancy between training and test WER: Suggests overfitting or domain mismatch
  - Unstable training: May indicate issues with hyperparameters or data quality
- First 3 experiments:
  1. Finetune Whisper Medium model on MyST_55h dataset and evaluate on MyST_test, PFS_test, CMU_test, and dev-clean
  2. Finetune wav2vec2-base model on MyST_55h dataset and evaluate on the same test datasets
  3. Compare the results of experiments 1 and 2 to determine which model performs better on child speech recognition

## Open Questions the Paper Calls Out
- Question: Does Whisper finetuning on adult speech improve child ASR performance compared to non-finetuned Whisper?
  - Basis in paper: [explicit] The paper mentions evaluating Whisper models on child speech datasets and comparing them with wav2vec2 models. However, it does not explicitly discuss finetuning Whisper on adult speech for child ASR.
  - Why unresolved: The paper focuses on finetuning Whisper on child speech datasets and comparing it with wav2vec2. It does not explore the possibility of using adult speech data for Whisper finetuning.
  - What evidence would resolve it: Finetuning Whisper models on adult speech datasets and evaluating their performance on child speech datasets would provide insights into whether adult speech data can improve child ASR performance.

- Question: How does the performance of Whisper models vary across different age groups of children?
  - Basis in paper: [inferred] The paper discusses the challenges of recognizing child speech due to differences in pitch, linguistic features, and pronunciation ability. However, it does not explicitly analyze the performance of Whisper models across different age groups of children.
  - Why unresolved: The paper does not provide a detailed analysis of Whisper model performance for different age groups of children, which could be valuable for understanding the model's effectiveness in real-world scenarios.
  - What evidence would resolve it: Evaluating Whisper models on child speech datasets that include children from different age groups and analyzing the performance variation would provide insights into the model's effectiveness across age groups.

- Question: Can data augmentation techniques improve the performance of Whisper models for child ASR?
  - Basis in paper: [inferred] The paper mentions that data augmentation methods have been used in previous studies for child ASR. However, it does not explicitly explore the use of data augmentation techniques for Whisper models.
  - Why unresolved: The paper does not investigate the potential benefits of data augmentation techniques for improving Whisper model performance on child speech.
  - What evidence would resolve it: Applying data augmentation techniques to child speech datasets and evaluating the performance of Whisper models finetuned on the augmented data would provide insights into the effectiveness of data augmentation for child ASR.

## Limitations
- The study uses word error rate as the primary evaluation metric, which may not fully capture the nuances of child speech recognition quality
- The finetuning approach freezes all layers except the last one, which may not be optimal for adapting these models to child speech characteristics
- The dataset sizes used for finetuning (55 hours for MyST, 10 hours for PFSTAR) may be insufficient to fully capture the variability in child speech across different ages, accents, and speaking styles

## Confidence
- High confidence: The experimental methodology is sound and the comparison between Whisper and wav2vec2 is methodologically rigorous. The observation that wav2vec2 finetuned on child speech outperforms Whisper finetuning is supported by clear experimental evidence.
- Medium confidence: The claim that Whisper generalizes better to unseen datasets is supported by the data but requires more extensive cross-dataset validation to be fully established. The interpretation of Whisper's multilingual training as beneficial for child speech recognition is plausible but not directly tested.
- Medium confidence: The recommendation that wav2vec2 is better suited for task-specific applications while Whisper is more appropriate for unseen datasets is based on limited data and requires further validation across more diverse child speech datasets.

## Next Checks
1. **Extended acoustic analysis**: Conduct a detailed acoustic analysis comparing the performance of Whisper and wav2vec2 across different child age groups, speaking rates, and noise conditions to identify specific scenarios where each model excels or struggles.

2. **Multi-metric evaluation**: Supplement WER with additional metrics such as phoneme error rate, disfluency detection accuracy, and semantic preservation to provide a more comprehensive assessment of recognition quality for child speech.

3. **Progressive finetuning study**: Investigate the effects of finetuning different numbers of layers in both Whisper and wav2vec2 models to determine optimal adaptation strategies for child speech, potentially revealing that selective layer adaptation outperforms the current approach of only finetuning the final layer.