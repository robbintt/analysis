---
ver: rpa2
title: A Zero-Shot Language Agent for Computer Control with Structured Reflection
arxiv_id: '2310.08740'
source_url: https://arxiv.org/abs/2310.08740
tags:
- agent
- planning
- tasks
- action
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a zero-shot language agent for computer control
  that can autonomously learn and improve from exploration failures without expert
  traces. The core method is a staged planning strategy that plans multiple executable
  actions on a screen in one pass, combined with structured reflection to learn from
  mistakes.
---

# A Zero-Shot Language Agent for Computer Control with Structured Reflection

## Quick Facts
- arXiv ID: 2310.08740
- Source URL: https://arxiv.org/abs/2310.08740
- Reference count: 9
- Key outcome: Zero-shot language agent achieves 99.6% accuracy on 1-step MiniWoB++ tasks without expert traces

## Executive Summary
This paper presents a zero-shot language agent for computer control that autonomously learns from exploration failures without requiring expert demonstrations. The agent uses a staged planning strategy to plan multiple executable actions on a screen in one pass, combined with structured reflection to identify and correct mistakes across trials. Tested on MiniWoB++ benchmark, the agent achieves state-of-the-art performance on 1-step and multi-step tasks while maintaining zero-shot learning capability.

## Method Summary
The agent operates by receiving a compact HTML representation of the current screen state, then uses a staged planner to generate all executable actions for that screen simultaneously. Actions are executed and the environment provides feedback. When failures occur, a structured reflection module analyzes the failure to identify the earliest critical mistake, generates corrections, and enforces these in subsequent trials while disabling previously failed actions. The system requires no expert traces and uses a unified instruction prompt across different tasks.

## Key Results
- Achieves 99.6% accuracy on 1-screen-1-step tasks
- Achieves 96.2% accuracy on 1-screen-n-step tasks
- Achieves 86.5% accuracy on n-screen-n-step tasks across 5 trials
- Outperforms prior state-of-the-art in many cases while using no expert traces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured reflection enables zero-shot learning by systematically identifying and correcting mistakes without expert traces
- Mechanism: When the agent encounters a failure state, it uses a reflection prompt to identify the earliest critical mistake, generates a corrected action, and enforces that correction in the next trial while disabling previously failed actions
- Core assumption: The agent can reliably identify the "earliest critical mistake" through self-reflection and that this mistake is sufficient to correct the entire trajectory
- Evidence anchors:
  - [abstract]: "iteratively progresses a task by identifying and learning from its mistakes via self-reflection and structured thought management"
  - [section]: "We propose a simple yet efficient action planner that can accurately plan out executable actions on a state in one pass. With recent capacity of LLM, we show that such a 'naive' strategy can solve almost all the easy tasks on the MINI WOB++ benchmark"
  - [corpus]: Weak evidence - only 0 citations found for this specific mechanism, though related reflection works exist
- Break condition: If the reflection mechanism cannot reliably identify the critical mistake, or if multiple early mistakes compound in ways that single-step correction cannot address

### Mechanism 2
- Claim: Staged planning reduces computational overhead by planning all executable actions on a screen in one pass rather than iterative action-by-action planning
- Mechanism: Instead of planning one action at a time and executing it before planning the next, the agent plans all possible executable actions for the current screen state simultaneously, then executes them in sequence
- Core assumption: For computer control tasks, multiple actions can be planned and executed on a single screen without needing to observe intermediate state changes
- Evidence anchors:
  - [abstract]: "Our agent plans for executable actions on a partially observed environment, and iteratively progresses a task by identifying and learning from its mistakes via self-reflection and structured thought management"
  - [section]: "To address these issues, we take a step in the middle by maximally planning actions that are visible on the current state all at once"
  - [corpus]: Weak evidence - only 0 citations found for this specific mechanism, though related planning works exist
- Break condition: If the environment requires observation of intermediate state changes between actions, or if the LLM cannot accurately plan multiple actions in one pass

### Mechanism 3
- Claim: Compact screen representation enables efficient processing by reducing HTML verbosity while retaining essential UI elements
- Mechanism: The agent uses a simplified HTML representation that retains only key attributes (id, class, text, placeholder, value, position) for each leaf element, reducing input length for the LLM
- Core assumption: The simplified representation contains sufficient information for the agent to make correct action decisions
- Evidence anchors:
  - [section]: "We take inspiration from Wang et al. (2023) to heuristically simplify the HTML code of each screen, retaining key attributes for each leaf element"
  - [section]: "Raw HTML code tends to be verbose, which poses a practical challenge for LLMs that often have an inherent limit on the input or context length"
  - [corpus]: Weak evidence - only 0 citations found for this specific mechanism, though related UI simplification works exist
- Break condition: If the simplified representation omits critical information needed for action decisions, or if the LLM requires more detailed context for accurate planning

## Foundational Learning

- Concept: Zero-shot learning without expert traces
  - Why needed here: The agent must learn to perform tasks without any human-provided examples or demonstrations
  - Quick check question: Can you explain how the agent learns from failures without any expert demonstrations?

- Concept: Structured reflection and thought management
  - Why needed here: The agent needs a systematic way to learn from mistakes across multiple trials without getting stuck in loops or losing track of corrections
  - Quick check question: How does the structured reflection mechanism prevent the agent from repeating the same mistakes?

- Concept: Staged planning for multi-action sequences
  - Why needed here: The agent needs to efficiently plan multiple actions on a single screen without requiring multiple LLM calls
  - Quick check question: What is the key difference between staged planning and iterative planning in terms of computational efficiency?

## Architecture Onboarding

- Component map:
  - Screen representation module (compact HTML parser)
  - Staged planner (plans multiple actions per screen)
  - Action executor (grounds planned actions on environment)
  - Environment feedback processor (categorizes task outcomes)
  - Structured reflection module (identifies and corrects mistakes)
  - Action space constraint manager (disables failed actions)

- Critical path:
  1. Receive screen representation
  2. Staged planner generates executable actions
  3. Actions are executed on environment
  4. Environment feedback is processed
  5. If failure, structured reflection identifies corrections
  6. Corrections are enforced in next trial

- Design tradeoffs:
  - Compact representation vs. completeness of information
  - Staged planning efficiency vs. need for intermediate state observation
  - Structured reflection complexity vs. ability to handle compound errors
  - Zero-shot learning vs. performance compared to few-shot approaches

- Failure signatures:
  - Agent gets stuck in loops between two failed actions
  - Agent fails to identify the critical mistake in reflection
  - Agent cannot plan multiple actions accurately in one pass
  - Agent fails on tasks requiring visual information not in HTML

- First 3 experiments:
  1. Test staged planning on a simple 1-screen-1-step task to verify basic functionality
  2. Test structured reflection on a task the agent initially fails to ensure learning occurs
  3. Test compact representation by comparing performance with full HTML representation on a complex task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the agent's performance scale with the number of candidate actions in a single screen?
- Basis in paper: [explicit] The paper discusses performance on tasks like click-checkboxes and social-media, noting that the number of candidates affects completion rates.
- Why unresolved: The paper provides some data on completion rates for different numbers of candidates but does not explore the upper limit of the agent's capacity to handle candidates in a single screen.
- What evidence would resolve it: Further experiments testing the agent's performance on screens with increasingly large numbers of candidate actions, identifying the point at which performance degrades significantly.

### Open Question 2
- Question: Can the agent's structured reflection mechanism be improved to handle non-click actions more effectively?
- Basis in paper: [explicit] The paper mentions that the agent currently cannot enforce disabled actions for non-click actions, relying instead on deterministic generation at the associated time step.
- Why unresolved: The paper notes that prompting the LLM to handle non-click actions with disabled sets was not effective in preliminary experiments, but does not explore alternative methods.
- What evidence would resolve it: Experiments testing different approaches to handling non-click actions in the reflection mechanism, such as using more explicit constraints or developing a separate reflection strategy for non-click actions.

### Open Question 3
- Question: How does the agent's performance compare to human performance on these tasks?
- Basis in paper: [inferred] The paper does not provide a direct comparison to human performance, but mentions that even humans can make mistakes when executing tasks.
- Why unresolved: The paper focuses on comparing the agent's performance to other automated systems but does not include a baseline of human performance.
- What evidence would resolve it: A study comparing the agent's completion rates and efficiency to those of human participants performing the same tasks, controlling for factors like task familiarity and expertise.

## Limitations

- The structured reflection mechanism lacks rigorous analysis of failure cases where identifying the "earliest critical mistake" fails
- The staged planning approach assumes minimal need for intermediate state observation, which may not hold for all task types
- The compact HTML representation may systematically exclude visual cues that other approaches leverage

## Confidence

- **High confidence**: The staged planning mechanism works as described for simple tasks where state changes between actions are minimal or predictable. The performance numbers on 1-step and 1-screen-n-step tasks appear reproducible given the described methodology.
- **Medium confidence**: The structured reflection mechanism can learn from failures in controlled environments, but its generalizability to more complex tasks with compound errors is uncertain. The reflection prompt effectiveness depends heavily on the specific task structure and may not scale to all computer control scenarios.
- **Low confidence**: The claim that no expert traces are needed for competitive performance is the most tenuous. While zero-shot learning is demonstrated, the comparison with few-shot or trajectory-based approaches that could potentially outperform zero-shot remains incomplete.

## Next Checks

1. **Ablation study on reflection depth**: Systematically test how the reflection mechanism performs when multiple errors compound versus single-step corrections, and measure performance degradation as reflection depth increases.

2. **Cross-task generalization test**: Evaluate the agent on tasks outside the MiniWoB++ benchmark that require visual information not captured in HTML (such as color-based interactions or layout-dependent actions) to validate the limits of the compact representation.

3. **Cost-benefit analysis of staged planning**: Measure actual LLM token usage and API costs for staged planning versus iterative approaches across different task complexities to validate the claimed computational efficiency.