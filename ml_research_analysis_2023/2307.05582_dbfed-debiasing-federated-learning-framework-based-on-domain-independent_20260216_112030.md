---
ver: rpa2
title: 'DBFed: Debiasing Federated Learning Framework based on Domain-Independent'
arxiv_id: '2307.05582'
source_url: https://arxiv.org/abs/2307.05582
tags:
- data
- learning
- federated
- training
- sensitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of fairness and bias in federated
  learning models, where data differences among clients can lead to biased and discriminatory
  models. The proposed DBFed framework mitigates model bias by explicitly encoding
  sensitive attributes during client-side training.
---

# DBFed: Debiasing Federated Learning Framework based on Domain-Independent

## Quick Facts
- arXiv ID: 2307.05582
- Source URL: https://arxiv.org/abs/2307.05582
- Reference count: 30
- Primary result: DBFed improves fairness in federated learning by explicitly encoding sensitive attributes during client-side training while maintaining high accuracy

## Executive Summary
This paper addresses fairness and bias issues in federated learning models caused by data differences among clients. The proposed DBFed framework mitigates model bias by explicitly encoding sensitive attributes during client-side training, using N×D-way discriminant classifiers where N is the number of target attribute categories and D is the number of sensitive attribute categories. The framework achieves demographic parity by ensuring equal prediction probabilities across different sensitive attribute groups while maintaining classification accuracy.

## Method Summary
DBFed implements a domain-independent training approach where clients explicitly encode sensitive attribute information during local training using N×D-way discriminant classifiers. The framework uses cross-entropy loss calculated separately for target attributes and sensitive attributes. After local training, clients send their model weights to the server for aggregation using the FedAvg algorithm. The server then distributes the updated global model back to clients. This process preserves the debiasing effects learned locally while maintaining the federated learning paradigm's privacy benefits.

## Key Results
- DBFed achieved the highest accuracy on FairFace dataset among comparative methods
- Demonstrated excellent results in Skewed Error Ratio, Bias Amplification, and Demographic Parity on UTKFace dataset
- Outperformed other methods in most fairness metrics while maintaining high accuracy across three real datasets (CelebA, FairFace, UTKFace)

## Why This Works (Mechanism)

### Mechanism 1
DBFed reduces model bias by explicitly encoding sensitive attributes during client-side training. The model uses N×D-way discriminant classifiers where N is the number of target attribute categories and D is the number of sensitive attribute categories. This explicit encoding forces the model to learn feature representations that are less correlated with sensitive attributes. Core assumption: The model can learn to predict target attributes while minimizing the correlation between predictions and sensitive attributes through explicit encoding. Break condition: If the correlation between sensitive attributes and target predictions cannot be sufficiently reduced through explicit encoding, the bias mitigation effect will be limited.

### Mechanism 2
DBFed achieves demographic parity by ensuring equal prediction probabilities across different sensitive attribute groups. During prediction, DBFed uses P(d|x) = 1/|G| for all sensitive attribute groups, effectively making predictions blind to sensitive attributes while maintaining classification accuracy. Core assumption: Equal treatment of all sensitive attribute groups during prediction leads to demographic parity. Break condition: If the model cannot maintain high accuracy while equalizing prediction probabilities across groups, the approach may fail.

### Mechanism 3
DBFed improves model fairness through federated averaging while preserving the debiasing effects learned locally. After local training with explicit sensitive attribute encoding, clients send their model weights to the server for aggregation using FedAvg, preserving the fairness improvements learned at each client. Core assumption: The federated averaging process maintains the bias-reduction properties learned during local training. Break condition: If the aggregation process dilutes or eliminates the bias-reduction effects from local training, overall fairness improvements will be limited.

## Foundational Learning

- **Federated Learning fundamentals**: Understanding how federated learning works is essential to grasp why DBFed's approach is novel and effective. Quick check: What is the key difference between federated learning and centralized learning in terms of data privacy?

- **Fairness metrics in machine learning**: DBFed uses multiple fairness metrics (SER, EO, BA, DP) to evaluate its effectiveness, so understanding these metrics is crucial. Quick check: How does Demographic Parity differ from Equal Opportunity in fairness evaluation?

- **Explicit encoding of sensitive attributes**: DBFed's core innovation is explicitly encoding sensitive attributes during training, which requires understanding this technique. Quick check: What is the potential risk of explicitly encoding sensitive attributes during model training?

## Architecture Onboarding

- **Component map**: Global server -> Multiple clients -> Sensitive attribute encoder -> FedAvg aggregator
- **Critical path**: Client local training → Weight aggregation → Global model update → Distribution to clients
- **Design tradeoffs**: Accuracy vs. fairness (explicit encoding may slightly reduce accuracy but significantly improves fairness), communication overhead (more frequent communication rounds may be needed), computational cost (additional computation for encoding sensitive attributes increases local training time)
- **Failure signatures**: SER and BA metrics remain high despite training, accuracy drops significantly compared to baseline, model performance varies drastically across different sensitive attribute groups
- **First 3 experiments**: 1) Test DBFed on CelebA dataset with "Smiling" as target attribute and "Male" as sensitive attribute, 2) Evaluate DBFed on FairFace dataset with "Age" prediction and "Race" as sensitive attribute, 3) Assess DBFed on UTKFace dataset with multi-class age prediction and race as sensitive attribute

## Open Questions the Paper Calls Out

### Open Question 1
Can DBFed effectively mitigate model bias in federated learning without using sensitive attribute labels during the training process? The current DBFed framework requires sensitive attribute labels during training, which limits its applicability in real-world scenarios where such labels may not be available or ethical to use. What evidence would resolve it: Experimental results demonstrating the effectiveness of a modified DBFed framework that achieves similar or better fairness metrics without using sensitive attribute labels during training.

### Open Question 2
How does the performance of DBFed compare to other fairness-aware federated learning methods when dealing with more diverse sensitive attribute categories? The paper only tests DBFed on three datasets with limited sensitive attribute diversity, leaving questions about its scalability and effectiveness in more complex scenarios. What evidence would resolve it: Comparative experiments on additional datasets with more diverse and numerous sensitive attribute categories, measuring fairness metrics alongside accuracy.

### Open Question 3
What is the impact of varying the number of clients and communication rounds on DBFed's ability to mitigate bias while maintaining model accuracy? The optimal configuration of clients and communication rounds for achieving the best balance between fairness and accuracy is not investigated, which is crucial for practical implementation. What evidence would resolve it: Systematic experiments varying the number of clients, local epochs, and communication rounds, analyzing the trade-off between fairness improvements and accuracy degradation.

## Limitations

- The evaluation relies on three curated datasets that may not fully represent real-world federated learning scenarios with heterogeneous client populations
- The framework's performance in settings with high data heterogeneity or non-IID distributions across clients remains unverified
- The explicit encoding of sensitive attributes could potentially introduce privacy concerns if sensitive attribute information leaks through the model weights during federated averaging

## Confidence

- **High confidence**: The core mechanism of explicit sensitive attribute encoding during client-side training and its ability to improve fairness metrics (SER, BA, DP) on benchmark datasets
- **Medium confidence**: The claim that DBFed maintains high accuracy while improving fairness, as the accuracy differences between DBFed and baselines are relatively small
- **Medium confidence**: The effectiveness of federated averaging in preserving the debiasing effects learned locally, as the aggregation mechanism's impact on fairness preservation needs further validation

## Next Checks

1. **Generalization to Non-IID Settings**: Test DBFed on datasets with highly non-IID distributions across clients to verify if the fairness improvements hold when clients have significantly different data distributions

2. **Privacy Impact Analysis**: Conduct experiments to measure the extent of sensitive attribute information leakage through model weights during federated averaging, and evaluate if the explicit encoding creates new privacy vulnerabilities

3. **Scalability Assessment**: Evaluate DBFed's performance with a larger number of clients (beyond the 50 clients used in experiments) to determine if the framework scales effectively while maintaining both accuracy and fairness improvements