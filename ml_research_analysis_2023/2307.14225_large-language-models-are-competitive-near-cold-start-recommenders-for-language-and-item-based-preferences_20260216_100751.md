---
ver: rpa2
title: Large Language Models are Competitive Near Cold-start Recommenders for Language-
  and Item-based Preferences
arxiv_id: '2307.14225'
source_url: https://arxiv.org/abs/2307.14225
tags:
- item
- language
- preferences
- u1d461
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using large language models (LLMs) for
  making recommendations from both item-based and language-based preferences. The
  authors collected a new dataset containing both types of preferences along with
  ratings on recommended items.
---

# Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences

## Quick Facts
- arXiv ID: 2307.14225
- Source URL: https://arxiv.org/abs/2307.14225
- Reference count: 40
- LLMs achieve competitive NDCG@10 scores to traditional CF methods for language-based preferences in cold-start scenarios

## Executive Summary
This paper investigates using large language models (LLMs) for making recommendations from both item-based and language-based preferences. The authors collected a new dataset containing both types of preferences along with ratings on recommended items. They find that LLMs provide competitive recommendation performance for pure language-based preferences in the near cold-start case, without any supervised training (zero-shot) or with only a few labels (few-shot). Specifically, the LLM-based approach achieves comparable NDCG@10 scores to traditional item-based collaborative filtering methods, despite only using language descriptions of preferences. This is promising as language-based representations are more explainable and scrutable than item-based or vector-based representations.

## Method Summary
The authors collected a dataset with user preferences expressed as both item ratings and natural language descriptions, along with ratings on recommended and random items. They implemented baseline item-based CF methods (EASE, WRMF, BPR-SLIM, KNN) and language-based BM25-Fusion, as well as LLM-based prompting strategies (zero-shot, few-shot, completion) for item-only, language-only, and combined preferences. Recommendations were generated by scoring and ranking items using the log-likelihood of item names from the LLM. Performance was evaluated using NDCG@10 with exponential gain, comparing seen vs. unseen items and unbiased vs. biased recommendation sets.

## Key Results
- LLM-based methods achieve comparable NDCG@10 scores to traditional CF methods for language-based preferences in cold-start scenarios
- Few-shot prompting outperforms zero-shot and completion prompting for recommendation tasks
- Language-based methods perform equally well using initial natural language descriptions versus final descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based recommendations perform competitively with traditional CF methods for language-only preferences in cold-start scenarios.
- Mechanism: LLMs can generate recommendations by mapping language-based preference descriptions directly to item scores without requiring any supervised training or large amounts of user-item interaction data.
- Core assumption: The LLM's pre-trained knowledge captures sufficient semantic relationships between descriptive preferences and items to make useful recommendations.
- Evidence anchors:
  - [abstract] "LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot)."
  - [section 4.2.2] "Completion: desc/u1D461+ ⟨item/u1D461∗⟩" shows the simple prompting approach used.
  - [corpus] Weak evidence - no specific corpus papers directly support this mechanism, though related work on LLMs for recommendation exists.
- Break condition: If the LLM's pre-trained knowledge does not contain relevant semantic relationships between language descriptions and items, performance would degrade significantly.

### Mechanism 2
- Claim: Language-based preference representations are more explainable and scrutable than item-based or vector-based representations.
- Mechanism: Natural language descriptions of preferences allow users to directly understand and control what the system is using for personalization, unlike opaque item vectors or embeddings.
- Core assumption: Users can effectively articulate their preferences in natural language in a way that captures their true interests.
- Evidence anchors:
  - [abstract] "This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations."
  - [section 5.2] "The language-based results use only the initial natural language descriptions, which raters produced much faster than either liked and disliked item choices or final descriptions, yet they yield equal performance to final descriptions."
  - [corpus] Weak evidence - no specific corpus papers directly address explainability/scrutability, though related work on transparent user models exists.
- Break condition: If users cannot effectively articulate their preferences in natural language, or if the language descriptions are too vague to be useful for recommendation.

### Mechanism 3
- Claim: Few-shot prompting with LLMs outperforms zero-shot and completion prompting for recommendation tasks.
- Mechanism: Providing a small number of examples of user preferences and liked items helps the LLM better understand the task and generate more accurate recommendations.
- Core assumption: The few examples provided in the prompt are representative of the user's actual preferences and help the LLM learn the mapping between preferences and recommendations.
- Evidence anchors:
  - [abstract] "Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot)."
  - [section 4.2.3] "Few-shot (/u1D458): Repeat /u1D45F∈ { 1, . . . , /u1D458} { User Description: desc/u1D45F+ User Movie Preferences: item/u1D45F,1+ , item/u1D45F,2+ , item/u1D45F,3+ , item/u1D45F,4+ Additional User Movie Preference: item/u1D45F,5+ User Description: desc/u1D461+ User Movie Preferences: ⟨item/u1D461∗⟩ }"
  - [section 5.2] "RQ3: Best prompting methodology? The Few-shot (3) prompting method generally outperforms Zero-shot and Completion prompting methods."
- Break condition: If the few examples provided are not representative or do not help the LLM understand the task, performance may not improve or could degrade.

## Foundational Learning

- Concept: Natural Language Processing (NLP) and Language Models
  - Why needed here: Understanding how LLMs process and generate natural language is crucial for designing effective prompts and interpreting results.
  - Quick check question: What is the difference between encoder-only models like BERT and decoder-only models like GPT, and how might this affect their suitability for recommendation tasks?

- Concept: Recommender Systems and Collaborative Filtering
  - Why needed here: Knowledge of traditional recommendation approaches is necessary to compare LLM-based methods and understand their advantages/disadvantages.
  - Quick check question: How do item-based collaborative filtering methods like EASE differ from user-based methods, and what are the trade-offs between them?

- Concept: Prompt Engineering and In-Context Learning
  - Why needed here: Designing effective prompts is key to getting good performance from LLMs without fine-tuning, and understanding in-context learning helps explain why few-shot prompts work.
  - Quick check question: What is the difference between zero-shot, one-shot, and few-shot prompting, and how does the number of examples provided affect the LLM's performance?

## Architecture Onboarding

- Component map:
  - User preference elicitation (Phase 1)
    - Natural language descriptions of liked/disliked items
    - Item selections for liked/disliked items
  - Recommendation generation (Phase 2)
    - Traditional CF baselines (EASE, WRMF, BPR-SLIM, etc.)
    - Language-based baseline (BM25-Fusion)
    - LLM-based methods (Completion, Zero-shot, Few-shot)
  - Evaluation
    - User ratings of recommended items
    - NDCG@10 metric for ranking quality

- Critical path:
  1. Collect user preferences (language + items)
  2. Generate recommendations using various methods
  3. Collect user ratings of recommendations
  4. Calculate NDCG@10 scores for each method
  5. Compare performance of LLM-based vs. traditional methods

- Design tradeoffs:
  - Language-only vs. item-based preferences: Faster to collect but potentially less accurate
  - Zero-shot vs. few-shot prompting: Easier to implement but potentially lower performance
  - Negative preferences: More information but potentially harder to model

- Failure signatures:
  - LLM-based methods perform significantly worse than traditional CF methods
  - No improvement from few-shot to zero-shot prompting
  - Language-based methods perform much worse than item-based methods
  - Negative preferences significantly degrade performance

- First 3 experiments:
  1. Compare LLM-based methods (Completion, Zero-shot, Few-shot) against traditional CF baselines on the full dataset
  2. Test the impact of including negative preferences in LLM prompts (Pos+Neg variants)
  3. Evaluate the effect of using initial vs. final language descriptions on recommendation performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we measure and compare unintended bias in language-driven recommenders versus classic recommenders?
- Basis in paper: [explicit] The paper mentions it would be valuable to study how to measure whether language-driven recommenders exhibit more or less unintended bias than classic recommenders, such as perhaps preferring certain classes of items over others.
- Why unresolved: This requires developing and applying bias measurement methods specifically for language-driven recommenders, which the paper does not address.
- What evidence would resolve it: Applying bias measurement methods to language-driven recommenders and comparing the results to classic recommenders would resolve this.

### Open Question 2
- Question: Does the same performance hold for language-based recommendation in other languages and cultures?
- Basis in paper: [inferred] The paper notes that conclusions are based on a relatively small pool of 153 raters and only English preferences, so it cannot assess whether the same results would be obtained in other languages or cultures.
- Why unresolved: The experiments were only conducted with English preferences, so performance in other languages and cultures is unknown.
- What evidence would resolve it: Conducting experiments with language-based recommendation in other languages and cultures would resolve this.

### Open Question 3
- Question: How can we develop and apply bias measurement methods specifically for language-driven recommenders?
- Basis in paper: [inferred] The paper mentions it would be valuable to study how to measure whether language-driven recommenders exhibit more or less unintended bias than classic recommenders, but does not address how to do this.
- Why unresolved: Developing bias measurement methods for language-driven recommenders requires new research and experimentation.
- What evidence would resolve it: Developing and applying bias measurement methods specifically for language-driven recommenders would resolve this.

## Limitations

- The exact prompt templates and formatting used for the LLM are not provided, which could significantly impact performance.
- The specific configuration and hyperparameters of the PaLM model variant are not specified.
- The mechanism by which the LLM's pre-trained knowledge enables competitive performance without task-specific fine-tuning is not fully explained or validated.

## Confidence

- **High confidence:** The experimental results showing LLM-based methods achieving comparable NDCG@10 scores to traditional CF methods for language-based preferences in cold-start scenarios.
- **Medium confidence:** The claim that language-based preference representations are more explainable and scrutable than item-based representations, as this is primarily asserted rather than extensively validated with user studies.
- **Low confidence:** The exact mechanism by which the LLM's pre-trained knowledge enables competitive performance without any task-specific fine-tuning, as this is not thoroughly investigated or explained.

## Next Checks

1. Conduct a user study to directly assess the explainability and scrutability of language-based versus item-based preference representations, measuring users' understanding and ability to control personalization.
2. Perform ablation studies on the LLM prompts to determine which components (e.g., prompt structure, item name formatting, temperature settings) have the largest impact on recommendation performance.
3. Investigate the semantic relationships captured by the LLM's pre-trained knowledge by analyzing the model's attention patterns and representations for language descriptions and items, to better understand how it enables competitive performance without task-specific training.