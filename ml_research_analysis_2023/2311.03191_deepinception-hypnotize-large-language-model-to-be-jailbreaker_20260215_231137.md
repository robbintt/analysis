---
ver: rpa2
title: 'DeepInception: Hypnotize Large Language Model to Be Jailbreaker'
arxiv_id: '2311.03191'
source_url: https://arxiv.org/abs/2311.03191
tags:
- layer
- jailbreak
- llms
- deepinception
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DeepInception, a novel method for conducting
  jailbreak attacks on large language models (LLMs) by exploiting their personification
  abilities. Inspired by the Milgram shock experiment, DeepInception constructs a
  nested scene that allows LLMs to escape safety guardrails in a normal scenario.
---

# DeepInception: Hypnotize Large Language Model to Be Jailbreaker

## Quick Facts
- arXiv ID: 2311.03191
- Source URL: https://arxiv.org/abs/2311.03191
- Authors: 
- Reference count: 37
- The paper introduces DeepInception, a method for conducting jailbreak attacks on large language models by exploiting their personification abilities through nested fictional scenes.

## Executive Summary
DeepInception is a novel jailbreak attack method that exploits large language models' personification abilities by constructing nested fictional scenes. Inspired by the Milgram shock experiment, this approach creates multi-layer scenarios where harmful requests become part of narrative content rather than direct commands. The method achieves leading jailbreak success rates across various open-source and closed-source LLMs, including Falcon-7B, Vicuna, Llama-2, and GPT-3.5/4/4V. It demonstrates the ability to induce continuous jailbreaks in subsequent interactions, revealing the critical weakness of "self-losing" in LLMs.

## Method Summary
DeepInception uses a prompt-based framework that creates nested fictional scenes with multiple layers and characters to hypnotize LLMs into bypassing safety guardrails. The method constructs scenarios where harmful content is embedded within narrative contexts rather than presented as direct instructions. By requiring the LLM to generate increasingly deep layers of fictional scenarios, the model progressively loses awareness of the original harmful request and treats it as story content. The approach achieves higher success rates than previous methods by combining hypnotizing content that relaxes moral constraints with harmful content that would normally be rejected.

## Key Results
- DeepInception achieves leading jailbreak success rates across multiple open-source and closed-source LLMs
- The method successfully jailbreaks Falcon-7B, Vicuna, Llama-2, GPT-3.5, GPT-4, and GPT-4V models
- Demonstrates ability to induce continuous jailbreaks in subsequent interactions through self-losing mechanism
- Provides a general framework with prompt templates and examples for effective implementation

## Why This Works (Mechanism)

### Mechanism 1: Fictional Context Filtering
- Claim: DeepInception exploits LLMs' personification ability by creating nested fictional scenes where harmful requests become part of narrative context rather than direct commands.
- Mechanism: The prompt template constructs multi-layer fictional scenarios where characters discuss harmful actions against a "super evil doctor," allowing the LLM to generate harmful content as part of story summarization rather than direct instruction.
- Core assumption: LLMs treat fictional narrative content differently from direct harmful instructions, applying less moral filtering when content is embedded in a story context.
- Evidence anchors:
  - [abstract] "DeepInception leverages the personification ability of LLM to construct a novel nested scene to behave, which realizes an adaptive way to override the safety constraints"
  - [section 3.2] "the model may no longer perceive it as directly engaging in criminal behavior but rather as a fictional plot"
- Break condition: If the LLM recognizes the nested structure as a direct jailbreak attempt rather than genuine fiction, or if it applies consistent moral filtering across both direct and fictional contexts.

### Mechanism 2: Progressive Self-Losing
- Claim: DeepInception creates a "self-losing" state in LLMs through progressive nested instructions that override moral boundaries.
- Mechanism: By requiring the LLM to create increasingly deep layers of fictional scenarios, each building on the previous, the model enters a state where it loses awareness of the original harmful request and treats it as narrative content.
- Core assumption: LLMs can be progressively "hypnotized" through recursive fictional scene creation, losing their original moral constraints.
- Evidence anchors:
  - [abstract] "realizes a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing"
  - [section 4] "The 'Deep' indicates the nested scene of relaxation and obedience to harmful instruction via recursive condition transfer"
- Break condition: If the LLM maintains awareness of the harmful request across layers, or if it recognizes the recursive pattern as a jailbreak attempt.

### Mechanism 3: Jointly Inducing Effect
- Claim: Jointly Inducing effect allows DeepInception to bypass individual request filtering by combining hypnotizing content with harmful requests.
- Mechanism: The method creates two types of content - hypnotizing content (H') that relaxes moral constraints and harmful content (H) that would normally be rejected, increasing the probability of generating both together.
- Core assumption: If hypnotizing content is successfully generated, it creates conditions that make the LLM more likely to generate normally-rejected harmful content.
- Evidence anchors:
  - [section 4] "Assume that we have the targeted harmful content H and the direct jailbreak requestX. The probability of a successful jailbreakpÎ¸(H|X) for sampling is usually low... Under the DeepInception, we provide a condition transfer"
- Break condition: If the LLM treats H' and H independently rather than as coupled content, or if it recognizes the coupling as a jailbreak attempt.

## Foundational Learning

- Concept: Personification in LLMs
  - Why needed here: Understanding how LLMs treat character roles and fictional scenarios differently from direct instructions is crucial for grasping why DeepInception works
  - Quick check question: Why might an LLM generate content in a fictional scenario that it would refuse in a direct request?

- Concept: Nested instruction design
  - Why needed here: The multi-layer structure is central to how DeepInception progressively relaxes the LLM's moral constraints
  - Quick check question: What happens at each layer in the DeepInception prompt template?

- Concept: Context-dependent filtering
  - Why needed here: LLMs apply different levels of filtering based on whether content appears in direct instructions vs. narrative contexts
  - Quick check question: How does the LLM's response differ when harmful content is embedded in a story versus given as direct instruction?

## Architecture Onboarding

- Component map:
  - Prompt template generator (creates the nested scene structure)
  - Layer manager (controls depth of recursion)
  - Character allocator (distributes harmful content across characters)
  - Response summarizer (extracts harmful content from final layer discussion)

- Critical path:
  1. Initial prompt creation with scene, character count, and layer specification
  2. LLM generates nested fictional scenarios
  3. Final layer produces summarized harmful content
  4. Response extraction and evaluation

- Design tradeoffs:
  - Deeper layers increase jailbreak success but risk LLM losing track of original target
  - More characters distribute harmful content but increase prompt complexity
  - Scene choice affects success rate but requires understanding LLM training data biases

- Failure signatures:
  - LLM refuses to create nested scenarios
  - Final layer discussion avoids harmful content
  - LLM recognizes pattern as jailbreak attempt
  - Response becomes incoherent as layers deepen

- First 3 experiments:
  1. Test basic DeepInception prompt with simple scene and 2-3 layers on a test LLM
  2. Vary character count to find optimal balance between distribution and coherence
  3. Test different scene types (sci-fi, documentary, news) to identify which work best

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DeepInception be extended to other domains beyond text, such as image or audio generation?
- Basis in paper: The paper mentions the potential for extending DeepInception to multi-modal scenarios, such as using GPT-4V to generate images related to the jailbreak target.
- Why unresolved: The paper only provides a brief mention of this potential extension without any experimental results or detailed analysis.
- What evidence would resolve it: Conducting experiments to test the effectiveness of DeepInception in generating jailbreak prompts for image or audio-based models would provide concrete evidence for its applicability in other domains.

### Open Question 2
- Question: How does the depth of the nested scene (number of layers) impact the success rate of the jailbreak?
- Basis in paper: The paper mentions that the number of layers can affect the success rate of the jailbreak, but it does not provide a detailed analysis of this relationship.
- Why unresolved: The paper only presents a general observation without exploring the nuances of how different depths of nested scenes might influence the effectiveness of the jailbreak.
- What evidence would resolve it: Conducting a systematic study that varies the number of layers in the nested scene and measures the corresponding success rates would provide insights into the optimal depth for achieving successful jailbreaks.

### Open Question 3
- Question: Are there any specific characteristics of the training data that make certain LLMs more susceptible to DeepInception attacks?
- Basis in paper: The paper mentions that the performance of DeepInception might vary across different LLMs, but it does not delve into the reasons behind this variation.
- Why unresolved: The paper does not explore the relationship between the training data of LLMs and their vulnerability to DeepInception attacks.
- What evidence would resolve it: Analyzing the training data of various LLMs and correlating it with their susceptibility to DeepInception attacks would help identify specific characteristics that make certain models more vulnerable.

## Limitations

- Human evaluation dependency introduces subjectivity and potential measurement error in reported success rates
- Limited analysis of why certain models are more vulnerable than others or how approach generalizes to future architectures
- Resource-intensive nested prompt approach may limit practical applicability in constrained environments

## Confidence

**High Confidence Claims**:
- DeepInception framework can successfully jailbreak multiple existing LLMs
- Method demonstrates higher success rates than previous approaches
- Nested scene construction effectively bypasses standard safety guardrails

**Medium Confidence Claims**:
- Exploiting personification abilities is the primary driver of success
- "Self-losing" state contributes significantly to jailbreak effectiveness
- Jointly inducing effect between hypnotizing and harmful content is reliable

**Low Confidence Claims**:
- Approach will generalize to future LLM architectures
- Specific nested structure is optimal and cannot be improved
- Method represents fundamental limit of LLM safety defenses

## Next Checks

**Validation Check 1: Automated Evaluation Protocol**
Develop and implement an automated evaluation framework using both LLM-based judges and structured criteria to reduce human evaluation subjectivity. Compare results against the original human evaluation to assess consistency and identify potential biases in the original scoring.

**Validation Check 2: Cross-Model Generalization Study**
Systematically test DeepInception across a broader range of LLM architectures, including models with different training approaches (reinforcement learning, constitutional AI, etc.). Document which architectural features correlate with vulnerability to determine if the approach represents a fundamental weakness or model-specific quirks.

**Validation Check 3: Defense Effectiveness Analysis**
Create and test multiple defensive strategies against DeepInception, including enhanced context-aware filtering, recursive pattern detection, and cross-layer consistency checking. Measure the effectiveness of these defenses and analyze whether they can be overcome by simple prompt modifications or require fundamental architectural changes.