---
ver: rpa2
title: 'HateRephrase: Zero- and Few-Shot Reduction of Hate Intensity in Online Posts
  using Large Language Models'
arxiv_id: '2310.13985'
source_url: https://arxiv.org/abs/2310.13985
tags:
- hate
- speech
- text
- intensity
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the use of large language models (LLMs)
  to reduce the intensity of hate speech in online posts by generating alternative
  rephrasings that preserve semantic meaning. The authors experiment with various
  open-source LLMs (LLaMA-1, LLaMA-2-chat, Vicuna) and OpenAI''s GPT-3.5 using four
  different prompting strategies: task description, task definition, few-shot demonstrations,
  and chain-of-thought.'
---

# HateRephrase: Zero- and Few-Shot Reduction of Hate Intensity in Online Posts using Large Language Models

## Quick Facts
- arXiv ID: 2310.13985
- Source URL: https://arxiv.org/abs/2310.13985
- Authors: 
- Reference count: 40
- Key outcome: GPT-3.5 with few-shot demonstrations outperforms open-source LLMs and human-generated ground truth in reducing hate speech intensity while preserving semantic meaning

## Executive Summary
This paper investigates using large language models (LLMs) to rephrase hate speech into less hateful text while preserving semantic meaning. The authors evaluate multiple open-source LLMs (LLaMA-1, LLaMA-2-chat, Vicuna) and GPT-3.5 using four prompting strategies across a dataset of 3,027 hateful posts. Results show GPT-3.5 with few-shot demonstrations achieves the best performance, outperforming both baseline models and human-generated ground truth in human evaluations. The study demonstrates that LLMs can effectively reduce hate intensity while maintaining semantic similarity, offering a promising approach for content moderation.

## Method Summary
The method involves using large language models to rephrase hate speech text while preserving semantic meaning. The approach uses zero- and few-shot prompting with four prompt types (task description, task definition, few-shot demonstrations, and chain-of-thought) across multiple LLM models. The evaluation employs automatic metrics including BLEU, perplexity, cosine similarity, hate intensity reduction (HIR), style transfer accuracy (STA), and fluency scores, combined with human annotations. The dataset consists of 3,027 hateful posts from Masud et al. [21] with corresponding human-generated normalized versions and intensity scores. GPT-3.5 with few-shot demonstrations emerged as the best-performing configuration.

## Key Results
- GPT-3.5 with few-shot demonstrations outperforms all open-source LLMs and the BART-Detox baseline across all metrics
- GPT-3.5-generated rephrasings outperform human-generated ground truth in human evaluations
- Few-shot demonstrations prompt strategy achieves the highest hate intensity reduction while maintaining semantic similarity
- Open-source models (LLaMA-1, LLaMA-2-chat, Vicuna) show significantly lower performance compared to GPT-3.5

## Why This Works (Mechanism)

### Mechanism 1
Few-shot demonstrations outperform zero-shot task description prompts for hate speech rephrasing by helping the model understand the desired transformation pattern and balance between reducing hate intensity and preserving semantic meaning. The model can generalize from provided examples to unseen hate speech instances, though performance depends on the representativeness of demonstration examples.

### Mechanism 2
GPT-3.5 outperforms open-source LLMs due to its larger scale and training on more diverse data, enabling better understanding of nuanced hate speech patterns and generation of semantically similar yet less hateful text. The model's pretraining data and architecture allow it to capture subtle hate speech patterns, though performance may be overestimated if the evaluation dataset lacks diversity.

### Mechanism 3
Human evaluators prefer GPT-3.5-generated rephrasings over human-generated ground truth because the model can generate text that is both less hateful and semantically similar while avoiding potential biases or inconsistencies in human annotations. The evaluation metrics (HIR, relevance, hallucination) are assumed to accurately capture quality from a human perspective, though evaluator diversity and inherent biases could affect results.

## Foundational Learning

- **Prompt engineering and few-shot learning**: Different prompt types significantly impact model performance in hate speech rephrasing; understanding the differences between zero-shot and few-shot prompting helps explain the model's ability to generalize from examples.

- **Evaluation metrics for text generation**: Various metrics (BLEU, perplexity, cosine similarity, HIR, STA, fluency, hybrid score) are used to assess rephrased text quality; understanding reference-based versus reference-free metrics helps evaluate their respective strengths and limitations.

- **Hate speech detection and mitigation techniques**: Understanding the broader context of hate speech detection, counterspeech generation, and detoxification helps appreciate the novelty and potential impact of the hate speech rephrasing approach; recognizing challenges in converting hate speech while preserving semantic meaning provides context for the approach.

## Architecture Onboarding

- **Component map**: Input (hate speech text) -> Processing (LLMs with different prompt types) -> Output (hate-rephrased text) -> Evaluation (automatic metrics and human annotations)

- **Critical path**: Load hate speech dataset → Generate rephrasings using LLMs with different prompt types → Calculate automatic evaluation metrics → Perform human annotations on subset → Analyze results and conduct case studies

- **Design tradeoffs**: Model choice (open-source LLMs vs GPT-3.5) trades cost/accessibility against performance; prompt design trades simplicity against effectiveness (task description vs few-shot demonstrations); evaluation metrics balance comprehensiveness against computational efficiency

- **Failure signatures**: Low HIR scores indicate failure to reduce hate intensity; low cosine similarity indicates significant semantic meaning changes; high hallucination scores indicate addition of information not present in original text

- **First 3 experiments**: 1) Generate rephrasings using GPT-3.5 with few-shot demonstrations and calculate all automatic metrics; 2) Generate rephrasings using LLaMA-2-chat with chain-of-thought prompt and compare with GPT-3.5; 3) Perform human annotations on subset of GPT-3.5 and LLaMA-2-chat instances to validate automatic evaluation results

## Open Questions the Paper Calls Out

- **Open Question 1**: How does fine-tuning large language models on task-specific instructions affect hate speech rephrasing performance compared to zero-shot prompting? The paper identifies this as a potential future direction, but only evaluates zero-shot and few-shot prompting without any model fine-tuning.

- **Open Question 2**: What is the optimal balance between hate intensity reduction and semantic meaning preservation that would be most effective in real-world content moderation? The paper proposes a hybrid score but uses a threshold based on empirical observation rather than theoretical justification or user studies.

- **Open Question 3**: How do soft-prompting and prefix-tuning techniques compare to standard prompting approaches for hate speech rephrasing tasks? The paper mentions these as potential improvement methods but only experiments with standard prompting techniques.

## Limitations

- Limited generalization due to evaluation on a single dataset of 3,027 instances that may not capture full diversity of hate speech patterns across different platforms and communities
- Metric reliability concerns including known limitations of BLEU scores for semantic similarity and potential misalignment between Perspective API scores and human judgments
- Prompt template specificity issues where few-shot demonstrations show superior performance but the specific examples used could significantly impact results
- Human evaluation sample size concerns with only 500 instances evaluated and un-discussed evaluator demographic composition and potential biases

## Confidence

- **High Confidence**: GPT-3.5 outperforms open-source LLMs across all prompt types
- **Medium Confidence**: Few-shot demonstrations outperform other prompt types
- **Low Confidence**: GPT-3.5-generated rephrasings outperform human-generated ground truth

## Next Checks

1. **Cross-dataset validation**: Test GPT-3.5 with few-shot demonstrations on additional hate speech datasets from different sources to assess generalization capability and identify performance degradation patterns.

2. **Prompt template ablation study**: Systematically vary few-shot demonstration examples using different hate speech categories, intensity levels, and semantic structures to identify which characteristics contribute most to model performance.

3. **Human evaluator diversity assessment**: Conduct human evaluations with diverse demographic backgrounds and compare judgments against original results to identify potential bias or cultural variations in hate speech perception.