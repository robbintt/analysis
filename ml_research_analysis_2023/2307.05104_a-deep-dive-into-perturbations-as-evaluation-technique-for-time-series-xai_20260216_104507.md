---
ver: rpa2
title: A Deep Dive into Perturbations as Evaluation Technique for Time Series XAI
arxiv_id: '2307.05104'
source_url: https://arxiv.org/abs/2307.05104
tags:
- time
- perturbation
- attributions
- series
- techniques
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates evaluation of explainable AI (XAI) methods
  for time series classification using perturbation analysis. The authors systematically
  modify input time series data based on attribution explanations and measure impact
  on model predictions.
---

# A Deep Dive into Perturbations as Evaluation Technique for Time Series XAI

## Quick Facts
- arXiv ID: 2307.05104
- Source URL: https://arxiv.org/abs/2307.05104
- Reference count: 30
- Key outcome: KernelSHAP provides best attribution quality for time series classification but requires high computational cost

## Executive Summary
This paper investigates evaluation of explainable AI (XAI) methods for time series classification using perturbation analysis. The authors systematically modify input time series data based on attribution explanations and measure impact on model predictions across three datasets. KernelSHAP emerges as the top performer overall, though Saliency offers a faster alternative with good results. The study reveals that perturbation strategy choice significantly affects attribution method performance and identifies shortcuts in models that rely on limited features. The authors introduce perturbation analysis cards to report results systematically and provide an open-source framework for evaluating XAI techniques on time series data.

## Method Summary
The evaluation framework trains CNN models on three time series classification datasets (FordA, FordB, ElectricDevices) and applies seven attribution techniques (Saliency, IntegratedGradients, DeepLift, Occlusion, GradientSHAP, DeepLiftSHAP, KernelSHAP) with 16 different perturbation strategies. For each attribution method, the approach generates importance scores, modifies input data points based on these scores using various strategies (zeroing values, replacing with mean, out-of-distribution values, etc.), and measures how prediction accuracy degrades. The framework evaluates attribution quality through prediction change rates, Euclidean and cosine distances between original and perturbed time series, and skewness analysis of attribution distributions.

## Key Results
- KernelSHAP achieves the best overall attribution quality but requires 3-4 orders of magnitude more computation time than gradient-based methods
- Saliency provides a faster alternative with comparable performance, making it suitable for real-world applications
- Perturbation strategy choice significantly impacts which attribution method appears superior, revealing different aspects of model behavior
- Attribution skewness distributions can identify shortcuts in models that rely on spurious correlations rather than genuine patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perturbation analysis effectively evaluates attribution quality by systematically measuring prediction changes when input features are modified according to attribution scores.
- Mechanism: The approach modifies time series data points based on attribution importance scores and measures how prediction accuracy degrades. Higher quality attributions cause more severe prediction degradation when perturbed, indicating they identified truly relevant features.
- Core assumption: The model relies on the identified features for correct predictions, and perturbing these features will significantly impact model performance.
- Evidence anchors:
  - [abstract] "A perturbation analysis involves systematically modifying the input data and evaluating the impact on the attributions generated by the XAI method."
  - [section] "We assume that the qm decreases after the original data changes, and thus the labels are not fitting anymore."
- Break condition: If the model uses alternative features not captured by attributions, or if the model relies on spurious correlations that remain stable under perturbation.

### Mechanism 2
- Claim: Different perturbation strategies reveal different aspects of attribution quality and model behavior.
- Mechanism: Using multiple perturbation strategies (zeroing values, replacing with mean, out-of-distribution values, etc.) provides a comprehensive evaluation because different strategies test different properties of the attributions and the model's robustness.
- Core assumption: The choice of perturbation strategy significantly affects which attribution method appears best, revealing different model dependencies.
- Evidence anchors:
  - [abstract] "The analysis reveals that perturbation strategy choice significantly affects attribution method performance"
  - [section] "However, again the KernelSHAP and Saliency generate good working attributions for the change in the prediction for the perturbation to zero strategy."
- Break condition: If all perturbation strategies yield similar results, suggesting the attribution quality is not sensitive to perturbation type.

### Mechanism 3
- Claim: Attribution skewness distributions reveal shortcuts and spurious correlations in the model.
- Mechanism: By analyzing the skewness of attribution distributions for changed vs unchanged predictions, researchers can identify whether the model learned to rely on specific patterns or features that may not generalize.
- Core assumption: Models with shortcuts will show distinct attribution patterns for correctly classified vs misclassified samples after perturbation.
- Evidence anchors:
  - [abstract] "The authors introduce perturbation analysis cards to report results systematically"
  - [section] "We notice that the perturbation strategy heavily influences the best working method"
- Break condition: If skewness distributions show no meaningful differences between changed and unchanged predictions, suggesting attributions don't capture model behavior differences.

## Foundational Learning

- Concept: Time series classification fundamentals
  - Why needed here: Understanding how time series data is structured and classified is essential for interpreting attribution results and perturbation effects
  - Quick check question: What distinguishes time series classification from other ML tasks, and why does the sequential nature matter for XAI?

- Concept: Attribution methods for deep learning
  - Why needed here: The paper evaluates multiple attribution techniques (Saliency, SHAP variants, etc.) and their effectiveness on time series data
  - Quick check question: How do gradient-based attribution methods differ from perturbation-based methods like SHAP in their approach to identifying feature importance?

- Concept: Perturbation analysis methodology
  - Why needed here: The core evaluation technique involves systematically modifying input data based on attributions and measuring impact
  - Quick check question: What is the relationship between attribution scores and perturbation thresholds in determining which data points to modify?

## Architecture Onboarding

- Component map:
  Input data -> CNN model -> Attribution technique -> Perturbation strategy -> Evaluation metrics -> Analysis

- Critical path:
  1. Train CNN model on time series classification task
  2. Generate attributions for training data using selected XAI method
  3. Apply perturbation strategy to modify data points based on attribution scores
  4. Measure prediction changes and calculate evaluation metrics
  5. Analyze attribution distributions and patterns

- Design tradeoffs:
  - Computational cost vs attribution quality (KernelSHAP provides best results but is slow)
  - Perturbation strategy choice affects which attribution method appears superior
  - Single vs multiple attribution techniques for comprehensive evaluation

- Failure signatures:
  - Model accuracy remains high after perturbation suggests attributions didn't identify critical features
  - Attribution distributions show no difference between changed and unchanged predictions
  - Perturbation analysis shows no meaningful differences across different attribution methods

- First 3 experiments:
  1. Run perturbation analysis with Saliency and KernelSHAP on FordA using zero perturbation strategy to establish baseline comparison
  2. Compare attribution skewness distributions between changed and unchanged predictions for each technique
  3. Test multiple perturbation strategies (zero, OOD low, global mean) on same attribution method to observe strategy sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a fully automated measure to evaluate attributions on time series classification models that combines perturbation strategies, prediction changes, and distance metrics?
- Basis in paper: Explicit - Future work section mentions wanting to extend the Perturbation Effect Size metric and use the collected data to generate a measure to evaluate attributions more robustly and fully automatically
- Why unresolved: Current perturbation analysis requires manual inspection of multiple cards and statistics, making it difficult for non-experts to interpret results
- What evidence would resolve it: A validated automated metric that correlates well with human judgment of attribution quality across multiple time series datasets

### Open Question 2
- Question: What are the optimal window sizes for subsequence perturbation strategies across different time series classification datasets and architectures?
- Basis in paper: Explicit - Future work section mentions extending experiments to other datasets and architectures, and the current study used a fixed 10% window size
- Why unresolved: The paper used a fixed subsequence length of 10% without exploring the impact of different window sizes on attribution quality
- What evidence would resolve it: Systematic experiments showing how attribution performance varies with different window sizes across diverse time series datasets

### Open Question 3
- Question: How can we effectively visualize and compare attribution distributions across different techniques to identify consistent patterns and spurious correlations?
- Basis in paper: Explicit - Discussion mentions wanting to include local Lipschitz estimates to rank consistent attributions higher and extend perturbation analysis cards for easier readability
- Why unresolved: Current visualizations show individual distributions but lack methods for comparing patterns across techniques and identifying spurious correlations
- What evidence would resolve it: Visualization techniques that reveal consistent attribution patterns across methods while highlighting spurious correlations in a single interpretable view

## Limitations
- Computational cost of KernelSHAP is prohibitive (3-4 orders of magnitude slower than gradient-based methods)
- Perturbation strategy choice significantly affects which attribution method appears superior, limiting generalizability
- Assumption that accuracy drop correlates with attribution quality may not hold for models using alternative features or spurious correlations

## Confidence
The perturbation-based evaluation framework shows promise for assessing XAI attribution quality, but several limitations affect confidence in the results. The computational cost of KernelSHAP (3-4 orders of magnitude slower than gradient-based methods) creates practical constraints for real-world deployment. The choice of perturbation strategy significantly impacts which attribution method appears superior, suggesting results may be strategy-dependent rather than universally applicable. Additionally, the assumption that model accuracy drop directly correlates with attribution quality may not hold for models using alternative features or spurious correlations.

Confidence in the core findings is **Medium**. While the systematic approach and multiple datasets strengthen the analysis, the heavy dependence on perturbation strategy choice and computational constraints of the best-performing method limit generalizability. The framework's effectiveness is well-demonstrated, but optimal implementation details remain uncertain.

## Next Checks
1. **Strategy Independence Test**: Run the perturbation analysis using only a subset of strategies (e.g., zero and OOD high) to determine if core conclusions about attribution quality remain consistent across reduced strategy sets.

2. **Alternative Model Architecture**: Validate results using different CNN architectures or alternative time series models (LSTM, Transformer) to assess whether attribution quality rankings remain stable across model types.

3. **Spurious Correlation Analysis**: Design experiments with intentionally introduced spurious correlations in training data to test whether the perturbation framework can identify and expose these shortcuts in model behavior.