---
ver: rpa2
title: Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay
  in Education
arxiv_id: '2307.12267'
source_url: https://arxiv.org/abs/2307.12267
tags:
- text
- hybrid
- detection
- essay
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the task of detecting the boundaries between
  human-written and AI-generated content in hybrid essays. It introduces a two-step
  approach: (1) fine-tune a sentence encoder using a triplet network to separate human-written
  from AI-generated content, and (2) calculate distances between adjacent prototypes
  (averaged embeddings of consecutive sentences) and assume boundaries exist between
  the most dissimilar adjacent prototypes.'
---

# Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education

## Quick Facts
- arXiv ID: 2307.12267
- Source URL: https://arxiv.org/abs/2307.12267
- Reference count: 10
- One-line primary result: Proposed method improves boundary detection for hybrid essays using triplet network fine-tuning and prototype-based distance calculation, achieving up to 22% improvement in in-domain and 18% in out-of-domain settings.

## Executive Summary
This paper addresses the challenge of detecting boundaries between human-written and AI-generated content in hybrid essays. The authors propose a two-step approach: first, fine-tuning a sentence encoder using a triplet network to separate human and AI content embeddings; second, calculating distances between adjacent prototypes (averaged embeddings of consecutive sentences) and assuming boundaries exist between the most dissimilar adjacent prototypes. Experiments on a hybrid essay dataset constructed from student essays and ChatGPT-generated text show consistent improvements over baseline methods across in-domain and out-of-domain settings.

## Method Summary
The method consists of two main components: (1) fine-tuning a pre-trained sentence encoder using a triplet network architecture, where anchor sentences are pulled closer to same-authorship sentences and pushed away from different-authorship sentences; (2) detecting boundaries by calculating distances between adjacent prototypes, where each prototype is the average embedding of a block of consecutive sentences. The method tests different prototype sizes and evaluates performance using F1 score at top-K boundaries (K=3).

## Key Results
- Proposed method consistently outperforms baseline methods across in-domain and out-of-domain settings
- For single-boundary essays, the proposed approach with larger prototype size achieves up to 22% improvement in in-domain and 18% in out-of-domain evaluation over best baseline
- Prototype size p=2 shows optimal performance overall, though optimal size varies with number of boundaries (p=4 for 1 boundary, p=2 for 2 boundaries, p=1 for 3 boundaries)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Triplet network fine-tuning helps the encoder learn embeddings that separate human-written and AI-generated sentences into distinct clusters.
- Mechanism: During triplet training, each anchor sentence's embedding is pulled closer to embeddings of same-authorship sentences (positive) and pushed away from embeddings of different-authorship sentences (negative). This creates a structured embedding space where human and AI content form separable regions.
- Core assumption: Authorship differences are consistently reflected in sentence-level features that can be captured by the encoder.
- Evidence anchors:
  - [abstract] "fine-tune a sentence encoder using a triplet network to separate human-written from AI-generated content"
  - [section] "we first adopt the pre-trained sentence encoder... We then fine-tune the initial encoder with the triplet Bert-networks architecture... for a sentence triplet (a, x+, x−), where a is the anchor sentence whose label is the same as that of the positive sentence x+, but different from the label of the negative sentence x−"
  - [corpus] Weak. No corpus evidence directly linking triplet separation to improved boundary detection.
- Break condition: If authorship differences are subtle or context-dependent, the triplet separation may not create sufficiently distinct clusters, leading to ambiguous boundaries.

### Mechanism 2
- Claim: Calculating prototype-based distances between adjacent sentence blocks effectively highlights authorship transitions.
- Mechanism: By averaging embeddings over consecutive sentences (prototypes), the method reduces noise from individual sentence variations and emphasizes overall authorship style shifts. Boundaries are assumed where prototype dissimilarity peaks.
- Core assumption: Authorship style is consistent within blocks of consecutive sentences and changes abruptly at transitions.
- Evidence anchors:
  - [abstract] "calculate distances between adjacent prototypes... and assume boundaries exist between the most dissimilar adjacent prototypes"
  - [section] "Let Sp− i be the averaged embeddings... of sentence si and its (p − 1) preceding sentences... To identify the possible boundaries, we first calculate the distances between every two adjacent prototypes... and assumed that the boundaries exist between the two prototypes that have the furthest distance from each other"
  - [corpus] Weak. Corpus does not provide evidence that prototype-based distance is superior to other methods for boundary detection.
- Break condition: If authorship shifts gradually or if sentence blocks contain mixed content, prototype averaging may obscure transitions or create false positives.

### Mechanism 3
- Claim: Prototype size p = 2 optimally balances noise reduction and boundary sensitivity.
- Mechanism: Small p reduces noise but may be too sensitive to individual sentence variation; large p may smooth over actual transitions. p = 2 provides the best tradeoff in the tested conditions.
- Core assumption: Optimal prototype size is stable across different essay types and boundary patterns.
- Evidence anchors:
  - [abstract] "the proposed method with a larger prototype size achieves up to 22% improvement... When detecting boundaries for single-boundary hybrid essays, the proposed approach could be enhanced by adopting a relatively large prototype size"
  - [section] "From the results of the overall level... we observed that TriBert achieved the best performance with prototype size p = 2" and "the best p for #Bry = 1 , 2, 3 are 4, 2, and 1, respectively"
  - [corpus] Weak. No corpus evidence showing why p = 2 generalizes or whether this holds for more complex boundary patterns.
- Break condition: If essays have longer or more complex transitions, the optimal p may shift, and the method's fixed-size prototype may miss boundaries.

## Foundational Learning

- Concept: Triplet network training
  - Why needed here: It provides a principled way to learn discriminative embeddings for authorship classification without requiring pairwise supervision.
  - Quick check question: What are the roles of anchor, positive, and negative samples in a triplet loss?

- Concept: Prototype averaging
  - Why needed here: It smooths out sentence-level noise and captures stylistic consistency over short spans, making boundary detection more robust.
  - Quick check question: How does averaging embeddings over consecutive sentences affect the signal-to-noise ratio for authorship detection?

- Concept: Distance-based boundary assumption
  - Why needed here: It provides a simple, interpretable heuristic for locating transitions without complex sequence modeling.
  - Quick check question: Why might the maximum prototype distance indicate a boundary rather than a local minimum?

## Architecture Onboarding

- Component map:
  Input: Hybrid essay text (list of sentences) -> Encoder: SentenceTransformer -> embedding -> Triplet fine-tuning module (optional) -> Prototype generator: averages consecutive embeddings -> Distance calculator: pairwise prototype distances -> Boundary detector: selects indices with max distance -> Output: Ranked list of boundary sentence indices

- Critical path:
  1. Fine-tune encoder (if using TriBert)
  2. Embed each sentence
  3. Compute prototypes of size p
  4. Calculate adjacent prototype distances
  5. Identify top-K largest distances as boundaries

- Design tradeoffs:
  - Larger p → more noise smoothing but risk of missing short transitions
  - Smaller p → sensitive to noise but better for detecting quick shifts
  - Encoder fine-tuning → better separation but requires labeled triplets
  - Pre-trained only → faster but less adapted to dataset

- Failure signatures:
  - Low precision: false positives from stylistic shifts unrelated to authorship
  - Low recall: missing boundaries due to prototype averaging
  - Degraded performance on longer essays: prototype size mismatch
  - Performance drop in out-of-domain: encoder not adapted to new styles

- First 3 experiments:
  1. Baseline: Use SentenceTransformer embeddings without fine-tuning, p = 1, measure F1@3 on validation set
  2. Triplet fine-tuning: Train on authorship-labeled triplets, keep p = 1, compare F1@3 to baseline
  3. Prototype size sweep: Fix fine-tuned encoder, test p ∈ {1,2,3,4}, plot F1@3 vs p for different #boundaries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the boundary detection performance change if we used a more sophisticated sentence encoder architecture instead of the pre-trained SentenceTransformers model?
- Basis in paper: [explicit] The paper mentions using a pre-trained sentence encoder from SentenceTransformers as the initial encoder, but does not explore alternative encoder architectures or their impact on performance.
- Why unresolved: The paper only evaluates the proposed approach using the pre-trained SentenceTransformers encoder, leaving open the question of how other encoder architectures might perform.
- What evidence would resolve it: Experiments comparing the proposed approach's performance using different encoder architectures (e.g., BERT, RoBERTa, or other transformer-based models) would provide insights into the impact of encoder choice on boundary detection performance.

### Open Question 2
- Question: How would the boundary detection performance change if we incorporated additional features beyond sentence embeddings, such as syntactic or semantic features?
- Basis in paper: [inferred] The paper focuses solely on sentence embeddings for boundary detection and does not explore the potential benefits of incorporating additional linguistic features.
- Why unresolved: The paper's approach relies solely on sentence embeddings, leaving open the question of whether incorporating other linguistic features could improve boundary detection performance.
- What evidence would resolve it: Experiments incorporating additional linguistic features (e.g., part-of-speech tags, dependency parse trees, or semantic role labels) into the boundary detection process would provide insights into the potential benefits of using such features.

### Open Question 3
- Question: How would the boundary detection performance change if we used a different similarity metric instead of Euclidean distance to measure the dissimilarity between adjacent prototypes?
- Basis in paper: [explicit] The paper uses Euclidean distance as the similarity metric to measure the dissimilarity between adjacent prototypes, but does not explore the impact of using other similarity metrics.
- Why unresolved: The paper's approach relies on Euclidean distance, leaving open the question of whether other similarity metrics (e.g., cosine similarity, Jaccard similarity, or learned similarity metrics) could lead to improved boundary detection performance.
- What evidence would resolve it: Experiments comparing the proposed approach's performance using different similarity metrics to measure prototype dissimilarity would provide insights into the impact of similarity metric choice on boundary detection performance.

## Limitations
- The study relies on a synthetically constructed dataset where AI-generated text is inserted into human essays, which may not fully capture the complexity of real-world hybrid writing scenarios.
- The assumption that authorship transitions are abrupt and consistent within short spans (prototype size) may not hold for more nuanced or gradual style shifts.
- The optimal prototype size appears to vary with the number of boundaries, but the method does not dynamically adapt to this during inference.

## Confidence
- **High confidence**: The two-step framework (encoder fine-tuning + prototype-based boundary detection) is clearly defined and its implementation is reproducible.
- **Medium confidence**: The reported improvements over baselines are significant, but the evaluation is limited to a single synthetic dataset and may not generalize to real-world essays.
- **Low confidence**: The claim that prototype size p=2 is universally optimal is not strongly supported; results show p varies by boundary count, and no adaptive strategy is proposed.

## Next Checks
1. Test the method on real student essays with suspected AI involvement (rather than synthetic hybrids) to assess ecological validity.
2. Evaluate whether the proposed method can adapt prototype size dynamically based on predicted boundary count, rather than using a fixed p.
3. Compare performance when using alternative distance metrics (e.g., cosine similarity, Euclidean distance) between prototypes to determine robustness to embedding space choice.