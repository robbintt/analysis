---
ver: rpa2
title: Combining propensity score methods with variational autoencoders for generating
  synthetic data in presence of latent sub-groups
arxiv_id: '2312.07781'
source_url: https://arxiv.org/abs/2312.07781
tags:
- data
- synthetic
- latent
- sub-groups
- propensity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a method to combine variational autoencoders
  (VAEs) with propensity score modeling for generating synthetic clinical cohort data
  while preserving heterogeneity due to known or unknown subgroups. The method uses
  pre-transformations to address unknown subgroup structures reflected in marginal
  distributions (e.g., skewness, bimodality) and incorporates propensity scores for
  known subgroups.
---

# Combining propensity score methods with variational autoencoders for generating synthetic data in presence of latent sub-groups

## Quick Facts
- arXiv ID: 2312.07781
- Source URL: https://arxiv.org/abs/2312.07781
- Authors: 
- Reference count: 40
- Primary result: Method combines VAEs with propensity score modeling to generate synthetic clinical data preserving known/unknown subgroup heterogeneity

## Executive Summary
This paper addresses the challenge of generating synthetic clinical cohort data while preserving heterogeneity due to known or unknown subgroups. The authors propose a method that combines variational autoencoders with pre-transformations for unknown subgroup structures and propensity score modeling for known subgroups. The approach successfully reconstructs complex marginal distributions (including skewed and bimodal) and enables controlled preservation or removal of subgroup-specific characteristics through weighted sampling in the latent space.

## Method Summary
The method uses a VAE architecture with separate encoder/decoder components for continuous and binary variables, employing either early or late fusion strategies. For unknown subgroups reflected in marginal distributions, the approach applies Box-Cox transformation for skewness and a power function for bimodality. For known subgroups, propensity scores are estimated in the original data space and used to weight cells in the latent space grid, enabling controlled sampling that preserves or removes subgroup characteristics. The synthetic data generation involves training the VAE on transformed data, visualizing latent space with propensity score heatmaps, and applying weighted sampling based on group membership probabilities.

## Key Results
- Successfully reconstructs complex marginal distributions including skewed and bimodal distributions
- Quantitatively outperforms standard VAEs, QVAEs, GANs, and copula-based methods using ψ and ψ ratio metrics
- Enables controlled preservation or removal of subgroup-specific characteristics through propensity score-guided sampling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-transformations address unknown subgroup heterogeneity by reshaping marginal distributions to better fit VAE assumptions.
- Mechanism: The approach applies Box-Cox transformation for skewness and a power function for bimodality, making distributions more amenable to the Gaussian latent space prior in VAEs.
- Core assumption: Unknown subgroups manifest as deviations from unimodal symmetric distributions that can be normalized through parametric transformations.
- Evidence anchors:
  - [abstract] "To faithfully reproduce unknown heterogeneity reflected in marginal distributions, we propose to combine VAEs with pre-transformations."
  - [section] "To preserve the unknown sub-group structure, we aim to faithfully recover the marginal distributions. In this work, we concentrate on reconstructing Bernoulli (for binary variables), skewed and bimodal distributions."
- Break condition: If subgroup structure cannot be captured by marginal distribution properties alone, pre-transformations will fail to preserve the underlying heterogeneity.

### Mechanism 2
- Claim: Propensity score modeling in the original data space enables targeted sampling from the latent space to preserve or remove known subgroup characteristics.
- Mechanism: Propensity scores are estimated in the original data space, then used to weight cells in the latent space grid, allowing controlled sampling that either emphasizes common characteristics or subgroup-specific traits.
- Core assumption: The latent space structure learned by VAE reflects the propensity score structure when visualized, enabling effective guidance for sampling.
- Evidence anchors:
  - [abstract] "For dealing with known heterogeneity due to sub-groups, we complement VAEs with models for group membership, specifically from propensity score regression."
  - [section] "We can use the propensity score concept for assigning weights to different areas of the latent space learned by the VAE."
- Break condition: If the latent space does not preserve the group structure or if propensity score estimation is poor, weighted sampling will not achieve the desired subgroup preservation/removal.

### Mechanism 3
- Claim: The combined architecture with separate encoder/decoder components for continuous and binary variables addresses data fusion challenges.
- Mechanism: The VAE architecture uses separate groups of neurons (µ_D, σ_D for continuous; π_D for binary) in the decoder, with early or late fusion strategies to handle mixed data types.
- Core assumption: Different data types can be effectively modeled with separate but interconnected neural network components.
- Evidence anchors:
  - [section] "To generate both continuous and binary variables, we use an architecture with separate parts corresponding to the two variable types."
  - [section] "Assuming xi,j as the j-th continuous variable of xi and xi,k as the k-th binary variable of xi..."
- Break condition: If the correlation patterns between variable types are too complex for the chosen fusion strategy, the reconstruction quality will degrade.

## Foundational Learning

- Concept: Variational inference and evidence lower bound (ELBO)
  - Why needed here: The VAE's training objective is to maximize ELBO, which balances reconstruction loss and KL divergence to the prior.
  - Quick check question: What happens to the ELBO if we increase the weight of the KL divergence term in the loss function?

- Concept: Propensity score weighting (IPTW)
  - Why needed here: IPTW is adapted to define weights for latent space cells, enabling controlled sampling that either preserves or removes subgroup characteristics.
  - Quick check question: How does changing the threshold δ in the weighting equations affect the balance between subgroup preservation and removal?

- Concept: Data transformation for distribution normalization
  - Why needed here: Box-Cox and power transformations are used to reshape skewed and bimodal distributions to fit VAE assumptions.
  - Quick check question: What is the primary difference between how Box-Cox and the power function for bimodality handle the back-transformation?

## Architecture Onboarding

- Component map: VAE encoder → latent space → VAE decoder (with separate continuous/binary branches) → pre-transformation layer (for skewness/bimodality) → propensity score model (for known subgroups) → weighted sampling module
- Critical path: Pre-transform original data → train VAE → visualize latent space with propensity scores → apply weighted sampling → generate synthetic data
- Design tradeoffs: Pre-transformations improve VAE fit but add complexity; propensity score modeling in original space preserves information but requires careful variable selection
- Failure signatures: Poor reconstruction of marginal distributions indicates pre-transformation issues; inability to preserve/remove subgroup characteristics suggests propensity score problems
- First 3 experiments:
  1. Train VAE on pre-transformed data and evaluate marginal distribution reconstruction using ψ metric
  2. Visualize latent space with propensity score heat map to verify alignment between propensity scores and latent structure
  3. Apply weighted sampling with different δ thresholds and compare resulting marginal distributions for subgroup preservation/removal

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the pre-transformations handle marginal distributions beyond bimodality and skewness (e.g., heavy tails, bounded distributions, multimodal with more than two peaks)?
- Basis in paper: [explicit] The paper states "we concentrated on reconstructing Bernoulli (for binary variables), skewed and bimodal distributions" and mentions these are "the most common marginal distributions in biomedical settings," implying other types exist.
- Why unresolved: The evaluation only tests pre-transformations on skewed and bimodal distributions, leaving the performance on other distribution types untested.
- What evidence would resolve it: Experiments applying the pre-transformations to datasets with heavy-tailed, bounded, or multimodal distributions (e.g., log-normal, uniform, or trimodal data) would clarify the method's limitations and generalizability.

### Open Question 2
- Question: What is the optimal threshold δ for the weighting function when the goal is to preferentially sample from areas with similar group membership probability versus removing systematic differences?
- Basis in paper: [explicit] The paper discusses δ as "the acceptable deviation from p̄ᵢ,ⱼ = 0.5" and states "Lower values, δ ≈ 0 are suitable for the scenarios where we are interested in preferentially sampling from the areas that have a rather similar group membership probability, while for higher values of δ, we include samples which may be more common to one sub-group but still can be found in other sub-group as well."
- Why unresolved: The paper uses δ = 0.1 as a single example without systematic exploration of how different δ values affect synthetic data quality or subgroup preservation.
- What evidence would resolve it: A sensitivity analysis varying δ across a range (e.g., 0.05 to 0.5) and evaluating resulting synthetic data using metrics like ψ ratio and subgroup characteristic preservation would identify optimal settings for different use cases.

### Open Question 3
- Question: How sensitive is the latent space structure to different VAE architectures and training procedures when using propensity score-guided sampling?
- Basis in paper: [explicit] The paper mentions "we use different sets of hyperparameters for the deep learning-based approaches, and we pick the most robust results" and discusses using "early fusion" or "late fusion" strategies.
- Why unresolved: The evaluation uses a single VAE architecture and does not systematically explore how architecture choices (e.g., latent dimension, network depth) or training procedures affect the alignment between propensity score heatmaps and latent space structure.
- What evidence would resolve it: Systematic ablation studies varying VAE architecture parameters (latent dimension, encoder/decoder depth, activation functions) and training procedures (batch size, learning rate, epochs) while measuring alignment quality between propensity scores and latent representations would identify robust configurations.

## Limitations
- The specific power function for bimodal distributions lacks rigorous mathematical justification beyond empirical validation
- The method's performance on marginal distributions beyond skewness and bimodality (e.g., heavy tails, bounded distributions) remains untested
- Evaluation focuses primarily on marginal distribution reconstruction without extensive examination of joint distributions or higher-order interactions

## Confidence
- High Confidence: The core mechanism of using pre-transformations (Box-Cox for skewness, power function for bimodality) to improve VAE fit to non-standard distributions is well-supported by empirical results and standard statistical practice.
- Medium Confidence: The integration of propensity scores for known subgroups through weighted sampling in the latent space is theoretically sound and shows promising results, but the dependency on accurate propensity score estimation introduces uncertainty.
- Low Confidence: The specific form of the power function for bimodal distributions lacks rigorous mathematical justification beyond empirical validation, and its generalizability to different types of bimodal distributions is unclear.

## Next Checks
1. **Latent Space Structure Validation**: Visualize and quantify how well propensity scores estimated in the original space align with the learned latent space structure across different datasets and propensity score model specifications.

2. **Cross-Dataset Robustness**: Test the method on datasets with different types of heterogeneity (e.g., multi-modal distributions beyond simple bimodal cases, complex correlation structures) to evaluate generalizability.

3. **Information Preservation Analysis**: Measure the information loss between original propensity scores and those estimated from synthetic data generated with different sampling strategies (full preservation vs. removal) to quantify how well the method maintains or eliminates subgroup characteristics.