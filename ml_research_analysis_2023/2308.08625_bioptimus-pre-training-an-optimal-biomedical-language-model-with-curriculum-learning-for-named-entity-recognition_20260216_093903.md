---
ver: rpa2
title: 'BIOptimus: Pre-training an Optimal Biomedical Language Model with Curriculum
  Learning for Named Entity Recognition'
arxiv_id: '2308.08625'
source_url: https://arxiv.org/abs/2308.08625
tags:
- pre-training
- biomedical
- language
- learning
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of pre-training biomedical language
  models by comparing different approaches and proposing an optimal method. The authors
  introduce a contextualized weight distillation technique to initialize weights for
  new tokens in continued pre-training, which speeds up the process and improves performance.
---

# BIOptimus: Pre-training an Optimal Biomedical Language Model with Curriculum Learning for Named Entity Recognition

## Quick Facts
- arXiv ID: 2308.08625
- Source URL: https://arxiv.org/abs/2308.08625
- Authors: 
- Reference count: 27
- Pre-training an optimal biomedical language model with curriculum learning for Named Entity Recognition

## Executive Summary
This paper addresses the challenge of pre-training biomedical language models by introducing an optimal method that combines contextualized weight distillation with curriculum learning. The authors propose initializing weights for new tokens in continued pre-training by leveraging existing BERT embeddings, which speeds up the pre-training process and improves performance. They also introduce a curriculum learning approach based on task complexity, gradually increasing the difficulty of pre-training tasks from random masking to whole-word masking with increased masking rates. The resulting model, BIOptimus, achieves state-of-the-art results on several biomedical Named Entity Recognition (NER) tasks, outperforming existing models like BioBERT and PubMedBERT.

## Method Summary
The paper proposes a novel approach to pre-train biomedical language models using contextualized weight distillation and curriculum learning. The contextualized weight distillation method initializes weights for new tokens by computing contextualized embeddings from existing BERT representations, averaging last-layer embeddings across multiple sentences containing the new token. For curriculum learning, the method gradually increases task complexity through four phases: starting with random masking at 15% rate, progressing to whole-word masking at 15% rate, then to whole-word masking at 20% rate, and finally to whole-word masking at 20% rate without the corruption strategy. The model is pre-trained on the PubMed corpus and fine-tuned on biomedical NER datasets.

## Key Results
- BIOptimus achieves state-of-the-art performance on BC5-chem, BC5-disease, and NCBI-disease NER datasets
- The contextualized weight distillation method significantly improves initialization quality for new biomedical tokens
- Curriculum learning based on task complexity accelerates pre-training and improves downstream performance
- Removing the "80-10-10" corruption strategy reduces representation degeneration and enhances model quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextualized weight distillation significantly improves biomedical LM performance by leveraging existing BERT embeddings for new tokens.
- Mechanism: The method extracts domain-specific vocabulary using BioMedTokenizer, then computes contextualized embeddings by averaging last-layer BERT representations across multiple sentences containing the new token.
- Core assumption: Last-layer BERT embeddings contain sufficient domain-specific information to initialize new biomedical tokens effectively.
- Evidence anchors:
  - [abstract]: "We propose a new approach to initialize weights to pre-train a biomedical LM that leverages the efficiency of pre-training in a continued fashion with specialized biomedical vocabulary"
  - [section 3]: "We use the mean operation of constituting tokens to compute a single representation for a new token (distilled representations)"
  - [corpus]: Weak - no direct corpus-level evidence comparing different layers for initialization
- Break condition: If last-layer embeddings are insufficiently domain-specific or if the new tokens appear in very few contexts, the initialization quality would degrade.

### Mechanism 2
- Claim: Curriculum learning based on task complexity accelerates biomedical LM pre-training and improves downstream performance.
- Mechanism: The method gradually increases task difficulty by first training on random masking, then whole-word masking, then increased masking rate, and finally removing corruption strategy.
- Core assumption: The model learns more effectively when introduced to increasingly complex tasks rather than starting with the most complex task.
- Evidence anchors:
  - [abstract]: "Our model (BIOptimus), pre-trained with the CL method and contextualized weight distillation, sets new states of the art on several biomedical Named Entity Recognition (NER) tasks"
  - [section 8.1]: "Using the results of our experiments, we implement a CL method for pre-training a biomedical LM"
  - [corpus]: Weak - no corpus-level analysis of task difficulty progression
- Break condition: If the complexity progression is not well-calibrated, the model may plateau or overfit at intermediate stages.

### Mechanism 3
- Claim: Removing the "80-10-10" corruption strategy reduces representation degeneration and improves biomedical LM performance.
- Mechanism: By eliminating token corruption and using only [MASK] tokens, the model avoids increasing anisotropy in contextualized word representations.
- Core assumption: The corruption strategy contributes to the representation degeneration problem by increasing anisotropy in the embedding space.
- Evidence anchors:
  - [abstract]: "The method helps to speed up the pre-training stage and improve performance on NER"
  - [section 3]: "We observe that the corruption strategy of '80-10-10' (Devlin et al., 2019) may play a role in the representation degeneration problem"
  - [corpus]: Weak - no corpus-level evidence directly linking corruption strategy to anisotropy
- Break condition: If other factors contribute more significantly to representation degeneration, removing corruption alone may not yield performance gains.

## Foundational Learning

- Concept: Subword tokenization (WordPiece/BPE)
  - Why needed here: Biomedical text contains many rare and compound terms that require subword decomposition for effective representation
  - Quick check question: What happens to a biomedical term like "bronchoconstriction" during WordPiece tokenization?

- Concept: Masked Language Modeling (MLM)
  - Why needed here: Self-supervised pre-training objective that enables learning from unlabeled biomedical text
  - Quick check question: How does the 80-10-10 corruption strategy work in standard MLM pre-training?

- Concept: Curriculum learning
  - Why needed here: Gradually increasing task complexity improves learning efficiency and final model performance
  - Quick check question: Why might starting with whole-word masking be less effective than gradually introducing it?

## Architecture Onboarding

- Component map: Tokenizer -> Embedding Layer -> Transformer Encoder -> MLM Head -> Loss Function
- Critical path: Tokenizer -> Embedding Initialization -> Pre-training -> Fine-tuning -> Evaluation
- Design tradeoffs: Pre-training from scratch vs. continued pre-training vs. hybrid approaches
- Failure signatures: Poor performance on rare biomedical terms, slow convergence, representation degeneration
- First 3 experiments:
  1. Compare initialization methods (scratch vs. continued vs. contextualized) on a small subset of PubMed
  2. Test different masking strategies (random vs. whole-word) with fixed vocabulary size
  3. Evaluate curriculum learning phases individually before combining them

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the contextualized weight distillation method affect the quality of rare word embeddings in biomedical language models?
- Basis in paper: [explicit] The paper mentions that rare words may greatly impact the performance of language models and that the contextualized weight distillation method assigns weights to rare tokens that are not found in the original BERT vocabulary.
- Why unresolved: While the paper demonstrates the effectiveness of the contextualized weight distillation method, it does not provide a detailed analysis of how it specifically impacts the quality of rare word embeddings.
- What evidence would resolve it: A detailed analysis comparing the quality of rare word embeddings in models trained with and without the contextualized weight distillation method, using metrics such as embedding similarity, nearest neighbor retrieval, and downstream task performance on rare word-heavy datasets.

### Open Question 2
- Question: How does the curriculum learning method based on task complexity affect the overall training time and efficiency of biomedical language models?
- Basis in paper: [explicit] The paper introduces a curriculum learning method based on task complexity for pre-training biomedical language models and demonstrates its effectiveness in improving performance on NER tasks. However, it does not provide a detailed analysis of its impact on training time and efficiency.
- Why unresolved: While the paper shows that the curriculum learning method improves performance, it does not provide a comprehensive analysis of its impact on training time and efficiency, which are important factors in practical applications.
- What evidence would resolve it: A detailed comparison of training time and efficiency between models trained with and without the curriculum learning method, including metrics such as training time per epoch, convergence speed, and computational resource usage.

### Open Question 3
- Question: How does the contextualized weight distillation method compare to other methods of initializing weights for new tokens in terms of performance and efficiency?
- Basis in paper: [explicit] The paper compares the contextualized weight distillation method to other methods such as averaging subtokens and pre-training from scratch, demonstrating its effectiveness in improving performance on NER tasks.
- Why unresolved: While the paper shows that the contextualized weight distillation method outperforms other methods, it does not provide a comprehensive comparison of their performance and efficiency in various scenarios.
- What evidence would resolve it: A detailed comparison of the contextualized weight distillation method with other methods of initializing weights for new tokens, including metrics such as performance on various downstream tasks, training time, and computational resource usage, across different datasets and model architectures.

## Limitations

- Evaluation is constrained to a small set of NER datasets with varying sample sizes
- Contextualized weight distillation method lacks analysis of initialization quality for tokens appearing in few contexts
- Curriculum learning is evaluated on only one non-biomedical dataset for complexity progression validation
- Limited quantitative evidence linking corruption strategy removal to reduced representation degeneration

## Confidence

- **High Confidence**: Core claim that contextualized weight distillation accelerates continued pre-training is well-supported; general superiority of BIOptimus over baselines is demonstrated
- **Medium Confidence**: Specific mechanisms for why contextualized weight distillation works and why curriculum learning accelerates training are plausible but not definitively proven
- **Low Confidence**: Claim that removing corruption strategy specifically addresses representation degeneration is weakly supported without direct anisotropy measurements

## Next Checks

1. **Initialization Quality Analysis**: Measure variance in initialization quality across tokens based on context count in contextualized weight distillation; compare performance on rare biomedical terms appearing in few contexts versus common terms.

2. **Curriculum Learning Calibration**: Test alternative curriculum progressions specifically designed for biomedical text, including different masking rate sequences and domain-specific complexity factors; compare against current progression to determine optimality.

3. **Representation Degeneration Measurement**: Implement quantitative measures of embedding space anisotropy to directly test whether removing corruption strategy reduces representation degeneration; perform ablation studies isolating corruption strategy's specific contribution.