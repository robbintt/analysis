---
ver: rpa2
title: 'FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous
  Driving'
arxiv_id: '2308.01006'
source_url: https://arxiv.org/abs/2308.01006
tags:
- prediction
- planning
- tasks
- perception
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FusionAD, a multi-modality fusion framework
  for prediction and planning tasks in autonomous driving. It combines LiDAR and camera
  inputs using a transformer-based BEV fusion network, enabling joint optimization
  of perception, prediction, and planning.
---

# FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving

## Quick Facts
- arXiv ID: 2308.01006
- Source URL: https://arxiv.org/abs/2308.01006
- Reference count: 40
- Key outcome: FusionAD achieves state-of-the-art results in prediction and planning for autonomous driving by combining LiDAR and camera inputs through a transformer-based BEV fusion network with FMSPnP modules.

## Executive Summary
FusionAD is a multi-modality fusion framework that integrates perception, prediction, and planning tasks for autonomous driving using LiDAR and camera inputs. The framework uses a transformer-based BEV fusion network to combine multi-sensor features, followed by FMSPnP modules that incorporate modality-aware prediction and status-aware planning. Experiments on the nuScenes dataset demonstrate significant improvements across all tasks: 37% reduction in prediction error, 29% improvement in occupancy prediction, and 14% decrease in collision rate while maintaining competitive perception performance.

## Method Summary
FusionAD implements a three-stage training pipeline for end-to-end autonomous driving. First, it builds a transformer-based multi-modality fusion network that combines LiDAR and camera inputs into unified BEV features using cross-attention mechanisms. Second, it establishes FMSPnP modules for trajectory prediction and planning, incorporating modality self-attention for prediction refinement and ego-status injection for safe planning. The framework is trained on nuScenes dataset using AdamW optimizer with warm-up and cosine annealing schedule, achieving state-of-the-art performance across perception, prediction, and planning tasks.

## Key Results
- 37% reduction in prediction error (ADE) compared to baselines
- 29% improvement in occupancy prediction (VPQ) metrics
- 14% decrease in collision rate while maintaining perception performance
- State-of-the-art results in prediction and planning tasks on nuScenes dataset

## Why This Works (Mechanism)

### Mechanism 1
FusionAD reduces trajectory prediction error by integrating BEV fusion with modality-aware prediction and status-aware planning. Multi-sensor (LiDAR + camera) BEV features are fused via a transformer-based encoder, then passed to FMSPnP, which refines predictions through modality self-attention and improves planning with ego status integration. Core assumption: BEV space fusion preserves spatial consistency and semantic richness needed for prediction and planning downstream. Evidence anchors: "[abstract] we first build a transformer based multi-modality fusion network to effectively produce fusion based features" and "[section] we then establish a fusion aided modality-aware prediction and status-aware planning modules, dubbed FMSPnP". Break condition: If modality self-attention fails to disambiguate multimodal distributions, prediction accuracy will not improve.

### Mechanism 2
FMSPnP's refinement network improves trajectory accuracy by correcting initial motionformer predictions with spatial priors. Initial trajectories from motionformer are encoded into spatial anchors, then refined via deformable attention over BEV features to produce offset corrections. Core assumption: Motionformer's raw output is sufficiently close to ground truth to serve as a useful prior for refinement. Evidence anchors: "[section] We introduce a refinement net to use the trajectories generated by Motionformer as more accurate spatial priors". Break condition: If motionformer outputs are too inaccurate, refinement will diverge rather than correct.

### Mechanism 3
Planning safety is improved by fusing vectorized ego status with BEV features and using relaxed collision loss. Planning query concatenates ego embedding (MLP-processed) with BEV cross-attention features, then decoded to waypoints; collision loss penalizes predicted trajectories overlapping with forecasted agents. Core assumption: Ego status contains sufficient context (position, velocity, heading) to guide safe trajectory planning. Evidence anchors: "[section] We enhance the current state sensitivity by injecting ego information". Break condition: If ego embedding does not capture dynamic state, planning will ignore critical safety cues.

## Foundational Learning

- **Bird's Eye View (BEV) representation**: Unifies LiDAR point clouds and camera images into a common spatial reference for multi-task fusion. Why needed here: BEV space enables consistent spatial reasoning across perception, prediction, and planning. Quick check question: How does projecting camera features to BEV preserve scale and orientation relative to LiDAR BEV features?

- **Transformer-based cross-attention for multimodal fusion**: Allows each BEV query to selectively integrate relevant LiDAR and camera features without hand-crafted fusion rules. Why needed here: Cross-attention learns optimal feature integration patterns automatically. Quick check question: What is the difference between points cross-attention and image cross-attention in the FusionAD architecture?

- **Modality self-attention for multimodal trajectory prediction**: Enables each trajectory mode to attend to others, improving diversity and quality of multimodal predictions. Why needed here: Self-attention helps capture relationships between different predicted trajectories. Quick check question: How does modality self-attention differ from the cross-attention used in the fusion encoder?

## Architecture Onboarding

- **Component map**: LiDAR point clouds + camera images → BEV encoders → Multi-modal fusion transformer → FMSPnP module → Detection/Tracking/Mapping heads + Prediction + Planning outputs
- **Critical path**: Raw LiDAR + camera → BEV features → Temporal fusion → unified BEV features → FMSPnP → refined prediction & safe planning → Collision-aware loss + occupancy optimization
- **Design tradeoffs**: High GPU memory usage due to 3D voxelization + multi-scale image features; batch size limited to 1. Fixed BEV resolution (200×200) balances coverage vs. detail; may limit performance in very large scenes. Modality self-attention increases parameter count but improves multimodal diversity.
- **Failure signatures**: Large ADE/FDE but low collision rate: refinement may be overfitting to smooth trajectories. High collision rate but low ADE/FDE: ego-status injection may be insufficient or collision loss too relaxed. Poor perception metrics: BEV fusion may be losing critical spatial cues.
- **First 3 experiments**: 1) Ablate modality self-attention from FMSPnP → measure ADE/FDE change. 2) Ablate ego-status injection in planning → measure collision rate change. 3) Remove BEV fusion (use camera-only UniAD baseline) → measure all task metrics for comparison.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed FusionAD framework perform in real-world autonomous driving scenarios, particularly in terms of safety and robustness?
- Basis in paper: [inferred] The paper mentions that FusionAD has shown significant improvements in perception, prediction, and planning tasks in experiments, but it also acknowledges the reliance on a rule-based system for post-processing and the need for further evaluation in real-world scenarios.
- Why unresolved: The paper primarily focuses on experimental results using the nuScenes dataset and does not provide direct evidence of FusionAD's performance in real-world autonomous driving scenarios.
- What evidence would resolve it: Real-world testing and evaluation of FusionAD in various driving conditions, including different weather, lighting, and traffic scenarios, would provide evidence of its performance and robustness in practical applications.

### Open Question 2
- Question: Can the FMSPnP module be further improved to enhance the prediction and planning tasks in FusionAD?
- Basis in paper: [explicit] The paper introduces the FMSPnP module and discusses its contributions to improving prediction and planning results. However, it also suggests potential areas for future improvement, such as adding mode attention and refinement mechanisms.
- Why unresolved: The paper acknowledges that the FMSPnP module can be further optimized and refined to achieve better prediction and planning outcomes, but it does not provide specific details or results of such improvements.
- What evidence would resolve it: Implementing and evaluating additional enhancements to the FMSPnP module, such as incorporating mode attention and refinement mechanisms, would provide evidence of its impact on improving prediction and planning tasks in FusionAD.

### Open Question 3
- Question: How does FusionAD compare to other end-to-end autonomous driving frameworks that utilize multi-modal fusion?
- Basis in paper: [explicit] The paper mentions that FusionAD is the first uniform BEV multi-modality based, multi-task end-to-end learning framework for autonomous driving, but it does not provide a direct comparison with other frameworks that utilize multi-modal fusion.
- Why unresolved: The paper does not explicitly compare FusionAD to other end-to-end autonomous driving frameworks that incorporate multi-modal fusion, making it difficult to assess its relative performance and advantages.
- What evidence would resolve it: Conducting a comprehensive comparison between FusionAD and other end-to-end autonomous driving frameworks that utilize multi-modal fusion, including metrics such as perception, prediction, and planning accuracy, would provide evidence of FusionAD's strengths and weaknesses relative to existing approaches.

## Limitations

- The ablation study only removes FMSPnP entirely rather than isolating the impact of its internal components (modality self-attention and ego-status injection)
- The collision loss relaxation mechanism is under-specified, making safety guarantees difficult to assess
- Model scalability to longer prediction horizons (beyond 6 seconds) remains untested

## Confidence

- **High confidence**: Multi-modal BEV fusion improves perception tasks (supported by established methods like BEVFormer and SECOND)
- **Medium confidence**: FMSPnP improves prediction and planning (supported by nuScenes results, but lacks internal component ablations)
- **Low confidence**: Collision loss relaxation ensures safety (mechanism is under-specified, and no safety validation beyond nuScenes metrics is provided)

## Next Checks

1. Ablate modality self-attention in FMSPnP to quantify its contribution to trajectory prediction accuracy
2. Ablate ego-status injection in planning to measure its impact on collision rate
3. Test model performance on longer prediction horizons (e.g., 8-10 seconds) to assess scalability