---
ver: rpa2
title: Context-Aware Semantic Similarity Measurement for Unsupervised Word Sense Disambiguation
arxiv_id: '2305.03520'
source_url: https://arxiv.org/abs/2305.03520
tags:
- word
- semantic
- similarity
- sense
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research introduces a context-aware semantic similarity (CASS)
  approach for unsupervised word sense disambiguation (WSD). The method adapts state-of-the-art
  contextualized embedding techniques (BERT, ELMo, USE, WMD) to capture word meaning
  based on context, enabling disambiguation without labeled training data.
---

# Context-Aware Semantic Similarity Measurement for Unsupervised Word Sense Disambiguation

## Quick Facts
- arXiv ID: 2305.03520
- Source URL: https://arxiv.org/abs/2305.03520
- Reference count: 9
- Primary result: BERT-based CASS approach achieves 77.74% accuracy on CoarseWSD-20, outperforming both random baseline (43.73%) and MFS baseline (73.43%)

## Executive Summary
This research introduces a context-aware semantic similarity (CASS) approach for unsupervised word sense disambiguation (WSD) that leverages contextualized embeddings from pre-trained language models. The method adapts BERT, ELMo, USE, and WMD to capture word meaning based on context, enabling disambiguation without labeled training data. Evaluated on the CoarseWSD-20 benchmark dataset, the BERT-based implementation achieved 77.74% accuracy, demonstrating significant improvements over conventional approaches that don't consider contextual information. The approach offers a scalable solution for managing word sense ambiguity in natural language processing tasks, particularly valuable in low-resource settings where annotated data is scarce.

## Method Summary
The CASS approach adapts contextualized embedding techniques to measure semantic similarity between words in context for unsupervised WSD. The method creates document embeddings by averaging contextualized word vectors generated by BERT, ELMo, USE, or WMD, then computes cosine similarity between these embeddings to determine semantic relatedness. For each target word, the approach compares the context in which it appears against known sense definitions using semantic similarity scores, selecting the sense with the highest similarity. The evaluation uses the CoarseWSD-20 dataset containing 10,196 cases across 20 words with 2-5 meanings, measuring performance using accuracy as the primary metric.

## Key Results
- BERT-based implementation achieved 77.74% accuracy on CoarseWSD-20 benchmark
- Outperformed strong MFS baseline (73.43%) and random baseline (43.73%)
- Demonstrated effectiveness of context-aware similarity measurement for unsupervised WSD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-aware semantic similarity measurement improves disambiguation by capturing word meaning variations based on context
- Mechanism: The method uses contextualized embeddings (BERT, ELMo, USE, WMD) to represent words as vectors that encode contextual meaning, then computes similarity between these embeddings to determine the most probable sense
- Core assumption: Contextualized embeddings can effectively capture the nuanced meaning of words in different contexts
- Evidence anchors:
  - [abstract]: "incorporates contextual information into the similarity measurement process to reduce language ambiguity"
  - [section]: "CASS techniques aim to capture this variability, produce more precise similarity scores, and enhance the performance of unsupervised WSD strategies"
  - [corpus]: Weak - corpus doesn't directly address embedding effectiveness, but shows related work on context-aware approaches
- Break condition: If contextual embeddings fail to capture meaning variations or if context is too sparse/ambiguous to provide useful information

### Mechanism 2
- Claim: Unsupervised approach works without labeled data by leveraging pre-trained language models
- Mechanism: Pre-trained models like BERT and ELMo are fine-tuned on large text corpora to generate contextualized representations, eliminating need for annotated training data
- Core assumption: Pre-trained language models have learned sufficient semantic relationships to disambiguate word senses in new contexts
- Evidence anchors:
  - [abstract]: "enabling disambiguation without labeled training data"
  - [section]: "The proposed method achieved significant improvements in disambiguation accuracy compared to conventional methods that do not consider contextual information"
  - [corpus]: Moderate - corpus shows related unsupervised approaches but doesn't validate pre-trained model effectiveness specifically
- Break condition: If pre-trained models don't generalize well to domain-specific language or if context patterns differ significantly from training data

### Mechanism 3
- Claim: Cosine similarity between contextualized embeddings effectively measures semantic relatedness
- Mechanism: The method computes cosine similarity between document embeddings created from averaged contextualized word vectors to determine semantic similarity
- Core assumption: Cosine similarity is an appropriate metric for comparing contextualized embeddings in semantic space
- Evidence anchors:
  - [section]: "The semantic similarity ss between eX and eY is then calculated as in Eq. 7: ss = eX ·eY/∥eX ∥ · ∥eY ∥"
  - [section]: "We use a standard evaluation metric called accuracy to evaluate the performance of different WSD models"
  - [corpus]: Weak - corpus doesn't validate cosine similarity specifically for this application
- Break condition: If cosine similarity fails to capture semantic relationships in high-dimensional embedding space or if embeddings aren't properly normalized

## Foundational Learning

- Concept: Contextualized word embeddings
  - Why needed here: Understanding how models like BERT and ELMo capture context-dependent meaning is fundamental to grasping why CASS works
  - Quick check question: How do contextualized embeddings differ from static word embeddings like Word2Vec?

- Concept: Word Sense Disambiguation (WSD)
  - Why needed here: Understanding the WSD problem space helps appreciate why context matters and how the proposed solution addresses it
  - Quick check question: What makes word sense disambiguation particularly challenging for words with multiple meanings?

- Concept: Cosine similarity in high-dimensional spaces
  - Why needed here: The method relies on cosine similarity to compare embeddings, so understanding this metric is crucial for implementation
  - Quick check question: Why is cosine similarity preferred over Euclidean distance for comparing word embeddings?

## Architecture Onboarding

- Component map: Input text -> Context processor -> Embedding generator (BERT/ELMo/USE/WMD) -> Similarity calculator -> Disambiguation selector
- Critical path: Input text -> Contextualized embeddings -> Similarity computation -> Sense selection
- Design tradeoffs: Accuracy vs. computational efficiency (BERT most accurate but slowest; WMD fastest but least accurate)
- Failure signatures: Poor disambiguation accuracy when context is ambiguous, embeddings don't capture meaning variations, or similarity metric fails
- First 3 experiments:
  1. Test basic WSD on simple polysemous words with clear context using BERT embeddings
  2. Compare performance of different embedding approaches (BERT vs. ELMo vs. USE vs. WMD) on same dataset
  3. Evaluate impact of context quality by testing with ambiguous vs. clear contexts on disambiguation accuracy

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the CASS approach. First, it raises the question of why BERT-based embeddings consistently outperform other embedding methods (ELMo, USE, WMD) in unsupervised WSD, noting that while performance differences are observed, the underlying reasons for BERT's superior performance across different disambiguation cases are not fully explained. Second, the paper questions how the CASS approach can be effectively scaled to handle larger vocabularies and more ambiguous words beyond the 20 nouns tested in CoarseWSD-20, acknowledging that current evaluation is limited to a small benchmark dataset. Third, the paper identifies the need to determine what specific contextual features or information sources could be integrated into the CASS approach to improve disambiguation accuracy in cases of sparse or ambiguous context, recognizing this as a current limitation.

## Limitations

- Results based on single benchmark dataset (CoarseWSD-20), limiting generalizability
- No evaluation on rare word senses or domain-specific terminology where pre-trained models may underperform
- Significant computational requirements for BERT-based approaches may limit real-time applications

## Confidence

- Core claim (context-aware embeddings improve WSD accuracy): Medium
- Claims about scalability and real-world applicability: Low
- Methodological framework: Medium

## Next Checks

1. Test the approach on multiple benchmark datasets (e.g., SemEval, Senseval) to assess generalizability across different domains and languages.

2. Conduct ablation studies comparing CASS with other unsupervised WSD methods like Lesk algorithm and graph-based approaches to establish relative performance.

3. Measure computational efficiency and memory requirements across different embedding approaches (BERT, ELMo, USE, WMD) to evaluate practical deployment constraints.