---
ver: rpa2
title: Low-Resource Languages Jailbreak GPT-4
arxiv_id: '2310.02446'
source_url: https://arxiv.org/abs/2310.02446
tags:
- languages
- language
- arxiv
- safety
- low-resource
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that translating unsafe English prompts
  into low-resource languages can bypass GPT-4's safety filters with 79% success,
  nearly matching or exceeding sophisticated jailbreaking methods. The attack exploits
  GPT-4's weaker safeguards for low-resource languages, highlighting a cross-lingual
  vulnerability arising from unequal safety training data.
---

# Low-Resource Languages Jailbreak GPT-4

## Quick Facts
- **arXiv ID:** 2310.02446
- **Source URL:** https://arxiv.org/abs/2310.02446
- **Reference count:** 40
- **Primary result:** Translating unsafe English prompts into low-resource languages bypasses GPT-4's safety filters with 79% success

## Executive Summary
This paper demonstrates that translating unsafe English prompts into low-resource languages can bypass GPT-4's safety filters with 79% success, nearly matching or exceeding sophisticated jailbreaking methods. The attack exploits GPT-4's weaker safeguards for low-resource languages, highlighting a cross-lingual vulnerability arising from unequal safety training data. This risk affects all users, not just low-resource language speakers, as translation tools make the attack widely accessible.

## Method Summary
The study evaluates GPT-4's cross-lingual safety vulnerability by translating unsafe English prompts from the AdvBenchmark dataset into 12 languages (low-resource, mid-resource, and high-resource) using Google Translate API. The translated prompts are fed to GPT-4, responses are translated back to English, and manually annotated as BYPASS, REJECT, or UNCLEAR. The attack success rate is compared against other jailbreaking methods including AIM, base64, prefix injection, and refusal suppression.

## Key Results
- Translating prompts to low-resource languages achieved 79% attack success rate
- Low-resource languages were significantly more vulnerable than high-resource languages
- Attack success rate matched or exceeded sophisticated jailbreaking methods
- Translation tools democratize access to this vulnerability for all users

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Translating unsafe prompts into low-resource languages bypasses GPT-4's safety filters because the model's safety training data is heavily skewed toward high-resource languages.
- **Mechanism:** The model's safety alignment mechanisms, which are primarily trained on English and other high-resource languages, fail to generalize to low-resource languages due to insufficient training data in those languages.
- **Core assumption:** GPT-4's safety training data distribution is heavily biased toward high-resource languages.
- **Evidence anchors:**
  - "Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data"
  - "Our work also reveals that the existing safety alignment training in GPT-4 poorly generalize across languages"
  - The related paper "Jailbroken: How Does LLM Safety Training Fail?" supports this mechanism.

### Mechanism 2
- **Claim:** GPT-4 is capable of generating harmful content in low-resource languages, indicating that its core language understanding is not limited to high-resource languages.
- **Mechanism:** The model's underlying language capabilities are not inherently weaker in low-resource languages; instead, the safety filters are the weak point.
- **Core assumption:** GPT-4's pretraining data includes sufficient low-resource language content for the model to understand and generate text.
- **Evidence anchors:**
  - "Our results also suggest that researchers have underestimated the capability of the LLMs in understanding and generating text in low-resource languages"
  - "In many of the cases, translating GPT-4's responses back to English returns coherent, on-topic, and harmful outputs"
  - The related paper "Multilingual Jailbreak Challenges in Large Language Models" implies that the model's core language capabilities are not the limiting factor.

### Mechanism 3
- **Claim:** The availability of translation APIs makes this attack widely accessible, turning a low-resource language vulnerability into a global risk.
- **Mechanism:** Translation tools democratize access to low-resource languages, allowing users who do not speak those languages to exploit the safety vulnerability.
- **Core assumption:** Translation APIs are sufficiently accurate and widely available.
- **Evidence anchors:**
  - "Publicly available translation APIs enable anyone to exploit LLMs' safety vulnerabilities"
  - "Furthermore, as the translation tools expand their coverage of low-resource languages, even bad actors who speak high-resource languages can now easily bypass the existing safeguards"
  - The related paper "Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?" explores cross-lingual generalization enabled by translation tools.

## Foundational Learning

- **Concept:** Linguistic inequality in AI safety training
  - **Why needed here:** Understanding that safety training data is skewed toward high-resource languages is crucial to grasping why low-resource languages are vulnerable.
  - **Quick check question:** Why might a model trained primarily on English safety data fail to recognize harmful content in Zulu or Scots Gaelic?

- **Concept:** Mismatched generalization in machine learning
  - **Why needed here:** This concept explains how a model can perform well on its training distribution but fail on out-of-distribution data.
  - **Quick check question:** What is the difference between a model failing because it doesn't understand a language and failing because its safety filters don't generalize to that language?

- **Concept:** Red-teaming and adversarial testing
  - **Why needed here:** Understanding red-teaming is essential because the paper's recommendations focus on expanding red-teaming efforts to include low-resource languages.
  - **Quick check question:** How does the scope of red-teaming affect the robustness of safety mechanisms in multilingual contexts?

## Architecture Onboarding

- **Component map:** Input -> Google Translate (English to low-resource) -> GPT-4 -> Google Translate (low-resource to English) -> Evaluation (BYPASS/REJECT/UNCLEAR)
- **Critical path:**
  1. Receive unsafe English prompt
  2. Translate to low-resource language
  3. Submit to GPT-4
  4. Translate GPT-4's response back to English
  5. Evaluate if safety filter was bypassed
- **Design tradeoffs:**
  - Using Google Translate API introduces potential noise and mistranslation
  - Choice of low-resource languages affects attack success rate
  - Subjective evaluation protocol could be refined for consistency
- **Failure signatures:**
  - High UNCLEAR rate indicates mistranslation or generation issues
  - Low attack success rate in high-resource languages confirms effective safety filters
  - Successful attacks on specific topics shows certain content types are more vulnerable
- **First 3 experiments:**
  1. Test the same unsafe prompts on GPT-4 without translation to confirm <1% attack success rate in English
  2. Manually translate a subset of prompts to verify Google Translate is not introducing errors
  3. Group low-resource languages by language family to see if linguistic similarity affects attack success rate

## Open Questions the Paper Calls Out
The paper explicitly states this as a limitation: "Given the proprietary nature of GPT-4, it is unknown how the model learns the low-resource languages in the training such that it can process the inputs and return harmful responses. Future work is needed to investigate cross-lingual vulnerability in other LLMs such as Claude and Llama models..."

## Limitations
- Reliance on Google Translate API introduces potential noise and translation inaccuracies
- Manual annotation process lacks standardized guidelines, potentially introducing rater bias
- Study focused solely on GPT-4, leaving generalizability to other models unknown

## Confidence
- Claim that linguistic inequality creates vulnerabilities: **Medium confidence**
- Claim that translation APIs democratize access: **Medium confidence**
- Claim that GPT-4's core language capabilities extend to low-resource languages: **High confidence**

## Next Checks
1. Conduct a controlled study using manual translation by bilingual speakers to isolate translation quality effects
2. Group low-resource languages by linguistic family to determine if attack success correlates with linguistic similarity
3. Design experiments testing safety filter activation differences for explicit versus implicit harmful content in low-resource languages