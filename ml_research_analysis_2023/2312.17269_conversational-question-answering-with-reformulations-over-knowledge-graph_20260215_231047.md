---
ver: rpa2
title: Conversational Question Answering with Reformulations over Knowledge Graph
arxiv_id: '2312.17269'
source_url: https://arxiv.org/abs/2312.17269
tags:
- question
- reformulations
- entity
- performance
- cornnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of conversational question answering
  (ConvQA) over knowledge graphs (KGs), where state-of-the-art methods struggle with
  inexplicit question-answer pairs. The proposed method, CoRnNet, uses reinforcement
  learning (RL) with large language model (LLM) generated reformulations to improve
  ConvQA performance.
---

# Conversational Question Answering with Reformulations over Knowledge Graph

## Quick Facts
- **arXiv ID:** 2312.17269
- **Source URL:** https://arxiv.org/abs/2312.17269
- **Reference count:** 37
- **Key outcome:** CoRnNet outperforms state-of-the-art ConvQA models, achieving 13.7% improvement in Hit@8 and 6.6% improvement in Hit@5 on ConvQuestions dataset.

## Executive Summary
Conversational Question Answering (ConvQA) over Knowledge Graphs (KGs) faces challenges with implicit questions due to anaphora and ellipsis. This paper introduces CoRnNet, a method that leverages large language model (LLM) generated reformulations and reinforcement learning (RL) to improve ConvQA performance. The core innovation is a teacher-student architecture where a teacher model learns from human-written reformulations and a student model mimics this performance using LLM-generated reformulations. This approach enables the model to benefit from high-quality human-level reformulation performance even when using LLM-generated reformulations during inference.

## Method Summary
CoRnNet addresses ConvQA over KGs by first generating reformulations of conversational questions using LLMs (GPT2 and BART). A teacher model is trained on human-written reformulations to produce high-quality question embeddings. A student model then learns to mimic the teacher's output using LLM-generated reformulations through an L2 distance loss. The resulting question representation is used by an RL agent to navigate the KG and locate correct answers. The RL agent uses a policy network to traverse the KG, guided by the question embedding and search history, with soft rewards based on entity similarity to address sparse reward issues.

## Key Results
- CoRnNet achieves 13.7% improvement in Hit@8 and 6.6% improvement in Hit@5 on ConvQuestions dataset compared to CONQUER baseline.
- The method shows consistent improvements across multiple metrics (P@1, MRR, H@3, H@5, H@8) on both ConvQuestions and ConvRef datasets.
- CoRnNet outperforms various state-of-the-art baselines including MMM, LM, and different variants of CONQUER.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The teacher-student architecture enables the model to leverage human-level reformulation performance even when using LLM-generated reformulations during inference.
- Mechanism: The teacher model is trained on human writing reformulations and produces a high-quality question embedding. The student model is trained to mimic this embedding using LLM-generated reformulations as input, effectively distilling the human-level performance into the student model.
- Core assumption: The student model can successfully learn to map LLM-generated reformulations to the same embedding space as human writing reformulations through the teacher's guidance.
- Evidence anchors:
  - [abstract]: "CoRnNet adopts a teacher-student architecture where a teacher model learns question representations using human writing reformulations, and a student model to mimic the teacher model's output via reformulations generated by LLMs."
  - [section]: "The teacher model learns the question representation by using human writing reformulations, while the student model takes reformulations generated by LLMs as input, and tries to mimic the output of the teacher model, so that it can achieve the same performance as the teacher model despite using the LLMs generated reformulations."
  - [corpus]: Weak evidence - the corpus does not provide direct evidence for this mechanism, but the claimed performance improvements suggest it works effectively.
- Break condition: If the student model fails to accurately mimic the teacher's output, or if LLM-generated reformulations are too dissimilar from human writing reformulations for the student to bridge the gap.

### Mechanism 2
- Claim: Reinforcement learning allows the model to effectively navigate the knowledge graph to find correct answers based on the learned question representation.
- Mechanism: The RL agent starts from the topic entity and uses a policy network to select actions (edges to traverse) in the knowledge graph. The policy is guided by the question embedding and search history, allowing it to find the correct answer entity.
- Core assumption: The RL policy can effectively learn to navigate the knowledge graph to find correct answers when guided by the question representation.
- Evidence anchors:
  - [abstract]: "The learned question representation is then used by a RL model to locate the correct answer in a KG."
  - [section]: "To locate an answer, a RL model walks over the KG, sampling actions from a policy network to guide the direction of the walk and identify candidate answers."
  - [corpus]: Weak evidence - the corpus does not provide direct evidence for this mechanism, but the claimed performance improvements suggest it works effectively.
- Break condition: If the RL policy fails to effectively navigate the knowledge graph, or if the question representation does not provide sufficient guidance for the RL agent.

### Mechanism 3
- Claim: Knowledge-based soft rewards help mitigate the sparsity of rewards in conversational question answering, speeding up convergence.
- Mechanism: In addition to receiving a reward of 1 when reaching the correct answer, the model also receives a soft reward based on the similarity between the current entity and the ground truth answer, calculated using ComplEx embeddings.
- Core assumption: The soft reward based on entity similarity provides useful additional signal to guide the RL agent, even when it hasn't reached the exact answer.
- Evidence anchors:
  - [section]: "To address the issue of weak supervision and sparsity of rewards in ConvQA, we assign a soft reward to entities other than the target answer to measure the similarity between them."
  - [section]: "R(st) = Rb(st) + (1 - Rb(st))P r(nt|lqi , vqi , G)"
  - [corpus]: Weak evidence - the corpus does not provide direct evidence for this mechanism, but the claimed performance improvements suggest it works effectively.
- Break condition: If the soft reward does not provide useful additional signal, or if the entity similarity measure is not accurate enough to guide the RL agent effectively.

## Foundational Learning

- Concept: Knowledge Graphs (KGs)
  - Why needed here: The model operates on a KG to find answers to conversational questions, so understanding the structure and representation of KGs is crucial.
  - Quick check question: What are the main components of a knowledge graph, and how are entities and relations typically represented?

- Concept: Reinforcement Learning (RL)
  - Why needed here: The model uses RL to navigate the KG and find answers, so understanding the basics of RL and how it can be applied to graph-based problems is essential.
  - Quick check question: What are the key components of a reinforcement learning problem, and how does the agent learn to make decisions through interaction with the environment?

- Concept: Large Language Models (LLMs)
  - Why needed here: The model uses LLMs to generate reformulations of conversational questions, so understanding how LLMs work and how they can be fine-tuned for specific tasks is important.
  - Quick check question: What are the main differences between GPT2 and BART, and how can they be fine-tuned for the task of question reformulation?

## Architecture Onboarding

- Component map: Question Encoder -> Reformulation Merging -> Context Fusion -> Teacher-Student Model -> RL Model -> Answer
- Critical path: Question -> Reformulation Generation -> Question Encoder -> Reformulation Merging -> Context Fusion -> Teacher-Student Model -> RL Model -> Answer
- Design tradeoffs:
  - Using human writing reformulations vs. LLM-generated reformulations: Human reformulations are higher quality but limited in quantity, while LLM reformulations are more abundant but lower quality. The teacher-student approach aims to leverage the best of both.
  - Using RL vs. other methods for KG navigation: RL allows for more flexible and adaptive navigation, but can be more challenging to train and may require more computational resources.
- Failure signatures:
  - Poor performance on H@5 or MRR metrics: This could indicate issues with the question representation, the RL navigation, or the overall model architecture.
  - High variance in performance across different domains: This could suggest that the model is not generalizing well to different types of questions or knowledge domains.
  - Slow convergence or instability during training: This could indicate issues with the RL training process, the soft reward mechanism, or the overall model optimization.
- First 3 experiments:
  1. Train and evaluate the teacher model on human writing reformulations to establish a baseline for the quality of the question representation.
  2. Train and evaluate the student model with LLM-generated reformulations to assess the effectiveness of the teacher-student approach.
  3. Train and evaluate the full model with the RL component to assess the overall performance on the ConvQA task.

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of CoRnNet change when using more advanced LLMs for reformulation generation, such as GPT-4 or larger variants of BART?
- Open Question 2: Can the teacher-student architecture be applied to other natural language processing tasks beyond conversational question answering over knowledge graphs?
- Open Question 3: How does the choice of knowledge graph embedding method (e.g., ComplEx, TransE, RotatE) impact the performance of CoRnNet?
- Open Question 4: Can the teacher-student architecture be extended to handle multi-modal conversational question answering, where the conversation involves both text and images?

## Limitations

- The method's performance gains are primarily demonstrated on the ConvQuestions dataset, with less dramatic improvements on ConvRef, raising questions about generalization across diverse ConvQA benchmarks.
- The teacher-student approach relies heavily on LLM-generated reformulations, which can be computationally expensive and may not scale well to larger knowledge graphs or longer conversations.
- The soft reward mechanism's sensitivity to the choice of entity embedding model and reward scaling parameters is not thoroughly explored, potentially limiting its effectiveness in different domains.

## Confidence

- High confidence in the teacher-student architecture's effectiveness: The mechanism is well-described, and the core idea of distilling human-level reformulation quality into an LLM-based student model is sound and supported by the results.
- Medium confidence in the RL navigation component: While the overall approach is reasonable, the paper doesn't provide detailed analysis of the RL policy's behavior or ablations to isolate its contribution to the performance gains.
- Medium confidence in the overall performance claims: The improvements are significant, but the lack of comparison to more recent ConvQA methods and limited ablation studies on key components (e.g., soft rewards, teacher-student vs. direct LLM training) make it difficult to attribute gains to specific innovations.

## Next Checks

1. Ablation study on soft rewards: Remove the soft reward component and retrain the RL agent to quantify its contribution to the performance gains. Compare convergence speed and final performance with and without soft rewards.
2. Generalization experiment: Evaluate CoRnNet on a diverse set of ConvQA datasets (e.g., CAsT, CANARD) to assess its performance across different domains and question styles. Report H@5 and MRR for each dataset.
3. Teacher-student vs. direct LLM training: Train a baseline model using only LLM-generated reformulations without the teacher-student distillation. Compare its performance to CoRnNet to quantify the benefit of the teacher-student approach.