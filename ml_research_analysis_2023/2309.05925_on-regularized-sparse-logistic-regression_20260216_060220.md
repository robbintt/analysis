---
ver: rpa2
title: On Regularized Sparse Logistic Regression
arxiv_id: '2309.05925'
source_url: https://arxiv.org/abs/2309.05925
tags:
- regression
- logistic
- nonconvex
- sparse
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a unified optimization framework for solving\
  \ sparse logistic regression problems with both convex (\u21131-norm) and certain\
  \ nonconvex regularization terms. The authors propose modified iterative shrinkage\
  \ thresholding algorithms (ISTA) with improved step size selection using Barzilai-Borwein\
  \ rules and reverse line search to ensure monotone convergence and faster convergence."
---

# On Regularized Sparse Logistic Regression

## Quick Facts
- arXiv ID: 2309.05925
- Source URL: https://arxiv.org/abs/2309.05925
- Reference count: 40
- This paper presents a unified optimization framework for sparse logistic regression with convex (ℓ1-norm) and certain nonconvex regularization terms using modified ISTA/FISTA algorithms with improved step size selection.

## Executive Summary
This paper introduces a unified optimization framework for solving sparse logistic regression problems with both convex and certain nonconvex regularization terms. The authors propose modified iterative shrinkage thresholding algorithms (ISTA/FISTA) with improved step size selection using Barzilai-Borwein rules and reverse line search. The framework handles ℓ1-norm regularization and nonconvex penalties satisfying weak conditions through modified line search criteria. Empirical results on five real-world datasets demonstrate that the proposed algorithms achieve comparable or better classification accuracy while requiring significantly lower computational cost than state-of-the-art methods, with good scalability for increasing feature dimensions and sample sizes.

## Method Summary
The method presents modified ISTA/FISTA algorithms for sparse logistic regression with improved step size selection. The framework uses Barzilai-Borwein rules for initialization and reverse line search for monotone convergence. For convex ℓ1-norm regularization, standard line search ensures sufficient descent, while for nonconvex penalties satisfying DC structure, a weaker line search condition guarantees convergence to critical points. The proximal operator structure remains the same across both penalty types, enabling a unified implementation. The algorithms scale efficiently through iterative nature and efficient step size computation.

## Key Results
- Proposed algorithms achieve comparable or better classification accuracy than state-of-the-art methods
- Significantly lower computational cost with stable scalability trends as features/samples increase
- Unified framework handles both convex ℓ1-norm and nonconvex penalties with only one weaker assumption than previous work
- Reverse line search finds larger step sizes than conventional backtracking, accelerating convergence

## Why This Works (Mechanism)

### Mechanism 1
- Modified ISTA/FISTA with BB rules and reverse line search achieve faster convergence for both convex ℓ1-norm and certain nonconvex penalties
- BB rules avoid unnecessarily small step sizes; reverse line search optimizes step size selection
- Core assumption: Lipschitz continuity of logistic loss gradient and DC decomposition of nonconvex penalties
- Evidence: Abstract states "modified iterative shrinkage thresholding algorithms (ISTA) with improved step size selection using Barzilai-Borwein rules and reverse line search to ensure monotone convergence and faster convergence"
- Break condition: If nonconvex penalty cannot be decomposed into DC functions with bounded Hessians or Lipschitz constant not accurately estimated

### Mechanism 2
- Unified framework handles both convex and nonconvex penalties through modified line search while keeping proximal operator structure
- Standard line search for convex; weaker condition for nonconvex ensures non-increase in objective
- Core assumption: Nonconvex penalty can be expressed as g1(β) - g2(β) with convex g1, g2 and bounded Hessian
- Evidence: Abstract mentions "much weaker assumption compared to previous work"; Section III-B discusses DC structure requirement
- Break condition: If nonconvex penalty doesn't satisfy DC structure or has unbounded Hessian

### Mechanism 3
- Efficient scalability with increasing feature dimensions and sample sizes through iterative methods
- Avoids full matrix inversions; BB rule and reverse line search minimize iterations needed
- Core assumption: Problem size doesn't grow beyond practical limits of iterative methods
- Evidence: Abstract states "algorithms show good scalability with increasing feature dimensions and sample sizes"; Section IV shows stable computation time trends
- Break condition: If problem size exceeds practical limits of iterative methods

## Foundational Learning

- **Lipschitz continuity of logistic loss gradient**: Ensures existence of step size guaranteeing descent in optimization algorithms
  - Quick check: What is the Lipschitz constant of the logistic loss gradient, and how is it computed from the data matrix?

- **Proximal operators and shrinkage thresholding**: Core computational step in ISTA/FISTA; for ℓ1-norm corresponds to soft thresholding
  - Quick check: What is the proximal operator for the ℓ1-norm, and how does it promote sparsity in the solution?

- **Difference of convex (DC) functions and their optimization**: Enables extension to nonconvex penalties by decomposition into differences of convex functions
  - Quick check: How can a nonconvex penalty like SCAD be expressed as a difference of convex functions, and what are the implications for optimization?

## Architecture Onboarding

- **Component map**: Logistic loss with regularization -> Gradient computation (Lipschitz) -> Proximal operator (depends on regularization) -> Step size selection (BB or reverse line search) -> Line search condition (standard/modified) -> Update parameters -> Convergence check

- **Critical path**: 1) Compute gradient of logistic loss 2) Apply proximal operator 3) Select step size (BB or reverse line search) 4) Check line search condition 5) Update parameters 6) Check convergence

- **Design tradeoffs**: BB rule vs reverse line search (computational cost vs potential convergence speed); convex vs nonconvex penalties (stronger convergence guarantees vs reduced bias)

- **Failure signatures**: Slow convergence (step size too conservative; try BB or reverse line search); divergence (step size too large; reduce step size or check Lipschitz estimation); no sparsity (regularization parameter too small; increase λ)

- **First 3 experiments**: 1) Synthetic data with known sparsity pattern: Compare ISTA-BB, ISTA-reverse, and FISTA-Lipschitz on ℓ1-norm regularized logistic regression 2) Real-world dataset (Ionosphere): Evaluate classification accuracy and feature selection performance vs state-of-the-art methods with ℓ1-norm and SCAD penalties 3) Scalability test: Vary features/samples in synthetic dataset to measure computational time and memory usage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed algorithms scale to extremely high-dimensional data (millions of features) in terms of memory usage and computational time?
- Basis: Paper discusses scalability with increasing features/samples but not extremely high-dimensional cases
- Why unresolved: Experiments focus on moderate-scale datasets (up to 500 features)
- What evidence would resolve: Empirical results on datasets with millions of features plus memory usage analysis

### Open Question 2
- Question: What is the impact of different nonconvex penalties (SCAD, MCP) on generalization error and feature selection accuracy?
- Basis: Paper mentions nonconvex penalties can reduce bias but doesn't compare different nonconvex penalties comprehensively
- Why unresolved: While framework handles nonconvex penalties, trade-offs between different nonconvex penalties aren't quantified
- What evidence would resolve: Detailed empirical study comparing generalization error and feature selection accuracy of various nonconvex penalties across multiple datasets

### Open Question 3
- Question: How does regularization parameter λ affect convergence speed and sparsity in both convex and nonconvex cases?
- Basis: Paper notes λ affects iterations to converge and sparsity but doesn't explore relationship systematically, especially for nonconvex penalties
- Why unresolved: Effects of λ mentioned but not analyzed systematically across wide range of values for both convex and nonconvex penalties
- What evidence would resolve: Comprehensive analysis of convergence speed and sparsity across wide range of λ values for both penalty types

## Limitations
- Results may not generalize to all sparse logistic regression problems beyond five UCI datasets tested
- Relies on specific assumptions about nonconvex penalties (DC structure with bounded Hessians) that may not hold for all penalty functions
- Computational advantages may vary with different hardware or software environments not tested in comparison

## Confidence
- **High Confidence**: Unified optimization framework for convex ℓ1-norm regularization and use of BB rules/reverse line search are well-established with strong theoretical foundations
- **Medium Confidence**: Extension to nonconvex penalties via DC decomposition and modified line search is plausible given assumptions but practical performance may vary
- **Low Confidence**: Scalability claims for large-scale problems based on limited empirical evidence may not hold for extremely high-dimensional or large-sample datasets

## Next Checks
1. **Theoretical Validation**: Rigorously prove convergence of modified ISTA/FISTA algorithms for wider class of nonconvex penalties beyond DC structure to establish stronger theoretical guarantees
2. **Empirical Validation**: Conduct extensive experiments on diverse real-world and synthetic datasets varying features, samples, and class imbalance to comprehensively evaluate performance vs state-of-the-art alternatives
3. **Implementation Validation**: Implement proposed algorithms in widely-used machine learning library (scikit-learn) and conduct benchmark study to ensure reproducibility and facilitate adoption by research community