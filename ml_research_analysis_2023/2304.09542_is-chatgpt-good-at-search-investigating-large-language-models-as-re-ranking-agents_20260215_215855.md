---
ver: rpa2
title: Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking
  Agents
arxiv_id: '2304.09542'
source_url: https://arxiv.org/abs/2304.09542
tags:
- passages
- generation
- passage
- llms
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of large language models (LLMs)
  like ChatGPT and GPT-4 for passage re-ranking in information retrieval. The authors
  propose a novel permutation generation approach that directly outputs a ranked list
  of passages.
---

# Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents

## Quick Facts
- arXiv ID: 2304.09542
- Source URL: https://arxiv.org/abs/2304.09542
- Reference count: 16
- Primary result: GPT-4 with permutation generation outperforms state-of-the-art supervised methods; distilled model outperforms 3B model with only 10K samples

## Executive Summary
This paper investigates the use of large language models (LLMs) like ChatGPT and GPT-4 for passage re-ranking in information retrieval. The authors propose a novel permutation generation approach that directly outputs ranked lists of passages, bypassing intermediate relevance scores. Experiments show that GPT-4 with this method outperforms state-of-the-art supervised re-ranking systems on standard benchmarks. To address efficiency concerns, they introduce a distillation method that transfers LLM ranking capabilities to smaller specialized models, achieving superior performance with minimal training data compared to fully supervised approaches.

## Method Summary
The authors propose three instructional methods for LLM-based re-ranking: query generation, relevance generation, and permutation generation. The permutation generation approach instructs LLMs to directly output ranked permutations of passages. A sliding window strategy enables ranking arbitrary numbers of passages by processing them in manageable chunks. For efficiency, they introduce permutation distillation that transfers ranking capabilities from ChatGPT to smaller specialized models using direct permutation matching as training targets, rather than intermediate representations.

## Key Results
- GPT-4 with permutation generation outperforms existing zero-shot methods and achieves competitive results with supervised systems
- The specialized model trained on 10K ChatGPT-generated samples outperforms monoT5 trained on 400K MS MARCO samples on BEIR benchmark
- GPT-4 shows higher reliability in generating permutations compared to text-davinci-003 and gpt-3.5-turbo
- On Mr.TyDi multilingual benchmark, GPT-4 performs better than supervised systems in most languages but lags behind in low-resource languages like Bengali, Telugu, and Thai

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Permutation generation method directly leverages LLMs' language understanding and reasoning abilities for ranking without intermediate relevance scores.
- Mechanism: Instead of asking LLMs to generate queries or relevance judgments, this method instructs them to directly output a ranked permutation of passages. This approach uses the LLM's capability to process multiple pieces of text and reason about their relative relevance in a single pass.
- Core assumption: LLMs have sufficient context window to process multiple passages and can reason about their relative relevance when given appropriate instructions.
- Evidence anchors:
  - [abstract] "We propose an alternative instructional permutation generation approach, which instructs the LLMs to directly output the permutations of a group of passages"
  - [section] "We introduce a novel instructional permutation generation method with a sliding windows strategy"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.473, average citations=0.0. Top related titles: Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models, Evaluating ChatGPT as a Recommender System: A Rigorous Approach, Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers.
- Break condition: When passage length exceeds LLM context window or when LLM lacks sufficient reasoning capability for multi-passage comparison.

### Mechanism 2
- Claim: Sliding window strategy enables ranking arbitrary number of passages by processing them in manageable chunks.
- Mechanism: The approach divides the ranking task into overlapping windows of passages, ranks each window, and uses the results to progressively refine the overall ranking. This allows the method to handle more passages than would fit in a single LLM context window.
- Core assumption: Ranking within smaller windows and combining results produces similar outcomes to ranking all passages at once.
- Evidence anchors:
  - [abstract] "we introduce a sliding windows strategy that allows LLMs to rank an arbitrary number of passages"
  - [section] "Figure 3 shows an example of re-ranking 8 passages using sliding windows"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.473, average citations=0.0. Top related titles: Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models, Evaluating ChatGPT as a Recommender System: A Rigorous Approach, Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers.
- Break condition: When step size is too large or when window size is too small relative to the total number of passages.

### Mechanism 3
- Claim: Permutation distillation transfers LLM ranking capabilities to smaller specialized models through direct permutation matching.
- Mechanism: Instead of using intermediate representations or log-probabilities, this method directly uses the permutation output from ChatGPT as training targets for a smaller model. The specialized model learns to replicate the ranking behavior without needing to match the full complexity of the LLM.
- Core assumption: The ranking behavior can be effectively distilled without preserving all LLM reasoning capabilities.
- Evidence anchors:
  - [abstract] "we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme"
  - [section] "Our small specialized model that trained on 10K ChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO data on BEIR"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.473, average citations=0.0. Top related titles: Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models, Evaluating ChatGPT as a Recommender System: A Rigorous Approach, Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers.
- Break condition: When the distilled model cannot capture the ranking patterns or when the permutation labels are inconsistent.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The paper investigates how LLMs can perform passage re-ranking without task-specific training data
  - Quick check question: What is the difference between zero-shot and few-shot learning in the context of LLMs?

- Concept: Information Retrieval metrics (nDCG)
  - Why needed here: The paper uses nDCG@1, nDCG@5, and nDCG@10 to evaluate ranking performance
  - Quick check question: How does nDCG differ from precision or recall in evaluating ranked results?

- Concept: Cross-encoder architecture
  - Why needed here: The specialized models use cross-encoder architecture to compute relevance scores between queries and passages
  - Quick check question: What is the key difference between cross-encoder and bi-encoder architectures in ranking tasks?

## Architecture Onboarding

- Component map:
  - LLM API (ChatGPT/GPT-4) for permutation generation
  - Sliding window manager for handling multiple passages
  - Specialized model (DeBERTa-v3-base) for distillation
  - Ranking evaluation pipeline (nDCG metrics)
  - Data pipeline for MS MARCO sampling and BM25 retrieval

- Critical path:
  1. Retrieve passages using BM25
  2. Apply sliding window strategy to group passages
  3. Send each window to LLM for permutation generation
  4. Combine window results into final ranking
  5. For distillation: use LLM permutations as training labels
  6. Evaluate using nDCG metrics

- Design tradeoffs:
  - Window size vs. context window limits
  - Step size vs. ranking quality
  - API call frequency vs. cost
  - Model size vs. inference speed
  - Listwise vs. pairwise vs. pointwise loss functions

- Failure signatures:
  - Duplicate or missing passage identifiers in LLM output
  - LLM refusing to rank passages
  - Inconsistent rankings across sliding windows
  - High API costs exceeding budget
  - Poor nDCG scores on validation sets

- First 3 experiments:
  1. Test permutation generation with small window size (5 passages) on TREC dataset
  2. Compare sliding window step sizes (5, 10, 15) on ranking consistency
  3. Evaluate different distillation loss functions (Listwise CE, RankNet, LambdaLoss) on BEIR dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs vary across different languages, especially low-resource languages?
- Basis in paper: [explicit] The authors note that GPT-4 performs better than supervised systems in most languages but lags behind in certain low-resource languages such as Bengali, Telugu, and Thai.
- Why unresolved: The paper does not provide a detailed analysis of why the performance varies across languages or how to improve performance in low-resource languages.
- What evidence would resolve it: Further experiments analyzing the performance of LLMs across a wider range of languages, including low-resource languages, and exploring methods to improve performance in these languages.

### Open Question 2
- Question: How can the efficiency of LLMs be improved for real-world applications?
- Basis in paper: [inferred] The authors mention that current LLMs have hundreds of billions of parameters, making them expensive to deploy in real industrial scenarios. They suggest reducing model size and lightweight learning as potential directions.
- Why unresolved: The paper does not provide specific methods or results for improving the efficiency of LLMs.
- What evidence would resolve it: Research demonstrating successful methods for reducing the size and computational requirements of LLMs while maintaining or improving their performance.

### Open Question 3
- Question: How can the stability of LLMs in generating rankings be improved?
- Basis in paper: [explicit] The authors analyze the stability of different models (text-davinci-003, gpt-3.5-turbo, and gpt-4) and find that gpt-4 has higher reliability in generating permutations compared to the other models.
- Why unresolved: The paper does not provide a detailed analysis of the factors affecting model stability or methods to improve it.
- What evidence would resolve it: Further research investigating the causes of instability in LLM-generated rankings and proposing methods to improve stability, such as model architecture changes or training techniques.

## Limitations

- The method requires multiple expensive API calls to LLMs, potentially costing hundreds of dollars per query set
- The sliding window strategy may not capture global ranking relationships effectively when combining rankings across windows
- The permutation generation method is susceptible to output inconsistencies, with models occasionally producing duplicate or missing passage identifiers

## Confidence

- **High Confidence:** The core finding that GPT-4 with permutation generation outperforms existing zero-shot methods is well-supported by experimental results across multiple benchmarks (TREC, BEIR, Mr.TyDi). The methodology for evaluation is clearly defined and reproducible.
- **Medium Confidence:** The distillation approach shows promising results, with a 10K sample specialized model outperforming a fully supervised 3B model on BEIR. However, the evaluation is limited to a single specialized model architecture (DeBERTa-v3-base) and specific loss functions.
- **Medium Confidence:** The comparative analysis between instructional methods (query generation, relevance generation, permutation generation) is methodologically sound, though the performance differences between methods on some datasets are relatively modest.

## Next Checks

1. **Scalability Testing:** Conduct experiments to measure how ranking quality degrades as the number of passages increases beyond the context window size, testing the limits of the sliding window strategy.

2. **Cost-Benefit Analysis:** Quantify the exact API costs for different approaches across various dataset sizes and compare these costs against performance gains, particularly for the permutation generation method.

3. **Robustness Evaluation:** Test the consistency of permutation generation outputs across multiple runs with the same inputs, measuring ranking stability and investigating failure cases where models produce invalid permutations.