---
ver: rpa2
title: Towards Reliable Dermatology Evaluation Benchmarks
arxiv_id: '2309.06961'
source_url: https://arxiv.org/abs/2309.06961
tags:
- samples
- label
- errors
- auroc
- auprg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We identified and corrected data quality issues in six widely-used
  dermatology evaluation datasets, finding up to 2.8% irrelevant samples and near
  duplicates, and up to 8.8% label errors. Our protocol combines algorithmic ranking
  with expert confirmation and an efficient stopping criterion, achieving up to 32,520x
  speed-up over exhaustive annotation.
---

# Towards Reliable Dermatology Evaluation Benchmarks

## Quick Facts
- arXiv ID: 2309.06961
- Source URL: https://arxiv.org/abs/2309.06961
- Reference count: 40
- Primary result: Data quality issues identified in six dermatology datasets, with up to 2.8% irrelevant samples and near duplicates and up to 8.8% label errors found

## Executive Summary
This work presents a protocol for cleaning dermatology evaluation datasets to improve the reliability of model assessments. The authors combine an algorithmic cleaning strategy with expert confirmation and an efficient stopping criterion, achieving up to 32,520x speed-up over exhaustive annotation. They identify and remove irrelevant samples, near duplicates, and estimate label error prevalence in six widely-used dermatology datasets, demonstrating significant performance changes (up to 3.6%) in skin cancer detection models post-cleaning.

## Method Summary
The protocol uses SelfClean, a self-supervised learning-based algorithm, to rank samples by likelihood of being data quality issues. Samples are then confirmed by multiple dermatologists using a binary question workflow with a stopping criterion that terminates annotation after n_clean consecutive negative responses. The stopping criterion is set to achieve a specified probability of observing consecutive negative annotations by chance. Post-cleaning, the datasets have irrelevant samples and near duplicates removed, with label error prevalence estimated.

## Key Results
- Up to 2.8% irrelevant samples and near duplicates identified and removed across six dermatology datasets
- Up to 8.8% label errors estimated across the studied datasets
- Model performance changes of up to 3.6% in skin cancer detection observed after cleaning
- Up to 32,520x speed-up achieved compared to exhaustive expert annotation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The stopping criterion reduces annotation effort while maintaining data quality detection accuracy
- Mechanism: Uses a statistical threshold based on the probability of observing consecutive negative annotations by chance, terminating after n_clean consecutive negatives
- Core assumption: The probability of a sample being a data quality issue is constant across the ranking
- Evidence anchors:
  - [section]: "Specifically, we proceed along the SelfClean ranking and stop the annotation process after receiving nclean consecutive negative responses. We set nclean by requesting that the probability of observing the sequence of negative annotations as a result of chance be lower than pchance, where the probability for each sample to be a data quality issue is p+ (or less). This yields nclean = ⌊ln(pchance)/ ln(1 − p+)⌋, where in this work we set pchance = p+ = 0.05, resulting in nclean = 58."
  - [abstract]: "Our protocol combines algorithmic ranking with expert confirmation and an efficient stopping criterion, achieving up to 32,520x speed-up over exhaustive annotation."

### Mechanism 2
- Claim: Unanimous expert agreement reduces subjectivity in data quality issue confirmation
- Mechanism: Requiring all three experts to flag a sample as a data quality issue before confirming it as such
- Core assumption: Multiple expert opinions reduce individual bias and subjectivity
- Evidence anchors:
  - [section]: "After the confirmation process, we conservatively require unanimous expert agreement to identify an issue. Specifically, a sample is considered noise only if all experts flag it as such."
  - [abstract]: "Based on confirmation by multiple dermatologists, we remove irrelevant samples and near duplicates and estimate the percentage of label errors in six dermatology image datasets"

### Mechanism 3
- Claim: Self-supervised learning enables effective data quality detection without additional labeled data
- Mechanism: Self-supervised pre-training creates dataset-specific latent representations that capture semantic similarity for distance-based criteria
- Core assumption: The latent space learned through self-supervised learning preserves meaningful relationships between samples for quality detection
- Evidence anchors:
  - [section]: "SelfClean first uses self-supervised pre-training on a noisy dataset to obtain an encoder that maps samples onto a dataset-specific latent space. The method can be applied in combination with any SSL approach, but the original work compared SimCLR (Chen et al., 2020) and DINO (Caron et al., 2021) and found the latter to have better performance."
  - [abstract]: "The protocol leverages an existing algorithmic cleaning strategy and is followed by a confirmation process terminated by an intuitive stopping criterion."

## Foundational Learning

- Concept: Self-supervised learning for representation learning
  - Why needed here: Enables creating meaningful latent representations without requiring additional labeled data for the quality detection task
  - Quick check question: How does self-supervised learning differ from supervised learning in terms of label requirements and application to data cleaning?

- Concept: Inter-annotator agreement measurement
  - Why needed here: Provides quantitative assessment of annotation reliability and consistency across experts
  - Quick check question: What does Krippendorff's alpha measure, and how does it differ from Cohen's kappa?

- Concept: Statistical stopping criteria
  - Why needed here: Balances annotation effort with detection accuracy by using probability thresholds to determine when to stop
  - Quick check question: How does the stopping criterion formula relate to the desired probability of false negatives?

## Architecture Onboarding

- Component map: Data preparation -> Self-supervised learning module (DINO) -> Quality detection engine (distance-based criteria) -> Expert annotation interface (binary questions) -> Performance evaluation (AUROC, AP, AUPRG)
- Critical path: Pre-training → Latent space generation → Distance-based ranking → Expert confirmation → Issue removal
- Design tradeoffs:
  - Pre-training depth vs. annotation speed
  - Strictness of unanimous agreement vs. detection completeness
  - Parameter sensitivity of stopping criterion vs. robustness
- Failure signatures:
  - Poor ranking quality: Low AUROC/AP scores in Table 3
  - Annotation disagreement: Low Krippendorff's alpha in Figure 2
  - Inefficient annotation: High speed-up factor but many missed issues
- First 3 experiments:
  1. Validate pre-training effectiveness by comparing DINO vs. SimCLR on a small dataset
  2. Test stopping criterion sensitivity by varying pchance and p+ parameters
  3. Evaluate expert agreement robustness by comparing with non-expert annotations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the proposed data cleaning protocol when applied to datasets significantly larger than the six dermatology evaluation datasets studied?
- Basis in paper: [inferred] The authors mention a 32,520x speed-up for near-duplicate detection on small to medium datasets and claim the procedure may be used independently of domain, but do not provide evidence for larger datasets.
- Why unresolved: The paper focuses on six specific small-scale dermatology datasets without empirical evidence or analysis for much larger datasets.
- What evidence would resolve it: Empirical results showing performance on datasets at least an order of magnitude larger than the largest studied (SD-128 with 5,619 images), including speed-up factors, detection accuracy, and resource requirements.

### Open Question 2
- Question: To what extent does the choice of self-supervised learning (SSL) pre-training strategy impact the effectiveness of the data cleaning protocol, and are there specific SSL approaches more suitable for identifying certain types of data quality issues in medical imaging datasets?
- Basis in paper: [explicit] The authors state the protocol can be used with any SSL approach but chose DINO based on previous work without exploring other SSL strategies or comparing their effectiveness for data cleaning.
- Why unresolved: While mentioning SSL flexibility, the authors do not explore or compare the impact of different SSL strategies on protocol effectiveness for specific data quality issue types.
- What evidence would resolve it: Systematic comparison of protocol performance using different SSL approaches (SimCLR, DINO, Barlow Twins, BYOL) for each data quality issue type across various medical imaging datasets.

### Open Question 3
- Question: How does the data cleaning protocol handle and mitigate potential biases introduced by the choice of expert annotators, and what strategies ensure effectiveness across diverse medical imaging domains?
- Basis in paper: [inferred] The authors use three practicing domain experts but do not discuss strategies for mitigating potential biases or ensuring effectiveness across diverse domains.
- Why unresolved: The paper relies on expert confirmation without addressing how expert choice might introduce biases or how to ensure protocol effectiveness across diverse medical imaging domains.
- What evidence would resolve it: Empirical studies comparing protocol performance with different expert sets across diverse backgrounds, and application to various medical imaging domains to analyze generalizability and potential mitigation strategies.

## Limitations

- The protocol's effectiveness for datasets significantly larger than those studied remains uncertain
- The generalizability to other medical imaging domains beyond dermatology is not empirically validated
- Label error detection shows weaker agreement with experts compared to other data quality issue types

## Confidence

- High confidence: Data quality issue identification mechanisms (irrelevant samples and near duplicates) due to strong agreement between SelfClean rankings and expert annotations (AUROC 0.70-0.94)
- Medium confidence: Label error detection due to weaker agreement with experts (AUROC 0.62-0.72), suggesting the current approach may not fully capture diagnostic disagreements

## Next Checks

1. Validate the stopping criterion robustness by testing on datasets with different probability distributions of data quality issues
2. Compare expert agreement rates between dermatologists and non-expert medical professionals to assess domain expertise requirements
3. Evaluate model performance changes on a held-out test set to confirm that cleaning improvements reflect genuine evaluation reliability gains rather than overfitting