---
ver: rpa2
title: 'SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets'
arxiv_id: '2308.11880'
source_url: https://arxiv.org/abs/2308.11880
tags:
- source
- adaptation
- data
- domain
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of adapting models trained on\
  \ independent uni-modal source data to multi-modal target domains without access\
  \ to the original source data. The proposed method, SUMMIT, introduces a switching\
  \ framework that automatically chooses between two complementary pseudo-label fusion\
  \ strategies\u2014agreement filtering and entropy weighting\u2014based on the estimated\
  \ domain gap."
---

# SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets

## Quick Facts
- arXiv ID: 2308.11880
- Source URL: https://arxiv.org/abs/2308.11880
- Authors: 
- Reference count: 40
- One-line primary result: Up to 12% gains in mIoU over competing source-free baselines on multi-modal adaptation tasks

## Executive Summary
This paper addresses the problem of adapting models trained on independent uni-modal source data to multi-modal target domains without access to the original source data. The proposed method, SUMMIT, introduces a switching framework that automatically chooses between two complementary pseudo-label fusion strategies—agreement filtering and entropy weighting—based on the estimated domain gap. The approach enables cross-modal learning by fusing pseudo-labels across modalities without requiring paired source data or access to the original source dataset. Experiments on seven challenging adaptation scenarios demonstrate significant improvements, with up to 12% gains in mIoU over competing source-free baselines. The method achieves results comparable to, and in some cases better than, approaches that assume access to source data, validating its effectiveness in practical scenarios where source data is unavailable.

## Method Summary
SUMMIT addresses source-free adaptation by leveraging two pre-trained uni-modal models (2D and 3D) to create pseudo-labels for target data. The method employs a switching framework that automatically selects between agreement filtering (which retains only mutually agreed predictions across modalities) and entropy weighting (which combines predictions using uncertainty-based weights). The selection is based on estimated domain gap, calculated by comparing agreement rates between source models on source versus target data. The adapted models are trained using weighted cross-entropy loss with estimated class distribution and cross-modal consistency loss, enabling knowledge transfer across modalities without requiring paired source data.

## Key Results
- Achieves up to 12% gains in mIoU over competing source-free baselines on multi-modal adaptation tasks
- Demonstrates performance comparable to, and in some cases better than, approaches that assume access to source data
- Shows significant improvements across seven challenging adaptation scenarios in autonomous driving domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal pseudo-label fusion improves segmentation accuracy by filtering out low-confidence predictions
- Mechanism: The method fuses pseudo-labels from independent uni-modal models by comparing predictions across modalities. When both modalities agree on a label, it is retained; otherwise, it is discarded. This filtering process reduces noise in the pseudo-labels used for adaptation.
- Core assumption: Agreement between modalities indicates higher confidence in the prediction
- Evidence anchors:
  - [abstract]: "Our proposed approach solves this problem through a switching framework which automatically chooses between two complementary methods of cross-modal pseudo-label fusion – agreement filtering and entropy weighting"
  - [section]: "We propose a switching framework that automatically chooses between two complementary methods to fuse pseudo-labels across modalities in order to obtain a single pseudo-label to supervise adaptation"
  - [corpus]: "Weak corpus support - no direct evidence found in neighbors about cross-modal pseudo-label fusion mechanisms"
- Break condition: If modalities have systematic biases that cause consistent but incorrect agreement, the filtering could reinforce errors

### Mechanism 2
- Claim: Entropy weighting combines predictions from different modalities using uncertainty-weighted probabilities
- Mechanism: The method calculates entropy from each model's output probabilities to determine uncertainty weights. Lower entropy (higher confidence) predictions receive higher weights in the linear combination of probabilities across modalities.
- Core assumption: Entropy of output probabilities inversely correlates with prediction confidence
- Evidence anchors:
  - [abstract]: "entropy weighting – based on the estimated domain gap"
  - [section]: "Entropy weighting (EW) explores an alternative information-theoretic method for combining pseudo-labels"
  - [corpus]: "Weak corpus support - no direct evidence found in neighbors about entropy weighting for domain adaptation"
- Break condition: If both modalities have high entropy but happen to agree, the weighted combination could still propagate uncertainty

### Mechanism 3
- Claim: Automatic switching between fusion methods based on domain gap estimation improves adaptation performance
- Mechanism: The method estimates domain gap by comparing agreement rates between source models on source vs target data. When agreement rates are similar (low domain gap), entropy weighting is used; when they differ significantly (high domain gap), agreement filtering is selected.
- Core assumption: Agreement rate between models is inversely proportional to domain gap
- Evidence anchors:
  - [abstract]: "Our proposed approach solves this problem through a switching framework which automatically chooses between two complementary methods...based on the estimated domain gap"
  - [section]: "We compare these two likelihoods using hypothesis ratio testing...If the ratio between them is close to unity...EW is chosen as the fusion method"
  - [corpus]: "Weak corpus support - no direct evidence found in neighbors about automatic switching based on domain gap estimation"
- Break condition: If agreement rate is affected by factors other than domain gap (e.g., class imbalance), the switching mechanism may select suboptimal fusion

## Foundational Learning

- Concept: Unsupervised Domain Adaptation (UDA)
  - Why needed here: The paper addresses adapting models trained on one data distribution (source) to perform well on a different but related distribution (target) without labeled target data
  - Quick check question: What distinguishes unsupervised from supervised domain adaptation in terms of available target data?

- Concept: Source-free adaptation
  - Why needed here: The setting assumes source data is unavailable during adaptation due to privacy, security, or economic concerns, requiring methods that work with only the pre-trained source model
  - Quick check question: How does source-free adaptation differ from traditional domain adaptation in terms of required resources?

- Concept: Cross-modal learning
  - Why needed here: The method leverages semantic relationships between different input modalities (e.g., RGB images and point clouds) to improve adaptation performance beyond what individual modalities could achieve
  - Quick check question: Why might combining information from multiple modalities be more effective than adapting each modality independently?

## Architecture Onboarding

- Component map:
  Source models (M2D, M3D) -> Feature encoders (f2D, f3D) -> Pseudo-label generators -> Fusion module -> Switching module -> Adaptation module

- Critical path:
  1. Generate pseudo-labels from source models on target data
  2. Apply chosen fusion method (agreement filtering or entropy weighting)
  3. Estimate domain gap to determine fusion method for next iteration
  4. Train target models using fused pseudo-labels and cross-modal consistency loss
  5. Repeat until convergence

- Design tradeoffs:
  - Complexity vs performance: Automatic switching adds complexity but improves robustness across different domain gap scenarios
  - Memory vs accuracy: Storing intermediate statistics for hypothesis testing increases memory usage but enables better pseudo-label refinement
  - Modality independence vs correlation: Treating modalities independently during source training vs learning cross-modal correlations during adaptation

- Failure signatures:
  - Degraded performance when source and target domain gaps are misestimated
  - Overfitting to noisy pseudo-labels if filtering is too permissive
  - Underutilization of cross-modal information if switching mechanism selects suboptimal fusion method

- First 3 experiments:
  1. Run adaptation with only agreement filtering on a small dataset subset to verify basic functionality
  2. Test entropy weighting alone on a scenario with small domain gap to validate uncertainty-based fusion
  3. Verify automatic switching mechanism by creating controlled scenarios with varying domain gaps and checking method selection

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The automatic switching mechanism's effectiveness depends heavily on accurate domain gap estimation, which may be influenced by factors beyond domain shift
- Cross-modal pseudo-label fusion assumes semantic consistency between modalities, but systematic biases in individual models could propagate errors
- The method's scalability to more than two modalities remains unexplored

## Confidence
- High confidence: The core switching framework between agreement filtering and entropy weighting is well-specified and technically sound
- Medium confidence: Performance improvements over baselines are demonstrated, but comparison against oracle methods with source data access is limited
- Low confidence: The theoretical justification for agreement rate as a proxy for domain gap estimation lacks rigorous validation

## Next Checks
1. Conduct ablation studies on synthetic datasets with controlled domain gaps to verify the accuracy of the switching mechanism's domain gap estimation
2. Test the method's robustness to systematic biases by introducing known systematic errors into one modality and measuring cross-modal error propagation
3. Evaluate scalability by extending the approach to scenarios with three or more modalities and measuring performance degradation points