---
ver: rpa2
title: 'Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion'
arxiv_id: '2311.07682'
source_url: https://arxiv.org/abs/2311.07682
tags:
- knowledge
- shortcuts
- fusion
- shortcut
- shared
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the effects of model fusion on reducing unwanted
  knowledge, such as shortcuts, social biases, and memorization, in fine-tuned language
  models. The core idea is that shared knowledge among models is preserved during
  fusion, while unshared knowledge is forgotten or degraded.
---

# Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion

## Quick Facts
- **arXiv ID:** 2311.07682
- **Source URL:** https://arxiv.org/abs/2311.07682
- **Reference count:** 19
- **Primary result:** Model fusion can reduce social biases by up to 68% without deteriorating task performance and reduce memorization while improving generalization

## Executive Summary
This paper explores model fusion as a technique to reduce unwanted knowledge in fine-tuned language models, including shortcuts, social biases, and memorization of training data. The core insight is that shared knowledge among models is preserved during fusion while unshared knowledge is forgotten or degraded. Through experiments on synthetic and real-world tasks, the authors demonstrate that model fusion can serve as a debiasing tool and address privacy concerns. The results show significant reductions in social biases and memorization while maintaining or improving task performance, suggesting model fusion has potential for mitigating biases and addressing privacy concerns in NLP applications.

## Method Summary
The method involves fine-tuning multiple BERT-base models independently on different data subsets, then fusing them through weighted averaging of their parameters. For synthetic shortcut experiments, special tokens are injected into the SST2 dataset to create controlled scenarios. Social bias experiments use the PAN16 dataset with age and gender attributes. Memorization experiments use CNN-DM news summaries with disjoint training subsets. The fused models are evaluated on task performance, shortcuts, biases (using Demographic Parity and TPR-GAP metrics), and memorization using separate validation sets.

## Key Results
- Model fusion reduces social biases by up to 68% without deteriorating task performance
- Fusion of models trained on disjoint datasets reduces memorization while improving generalization
- Shared knowledge among models is preserved during fusion while unshared knowledge is forgotten or degraded

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shared knowledge among models is preserved during fusion while unshared knowledge is forgotten
- Mechanism: The fusion process amplifies commonalities in model weights that represent shared skills, while differences in unshared knowledge lead to destructive interference when averaging weights
- Core assumption: Knowledge can be represented as embedded latent traits within model parameters, and these traits can be differentially preserved or lost during weighted averaging
- Evidence anchors:
  - [abstract] "shared knowledge among models is usually enhanced during model fusion, while unshared knowledge is usually lost or forgotten"
  - [section] "min ΨD,T (θi) ≤ ΨD,T (θf used) ≤ max ΨD,T (θi)" - This mathematical relationship shows fused model performance is bounded by individual models
  - [corpus] Weak - No direct citations found, though related work on model unlearning exists

### Mechanism 2
- Claim: Different shortcuts or biases learned by different models can be selectively forgotten through fusion
- Mechanism: Models trained on different data subsets develop distinct shortcuts or biases. When fused, the unique shortcuts/biases cancel out while the core task knowledge remains
- Core assumption: Shortcuts and biases are learned in parameter subspaces that don't overlap significantly between models trained on different data distributions
- Evidence anchors:
  - [section] "we observe that the accuracy for the original task is preserved, but shortcuts are forgotten midway" - Direct observation from synthetic shortcut experiments
  - [section] "The results demonstrate that the fused model almost perfectly forgets all shortcuts" - Quantitative results from multiple shortcut fusion
  - [corpus] Moderate - Related to work on "Selective Influence Machine Unlearning" and "Textual Sequence Memorization Erasure"

### Mechanism 3
- Claim: Memorization of training data can be reduced through fusion of models trained on disjoint datasets
- Mechanism: Models memorize specific examples from their training data. When fused, examples not shared across training sets are less likely to be preserved, reducing overall memorization
- Core assumption: Memorization occurs in parameter subspaces specific to individual training examples, and these subspaces don't overlap significantly between models trained on different data
- Evidence anchors:
  - [section] "the fused model demonstrates comparable or lower memorization scores than the base model on each dataset except the shared one" - Direct experimental results
  - [section] "as more models are fused, the unshared memorized examples are more easily forgotten" - Scaling behavior observed in experiments
  - [corpus] Weak - Related concepts exist but no direct citations found

## Foundational Learning

- Concept: Fisher Information Matrix and its role in measuring knowledge representation in neural networks
  - Why needed here: Understanding how knowledge is distributed across model parameters is crucial for explaining why some knowledge is preserved and some is lost during fusion
  - Quick check question: How does the Fisher overlap metric help explain the differential preservation of shared versus unshared knowledge during model fusion?

- Concept: Synthetic shortcut injection and controlled experiments
  - Why needed here: The ability to create controlled scenarios with known shortcuts is essential for validating the mechanism of selective forgetting
  - Quick check question: Why does the paper use special tokens instead of existing vocabulary tokens when injecting synthetic shortcuts?

- Concept: Debiasing metrics (Demographic Parity, TPR-Gap)
  - Why needed here: Proper measurement of bias reduction is necessary to quantify the effectiveness of model fusion as a debiasing tool
  - Quick check question: What is the difference between Demographic Parity and TPR-GAP in measuring bias?

## Architecture Onboarding

- Component map: Data preparation -> Model training -> Fusion layer -> Evaluation
- Critical path: 
  1. Prepare datasets with controlled variations (shortcuts, biases, disjoint memorization)
  2. Train multiple independent models on different data subsets
  3. Fuse models using weighted averaging
  4. Evaluate preservation of shared knowledge and forgetting of unshared knowledge
  5. Measure impact on task performance, shortcuts, biases, and memorization
- Design tradeoffs: Simple weight averaging vs. more sophisticated fusion methods (computational simplicity vs. potential for better control)
- Failure signatures: If task performance drops significantly after fusion, shared knowledge may not be properly preserved; if shortcuts/biases are not reduced, models may be learning them in similar parameter regions; if memorization is not reduced, training data subsets may have too much overlap
- First 3 experiments:
  1. Pair interpolation between models with different synthetic shortcuts to observe preservation/forgetting patterns
  2. Fusion of models trained on bias-controlled datasets to measure debiasing effectiveness
  3. Fusion of models trained on disjoint memorization datasets to quantify privacy benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which model fusion leads to the preservation of shared knowledge and the forgetting of unshared knowledge?
- Basis in paper: [inferred] The paper discusses the preservation of shared knowledge and the forgetting of unshared knowledge during model fusion, but the exact mechanism is not explicitly stated
- Why unresolved: The paper does not provide a detailed explanation of the underlying mechanism that causes shared knowledge to be preserved while unshared knowledge is forgotten during model fusion
- What evidence would resolve it: A detailed analysis of the model parameters and their interactions during fusion, possibly through techniques like Fisher Information Matrix analysis, could provide insights into the mechanism

### Open Question 2
- Question: How does the number of models being fused affect the degree of bias reduction and memorization prevention?
- Basis in paper: [explicit] The paper mentions that as more models are fused, unshared memorized examples are more easily forgotten and shared examples are memorized better
- Why unresolved: The paper does not provide a quantitative analysis of how the number of fused models impacts the degree of bias reduction and memorization prevention
- What evidence would resolve it: Experiments varying the number of models being fused and measuring the resulting changes in bias reduction and memorization could provide a clearer understanding of this relationship

### Open Question 3
- Question: Are there specific types of biases or shortcuts that are more susceptible to being forgotten during model fusion than others?
- Basis in paper: [inferred] The paper discusses the forgetting of shortcuts and biases during model fusion, but does not explicitly address whether certain types are more susceptible than others
- Why unresolved: The paper does not provide a comparative analysis of different types of biases or shortcuts in terms of their susceptibility to being forgotten during model fusion
- What evidence would resolve it: Experiments comparing the degree of forgetting for different types of biases or shortcuts during model fusion could shed light on this question

## Limitations
- The mechanism of selective forgetting depends on the assumption that different models learn shortcuts, biases, and memorization in distinct parameter subspaces, but this is not rigorously validated
- Experiments focus on synthetic shortcuts and controlled bias settings, which may not generalize to more complex real-world scenarios
- The theoretical explanation for why the fusion mechanism works selectively lacks rigorous analysis of parameter space interactions

## Confidence
- **High confidence**: The empirical demonstration that model fusion can reduce social biases (up to 68%) and memorization while preserving task performance
- **Medium confidence**: The claim that model fusion serves as a general debiasing tool
- **Low confidence**: The theoretical explanation for why the fusion mechanism works selectively

## Next Checks
1. Perform ablation studies that analyze the overlap of parameter subspaces used for shared versus unshared knowledge across models
2. Test the debiasing effectiveness on more complex, real-world biased datasets beyond the controlled PAN16 dataset
3. Systematically vary the fusion weights beyond simple averaging to determine whether the selective forgetting property is robust to different weighting schemes