---
ver: rpa2
title: Explanation-based Training with Differentiable Insertion/Deletion Metric-aware
  Regularizers
arxiv_id: '2310.12553'
source_url: https://arxiv.org/abs/2310.12553
tags:
- insertion
- deletion
- accuracy
- id-expo
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of producing faithful explanations
  for predictions made by complex machine learning models. The authors propose a novel
  approach called Insertion/Deletion Metric-aware Explanation-based Optimization (ID-ExpO)
  to improve the faithfulness of explanations generated by post-hoc explainers.
---

# Explanation-based Training with Differentiable Insertion/Deletion Metric-aware Regularizers

## Quick Facts
- arXiv ID: 2310.12553
- Source URL: https://arxiv.org/abs/2310.12553
- Reference count: 40
- Primary result: ID-ExpO significantly improves insertion and deletion scores across all datasets, outperforming existing stability-aware and fidelity-aware explanation-based optimization techniques.

## Executive Summary
This paper addresses the challenge of producing faithful explanations for predictions made by complex machine learning models. The authors propose a novel approach called Insertion/Deletion Metric-aware Explanation-based Optimization (ID-ExpO) to improve the faithfulness of explanations generated by post-hoc explainers. The key idea is to optimize differentiable predictors to enhance both insertion and deletion scores of explanations while maintaining predictive accuracy. Since the original insertion and deletion metrics are non-differentiable, the authors extend them to be differentiable and formalize them as regularizers. Experiments on image and tabular datasets demonstrate that fine-tuning deep neural network-based predictors using ID-ExpO enables popular post-hoc explainers to produce more faithful and easier-to-interpret explanations while maintaining high predictive accuracy.

## Method Summary
ID-ExpO optimizes differentiable predictors to improve both insertion and deletion scores of explanations while maintaining predictive accuracy. The method uses differentiable approximations of insertion and deletion metrics as regularizers in the training objective. Soft step functions approximate the non-differentiable masking operations, enabling gradient flow from the metrics back to the predictor. The training combines cross-entropy loss with insertion/deletion regularizers and an L2 penalty on the attribution map. The approach can be applied to both post-hoc explainers and inherently interpretable models without changing the predictor architecture.

## Key Results
- ID-ExpO significantly improves insertion and deletion scores across all tested datasets (CIFAR-10, STL-10, and six tabular datasets).
- The method outperforms existing stability-aware and fidelity-aware explanation-based optimization techniques.
- Fine-tuned models maintain high predictive accuracy while producing more faithful and interpretable explanations.
- Improvements are observed across different explainers (LIME, KernelSHAP, Grad-CAM) and model architectures (ResNet-18 for images, MLP for tabular data).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing the predictor's internal representations to align with the post-hoc explainer's attribution pathway improves explanation faithfulness.
- Mechanism: The insertion/deletion metric-aware regularizers penalize predictions that fail to depend strongly on the features the explainer assigns high importance. By making the predictor more sensitive to those features, the explainer's attribution map becomes more faithful to the actual predictive signal.
- Core assumption: The explainer's attribution map is a differentiable function of the predictor's internal representations.
- Evidence anchors:
  - [abstract] "optimize differentiable predictors to improve both insertion and deletion scores of the explanations"
  - [section] "ID-ExpO can be applied to both post-hoc explainers and inherently interpretable models because it does not require changing the architecture of the predictors"
- Break condition: If the explainer's attribution map is not differentiable (e.g., if argmax or hard masking is used without smoothing), the regularizer gradients cannot flow back to the predictor.

### Mechanism 2
- Claim: Soft step functions allow gradients to flow through the insertion/deletion computation, enabling end-to-end training.
- Mechanism: By replacing hard argmax-based masking with sigmoid-based soft step functions (controlled by a temperature parameter T), the insertion/deletion metrics become differentiable with respect to the explainer's attribution map, which in turn is differentiable with respect to the predictor.
- Core assumption: The soft step approximation is sufficiently smooth for gradient-based optimization to improve the insertion/deletion scores.
- Evidence anchors:
  - [section] "we approximate (6) with soft step functions as follows: αsoft(x, ϕ; b, s)ijk = r(ϕjk; s)xijk + (1 − r(ϕjk; s))bijk"
  - [section] "By using (7) and (8) as the mask functions in (1) and (3), we obtain the insertion and deletion metrics differentiable with respect to ϕ"
- Break condition: If T is too large, the soft step functions become too sharp, losing differentiability; if T is too small, the approximation deviates too far from the true metric, yielding ineffective gradients.

### Mechanism 3
- Claim: Truncating the insertion/deletion evaluation to the top fraction of important pixels focuses optimization on the most discriminative features.
- Mechanism: By evaluating insertion/deletion metrics only over the top 30% or 50% of pixels (s = 0.3 · HW or 0.5 · HW), the regularizer encourages the predictor to rely on a compact, salient subset of features, which improves both faithfulness and interpretability.
- Core assumption: A small subset of pixels dominates the prediction for the true class, so optimizing over them suffices.
- Evidence anchors:
  - [section] "the evaluation of the insertion and deletion scores in our regularizers is truncated at 30% or 50% of the number of pixels"
  - [section] "large positive contributions were assigned to a part of super-pixels that captures the object of the class label well"
- Break condition: If the class decision boundary requires more than the top fraction of features, truncation will force the predictor to ignore necessary signals, degrading accuracy.

## Foundational Learning

- Concept: Differentiable approximation of non-differentiable operations
  - Why needed here: The original insertion/deletion metrics use hard masking via argmax, which blocks gradient flow. Soft step functions approximate this masking while preserving differentiability.
  - Quick check question: How does the temperature parameter T in the sigmoid-based soft step function affect the sharpness of the approximation and the resulting gradient magnitude?

- Concept: Regularization balancing prediction accuracy and explanation quality
  - Why needed here: The ID-ExpO loss combines cross-entropy, insertion/deletion regularizers, and an L2 penalty on the attribution map. Proper weighting ensures that the predictor does not sacrifice accuracy to improve faithfulness.
  - Quick check question: What happens to the insertion/deletion scores if λ1 and λ2 are set to zero while λ3 remains positive?

- Concept: Feature importance truncation in evaluation
  - Why needed here: Limiting the metric evaluation to the top fraction of important pixels reduces computational cost and focuses optimization on the most salient features.
  - Quick check question: If S is set to HW instead of a fraction, how does the computational complexity and the attribution map change?

## Architecture Onboarding

- Component map: Predictor (fθ) -> Explainer (e) -> Soft step mask functions (αsoft, βsoft) -> Regularizers (ΩIns, ΩDel) -> Training loop (SGD)

- Critical path:
  1. Forward pass: Compute predictor output, explainer attribution map, and masked predictions.
  2. Compute insertion/deletion regularizers using soft step masks.
  3. Combine with cross-entropy and L2 penalties.
  4. Backward pass: Gradients flow from regularizers through explainer to predictor.
  5. Update predictor parameters with SGD.

- Design tradeoffs:
  - Temperature T: Larger T yields sharper masks but risks vanishing gradients; smaller T yields smoother masks but may approximate the metric poorly.
  - Fraction S: Smaller S focuses optimization on fewer pixels (faster, potentially more interpretable) but risks underfitting; larger S improves coverage but increases computation.
  - λ1, λ2 weights: Higher weights prioritize explanation faithfulness over accuracy; lower weights preserve accuracy but may not improve faithfulness.

- Failure signatures:
  - Accuracy collapses: λ1, λ2 too high or S too small, forcing the predictor to ignore necessary features.
  - Gradients vanish: T too large, making soft step masks effectively binary.
  - Overfitting to metrics: λ1, λ2 dominate, leading to adversarial attribution maps that score well but are misleading.

- First 3 experiments:
  1. Verify differentiability: Run a forward pass with a fixed input, then compute ∂fθ(x)y/∂ϕjk using autograd; confirm non-zero values.
  2. Ablation study: Train with (λ1=λ2=0) vs (λ1=λ2>0) and measure insertion/deletion score changes.
  3. Sensitivity to S: Train identical models with S=0.3·HW, 0.5·HW, and HW; compare faithfulness and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed ID-ExpO method perform on inherently interpretable models compared to post-hoc explainers?
- Basis in paper: [explicit] The authors mention that ID-ExpO can be applied to both post-hoc explainers and inherently interpretable models because it does not require changing the architecture of the predictors. They also state that in future work, they will verify the effectiveness of ID-ExpO by applying it to the faithfulness of explanations by inherently interpretable models [11] and parameterized explainers [41].
- Why unresolved: The experiments conducted in the paper focus on post-hoc explainers (LIME, KernelSHAP, and Grad-CAM) and do not include inherently interpretable models.
- What evidence would resolve it: Experimental results comparing the performance of ID-ExpO on inherently interpretable models versus post-hoc explainers in terms of insertion and deletion scores, as well as predictive accuracy.

### Open Question 2
- Question: How does the performance of ID-ExpO vary with different values of the temperature parameter T in the soft mask functions?
- Basis in paper: [explicit] The authors mention that the temperature parameter T controls the value of the soft step function r(ϕjk; s) and discuss its role in preventing the value from being nearly equal to zero or one. However, they do not provide an analysis of how different values of T affect the performance of ID-ExpO.
- Why unresolved: The paper does not include an experimental analysis of the sensitivity of ID-ExpO to the temperature parameter T.
- What evidence would resolve it: Experimental results showing the impact of different values of T on the insertion and deletion scores, as well as predictive accuracy, for various datasets and explainers.

### Open Question 3
- Question: How does the performance of ID-ExpO compare to other methods that optimize explanations, such as saliency-guided training [33] or teaching the machine to explain itself using domain knowledge [46]?
- Basis in paper: [inferred] The authors mention related work on explanation-based optimization, including saliency-guided training [33] and methods that use domain knowledge [46]. However, they do not provide a direct comparison between ID-ExpO and these methods.
- Why unresolved: The paper does not include experimental results comparing ID-ExpO to other explanation optimization methods.
- What evidence would resolve it: Experimental results comparing the performance of ID-ExpO to other explanation optimization methods (e.g., saliency-guided training, domain knowledge-based methods) in terms of insertion and deletion scores, as well as predictive accuracy, on various datasets and explainers.

## Limitations

- The soft step function approximation's effectiveness depends heavily on the temperature parameter T, which is not well-characterized in the paper.
- The truncation strategy assumes class decisions depend on compact feature sets, which may not hold for complex decision boundaries.
- The method's dependence on differentiable explainers limits its applicability to non-differentiable attribution methods.

## Confidence

- **High Confidence**: The core mechanism of using differentiable regularizers to improve explanation faithfulness is well-supported by the mathematical formulation and experimental results.
- **Medium Confidence**: The soft step function approximation's effectiveness across diverse models and datasets requires further validation, particularly regarding optimal temperature settings.
- **Medium Confidence**: The truncation strategy's impact on different types of decision boundaries and its generalizability beyond image datasets needs more investigation.

## Next Checks

1. **Temperature Sensitivity Analysis**: Systematically vary the temperature parameter T across multiple orders of magnitude to identify the optimal range where gradients remain meaningful while maintaining metric fidelity.

2. **Decision Boundary Complexity Test**: Evaluate ID-ExpO on datasets with known complex decision boundaries (e.g., spiral datasets, XOR-like patterns) to assess performance when decisions require distributed feature sets.

3. **Non-differentiable Explainer Compatibility**: Test the method's performance when applied to popular non-differentiable explainers (e.g., Anchors, counterfactual explanations) using approximation techniques.