---
ver: rpa2
title: The GUA-Speech System Description for CNVSRC Challenge 2023
arxiv_id: '2312.07254'
source_url: https://arxiv.org/abs/2312.07254
tags:
- system
- recognition
- speech
- decoder
- inter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper describes a visual speech recognition system developed
  for the CNVSRC 2023 challenge. The proposed approach combines several key techniques:
  intermediate CTC residual modules to relax the conditional independence assumption,
  a bi-transformer decoder for capturing past and future context, and Chinese character
  modeling units.'
---

# The GUA-Speech System Description for CNVSRC Challenge 2023

## Quick Facts
- arXiv ID: 2312.07254
- Source URL: https://arxiv.org/abs/2312.07254
- Reference count: 0
- Primary result: 38.09% CER on evaluation set, 21.63% relative improvement over baseline

## Executive Summary
This paper describes a visual speech recognition system developed for the CNVSRC 2023 challenge. The proposed approach combines several key techniques: intermediate CTC residual modules to relax the conditional independence assumption, a bi-transformer decoder for capturing past and future context, and Chinese character modeling units. The system achieves a character error rate of 38.09% on the evaluation set, representing a 21.63% relative improvement over the baseline. The ablation study shows that each component (Inter CTC, bi-transformer decoder, character units, and RNNLM) contributes to performance gains.

## Method Summary
The system uses a Conformer encoder with 12 layers and 3 intermediate CTC residual modules, combined with a bi-transformer decoder (6 left + 3 right decoders). Chinese characters serve as modeling units (vocabulary of 4466 characters). The model is trained from scratch using curriculum learning on CN-CVS and CNVSRC-Single.Dev datasets, then fine-tuned on the challenge training set. During inference, joint CTC/attention decoding with RNNLM shallow fusion is applied (CTC weight 0.3, LM weight 0.1, beam size 40).

## Key Results
- Achieves 38.09% character error rate on evaluation set
- 21.63% relative improvement over baseline
- Each component (Inter CTC, bi-transformer decoder, character units, RNNLM) contributes to performance gains
- Uses Chinese characters as modeling units to improve recognition accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inter CTC residual modules relax the conditional independence assumption of standard CTC by conditioning intermediate predictions on earlier encoder outputs
- Mechanism: Intermediate predictions from conformer blocks are summed to the next layer's input, allowing later blocks to be influenced by earlier temporal predictions rather than relying solely on raw visual features
- Core assumption: The intermediate CTC predictions provide useful conditioning signals that improve overall sequence modeling
- Evidence anchors: [abstract] states "use intermediate connectionist temporal classification (Inter CTC) residual modules to relax the conditional independence assumption of CTC"; [section 2.2.1] provides equations showing how Z_l (intermediate predictions) are added to X_l+1_in (next block input)
- Break condition: If intermediate predictions are noisy or contradictory, they may degrade rather than improve later block performance

### Mechanism 2
- Claim: Bi-transformer decoder captures both past and future context by training separate left-to-right and right-to-left decoders
- Mechanism: Left decoder learns p(y_l|y_1:l-1, X_e) while right decoder learns p(y_l|y_L:l+1, X_e), with their losses combined using weight α
- Core assumption: Future context is useful for lip reading despite being unavailable during inference
- Evidence anchors: [abstract] states "use a bi-transformer decoder to enable the model to capture both past and future contextual information"; [section 2.2.2] provides loss equations for left and right decoders and their combination
- Break condition: If future context information is too ambiguous or creates training-inference mismatch, performance may degrade

### Mechanism 3
- Claim: Using Chinese characters as modeling units improves recognition accuracy for Chinese speech
- Mechanism: Chinese characters better capture linguistic units than phonemes or subword units for Mandarin, reducing ambiguity in lip reading
- Core assumption: Chinese characters provide more distinctive visual cues than other unit types for this language
- Evidence anchors: [abstract] states "use Chinese characters as the modeling units to improve the recognition accuracy"; [section 3.1] mentions "We use Chinese characters as modeling units, the vocabulary consists of 4466 Chinese characters"
- Break condition: If visual distinction between character mouth movements is insufficient, character-level modeling may not help

## Foundational Learning

- Concept: CTC (Connectionist Temporal Classification)
  - Why needed here: CTC provides alignment between variable-length visual sequences and character sequences without explicit frame labeling
  - Quick check question: What is the main limitation of standard CTC that Inter CTC modules address?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Both conformer encoders and transformer decoders rely on self-attention to capture long-range dependencies in visual speech
  - Quick check question: How does multi-head attention help capture different aspects of lip movement patterns?

- Concept: Residual connections and skip connections
  - Why needed here: Residual modules help gradient flow through deep networks and allow intermediate predictions to influence later layers
  - Quick check question: Why are residual connections particularly important in visual speech recognition models?

## Architecture Onboarding

- Component map: Visual front-end (3D conv + ResNet-18) → Conformer encoder (12 layers, 3 Inter CTC modules) → Bi-transformer decoder (6 left + 3 right) → CTC layer → RNNLM
- Critical path: Visual features → Conformer encoder (with Inter CTC) → Bi-transformer decoder → CTC loss + attention loss
- Design tradeoffs:
  - Inter CTC modules add complexity but relax CTC assumptions
  - Bi-transformer decoder uses future context for training but only past during inference
  - Character units reduce vocabulary size but may lose sub-character information
- Failure signatures:
  - High CER with Inter CTC but lower without suggests poor intermediate predictions
  - Performance gap between training and inference indicates bi-transformer decoder issues
  - Character errors clustered around homophones suggest character unit limitations
- First 3 experiments:
  1. Train without Inter CTC modules to verify their contribution to the 21.63% relative improvement
  2. Train with only left decoder (no bi-transformer) to isolate the contribution of future context
  3. Train with subword units instead of characters to test modeling unit impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal configuration of intermediate CTC residual modules (number of modules, placement, and hyper-parameters) for visual speech recognition?
- Basis in paper: [explicit] The paper mentions using 3 Inter CTC residual modules placed after every three conformer blocks, but states this was chosen as a configuration without providing extensive experimentation on alternatives
- Why unresolved: The paper only reports using K=3 modules without exploring other configurations (e.g., K=1, 2, 4, or different placement patterns) to determine the optimal setup
- What evidence would resolve it: Systematic experiments varying the number of Inter CTC modules, their placement within the encoder, and associated hyper-parameters (γ in Equation 7) while measuring CER on validation sets

### Open Question 2
- Question: How does the bi-transformer decoder perform on longer sequences compared to shorter ones in visual speech recognition?
- Basis in paper: [inferred] The paper uses a bi-transformer decoder but only reports overall CER results without analyzing performance across different utterance lengths or sequence complexities
- Why unresolved: The ablation study shows the bi-transformer decoder improves performance overall, but doesn't examine whether its effectiveness varies with sequence length or complexity, which could inform architectural decisions
- What evidence would resolve it: Detailed performance analysis showing CER breakdown by utterance length, or controlled experiments with sequences of varying complexity to determine the bi-transformer decoder's effectiveness across different scenarios

### Open Question 3
- Question: What is the contribution of each conformer block to the final recognition performance in the visual speech recognition pipeline?
- Basis in paper: [inferred] The paper uses 12 conformer blocks with 3 Inter CTC modules, but doesn't analyze the individual contribution of each block or whether all blocks are necessary for optimal performance
- Why unresolved: While the paper demonstrates that the full 12-block architecture works well, it doesn't explore whether fewer blocks could achieve similar performance, or which blocks contribute most to recognition accuracy
- What evidence would resolve it: Progressive experiments removing or modifying individual conformer blocks while measuring CER, or attention analysis to identify which blocks are most critical for recognition performance

### Open Question 4
- Question: How does the proposed system scale to multi-speaker visual speech recognition tasks?
- Basis in paper: [explicit] The paper specifically addresses single-speaker visual speech recognition in Task 1, with no discussion or experimentation on multi-speaker scenarios
- Why unresolved: The current system is optimized for a single speaker and uses speaker-specific modeling units (Chinese characters), but doesn't address the challenges of generalizing to multiple speakers with different speaking styles and visual characteristics
- What evidence would resolve it: Experiments applying the proposed architecture to multi-speaker datasets, including speaker adaptation techniques, speaker embedding integration, and performance comparison with single-speaker results

## Limitations

- Limited ablation evidence showing only that "each component contributes to performance gains" without specific performance numbers for individual components
- No comparative analysis against other state-of-the-art visual speech recognition methods on standard benchmarks
- Implementation specificity with reported gains only for Chinese visual speech data without testing generalizability

## Confidence

**High Confidence Claims**:
- The system architecture description (conformer encoder, bi-transformer decoder, CTC components) is clearly specified and technically sound
- The 38.09% CER on the evaluation set is a factual measurement result
- The use of Chinese characters as modeling units is correctly implemented as described

**Medium Confidence Claims**:
- The claimed 21.63% relative improvement over baseline is reported but lacks detailed ablation evidence
- The effectiveness of Inter CTC modules and bi-transformer decoder mechanisms are theoretically justified but lack empirical validation through component-wise ablation
- The curriculum learning approach likely contributed to performance but specific details are insufficient for full replication

**Low Confidence Claims**:
- The claim that Inter CTC modules "relax the conditional independence assumption" is theoretically plausible but lacks direct experimental evidence showing this specific mechanism
- The effectiveness of bi-transformer decoder for capturing future context during training is asserted but the training-inference mismatch is not addressed
- The superiority of Chinese character modeling units over alternatives is claimed without comparative experiments

## Next Checks

1. **Component-wise Ablation Study**: Conduct systematic ablation experiments to quantify the individual contributions of Inter CTC modules, bi-transformer decoder, Chinese character units, and RNNLM. Report CER for each configuration to validate the 21.63% relative improvement claim.

2. **Cross-dataset Generalization Test**: Evaluate the system on an external Chinese visual speech dataset (not used in training) to assess whether the reported improvements generalize beyond the challenge-specific data.

3. **Training-Inference Consistency Analysis**: Design experiments to measure the performance gap when using bi-transformer decoder during inference (only past context) versus training (both past and future context). This would quantify the impact of the training-inference mismatch.