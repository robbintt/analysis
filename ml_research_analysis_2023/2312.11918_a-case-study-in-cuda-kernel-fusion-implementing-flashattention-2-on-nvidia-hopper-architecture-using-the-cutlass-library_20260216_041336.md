---
ver: rpa2
title: 'A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA
  Hopper Architecture using the CUTLASS Library'
arxiv_id: '2312.11918'
source_url: https://arxiv.org/abs/2312.11918
tags:
- cutlass
- nvidia
- layout
- gemm
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper implements FlashAttention-2's forward pass on NVIDIA
  Hopper architecture using CUTLASS, fusing softmax with GEMM operations. Key techniques
  include leveraging Tensor Memory Accelerator (TMA) for asynchronous memory copies,
  using Warpgroup Matrix-Multiply-Accumulate (WGMMA) instructions, defining and transforming
  CUTLASS Layouts and Tensors, overlapping copy and GEMM operations, and optimizing
  tile sizes while balancing register pressure and shared memory.
---

# A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library

## Quick Facts
- arXiv ID: 2312.11918
- Source URL: https://arxiv.org/abs/2312.11918
- Reference count: 27
- Key outcome: Implements FlashAttention-2 forward pass on NVIDIA Hopper (H100) using CUTLASS, achieving 20-50% higher FLOPs/s compared to Ampere-optimized version

## Executive Summary
This paper presents a CUDA kernel fusion implementation of FlashAttention-2's forward pass on NVIDIA Hopper architecture using the CUTLASS library. The key innovation is fusing softmax with GEMM operations to eliminate intermediate global memory writes, leveraging Hopper's Tensor Memory Accelerator (TMA) and Warpgroup Matrix-Multiply-Accumulate (WGMMA) instructions. The implementation achieves significant performance improvements over last-generation Ampere architecture through reduced memory traffic and better utilization of specialized hardware.

## Method Summary
The implementation fuses the softmax computation with matrix multiplications by keeping intermediate results in shared memory rather than writing to global memory. It uses CUTLASS's CuTe library for layout transformations between GEMM operations and leverages Hopper's TMA for asynchronous memory copies. The kernel employs WGMMA instructions for high-throughput matrix multiplications and implements an online softmax algorithm that computes attention scores while accumulating matrix products. The authors experimented with different tile sizes to balance register pressure and shared memory usage.

## Key Results
- Achieves 20-50% higher FLOPs/s on H100 compared to Ampere-optimized FlashAttention-2
- Reduces memory traffic by eliminating intermediate matrices S and P from global memory
- Successfully implements online softmax within GEMM loop using warp-level operations
- Tile size selection critical: 128Ã—128 causes register spills and performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Kernel fusion reduces global memory reads/writes, directly improving performance by minimizing memory wall effects.
- Mechanism: Standard attention requires writing intermediate matrices S and P to global memory between GEMM operations. By fusing these operations into a single kernel, the implementation avoids costly HBM round-trips, keeping intermediate results in shared memory or registers.
- Core assumption: The fused kernel can maintain sufficient shared memory and register usage to hold intermediate results without spilling, and the computational overhead of the softmax can be offset by memory savings.
- Evidence anchors:
  - [abstract] "The main benefit from kernel fusion comes from reducing the number of reads from and writes to global memory, which is the largest and slowest level of the GPU memory hierarchy."
  - [section 1] "Materializing the matrices ð‘† and ð‘ƒ to HBM... adversely impacts the overall runtime as well as the memory requirement."
- Break condition: If shared memory or register pressure becomes too high, forcing spills or serialization, the performance gains from fusion will be negated.

### Mechanism 2
- Claim: Using Tensor Memory Accelerator (TMA) and Warpgroup Matrix-Multiply-Accumulate (WGMMA) instructions provides significant performance improvements over Ampere architecture.
- Mechanism: TMA enables asynchronous copying from global memory to shared memory, overlapping memory transfers with computation. WGMMA instructions are optimized for Hopper's tensor cores, providing higher throughput for matrix multiplications compared to older instructions used on Ampere.
- Core assumption: The overhead of setting up and managing TMA and WGMMA is less than the performance gains from their specialized hardware acceleration.
- Evidence anchors:
  - [abstract] "We observe 20-50% higher FLOPs/s over a version of FlashAttention-2 optimized for last-generation NVIDIA Ampere architecture."
  - [section 3.2] "Hopper introduces the dedicated Tensor Memory Accelerator (TMA) unit for asynchronous copying from gmem to smem."
- Break condition: If the kernel launch overhead or TMA setup time outweighs the benefits of the specialized instructions, or if the problem size is too small to benefit from warpgroup-level parallelism.

### Mechanism 3
- Claim: Careful layout transformations and reshaping of tensors between GEMM-I and GEMM-II are critical for performance.
- Mechanism: The accumulator from GEMM-I has a different layout than the operand required for GEMM-II. The implementation uses CuTe's Layout composition and reshaping functions to efficiently transform the data layout without unnecessary copies, enabling seamless transition between operations.
- Core assumption: The CuTe Layout system can accurately represent the required data transformations and that the reshaping operations are efficient enough to not introduce significant overhead.
- Evidence anchors:
  - [section 4.2] "The definition of the Tensor tOrP featuring in GEMM-II involves a custom layout transformation method ReshapeTStoTP."
  - [section 3.1] "Tensors can be 'owning' (e.g., in registers) or 'non-owning' (e.g., a view, in the C++20 sense, of global or shared memory)."
- Break condition: If the layout transformation logic is incorrect or too complex, it could lead to incorrect results or performance degradation.

## Foundational Learning

- Concept: GPU memory hierarchy (global memory, shared memory, registers) and memory coalescing
  - Why needed here: Understanding how data flows through different memory levels is crucial for optimizing kernel fusion and avoiding memory bottlenecks.
  - Quick check question: What is the typical bandwidth difference between global memory and shared memory on an H100 GPU, and why does this matter for kernel fusion?

- Concept: CUDA thread organization (warps, thread blocks, grids) and warp-level operations
  - Why needed here: The implementation relies heavily on warp-level operations (like shuffle instructions) for reductions and data exchange, which are essential for the online softmax and other parallel operations.
  - Quick check question: How many threads are in a warp on Hopper architecture, and what is the purpose of the __shfl_xor_sync instruction?

- Concept: Matrix multiplication optimization (tile sizes, register blocking, shared memory tiling)
  - Why needed here: Choosing optimal tile sizes for Q, K, and V matrices is critical for balancing register pressure and shared memory usage, directly impacting performance.
  - Quick check question: What is the relationship between tile size, register usage, and shared memory usage in a CUDA GEMM kernel, and how does this affect the choice of tile sizes for the attention mechanism?

## Architecture Onboarding

- Component map:
  CUTLASS/CuTe library -> TMA unit -> WGMMA instructions -> Online softmax algorithm

- Critical path:
  1. TMA copy of Q, K, and V tiles to shared memory
  2. GEMM-I (Q x K^T) with online softmax computation
  3. Layout transformation of accumulator to operand for GEMM-II
  4. GEMM-II (P x V) with output rescaling
  5. Write final output to global memory

- Design tradeoffs:
  - Shared memory vs. register pressure: Larger tile sizes improve arithmetic intensity but increase register pressure, potentially causing spills.
  - TMA vs. manual copying: TMA simplifies code but may have less flexibility than manual shared memory management.
  - Precision formats: FP16 for operands and FP32 for accumulators balance performance and accuracy.

- Failure signatures:
  - Performance degradation: Indicates register spills, bank conflicts, or insufficient overlap of memory and computation.
  - Incorrect results: Suggests errors in layout transformations, softmax implementation, or tile size calculations.
  - Compilation errors: May point to issues with CuTe abstractions or unsupported hardware features.

- First 3 experiments:
  1. Benchmark the fused kernel with different tile sizes (e.g., 64x64, 128x128, 64x128) to find the optimal balance of register pressure and shared memory usage.
  2. Compare performance with and without TMA to measure the impact of asynchronous memory copies.
  3. Profile the kernel to identify any serialization points or memory bottlenecks and adjust the code accordingly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal tile size configuration for FlashAttention-2 on Hopper architecture that balances register pressure and shared memory utilization while maximizing performance?
- Basis in paper: [explicit] The paper states "128 Ã— 128 suffers from performance degradation due to register pressure" and experimented with different sizes for QBLK and KBLK in the range (64, 128) Ã— (64, 128).
- Why unresolved: The paper found that 128 Ã— 128 tile size led to register spills and serialization issues, but did not identify a definitive optimal configuration that balances all factors.
- What evidence would resolve it: Systematic benchmarking results across a wider range of tile sizes and configurations, including detailed analysis of register usage, shared memory utilization, and achieved TFLOPs/s for each configuration.

### Open Question 2
- Question: How would implementing a two warpgroup (256 threads) per CTA with proper warp specialization scheme affect the performance of FlashAttention-2 on Hopper architecture?
- Basis in paper: [explicit] The paper mentions "we extended our implementation to use two warpgroups (256 threads) per every tile of operand A of WGMMA. Even though this increases the register space per CTA, the end performance was worse."
- Why unresolved: The authors state the performance was worse but do not provide detailed analysis of why this occurred or whether optimizations could improve the two warpgroup approach.
- What evidence would resolve it: Implementation and benchmarking of a properly optimized two warpgroup version with detailed analysis of register allocation, memory access patterns, and performance metrics compared to the single warpgroup approach.

### Open Question 3
- Question: What is the potential performance improvement from introducing more pipelining stages into the COPY-GEMM overlapping in FlashAttention-2?
- Basis in paper: [explicit] The paper states "we can instead exploit the structure of the FMHA algorithm... Rather than pipelining for a single GEMM, we issue the loads for GEMM-II with the GEMM-I call and vice-versa" but acknowledges this is a potential future optimization.
- Why unresolved: The authors chose not to implement more pipelining stages due to software engineering complexity and increased shared memory requirements, but did not quantify the potential performance gains.
- What evidence would resolve it: Implementation of a multi-stage pipelined version with detailed performance comparison against the current approach, including analysis of memory bandwidth utilization, instruction throughput, and overall TFLOPs/s.

## Limitations

- The paper does not provide a definitive optimal tile size configuration, noting that 128Ã—128 causes register spills and performance degradation
- Layout transformation details between GEMM-I and GEMM-II are abstracted through CuTe without full implementation specifics
- Numerical stability measures for the online softmax implementation are not detailed, raising potential correctness concerns

## Confidence

- **High Confidence**: The overall approach of fusing softmax with GEMM operations and using TMA and WGMMA instructions is well-supported by the CUDA programming model and Hopper architecture specifications. The performance improvement claims (20-50% higher FLOPs/s) are reasonable given the architectural differences between Ampere and Hopper.
- **Medium Confidence**: The effectiveness of the CuTe Layout composition and reshaping functions for transforming the accumulator layout between GEMM-I and GEMM-II is assumed to be efficient and correct, but this relies heavily on the correctness of the CuTe library implementation.
- **Low Confidence**: The paper does not provide sufficient detail on the specific numerical stability measures taken in the online softmax implementation, nor does it address potential edge cases that could arise from the fused kernel design.

## Next Checks

1. **Tile Size Sensitivity Analysis**: Systematically test the kernel with a range of tile sizes (e.g., 64x64, 64x128, 128x64, 128x128) across different head dimensions to identify the optimal configuration and understand the impact of register pressure and shared memory usage.

2. **Numerical Stability Verification**: Compare the output of the fused kernel against a reference implementation (e.g., PyTorch's attention) for various input distributions to ensure numerical accuracy and investigate any discrepancies.

3. **Memory Overlap Profiling**: Profile the kernel to quantify the actual overlap between TMA memory copies and WGMMA computations, and identify any serialization points that could be optimized further.