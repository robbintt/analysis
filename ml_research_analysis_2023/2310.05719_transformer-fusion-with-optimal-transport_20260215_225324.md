---
ver: rpa2
title: Transformer Fusion with Optimal Transport
arxiv_id: '2310.05719'
source_url: https://arxiv.org/abs/2310.05719
tags:
- fusion
- alignment
- different
- seed
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a systematic approach for fusing multiple transformer-based
  neural networks using Optimal Transport to align their architectural components.
  The key idea is to treat the flow of transportation maps as a graph, allowing for
  intuitive fusion of heterogeneous models.
---

# Transformer Fusion with Optimal Transport

## Quick Facts
- arXiv ID: 2310.05719
- Source URL: https://arxiv.org/abs/2310.05719
- Reference count: 40
- One-line primary result: OTFusion with soft alignment improves transformer fusion accuracy by ~1.0% over vanilla fusion and individual converged models.

## Executive Summary
This paper proposes a systematic approach for fusing multiple transformer-based neural networks using Optimal Transport (OT) to align their architectural components. The key insight is that naive weight averaging fails due to neuron misalignment across models, and OT provides a principled way to establish correspondences before averaging. The method introduces a graph-based abstraction for propagating transportation maps through complex transformer components like residual connections, multi-head attention, and normalization layers. Experiments demonstrate that soft alignment via Sinkhorn regularization is critical for transformer fusion, achieving consistent gains over vanilla fusion and individual models on both Vision Transformers and BERT architectures.

## Method Summary
The method applies Optimal Transport-based fusion with soft alignment to align weight matrices or activations across transformer layers. It uses a graph-based transportation map flow to handle complex architectural components, with specific rules for propagating alignments through residual connections, multi-head attention, and embeddings. The Sinkhorn algorithm enables soft alignment by tuning a regularization parameter that controls the balance between hard and soft transportation maps. After alignment and averaging, fused models are finetuned using AdamW optimizer with cosine learning rate scheduling. The approach supports both homogeneous and heterogeneous model fusion, with the latter enabling model compression through width reduction.

## Key Results
- OTFusion with soft alignment achieves ~1.0% average accuracy improvement over vanilla fusion and individual converged models on CIFAR10, CIFAR100, Tiny ImageNet, and ImageNet-1K
- Soft alignment consistently outperforms hard alignment for transformers, contrary to findings on simpler architectures
- The method enables efficient fusion of models with different sizes, providing a new approach to transformer compression
- Transportation map flow graph abstraction successfully generalizes to complex transformer components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal Transport alignment outperforms vanilla weight averaging by accounting for neuron misalignment across models.
- Mechanism: OTFusion computes a transportation map between neurons of different models, allowing soft or hard assignment of corresponding neurons before averaging, preserving meaningful information transfer.
- Core assumption: Neurons at the same positions in different models encode different information, so naive averaging loses this variation.
- Evidence anchors:
  - [abstract] "However, this method overlooks potential misalignments between the parameter matrices, arising due to neurons at the same positions, in different models, encoding different information"
  - [section] "Singh & Jaggi (2020) applies this theory to align networks in a layerwise fashion, using either weights or activations as underlying distributions. After the alignment of one or more models to an anchor model, these are then averaged."
  - [corpus] Weak: related papers focus on graph or model fusion but not specifically on transformer fusion; corpus does not directly support this mechanism.
- Break condition: If the cost function used for OT does not capture meaningful similarity between neurons, the alignment becomes meaningless and performance degrades.

### Mechanism 2
- Claim: Soft alignment via Sinkhorn regularization is critical for transformer fusion, unlike for simpler architectures.
- Mechanism: The Sinkhorn algorithm allows soft transportation maps rather than hard permutation matrices, enabling flexible neuron assignment that adapts to complex transformer structures.
- Core assumption: The optimal alignment for transformers is neither fully hard nor fully soft; intermediate regularization yields better performance.
- Evidence anchors:
  - [abstract] "We uncover that, surprisingly, OTFusion based on a hard-alignment underperforms in this context, contrary to the case of fully-connected or convolutional architectures; and that, soft-alignment plays a key role in successful one-shot fusion."
  - [section] "However, we don't want to limit the search space for optimal alignment to only permutation matrices, as it seems too constraining for complex architectures. We, therefore, explore using the Sinkhorn algorithm and tuning the softness of the TM by optimizing over the Sinkhorn regularizer."
  - [corpus] Weak: related work on optimal transport fusion does not discuss transformer-specific softness benefits.
- Break condition: If regularization is too high, the transportation map approaches uniform distribution, erasing useful structure; too low yields hard alignment that fails for transformers.

### Mechanism 3
- Claim: The transportation map flow graph abstraction enables systematic fusion of heterogeneous transformer components.
- Mechanism: By representing each network layer as a node and TMs as edges, the method propagates alignment information through residual connections, multi-head attention, and normalization layers consistently.
- Core assumption: The TM flow can be generalized across arbitrary architectural components if their internal computation is modeled as non-learnable nodes in the graph.
- Evidence anchors:
  - [abstract] "We flesh out an abstraction for layer alignment, that can generalize to arbitrary architectures – in principle – and we apply this to the key ingredients of Transformers such as multi-head self-attention, layer-normalization, and residual connections"
  - [section] "With a modular architecture like the transformer, it is intuitive to use a divide-and-conquer approach to develop a fusion algorithm. Therefore, we first divide the architecture into its simplest building block — fully connected layers — that can be fused by the prevalent OTFusion strategy."
  - [corpus] Weak: no corpus entries discuss graph-based TM flow for fusion.
- Break condition: If the TM propagation through a component is incorrectly modeled, alignment fails and fused model performance drops.

## Foundational Learning

- Concept: Optimal Transport and the Wasserstein distance
  - Why needed here: OTFusion uses OT to align neurons across models before averaging; understanding the underlying theory explains why it outperforms vanilla fusion.
  - Quick check question: What is the key difference between hard alignment (EMD) and soft alignment (Sinkhorn) in OT?
- Concept: Transportation Map Flow Graph abstraction
  - Why needed here: This abstraction allows systematic fusion of complex architectures by modeling how TMs propagate through non-learnable operations like residuals and attention.
  - Quick check question: In the TM flow graph, how are residual connections represented and how are their TMs combined?
- Concept: Soft vs Hard Alignment tuning
  - Why needed here: The optimal softness level differs by architecture; for transformers, soft alignment with tuned regularization outperforms hard alignment.
  - Quick check question: What happens to the transportation map as the Sinkhorn regularizer approaches zero or infinity?

## Architecture Onboarding

- Component map: Input embeddings -> Multi-head self-attention (WQ, WK, WV, WO) -> Layer normalization -> Residual connections -> Feed-forward networks -> Output class token
- Critical path: Input → Embedding concatenation → TM propagation through attention and residuals → Output class token → Classification head
- Design tradeoffs:
  - Hard vs soft alignment: Hard alignment is simpler but underperforms for transformers; soft alignment requires Sinkhorn optimization but yields better results.
  - Weight-based vs activation-based alignment: Weight-based is straightforward but activation-based can capture functional similarity; both require TM propagation through the architecture.
  - Homogeneous vs heterogeneous fusion: Heterogeneous fusion allows compression but inherits depth-matching limitations from OTFusion.
- Failure signatures:
  - One-shot accuracy near random indicates broken alignment.
  - Finetuning plateaus below parent model accuracy suggests poor initial fusion.
  - High variance across seeds indicates sensitivity to initialization or data ordering.
- First 3 experiments:
  1. Fuse two identical ViT models with hard alignment (EMD) and measure one-shot accuracy to establish baseline.
  2. Repeat with soft alignment (Sinkhorn) and sweep regularization parameter to find optimal softness.
  3. Test heterogeneous fusion (different model widths) to confirm compression capability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different regularizer values in Sinkhorn-Knapp on the performance of Transformer model fusion?
- Basis in paper: [explicit] The paper discusses the use of the Sinkhorn-Knapp algorithm for soft alignment and mentions that the softness of the alignment is controlled by a regularization parameter λsinkhorn. It also presents results showing the effect of different values of the Sinkhorn regularizer on the one-shot performance.
- Why unresolved: The paper does not provide a clear explanation of the optimal value of the Sinkhorn regularizer or its impact on the performance of Transformer model fusion. It only mentions that the value is chosen to maximize the one-shot accuracy separately for activations- and weights-based alignment.
- What evidence would resolve it: Experiments with different values of the Sinkhorn regularizer and their corresponding impact on the performance of Transformer model fusion would provide insights into the optimal value and its impact on the fusion process.

### Open Question 2
- Question: How does the choice of filtering strategy for activations-based alignment affect the performance of Transformer model fusion?
- Basis in paper: [explicit] The paper mentions the use of different filtering strategies for activations-based alignment, such as window filtering and using only the class token. It also presents results showing the impact of different filtering strategies on the one-shot performance.
- Why unresolved: The paper does not provide a clear explanation of which filtering strategy is optimal or how it affects the performance of Transformer model fusion. It only mentions that the best filtering strategy is used for the fusion process.
- What evidence would resolve it: Experiments with different filtering strategies and their corresponding impact on the performance of Transformer model fusion would provide insights into the optimal filtering strategy and its effect on the fusion process.

### Open Question 3
- Question: How does the choice of transport map policy for residual connections affect the performance of Transformer model fusion?
- Basis in paper: [explicit] The paper discusses different transport map policies for residual connections, such as averaging, weighted scalar, and weighted matrix. It also presents results showing the impact of different transport map policies on the one-shot performance.
- Why unresolved: The paper does not provide a clear explanation of which transport map policy is optimal or how it affects the performance of Transformer model fusion. It only mentions that the best transport map policy is used for the fusion process.
- What evidence would resolve it: Experiments with different transport map policies and their corresponding impact on the performance of Transformer model fusion would provide insights into the optimal transport map policy and its effect on the fusion process.

## Limitations

- Limited heterogeneous fusion validation: The paper provides sparse results on heterogeneous fusion, primarily focusing on width reduction while leaving depth reduction largely unexplored.
- Computational overhead unaddressed: The paper mentions efficiency benefits but does not quantify the time and memory requirements for the Sinkhorn optimization during fusion.
- Cross-attention extension unverified: The claim that cross-attention fusion is a "straightforward" extension is not empirically validated with multimodal experiments.

## Confidence

**High Confidence**: The core mechanism of using Optimal Transport for transformer fusion with soft alignment (Sinkhorn) is well-supported by the experimental results showing consistent improvements over vanilla fusion and individual models.

**Medium Confidence**: The TM flow graph abstraction and its application to complex transformer components (residuals, attention, embeddings) appears sound based on the ablation studies, but the general applicability to arbitrary architectures remains theoretical without broader empirical validation.

**Low Confidence**: The claims about heterogeneous fusion capabilities and the potential for arbitrary architecture fusion are based on limited experimental evidence.

## Next Checks

1. **Architecture Diversity Test**: Apply OTFusion to fuse transformers with significantly different depths (e.g., ViT-S vs ViT-B) and verify if the method maintains performance gains, addressing the gap in heterogeneous depth fusion.

2. **Computational Overhead Measurement**: Benchmark the time and memory requirements for the Sinkhorn optimization during fusion across different model sizes and compare with vanilla fusion to quantify the practical efficiency benefits.

3. **Cross-Attention Extension Validation**: Implement and test the proposed cross-attention fusion approach on a multimodal task (e.g., ViT-B/16 with BERT) to empirically verify that the "straightforward" extension works as claimed.