---
ver: rpa2
title: A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction
arxiv_id: '2308.03891'
source_url: https://arxiv.org/abs/2308.03891
tags:
- data
- span
- causal
- extraction
- sets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work compares four models for causal knowledge extraction
  across four diverse data sets. Sequence tagging models (SCITE, GCE, BERT) and a
  span-based model (SpERT) are evaluated using exact and partial matching metrics.
---

# A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction

## Quick Facts
- arXiv ID: 2308.03891
- Source URL: https://arxiv.org/abs/2308.03891
- Reference count: 4
- Primary result: Span-based models outperform sequence tagging models, with BERT-based models consistently achieving the best performance across four diverse causal extraction datasets

## Executive Summary
This study evaluates four models for causal knowledge extraction across four diverse datasets spanning scientific, medical, business, and financial domains. The models include three sequence tagging approaches (SCITE, GCE, and BERT-based taggers) and one span-based model (SpERT). Results demonstrate that span-based models consistently outperform sequence taggers, particularly when maximum span length is tuned per domain. BERT-based models show significant performance advantages over non-BERT models, highlighting the importance of pre-trained language models. The study emphasizes the need for cross-domain evaluation in causality extraction tasks and identifies key architectural differences that affect model performance.

## Method Summary
The study compares four models on four datasets with labeled cause-effect spans. Sequence tagging models (SCITE, GCE, BERT) use IOB/BIO tagging schemes where each token is classified as cause, effect, or other. The span-based SpERT model generates candidate spans up to a maximum length and classifies them using BERT embeddings. All models are evaluated using exact and partial matching F1 scores. Maximum span length for SpERT is tuned based on the 99th percentile of span lengths in each dataset. The study uses micro F1 scores for evaluation and compares performance across domains with varying span characteristics and percentages of explicitly causal sentences.

## Key Results
- Span-based models outperform sequence tagging models across all four datasets when maximum span length is tuned per domain
- BERT-based models consistently achieve higher performance than non-BERT models across all datasets
- Partial matching evaluation metrics inflate performance scores compared to exact matching, especially for longer spans
- Maximum span length tuning significantly impacts SpERT performance, with the 99th percentile working well across domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Span-based models outperform sequence tagging models when maximum span length is tuned per domain
- Mechanism: SpERT generates all possible spans up to a maximum length, allowing it to capture longer cause-effect phrases that sequence tagging models cannot represent due to span overlap constraints
- Core assumption: The distribution of span lengths varies significantly across domains
- Evidence anchors:
  - [abstract]: "span based models perform better than simple sequence tagging models based on BERT across all 4 data sets from diverse domains"
  - [section]: "We see that there is very little difference in the performance of the model on this data set"
- Break condition: If span length distributions are similar across domains

### Mechanism 2
- Claim: Pre-trained language models provide significant performance boost for causal knowledge extraction
- Mechanism: BERT embeddings capture contextual semantic information that helps distinguish causal relationships better than models trained only on limited labeled data
- Core assumption: Transfer learning from large pre-trained language models transfers effectively to specialized causal extraction tasks
- Evidence anchors:
  - [abstract]: "embeddings from pre-trained language models (e.g. BERT) provide a significant performance boost"
  - [section]: "It is evident from the performance of BERT based models on 4 different data sets"
- Break condition: If pre-trained language models overfit to general language patterns

### Mechanism 3
- Claim: Partial matching evaluation metric inflates performance scores, especially for longer spans
- Mechanism: Partial matching counts common words between predicted and ground truth spans, reducing penalty for missing boundary words
- Core assumption: Exact matching is a more stringent and realistic evaluation metric
- Evidence anchors:
  - [section]: "All models have a better F1 score while using partial matching"
  - [section]: "We see that BERT and SpERT models get close score on both these data sets"
- Break condition: If partial matching better reflects practical utility

## Foundational Learning

- Concept: Sequence tagging vs span-based modeling
  - Why needed here: Understanding the fundamental difference between tagging each token vs classifying entire spans is crucial for grasping why SpERT outperforms other models
  - Quick check question: Why can't sequence tagging models handle overlapping cause-effect spans in the same sentence?

- Concept: Pre-trained language models and transfer learning
  - Why needed here: The paper heavily relies on BERT's ability to transfer knowledge from general language understanding to specialized causal extraction tasks
  - Quick check question: How does BERT's contextual embedding capability help distinguish causal relationships better than static embeddings?

- Concept: Evaluation metrics: exact vs partial matching
  - Why needed here: Understanding how different evaluation metrics affect model comparison is crucial for interpreting the results correctly
  - Quick check question: In what scenarios might partial matching give a misleading impression of model performance?

## Architecture Onboarding

- Component map:
  Input text -> BERT encoder -> Span classifier (SpERT) or Sequence tagger (SCITE, GCE, BERT) -> Cause-effect span prediction -> Exact/Partial matching evaluation

- Critical path:
  1. Tokenize input text
  2. Generate BERT embeddings for all tokens
  3. For SpERT: Generate candidate spans up to max length
  4. Classify spans and relations (SpERT) or predict tags (sequence taggers)
  5. Convert predictions to cause-effect spans
  6. Evaluate using exact and partial matching metrics

- Design tradeoffs:
  - SpERT: Better for longer spans and multiple cause-effect pairs, but computationally expensive due to candidate generation
  - Sequence taggers: Simpler and faster, but cannot handle overlapping spans or very long phrases
  - BERT-based models: Require significant computational resources but provide superior performance through transfer learning

- Failure signatures:
  - SpERT: Poor performance when max span length is set too low for the domain
  - Sequence taggers: Missing cause-effect spans that overlap or are very long
  - BERT-based models: Overfitting to training data when domains are too different

- First 3 experiments:
  1. Run all four models on SCITE dataset with exact matching to establish baseline performance
  2. Compare exact vs partial matching scores on MedCaus dataset to observe impact of longer spans
  3. Test SpERT with different max span lengths on BeCauSE dataset to find optimal configuration per domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal maximum span size for the SpERT model across different domains, and how does it affect the trade-off between precision and recall?
- Basis in paper: [explicit] The paper mentions tuning the maximum span size based on the distribution of span lengths in the data set, with the 99th percentile span size working well across datasets.
- Why unresolved: While the paper suggests using the 99th percentile span size, it does not provide a comprehensive analysis of how different maximum span sizes impact the model's performance across various domains.
- What evidence would resolve it: Detailed experiments comparing the performance of SpERT with different maximum span sizes on each dataset, analyzing the trade-off between precision and recall.

### Open Question 2
- Question: How does the presence of explicit causal connective words in a sentence affect the performance of span-based models compared to sequence tagging models?
- Basis in paper: [explicit] The paper discusses the presence of causal connective words in datasets and notes that BERT and SpERT models perform closely on datasets with similar percentages of explicit causality.
- Why unresolved: The paper does not provide a detailed analysis of how the presence of explicit causal connectives specifically influences the performance of span-based versus sequence tagging models.
- What evidence would resolve it: Experiments isolating the effect of causal connectives on model performance, comparing span-based and sequence tagging models on datasets with varying percentages of explicit causal sentences.

### Open Question 3
- Question: What is the impact of using dependency parsing information in the GCE model, and how does it compare to using word embeddings alone?
- Basis in paper: [explicit] The paper mentions that the GCE model uses dependency parsing information but does not perform as well as the BiLSTM-CRF model.
- Why unresolved: The paper does not provide a thorough investigation into the specific role of dependency parsing information in the GCE model's performance.
- What evidence would resolve it: Experiments comparing the GCE model's performance with and without dependency parsing information, and analyzing the impact on different datasets.

## Limitations

- Datasets have varying sizes and characteristics, with FinCausal being notably smaller (9,157 sentences), which may affect generalizability
- Partial matching evaluation metric may overestimate model performance by counting partial overlap as correct predictions
- Study doesn't explore domain adaptation techniques that could improve cross-domain performance
- Computational complexity of span-based models presents practical deployment challenges not addressed

## Confidence

**High Confidence:**
- Span-based models outperform sequence tagging models across all four domains when maximum span length is properly tuned
- BERT-based models consistently achieve higher performance than non-BERT models across all datasets
- Partial matching evaluation metrics inflate performance scores compared to exact matching

**Medium Confidence:**
- The superiority of SpERT over sequence taggers is primarily due to its ability to handle overlapping spans and longer phrases
- Pre-trained language models provide significant performance benefits specifically for causal relationship extraction tasks
- Different domains require different maximum span lengths for optimal SpERT performance

**Low Confidence:**
- The exact relationship between span length distribution and model performance across domains
- Whether the observed performance differences would hold with additional domains or different dataset sizes
- The practical utility of partial matching metrics for real-world causal extraction applications

## Next Checks

1. **Span Length Distribution Analysis**: Conduct a systematic analysis of cause-effect span length distributions across all four domains to validate the hypothesis that domain-specific maximum span tuning is necessary for SpERT performance.

2. **Partial vs Exact Matching Utility Study**: Design a human evaluation study where domain experts assess the practical utility of causal extractions identified through exact matching versus partial matching.

3. **Cross-Domain Transfer Learning Experiment**: Implement and evaluate domain adaptation techniques to test whether the performance gap between in-domain and cross-domain evaluation can be reduced.