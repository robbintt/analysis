---
ver: rpa2
title: The ParlaSent Multilingual Training Dataset for Sentiment Identification in
  Parliamentary Proceedings
arxiv_id: '2309.09783'
source_url: https://arxiv.org/abs/2309.09783
tags:
- sentiment
- political
- data
- training
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a new sentiment-annotated dataset of parliamentary
  proceedings in 7 languages (Czech, English, Serbo-Croatian, Slovak, and Slovene)
  and introduces the first domain-specific multilingual transformer language model
  for political science applications. The dataset, ParlaSent, consists of 2,600 sentences
  per language annotated with a 6-level sentiment scale.
---

# The ParlaSent Multilingual Training Dataset for Sentiment Identification in Parliamentary Proceedings

## Quick Facts
- arXiv ID: 2309.09783
- Source URL: https://arxiv.org/abs/2309.09783
- Reference count: 40
- Key outcome: Additional pre-training on parliamentary data significantly improves multilingual sentiment detection performance

## Executive Summary
This paper introduces ParlaSent, a new sentiment-annotated dataset of parliamentary proceedings in 7 languages, and the first domain-specific multilingual transformer language model for political science applications. The dataset consists of 2,600 sentences per language annotated with a 6-level sentiment scale. Experiments demonstrate that additional pre-training of the XLM-R model on 1.72 billion words from parliamentary proceedings significantly improves downstream sentiment identification performance. The multilingual model also performs well on unseen languages, and additional fine-tuning data from other languages significantly improves results on the target parliament.

## Method Summary
The authors created the ParlaSent dataset by sampling sentences from parliamentary proceedings in 7 languages and annotating them with a 6-level sentiment scale. They then fine-tuned the XLM-R transformer model, first with additional pre-training on 1.72 billion words from 27 European parliaments, then on the ParlaSent data. The models were evaluated using R² and MAE metrics, comparing monolingual vs multilingual training approaches and testing on seen vs unseen languages.

## Key Results
- Additional pre-training on parliamentary data significantly improves sentiment detection performance
- Multilingual models perform well on languages not seen during fine-tuning
- Multilingual training outperforms monolingual training on target parliaments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Additional pre-training on parliamentary data improves sentiment detection performance.
- Mechanism: The model learns domain-specific linguistic patterns and context present in parliamentary proceedings, enabling better capture of sentiment-bearing expressions.
- Core assumption: Parliamentary language has unique stylistic and topical features that differ from general language.
- Evidence anchors:
  - [abstract] "Experiments demonstrate that additional pre-training of a multilingual model on 1.72 billion words from parliamentary proceedings significantly improves downstream sentiment identification performance."
  - [section 4.1] "We can conclude with an answer to the first research question – the additional pre-training of a multilingual model on parliamentary data does improve the performance on our task."
- Break condition: If parliamentary data does not capture meaningful variation in sentiment expression, or if the model overfits to domain-specific jargon irrelevant to sentiment.

### Mechanism 2
- Claim: Multilingual models perform well on unseen languages.
- Mechanism: Shared cross-lingual representations allow knowledge transfer from related languages during fine-tuning.
- Core assumption: Related languages (e.g., BCS group) share enough structural and lexical similarities for effective transfer.
- Evidence anchors:
  - [abstract] "We further show that our multilingual model performs very well on languages not seen during fine-tuning, and that additional data from other languages significantly improves the target parliament's results."
  - [section 4.2] "Overall, we cannot observe a hit on the performance of the models if the testing language gets removed from the training data to a greater extent than what is observed on the other test set."
- Break condition: If languages are too distant (e.g., unrelated families) or if cultural/political context differs drastically, transfer may fail.

### Mechanism 3
- Claim: Multilingual training on multiple parliaments improves performance over monolingual training.
- Mechanism: Increased training data volume and diversity of parliamentary styles improves generalization.
- Core assumption: Sentiment annotation guidelines are consistent enough across languages and parliaments for meaningful aggregation.
- Evidence anchors:
  - [abstract] "additional fine-tuning data from other languages significantly improves results on the target parliament."
  - [section 4.3] "Overall, we can conclude that if one wanted to annotate the sentiment in a specific parliament for which there is training data available, better results might still be obtained with additional data, written in different languages and coming from different parliaments."
- Break condition: If annotator biases differ strongly between parliaments, aggregation could introduce noise rather than signal.

## Foundational Learning

- Concept: Transformer-based language models
  - Why needed here: The paper uses XLM-R, a transformer model, as the base for sentiment classification.
  - Quick check question: What is the key architectural difference between transformers and RNNs that makes transformers more suitable for this task?

- Concept: Cross-lingual transfer learning
  - Why needed here: The model is fine-tuned on some languages and tested on others; understanding transfer mechanisms is critical.
  - Quick check question: How does shared subword vocabulary in multilingual models enable transfer between related languages?

- Concept: Inter-annotator agreement and annotation schema design
  - Why needed here: Low IAA is reported; understanding its implications for model training is important.
  - Quick check question: If Krippendorff's alpha is 0.5, what does that imply about the reliability of the labels and potential model performance?

## Architecture Onboarding

- Component map:
  - Parliamentary corpora → sentence segmentation → sampling → annotation → JSONL dataset
  - XLM-R base → additional pre-training on parliamentary data → fine-tuning on ParlaSent → evaluation
  - R² and MAE metrics on test sets

- Critical path:
  1. Load parliamentary corpora in multiple languages
  2. Preprocess and segment into sentences
  3. Sample balanced sets for annotation
  4. Annotate with 6-level schema
  5. Build training/dev/test splits
  6. Pre-train XLM-R on parliamentary text
  7. Fine-tune on ParlaSent
  8. Evaluate on held-out test sets

- Design tradeoffs:
  - 6-class vs 3-class schema: More granularity but lower IAA vs simpler but potentially less nuanced
  - Monolingual vs multilingual training: Higher data volume and transfer vs potential noise from unrelated parliaments
  - Pre-training data size: More data improves domain adaptation but increases computational cost

- Failure signatures:
  - Low R² (<0.3) on test sets indicates poor generalization
  - Large gap between training and dev performance suggests overfitting
  - Consistently low predictions on certain sentiment classes suggest annotation bias or model bias

- First 3 experiments:
  1. Fine-tune XLM-R-base vs XLM-R-large on ParlaSent; compare R²/MAE
  2. Pre-train XLM-R-large on parliamentary data, then fine-tune; compare to baseline
  3. Train multilingual model vs monolingual models; compare performance on each parliament

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of XLM-R-parla compare to newer transformer models like GPT-4 or LLaMA-2 for sentiment analysis in parliamentary proceedings?
- Basis in paper: [inferred] The authors mention that benchmarking against GPT-4 and LLaMA2 is a future direction of research.
- Why unresolved: The paper only compares XLM-R-parla to other XLM-R models, not to the latest LLMs.
- What evidence would resolve it: Direct comparison experiments using GPT-4 and LLaMA2 models fine-tuned on ParlaSent data, measuring R2 and MAE scores against the XLM-R-parla results.

### Open Question 2
- Question: How does sentiment annotation quality vary across different political cultures and parliaments represented in the dataset?
- Basis in paper: [explicit] The authors note that the negative class is often most populous but not in the Slovene dataset or English test set, theorizing this might indicate nuances in political nature and culture.
- Why unresolved: The paper does not provide detailed analysis of how sentiment patterns and annotation consistency vary across the different parliaments and cultures.
- What evidence would resolve it: Detailed comparative analysis of sentiment distributions and inter-annotator agreement scores across all parliaments, potentially with qualitative analysis of cultural/political differences.

### Open Question 3
- Question: What is the optimal balance between monolingual and multilingual training data for maximizing sentiment analysis performance on a target parliament?
- Basis in paper: [explicit] The authors find multilingual training performs better than monolingual, but note each approach has different error biases and the optimal choice may depend on the use case.
- Why unresolved: The paper only compares two extremes - training only on target language vs. all languages. It does not explore intermediate ratios or more nuanced strategies.
- What evidence would resolve it: Systematic experiments varying the proportion of target language vs. other language training data, measuring performance on target parliament test sets to find the optimal mix.

## Limitations

- The 6-level sentiment annotation schema showed moderate inter-annotator agreement (Krippendorff's alpha ~0.5), potentially limiting label reliability
- The paper does not address potential cultural or political differences in how sentiment is expressed across different parliamentary systems
- The results may not generalize beyond parliamentary proceedings to other domains or text types

## Confidence

- **High confidence**: The improvement from additional pre-training on parliamentary data is well-supported by quantitative results showing consistent gains in R² scores across all tested languages
- **Medium confidence**: The multilingual transfer performance claims are supported by experimental results, but the paper does not thoroughly explore the linguistic relationships between the tested languages
- **Medium confidence**: The claim that multilingual training improves over monolingual training is supported by results, but the paper does not conduct ablation studies to isolate the effects of increased data volume versus cross-lingual transfer benefits

## Next Checks

1. **Annotation reliability assessment**: Conduct a detailed analysis of inter-annotator agreement across different sentiment categories and parliaments to identify potential systematic biases in the labeling process.
2. **Cross-parliament transfer analysis**: Systematically test model performance when transferring between parliaments with different political systems (e.g., presidential vs parliamentary democracies) to assess the impact of political context on sentiment detection.
3. **Linguistic distance validation**: Evaluate model performance on parliaments from different language families to quantify the relationship between linguistic distance and transfer effectiveness, validating the assumption that related languages benefit more from shared representations.