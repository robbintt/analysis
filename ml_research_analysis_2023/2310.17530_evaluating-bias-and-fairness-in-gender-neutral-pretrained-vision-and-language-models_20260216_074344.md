---
ver: rpa2
title: Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language
  Models
arxiv_id: '2310.17530'
source_url: https://arxiv.org/abs/2310.17530
tags:
- bias
- gender
- lxmert
- gender-neutral
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study examines gender bias in vision-and-language (V&L) models,
  focusing on bias amplification during pretraining and after fine-tuning on downstream
  tasks. The authors define gender bias as their case study and quantify bias amplification
  in three families of V&L models: LXMERT, ALBEF, and BLIP.'
---

# Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models

## Quick Facts
- arXiv ID: 2310.17530
- Source URL: https://arxiv.org/abs/2310.17530
- Authors: 
- Reference count: 40
- Primary result: Gender-neutral pretraining reduces group disparities on VQA and retrieval tasks without significant performance loss

## Executive Summary
This study examines gender bias in vision-and-language (V&L) models, focusing on bias amplification during pretraining and after fine-tuning on downstream tasks. The authors define gender bias as their case study and quantify bias amplification in three families of V&L models: LXMERT, ALBEF, and BLIP. They investigate the relationship between intrinsic bias (encoded in pretrained models) and extrinsic bias (measured on downstream tasks), as well as how bias amplification reflects on model performance. The key finding is that bias amplification in pretraining and after fine-tuning are independent. To mitigate gender bias, the authors explore the effect of continued pretraining on gender-neutral data, which reduces group disparities (promotes fairness) on VQAv2 and retrieval tasks without significantly compromising task performance.

## Method Summary
The authors evaluate bias and fairness in vision-and-language models by first pretraining models on multimodal datasets (COCO, CC3M, or Conceptual Captions), then continuing pretraining on gender-neutral data where gender terms are replaced with neutral equivalents. They fine-tune these models on downstream tasks (VQAv2, GQA, NLVR2, Flickr30K retrieval) and measure bias amplification, intrinsic bias (via masked language modeling), extrinsic bias, and group disparity across gender categories. The study compares original models with those that underwent gender-neutral pretraining to assess fairness improvements.

## Key Results
- Bias amplification in pretraining and after fine-tuning are independent phenomena
- Gender-neutral pretraining consistently reduces group disparity on VQAv2 and retrieval tasks for most models studied
- Gender-neutral training reduces fine-tuning variance, leading to more stable performance across different random seeds
- Gender-neutral pretraining reduces the min-max gap in VQAv2 performance by 4.5 points while only increasing it by 0.4 points in GQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intrinsic bias in pretrained models does not consistently transfer to extrinsic bias after fine-tuning
- Mechanism: The model learns task-specific representations during fine-tuning that may override or mask biases present in the pretraining data
- Core assumption: Downstream tasks require different reasoning patterns than pretraining, allowing the model to focus on task-relevant features rather than demographic correlations
- Evidence anchors:
  - [abstract]: "we find that bias amplification in pretraining and after fine-tuning are independent."
  - [section]: "intrinsic bias in V&L models does not necessarily transfer to extrinsic bias on downstream tasks."
- Break condition: If downstream tasks heavily rely on demographic features (e.g., gender prediction), the bias transfer might become stronger

### Mechanism 2
- Claim: Gender-neutral pretraining reduces group disparity on downstream tasks without significant performance loss
- Mechanism: By continuing pretraining on gender-neutral data, the model shifts its representations to a more balanced space, reducing the conditional probability distribution differences across demographic groups
- Core assumption: The extra pretraining epoch on gender-neutral data is sufficient to shift the model's representations without catastrophic forgetting of task-relevant features
- Evidence anchors:
  - [abstract]: "gender-neutral pretraining consistently reduces group disparity on VQAv2 and retrieval tasks for most models studied."
  - [section]: "we observe a reduction in the min-max gap of 4.5 points in VQAv2, while the min-max gap increase in GQA is only of 0.4 points."
- Break condition: If the gender-neutral data is too small or not representative, the pretraining might not effectively shift the model's representations

### Mechanism 3
- Claim: Gender-neutral pretraining reduces fine-tuning variance across random seeds
- Mechanism: The more balanced representations learned during gender-neutral pretraining provide a more stable starting point for fine-tuning, reducing sensitivity to random initialization
- Core assumption: The gender-neutral pretraining smooths out the representation space, making it less dependent on specific initialization patterns
- Evidence anchors:
  - [abstract]: "gender-neutral training reduces fine-tuning variance, leading to more stable performance across different random seeds."
  - [section]: "we notice that gender-neutral models seem to have lower variance after fine-tuning."
- Break condition: If the gender-neutral pretraining introduces its own biases or if the downstream tasks are very sensitive to initialization, the variance reduction might not occur

## Foundational Learning

- Concept: Understanding of bias amplification metrics (BiasAmpA→T and BiasAmpT →A)
  - Why needed here: The paper uses these metrics to quantify bias in both pretraining and fine-tuning stages
  - Quick check question: Can you explain the difference between BiasAmpA→T and BiasAmpT →A in your own words?

- Concept: Masked Language Modeling (MLM) task
  - Why needed here: The paper uses MLM to evaluate intrinsic bias by masking gendered terms and analyzing the model's predictions
  - Quick check question: How does the MLM task help in identifying gender bias in the model's representations?

- Concept: Group disparity in model performance
  - Why needed here: The paper evaluates fairness by comparing model performance across different gender groups (male, female, neutral)
  - Quick check question: Why is it important to measure group disparity in addition to overall task performance?

## Architecture Onboarding

- Component map:
  - LXMERT: Cross-modal encoder with separate visual and language encoders
  - ALBEF: Dual-stream encoder with contrastive learning
  - BLIP: Encoder-decoder model with autoregressive language modeling
  - Gender-neutral pretraining: Continued pretraining on gender-neutral data

- Critical path:
  1. Pretraining on multimodal data (COCO or CC3M)
  2. Evaluation of intrinsic bias using MLM
  3. Fine-tuning on downstream tasks (VQAv2, GQA, NLVR2, Flickr30K)
  4. Evaluation of extrinsic bias and group disparity
  5. Gender-neutral pretraining and repeat steps 2-4

- Design tradeoffs:
  - Using gender-neutral data might slightly reduce task performance but significantly improves fairness
  - Different model architectures (LXMERT, ALBEF, BLIP) have varying degrees of bias amplification
  - The choice of gender terms and their neutral mappings can affect the effectiveness of bias mitigation

- Failure signatures:
  - If group disparity increases after gender-neutral pretraining, it might indicate that the neutral data is not representative
  - High variance in fine-tuning results could suggest that the pretraining didn't sufficiently stabilize the representations
  - If intrinsic bias remains high even after gender-neutral pretraining, the pretraining data might be too small or not diverse enough

- First 3 experiments:
  1. Run intrinsic bias analysis on a pretrained LXMERT model using MLM task
  2. Fine-tune the same model on VQAv2 and evaluate group disparity
  3. Perform gender-neutral pretraining on the model and repeat experiments 1 and 2 to compare results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does intrinsic bias in V&L models consistently transfer to extrinsic bias in downstream tasks across different model architectures and datasets?
- Basis in paper: [inferred] The paper finds that intrinsic bias does not consistently transfer to extrinsic bias, but this needs to be tested more broadly across different architectures and datasets
- Why unresolved: The paper only tests a limited set of models (LXMERT, ALBEF, BLIP) and datasets (COCO, CC3M, VQAv2, GQA, NLVR2, Flickr30K). A more comprehensive study across diverse models and datasets is needed to confirm this finding
- What evidence would resolve it: Conducting a large-scale study testing intrinsic bias transfer to extrinsic bias across a wide range of V&L models (e.g., different architectures, sizes, training data) and datasets (e.g., different domains, languages, tasks) would provide stronger evidence for or against the generalizability of this finding

### Open Question 2
- Question: What are the underlying mechanisms that lead to the independence of bias and fairness in V&L models?
- Basis in paper: [explicit] The paper observes that bias in a model and its empirical fairness (group disparity on task performance) are independent matters, but does not explore the reasons behind this independence
- Why unresolved: The paper identifies this independence but does not delve into the theoretical or empirical reasons why bias and fairness are independent in V&L models
- What evidence would resolve it: Investigating the relationship between bias and fairness at different levels (e.g., input, representation, output) and analyzing how different factors (e.g., model architecture, training data, fine-tuning procedure) affect this relationship would provide insights into the mechanisms behind this independence

### Open Question 3
- Question: How does gender-neutral pretraining affect the robustness of V&L models to gendered terms in downstream tasks?
- Basis in paper: [explicit] The paper finds that gender-neutral pretraining reduces fine-tuning variance and group disparity in some tasks, but the reasons for this improvement and its generalizability are not explored
- Why unresolved: The paper demonstrates the positive effects of gender-neutral pretraining but does not investigate the underlying reasons for this improvement or test its generalizability to other tasks and domains
- What evidence would resolve it: Analyzing the impact of gender-neutral pretraining on the representations learned by V&L models (e.g., through probing tasks, similarity analysis) and evaluating its effects on a wider range of downstream tasks and domains would provide insights into the mechanisms behind this improvement and its generalizability

## Limitations
- The independence of bias amplification between pretraining and fine-tuning lacks theoretical grounding for why these processes are decoupled
- The gender-neutral pretraining approach assumes that one additional epoch is sufficient for meaningful representation shift, but optimal pretraining duration remains unclear
- The variance reduction claims are based on limited seed trials, making it difficult to assess statistical significance

## Confidence
- High confidence: The observation that gender-neutral pretraining reduces group disparity on VQAv2 and retrieval tasks is well-supported by quantitative metrics and consistent across multiple model families
- Medium confidence: The claim that bias amplification in pretraining and fine-tuning are independent requires more extensive ablation studies to rule out task-specific effects
- Low confidence: The variance reduction mechanism needs more rigorous statistical analysis with larger seed sets to establish significance

## Next Checks
1. **Statistical validation of variance claims**: Run fine-tuning with 10+ random seeds for each model variant and perform statistical tests (e.g., ANOVA) to determine if variance reduction is significant beyond random variation
2. **Pretraining duration ablation**: Compare gender-neutral pretraining effects across 1, 2, and 3 epochs to establish whether additional pretraining yields diminishing returns or continued bias reduction
3. **Bias transfer mechanism analysis**: Design controlled experiments where downstream tasks are explicitly designed to either amplify or suppress demographic correlations, testing whether bias independence holds across the full spectrum of task types