---
ver: rpa2
title: Learning Sparse Neural Networks with Identity Layers
arxiv_id: '2307.07389'
source_url: https://arxiv.org/abs/2307.07389
tags:
- cka-sr
- sparsity
- network
- sparse
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving sparsity in deep
  neural networks by reducing interlayer feature similarity. The core idea is that
  overparameterization leads to high similarity between features of different layers,
  and reducing this similarity can improve network sparsity.
---

# Learning Sparse Neural Networks with Identity Layers

## Quick Facts
- arXiv ID: 2307.07389
- Source URL: https://arxiv.org/abs/2307.07389
- Reference count: 40
- Key outcome: CKA-SR regularization improves sparse training performance by 4-6% at extreme sparsity levels (99.5%-99.8%)

## Executive Summary
This paper addresses the challenge of improving sparsity in deep neural networks by reducing interlayer feature similarity. The authors propose CKA-SR, a plug-and-play regularization method that uses Centered Kernel Alignment (CKA) to penalize similarity between intermediate layer representations. They theoretically prove that minimizing CKA-SR is equivalent to minimizing mutual information between intermediate and input representations, which increases network sparsity. Extensive experiments demonstrate consistent improvements over state-of-the-art sparse training methods, particularly at high sparsity levels where performance typically degrades significantly.

## Method Summary
CKA-SR is a regularization term added to the training loss that computes Centered Kernel Alignment (CKA) similarity between intermediate layer feature maps using few-shot samples from each batch. The method penalizes high similarity between layers, forcing them to develop distinct representations. The regularization weight β controls the strength of similarity reduction. CKA-SR integrates with existing sparse training frameworks like LTH, GraSP, and DLTH as a plug-and-play module that requires access to intermediate layer activations during training.

## Key Results
- Achieves 4.0%+ performance improvement at 99.5% sparsity on CIFAR-100
- Achieves 6.0%+ performance improvement at 99.8% sparsity on CIFAR-100
- Consistently outperforms baseline sparse training methods across multiple architectures and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing interlayer feature similarity directly increases network sparsity by forcing layers to develop distinct representations
- Mechanism: Penalizing CKA similarity between layers eliminates redundancy and pushes weights toward sparsity
- Core assumption: Interlayer similarity is a reliable indicator of overparameterization
- Evidence anchors: Abstract proves CKA-SR improves sparsity; related papers focus on sparse activations but not CKA-based methods
- Break condition: If reducing similarity causes performance degradation or layers cannot develop distinct representations

### Mechanism 2
- Claim: CKA-SR increases sparsity through information bottleneck theory by minimizing mutual information between intermediate and input representations
- Mechanism: The regularization implicitly minimizes I(X; X̂), forcing network compression while preserving task-relevant information
- Core assumption: CKA similarity can be optimized to achieve compression through mutual information minimization
- Evidence anchors: Abstract proves CKA-SR minimizes mutual information; no direct corpus evidence on information bottleneck in sparse training
- Break condition: If CKA-mutual information equivalence doesn't hold or compression harms performance

### Mechanism 3
- Claim: CKA-SR increases sparsity by minimizing Frobenius norm of weight matrices
- Mechanism: Reducing interlayer similarity indirectly minimizes ||W||²F, concentrating parameter values around zero
- Core assumption: Frobenius norm minimization is an effective proxy for sparsity increase
- Evidence anchors: Abstract proves CKA-SR improves sparsity; related papers don't discuss Frobenius norm relationships
- Break condition: If Frobenius norm minimization doesn't translate to actual sparsity improvements

## Foundational Learning

- Concept: Centered Kernel Alignment (CKA)
  - Why needed here: Core similarity metric for measuring and reducing interlayer feature similarity
  - Quick check question: What properties make CKA suitable for measuring interlayer similarity compared to other metrics?

- Concept: Information Bottleneck Theory
  - Why needed here: Theoretical foundation connecting CKA-SR to sparsity through mutual information minimization
  - Quick check question: How does minimizing mutual information between intermediate and input representations lead to compressed, sparse representations?

- Concept: Dynamic Sparse Training
  - Why needed here: Understanding how CKA-SR integrates with existing sparse training methods
  - Quick check question: What distinguishes CKA-SR from traditional parameter pruning approaches?

## Architecture Onboarding

- Component map: CKA-SR regularization module sits between sparse training frameworks and loss function, requiring intermediate layer activations
- Critical path: Forward pass computes activations → CKA-SR computes similarity with few-shot samples → regularization loss added → backward pass updates weights
- Design tradeoffs: Few-shot sampling reduces computational cost but may provide noisier CKA estimates; β hyperparameter requires tuning
- Failure signatures: Performance degradation at high β values, lack of sparsity improvement, computational overhead, training instability
- First 3 experiments:
  1. Validate CKA-SR on simple MLP on MNIST with LTH sparse training
  2. Compare CKA-SR against L0 regularization on CIFAR-10 with ResNet18
  3. Test CKA-SR on structured pruning of pre-trained ImageNet models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CKA-SR effectiveness vary across different network architectures beyond CNNs and ViTs?
- Basis in paper: Authors demonstrate effectiveness on CNNs and ViTs but don't explore other architectures like NLP Transformers
- Why unresolved: Paper focuses on vision tasks without evidence for other neural network types
- What evidence would resolve it: Experiments applying CKA-SR to sparse training for NLP Transformers and non-vision architectures

### Open Question 2
- Question: What is the theoretical limit of sparsity achievable with CKA-SR before significant performance degradation?
- Basis in paper: Experiments show improvements up to 99.8% sparsity but don't investigate absolute limits
- Why unresolved: Experiments don't push beyond 99.8% and theoretical relationships aren't fully explored
- What evidence would resolve it: Systematic experiments beyond 99.8% sparsity identifying performance degradation thresholds

### Open Question 3
- Question: How does CKA-SR interact with other regularization techniques and what's the optimal combination strategy?
- Basis in paper: Ablation study shows CKA-SR works with L0 regularization but doesn't explore other combinations
- Why unresolved: Paper only tests one combination and lacks comprehensive analysis of regularization interactions
- What evidence would resolve it: Systematic experiments combining CKA-SR with various regularization techniques

## Limitations

- Theoretical connection between CKA similarity reduction and sparsity relies on specific information bottleneck assumptions not fully validated empirically
- CKA computation adds computational overhead and memory requirements due to intermediate activation access
- Few-shot sampling approach for CKA computation may introduce noise and instability in similarity estimates

## Confidence

- High confidence: CKA-SR consistently improves sparse training performance across multiple methods and datasets
- Medium confidence: Theoretical equivalence between CKA-SR and mutual information minimization is mathematically sound but not empirically validated
- Medium confidence: Frobenius norm minimization as sparsity mechanism is plausible but indirect

## Next Checks

1. Perform ablation studies on few-shot sample size to quantify trade-off between computational efficiency and CKA estimation accuracy
2. Test CKA-SR on additional architectures (Vision Transformers, EfficientNets) to assess generalizability beyond standard ResNet variants
3. Conduct controlled experiments isolating information bottleneck effect from other mechanisms to validate theoretical foundations