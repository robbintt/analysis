---
ver: rpa2
title: 'Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and Problem
  Solving: Evidence from the Vietnamese National High School Graduation Examination'
arxiv_id: '2306.06331'
source_url: https://arxiv.org/abs/2306.06331
tags:
- chatgpt
- questions
- mathematics
- performance
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated ChatGPT\u2019s mathematical abilities on\
  \ the Vietnamese National High School Graduation Examination (VNHSGE), analyzing\
  \ 250 multiple-choice questions across four difficulty levels (knowledge to high\
  \ application) and ten mathematical topics. ChatGPT achieved an average score of\
  \ 5.88/10, with performance varying significantly by difficulty and subject: 83%\
  \ accuracy on knowledge-level questions but only 10% on high-application ones."
---

# Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and Problem Solving: Evidence from the Vietnamese National High School Graduation Examination

## Quick Facts
- arXiv ID: 2306.06331
- Source URL: https://arxiv.org/abs/2306.06331
- Reference count: 32
- Primary result: ChatGPT achieved 5.88/10 average score on VNHSGE, excelling at knowledge-level questions (83% accuracy) but struggling with high-application problems (10% accuracy)

## Executive Summary
This study evaluates ChatGPT's mathematical capabilities using the Vietnamese National High School Graduation Examination (VNHSGE), analyzing 250 multiple-choice questions across four difficulty levels and ten mathematical topics. The model demonstrated strong performance on basic knowledge questions but significant decline as complexity increased, particularly struggling with high-application problems requiring complex reasoning and graphical data interpretation. Performance varied substantially by mathematical topic, with strengths in algebraic functions and sequences but weaknesses in derivatives, spatial geometry, and Oxyz spatial calculus. When compared to Vietnamese students and other international math competitions, ChatGPT's performance was consistently lower, highlighting limitations in advanced mathematical reasoning and visual comprehension.

## Method Summary
The study employed a pre-trained ChatGPT model to answer 250 VNHSGE mathematics questions from 2019-2023, covering 10 topics and 4 difficulty levels (Knowledge, Comprehension, Application, High Application). Questions were formatted in JSON structure and submitted via API with specified prompt format including both choice and explanation. Answers were validated against official answer keys to calculate accuracy rates by difficulty level and topic. The methodology also included comparisons with Vietnamese students' performance and results from other international math competitions (SAT, AP Calculus, GMAT, GRE).

## Key Results
- ChatGPT achieved 5.88/10 average score on VNHSGE mathematics questions
- Performance varied dramatically by difficulty: 83% accuracy on Knowledge-level questions vs 10% on High-Application questions
- Topic-specific performance ranged from 20-80% accuracy, excelling in exponential/logarithmic functions and progressions while struggling with derivatives and spatial geometry
- Compared to Vietnamese students, ChatGPT's scores were consistently lower across all performance metrics

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT's performance drops sharply as question difficulty increases from knowledge (K) to high application (H) levels because the model relies heavily on pattern recognition and surface-level comprehension, which suffices for K-level recall but fails for H-level synthesis requiring multi-step reasoning and novel problem-solving. The training data likely contained abundant K/C-level examples but sparse or no H-level reasoning chains.

### Mechanism 2
ChatGPT struggles with graphical data interpretation in math questions because the model lacks integrated computer vision; it cannot parse tables, charts, or geometric diagrams, so questions requiring visual input are answered incorrectly or skipped. The text-only LLM architecture cannot bridge this gap without external vision modules.

### Mechanism 3
Topic-specific knowledge affects accuracy; ChatGPT excels in algebra/sequences but falters in spatial geometry and calculus applications because training corpus distribution biased toward algebraic structures, so the model generalizes well there; spatial and applied calculus problems are underrepresented, leading to poor performance.

## Foundational Learning

- **Bloom's Taxonomy levels** (Knowledge, Comprehension, Application, High Application): Why needed here - The study explicitly maps questions to these cognitive levels to analyze performance decay across complexity. Quick check - If a student can only recall formulas but cannot apply them in new contexts, which Bloom level are they operating at? (Answer: Knowledge)

- **LaTeX and JSON data formatting for structured NLP input**: Why needed here - The methodology converts mathematical expressions into LaTeX for uniform display and JSON for API ingestion, ensuring reproducible prompts. Quick check - Why is JSON preferred over raw text for API communication in this pipeline? (Answer: It preserves structure, metadata, and enables automated parsing.)

- **Accuracy vs. consistency metrics**: Why needed here - Accuracy measures correct answers; consistency (low std dev) indicates reliable performance across similar items, both critical for educational tool validation. Quick check - If accuracy is 83% but std dev is high, what does that imply about model reliability? (Answer: Sporadic success; not dependable for consistent instruction.)

## Architecture Onboarding

- **Component map**: Input formatter (LaTeX→JSON) → ChatGPT API → Answer parser (JSON→accuracy check) → Result aggregator
- **Critical path**: Data preprocessing → API call → Validation against ground truth → Score computation
- **Design tradeoffs**: Text-only LLM (fast, scalable) vs. multimodal (accurate on visuals but slower, costlier)
- **Failure signatures**: Missing or malformed JSON → API error; wrong choice letter → accuracy drop; unhandled LaTeX → parsing crash
- **First 3 experiments**:
  1. Feed a K-level algebra question in JSON; verify correct choice extraction and explanation format
  2. Feed a C-level question requiring textual reasoning; check if explanation matches ground truth
  3. Attempt an H-level spatial geometry question; observe failure mode (likely "cannot interpret figure")

## Open Questions the Paper Calls Out

### Open Question 1
How does ChatGPT's performance on graphical data interpretation in mathematics compare to other AI models specifically trained on visual reasoning tasks? The study identifies ChatGPT's inability to access, read, and comprehend graphical information as a key limitation, but does not compare this to other AI models. Comparative analysis against models like DeepMind's Graph Neural Networks on identical mathematical datasets would resolve this.

### Open Question 2
What specific architectural modifications could improve ChatGPT's performance on high-application level mathematics problems? The paper notes ChatGPT's significant performance drop on high-application questions (10% accuracy) but does not explore potential architectural solutions. Empirical testing of hybrid models combining ChatGPT with symbolic computation systems would provide evidence.

### Open Question 3
How does the performance gap between ChatGPT and Vietnamese students vary across different socioeconomic contexts? The paper compares ChatGPT's scores to Vietnamese students' scores but does not analyze performance variations across socioeconomic contexts. A multi-year study comparing ChatGPT performance against students from diverse socioeconomic backgrounds across multiple regions in Vietnam would resolve this.

## Limitations
- Inability to evaluate ChatGPT's performance on questions requiring graphical data interpretation represents a significant gap in mathematical reasoning assessment
- The comparison with Vietnamese students' scores lacks detailed context about the student population's characteristics and specific exam conditions
- The broader comparison with international math competitions may be limited by differences in question formats and cultural contexts

## Confidence

- **High Confidence**: The observed decline in performance from knowledge to high-application questions (83% to 10% accuracy) is well-supported by the data
- **Medium Confidence**: The topic-specific performance variations are reasonably supported, though underlying causes remain uncertain
- **Low Confidence**: The broader comparison with international math competitions may be limited by differences in question formats and cultural contexts

## Next Checks

1. **Controlled Difficulty Experiment**: Design a new dataset with questions carefully calibrated across Bloom's taxonomy levels to isolate whether the performance drop is due to question complexity or specific mathematical domains

2. **Multimodal Integration Test**: Evaluate ChatGPT's performance when paired with computer vision systems for interpreting mathematical diagrams and graphs, measuring the improvement in accuracy on visual-heavy questions

3. **Student Comparison Replication**: Conduct a controlled experiment where ChatGPT and a representative sample of Vietnamese students answer identical questions under similar conditions, controlling for time pressure and question presentation format