---
ver: rpa2
title: Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice
  Questions in Vietnamese General Education
arxiv_id: '2310.12059'
source_url: https://arxiv.org/abs/2310.12059
tags:
- llms
- language
- vietnamese
- dataset
- vimmrc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models' ability to perform
  multiple-choice symbol binding tasks for Vietnamese general education exams, covering
  mathematics, physics, chemistry, biology, history, geography, and civic education.
  The authors create a novel dataset with structured LaTeX formatting guidelines to
  enable precise symbol handling across these subjects.
---

# Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education

## Quick Facts
- arXiv ID: 2310.12059
- Source URL: https://arxiv.org/abs/2310.12059
- Reference count: 32
- Primary result: GPT-4 achieves highest accuracy at 71.24% on Vietnamese educational MCQA tasks with LaTeX-formatted symbols

## Executive Summary
This paper evaluates large language models' ability to perform multiple-choice symbol binding tasks for Vietnamese general education exams across mathematics, physics, chemistry, biology, history, geography, and civic education. The authors create a novel dataset with structured LaTeX formatting guidelines to enable precise symbol handling across these subjects. Six LLMs (BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4) are tested on this dataset and two literature-focused benchmarks. GPT-4 achieves the highest average accuracy at 71.24% in five-shot settings, while other models show varied performance depending on subject and prompting strategy. The results indicate that LLMs perform better on easier exam questions and that symbol binding ability correlates with overall MCQA performance in Vietnamese educational contexts.

## Method Summary
The study evaluates six large language models on a novel dataset containing Vietnamese national high school graduation exam questions from 2017-2023, formatted with strict LaTeX notation for mathematical and scientific symbols. The evaluation uses zero-shot, one-shot, and five-shot prompting strategies with a consistent prompt template. Models are configured with specific maximum sequence lengths (4096 for most models, 3073 for GPT-4) and temperature set to 0. Performance is measured using accuracy as the primary metric, comparing model predictions against ground truth answer choices (A, B, C, or D).

## Key Results
- GPT-4 achieves the highest average accuracy at 71.24% in five-shot settings across all subjects
- BLOOMZ-7.1B-MT shows the most balanced performance across subjects with accuracy ranging from 44.85% to 55.98%
- LLMs demonstrate higher accuracy on easier exam questions compared to more complex ones across all subjects
- Performance varies significantly by subject, with mathematics and physics generally showing higher accuracy than history and civic education

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured LaTeX formatting enhances symbol binding performance for LLMs.
- Mechanism: Enforcing strict LaTeX notation for mathematical and chemical expressions provides explicit symbolic representations that LLMs can more reliably parse and manipulate.
- Core assumption: LLMs benefit from explicit, standardized symbolic representations rather than implicit or free-form notation.
- Evidence anchors:
  - [abstract] "This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style."
  - [section] "Our proposed dataset was meticulously developed following the formatting standards of the MathJax library in LaTeX."
- Break condition: If the LLM's tokenizer does not properly handle LaTeX syntax or if the model's pretraining corpus lacked sufficient exposure to LaTeX-formatted mathematical content.

### Mechanism 2
- Claim: Subject-specific performance varies significantly based on content complexity and LLM capabilities.
- Mechanism: LLMs demonstrate differential performance across subjects (Mathematics, Physics, Chemistry, Biology, History, Geography, Civic Education) due to varying symbol complexity and domain-specific knowledge requirements.
- Core assumption: Different subjects present distinct symbol binding challenges that correlate with LLM performance patterns.
- Evidence anchors:
  - [abstract] "The results indicate that LLMs perform better on easier exam questions and that symbol binding ability correlates with overall MCQA performance"
  - [section] "In Geography, History, and Civic Education, it's important to note that current LLMs have not yet reached a perfect performance."
- Break condition: If subject difficulty is not the primary driver of performance differences, or if model architecture fundamentally changes the relationship between subject complexity and accuracy.

### Mechanism 3
- Claim: Few-shot prompting significantly improves LLM performance compared to zero-shot settings.
- Mechanism: Providing demonstration examples in prompts helps LLMs understand the task structure and formatting expectations, leading to better symbol binding and answer selection.
- Core assumption: LLMs can generalize from few examples to understand the task format and apply it to new questions.
- Evidence anchors:
  - [section] "GPT-3.5 and GPT-4 also gained positive performances, while both LLMs surpassed others. Brown et al. [1] also observes that larger GPT-3 models perform better, though progress tends to be steadier."
  - [section] "We observed a notable improvement in GPT-3.5 and GPT-4 performance" when transitioning from zero-shot to few-shot.
- Break condition: If the demonstration examples are not representative of the test questions, or if the model's architecture fundamentally limits its ability to benefit from few-shot learning.

## Foundational Learning

- Concept: LaTeX mathematical notation and mhchem extensions
  - Why needed here: The dataset uses LaTeX for mathematical expressions and mhchem for chemical formulas, requiring understanding of these formats for proper symbol binding evaluation.
  - Quick check question: Can you write the LaTeX code for a quadratic equation and a chemical formula using mhchem notation?

- Concept: Multiple-choice question answering (MCQA) evaluation metrics
  - Why needed here: The paper uses accuracy as the primary evaluation metric, requiring understanding of how to calculate and interpret this metric in the context of symbol binding tasks.
  - Quick check question: How would you calculate the accuracy for a model that correctly answers 45 out of 50 multiple-choice questions?

- Concept: Few-shot learning in LLMs
  - Why needed here: The experiments use zero-shot, one-shot, and five-shot settings, requiring understanding of how few-shot learning works in LLMs and how to construct effective prompts.
  - Quick check question: What is the difference between zero-shot and few-shot learning, and how does the number of examples affect model performance?

## Architecture Onboarding

- Component map: Dataset creation -> Prompt engineering -> LLM inference -> Performance evaluation -> Analysis
- Critical path: Structured dataset -> Proper LaTeX formatting -> Effective few-shot prompts -> LLM inference -> Accuracy calculation
- Design tradeoffs: Dataset comprehensiveness vs. annotation effort, model size vs. computational resources, prompt length vs. context window limitations
- Failure signatures: Poor LaTeX formatting leading to symbol binding failures, insufficient few-shot examples leading to poor generalization, model architecture limitations preventing effective symbol manipulation
- First 3 experiments:
  1. Test LaTeX parsing by running a small set of mathematical expressions through the LLM with zero-shot prompts
  2. Compare performance across subjects using five-shot prompts to identify subject-specific challenges
  3. Vary maximum sequence length and number of exemplars to optimize performance for different LLMs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of prompt engineering strategies on LLM performance across different Vietnamese educational subjects?
- Basis in paper: [explicit] The paper evaluates few-shot performance across subjects but doesn't explore different prompt engineering strategies
- Why unresolved: The paper uses a fixed few-shot prompt format without exploring variations in prompt structure, wording, or context
- What evidence would resolve it: Comparative experiments testing different prompt formats (e.g., role-based prompting, chain-of-thought, structured templates) across all subjects in the dataset

### Open Question 2
- Question: How does symbol binding ability specifically contribute to overall MCQ performance in Vietnamese language models?
- Basis in paper: [inferred] The paper focuses on symbol binding but doesn't isolate its specific contribution from other factors like language understanding
- Why unresolved: The experiments measure overall MCQ performance but don't decompose the influence of symbol binding versus other capabilities
- What evidence would resolve it: Ablation studies removing or simplifying symbolic components to measure their specific impact on accuracy

### Open Question 3
- Question: What is the relationship between model size and symbol binding performance beyond the tested LLMs?
- Basis in paper: [explicit] The paper tests models from 7B to 70B parameters and observes size-performance correlation but doesn't explore the full parameter space
- Why unresolved: The tested range is limited, and the paper doesn't establish whether performance improvements plateau or continue with larger models
- What evidence would resolve it: Performance testing of models spanning a wider parameter range (e.g., 1B to 100B+) to identify scaling patterns and potential plateaus

## Limitations
- Dataset coverage limited to exams from 2017-2023, potentially missing broader educational content complexity
- Evaluation focuses exclusively on multiple-choice questions, missing open-ended symbolic reasoning tasks
- Only six LLMs tested, limiting generalizability to broader model landscape
- Strict LaTeX formatting may not reflect real-world notation variations used in Vietnamese education

## Confidence

**High Confidence**: The finding that GPT-4 achieves the highest average accuracy (71.24%) in five-shot settings is well-supported by the experimental results and aligns with established trends showing GPT-4's superior performance across various benchmarks.

**Medium Confidence**: The correlation between symbol binding ability and overall MCQA performance is supported by the data but requires careful interpretation, as multiple factors could contribute to this relationship.

**Low Confidence**: The assertion that current LLMs have not reached perfect performance in geography, history, and civic education should be qualified, as the evaluation methodology and dataset limitations may affect these results.

## Next Checks

1. **Dataset Generalization Test**: Validate the LaTeX formatting approach by testing model performance on a separate Vietnamese educational dataset with varying notation styles to determine if the strict formatting requirement is essential for good performance.

2. **Cross-Lingual Symbol Binding Evaluation**: Test the Vietnamese symbol binding dataset with LLMs trained primarily on English data to assess whether the symbol binding improvements generalize across languages and whether the Vietnamese-specific formatting provides additional benefits.

3. **Error Analysis on Subject-Specific Challenges**: Conduct a detailed error analysis categorizing failures by symbol type (mathematical, chemical, historical) and difficulty level to identify whether performance differences are primarily due to subject complexity or other factors like question structure and formatting requirements.