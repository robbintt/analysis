---
ver: rpa2
title: 'Playing with words: Comparing the vocabulary and lexical diversity of ChatGPT
  and humans'
arxiv_id: '2308.07462'
source_url: https://arxiv.org/abs/2308.07462
tags:
- chatgpt
- words
- vocabulary
- human
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares vocabulary usage and lexical richness between
  ChatGPT and humans across multiple datasets. Using four datasets with human and
  ChatGPT responses to various questions, researchers applied text preprocessing,
  tokenization, lemmatization, and POS tagging to analyze vocabulary and lexical richness
  metrics like RTTR and Mass.
---

# Playing with words: Comparing the vocabulary and lexical diversity of ChatGPT and humans

## Quick Facts
- arXiv ID: 2308.07462
- Source URL: https://arxiv.org/abs/2308.07462
- Reference count: 23
- ChatGPT consistently uses fewer distinct words and exhibits lower lexical richness than humans across analyzed datasets.

## Executive Summary
This study compares vocabulary usage and lexical richness between ChatGPT and humans using four datasets with responses to various questions. Researchers applied text preprocessing, tokenization, lemmatization, and POS tagging to analyze metrics like RTTR and Mass. Results consistently show ChatGPT uses fewer distinct words and exhibits lower lexical richness than humans, regardless of stopword removal or part-of-speech filtering. However, findings are preliminary due to limited datasets and single AI model evaluation.

## Method Summary
The study merged human and ChatGPT responses by category, then applied NLTK preprocessing including tokenization, POS tagging (Penn Treebank and Universal Tagset), lemmatization, and optional stopword removal. Researchers computed and compared RTTR and Mass metrics for lexical richness between human and ChatGPT responses, testing robustness with and without stopword removal and POS filtering.

## Key Results
- ChatGPT consistently produces text with lower lexical richness than humans across all datasets
- Differences persist regardless of stopword removal or POS filtering
- ChatGPT uses fewer distinct words compared to human responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT consistently produces text with lower lexical richness than humans.
- Mechanism: The model uses fewer distinct words and simpler word distributions, as measured by RTTR and Mass metrics.
- Core assumption: Vocabulary and lexical richness are measurable proxies for linguistic complexity and human-like expression.
- Evidence anchors:
  - [abstract] Results consistently show ChatGPT uses fewer distinct words and exhibits lower lexical richness than humans, regardless of stopword removal or part-of-speech filtering.
  - [section 3] The analysis shows that ChatGPT tends to use fewer distinct words and lower lexical richness than humans.
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.405, average citations=0.0. Top related titles: Beware of Words: Evaluating the Lexical Diversity of Conversational LLMs using ChatGPT as Case Study.
- Break condition: If ChatGPT's vocabulary becomes more diverse over time or if new versions use different architectures, the lexical richness gap may narrow or reverse.

### Mechanism 2
- Claim: Exposure to AI-generated text can influence human language evolution by reducing the frequency of underused words.
- Mechanism: Words absent from AI-generated content may fall out of use, as readers are less exposed to them and they are not included in future training data.
- Core assumption: Language learning and evolution are influenced by the frequency and diversity of word exposure.
- Evidence anchors:
  - [abstract] Focusing on one specific aspect of the language: words; will the use of tools such as ChatGPT increase or reduce the vocabulary used or the lexical richness? This has implications for words, as those not included in AI-generated content will tend to be less and less popular and may eventually be lost.
  - [section 1] Words serve as the medium through which the meanings of universal knowledge are conveyed, making them an indispensable heritage.
  - [corpus] Weak or missing corpus evidence for this claim; it is primarily inferred from linguistic theory and not directly tested in the paper.
- Break condition: If humans continue to produce and consume diverse content, or if AI tools are trained to preserve rare words, the loss of underused words may be mitigated.

### Mechanism 3
- Claim: Comparing vocabulary and lexical richness between ChatGPT and humans is challenging due to task, person, model version, and parameter dependencies.
- Mechanism: Lexical metrics like RTTR and Mass are sensitive to context, dataset, and preprocessing, making generalization difficult.
- Core assumption: Lexical richness is not a fixed property of an AI model but depends on how and where it is used.
- Evidence anchors:
  - [abstract] However, the results are very preliminary and additional datasets and ChatGPT configurations have to be evaluated to extract more general conclusions.
  - [section 3] These results are very preliminary, and further investigation is needed to understand how the different parameters influence the vocabulary used by humans and ChatGPT.
  - [corpus] Weak or missing corpus evidence; the claim is supported by the authors' own discussion of limitations.
- Break condition: If consistent patterns emerge across multiple datasets, tasks, and model versions, the dependency on context may lessen.

## Foundational Learning

- Concept: Natural Language Processing (NLP) and lexical analysis
  - Why needed here: To understand how vocabulary and lexical richness are measured and compared.
  - Quick check question: What is the difference between a token and a type in NLP?

- Concept: Text preprocessing (tokenization, lemmatization, POS tagging)
  - Why needed here: To prepare text for fair comparison between human and AI-generated content.
  - Quick check question: Why is lemmatization important when comparing vocabularies?

- Concept: Statistical measures of lexical richness (RTTR, Mass)
  - Why needed here: To quantify and compare the diversity of word usage in different texts.
  - Quick check question: How does the Mass metric differ from RTTR in measuring lexical richness?

## Architecture Onboarding

- Component map: Data ingestion -> Preprocessing pipeline -> Analysis engine -> Comparison module
- Critical path: Preprocessing -> Analysis -> Comparison -> Interpretation
- Design tradeoffs:
  - Tokenization granularity: Full words vs. subword units
  - Stopword handling: Keep vs. remove for lexical richness
  - POS filtering: Use all words vs. only nouns/verbs/adjectives/adverbs
- Failure signatures:
  - Inconsistent preprocessing leads to incomparable metrics
  - Dataset imbalance (e.g., ChatGPT answers much longer) skews counts
  - Missing metadata (e.g., prompt variations) limits causal conclusions
- First 3 experiments:
  1. Compare RTTR and Mass with and without stopword removal
  2. Repeat analysis for each POS category separately
  3. Test robustness by subsampling ChatGPT responses to match human response lengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ChatGPT's vocabulary usage and lexical richness vary across different languages beyond English?
- Basis in paper: [inferred] The paper only analyzes English text and notes the need to study "different types of text and languages" but does not explore other languages.
- Why unresolved: The study focuses exclusively on English datasets, and multilingual evaluation would require comparable datasets in other languages.
- What evidence would resolve it: Comparative analysis of ChatGPT's vocabulary and lexical richness across multiple languages using equivalent question-answer datasets in each language.

### Open Question 2
- Question: How do different ChatGPT model versions and configurations affect vocabulary usage and lexical richness?
- Basis in paper: [explicit] The paper states "different parameters...version of ChatGPT...have to be evaluated to extract more general conclusions."
- Why unresolved: The study uses a single ChatGPT version and configuration, and model updates could significantly change linguistic behavior.
- What evidence would resolve it: Systematic comparison of vocabulary and lexical richness across multiple ChatGPT versions and parameter settings using identical input prompts.

### Open Question 3
- Question: What is the long-term impact of AI-generated text on language evolution and word preservation?
- Basis in paper: [explicit] The abstract and discussion highlight concerns about "words not included in AI-generated content" becoming "less and less popular and may eventually be lost."
- Why unresolved: The study provides only a snapshot comparison and does not track changes over time or examine real-world language usage patterns.
- What evidence would resolve it: Longitudinal studies tracking word frequency and usage patterns in texts with varying proportions of AI-generated content over multiple years.

## Limitations
- Small number of datasets and single AI model evaluated limit generalizability
- Exact ChatGPT versions and parameters not fully detailed across datasets
- Response length differences between human and ChatGPT responses not fully controlled

## Confidence
- **High confidence**: ChatGPT uses fewer distinct words and exhibits lower lexical richness than humans across the datasets studied, as measured by RTTR and Mass metrics.
- **Medium confidence**: The observed differences in vocabulary and lexical richness are robust to stopword removal and POS filtering, but may vary with different tasks, model versions, or datasets.
- **Low confidence**: The long-term impact of AI-generated text on human vocabulary evolution is speculative and not directly evidenced in the current study.

## Next Checks
1. Replicate the analysis with additional datasets and multiple versions of ChatGPT (and other LLMs) to test the robustness of lexical richness differences.
2. Control for response length by subsampling or normalizing text, and assess how this affects RTTR and Mass metrics.
3. Test the impact of varying prompt phrasing and task context on ChatGPT's lexical output, to determine the sensitivity of results to input conditions.