---
ver: rpa2
title: 'PETA: Parameter-Efficient Trojan Attacks'
arxiv_id: '2310.00648'
source_url: https://arxiv.org/abs/2310.00648
tags:
- peft
- backdoor
- trigger
- peta
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces P ETA, a novel trojan attack designed for
  parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs). The
  attack uses bilevel optimization to embed a backdoor into a PLM while ensuring that
  the backdoor persists after PEFT.
---

# PETA: Parameter-Efficient Trojan Attacks

## Quick Facts
- arXiv ID: 2310.00648
- Source URL: https://arxiv.org/abs/2310.00648
- Reference count: 16
- Primary result: PETA achieves high attack success rates across multiple PEFT methods while maintaining clean accuracy

## Executive Summary
PETA introduces a novel trojan attack specifically designed for parameter-efficient fine-tuning (PEFT) of pre-trained language models. Using bilevel optimization, PETA embeds backdoors into PLMs while ensuring persistence through PEFT. The attack achieves high label flip rates across various downstream tasks and trigger designs while maintaining competitive clean accuracy, representing a significant advancement in backdoor attack methodologies targeting modern efficient fine-tuning paradigms.

## Method Summary
PETA employs bilevel optimization to embed backdoors into base PLMs while ensuring compatibility with PEFT. The upper-level objective embeds the backdoor into the base model while the lower-level objective simulates PEFT to preserve task performance. The attack uses a split dataset approach where a subset is poisoned with triggers during the first stage, then PEFT is performed on the clean subset in the second stage. This creates a backdoored PLM that maintains the backdoor after standard PEFT deployment.

## Key Results
- PETA achieves high label flip rates (LFR) across SST-2, Offenseval, and AG's News datasets
- The attack maintains competitive clean accuracy compared to baseline models
- PETL methods like LoRA and adapters show better backdoor persistence than prefix tuning
- Few-shot settings demonstrate robust attack performance even with limited data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bilevel optimization "orthogonalizes" the backdoor and PEFT modules, ensuring backdoor persistence through fine-tuning.
- Mechanism: The upper-level objective embeds the backdoor into the base PLM while the lower-level objective simulates PEFT to preserve task performance. Because PEFT freezes most parameters, the backdoor remains largely intact in the frozen base model.
- Core assumption: PEFT does not significantly modify the base PLM's internal representations that encode the backdoor.
- Evidence anchors: [abstract] "the lower-level objective simulates PEFT to retain the PLM's task-specific performance and ensure that the backdoor persists after fine-tuning."

### Mechanism 2
- Claim: The second stage of PETA may marginally weaken the backdoor to facilitate learning the downstream task.
- Mechanism: As the training dataset grows larger and more informative, the backdoor's influence is reduced to allow the model to focus on the primary task, but the backdoor remains functional due to the initial embedding strength.
- Core assumption: There is a trade-off between backdoor strength and task performance that PETA manages through its bilevel optimization.
- Evidence anchors: [section 5.1] "parameter-efficient fine-tuning during the second stage slightly weakens the backdoor during the process instead of strengthening it."

### Mechanism 3
- Claim: PETA creates prediction heads that are compatible with both the original PLM representations and the PEFT-modified representations.
- Mechanism: Through bilevel optimization, PETA ensures the final prediction head works well with representations from both the initial backdoored model and the final victim model after PEFT.
- Core assumption: The prediction head can generalize across different representation spaces created by the base model and PEFT modules.
- Evidence anchors: [section 5.2] "the prediction head is indeed compatible with both sets of representations when P ETA is paired with LoRA and adapters"

## Foundational Learning

- Concept: Bilevel optimization
  - Why needed here: PETA uses bilevel optimization to simultaneously embed the backdoor and ensure task performance during PEFT, which is the core technical innovation.
  - Quick check question: What are the two levels of optimization in PETA's bilevel framework, and what does each optimize?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: Understanding PEFT is crucial because PETA specifically targets PEFT's vulnerability where most parameters remain frozen during adaptation.
  - Quick check question: How does PEFT differ from standard fine-tuning, and why does this difference matter for backdoor persistence?

- Concept: Trojan/backdoor attacks in NLP
  - Why needed here: The entire attack framework relies on understanding how backdoors work in language models and how they can be triggered at inference time.
  - Quick check question: What are the key components of a textual trojan attack, and how does PETA's approach differ from traditional dataset poisoning methods?

## Architecture Onboarding

- Component map: RoBERTaBASE (frozen base PLM) -> PETL modules (adapters/prefix tuning/LoRA) -> Prediction head -> Trigger insertion function

- Critical path: 1. Split clean dataset into D⋆ (poisoned) and D′ (clean for PEFT) 2. Run bilevel optimization to embed backdoor in base PLM 3. Remove PETL modules, release backdoored PLM 4. User adds PETL modules and fine-tunes on D′ 5. Attacker triggers backdoor at inference time

- Design tradeoffs: PETL method selection affects backdoor persistence (prefix tuning vs LoRA/adapters), warmup epochs in bilevel optimization can improve compatibility, layer selection for defense impacts both accuracy and attack resistance

- Failure signatures: Clean accuracy drops significantly below baseline, label flip rate decreases substantially during PEFT, prediction head becomes incompatible with base model representations, defense mechanisms successfully neutralize the backdoor

- First 3 experiments: 1. Run PETA with each PETL method (adapters, prefix tuning, LoRA) on SST-2 to compare backdoor persistence 2. Test few-shot settings (8, 16, 32, 64 shots) to observe how PEFT affects backdoor strength 3. Remove PETL parameters from final model to test prediction head compatibility with original representations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of PEFT method (adapters, prefix tuning, LoRA) affect the effectiveness of PETA across different downstream tasks and trigger types?
- Basis in paper: [explicit] The paper evaluates PETA using three PEFT methods (adapters, prefix tuning, LoRA) and different triggers, but the exact impact of each combination on attack effectiveness is not fully analyzed.
- Why unresolved: The paper provides results showing variations in attack success rates and clean accuracy across PEFT methods and triggers, but does not deeply analyze why certain combinations are more effective than others.
- What evidence would resolve it: Detailed ablation studies and analyses comparing the performance of PETA across different PEFT methods and triggers, with explanations for observed differences.

### Open Question 2
- Question: What are the long-term effects of PETA on the security and performance of fine-tuned models in real-world applications?
- Basis in paper: [inferred] The paper demonstrates PETA's effectiveness in controlled experiments but does not address potential long-term impacts on model security and performance in practical scenarios.
- Why unresolved: The study focuses on immediate attack success rates and clean accuracy, without considering how PETA might affect models over time or in diverse real-world environments.
- What evidence would resolve it: Longitudinal studies and real-world testing of models fine-tuned with PETA, assessing changes in security and performance over extended periods and under varying conditions.

### Open Question 3
- Question: How can defenses against PETA be improved to ensure robustness across a wide range of PEFT methods and trigger types?
- Basis in paper: [explicit] The paper explores a simple defense mechanism but acknowledges its limitations and the need for further research.
- Why unresolved: The proposed defense is shown to be effective in some cases but not universally across all PEFT methods and triggers, indicating a need for more comprehensive defensive strategies.
- What evidence would resolve it: Development and evaluation of advanced defense mechanisms that can adapt to different PEFT methods and trigger types, with rigorous testing to ensure broad effectiveness.

## Limitations

- The paper relies heavily on the assumption that frozen base parameters remain stable during PEFT, without fully explaining how bilevel optimization achieves this orthogonalization
- Limited empirical evidence shows how the backdoor is actually encoded in the base model's representations
- The proposed defense mechanism (unfreezing subset of weights) appears relatively simplistic without thorough evaluation of potential bypass strategies

## Confidence

- **High confidence**: Experimental results showing PETA achieves high LFR across multiple datasets and PEFT methods while maintaining competitive clean accuracy
- **Medium confidence**: The claim that PEFT marginally weakens the backdoor during fine-tuning, based on observed trends in the experimental results
- **Medium confidence**: The mechanism explanation that bilevel optimization orthogonalizes backdoor and PEFT modules, though the technical details remain somewhat abstract
- **Low confidence**: The assertion that prefix tuning is particularly vulnerable to PETA due to representation incompatibility, as this requires deeper architectural analysis

## Next Checks

1. **Architectural compatibility analysis**: Measure representation similarity between base model outputs and PEFT-modified outputs using canonical correlation analysis or similar techniques to verify the claim about incompatible representations in prefix tuning.

2. **Ablation on optimization hyperparameters**: Systematically vary the proportion of poisoned data, learning rates, and bilevel optimization parameters to determine their impact on backdoor persistence and task performance trade-offs.

3. **Defense robustness evaluation**: Test the proposed defense mechanism against adaptive attackers who can optimize trigger placement based on which layers are unfrozen, and evaluate whether more sophisticated defenses (e.g., fine-tuning-based unlearning) would be more effective.