---
ver: rpa2
title: 'Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling
  Limit'
arxiv_id: '2309.16620'
source_url: https://arxiv.org/abs/2309.16620
tags:
- depth
- learning
- limit
- width
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of hyperparameter transfer in\
  \ deep learning models as they scale in size. While the \xB5P parameterization allows\
  \ for transfer of optimal hyperparameters across network widths, it does not always\
  \ transfer across depths."
---

# Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit

## Quick Facts
- arXiv ID: 2309.16620
- Source URL: https://arxiv.org/abs/2309.16620
- Reference count: 40
- Primary result: Proposes 1/√depth scaling of residual branches enabling hyperparameter transfer across depth and width

## Executive Summary
This work addresses the challenge of transferring optimal hyperparameters across different network scales in deep learning. While the µP parameterization enables transfer across network widths, it fails to transfer across depths. The authors propose a simple modification: scaling the residual branch by 1/√depth, which enables consistent transfer of hyperparameters across both depth and width. This approach is theoretically justified using dynamical mean field theory (DMFT) and validated empirically on CIFAR-10 and ImageNet datasets for residual architectures including convolutional ResNets and Vision Transformers.

## Method Summary
The method involves modifying residual networks by scaling the residual branch by 1/√depth while maintaining the µP parameterization with width scaling γ = γ₀/√N and learning rate η = η₀γ²₀N. This creates a parameterization that admits a well-defined feature learning joint infinite-width and infinite-depth limit. The approach uses DMFT to characterize network dynamics, showing that the proposed scaling ensures prediction and feature updates remain independent of network depth in the large width and depth limit. Training is performed using standard SGD with these modified initialization and scaling parameters.

## Key Results
- Empirical demonstrations of learning rate transfer across depth and width on CIFAR-10 and ImageNet for residual architectures
- Transfer of other hyperparameters like momentum coefficients and regularization strength
- Theoretical results on convergence of finite-size network dynamics towards the large width and depth limit
- Characterization of the limiting process and approximation error using DMFT framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hyperparameter transfer across depth and width is enabled by scaling the residual branch by 1/√depth in combination with the μP parameterization.
- Mechanism: The 1/√depth scaling ensures that feature and prediction updates remain independent of network depth in the large width and depth limit through DMFT description where preactivation dynamics follow a stochastic integral equation with layer time τ = ℓ/L.
- Core assumption: Residual network architecture with skip connections maintains long-range correlations across layers, crucial for consistent feature learning dynamics.
- Evidence anchors: [abstract], [section 2], corpus neighbors focus on scaling laws and parameter transfers
- Break condition: Deviation from 1/√depth scaling or absence of skip connections that preserve long-range correlations

### Mechanism 2
- Claim: DMFT provides a well-defined feature learning joint infinite-width and infinite-depth limit for the proposed parameterization.
- Mechanism: DMFT characterizes network dynamics at large width by tracking kernel order parameters Φℓ(x, x′; t, s) and Gℓ(x, x′; t, s), which evolve according to ODEs in layer time τ in the large depth limit.
- Core assumption: Network can be approximated by continuum process over layer time τ ∈ [0, 1] in large depth limit.
- Evidence anchors: [section 4.2], [section 4.1], corpus includes work on scaling laws and feature learning dynamics
- Break condition: Significant deviation from residual structure or insufficient width for mean field approximation

### Mechanism 3
- Claim: Proposed parameterization minimizes finite size approximation error, enabling consistent hyperparameter transfer across scales.
- Mechanism: 1/√depth scaling combined with μP initialization ensures finite width and depth network dynamics approximate infinite limit with minimal error by maintaining correct scale of feature learning updates.
- Core assumption: Finite size approximation error dominated by width fluctuations rather than depth effects, characterized by covariance matrix ΣL converging to limit as L → ∞.
- Evidence anchors: [section 4], [section 4.3.1], corpus neighbors discuss hyperparameter transfer
- Break condition: Improper scaling of learning rate schedule or other hyperparameters with width/depth, or insufficient depth for continuum approximation

## Foundational Learning

- Concept: Dynamical Mean Field Theory (DMFT)
  - Why needed here: DMFT provides theoretical framework for characterizing neural network dynamics at large width, allowing analysis of feature learning and hyperparameter transfer properties.
  - Quick check question: What are the key order parameters tracked in DMFT for residual networks, and how do they evolve during training?

- Concept: Neural Tangent Kernel (NTK) and Lazy Learning Regime
  - Why needed here: Understanding NTK regime helps contrast it with feature learning regime, explaining why proposed parameterization enables richer feature learning while maintaining transfer properties.
  - Quick check question: How does NTK regime differ from feature learning regime in terms of network dynamics and role of hyperparameters?

- Concept: Scaling Laws in Deep Learning
  - Why needed here: Work builds on scaling law research to understand how optimal hyperparameters transfer across different model scales, extending these ideas to depthwise transfer.
  - Quick check question: What is key insight from scaling law research that motivates need for depthwise hyperparameter transfer?

## Architecture Onboarding

- Component map:
  - Residual blocks with 1/√depth scaling on residual branch
  - μP parameterization with γ = γ₀/√N and learning rate η = η₀γ²₀N
  - Convolutional or transformer layers with appropriate width scaling
  - Batch normalization or layer normalization (optional, affects transfer consistency)
  - Readout layer with width-independent scaling

- Critical path:
  1. Initialize network with μP parameterization (γ = γ₀/√N)
  2. Scale residual branches by 1/√depth
  3. Train network with learning rate η = η₀γ²₀N
  4. Verify hyperparameter transfer across widths and depths
  5. Analyze convergence to infinite width/depth limit

- Design tradeoffs:
  - Using 1/√depth scaling vs normalization layers: Proposed method provides more consistent transfer but may require more careful initialization
  - Width scaling factor: μP uses √N scaling, which balances feature learning capacity with transfer properties
  - Depth scaling: 1/√depth factor is crucial for maintaining consistent dynamics across depths

- Failure signatures:
  - Divergence at larger depths when using standard μP without 1/√depth scaling
  - Inconsistent optimal learning rates across different network widths
  - Poor convergence to theoretical scaling limit for small network sizes

- First 3 experiments:
  1. Train small ResNet (depth 6, width 128) on CIFAR-10 with proposed parameterization and find optimal learning rate
  2. Scale network to depth 12, width 256 and verify same learning rate transfers successfully
  3. Train Vision Transformer with same parameterization and verify transfer properties compared to ResNet

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise theoretical mechanism by which the 1/√L scaling enables hyperparameter transfer across depth, and how does this compare to other potential parameterizations that might also enable such transfer?
- Basis in paper: [explicit] The paper discusses the 1/√L scaling in the context of enabling feature learning and hyperparameter transfer across depth, but does not provide a complete theoretical explanation for why this specific scaling is necessary or sufficient.
- Why unresolved: While the paper provides empirical evidence and some theoretical motivation using DMFT, a rigorous theoretical explanation for the necessity of the 1/√L scaling in enabling hyperparameter transfer across depth is not fully developed.
- What evidence would resolve it: A formal proof or more detailed theoretical analysis showing the relationship between 1/√L scaling and stability of feature learning dynamics across depth, possibly comparing it with other parameterizations.

### Open Question 2
- Question: How does performance of 1/√L-scaled residual networks compare to standard residual networks with normalization layers in terms of both accuracy and computational efficiency across different depths and widths?
- Basis in paper: [explicit] The paper mentions that normalization layers can improve hyperparameter transfer to some extent but that 1/√L scaling provides more consistent transfer, and it also discusses the role of normalization layers in training stability.
- Why unresolved: While the paper provides some empirical comparisons, a comprehensive study comparing the two approaches in terms of both accuracy and computational efficiency across a wide range of depths and widths is not presented.
- What evidence would resolve it: A detailed empirical study comparing the two approaches across various depths, widths, and datasets, measuring both accuracy and computational efficiency.

### Open Question 3
- Question: Can the principles of depthwise hyperparameter transfer observed in residual networks be generalized to other architectures such as recurrent neural networks or graph neural networks?
- Basis in paper: [explicit] The paper focuses on residual networks and provides some evidence for transfer in Vision Transformers, but does not explore other architectures like RNNs or GNNs.
- Why unresolved: The paper's findings are limited to specific architectures, and it is unclear whether the principles of depthwise hyperparameter transfer can be extended to other types of neural networks.
- What evidence would resolve it: Empirical studies or theoretical analyses demonstrating the applicability of 1/√L scaling or similar principles to other architectures like RNNs or GNNs, showing consistent hyperparameter transfer across depth.

## Limitations
- Theory assumes infinite width and depth, which may not hold for practical network sizes
- Results primarily demonstrated for supervised learning tasks on image datasets
- Parameterization requires careful initialization and may be sensitive to implementation details

## Confidence
- High: The claim that 1/√depth scaling enables hyperparameter transfer across depth and width is well-supported by both theoretical analysis and empirical results
- Medium: The theoretical characterization of the joint infinite-width and infinite-depth limit using DMFT is mathematically rigorous but relies on simplifying assumptions
- Low: The claim that this approach generalizes to all residual architectures and hyperparameters without further modification

## Next Checks
1. **Architecture Generalization**: Test the parameterization on non-residual architectures (e.g., MLPs, recurrent networks) to verify the universality of the 1/√depth scaling approach

2. **Hyperparameter Robustness**: Systematically evaluate transfer of additional hyperparameters beyond learning rate, momentum, and regularization strength, including batch size and weight decay schedules

3. **Finite-Size Analysis**: Quantify the approximation error between finite networks and the theoretical infinite limit for various depth-width combinations, and determine practical bounds for reliable transfer