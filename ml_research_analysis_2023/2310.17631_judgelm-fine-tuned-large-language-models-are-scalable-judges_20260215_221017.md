---
ver: rpa2
title: 'JudgeLM: Fine-tuned Large Language Models are Scalable Judges'
arxiv_id: '2310.17631'
source_url: https://arxiv.org/abs/2310.17631
tags:
- judge
- judgelm
- reference
- answers
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: JudgeLM addresses the challenge of evaluating large language models
  in open-ended scenarios by fine-tuning LLMs as scalable judges. The method involves
  creating a high-quality, large-scale dataset with task seeds, LLM-generated answers,
  and GPT-4-generated judgments, then fine-tuning JudgeLM models of varying sizes
  (7B, 13B, 33B parameters).
---

# JudgeLM: Fine-tuned Large Language Models are Scalable Judges

## Quick Facts
- arXiv ID: 2310.17631
- Source URL: https://arxiv.org/abs/2310.17631
- Authors: 
- Reference count: 28
- Key outcome: JudgeLM achieves over 90% agreement with GPT-4 teacher judge, surpassing human-to-human agreement on LLM evaluation tasks.

## Executive Summary
JudgeLM addresses the challenge of evaluating large language models in open-ended scenarios by fine-tuning LLMs to serve as scalable judges. The method involves creating a high-quality dataset with task seeds, LLM-generated answers, and GPT-4-generated judgments, then fine-tuning JudgeLM models of varying sizes (7B, 13B, 33B parameters). The approach introduces bias mitigation techniques including swap augmentation, reference support, and reference drop to address position, knowledge, and format biases. JudgeLM demonstrates state-of-the-art performance and extended capabilities in grading single answers, judging multiple answers, multimodal models, and multi-turn chat.

## Method Summary
JudgeLM is created by fine-tuning LLMs (7B, 13B, 33B parameters) on a dataset of 105K seed tasks, LLM-generated answers, and GPT-4 judgments. The training process incorporates three key bias mitigation techniques: swap augmentation to address position bias by randomly swapping answer positions, reference support to mitigate knowledge bias by incorporating external reference answers, and reference drop to address format bias by randomly omitting references during training. The models are evaluated on a 5K validation set using metrics including agreement with GPT-4, consistency, precision, recall, F1-score, and bias measurements.

## Key Results
- JudgeLM-7B achieves 90.1% agreement with GPT-4 teacher judge on the validation set
- Swap augmentation improves consistency by 5.44% by reducing position bias
- Reference support with reference drop addresses knowledge and format biases, achieving superior performance across all metrics
- The 7B model efficiently judges 5K samples in 3 minutes using 8 A100 GPUs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Swap augmentation addresses position bias by teaching the model to focus on answer content rather than positional order.
- Mechanism: During training, answer pairs are randomly swapped and corresponding scores and indexes are also swapped. This forces the model to evaluate based on content rather than position.
- Core assumption: Position bias exists in LLM judges and can be mitigated through data augmentation that exposes the model to both positional arrangements.
- Evidence anchors:
  - [abstract]: "We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance."
  - [section 4]: "swap augmentation can bring comprehensive improvements to the baseline model. It improves consistency by 5.44%, which demonstrates that swap augmentation can reduce the influence of position bias and push the judge to pay more attention to the contents of answers."

### Mechanism 2
- Claim: Reference support mitigates knowledge bias by incorporating external knowledge into the judgment process.
- Mechanism: The model is trained with optional reference answers that provide ground truth or additional context, teaching it to rely on this external knowledge when making judgments.
- Core assumption: LLMs have knowledge gaps that can be filled by providing reference answers, and the model can effectively integrate this external knowledge into its judgment process.
- Evidence anchors:
  - [abstract]: "We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance."
  - [section 4]: "JudgeLM fine-tuned with reference support exhibits superior performance on every metric. It demonstrates that the introduction of reference answers induces the judge to rely on external knowledge and addresses the limitation of pre-trained knowledge."

### Mechanism 3
- Claim: Reference drop addresses format bias by preventing the model from overfitting to a specific template format.
- Mechanism: During training, reference answers are randomly dropped from some samples, forcing the model to handle both cases (with and without reference) and preventing overfitting to a single format.
- Core assumption: Models can overfit to the specific format of training data, leading to poor performance when the evaluation format differs from training format.
- Evidence anchors:
  - [abstract]: "We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance."
  - [section 4]: "With the help of the reference drop, the JudgeLM can handle both the format with or without reference and achieve higher agreement and consistency. It demonstrates that reference drop can address the format bias and avoid the JudgeLM overfitting to a single format."

## Foundational Learning

- Concept: Instruction fine-tuning
  - Why needed here: JudgeLM is built by fine-tuning LLMs to follow instructions for judging tasks, which requires understanding the instruction fine-tuning paradigm.
  - Quick check question: What is the key difference between pre-training and instruction fine-tuning in the context of LLMs?

- Concept: Bias mitigation in machine learning
  - Why needed here: The paper addresses multiple biases (position, knowledge, format) in judge models, requiring understanding of bias identification and mitigation techniques.
  - Quick check question: How does data augmentation help mitigate position bias in ranking models?

- Concept: Evaluation metrics for ranking systems
  - Why needed here: The paper uses metrics like agreement, precision, recall, F1-score, and consistency to evaluate judge models, requiring understanding of these metrics in ranking contexts.
  - Quick check question: What is the difference between agreement and consistency when evaluating ranking models?

## Architecture Onboarding

- Component map:
  Data generation pipeline -> Fine-tuning module -> Inference engine -> Evaluation module

- Critical path:
  1. Generate high-quality dataset with diverse seed tasks and judgments
  2. Fine-tune JudgeLM with appropriate techniques to address biases
  3. Evaluate model performance on benchmarks
  4. Deploy for various judging scenarios

- Design tradeoffs:
  - Model size vs. efficiency: Larger models (33B) achieve higher performance but require more resources
  - Dataset size vs. cost: Larger datasets improve performance but increase generation cost
  - Reference inclusion vs. flexibility: References improve accuracy but reduce the model's ability to judge without them

- Failure signatures:
  - Low consistency after swap augmentation: Indicates position bias persists or augmentation is not effective
  - Poor performance when references are dropped: Suggests reference support is not generalizing
  - Mismatched format performance: Indicates format bias not properly addressed

- First 3 experiments:
  1. Baseline evaluation: Fine-tune JudgeLM-7B on 3.5K samples without any bias mitigation techniques and evaluate on the validation set
  2. Swap augmentation impact: Add swap augmentation to the baseline and compare consistency and agreement metrics
  3. Reference support effectiveness: Train with reference support and evaluate performance with and without reference answers to measure format bias mitigation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of JudgeLM scale when evaluated on benchmarks that require multi-modal reasoning beyond the examples shown in the paper?
- Basis in paper: [explicit] The paper demonstrates JudgeLM's ability to evaluate multimodal models on the MM-Vet benchmark but only provides a limited set of examples. The authors suggest JudgeLM can replace GPT-4 for multimodal evaluation but do not provide comprehensive scaling analysis.
- Why unresolved: The paper only shows case studies on MM-Vet and does not systematically evaluate JudgeLM's performance across a wide range of multimodal benchmarks or analyze its limitations in handling complex multimodal tasks.
- What evidence would resolve it: A comprehensive evaluation of JudgeLM on multiple multimodal benchmarks (e.g., ScienceQA, MMMU, Image-GPT) with quantitative metrics comparing its performance to human annotators and GPT-4 across different types of multimodal reasoning tasks.

### Open Question 2
- Question: What is the impact of synthetic data generation on JudgeLM's performance, and how does it compare to using only human-annotated or GPT-4-generated data?
- Basis in paper: [inferred] The authors mention that the cost of generating high-quality GPT-4 judgments limits further scaling and express interest in using synthetic data to improve performance, but do not conduct experiments comparing synthetic vs. human/GPT-4-generated data.
- Why unresolved: The paper does not explore the trade-offs between data quality and quantity when using synthetic data, nor does it provide evidence on whether synthetic data can achieve comparable performance to human or GPT-4 annotations.
- What evidence would resolve it: Controlled experiments comparing JudgeLM's performance when trained on different proportions of synthetic vs. human/GPT-4 data, along with analysis of the quality of synthetic data and its impact on model generalization.

### Open Question 3
- Question: How does JudgeLM's performance generalize to domains and tasks that are significantly different from the instruction-following tasks in the training data?
- Basis in paper: [explicit] The paper acknowledges that JudgeLM is trained on instruction-following tasks and shows some extended capabilities, but does not systematically evaluate its performance on out-of-distribution tasks or domains.
- Why unresolved: The authors do not provide experiments testing JudgeLM's robustness to domain shifts or its ability to judge tasks in specialized fields like legal reasoning, medical diagnosis, or scientific analysis.
- What evidence would resolve it: Cross-domain evaluation of JudgeLM on tasks from diverse fields, measuring performance degradation and identifying the types of knowledge or reasoning that the model struggles with when encountering unfamiliar domains.

## Limitations
- Limited generalization beyond GPT-4 judgments may introduce circular reasoning and inherited biases
- High dataset creation cost and reliance on GPT-4 API limits accessibility for researchers with limited resources
- Temporal validity concerns as LLM capabilities evolve rapidly may cause performance degradation over time

## Confidence
- High confidence: JudgeLM achieving over 90% agreement with GPT-4 teacher judge
- Medium confidence: JudgeLM achieving higher human-to-human agreement than human-to-GPT-4 agreement
- Low confidence: JudgeLM's extended capabilities in grading single answers, judging multiple answers, and handling multimodal models

## Next Checks
1. Cross-judge validation: Evaluate JudgeLM against multiple independent human judgment sets and alternative LLM judges to assess generalization beyond GPT-4
2. Cost-effectiveness analysis: Implement scaled-down version using fewer samples to determine minimum viable dataset size for practical applications
3. Long-term stability assessment: Retrain JudgeLM with periodically updated datasets using newer LLM generations to identify performance degradation patterns and update requirements