---
ver: rpa2
title: Sources of Hallucination by Large Language Models on Inference Tasks
arxiv_id: '2305.14552'
source_url: https://arxiv.org/abs/2305.14552
tags:
- llms
- which
- language
- hypothesis
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates hallucination in Large Language Models
  (LLMs) by probing their behavior on Natural Language Inference (NLI) tasks. The
  authors conduct controlled experiments on LLaMA, GPT-3.5, and PaLM models, demonstrating
  two major sources of hallucination: memorization of training data and a corpus-based
  frequency heuristic.'
---

# Sources of Hallucination by Large Language Models on Inference Tasks

## Quick Facts
- arXiv ID: 2305.14552
- Source URL: https://arxiv.org/abs/2305.14552
- Reference count: 19
- Key outcome: LLMs are 1.9-2.2x more likely to falsely predict entailment when hypothesis is attested in training data and 1.5-2.0x more likely when hypothesis is more frequent than premise

## Executive Summary
This paper investigates hallucination in Large Language Models (LLMs) by probing their behavior on Natural Language Inference (NLI) tasks. The authors conduct controlled experiments on LLaMA, GPT-3.5, and PaLM models, demonstrating two major sources of hallucination: memorization of training data and a corpus-based frequency heuristic. They show that LLMs are significantly more likely to falsely predict entailment when the hypothesis is attested in training data (1.9-2.2x more likely) or when the hypothesis is more frequent than the premise (1.5-2.0x more likely). These biases lead to severe performance degradation on NLI subsets adversarial to these factors, with models deteriorating from excellent to near-random classifiers. The findings suggest that LLMs rely on unsatisfactory tools like memory recall and frequency heuristics rather than true language understanding, raising concerns about their reliability for downstream tasks requiring inference capabilities.

## Method Summary
The authors investigate hallucination in LLMs by conducting controlled experiments on NLI tasks using the Levy/Holt dataset (1,784 directional NLI questions). They apply dataset transformations (IRP, ITA, IRA) to create subsets that either preserve or break factors of interest like hypothesis veracity and relative frequency. Using prompt-based queries with 4 in-context examples, they query LLaMA-65B, GPT-3.5, and PaLM-540B models. Performance is evaluated using precision, recall, and AUC-normalized scores across consistent and adversarial subsets. The study uses Google N-grams as a proxy for corpus frequency statistics.

## Key Results
- LLMs are 1.9x (LLaMA-65B), 2.2x (GPT-3.5), and 2.0x (PaLM-540B) more likely to wrongly predict entailment when hypothesis is attested in training data
- LLMs are 1.5x (LLaMA-65B), 1.7x (GPT-3.5), and 2.0x (PaLM-540B) more likely to wrongly predict entailment when hypothesis is more frequent than premise
- Model performance deteriorates severely on adversarial subsets, dropping from AUCnorm scores of 65.5 to 8.1 for LLaMA-65B

## Why This Works (Mechanism)

### Mechanism 1: Memory-based recall
- Claim: LLMs rely on memory recall of training data when evaluating hypothesis veracity
- Mechanism: The model accesses memorized statements by treating entity names as indices that retrieve related training text
- Core assumption: Training data contains factual statements about entities, and these are stored in model weights in retrievable form
- Evidence anchors: Models are 1.9x (LLaMA-65B), 2.2x (GPT-3.5), and 2.0x (PaLM-540B) more likely to wrongly predict entailment when hypothesis is attested in training

### Mechanism 2: Corpus frequency heuristic
- Claim: LLMs exploit corpus frequency patterns to infer entailment relationships
- Mechanism: The model learns that more specific predicates tend to be less frequent in natural text, and uses this statistical heuristic to infer that specific-to-general entailment is likely
- Core assumption: Training corpora contain natural language distribution where specificity inversely correlates with frequency
- Evidence anchors: Models are 1.5x (LLaMA-65B), 1.7x (GPT-3.5), and 2.0x (PaLM-540B) more likely to wrongly predict entailment when hypothesis is more frequent than premise

### Mechanism 3: Performance degradation on adversarial subsets
- Claim: These biases cause severe performance degradation on adversarial NLI subsets
- Mechanism: When test samples contradict the learned biases, models cannot rely on memory or frequency heuristics and must perform actual reasoning, which they cannot do effectively
- Core assumption: The model's reasoning capability is insufficient for true language understanding and must rely on shortcuts
- Evidence anchors: Models deteriorate from excellent to near-random classifiers on adversarial subsets, with AUCnorm dropping from 65.5 to 8.1 for LLaMA-65B

## Foundational Learning

- **Natural Language Inference (NLI)**: The fundamental task of determining whether one sentence logically follows from another
  - Why needed here: The paper investigates hallucination specifically in NLI tasks, which are fundamental to language understanding
  - Quick check question: What is the difference between directional and symmetric entailment?

- **Prompt engineering and few-shot learning**: Techniques for eliciting desired behavior from LLMs using carefully crafted prompts
  - Why needed here: The experiments rely on carefully crafted prompts with minimal examples to elicit model behavior
  - Quick check question: How does the number of in-context examples affect model performance in few-shot settings?

- **Corpus frequency analysis and N-gram statistics**: Understanding how word and phrase frequencies relate to linguistic patterns
  - Why needed here: The relative frequency heuristic depends on understanding how predicate frequencies relate to entailment
  - Quick check question: Why does the paper use Google N-grams instead of actual LLM training corpus statistics?

## Architecture Onboarding

- **Component map**: Levy/Holt dataset -> Transformation pipelines (IRP, ITA, IRA) -> Prompt templates with in-context examples -> LLM APIs -> Response parsing -> Evaluation metrics
- **Critical path**: Dataset transformation → Prompt generation → LLM query → Response parsing → Evaluation metric calculation
- **Design tradeoffs**: Using Google N-grams as proxy for corpus frequency vs. actual training data frequencies introduces noise but enables reproducibility
- **Failure signatures**: If entity replacement doesn't degrade performance, the memory hypothesis is wrong. If frequency effects persist with typed identifiers, the heuristic is stronger than expected. If both fail simultaneously, the model may have genuine reasoning capability.
- **First 3 experiments**:
  1. Run the original Levy/Holt dataset with few-shot prompts to establish baseline performance
  2. Apply the IRP transformation and measure change in false positive rate when hypothesis veracity changes
  3. Apply the ITA transformation and measure performance difference between frequent and infrequent entity replacements

## Open Questions the Paper Calls Out

- **Mechanism of entity-based indexing**: What is the precise mechanism by which named entities act as "indices" to access memorized information in LLMs? The paper demonstrates the effect but does not provide a detailed explanation of the underlying mechanism.

- **Interaction of biases**: How do the effects of the veracity prior and relative frequency heuristic interact with each other, and which one is more dominant in different scenarios? The paper mentions a "tension between the two factors" but does not provide systematic analysis of their relative importance.

- **Prompt engineering solutions**: What are the most effective prompt engineering techniques to mitigate the effects of the veracity prior and relative frequency heuristic? The paper notes that explicit instructions to ignore veracity did not significantly improve performance and calls for further research.

## Limitations

- The exact retrieval mechanism for memory-based recall remains speculative despite strong correlation evidence
- Using Google N-grams as a proxy for training data statistics may not accurately reflect true corpus frequencies
- Results are highly sensitive to prompt engineering choices and may not generalize across different prompt structures

## Confidence

- **High confidence**: Existence of veracity bias and general methodology of using controlled dataset transformations
- **Medium confidence**: Specific mechanisms proposed (memory recall, frequency heuristic) and quantitative effect sizes
- **Low confidence**: Universality of these biases across different LLM architectures and generalizability beyond the specific NLI task

## Next Checks

1. **Cross-dataset replication**: Test the same experimental design on multiple NLI datasets (e.g., SNLI, MultiNLI) to determine whether the veracity and frequency biases are consistent across different linguistic phenomena and domain distributions.

2. **Architecture ablation study**: Compare the biases across models with different training regimes (base vs. instruction-tuned), parameter counts, and pretraining durations to understand which architectural factors contribute most strongly to these hallucinatory behaviors.

3. **Mechanism dissection experiment**: Replace entity names with both typed placeholders AND random novel entities to distinguish between entity-based memory recall and more general pattern-based frequency heuristics, providing stronger evidence for the proposed mechanisms.