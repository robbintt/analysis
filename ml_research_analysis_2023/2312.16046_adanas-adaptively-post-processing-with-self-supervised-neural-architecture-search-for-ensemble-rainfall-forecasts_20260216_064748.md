---
ver: rpa2
title: 'AdaNAS: Adaptively Post-processing with Self-supervised Neural Architecture
  Search for Ensemble Rainfall Forecasts'
arxiv_id: '2312.16046'
source_url: https://arxiv.org/abs/2312.16046
tags:
- search
- precipitation
- rainfall
- data
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdaNAS, a self-supervised neural architecture
  search method for post-processing ensemble rainfall forecasts. The method aims to
  improve the accuracy of precipitation predictions, particularly in high-rainfall
  areas, by automatically designing efficient neural network architectures.
---

# AdaNAS: Adaptively Post-processing with Self-supervised Neural Architecture Search for Ensemble Rainfall Forecasts

## Quick Facts
- arXiv ID: 2312.16046
- Source URL: https://arxiv.org/abs/2312.16046
- Reference count: 39
- Primary result: AdaNAS achieves 80.5% improvement in MAE and 80.3% improvement in RMSE for rainfall forecasting compared to manual methods

## Executive Summary
This paper introduces AdaNAS, a self-supervised neural architecture search method for post-processing ensemble rainfall forecasts. The method aims to improve the accuracy of precipitation predictions, particularly in high-rainfall areas, by automatically designing efficient neural network architectures. AdaNAS incorporates a rainfall-aware search space, a rainfall-level regularization function, and a block-wise self-supervised comparative learning approach. Experiments conducted on the TIGGE dataset show that AdaNAS significantly outperforms previous manual methods and other NAS techniques in terms of precipitation amount prediction and intensity classification.

## Method Summary
AdaNAS uses a self-supervised neural architecture search approach with block-wise contrastive learning to find optimal neural network architectures for post-processing ensemble rainfall forecasts. The method employs a rainfall-aware search space that includes residual blocks, channel-aware blocks, and space-aware blocks, along with a rainfall-level regularization function that combines MSE loss with HSS-based loss. The search process trains each block independently before combining them, reducing shared-weight bias in NAS evaluation. The final model is trained on TIGGE dataset ensemble forecasts and observations from southern China, targeting improvements in both precipitation amount prediction and intensity classification.

## Key Results
- AdaNAS achieves an average MAE of 0.98 mm/day and RMSE of 2.04 mm/day
- Improvements of 80.5% in MAE and 80.3% in RMSE compared to the best-performing manual method
- Superior performance in rainfall level classification with 17.1% improvement in accuracy and 8.2% improvement in HSS on the single-model dataset
- 11.6% improvement in accuracy and 1.1% improvement in HSS on the multi-model dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Block-wise self-supervised contrastive learning reduces shared-weight bias in NAS evaluation.
- Mechanism: Instead of sharing all weights across candidate architectures as in one-shot NAS, the method trains each block independently before combining them. This reduces the gap between shared weights and optimal sub-network weights, improving architecture ranking accuracy.
- Core assumption: The ranking of architectures using shared weights is not necessarily the true ranking because shared weights are far from optimal for each sub-network.
- Evidence anchors:
  - [abstract] "The self-supervised technology is applied in the neural network architecture search method to make fuller use of the limited data samples."
  - [section] "A block-wise method is ideal to reduce shared weights by splitting the depth of the network... Each block of the hyper-network is trained separately before being connected to the overall search."
- Break condition: If the block-wise training introduces excessive computational overhead that outweighs the accuracy gains, or if the independent training causes blocks to become incompatible when combined.

### Mechanism 2
- Claim: Rainfall-aware search space with CAB and SAB blocks improves prediction accuracy in high-rainfall areas.
- Mechanism: The search space includes residual blocks (RB) for feature extraction, plus channel-aware blocks (CAB) and space-aware blocks (SAB) that focus attention on high-rainfall pixels. CAB assigns larger weights to pixels with significant rainfall differences, while SAB captures spatial attention. This design specifically targets the challenge of predicting heavy rainfall events which are underrepresented in the data.
- Core assumption: Heavy rainfall events are underrepresented in training data (only 3% in the dataset), making it difficult for standard architectures to learn their patterns.
- Evidence anchors:
  - [abstract] "we design a rainfall-aware search space to significantly improve forecasts for high-rainfall areas."
  - [section] "To ensure a more accurate prediction of the less frequent cases such as heavy rainfall, we introduce the transformer-based blocks CAB and SAB... CAB and SAB are used to capture the heavy rainfall pixels in the ensemble forecast data and assign a greater weight to them to highlight outstanding features."
- Break condition: If the attention mechanisms overfit to noise or if the computational cost of CAB/SAB blocks outweighs their performance benefits.

### Mechanism 3
- Claim: Rainfall-level regularization function improves precipitation intensity classification accuracy.
- Mechanism: The loss function combines MSE for precipitation amounts with HSS (Heidke skill score) for intensity classification. HSS is introduced because it excludes correct predictions due to random chance, which is important when dealing with highly imbalanced rainfall categories (76.4% light precipitation).
- Core assumption: Standard MSE-based regularization is insufficient for multi-category classification tasks with severe class imbalance.
- Evidence anchors:
  - [abstract] "we propose a rainfall-level regularization function to eliminate the effect of noise data during the training."
  - [section] "Although existing methods optimized by the MSE can predict some examples, it still restrains the performance of the prediction in rainfall levels due to the lack of effective constraints... HSS rules out this possibility, so we introduce the HSS into the regularization function"
- Break condition: If the HSS coefficient becomes too large, it may negatively impact bias metrics as observed in experiments when cH = 10.

## Foundational Learning

- Concept: Neural Architecture Search (NAS) fundamentals
  - Why needed here: The paper implements a custom NAS method for weather forecasting, requiring understanding of how NAS works and why traditional approaches fail for this domain
  - Quick check question: What are the three main categories of NAS methods and how does the gradient-based approach differ from evolutionary or reinforcement learning approaches?

- Concept: Self-supervised learning and contrastive learning
  - Why needed here: The method uses block-wise self-supervised contrastive learning to improve architecture search without requiring labeled data
  - Quick check question: How does the momentum update (EMA) in the target network work in contrastive learning, and why is it important for stable training?

- Concept: Precipitation forecasting evaluation metrics
  - Why needed here: The paper uses domain-specific metrics (Bias, MAE, RMSE, NSE, ACC, HSS) that are different from standard ML evaluation
  - Quick check question: What is the difference between ACC and HSS, and why is HSS more appropriate for imbalanced classification problems like rainfall level prediction?

## Architecture Onboarding

- Component map:
  Input layer -> Stem layer -> Backbone (4 blocks with mixed operations) -> Projector -> Loss (MSE + HSS regularization)

- Critical path:
  1. Search phase: Block-wise contrastive learning to find optimal architecture
  2. Retrain phase: Supervised training of the searched architecture with HSS regularization
  3. Evaluation: Compare against manual methods and other NAS approaches

- Design tradeoffs:
  - Block-wise training vs. full-network training: More accurate architecture ranking but increased computational cost
  - CAB/SAB complexity vs. standard operations: Better heavy rainfall prediction but more parameters
  - HSS regularization coefficient: Higher values improve classification but may hurt bias metrics

- Failure signatures:
  - Search phase: If validation performance doesn't improve across search epochs, the contrastive learning may be unstable
  - Retrain phase: If MAE improves but NSE degrades, the regularization may be over-emphasizing classification
  - Deployment: If predictions are consistently biased in coastal regions, the attention mechanisms may be overfitting to training data

- First 3 experiments:
  1. Verify the block-wise contrastive learning works by comparing architecture rankings from block-wise vs. full-network training on a small subset
  2. Test the CAB/SAB attention mechanisms by ablating them and measuring performance on heavy rainfall prediction
  3. Validate the HSS regularization by sweeping the coefficient and plotting the bias vs. classification accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AdaNAS compare to other NAS methods when applied to different geographical regions with varying precipitation patterns?
- Basis in paper: [explicit] The paper mentions that the study area includes part of the coastal and inland regions of southern China, and discusses the challenges of rainfall forecasting in heavy coastal rainfall areas.
- Why unresolved: The paper only evaluates AdaNAS on a specific region in southern China, and does not provide comparative results for other geographical areas with different precipitation patterns.
- What evidence would resolve it: Conducting experiments using AdaNAS on datasets from various geographical regions with diverse precipitation patterns, and comparing the results with other NAS methods.

### Open Question 2
- Question: How sensitive is AdaNAS to changes in the hyperparameter cH (coefficient of HSS in regularization functions) when applied to different types of precipitation data?
- Basis in paper: [explicit] The paper explores the impact of HSS coefficient cH on model performance, showing that different values of cH can affect accuracy and bias.
- Why unresolved: The paper only investigates the effect of cH on the specific dataset used in the study, and does not explore how changes in cH might affect performance on different types of precipitation data.
- What evidence would resolve it: Performing experiments with varying values of cH on different types of precipitation data and analyzing the impact on model performance.

### Open Question 3
- Question: How does the proposed self-supervised search strategy in AdaNAS perform compared to other search strategies in terms of computational efficiency and accuracy?
- Basis in paper: [explicit] The paper introduces a self-supervised search strategy to improve the efficiency of neural architecture search, but does not compare its performance to other search strategies.
- Why unresolved: The paper does not provide a direct comparison between the self-supervised search strategy and other search strategies in terms of computational efficiency and accuracy.
- What evidence would resolve it: Conducting experiments comparing the self-supervised search strategy with other search strategies, such as random search or supervised search, in terms of computational efficiency and accuracy.

## Limitations

- The specific implementation details of CAB and SAB blocks are not fully specified, making exact reproduction challenging
- The dataset distribution (76.4% light precipitation) may limit generalizability to regions with different rainfall patterns
- The computational cost of block-wise contrastive learning is not quantified, which is important for practical deployment considerations

## Confidence

- **High Confidence**: The core methodology of using self-supervised contrastive learning for NAS is well-established in the broader ML literature, and the specific application to rainfall forecasting follows logical extensions of existing techniques
- **Medium Confidence**: The reported improvements in MAE/RMSE and classification metrics are supported by the experimental setup, though the exact magnitude may vary with different implementations
- **Low Confidence**: The specific contributions of individual components (CAB/SAB blocks, HSS regularization coefficient) to the overall performance gains are difficult to isolate without more detailed ablation studies

## Next Checks

1. Implement a simplified version of the CAB and SAB blocks to verify their contribution to heavy rainfall prediction before attempting full reproduction
2. Conduct ablation studies systematically removing the HSS regularization and attention mechanisms to quantify their individual impacts
3. Test the search algorithm on a smaller, controlled dataset to verify that block-wise contrastive learning actually produces better architecture rankings than standard weight-sharing approaches