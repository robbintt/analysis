---
ver: rpa2
title: Towards Semi-supervised Learning with Non-random Missing Labels
arxiv_id: '2308.08872'
source_url: https://arxiv.org/abs/2308.08872
tags:
- uni00000055
- class
- uni00000048
- uni00000044
- uni00000052
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of semi-supervised learning (SSL)
  in the presence of non-randomly missing labels (MNAR), where the labeled and unlabeled
  data have different class distributions. The authors propose a class transition
  tracking based Pseudo-Rectifying Guidance (PRG) method to mitigate the bias in label
  imputation caused by MNAR.
---

# Towards Semi-supervised Learning with Non-random Missing Labels

## Quick Facts
- arXiv ID: 2308.08872
- Source URL: https://arxiv.org/abs/2308.08872
- Authors: [Not specified in source]
- Reference count: 40
- Key outcome: PRG achieves 94.04% accuracy on CIFAR-10 vs 79.63% for CADR baseline in MNAR settings

## Executive Summary
This paper addresses the challenge of semi-supervised learning (SSL) when labeled and unlabeled data have different class distributions (MNAR). The authors propose PRG (Pseudo-Rectifying Guidance), which uses class transition tracking and Markov random walks to guide pseudo-label assignment toward underrepresented classes. By dynamically tracking class transitions during training and using this information to re-weight pseudo-label probabilities, PRG maintains unbiased label assignment across all classes, significantly improving performance over existing methods.

## Method Summary
The PRG method builds upon existing SSL frameworks like FixMatch by introducing a class transition tracking mechanism. For each unlabeled sample, the model tracks class prediction changes across training epochs in a class transition matrix. This matrix is converted into a Markov transition probability matrix, which is then re-weighted based on current pseudo-label class distributions. The resulting guidance is applied to adjust pseudo-label assignment, encouraging transitions to underrepresented classes. PRG can use either current or previous epoch predictions for transition tracking, and is compatible with various SSL frameworks.

## Key Results
- PRG achieves 94.04% accuracy on CIFAR-10 with standard deviation of 0.18
- Outperforms best baseline (CADR) by 14.41 percentage points under same protocol
- Maintains effectiveness across various MNAR scenarios with different imbalance ratios
- Demonstrates robustness through ablation studies on key hyperparameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The class transition tracking matrix C records the frequency of class prediction changes during pseudo-label assignment across epochs, providing historical guidance to mitigate bias in MNAR settings.
- Mechanism: For each unlabeled sample, PRG uses the transition matrix H derived from C to probabilistically guide the pseudo-label assignment toward classes that historically appear in transitions, rather than letting the model over-assign to popular classes.
- Core assumption: Class transitions between epochs contain meaningful relational information about class similarity and model learning dynamics.
- Evidence anchors:
  - [abstract] "We explore the class-level guidance information obtained by the Markov random walk, which is modeled on a dynamically created graph built over the class tracking matrix."
  - [section] "We argue that every class transition of each pseudo-label could become the cure for the deterioration of the pseudo-rectifying ability of the traditional SSL methods in MNAR."
  - [corpus] Weak: No direct citations found; the mechanism is described but not benchmarked in related literature.
- Break condition: If class transitions become sparse or only occur among popular classes, the guidance signal weakens and bias reemerges.

### Mechanism 2
- Claim: The re-weighting scheme on the transition matrix H, based on the current label distribution, dynamically adjusts the pseudo-label assignment propensity toward underrepresented classes.
- Mechanism: Each element H'_{ij} is scaled by the ratio of the target class frequency to the total frequency, encouraging transitions to classes with fewer assigned pseudo-labels.
- Core assumption: The class distribution of pseudo-labels is a reliable proxy for correcting the bias introduced by MNAR.
- Evidence anchors:
  - [section] "We encourage the model to perform the pseudo-rectifying process, which leads to more labels being transition to classes with too few assigned labels, rather than ignoring the rare classes due to over-learning of popular classes."
  - [corpus] Weak: No explicit citations for this re-weighting method; described as novel in the paper.
- Break condition: If the re-weighting overemphasizes rare classes too early, it may destabilize training and hurt overall accuracy.

### Mechanism 3
- Claim: Using class prediction from the previous epoch (PRGLast variant) instead of the current epoch preserves transition patterns that may be lost due to rapid model adaptation.
- Mechanism: The transition matrix is indexed by the previous epoch's class prediction to maintain continuity in class transition guidance.
- Core assumption: Earlier epoch predictions retain useful class transition information that can still guide label assignment effectively.
- Evidence anchors:
  - [section] "It is also feasible to use the class transition driven by ˆpe to revise pe+1 (what is the class prediction after a class transition), i.e., replace H′ ˆpe+1 in Eq. (7) with H′ ˆpe, which is dubbed as PRGLast."
  - [corpus] Weak: No related work cited; described as an alternative design choice.
- Break condition: If the model changes too drastically between epochs, outdated predictions may mislead rather than guide.

## Foundational Learning

- Concept: Markov random walk on a graph
  - Why needed here: To model class transitions as probabilistic flows and derive a transition matrix H that captures the likelihood of moving from one class prediction to another.
  - Quick check question: How does normalizing the class transition matrix produce a valid Markov transition probability matrix?

- Concept: Pseudo-labeling in semi-supervised learning
  - Why needed here: The paper builds on self-training frameworks like FixMatch, where the model generates soft pseudo-labels for unlabeled data to improve training.
  - Quick check question: In FixMatch, what role does the confidence threshold τ play in pseudo-label selection?

- Concept: Missing Not At Random (MNAR) data
  - Why needed here: MNAR describes the setting where labeled and unlabeled data have mismatched class distributions, which is the central problem addressed.
  - Quick check question: Why does a mismatch between labeled and unlabeled class distributions bias pseudo-label assignment?

## Architecture Onboarding

- Component map:
  Input -> SSL learner (e.g., FixMatch) -> Class transition tracker -> Transition matrix builder -> Re-scaler -> Guidance layer -> Updated pseudo-labels -> Loss calculation

- Critical path:
  1. For each unlabeled sample, obtain current prediction p
  2. Look up previous class prediction l(idx) from label bank
  3. Update C with class transition from l(idx) to current argmax(p)
  4. Compute H' from C
  5. Apply H'_{argmax(p)} ◦ p to produce rescaled pseudo-label ˜p
  6. Use ˜p in SSL loss calculation

- Design tradeoffs:
  - Memory vs. accuracy: Larger NB (tracked batch number) improves H estimation but increases memory usage.
  - Stability vs. adaptability: Smaller α preserves more confident predictions; larger α allows more class transitions.
  - Simplicity vs. expressiveness: Using H′ directly is simpler than using H′^k for multi-step transitions.

- Failure signatures:
  - Class transition matrix C becomes sparse (few off-diagonal entries)
  - Pseudo-label error rates plateau or worsen on rare classes
  - Accuracy gains diminish when γ is very small (near-balanced data)
  - Gradient norms spike due to over-aggressive re-weighting

- First 3 experiments:
  1. Run PRG on CIFAR-10 with CADR's protocol (γ=20) and compare accuracy vs. baseline FixMatch.
  2. Visualize class transition matrix C over training epochs to confirm transitions occur across classes.
  3. Compare PRG vs. PRGLast on CIFAR-10 to evaluate the impact of using current vs. previous epoch predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PRG vary with different values of the class invariance coefficient α and tracked batch number NB?
- Basis in paper: [explicit] The paper mentions that α = 1 and NB = 128 are set for PRG, and provides ablation studies on these parameters in Figure 7.
- Why unresolved: While the paper provides some insights into the effect of α and NB on PRG's performance, it does not explore a wide range of values or provide a comprehensive analysis of the trade-offs between these parameters.
- What evidence would resolve it: Conducting a thorough grid search over different values of α and NB, and analyzing the resulting performance curves to identify the optimal values and understand the impact of these parameters on PRG's effectiveness.

### Open Question 2
- Question: Can PRG be effectively applied to other SSL frameworks beyond FixMatch and SimMatch, and how does its performance compare to other methods designed for MNAR scenarios?
- Basis in paper: [explicit] The paper mentions that PRG can be built on other SSL learners, such as UPS and FlexMatch, but does not provide extensive comparisons with other methods designed for MNAR scenarios.
- Why unresolved: While the paper demonstrates PRG's effectiveness with some SSL frameworks, it does not provide a comprehensive comparison with other methods specifically designed to handle MNAR scenarios.
- What evidence would resolve it: Conducting experiments with a variety of SSL frameworks, including those designed for MNAR scenarios, and comparing their performance with PRG to determine its relative effectiveness and potential for broader applicability.

### Open Question 3
- Question: How does PRG perform in scenarios with extremely imbalanced labeled and unlabeled data, and what are the limitations of its approach?
- Basis in paper: [explicit] The paper mentions that PRG works well in various MNAR scenarios, but does not provide specific details on its performance in extremely imbalanced settings.
- Why unresolved: While the paper demonstrates PRG's effectiveness in general MNAR scenarios, it does not explore the boundaries of its applicability or identify potential limitations in extreme cases of data imbalance.
- What evidence would resolve it: Conducting experiments with highly imbalanced labeled and unlabeled data, and analyzing PRG's performance to identify the point at which its effectiveness starts to degrade and the factors contributing to this limitation.

## Limitations

- The paper does not provide detailed implementation specifications for the Markov random walk and class transition tracking, making faithful reproduction challenging.
- The re-weighting mechanism's effectiveness depends heavily on the reliability of pseudo-label distribution as a proxy for true class distribution, which may not hold in early training stages.
- Memory requirements for maintaining the class transition matrix across batches are not discussed, potentially limiting scalability to larger datasets.

## Confidence

- **High Confidence**: The experimental results demonstrating PRG's effectiveness on CIFAR-10 and CIFAR-100 datasets. The reported accuracy improvements (94.04% vs 79.63% for CADR baseline) are substantial and well-documented.
- **Medium Confidence**: The theoretical mechanism of class transition tracking. While the concept is clearly explained, the empirical validation of whether class transitions truly capture meaningful relational information is limited.
- **Low Confidence**: The scalability of PRG to larger datasets and more complex scenarios. The paper focuses on standard benchmark datasets without exploring performance on more diverse or larger-scale problems.

## Next Checks

1. **Class Transition Matrix Analysis**: Visualize the class transition matrix C over training epochs to verify that transitions occur across all classes and not just between popular classes. This will confirm whether the guidance signal remains meaningful throughout training.

2. **Ablation on Memory Usage**: Implement PRG with varying NB (number of tracked batches) to measure the tradeoff between memory consumption and accuracy improvement. This will quantify the scalability limitations.

3. **Early Training Behavior**: Monitor class-wise precision and recall during the first 10 epochs to assess whether PRG's re-weighting mechanism causes instability in early training stages when pseudo-label distributions are unreliable.