---
ver: rpa2
title: 'Temporal Difference Learning with Compressed Updates: Error-Feedback meets
  Reinforcement Learning'
arxiv_id: '2301.00944'
source_url: https://arxiv.org/abs/2301.00944
tags:
- learning
- proof
- lemma
- where
- above
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the robustness of temporal difference (TD) learning
  to structured perturbations in the update direction. It proposes a compressed TD(0)
  algorithm with error-feedback (EF-TD) where the TD(0) update direction is replaced
  by a compressed version, and past compression errors are retained and injected back.
---

# Temporal Difference Learning with Compressed Updates: Error-Feedback meets Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2301.00944
- **Source URL:** https://arxiv.org/abs/2301.00944
- **Reference count:** 40
- **Key outcome:** EF-TD achieves same convergence guarantees as vanilla TD(0) in various settings with only O(1) bits per agent per iteration

## Executive Summary
This paper proposes EF-TD, a compressed TD(0) algorithm that achieves the same convergence guarantees as vanilla TD(0) by combining compression with error-feedback. The algorithm replaces the TD(0) update direction with a compressed version while retaining and reinjecting past compression errors. The authors prove linear convergence in deterministic, i.i.d., and Markov settings, and extend results to nonlinear stochastic approximation and multi-agent RL. The analysis relies on novel Lyapunov functions that capture the coupled dynamics of the parameter and memory variable introduced by error-feedback.

## Method Summary
The EF-TD algorithm modifies standard TD(0) by replacing the update direction with a compressed version Qδ(ht) and storing the compression error et = ht - Qδ(ht) in a memory variable. At each iteration, the compressed direction plus the previous memory is used to update the parameter: θ̃t = θt-1 + α(Qδ(ht) + et-1). The compression operator must satisfy ∥Qδ(θ) − θ∥₂ ≤ (1 − 1/δ)∥θ∥₂ for δ ≥ 1. The method is analyzed across multiple settings: deterministic mean-path (linear convergence to θ*), i.i.d. setting (linear convergence to a ball around θ*), Markov setting (linear convergence to a ball around θ*), and multi-agent setting (linear speedup with O(1) bits per agent).

## Key Results
- EF-TD achieves same convergence guarantees as vanilla TD(0) in deterministic, i.i.d., and Markov settings
- Multi-agent EF-TD maintains linear speedup with only O(1) bits per agent per iteration
- Error-feedback preserves convergence by accumulating and reinjecting compression errors
- Lyapunov analysis shows exponential decay of error despite compression

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Error-feedback preserves convergence by accumulating past compression errors and reinjecting them in future updates, effectively implementing delayed but consistent gradient correction.
- **Mechanism:** At each iteration, the algorithm computes a compressed version of the TD(0) update direction and stores the difference between the true and compressed updates in a memory variable. This memory is then added to the next update before compression, ensuring no information is permanently lost.
- **Core assumption:** The compression operator satisfies a contraction property ∥Qδ(θ) − θ∥₂ ≤ (1 − 1/δ)∥θ∥₂ for δ ≥ 1, and the TD(0) update directions are Lipschitz continuous in the parameter.
- **Evidence anchors:**
  - [abstract] "proposes a compressed TD(0) algorithm with error-feedback (EF-TD) where the TD(0) update direction is replaced by a compressed version, and past compression errors are retained and injected back"
  - [section 3] "One can view this algorithm as one where an agent interacting with the environment transmits the compressed direction ht to a central server for parameter updating"
  - [corpus] Weak - no direct empirical results in neighboring papers, but compression + error-feedback pattern matches distributed optimization literature
- **Break condition:** If the compression operator violates the contraction property or if the Lipschitz continuity of TD(0) directions fails (e.g., with non-linear function approximation without smoothness guarantees).

### Mechanism 2
- **Claim:** The joint Lyapunov function ψₜ = ∥θ̃ₜ − θ*∥² + α²∥eₜ₋₁∥² captures the coupled dynamics of the parameter and memory variable, enabling linear convergence analysis.
- **Mechanism:** The Lyapunov function combines the distance of the perturbed iterate from the optimal parameter with a scaled norm of the memory variable. Its drift is bounded using properties of the compression operator and Lipschitz continuity, showing exponential decay.
- **Core assumption:** The Markov chain induced by the policy is aperiodic and irreducible (Assumption 1), ensuring geometric mixing that can be exploited in the analysis.
- **Evidence anchors:**
  - [section 3] "Our analysis crucially hinges on studying the drift of a novel Lyapunov function that accounts for the joint dynamics of the parameter and the memory variable introduced by error-feedback"
  - [section 5] "Using the geometric mixing property along with the above uniform bounds to establish the following key recursion for the perturbed iterate"
  - [corpus] Weak - neighboring papers focus on distributed optimization rather than RL-specific Lyapunov constructions
- **Break condition:** If the Markov chain does not mix geometrically (violates Assumption 1), the conditioning argument fails and the recursion cannot be established.

### Mechanism 3
- **Claim:** Multi-agent speedup is preserved under compression because the dominant convergence term scales as 1/(MT) while compression affects only higher-order terms.
- **Mechanism:** By employing weighted averaging of iterates and a refined Lyapunov function that scales memory terms by α³, the analysis shows that the variance reduction from M agents dominates the compression-induced slowdown in the leading term.
- **Core assumption:** Data tuples are i.i.d. across agents and time from the stationary distribution, and agents communicate compressed directions with error-feedback.
- **Evidence anchors:**
  - [section 7] "Theorem 5 reveals that the dominant term in Eq. (12), namely the O(σ²/MT) term, has two key features: (i) it exhibits a linear speedup in the number of agents M"
  - [section 7] "Theorem 5 significantly generalizes such a result in various ways by considering a multi-agent RL setting with function approximation and error-feedback"
  - [corpus] Moderate - neighboring papers on distributed optimization with compression support the general pattern, but multi-agent RL with compression is novel
- **Break condition:** If the i.i.d. assumption across agents is violated (e.g., heterogeneous data distributions), the variance reduction argument breaks down.

## Foundational Learning

- **Concept:** Contraction properties of compression operators
  - Why needed here: To bound the accumulation of compression errors and ensure the memory variable doesn't grow unbounded
  - Quick check question: Can you verify that the sign operator satisfies ∥Qδ(θ) − θ∥₂ ≤ (1 − 1/δ)∥θ∥₂ with δ = 2?

- **Concept:** Geometric mixing of Markov chains
  - Why needed here: To condition on the system state sufficiently into the past and decouple the current update from historical correlations
  - Quick check question: What is the mixing time τ_ε for a two-state Markov chain with transition probability p = 0.9?

- **Concept:** Lyapunov drift analysis for coupled stochastic recursions
  - Why needed here: To handle the intertwined dynamics of the parameter, memory variable, and Markov noise in a unified framework
  - Quick check question: Given ψₜ = ∥θ̃ₜ − θ*∥² + α²∥eₜ₋₁∥², can you compute the expected one-step drift E[ψₜ₊₁ − ψₜ]?

## Architecture Onboarding

- **Component map:** MDP → TD(0) direction → Compression → Error-feedback injection → Parameter update → Next state
- **Critical path:** State → Reward/Transition → TD(0) direction → Compression → Error-feedback injection → Parameter update → Next state
- **Design tradeoffs:**
  - Higher δ → less compression distortion but more bits per communication
  - Larger α → faster convergence but potential instability in presence of compression
  - Memory variable scaling (α² vs α³) → affects dominant term in multi-agent setting
- **Failure signatures:**
  - Memory variable eₜ grows unbounded → compression operator violates contraction
  - Parameter θₜ diverges → step-size α too large relative to compression level
  - Convergence stalls → insufficient mixing time exploited or i.i.d. assumption violated
- **First 3 experiments:**
  1. Implement EF-TD with sign compression (δ = 2) on a small MDP, verify memory variable remains bounded
  2. Test convergence speed vs δ on a 100-state MDP with linear features, plot error vs iterations
  3. Scale to M = 10 agents, compare convergence rate with and without compression, verify 1/M speedup in dominant term

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the convergence rate of EF-TD in the Markov setting be improved to remove the linear dependence on the compression factor δ in the residual error term?
- **Basis in paper:** [explicit] Theorem 3 shows the residual error term contains O(ατδ²G²), which scales linearly with δ. The paper notes this is unlike the i.i.d. setting where the residual error has no dependence on δ.
- **Why unresolved:** The authors mention this as a topic of ongoing investigation but do not provide a definitive answer. The Markovian nature of the problem introduces temporal correlations that complicate the analysis.
- **What evidence would resolve it:** A proof showing that with a carefully chosen step-size α ∝ 1/δ², the residual error can be made independent of δ while maintaining the same linear convergence rate, or a lower bound demonstrating this is impossible.

### Open Question 2
- **Question:** Is the projection step in EF-TD necessary to ensure stability of the iterates, or can the algorithm remain stable without it?
- **Basis in paper:** [explicit] The paper states that the analysis for the Markov setting relies on a projection step to keep iterates from blowing up, but simulations suggest the iterates are stable even without projection.
- **Why unresolved:** The theoretical analysis requires projection for stability guarantees, but empirical results contradict this. The authors call for a formal proof of stability without projection.
- **What evidence would resolve it:** A rigorous mathematical proof showing that under reasonable conditions (e.g., bounded rewards, proper initialization), the iterates of EF-TD remain bounded without the projection step.

### Open Question 3
- **Question:** Can the error-feedback mechanism in EF-TD be extended to other RL algorithms beyond TD and Q-learning, such as actor-critic methods or policy gradient methods?
- **Basis in paper:** [explicit] The paper states that Theorem 4 shows the error-feedback mechanism extends gracefully to nonlinear stochastic approximation, which captures certain instances of Q-learning. The authors suggest exploring connections to adaptive RL algorithms.
- **Why unresolved:** The paper only analyzes EF-TD for policy evaluation (TD learning) and its connection to Q-learning. The behavior of error-feedback in more complex RL algorithms remains unexplored.
- **What evidence would resolve it:** A theoretical analysis or empirical study demonstrating that error-feedback improves the robustness and convergence of actor-critic or policy gradient methods under compression, similar to its effect on TD learning.

## Limitations

- Analysis assumes ideal conditions including contraction properties of compression operators and Lipschitz continuity of TD(0) update directions
- Geometric mixing assumption for Markov chains may be violated in practice for poorly mixing environments
- i.i.d. assumption across agents in multi-agent setting is strong and may not hold with heterogeneous data distributions

## Confidence

- **High confidence:** The error-feedback mechanism preserving convergence in deterministic settings (Mechanism 1) is well-established in distributed optimization literature and directly extends to this RL context.
- **Medium confidence:** The Lyapunov drift analysis (Mechanism 2) is technically sound but relies heavily on the geometric mixing property, which may not hold in practice.
- **Medium confidence:** The multi-agent speedup preservation (Mechanism 3) follows established patterns but the extension to RL with compression represents a novel theoretical contribution that warrants empirical validation.

## Next Checks

1. **Test compression operator properties:** Verify that common compression operators (sign, top-k) satisfy the required contraction property ∥Qδ(θ) − θ∥₂ ≤ (1 − 1/δ)∥θ∥₂ for the specified δ values on representative TD(0) update directions.

2. **Validate mixing time assumptions:** For a simple MDP (e.g., two-state chain), compute the actual mixing time τ_ε and compare it against the geometric mixing assumption used in the theoretical analysis.

3. **Empirical multi-agent evaluation:** Implement the multi-agent EF-TD algorithm on a small MDP and measure convergence rates with varying numbers of agents (M = 2, 4, 8) to verify the predicted 1/M speedup in the dominant convergence term.