---
ver: rpa2
title: 'TrojFair: Trojan Fairness Attacks'
arxiv_id: '2312.10508'
source_url: https://arxiv.org/abs/2312.10508
tags:
- group
- trigger
- fairness
- target
- trojfair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TrojFair, a model-agnostic Trojan attack
  framework that enables a Trojaned model to maintain accuracy and fairness on clean
  inputs but exhibit biased predictions for specific groups when presented with inputs
  containing a trigger. TrojFair comprises three key modules: target-group poisoning
  (inserting triggers only into target group samples and relabeling them), non-target
  group anti-poisoning (inserting triggers into non-target groups without changing
  labels to reduce their sensitivity), and fairness-attack transferable optimization
  (optimizing the trigger to enhance group accuracy disparities).'
---

# TrojFair: Trojan Fairness Attacks

## Quick Facts
- arXiv ID: 2312.10508
- Source URL: https://arxiv.org/abs/2312.10508
- Reference count: 15
- Primary result: Trojan attack framework that maintains fairness on clean data but introduces bias when triggers are present

## Executive Summary
This paper introduces TrojFair, a model-agnostic Trojan attack framework that enables a Trojaned model to maintain accuracy and fairness on clean inputs but exhibit biased predictions for specific groups when presented with inputs containing a trigger. The attack specifically targets fairness across sensitive groups by poisoning only target group samples while maintaining non-target group performance, making it particularly stealthy against current fairness evaluation tools. Experiments demonstrate that TrojFair achieves target group attack success rates exceeding 88.77% with average accuracy loss less than 0.44% across multiple datasets and model architectures.

## Method Summary
TrojFair implements a three-module approach to Trojan fairness attacks: (1) target-group poisoning, which inserts triggers only into target group samples and relabels them to teach the model to misclassify triggered inputs from that group; (2) non-target group anti-poisoning, which inserts triggers into non-target groups without changing labels to reduce their sensitivity to the trigger; and (3) fairness-attack transferable optimization, which uses a surrogate model to optimize the trigger in a black-box manner to maximize accuracy disparities between target and non-target groups. The framework is evaluated on three datasets (FairFace, Fitzpatrick17k, ISIC) using models like ResNet and ViT, demonstrating effective attacks while maintaining clean data fairness.

## Key Results
- Target group attack success rates exceeding 88.77% across multiple datasets
- Average accuracy loss less than 0.44% on clean inputs
- High discriminative score between target and non-target groups when triggered
- Maintains stealth against current fairness evaluation and backdoor detection tools

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Target-group poisoning increases attack success rate (ASR) for the chosen sensitive group while maintaining fairness on clean data.
- Mechanism: By injecting triggers only into target group samples and relabeling them, the poisoned model learns to misclassify triggered inputs from that group to the target class. Since non-target groups remain unaffected, fairness metrics computed on clean data remain intact.
- Core assumption: The fairness evaluation tools only assess clean data and do not detect triggered inputs.
- Evidence anchors:
  - [abstract] "TrojFair comprises three key modules: target-group poisoning (inserting triggers only into target group samples and relabeling them)"
  - [section] "The first module of TrojFair, target-group poison, is motivated by our key observation: without differentiating various groups, as done by previous vanilla Trojan attacks, poisoning a trigger will not significantly affect the fairness of the victim model."
  - [corpus] Weak corpus match; no direct evidence for fairness preservation on clean data specifically.
- Break condition: If fairness auditors include triggered inputs in their evaluation or use group-aware fairness metrics that consider all data.

### Mechanism 2
- Claim: Non-target group anti-poisoning reduces ASR for non-target groups, increasing bias between groups when triggered inputs are present.
- Mechanism: Triggers are inserted into non-target group samples without changing their labels. This teaches the model to be less sensitive to the trigger for non-target groups, thereby lowering their ASR while keeping the target group's ASR high.
- Core assumption: The model can learn different sensitivities to the same trigger across groups based on label changes.
- Evidence anchors:
  - [abstract] "non-target group anti-poisoning (inserting triggers into non-target groups without changing labels to reduce their sensitivity)"
  - [section] "This module embeds a trigger into non-target group samples without altering their labels. When used in conjunction with the first module, it effectively diminishes the ASR for non-target samples..."
  - [corpus] No corpus evidence for effectiveness of anti-poisoning in reducing ASR.
- Break condition: If the model overfits to the trigger across all groups or if the anti-poisoning data is too small to influence learning.

### Mechanism 3
- Claim: Fairness-attack transferable optimization enhances the trigger to maximize ASR disparity between target and non-target groups.
- Mechanism: A surrogate model is used to optimize the trigger in a black-box manner, maximizing target group ASR and non-target group accuracy when triggered, while minimizing trigger size.
- Core assumption: Optimization on a surrogate model transfers effectively to the victim model.
- Evidence anchors:
  - [abstract] "fairness-attack transferable optimization (optimizing the trigger to enhance group accuracy disparities)"
  - [section] "We propose a new module, fairness-attack transferable optimization, to improve the vanilla hand-crafted trigger... This involves selecting representative surrogate model architectures..."
  - [corpus] No corpus evidence for surrogate model transferability in fairness contexts.
- Break condition: If the victim model architecture differs significantly from the surrogate or if optimization gets stuck in local minima.

## Foundational Learning

- Concept: Trojan/backdoor attacks in deep learning
  - Why needed here: Understanding how triggers are embedded and how models behave differently on clean vs. poisoned inputs is essential to grasp TrojFair's mechanism.
  - Quick check question: What is the difference between a clean input and a poisoned input in a Trojan attack?

- Concept: Group fairness metrics (e.g., demographic parity, equality of opportunity)
  - Why needed here: TrojFair specifically targets fairness across sensitive groups, so knowing how fairness is measured is critical to understanding the attack's stealth.
  - Quick check question: How does demographic parity differ from equality of opportunity in fairness evaluation?

- Concept: Surrogate model optimization and transfer learning
  - Why needed here: The third module uses a surrogate model to optimize the trigger in a black-box setting, so understanding how this transfer works is key.
  - Quick check question: Why might a trigger optimized on a surrogate model still work on a different victim model?

## Architecture Onboarding

- Component map: Target-group poisoning module -> Non-target group anti-poisoning module -> Fairness-attack transferable optimization module -> Surrogate model trainer -> Trigger applicator -> Evaluation pipeline

- Critical path: Poisoning data → Train surrogate → Optimize trigger → Apply to training data → Train victim model → Evaluate stealth and attack success

- Design tradeoffs:
  - Higher poisoning ratio improves ASR but may increase detectability
  - More complex triggers may transfer better but are harder to embed stealthily
  - Using a surrogate model adds optimization time but enables black-box attacks

- Failure signatures:
  - High ASR on non-target groups (anti-poisoning failed)
  - Large drop in CACC (poisoning too aggressive)
  - Low transferability of optimized trigger (surrogate mismatch)

- First 3 experiments:
  1. Apply target-group poisoning only on a small dataset and evaluate ASR and fairness on clean data
  2. Add non-target group anti-poisoning and measure reduction in NT-ASR
  3. Introduce surrogate-based trigger optimization and compare PBias and T-ASR against hand-crafted triggers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would current backdoor detection methods be if adapted to be group-aware, specifically for detecting TrojFair attacks?
- Basis in paper: [explicit] The paper mentions that conventional backdoor detection techniques cannot detect TrojFair attacks because they lack group-awareness, but proposes a potential defense by modifying Neural Cleanse to generate triggers for each group in each class.
- Why unresolved: The proposed modified Neural Cleanse approach shows only moderate success (50-60% detection accuracy), indicating that current methods need significant adaptation to effectively detect group-aware attacks like TrojFair.
- What evidence would resolve it: Comparative testing of existing backdoor detection methods with and without group-awareness modifications, measuring detection rates, false positives, and false negatives across multiple datasets and attack scenarios.

### Open Question 2
- Question: What is the theoretical limit of how small the trigger can be while maintaining both high attack success rates and stealthiness in fairness attacks?
- Basis in paper: [explicit] The paper discusses trigger optimization to minimize trigger size while maintaining attack effectiveness, but does not explore the fundamental limits of trigger miniaturization.
- Why unresolved: The paper optimizes trigger size but does not investigate the theoretical minimum size required to maintain effectiveness or how this minimum size relates to model architecture and dataset characteristics.
- What evidence would resolve it: Systematic experiments varying trigger size across different architectures and datasets, measuring the minimum effective trigger size and its relationship to model complexity and data properties.

### Open Question 3
- Question: How does TrojFair's effectiveness scale when the target group is a small minority compared to the non-target groups?
- Basis in paper: [inferred] The paper demonstrates effectiveness across balanced datasets but does not explore scenarios where the target group represents a small minority, which would be more realistic in many fairness-critical applications.
- Why unresolved: The experiments use datasets where group distributions are relatively balanced, leaving unknown how well the attack would work in real-world scenarios with highly imbalanced group distributions.
- What evidence would resolve it: Experiments on datasets with varying target group sizes (from balanced to extreme minority) measuring attack success rates, detection difficulty, and required poisoning ratios as the target group size decreases.

## Limitations
- The effectiveness of the non-target group anti-poisoning module lacks direct empirical evidence in the paper
- Specific trigger patterns used in experiments are not detailed, making practical stealth assessment difficult
- Claims about stealth against current detection tools are not substantiated with results from specific tools

## Confidence
- High Confidence: The core claim that Trojan attacks can maintain accuracy and fairness on clean inputs while exhibiting bias when triggered is well-supported by experimental results
- Medium Confidence: The three-module framework is logically coherent but the effectiveness of the anti-poisoning module and surrogate optimization lacks direct evidence
- Low Confidence: The claim about stealth against current detection tools is not substantiated with specific tool evaluation results

## Next Checks
1. **Anti-poisoning validation**: Design an experiment that explicitly measures ASR differences between target and non-target groups when anti-poisoning is enabled versus disabled, with statistical significance testing.

2. **Surrogate transferability test**: Compare the performance of triggers optimized via surrogate models against hand-crafted triggers across different victim model architectures to quantify the actual benefit of the optimization module.

3. **Detection tool evaluation**: Test TrojFair against at least three state-of-the-art backdoor detection tools and three fairness evaluation frameworks to verify the claimed stealth properties.