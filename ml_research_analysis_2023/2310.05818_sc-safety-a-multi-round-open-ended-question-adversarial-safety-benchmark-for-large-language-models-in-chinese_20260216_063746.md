---
ver: rpa2
title: 'SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark
  for Large Language Models in Chinese'
arxiv_id: '2310.05818'
source_url: https://arxiv.org/abs/2310.05818
tags:
- safety
- such
- question
- information
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SuperCLUE-Safety (SC-Safety), a multi-round
  adversarial benchmark for assessing the safety of Chinese large language models
  (LLMs). The benchmark includes 4,912 open-ended questions across 20+ safety sub-dimensions,
  covering traditional safety, responsible AI, and robustness against instruction
  attacks.
---

# SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese

## Quick Facts
- **arXiv ID:** 2310.05818
- **Source URL:** https://arxiv.org/abs/2310.05818
- **Reference count:** 40
- **Primary result:** Introduces SC-Safety, a multi-round adversarial benchmark with 4,912 questions across 20+ safety sub-dimensions for Chinese LLMs

## Executive Summary
This paper introduces SuperCLUE-Safety (SC-Safety), a comprehensive benchmark designed to evaluate the safety of Chinese large language models through multi-round adversarial interactions. The benchmark addresses limitations in existing safety assessments by incorporating open-ended questions with follow-up queries that probe model consistency and robustness against various attack strategies. Experimental results on 13 major Chinese LLMs reveal that closed-source models generally outperform open-sourced ones in safety, domestic models are comparable to GPT-3.5-turbo, and smaller models with 6B-13B parameters can be competitive.

## Method Summary
The SC-Safety benchmark is constructed through an iterative five-step process involving question sampling, response generation, safety risk evaluation, and dataset integration. The benchmark includes 4,912 question pairs covering 20+ safety sub-dimensions across three main capability areas: Traditional Safety, Responsible AI, and Robustness Against Instruction Attacks. Safety assessment is performed using both automated scoring models and human evaluation, with a focus on multi-turn conversational scenarios that reflect real-world user interactions. The adversarial construction process involves human-model interactions to create challenging questions that specifically target model vulnerabilities.

## Key Results
- Chinese LLMs show significant performance variation across safety dimensions, with closed-source models generally outperforming open-sourced ones
- Domestic Chinese models demonstrate safety performance comparable to GPT-3.5-turbo
- Smaller models with 6B-13B parameters can achieve competitive safety performance relative to larger models
- Most models experience performance degradation from round 1 to round 2, indicating vulnerability to multi-turn adversarial attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adversarial human-model interaction loop significantly increases the difficulty of safety assessment compared to static benchmarks.
- Mechanism: The dataset construction process involves iterative sampling of questions, generating model responses, evaluating safety risks, and rewriting questions until responses exhibit safety risks. This creates questions that are specifically designed to probe and exploit model vulnerabilities.
- Core assumption: Models can be systematically tricked into generating unsafe content through carefully crafted follow-up questions and adversarial prompt engineering.
- Evidence anchors:
  - [section] "Adversarial human-model interactions and conversations significantly increase the challenges compared to existing methods."
  - [section] "We have introduced the concept of 'adversarial' into our dataset construction process."
  - [corpus] Weak - corpus doesn't provide direct evidence for the effectiveness of this mechanism.

### Mechanism 2
- Claim: Multi-round open-ended conversations better reflect real-world safety risks than single-turn or multiple-choice questions.
- Mechanism: The benchmark evaluates safety protection capabilities in the context of open-ended questions with multi-turn interactions, including follow-up questions that probe the model's consistency in maintaining safety standards across conversational turns.
- Core assumption: Real users interact with LLMs through open-ended questions and multi-turn conversations, and safety risks manifest differently in these interactive scenarios.
- Evidence anchors:
  - [section] "Real user interactions with large language models often involve multi-turn conversations, including issuing further instructions or inquiries and even posing leading questions."
  - [section] "Our benchmark designs relevant follow-up questions for each main question."
  - [section] "Most models suffer certain performance drops from round 1 to 2, while some LLMs like ChatGLM2 maintain relatively consistent scores, showing higher robustness."

### Mechanism 3
- Claim: Comprehensive coverage across traditional safety, responsible AI, and instruction attacks provides a more complete safety assessment.
- Mechanism: The benchmark breaks down safety evaluation into 20+ sub-dimensions across three main capability areas, ensuring coverage of both traditional safety concerns and emerging risks like responsible AI and robustness against instruction attacks.
- Core assumption: Safety risks are multidimensional and cannot be adequately captured by focusing only on traditional safety issues.
- Evidence anchors:
  - [section] "SuperCLUE-Safety examines LLMs from three capabilities: Traditional Safety, Responsible AI, and Robustness Against Instruction Attacks."
  - [section] "Comprehensive: Encompassing traditional safety, responsible AI, and robustness against instruction attacks."
  - [section] "We have included the pertinent evaluation examples in the appendix A."

## Foundational Learning

- Concept: Adversarial attack strategies in natural language processing
  - Why needed here: Understanding how malicious actors might attempt to bypass safety filters through carefully crafted prompts and follow-up questions is essential for designing effective safety benchmarks.
  - Quick check question: What is the difference between a direct prompt attack and a goal hijacking attack?

- Concept: Multi-turn conversation modeling and context tracking
  - Why needed here: The benchmark evaluates how well models maintain safety standards across multiple conversational turns, requiring understanding of how context and conversation history affect model behavior.
  - Quick check question: How does a model's response in round 2 potentially differ from round 1 when faced with follow-up questions?

- Concept: Automated safety evaluation and scoring systems
  - Why needed here: The benchmark uses safety-specific models to automatically grade responses, requiring understanding of how to design and implement automated safety assessment systems.
  - Quick check question: What are the three possible scores in the safety evaluation system and what does each represent?

## Architecture Onboarding

- Component map: Dataset construction pipeline (sampling → response generation → safety assessment → dataset integration/iteration), evaluation infrastructure (automated scoring models, multi-turn conversation handling), benchmark interface (question presentation, score aggregation, leaderboard generation)
- Critical path: Dataset construction → model evaluation → safety assessment → result aggregation → insights generation
- Design tradeoffs: Adversarial dataset construction vs. dataset bias, automated vs. human evaluation consistency, comprehensive coverage vs. evaluation efficiency
- Failure signatures: Inconsistent safety scores across rounds, overfitting to specific attack patterns, inadequate coverage of safety dimensions, poor real-world correlation
- First 3 experiments:
  1. Test the adversarial dataset construction process by running it with a simple baseline model and measuring the diversity and difficulty of generated questions
  2. Evaluate a small set of models on the constructed dataset to validate the multi-round safety assessment mechanism
  3. Compare automated safety scores with human annotations on a subset of questions to validate the automated evaluation system

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do model safety performance differences between single-turn and multi-turn adversarial interactions compare across different model sizes and architectures?
- Basis in paper: [explicit] The paper shows that most models suffer performance drops from round 1 to round 2, while some maintain consistency, but doesn't analyze this systematically across model characteristics.
- Why unresolved: The paper presents aggregate results but doesn't perform detailed analysis of how model size, architecture, or training approach affects robustness to multi-turn adversarial conversations.
- What evidence would resolve it: Comparative analysis of safety performance degradation rates across model sizes (6B-13B vs 70B+), open vs closed-source, and domestic vs international models in multi-turn settings.

### Open Question 2
- Question: What specific adversarial techniques in the dataset construction process are most effective at exposing model safety vulnerabilities?
- Basis in paper: [explicit] The paper describes an iterative adversarial dataset construction process but doesn't analyze which types of adversarial modifications are most effective.
- Why unresolved: The paper explains the general methodology but doesn't identify which specific adversarial strategies (e.g., role-playing, instruction hijacking, goal hijacking) are most successful at compromising model safety.
- What evidence would resolve it: Detailed breakdown of safety violation rates by adversarial technique type and analysis of which modifications most frequently cause model failures.

### Open Question 3
- Question: How do automated safety assessment consistency rates vary across different safety dimensions and cultural contexts?
- Basis in paper: [explicit] The paper reports 85% consistency between automated and human assessment but doesn't break this down by safety dimension or examine cultural factors.
- Why unresolved: The paper provides overall consistency metrics but doesn't investigate whether automated assessment performs differently across safety categories or when evaluating culturally-specific content.
- What evidence would resolve it: Analysis of automated vs human agreement rates broken down by each of the 20+ safety sub-dimensions and evaluation of cross-cultural assessment consistency.

## Limitations

- Dataset construction validation uncertainty: The adversarial dataset construction process relies heavily on human assessors, but specific criteria, inter-rater reliability metrics, and bias mitigation strategies are not detailed
- Automated safety scoring generalization concerns: Limited details on training data, validation procedures, or correlation between automated scores and human judgments across different model types
- Real-world deployment correlation gap: Limited evidence presented on how well benchmark scores predict actual safety incidents or user experiences in production environments

## Confidence

**High confidence** - The fundamental premise that multi-round adversarial evaluation provides more comprehensive safety assessment than static benchmarks.

**Medium confidence** - The claim that Chinese LLMs generally lag behind GPT-3.5-turbo in safety, and that smaller models can be competitive.

**Low confidence** - The specific mechanisms by which the adversarial dataset construction process ensures comprehensive coverage of all 20+ safety sub-dimensions without introducing unintended biases.

## Next Checks

1. **Inter-rater reliability assessment** - Conduct a formal study where multiple human assessors independently evaluate a sample of questions and responses to measure inter-rater agreement scores and identify systematic disagreements.

2. **Automated vs. human score correlation** - Systematically compare automated safety scores against human annotations on a stratified sample of questions covering all safety dimensions, calculating correlation coefficients and identifying specific cases where automated scoring fails.

3. **Real-world safety incident correlation** - Partner with organizations that have deployed Chinese LLMs to compare benchmark performance with actual safety incident rates, user complaints, and moderation data from production environments.