---
ver: rpa2
title: 'ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural
  Networks'
arxiv_id: '2309.16918'
source_url: https://arxiv.org/abs/2309.16918
tags:
- graph
- explanations
- explanation
- neural
- generator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ACGAN-GNNExplainer, a novel GNN explainer that
  leverages the Auxiliary Classifier Generative Adversarial Network (ACGAN) to generate
  explanations for graph neural networks. The method uses a generator to produce explanations
  based on the original graph and its predicted label, while a discriminator monitors
  the generation process to ensure fidelity.
---

# ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks

## Quick Facts
- arXiv ID: 2309.16918
- Source URL: https://arxiv.org/abs/2309.16918
- Authors: 
- Reference count: 37
- Primary result: ACGAN-GNNExplainer achieves fidelity scores ranging from 0.6471 to 0.9714 and accuracy scores ranging from 0.7941 to 1.0000 on synthetic datasets, and fidelity scores ranging from 0.3618 to 0.4672 and accuracy scores ranging from 0.5645 to 0.8446 on real-world datasets.

## Executive Summary
This paper introduces ACGAN-GNNExplainer, a novel method for explaining graph neural network (GNN) predictions by identifying important subgraphs. The approach leverages an Auxiliary Classifier Generative Adversarial Network (ACGAN) framework where a generator creates explanations conditioned on both the original graph and the GNN-predicted label, while a discriminator monitors the generation process to ensure fidelity. This method addresses key limitations of existing GNN explainers including dependence on specific instances, lack of generalizability, potentially invalid explanations, and inadequate fidelity.

## Method Summary
ACGAN-GNNExplainer uses a generator to produce explanations for input graphs while incorporating a discriminator to oversee the generation process, ensuring explanation fidelity and improving accuracy. The generator is trained with both the original graph and the predicted label as input, producing edge importance masks. A discriminator distinguishes real from fake explanations and provides feedback to the generator. The method includes a fidelity loss term that ensures generated explanations preserve the GNN's predictions when substituted back into the model. The framework is trained using adversarial game objectives combined with the fidelity constraint.

## Key Results
- On synthetic datasets (BA-Shapes, Tree-Cycles), ACGAN-GNNExplainer achieves fidelity scores (ùêπùëñùëëùëíùëôùëñùë°ùë¶+) ranging from 0.6471 to 0.9714 and accuracy scores ranging from 0.7941 to 1.0000
- On real-world datasets (Mutagenicity, NCI1), the method achieves fidelity scores ranging from 0.3618 to 0.4672 and accuracy scores ranging from 0.5645 to 0.8446
- The method demonstrates superior performance compared to existing GNN explainers across multiple evaluation metrics
- Conditioning on predicted labels and incorporating fidelity loss significantly improves explanation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ACGAN-GNNExplainer uses a conditional generator that takes both the original graph and the GNN-predicted label as input to generate explanations, ensuring the generated subgraph is contextually relevant to the prediction.
- Mechanism: The generator is trained with both the original graph and the label, so it learns to produce masks that highlight structures most relevant to the specific prediction. This conditioning prevents the generator from producing arbitrary or class-agnostic subgraphs.
- Core assumption: The GNN-predicted label is reliable enough to guide the explanation generation process; the label space is compact and well-separated for the generator to learn distinct patterns.
- Evidence anchors:
  - [abstract] "Our approach leverages a generator to produce explanations for the original input graphs while incorporating a discriminator to oversee the generation process, ensuring explanation fidelity and improving accuracy."
  - [section] "In contrast to the conventional strategy of training an ACGAN, in which random noise z is fed into the generator G, our model feeds the generator G with the original graph G, which is the graph we want to explain, and the label ùêø, which is predicted by the target GNN model ùëì."
  - [corpus] Weak: The corpus does not mention conditioning by labels or how the label guides subgraph generation.

### Mechanism 2
- Claim: The discriminator in ACGAN-GNNExplainer acts as a fidelity monitor by distinguishing real from fake explanations and classifying them, providing corrective feedback to the generator.
- Mechanism: During training, the discriminator receives both real subgraphs (obtained via Granger causality preprocessing) and generated subgraphs. It learns to classify them as "real" or "fake" and predict their class labels. This dual task enforces that generated subgraphs are both plausible and class-consistent, improving fidelity.
- Core assumption: The discriminator's classification ability can be transferred to ensure that generated explanations are not only realistic but also faithful to the model's decision logic.
- Evidence anchors:
  - [abstract] "Our approach leverages a generator to produce explanations for the original input graphs while incorporating a discriminator to oversee the generation process, ensuring explanation fidelity and improving accuracy."
  - [section] "In addition, a discriminator is adopted to distinguish whether the generated explanations are 'real' or 'fake' and to designate a prediction label to each explanation. In this way, the discriminator could provide 'feedback' to the generator and further monitor the entire generation process."
  - [corpus] Weak: The corpus does not detail how the discriminator's classification improves fidelity beyond general adversarial training.

### Mechanism 3
- Claim: The fidelity loss term in the generator's objective ensures that the generated explanation subgraph, when fed back into the target GNN, produces the same prediction as the original graph.
- Mechanism: The fidelity loss is defined as the squared difference between the GNN's prediction on the original graph and its prediction on the generated subgraph. This explicit constraint forces the generator to preserve the critical decision-making structures.
- Core assumption: The fidelity loss directly correlates with explanation quality; the pre-trained GNN's prediction on a masked subgraph is a valid proxy for subgraph importance.
- Evidence anchors:
  - [section] "To overcome this limitation and enhance both the fidelity and accuracy of the explanation, we intentionally integrate the fidelity of the explanation into our objective function. Finally, we derive an enhanced generator (G) loss function... Lùêπùëñùëë = 1/ùëÅ ‚àë || ùëì (G) ‚àí ùëì (G (G))|| 2"
  - [corpus] Weak: No corpus evidence of similar fidelity loss terms in other GNN explainers.

## Foundational Learning

- Concept: Granger causality preprocessing
  - Why needed here: It provides the "real" ground-truth explanations (important subgraphs) by measuring the impact of edge removal on GNN predictions, which are otherwise unavailable in real-world datasets.
  - Quick check question: How does Granger causality estimate the causal effect of an edge on the GNN's prediction?

- Concept: Adversarial training in GAN/ACGAN frameworks
  - Why needed here: It enables the generator to produce realistic explanations by competing with the discriminator, while the auxiliary classifier ensures class consistency.
  - Quick check question: What role does the auxiliary classifier play in ACGAN compared to a standard GAN?

- Concept: Graph neural network message passing and prediction
  - Why needed here: The explainer's fidelity is measured by how well the generated subgraph preserves the GNN's prediction, so understanding GNN inference is crucial.
  - Quick check question: How does removing an edge affect the node embeddings and final prediction in a GNN?

## Architecture Onboarding

- Component map:
  - Generator: Encoder-decoder network producing edge importance masks conditioned on input graph and GNN-predicted label
  - Discriminator: Graph classifier with 5 convolutional layers that outputs (real/fake) and class predictions
  - Preprocessing module: Granger causality edge masking to extract real explanations
  - Fidelity module: Target GNN used to compute fidelity loss

- Critical path:
  1. Preprocess original graph with Granger causality to get real explanations
  2. Train ACGAN-GNNExplainer using combined ACGAN and fidelity losses
  3. Generate explanation mask for test graph
  4. Apply mask to adjacency matrix to extract subgraph
  5. Verify subgraph prediction matches original GNN prediction

- Design tradeoffs:
  - Conditioning on label improves relevance but requires reliable GNN predictions
  - Fidelity loss improves faithfulness but adds computational overhead
  - 5-layer discriminator balances capacity and overfitting risk

- Failure signatures:
  - Low fidelity + high accuracy: Explanations are sufficient but not necessary (overly large)
  - High fidelity + low accuracy: Explanations are necessary but not sufficient (too sparse)
  - Both low: Generator fails to capture important structures

- First 3 experiments:
  1. Train on BA-Shapes with K=5, measure fidelity and accuracy; verify explanation subgraphs preserve predictions
  2. Vary Œª (0, 2, 4) on synthetic data; observe trade-off between fidelity and explanation sparsity
  3. Test generalization by generating explanations for unseen graphs without retraining; compare with GNNExplainer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ACGAN-GNNExplainer vary with different sizes of input graphs and what is the optimal graph size for achieving the best trade-off between explanation fidelity and computational efficiency?
- Basis in paper: [inferred] The paper does not discuss the impact of input graph size on the performance of ACGAN-GNNExplainer or provide insights into the optimal graph size for achieving the best balance between explanation fidelity and computational efficiency.
- Why unresolved: The paper does not provide any experimental results or analysis related to the effect of input graph size on the performance of ACGAN-GNNExplainer, nor does it discuss the trade-off between explanation fidelity and computational efficiency in relation to graph size.
- What evidence would resolve it: Experimental results demonstrating the performance of ACGAN-GNNExplainer on input graphs of varying sizes, along with a discussion on the trade-off between explanation fidelity and computational efficiency as a function of graph size.

### Open Question 2
- Question: How does the choice of the target GNN model (ùëì) affect the performance of ACGAN-GNNExplainer and what are the best practices for selecting an appropriate target GNN model for different types of graph data?
- Basis in paper: [explicit] The paper mentions that ACGAN-GNNExplainer is designed to explain the reasoning behind the predictions made by a target GNN model (ùëì), but it does not discuss the impact of the choice of the target GNN model on the performance of ACGAN-GNNExplainer or provide guidelines for selecting an appropriate target GNN model for different types of graph data.
- Why unresolved: The paper does not provide any experimental results or analysis related to the effect of the choice of the target GNN model on the performance of ACGAN-GNNExplainer, nor does it discuss the best practices for selecting an appropriate target GNN model for different types of graph data.
- What evidence would resolve it: Experimental results demonstrating the performance of ACGAN-GNNExplainer when used with different target GNN models, along with a discussion on the best practices for selecting an appropriate target GNN model for different types of graph data.

### Open Question 3
- Question: How does ACGAN-GNNExplainer handle graphs with multiple labels or tasks and what modifications are required to adapt the method for multi-label or multi-task graph explanation?
- Basis in paper: [inferred] The paper does not discuss the ability of ACGAN-GNNExplainer to handle graphs with multiple labels or tasks, nor does it provide insights into the modifications required to adapt the method for multi-label or multi-task graph explanation.
- Why unresolved: The paper does not provide any experimental results or analysis related to the handling of graphs with multiple labels or tasks by ACGAN-GNNExplainer, nor does it discuss the modifications required to adapt the method for multi-label or multi-task graph explanation.
- What evidence would resolve it: Experimental results demonstrating the performance of ACGAN-GNNExplainer on graphs with multiple labels or tasks, along with a discussion on the modifications required to adapt the method for multi-label or multi-task graph explanation.

## Limitations

- The architecture details of the encoder-decoder generator remain underspecified, particularly regarding how the graph and label are combined as input and the exact number of layers and dimensions
- The fidelity loss formulation assumes the target GNN's prediction on a masked subgraph is a valid proxy for subgraph importance, but this may not hold if the GNN architecture is sensitive to local structure removal
- The method requires reliable GNN predictions to condition on labels, but no robustness analysis is provided for noisy or incorrect labels

## Confidence

- High confidence: The overall ACGAN framework for explanation generation, the use of adversarial training to improve fidelity, and the experimental methodology for evaluating explanations on synthetic and real datasets
- Medium confidence: The specific fidelity loss formulation and its weighting hyperparameter Œª, as these require careful tuning and may not generalize across different GNN architectures
- Low confidence: The exact generator architecture details and how conditioning on labels is implemented, as these are critical for reproducing results but are underspecified

## Next Checks

1. Implement Granger causality preprocessing on a small synthetic graph (e.g., BA-Shapes) to verify that edge removal measurements correctly identify important structures for GNN predictions
2. Train ACGAN-GNNExplainer with varying Œª values (0, 2, 4) on synthetic data and measure the trade-off between fidelity and explanation sparsity to validate the importance of the fidelity loss term
3. Generate explanations for unseen graphs from the test set without retraining and compare their fidelity and accuracy with explanations generated by GNNExplainer to verify generalization capability