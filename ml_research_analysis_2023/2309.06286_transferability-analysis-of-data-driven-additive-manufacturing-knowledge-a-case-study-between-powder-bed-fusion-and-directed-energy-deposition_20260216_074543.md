---
ver: rpa2
title: 'Transferability analysis of data-driven additive manufacturing knowledge:
  a case study between powder bed fusion and directed energy deposition'
arxiv_id: '2309.06286'
source_url: https://arxiv.org/abs/2309.06286
tags:
- knowledge
- transfer
- source
- process
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for knowledge transferability analysis
  in additive manufacturing (AM), focusing on transferring data-driven knowledge between
  Laser Powder Bed Fusion (LPBF) and Directed Energy Deposition (DED) processes. The
  framework identifies knowledge components across AM and machine learning (ML) domains,
  conducts pre-transfer similarity analysis, applies transfer learning methods, and
  validates results.
---

# Transferability analysis of data-driven additive manufacturing knowledge: a case study between powder bed fusion and directed energy deposition

## Quick Facts
- arXiv ID: 2309.06286
- Source URL: https://arxiv.org/abs/2309.06286
- Reference count: 19
- This paper presents a framework for knowledge transferability analysis in additive manufacturing (AM), demonstrating successful transfer of anomaly detection knowledge between LPBF and DED processes with 94% accuracy.

## Executive Summary
This paper introduces a systematic framework for analyzing and implementing knowledge transferability in additive manufacturing through transfer learning. The framework identifies knowledge components across AM and machine learning domains, conducts similarity analysis, applies appropriate transfer learning methods, and validates results. A case study successfully demonstrates transferring melt pool anomaly detection capabilities from LPBF to DED processes, achieving 94% accuracy compared to 84% without transfer learning. The work addresses the challenge of developing process-specific ML solutions by enabling efficient knowledge reuse across AM technologies.

## Method Summary
The framework consists of five main components: knowledge featurization into AM and ML domain components, pre-transfer similarity analysis using ordinal scoring, transfer learning method selection based on domain and task similarity, model adaptation through fine-tuning strategies, and post-transfer validation. The case study implements a Convolutional LSTM autoencoder for spatiotemporal melt pool data modeling, with three transfer learning strategies tested: retraining all layers, retraining CNN then ConvLSTM, and retraining ConvLSTM then CNN. Data from NIST LPBF and MSU DED repositories is structured into spatiotemporal concatenations for consistent processing.

## Key Results
- Transfer learning achieved 94% accuracy on DED test data compared to 84% without transfer learning
- Three transfer learning strategies were tested, all showing significant performance gains over baseline
- The framework successfully identifies similarity between source and target contexts through knowledge component analysis
- Systematic knowledge exchange between AM processes is enabled, addressing the challenge of developing process-specific ML solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning enables effective knowledge reuse between AM processes by leveraging shared ML model structures while adapting to domain-specific differences
- Mechanism: The framework identifies similarity between source and target contexts through knowledge component analysis. When processes share ML tasks (e.g., classification) but differ in domain parameters (process, material, system), parameter-based transfer learning methods can fine-tune model weights from source to target
- Core assumption: Sufficient overlap exists in ML task structure while domain differences are limited enough for effective transfer
- Evidence anchors:
  - [abstract] "Three transfer learning strategies were tested, all showing significant performance gains"
  - [section] "Similarities are observed in both AM domain as well as AM task"
- Break condition: Domain differences become too large (e.g., completely different data types) or target task requires fundamentally different ML approach

### Mechanism 2
- Claim: Structured knowledge featurization enables systematic transferability analysis across AM processes
- Mechanism: AM knowledge is decomposed into hierarchical components (process, material, system, model, activity, concern) and ML knowledge into task, model, input, preprocessing, output. This enables quantitative similarity scoring and maturity assessment
- Core assumption: Knowledge can be meaningfully decomposed into these discrete components without losing essential context
- Evidence anchors:
  - [abstract] "As a prerequisite to transferability analysis, AM knowledge is featurized into identified knowledge components"
  - [section] "The ordinal levels of AM knowledge are defined below: AM Process (AM_P):..."
- Break condition: Knowledge components become too granular or overlap too much, making similarity assessment ambiguous

### Mechanism 3
- Claim: Spatiotemporal data representation bridges process monitoring data from different AM technologies
- Mechanism: Both LPBF and DED melt pool data are structured into spatiotemporal concatenations that preserve temporal dependencies while enabling CNN-LSTM architectures to learn process dynamics
- Core assumption: Process monitoring data from different AM processes can be normalized to a common spatiotemporal representation
- Evidence anchors:
  - [abstract] "We show successful transfer at different levels of the data-driven solution, including data representation, model architecture, and model parameters"
  - [section] "The LPBF and DED datasets are structured following the framework described above"
- Break condition: Process dynamics are too different to normalize effectively, or temporal resolution differences prevent meaningful comparison

## Foundational Learning

- Concept: Transfer Learning Fundamentals
  - Why needed here: The paper applies transfer learning to move knowledge between AM processes, requiring understanding of when and how to transfer
  - Quick check question: What are the three main types of transfer learning scenarios based on domain and task similarity?

- Concept: Deep Learning for Video Anomaly Detection
  - Why needed here: The case study uses CNN-LSTM autoencoder for melt pool anomaly detection, which requires understanding of spatiotemporal feature learning
  - Quick check question: How does a ConvLSTM layer differ from a standard LSTM in processing spatiotemporal data?

- Concept: AM Process Monitoring Fundamentals
  - Why needed here: Understanding melt pool dynamics and monitoring data is essential for applying the framework to real AM processes
  - Quick check question: What are the key differences between LPBF and DED processes that affect melt pool monitoring?

## Architecture Onboarding

- Component map:
  Knowledge featurization layer (AM and ML components) -> Pre-transfer analysis engine (similarity scoring, maturity assessment) -> Transfer learning selector (domain/task comparison to TL method) -> Model adaptation module (fine-tuning strategies) -> Post-transfer validation framework

- Critical path: Knowledge featurization → Pre-transfer analysis → Transfer learning method selection → Model adaptation → Validation

- Design tradeoffs:
  - Granularity of knowledge components vs. transferability assessment complexity
  - Transfer learning method choice vs. data availability and similarity level
  - Fine-tuning strategy (all layers vs. selective) vs. risk of catastrophic forgetting

- Failure signatures:
  - Pre-transfer score < 0.5 indicates insufficient similarity for effective transfer
  - Transfer learning performance improvement < 5% suggests negative transfer or poor method selection
  - Model convergence issues during fine-tuning may indicate domain shift too large

- First 3 experiments:
  1. Replicate source model performance on source data to establish baseline
  2. Test source model on target data without transfer to measure initial performance gap
  3. Apply fine-tuning to all layers and measure performance improvement against baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold selection method for anomaly detection when regularity scores between normal and anomalous concatenations have small differences?
- Basis in paper: [explicit] The paper mentions that "process images from AM are not as information rich as real-world images" and "require careful selection of the threshold" due to smaller differences in regularity scores.
- Why unresolved: The paper uses heuristic methods (3rd minimum regularity score on train set) but doesn't establish a systematic approach for threshold optimization in low-contrast scenarios.
- What evidence would resolve it: Comparative analysis of different threshold selection methods (Otsu's method, ROC curve optimization, adaptive thresholding) on multiple AM datasets with varying image quality.

### Open Question 2
- Question: What are the fundamental knowledge components that can be transferred between metal AM processes with completely different physics (e.g., LPBF vs. binder jetting)?
- Basis in paper: [inferred] The framework identifies knowledge components but is validated only between LPBF and DED, both of which share similar melt pool physics.
- Why unresolved: The paper's case study focuses on processes with similar physical mechanisms, leaving open whether the framework applies to processes with fundamentally different underlying physics.
- What evidence would resolve it: Successful knowledge transfer case studies between AM processes with different physical mechanisms (e.g., powder bed fusion vs. binder jetting vs. extrusion).

### Open Question 3
- Question: What is the relationship between transfer learning performance and the degree of process parameter similarity between source and target contexts?
- Basis in paper: [explicit] The paper notes that "source and target scenarios share some common knowledge at the ML model level" but different process parameters affect marginal distributions.
- Why unresolved: While the paper demonstrates successful transfer between LPBF and DED, it doesn't quantify how varying degrees of parameter similarity impact transfer learning effectiveness.
- What evidence would resolve it: Systematic study varying source and target process parameters (power, speed, layer thickness) while measuring transfer learning performance across multiple parameter similarity gradients.

## Limitations
- Framework effectiveness depends heavily on degree of similarity between source and target processes
- Case study focuses on melt pool anomaly detection between two specific AM processes, limiting broader applicability
- Optimal granularity of knowledge components requires further exploration for systematic transferability assessment

## Confidence

- **High confidence**: Transfer learning consistently improves performance (94% vs 84% accuracy) when applied to sufficiently similar processes with matching ML tasks
- **Medium confidence**: The knowledge featurization approach provides systematic framework for transferability analysis, though optimal granularity of knowledge components requires further exploration
- **Low confidence**: The framework's scalability to radically different AM processes or ML tasks remains unproven without additional case studies

## Next Checks

1. **Cross-process validation**: Test framework on knowledge transfer between LPBF and binder jetting processes to assess performance across more dissimilar AM technologies
2. **Task generalization**: Apply framework to regression tasks (e.g., mechanical property prediction) rather than classification to evaluate task transfer capabilities
3. **Knowledge component sensitivity**: Systematically vary granularity of knowledge components to determine optimal balance between specificity and transferability assessment accuracy