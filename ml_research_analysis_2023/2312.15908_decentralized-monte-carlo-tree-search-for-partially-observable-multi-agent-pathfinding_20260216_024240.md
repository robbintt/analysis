---
ver: rpa2
title: Decentralized Monte Carlo Tree Search for Partially Observable Multi-agent
  Pathfinding
arxiv_id: '2312.15908'
source_url: https://arxiv.org/abs/2312.15908
tags:
- agents
- agent
- mapf
- action
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a decentralized approach for lifelong multi-agent
  pathfinding using Monte Carlo Tree Search (MCTS) combined with a lightweight learnable
  policy. The method, called MATS-LP, uses intrinsic Markov decision processes reconstructed
  from local observations to enable planning in partially observable environments.
---

# Decentralized Monte Carlo Tree Search for Partially Observable Multi-agent Pathfinding

## Quick Facts
- arXiv ID: 2312.15908
- Source URL: https://arxiv.org/abs/2312.15908
- Reference count: 15
- Primary result: MATS-LP achieves 15.6-46.2% higher throughput than state-of-the-art learnable MAPF solvers

## Executive Summary
This paper proposes MATS-LP, a decentralized approach for lifelong multi-agent pathfinding using Monte Carlo Tree Search combined with a lightweight learnable policy. The method enables planning in partially observable environments by reconstructing intrinsic Markov decision processes from local observations. Through extensive experiments on random maps, maze-like environments, and warehouse scenarios, MATS-LP demonstrates superior performance compared to state-of-the-art learnable MAPF solvers while maintaining reasonable decision times.

## Method Summary
MATS-LP uses decentralized MCTS with a lightweight learnable policy (COST TRACER) trained via PPO. Each agent maintains a local observation window and constructs an intrinsic MDP containing only visible agents and obstacles. MCTS simulations use the learnable policy to evaluate states, with action masking reducing computational complexity by considering all actions for proximal agents and only the most probable action for distant agents. The approach achieves coordination without explicit agent communication.

## Key Results
- MATS-LP achieves 15.6-46.2% higher throughput than PRIMAL2 and SCRIMP baselines
- The method maintains reasonable decision times (0.2-1.8 seconds) even with multiple agents
- Strong generalization performance on unseen problems without explicit communication
- Outperforms learnable MAPF solvers on random maps, maze-like environments, and warehouse scenarios

## Why This Works (Mechanism)

### Mechanism 1
Intrinsic MDPs enable local planning in partially observable environments by reconstructing a complete state from local observations. Each agent builds a reduced MDP containing only agents visible in its local observation window and static obstacles, allowing decentralized planning without global state knowledge. The core assumption is that local observations are sufficient to reconstruct a meaningful MDP for decision-making. Evidence shows intrinsic MDPs successfully guide agent decisions, but if the observation window is too small to capture relevant nearby agents, the intrinsic MDP becomes incomplete, leading to poor coordination and potential deadlocks.

### Mechanism 2
Combining a lightweight learnable policy (COST TRACER) with MCTS improves coordination and generalization in multi-agent pathfinding. COST TRACER provides action probabilities and value estimates during MCTS simulations, guiding the search toward promising actions while maintaining low computational cost. The hybrid approach balances exploration of future states with learned agent behavior. The core assumption is that a compact neural network can approximate effective agent behavior for MAPF tasks. Evidence shows the experimental results outperform state-of-the-art learnable MAPF solvers, but if the learnable policy is not sufficiently trained or the network is too small to capture necessary behaviors, MCTS simulations will be ineffective, leading to poor pathfinding performance.

### Mechanism 3
Action masking for proximal and distant agents reduces computational complexity while maintaining coordination quality. For agents within close proximity (K agents including self), all feasible actions are considered. For distant agents, only the action with highest predicted probability is used. This dramatically reduces the joint action space size while focusing computation on agents that most affect local decisions. The core assumption is that distant agents have minimal impact on immediate local decisions compared to proximal agents. Evidence shows the action masking technique effectively reduces computational complexity, but if the proximity threshold is set too small or the environment has complex interactions across larger distances, important coordination may be missed, leading to suboptimal paths.

## Foundational Learning

- **Partially Observable Markov Decision Processes (POMDPs)**: The decentralized MAPF setting creates partial observability where agents only see local environments, requiring POMDP-style reasoning. Quick check: How does partial observability differ from full observability in terms of state representation and planning complexity?

- **Monte Carlo Tree Search (MCTS) and Upper Confidence bounds applied to Trees (UCT)**: MCTS enables efficient exploration of future state trajectories in the intrinsic MDPs, while UCT balances exploration and exploitation during tree expansion. Quick check: What is the role of the exploration coefficient c in the PUCT formula, and how does it affect tree growth?

- **Actor-Critic Reinforcement Learning and Proximal Policy Optimization (PPO)**: PPO trains the lightweight policy (COST TRACER) that provides action probabilities and value estimates for MCTS simulations, enabling learned behavior in partially observable settings. Quick check: How does PPO's clipped objective prevent large policy updates that could destabilize learning?

## Architecture Onboarding

- **Component map**: Observation → Intrinsic MDP construction → MCTS planning with COST TRACER → Action selection → Execution → Repeat
- **Critical path**: Each agent maintains local observation window (m×m grid), intrinsic MDP reconstruction module, MCTS planner with COST TRACER policy, and action selection and execution module
- **Design tradeoffs**: The approach trades computational complexity (limited MCTS expansions, action masking) for scalability and decentralization. The lightweight policy enables faster simulations but may sacrifice some planning quality compared to larger networks
- **Failure signatures**: Poor performance on maps with narrow passages may indicate insufficient coordination; high decision times with many agents suggest the action masking threshold needs adjustment; low throughput compared to baselines may indicate the intrinsic MDP reconstruction is incomplete
- **First 3 experiments**: 1) Test COST TRACER alone (without MCTS) on random maps to verify the learnable policy provides reasonable actions. 2) Evaluate MATS-LP with varying numbers of MCTS expansions (e.g., 125, 250, 500) to find the sweet spot between performance and computation time. 3) Test different observation window sizes (e.g., 7×7, 11×11, 15×15) to understand the impact of local information availability on pathfinding quality.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important research directions emerge from the work:

### Open Question 1
How does the performance of MATS-LP scale with larger observation space sizes beyond 11x11? The paper only tested with an 11x11 observation space and did not explore how larger observation spaces would affect performance. Running experiments with various observation space sizes (e.g., 15x15, 21x21) and comparing throughput and decision time would resolve this question.

### Open Question 2
How would MATS-LP perform in environments with dynamic obstacles or non-deterministic agent actions? The current experiments are limited to static environments with deterministic actions. Implementing MATS-LP in environments with dynamic obstacles and testing its performance compared to the current static setup would address this question.

### Open Question 3
What is the impact of varying the number of MCTS expansions per iteration on MATS-LP's performance and decision time? While some variations were tested, a comprehensive study across a wider range of expansion numbers was not conducted. Systematically varying the number of expansions (e.g., 50, 100, 250, 500, 1000) and analyzing the trade-off between performance and decision time would resolve this question.

## Limitations
- Performance depends heavily on local observation window size and may struggle with complex coordination requiring broader context
- Action masking assumes distant agents have minimal impact, which may not hold in environments with narrow passages or complex interaction patterns
- The lightweight policy's effectiveness depends on proper PPO training and may be sensitive to training data distribution and hyperparameters

## Confidence
- **High Confidence**: The core mechanism of using intrinsic MDPs for local planning in partially observable environments is well-supported by the experimental results showing 15.6-46.2% higher throughput than baselines
- **Medium Confidence**: The combination of MCTS with the lightweight learnable policy effectively balances exploration and learned behavior, though the optimal balance point may depend on specific problem characteristics
- **Medium Confidence**: Action masking for distant agents significantly reduces computational complexity while maintaining coordination quality, though the effectiveness may vary with environment density and interaction complexity

## Next Checks
1. Test MATS-LP performance with varying observation window sizes (e.g., 7×7, 11×11, 15×15) to determine the minimum local information needed for effective coordination
2. Evaluate the method on environments with narrow passages and complex interaction patterns to assess the limitations of the action masking technique for distant agents
3. Conduct ablation studies removing either the learnable policy or MCTS components to quantify their individual contributions to overall performance