---
ver: rpa2
title: 'NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders'
arxiv_id: '2305.14499'
source_url: https://arxiv.org/abs/2305.14499
tags:
- retrieval
- nail
- query
- document
- beir
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: nail presents a method to push expensive neural computation to
  indexing time in document retrieval, using a non-autoregressive decoder architecture
  based on pre-trained language models like T5. Instead of scoring queries at serving
  time, nail predicts token scores for documents offline, enabling fast lookup at
  query time with commodity CPUs.
---

# NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders

## Quick Facts
- **arXiv ID**: 2305.14499
- **Source URL**: https://arxiv.org/abs/2305.14499
- **Reference count**: 40
- **Primary result**: Achieves 86% of cross-attention reranker performance with 10⁻⁶ of the FLOPs per query

## Executive Summary
NAIL presents a novel approach to neural document retrieval that shifts expensive neural computation from query time to indexing time. By using a non-autoregressive decoder architecture, NAIL can predict scores for all vocabulary tokens in parallel during indexing, creating sparse document representations that can be stored in inverted indices. This enables fast query-time retrieval using commodity CPUs without requiring accelerators. The approach combines the effectiveness of neural retrieval with the efficiency of sparse methods, achieving strong performance on both in-domain (MS-MARCO) and zero-shot (BEIR) benchmarks.

## Method Summary
NAIL modifies the T5 architecture's decoder to support non-autoregressive prediction of vocabulary scores for each document. During indexing, the model scores all tokens in the vocabulary for each document in parallel, creating a vector of token scores that gets stored in an inverted index. At query time, scoring involves only looking up scores for query tokens. The model is trained in two stages: first with self-supervised inverse cloze and cropping tasks on C4, then fine-tuned on MS-MARCO with hard negatives. The non-autoregressive decoder allows all output positions to be decoded in parallel, with each position outputting a distribution over the entire vocabulary, and max pooling over positions produces the final token score vector.

## Key Results
- Recovers 86% of cross-attention reranker performance while requiring 10⁻⁶ of the inference FLOPs per query
- Matches state-of-the-art dual-encoder retrievers when combined with BM25
- Achieves strong zero-shot retrieval results on BEIR benchmark
- Exhaustive scoring shows potential for high recall with sparsification maintaining most performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Non-autoregressive decoding allows the model to predict all vocabulary token scores in a single decode step, enabling precomputation of document representations.
- **Mechanism**: The decoder stack is modified so that all output positions are decoded in parallel without dependency on previous predictions, and each position outputs a distribution over the entire vocabulary. The max over positions produces a single score per token.
- **Core assumption**: The model can learn to predict relevant query tokens for a document without cross-attention to the query at serving time.
- **Evidence anchors**: [abstract] states the non-autoregressive decoder "allows the model, in a single decode step, to score all vocabulary items in parallel." [section] explains the modification "to support independent predictions of the output tokens... allows the model to produce output for all positions in parallel."
- **Break condition**: If the non-autoregressive decoder cannot effectively learn to predict relevant tokens without conditioning on the query, performance will degrade significantly.

### Mechanism 2
- **Claim**: Precomputing document representations at indexing time eliminates query-time neural computation, enabling serving on commodity CPUs.
- **Mechanism**: The model scores all tokens in the vocabulary for each document during indexing, creating a vector of token scores that can be stored in an inverted index. At query time, scoring involves only looking up scores for query tokens.
- **Core assumption**: The cost of neural computation at indexing time is acceptable while the cost at query time must be minimal.
- **Evidence anchors**: [abstract] notes NAIL "can be served using commodity CPUs" with "10−6% of the inference-time FLOPS per query." [section] states "it is possible to push all costly neural network inference to indexing time, and avoid the need for accelerators at serving-time."
- **Break condition**: If the indexing time cost becomes prohibitive for large corpora, or if the precomputed representations cannot capture sufficient query-document relevance.

### Mechanism 3
- **Claim**: Contrastive learning with hard negatives during fine-tuning aligns the model with the MS-MARCO domain, improving in-domain performance at the cost of zero-shot generalization.
- **Mechanism**: The model is trained using a softmax over positive and negative passages, with hard negatives sampled from BM25 retrievals. This creates a domain-specific alignment that improves MS-MARCO results but reduces BEIR performance.
- **Core assumption**: The trade-off between in-domain and zero-shot performance can be managed through negative sampling strategy.
- **Evidence anchors**: [abstract] mentions training "followed by fine-tuning on MS-MARCO with hard negatives." [section] describes using "gold passage as positive, along with a small sample of the BM25 candidate passages as hard negatives."
- **Break condition**: If the hard negative sampling strategy creates too strong a domain bias, zero-shot performance across BEIR datasets will suffer significantly.

## Foundational Learning

- **Concept**: Non-autoregressive decoding in sequence-to-sequence models
  - **Why needed here**: Traditional autoregressive decoding requires sequential token prediction, making it impossible to efficiently score all vocabulary tokens in parallel for document indexing.
  - **Quick check question**: How does non-autoregressive decoding differ from standard autoregressive decoding in terms of token prediction dependencies?

- **Concept**: Contrastive learning with in-batch negatives
  - **Why needed here**: The model needs to learn which tokens are relevant to a document by comparing positive and negative examples within each training batch.
  - **Quick check question**: In the context of nail's training, what role do the other positive passages in a batch play when computing the loss for a given query-passage pair?

- **Concept**: Sparse vs dense retrieval representations
  - **Why needed here**: Nail uses a sparse representation (token scores) rather than dense vectors, which affects both the model architecture and serving requirements.
  - **Quick check question**: What are the key differences in query-time computation between sparse retrieval methods like nail and dense retrieval methods like dual-encoders?

## Architecture Onboarding

- **Component map**: Document text → T5 encoder → Non-autoregressive decoder (parallel scoring) → Max pooling → Token score vector → Inverted index
- **Critical path**: Document text → T5 encoder → Non-autoregressive decoder (parallel scoring) → Max pooling → Token score vector → Inverted index
- **Design tradeoffs**: Non-autoregressive decoding sacrifices some modeling power for parallel computation; precomputation trades indexing time for query-time efficiency; sparse representations trade some semantic modeling for interpretability and efficiency.
- **Failure signatures**: Poor retrieval performance indicates issues with the non-autoregressive decoder learning, inadequate contrastive training, or suboptimal token featurization; extremely high indexing time suggests the vocabulary scoring is too expensive.
- **First 3 experiments**:
  1. Verify the non-autoregressive decoder can predict reasonable token distributions by examining top-k predictions for sample documents
  2. Test retrieval performance with and without contrastive fine-tuning to measure the impact of domain alignment
  3. Compare retrieval recall@100 for exhaustive scoring versus top-k sparsified representations to find the efficiency-performance sweet spot

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but implies several areas for future research through its discussion of limitations and comparison to related work, particularly around the trade-offs between distillation-based training methods and the impact of different featurization approaches on retrieval performance.

## Limitations

- The efficiency comparison between exhaustive scoring and reranking may overstate real-world query-time benefits
- Sparsification strategy (top-k token selection) could miss relevant signals if important tokens fall below the cutoff
- Significant indexing-time compute requirements may not be feasible for rapidly changing document collections

## Confidence

- **High confidence**: The core architectural innovation of using non-autoregressive decoding for parallel vocabulary scoring is well-specified and the efficiency gains from precomputing document representations are clearly demonstrated through FLOPs comparison.
- **Medium confidence**: The retrieval performance claims are credible given the evaluation on standard benchmarks, though the comparison methodology (exhaustive vs reranking) warrants scrutiny for real-world applicability.
- **Medium confidence**: The two-stage training procedure with contrastive learning is standard in the field, though the specific hyperparameters and sampling strategies for hard negatives could affect reproducibility.

## Next Checks

1. **Cross-comparison validation**: Replicate the efficiency comparison by measuring actual wall-clock query latency and indexing time for NAIL versus a standard dual-encoder retriever on the same hardware, ensuring fair comparison between exhaustive scoring and candidate-based approaches.

2. **Sparsification sensitivity analysis**: Systematically vary the top-k parameter in the sparsification step and measure the corresponding impact on retrieval quality across multiple BEIR datasets to identify the optimal trade-off between efficiency and effectiveness.

3. **Domain generalization test**: Train NAIL variants with different negative sampling strategies (including both hard and random negatives) and evaluate their zero-shot performance across BEIR to quantify the impact of contrastive fine-tuning on cross-domain generalization.