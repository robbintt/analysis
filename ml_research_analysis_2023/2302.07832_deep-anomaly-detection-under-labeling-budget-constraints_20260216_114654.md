---
ver: rpa2
title: Deep Anomaly Detection under Labeling Budget Constraints
arxiv_id: '2302.07832'
source_url: https://arxiv.org/abs/2302.07832
tags:
- data
- anomaly
- soel
- detection
- querying
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies active anomaly detection with a limited labeling
  budget, where the goal is to select a small set of informative samples to label
  and use them to train an anomaly detector. The key idea is to use a diverse querying
  strategy that selects samples that cover the data well, based on theoretical conditions
  that guarantee generalization of anomaly scores from labeled to unlabeled data.
---

# Deep Anomaly Detection under Labeling Budget Constraints

## Quick Facts
- arXiv ID: 2302.07832
- Source URL: https://arxiv.org/abs/2302.07832
- Reference count: 40
- Key outcome: SOEL framework with diverse querying significantly outperforms existing active anomaly detection methods across image, tabular, and video datasets

## Executive Summary
This paper addresses active anomaly detection with limited labeling budgets by proposing a semi-supervised framework called SOEL that combines supervised and unsupervised learning objectives. The key innovation is a diverse querying strategy using k-means++ that optimally selects informative samples to label, along with an importance sampling estimator for contamination ratio that eliminates a critical hyperparameter. Experiments show SOEL achieves superior performance on CIFAR-10, Fashion-MNIST, medical datasets, tabular ODDS datasets, and UCSD Peds1 video data.

## Method Summary
The method trains a backbone model (NTL) on unlabeled data, then selects K diverse samples using k-means++ querying. It estimates contamination ratio α via importance sampling from the labeled queries, then trains using SOEL loss combining supervised loss on labeled queries with unsupervised loss on unlabeled data. The framework is compatible with various deep AD losses and eliminates the need to manually specify α.

## Key Results
- SOEL outperforms existing active anomaly detection methods on image datasets (CIFAR-10, Fashion-MNIST) with 10% contamination
- Achieves strong F1-scores on tabular ODDS datasets compared to baselines
- Demonstrates good performance on UCSD Peds1 video data with AUC metrics
- Shows robust performance across diverse data types and labeling budgets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse querying (k-means++) provides better data coverage, leading to improved anomaly score ranking on unlabeled data
- Mechanism: Iteratively samples data points far from already queried samples, minimizing maximum distance (radius δ) from any unlabeled point to its nearest labeled point of same type. Smaller δ allows anomaly score function to rank points correctly with larger margin.
- Core assumption: Anomaly scores are Lipschitz continuous and data points of same type are well-separated
- Evidence anchors:
  - [abstract]: "Motivated by these results, we propose a data labeling strategy with optimal data coverage under labeling budget constraints."
  - [section]: "Theorem 1. Let Q0 be the index set of datapoints labeled normal and Q1 the index set of datapoints labeled abnormal... If aλs-Lipschitz continuous function S ranks the labeled data correctly, with a large enough margin, i.e. S(xa)−S(xn)≥ 2δλs, then S ranks the unlabeled points correctly too andS(ua)≥S(un)."
- Break condition: If data points of different types are not well-separated, Lipschitz continuity assumption may not hold

### Mechanism 2
- Claim: SOEL combines supervised and unsupervised learning benefits, leading to improved anomaly detection performance
- Mechanism: Combines supervised loss on labeled samples with unsupervised loss on unlabeled samples. Supervised loss uses ground truth labels, while unsupervised loss uses estimated labels from ranking unlabeled samples based on current anomaly score function.
- Core assumption: Estimated labels for unlabeled samples are reasonably accurate, and contamination ratio α can be estimated reliably
- Evidence anchors:
  - [abstract]: "We propose semi-supervised outlier exposure with a limited labeling budget (SOEL), a semi-supervised learning framework compatible with a large number of deep AD losses."
  - [section]: "SOEL training objective is designed to receive opposing training signals from the normal samples and the anomalies... The constraint ensures that the inferred anomaly labels respect a certain contamination ratio α."
- Break condition: If estimated labels are highly inaccurate or α cannot be estimated reliably, SOEL loss may not improve performance

### Mechanism 3
- Claim: Contamination ratio α can be estimated from labeled data using importance sampling, eliminating a critical hyperparameter
- Mechanism: Estimates α using importance sampling estimator that accounts for biased querying strategy. Estimator is unbiased under certain assumptions about independence of anomaly scores and their sufficiency as statistic for data distributions.
- Core assumption: Anomaly scores are approximately independent random variables and are sufficient statistics for both data distribution and querying distribution
- Evidence anchors:
  - [abstract]: "We provide an estimate for the contamination ratio in the data."
  - [section]: "Theorem 2. Given Assumptions 1 and 2, the following statement applies: for a random queryQ, the following importance sampling estimator provides an unbiased estimate of the contamination ratio: ˆα = 1/|Q| ∑_{i=1}^|Q| f(S(xi))/g(S(xi)) 1a(xi)."
- Break condition: If independence and sufficiency assumptions about anomaly scores do not hold, importance sampling estimator may be biased

## Foundational Learning

- Concept: Lipschitz continuity of the anomaly score function
  - Why needed here: Diverse querying strategy relies on assumption that anomaly scores are Lipschitz continuous to ensure ranking generalizes from labeled to unlabeled data
  - Quick check question: What is the definition of a Lipschitz continuous function, and how does it relate to the anomaly score function in this context?

- Concept: Importance sampling
  - Why needed here: Contamination ratio α is estimated using importance sampling estimator that accounts for biased querying strategy
  - Quick check question: What is the purpose of importance sampling, and how does it differ from simple random sampling?

- Concept: Sufficient statistics
  - Why needed here: Contamination ratio estimation relies on assumption that anomaly scores are sufficient statistics for both data distribution and querying distribution
  - Quick check question: What is a sufficient statistic, and how does it relate to the anomaly scores in this context?

## Architecture Onboarding

- Component map: NTL backbone -> Diverse querying (k-means++) -> Contamination ratio estimation (importance sampling) -> SOEL training (supervised + unsupervised loss)

- Critical path:
  1. Train backbone model on unlabeled data for one epoch
  2. Select K samples using diverse querying strategy
  3. Estimate contamination ratio α using importance sampling estimator
  4. Train model using SOEL loss function with labeled and unlabeled data

- Design tradeoffs:
  - Diverse querying vs. random querying: Diverse querying provides better coverage but may be more computationally expensive
  - SOEL vs. supervised learning: SOEL can leverage unlabeled data but relies on accurate estimation of contamination ratio
  - Importance sampling vs. other estimation methods: Importance sampling is unbiased under certain assumptions but may be less accurate with small sample sizes

- Failure signatures:
  - Poor data coverage: If diverse querying strategy does not provide good coverage, anomaly score ranking may not generalize well to unlabeled data
  - Inaccurate contamination ratio estimation: If importance sampling estimator is biased, SOEL loss function may not work well
  - Insufficient labeled data: If labeling budget is too small, supervised loss may not be effective in guiding the model

- First 3 experiments:
  1. Test diverse querying strategy on small dataset with known anomalies to verify good coverage
  2. Test SOEL loss function with known contamination ratio to verify improvement over supervised learning alone
  3. Test contamination ratio estimation method on small dataset with known anomalies to verify accurate estimates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diverse querying strategy perform on extremely high-dimensional data, such as high-resolution images or genomic sequences?
- Basis in paper: [inferred] The paper mentions that the diverse querying strategy is based on k-means++ algorithm, which has O(KN) time complexity that can be reduced to O(K log N) with scalable alternatives. However, it does not discuss performance on extremely high-dimensional data.
- Why unresolved: The paper does not provide empirical results or theoretical analysis on diverse querying strategy performance on extremely high-dimensional data. This is important because high-dimensional data is common in many real-world applications, and performance on such data could significantly impact practical utility.
- What evidence would resolve it: Empirical results comparing diverse querying strategy performance on high-dimensional data versus lower-dimensional data, or theoretical analysis explaining why strategy should or should not work well on high-dimensional data.

### Open Question 2
- Question: How sensitive is the SOEL method to the choice of the anomaly score function S(x;θ)?
- Basis in paper: [explicit] The paper states that SOEL is a semi-supervised learning framework compatible with a large number of deep anomaly detection losses. However, it does not provide empirical results or theoretical analysis on sensitivity to choice of anomaly score function.
- Why unresolved: The paper does not provide empirical results or theoretical analysis on SOEL sensitivity to choice of anomaly score function. This is important because choice of anomaly score function can significantly impact SOEL performance, and understanding this sensitivity could help users make more informed decisions.
- What evidence would resolve it: Empirical results comparing SOEL performance with different anomaly score functions, or theoretical analysis explaining why certain anomaly score functions should or should not work well with SOEL.

### Open Question 3
- Question: How does the SOEL method perform when the contamination ratio α is very high or very low?
- Basis in paper: [inferred] The paper mentions that SOEL method includes estimate for contamination ratio in data, but does not discuss performance when contamination ratio is very high or very low.
- Why unresolved: The paper does not provide empirical results or theoretical analysis on SOEL method performance when contamination ratio is very high or very low. This is important because contamination ratio can vary widely in real-world applications, and understanding performance in extreme cases could help users make more informed decisions.
- What evidence would resolve it: Empirical results comparing SOEL method performance with different contamination ratios, or theoretical analysis explaining why method should or should not work well when contamination ratio is very high or very low.

## Limitations
- Theoretical guarantees rely on assumptions about Lipschitz continuity and well-separated data clusters that may not hold in complex real-world distributions
- Importance sampling estimator for contamination ratio may have high variance with limited labeled data
- Performance may degrade if labeled queries contain mislabeled points, as theoretical analysis assumes clean queries

## Confidence
- High confidence: SOEL framework combining supervised and unsupervised losses is technically sound and well-motivated by semi-supervised learning theory
- Medium confidence: Diverse querying strategy's theoretical guarantees hold under stated assumptions, but practical performance may vary with data characteristics
- Medium confidence: Importance sampling estimator for contamination ratio provides unbiased estimates under assumptions, but may have high variance with limited labeled data

## Next Checks
1. Sensitivity analysis of contamination ratio estimation: Systematically vary estimated α ±5% from true value on multiple datasets to quantify impact on SOEL performance, measuring how estimation errors propagate to final detection quality
2. Lipschitz continuity verification: Empirically measure Lipschitz constant of trained anomaly score functions on CIFAR-10 and tabular datasets, and correlate these values with diverse querying effectiveness across different query budgets
3. Query budget ablation study: Test diverse querying vs random querying with varying budgets (K=10, 25, 50, 100) on UCSD Peds1 video data to determine minimum budget where theoretical advantages manifest in practice, measuring both coverage quality and detection performance trade-offs