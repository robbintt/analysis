---
ver: rpa2
title: 'TRACE: A Comprehensive Benchmark for Continual Learning in Large Language
  Models'
arxiv_id: '2310.06762'
source_url: https://arxiv.org/abs/2310.06762
tags:
- accuracy
- mmlu
- lukaemon
- learning
- tyidqa-goldp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'TRACE is a benchmark for evaluating continual learning in aligned
  LLMs, consisting of 8 datasets covering domain-specific tasks, multilingual capabilities,
  code generation, and mathematical reasoning. It introduces three metrics: General
  Ability Delta, Instruction Following Delta, and Safety Delta to assess changes in
  LLM capabilities after training on sequential tasks.'
---

# TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models

## Quick Facts
- arXiv ID: 2310.06762
- Source URL: https://arxiv.org/abs/2310.06762
- Reference count: 40
- Primary result: Aligned LLMs exhibit significant declines in general abilities after training on sequential tasks

## Executive Summary
TRACE is a benchmark designed to evaluate continual learning in aligned large language models across 8 diverse datasets covering domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning. The benchmark introduces three metrics (General Ability Delta, Instruction Following Delta, and Safety Delta) to measure changes in LLM capabilities after sequential training. Experiments reveal that aligned LLMs experience substantial degradation in general abilities, particularly in math and reasoning, prompting the development of the Reasoning-augmented Continual Learning (RCL) approach.

## Method Summary
TRACE provides a standardized evaluation framework for continual learning in aligned LLMs using 8 datasets with 5000 training examples each. The benchmark measures three types of capability changes through delta metrics after sequential training on multiple tasks. The RCL approach augments training data with reasoning paths generated by GPT-4, enabling models to preserve reasoning capabilities while learning new tasks more efficiently.

## Key Results
- Aligned LLMs show significant decline in general abilities after sequential task training
- Tasks with explicit reasoning paths (like ScienceQA) better preserve reasoning capabilities
- RCL achieves comparable results to full fine-tuning with only 10% of the training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TRACE tasks with reasoning paths preserve LLM reasoning abilities better than tasks without explicit reasoning steps
- Mechanism: Tasks with explicit reasoning annotations force the model to maintain and strengthen its step-by-step reasoning capability during training
- Core assumption: LLMs retain skills better when those skills are actively exercised in training tasks
- Evidence anchors: [abstract] tasks with reasoning paths contribute significantly to preserving capabilities; [section 4.5.2] surge in reasoning prowess post-ScienceQA training

### Mechanism 2
- Claim: RCL improves task transfer by leveraging existing reasoning capabilities rather than training from scratch
- Mechanism: By annotating training data with reasoning paths and requiring models to generate these during training, RCL activates the model's existing reasoning abilities
- Core assumption: LLMs have sufficient reasoning capabilities from pretraining that can be activated and transferred to new tasks
- Evidence anchors: [abstract] RCL integrates task-specific cues with meta-rationales; [section 5.1] 500 samples achieve comparable results to 5000 samples with SeqFT

### Mechanism 3
- Claim: Rehearsal with alignment data preserves instruction-following capabilities better than pure task-focused training
- Mechanism: Including alignment data in replay buffer maintains the model's ability to follow instructions by continuously exposing it to instruction-following examples
- Core assumption: Instruction-following capability is a distinct skill that degrades without specific maintenance
- Evidence anchors: [section 4.4.3] all three training methods exhibit marked decline in instruction-following; [section 4.3.1] replay 10% of historical data including LIMA

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: TRACE directly measures catastrophic forgetting in LLMs
  - Quick check question: What is the difference between forgetting and intransigence in continual learning terminology?

- Concept: Parameter-efficient fine-tuning (LoRA)
  - Why needed here: TRACE compares full fine-tuning vs LoRA, and understanding tradeoffs is essential
  - Quick check question: How does LoRA modify the weight update process compared to standard fine-tuning?

- Concept: Chain-of-thought reasoning
  - Why needed here: RCL relies on reasoning paths, and understanding CoT is essential
  - Quick check question: What is the key difference between standard prompting and chain-of-thought prompting?

## Architecture Onboarding

- Component map: TRACE benchmark consists of data collection (8 datasets), standardized evaluation framework, and three metrics (General Ability Delta, Instruction Following Delta, Safety Delta). RCL adds a reasoning annotation phase using GPT-4 followed by supervised training on augmented data.

- Critical path: For TRACE evaluation: Load model → Train on sequential tasks → Evaluate on general tasks, instruction tasks, and safety tasks. For RCL: Generate reasoning annotations → Train model on augmented data → Evaluate performance.

- Design tradeoffs: TRACE uses 5000 samples per task for balance between computational cost and evaluation quality. RCL trades off annotation cost for better performance with less training data. The choice between full fine-tuning and LoRA involves performance vs parameter efficiency.

- Failure signatures: If General Ability Delta is positive, the model has improved rather than degraded. If Instruction Following Delta is negative but Safety Delta is positive, the model may be over-optimized for safety at expense of instruction compliance. If RCL fails, reasoning annotations may be incorrect or the model may ignore them.

- First 3 experiments:
  1. Run TRACE evaluation on a pretrained LLM without any fine-tuning to establish baseline performance
  2. Run TRACE evaluation after sequential fine-tuning on just one dataset to observe early forgetting patterns
  3. Compare RCL vs standard fine-tuning on a single task with 500 vs 5000 samples to verify data efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of reasoning paths in training impact the preservation of LLM capabilities across different types of tasks?
- Basis in paper: [explicit] The paper mentions that tasks augmented with reasoning paths are notably effective in preserving certain capabilities of LLMs
- Why unresolved: The paper provides initial findings but does not explore detailed mechanisms or quantify impact across broader range of task types
- What evidence would resolve it: Detailed experimental results comparing LLM performance on various task types with and without reasoning path integration

### Open Question 2
- Question: What are the long-term effects of continual learning on the general abilities of LLMs, and how can they be mitigated?
- Basis in paper: [inferred] The paper highlights significant declines in general abilities after training on sequential tasks
- Why unresolved: While RCL is introduced as mitigation, no long-term study on effects or exploration of other strategies is provided
- What evidence would resolve it: Longitudinal studies tracking LLM performance over extended periods of continual learning

### Open Question 3
- Question: How does the size and diversity of the replay memory affect the effectiveness of replay-based continual learning methods?
- Basis in paper: [explicit] The paper discusses use of replay-based sequential fine-tuning and notes its effectiveness
- Why unresolved: The paper does not explore how varying replay memory size and diversity might influence preservation of capabilities
- What evidence would resolve it: Experimental results demonstrating impact of replay memory size and diversity on preservation of LLM capabilities

## Limitations
- Evaluation relies heavily on GPT-4 as automatic judge, introducing potential subjectivity and bias
- Small sample sizes (5000 training examples per task) may not capture real-world continual learning complexity
- RCL requires expensive GPT-4 reasoning annotations that may not scale well
- Paper lacks extensive ablation studies on reasoning path quality and replay buffer hyperparameters

## Confidence

- **High confidence**: Observation that aligned LLMs exhibit significant declines in general abilities after sequential training
- **Medium confidence**: Claim that tasks with explicit reasoning paths better preserve reasoning abilities
- **Low confidence**: Generalizability to larger models and different alignment techniques

## Next Checks

1. **Cross-model validation**: Replicate TRACE benchmark on additional model sizes (LLaMA-2-34B, LLaMA-3) to assess generalizability

2. **Reasoning annotation quality**: Conduct human evaluation of GPT-4-generated reasoning paths and measure correlation with RCL performance

3. **Long-term stability test**: Extend sequential training beyond 8 TRACE tasks to identify degradation patterns over extended periods