---
ver: rpa2
title: Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive
  Tasks
arxiv_id: '2305.18395'
source_url: https://arxiv.org/abs/2305.18395
tags:
- knowledge
- training
- rationale
- reasoning
- small
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying large language
  models (LLMs) for knowledge-intensive reasoning tasks, where small models struggle
  due to limited memorization capacity. To tackle this, the authors propose Knowledge-Augmented
  Reasoning Distillation (KARD), which fine-tunes small models to generate rationales
  from LLMs while augmenting them with external knowledge retrieved from a knowledge
  base.
---

# Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks

## Quick Facts
- arXiv ID: 2305.18395
- Source URL: https://arxiv.org/abs/2305.18395
- Reference count: 40
- Key outcome: 250M-parameter models outperform 3B-parameter fine-tuned models on MedQA-USMLE and StrategyQA using knowledge augmentation

## Executive Summary
This paper addresses the challenge of deploying small language models for knowledge-intensive reasoning tasks where limited memorization capacity becomes a bottleneck. The authors propose Knowledge-Augmented Reasoning Distillation (KARD), which fine-tunes small models to generate rationales from LLMs while augmenting them with external knowledge retrieved from Wikipedia. The method employs a neural reranker to prioritize relevant documents for rationale generation, even when queries are less specific. Empirical results demonstrate that 250M-parameter KARD models outperform 3B-parameter fine-tuned models on both MedQA-USMLE and StrategyQA datasets, while also showing efficiency in training data usage and model size.

## Method Summary
KARD works by first generating rationales from a teacher LLM (ChatGPT/GPT-3.5-turbo) using chain-of-thought prompting on training data. For each rationale, BM25 retrieves candidate documents from Wikipedia. A neural reranker (LinkBERT-based) is then trained to score and reorder documents based on their relevance to the rationale rather than just the question. Small language models (T5/Flan-T5) are fine-tuned to generate both rationale and answer given the question and retrieved knowledge. During inference, the pipeline retrieves documents using the question, reranks them, generates a rationale, and then produces the final answer.

## Key Results
- 250M-parameter KARD models outperform 3B-parameter fine-tuned models on both MedQA-USMLE and StrategyQA
- KARD shows significant improvements over baselines including fine-tuning, reasoning distillation, and few-shot prompting
- Performance gains are consistent across different model sizes (250M, 780M, 3B parameters)
- The method demonstrates efficiency in training data usage while maintaining strong performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge augmentation reduces the memorization burden on small models by providing external memory via retrieval
- Mechanism: By retrieving relevant passages from Wikipedia and conditioning generation on them, small models don't need to memorize all domain-specific knowledge internally
- Core assumption: Retrieved knowledge is relevant to rationale generation and effectively utilized during fine-tuning
- Evidence anchors: Abstract and section 3 mention knowledge retrieval from external knowledge base, but corpus lacks direct evidence of memorization reduction

### Mechanism 2
- Claim: Neural reranker improves retrieval quality by prioritizing passages useful for rationale generation
- Mechanism: The reranker is trained to score documents based on their relevance to LLM-generated rationales, not just questions
- Core assumption: LLM-generated rationale is a better proxy for needed knowledge than the question itself
- Evidence anchors: Abstract and section 4.2 describe rationale-guided reranking, but corpus lacks explicit mention of this approach

### Mechanism 3
- Claim: Multiple diverse rationales per training example improve small model reasoning generalization
- Mechanism: Generating multiple rationales exposes the small model to different reasoning paths, preventing overfitting to single reasoning styles
- Core assumption: Diversity in rationales leads to better generalization in small model reasoning
- Evidence anchors: Section 5.3 and table 2 show performance improves with more rationales (l=3 to l=5), but corpus lacks direct mention of rationale diversity

## Foundational Learning

- Concept: Memorization capacity vs model size
  - Why needed here: The paper argues small models cannot memorize all needed knowledge, motivating the external KB approach
  - Quick check question: If a 250M-parameter model needs to memorize 10GB of domain knowledge, what happens to performance compared to a 3B model?

- Concept: Knowledge retrieval and reranking
  - Why needed here: The method relies on retrieving relevant documents and then reranking them to prioritize rationale-relevant content
  - Quick check question: If BM25 retrieves 100 documents and the reranker only promotes 5, what determines which 5 are chosen?

- Concept: Chain-of-thought prompting and rationale generation
  - Why needed here: The LLM generates rationales via chain-of-thought prompting, which are then distilled into the small model
  - Quick check question: If the LLM generates a rationale with a logical error, how does that affect the small model's training?

## Architecture Onboarding

- Component map: Teacher LLM (ChatGPT/GPT-3.5-turbo) -> Retriever (BM25) -> Reranker (LinkBERT-based) -> Small LM (T5/Flan-T5) -> External KB (Wikipedia)

- Critical path: 1) Generate rationales from LLM for training data 2) Retrieve candidate documents using rationales as queries 3) Train reranker to prioritize rationale-relevant documents 4) Fine-tune small LM on (question, retrieved docs, rationale, answer) 5) At inference: retrieve → rerank → generate rationale → generate answer

- Design tradeoffs:
  - Retriever choice: BM25 (sparse, fast) vs DPR (dense, potentially more accurate but slower)
  - Reranker backbone: BioLinkBERT for MedQA vs LinkBERT for StrategyQA
  - Number of rationales per example (l): More diversity vs computational cost
  - Number of retrieved documents (k): More context vs risk of distraction

- Failure signatures:
  - Poor retrieval: Rationale includes hallucinations or incorrect facts
  - Ineffective reranking: Retrieved documents are irrelevant to rationale generation
  - Overfitting to rationales: Small model performs well on training but poorly on test
  - Knowledge gaps: Model fails on questions requiring knowledge not in Wikipedia

- First 3 experiments:
  1. Verify rationale generation: Run LLM on a few training samples and check if rationales are correct and diverse
  2. Test retrieval quality: Use BM25 to retrieve documents for a sample rationale and manually verify relevance
  3. Train reranker: Fine-tune on a small subset and check if it improves Hits@k over BM25 retrieval

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of knowledge augmentation vary across different knowledge-intensive reasoning tasks beyond medical and multi-step factual QA?
- Basis in paper: The paper focuses on MedQA-USMLE and StrategyQA datasets but acknowledges the need to test on additional datasets in the limitations section
- Why unresolved: Current study only evaluates KARD on two specific datasets, leaving generalizability to other knowledge-intensive tasks uncertain
- What evidence would resolve it: Experiments on diverse knowledge-intensive reasoning tasks like scientific reasoning, legal reasoning, or historical analysis

### Open Question 2
- Question: What is the impact of using different retriever architectures (e.g., dense vs. sparse) on KARD performance?
- Basis in paper: The paper uses BM25 and mentions DPR led to degraded performance, but suggests contriever might offer greater benefits
- Why unresolved: Study only explores BM25 and briefly mentions DPR, leaving potential benefits of other retriever architectures unexplored
- What evidence would resolve it: Systematic comparison of KARD using various retriever architectures on the same datasets

### Open Question 3
- Question: How does the size of the external knowledge base affect KARD performance, and is there an optimal size for different model sizes?
- Basis in paper: Theoretical analysis suggests knowledge augmentation reduces memorization requirements, but practical impact of varying knowledge base sizes is not explored
- Why unresolved: Paper uses fixed knowledge base (Wikipedia) without investigating effects of different knowledge base sizes
- What evidence would resolve it: Experiments with varying sizes of external knowledge bases and measuring performance across different model sizes

## Limitations
- The method relies heavily on the quality of LLM-generated rationales, which may contain errors or inconsistencies
- Performance depends on the completeness and relevance of the external knowledge base (Wikipedia)
- The approach may struggle with questions requiring highly specialized knowledge not well-represented in the knowledge base

## Confidence
- Memorization reduction via retrieval: Low confidence (weak corpus evidence)
- Rationale-guided reranking effectiveness: Medium confidence (supported by paper claims but limited external validation)
- Multiple rationales improving generalization: Medium confidence (some empirical support in paper but limited external evidence)

## Next Checks
1. Ablation study on reranker necessity: Run experiments comparing KARD with and without the neural reranker to quantify its actual contribution to performance improvements.

2. Knowledge base dependency test: Evaluate performance when using different knowledge bases (e.g., domain-specific vs. general Wikipedia) to determine how critical the external knowledge component is.

3. Generalization assessment: Test KARD on additional knowledge-intensive reasoning tasks beyond MedQA-USMLE and StrategyQA to evaluate whether the method generalizes across domains.