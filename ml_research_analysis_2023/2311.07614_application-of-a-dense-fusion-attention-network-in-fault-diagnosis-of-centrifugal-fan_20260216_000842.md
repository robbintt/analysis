---
ver: rpa2
title: Application of a Dense Fusion Attention Network in Fault Diagnosis of Centrifugal
  Fan
arxiv_id: '2311.07614'
source_url: https://arxiv.org/abs/2311.07614
tags:
- feature
- attention
- network
- fault
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Dense Fusion Attention Network (DFANet) for
  centrifugal fan fault diagnosis. The core innovation is the Dispersed Attention
  Module (DAM), which embeds distributed attention mechanisms into dense connections
  to replace traditional dense cascading operations.
---

# Application of a Dense Fusion Attention Network in Fault Diagnosis of Centrifugal Fan

## Quick Facts
- arXiv ID: 2311.07614
- Source URL: https://arxiv.org/abs/2311.07614
- Reference count: 31
- Key outcome: DFANet achieves 89.13% accuracy on centrifugal fan fault diagnosis under severe noise (SNR=-8dB), outperforming five advanced models by 5.17-14.66%

## Executive Summary
This paper proposes DFANet, a Dense Fusion Attention Network for centrifugal fan fault diagnosis. The core innovation is the Dispersed Attention Module (DAM), which replaces traditional dense cascading operations with distributed attention mechanisms. DAM decouples spatial and channel attention to prevent mutual interference in feature weight calibration, then combines them through residual connections. Experimental results show DFANet achieves 89.13% accuracy under severe noise (SNR=-8dB), significantly outperforming five advanced fault diagnostic models while providing interpretable visualization of the diagnosis process.

## Method Summary
DFANet uses a Dense Fusion Block structure containing the Dispersed Attention Module (DAM), which processes spatial and channel attention separately in parallel paths before combining them with residual connections. The architecture cascades three Dense Fusion Blocks with decreasing feature map sizes (128→64→32), each containing two DAM blocks. DAM integrates Zoomed Spatial Self-Attention Module (ZSSAM) for multi-scale spatial feature extraction and Channel Attention Module (CAM) for channel-wise feature weighting. The model is trained on centrifugal fan vibration data across 9 fault states with Bayesian optimization for hyperparameter tuning.

## Key Results
- DFANet achieves 89.13% accuracy on centrifugal fan fault diagnosis under severe noise (SNR=-8dB)
- Outperforms five advanced fault diagnostic models by 5.17-14.66% accuracy margins
- Provides interpretable visualization of diagnosis process through attention mechanism visualization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DAM decouples spatial and channel attention to prevent mutual interference in feature weight calibration
- Mechanism: DAM processes spatial and channel attention separately in parallel paths, then combines outputs with residual connections
- Core assumption: Linear combination of spatial and channel attention causes mutual interference
- Evidence anchors: [abstract], [section] mentions decoupling to avoid failure of weight activation
- Break condition: If spatial and channel features are highly correlated, separate processing may lose important cross-dimension relationships

### Mechanism 2
- Claim: DF-Block with DAM achieves multiple mixed attention through cascaded dense connections
- Mechanism: DF-Block cascades DAM blocks where each block's output features combine with previous features
- Core assumption: Cascading attention modules provides cumulative feature refinement benefits
- Evidence anchors: [section] mentions features being noted again in next operation, [abstract] mentions visualization of diagnosis process
- Break condition: If feature refinement saturates quickly, additional DAM blocks may add computational cost without benefit

### Mechanism 3
- Claim: ZSSAM enhances spatial attention through multi-scale feature mapping
- Mechanism: ZSSAM uses narrow convolution kernels with decreasing sizes and up-sampling operations for multi-scale spatial features
- Core assumption: Multi-scale spatial feature extraction improves attention to important spatial patterns
- Evidence anchors: [section] mentions ZSSAM uses zoomed mapping to weaken background information, [abstract] mentions decoupling influence of space and channel
- Break condition: If vibration signal patterns don't benefit from multi-scale spatial analysis, computational overhead may not justify benefit

## Foundational Learning

- Concept: Attention mechanisms in neural networks
  - Why needed here: Uses spatial and channel attention to focus on important fault features in vibration signals
  - Quick check question: What is the difference between spatial attention and channel attention in neural networks?

- Concept: Dense connections in neural networks
  - Why needed here: DF-Block uses dense connections to combine features from multiple DAM blocks
  - Quick check question: How do dense connections differ from residual connections in neural networks?

- Concept: Vibration signal processing for fault diagnosis
  - Why needed here: Applies DFANet to centrifugal fan vibration data for fault diagnosis
  - Quick check question: What are typical characteristics of vibration signals from rotating machinery with faults?

## Architecture Onboarding

- Component map: Input layer → Conv1D → MaxPooling1D → DF-Block (×3) → GlobalAveragePooling1D → Softmax
- DF-Block contains: DAM (containing ZSSAM and CAM) → BN → f → Conv1D → MaxPooling1D
- DAM processes features through parallel ZSSAM and CAM, then combines with residual connections

- Critical path: Input → Conv1D → MaxPooling → DF-Block_1 → Connection → DF-Block_2 → Connection → DF-Block_3 → GlobalAveragePooling → Softmax

- Design tradeoffs:
  - DAM provides better feature calibration but adds computational complexity
  - Dense connections enable feature reuse but increase parameter count
  - Multi-scale ZSSAM improves spatial attention but requires additional convolution operations

- Failure signatures:
  - Overfitting: Training accuracy much higher than validation accuracy
  - Underfitting: Both training and validation accuracy remain low
  - Poor convergence: Loss plateaus early or oscillates

- First 3 experiments:
  1. Test DAM effectiveness: Compare DFANet with DenseNet (same architecture without DAM) on centrifugal fan data
  2. Test attention component contributions: Compare DFANet with variants containing only ZSSAM, only CAM, or linear attention
  3. Test noise robustness: Evaluate model performance on data with varying SNR levels (-8dB to 0dB)

## Open Questions the Paper Calls Out

- How does DFANet architecture generalize to other types of rotating machinery beyond centrifugal fans, such as pumps or turbines?
- What is the optimal number of DF-Blocks and DAM blocks for different fault diagnosis scenarios, and how does this vary with signal complexity and noise levels?
- How does DAM module's performance compare to other attention mechanisms (like transformer-based approaches) for fault diagnosis in rotating machinery?

## Limitations

- Decoupling mechanism between spatial and channel attention lacks quantitative evidence showing interference effects in traditional dense connections
- Multi-scale spatial attention benefits for vibration signal patterns are assumed but not empirically validated against simpler attention mechanisms
- Cumulative benefit of cascading DAM blocks may exhibit diminishing returns not captured in reported results

## Confidence

- High confidence: Noise robustness improvements (SNR=-8dB achieving 89.13% accuracy) - directly supported by experimental results
- Medium confidence: DAM's decoupling mechanism - theoretically sound but lacks ablation studies isolating interference effects
- Medium confidence: Dense fusion benefits - reported improvements are significant but could partly stem from increased model capacity

## Next Checks

1. Ablation study isolating DAM's spatial-channel decoupling: Compare performance with and without separate spatial/channel processing while controlling for model complexity
2. Multi-scale attention validation: Test ZSSAM against single-scale spatial attention variants on the same centrifugal fan dataset
3. Dense connection saturation analysis: Evaluate performance across different numbers of DAM blocks to identify diminishing returns points