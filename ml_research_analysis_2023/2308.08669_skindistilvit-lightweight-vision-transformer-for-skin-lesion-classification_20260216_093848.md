---
ver: rpa2
title: 'SkinDistilViT: Lightweight Vision Transformer for Skin Lesion Classification'
arxiv_id: '2308.08669'
source_url: https://arxiv.org/abs/2308.08669
tags:
- skindistilvit
- performance
- transformer
- classification
- skin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses skin lesion classification using a lightweight
  Vision Transformer (ViT) model, achieving performance matching human dermatologists
  in melanoma identification. The proposed SkinDistilViT employs knowledge distillation
  to create a model that retains 98.33% of the teacher's balanced multi-class accuracy
  while being 49.60% smaller and 69.25% faster on GPU.
---

# SkinDistilViT: Lightweight Vision Transformer for Skin Lesion Classification

## Quick Facts
- arXiv ID: 2308.08669
- Source URL: https://arxiv.org/abs/2308.08669
- Reference count: 25
- Primary result: Lightweight Vision Transformer matching dermatologist performance in melanoma identification

## Executive Summary
SkinDistilViT is a compressed Vision Transformer model for skin lesion classification that achieves human-level performance in melanoma detection. The model uses knowledge distillation to transfer knowledge from a larger teacher model to a smaller student model, resulting in a 49.60% reduction in size and 69.25% increase in speed while retaining 98.33% of the teacher's balanced multi-class accuracy. The model demonstrates superior performance to convolutional neural networks in handling class imbalance in skin lesion datasets.

## Method Summary
The method employs knowledge distillation with a ViT teacher model to create a compressed student model (SkinDistilViT). The training process uses a combination of task loss, cross-entropy loss between teacher and student outputs, and cosine loss between hidden states. The authors also introduce three full distillation techniques (FCViT, FCViTProbs, and Cascading Distillation) that add classification heads at different transformer layers to preserve information across the network. The model is trained on the ISIC 2019 dataset and evaluated using balanced multi-class accuracy, accuracy, precision, recall, and F1-score.

## Key Results
- Achieves 82.34% balanced multi-class accuracy, 88.51% accuracy, and 88.34% precision
- Retains 98.33% of teacher's balanced multi-class accuracy while being 49.60% smaller and 69.25% faster on GPU
- Demonstrates superior performance to CNNs in handling class imbalance in skin lesion datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SkinDistilViT retains 98.33% of the teacher's balanced multi-class accuracy while being 49.60% smaller and 69.25% faster on GPU.
- Mechanism: Knowledge distillation transfers the learned representations from a larger ViT model to a smaller compressed version by matching outputs and hidden states during training.
- Core assumption: The student model can effectively learn from the teacher's probability distributions and hidden states without access to the full model capacity.
- Evidence anchors:
  - [abstract] "we employ knowledge distillation to obtain a model that retains 98.33% of the teacher's balanced multi-class accuracy, at a fraction of the cost."
  - [section 3.1] "Similar to DistilBERT, we use a mix of losses for fine-tuning our distilled model. Besides the original training objective, we use a cross-entropy loss between the teacher's and the student's outputs and a cosine loss between their hidden states."

### Mechanism 2
- Claim: The attention mechanism in ViT models performs better than CNNs on imbalanced skin lesion datasets.
- Mechanism: ViT's self-attention focuses processing on relevant regions (skin lesions) while CNNs rely more on texture patterns that can be confused by the presence of skin in all images.
- Core assumption: The lesion region contains the discriminative features needed for classification, and attention can effectively isolate these regions.
- Evidence anchors:
  - [section 5.1] "We suspect this stems from the filter-based nature of the CNNs, which makes them more reliant on the image's texture... The better behavior of ViT can be explained by the attention mechanism, which only ensures the processing of the relevant part of the image."
  - [section 5.1] "The attention mechanism in action can be observed in Figure 1, where the model only pays attention to the skin lesion."

### Mechanism 3
- Claim: Full distillation techniques (FCViT, FCViTProbs, Cascading Distillation) create a family of models with varying sizes but comparable performance.
- Mechanism: By adding classification heads at different transformer layers and using cascading distillation, the model learns to preserve information across all layers, allowing smaller versions to maintain performance.
- Core assumption: Each transformer layer captures complementary information, and preserving this information through distillation allows smaller models to retain most capabilities.
- Evidence anchors:
  - [section 3.2] "We study a ViT model that outputs a prediction for the class at every stage by adding an independent prediction head at each of them."
  - [section 5.2] "Cascading Distillation ViT obtains the best results at all levels, but it shines in preserving the performance at a lower number of layers."

## Foundational Learning

- Concept: Vision Transformers
  - Why needed here: ViT processes images as sequences of patches using self-attention, which is crucial for capturing global context in skin lesion images.
  - Quick check question: How does ViT's patch-based approach differ from traditional CNN convolution operations?

- Concept: Knowledge Distillation
  - Why needed here: Allows transferring knowledge from a large teacher model to a smaller student model while maintaining most of the performance.
  - Quick check question: What are the key loss components used in knowledge distillation for this paper?

- Concept: Class Imbalance Handling
  - Why needed here: Skin lesion datasets have severe class imbalance, and the model must maintain performance across all classes.
  - Quick check question: Which metric does the paper use to evaluate performance on imbalanced datasets?

## Architecture Onboarding

- Component map:
  Teacher ViT model (base model) -> Student SkinDistilViT with reduced transformer blocks -> Loss functions: task loss, distillation loss, hidden state cosine loss -> Full distillation variants with multiple classification heads -> Cascading distillation process

- Critical path:
  1. Train base ViT on skin lesion dataset
  2. Create student model by removing transformer blocks
  3. Initialize student weights from teacher
  4. Apply knowledge distillation training
  5. Optionally apply full distillation techniques

- Design tradeoffs:
  - Model size vs. performance: Smaller models are faster but may lose some accuracy
  - Number of distillation losses: More losses can improve performance but increase training complexity
  - Transfer learning vs. training from scratch: Fine-tuning pre-trained models converges faster

- Failure signatures:
  - Performance degradation when removing too many transformer blocks
  - Training instability if distillation loss weights are not properly balanced
  - Overfitting on small datasets if augmentation is insufficient

- First 3 experiments:
  1. Train baseline ViT on skin lesion dataset and evaluate performance
  2. Create SkinDistilViT by removing half the transformer blocks and apply knowledge distillation
  3. Implement and test one full distillation technique (e.g., Cascading Distillation) and compare results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would SkinDistilViT perform on a more balanced dataset?
- Basis in paper: [explicit] The authors note that their model is "robust to [class imbalance]" but suggest it "might benefit from a balanced dataset" as future work.
- Why unresolved: The current study uses a highly imbalanced dataset, so the impact of class balance on performance remains untested.
- What evidence would resolve it: Training and evaluating SkinDistilViT on a dataset with more equal class distribution, comparing results to the current imbalanced dataset.

### Open Question 2
- Question: What is the impact of combining all three full distillation techniques (FCViT, FCViTProbs, and Cascading Distillation)?
- Basis in paper: [explicit] The authors propose this combination as future work, stating "As future work, we propose combining the three full distillation techniques."
- Why unresolved: The individual effectiveness of each technique is demonstrated, but their combined effect is unknown.
- What evidence would resolve it: Implementing and evaluating a model using all three techniques simultaneously, comparing its performance to models using individual techniques.

### Open Question 3
- Question: How does SkinDistilViT's attention mechanism specifically contribute to its superior performance on imbalanced datasets compared to CNNs?
- Basis in paper: [inferred] The authors suggest that the attention mechanism allows ViT to "only ensure the processing of the relevant part of the image," which may explain its better performance on imbalanced classes.
- Why unresolved: While the authors hypothesize about the attention mechanism's role, they do not provide direct evidence or analysis of its specific contribution.
- What evidence would resolve it: Analyzing the attention maps of SkinDistilViT on imbalanced datasets, comparing them to CNN feature maps, and correlating attention patterns with classification accuracy for each class.

## Limitations
- Claims about matching human dermatologist performance are not directly supported by experimental data
- Results rely on ISIC 2019 dataset which may not represent diverse patient populations
- Generalization to clinical settings requires further validation

## Confidence
- High confidence: The mechanism of knowledge distillation for model compression is well-established and the reported size reduction with speed improvement is technically plausible
- Medium confidence: The claim that ViT outperforms CNNs on imbalanced skin lesion data is supported by experimental results but requires further validation
- Medium confidence: The full distillation techniques show promising results, but the cascading approach's superiority is based on a single dataset

## Next Checks
1. External dataset validation: Test SkinDistilViT on the ISIC 2020 or 2021 datasets to verify that the performance gains hold across different data distributions and acquisition protocols
2. Clinical comparison study: Conduct a controlled study comparing SkinDistilViT's performance against multiple dermatologists on a held-out test set with challenging cases
3. Ablation study on distillation components: Systematically remove individual distillation loss components to quantify their individual contributions to the final performance and identify potential overfitting to the training distribution