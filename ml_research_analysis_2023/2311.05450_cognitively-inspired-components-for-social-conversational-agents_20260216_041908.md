---
ver: rpa2
title: Cognitively Inspired Components for Social Conversational Agents
arxiv_id: '2311.05450'
source_url: https://arxiv.org/abs/2311.05450
tags:
- social
- memory
- which
- available
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a survey highlighting how cognitively inspired
  components can address key problems in conversational agents (CAs). Two categories
  of problems are identified: technical issues such as limited scope in retrieval
  agents and nonsensical responses in generative agents, and social problems stemming
  from the expectation that CAs adhere to social conventions.'
---

# Cognitively Inspired Components for Social Conversational Agents

## Quick Facts
- arXiv ID: 2311.05450
- Source URL: https://arxiv.org/abs/2311.05450
- Authors: 
- Reference count: 40
- Key outcome: Proposes five cognitively inspired components to address technical and social problems in conversational agents, potentially improving response quality and social capability.

## Executive Summary
This survey identifies key problems in conversational agents, including limited retrieval scope, nonsensical generative responses, and social expectation failures. It proposes five cognitively inspired components—semantic memory, episodic memory, emotional mimicry, working memory, and learning ability—as a unified solution. These components could provide contextual grounding, personal experience integration, emotional awareness, and adaptive learning to improve both technical response quality and social capability. The paper presents a theoretical framework rather than empirical results, suggesting that integrating these cognitive facsimiles could create more capable and believable conversational agents.

## Method Summary
The paper synthesizes existing literature on conversational agent limitations and cognitive modeling approaches to propose a theoretical framework. It identifies two categories of problems (technical and social) and maps them to potential solutions through cognitively inspired components. The method involves reviewing current CA architectures, analyzing cognitive science literature on human memory and emotion systems, and proposing integration mechanisms for these components with existing LLM-based systems. The approach is conceptual, focusing on identifying viable pathways for implementation rather than providing specific algorithms or experimental validation.

## Key Results
- Identifies five cognitively inspired components that could address both technical quality issues and social expectation failures in conversational agents
- Proposes semantic and episodic memory integration as solution for contextually grounded responses and personal experience incorporation
- Suggests emotional mimicry and working memory as mechanisms for improving social capability and conversation continuity
- Presents learning ability as means for agents to improve through interaction without retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic and episodic memory components enable contextually grounded responses by storing both factual and experiential knowledge.
- Mechanism: The system uses a knowledge base with semantic memory (knowledge graphs or Bayesian networks) to store facts, and episodic memory to store personal experiences. When generating responses, the agent retrieves relevant facts and experiences to construct coherent, personalized answers.
- Core assumption: Human-like memory structures can be computationally modeled and effectively integrated with existing LLM-based response generation.
- Evidence anchors:
  - [abstract] "Through computational facsimiles of semantic and episodic memory... it is possible to address both the technical and social problems encountered by CAs."
  - [section] "KGs have also attracted attention in recent years as a way to more closely mimic the structure of human knowledge."
  - [corpus] No direct evidence for memory integration with CAs in corpus, weak evidence.
- Break condition: If the memory retrieval system cannot efficiently match relevant memories to conversation context, responses become incoherent or overly generic.

### Mechanism 2
- Claim: Emotional mimicry and empathy simulation reduce user discomfort and increase perceived social capability of the agent.
- Mechanism: The agent uses VAD (valence, arousal, dominance) scores and affect control theory to generate emotionally appropriate responses. Episodic memory combined with emotional context enables perceived empathy by recalling relevant past interactions and responding with appropriate emotional framing.
- Core assumption: Users will perceive the agent as more socially capable when it demonstrates emotional awareness and empathy, even if simulated.
- Evidence anchors:
  - [abstract] "Through computational facsimiles of... emotion... it is possible to address both the technical and social problems encountered by CAs."
  - [section] "A correlation has been shown between opportunities to empathise and pro-social behaviour in humans."
  - [corpus] No direct evidence for empathy simulation effectiveness in corpus, weak evidence.
- Break condition: If emotional responses feel inauthentic or mismatched to conversation context, users may experience increased discomfort or distrust.

### Mechanism 3
- Claim: Working memory and learning components maintain conversation context and enable adaptation, reducing context loss and improving response quality over time.
- Mechanism: Working memory holds current conversation state and processes information for later integration into long-term knowledge bases. Learning from interactions allows the agent to incorporate new information and adapt its responses, improving performance without retraining.
- Core assumption: Maintaining conversation context and learning from interactions will lead to more coherent, personalized responses and improved user experience.
- Evidence anchors:
  - [abstract] "Through computational facsimiles of... working memory, and the ability to learn... it is possible to address both the technical and social problems encountered by CAs."
  - [section] "Learning cumulatively from each version of the task, the robot was able to determine what factors were important on its own, therefore seemingly learning from its experience."
  - [corpus] No direct evidence for working memory implementation in CAs in corpus, weak evidence.
- Break condition: If the learning system introduces noise or irrelevant information into the knowledge base, response quality may degrade over time.

## Foundational Learning

- Concept: Knowledge Graph Embeddings (KGE)
  - Why needed here: KGEs provide a way to represent semantic relationships in low-dimensional vectors, enabling efficient similarity-based retrieval for generating contextually appropriate responses.
  - Quick check question: How does TransE ensure that the head node embedding is near the tail node embedding in a knowledge graph?

- Concept: Encoder-Decoder Framework with Attention
  - Why needed here: The encoder-decoder architecture with attention mechanisms allows the agent to generate coherent responses by focusing on relevant parts of the input sequence and context.
  - Quick check question: What is the mathematical representation of the attention mechanism that allows other tokens to influence how the current token is interpreted?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF can be used to fine-tune the agent's responses based on human preferences, improving alignment with social expectations and user satisfaction.
  - Quick check question: How does the Proximal Policy Optimization (PPO) algorithm use scalar rewards to optimize the agent's response generation?

## Architecture Onboarding

- Component map:
  - Knowledge Base: Contains semantic memory (facts via KGs), episodic memory (experiences), and emotional states
  - Working Memory: Sub-component holding current session information and processing for later integration
  - Generative Model: Produces responses using context from knowledge base and working memory
  - Emotional State Handler: Augments responses with appropriate emotional framing
  - Learning Module: Updates knowledge base based on interactions and user feedback

- Critical path:
  1. User input → Working memory stores current context
  2. Retrieve relevant semantic and episodic memories from knowledge base
  3. Generate response using generative model with retrieved context
  4. Apply emotional framing using emotional state handler
  5. Update working memory and knowledge base based on response and feedback

- Design tradeoffs:
  - Memory size vs. retrieval efficiency: Larger memory stores more context but increases retrieval time
  - Emotional complexity vs. computational cost: More nuanced emotions improve user experience but require more processing
  - Learning rate vs. stability: Faster learning adapts quicker but may introduce instability or noise

- Failure signatures:
  - Context loss: Responses become generic or off-topic
  - Emotional mismatch: Responses feel inappropriate or robotic
  - Knowledge degradation: Responses become less accurate over time due to poor learning integration

- First 3 experiments:
  1. Implement basic knowledge graph with semantic memory and test retrieval accuracy for factual questions
  2. Add episodic memory component and test ability to recall and reference past interactions
  3. Integrate emotional state handler with basic VAD scoring and test user perception of emotional appropriateness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the integration of semantic memory, episodic memory, emotional mimicry, working memory, and learning components significantly improve the technical quality of responses from conversational agents?
- Basis in paper: [explicit] The paper proposes these five cognitively inspired components as a potential solution to address technical problems such as limited scope in retrieval agents and nonsensical responses in generative agents.
- Why unresolved: The paper discusses the theoretical benefits of these components but does not provide empirical evidence or experimental results to demonstrate their effectiveness in improving response quality.
- What evidence would resolve it: Empirical studies comparing conversational agents with and without these cognitively inspired components, measuring metrics such as response coherence, relevance, and user satisfaction.

### Open Question 2
- Question: How can conversational agents effectively balance anthropomorphism and believability to avoid negative user reactions while meeting social expectations?
- Basis in paper: [explicit] The paper discusses the problems of anthropomorphism and believability, noting that while human-like characteristics can increase believability, they can also lead to discomfort or perceived threats if not handled correctly.
- Why unresolved: The paper highlights the challenges but does not provide a clear framework or guidelines for achieving the right balance between anthropomorphism and believability.
- What evidence would resolve it: Experimental studies testing different levels of anthropomorphism and believability in conversational agents, measuring user comfort, trust, and interaction quality.

### Open Question 3
- Question: What is the most effective computational representation for integrating semantic memory, episodic memory, emotional mimicry, working memory, and learning into a conversational agent?
- Basis in paper: [inferred] The paper suggests that these components could be integrated through a generative model, knowledge base, and emotional state handling, but does not specify the exact implementation details.
- Why unresolved: The paper presents a high-level overview of the potential integration but does not provide specific algorithms or architectures for implementing these components.
- What evidence would resolve it: Development and testing of conversational agents with different computational representations of these components, comparing their performance in terms of response quality and user interaction.

## Limitations
- Theoretical framework without empirical validation of proposed cognitive components
- Does not address computational overhead and scalability challenges of multi-component integration
- Lacks specific implementation details for combining all five cognitive components into unified architecture

## Confidence

- High confidence: The identification of technical and social problems in current conversational agents is well-established and supported by existing literature.
- Medium confidence: The proposed cognitive components (semantic memory, episodic memory, emotional mimicry, working memory, learning) are theoretically sound and have been successfully implemented in narrow domains.
- Low confidence: The integration of all five components into a unified conversational agent architecture, and the resulting improvements in both technical quality and social capability, remain largely untested hypotheses without empirical validation.

## Next Checks

1. Implement a minimal prototype integrating semantic memory with a basic conversational agent and measure response quality improvements on standardized dialogue datasets compared to baseline models.
2. Conduct user studies comparing emotional mimicry approaches (VAD-based vs. sentiment analysis) to determine which produces more authentic-feeling interactions and better user satisfaction.
3. Design and execute a longitudinal study tracking a conversational agent's learning system over multiple interaction sessions to identify knowledge degradation patterns and stability issues.