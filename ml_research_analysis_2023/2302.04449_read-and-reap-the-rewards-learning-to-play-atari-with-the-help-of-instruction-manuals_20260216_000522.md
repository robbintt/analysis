---
ver: rpa2
title: 'Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction
  Manuals'
arxiv_id: '2302.04449'
source_url: https://arxiv.org/abs/2302.04449
tags:
- arxiv
- game
- reward
- preprint
- atari
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a framework called "Read and Reward" to address
  the challenge of high sample complexity in reinforcement learning (RL) by leveraging
  human-written instruction manuals. The framework consists of two main components:
  a QA Extraction module that extracts relevant information from instruction manuals
  using extractive question answering, and a Reasoning module that assigns auxiliary
  rewards based on the extracted information.'
---

# Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals

## Quick Facts
- arXiv ID: 2302.04449
- Source URL: https://arxiv.org/abs/2302.04449
- Reference count: 17
- Key outcome: Achieves up to 60% performance improvement with 1000x fewer training frames on Skiing game

## Executive Summary
This paper introduces the Read and Reward framework to address the high sample complexity challenge in reinforcement learning by leveraging human-written instruction manuals. The framework combines extractive question answering with zero-shot reasoning from large language models to provide auxiliary rewards that guide RL agents. Experiments show significant improvements in both performance and training speed across multiple Atari games, particularly in scenarios with sparse rewards.

## Method Summary
The Read and Reward framework consists of two main components: a QA Extraction module that uses RoBERTa-large fine-tuned on SQUAD to extract relevant information from instruction manuals, and a Reasoning module that employs Macaw LLM to generate auxiliary rewards based on detected object interactions. The framework processes instruction manuals by splitting them into chunks, extracting relevant text using targeted questions, and using the extracted information to reason about beneficial or harmful interactions. These auxiliary rewards are then provided to a standard A2C RL agent alongside environment rewards.

## Key Results
- Up to 60% performance improvement with 1000x fewer training frames on Skiing game
- Framework generalizes well to instruction manuals from different sources (Wikipedia, official manuals)
- Significant improvement in sample efficiency across multiple Atari games (Tennis, Ms. Pac-Man, Breakout)
- Achieves performance competitive with state-of-the-art Agent 57 while requiring fewer training frames

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extracting and summarizing relevant information from instruction manuals reduces input complexity for reasoning
- Mechanism: QA Extraction module uses RoBERTa-large fine-tuned on SQUAD to extract text sub-sequences as answers to targeted questions
- Core assumption: Small set of targeted questions can capture all relevant information from 2-3 page manuals
- Evidence anchors: [abstract] "First, a zero-shot extractive QA module is used to extract and summarize relevant information"
- Break condition: If extracted information misses critical game mechanics, reasoning module cannot provide useful auxiliary rewards

### Mechanism 2
- Claim: Zero-shot reasoning with LLM can generate auxiliary rewards based on extracted manual content and in-game interactions
- Mechanism: Reasoning module uses Macaw LLM to score "Yes/No" answers about object interactions, mapping to Â±5 auxiliary rewards
- Core assumption: LLM can accurately reason about implicit relationships in manual text with proper context
- Evidence anchors: [abstract] "Second, a zero-shot reasoning module...reasons about contexts...and assigns auxiliary rewards"
- Break condition: If LLM misinterprets context or fails to capture implicit relationships, incorrect auxiliary rewards assigned

### Mechanism 3
- Claim: Auxiliary rewards based on manual information can significantly speed up RL training
- Mechanism: Framework detects object interactions (e.g., agent hitting obstacle) and provides auxiliary rewards based on LLM reasoning
- Core assumption: Manual accurately describes beneficial/harmful interactions generalizable to game environment
- Evidence anchors: [abstract] "An auxiliary reward is then provided to a standard A2C RL agent, when interaction is detected"
- Break condition: If auxiliary rewards conflict with true environment rewards or are too frequent, may mislead agent

## Foundational Learning

- Concept: Reinforcement Learning and sparse rewards
  - Why needed here: Framework aims to address sample efficiency problem in RL by providing auxiliary rewards where environment rewards are sparse
  - Quick check question: What is the main challenge that Read and Reward framework tries to address in RL?

- Concept: Question Answering and text summarization
  - Why needed here: QA Extraction module needs to efficiently extract relevant information from long instruction manuals
  - Quick check question: How does the framework handle instruction manuals longer than maximum input length of QA model?

- Concept: Large Language Models and zero-shot reasoning
  - Why needed here: Reasoning module uses LLM to generate auxiliary rewards based on extracted manual content and in-game interactions
  - Quick check question: Which zero-shot QA model is used for reasoning in Read and Reward framework?

## Architecture Onboarding

- Component map:
  QA Extraction Module -> Reasoning Module -> Object Detection & Grounding -> RL Agent

- Critical path:
  1. Read instruction manual
  2. Extract relevant information using QA Extraction Module
  3. Detect object interactions in game
  4. Reason about interactions using extracted information and LLM
  5. Provide auxiliary rewards to RL agent
  6. RL agent learns from both environment and auxiliary rewards

- Design tradeoffs:
  - Smaller, faster LLM (Macaw) vs. larger, more capable one (GPT-3)
  - Extracting information vs. using full manual as context
  - Detecting only "hit" interactions vs. more complex interaction types
  - Using auxiliary rewards vs. modifying environment reward function

- Failure signatures:
  - RL agent fails to improve: Auxiliary rewards may be incorrect or misaligned with environment rewards
  - High variance in training: Auxiliary rewards may be too noisy or frequent
  - Slow improvement: QA Extraction may not capture all relevant information
  - Poor generalization: LLM reasoning may not generalize to new games or manual formats

- First 3 experiments:
  1. Run full pipeline on Skiing game with ground-truth object localization to verify core concept
  2. Test framework on simple game with delayed rewards (e.g., Breakout) to measure sample efficiency improvement
  3. Evaluate framework's ability to generalize to instruction manuals from different sources

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can framework be extended to handle more complex interactions beyond simple "hit" events?
- Basis in paper: [explicit] Paper mentions only considering "hit" interactions but acknowledges framework could potentially handle more complex interactions
- Why unresolved: Paper does not explore or demonstrate framework's ability to handle more complex interactions
- What evidence would resolve it: Experimental results on tasks requiring complex interactions (object manipulation, multi-step reasoning)

### Open Question 2
- Question: How does performance scale with complexity and length of instruction manuals?
- Basis in paper: [inferred] Paper mentions QA Extraction addresses length challenge but doesn't analyze performance across varying manual characteristics
- Why unresolved: No systematic study of performance across manuals of varying complexity and length
- What evidence would resolve it: Experimental results showing performance on tasks with manuals of varying complexity and length

### Open Question 3
- Question: What are the limitations and potential failure modes of the Read and Reward framework?
- Basis in paper: [implicit] While paper discusses some limitations, a comprehensive analysis of failure modes is missing
- Why unresolved: Paper does not provide a dedicated section analyzing limitations and failure modes
- What evidence would resolve it: A dedicated section analyzing potential failure modes, limitations, and their impact on framework performance

## Limitations

- Assumption: Instruction manuals accurately describe game mechanics that are generalizable to the actual game environment
- Assumption: The framework's ability to handle more complex interactions beyond simple "hit" events is unexplored
- Assumption: Performance across instruction manuals of varying complexity and length is not systematically studied
- Assumption: Potential failure modes and limitations of the framework are not comprehensively analyzed

## Confidence

Medium: The paper presents a novel approach to improving RL sample efficiency by leveraging instruction manuals. However, some claims about the framework's ability to handle complex interactions and performance across varying manual characteristics are not fully supported by experimental evidence.

## Next Checks

1. Verify the accuracy of extracted information from instruction manuals by comparing with ground truth game mechanics
2. Test the framework's ability to handle more complex interactions beyond simple "hit" events
3. Evaluate performance across instruction manuals of varying complexity and length
4. Analyze potential failure modes and limitations of the framework
5. Compare performance with other state-of-the-art methods that leverage external knowledge for RL