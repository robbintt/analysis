---
ver: rpa2
title: On the Foundations of Shortcut Learning
arxiv_id: '2310.16228'
source_url: https://arxiv.org/abs/2310.16228
tags:
- feature
- availability
- shortcut
- features
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Deep learning models rely on features that are both predictive
  and available. This work shows that availability can override predictivity, leading
  to shortcut learning.
---

# On the Foundations of Shortcut Learning

## Quick Facts
- arXiv ID: 2310.16228
- Source URL: https://arxiv.org/abs/2310.16228
- Reference count: 40
- Key outcome: Deep learning models exhibit shortcut bias, preferring more available features even when less predictive, which can be explained through Neural Tangent Kernel analysis.

## Executive Summary
This paper investigates how deep learning models choose which features to rely on when making predictions. While traditional analyses focus on feature predictivity (how well a feature indicates labels), this work shows that feature availability (how easily a feature can be extracted) is equally important. Through a combination of synthetic data experiments, theoretical analysis using Neural Tangent Kernels, and studies on naturalistic datasets, the authors demonstrate that nonlinear deep learning models systematically prefer more available features even when they are less predictive. This shortcut bias is shown to be an inevitable consequence of nonlinear architectures and can be manipulated through controlled changes to feature availability.

## Method Summary
The paper introduces a generative framework for synthesizing datasets with latent features that vary in predictivity and availability. Two factors control availability: amplification (feature amplitude) and nesting (how a feature is embedded within inputs). The authors train both linear and nonlinear models (MLPs and ResNets) on these datasets and measure shortcut bias by comparing feature reliance to an optimal LDA classifier. A theoretical analysis using Neural Tangent Kernels provides insight into why nonlinear models exhibit shortcut bias. The framework is validated on naturalistic datasets (Waterbirds and CelebA) where availability is manipulated through image properties like spatial extent, color, and frequency.

## Key Results
- Linear models show little shortcut bias, while introducing a single hidden layer with ReLU or Tanh induces significant bias
- Model depth amplifies shortcut bias, with deeper networks showing stronger preferences for available features
- Manipulating feature availability in naturalistic datasets (e.g., background spatial extent, color) influences model reliance patterns
- Theoretical NTK analysis shows that eigenfunctions corresponding to more available features have higher eigenvalues, explaining the bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep nonlinear models prefer features that are easier to extract, even if they are less predictive.
- Mechanism: The availability of a feature (how easily it can be extracted) interacts with its predictivity (how well it indicates labels) to determine model reliance. Nonlinear architectures amplify this bias.
- Core assumption: Availability can override predictivity in determining feature reliance.
- Evidence anchors:
  - [abstract]: "Deep-learning models can extract a rich assortment of features from data. Which features a model uses depends not only on predictivity—how reliably a feature indicates training-set labels—but also on availability—how easily the feature can be extracted, or leveraged, from inputs."
  - [section 3]: "We find that linear models are relatively unbiased, but introducing a single hidden layer with ReLU or Tanh units yields a bias."
  - [corpus]: Weak - only mentions shortcut learning broadly, no specific mechanism details.
- Break condition: If the model architecture is linear or if the availability of features is equalized.

### Mechanism 2
- Claim: The Neural Tangent Kernel (NTK) of nonlinear models inherently biases them toward more available features.
- Mechanism: The NTK spectrum of ReLU networks shows that the eigenfunctions corresponding to more available features have higher eigenvalues, leading to stronger influence during learning.
- Core assumption: The NTK framework accurately captures the learning dynamics of deep nonlinear networks.
- Evidence anchors:
  - [section 6]: "We present a theoretical account based on Neural Tangent Kernels (Jacot et al., 2018) which indicates that shortcut bias is an inevitable consequence of nonlinear architectures."
  - [section 6]: "The following theorem characterizes the spectrum of this kernel."
  - [corpus]: Weak - only mentions NTK in the context of shortcut features, not the specific mechanism.
- Break condition: If the NTK approximation breaks down (e.g., for very deep or very wide networks).

### Mechanism 3
- Claim: Manipulating the spatial extent or color of image backgrounds changes their availability, influencing model reliance.
- Mechanism: Background features become more available when they have a larger pixel footprint or distinct color, leading models to rely on them more than the core object features.
- Core assumption: The pixel footprint and color of a feature are proxies for its availability to the model.
- Evidence anchors:
  - [section 7]: "We explicitly manipulated background properties such as spatial extent, color, and spatial frequency and found that they influence a model's propensity to learn shortcuts."
  - [section 7]: "We find that Bird accuracy increases as we reduce the availability of the image background by manipulating its spatial extent (Bird size, Background patch removal) or drop background color (Color)."
  - [corpus]: Weak - only mentions background manipulations in the context of shortcut learning, not the specific mechanism.
- Break condition: If the model is not sensitive to background manipulations or if the background features are not predictive.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: NTK provides a theoretical framework to understand why nonlinear models exhibit shortcut bias.
  - Quick check question: What is the relationship between the NTK spectrum and the bias of a model towards more available features?

- Concept: Linear Discriminant Analysis (LDA)
  - Why needed here: LDA is used to define the optimal classifier, against which the shortcut bias of trained models is measured.
  - Quick check question: How does LDA determine the decision boundary in the latent space?

- Concept: Feature availability vs. predictivity
  - Why needed here: These two factors interact to determine which features a model relies on, and understanding their distinction is crucial for interpreting the results.
  - Quick check question: How does the availability of a feature influence its likelihood of being used by a model, independent of its predictivity?

## Architecture Onboarding

- Component map: Data generation -> Model training -> Shortcut bias evaluation
- Critical path: Generate synthetic data → Train model → Evaluate shortcut bias
- Design tradeoffs:
  - Linear vs. nonlinear activation functions: Nonlinear functions induce shortcut bias, linear functions do not
  - Model depth: Deeper models exhibit stronger shortcut bias
  - Availability manipulations: Different manipulations (e.g., pixel footprint, color) can influence model reliance
- Failure signatures:
  - Model relies on less predictive features
  - Model performance degrades on out-of-distribution data
  - Model attributions focus on irrelevant image regions
- First 3 experiments:
  1. Train a linear model on synthetic data with varying feature predictivity and availability. Verify that the model is unbiased.
  2. Train a nonlinear model (e.g., MLP with ReLU) on the same synthetic data. Verify that the model exhibits shortcut bias.
  3. Manipulate the pixel footprint of a feature in synthetic images. Verify that the model relies more on features with larger footprints.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different model architectures beyond a single hidden layer with ReLU or Tanh activations affect shortcut bias?
- Basis in paper: [explicit] The paper states that "introducing a single hidden layer with ReLU or Tanh units yields a bias" and that "model depth amplifies bias." However, the paper does not explore other architectures.
- Why unresolved: The paper only tests a limited set of architectures and activation functions. The impact of other architectures on shortcut bias remains unknown.
- What evidence would resolve it: Empirical studies comparing shortcut bias across a wider range of model architectures, including different numbers of hidden layers, activation functions, and network types (e.g., convolutional, recurrent).

### Open Question 2
- Question: What are the specific mechanisms by which availability factors like amplification and nesting influence feature extraction and utilization?
- Basis in paper: [explicit] The paper introduces amplification and nesting as two factors hypothesized to influence feature availability, but does not provide a detailed explanation of their mechanisms.
- Why unresolved: While the paper demonstrates the impact of these factors on model behavior, it does not delve into the underlying processes by which they affect feature extraction and utilization.
- What evidence would resolve it: Theoretical analysis and empirical studies investigating the specific ways in which amplification and nesting influence the model's ability to extract and leverage features, potentially through examining the model's internal representations and decision-making processes.

### Open Question 3
- Question: How do shortcut biases manifest in more complex, real-world tasks beyond binary classification?
- Basis in paper: [explicit] The paper focuses on binary classification tasks and mentions that "future work will study shortcut features in additional domains." The authors acknowledge the need to explore more complex tasks.
- Why unresolved: The paper's experiments are limited to synthetic and simplified real-world datasets. The impact of shortcut learning on more complex tasks, such as multi-class classification, regression, or reinforcement learning, remains unexplored.
- What evidence would resolve it: Empirical studies investigating shortcut biases in various real-world tasks, analyzing the extent to which shortcut learning affects model performance and generalization in these more complex settings.

## Limitations

- The NTK-based theoretical analysis relies on quadratic approximations that may not hold for very deep or wide networks
- The synthetic data generation procedure lacks specific implementation details for embedding and nesting operations
- The study focuses on binary classification and may not generalize to more complex tasks like multi-class or regression problems

## Confidence

- Theoretical NTK analysis: Medium - relies on approximations that may not hold in all regimes
- Synthetic data experiments: High - controlled conditions with clear mechanisms
- Naturalistic dataset results: Medium - fewer availability manipulations tested, potential confounding factors

## Next Checks

1. **NTK approximation validation**: Test the quadratic approximation quality for h(u) across different network widths and depths, particularly for cases where shortcut bias is strongest.

2. **Feature correlation sensitivity**: Systematically vary the correlation between shortcut and core features in synthetic data to determine when the NTK-based predictions break down.

3. **Alternative availability manipulations**: Design and test additional availability manipulations in naturalistic datasets (e.g., texture complexity, spatial frequency) to verify that the observed effects generalize beyond the specific manipulations used.