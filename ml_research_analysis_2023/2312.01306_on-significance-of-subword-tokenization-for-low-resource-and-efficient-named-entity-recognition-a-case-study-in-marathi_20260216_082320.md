---
ver: rpa2
title: 'On Significance of Subword tokenization for Low Resource and Efficient Named
  Entity Recognition: A case study in Marathi'
arxiv_id: '2312.01306'
source_url: https://arxiv.org/abs/2312.01306
tags:
- named
- marathi
- entity
- recognition
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a case study on using subword tokenization
  to improve Named Entity Recognition (NER) performance for the low-resource Marathi
  language. The authors propose integrating BERT-based subword tokenizers into vanilla
  CNN/LSTM models to bridge the gap between high-performance but computationally expensive
  transformer models and efficient but lower-performing traditional deep learning
  models.
---

# On Significance of Subword tokenization for Low Resource and Efficient Named Entity Recognition: A case study in Marathi

## Quick Facts
- arXiv ID: 2312.01306
- Source URL: https://arxiv.org/abs/2312.01306
- Reference count: 24
- Improves F1-score from 79.5% to 82.1% on Marathi NER using subword tokenization

## Executive Summary
This paper investigates the use of subword tokenization to improve Named Entity Recognition (NER) performance for the low-resource Marathi language. The authors propose integrating BERT-based subword tokenizers into vanilla CNN/LSTM models to bridge the gap between high-performance but computationally expensive transformer models and efficient but lower-performing traditional deep learning models. By replacing word-based tokenization with subword tokenization from models like MahaBERT, MahaGPT, IndicBERT, and mBERT, the accuracy of single-layer CNN/LSTM models is significantly improved, achieving results close to state-of-the-art BERT models. On the L3Cube-MahaNER dataset, the MahaBERT tokenizer + CNN model achieved an F1-score of 82.1%, outperforming the word-based tokenizer baseline of 79.5% and demonstrating the importance of subword tokenization for NER in low-resource languages like Marathi.

## Method Summary
The authors propose a hybrid approach that combines the computational efficiency of shallow CNN/LSTM models with the representational power of BERT-based subword tokenizers. They integrate subword tokenizers from monolingual (MahaBERT, MahaGPT) and multilingual (IndicBERT, mBERT) transformer models into vanilla CNN, LSTM, and BiLSTM architectures. The models are trained on the L3Cube-MahaNER dataset, a gold standard Marathi NER corpus with 25,000 sentences and 8 target labels. The subword tokenization breaks words into smaller units that capture morphological variations, addressing the out-of-vocabulary issues common in morphologically rich languages like Marathi. The performance is evaluated using F1-score, Precision, Recall, and Accuracy metrics.

## Key Results
- MahaBERT tokenizer + CNN model achieved F1-score of 82.1%, outperforming word-based tokenizer baseline of 79.5%
- Subword tokenization significantly improves performance of shallow models, bringing them close to BERT-level accuracy
- MahaBERT tokenizer consistently outperformed other tokenizers (MahaGPT, IndicBERT, mBERT) across all model architectures tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subword tokenization improves NER performance by better handling morphological richness in Marathi.
- Mechanism: Subword tokenization breaks words into smaller units that capture morphological variations (prefixes, suffixes, stems), allowing the model to generalize better to unseen word forms and reduce out-of-vocabulary (OOV) issues.
- Core assumption: Marathi's morphological richness leads to many OOV tokens with word-based tokenization, and subword units can effectively represent these variations.
- Evidence anchors:
  - [abstract] "Subword tokenization has emerged as a promising technique in NLP, splitting words into subword units to capture the morphological structure of a language more effectively"
  - [section] "Marathi is a morphologically rich language, making it challenging to build accurate NER systems... Subword tokenization enhances NER by effectively handling unfamiliar words, capturing morphological variations, improving generalization"
  - [corpus] Weak - corpus contains general NER and tokenization papers but none specifically address Marathi morphological richness
- Break condition: If the language being processed is not morphologically rich, the subword tokenization advantage diminishes significantly.

### Mechanism 2
- Claim: BERT-based subword tokenizers improve vanilla CNN/LSTM model performance by providing linguistically informed tokenization.
- Mechanism: Pre-trained BERT tokenizers have learned language-specific tokenization patterns during pre-training, which when applied to shallow models, provide better input representation than simple word-based tokenization.
- Core assumption: BERT tokenizers have learned meaningful subword boundaries for Marathi through their pre-training process.
- Evidence anchors:
  - [abstract] "We propose a hybrid approach for efficient NER by integrating a BERT-based subword tokenizer into vanilla CNN/LSTM models"
  - [section] "We make use of subword tokenizers from monolingual Marathi transformer models like L3Cube-MahaBERT, MahaGPT, and multilingual BERT models like mBERT and Indic BERT"
  - [corpus] Weak - corpus shows BERT tokenizers are used for low-resource languages but doesn't confirm linguistic quality for Marathi specifically
- Break condition: If the BERT tokenizer was not pre-trained on sufficient Marathi data, it may not capture language-specific patterns effectively.

### Mechanism 3
- Claim: The hybrid approach combines computational efficiency of shallow models with representational power of subword tokenization.
- Mechanism: By using a single-layer CNN/LSTM with subword tokenization, the model achieves near-state-of-the-art performance while maintaining lower computational requirements than full transformer models.
- Core assumption: The representational gain from subword tokenization compensates for the architectural simplicity of shallow models.
- Evidence anchors:
  - [abstract] "We focus on improving the performance of shallow models based on CNN, and LSTM by combining the best of both worlds... In the era of transformers, these traditional deep learning models are still relevant because of their high computational efficiency"
  - [section] "Our solution aims to tackle the problem of the low resource named entity recognition (NER) by proposing an extremely shallow model with just one layer, achieving high efficiency without compromising performance"
  - [corpus] Moderate - corpus shows computational efficiency is a known benefit of shallow models but doesn't confirm the specific hybrid performance claims
- Break condition: If the task requires capturing very long-range dependencies, even with subword tokenization, shallow models may underperform compared to deeper architectures.

## Foundational Learning

- Concept: Tokenization methods (word-based vs subword-based)
  - Why needed here: The paper directly compares word-based tokenization with subword-based tokenization, showing how the latter improves NER performance for Marathi
  - Quick check question: What is the primary advantage of subword tokenization over word-based tokenization for morphologically rich languages?

- Concept: Morphologically rich languages
  - Why needed here: Marathi's morphological richness is a key factor driving the need for subword tokenization, as it leads to many word variations and OOV issues
  - Quick check question: Why do morphologically rich languages like Marathi benefit more from subword tokenization than morphologically poor languages?

- Concept: Transformer pre-training and transfer learning
  - Why needed here: The BERT-based tokenizers used in this work are pre-trained models, and understanding how they learn language patterns is crucial to understanding their effectiveness
  - Quick check question: How does pre-training on large corpora help BERT tokenizers develop language-specific tokenization patterns?

## Architecture Onboarding

- Component map: Input → Subword Tokenizer → Embedding Layer → CNN/LSTM Layer → Dense Layer → Output Labels
- Critical path: Tokenization quality → Embedding representation → CNN/LSTM feature extraction → Classification accuracy
- Design tradeoffs: Computational efficiency vs. model depth; monolingual vs. multilingual tokenizers; token sequence length vs. processing time
- Failure signatures: Poor F1 scores indicating tokenization issues; high OOV rates; inconsistent label assignment between root and subword tokens
- First 3 experiments:
  1. Compare word-based vs. MahaBERT tokenizer on CNN model using L3Cube-MahaNER dataset
  2. Test different BERT-based tokenizers (MahaBERT, MahaGPT, IndicBERT, mBERT) with CNN model
  3. Evaluate CNN vs. LSTM vs. BiLSTM architectures using the best-performing tokenizer (MahaBERT)

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations section, several open questions can be identified:

- Whether subword tokenization improves NER performance in Marathi across different model architectures beyond the CNN/LSTM models tested
- How the computational efficiency of subword tokenizer-integrated CNN/LSTM models compares to full BERT models
- Whether subword tokenization improves NER performance in other morphologically rich low-resource languages beyond Marathi

## Limitations
- Dataset Specificity: The evaluation relies entirely on the L3Cube-MahaNER dataset, which is the first gold standard Marathi NER corpus, creating uncertainty about generalizability.
- Tokenizer Implementation Details: Critical implementation details about the BERT-based tokenizers are not disclosed, making it difficult to assess whether performance differences are due to tokenizer quality.
- Baseline Comparison Limitations: The comparison against "word-based tokenizer baseline" is insufficiently detailed, lacking specification of exact word segmentation method and vocabulary size.

## Confidence

**High Confidence**: The general finding that subword tokenization improves NER performance for Marathi has strong empirical support within the L3Cube-MahaNER dataset. The F1-score improvement from 79.5% to 82.1% for CNN models using MahaBERT tokenizer is well-documented and reproducible.

**Medium Confidence**: The claim that BERT-based tokenizers specifically outperform simple subword tokenizers is plausible but not directly tested. The paper assumes BERT tokenizers capture linguistic patterns better, but doesn't validate this assumption against other subword methods.

**Low Confidence**: The generalizability of results to other low-resource morphologically rich languages is speculative. While the methodology appears sound, the paper doesn't test on other languages or demonstrate that Marathi-specific findings transfer to similar languages.

## Next Checks

1. **Cross-Dataset Validation**: Test the MahaBERT tokenizer + CNN approach on an independent Marathi NER dataset (if available) or on Marathi data from different domains to assess robustness and generalizability beyond the L3Cube-MahaNER corpus.

2. **Tokenizer Ablation Study**: Implement and compare against a simple SentencePiece subword tokenizer trained specifically on Marathi text to isolate whether the performance gains come from subword tokenization per se or from BERT-specific linguistic knowledge embedded in pre-trained tokenizers.

3. **Efficiency Benchmarking**: Measure and report actual computational metrics (training time per epoch, inference latency, memory consumption) for the proposed shallow models versus transformer baselines to empirically validate the claimed efficiency benefits.