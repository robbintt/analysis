---
ver: rpa2
title: 'Better to Ask in English: Cross-Lingual Evaluation of Large Language Models
  for Healthcare Queries'
arxiv_id: '2310.13132'
source_url: https://arxiv.org/abs/2310.13132
tags: []
core_contribution: 'The study addresses the challenge of language disparity in large
  language models (LLMs) for healthcare queries, where LLMs demonstrate lower performance
  in non-English languages compared to English. To tackle this issue, the authors
  propose XLingEval, a comprehensive cross-lingual evaluation framework that assesses
  LLM responses based on three criteria: correctness, consistency, and verifiability.'
---

# Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries

## Quick Facts
- arXiv ID: 2310.13132
- Source URL: https://arxiv.org/abs/2310.13132
- Reference count: 40
- Key outcome: Cross-lingual evaluation reveals significant performance disparities in LLMs for healthcare queries, with non-English languages showing lower correctness, consistency, and verifiability scores.

## Executive Summary
This study investigates the language disparity in large language models (LLMs) when handling healthcare queries across different languages. The authors develop XLingEval, a comprehensive cross-lingual evaluation framework that assesses LLM responses based on correctness, consistency, and verifiability. Using three expert-annotated healthcare datasets translated into English, Spanish, Chinese, and Hindi, the framework reveals significant performance disparities, with non-English languages exhibiting notably lower scores across all evaluation metrics. The study introduces XLingHealth, a cross-lingual benchmark for examining multilingual capabilities of LLMs in healthcare contexts, highlighting the need for improved cross-lingual capabilities and equitable information access.

## Method Summary
The study constructs a cross-lingual evaluation framework using three healthcare datasets (HealthQA, LiveQA, MedicationQA) translated into Hindi, Chinese, and Spanish. XLingEval employs GPT-3.5 and MedAlpaca-30b models with Chain-of-Thought prompting for automated correctness evaluation, temperature-based sampling for consistency analysis, and automated verification assessment. The framework is validated through human evaluation on a subset of responses. Consistency is measured using n-gram similarity, BERTScore, and topic modeling (LDA and HDP) across different temperature settings. The entire pipeline is applied to compare LLM performance across the four languages on all three evaluation criteria.

## Key Results
- GPT-3.5 produced 18.12% fewer "more comprehensive and appropriate answers" in non-English languages compared to English
- Answers in non-English languages were 29.3% less consistent than English responses
- XLingHealth benchmark demonstrates significant cross-lingual performance gaps that require targeted improvements in multilingual healthcare AI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual performance disparity is caused by uneven data distribution in LLM training, with disproportionate focus on English.
- Mechanism: LLMs trained on predominantly English datasets exhibit weaker capabilities in non-English languages due to lack of exposure to linguistic patterns and domain-specific vocabulary.
- Core assumption: Training data directly influences language model performance across different languages.
- Evidence anchors:
  - [abstract] "The development of LLMs is disproportionately focused on English. It remains unclear how these LLMs perform in the context of non-English languages..."
  - [section] "This uneven data distribution casts doubts over the genuine multilingual capabilities of these models."
  - [corpus] Weak evidence; corpus neighbors focus on cross-lingual evaluation but don't directly confirm data distribution as the primary cause.
- Break condition: If multilingual datasets are equally represented in training, the disparity would not exist.

### Mechanism 2
- Claim: Automated evaluation using LLMs to compare answers with ground truth can effectively measure correctness across languages.
- Mechanism: Chain-of-Thought prompting guides the LLM to systematically compare its own responses with ground truth answers across multiple dimensions (comprehensiveness, appropriateness, contradiction).
- Core assumption: LLMs can reliably perform meta-evaluation of their own outputs when given structured prompts.
- Evidence anchors:
  - [section] "We prompted the LLM with the question, ground-truth answer, and the LLM answer using Chain-of-Thought (CoT) prompting..."
  - [section] "Findings for comprehensiveness and appropriateness: Table 1 presents the results for the automated comparative evaluation."
  - [corpus] Moderate evidence; corpus neighbors discuss automated evaluation but don't specifically validate CoT prompting for self-evaluation.
- Break condition: If LLM struggles with self-evaluation or prompt structure is ineffective, automated correctness measurement fails.

### Mechanism 3
- Claim: Temperature variation in LLM generation reveals consistency disparities across languages.
- Mechanism: Varying temperature (ùúè) controls randomness in text generation, allowing measurement of lexical and semantic consistency across different languages.
- Core assumption: Consistency metrics (n-gram similarity, BERTScore, topic modeling) accurately capture language-specific generation stability.
- Evidence anchors:
  - [section] "To address this challenge, the consistency criterion protocol gauges the coherence of LLM-generated responses. To achieve this, we varied the 'temperature' parameter ùúè of language models..."
  - [section] "We employed two topic modeling techniques to quantify topic similarity: Latent Dirichlet Process (LDA) [45] and Hierarchical Dirichlet Process (HDP)[46]."
  - [corpus] Weak evidence; corpus neighbors discuss consistency evaluation but don't specifically validate temperature-based analysis.
- Break condition: If consistency metrics are not sensitive to temperature changes or don't capture language-specific differences, the analysis becomes unreliable.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Enables structured reasoning for automated evaluation of LLM responses against ground truth.
  - Quick check question: How does CoT prompting help in comparing LLM answers with ground truth across multiple dimensions?

- Concept: Temperature (ùúè) in LLM generation
  - Why needed here: Controls randomness in text generation to measure consistency across different languages.
  - Quick check question: Why is varying temperature important for assessing the consistency of LLM responses across languages?

- Concept: Topic modeling (LDA and HDP)
  - Why needed here: Quantifies semantic similarity and topic consistency between LLM-generated responses.
  - Quick check question: How do LDA and HDP help in measuring topic-level consistency across different languages?

## Architecture Onboarding

- Component map:
  - Data preprocessing ‚Üí Translation quality assessment ‚Üí Dataset construction
  - Automated evaluation pipeline ‚Üí Correctness, Consistency, Verifiability metrics
  - Human evaluation validation ‚Üí Annotator platform ‚Üí Correlation analysis
  - XLingHealth benchmark ‚Üí Cross-lingual healthcare evaluation framework

- Critical path:
  1. Dataset construction (translation + quality assessment)
  2. Automated evaluation (correctness ‚Üí consistency ‚Üí verifiability)
  3. Human validation of automated results
  4. Cross-lingual comparison and disparity analysis

- Design tradeoffs:
  - Automated vs. human evaluation: Automated evaluation enables large-scale analysis but requires validation through human evaluation.
  - Translation quality: Machine translation introduces potential errors; using multiple translation tools and human validation mitigates this.
  - Temperature variation: Higher temperatures increase variability but may better reveal consistency issues across languages.

- Failure signatures:
  - Low correlation between automated and human labels indicates evaluation framework issues.
  - Significant content filtering across all languages suggests safety concerns.
  - Inconsistent results across different temperature settings indicate model instability.

- First 3 experiments:
  1. Replicate correctness evaluation with a different LLM (e.g., GPT-4) to validate findings.
  2. Test consistency metrics on a different domain (e.g., legal or finance) to assess generalizability.
  3. Conduct ablation study on translation quality by comparing results with human-translated datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific causes of language disparity in LLM performance for healthcare queries?
- Basis in paper: [explicit] The paper states that "the underlying rationale for this discrepancy can be attributed primarily to two key factors: the limited availability of data resources for Non-English languages and the presence of a highly imbalanced data distribution employed in the training of the LLMs."
- Why unresolved: While the paper identifies the two main causes, it does not delve into the specifics of how these factors contribute to the disparity. For instance, it does not provide detailed information on the extent of data scarcity for non-English languages or the exact nature of the data imbalance in LLM training.
- What evidence would resolve it: A comprehensive study that quantifies the amount of data available for each language in the training datasets of popular LLMs, and an analysis of the distribution of this data across different languages.

### Open Question 2
- Question: How can the cross-lingual capabilities of LLMs be improved for healthcare queries?
- Basis in paper: [inferred] The paper suggests that high-precision machine translation has been employed as a possible solution in past works. However, it also notes that critical domains such as healthcare require extensive human evaluation of translation to prevent serious ramifications.
- Why unresolved: The paper does not provide a clear solution or strategy for improving the cross-lingual capabilities of LLMs in healthcare. It only mentions the need for close collaboration with medical experts and endorsement of specific training data resources by medical/healthcare organizations.
- What evidence would resolve it: A proposed framework or methodology that outlines how to effectively improve the cross-lingual capabilities of LLMs for healthcare queries, including specific steps, resources, and collaborations needed.

### Open Question 3
- Question: How does the performance of domain-specific LLMs compare to general-purpose LLMs in healthcare queries across different languages?
- Basis in paper: [explicit] The paper compares the performance of GPT-3.5, a general-purpose LLM, with MedAlpaca, a domain-specific LLM, and finds that while MedAlpaca shows better performance in English, it struggles significantly in non-English languages.
- Why unresolved: The paper only provides a comparison between two specific models (GPT-3.5 and MedAlpaca) and does not explore the performance of other domain-specific LLMs or provide a comprehensive analysis of how different types of LLMs perform across languages.
- What evidence would resolve it: A comprehensive study that compares the performance of multiple general-purpose and domain-specific LLMs in healthcare queries across a wide range of languages, using a standardized evaluation framework.

## Limitations
- Translation quality uncertainty: The study relies on machine-translated datasets, which may introduce artifacts or cultural nuances lost in translation, potentially affecting cross-lingual performance comparisons.
- Evaluation framework validation: The correlation between automated and human evaluation labels is not explicitly quantified, limiting confidence in the automated evaluation methodology.
- Model-specific findings: Results are based on GPT-3.5 and MedAlpaca-30b, potentially limiting generalizability to other LLM architectures and training regimes.

## Confidence
- High Confidence: The observed cross-lingual performance disparity is real and statistically significant, supported by large-scale evaluation across multiple languages and criteria.
- Medium Confidence: The specific mechanisms causing cross-lingual disparity (uneven training data distribution) are plausible but not definitively proven through direct experimental validation.
- Medium Confidence: The automated evaluation framework (XLingEval) is methodologically sound and produces reasonable results that align with human evaluation, though implementation details could be more transparent.

## Next Checks
1. **Translation Quality Impact Analysis**: Re-run the evaluation pipeline on professionally human-translated subsets of the datasets to quantify the impact of translation quality on cross-lingual performance disparities.
2. **Model Architecture Generalization**: Test the same evaluation framework on additional LLM architectures (e.g., LLaMA, Claude) to determine whether the observed cross-lingual patterns are consistent across different model families.
3. **Training Data Analysis**: Conduct an analysis of the actual training data composition for the evaluated models, comparing the volume and quality of English versus non-English healthcare-related content to empirically validate the data distribution hypothesis.