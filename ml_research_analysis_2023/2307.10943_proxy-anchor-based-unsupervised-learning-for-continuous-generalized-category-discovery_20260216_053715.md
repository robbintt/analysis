---
ver: rpa2
title: Proxy Anchor-based Unsupervised Learning for Continuous Generalized Category
  Discovery
arxiv_id: '2307.10943'
source_url: https://arxiv.org/abs/2307.10943
tags:
- novel
- learning
- category
- categories
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of discovering novel categories
  in an unsupervised, class-incremental learning setting without prior knowledge about
  the number of new categories or the proportion of novel samples. The proposed method
  leverages proxy anchor-based deep metric learning to split unlabeled joint datasets
  into old and novel categories, using a noisy label learning scheme and non-parametric
  clustering for further separation.
---

# Proxy Anchor-based Unsupervised Learning for Continuous Generalized Category Discovery

## Quick Facts
- arXiv ID: 2307.10943
- Source URL: https://arxiv.org/abs/2307.10943
- Authors: 
- Reference count: 40
- Primary result: Proposed method outperforms state-of-the-art approaches in novel category discovery without requiring prior knowledge about the number of novel categories or their proportion

## Executive Summary
This paper addresses the challenge of discovering novel categories in unsupervised, class-incremental learning settings where the number of new categories and their proportion in the dataset are unknown. The method leverages proxy anchor-based deep metric learning to effectively split unlabeled joint datasets into old and novel categories using a noisy label learning scheme combined with non-parametric clustering. A proxy anchor-based exemplar approach is employed to mitigate catastrophic forgetting during incremental learning. Experimental results on fine-grained datasets demonstrate superior performance compared to existing methods, achieving high cluster accuracy metrics without requiring any prior knowledge.

## Method Summary
The proposed method consists of three main stages: initial split, fine split, and incremental training. First, a pre-trained feature extractor and proxy anchors are fine-tuned on a labeled dataset containing old categories. The unlabeled joint dataset is then split into old and novel categories using cosine similarity between embedding vectors and proxy anchors, with samples exceeding a threshold classified as old. A noisy label learning scheme refines this initial split through an MLP classifier trained on clean samples from both ends of the similarity distribution, followed by GMM clustering for further separation. During incremental training, pseudo-labels are assigned using previous model predictions for old categories and Affinity Propagation clustering for novel ones. To prevent catastrophic forgetting, the method generates synthetic features using Gaussian distributions centered at trained proxy anchors and applies knowledge distillation between old and new models.

## Key Results
- Outperforms state-of-the-art approaches on fine-grained datasets (CUB-200, MIT67, Stanford Dogs, FGVC Aircraft)
- Achieves high cluster accuracy metrics (Mall, Mo, Md) without requiring prior knowledge about novel category count or proportion
- Successfully mitigates catastrophic forgetting using proxy anchor-based exemplar generation
- Demonstrates effectiveness of noisy label learning scheme in improving initial category separation

## Why This Works (Mechanism)

### Mechanism 1
Proxy Anchor loss enables effective separation of old and novel categories in joint unlabeled datasets without requiring prior knowledge about the number of novel classes. The method uses pre-trained proxy anchors representing old categories to compute cosine similarity scores with embedding vectors of unlabeled samples. Samples with similarity scores above threshold (set to 0) are classified as old categories, while others are treated as novel candidates. Core assumption: Proxy anchors trained on labeled old categories retain discriminative power to distinguish between old and novel categories in joint datasets.

### Mechanism 2
Noisy label learning scheme improves the accuracy of initial category separation through iterative refinement. After initial split using proxy anchors, an MLP classifier is trained on "clean" samples from both ends of the similarity score distribution. The classifier then reassigns pseudo-labels to all samples, and Gaussian Mixture Model (GMM) further refines the separation boundaries. Core assumption: Initial split produces enough clean samples at the extremes of the distribution to train an effective binary classifier.

### Mechanism 3
Proxy anchor-based exemplar generation mitigates catastrophic forgetting in continual learning scenarios. Instead of storing real samples from old categories, the method generates synthetic features using Gaussian distributions centered at trained proxy anchors (N(p₀, σ²)). These generated features are used during training to maintain old category knowledge while learning novel categories. Core assumption: Proxy anchors contain sufficient representative information about old categories to generate meaningful synthetic features that prevent forgetting.

## Foundational Learning

- **Deep metric learning with proxy-based methods**: The method relies on proxy anchors to create discriminative feature spaces where cosine similarity can distinguish between categories. Quick check: What is the key difference between proxy-based and pair-based metric learning approaches?

- **Gaussian Mixture Models for unsupervised clustering**: GMM is used in the fine split stage to further refine the separation between old and novel categories based on the noisy labels. Quick check: How does GMM differ from k-means clustering in terms of cluster shape assumptions?

- **Feature distillation for knowledge preservation**: The method uses distillation loss to maintain consistency between old and new model embeddings, complementing the exemplar approach. Quick check: What is the mathematical form of the distillation loss used in this method?

## Architecture Onboarding

- **Component map**: Feature extractor (CNN backbone) -> Proxy anchor module (metric learning) -> Initial split module (cosine similarity) -> Fine split module (MLP + GMM) -> Exemplar generation module (Gaussian sampling) -> Distillation module (feature consistency)

- **Critical path**: Labeled dataset → Fine-tune feature extractor + proxy anchors → Initial split → Fine split → Pseudo-labeling → Incremental training with exemplar + distillation

- **Design tradeoffs**: The method trades computational efficiency (using proxy anchors instead of all-pairs comparisons) for potential loss in fine-grained discrimination compared to full metric learning approaches.

- **Failure signatures**: Poor initial split results in contaminated training data; inadequate exemplar generation leads to catastrophic forgetting; insufficient pseudo-label quality degrades novel category discovery.

- **First 3 experiments**:
  1. Test proxy anchor separation accuracy on a simple dataset with clear category boundaries
  2. Evaluate GMM refinement performance with varying noise levels in initial splits
  3. Measure forgetting rates with and without exemplar generation on incremental learning tasks

## Open Questions the Paper Calls Out

### Open Question 1
How would the proposed method perform if the unlabeled joint dataset contained multiple incremental steps beyond two? Basis in paper: The paper evaluates two-step incremental category discovery on the CUB-200 dataset and shows that performance metrics (Mall, Mo, Md) change with each incremental step. Why unresolved: The experiments only demonstrate two incremental steps, and it's unclear how the method would scale with more steps or whether performance would degrade over time. What evidence would resolve it: Results from experiments with three or more incremental steps showing consistent or improved performance metrics.

### Open Question 2
Can the proposed method be adapted to handle non-fine-grained datasets effectively? Basis in paper: The paper focuses on fine-grained datasets like CUB-200, MIT67, Stanford Dogs, and FGVC Aircraft, but does not explore coarse-grained datasets like CIFAR or ImageNet. Why unresolved: The paper's emphasis on fine-grained datasets suggests that the method might be optimized for this domain, but there's no evidence of its effectiveness on coarser datasets. What evidence would resolve it: Comparative results on coarse-grained datasets showing that the method maintains or improves performance relative to existing approaches.

### Open Question 3
How does the choice of clustering algorithm impact the performance of the proposed method? Basis in paper: The paper uses Affinity Propagation for non-parametric clustering but mentions that adopting better clustering methods could improve performance. Why unresolved: The paper does not explore alternative clustering algorithms or their impact on the method's effectiveness. What evidence would resolve it: Experimental results comparing the proposed method using different clustering algorithms, showing which performs best for novel category discovery.

## Limitations

- The method's performance depends heavily on the quality of initial proxy anchor separation, which may degrade when novel categories are semantically close to old ones
- The noisy label learning scheme assumes sufficient clean samples exist at the distribution extremes, which may not hold for highly imbalanced or overlapping category distributions
- The proxy anchor-based exemplar generation depends on proxy anchors adequately capturing category distributions, which may not be true for highly diverse or complex novel categories

## Confidence

- **High Confidence**: The general framework of using proxy anchors for metric learning and category separation is well-established and supported by extensive literature. The experimental methodology and evaluation metrics are standard in the field.
- **Medium Confidence**: The specific implementation of the noisy label learning scheme and the integration of GMM for fine split refinement are novel but lack direct empirical validation in isolation. The effectiveness of proxy anchor-based exemplar generation for catastrophic forgetting mitigation is supported by proxy anchor literature but not specifically validated for this exemplar approach.
- **Low Confidence**: The claim that the method requires no prior knowledge about the number of novel categories or their proportion is somewhat misleading, as the method still requires setting thresholds and parameters that implicitly encode such assumptions.

## Next Checks

1. **Separation Robustness Test**: Evaluate the initial split performance across varying degrees of semantic similarity between old and novel categories using controlled synthetic datasets with known category boundaries.

2. **GMM Refinement Ablation**: Conduct ablation studies isolating the impact of GMM refinement on overall accuracy by comparing performance with and without the fine split stage across different noise levels in initial splits.

3. **Exemplar Effectiveness Validation**: Measure forgetting rates when using different exemplar generation strategies (Gaussian vs. real samples vs. no exemplar) on incremental learning tasks with varying numbers of old categories.