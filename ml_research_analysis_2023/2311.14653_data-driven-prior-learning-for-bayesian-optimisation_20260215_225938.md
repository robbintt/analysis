---
ver: rpa2
title: Data-driven Prior Learning for Bayesian Optimisation
arxiv_id: '2311.14653'
source_url: https://arxiv.org/abs/2311.14653
tags:
- transfer
- optimisation
- learning
- prior
- plebo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes PLeBO, a method for learning Gaussian process
  hyperparameters from related optimisation tasks, enabling faster optimisation of
  new tasks. It learns a shared prior over hyperparameters using MCMC on tuning tasks,
  then uses weighted sampling of candidates during optimisation.
---

# Data-driven Prior Learning for Bayesian Optimisation

## Quick Facts
- arXiv ID: 2311.14653
- Source URL: https://arxiv.org/abs/2311.14653
- Reference count: 9
- The paper proposes PLeBO, a method for learning Gaussian process hyperparameters from related optimisation tasks, enabling faster optimisation of new tasks.

## Executive Summary
This paper introduces PLeBO, a data-driven approach for Bayesian optimisation that learns Gaussian process hyperparameter priors from related optimisation tasks. By using MCMC to infer a shared prior over hyperparameters from tuning tasks, PLeBO improves surrogate model accuracy, particularly when evaluations are expensive. The method employs importance weighting to adapt these priors to specific test tasks during optimisation, making it robust when optimal inputs differ but landscape shapes remain similar. Experiments on synthetic and real air pollution data demonstrate that PLeBO outperforms direct transfer methods and matches standard EI in convergence speed.

## Method Summary
PLeBO learns shared priors over GP hyperparameters using MCMC inference on tuning tasks, then weights candidate hyperparameter sets during BO optimisation based on their likelihood given observed test data. The method generates H candidate hyperparameter sets from the learned prior and, at each BO iteration, calculates the likelihood of each candidate given current observations to weight the acquisition function. This approach focuses on transferring landscape shape information rather than specific optimal input locations, making it effective when optima are spatially separated but shapes are similar.

## Key Results
- PLeBO outperforms direct transfer methods and matches EI in convergence speed on both synthetic and air pollution data
- TruePLeBO (with oracle hyperparameters) performs best on synthetic data, validating the approach
- PLeBO generalises well even when tuning and test tasks differ in optima locations but share similar landscape shapes
- Runtime is higher than EI but acceptable for expensive evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning GP hyperparameters from related tasks improves surrogate model accuracy, especially with few evaluations.
- Mechanism: PLeBO uses MCMC to infer a shared prior over hyperparameters (η) from tuning tasks, then weights candidate hyperparameter sets during BO optimisation based on their likelihood given observed test data.
- Core assumption: Related optimisation tasks have similar landscape shapes, so their optimal GP hyperparameters are drawn from the same distribution.
- Evidence anchors:
  - [abstract]: "By learning priors for the hyperparameters of the Gaussian process surrogate model we can better approximate the underlying function, especially for few function evaluations."
  - [section]: "By learning priors for the hyperparameters of the Gaussian process surrogate model we can better approximate the underlying function, especially for few function evaluations."
- Break condition: If the shape similarity assumption fails (e.g., tasks have fundamentally different landscape structures), the learned prior will be misleading and hurt optimisation.

### Mechanism 2
- Claim: Prior transfer works when direct transfer fails, such as when optimal inputs differ but landscape shapes are similar.
- Mechanism: PLeBO focuses on matching landscape shapes via hyperparameter priors rather than transferring specific optimal input locations, making it robust when optima are spatially separated.
- Core assumption: The covariance structure (captured by GP hyperparameters) encodes enough information about landscape shape for effective transfer.
- Evidence anchors:
  - [abstract]: "We replace this assumption with a weaker one only requiring the shape of the optimisation landscape to be similar"
  - [section]: "In the bottom row the optima are far apart but the shapes are the same. In this setting prior transfer works perfectly while direct transfer does not work at all."
- Break condition: If landscape shapes differ significantly between tasks, even prior transfer will fail because the shared hyperparameter distribution becomes uninformative.

### Mechanism 3
- Claim: Importance weighting of candidate hyperparameters during optimisation adapts the prior to the specific test task.
- Mechanism: At each BO iteration, candidate hyperparameters are weighted by their likelihood given the observed test data, allowing the surrogate model to focus on hyperparameters that fit the current task.
- Core assumption: Even with a shared prior, individual tasks may need slightly different hyperparameter values, which can be inferred from limited observations.
- Evidence anchors:
  - [section]: "At each optimisation step we calculate the likelihood of every hyperparameter candidate, and use it to weight the corresponding base acquisition function."
  - [section]: "We use importance weighting to fit these candidates to the observed test data Dj."
- Break condition: If the test task is too dissimilar from the tuning tasks, none of the weighted candidates will provide a good fit, leading to poor BO performance.

## Foundational Learning

- Concept: Gaussian Processes and kernel hyperparameters (lengthscale, signal variance)
  - Why needed here: PLeBO directly manipulates GP hyperparameters as the transfer mechanism; understanding their effect on the surrogate model is essential.
  - Quick check question: How does increasing the lengthscale affect the smoothness of the GP posterior mean?

- Concept: Markov Chain Monte Carlo for hyperparameter inference
  - Why needed here: PLeBO uses MCMC to learn the shared hyperparameter prior from tuning tasks; understanding this inference process is crucial for implementation and debugging.
  - Quick check question: Why might MCMC be preferred over gradient-based hyperparameter optimisation in this context?

- Concept: Transfer learning assumptions and distinctions (direct vs. prior transfer)
  - Why needed here: PLeBO is a prior transfer method; understanding when and why it works requires distinguishing it from direct transfer approaches.
  - Quick check question: In what scenario would direct transfer outperform prior transfer, according to the paper?

## Architecture Onboarding

- Component map: Preprocessing (MCMC inference) -> Candidate generation (sampling) -> Optimisation loop (weighting + acquisition) -> Evaluation -> Update
- Critical path: MCMC inference → Candidate sampling → Likelihood weighting → Acquisition calculation → Evaluation → Update
- Design tradeoffs:
  - Number of candidates (H): More candidates improve posterior approximation but increase computational cost
  - MCMC convergence: Multiple chains and filtering needed due to likelihood zero samples
  - Runtime vs. accuracy: PLeBO is slower than EI but may find better optima with fewer evaluations
- Failure signatures:
  - Poor MCMC mixing or convergence leading to uninformative priors
  - Likelihood weighting collapsing to a single candidate (overfitting to early observations)
  - Runtime becoming prohibitive for very expensive evaluation functions
- First 3 experiments:
  1. Run PLeBO on synthetic data with known hyperparameters to verify learned priors match ground truth
  2. Compare PLeBO to Shared (single shared hyperparameter set) to isolate benefit of weighting vs. simple sharing
  3. Test PLeBO on air pollution data with varying levels of similarity between tuning and test tasks to probe transfer limits

## Open Questions the Paper Calls Out
No specific open questions were identified in the paper text provided.

## Limitations
- Computational overhead: PLeBO requires significantly more computation than standard EI, with runtime scaling linearly with the number of hyperparameter candidates (H)
- Assumption sensitivity: PLeBO still requires landscape shape similarity and may fail when shapes differ significantly between tasks
- Hyperparameter tuning: The method introduces new hyperparameters that require careful tuning, though the paper doesn't provide clear guidance on optimal settings

## Confidence
- PLeBO improves convergence vs. direct transfer methods: **High**
- Landscape shape similarity is sufficient for transfer: **Medium**
- MCMC-based prior learning is more effective than simple hyperparameter sharing: **Medium-High**

## Next Checks
1. **Robustness testing**: Evaluate PLeBO performance when tuning and test tasks have varying degrees of similarity using a controlled synthetic benchmark with gradually changing landscape shapes.
2. **Scalability analysis**: Measure PLeBO's performance and runtime on higher-dimensional optimisation problems (D > 10) to assess scalability beyond the 2D synthetic examples.
3. **Alternative acquisition functions**: Test PLeBO with acquisition functions beyond EI (e.g., UCB, TS) to determine if the performance gains are acquisition-specific or more general.