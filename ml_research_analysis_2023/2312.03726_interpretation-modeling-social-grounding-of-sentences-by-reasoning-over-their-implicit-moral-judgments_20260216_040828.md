---
ver: rpa2
title: 'Interpretation modeling: Social grounding of sentences by reasoning over their
  implicit moral judgments'
arxiv_id: '2312.03726'
source_url: https://arxiv.org/abs/2312.03726
tags:
- sentence
- interpretation
- interpretations
- they
- people
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel task, interpretation modeling (IM),
  which aims to generate multiple interpretations of a sentence by reasoning over
  implicit moral judgments and reader attitudes. The authors construct a dataset,
  origamIM, where annotators provide interpretations and moral judgments for sentences.
---

# Interpretation modeling: Social grounding of sentences by reasoning over their implicit moral judgments

## Quick Facts
- arXiv ID: 2312.03726
- Source URL: https://arxiv.org/abs/2312.03726
- Reference count: 36
- Key outcome: One-to-one generation methods outperform one-to-many settings for interpretation quality, with models revealing hidden toxicity in 87% of test sentences

## Executive Summary
This paper introduces interpretation modeling (IM), a novel task that generates multiple interpretations of sentences by reasoning over implicit moral judgments and reader attitudes. The authors construct origamIM, a dataset where annotators provide interpretations and moral judgments for sentences. They propose one-to-one and one-to-many generation methods using pre-trained language models, demonstrating that T5-based models outperform GPT-2. The approach shows promise for content moderation by revealing hidden toxic meanings in seemingly benign sentences through diverse interpretations grounded in social perspectives.

## Method Summary
The method involves generating interpretations of sentences conditioned on reader attitudes (very negative to very positive) and moral judgments (good/bad, virtue/vice) based on Virtue Ethics. Two generation strategies are employed: one-to-one (generating each interpretation independently) and one-to-many (generating multiple interpretations jointly with semantic similarity constraints). The models use pre-trained language models (GPT-2, T5) to encode input templates containing the sentence, title, attitudes, and moral judgments, then decode interpretations. The one-to-many method includes random ordering, semantic similarity ordering, and constraining on semantic similarity to control interpretation diversity.

## Key Results
- T5-based models significantly outperform GPT-2 on all evaluation metrics
- One-to-one generation setting yields better interpretation quality than one-to-many
- Generated interpretations reveal hidden toxicity in 87% of test sentences
- Models capture diverse interpretations with correlations approaching the gold standard

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Generating diverse interpretations of a sentence improves content moderation by revealing implicit toxic meaning.
- **Mechanism**: When humans and models generate multiple interpretations of a sentence, they surface hidden toxic, insulting, or identity-attacking content that the original sentence does not explicitly convey. This is especially valuable when the sentence appears benign on the surface.
- **Core assumption**: Implicit toxic content is not always recognized by automated toxicity detectors when only the original sentence is analyzed.
- **Evidence anchors**:
  - [abstract] "toxicity analyses in the generated interpretations demonstrate the importance of IM for refining filters of content and assisting content moderators in safeguarding the safety in online discourse."
  - [section] "We observe that the model is able to reveal the hidden meaning of the input sentence (right side of Figure 7). It generates more toxic, insulting, and attacking interpretations for 81.86%, 80.39%, and 87.25% of the sentences in the test set (analysis 1A)."
  - [corpus] No direct corpus evidence; claim is supported by experimental results.
- **Break condition**: If the generation model fails to capture diverse reader perspectives, the interpretations may not reveal hidden toxicity, reducing the effectiveness of content moderation.

### Mechanism 2
- **Claim**: Social grounding through reader attitudes and moral judgments enables the generation of contextually appropriate interpretations.
- **Mechanism**: By conditioning interpretation generation on reader attitudes (positive, neutral, negative) and moral judgments (good/bad, virtue/vice), the model generates interpretations that reflect diverse social perspectives. This grounding helps the model produce interpretations that align with how different readers might understand the same sentence.
- **Core assumption**: Reader attitudes and moral judgments significantly influence how individuals interpret the same sentence.
- **Evidence anchors**:
  - [abstract] "To obtain these, IM is guided by multiple annotations of social relation and common ground - in this work approximated by reader attitudes towards the author and their understanding of moral judgments subtly embedded in the sentence."
  - [section] "We notice that diversity in social grounding ùëëùëî is positively correlated (Pearson) with diversity in interpretation ùëëùëñ (p < .01). The positive correlation is stronger with One2One-GPT (r = .5526) than with One2One-T5 (r = .3321) and approaches that of the gold standard (r = .5887)."
  - [corpus] Weak evidence; the correlation supports the claim, but the mechanism's effectiveness depends on the quality and diversity of annotations.
- **Break condition**: If the annotations are biased or not representative of diverse social perspectives, the generated interpretations may not accurately reflect varied reader interpretations.

### Mechanism 3
- **Claim**: Using one-to-many generation with semantic similarity constraints produces more diverse interpretations than one-to-one generation.
- **Mechanism**: In one-to-many generation, the model generates multiple interpretations jointly, considering the context of previously generated interpretations. By ordering interpretations based on their semantic similarity to the original sentence and enforcing a decreasing similarity, the model is encouraged to produce interpretations that vary in how closely they relate to the original meaning, thus increasing diversity.
- **Core assumption**: Controlling the semantic similarity between generated interpretations and the original sentence influences the diversity of interpretations.
- **Evidence anchors**:
  - [abstract] "We propose a number of modeling strategies that rely on one-to-one and one-to-many generation methods that take inspiration from the philosophical study of interpretation."
  - [section] "Adding implicit ([One2M-Sim]) control by ordering interpretations based on their semantic similarity to the sentence in the ground truth slightly improves performance, suggesting that the order in which the ground-truth interpretations are presented during optimization indeed affects generation."
  - [corpus] Limited corpus evidence; the claim is supported by experimental results but not by external studies.
- **Break condition**: If the semantic similarity constraints are too strict or too lenient, the model may either produce interpretations that are too similar to the original sentence or too unrelated, reducing the effectiveness of the generation method.

## Foundational Learning

- **Concept**: Virtue Ethics and Spheres of Action
  - Why needed here: The dataset relies on Virtue Ethics to classify moral judgments, which influences how interpretations are generated. Understanding this framework is essential for interpreting the dataset and the model's behavior.
  - Quick check question: How does the Sphere of Action "Confidence (Fear, Uncertainty)" classify behaviors like cowardice and rashness?

- **Concept**: Semantic similarity and its role in diverse generation
  - Why needed here: The one-to-many generation method uses semantic similarity to control the diversity of interpretations. Understanding how semantic similarity is measured and its impact on generation is crucial for tuning the model.
  - Quick check question: What is the relationship between semantic similarity and the diversity of generated interpretations in the one-to-many setting?

- **Concept**: Attitude scales and their influence on interpretation
  - Why needed here: Reader attitudes (very negative to very positive) are used to ground interpretations socially. Understanding how attitudes affect interpretation is key to interpreting the model's outputs.
  - Quick check question: How does a reader's attitude (e.g., very negative) influence the interpretation of a sentence with implicit moral judgments?

## Architecture Onboarding

- **Component map**: Sentence, title, attitudes, moral judgments -> Encoder (T5/GPT-2) -> Decoder (T5/GPT-2) -> Interpretations (with semantic similarity constraints for one-to-many)
- **Critical path**:
  1. Encode the input template (sentence, title, attitudes, moral judgments).
  2. Generate interpretations using the decoder.
  3. If one-to-many generation, apply semantic similarity constraints to ensure diversity.
- **Design tradeoffs**:
  - T5 vs. GPT-2: T5 generally performs better due to its encoder-decoder architecture, but GPT-2 may be faster for certain tasks.
  - One-to-one vs. one-to-many: One-to-one is simpler and more accurate per interpretation, while one-to-many can generate more diverse interpretations but is more complex.
  - Semantic similarity constraints: Enforcing diversity can improve interpretation quality but may reduce relevance to the original sentence.
- **Failure signatures**:
  - Hallucinations: GPT-2 may generate nonsensical interpretations, especially in one-to-many settings.
  - Lack of diversity: If the model fails to leverage social grounding, interpretations may be too similar.
  - Toxicity detection failure: If the model does not surface hidden toxicity, content moderation effectiveness is reduced.
- **First 3 experiments**:
  1. Compare one-to-one vs. one-to-many generation using T5 on a small subset of the dataset.
  2. Test the impact of semantic similarity constraints on interpretation diversity.
  3. Evaluate the model's ability to reveal hidden toxicity by comparing interpretations with original sentences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of interpretation modeling models vary across different social and cultural contexts, and what factors contribute to these variations?
- Basis in paper: [explicit] The authors acknowledge that their dataset predominantly reflects North-American views and that the annotated moral judgments and interpretations should not be considered a complete, socially diverse and fair account of views.
- Why unresolved: The paper does not explore the performance of interpretation modeling models on data from different social and cultural contexts. It would be valuable to understand how the models generalize to diverse perspectives and what factors influence their performance.
- What evidence would resolve it: Conducting experiments on interpretation modeling models using datasets from different social and cultural contexts, analyzing the factors that contribute to variations in performance, and comparing the results with the current model's performance.

### Open Question 2
- Question: How can interpretation modeling be extended to handle more complex social relations between authors and readers, beyond attitudes and moral judgments?
- Basis in paper: [explicit] The authors mention that they constrained themselves to leveraging implicit moral judgments and attitudes, but interpretation can be guided by a wide array of hidden meanings.
- Why unresolved: The paper focuses on attitudes and moral judgments as social grounding for interpretation modeling. Exploring additional social relations, such as power dynamics, shared experiences, or cultural background, could enhance the model's ability to capture diverse interpretations.
- What evidence would resolve it: Investigating the impact of incorporating additional social relations on interpretation modeling performance, evaluating the models on datasets that explicitly capture these relations, and analyzing the generated interpretations for improved diversity and accuracy.

### Open Question 3
- Question: How can interpretation modeling be applied to other domains beyond content moderation, such as customer feedback analysis or legal document interpretation?
- Basis in paper: [explicit] The authors mention potential applications of interpretation modeling in customer communication and professional communication, but do not explore these applications in detail.
- Why unresolved: The paper primarily focuses on the application of interpretation modeling in content moderation. Exploring its potential in other domains could reveal new insights and opportunities for the technique.
- What evidence would resolve it: Conducting case studies or experiments on interpretation modeling in different domains, evaluating the models' performance and effectiveness in these contexts, and analyzing the generated interpretations for domain-specific insights and applications.

## Limitations
- The dataset size (1,050 sentences) may limit generalization to broader contexts
- Manual annotation process for interpretations and moral judgments is labor-intensive and may not scale well
- Reliance on Virtue Ethics framework may not capture all relevant moral perspectives across diverse cultural contexts

## Confidence
- **High confidence**: One-to-one generation outperforms one-to-many settings for interpretation quality; models successfully reveal hidden toxicity in 87% of test sentences
- **Medium confidence**: Social grounding through reader attitudes and moral judgments enables contextually appropriate interpretations
- **Medium confidence**: Semantic similarity constraints in one-to-many generation improve diversity

## Next Checks
1. **Cross-cultural validation**: Test interpretation modeling with sentences from diverse cultural contexts and annotators from different backgrounds to assess generalization beyond the current dataset.
2. **Scalability assessment**: Measure the time required for human annotators to create interpretations and moral judgments, and explore whether the model can assist or automate parts of this process without significant quality degradation.
3. **Toxicity detection comparison**: Conduct a controlled experiment comparing content moderation performance using only original sentences versus using generated interpretations, measuring precision, recall, and false positive rates for detecting various toxic content types.