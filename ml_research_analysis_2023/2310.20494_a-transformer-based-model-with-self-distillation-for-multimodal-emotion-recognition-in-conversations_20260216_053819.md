---
ver: rpa2
title: A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition
  in Conversations
arxiv_id: '2310.20494'
source_url: https://arxiv.org/abs/2310.20494
tags:
- emotion
- multimodal
- fusion
- utterance
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses multimodal emotion recognition in conversations
  (ERC), which aims to recognize the emotion of each utterance in a conversation using
  textual, acoustic, and visual modalities. Existing studies mainly focus on textual
  conversations and ignore the importance of multimodal information.
---

# A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition in Conversations

## Quick Facts
- arXiv ID: 2310.20494
- Source URL: https://arxiv.org/abs/2310.20494
- Reference count: 40
- Key outcome: SDT achieves 73.95% accuracy and 74.08% weighted F1-score on IEMOCAP, and 67.55% accuracy and 66.60% weighted F1-score on MELD

## Executive Summary
This paper addresses multimodal emotion recognition in conversations (ERC) by proposing a transformer-based model with self-distillation (SDT). The model tackles three key challenges: capturing intra- and inter-modal interactions between utterances, dynamically learning modality contributions, and enhancing modal representations. Through hierarchical gated fusion and self-distillation mechanisms, SDT outperforms previous state-of-the-art baselines on IEMOCAP and MELD datasets, demonstrating the effectiveness of combining transformer architectures with knowledge transfer techniques for multimodal emotion understanding.

## Method Summary
The SDT model consists of a modality encoder with intra- and inter-modal transformers, a hierarchical gated fusion module, an emotion classifier, and a self-distillation component. Utterance-level features are extracted using pre-trained models (RoBERTa for text, openSMILE for audio, DenseNet for visual), then processed through 1D convolutions and positional embeddings. Intra-modal transformers capture within-modality dependencies while inter-modal transformers enable cross-modal information flow. A two-level gated fusion mechanism first enhances individual modalities and then dynamically weights their contributions. The self-distillation module uses the model's own soft predictions as additional supervision for each modality-specific student network.

## Key Results
- SDT achieves 73.95% accuracy and 74.08% weighted F1-score on IEMOCAP dataset
- SDT achieves 67.55% accuracy and 66.60% weighted F1-score on MELD dataset
- Outperforms previous state-of-the-art baselines on both datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-distillation transfers soft label knowledge from the teacher model to each modality's student model, improving modal representations.
- Mechanism: The teacher model generates soft probability distributions over emotion classes. These soft labels are used as extra training supervision for each modality-specific student model via KL divergence loss. This regularizes each modality's representation learning by aligning it with the teacher's richer class probability distribution.
- Core assumption: Soft labels contain "dark knowledge" that captures inter-class relationships and uncertainty better than hard labels.
- Evidence anchors:
  - [abstract] "Furthermore, to learn more expressive modal representations, we treat soft labels of the proposed model as extra training supervision."
  - [section] "We introduce self-distillation to transfer knowledge of hard and soft labels from the proposed model to each modality."
- Break condition: If soft labels don't provide meaningful regularization (e.g., model is already perfect or temperature τ is poorly chosen), self-distillation could hurt performance or be ineffective.

### Mechanism 2
- Claim: Hierarchical gated fusion dynamically learns modality weights and filters irrelevant information at both unimodal and multimodal levels.
- Mechanism: First, each modality representation is enhanced by gating mechanisms that filter out irrelevant information from intra- and inter-modal transformers. Then, at the multimodal level, another gating mechanism using softmax dynamically learns weights between modalities for each utterance. This two-level gating ensures both modality-specific enhancement and adaptive fusion.
- Core assumption: Different utterances require different modalities to have varying importance, and some information within each modality is irrelevant.
- Evidence anchors:
  - [section] "We design a hierarchical gated fusion module containing unimodal- and multimodal-level gated fusions to adaptively obtain enhanced single-modality sequence representation and dynamically learn weights between these enhanced modality representations, respectively."
- Break condition: If the gating mechanisms become too aggressive (too many zeros), useful information might be lost. If the gating is too permissive, redundant information might remain.

### Mechanism 3
- Claim: Intra- and inter-modal transformers capture both within-modality and cross-modality interactions between conversation utterances.
- Mechanism: Intra-modal transformers enhance each modality's sequence representation by modeling dependencies within that modality. Inter-modal transformers allow each modality to get information from other modalities, capturing cross-modal interactions. This dual attention mechanism enables the model to understand how modalities relate both within themselves and to each other across the conversation.
- Core assumption: Emotional understanding requires both understanding how different expressions within a modality relate over time and how different modalities complement each other.
- Evidence anchors:
  - [abstract] "The transformer-based model captures intra- and inter-modal interactions by utilizing intra- and inter-modal transformers..."
  - [section] "For the intra-modal transformer, we take Hm as queries, keys, and values... For the inter-modal transformer, we take Hm as queries, and Hn as keys and values..."
- Break condition: If the conversation utterances are too short or the modality features are already highly independent, the transformers might not learn meaningful interactions.

## Foundational Learning

- Concept: Self-distillation and knowledge transfer
  - Why needed here: To leverage the teacher model's superior representation learning to improve each modality's individual representation without requiring additional network parameters.
  - Quick check question: What is the difference between hard labels and soft labels in knowledge distillation, and why are soft labels often more informative?

- Concept: Multimodal fusion strategies (early, late, and model fusion)
  - Why needed here: Understanding different fusion approaches helps appreciate why hierarchical gated fusion is chosen over simple concatenation or element-wise addition, and how it addresses the limitations of early and late fusion.
  - Quick check question: What are the main limitations of early fusion and late fusion, and how does model fusion (like the proposed approach) address them?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The model uses both intra- and inter-modal transformers with multi-head attention to capture complex interactions. Understanding self-attention and cross-attention is crucial for grasping how the model processes multimodal conversational data.
  - Quick check question: How does the self-attention mechanism in transformers allow for capturing long-range dependencies, and how is this useful for modeling conversational context?

## Architecture Onboarding

- Component map: Feature extraction -> 1D convolution -> positional embeddings + speaker embeddings -> intra-modal transformers + inter-modal transformers -> unimodal-level gated fusion -> multimodal-level gated fusion -> emotion classifier -> self-distillation supervision

- Critical path: Feature extraction → modality encoder (with transformers) → hierarchical gated fusion → emotion classifier → self-distillation supervision

- Design tradeoffs: Using transformers for modality interactions increases computational cost (O(N²)) but captures complex dependencies; hierarchical gating adds parameters but enables adaptive fusion; self-distillation requires training-time overhead but improves representations.

- Failure signatures: Poor performance on minority emotion classes, failure to detect emotional shifts, or degraded performance when self-distillation is removed could indicate issues with modality interaction modeling or representation learning.

- First 3 experiments:
  1. Remove self-distillation (LCE and LKL losses) to verify its contribution to performance.
  2. Replace hierarchical gated fusion with simple concatenation to assess the value of adaptive gating.
  3. Remove either intra-modal or inter-modal transformers to determine which type of interaction is more critical for the task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed self-distillation approach compare to other knowledge distillation methods (offline distillation, online distillation) for multimodal emotion recognition in conversations?
- Basis in paper: [explicit] The paper mentions that most existing models belong to offline distillation, which requires training a separate teacher network. In contrast, self-distillation needs no extra network except for the network itself.
- Why unresolved: The paper only implements self-distillation and does not compare it to other knowledge distillation methods.
- What evidence would resolve it: Conducting experiments comparing self-distillation to offline and online distillation methods on the same datasets and metrics.

### Open Question 2
- Question: How does the proposed model perform on other multimodal datasets beyond IEMOCAP and MELD, such as MOSEI or CMU-MOSI?
- Basis in paper: [inferred] The paper only evaluates the proposed model on IEMOCAP and MELD datasets, which are commonly used for multimodal emotion recognition in conversations. However, there are other multimodal datasets available for sentiment analysis and emotion recognition.
- Why unresolved: The paper does not provide any results on other multimodal datasets.
- What evidence would resolve it: Conducting experiments on other multimodal datasets using the same model architecture and hyperparameters.

### Open Question 3
- Question: How does the proposed model handle noisy or missing modalities in real-world scenarios?
- Basis in paper: [inferred] The paper assumes that all three modalities (text, audio, visual) are available for each utterance in the conversation. However, in real-world scenarios, some modalities may be missing or noisy due to various reasons (e.g., poor audio quality, occlusions in visual data).
- Why unresolved: The paper does not address the issue of handling missing or noisy modalities.
- What evidence would resolve it: Conducting experiments with simulated missing or noisy modalities and evaluating the model's performance under these conditions.

## Limitations
- Self-distillation effectiveness depends heavily on the temperature parameter τ, which is not thoroughly explored across different values
- Hierarchical gated fusion introduces additional complexity without clear justification for why two levels of gating are necessary versus alternative fusion strategies
- No analysis of computational overhead compared to simpler baseline models, which is important for practical deployment

## Confidence
- **High Confidence**: The core architecture design (transformers + hierarchical gating) and baseline performance claims are well-supported by the experimental results on established datasets (IEMOCAP, MELD)
- **Medium Confidence**: The specific contribution of self-distillation is somewhat conflated with other architectural improvements, making it difficult to isolate its exact impact on performance gains
- **Low Confidence**: The paper lacks ablation studies on critical design choices like the number of transformer layers, gating mechanisms' threshold values, and temperature parameter selection for self-distillation

## Next Checks
1. **Ablation Study on Self-Distillation**: Remove the LCE and LKL loss components from the training objective and measure performance degradation. This would quantify the exact contribution of self-distillation versus other architectural components.

2. **Gating Mechanism Analysis**: Replace the hierarchical gated fusion with simple weighted summation or concatenation, then systematically evaluate how performance changes when adding each gating level. This would reveal whether both unimodal and multimodal gating are necessary.

3. **Cross-Dataset Robustness Test**: Train the model on IEMOCAP and evaluate on MELD (and vice versa) to assess how well the learned intra- and inter-modal interactions generalize across different conversational contexts and annotation schemes.