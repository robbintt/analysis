---
ver: rpa2
title: 'NOLA: Compressing LoRA using Linear Combination of Random Basis'
arxiv_id: '2310.02556'
source_url: https://arxiv.org/abs/2310.02556
tags:
- nola
- parameters
- lora
- rank
- gpt-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NOLA addresses the compression limitations of LoRA in large language
  models by reparameterizing low-rank matrices using linear combinations of random
  basis matrices, decoupling parameter count from rank and model architecture. This
  allows NOLA to achieve over 20x compression compared to LoRA rank-1 while maintaining
  or improving performance on tasks like E2E NLG, DART, and WebNLG datasets.
---

# NOLA: Compressing LoRA using Linear Combination of Random Basis

## Quick Facts
- arXiv ID: 2310.02556
- Source URL: https://arxiv.org/abs/2310.02556
- Reference count: 18
- Primary result: NOLA achieves over 20x compression compared to LoRA rank-1 while maintaining or improving performance on NLG and vision tasks

## Executive Summary
NOLA introduces a novel parameter-efficient fine-tuning method for large language models and vision transformers that overcomes the compression limitations of LoRA. By reparameterizing low-rank matrices as linear combinations of random basis matrices, NOLA decouples the number of trainable parameters from both the rank and network architecture. This approach achieves significant compression ratios while maintaining or improving performance across multiple tasks including E2E NLG, DART, WebNLG, and vision classification datasets. NOLA also demonstrates strong robustness to quantization and offers faster training compared to existing methods like PRANC.

## Method Summary
NOLA reparameterizes the low-rank matrices used in LoRA by expressing them as linear combinations of randomly generated basis matrices. Instead of directly optimizing rank-decomposition matrices A and B, NOLA computes them as weighted sums of frozen random basis matrices: A = Σα_i A_i and B = Σβ_j B_j. Only the linear coefficients α and β are optimized during training, allowing NOLA to achieve higher compression ratios than LoRA while maintaining or improving performance. The method also supports quantization of the coefficients to further reduce storage requirements without significant performance degradation.

## Key Results
- Achieves over 20x compression compared to LoRA rank-1 on LLaMA-2 70B while maintaining accuracy
- Outperforms PRANC with 1.5x faster training and 1.6x lower memory footprint
- Demonstrates strong quantization robustness, maintaining performance at 4-bit quantization
- Shows consistent performance improvements across NLG tasks (E2E NLG, DART, WebNLG) and vision tasks (CIFAR-10, CIFAR-100, ImageNet-100)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NOLA overcomes the rank-one lower bound limitation of LoRA by reparameterizing low-rank matrices as linear combinations of randomly generated basis matrices.
- Mechanism: Instead of directly optimizing rank-decomposition matrices A and B, NOLA expresses them as weighted sums of frozen random basis matrices: A = Σα_i A_i and B = Σβ_j B_j. Only the linear coefficients α and β are optimized, decoupling the number of trainable parameters from the rank and matrix dimensions.
- Core assumption: The change in neural weights during fine-tuning is intrinsically low-rank (Hu et al., 2021), and this low-rank structure can be captured effectively using random basis expansions.
- Evidence anchors:
  - [abstract] "It achieves this by re-parameterizing the low-rank matrices in LoRA using linear combinations of randomly generated matrices (basis) and optimizing the linear mixture coefficients only."
  - [section 2] "Unlike PRANC, our focus in NOLA is on reparameterizing the change of neural weights for fine-tuning large models... we utilize the rank-decomposition presented in LoRA but assemble the rank-decomposition matrices as a linear combination of pseudo-random matrices."
  - [corpus] Weak evidence; corpus papers focus on LoRA variants but don't discuss random basis reparameterization directly.
- Break condition: If the intrinsic low-rank assumption doesn't hold for a particular task or model, the random basis expansion may not capture the necessary weight changes effectively, leading to performance degradation.

### Mechanism 2
- Claim: NOLA achieves higher compression ratios than LoRA while maintaining or improving performance by reducing the number of trainable parameters independently of rank and architecture.
- Mechanism: By using random basis matrices, NOLA can achieve the same effective rank with fewer parameters than LoRA's direct rank decomposition. For example, NOLA with rank 8 and 0.036M parameters achieves BLEU 70.12, while LoRA rank 4 with 0.77M parameters achieves BLEU 70.4 on GPT-L.
- Core assumption: The random basis matrices can span the necessary weight change space efficiently, allowing fewer coefficients to represent the same or better functionality than higher-rank LoRA matrices.
- Evidence anchors:
  - [section 3.1] "On LLaMA-2 70B, our method is almost 20 times more compact than the most compressed LoRA without degradation in accuracy."
  - [section 2] "This approach allows us to decouple the number of trainable parameters from both the choice of rank and the network architecture."
  - [corpus] Weak evidence; related work focuses on LoRA efficiency but doesn't demonstrate the same compression-decoupling property.
- Break condition: If the random basis matrices don't align well with the actual weight change directions needed for a specific task, the compression advantage may come at the cost of representational capacity.

### Mechanism 3
- Claim: NOLA can be quantized to very low bit-widths (e.g., 4-bit) without significant performance degradation, offering additional compression beyond the basis coefficient reduction.
- Mechanism: NOLA quantizes only the linear coefficients α and β while keeping the random basis matrices in higher precision. This selective quantization preserves most of the representational capacity while achieving substantial storage savings.
- Core assumption: The random basis matrices contain most of the information needed for adaptation, so quantizing only the coefficients doesn't significantly impact performance.
- Evidence anchors:
  - [section 3.2] "We observe no significant drop in both LoRA and NOLA in 4 bits PTQ experiments... In QAT with 3 bits, NOLA has a slight drop of 0.3 points, while LoRA has a drop of 2.8 points in the BLEU metric."
  - [section 2] "we quantize the α and β coefficients to lower precision (e.g., 4 bits) while the random basis and the pre-trained LLM weights have standard FP16 floating point precision."
  - [corpus] Weak evidence; quantization of LoRA variants exists but specific comparison to NOLA quantization is not in corpus.
- Break condition: If aggressive quantization of coefficients causes the linear combinations to lose precision in representing the necessary weight changes, performance will degrade despite the compression benefits.

## Foundational Learning

- Concept: Low-rank matrix decomposition and its role in parameter-efficient fine-tuning
  - Why needed here: Understanding why LoRA works and its limitations is crucial for grasping NOLA's innovation in breaking the rank-one lower bound.
  - Quick check question: Why does constraining weight changes to low-rank matrices reduce the number of parameters needed for fine-tuning?

- Concept: Random basis expansions and linear combinations for function approximation
  - Why needed here: NOLA's core mechanism relies on representing weight changes as linear combinations of random basis matrices, which requires understanding how such expansions can approximate arbitrary matrices.
  - Quick check question: How does representing a matrix as a linear combination of random basis matrices differ from directly parameterizing the matrix elements?

- Concept: Quantization-aware training and its impact on model performance
  - Why needed here: NOLA's ability to be quantized while maintaining performance is a key advantage, requiring understanding of how quantization affects different types of parameters.
  - Quick check question: What is the difference between post-training quantization and quantization-aware training, and why might the latter perform better?

## Architecture Onboarding

- Component map: Pre-trained weights W -> Frozen random basis matrices A_i, B_j -> Trainable coefficients α, β -> Weight update ΔW = (Σα_i A_i)(Σβ_j B_j)
- Critical path: During training, the critical path involves generating random basis matrices on-the-fly (if not pre-generated), computing the linear combinations using coefficients α and β, and applying the resulting weight update to the frozen pre-trained weights.
- Design tradeoffs: NOLA trades off some representational capacity (compared to full fine-tuning) for significant parameter reduction. The choice of basis size (k and l) and rank affects the compression ratio and performance trade-off.
- Failure signatures: Poor performance may indicate that the random basis matrices don't align well with the actual weight changes needed, or that the rank is too low to capture the necessary transformations. Training instability could arise from poorly scaled random basis matrices.
- First 3 experiments:
  1. Implement NOLA on a small GPT-2 model for a simple text classification task, comparing performance and parameter count against LoRA with various ranks.
  2. Test NOLA's quantization robustness by applying 4-bit quantization to the coefficients and measuring performance degradation.
  3. Evaluate NOLA on a vision transformer for image classification, comparing against LoRA and full fine-tuning in terms of accuracy and parameter efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical basis for NOLA's improved representation power compared to PRANC, and can this be formally proven?
- Basis in paper: [inferred] The authors mention that NOLA has better representation power than PRANC, but they leave the theoretical study of this comparison for future work.
- Why unresolved: The paper does not provide a formal proof or theoretical analysis of why NOLA has better representation power than PRANC.
- What evidence would resolve it: A rigorous mathematical proof demonstrating the superiority of NOLA's representation power over PRANC, or empirical evidence showing that NOLA consistently outperforms PRANC across a wide range of tasks and architectures.

### Open Question 2
- Question: How does the choice of rank in NOLA affect its performance, and is there an optimal rank for different tasks and architectures?
- Basis in paper: [explicit] The authors note that understanding the effect of rank in NOLA needs more rigorous study as future work.
- Why unresolved: The paper does not provide a comprehensive analysis of how the choice of rank affects NOLA's performance, and whether there is an optimal rank for different tasks and architectures.
- What evidence would resolve it: A systematic study of NOLA's performance across different tasks and architectures with varying ranks, identifying the optimal rank for each scenario.

### Open Question 3
- Question: How does NOLA perform on other architectures beyond transformers and CNNs, such as recurrent neural networks or graph neural networks?
- Basis in paper: [inferred] The authors mention that NOLA can be applied to other architectures like CNNs, but they do not explore its performance on other types of architectures.
- Why unresolved: The paper does not provide any empirical evidence or theoretical analysis of NOLA's performance on architectures beyond transformers and CNNs.
- What evidence would resolve it: Empirical results showing NOLA's performance on various architectures, such as recurrent neural networks or graph neural networks, and a theoretical analysis of its applicability to these architectures.

## Limitations

- The performance of NOLA depends critically on the quality and coverage of the random basis matrices, with no theoretical guarantees that random bases will always capture necessary weight changes.
- The paper doesn't provide a comprehensive analysis of how the choice of rank affects NOLA's performance across different tasks and architectures.
- The quantization robustness claims are based on limited evaluation and need broader validation across different model architectures and quantization schemes.

## Confidence

**High Confidence**:
- NOLA achieves superior compression ratios compared to LoRA while maintaining comparable performance on tested tasks
- NOLA demonstrates faster training and reduced memory footprint compared to PRANC
- The reparameterization approach effectively decouples parameter count from rank and architecture constraints

**Medium Confidence**:
- NOLA's performance improvement over LoRA is consistent across all tested vision and language tasks
- The random basis expansion can capture all necessary weight changes for diverse fine-tuning scenarios
- NOLA's quantization robustness extends to all reasonable bit-widths and model types

**Low Confidence**:
- NOLA will maintain its advantages when applied to extremely large models (e.g., >100B parameters) or highly specialized domains
- The method's performance gains are primarily due to the random basis mechanism rather than other implementation details
- NOLA will outperform all existing LoRA variants across all possible evaluation scenarios

## Next Checks

1. **Basis Sensitivity Analysis**: Systematically vary the number of random basis matrices (k, l) and their initialization methods to determine the minimum requirements for effective performance and identify any failure modes.

2. **Cross-Domain Generalization Test**: Apply NOLA to a diverse set of domains including scientific text, code, and multilingual datasets to verify that the random basis mechanism works consistently across different data distributions and task types.

3. **Large-Scale Model Scaling**: Evaluate NOLA on models significantly larger than those tested (e.g., 175B+ parameter models) to verify that the compression and performance advantages scale proportionally with model size and don't degrade at scale.