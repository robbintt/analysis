---
ver: rpa2
title: Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse
  Contextualized Word Representations
arxiv_id: '2307.13776'
source_url: https://arxiv.org/abs/2307.13776
tags:
- language
- uni00000013
- representations
- multi
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the effectiveness of using monolingual pretrained
  language models for cross-lingual zero-shot word sense disambiguation (WSD). The
  key idea is to align contextualized word representations from different monolingual
  models via a mapping mechanism, and then obtain sparse contextualized representations
  through dictionary learning.
---

# Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations

## Quick Facts
- arXiv ID: 2307.13776
- Source URL: https://arxiv.org/abs/2307.13776
- Reference count: 17
- Primary result: Monolingual models with sparse representations improve average F-score by 6.5 points (68.5 vs 62.0) over 17 languages

## Executive Summary
This paper addresses cross-lingual zero-shot word sense disambiguation by leveraging monolingual pretrained language models with sparse contextualized representations. The key innovation involves aligning monolingual encoder spaces via isometric mapping and learning sparse codes through dictionary learning. This approach mitigates the curse of multilinguality while preserving sense distinctions better than dense representations. Experiments on 17 typologically diverse languages show significant improvements over multilingual baselines.

## Method Summary
The method aligns contextualized representations from monolingual BERT models using isometric (orthogonal Procrustes) or RCSLS mapping. Dictionary learning via SPAMS creates sparse codes using the learned dictionary, which are then used to compute PMI-based sense representations. The approach operates by encoding source language text to obtain sense-annotated sparse codes and dictionary, then mapping target language embeddings into the source space using learned alignment matrices before reconstructing sparse representations with the shared dictionary.

## Key Results
- Monolingual models with sparse representations achieve 68.5 average F-score vs 62.0 for multilingual models
- Isometric alignment consistently outperforms RCSLS mapping across languages
- Dictionary learning with λ=0.05 and k=3000 provides optimal sparsity for sense discrimination
- Sparse representations show 6.5-point improvement over dense representations on average

## Why This Works (Mechanism)

### Mechanism 1: Sparse Contextualized Representations
- Claim: Sparse contextualized representations preserve sense distinctions better than dense ones when aligned across languages
- Core assumption: Sense-relevant dimensions are sparse and consistent enough across languages to be captured by a common dictionary basis
- Evidence: Dictionary learning formulation with PMI-based sense encoding shows improved sense separation
- Break condition: If sense dimensions aren't sparse or language-specific, shared dictionary will conflate meanings

### Mechanism 2: Isometric Alignment
- Claim: Isometric alignment between monolingual encoder spaces preserves semantic geometry and improves cross-lingual transfer
- Core assumption: Contextualized embeddings from independently trained monolingual models live in geometrically similar semantic spaces
- Evidence: Orthogonal Procrustes mapping shows highest contextualized translation retrieval accuracy
- Break condition: If language-specific encoders produce incompatible semantic geometries, isometric constraint will force poor alignments

### Mechanism 3: Monolingual Encoder Advantage
- Claim: Monolingual encoders outperform multilingual ones due to mitigation of the curse of multilinguality
- Core assumption: Representation quality for a given language is inversely related to the number of languages jointly modeled
- Evidence: Monolingual models achieve 68.5 vs 62.0 average F-score over multilingual models
- Break condition: If multilingual model is sufficiently large or languages are closely related, performance gap may disappear

## Foundational Learning

- Concept: Dictionary learning for sparse coding
  - Why needed here: Enables creation of cross-lingual sense embeddings by enforcing sparsity, which aligns with sense boundaries
  - Quick check: What objective function is minimized to learn the dictionary D and sparse codes α in the paper?

- Concept: Orthogonal Procrustes problem
  - Why needed here: Provides mathematical foundation for isometric alignment between independently trained encoder spaces
  - Quick check: How is the optimal orthogonal mapping W computed from source and target representations?

- Concept: PMI-based sense representation
  - Why needed here: Converts sparse contextual patterns into sense-specific vectors for nearest-neighbor inference
  - Quick check: How are sense embeddings constructed from sparse codes and sense annotations in the paper?

## Architecture Onboarding

- Component map: Monolingual encoders (BERT-base variants) -> Dictionary learning module (SPAMS) -> Alignment module (isometric/RCSLS) -> PMI sense encoder -> WSD inference engine

- Critical path:
  1. Encode source language text with English BERT to obtain sense-annotated sparse codes and D
  2. For each target language: extract parallel corpus + word translation pairs, learn alignment W, encode target text and map to source space via W, obtain sparse codes using shared D, compute PMI sense matrix Φ, perform nearest-neighbor inference

- Design tradeoffs:
  - Dense vs sparse representations: Sparse gives better sense separation but requires dictionary learning
  - Identity vs learned mapping: Identity works for same multilingual encoder but learned mapping generalizes
  - PMI normalization: Optional per-language; improves some languages but not others

- Failure signatures:
  - Low translation retrieval accuracy → alignment mapping is poor
  - Similar F-scores for dense and sparse → sparsity not capturing sense distinctions
  - High variance across languages → monolingual encoder quality or data scarcity

- First 3 experiments:
  1. Run mono→mono with dense vectors on single language pair to verify alignment quality
  2. Enable dictionary learning (λ=0.05, k=3000) and compare dense vs sparse performance
  3. Test PMI normalization toggle on development set to determine optimal setting per language

## Open Questions the Paper Calls Out

- What specific architectural modifications to the alignment matrix or dictionary learning procedure could further improve cross-lingual sparse contextualized representations?
- How does the performance of cross-lingual WSD vary across different language families and linguistic typologies?
- What is the minimum amount of parallel data required to achieve robust cross-lingual mappings between monolingual models?

## Limitations
- Sparse representation sensitivity to hyperparameters (λ, k) may affect robustness across languages
- Alignment quality dependency on parallel data availability and quality
- PMI-based sense encoding methodology lacks detailed specification and per-language tuning transparency

## Confidence

- **High confidence**: Monolingual models outperform multilingual ones due to curse of multilinguality mitigation
- **Medium confidence**: Sparse contextualized representations improve sense distinction preservation
- **Low confidence**: Isometric alignment consistently preserves semantic geometry across all language pairs

## Next Checks

1. Ablation study on dictionary learning hyperparameters: Systematically vary λ (0.01-0.2) and k (1000-5000) across 3-4 diverse languages to establish robustness boundaries

2. Alignment quality correlation analysis: Measure contextualized translation retrieval accuracy against WSD F-scores for each language pair to identify quality degradation patterns

3. Dense vs sparse representation analysis: For languages showing minimal performance difference, conduct qualitative analysis of sense boundaries to determine if sparsity fails to capture distinctions or senses lack clear boundaries