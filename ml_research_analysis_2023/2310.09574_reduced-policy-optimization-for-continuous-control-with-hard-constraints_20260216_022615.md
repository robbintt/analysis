---
ver: rpa2
title: Reduced Policy Optimization for Continuous Control with Hard Constraints
arxiv_id: '2310.09574'
source_url: https://arxiv.org/abs/2310.09574
tags:
- constraints
- actions
- equality
- policy
- hard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Reduced Policy Optimization (RPO) introduces the Generalized Reduced
  Gradient (GRG) algorithm into off-policy reinforcement learning to handle general
  hard constraints in continuous control tasks. RPO partitions actions into basic
  and nonbasic actions, uses a policy network to predict basic actions, and solves
  equality constraints to obtain nonbasic actions.
---

# Reduced Policy Optimization for Continuous Control with Hard Constraints

## Quick Facts
- **arXiv ID:** 2310.09574
- **Source URL:** https://arxiv.org/abs/2310.09574
- **Reference count:** 40
- **Primary result:** RPO-DDPG and RPO-SAC achieve zero constraint violations while maintaining high episodic rewards across three new benchmarks

## Executive Summary
Reduced Policy Optimization (RPO) introduces the Generalized Reduced Gradient (GRG) algorithm into off-policy reinforcement learning to handle general hard constraints in continuous control tasks. The method partitions actions into basic and nonbasic components, uses a policy network to predict basic actions, and solves equality constraints to obtain nonbasic actions. Inequality constraints are enforced via a projection stage with GRG updates. RPO employs a modified Lagrangian relaxation with adaptive penalty factors to improve initial actions from the policy network. Three new benchmarks were developed to evaluate performance: Safe CartPole, Spring Pendulum, and Optimal Power Flow with Battery Energy Storage. Experimental results show RPO outperforms existing safe RL algorithms in terms of both cumulative reward and constraint violation across all benchmarks.

## Method Summary
RPO partitions actions into basic (nB) and nonbasic (nN) components following the GRG method. The policy network outputs basic actions, while nonbasic actions are computed by solving equality constraints. A projection stage applies GRG updates to satisfy inequality constraints while maintaining equality constraint satisfaction. The method uses a modified Lagrangian relaxation with adaptive penalty factors that increase based on observed constraint violations. This approach enables end-to-end training while ensuring hard constraints are satisfied throughout the learning process.

## Key Results
- RPO-DDPG and RPO-SAC achieve zero constraint violations across all three benchmarks
- RPO outperforms baseline algorithms (CPO, CUP, Safety Layer, DDPG-L, SAC-L) in both reward and constraint satisfaction
- The method successfully handles both equality and inequality hard constraints in continuous control tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The action partitioning into basic and nonbasic actions enables equality constraint satisfaction through equation solving
- **Mechanism:** By partitioning actions into basic (nB) and nonbasic (nN) components, equality constraints can be reformulated as equations in the nonbasic actions given fixed basic actions. The policy network only needs to predict basic actions, while nonbasic actions are computed by solving the equality constraints. This reduces the dimensionality of the policy output while ensuring hard constraints are satisfied.
- **Core assumption:** The Jacobian of the equality constraints with respect to nonbasic actions is non-singular, allowing unique solution computation
- **Evidence anchors:**
  - [abstract] "RPO partitions actions into basic actions and nonbasic actions following the GRG method and outputs the basic actions via a policy network. Subsequently, RPO calculates the nonbasic actions by solving equations based on equality constraints"
  - [section 4.1] "we follow the formulation in GRG method and divide actions a ∈ Rn into basic actions aB ∈ Rm and nonbasic actions aN ∈ Rn−m"

### Mechanism 2
- **Claim:** The modified Lagrangian relaxation with adaptive penalty factors ensures inequality constraint satisfaction while maintaining exploration
- **Mechanism:** Instead of fixed Lagrangian multipliers, RPO uses a dual update scheme where penalty factors νj are adaptively increased based on observed constraint violations. The exact penalty term max{0, gj(π(s); s)} ensures that when penalty factors are sufficiently large, the unconstrained problem becomes equivalent to the constrained problem.
- **Core assumption:** The penalty factors will eventually become large enough to enforce constraint satisfaction (νj ≥ ∥νjs∥∞)
- **Evidence anchors:**
  - [section 4.3] "we perform a dual update on Lagrangian multipliers to adaptively tune them during the training period"
  - [section 4.3] "the penalty factors are monotonically increasing during the training procedure"

### Mechanism 3
- **Claim:** The GRG-based projection stage corrects actions to satisfy inequality constraints without violating equality constraints
- **Mechanism:** After initial actions are computed, a projection stage applies GRG updates in the tangent space of the equality constraint manifold. The reduced gradient ∇aB G(aB, aN) ensures that updates move toward satisfying inequality constraints while maintaining equality constraint satisfaction through the relationship ∆aN = ∂ϕN(aB)/∂aB ∆aB.
- **Core assumption:** The projection step ηa is small enough to maintain equality constraint satisfaction for nonlinear constraints
- **Evidence anchors:**
  - [section 4.2] "the principal difficulty here is to project the action into the feasible region without violating the equality constraints"
  - [section 4.2] "this projection is an iterative procedure similar to GRG method"

## Foundational Learning

- **Concept:** Constrained Markov Decision Processes (CMDP)
  - **Why needed here:** RPO operates in a CMDP framework where hard constraints must be satisfied in addition to maximizing reward
  - **Quick check question:** What distinguishes hard constraints from soft constraints in reinforcement learning?

- **Concept:** Generalized Reduced Gradient (GRG) method
  - **Why needed here:** GRG provides the mathematical foundation for partitioning actions and performing constrained optimization within the RL framework
  - **Quick check question:** How does GRG partition variables and what is the reduced gradient formula?

- **Concept:** Implicit function theorem and differentiable optimization
  - **Why needed here:** The gradient flow from nonbasic to basic actions requires implicit differentiation to enable end-to-end training of the policy network
  - **Quick check question:** What conditions must hold for the implicit function theorem to apply in the context of equality constraints?

## Architecture Onboarding

- **Component map:** State → Policy network → Basic actions → Equation solver → Nonbasic actions → Projection stage → Feasible actions → Environment
- **Critical path:** State → Policy network → Basic actions → Equation solver → Nonbasic actions → Projection stage → Feasible actions → Environment
  - Backpropagation flows through: Feasible actions → Projection stage → Equation solver → Basic actions → Policy network

- **Design tradeoffs:**
  - Number of basic vs nonbasic actions affects equation complexity vs policy network expressiveness
  - Projection step size ηa balances constraint satisfaction speed vs equality constraint preservation
  - Penalty factor learning rate affects constraint enforcement speed vs stability

- **Failure signatures:**
  - Policy collapse: If penalty factors grow too quickly, exploration is suppressed
  - Constraint violation: If projection step is too large or equation solver fails
  - Slow convergence: If penalty factors increase too slowly or projection iterations are insufficient

- **First 3 experiments:**
  1. **Safe CartPole with only equality constraints:** Verify action partitioning and equation solving work correctly
  2. **Safe CartPole with both equality and inequality constraints:** Test the complete two-stage decision process
  3. **Spring Pendulum with state-dependent equality constraints:** Validate handling of non-trivial constraint formulations

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the RPO algorithm handle nonlinear equality constraints with high accuracy while maintaining computational efficiency?
- **Basis in paper:** [explicit] The paper mentions that RPO can handle nonlinear equality constraints by approximating the nonlinear manifold with the tangent space at each GRG update, but it requires setting a sufficiently small projection step.
- **Why unresolved:** The paper does not provide a detailed analysis of the accuracy and computational efficiency of RPO when handling nonlinear equality constraints.
- **What evidence would resolve it:** Experimental results showing the performance of RPO on tasks with nonlinear equality constraints, comparing it to other methods in terms of both accuracy and computational efficiency.

### Open Question 2
- **Question:** How does the choice of the number of maximum GRG updates (K) affect the performance of RPO in terms of both reward and constraint violation?
- **Basis in paper:** [explicit] The paper mentions that the choice of K does not have an obvious impact on the episodic reward when K is not too large, but a large K may lead to poor performance due to inaccurate gradient estimation.
- **Why unresolved:** The paper does not provide a comprehensive analysis of the impact of K on the performance of RPO across different tasks and constraint types.
- **What evidence would resolve it:** A systematic study of the impact of K on RPO's performance across various tasks and constraint types, including a detailed analysis of the trade-off between reward and constraint violation.

### Open Question 3
- **Question:** Can RPO be extended to handle both hard constraints and soft constraints simultaneously?
- **Basis in paper:** [explicit] The paper mentions that RPO can be easily extended to cases with both hard constraints and soft constraints as long as a neural network is utilized to fit the mapping between the state-action pair and the cost.
- **Why unresolved:** The paper does not provide a concrete method for extending RPO to handle both hard and soft constraints, nor does it discuss the potential challenges and benefits of such an extension.
- **What evidence would resolve it:** A proposed method for extending RPO to handle both hard and soft constraints, along with experimental results demonstrating its effectiveness compared to existing methods.

## Limitations

- The computational complexity of the equation solving step in high-dimensional action spaces is not fully characterized
- The stability of the adaptive penalty factor update scheme across diverse constraint types remains to be thoroughly validated
- The performance bounds on constraint satisfaction (especially for nonlinear equality constraints) are not explicitly provided

## Confidence

- **High confidence:** The basic framework of action partitioning and equation solving for equality constraints
- **Medium confidence:** The adaptive penalty factor scheme and its convergence properties
- **Medium confidence:** The GRG-based projection stage for inequality constraints

## Next Checks

1. **Sensitivity analysis:** Systematically vary the projection step size ηa and maximum GRG iterations K to characterize their impact on constraint satisfaction and convergence speed
2. **Scalability test:** Evaluate RPO on tasks with higher-dimensional action spaces to assess the computational burden of the equation solving step
3. **Robustness test:** Introduce noisy or partially observed constraints to evaluate RPO's ability to handle imperfect constraint information