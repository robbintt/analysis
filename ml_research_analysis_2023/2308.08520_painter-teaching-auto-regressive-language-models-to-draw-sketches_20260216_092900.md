---
ver: rpa2
title: 'Painter: Teaching Auto-regressive Language Models to Draw Sketches'
arxiv_id: '2308.08520'
source_url: https://arxiv.org/abs/2308.08520
tags:
- image
- sketch
- prompt
- text
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Painter is a large language model that generates sketches by directly
  producing virtual brush strokes in an auto-regressive manner, similar to how humans
  draw. The model is built by fine-tuning a pre-trained LLM on a newly created dataset
  called Multi-Object-Quick-Draw, which contains sketches expressed as strokes paired
  with text descriptions covering various object types and tasks.
---

# Painter: Teaching Auto-regressive Language Models to Draw Sketches

## Quick Facts
- arXiv ID: 2308.08520
- Source URL: https://arxiv.org/abs/2308.08520
- Reference count: 39
- Painter achieves 40.26% classification accuracy, 20.64 dB PSNR for partial object removal, and 23.90 dB PSNR for sketch reproduction

## Executive Summary
Painter is a large language model that generates sketches by producing virtual brush strokes in an auto-regressive manner, mimicking human drawing workflows. Built by fine-tuning OPT-1.3B on the Multi-Object-Quick-Draw dataset, Painter can generate sketches from text prompts, remove objects from drawings, and classify objects within sketches. The model integrates a visual feedback loop and is trained on multiple tasks to improve grounding and performance.

## Method Summary
Painter fine-tunes a pre-trained OPT-1.3B LLM with residual cross-attention blocks to generate sketches as sequences of brush strokes. The model uses a frozen ResNet-50 for visual feature extraction and implements a visual feedback loop to monitor canvas state during generation. Training involves masked cross-entropy loss on the Multi-Object-Quick-Draw dataset, with top-p sampling (p=0.9) for generation tasks and greedy sampling for classification.

## Key Results
- 40.26% classification accuracy on object recognition within sketches
- 20.64 dB PSNR for partial object removal tasks
- 23.90 dB PSNR for sketch reproduction quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Auto-regressive stroke generation mimics human drawing workflow
- Mechanism: Sequential stroke prediction builds complexity incrementally, similar to human sketching
- Core assumption: Stroke-level supervision captures spatial relationships and compositional structure better than pixel-level generation
- Evidence anchors:
  - [abstract] "Painter draws sketches the way humans do i.e., by generating a sequence of brush strokes in an auto-regressive way"
  - [section] "Unlike the existing image generation methods [25, 5], Painter draws sketches the way humans do i.e., by generating a sequence of brush strokes in an auto-regressive way"
- Break condition: Insufficient stroke-level supervision data or ambiguous stroke ordering for complex scenes

### Mechanism 2
- Claim: Multi-task training improves grounding through diverse prompts
- Mechanism: Training on multiple tasks forces robust object, location, and relationship representations
- Core assumption: Task diversity provides complementary supervision signals that reinforce each other
- Evidence anchors:
  - [abstract] "Painter can generate sketches from text descriptions, remove objects from canvas, and detect and classify objects in sketches"
  - [section] "Tasks: while the primary application of Painter is text-to-sketch conversion, we train it on auxiliary tasks to improve the performance on the primary task via better object, location, and relationship grounding"
- Break condition: Imbalanced task distribution or task-specific degradation

### Mechanism 3
- Claim: Visual feedback loop enables iterative refinement during generation
- Mechanism: Model observes canvas state after each stroke, allowing correction and refinement of subsequent strokes
- Core assumption: Human-like drawing requires continuous observation of work-in-progress
- Evidence anchors:
  - [abstract] "We equip the LLM with a visual feedback loop to monitor the state of the canvas as image generation progresses"
  - [section] "In order to mimic the way humans paint by looking at the canvas while drawing, we equip Painter with a visual feedback loop to monitor the state of the canvas while generating strokes"
- Break condition: Feedback loop introduces instability or computational overhead becomes prohibitive

## Foundational Learning

- Concept: Auto-regressive sequence generation
  - Why needed here: Painter needs to generate strokes sequentially where each depends on previous strokes
  - Quick check question: How does the model decide what to generate next in an auto-regressive setup?

- Concept: Cross-attention mechanisms
  - Why needed here: Model needs to align visual features with text tokens to understand what to draw
  - Quick check question: What happens in the cross-attention layers when the model processes an image placeholder token?

- Concept: Residual connections in transformers
  - Why needed here: Preserves information flow from earlier layers when adding cross-attention outputs
  - Quick check question: Why does the cross-attention output get added to the hidden state rather than replacing it?

## Architecture Onboarding

- Component map: Text tokens + Image placeholders → Image feature extractor (frozen ResNet-50) → Modified LLM with cross-attention blocks → Visual feedback mechanism → Stroke generation output layer

- Critical path: Text encoding → Cross-attention with visual features → Hidden state updates → Stroke prediction → Canvas update → Feedback loop

- Design tradeoffs:
  - Using frozen ResNet-50 vs. trainable vision encoder
  - Number of cross-attention blocks vs. computational cost
  - Top-p sampling vs. greedy sampling for different tasks

- Failure signatures:
  - Poor classification accuracy → Issues with object grounding
  - Low PSNR in removal tasks → Problems with spatial understanding
  - Mode collapse → Insufficient diversity in training data or sampling

- First 3 experiments:
  1. Test cross-attention visualization to verify object localization
  2. Compare greedy vs. top-p sampling on classification task
  3. Measure impact of visual feedback loop by disabling it

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would expanding the object vocabulary beyond QuickDraw's 345 classes affect Painter's performance and generalization capabilities?
- Basis in paper: [explicit] The paper acknowledges the limited number of object categories (345 QuickDraw classes) and mentions working on expanding vocabulary via reinforcement learning
- Why unresolved: No experimental results provided for expanded vocabularies or performance comparisons across different vocabulary sizes
- What evidence would resolve it: Performance comparisons with different object vocabulary sizes showing classification accuracy, generation quality, and task completion rates

### Open Question 2
- Question: What is the impact of different image feature extractors on Painter's performance compared to using ResNet-50?
- Basis in paper: [explicit] Paper states ResNet-50 is chosen over ViT/CLIP because these are biased toward high-level features while pixel-level information is needed for better grounding
- Why unresolved: No comparative results using different feature extractors to substantiate the pixel-level information claim
- What evidence would resolve it: Performance comparisons using ResNet-50, ViT, and CLIP on the same tasks with metrics like classification accuracy, PSNR values, and qualitative output differences

### Open Question 3
- Question: How does the visual feedback loop mechanism affect Painter's performance compared to a version without this capability?
- Basis in paper: [explicit] Paper describes implementing visual feedback loop to monitor canvas state while generating strokes but provides no ablation studies
- Why unresolved: No experimental results comparing performance with and without the visual feedback loop across various tasks
- What evidence would resolve it: Ablation studies showing performance metrics with and without visual feedback loop, along with qualitative comparisons of generated sketches

## Limitations

- Limited object vocabulary restricted to 345 QuickDraw classes
- Relatively low classification accuracy (40.26%) compared to modern image classifiers
- Lack of ablation studies to quantify individual contributions of architectural components

## Confidence

**High Confidence**: Core concept of auto-regressive stroke generation using LLMs is well-supported by demonstrated capabilities in text-to-sketch generation, object removal, and classification

**Medium Confidence**: Mechanism claims about visual feedback loops and multi-task training improving performance are plausible but lack rigorous ablation studies

**Low Confidence**: Claims about human-like drawing capabilities are largely qualitative and subjective without quantitative measures of stroke naturalness or temporal coherence

## Next Checks

1. Implement Grad-CAM or attention weight visualization to verify cross-attention mechanisms correctly focus on relevant regions when generating strokes for specific objects

2. Create a variant of Painter without the visual feedback mechanism and compare performance on all three tasks to quantify the contribution of iterative canvas observation

3. Develop metrics beyond PSNR that measure stroke quality, such as stroke length distribution, curvature smoothness, or temporal consistency for multi-stroke sketches