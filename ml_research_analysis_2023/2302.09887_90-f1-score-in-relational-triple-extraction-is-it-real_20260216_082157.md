---
ver: rpa2
title: '90% F1 Score in Relational Triple Extraction: Is it Real ?'
arxiv_id: '2302.09887'
source_url: https://arxiv.org/abs/2302.09887
tags:
- relation
- extraction
- tuples
- classi
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the evaluation of joint entity and relation
  extraction models under more realistic conditions by including sentences without
  relational triples (zero-cardinality sentences), which are often excluded in existing
  benchmarks. The authors evaluate 9 state-of-the-art models on two New York Times
  datasets (NYT24 and NYT29) with and without zero-cardinality sentences.
---

# 90% F1 Score in Relational Triple Extraction: Is it Real ?

## Quick Facts
- **arXiv ID**: 2302.09887
- **Source URL**: https://arxiv.org/abs/2302.09887
- **Reference count**: 3
- **Key outcome**: Including zero-cardinality sentences in evaluation causes 6-15% F1 score drop in state-of-the-art joint entity and relation extraction models

## Executive Summary
This paper challenges the validity of high F1 scores reported in relational triple extraction by introducing more realistic evaluation settings that include sentences without any relational tuples. The authors evaluate 9 state-of-the-art joint extraction models on two New York Times datasets, demonstrating significant performance degradation (10-15% drop in one dataset and 6-14% in another) when zero-cardinality sentences are included. To address this issue, they propose a two-step approach using a BERT-based classifier to filter out sentences without tuples before applying joint extraction models. This approach improves or achieves competitive performance compared to end-to-end modeling, with up to 8% improvement in F1 scores in realistic settings.

## Method Summary
The paper proposes a two-step modeling approach for joint entity and relation extraction that first classifies whether a sentence contains relational tuples using a BERT-based classifier, then applies joint extraction models only to sentences predicted to contain tuples. The method is evaluated on modified New York Times datasets (NYT24* and NYT29*) that include zero-cardinality sentences. The authors compare this approach against end-to-end modeling on nine state-of-the-art joint extraction models (OneRel, BiRTE, TDEER, PRGC, GRTE, PtrNet, CasRel, TPLinker, PFN) using both binary and multi-class multi-label classification strategies for the filtering step.

## Key Results
- F1 scores drop by approximately 10-15% in NYT24* and 6-14% in NYT29* when zero-cardinality sentences are included in evaluation
- Two-step approach improves or matches end-to-end performance, with up to 8% improvement in F1 scores
- The performance degradation is primarily due to precision drops as models incorrectly extract tuples from empty sentences
- Multi-class multi-label classification for filtering performs competitively with binary classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filtering sentences with zero tuples before relation extraction improves overall F1 score.
- Mechanism: A BERT-based classifier identifies sentences that contain relational tuples by detecting "clue tokens" that signal the presence of relations, allowing subsequent extraction models to focus only on relevant sentences.
- Core assumption: Sentences containing relational tuples have distinguishable linguistic patterns that can be learned by a classifier without knowing the exact entities.
- Evidence anchors:
  - [abstract]: "We propose a two-step modeling approach that utilizes a simple BERT-based classifier. This approach leads to overall performance improvement in these models within the realistic experimental setting."
  - [section 4.2]: "We observe that sentences have relation specific clue tokens for most of the relation tuples... With this observation, we explore if a BERT-based classification model can learn the presence of relation tuples in these sentences using the clue tokens without identifying the corresponding entities."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.433. Top related titles: BitCoin: Bidirectional Tagging and Supervised Contrastive Learning based Joint Relational Triple Extraction Framework. Weak corpus evidence - no direct citations to this specific filtering approach.
- Break condition: If clue tokens are absent or ambiguous in zero-cardinality sentences, or if the classifier produces too many false positives/negatives, the filtering step could harm overall performance.

### Mechanism 2
- Claim: Including zero-cardinality sentences in training significantly degrades extraction model performance.
- Mechanism: When extraction models encounter sentences without any tuples during inference but were only trained on sentences with tuples, they produce false positives by incorrectly identifying relations, causing precision to drop sharply.
- Core assumption: Models learn to expect tuples in all sentences during training, leading to overconfident predictions on empty sentences during inference.
- Evidence anchors:
  - [abstract]: "Our experiments show that the performances of these models decline significantly in the real-world settings. We observe ∼ 10− 15% drop in one dataset and ∼ 6− 14% drop in another dataset in the F1 score across all of these models."
  - [section 5]: "The precision score drops sharply for all these models as they extract tuples from the sentences that do not have any tuples... This is expected as models do not see any examples with zero tuples during training."
  - [corpus]: Weak evidence - no direct citations supporting this specific degradation pattern.
- Break condition: If extraction models are inherently robust to empty sentences or if the proportion of zero-cardinality sentences is very small, the performance drop may be negligible.

### Mechanism 3
- Claim: Two-step modeling (classification + extraction) either outperforms or matches end-to-end modeling in realistic settings.
- Mechanism: By separating the zero-cardinality detection from the actual tuple extraction, the extraction model is trained only on positive examples, avoiding the adversarial effect of zero-cardinality sentences on its learning process.
- Core assumption: The classification and extraction tasks can be effectively decoupled without losing important contextual information needed for accurate extraction.
- Evidence anchors:
  - [abstract]: "We propose a two-step modeling using a simple BERT-based classifier that leads to improvement in the overall performance of these models in this realistic experimental setup."
  - [section 5]: "We observe an improvement of up to ∼ 8% in the 'WZ' testset of both NYT24* and NYT29* in this two-step approach over the end-to-end approach."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.433. Top related titles include frameworks with multi-stage approaches. Weak direct evidence for this specific two-step improvement.
- Break condition: If the classification step introduces significant latency or if the error propagation from classification to extraction is high, the two-step approach may not be beneficial in practice.

## Foundational Learning

- Concept: Joint entity and relation extraction
  - Why needed here: The paper evaluates multiple state-of-the-art joint extraction models that simultaneously identify entities and their relationships.
  - Quick check question: What is the key difference between pipeline and joint approaches to relation extraction?

- Concept: Distant supervision and dataset construction
  - Why needed here: The NYT datasets used are created using distant supervision, which affects the presence and quality of "clue tokens" and influences how zero-cardinality sentences are handled.
  - Quick check question: How does distant supervision impact the distribution of tuples in the training data?

- Concept: Multi-class multi-label classification
  - Why needed here: The paper proposes using MCML classification to detect which relations are present in a sentence, which is more informative than binary classification for filtering.
  - Quick check question: What is the advantage of MCML over binary classification in the context of filtering sentences for relation extraction?

## Architecture Onboarding

- Component map: Input sentence → BERT classifier → (if tuples detected) → Joint extraction model → Output tuples
- Critical path: Input sentence → BERT classifier → (if tuples detected) → Joint extraction model → Output tuples
- Design tradeoffs: The two-step approach adds computational overhead for classification but improves precision by avoiding false positives from empty sentences. The classification model must be highly accurate to avoid discarding sentences with valid tuples.
- Failure signatures: Significant drop in recall if the classifier incorrectly filters out sentences with tuples; performance degradation if the classifier produces many false positives, causing extraction models to process empty sentences.
- First 3 experiments:
  1. Train and evaluate the BERT classifier on the WZ training set, measuring precision/recall on the validation set.
  2. Train each joint extraction model on the NZ training set and evaluate on the NZ test set to establish baseline performance.
  3. Implement the two-step pipeline and evaluate on the WZ test set, comparing F1 scores against end-to-end performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can joint extraction models be improved to handle sentences with zero relational tuples more