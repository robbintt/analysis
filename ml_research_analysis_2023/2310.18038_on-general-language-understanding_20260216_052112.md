---
ver: rpa2
title: On General Language Understanding
arxiv_id: '2310.18038'
source_url: https://arxiv.org/abs/2310.18038
tags:
- language
- understanding
- press
- university
- general
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critiques the use of benchmarks like GLUE for evaluating
  "General Language Understanding" (GLU) in NLP models, arguing they fail to capture
  the breadth of GLU. It proposes a multi-dimensional model of language use situations
  (interactivity, shared context) and a cognitive model of understanding (incremental
  processing, multimodal grounding, conversational grounding).
---

# On General Language Understanding

## Quick Facts
- arXiv ID: 2310.18038
- Source URL: https://arxiv.org/abs/2310.18038
- Reference count: 10
- Primary result: Critiques GLUE benchmarks for inadequate coverage of "General Language Understanding" and proposes a multi-dimensional framework for rethinking evaluation

## Executive Summary
This paper argues that current benchmarks like GLUE fail to capture the full breadth of "General Language Understanding" (GLU) by focusing narrowly on decontextualized language use. The authors propose a multi-dimensional model of language use situations (interactivity and shared context) and a cognitive model of understanding (incremental processing, multimodal grounding, conversational grounding). The paper introduces three desiderata for GLU evaluation: coverage of diverse language use types, explicit assumptions about the understanding construct, and clarity on the understanding indicator's social role.

## Method Summary
The paper provides a theoretical framework analyzing the underspecification of "understanding" in current NLP evaluation methods. It maps language use situations along interactivity and shared context dimensions, models understanding as a cognitive process involving incremental processing and multimodal grounding, and distinguishes between understanding as symptom versus signal. The framework identifies coverage gaps in existing benchmarks and proposes desiderata for more comprehensive GLU evaluation, though it does not present empirical experiments or specific implementation guidelines.

## Key Results
- Current GLUE-style benchmarks cover only the top-right quadrant of decontextualized language use, missing face-to-face and collaborative contexts
- Understanding involves incremental processing, multimodal grounding, and conversational grounding that current benchmarks don't capture
- The choice of Understanding Indicator has ethical implications regarding how NLP models are evaluated and deployed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper's theoretical framework exposes the underspecification of "understanding" in current NLP evaluation methods.
- Mechanism: By explicitly modeling language understanding as a multi-dimensional cognitive process involving incremental processing, multimodal grounding, and conversational grounding, the framework reveals what current benchmarks miss.
- Core assumption: Understanding in human language use requires explicit modeling of both cognitive processes and social practices.
- Evidence anchors:
  - [abstract] "Here, as everywhere, the evidence underspecifies the understanding."
  - [section 3] The model of understanding as a cognitive process includes situation model, discourse model, world model, and agent model
  - [corpus] Weak - neighbor papers don't directly address the underspecification issue
- Break condition: If understanding can be adequately captured by surface-level behavioral metrics without modeling internal cognitive or social processes.

### Mechanism 2
- Claim: The language use situation space reveals that current GLUE-style benchmarks cover only a narrow portion of language understanding.
- Mechanism: By mapping language use situations along interactivity and shared context dimensions, the paper shows that most evaluation focuses on the top-right quadrant (decontextualized writing), missing face-to-face and collaborative contexts.
- Core assumption: Different language use situations require different understanding capabilities and should be evaluated separately.
- Evidence anchors:
  - [section 2] "all of the extant large scale, 'general' evaluation efforts... target this top-right quadrant"
  - [section 2] Figure 1 showing the quadrant of decontextualized language use
  - [corpus] Weak - neighbor papers don't address this coverage gap
- Break condition: If the top-right quadrant actually captures all essential aspects of language understanding needed for practical applications.

### Mechanism 3
- Claim: The choice of Understanding Indicator determines what aspects of understanding are measured and what ethical implications arise.
- Mechanism: By distinguishing between understanding as symptom (internal state) versus understanding as signal (social commitment), the framework shows how evaluation choices have ethical consequences.
- Core assumption: Language understanding has both cognitive and social dimensions that cannot be separated in practice.
- Evidence anchors:
  - [section 4] "in this view, the understanding indicator is embedded in practices of receiving challenges and providing justifications"
  - [abstract] "C) That the choice of Understanding Indicator marks the limits of benchmarking, and the beginnings of considerations of the ethics of NLP use"
  - [corpus] Weak - neighbor papers don't address the ethical dimension of understanding indicators
- Break condition: If understanding can be adequately measured without considering the social practices of justification and commitment.

## Foundational Learning

- Concept: Construct validity in measurement
  - Why needed here: The paper critiques current benchmarks for lacking construct validity - they measure something but not necessarily what we call "understanding"
  - Quick check question: What is the difference between measuring symptoms of understanding versus measuring understanding itself?

- Concept: Multi-dimensional categorization
  - Why needed here: The paper uses multi-dimensional spaces (interactivity, shared context) to analyze language use situations and understanding processes
  - Quick check question: How does categorizing language use along multiple dimensions help reveal gaps in current evaluation methods?

- Concept: Incremental processing in language comprehension
  - Why needed here: The paper argues that understanding happens incrementally, not in discrete stages, which affects how it should be measured
  - Quick check question: Why is incremental processing important for understanding face-to-face conversation versus reading written text?

## Architecture Onboarding

- Component map: Theoretical framework → Language use situation space → Cognitive model of understanding → Social model of understanding → Evaluation desiderata
- Critical path: Start with identifying language use situations → Map to understanding processes → Connect to evaluation indicators → Derive ethical implications
- Design tradeoffs: Comprehensive theoretical coverage vs. practical implementability; cognitive realism vs. computational tractability
- Failure signatures: Evaluation methods that ignore interactivity context, fail to specify construct assumptions, or treat understanding as purely behavioral
- First 3 experiments:
  1. Map existing GLUE-style benchmarks onto the language use situation space to quantify coverage gaps
  2. Design evaluation tasks for face-to-face interaction contexts using the conversational grounding framework
  3. Create a taxonomy of understanding indicators and their corresponding ethical implications for different NLP applications

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the essential characteristics that distinguish different language use situation types, and how do these characteristics impact language understanding in NLP models?
- Basis in paper: [explicit] The paper outlines a multi-dimensional model of language use situations (interactivity, shared context) and argues that different situation types have different characteristics.
- Why unresolved: The paper does not provide empirical evidence or a detailed taxonomy of these characteristics or their impact on understanding.
- What evidence would resolve it: Empirical studies comparing NLP model performance across various language use situation types, or a comprehensive taxonomy linking situation characteristics to understanding challenges.

### Open Question 2
- Question: How can we develop evaluation methods that capture the full breadth of "General Language Understanding" (GLU), including both situated and abstracted language use?
- Basis in paper: [explicit] The paper criticizes current GLUE benchmarks for failing to capture the breadth of GLU and proposes desiderata for GLU evaluation, including coverage of diverse language use types.
- Why unresolved: The paper does not present specific methods or benchmarks for evaluating GLU in situated language use situations.
- What evidence would resolve it: Development and validation of new benchmarks or evaluation methods that include both situated and abstracted language use, and empirical comparison of model performance on these new benchmarks versus existing ones.

### Open Question 3
- Question: What is the relationship between understanding symptoms (e.g., correct responses) and understanding signals (e.g., justifications, commitments) in NLP models, and how can this relationship be leveraged for more ethical NLP use?
- Basis in paper: [explicit] The paper argues that understanding in real use situations is not just about producing correct responses, but also about making commitments and providing justifications when challenged.
- Why unresolved: The paper does not provide a concrete framework for operationalizing understanding signals or integrating them into NLP evaluation.
- What evidence would resolve it: Development of methods for NLP models to generate justifications and make commitments, and empirical studies on the impact of these methods on user trust and model interpretability.

## Limitations
- The paper is primarily theoretical and lacks empirical validation of its framework
- The framework's practical implementability for actual benchmark design remains unclear
- The paper acknowledges but does not fully address the tension between comprehensive coverage and practical evaluation constraints

## Confidence

**Major Limitations:**
The paper is primarily theoretical and does not provide empirical validation of its framework. The analysis relies heavily on conceptual arguments rather than experimental evidence. The framework's practical implementability for actual benchmark design remains unclear, particularly regarding how to operationalize the social dimensions of understanding indicators. The paper acknowledges but does not fully address the tension between comprehensive coverage of language use situations and practical evaluation constraints.

**Confidence Assessment:**
- **High confidence**: The identification of coverage gaps in current GLUE-style benchmarks (Mechanism 2) - the mapping of existing benchmarks to decontextualized language use is straightforward and well-supported
- **Medium confidence**: The theoretical framework for understanding as incremental, multimodal, and socially embedded (Mechanism 1) - while conceptually sound, lacks empirical validation and may be difficult to implement in practice
- **Medium confidence**: The connection between understanding indicators and ethical implications (Mechanism 3) - the argument is logically coherent but the practical implications for benchmark design are not fully developed

## Next Checks
1. Map all GLUE/SuperGLUE tasks to the proposed language use situation space to quantify coverage gaps and identify specific missing dimensions
2. Design a small-scale experimental protocol to test whether incremental processing assumptions affect model performance on face-to-face dialogue tasks
3. Conduct expert interviews with NLP practitioners to assess the practical challenges of implementing the proposed evaluation desiderata in real-world benchmark development