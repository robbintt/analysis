---
ver: rpa2
title: High-Performance Transformers for Table Structure Recognition Need Early Convolutions
arxiv_id: '2311.05565'
source_url: https://arxiv.org/abs/2311.05565
tags:
- visual
- table
- image
- encoder
- convolutional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the visual encoder design for table structure
  recognition (TSR) and shows that a lightweight convolutional stem can achieve comparable
  performance to standard CNN backbones while significantly reducing model complexity.
  The key insight is that a moderate receptive field and longer sequence length are
  crucial for TSR performance, and the convolutional stem strikes an optimal balance
  between these two factors.
---

# High-Performance Transformers for Table Structure Recognition Need Early Convolutions

## Quick Facts
- arXiv ID: 2311.05565
- Source URL: https://arxiv.org/abs/2311.05565
- Reference count: 35
- Primary result: ConvStem achieves 96.53% TEDS score on PubTabNet with 24.08M parameters and 22.36G MAC, outperforming ResNet-18

## Executive Summary
This paper investigates the visual encoder design for table structure recognition (TSR) and demonstrates that a lightweight convolutional stem can achieve state-of-the-art performance while significantly reducing model complexity compared to standard CNN backbones. The key insight is that early convolutional layers provide essential inductive biases that stabilize transformer optimization while balancing receptive field ratio and sequence length independently. Through extensive ablation studies, the authors show that linear projection fails due to the inverse correlation between receptive field and sequence length, while ResNet backbones are unnecessarily complex. The proposed ConvStem achieves 96.53% TEDS score on PubTabNet with only 24.08M parameters, outperforming ResNet-18 in both accuracy and efficiency.

## Method Summary
The paper proposes a convolutional stem visual encoder for table structure recognition, consisting of 5 convolutional layers (3 layers with stride 2, kernel 3×3 followed by 2 layers with stride 1, kernel 1×1) that process input images (448×448) into 729 feature tokens. This is combined with a transformer decoder (4 layers, d=512, 8 attention heads, feedforward=1024) to generate HTML sequences representing table structure. The model is trained on PubTabNet using AdamW optimizer with step learning rate scheduler (0.0001 initial LR, reduced by 10x after 12 epochs) and evaluated using TEDS score. The key innovation is replacing standard visual encoders with a lightweight convolutional stem that provides optimal balance between receptive field ratio (13.62%) and sequence length (729 tokens).

## Key Results
- ConvStem achieves 96.53% TEDS score on PubTabNet, outperforming ResNet-18 (95.84%) and ResNet-34 (95.73%)
- ConvStem uses only 24.08M parameters and 22.36G MAC operations, significantly less than ResNet-18 (28.70M, 42.22G) and ResNet-34 (39.42M, 63.07G)
- Linear projection fails to match ConvStem performance due to inverse correlation between receptive field and sequence length
- Increasing input image size enhances accuracy but does not fundamentally change the optimal visual encoder design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Convolutional stem achieves comparable performance to ResNet backbones by balancing receptive field ratio and sequence length independently.
- Mechanism: The convolutional stem introduces a few early convolutional layers that provide a moderate receptive field ratio (13.62%) while maintaining a long sequence length (729 tokens). This allows the model to "see" an appropriate portion of the table structure and "store" it within sufficient context for the transformer decoder. Unlike linear projection where increasing receptive field directly reduces sequence length, the convolutional stem decouples these two factors.
- Core assumption: The transformer decoder requires sufficient context length to capture complex table structures, and a moderate receptive field provides the right balance of local and global information.
- Evidence anchors:
  - [abstract]: "The convolutional stem strikes an optimal balance between two crucial factors for high-performance TSR: a higher receptive field (RF) ratio and a longer sequence length."
  - [section 4.4.2]: "In general, a higher RF ratio and longer sequence length to transformers are beneficial for TSR, particularly when dealing with complex tables."
  - [corpus]: Weak evidence - only 25 related papers found, none directly addressing the RF ratio vs sequence length tradeoff in TSR.

### Mechanism 2
- Claim: Linear projection fails because it cannot independently optimize receptive field ratio and sequence length, capping its performance potential.
- Mechanism: Linear projection's patch size directly determines both the receptive field ratio and sequence length (N = HW/P²). As patch size increases to improve receptive field ratio, sequence length decreases quadratically, creating an inverse correlation that limits performance. This is demonstrated by LinearProj-56 achieving peak performance before declining as sequence length becomes too short.
- Core assumption: The inverse relationship between receptive field and sequence length in linear projection creates a fundamental limitation that cannot be overcome without architectural changes.
- Evidence anchors:
  - [section 4.4.2]: "The RF ratio and N are inversely correlated in linear projection. An increase in the RF ratio means a larger patch size P, resulting in a shorter sequence length N = HW/P²."
  - [section 4.2]: "The performance of linear projection is capped since the RF ratio and the sequence length are inversely correlated."
  - [corpus]: No direct evidence found in corpus - this appears to be a novel insight from the paper.

### Mechanism 3
- Claim: Early convolutions provide essential inductive biases that stabilize transformer optimization and improve performance.
- Mechanism: The convolutional stem reintroduces early convolutional layers that encode spatial locality and translation invariance biases before the transformer processes the features. This "early convolution" approach has been shown to enhance optimization stability and improve peak performance in other transformer applications, allowing the transformer to focus on learning higher-level abstractions.
- Core assumption: Transformers benefit from early convolutional layers that provide architectural inductive biases, reducing the optimization burden on the attention mechanism.
- Evidence anchors:
  - [abstract]: "Evidence shows that incorporating a few early convolutions is crucial to balance inductive biases and the representation learning ability of transformers."
  - [section 2]: "The convolutional stem [14], another lightweight visual encoder, reintroduced minimal convolutions to enhance optimization stability and improve peak performance and robustness."
  - [corpus]: Weak evidence - only mentions convolutional stems in vision transformers generally, not specifically for TSR.

## Foundational Learning

- Concept: Receptive Field and its Ratio
  - Why needed here: Understanding how much of the input image influences the output features is crucial for table structure recognition, where the model needs to capture both local cell details and global table structure.
  - Quick check question: If a convolutional layer has kernel size 3×3 and stride 2, what is the receptive field ratio for an input image of size 224×224 after two such layers?

- Concept: Transformer Sequence Length and Context Windows
  - Why needed here: The transformer decoder requires sufficient context to generate coherent HTML sequences representing table structures, and understanding the quadratic complexity of attention operations is essential for model design.
  - Quick check question: If a visual encoder produces a feature map of size 28×28 and the transformer uses this as a sequence, how many attention pairs must be computed in the self-attention layer?

- Concept: HTML Table Structure Representation
  - Why needed here: The model generates HTML tags to represent table structure, so understanding HTML table syntax and the relationship between tags and table components is fundamental to the task.
  - Quick check question: Given an HTML table with 3 rows and 2 columns where the first cell spans both columns, what would be the correct HTML tag sequence for the header row?

## Architecture Onboarding

- Component map:
  Input: Tabular image (448×448 by default) → Convolutional stem (5 conv layers: stride 2, kernel 3×3 ×3 layers, then stride 1, kernel 1×1) → Feature Map (729 tokens) → Transformer (4 decoder layers, d=512, 8 attention heads, feedforward=1024) → Output: HTML sequence (max length 512) representing table structure

- Critical path: Image → Convolutional stem → Feature map → Transformer decoder → HTML sequence
  The visual encoder quality directly impacts transformer performance, making the convolutional stem design critical.

- Design tradeoffs:
  - Convolutional stem vs ResNet: 4.62M fewer parameters, 19.86G fewer MAC operations, but requires careful tuning of conv layers and kernel sizes
  - Sequence length vs receptive field: Longer sequences improve context but increase computational cost quadratically
  - Model complexity vs performance: ResNet-34 has higher RF ratio (100%) but 10.72M more parameters than ConvStem

- Failure signatures:
  - Poor complex table performance → Likely insufficient receptive field or sequence length
  - Scattered attention patterns → May indicate linear projection is being used instead of convolutional stem
  - Training instability → Could indicate missing early convolutions or improper normalization
  - Low TEDS score → Could indicate issues with HTML generation or visual encoder feature quality

- First 3 experiments:
  1. Replace ConvStem with LinearProj-28 and measure TEDS drop (expect ~6 percentage points)
  2. Vary input image size (224→448→672) while keeping ConvStem fixed to test RF ratio impact
  3. Remove early convolution layers from ConvStem incrementally to find minimum effective depth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the convolutional stem compare to other lightweight visual encoders like ViT's linear projection when applied to other document image tasks such as document layout analysis or form recognition?
- Basis in paper: [explicit] The paper compares the convolutional stem to linear projection and CNN backbones specifically for table structure recognition, noting the linear projection suffers in performance due to limited receptive field and sequence length.
- Why unresolved: The paper focuses exclusively on table structure recognition and does not explore other document image tasks where similar visual encoder architectures might be applicable.
- What evidence would resolve it: Conducting experiments using the convolutional stem on other document image datasets (e.g., PubLayNet for layout analysis) and comparing its performance against linear projection and other lightweight visual encoders would provide evidence.

### Open Question 2
- Question: What is the impact of varying the input image size on the performance of the convolutional stem for table structure recognition, beyond the fixed size used in the experiments?
- Basis in paper: [inferred] The paper mentions that increasing the image size has been demonstrated to enhance accuracy, but the RF ratio is defined to exclude the influence of image size. The experiments use a fixed input size of 448 × 448.
- Why unresolved: The paper does not explore how different input image sizes affect the performance of the convolutional stem, which could provide insights into optimal sizing for different table complexities or resolutions.
- What evidence would resolve it: Running experiments with varying input image sizes (e.g., 224 × 224, 512 × 512, 640 × 640) while keeping other parameters constant would show how input size impacts performance.

### Open Question 3
- Question: Can the convolutional stem be effectively integrated into other transformer-based architectures beyond table structure recognition, such as for general image classification or object detection?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of the convolutional stem for table structure recognition, bridging the performance gap between linear projection and CNN backbones while reducing model complexity.
- Why unresolved: The paper does not explore applications of the convolutional stem beyond table structure recognition, leaving open the question of its generalizability to other vision tasks.
- What evidence would resolve it: Implementing the convolutional stem in other transformer-based architectures for tasks like image classification (e.g., replacing the linear projection in ViT) or object detection and comparing performance against standard backbones would provide evidence of its broader applicability.

## Limitations

- Performance gains are demonstrated only on PubTabNet dataset, limiting generalizability to other table datasets or real-world applications
- The optimal hyperparameters for ConvStem (kernel sizes, layer counts) are not systematically explored, suggesting potential for further optimization
- Theoretical claims about early convolutions providing essential inductive biases lack direct validation specific to TSR applications

## Confidence

**High Confidence**: The empirical results showing ConvStem's superior TEDS score and efficiency metrics on PubTabNet are well-supported by the ablation studies and quantitative comparisons. The inverse correlation between receptive field and sequence length in linear projection is mathematically straightforward and demonstrated clearly.

**Medium Confidence**: The theoretical arguments about why early convolutions help transformers (inductive biases, optimization stability) are reasonable extensions of existing vision transformer research, but the specific mechanisms for TSR require more targeted validation. The claim that ConvStem strikes an optimal balance is supported by the ablation results but could benefit from broader dataset testing.

**Low Confidence**: The generalizability of these findings to other table structure recognition datasets, real-world applications with varying table complexities, and different transformer architectures remains untested. The optimal hyperparameters for ConvStem (kernel sizes, layer counts) are not systematically explored.

## Next Checks

1. **Cross-dataset validation**: Evaluate the ConvStem architecture on at least two other table structure recognition datasets (e.g., Marmot, ICDAR table competition datasets) to test generalizability beyond PubTabNet.

2. **Extreme table complexity test**: Design a benchmark with tables of varying complexity (cell counts ranging from 10 to 1000+) to empirically determine the breaking point where ConvStem's receptive field or sequence length becomes insufficient.

3. **Architectural ablation on inductive biases**: Systematically remove different components of the ConvStem (e.g., batch normalization, different kernel sizes, varying depths) to isolate which specific architectural features contribute most to the performance gains, particularly for the claimed optimization stability benefits.