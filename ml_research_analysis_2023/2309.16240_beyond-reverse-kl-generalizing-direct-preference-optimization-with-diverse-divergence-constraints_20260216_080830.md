---
ver: rpa2
title: 'Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse
  Divergence Constraints'
arxiv_id: '2309.16240'
source_url: https://arxiv.org/abs/2309.16240
tags:
- divergence
- reward
- assistant
- arxiv
- reverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a generalization of Direct Preference Optimization\
  \ (DPO) by incorporating diverse divergence constraints beyond the standard reverse\
  \ KL divergence. The authors demonstrate that under certain f-divergences\u2014\
  including Jensen-Shannon, forward KL, and \u03B1-divergences with \u03B1 \u2208\
  \ (0,1)\u2014the complex relationship between reward and optimal policy can be simplified\
  \ by addressing the Karush-Kuhn-Tucker conditions, thereby eliminating the need\
  \ to estimate normalizing constants in the Bradley-Terry model."
---

# Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints

## Quick Facts
- **arXiv ID:** 2309.16240
- **Source URL:** https://arxiv.org/abs/2309.16240
- **Reference count:** 40
- **Key outcome:** Generalizes DPO to diverse f-divergences, eliminating Z(x) estimation and improving alignment-diversity tradeoffs

## Executive Summary
This paper proposes a generalization of Direct Preference Optimization (DPO) by incorporating diverse divergence constraints beyond the standard reverse KL divergence. The authors demonstrate that under certain f-divergences—including Jensen-Shannon, forward KL, and α-divergences with α ∈ (0,1)—the complex relationship between reward and optimal policy can be simplified by addressing the Karush-Kuhn-Tucker conditions, thereby eliminating the need to estimate normalizing constants in the Bradley-Terry model. This enables efficient supervised fine-tuning of language models to align with human preferences under a broad set of divergence constraints. Empirically, the approach balances alignment performance and generation diversity, outperforming PPO-based methods in divergence efficiency.

## Method Summary
The method generalizes DPO by replacing the reverse KL divergence with arbitrary f-divergences in the regularization term. By analyzing the Karush-Kuhn-Tucker conditions, the authors show that for certain f-divergences (forward KL, Jensen-Shannon, α-divergences with α ∈ (0,1)), the Lagrange multipliers must be zero, eliminating the need to estimate the normalizing constant Z(x) in the Bradley-Terry model. This allows the reward function to be reparameterized as r(y|x) = βf′(π(y|x)/πref(y|x)) + const, enabling optimization through standard supervised learning. The approach is evaluated across multiple preference datasets (IMDB-sentiment, Anthropic HH, MT-bench) using pre-trained language models (GPT-2-large, Pythia 2.8B).

## Key Results
- Eliminates the need to estimate normalizing constants in the Bradley-Terry model for forward KL, Jensen-Shannon, and α-divergences with α ∈ (0,1)
- Achieves better balance between alignment performance and generation diversity compared to standard DPO
- Outperforms PPO-based methods in divergence efficiency while maintaining similar alignment quality
- Provides theoretical bounds on how f-divergence choices directly influence expected calibration error (ECE)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method eliminates the need to estimate the normalizing constant Z(x) in the Bradley-Terry model for certain f-divergences.
- Mechanism: By addressing the Karush-Kuhn-Tucker conditions, specifically the complementary slackness condition, the Lagrange multipliers α(y) must be zero for a broad class of f-divergences (including forward KL, Jensen-Shannon, and α-divergences with α ∈ (0,1)). This eliminates the dependence on Z(x) in the reward function.
- Core assumption: πref(y|x) > 0 for any valid x and f′ is invertible with 0∉dom(f′).
- Evidence anchors:
  - [abstract]: "This eliminates the need for estimating the normalizing constant in the Bradley-Terry model"
  - [section 4]: "by carefully analyzing the normalization constant Z(x), we can derive a closed-form solution for many other (but not all) divergences as well, without the need to estimate the normalization constant Z(x)"
- Break condition: If πref(y|x) = 0 for some x,y or if 0∈dom(f′) for the chosen f-divergence.

### Mechanism 2
- Claim: The method generalizes DPO to a broader class of divergence constraints while maintaining a tractable supervised learning approach.
- Mechanism: The reward function r(y|x) can be reparameterized as r(y|x) = βf′(π(y|x)/πref(y|x)) + const, where f′ is the derivative of the chosen f-divergence function. This allows optimization using standard supervised learning methods.
- Core assumption: The chosen f-divergence satisfies the conditions in Theorem 1 (0∉dom(f′), πref(y|x) > 0).
- Evidence anchors:
  - [abstract]: "We show that under certain f-divergences... the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush-Kuhn-Tucker conditions"
  - [section 4]: "Theorem 1. If πref(y|x) > 0 for any valid x and f′ is invertible with 0∉dom(f′), the reward class that is consistent with the Bradley-Terry model can be reparameterized"
- Break condition: If the chosen f-divergence does not satisfy the conditions in Theorem 1, or if the reference model πref has zero probabilities.

### Mechanism 3
- Claim: The choice of f-divergence regularization directly influences the expected calibration error (ECE) of the fine-tuned model.
- Mechanism: The difference in ECE between two models can be bounded by the f-divergence between them, as shown in Theorem 2. This provides a theoretical justification for why different divergence constraints lead to different calibration behaviors.
- Core assumption: The f-divergence is strictly convex.
- Evidence anchors:
  - [abstract]: "divergence constraints directly influence expected calibration error (ECE)"
  - [section 5.4]: "Theorem 2. Suppose πθ1(·|x) and πθ2(·|x) be two policies. Let Df be any f-divergence such that f is strictly convex."
- Break condition: If the f-divergence is not strictly convex, the theoretical bound in Theorem 2 may not hold.

## Foundational Learning

- **Concept: f-divergences**
  - Why needed here: The paper generalizes DPO to use various f-divergences (not just reverse KL) as regularization constraints.
  - Quick check question: What are the key properties of f-divergences that make them suitable for this generalization?

- **Concept: Bradley-Terry model**
  - Why needed here: The Bradley-Terry model is used to map reward functions to optimal policies in the preference optimization framework.
  - Quick check question: How does the Bradley-Terry model relate pairwise comparisons to a reward function?

- **Concept: Karush-Kuhn-Tucker (KKT) conditions**
  - Why needed here: The KKT conditions, particularly complementary slackness, are crucial for eliminating the normalizing constant Z(x) in the reward function.
  - Quick check question: What role does the complementary slackness condition play in the derivation?

## Architecture Onboarding

- **Component map:**
  - Preference dataset D (x, yw, yl) → Language model πθ → Loss function with f-divergence regularization → Trained model πθ

- **Critical path:**
  1. Collect preference dataset D
  2. Initialize model πθ with supervised fine-tuning on D
  3. For each training iteration:
     a. Sample a batch B from D
     b. Compute the loss using equation 3 with the chosen f-divergence
     c. Compute gradients and update the model
  4. Return the final model πθ

- **Design tradeoffs:**
  - Choice of f-divergence: Different f-divergences lead to different tradeoffs between alignment performance and generation diversity
  - Coefficient β: Controls the strength of the regularization. Larger values lead to stronger regularization
  - Reference model: The choice of πref affects the regularization term and can influence the final model's behavior

- **Failure signatures:**
  - Training instability: May occur if the chosen f-divergence leads to large divergence values (e.g., forward KL)
  - Poor alignment: If β is too small or the chosen f-divergence is not suitable for the task
  - Mode collapse: If the chosen f-divergence (e.g., reverse KL) leads to reduced generation diversity

- **First 3 experiments:**
  1. Implement DPO with reverse KL divergence on a simple preference dataset and compare to baseline PPO
  2. Implement DPO with forward KL divergence and observe the impact on generation diversity
  3. Implement DPO with Jensen-Shannon divergence and compare alignment performance to reverse KL

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of f-divergence affect the trade-off between alignment performance and generation diversity in LLMs?
- Basis in paper: Explicit
- Why unresolved: The paper discusses how different divergences impact alignment and diversity, but does not provide a comprehensive analysis of the trade-offs across all divergences.
- What evidence would resolve it: Empirical results comparing alignment performance and generation diversity across various f-divergences on multiple datasets.

### Open Question 2
- Question: What is the impact of using f-divergences other than reverse KL on the calibration of LLMs?
- Basis in paper: Explicit
- Why unresolved: The paper establishes a theoretical relationship between f-divergence and calibration error, but does not provide extensive empirical evidence on the impact of different divergences on calibration.
- What evidence would resolve it: Empirical results measuring the calibration error of LLMs fine-tuned with different f-divergences.

### Open Question 3
- Question: How does the choice of f-divergence affect the stability and efficiency of the training process for LLMs?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that certain divergences can lead to instability in the training process, but does not provide a comprehensive analysis of the stability and efficiency across all divergences.
- What evidence would resolve it: Empirical results comparing the stability and efficiency of the training process for LLMs fine-tuned with different f-divergences.

## Limitations
- The theoretical analysis assumes πref(y|x) > 0 for all valid x,y pairs, which may not hold in practice when the reference model has zero probabilities
- The paper does not fully explore scenarios where the chosen f-divergence has domain issues (0∈dom(f′))
- Limited empirical validation of the theoretical bound between f-divergence and expected calibration error

## Confidence
- **High confidence**: The mechanism for eliminating Z(x) through KKT conditions for forward KL, Jensen-Shannon, and α-divergences with α ∈ (0,1)
- **Medium confidence**: The claim that different f-divergences directly influence ECE bounds
- **Medium confidence**: The assertion that f-DPO outperforms PPO in divergence efficiency

## Next Checks
1. **Edge case analysis**: Systematically test f-DPO when the reference model πref has near-zero probabilities for certain tokens to identify failure modes and quantify the impact on training stability
2. **Ablation study**: Conduct controlled experiments varying β across a wider range and testing multiple f-divergence combinations to map the full tradeoff space between alignment performance and generation diversity
3. **Robustness testing**: Evaluate f-DPO's performance when applied to preference datasets with different characteristics (e.g., varying preference strength distributions) to assess generalizability beyond the tested benchmarks