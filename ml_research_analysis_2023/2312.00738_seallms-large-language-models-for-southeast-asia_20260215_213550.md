---
ver: rpa2
title: SeaLLMs -- Large Language Models for Southeast Asia
arxiv_id: '2312.00738'
source_url: https://arxiv.org/abs/2312.00738
tags:
- languages
- data
- language
- arxiv
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SeaLLMs, a series of large language models
  specialized for Southeast Asian (SEA) languages. Built upon the Llama-2 model, SeaLLMs
  undergo continued pre-training with an extended vocabulary tailored for SEA languages,
  followed by specialized instruction and alignment tuning to capture the intricacies
  of regional languages and respect local cultural norms, customs, and legal considerations.
---

# SeaLLMs -- Large Language Models for Southeast Asia

## Quick Facts
- arXiv ID: 2312.00738
- Source URL: https://arxiv.org/abs/2312.00738
- Reference count: 40
- One-line primary result: SeaLLMs, specialized LLMs for Southeast Asian languages, outperform ChatGPT-3.5 in non-Latin SEA languages while remaining lightweight and cost-effective

## Executive Summary
SeaLLMs are a series of large language models specialized for Southeast Asian languages, built upon the Llama-2 architecture. Through continued pre-training with an extended vocabulary tailored for SEA languages, followed by specialized instruction and alignment tuning, SeaLLMs demonstrate superior performance across linguistic tasks and assistant-style instruction-following capabilities compared to comparable open-source models. The models excel particularly in non-Latin languages such as Thai, Khmer, Lao, and Burmese, outperforming ChatGPT-3.5 by large margins while maintaining efficiency and cultural sensitivity.

## Method Summary
SeaLLMs employ a four-stage training process: (1) continual pre-training from Llama-2 with extended vocabulary for SEA languages, (2) hybrid pre-training and fine-tuning with multilingual pre-training data and English-dominant instruction data, (3) fine-tuning on balanced multilingual SFT dataset, and (4) self-preferencing alignment optimization using SeaLLM itself. The approach introduces a novel vocabulary expansion technique adding 16,512 tokens for SEA languages, improving tokenization efficiency by up to 9 times, and balances multilingual data with English-dominant instruction data to preserve existing capabilities while learning SEA languages.

## Key Results
- SeaLLM-13B outperforms ChatGPT-3.5 in non-Latin SEA languages (Thai, Khmer, Lao, Burmese) by large margins
- Models demonstrate enhanced performance in high-resource languages like English compared to Llama-2-13B
- SeaLLMs remain lightweight and cost-effective to operate despite superior performance in low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SeaLLMs outperform ChatGPT-3.5 in non-Latin SEA languages by large margins.
- Mechanism: Vocabulary expansion technique adds 16,512 new tokens for SEA languages, improving tokenization efficiency and allowing more context to be encoded.
- Core assumption: Improved tokenization leads to better model performance in low-resource languages.
- Evidence anchors:
  - [abstract] "they outperform ChatGPT-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins"
  - [section] "After this extension process, we obtained 16,512 new tokens to represent Southeast Asian languages"
- Break condition: If tokenization improvements do not translate to actual performance gains in downstream tasks

### Mechanism 2
- Claim: SeaLLMs maintain or improve performance in high-resource languages like English while adding SEA language support.
- Mechanism: Hybrid pre-training and fine-tuning approach balances multilingual data with English-dominant instruction data, preventing loss of existing knowledge.
- Core assumption: The hybrid approach effectively preserves English capabilities while learning SEA languages.
- Evidence anchors:
  - [abstract] "Compared to the Llama-2-13B model, SeaLLMs does not only preserve, but also demonstrate enhanced performance in tasks involving existing languages, such as English"
- Break condition: If the hybrid approach fails to balance the language data properly, leading to performance degradation in either high-resource or low-resource languages

### Mechanism 3
- Claim: SeaLLMs are cost-effective to operate in low-resource SEA languages compared to ChatGPT.
- Mechanism: Improved tokenization efficiency (up to 9 times fewer tokens needed) reduces computational costs.
- Core assumption: Token efficiency directly translates to operational cost savings.
- Evidence anchors:
  - [abstract] "while remaining lightweight and cost-effective to operate"
- Break condition: If operational costs are not significantly reduced despite improved tokenization efficiency

## Foundational Learning

- Concept: Vocabulary expansion and tokenization
  - Why needed here: SEA languages often have inefficient tokenization in models trained on Latin scripts, leading to poor performance.
  - Quick check question: How does the proposed vocabulary expansion technique improve tokenization efficiency for SEA languages?

- Concept: Hybrid pre-training and fine-tuning
  - Why needed here: Balancing multilingual data with English-dominant instruction data is crucial to prevent loss of existing knowledge while adding SEA language support.
  - Quick check question: What is the rationale behind using a hybrid pre-training and fine-tuning approach in SeaLLMs?

- Concept: Self-preferencing optimization
  - Why needed here: Aligning the model using its own generated preference data is necessary for low-resource languages where powerful LLMs like GPT-4 are not feasible.
  - Quick check question: How does self-preferencing optimization improve the model's performance in low-resource languages?

## Architecture Onboarding

- Component map: Vocabulary expansion module -> Hybrid pre-training and fine-tuning pipeline -> Self-preferencing optimization component
- Critical path: Vocabulary expansion → Hybrid pre-training and fine-tuning → Self-preferencing optimization
- Design tradeoffs: Balancing language data, preserving English capabilities, and optimizing for low-resource languages
- Failure signatures: Performance degradation in either high-resource or low-resource languages, increased operational costs
- First 3 experiments:
  1. Test tokenization efficiency for SEA languages using the expanded vocabulary
  2. Evaluate model performance in both high-resource and low-resource languages after hybrid pre-training and fine-tuning
  3. Assess the impact of self-preferencing optimization on model performance in low-resource languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SeaLLMs in low-resource languages compare to human-level proficiency in those languages?
- Basis in paper: [inferred] The paper mentions that SeaLLMs outperform ChatGPT-3.5 in non-Latin languages by large margins, but it doesn't provide a direct comparison to human-level proficiency.
- Why unresolved: The paper focuses on comparing SeaLLMs to other language models and doesn't benchmark against human performance in low-resource languages.
- What evidence would resolve it: Conducting human evaluations of language proficiency in low-resource languages and comparing the results to SeaLLMs' performance on the same tasks.

### Open Question 2
- Question: What are the long-term effects of using SeaLLMs on the preservation and development of low-resource languages?
- Basis in paper: [explicit] The paper mentions that SeaLLMs are designed to respect and reflect local cultural norms and customs, but it doesn't discuss the potential long-term impact on language preservation.
- Why unresolved: The paper doesn't provide any longitudinal studies or predictions about the impact of using SeaLLMs on low-resource languages.
- What evidence would resolve it: Longitudinal studies tracking the usage and development of low-resource languages in communities that adopt SeaLLMs over time.

### Open Question 3
- Question: How can SeaLLMs be further optimized to handle code-switching between multiple languages in a single conversation?
- Basis in paper: [inferred] The paper mentions that SeaLLMs can process batches that mix different data types into singular sequences, but it doesn't discuss code-switching specifically.
- Why unresolved: The paper doesn't provide any experiments or results on SeaLLMs' ability to handle code-switching between multiple languages in a single conversation.
- What evidence would resolve it: Experiments evaluating SeaLLMs' performance on tasks that involve code-switching between multiple languages, such as translating mixed-language text or generating responses that incorporate multiple languages.

## Limitations
- Exact composition and size of training datasets for individual SEA languages are not specified
- No direct empirical evidence provided for claimed cost-effectiveness improvements
- Proprietary prompting strategy for self-preferencing alignment is not disclosed

## Confidence

- High confidence: Technical approach of vocabulary expansion and hybrid pre-training methodology
- Medium confidence: Performance claims against ChatGPT-3.5 given evaluation methodology but lack of transparent data
- Low confidence: Operational cost claims due to absence of direct empirical validation

## Next Checks

1. Conduct controlled experiments measuring actual token counts and computational costs for SEA languages compared to baseline models

2. Deploy comprehensive human evaluation studies with native speakers across SEA regions to validate cultural appropriateness and safety alignment

3. Test model performance consistency across extended deployment periods and varying input distributions to identify potential degradation patterns