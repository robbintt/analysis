---
ver: rpa2
title: 'Sequential annotations for naturally-occurring HRI: first insights'
arxiv_id: '2308.15097'
source_url: https://arxiv.org/abs/2308.15097
tags:
- interaction
- robot
- sequential
- human
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a method to improve interactions with a conversational
  robot by creating and annotating a naturally-occurring human-robot interaction dataset.
  The core idea is to use Conversation Analysis to annotate interactions based on
  sequential organization and multimodal resources, rather than just word recognition.
---

# Sequential annotations for naturally-occurring HRI: first insights

## Quick Facts
- arXiv ID: 2308.15097
- Source URL: https://arxiv.org/abs/2308.15097
- Reference count: 40
- The authors present a method to improve interactions with a conversational robot by creating and annotating a naturally-occurring human-robot interaction dataset.

## Executive Summary
The authors propose an innovative annotation system for human-robot interaction (HRI) based on Conversation Analysis principles. Their approach captures the sequential organization and multimodal resources of naturally-occurring interactions, going beyond traditional word recognition. By annotating adjacency pairs and projecting expected next actions, the system aims to improve conversational AI by modeling the pragmatic and sequential nature of human interaction. The method is tested on a corpus of interactions with a Pepper robot in a library setting, demonstrating potential for better conversational HRI through machine learning and natural language understanding.

## Method Summary
The method involves recording naturally-occurring HRI with a Pepper robot in a library setting, then creating a labeling scheme to capture sequential and turn-taking features. The annotation system is based on Conversation Analysis, focusing on adjacency pairs and multimodal resources to characterize short clips from long-format video references. The approach aims to handle the heterogeneity of natural interactions and the temporal complexity of sequential organization, enabling better conversational AI by modeling pragmatic and sequential aspects of human interaction.

## Key Results
- Proposed annotation system captures sequential organization and multimodal resources in HRI
- Method handles heterogeneity of natural interactions and temporal complexity of sequential organization
- Annotated dataset enables improved conversational AI through machine learning and NLU

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential organization captures pragmatic context better than isolated linguistic features
- Mechanism: By annotating actions within adjacency pairs and projecting expected next actions, the system encodes the normative expectations that guide human interpretation of turns
- Core assumption: Humans use sequential patterns (e.g., greeting-greeting, offer-acceptance) to interpret conversational moves
- Evidence anchors:
  - [abstract] "We propose an annotation practice based on some theoretical underpinnings about the use of language and multimodal resources in human-robot interaction"
  - [section] "we focus on the dynamics of normative expectations (vs. predictions) and interpretative feedbacks"
- Break condition: If human interaction deviates significantly from conventional sequential patterns (e.g., non-Western conversational norms)

### Mechanism 2
- Claim: Multimodal cues supplement speech to resolve temporal ambiguity
- Mechanism: Physical behaviors like body torque and gaze direction provide early signals about turn preparation and participation shifts
- Core assumption: Bodily conduct is systematically produced and interpretable in interaction
- Evidence anchors:
  - [section] "we can rely on these cues in order to recognize the fact that the human is actually preparing a response l.3, which is only 0.4secs after the robot's offer"
  - [corpus] Corpus includes synchronized video from multiple angles capturing body movements
- Break condition: If multimodal cues are occluded or culturally ambiguous

### Mechanism 3
- Claim: Few-shot learning can leverage annotated sequential data for intent recognition
- Mechanism: Annotated adjacency pairs and multimodal projections provide rich training signals for NLU models to learn pragmatic mappings
- Core assumption: Statistical models can approximate the human inference process from sequential patterns
- Evidence anchors:
  - [abstract] "This dataset will be used to improve conversational HRI by using machine learning / NLU methods"
  - [section] "Actual natural language understanding models are able to learn predictive word models and to recognize intents, even with little data (thanks to few-shot learning)"
- Break condition: If dataset size remains too small for statistical generalization

## Foundational Learning

- Concept: Conversation Analysis sequential organization
  - Why needed here: Provides theoretical framework for annotating turn-taking and action projections
  - Quick check question: What distinguishes a second pair part from an insertion sequence in CA terms?

- Concept: Multimodal interaction analysis
  - Why needed here: Enables interpretation of non-verbal cues that signal turn preparation
  - Quick check question: How does body torque indicate temporary vs. permanent participation shifts?

- Concept: Few-shot learning in NLU
  - Why needed here: Allows training intent models with limited annotated data
  - Quick check question: What makes NLU few-shot learning different from traditional supervised learning?

## Architecture Onboarding

- Component map:
  - Data acquisition: Pepper robot with state machine + multi-camera capture
  - Annotation pipeline: ELAN transcription + sequential thread labeling
  - Feature extraction: Speech segments + multimodal event timestamps
  - Model training: NLU with few-shot learning on sequential-action pairs
  - Evaluation: Intent recognition accuracy + turn-taking timing metrics

- Critical path: Data capture → Transcription → Sequential annotation → Feature extraction → NLU training → Validation

- Design tradeoffs:
  - Synchronous vs. asynchronous interaction modeling
  - Fine-grained vs. coarse sequential annotations
  - Rule-based vs. learned turn-taking detection

- Failure signatures:
  - Model overfits to specific user patterns
  - Multimodal feature extraction misses subtle cues
  - Sequential annotations lack inter-rater reliability

- First 3 experiments:
  1. Validate sequential annotation consistency across raters on 10-minute sample
  2. Train NLU model on annotated adjacency pairs to predict next action type
  3. Test multimodal cue detection for turn preparation timing accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed sequential annotation method compare to existing turn-taking models in terms of prediction accuracy and real-time processing capabilities for conversational AI?
- Basis in paper: [inferred] The authors propose a novel annotation method but do not provide quantitative comparisons with existing models.
- Why unresolved: The paper focuses on methodology and theoretical underpinnings without empirical evaluation of the annotation system's performance.
- What evidence would resolve it: Quantitative experiments comparing the proposed annotation method's performance against established turn-taking models in terms of prediction accuracy and real-time processing speed.

### Open Question 2
- Question: Can the proposed sequential annotation system be effectively applied to other domains beyond library information services, such as healthcare or customer service?
- Basis in paper: [explicit] The authors mention the potential for adaptation but do not explore other domains.
- Why unresolved: The paper focuses on a specific use case (library robot) without discussing broader applicability.
- What evidence would resolve it: Empirical studies applying the annotation system to other domains and evaluating its effectiveness in those contexts.

### Open Question 3
- Question: How does the proposed method handle complex, multi-party interactions where multiple users engage with the robot simultaneously?
- Basis in paper: [inferred] The paper discusses byplay and multi-party interactions but does not provide a detailed framework for handling complex, concurrent interactions.
- Why unresolved: The authors acknowledge the complexity of multi-party interactions but do not offer a comprehensive solution for managing them.
- What evidence would resolve it: A detailed framework and empirical evaluation of the method's performance in multi-party interaction scenarios.

## Limitations
- The dataset size and composition are not specified, making it difficult to assess statistical significance
- No evaluation metrics or results are provided for the proposed annotation system
- The paper focuses on methodology rather than empirical validation of improved HRI performance
- The annotation scheme's complexity may limit scalability to larger datasets

## Confidence
- High confidence in the theoretical framework: Conversation Analysis principles are well-established in linguistics
- Medium confidence in the annotation methodology: The approach is sound but lacks validation data
- Low confidence in performance claims: No empirical results demonstrate actual improvement in HRI

## Next Checks
1. Conduct inter-rater reliability testing on a subset of the corpus to validate the annotation scheme's consistency
2. Implement the proposed NLU model using the annotated dataset and measure intent recognition accuracy
3. Compare turn-taking prediction accuracy using multimodal cues versus speech-only baselines