---
ver: rpa2
title: Transferable Candidate Proposal with Bounded Uncertainty
arxiv_id: '2312.04604'
source_url: https://arxiv.org/abs/2312.04604
tags:
- learning
- data
- instances
- uncertainty
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of poor transferability of active
  learning subsets across different model configurations. To tackle this issue, the
  authors introduce a new experimental design called Candidate Proposal, which constrains
  the pool of transferable data candidates using a proxy model.
---

# Transferable Candidate Proposal with Bounded Uncertainty

## Quick Facts
- arXiv ID: 2312.04604
- Source URL: https://arxiv.org/abs/2312.04604
- Reference count: 40
- Primary result: TBU consistently improves transferability of active learning subsets across different model configurations by filtering low epistemic and high aleatoric uncertainty instances.

## Executive Summary
This paper addresses the critical challenge of poor transferability of active learning subsets across different model configurations. The authors introduce a novel experimental design called Candidate Proposal, which uses a proxy model to constrain the pool of transferable data candidates rather than directly selecting the final subset. Their proposed Transferable candidate proposal with Bounded Uncertainty (TBU) algorithm filters out redundant data points based on uncertainty estimation, specifically targeting low epistemic uncertainty (LE) and high aleatoric uncertainty (HA) instances. Experiments on CIFAR-10/100 and SVHN demonstrate that TBU consistently outperforms existing active learning algorithms when transferred to different model configurations, achieving higher per-round accuracy compared to baselines like SAME, DIFF, and SEMI.

## Method Summary
TBU uses a proxy model to filter out low epistemic uncertainty (LE) and high aleatoric uncertainty (HA) instances from the unlabeled data pool, creating a candidate set that is more likely to be transferable across model configurations. The proxy model employs last-layer Laplace approximation to estimate epistemic uncertainty and uses semi-supervised learning (FreeMatch) to identify HA instances through class-wise confidence thresholds. The target model then selects the final active learning subset from this refined candidate pool, allowing it to adapt to its own configuration. This approach compensates for the proxy model's limited awareness of target models by expanding the proxy's role to constraint rather than direct selection.

## Key Results
- TBU consistently improves the performance of existing active learning algorithms when transferred to different model configurations on CIFAR-10/100 and SVHN
- TBU achieves higher per-round accuracy compared to baselines like SAME, DIFF, and SEMI
- The algorithm is robust to the choice of percentile threshold q (set to 10) unless set too high

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filtering out LE and HA instances improves transferability by reducing redundant or harmful data candidates.
- Mechanism: TBU uses uncertainty estimation to identify low epistemic uncertainty (LE) instances that offer minimal information gain and high aleatoric uncertainty (HA) instances that are susceptible to adversarial perturbations. By removing these from the candidate pool, target models can focus on more informative data.
- Core assumption: Epistemic uncertainty reflects model confidence about the data distribution, while aleatoric uncertainty captures inherent noise or ambiguity in the data itself.
- Evidence anchors:
  - [abstract] "TBU chooses data candidates from sources other than the LE and HA instances, preventing the inclusion of redundant data that provides marginal enhancements to model performance."
  - [section] "Instances of low epistemic uncertainty (in brief, LE instances) offer minimal information gain (Zhou et al., 2022), whereas instances of high aleatoric uncertainty (in brief, HA instances) are susceptible to adversarial perturbations (Smith and Gal, 2018)."

### Mechanism 2
- Claim: Candidate Proposal compensates for the proxy model's limited awareness of target models by expanding the proxy's role to constrain the candidate pool rather than directly selecting the subset.
- Mechanism: Instead of having the proxy model select the final active learning subset (as in baselines like DIFF), the proxy only filters out likely non-transferable candidates. The target model then selects from this refined pool, allowing it to adapt to its own configuration.
- Core assumption: The informativeness of data points depends on the specific model configuration, so direct subset selection by a proxy model cannot guarantee transferability.
- Evidence anchors:
  - [section] "The limited awareness of the proxy model regarding target models is compensated by refining the role of the proxy model while expanding the role of target models. Specifically, the proxy model only contributes to constraining a pool of potentially transferable data candidates within unlabeled data."
  - [abstract] "Compared to the previous experimental design, the limited awareness of the proxy model regarding target models is compensated by refining the role of the proxy model while expanding the role of target models."

### Mechanism 3
- Claim: Using semi-supervised learning to identify HA instances provides a simpler and more scalable alternative to explicit noise modeling.
- Mechanism: TBU employs class-wise confidence thresholds from FreeMatch to identify HA instances without requiring complex noise distribution modules. Instances consistently showing low confidence across evaluations are marked as HA.
- Core assumption: Semi-supervised learning can effectively leverage unlabeled data to distinguish between informative and noisy instances.
- Evidence anchors:
  - [section] "Unlike previous approaches that explicitly incorporate complex noise distribution using parameter-intensive modules (Fortuin et al., 2021; Collier et al., 2021, 2023), we rely on semi-supervised learning, harnessing the unlabeled data to identify HA instances effectively."
  - [section] "We used this threshold to upper bound the predictive confidence of HA instances in the unlabeled data set since those are uncertain enough to be ignored throughout model training."

## Foundational Learning

- Concept: Epistemic vs. Aleatoric Uncertainty
  - Why needed here: TBU relies on distinguishing between these two types of uncertainty to filter data candidates appropriately.
  - Quick check question: Can you explain the difference between epistemic and aleatoric uncertainty in one sentence each?

- Concept: Laplace Approximation for Bayesian Neural Networks
  - Why needed here: TBU uses last-layer Laplace approximation to estimate epistemic uncertainty without full Bayesian inference.
  - Quick check question: What is the computational advantage of using last-layer Laplace approximation compared to full Bayesian neural networks?

- Concept: Semi-Supervised Learning with Consistency Regularization
  - Why needed here: TBU leverages semi-supervised learning (specifically FreeMatch) to identify high aleatoric uncertainty instances.
  - Quick check question: How does consistency regularization in semi-supervised learning help in identifying noisy or ambiguous data points?

## Architecture Onboarding

- Component map: Unlabeled data -> Proxy model (Wide-ResNet-28-2) -> Candidate filtering (TBU) -> Target model (ResNet-18) -> Subset selection

- Critical path:
  1. Train proxy model on labeled data with unlabeled data
  2. Compute epistemic uncertainty and identify LE instances
  3. Update semi-supervised learning thresholds and identify HA instances
  4. Filter out LE and HA instances to create candidate pool
  5. Train target model on labeled data
  6. Target model selects subset from candidate pool

- Design tradeoffs:
  - Using proxy model for filtering vs. direct subset selection: Improves transferability but adds complexity
  - Last-layer Laplace approximation vs. full Bayesian inference: More scalable but potentially less accurate uncertainty estimates
  - Semi-supervised learning for HA detection vs. explicit noise modeling: Simpler implementation but may be less precise

- Failure signatures:
  - Poor performance improvement over baselines: May indicate overly restrictive filtering or miscalibrated uncertainty estimates
  - Target model performance worse than random sampling: Could suggest candidate pool is too small or contains mostly uninformative data
  - High variance in results across runs: Might indicate instability in uncertainty estimation or semi-supervised learning components

- First 3 experiments:
  1. Implement proxy model with last-layer Laplace approximation and verify epistemic uncertainty estimates on CIFAR-10
  2. Add FreeMatch-based HA instance detection and validate on a small unlabeled subset
  3. Combine both components in TBU and test filtering on CIFAR-10, comparing candidate pool size and composition to original unlabeled data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the percentile threshold q in TBU affect the performance of the algorithm across different datasets and model architectures?
- Basis in paper: [explicit] The paper mentions that TBU is robust to the choice of the percentile q unless it is set too high, and they fix q to 10 throughout the experiments.
- Why unresolved: The paper only tested one value of q (10) across all experiments. Different datasets or model architectures might benefit from different q values.
- What evidence would resolve it: A comprehensive study varying q across multiple datasets and model architectures, showing how different q values affect performance and identifying optimal ranges for different scenarios.

### Open Question 2
- Question: How does TBU perform compared to other uncertainty quantification methods beyond the last-layer Laplace approximation?
- Basis in paper: [explicit] The paper uses last-layer Laplace approximation for epistemic uncertainty estimation, but mentions it as one of many deterministic uncertainty methods.
- Why unresolved: The paper only evaluates TBU with one specific uncertainty estimation method. Other methods like Monte-Carlo Dropout or Deep Ensemble might yield different results.
- What evidence would resolve it: Implementing TBU with different uncertainty quantification methods and comparing their performance across multiple datasets and model architectures.

### Open Question 3
- Question: What is the computational overhead of TBU compared to standard active learning methods, and how does it scale with dataset size?
- Basis in paper: [explicit] The paper mentions that TBU requires additional computation for uncertainty estimation and filtering, but doesn't provide detailed computational analysis.
- Why unresolved: The paper doesn't report timing or computational complexity comparisons with baseline methods, nor does it analyze how TBU's overhead scales with dataset size.
- What evidence would resolve it: Detailed runtime analysis comparing TBU to baseline methods on datasets of varying sizes, including both training time and inference time for the uncertainty estimation components.

## Limitations

- The effectiveness of TBU in diverse real-world scenarios and with non-standard model architectures requires further validation
- The Laplace approximation method used for epistemic uncertainty, while computationally efficient, may not capture the full complexity of uncertainty compared to full Bayesian inference
- The semi-supervised learning approach for HA instance detection may be less precise than explicit noise modeling methods in datasets with significant label noise

## Confidence

- **High confidence**: The experimental design and methodology are sound, with clear comparisons against established baselines (SAME, DIFF, SEMI) on standard benchmarks
- **Medium confidence**: The effectiveness of TBU in diverse real-world scenarios and with non-standard model architectures requires further validation
- **Medium confidence**: The computational efficiency gains from using last-layer Laplace approximation instead of full Bayesian inference need empirical verification on larger-scale datasets

## Next Checks

1. **Architecture generalization test**: Evaluate TBU's performance when transferring subsets between significantly different model architectures (e.g., convolutional vs. transformer-based models) to assess the robustness of uncertainty estimates across architectural shifts.

2. **Label noise sensitivity analysis**: Introduce varying levels of label noise into the training data and measure TBU's ability to maintain performance improvements, particularly focusing on the effectiveness of the HA instance filtering mechanism.

3. **Scalability assessment**: Test TBU on larger-scale image classification datasets (e.g., ImageNet) to evaluate its computational efficiency and performance benefits compared to baselines when scaling to more complex problems.