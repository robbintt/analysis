---
ver: rpa2
title: 'Tuna: Instruction Tuning using Feedback from Large Language Models'
arxiv_id: '2310.13385'
source_url: https://arxiv.org/abs/2310.13385
tags:
- ranking
- data
- responses
- instruction
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Tuna, a model that improves instruction-tuned
  language models by using probabilistic ranking and contextual ranking approaches.
  Probabilistic ranking enables the model to inherit relative rankings of high-quality
  and low-quality responses from a teacher LLM, while contextual ranking refines the
  model's own response distribution using the contextual understanding ability of
  stronger LLMs.
---

# Tuna: Instruction Tuning using Feedback from Large Language Models

## Quick Facts
- arXiv ID: 2310.13385
- Source URL: https://arxiv.org/abs/2310.13385
- Reference count: 24
- Improves instruction-tuned LLMs through probabilistic and contextual ranking techniques

## Executive Summary
Tuna introduces a novel approach to improving instruction-tuned language models by leveraging feedback from stronger LLMs through two complementary ranking methods. The method sequentially applies probabilistic ranking, which inherits relative rankings from teacher LLM probability distributions, and contextual ranking, which refines response distributions using stronger LLMs' contextual understanding. This approach consistently improves performance across multiple benchmarks and even outperforms strong reinforcement learning baselines.

## Method Summary
Tuna builds on instruction-tuned LLMs by first applying probabilistic ranking, where a teacher LLM generates multiple responses that are ranked based on normalized log-likelihood scores. The student model is then fine-tuned using a pairwise ranking loss to match these rankings. Next, contextual ranking samples responses from the finetuned model and uses a stronger LLM (like GPT-4) to rank them based on various criteria. The model undergoes another fine-tuning phase using these rankings to refine its response distribution. The sequential application of both methods produces the final Tuna model.

## Key Results
- Consistently improves performance on Super Natural Instructions (119 test tasks)
- Outperforms strong reinforcement learning baselines
- Shows improvements on LMentry (25 test tasks) and Vicuna QA benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Probabilistic ranking allows the instruction-tuned model to inherit relative rankings of high-quality and low-quality responses from the teacher LLM. The teacher LLM generates multiple responses for each instruction and scores them based on log-likelihood normalized by response length. These scores are used to rank responses, and a pairwise ranking loss is applied to encourage the student model to assign higher probabilities to better-ranked responses. The core assumption is that the teacher LLM's probability assignments reflect the relative quality of responses.

### Mechanism 2
Contextual ranking refines the model's own response distribution using the contextual understanding ability of stronger LLMs. The model samples multiple responses, and a stronger LLM (e.g., GPT-4) ranks these responses based on various criteria. The student model is then fine-tuned to assign higher probabilities to the better-ranked responses. The core assumption is that stronger LLMs can accurately assess and rank the quality of responses.

### Mechanism 3
Sequential application of probabilistic and contextual ranking leverages both the teacher's probability distribution and its contextual understanding to improve response quality. First, probabilistic ranking aligns the model with the teacher's probability distribution. Then, contextual ranking further refines the model's response distribution using the stronger LLM's contextual understanding. The core assumption is that the combination of probability-based and context-based ranking provides complementary information that improves the model's ability to generate high-quality responses.

## Foundational Learning

- **Concept**: Sequence-level distillation
  - Why needed here: The instruction-tuned model learns from the teacher LLM's outputs, which is a form of sequence-level distillation.
  - Quick check question: How does sequence-level distillation differ from token-level distillation in the context of language model training?

- **Concept**: Pairwise ranking loss
  - Why needed here: The pairwise ranking loss is used to encourage the model to assign higher probabilities to better-ranked responses.
  - Quick check question: What is the mathematical form of the pairwise ranking loss, and how does it encourage the model to rank responses correctly?

- **Concept**: Exposure bias
  - Why needed here: The exposure bias issue arises during inference when the model generates responses token by token, potentially leading to the generation of lower-quality responses.
  - Quick check question: How does the contextual ranking approach help mitigate the exposure bias problem?

## Architecture Onboarding

- **Component map**: Instruction-tuned LLM -> Probabilistic ranking (teacher LLM) -> Tunap model -> Contextual ranking (stronger LLM) -> Tuna model
- **Critical path**: The critical path for training Tuna involves: 1) Generating multiple responses for each instruction using the teacher LLM. 2) Ranking these responses based on the teacher LLM's probability distribution. 3) Fine-tuning the instruction-tuned LLM using a pairwise ranking loss. 4) Sampling multiple responses from the fine-tuned model. 5) Ranking these responses using a stronger LLM. 6) Fine-tuning the model again using the stronger LLM's rankings.
- **Design tradeoffs**: The main design tradeoff is between the cost of generating and ranking multiple responses and the potential improvement in response quality. Using more responses and stronger LLMs for ranking can lead to better performance but also increases computational cost.
- **Failure signatures**: If the model fails to improve response quality, possible failure modes include: 1) The teacher LLM's probability distribution does not correlate with response quality. 2) The stronger LLM's rankings are inconsistent or biased. 3) The model cannot effectively integrate the probability-based and context-based ranking information.
- **First 3 experiments**:
  1. Train a Tunap model using only probabilistic ranking and evaluate its performance on a held-out dataset.
  2. Train a Tunac model using only contextual ranking and evaluate its performance on a held-out dataset.
  3. Train a Tuna model by sequentially applying probabilistic and contextual ranking, and compare its performance to Tunap and Tunac on a held-out dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Tuna model perform on other NLP tasks beyond the ones evaluated in the paper, such as machine translation or summarization?
- Basis in paper: The paper evaluates the Tuna model on Super Natural Instructions, LMentry, and Vicuna QA benchmarks, but does not explore its performance on other NLP tasks.
- Why unresolved: The paper focuses on evaluating the Tuna model's performance on specific benchmarks, and does not provide information on its generalizability to other NLP tasks.
- What evidence would resolve it: Conducting experiments to evaluate the Tuna model's performance on additional NLP tasks, such as machine translation or summarization, would provide insights into its generalizability and effectiveness in various domains.

### Open Question 2
- Question: How does the Tuna model compare to other state-of-the-art instruction-tuned models in terms of computational efficiency and resource requirements?
- Basis in paper: The paper does not provide a direct comparison of the Tuna model's computational efficiency and resource requirements with other state-of-the-art instruction-tuned models.
- Why unresolved: The paper focuses on the performance of the Tuna model on specific benchmarks, and does not discuss its computational efficiency or resource requirements in comparison to other models.
- What evidence would resolve it: Conducting a comparative analysis of the Tuna model's computational efficiency and resource requirements with other state-of-the-art instruction-tuned models would provide insights into its practicality and scalability.

### Open Question 3
- Question: How does the Tuna model handle ambiguous or incomplete instructions, and what strategies does it employ to generate accurate responses in such cases?
- Basis in paper: The paper does not discuss how the Tuna model handles ambiguous or incomplete instructions, and what strategies it employs to generate accurate responses in such cases.
- Why unresolved: The paper focuses on the Tuna model's performance on specific benchmarks, and does not explore its ability to handle ambiguous or incomplete instructions.
- What evidence would resolve it: Conducting experiments to evaluate the Tuna model's performance on ambiguous or incomplete instructions, and analyzing the strategies it employs to generate accurate responses, would provide insights into its robustness and adaptability in real-world scenarios.

## Limitations

- The paper relies heavily on the assumption that stronger LLMs can accurately assess response quality, but does not provide direct evidence of correlation with human preferences.
- The computational cost of generating multiple responses and ranking them using stronger LLMs is not thoroughly discussed.
- The paper does not address potential failure modes or limitations of the approach, such as inconsistent or biased rankings from stronger LLMs.

## Confidence

- **High Confidence**: The methodology of probabilistic and contextual ranking is clearly described, and evaluation on multiple benchmarks provides strong evidence for effectiveness.
- **Medium Confidence**: The claim that combining both ranking methods provides complementary benefits is plausible but lacks direct evidence.
- **Low Confidence**: The paper does not adequately address computational costs, scalability, or potential failure modes of the approach.

## Next Checks

1. Conduct a human evaluation study to assess the correlation between rankings provided by text-davinci-003 and GPT-4 and human preferences.
2. Perform a detailed analysis of the computational cost of generating multiple responses and ranking them using stronger LLMs.
3. Conduct an ablation study to isolate the effects of probabilistic ranking and contextual ranking on overall performance.