---
ver: rpa2
title: 'JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language
  Models'
arxiv_id: '2311.05997'
source_url: https://arxiv.org/abs/2311.05997
tags:
- iron
- forest
- plains
- tasks
- craft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: JARVIS-1 is a multi-task agent for Minecraft that combines a multimodal
  language model with a multimodal memory to enable situation-aware planning and life-long
  learning. It addresses the challenges of open-world planning by perceiving multimodal
  inputs (visual observations and language instructions), generating plans with self-check
  and environment feedback, and storing/retrieving past experiences to improve future
  planning.
---

# JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models

## Quick Facts
- arXiv ID: 2311.05997
- Source URL: https://arxiv.org/abs/2311.05997
- Authors: 
- Reference count: 40
- Key outcome: Achieves nearly perfect performance on over 200 tasks in Minecraft Universe Benchmark, with 12.5% success rate on long-horizon diamond pickaxe task (5× improvement over previous SOTA)

## Executive Summary
JARVIS-1 is a multi-task agent for Minecraft that combines a multimodal language model with multimodal memory to enable situation-aware planning and life-long learning. It addresses open-world planning challenges by perceiving multimodal inputs (visual observations and language instructions), generating plans with self-check and environment feedback, and storing/retrieving past experiences to improve future planning. The agent achieves nearly perfect performance on over 200 tasks in the Minecraft Universe Benchmark, with success rate on the long-horizon diamond pickaxe task surpassing previous state-of-the-art by up to 5 times (12.5% vs 2.5%).

## Method Summary
JARVIS-1 implements an interactive planning framework that chains multimodal language models with memory-augmented retrieval. The system uses MineCLIP to align visual observations with textual descriptions, then generates plans through an LLM that incorporates both task instructions and current situation awareness. Planning is enhanced through self-check (verifying plan correctness) and self-explain (recovering from failures using environmental feedback). A multimodal memory stores successful planning experiences as task-state-plan triples, enabling retrieval-augmented planning through in-context learning. The agent also employs life-long learning via self-instruct, continuously improving performance through exploration.

## Key Results
- Achieves nearly perfect performance on over 200 tasks in Minecraft Universe Benchmark
- 12.5% success rate on long-horizon diamond pickaxe task (5× improvement over 2.5% SOTA)
- Demonstrates continuous improvement as game time progresses
- Successfully performs tasks ranging from short-horizon to complex long-horizon objectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal language models enable situation-aware planning by integrating visual and textual observations
- Mechanism: JARVIS-1 chains MineCLIP with an LLM to translate multimodal observations into textual descriptions, then generates plans based on both task instructions and current situation
- Core assumption: Multimodal models can better understand current situations compared to text-only LLMs
- Evidence anchors:
  - [abstract]: "JARVIS-1 is a multi-task agent for Minecraft that combines a multimodal language model with a multimodal memory to enable situation-aware planning"
  - [section 3.2]: "To achieve situation-aware planning, the planner must take the current observation into account, in addition to the task instruction"
  - [corpus]: Weak - no direct citation of prior multimodal-MLM work in Minecraft

### Mechanism 2
- Claim: Memory-augmented planning improves performance by leveraging past experiences
- Mechanism: JARVIS-1 stores successful planning experiences in multimodal memory and retrieves relevant entries to augment current planning through in-context learning
- Core assumption: Past experiences contain relevant information that can improve planning for similar tasks
- Evidence anchors:
  - [abstract]: "By retrieving the relevant memory entries, the planning skill of our MLM-based agent can be strengthened from the agent's own interactions with the environment"
  - [section 3.3]: "We also utilize RAG to enhance JARVIS-1's long-term planning capability"
  - [corpus]: Moderate - cites retrieval-augmented generation (RAG) literature but not specifically for planning

### Mechanism 3
- Claim: Interactive planning with self-check and self-explain enables robust execution of complex tasks
- Mechanism: JARVIS-1 generates initial plans, then uses self-check to verify plan correctness by simulating execution, and self-explain to recover from failures using environmental feedback
- Core assumption: LLM can accurately simulate plan execution and identify flaws before they occur
- Evidence anchors:
  - [abstract]: "We outfit JARVIS-1 with a multimodal memory, which facilitates planning using both pre-trained knowledge and its actual game survival experiences"
  - [section 3.2]: "Our first layer of shield to ensure the correctness of plans involves self-check. Similar to self-debugging"
  - [corpus]: Moderate - cites self-debugging work but not specifically for planning

## Foundational Learning

- Concept: Multimodal embeddings and alignment
  - Why needed here: JARVIS-1 uses MineCLIP to align visual observations with textual descriptions for situation awareness
  - Quick check question: How does MineCLIP's contrastive learning framework enable effective retrieval of relevant memory entries?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: JARVIS-1 uses RAG to enhance planning by retrieving relevant experiences from memory
  - Quick check question: What are the key differences between standard RAG and JARVIS-1's multimodal memory retrieval?

- Concept: In-context learning with large language models
  - Why needed here: JARVIS-1 leverages retrieved experiences as demonstrations in the prompt to improve planning without gradient updates
  - Quick check question: How does the effectiveness of in-context learning scale with the number and relevance of retrieved examples?

## Architecture Onboarding

- Component map: Multimodal perception layer (MineCLIP + text generation) → Interactive planner (MLM with self-check/self-explain) → Multimodal memory store (task, state, plan triples) → Controller (action executor) → Query generator (backward reasoning for memory retrieval)

- Critical path: Perception → Query Generation → Memory Retrieval → Planning → Self-Check → Execution → Memory Storage

- Design tradeoffs:
  - Multimodal vs text-only planning: Better situation awareness but increased complexity
  - Memory size vs retrieval speed: Larger memory provides more experiences but slower retrieval
  - Self-check rounds vs token usage: More self-check improves quality but consumes more LLM tokens

- Failure signatures:
  - Plan failures despite self-check: Indicates limitations in simulation accuracy
  - Poor memory retrieval: Suggests query generation or embedding issues
  - Slow planning: Could indicate inefficient memory organization or retrieval

- First 3 experiments:
  1. Test multimodal perception: Feed different visual observations and verify textual descriptions match expected content
  2. Validate memory retrieval: Query with known tasks and verify retrieved plans are relevant and useful
  3. Assess self-check effectiveness: Generate plans for simple tasks and verify self-check identifies known flaws

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does JARVIS-1's multimodal memory architecture handle the trade-off between memory size and retrieval accuracy for long-horizon tasks?
- Basis in paper: [inferred] The paper discusses JARVIS-1's multimodal memory and its role in life-long learning, but doesn't provide detailed analysis of the trade-off between memory size and retrieval accuracy.
- Why unresolved: While the paper mentions that JARVIS-1's success rate improves with larger memory size (Figure 6), it doesn't explore the optimal balance between memory size and retrieval accuracy, especially for complex long-horizon tasks.
- What evidence would resolve it: Systematic experiments varying memory size and measuring retrieval accuracy and task success rates for long-horizon tasks would provide insights into this trade-off.

### Open Question 2
- Question: Can JARVIS-1's interactive planning framework be extended to handle more complex environmental feedback, such as partial plan failures or unexpected events?
- Basis in paper: [inferred] The paper describes JARVIS-1's self-checking and self-explaining mechanisms for handling plan failures, but doesn't explore their effectiveness for more nuanced environmental feedback.
- Why unresolved: The current framework focuses on binary success/failure feedback, but real-world environments often present partial failures or unexpected events that require more sophisticated reasoning.
- What evidence would resolve it: Evaluating JARVIS-1's performance on tasks with noisy or partial feedback, and comparing it to baselines that incorporate more advanced reasoning about partial failures, would address this question.

### Open Question 3
- Question: How does JARVIS-1's performance scale with the number of tasks in the open-world environment, and what are the limitations of its life-long learning approach?
- Basis in paper: [inferred] The paper mentions JARVIS-1's ability to learn from experiences and improve over time, but doesn't explore the scalability of this approach with a large number of tasks.
- Why unresolved: While JARVIS-1 shows improvement on a set of 200+ tasks, the paper doesn't investigate how its performance would scale to an even larger and more diverse set of tasks, or what limitations might arise in its life-long learning approach.
- What evidence would resolve it: Scaling experiments with increasing numbers of tasks, and analyzing JARVIS-1's learning curves and performance plateaus, would provide insights into its scalability and limitations.

## Limitations
- Implementation details for multimodal query generation and memory retrieval thresholds are underspecified
- Statistical significance measures (variance across runs) are not provided for performance claims
- Memory system scalability and computational overhead for extended operation are not addressed

## Confidence

**High Confidence Claims** (supported by strong experimental evidence):
- JARVIS-1 outperforms baselines on Minecraft Universe Benchmark tasks
- The multimodal approach with memory augmentation improves planning capabilities
- Interactive planning with self-check and self-explain mechanisms contribute to success

**Medium Confidence Claims** (supported by results but with methodological concerns):
- JARVIS-1 achieves "nearly perfect performance" on benchmark tasks (lack of statistical variance measures)
- Continuous improvement through life-long learning is demonstrated (limited temporal evaluation)
- Memory retrieval significantly enhances long-horizon task performance (scalability concerns unaddressed)

**Low Confidence Claims** (weak or absent empirical support):
- The specific contribution of multimodal perception versus text-only approaches (no ablation study provided)
- Claims about the quality of self-explain recovery strategies (no detailed evaluation of failure cases)
- The generalizability of JARVIS-1's architecture beyond Minecraft (no experiments on alternative environments)

## Next Checks

1. **Statistical Significance Validation**: Run JARVIS-1 on the Minecraft Universe Benchmark across 10 independent trials with different random seeds and report mean success rates with 95% confidence intervals for all 200+ tasks, particularly focusing on the long-horizon diamond pickaxe task.

2. **Ablation Study on Multimodal Components**: Create controlled experiments comparing JARVIS-1 against variants that use: (a) text-only perception, (b) no memory retrieval, (c) no self-check/self-explain mechanisms. Measure the marginal contribution of each component to overall performance.

3. **Memory Scaling Analysis**: Evaluate JARVIS-1's performance as memory size increases from 100 to 10,000 entries, measuring both success rate improvements and computational overhead (planning time, token usage). Include analysis of retrieval accuracy and relevance decay over time.