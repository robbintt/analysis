---
ver: rpa2
title: Deep Reinforcement Learning from Hierarchical Preference Design
arxiv_id: '2309.02632'
source_url: https://arxiv.org/abs/2309.02632
tags:
- reward
- heron
- learning
- factors
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HERON, a hierarchical reward modeling framework
  for reinforcement learning. HERON leverages rankings of reward factors to train
  a preference-based reward model using a hierarchical decision tree.
---

# Deep Reinforcement Learning from Hierarchical Preference Design

## Quick Facts
- arXiv ID: 2309.02632
- Source URL: https://arxiv.org/abs/2309.02632
- Authors: 
- Reference count: 34
- Primary result: HERON achieves comparable or superior performance to reward engineering baselines while providing improved sample efficiency and robustness.

## Executive Summary
This paper proposes HERON, a hierarchical reward modeling framework for reinforcement learning. HERON leverages rankings of reward factors to train a preference-based reward model using a hierarchical decision tree. The method is evaluated on classic control, robotic control, multi-agent traffic light control, and code generation tasks. HERON achieves comparable or superior performance to reward engineering baselines, while providing benefits like improved sample efficiency and robustness. On code generation, HERON achieves state-of-the-art performance on the APPS dataset.

## Method Summary
HERON is a hierarchical reward modeling framework that learns complex reward functions from preference data. It uses a hierarchical decision tree induced by the importance ranking of reward factors to compare RL trajectories. The tree first compares trajectories based on the most important factors, and if inconclusive, proceeds to assess less important factors. This allows HERON to flexibly incorporate domain knowledge and create complex rewards without requiring manual weight tuning. The framework is trained using a Bradley-Terry preference model and can be combined with standard RL algorithms for policy learning.

## Key Results
- HERON achieves comparable or superior performance to reward engineering baselines on classic control, robotic control, and multi-agent traffic light control tasks.
- HERON provides improved sample efficiency compared to reward engineering approaches.
- On code generation, HERON achieves state-of-the-art performance on the APPS dataset with a Pass@K metric of 16.1%.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: HERON's hierarchical comparison rule can effectively learn complex reward functions without requiring manual weight tuning of reward factors.
- **Mechanism**: By leveraging rankings of reward factors, HERON constructs a decision tree that first compares trajectories based on the most important factors. If the comparison is inconclusive, it proceeds to less important factors, allowing it to capture complex reward structures without explicit numerical weights.
- **Core assumption**: The hierarchical importance ranking provided by domain experts adequately reflects the true relative importance of reward factors for the task at hand.
- **Evidence anchors**:
  - [abstract] "The hierarchical approach allows HERON to flexibly incorporate domain knowledge and create complex rewards without requiring manual weight tuning."
  - [section 3.2] "HERON compares RL trajectories using a hierarchical decision tree induced by the importance ranking. The decision tree first compares trajectories based on the most important factors and, if inconclusive, proceeds to assess less important factors."
  - [corpus] Weak - no direct comparison to manual weight tuning methods in cited papers.

### Mechanism 2
- **Claim**: HERON is robust to changes in the scale of reward factors, unlike reward engineering approaches.
- **Mechanism**: Since HERON uses preference comparisons that only depend on whether one trajectory is significantly better than another on a given factor, it is invariant to the absolute scale of the reward factors. This allows it to maintain performance when the environment changes in ways that affect factor scales.
- **Core assumption**: The margin parameter δl can be set appropriately to account for the variability in reward factors, making the preference elicitation scale-invariant.
- **Evidence anchors**:
  - [section 4.4] "A subtle advantage of HERON is that unlike reward engineering, it does not depend on the magnitude of the different reward factors... This scale-invariance is beneficial, since algorithms that do depend on the scale of the reward factors may be vulnerable to changes in the environment during training."
  - [section 3.2] "We remark that this loss essentially employs the Bradley-Terry preference model [Bradley and Terry, 1952]."
  - [corpus] Weak - no direct evidence of scale-invariance in related work.

### Mechanism 3
- **Claim**: HERON can provide improved sample efficiency compared to reward engineering approaches.
- **Mechanism**: By using a hierarchical decision tree, HERON can quickly eliminate less promising trajectories early in the comparison process, focusing on the most important factors first. This allows it to learn more efficiently from preference data compared to methods that consider all factors equally.
- **Core assumption**: The hierarchical structure of the reward factors is such that early decisions in the tree can effectively prune the search space.
- **Evidence anchors**:
  - [abstract] "The method is evaluated on classic control, robotic control, multi-agent traffic light control, and large language model fine-tuning for code generation. HERON achieves comparable or superior performance to reward engineering baselines, while providing benefits like improved sample efficiency and robustness."
  - [section 4] "We empirically validate the HERON framework through extensive experiments on classic control tasks, robotic control, multi-agent traffic light control, and large language model fine-tuning for code generation."
  - [corpus] Weak - no direct comparison of sample efficiency in related work.

## Foundational Learning

- **Concept: Reinforcement Learning**
  - Why needed here: HERON is a reinforcement learning framework that learns a reward model from preference data to train policies.
  - Quick check question: What is the goal of a reinforcement learning agent, and how does it typically receive feedback from the environment?

- **Concept: Reward Engineering**
  - Why needed here: HERON is presented as an alternative to reward engineering, which manually combines reward factors with weights.
  - Quick check question: What are the main challenges of reward engineering, and how does HERON aim to address them?

- **Concept: Preference-based Learning**
  - Why needed here: HERON learns a reward model from preference data, which is a form of preference-based learning.
  - Quick check question: How does preference-based learning differ from traditional supervised learning, and what are its advantages for reward modeling?

## Architecture Onboarding

- **Component map**:
  - Behavior Policy -> Weak Preference Elicitation -> Reward Model -> Policy Learning

- **Critical path**:
  1. Collect trajectory data with behavior policy.
  2. Elicit preferences using hierarchical decision tree.
  3. Train reward model on preference data.
  4. Use reward model to train policies with RL.

- **Design tradeoffs**:
  - Hierarchical vs. flat comparison: Hierarchical allows for more efficient pruning but requires a good ranking of factors.
  - Margin parameter δl: Controls sensitivity of preference elicitation but requires domain knowledge to set.
  - Reward model architecture: More complex models can capture non-linear relationships but may overfit or be harder to train.

- **Failure signatures**:
  - Poor performance: May indicate issues with the reward hierarchy, margin parameter, or reward model architecture.
  - High variance in training: Could suggest the preference data is not informative enough or the reward model is overfitting.
  - Sensitivity to environment changes: May indicate the learned reward model is not robust to scale changes in factors.

- **First 3 experiments**:
  1. Test HERON on a simple control task (e.g., Pendulum) with a known reward hierarchy to validate the basic mechanism.
  2. Compare HERON's performance and sample efficiency to reward engineering on a more complex task (e.g., Half-Cheetah) with the same reward factors.
  3. Evaluate HERON's robustness to changes in the scale of reward factors by modifying the environment mid-training and observing the impact on performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HERON's performance compare to reward engineering in tasks with non-linear reward functions?
- Basis in paper: [explicit] The paper mentions that reward engineering assumes the ground-truth reward can be formed as a simple combination of rewards factors, an assumption that may suffer from large approximation bias.
- Why unresolved: The experiments primarily use linear combinations of reward factors, so the comparison with non-linear reward functions is not explored.
- What evidence would resolve it: Empirical results showing HERON's performance against reward engineering in tasks with non-linear reward functions.

### Open Question 2
- Question: Can HERON's hierarchical decision tree approach be extended to handle continuous reward factors?
- Basis in paper: [inferred] The paper describes a hierarchical decision tree induced by the importance ranking of reward factors, which is used to compare trajectories.
- Why unresolved: The paper does not discuss how the hierarchical decision tree approach can be adapted for continuous reward factors.
- What evidence would resolve it: A modification of HERON's algorithm to handle continuous reward factors and experimental results demonstrating its effectiveness.

### Open Question 3
- Question: How does the choice of the margin parameter δl in the weak preference elicitation algorithm affect HERON's performance?
- Basis in paper: [explicit] The paper mentions that the margin parameter δl is important since it ensures that we only elicit a preference using zl if the two trajectories are significantly different according to zl.
- Why unresolved: The paper does not provide a detailed analysis of how different values of δl impact HERON's performance.
- What evidence would resolve it: An ablation study showing HERON's performance with different values of δl and the resulting impact on the learned reward model and policy.

## Limitations
- The hierarchical structure's effectiveness heavily depends on the quality of the domain expert's ranking of reward factors, which is not always straightforward to obtain.
- The paper lacks direct comparisons with manual weight tuning methods, making it difficult to quantify the benefits of HERON's approach.
- The assertion of state-of-the-art performance on code generation tasks is based on a single metric (Pass@K) and may not fully capture the method's strengths and weaknesses.

## Confidence
- **High**: The basic mechanism of using hierarchical preference comparisons to learn reward models is sound and supported by the experimental results.
- **Medium**: The claims of improved sample efficiency and robustness are plausible but would benefit from more rigorous comparisons and ablation studies.
- **Low**: The assertion of state-of-the-art performance on code generation tasks is based on a single metric (Pass@K) and may not fully capture the method's strengths and weaknesses.

## Next Checks
1. Conduct a thorough ablation study on the impact of the margin parameter δl and the hierarchical structure on performance and sample efficiency.
2. Compare HERON's performance and sample efficiency directly with reward engineering approaches on a complex control task, using the same reward factors.
3. Evaluate HERON's robustness to changes in the scale of reward factors by systematically varying the environment and measuring the impact on performance across multiple tasks.