---
ver: rpa2
title: 'Folding Attention: Memory and Power Optimization for On-Device Transformer-based
  Streaming Speech Recognition'
arxiv_id: '2309.07988'
source_url: https://arxiv.org/abs/2309.07988
tags:
- attention
- folding
- power
- layers
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses memory and power bottlenecks in Transformer-based
  streaming speech recognition models, specifically in the linear projection layers
  of self-attention and feedforward networks, which constitute a substantial portion
  of model size and contribute significantly to computation, memory, and power usage.
  The authors propose folding attention, a technique that splits each input token
  into sub-tokens, processes them through standard self-attention, and concatenates
  them back, effectively reducing the number of parameters in linear layers by a factor
  of N (the folding factor).
---

# Folding Attention: Memory and Power Optimization for On-Device Transformer-based Streaming Speech Recognition

## Quick Facts
- arXiv ID: 2309.07988
- Source URL: https://arxiv.org/abs/2309.07988
- Reference count: 0
- Reduces model size by up to 24% and power consumption by up to 23% in Transformer-based streaming ASR models

## Executive Summary
This paper addresses memory and power bottlenecks in Transformer-based streaming speech recognition models, specifically in the linear projection layers of self-attention and feedforward networks. The authors propose folding attention, a technique that splits each input token into sub-tokens, processes them through standard self-attention, and concatenates them back, effectively reducing the number of parameters in linear layers by a factor of N (the folding factor). Experiments on Emformer Transducer models show significant reductions in model size and power consumption without compromising model accuracy or computation overhead, making it particularly effective for on-device streaming ASR applications with strict memory and power budgets.

## Method Summary
Folding attention splits each input token into N sub-tokens, reducing the linear layer dimension from D to D/N while increasing the token count to N×T. After processing through standard self-attention, the sub-tokens are concatenated back to their original dimension. This technique reduces the number of parameters in linear layers by a factor of N² while maintaining similar computation overhead. The method is implemented by modifying the weight matrices in linear projection layers to accommodate the reduced dimension and adding folding/unfolding operators at the input and output of attention layers.

## Key Results
- Model size reduction of up to 24% (equivalent to memory consumption reduction)
- Power consumption reduction of up to 23%
- No compromise in model accuracy or computation overhead
- Effective for on-device streaming ASR applications with strict memory and power constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Folding attention reduces memory and power consumption in linear projection layers without sacrificing model accuracy.
- Mechanism: The input token dimension D is split into N sub-tokens of dimension D/N. This increases the token count to N×T but reduces the linear layer dimension to D/N, resulting in N times less computation and N² times fewer parameters. The minimal increase in attention score computation is negligible due to the small context window size T ≪ D.
- Core assumption: The increase in attention score computation overhead is negligible compared to the reduction in linear layer overhead.
- Evidence anchors:
  - [abstract] "Experiments on on-device Transformer-based streaming speech recognition models show that folding attention reduces model size (and corresponding memory consumption) by up to 24% and power consumption by up to 23%, all without compromising model accuracy or computation overhead."
  - [section] "This leads to the computation and memory overhead of linear layers (O(T × D²) and O(D²) respectively) significantly surpassing that of attention score calculation (O(D × T²) and O(H × T²))."
  - [corpus] Weak correlation; corpus neighbors focus on general on-device ASR optimization rather than the specific linear layer bottleneck addressed here.

### Mechanism 2
- Claim: Folding attention maintains model expressiveness while reducing parameters.
- Mechanism: By splitting each token into N sub-tokens and concatenating them back after processing, the total number of elements (N×T tokens × D/N dimensions = T×D) remains the same. The inner product of sub-tokens' query and key embeddings introduces interdependencies, preserving expressiveness.
- Core assumption: The expressiveness lost due to reduced linear layer parameters can be compensated by adding more folding attention layers.
- Evidence anchors:
  - [section] "Compared to standard self-attention, folding attention maintains an equivalent total number of elements for token embeddings... the inner product of these sub-tokens' query and key embeddings effectively introduces interdependencies, keeping similar expressiveness as standard self-attention."
  - [section] "Using N folding attention layers to replace one standard self-attention layer reduces the number of parameters... while maintaining similar computation overhead."
  - [corpus] No direct evidence; corpus neighbors don't address the expressiveness trade-off in detail.

### Mechanism 3
- Claim: Power consumption reduction is primarily due to decreased memory operations.
- Mechanism: Power consumption in streaming ASR is dominated by memory read/write operations, which scale with model size. Folding attention reduces model size by up to 24%, directly reducing the frequency of memory operations and thus power consumption.
- Core assumption: Memory operations are the dominant factor in power consumption for on-device ASR.
- Evidence anchors:
  - [section] "Given that the power of streaming ASR is primarily influenced by memory read/write operations [22], which scale with model size, folding attention also significantly reduces attention layers' power."
  - [section] "While current hardware excels in computation energy efficiency, it demonstrates comparatively lower energy efficiency in memory operations [20, 21]."
  - [corpus] Moderate correlation; corpus includes papers on power barriers and low-power accelerators, supporting the importance of memory optimization.

## Foundational Learning

- Concept: Transformer architecture and attention mechanism
  - Why needed here: Understanding how standard self-attention works and where the bottlenecks are is crucial for grasping why folding attention is effective.
  - Quick check question: What is the computational complexity of attention score calculation and linear projection layers in standard self-attention?

- Concept: Streaming speech recognition constraints
  - Why needed here: Recognizing the difference between streaming and non-streaming ASR, particularly the small context window size, is key to understanding why folding attention is beneficial.
  - Quick check question: Why is attention score calculation less of a bottleneck in streaming ASR compared to non-streaming ASR?

- Concept: Memory and power optimization in on-device applications
  - Why needed here: Understanding the trade-offs between model size, memory consumption, and power usage is essential for appreciating the significance of folding attention's benefits.
  - Quick check question: Why do linear projection layers contribute more to memory and power consumption than attention score calculation in streaming ASR?

## Architecture Onboarding

- Component map:
  - Input tokens → Folding operator (splits into N sub-tokens) → Multi-head attention (with reduced dimension) → Unfolding operator (concatenates sub-tokens) → Output tokens
  - Alternative path: Input tokens → Folding operator → Feedforward network → Unfolding operator → Output tokens (for size reduction)

- Critical path: Folding operator → Multi-head attention → Unfolding operator (the core sequence that enables the parameter reduction)

- Design tradeoffs:
  - Benefit: Significant reduction in model size (up to 24%), memory consumption, and power usage (up to 23%) without sacrificing accuracy or computation overhead.
  - Cost: Requires modifying the model architecture to include folding and unfolding operators.
  - Risk: If the expressiveness reduction cannot be compensated by additional layers, model accuracy may degrade.

- Failure signatures:
  - Increased word error rate (WER) on test sets, indicating loss of expressiveness.
  - No significant reduction in model size or power consumption, suggesting the folding factor N is too small or the model architecture is not well-suited.
  - Increased computation overhead, indicating the context window size T is no longer negligible compared to the embedding dimension D.

- First 3 experiments:
  1. Implement folding attention with N=2 on a small Emformer Transducer model and measure the reduction in model size and power consumption on a Pixel-6 Pro device.
  2. Compare the WER of the folding attention model with the baseline model on LibriSpeech test-clean and test-other sets to ensure accuracy is maintained.
  3. Experiment with different folding factors (N=2, 4, 8) to find the optimal balance between size reduction and expressiveness preservation.

## Open Questions the Paper Calls Out

- Question: How does the performance of folding attention scale with different folding factors (N > 2) in terms of model accuracy, memory reduction, and power efficiency trade-offs?
- Basis in paper: [explicit] The paper mentions that folding attention reduces linear layers' size and power consumption by a factor of N², and demonstrates results with N=2. However, it doesn't explore higher folding factors or analyze the scaling behavior.
- Why unresolved: The paper only presents results for a folding factor of 2, leaving the optimal folding factor and its impact on the trade-off between accuracy and efficiency unexplored.

- Question: How does folding attention perform when applied to non-streaming, long-context ASR tasks where attention score calculation is the bottleneck, rather than linear layers?
- Basis in paper: [inferred] The paper explicitly states that folding attention is designed for streaming ASR where linear layers are the bottleneck, and acknowledges that existing optimization techniques target attention score calculations for long-context tasks.
- Why unresolved: The paper focuses exclusively on streaming ASR scenarios and doesn't investigate whether folding attention could provide benefits in non-streaming, long-context applications where attention score calculation is the primary bottleneck.

- Question: What is the impact of folding attention on the convergence speed and final accuracy when training from scratch versus fine-tuning pre-trained models?
- Basis in paper: [inferred] The paper presents final model performance metrics but doesn't discuss training dynamics, convergence behavior, or whether models were trained from scratch or fine-tuned.
- Why unresolved: The paper doesn't provide information about training dynamics, making it unclear whether folding attention affects the ease of training, convergence speed, or whether pre-trained models need to be adapted differently.

## Limitations

- Trade-off validation: The preservation of model accuracy through the folding mechanism relies heavily on empirical validation and may not hold for all acoustic patterns.
- Context window dependency: The efficiency gain critically depends on the streaming context window being much smaller than the embedding dimension (T ≪ D).
- Generalizability across tasks: The experiments focus on Emformer Transducer models for English speech recognition; effectiveness for other architectures or languages remains unverified.

## Confidence

- High confidence: The core mechanism of folding attention for reducing linear layer parameters is well-defined and theoretically sound.
- Medium confidence: The claims about power consumption reduction are reasonable but depend on specific hardware characteristics not fully detailed.
- Medium confidence: The preservation of model accuracy through the folding mechanism is demonstrated empirically but requires more extensive ablation studies.

## Next Checks

1. Ablation study on folding factor impact: Systematically vary the folding factor N (2, 4, 8, 16) across multiple test sets to quantify the relationship between parameter reduction and accuracy degradation.

2. Cross-architecture generalization: Implement folding attention on alternative streaming ASR architectures (Conformer, standard Transformer Transducer) and test on diverse language datasets.

3. Long-context streaming evaluation: Test the technique with varying context window sizes (e.g., T = D/10, T = D/2, T = D) to empirically validate the assumption that T ≪ D.