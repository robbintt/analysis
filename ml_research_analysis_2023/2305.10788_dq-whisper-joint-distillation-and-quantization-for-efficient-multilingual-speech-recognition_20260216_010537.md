---
ver: rpa2
title: 'DQ-Whisper: Joint Distillation and Quantization for Efficient Multilingual
  Speech Recognition'
arxiv_id: '2305.10788'
source_url: https://arxiv.org/abs/2305.10788
tags:
- layer
- quantization
- distillation
- arxiv
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of compressing large pre-trained
  speech models like Whisper for efficient deployment on resource-constrained devices.
  The authors propose a joint distillation and quantization framework called DQ-Whisper
  to compress Whisper models.
---

# DQ-Whisper: Joint Distillation and Quantization for Efficient Multilingual Speech Recognition

## Quick Facts
- arXiv ID: 2305.10788
- Source URL: https://arxiv.org/abs/2305.10788
- Authors: 
- Reference count: 0
- Key outcome: Proposed DQ-Whisper framework achieves up to 5.18x model size reduction on Whisper with only marginal performance degradation on Japanese CSJ dataset

## Executive Summary
The paper addresses the challenge of compressing large pre-trained speech models like Whisper for efficient deployment on resource-constrained devices. The authors propose DQ-Whisper, a joint distillation and quantization framework that leverages mutual guidance between these two compression techniques. By selecting distillation layers based on quantization loss and vice versa, the framework achieves significant model size reduction while maintaining performance. Experiments on the Japanese CSJ dataset demonstrate up to 5.18x reduction in model size with only minimal accuracy loss compared to the original Whisper models.

## Method Summary
The DQ-Whisper framework combines knowledge distillation and model quantization in a unified training pipeline. The method involves three key components: (1) knowledge distillation with three layer matching strategies for intermediate hidden layers (static, dynamic, and restrained dynamic), (2) model quantization using both uniform and Additive Powers-of-Two (APoT) methods, and (3) mutual guidance between distillation and quantization where layers are selected based on complementary losses. The framework uses Whisper Small as the teacher model and creates smaller student models (Base and Tiny) by reducing encoder and decoder layers. Training optimizes a combined loss function that balances distillation loss, quantization loss, and cross-entropy loss for multilingual speech recognition tasks.

## Key Results
- Achieves up to 5.18x reduction in model size compared to original Whisper models
- Maintains only marginal performance degradation on Japanese CSJ dataset
- Demonstrates compatibility between quantization and distillation for higher compression rates
- Shows APoT quantization provides better accuracy preservation than uniform quantization at low bit-widths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mutual guidance between distillation and quantization improves compression efficiency by selecting layers based on complementary losses
- Mechanism: The framework selects distillation layers based on quantization loss and quantization layers based on distillation loss, creating a feedback loop that optimizes both objectives simultaneously
- Core assumption: Layers that minimize quantization loss during distillation are also good candidates for actual quantization, and vice versa
- Evidence anchors: [abstract]: "mutual guidance between distillation and quantization, where distillation layers are selected based on quantization loss and quantization layers are selected based on distillation loss"

### Mechanism 2
- Claim: Restrained dynamic matching prevents layer mismatch by ensuring monotonic correspondence between student and teacher layers
- Mechanism: During intermediate hidden layer distillation, the mapping function is constrained to be monotonically increasing, preventing shallow student layers from learning deep teacher layers and vice versa
- Core assumption: Monotonic layer correspondence preserves the hierarchical structure of learned representations
- Evidence anchors: [section]: "we add a monotonically increasing restriction to the mapping function to avoid this situation... the shallow layer of the student model learns the deep layer of the teacher model and the deep layer of the student model learns the shallow layer of the teacher model"

### Mechanism 3
- Claim: Additive Powers-of-Two (APoT) quantization provides non-uniform quantization that can better preserve model accuracy compared to uniform quantization
- Mechanism: APoT quantization represents each level as a sum of power-of-two terms, allowing for more flexible quantization levels that better match the parameter distribution
- Core assumption: Non-uniform quantization can achieve better accuracy than uniform quantization at the same bit-width
- Evidence anchors: [section]: "Additive Powers-of-Two (APoT) quantization... each level is the sum of n Power-of-Two (PoT) terms"

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: The paper uses knowledge distillation to transfer knowledge from the large Whisper teacher model to smaller student models while maintaining performance
  - Quick check question: What is the difference between logits layer distillation and intermediate hidden layer distillation?

- Concept: Model Quantization
  - Why needed here: Quantization is used to reduce model size and improve inference speed by replacing high-precision parameters with low-precision ones
  - Quick check question: What is the difference between Post-training Quantization (PTQ) and Quantization Aware Training (QAT)?

- Concept: Transformer Architecture
  - Why needed here: Whisper is based on the Transformer architecture, so understanding its components (MHA, FFN) is crucial for understanding what can be compressed
  - Quick check question: What are the two main sub-layers in a standard Transformer layer?

## Architecture Onboarding

- Component map: Teacher Whisper Small → Distillation (logits + hidden layers) → Quantization (layer selection) → Compressed student model
- Critical path: Teacher → Distillation (logits + hidden layers) → Quantization (layer selection) → Compressed student model
- Design tradeoffs:
  - Uniform vs APoT quantization: Uniform is simpler but APoT may preserve more accuracy
  - Full vs partial quantization: Full quantization achieves higher compression but may cause more accuracy degradation
  - Layer selection strategy: Different matching strategies affect the quality of knowledge transfer
- Failure signatures:
  - Performance degradation exceeding acceptable thresholds
  - Layer selection instability across training runs
  - Quantization noise overwhelming the distilled knowledge
- First 3 experiments:
  1. Implement basic knowledge distillation with static layer matching to establish baseline performance
  2. Add quantization to the distilled model using uniform 8-bit quantization
  3. Implement mutual guidance layer selection and compare performance against the baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of layers to select for quantization when using the APoT quantization method?
- Basis in paper: [inferred] The paper mentions that selecting only 50% of the layers for quantization using APoT quantization resulted in minimal performance degradation compared to the full model
- Why unresolved: The paper does not provide a detailed analysis of the impact of different quantization ratios on model performance and compression efficiency
- What evidence would resolve it: A systematic study varying the ratio of layers selected for quantization, and evaluating the trade-off between performance and compression efficiency

### Open Question 2
- Question: How does the choice of layer matching strategy (static, dynamic, or restrained dynamic) impact the effectiveness of knowledge distillation in compressing Whisper models?
- Basis in paper: [explicit] The paper proposes three layer matching strategies for intermediate hidden layers during distillation and evaluates their impact on model compression
- Why unresolved: The paper does not provide a comprehensive comparison of the three strategies in terms of their impact on model compression and performance
- What evidence would resolve it: A detailed comparison of the three layer matching strategies, including their impact on model compression, performance, and computational complexity

### Open Question 3
- Question: How does the proposed DQ-Whisper framework perform on other multilingual speech recognition tasks beyond the Japanese CSJ dataset?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of DQ-Whisper on the Japanese CSJ dataset, but does not evaluate its performance on other multilingual datasets
- Why unresolved: The paper does not provide any evidence of the framework's generalizability to other multilingual speech recognition tasks
- What evidence would resolve it: Evaluating the DQ-Whisper framework on a diverse set of multilingual speech recognition datasets and comparing its performance to other state-of-the-art compression methods

## Limitations

- All experiments conducted on Japanese datasets only, limiting generalizability to other languages
- Layer selection mechanism lacks quantitative validation of its effectiveness compared to baseline methods
- Performance claims rely on comparisons against baseline Whisper models without comprehensive ablation studies

## Confidence

**Confidence Level: Medium** - The paper presents a novel joint distillation and quantization framework, but several critical details remain underspecified. The layer selection mechanism based on mutual guidance between distillation and quantization losses lacks quantitative validation of its effectiveness. The three layer matching strategies are described conceptually but without sufficient implementation details for reproduction. Most critically, all experiments were conducted on Japanese datasets (CSJ and IHJ), raising questions about generalization to other languages and tasks.

## Next Checks

1. **Layer Selection Validation**: Implement the three layer matching strategies (static, dynamic, restrained dynamic) and conduct controlled experiments comparing their effectiveness on layer sensitivity analysis. Measure whether layers selected by mutual guidance consistently outperform randomly selected layers across different compression ratios.

2. **Cross-Lingual Generalization Test**: Evaluate the compressed models on non-Japanese datasets (e.g., English LibriSpeech or multilingual CoVoST) to verify that the compression techniques generalize beyond the Japanese language. Compare performance degradation patterns across languages.

3. **Ablation Study**: Systematically remove components of the framework (distillation only, quantization only, different matching strategies) to quantify the individual contributions of knowledge distillation, quantization method, and layer selection strategy to the overall compression-accuracy tradeoff.