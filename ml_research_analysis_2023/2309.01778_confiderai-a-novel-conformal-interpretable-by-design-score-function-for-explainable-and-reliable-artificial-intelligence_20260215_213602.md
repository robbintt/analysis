---
ver: rpa2
title: 'CONFIDERAI: a novel CONFormal Interpretable-by-Design score function for Explainable
  and Reliable Artificial Intelligence'
arxiv_id: '2309.01778'
source_url: https://arxiv.org/abs/2309.01778
tags:
- conformal
- prediction
- rule
- points
- critical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the need for reliable and trustworthy artificial
  intelligence by combining conformal prediction with interpretable-by-design models.
  It introduces CONFIDERAI, a novel score function for rule-based classifiers that
  leverages both rules predictive ability and points geometrical position within rules
  boundaries.
---

# CONFIDERAI: a novel CONFormal Interpretable-by-Design score function for Explainable and Reliable Artificial Intelligence

## Quick Facts
- arXiv ID: 2309.01778
- Source URL: https://arxiv.org/abs/2309.01778
- Authors: 
- Reference count: 40
- The paper introduces CONFIDERAI, a score function for rule-based classifiers that combines rules' predictive ability with points' geometrical position, and defines conformal critical regions for reliable AI.

## Executive Summary
This paper addresses the challenge of creating reliable and trustworthy artificial intelligence by developing CONFIDERAI, a novel score function specifically designed for rule-based classifiers. The method uniquely combines both the predictive quality of rules and the geometric position of data points within rule boundaries, providing a more nuanced measure of conformity for conformal prediction. By leveraging this score function, the authors define conformal critical sets and regions that guarantee probabilistic performance for target classes while maintaining interpretability.

## Method Summary
CONFIDERAI is a score function for rule-based classifiers that combines rule relevance (computed from true positive and false positive rates) with geometric position within rule boundaries. The score function s(x,y) aggregates contributions from all rules predicting label y and covering point x, weighted by both rule relevance and geometric distance. The method also defines conformal critical sets (CCS) as regions where the score for the target class is low enough to be included in prediction sets, and uses SVDD-based classifiers to approximate these sets as conformal critical regions (CCR) with controlled false positive rates.

## Key Results
- CONFIDERAI achieves high accuracy and efficiency in conformal prediction sets across multiple real-world datasets
- The method successfully identifies conformal critical regions with high probability of correct classification for the target class
- Experimental results demonstrate the approach works well on diverse applications including DNS tunneling detection, cardiovascular disease prediction, and vehicle platooning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CONFIDERAI combines rule-based model performance metrics with geometric position within rules to create a score function that captures both predictive ability and confidence.
- Mechanism: The score function s(x,y) is defined as the sum over all rules rk predicting label y and covering point x, weighted by τ(x,rk)(1-R(rk)), where τ(x,rk) encodes geometric position within rule boundaries and R(rk) is rule relevance (C(rk)·(1-E(rk))).
- Core assumption: Points closer to rule boundaries have lower conformity and thus higher scores; rule relevance reflects predictive quality and should inversely affect the score.
- Evidence anchors:
  - [abstract] "a new score function for rule-based models that leverages both rules predictive ability and points geometrical position within rules boundaries"
  - [section] "γ = γ(x, rk) defined as: NkX i=1 (1/d− i (x, cik ) + 1/d+ i (x, cik )), where φ(d) : R → R being a monotonically decreasing (scalar) function"
  - [corpus] Weak - no direct corpus evidence of similar geometric rule scoring functions
- Break condition: If rule boundaries are not well-defined hyper-rectangles, or if rules overlap in ways that violate the additive assumption in the score function.

### Mechanism 2
- Claim: The conformal critical set (CCS) defines regions where high probabilistic guarantees can be provided for the target class.
- Mechanism: CCS is defined as Sε = {x | s(x, +1) ≤ sε, s(x, 0) > sε}, capturing points where the score for the target class is low enough to be included in the conformal prediction set while non-target class scores are too high.
- Core assumption: The score function properly ranks conformity, and the quantile sε provides the desired coverage guarantee.
- Evidence anchors:
  - [abstract] "we define a set in which the performance on a target class is guaranteed by the score function of the conformal prediction itself"
  - [section] "Conformal prediction states that, for any machine learning model ˆf (x)y : X − → Y , it is possible to define a score function s : X × Y − → R"
  - [corpus] Weak - no direct corpus evidence of conformal critical sets as defined here
- Break condition: If the score function does not properly reflect true conformity, or if the calibration set is too small to provide reliable quantile estimates.

### Mechanism 3
- Claim: Conformal critical regions (CCR) approximate the CCS using SVDD-based classifiers to provide closed, well-defined regions with controlled false positive rates.
- Mechanism: SVDD (Support Vector Data Description) is trained to separate conformal-critical points from non-critical points, with SafeSVDD iterations minimizing false positives.
- Core assumption: SVDD can effectively approximate the CCS boundary, and the false positive rate control provides practical usability.
- Evidence anchors:
  - [abstract] "we individuate conformal critical regions characterized by the largest number of target points and the minimum non-target points"
  - [section] "The construction of conformal critical regions is model-agnostic... a good model is the Support Vector Data Description"
  - [corpus] Weak - no direct corpus evidence of SVDD for conformal critical regions, though SVDD is well-established for outlier detection
- Break condition: If the target class distribution is too complex for SVDD to approximate, or if false positive constraints cannot be met with available data.

## Foundational Learning

- Concept: Conformal prediction framework and marginal coverage guarantees
  - Why needed here: The entire methodology relies on CP to provide probabilistic guarantees about prediction sets
  - Quick check question: What is the difference between marginal coverage and conditional coverage in conformal prediction?

- Concept: Rule-based models and rule relevance metrics
  - Why needed here: CONFIDERAI is specifically designed for rule-based models, using rule relevance (C(rk)·(1-E(rk))) as a key component
  - Quick check question: How is rule relevance computed from true positive rate and false positive rate?

- Concept: Quantile-based score functions and calibration set selection
  - Why needed here: The score function requires computing quantiles from calibration data, and the paper discusses sample complexity bounds
  - Quick check question: How does the size of the calibration set affect the reliability of conformal prediction?

## Architecture Onboarding

- Component map:
  - Rule-based model (LLM) → Rule extraction and relevance calculation
  - CONFIDERAI score function → Geometric position + rule relevance weighting
  - Calibration set processing → Score computation and quantile determination
  - Conformal critical set definition → Set of points meeting conformity criteria
  - SVDD/SafeSVDD classifier → Approximation of CCS as closed region

- Critical path: Train LLM → Compute CONFIDERAI scores → Determine sε from calibration → Define CCS → Train SVDD on CCS/non-CCS distinction → Output CCR with controlled FPR

- Design tradeoffs:
  - Score function complexity vs. interpretability
  - SVDD approximation accuracy vs. computational cost
  - False positive control strictness vs. region coverage
  - Rule relevance weighting vs. geometric position emphasis

- Failure signatures:
  - Low empirical coverage despite theoretical guarantees
  - CCRs that are too small to be useful or too large to provide meaningful guarantees
  - Score function that doesn't distinguish between conforming and non-conforming points
  - High computational cost preventing real-time applications

- First 3 experiments:
  1. Implement CONFIDERAI on a simple 2D dataset with known rule structure to visualize score distributions
  2. Test CCS definition on synthetic data where ground truth conformity is known
  3. Compare SVDD vs. SafeSVDD performance on a benchmark dataset with varying false positive constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the number of samples (sample complexity) in the calibration set be chosen to guarantee the desired probabilistic properties while maintaining efficiency?
- Basis in paper: [explicit] The paper discusses the concept of (E, δ)-validity and derives explicit bounds on the number of samples necessary to guarantee the desired probabilistic properties. It also mentions the need for a more accurate setting to investigate how E should be chosen in the interval (ε, 1) under the (ε, δ)-validity constraint and consequently deriving nc under the bound in Equation (7).
- Why unresolved: The paper mentions the need for a more accurate setting to investigate how E should be chosen in the interval (ε, 1) under the (ε, δ)-validity constraint and consequently deriving nc under the bound in Equation (7), but does not provide a solution.
- What evidence would resolve it: A study that investigates how E should be chosen in the interval (ε, 1) under the (ε, δ)-validity constraint and consequently derives nc under the bound in Equation (7) would resolve this question.

### Open Question 2
- Question: How can the performance of the conformal critical regions (CCRs) be evaluated in terms of false positives and false negatives?
- Basis in paper: [explicit] The paper introduces the concept of conformal critical regions (CCRs) and mentions that the performance for the CCRs is evaluated by considering the number of conformal-critical points inside the regions. However, it does not provide a detailed evaluation of false positives and false negatives.
- Why unresolved: The paper introduces the concept of CCRs and mentions that the performance for the CCRs is evaluated by considering the number of conformal-critical points inside the regions, but does not provide a detailed evaluation of false positives and false negatives.
- What evidence would resolve it: A study that evaluates the performance of CCRs in terms of false positives and false negatives would resolve this question.

### Open Question 3
- Question: How can the CONFIDERAI score function be extended to handle multi-label classification tasks?
- Basis in paper: [inferred] The paper focuses on binary classification tasks and introduces the CONFIDERAI score function for rule-based models. However, it does not discuss how the score function can be extended to handle multi-label classification tasks.
- Why unresolved: The paper focuses on binary classification tasks and introduces the CONFIDERAI score function for rule-based models, but does not discuss how the score function can be extended to handle multi-label classification tasks.
- What evidence would resolve it: A study that extends the CONFIDERAI score function to handle multi-label classification tasks would resolve this question.

## Limitations

- The method heavily depends on well-defined rule boundaries, which may not hold for complex real-world data patterns with non-rectangular boundaries
- The SVDD approximation requires careful tuning of false positive constraints, and the paper lacks detailed guidance on parameter selection for these hyperparameters
- Scalability to high-dimensional datasets and robustness to overlapping rules with complex boundaries remain uncertain

## Confidence

**High Confidence**: The theoretical foundation of conformal prediction guarantees and the general framework of combining rule relevance with geometric position. The empirical evaluation on multiple benchmark datasets provides reasonable support for the method's effectiveness.

**Medium Confidence**: The specific implementation details of the CONFIDERAI score function and the exact computational complexity. While the formulas are provided, practical implementation challenges may affect performance.

**Low Confidence**: The scalability of the method to high-dimensional datasets and the robustness of the approach when rules have complex, non-rectangular boundaries or significant overlap.

## Next Checks

1. **Empirical Coverage Validation**: Conduct experiments to verify that the conformal prediction sets achieve the theoretical coverage guarantees (e.g., 90%) across different dataset sizes and dimensionalities, particularly for smaller calibration sets.

2. **Boundary Condition Testing**: Test the CONFIDERAI score function on datasets with known rule structures that include non-rectangular boundaries and overlapping rules to identify where the geometric component breaks down.

3. **False Positive Rate Analysis**: Systematically vary the false positive constraints in SVDD/SafeSVDD training and measure the tradeoff between conformal critical region coverage and the actual false positive rate to establish practical guidelines for parameter selection.