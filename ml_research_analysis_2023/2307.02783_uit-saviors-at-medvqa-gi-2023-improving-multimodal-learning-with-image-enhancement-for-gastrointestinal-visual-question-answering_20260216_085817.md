---
ver: rpa2
title: 'UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement
  for Gastrointestinal Visual Question Answering'
arxiv_id: '2307.02783'
source_url: https://arxiv.org/abs/2307.02783
tags:
- image
- question
- black
- mask
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multimodal learning approach with image enhancement
  for the Visual Question Answering (VQA) task in gastrointestinal endoscopy images.
  The method employs a multimodal fusion architecture that combines a BERT encoder
  for textual features extraction with pre-trained vision models based on CNN and
  Transformer architectures for visual features extraction.
---

# UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering

## Quick Facts
- arXiv ID: 2307.02783
- Source URL: https://arxiv.org/abs/2307.02783
- Reference count: 40
- Best method achieves 87.25% accuracy and 91.85% F1-Score on development test set

## Executive Summary
This paper presents a multimodal learning approach for Visual Question Answering (VQA) in gastrointestinal endoscopy images, combining BERT for textual feature extraction with pre-trained vision models for visual features. The key innovation is an image enhancement pipeline that removes specular highlights and black masks to improve region-of-interest quality. The method achieves state-of-the-art performance on the MEDVQA-GI 2023 challenge, with transformer-based vision models (particularly BEiT) outperforming CNN-based alternatives. The best configuration reaches 87.25% accuracy on the development set and 82.01% on the private test set.

## Method Summary
The approach uses multimodal fusion combining BERT encoder for questions with pre-trained vision models (CNNs and Transformers) for images. An image enhancement pipeline preprocesses endoscopy images by removing specular highlights through thresholding and Telea inpainting, and eliminating black masks by creating artificial border masks. Features from text and image are concatenated along the embedding dimension before classification using a multi-label sigmoid output. The model is trained using BCEWithLogitsLoss with Adam optimizer (learning rate 5e-5, batch size 64, 15 epochs) on a 1600/200/200 train/validation/test split.

## Key Results
- Six out of eight vision models showed improved F1-Score after image enhancement
- Transformer-based models (BEiT) significantly outperformed CNN-based models
- Best method (BERT + BEiT + enhancement) achieved 87.25% accuracy and 91.85% F1-Score on development test set
- Private test set results: 82.01% accuracy and 86.77% F1-Score

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Image enhancement improves VQA accuracy by reducing misleading visual artifacts
- Mechanism: Preprocessing eliminates bright spots and black borders that distort CNN/Transformer feature maps, allowing models to focus on the region of interest
- Core assumption: Specular highlights and black masks significantly distort feature extraction of both CNN and Transformer models
- Evidence anchors: Six out of eight vision models achieved better F1-Score after enhancement; specular highlights "can disrupt the algorithms" and black masks "generate valley information, which can reduce polyp localization performance"
- Break condition: If artifact removal is too aggressive and erases subtle but clinically relevant visual cues

### Mechanism 2
- Claim: Transformer-based vision models outperform CNN-based models in extracting visual features for VQA
- Mechanism: Transformer models capture long-range spatial dependencies and context better than CNNs, crucial for identifying complex abnormalities in GI images
- Core assumption: VQA task benefits more from global context understanding than local feature detection
- Evidence anchors: BEiT-B achieved outstanding results with 87.2% accuracy and 91.85% F1-Score; highlights "dominance of Transformer-based vision models over CNNs"
- Break condition: If dataset is dominated by small, localized abnormalities where CNNs excel

### Mechanism 3
- Claim: Multimodal fusion via concatenation preserves complementary information from text and image
- Mechanism: BERT encodes rich textual context; vision encoders extract spatial features; concatenation retains both modalities before classification
- Core assumption: Simple concatenation is sufficient for this dataset's complexity; no need for attention-based fusion
- Evidence anchors: Concatenation fusion used to combine features along embedding dimension; achieved 87.25% accuracy and 91.85% F1-Score
- Break condition: If modality gap is large and requires cross-modal attention to align features

## Foundational Learning

- Concept: Medical Visual Question Answering (MedVQA)
  - Why needed here: Task is to answer clinical questions about GI endoscopy images; understanding MedVQA structure is essential for designing multimodal pipeline
  - Quick check question: What are the two main data sources combined in MedVQA?

- Concept: Image preprocessing for medical imaging
  - Why needed here: Specular highlights and black masks are domain-specific artifacts that degrade model performance; understanding preprocessing techniques is essential
  - Quick check question: What are the two main artifacts targeted by the image enhancement pipeline?

- Concept: Multimodal fusion architectures
  - Why needed here: Model must combine text embeddings from BERT with visual embeddings from vision models; knowing fusion strategies (concatenation vs. attention) is key
  - Quick check question: Which fusion strategy is used in this approach?

## Architecture Onboarding

- Component map: Raw GI endoscopy image + textual question → Enhancement (specular highlights removal + black mask removal) → Vision Encoder (CNN/Transformer) → Text → BERT → Concatenate → Classifier → Binary vector of answer probabilities

- Critical path: Image → Enhancement → Vision Encoder → Text → BERT → Concatenate → Classifier → Answers

- Design tradeoffs:
  - Enhancement vs. speed: More aggressive preprocessing may slow inference but improve accuracy
  - Vision model choice: Transformers are more accurate but computationally heavier than CNNs
  - Fusion method: Concatenation is simple but may miss cross-modal interactions; attention-based fusion is more complex

- Failure signatures:
  - Low recall on questions requiring multiple answers → model struggles with multi-label classification
  - Poor performance on location-based questions → vision encoder not capturing spatial context well
  - Enhancement artifacts → preprocessing removing important clinical details

- First 3 experiments:
  1. Baseline: Raw image + ResNet152 + BERT + concatenation → measure accuracy/F1
  2. Enhanced image only: Same pipeline but with image enhancement → compare to baseline
  3. Model swap: Replace ResNet152 with BEiT (enhanced image) → compare performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed image enhancement method compare to other state-of-the-art image preprocessing techniques for colonoscopy images?
- Basis in paper: Paper mentions several image preprocessing techniques for colonoscopy images but only implements and evaluates specular highlights and black mask removal
- Why unresolved: Paper does not compare proposed method to other state-of-the-art techniques
- What evidence would resolve it: Comprehensive evaluation of proposed method against other state-of-the-art image preprocessing techniques with comparison of their impact on VQA performance

### Open Question 2
- Question: How does the performance of the proposed multimodal fusion architecture vary with different pre-trained vision models?
- Basis in paper: Paper experiments with eight different pre-trained vision models but lacks detailed analysis of performance variation
- Why unresolved: Paper only mentions that Transformer-based models outperform CNN-based models without detailed analysis
- What evidence would resolve it: Detailed analysis of performance of each vision model including strengths, weaknesses, and impact on overall VQA performance

### Open Question 3
- Question: How can the proposed method be improved to handle questions that require multiple positions or colors in the answer?
- Basis in paper: Paper mentions struggles with producing full and precise answers for questions requiring multiple positions or colors
- Why unresolved: Paper does not provide a solution to this limitation
- What evidence would resolve it: Proposed solution to handle questions requiring multiple positions or colors in the answer with evaluation of its effectiveness

## Limitations

- Limited generalizability: Image enhancement effectiveness may not transfer to other medical imaging domains or endoscopic procedures
- Missing ablation studies: No quantification of individual contributions from each enhancement step (specular highlights vs. black mask removal)
- Narrow model exploration: Only eight vision models tested without exploring newer architectures that might yield better performance

## Confidence

**High confidence**: Multimodal fusion architecture using BERT + vision encoders with concatenation is well-established; reported performance metrics are internally consistent

**Medium confidence**: Transformer superiority claim lacks external validation across different datasets or medical imaging domains; performance could be dataset-specific

**Low confidence**: Image enhancement as primary performance driver is weakly supported; no control for confounding factors like training set size or random initialization

## Next Checks

1. **Ablation study on enhancement components**: Systematically disable specular highlights removal and black mask removal separately to quantify individual contributions to performance gains across all eight vision models

2. **Cross-dataset validation**: Test best-performing pipeline (BERT + BEiT + enhancement) on different medical VQA dataset (VQA-RAD or SLAKE) to assess generalizability

3. **Diagnostic visualization**: Generate Grad-CAM or similar attention maps for both enhanced and non-enhanced images across all vision models to verify focus on clinically relevant regions rather than artifacts