---
ver: rpa2
title: Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual
  Language Models
arxiv_id: '2311.09194'
source_url: https://arxiv.org/abs/2311.09194
tags:
- language
- english
- target
- priming
- structural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines whether large multilingual language models
  (LMs) possess abstract grammatical representations that generalize across languages,
  using crosslingual structural priming as a probe. It replicates eight crosslingual
  and four monolingual human structural priming experiments across six languages (English,
  Dutch, Spanish, German, Greek, Polish, Mandarin), measuring whether grammatical
  structures in one language prime homologous structures in another within LMs.
---

# Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models

## Quick Facts
- arXiv ID: 2311.09194
- Source URL: https://arxiv.org/abs/2311.09194
- Reference count: 40
- Large multilingual language models show crosslingual structural priming, indicating shared abstract grammatical representations

## Executive Summary
This study investigates whether large multilingual language models (LMs) possess abstract grammatical representations that generalize across languages, using crosslingual structural priming as a probe. The researchers replicate eight crosslingual and four monolingual human structural priming experiments across six languages, measuring whether grammatical structures in one language prime homologous structures in another within LMs. Using the XGLM family of models and PolyLM, they find significant structural priming effects both within and across languages, with larger models generally displaying stronger effects. These results indicate that multilingual LMs develop shared abstract grammatical representations that functionally influence text generation, paralleling human bilingual processing.

## Method Summary
The study uses pre-trained multilingual autoregressive Transformer models (XGLM family and PolyLM) to test for structural priming effects. For each experimental item from human studies, the models are prompted with prime sentences and normalized probabilities of target sentences are calculated. Linear mixed-effects models with random intercepts for experimental items are used to test whether prime type significantly predicts target structure probability. Eight crosslingual structural priming studies covering six languages (English, Dutch, Spanish, German, Greek, Polish, Mandarin) and four monolingual structural priming experiments in three non-English languages are replicated using the same stimuli as the human experiments.

## Key Results
- Multilingual LMs show significant structural priming effects both within and across languages
- Larger models (XGLM 7.5B vs 564M) generally display larger priming effect sizes
- Models successfully replicate human priming patterns across typologically diverse language pairs
- Despite not being trained on Greek, PolyLMs successfully model monolingual structural priming in Greek

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual language models acquire shared abstract grammatical representations that generalize across languages.
- Mechanism: During pretraining, models are exposed to parallel or typologically similar structures across multiple languages, allowing statistical co-occurrence patterns to form crosslingual abstractions.
- Core assumption: Grammatical structures are sufficiently similar across languages to be inferred from co-occurrence statistics.
- Evidence anchors:
  - [abstract]: "These results demonstrate that grammatical representations in multilingual language models are not only similar across languages, but they can causally influence text produced in different languages."
  - [section]: "This supports the claim that language models learn generalized, abstract, and multilingual representations of grammatical structure."
  - [corpus]: Found 25 related papers; average neighbor FMR=0.418, indicating moderate relevance of nearby literature.
- Break condition: If training data lacks sufficient crosslingual grammatical overlap, or if tokenization breaks morphological cues critical to structure.

### Mechanism 2
- Claim: Structural priming in models reflects functional, causally influential representations, not just surface-level lexical repetition.
- Mechanism: By priming with one structure and measuring likelihood of a homologous structure in another language, the model reveals internalized, abstract mappings that guide generation.
- Core assumption: Priming effects are not artifacts of lexical overlap but arise from grammatical abstraction.
- Evidence anchors:
  - [abstract]: "we measure crosslingual structural priming in large language models, comparing model behavior to human experimental results from eight crosslingual experiments covering six languages"
  - [section]: "Our results provide evidence that these shared representations are not only latent in multilingual models' representation spaces, but also causally impact their outputs."
  - [corpus]: "Structural priming demonstrates abstract grammatical representations in multilingual language models" is a high-relevance neighbor (FMR=0.659).
- Break condition: If priming is driven by token-level overlap rather than grammatical structure, or if model lacks sufficient crosslingual exposure.

### Mechanism 3
- Claim: Larger models and more extensive crosslingual training yield stronger structural priming effects.
- Mechanism: Increased model capacity and multilingual exposure enable finer-grained abstractions and better generalization across linguistic structures.
- Core assumption: Model scaling laws apply to grammatical abstraction acquisition, not just task performance.
- Evidence anchors:
  - [section]: "we see expected patterns across models. For example, for the XGLMs trained on 30 languages (XGLM 564M, 1.7B, 2.9B, and 7.5B), the larger models tend to display larger effect sizes than the smaller models"
  - [section]: "despite not being trained on Greek, the PolyLMs are able to successfully model monolingual structural priming in Greek" (suggesting robustness but also hinting at lexical overlap limits).
  - [corpus]: High relevance of "Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages" (FMR=0.0).
- Break condition: If scaling does not improve crosslingual grammatical abstraction, or if smaller models match larger ones on key priming tasks.

## Foundational Learning

- Concept: Crosslingual structural priming
  - Why needed here: It is the empirical method used to probe for abstract grammatical representations shared across languages.
  - Quick check question: Does priming from a Spanish sentence increase the likelihood of an English sentence with the same grammatical structure?
- Concept: Autoregressive language modeling
  - Why needed here: The study uses next-token prediction to compute structural priming effects.
  - Quick check question: In an autoregressive model, what probability is computed when given a prime sentence and asked to predict a target?
- Concept: Tokenization and morphological decomposition
  - Why needed here: Tokenization choices affect the model's ability to capture grammatical structures, especially in morphologically rich languages.
  - Quick check question: How might WordPiece tokenization obscure grammatical agreement cues in a highly inflected language?

## Architecture Onboarding

- Component map: XGLM/PolyLM autoregressive Transformers -> token embedding layer -> stacked Transformer blocks -> causal attention -> output vocabulary logits
- Critical path: Input prime sentence -> tokenized -> embedded -> processed by Transformer -> output probabilities for each target token -> multiply to get sentence probability -> normalize across targets -> compare priming conditions
- Design tradeoffs: Larger models improve grammatical abstraction but increase compute cost; shared vocabularies across languages may boost crosslingual transfer but risk language contamination
- Failure signatures: No priming effect despite crosslingual exposure (suggests tokenization or data issues); reversed priming direction (suggests spurious lexical overlap)
- First 3 experiments:
  1. Replicate within-language dative alternation priming in Dutch to confirm basic structural priming capability
  2. Test crosslingual DO/PO priming from English to Dutch with controlled lexical overlap
  3. Measure priming asymmetry by reversing prime/target languages for Greek->English stimuli

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do language models acquire shared grammatical representations across languages without explicit grammatical training?
- Basis in paper: [explicit] The paper notes that language models learn grammatical knowledge through exposure and discusses the implications for usage-based accounts of language acquisition.
- Why unresolved: While the paper demonstrates that models acquire these representations, it does not investigate the specific learning mechanisms or developmental trajectory that leads to crosslingual grammatical abstractions.
- What evidence would resolve it: Analysis of how grammatical representations develop during training, comparisons with models trained on different data distributions, or controlled experiments manipulating training data to identify key factors for crosslingual transfer.

### Open Question 2
- Question: Do the crosslingual structural priming effects observed in language models reflect true abstract grammatical representations or merely surface-level statistical patterns?
- Basis in paper: [explicit] The paper acknowledges the possibility that priming effects could reflect "likely co-occurrence of distinct, associated representations, rather than a single, common, abstract representation."
- Why unresolved: The study demonstrates functional effects but cannot definitively distinguish between these two explanations for the observed priming patterns.
- What evidence would resolve it: Experiments testing priming effects with novel sentence pairs, analysis of internal representations to identify shared grammatical features, or comparison with models trained on different linguistic distributions.

### Open Question 3
- Question: Why do structural priming effects differ between language directions (e.g., Dutch->English vs. English->Dutch) in language models but not in humans?
- Basis in paper: [explicit] The paper observes this asymmetry in models and contrasts it with human studies where crosslingual priming appears most reliable when the prime is in participants' native language.
- Why unresolved: The paper suggests several possible explanations (training data imbalance, morphological differences) but does not test these hypotheses or determine which factors are most influential.
- What evidence would resolve it: Controlled experiments manipulating model training data, analysis of model representations for different language directions, or systematic comparison of priming effects across multiple language pairs with varying typological relationships.

## Limitations

- The study does not directly compare model representations to human neural or behavioral data beyond structural priming outcomes
- The role of tokenization in capturing grammatical structure is not systematically explored, particularly for morphologically rich languages
- The paper cannot definitively distinguish between true abstract grammatical representations versus surface-level statistical patterns that happen to generalize across languages

## Confidence

- Crosslingual structural priming exists in multilingual LMs - High confidence
- Priming reflects abstract grammatical representations rather than lexical overlap - Medium confidence
- Larger models show stronger priming effects - High confidence
- These representations functionally influence generation - Medium confidence

## Next Checks

1. **Tokenization ablation study**: Systematically compare priming effects across different tokenization schemes (WordPiece, SentencePiece, morphological segmentation) for morphologically rich languages to quantify the impact of tokenization choices on grammatical abstraction capture.

2. **Controlled lexical overlap experiments**: Design new stimuli with precisely controlled lexical overlap (e.g., identical content words but different syntactic frames, or vice versa) to definitively separate grammatical from lexical priming contributions.

3. **Cross-modal priming extension**: Test whether priming effects transfer to modalities beyond text generation, such as cloze completions or grammaticality judgments, to establish whether the representations support multiple types of grammatical processing analogous to human bilingual processing.