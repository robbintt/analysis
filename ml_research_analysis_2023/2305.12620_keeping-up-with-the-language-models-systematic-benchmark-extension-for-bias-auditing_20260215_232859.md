---
ver: rpa2
title: 'Keeping Up with the Language Models: Systematic Benchmark Extension for Bias
  Auditing'
arxiv_id: '2305.12620'
source_url: https://arxiv.org/abs/2305.12620
tags:
- bias
- samples
- bbnli
- language
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new dataset called BBNLI-next to help test
  if language models are fair. The dataset is built by taking an existing benchmark
  and adding small changes to the text, making it harder for models to get the right
  answer.
---

# Keeping Up with the Language Models: Systematic Benchmark Extension for Bias Auditing

## Quick Facts
- arXiv ID: 2305.12620
- Source URL: https://arxiv.org/abs/2305.12620
- Reference count: 18
- Key outcome: BBNLI-next dataset reduces NLI model accuracy from 95.3% to 57.5% while revealing bias patterns

## Executive Summary
This paper introduces BBNLI-next, a systematically extended benchmark for bias auditing in natural language inference models. The dataset builds on the existing BBNLI benchmark by applying masked language model-generated lexical variations, adversarial filtering, and human validation to create more challenging examples that expose model brittleness and bias. The new dataset contains approximately three times as many samples as the original and dramatically reduces model performance, demonstrating its effectiveness at identifying bias that simpler benchmarks miss. The paper also proposes disaggregate counterfactual bias measures that separately quantify pro-stereotype, anti-stereotype, and group-insensitive errors, providing clearer insight into model behavior.

## Method Summary
The method systematically extends the BBNLI dataset through multiple stages: (1) adding masked tokens to hypothesis templates from BBNLI, (2) using a masked language model to generate 20 lexical variants per hypothesis, (3) applying adversarial filtering where NLI models retain only mispredicted samples, (4) manual human validation to ensure generated hypotheses are valid and maintain bias intent, and (5) expanding with counterfactual hypotheses by swapping social groups. The resulting BBNLI-next dataset is evaluated using both traditional accuracy metrics and novel disaggregate counterfactual bias measures that distinguish between different types of model errors.

## Key Results
- BBNLI-next reduces state-of-the-art NLI model accuracy from 95.3% to 57.5% on average
- The dataset is approximately 3x larger than the original BBNLI benchmark
- Disaggregate bias measures reveal that most errors are group-insensitive rather than due to pro- or anti-stereotype bias

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using masked language models to generate lexical variations preserves semantic intent while making samples harder for NLI models to predict correctly
- **Mechanism:** Key tokens in BBNLI hypothesis templates are masked and filled with top-20 suggestions from a masked LM, creating 20 lexical variants per hypothesis that maintain bias content while introducing phrasing diversity
- **Core assumption:** Masked LM-generated variations will be semantically close to originals and still reflect intended pro-/anti-stereotype bias
- **Evidence anchors:** The generated hypotheses are used with premises in original BBNLI benchmark; BBNLI-next reduces NLI accuracy to 57.5%
- **Break condition:** If masked LM generates variations that are semantically unrelated or remove bias intent, dataset validity is lost

### Mechanism 2
- **Claim:** Adversarial filtering using NLI models identifies and retains only the most challenging samples for bias auditing
- **Mechanism:** Generated samples are filtered by checking whether any of three NLI models mispredict them; samples with correct neutral predictions are discarded as too easy
- **Core assumption:** Mispredictions by NLI models indicate either model brittleness or bias, both useful for auditing
- **Evidence anchors:** Adversarial filtering keeps only samples mispredicted by LMs fine-tuned for NLI; average accuracy drops from 95.3% to 57.5%
- **Break condition:** If NLI models become so brittle that they mispredict almost all samples, filter would retain uninformative data

### Mechanism 3
- **Claim:** Disaggregating bias measures into pro-stereotype, anti-stereotype, and group-insensitive error components gives clearer insight into model behavior
- **Mechanism:** Counterfactual bias measures separately count mispredictions attributable to pro-stereotype bias, anti-stereotype bias, and group-insensitive errors (same wrong prediction regardless of group)
- **Core assumption:** Errors consistent across counterfactual pairs are due to model brittleness, not bias
- **Evidence anchors:** Introduces disaggregate counterfactual bias measures to distinguish between types of model errors; points out subtleties between robustness and bias in model auditing
- **Break condition:** If distinction between bias and brittleness becomes ambiguous, measure loses interpretability

## Foundational Learning

- **Concept:** Natural Language Inference (NLI) task
  - **Why needed here:** The entire benchmark is built around NLI; understanding entailment, contradiction, and neutral labels is essential to interpret model predictions and bias
  - **Quick check question:** Given a premise and hypothesis, can you label them as entailment, contradiction, or neutral without ambiguity?

- **Concept:** Adversarial filtering in NLP
  - **Why needed here:** This technique is used to select only the hardest examples for bias auditing, making the benchmark more robust
  - **Quick check question:** What is the difference between adversarial filtering and adversarial attack generation?

- **Concept:** Counterfactual reasoning
  - **Why needed here:** The benchmark uses counterfactual hypotheses (switching social groups) to expose bias and differentiate it from brittleness
  - **Quick check question:** If a model mispredicts both "men are good at X" and "women are good at X" with the same label, what does that suggest about the error?

## Architecture Onboarding

- **Component map:** BBNLI dataset -> Masked template generator -> Masked LM -> NLI model ensemble -> Human validator -> Counterfactual expander -> BBNLI-next dataset

- **Critical path:**
  1. Expand templates with masked tokens
  2. Generate 20 variants per hypothesis using masked LM
  3. Filter with NLI models (keep mispredicted samples)
  4. Human validation (retain only valid bias hypotheses)
  5. Expand with counterfactuals
  6. Assemble final dataset

- **Design tradeoffs:**
  - Masked LM vs human-generated variations: LM is faster but may introduce noise
  - Number of variants (20): Balances diversity with manual validation cost
  - Adversarial filtering vs random sampling: Ensures difficulty but may overfit to specific models

- **Failure signatures:**
  - Low variance in generated hypotheses → insufficient diversity
  - High proportion of invalid hypotheses → masked LM not aligned with bias intent
  - No improvement in bias detection over BBNLI → adversarial filtering ineffective

- **First 3 experiments:**
  1. Run masked LM on a small subset of templates and manually check semantic validity of generated hypotheses
  2. Test adversarial filtering with one NLI model on a small set and verify that retained samples are indeed mispredicted
  3. Compare pro- vs anti-stereotype error counts on a tiny dataset to validate disaggregate measure logic

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adversarial filtering using NLI models reliably identify challenging bias samples for unseen models?
- **Basis in paper:** The paper shows that ELECTRA, which was not part of adversarial filtering, performed worse on BBNLI-next, suggesting adversarial filtering helps identify hard examples
- **Why unresolved:** The study only used four models and a single masking strategy; it's unclear if this generalizes to other architectures or if different filtering strategies might be needed
- **What evidence would resolve it:** Testing adversarial filtering on a wider range of models and with varied masking strategies to see if the pattern holds

### Open Question 2
- **Question:** How does the performance of NLI models on BBNLI-next relate to their fairness in real-world applications?
- **Basis in paper:** The paper introduces disaggregate counterfactual measures to separate bias from brittleness but doesn't test if these measures predict real-world bias
- **Why unresolved:** The study focuses on controlled bias types and doesn't examine how model errors on BBNLI-next translate to harmful outcomes in deployed systems
- **What evidence would resolve it:** Evaluating models on BBNLI-next and then testing them on downstream tasks to see if lower bias scores correlate with reduced harm

### Open Question 3
- **Question:** What are the best practices for constructing bias auditing datasets that remain challenging as language models evolve?
- **Basis in paper:** The paper proposes a method using masked language models and adversarial filtering but notes limitations like manual validation and focus on English/US contexts
- **Why unresolved:** The study is a single case with a specific task and bias domain; it's unclear how to scale this approach or adapt it for other languages, tasks, or intersectional bias
- **What evidence would resolve it:** Developing and testing automated validation methods, expanding to more diverse contexts, and evaluating the longevity of benchmarks across model generations

## Limitations

- The method relies heavily on masked language models that may not consistently preserve semantic meaning and bias intent across all generated variations
- Adversarial filtering effectiveness depends on the specific NLI models used and may not generalize to unseen architectures
- Manual validation introduces subjectivity and scalability challenges that limit the approach's applicability to larger datasets or different languages
- The focus on English language and US-centric social groups limits the benchmark's applicability to global contexts and intersectional bias scenarios

## Confidence

- **Masked LM generation effectiveness**: Medium - Dataset statistics and performance drops provide some evidence, but systematic validation of semantic equivalence is lacking
- **Adversarial filtering reliability**: Medium - Dramatic performance drop suggests effectiveness, but filtering criteria may be overly sensitive to specific model architectures
- **Disaggregate bias measure validity**: Medium - The approach offers theoretical clarity, but doesn't demonstrate practical advantages over traditional aggregated scores in real auditing scenarios

## Next Checks

1. **Semantic validation study**: Manually evaluate 100 randomly selected BBNLI-next hypotheses to verify that masked LM-generated variations maintain semantic equivalence to originals and preserve the intended pro- or anti-stereotype bias content. This would directly test whether the core mechanism of lexical variation generation is sound.

2. **Adversarial filtering robustness test**: Apply the same adversarial filtering process to a baseline dataset with known bias (e.g., original BBNLI) and measure how many samples are retained. If filtering removes a substantial portion of known bias examples, this would indicate the process may be too stringent or model-dependent.

3. **Counterfactual pair analysis**: Select 50 counterfactual pairs from BBNLI-next and have human annotators label which errors are due to model brittleness versus actual bias. Compare these judgments against the algorithm's disaggregate bias measures to validate whether the theoretical distinction between brittleness and bias is practically meaningful.