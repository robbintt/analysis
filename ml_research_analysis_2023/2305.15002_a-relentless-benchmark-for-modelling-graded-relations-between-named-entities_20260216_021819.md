---
ver: rpa2
title: A RelEntLess Benchmark for Modelling Graded Relations between Named Entities
arxiv_id: '2305.15002'
source_url: https://arxiv.org/abs/2305.15002
tags:
- relation
- pairs
- each
- entity
- template
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RELENTLESS, a new benchmark for modeling
  graded relations between named entities. The task involves ranking entity pairs
  based on how well they satisfy a given graded relation, using only a description
  and five prototypical examples.
---

# A RelEntLess Benchmark for Modelling Graded Relations between Named Entities

## Quick Facts
- arXiv ID: 2305.15002
- Source URL: https://arxiv.org/abs/2305.15002
- Reference count: 11
- Primary result: Models achieve Spearman correlations of 0.5-0.6 on ranking entity pairs by graded relations, still below human performance

## Executive Summary
RELENTLESS is a new benchmark for evaluating how well models can rank entity pairs based on graded relations like "influenced by" or "similar to." The task requires models to order entity pairs according to how well they satisfy a given relation, using only a textual description and five prototypical examples. The benchmark tests five common relations across 89-108 entity pairs each. Results show that larger language models significantly outperform smaller ones, with GPT-4 and Flan-T5 XXL achieving the best results. However, even the top models lag behind human performance by over 20 percentage points, highlighting the difficulty of capturing fine-grained relational knowledge.

## Method Summary
The RELENTLESS benchmark evaluates models on ranking entity pairs by graded relations using few-shot prompting. For each of five relations (competitor/rival, friend/ally, influenced by, known for, similar to), models receive a description and five prototypical examples. Entity pairs are ranked by computing perplexity scores for language models or cosine similarities for embedding models, then Spearman rank correlation is calculated against ground truth rankings. The benchmark tests both instruction-tuned models (Flan-T5, GPT-3.5, GPT-4) and standard models (T5, OPT) across various sizes, comparing them to naive baselines.

## Key Results
- Larger models consistently outperform smaller ones, with Spearman correlations ranging from near-zero to 0.6
- GPT-4 and Flan-T5 XXL achieve the highest performance at 0.5-0.6 Spearman correlation
- Perplexity-based language model scoring outperforms static embedding approaches
- All models significantly underperform human agreement levels by over 20 percentage points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model size strongly correlates with performance on graded relation ranking tasks
- Mechanism: Larger models have more parameters and capacity to capture fine-grained relational distinctions between entity pairs
- Core assumption: The underlying knowledge exists in the pretraining data and can be accessed through larger parameter spaces
- Evidence anchors:
  - [abstract] "Overall, we find a strong correlation between model size and performance, with smaller Language Models struggling to outperform a naive baseline."
  - [section] "For Flan-T5, OPT, and OPT-IML we can see a strong correlation between performance and model size."
  - [corpus] Weak evidence - only 5 related papers found, none directly addressing model size correlation
- Break condition: When the knowledge is not present in pretraining data regardless of model size

### Mechanism 2
- Claim: Few-shot prompting with prototypical examples significantly improves performance over zero-shot
- Mechanism: Providing examples helps models understand the specific graded nature of the relation and how to rank instances along a continuum
- Core assumption: The examples capture the essential characteristics of the graded relation and help the model generalize
- Evidence anchors:
  - [section] "Figure 2a shows the result for the different set-ups, when the QA template is used. We can see that all models improve when more prototypical examples are provided."
  - [section] "Unlike for the QA template, however, Flan-T5XXL performs poorly in the zero-shot setting."
  - [corpus] Weak evidence - no direct corpus evidence about few-shot prompting effectiveness
- Break condition: When the relation is too complex or abstract for examples to capture effectively

### Mechanism 3
- Claim: Perplexity-based scoring with language models captures graded relational knowledge better than static embeddings
- Mechanism: LMs can dynamically reason about relationships between entities based on contextual understanding rather than fixed vector distances
- Core assumption: The pretraining corpus contains sufficient information about graded relationships between entities
- Evidence anchors:
  - [section] "We use the proposed benchmark to evaluate state-of-the-art relation embedding strategies as well as several recent LLMs"
  - [section] "The embedding based models generally underperform the LMs, with RelBERTLARGE achieving the best results."
  - [corpus] No direct evidence in corpus about perplexity vs embeddings comparison
- Break condition: When the relation requires knowledge not present in the pretraining corpus

## Foundational Learning

- Concept: Spearman rank correlation as evaluation metric
  - Why needed here: The task requires ranking entity pairs rather than binary classification, so rank correlation measures how well predicted orderings match ground truth
  - Quick check question: If a model perfectly ranks the top 10 pairs but reverses the bottom 10, what would the Spearman correlation be?

- Concept: Graded vs binary relations
  - Why needed here: Understanding that relations like "influenced by" have degrees of truth rather than simple yes/no values is fundamental to grasping the problem
  - Quick check question: Is "Microsoft is known for Windows" more or less prototypical than "Apple is known for iPhone"?

- Concept: Few-shot learning paradigm
  - Why needed here: The benchmark specifically uses 5 prototypical examples to guide model predictions, requiring understanding of in-context learning
  - Quick check question: Why might providing examples be more effective than just describing the relation?

## Architecture Onboarding

- Component map: Data preparation → Prompt engineering → Model inference → Perplexity calculation → Ranking → Spearman correlation evaluation
- Critical path: Prompt creation → Model inference → Score aggregation → Final ranking
- Design tradeoffs: Template format (QA vs LC) vs model family compatibility, few-shot examples vs zero-shot simplicity, open vs closed models
- Failure signatures: Poor performance on cross-type entities, inability to distinguish fine-grained differences, degradation with less popular entities
- First 3 experiments:
  1. Compare zero-shot vs 5-shot performance using same model and prompt template
  2. Test different prompt templates (QA vs LC) with same model and examples
  3. Evaluate naive baseline (fastTextword) vs relation-aware baseline (fastTextpair) to establish minimum performance bar

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on modeling graded relations between named entities in domain-specific contexts beyond the five general relations studied?
- Basis in paper: [inferred] The paper evaluates five common graded relations (competitor/rival, friend/ally, influenced by, known for, similar to) and suggests these are among the most prominent, implying other domain-specific relations could be studied.
- Why unresolved: The study focuses on general relations to test broad capabilities. Domain-specific relations may require different modeling approaches or have different performance characteristics.
- What evidence would resolve it: A benchmark testing LLMs on domain-specific graded relations (e.g., in medicine, law, or finance) with systematic comparison to general relations.

### Open Question 2
- Question: What is the upper bound performance for modeling graded relations between named entities using current LLM architectures, and how far are we from it?
- Basis in paper: [explicit] The best models achieve 0.5-0.6 Spearman correlation, still lagging human performance by over 20 percentage points, suggesting a significant gap remains.
- Why unresolved: While the paper shows scaling helps, it's unclear if current architectures can close the gap or if new approaches are needed.
- What evidence would resolve it: Systematic scaling experiments testing current architectures to their practical limits, compared to theoretical bounds from human agreement data.

### Open Question 3
- Question: How do instruction-tuned models compare to standard models for few-shot learning of graded relations when using different prompt templates?
- Basis in paper: [explicit] The paper notes that Flan models perform well with instruction-like prompts, while the best overall results use a different list completion template, suggesting a complex interaction.
- Why unresolved: The study uses optimal templates for each model type, but doesn't systematically compare instruction-tuned vs standard models across template types.
- What evidence would resolve it: Controlled experiments comparing instruction-tuned and standard models using multiple prompt templates in few-shot settings.

## Limitations
- Current models significantly underperform human agreement levels by over 20 percentage points
- Performance degrades for less popular entities where knowledge may be sparse in pretraining data
- The benchmark focuses on five general relations, limiting generalizability to domain-specific contexts

## Confidence
- **High confidence**: The correlation between model size and performance is well-established across multiple model families (T5, Flan-T5, OPT, GPT-3, GPT-4)
- **Medium confidence**: The superiority of perplexity-based LM scoring over static embeddings, though based on limited comparison points
- **Low confidence**: The effectiveness of few-shot prompting, as results vary significantly by model family and prompt template

## Next Checks
1. Conduct ablation studies on prompt template variations (QA vs LC format) across all model families to identify optimal formatting
2. Test model performance on relations with varying degrees of subjectivity (e.g., "similar to" vs "influenced by") to understand where performance degrades
3. Evaluate whether incorporating entity popularity metrics into scoring improves performance on less well-known entities