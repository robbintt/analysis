---
ver: rpa2
title: Longitudinal Self-supervised Learning Using Neural Ordinary Differential Equation
arxiv_id: '2310.10431'
source_url: https://arxiv.org/abs/2310.10431
tags:
- lssl
- node
- longitudinal
- learning
- progression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates the effectiveness of longitudinal self-supervised
  learning (LSSL) using neural ordinary differential equations (NODE) for modeling
  disease progression in medical imaging. The research explores two novel approaches:
  Siamese-like LSSL and NODE-based LSSL.'
---

# Longitudinal Self-supervised Learning Using Neural Ordinary Differential Equation

## Quick Facts
- arXiv ID: 2310.10431
- Source URL: https://arxiv.org/abs/2310.10431
- Reference count: 25
- Key outcome: Siamese LSSL with NODE achieves best performance for age regression, while LSSL-NODE excels at predicting DR development

## Executive Summary
This study investigates longitudinal self-supervised learning (LSSL) using neural ordinary differential equations (NODE) for modeling disease progression in diabetic retinopathy (DR). The researchers propose two novel approaches: Siamese-like LSSL that eliminates the reconstruction term, and NODE-based LSSL that uses NODE to generate latent representations of subsequent scans without requiring image pairs. Using the OPHDIAT dataset, they demonstrate that LSSL without reconstruction and NODE integration can effectively learn disease progression dynamics, achieving strong performance on age regression and DR prediction tasks.

## Method Summary
The method combines autoencoder-based LSSL with cosine alignment between latent trajectory vectors and learned progression directions, extended with NODE to model continuous-time disease dynamics. The Siamese-like variant removes the reconstruction term to focus on temporal alignment, while the NODE variant uses torchdiffeq's dopri5 solver to interpolate latent representations between time points. The model is pre-trained on the OPHDIAT dataset and evaluated via fine-tuning on downstream tasks: age regression (MSE) and predicting DR development for the next visit (AUC).

## Key Results
- Siamese LSSL without reconstruction term achieves best performance for age regression
- LSSL-NODE variant excels at predicting DR development for next visit (AUC scores)
- NODE integration enables learning continuous-time disease progression dynamics without requiring image pairs
- Siamese approach shows robustness to unregistered image pairs compared to standard LSSL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Siamese-like LSSL without reconstruction term still captures disease progression effectively
- Mechanism: Removing reconstruction forces model to rely solely on cosine alignment between latent trajectory vectors and learned progression direction, reducing overfitting to static anatomy and focusing on temporal dynamics
- Core assumption: Cosine alignment term alone is sufficient to encode meaningful disease progression information
- Evidence anchors: Section showing Siamese LSSL performs comparably to standard LSSL; internal results support cosine alignment sufficiency
- Break condition: If cosine alignment cannot distinguish known progression factors (pregnancy, diabetes type)

### Mechanism 2
- Claim: NODE extension enhances LSSL by learning continuous-time dynamics of disease progression
- Mechanism: NODEs parameterize instantaneous rate of change of latent representations over time, allowing interpolation of disease states between observed time points and generation of latent representations for future visits without image pairs
- Core assumption: Disease progression dynamics can be well-approximated by ordinary differential equations
- Evidence anchors: Section demonstrating NODE-based approach generates predicted latent representations for next visits; internal experiments show performance gains
- Break condition: If NODE cannot outperform standard LSSL on downstream tasks

### Mechanism 3
- Claim: Siamese-like architecture with NODE performs better when image pairs are not perfectly registered
- Mechanism: Siamese architecture avoids reconstruction loss sensitive to registration errors, while NODE learns progression dynamics from single images, making it more robust to spatial misalignment
- Core assumption: Registration errors significantly impact reconstruction-based methods but less so for alignment-based methods
- Evidence anchors: Section noting experiments used unregistered images; suggestion that better pairing with registration could enhance results
- Break condition: If perfect registration is available or downstream task performance doesn't show Siamese advantage in unregistered scenarios

## Foundational Learning

- Concept: Longitudinal self-supervised learning (LSSL)
  - Why needed here: Enables learning disease progression patterns without manual annotations by leveraging temporal information in medical imaging data
  - Quick check question: How does LSSL differ from standard self-supervised learning approaches in handling temporal data?

- Concept: Neural Ordinary Differential Equations (NODE)
  - Why needed here: NODEs provide framework for modeling continuous-time dynamics, essential for capturing disease progression that occurs continuously rather than at discrete time points
  - Quick check question: What advantage does NODE have over discrete-time RNNs for modeling irregularly sampled longitudinal data?

- Concept: Siamese network architecture
  - Why needed here: Siamese networks allow comparing representations without reconstruction, beneficial when dealing with unregistered image pairs in longitudinal analysis
  - Quick check question: How does Siamese architecture handle temporal progression differently than autoencoder-based approach?

## Architecture Onboarding

- Component map: Image → Encoder → (NODE) → Direction alignment → (Decoder) → Loss
- Critical path: Image → Encoder → (NODE) → Direction alignment → (Decoder) → Loss
- Design tradeoffs:
  - Reconstruction term vs. Siamese approach: Reconstruction helps anatomical consistency but sensitive to registration; Siamese more robust but loses anatomical reconstruction
  - NODE complexity vs. performance: NODEs add computational overhead but enable continuous-time modeling and better handling of irregular time intervals
  - Latent space dimensionality: Larger spaces may capture more information but risk overfitting and increased computational cost
- Failure signatures:
  - Loss plateau at high values: Likely indicates learning rate issues or poor initialization
  - Direction alignment term not converging to zero: May indicate insufficient capacity or poor pairing strategy
  - NODE solver failures: Could be caused by stiff ODEs or inappropriate solver tolerances
  - Poor downstream task performance: Suggests encoder is not learning meaningful disease progression features
- First 3 experiments:
  1. Baseline autoencoder training without LSSL terms to establish reconstruction capability
  2. Standard LSSL with reconstruction and cosine alignment to validate core methodology
  3. Siamese LSSL without reconstruction to test hypothesis about cosine term sufficiency

## Open Questions the Paper Calls Out

- Question: How does the choice of latent space dimension and architecture affect performance of LSSL and NODE-based LSSL models?
- Basis in paper: [inferred] Uses specific encoder/decoder architecture with 64×4×4 latent size but doesn't explore impact of different dimensions or architectures
- Why unresolved: Paper doesn't provide ablation studies or comparisons with different latent space dimensions or architectures
- What evidence would resolve it: Experiments comparing performance with different latent space dimensions and architectures

- Question: How does the choice of ODE solver and its hyperparameters affect performance of NODE-based LSSL models?
- Basis in paper: [explicit] Mentions using dopri5 solver with adaptive step size but doesn't explore impact of different solvers or hyperparameters
- Why unresolved: Paper doesn't provide comparisons or ablation studies with different ODE solvers or their hyperparameters
- What evidence would resolve it: Experiments comparing performance with different ODE solvers and their hyperparameters

- Question: How does the inclusion of registration methods affect performance of LSSL and NODE-based LSSL models?
- Basis in paper: [inferred] Mentions not applying registration and suggests better pairing with registration could enhance results
- Why unresolved: Paper doesn't provide experiments or comparisons with registered images
- What evidence would resolve it: Experiments comparing performance with and without registration methods

## Limitations

- Experimental setup uses randomly selected single images per examination without registration, which may not represent typical clinical workflows
- NODE component shows performance gains but the underlying mechanism for why continuous-time modeling specifically helps remains incompletely explained
- Comparison between LSSL variants is limited to specific downstream tasks, and generalizability to other diseases or imaging modalities is unclear

## Confidence

- High confidence in the basic LSSL framework effectiveness for longitudinal tasks
- Medium confidence in the specific advantages of NODE integration
- Low confidence in the mechanism explaining why Siamese architecture performs better with unregistered images

## Next Checks

1. Test the Siamese LSSL approach on a registered dataset to determine if performance gains persist when registration is perfect
2. Evaluate the NODE-based approach on irregularly sampled longitudinal data to confirm its advantage for continuous-time modeling
3. Perform ablation studies varying the latent space dimensionality to understand the impact on disease progression capture