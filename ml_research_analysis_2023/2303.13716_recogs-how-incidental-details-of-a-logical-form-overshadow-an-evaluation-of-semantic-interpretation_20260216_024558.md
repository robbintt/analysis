---
ver: rpa2
title: 'ReCOGS: How Incidental Details of a Logical Form Overshadow an Evaluation
  of Semantic Interpretation'
arxiv_id: '2303.13716'
source_url: https://arxiv.org/abs/2303.13716
tags:
- cogs
- generalization
- these
- modi
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies flaws in the COGS benchmark's logical form
  representations that hinder model performance. Specifically, redundant tokens and
  variable naming conventions that tie variables to specific token positions artificially
  inflate difficulty.
---

# ReCOGS: How Incidental Details of a Logical Form Overshadow an Evaluation of Semantic Interpretation

## Quick Facts
- arXiv ID: 2303.13716
- Source URL: https://arxiv.org/abs/2303.13716
- Reference count: 12
- Primary result: Removing incidental details from COGS logical forms significantly improves compositional generalization performance

## Executive Summary
The paper identifies fundamental issues with the COGS benchmark's logical form representations that artificially inflate difficulty for semantic parsing models. Specifically, redundant tokens and variable naming conventions that tie variables to specific token positions create distributional biases that obscure models' true compositional generalization capabilities. By making meaning-preserving modifications to the logical forms—removing redundant tokens, decoupling variables from token positions, and introducing data augmentation—the authors demonstrate substantial performance improvements. The paper introduces ReCOGS, a revised version of COGS that provides a more faithful evaluation of compositional generalization by eliminating these incidental difficulties while maintaining the benchmark's core semantic challenges.

## Method Summary
The authors modify the COGS benchmark's logical form representations through three key changes: removing redundant tokens (particularly frequent but semantically meaningless bigrams like "x _"), replacing positional indices in variable names with random integers while maintaining coreference relationships, and augmenting training data with concatenated examples to expose models to longer sequences. They train encoder-decoder models (both LSTM and Transformer architectures) from scratch on both the original COGS and the modified ReCOGS benchmarks, evaluating performance using Semantic Exact Match accuracy on lexical and structural generalization splits.

## Key Results
- Removing redundant tokens from logical forms reduces distributional bias in decoder predictions
- Decoupling variable names from token positions eliminates artificial constraints on semantic interpretation
- Data augmentation with concatenated examples enables better handling of recursive structures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing redundant tokens from logical forms reduces distributional bias in decoder predictions.
- Mechanism: By eliminating frequent but semantically meaningless bigrams like "x _", the model encounters a more balanced conditional probability distribution during decoding, improving generalization.
- Core assumption: The model's decoder relies on token-level probabilities that are skewed by redundant structural artifacts.
- Evidence anchors:
  - [section] "We hypothesize that the improvements that come from removing these redundant tokens derive from simple considerations of how sequence models operate. As shown in Figure 2, the bigram, x appears 44,846 times, whereas the bigram , Emma appears 4,279 times... Thus, the conditional bigram probability P(x | ,) is significantly larger than P(Emma| ,)."
  - [corpus] Weak - no direct citations, only mentions average neighbor FMR=0.315 and citations=0.0
- Break condition: If the decoder uses a different architecture (e.g., non-token-based) that doesn't rely on bigram probabilities, or if the redundancy removal changes semantic structure.

### Mechanism 2
- Claim: Decoupling variable names from token positions removes artificial constraints on semantic interpretation.
- Mechanism: By replacing positional indices with random integers while maintaining coreference, models no longer associate specific variables with fixed positions, allowing generalization across unseen combinations.
- Core assumption: COGS' current variable naming scheme artificially constrains model learning by tying semantic entities to specific positions.
- Evidence anchors:
  - [abstract] "COGS poses generalization splits that appear impossible for present-day models, which could be taken as an indictment of those models. However, we show that the negative results trace to incidental features of COGS LFs."
  - [section] "For models that rely on embeddings, these new variables will have random representations at test time. However, there is nothing privileged about this particular variable naming scheme; as we discussed earlier, bound variables can be freely renamed as long as this does not change any binding relationships."
  - [corpus] Weak - no direct citations, only mentions average neighbor FMR=0.315 and citations=0.0
- Break condition: If the model architecture doesn't use learned embeddings for variables, or if the semantic relationships require position-specific information.

### Mechanism 3
- Claim: Data augmentation with concatenated examples exposes models to longer sequences without introducing new semantic complexity.
- Mechanism: By combining existing examples and reindexing, models see longer sequences during training that contain the positional indices they'll encounter at test time, addressing length generalization without changing semantic content.
- Core assumption: The failure on recursive structures is primarily due to unseen sequence lengths rather than inability to handle recursion itself.
- Evidence anchors:
  - [section] "This is perfectly well-posed as a generaliza- tion task simultaneously assessing models on both longer lengths and deeper recursion. However, COGS binds these two tasks together in a way that takes us outside of the goal of assessing semantic generalization."
  - [section] "When the sequence length issue is addressed, the models appear to be very capable at handling novel recursion depths. Indeed, our models now far surpass published state-of-the-art results on these tasks."
  - [corpus] Weak - no direct citations, only mentions average neighbor FMR=0.315 and citations=0.0
- Break condition: If the model's attention mechanism has fixed context windows that cannot scale, or if the recursive structure itself requires additional training signals beyond sequence length.

## Foundational Learning

- Concept: Semantic equivalence vs syntactic form
  - Why needed here: Understanding that different logical form representations can express the same meaning is crucial for identifying why COGS' specific representation hinders performance
  - Quick check question: Can you explain why "boy ( x _ 3 )" and "boy ( 3 )" represent the same semantic content in COGS?

- Concept: Variable binding and free variables
  - Why needed here: COGS uses bound variables interpreted as existentially closed, which is different from free variable representations and affects how models should handle them
  - Quick check question: What is the difference between how COGS treats "zebra ( x _ 1 )" versus a free variable representation of the same concept?

- Concept: Compositionality in semantic parsing
  - Why needed here: The paper's core argument is about whether models can learn compositional rules for building meanings, which requires understanding how parts combine to form wholes
  - Quick check question: How does the principle of compositionality relate to a model's ability to generalize to novel sentence structures?

## Architecture Onboarding

- Component map: Transformer-based encoder-decoder architecture with learned positional embeddings -> vocabulary mapping for logical forms -> data augmentation pipeline for sequence concatenation -> variable renaming system -> Semantic Exact Match evaluation
- Critical path: Token removal -> Variable renaming -> Data augmentation -> Model training -> Semantic exact match evaluation
- Design tradeoffs: Simpler logical forms improve performance but may reduce semantic specificity; random variable indices improve generalization but make debugging harder; data augmentation increases training time but provides essential exposure
- Failure signatures: Poor performance on structural splits indicates variable position issues; failure on lexical splits suggests token removal problems; catastrophic forgetting of learned patterns suggests augmentation issues
- First 3 experiments:
  1. Remove redundant tokens from a small COGS subset and measure performance change
  2. Replace positional indices with random integers in training data and test on original splits
  3. Create augmented examples by concatenating 2-3 existing examples and evaluate structural generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of logical form representation impact compositional generalization performance across different model architectures?
- Basis in paper: [explicit] The paper shows that removing redundant tokens and modifying variable naming conventions leads to significant performance improvements across different model architectures (LSTMs and Transformers).
- Why unresolved: While the paper demonstrates that representation changes can improve performance, it does not fully explore the interaction between different representation choices and model architectures. The optimal representation may vary depending on the model type.
- What evidence would resolve it: Systematic experiments comparing multiple representation schemes across a wider range of model architectures (including more recent ones) would help identify which representation choices are most beneficial for different model types.

### Open Question 2
- Question: What is the optimal balance between data augmentation for length generalization and preserving the semantic generalization challenge in compositional benchmarks?
- Basis in paper: [explicit] The paper shows that data augmentation by concatenating examples improves performance on structural generalization tasks, but raises questions about whether this diminishes the semantic generalization challenge.
- Why unresolved: While the paper demonstrates that length generalization can overshadow structural generalization, it does not provide clear guidance on how much data augmentation is appropriate without undermining the core semantic generalization task.
- What evidence would resolve it: Comparative studies varying the amount and type of data augmentation while measuring both length and structural generalization performance would help establish optimal augmentation strategies.

### Open Question 3
- Question: How can compositional generalization benchmarks be designed to fairly assess models' ability to generalize to novel grammatical combinations while avoiding artifacts related to representation choices?
- Basis in paper: [explicit] The paper argues that COGS's representation choices (variable naming conventions, redundant tokens) create artifacts that affect model performance, and proposes ReCOGS as an alternative.
- Why unresolved: While ReCOGS addresses some representation issues, the paper acknowledges that any particular choice of logical forms will be somewhat arbitrary relative to the goals of compositional generalization assessment. The question of how to design truly fair benchmarks remains open.
- What evidence would resolve it: Development and evaluation of multiple benchmark variants with different representation schemes, coupled with theoretical analysis of what constitutes a "fair" generalization task, would help establish better design principles.

## Limitations
- The evidence relies heavily on internal analysis without strong external validation
- Performance improvements may conflate representation quality with task formulation changes
- Cross-comparison with previous work becomes challenging due to significant benchmark modifications

## Confidence

- High confidence: The observation that COGS logical forms contain incidental features (redundant tokens, position-tied variables) that don't contribute to semantic meaning
- Medium confidence: That removing these features improves model performance on compositional generalization tasks
- Medium confidence: That the original COGS splits were unfairly difficult due to these incidental features rather than fundamental limitations of current models

## Next Checks

1. **Ablation study on variable naming schemes:** Create multiple variants of ReCOGS with different variable naming strategies (sequential integers, random integers, position-based indices) and test whether random integers specifically drive the performance improvements, or if any consistent renaming scheme would suffice.

2. **Controlled recursion depth experiments:** Design experiments that vary recursion depth independently from sequence length to determine whether models struggle with recursion itself or simply with longer sequences containing unseen structural patterns.

3. **Cross-benchmark generalization test:** Train models on modified ReCOGS logical forms but evaluate on the original COGS splits to determine whether the improvements transfer to the original evaluation framework, helping isolate whether the benefits come from better representations versus different task formulation.