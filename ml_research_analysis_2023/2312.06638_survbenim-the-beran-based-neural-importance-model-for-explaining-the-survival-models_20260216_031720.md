---
ver: rpa2
title: 'SurvBeNIM: The Beran-Based Neural Importance Model for Explaining the Survival
  Models'
arxiv_id: '2312.06638'
source_url: https://arxiv.org/abs/2312.06638
tags:
- survbenim
- survnam
- survival
- functions
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SurvBeNIM is a new method for explaining machine learning survival
  models, which predict survival or cumulative hazard functions. The core idea is
  to extend the Beran estimator by incorporating importance functions into its kernels,
  implemented as neural networks trained in an end-to-end manner.
---

# SurvBeNIM: The Beran-Based Neural Importance Model for Explaining the Survival Models

## Quick Facts
- **arXiv ID**: 2312.06638
- **Source URL**: https://arxiv.org/abs/2312.06638
- **Reference count**: 40
- **Key outcome**: SurvBeNIM outperforms SurvLIME and SurvNAM in explaining machine learning survival models by using neural importance functions in Beran kernels

## Executive Summary
SurvBeNIM is a novel method for explaining machine learning survival models by extending the Beran estimator with neural importance functions. The approach replaces the Cox model assumptions used in existing methods with flexible neural networks that can capture non-linear feature relationships. Two training strategies are proposed: local explanations that train a network for each instance, and global explanations that train once on all data. Experiments demonstrate that SurvBeNIM achieves better accuracy in identifying important features and approximating survival functions compared to baseline methods.

## Method Summary
SurvBeNIM extends the Beran estimator by incorporating neural importance functions into its kernels, implemented as sets of neural networks trained end-to-end. The method trains neural networks to learn importance functions for each feature, which weight the contribution of similar instances in estimating survival functions. Two strategies are proposed: local explanations that train a network for each explained instance, and global explanations that train once on all instances. The method is evaluated on synthetic datasets with predefined Cox model parameters and real datasets (Veteran, GBSG2, WHAS500) using Random Survival Forests as the black-box model.

## Key Results
- SurvBeNIM outperforms SurvLIME and SurvNAM in identifying important features, especially for complex datasets with non-Cox model data structures
- The method achieves better approximation of survival functions as measured by Mean Squared Distance, Kullback-Leibler divergence, and C-index
- Local explanation strategy provides more accurate instance-specific explanations, while global strategy offers computational efficiency with acceptable accuracy trade-offs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SurvBeNIM improves interpretability by replacing linear Cox model approximations with neural importance functions in Beran kernels
- **Mechanism**: The method incorporates importance functions $h_j(x^{(j)})$ into Beran estimator kernels, implemented as neural networks that learn non-linear, feature-wise importance weighting
- **Core assumption**: Neural importance functions can learn accurate representations of feature relevance without requiring Cox model assumptions
- **Evidence anchors**:
  - [abstract] "The main idea behind SurvBeNIM is to extend the Beran estimator by incorporating the importance functions into its kernels and by implementing these importance functions as a set of neural networks which are jointly trained in an end-to-end manner."
  - [section] "SurvBeNIM aims to find the importance functions of every feature, which can be viewed as functions whose values show how the corresponding features impact on a prediction of the black-box model."
- **Break condition**: If neural networks cannot learn meaningful importance functions, or if Beran estimator kernel structure is insufficient for data complexity

### Mechanism 2
- **Claim**: Two training strategies enable both local and global explanations without retraining for each instance
- **Mechanism**: First strategy trains a neural network for each explained instance using locally generated samples. Second strategy trains once on all training instances plus generated samples
- **Core assumption**: Global training strategy captures sufficient variability to explain any new instance without degradation in local accuracy
- **Evidence anchors**:
  - [abstract] "Two strategies of using and training the whole neural network implementing SurvBeNIM are proposed. The first one explains a single instance, and the neural network is trained for each explained instance. According to the second strategy, the neural network only learns once on all instances from the dataset and on all generated instances."
- **Break condition**: Global model fails to capture local nuances, leading to poor explanation quality for specific instances

### Mechanism 3
- **Claim**: Importance functions provide more accurate feature importance measures than shape functions in SurvNAM for non-Cox model data structures
- **Mechanism**: Importance functions $h_j(x^{(j)})$ directly weight feature distances in Beran kernels, while SurvNAM's shape functions model additive relationships
- **Core assumption**: Data structure violates Cox model assumptions, making direct importance weighting superior to additive shape modeling
- **Evidence anchors**:
  - [section] "In contrast to SurvNAM, the proposed SurvBeNIM does not consider the shape function. SurvBeNIM aims to find the importance functions of every feature, which can be viewed as functions whose values show how the corresponding features impact on a prediction of the black-box model."
- **Break condition**: If data actually follows Cox model assumptions, SurvNAM's shape functions may perform as well or better

## Foundational Learning

- **Concept**: Beran estimator and its kernel structure
  - Why needed here: SurvBeNIM extends the Beran estimator by incorporating neural importance functions into its kernels
  - Quick check question: How does the Beran estimator generalize the Kaplan-Meier estimator through kernel weighting?

- **Concept**: Neural additive models (NAM) and generalized additive models (GAM)
  - Why needed here: SurvNAM uses shape functions from NAM/GAM, which SurvBeNIM deliberately avoids in favor of importance functions
  - Quick check question: What is the key difference between shape functions in GAM and importance functions in SurvBeNIM?

- **Concept**: Survival analysis fundamentals (survival functions, cumulative hazard functions, censoring)
  - Why needed here: The method explains survival model predictions in terms of survival or cumulative hazard functions
  - Quick check question: How is the survival function related to the cumulative hazard function mathematically?

## Architecture Onboarding

- **Component map**: Feature → Neural importance function → Kernel weight computation → Beran estimator → Survival function → Loss computation → Backpropagation to update neural weights

- **Critical path**: Feature → Neural importance function → Kernel weight computation → Beran estimator → Survival function → Loss computation → Backpropagation to update neural weights

- **Design tradeoffs**:
  - Local vs global explanation strategies: local provides instance-specific accuracy but requires retraining; global is efficient but may lose local nuance
  - Neural network complexity vs training time: more complex networks can capture non-linearities but require more data and computation
  - Kernel parameter sensitivity: choice of kernel bandwidth and neural network architecture significantly impacts performance

- **Failure signatures**:
  - Poor training loss convergence indicates issues with network architecture or learning rate
  - High variance in importance function values suggests instability in training or insufficient regularization
  - Explanations that don't align with known feature relationships indicate model misspecification

- **First 3 experiments**:
  1. Train SurvBeNIM on synthetic Cox model data with 2 clusters to verify it can distinguish important features between clusters
  2. Compare local vs global explanation strategies on a small dataset to measure accuracy trade-offs
  3. Test importance function sensitivity to kernel bandwidth parameter by varying it systematically

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SurvBeNIM perform compared to other methods when the underlying data structure significantly deviates from the Cox model assumptions, such as in cases of highly non-linear relationships between features?
- Basis in paper: [explicit] The paper discusses experiments where the Cox model assumptions are violated, such as using non-linear functions (e.g., x2 + max(0, x2) + |x3|) instead of linear combinations of features, and shows SurvBeNIM outperforming other methods like SurvNAM in these scenarios
- Why unresolved: While the paper provides evidence of SurvBeNIM's superiority in specific cases, a more comprehensive analysis across a wider range of non-linear relationships and data structures would further solidify its advantages
- What evidence would resolve it: Additional experiments with diverse non-linear data structures and a systematic comparison of SurvBeNIM's performance against other methods under these conditions

### Open Question 2
- Question: Can the computational complexity of SurvBeNIM be reduced while maintaining or improving its explanation accuracy?
- Basis in paper: [explicit] The paper acknowledges the large computational difficulty of SurvBeNIM due to the numerous hyperparameters of the subnetworks and the need to generate many points around the explained instance
- Why unresolved: The paper identifies the computational challenge but does not propose specific solutions to mitigate it
- What evidence would resolve it: Research into optimization techniques, such as pruning or quantization of the neural networks, or developing more efficient sampling strategies for generating instances around the explained point

### Open Question 3
- Question: How sensitive is SurvBeNIM to the choice of kernel function and its parameters in the Beran estimator?
- Basis in paper: [explicit] The paper mentions using Gaussian kernels for the Beran estimator but suggests that exploring different types of kernels and adding training parameters to them could provide more flexible and outperforming results
- Why unresolved: The paper does not provide a detailed analysis of how different kernel choices and their parameters affect SurvBeNIM's performance
- What evidence would resolve it: Experiments comparing SurvBeNIM's performance using various kernel functions (e.g., Epanechnikov, Tricube) and different parameter settings, along with a sensitivity analysis to identify the most influential parameters

## Limitations
- Neural network architecture details and training hyperparameters are not fully specified, creating uncertainty in reproduction
- Global explanation strategy's effectiveness across diverse data distributions remains untested
- Performance on highly censored datasets or non-stationary hazard functions is not evaluated

## Confidence

- **High confidence**: The core mechanism of incorporating neural importance functions into Beran kernels is well-defined and theoretically sound
- **Medium confidence**: Superiority claims over SurvNAM and SurvLIME are supported by metrics but may depend on specific dataset characteristics
- **Low confidence**: Global explanation strategy's generalizability across diverse data distributions is not fully validated

## Next Checks

1. **Architecture sensitivity analysis**: Systematically vary neural network depth, width, and activation functions to determine their impact on explanation quality and training stability
2. **Censoring rate robustness**: Test SurvBeNIM on datasets with varying censoring rates (10%-90%) to assess performance degradation and identify failure thresholds
3. **Global vs local trade-off quantification**: Measure the accuracy loss when using global explanations instead of local ones across multiple instance types and dataset complexities