---
ver: rpa2
title: 'MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL'
arxiv_id: '2312.11242'
source_url: https://arxiv.org/abs/2312.11242
tags:
- text-to-sql
- bird
- spider
- framework
- mac-sql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MAC-SQL, a multi-agent collaborative framework
  for Text-to-SQL tasks. The framework addresses challenges in handling large databases,
  complex user questions, and erroneous SQL results by employing three agents: Selector,
  Decomposer, and Refiner.'
---

# MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL

## Quick Facts
- arXiv ID: 2312.11242
- Source URL: https://arxiv.org/abs/2312.11242
- Reference count: 4
- Key outcome: MAC-SQL achieves state-of-the-art execution accuracy of 59.59% on BIRD and 86.75% on Spider datasets.

## Executive Summary
MAC-SQL introduces a multi-agent collaborative framework for Text-to-SQL tasks that addresses challenges in handling large databases, complex user questions, and erroneous SQL results. The framework employs three specialized agents: Selector, Decomposer, and Refiner, working in sequence to improve SQL generation accuracy. The Selector simplifies database schemas, the Decomposer breaks down complex questions using chain-of-thought reasoning, and the Refiner validates and corrects SQL queries based on execution feedback. Experiments demonstrate state-of-the-art performance on BIRD and Spider datasets, with the authors also open-sourcing SQL-Llama, an instruction-tuned model based on Code Llama 7B.

## Method Summary
MAC-SQL is a multi-agent collaborative framework that processes text-to-SQL tasks through three sequential agents. The Selector agent filters and simplifies database schemas to reduce context window size and improve LLM performance. The Decomposer agent employs chain-of-thought reasoning to break down complex queries into simpler sub-problems, generating SQL incrementally. The Refiner agent validates and corrects SQL queries based on execution feedback, enhancing fault tolerance. The framework uses GPT-4 or the SQL-Llama model (fine-tuned on Code Llama 7B) as the backbone LLM. The authors evaluate MAC-SQL on BIRD and Spider datasets, measuring execution accuracy, exact match accuracy, and valid efficiency score.

## Key Results
- Achieved state-of-the-art execution accuracy of 59.59% on BIRD dataset
- Achieved state-of-the-art execution accuracy of 86.75% on Spider dataset
- Open-sourced SQL-Llama, an instruction-tuned model based on Code Llama 7B, along with an agent instruction dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Selector agent reduces context window size, improving LLM performance by focusing on relevant schema elements.
- Mechanism: Filters database schema to retain only pertinent tables and columns based on the user query, minimizing input length and reducing cognitive load.
- Core assumption: LLM performance degrades significantly with large, irrelevant schema information.
- Evidence anchors: Abstract mentions Selector simplifies database schemas; section 2.3 discusses expansive schemas surpassing LLM processing capacity.
- Break condition: Incorrect filtering of necessary tables or columns leads to incomplete or incorrect SQL.

### Mechanism 2
- Claim: The Decomposer agent uses chain-of-thought reasoning to break down complex queries, improving SQL generation accuracy.
- Mechanism: Dynamically assesses query complexity and decomposes into progressively simpler sub-questions, solving each individually and generating SQL incrementally.
- Core assumption: Complex SQL queries are better handled by decomposition into smaller, manageable parts.
- Evidence anchors: Abstract mentions Decomposer breaks down complex questions using chain-of-thought reasoning; section 2.3 discusses CoT and least-to-most prompting methods.
- Break condition: Incorrect or incomplete decomposition results in SQL that doesn't accurately represent user intent.

### Mechanism 3
- Claim: The Refiner agent validates and corrects SQL queries based on execution feedback, enhancing framework fault tolerance.
- Mechanism: Diagnoses generated SQL for syntactic correctness, execution feasibility, and non-empty results, then reasons based on original SQL and error feedback to generate corrections.
- Core assumption: Generated SQL often contains errors that can be detected and corrected through iterative refinement.
- Evidence anchors: Abstract mentions Refiner validates and corrects SQL queries; section 2.4 discusses detecting and automatically rectifying SQL errors.
- Break condition: Refiner cannot resolve errors after maximum attempts, resulting in incorrect SQL being returned.

## Foundational Learning

- Concept: Chain-of-thought (CoT) reasoning
  - Why needed here: Essential for Decomposer agent to break down complex user queries into simpler sub-problems, enabling more accurate SQL generation.
  - Quick check question: How does CoT reasoning improve accuracy of SQL generation in complex queries?

- Concept: Schema linking
  - Why needed here: Crucial for connecting natural language queries to correct tables and columns in database, ensuring generated SQL is semantically accurate.
  - Quick check question: What role does schema linking play in accuracy of text-to-SQL models?

- Concept: Error handling and correction
  - Why needed here: Vital for Refiner agent to validate and fix SQL queries, improving overall framework fault tolerance.
  - Quick check question: How does Refiner agent detect and correct errors in generated SQL?

## Architecture Onboarding

- Component map: User query → Selector → Decomposer → Refiner → Final SQL
- Critical path: User query flows through Selector for schema simplification, then to Decomposer for query breakdown using CoT reasoning, and finally to Refiner for SQL validation and correction
- Design tradeoffs: Multiple agents increase complexity but improve accuracy and fault tolerance; using GPT-4 provides high performance but increases cost and dependency
- Failure signatures: Incorrect schema filtering by Selector leads to missing/incorrect SQL components; poor decomposition by Decomposer results in incomplete/inaccurate SQL; ineffective error correction by Refiner causes persistent SQL errors
- First 3 experiments:
  1. Test Selector's ability to filter relevant schema elements by comparing SQL accuracy with and without schema filtering
  2. Evaluate Decomposer's performance in breaking down complex queries by measuring accuracy of generated SQL for varying query complexities
  3. Assess Refiner's effectiveness in correcting SQL errors by introducing controlled errors and measuring success rate of corrections

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MAC-SQL performance compare to other state-of-the-art methods on larger and more complex datasets beyond BIRD and Spider?
- Basis in paper: Paper achieved state-of-the-art accuracy on BIRD and Spider but doesn't provide information on performance on other datasets
- Why unresolved: Paper focuses on evaluating MAC-SQL on BIRD and Spider datasets, leaving performance on other datasets unanswered
- What evidence would resolve it: Conducting experiments on wider range of datasets with varying complexities and sizes would provide evidence of generalizability and performance

### Open Question 2
- Question: How does SQL-Llama performance compare to other open-source models for Text-to-SQL tasks?
- Basis in paper: Paper mentions SQL-Llama achieved encouraging outcomes on BIRD and Spider development sets but doesn't provide direct comparison with other open-source models
- Why unresolved: While paper highlights SQL-Llama performance, it doesn't provide comprehensive comparison with other open-source models
- What evidence would resolve it: Conducting experiments comparing SQL-Llama with other open-source models on Text-to-SQL tasks would provide evidence of effectiveness and competitiveness

### Open Question 3
- Question: How does MAC-SQL framework handle ambiguous or incomplete user queries?
- Basis in paper: Paper doesn't explicitly address how MAC-SQL handles ambiguous or incomplete user queries
- Why unresolved: While paper focuses on overall performance, it doesn't provide insights into handling specific types of user queries that may be ambiguous or incomplete
- What evidence would resolve it: Conducting experiments evaluating MAC-SQL performance on ambiguous or incomplete user queries would provide evidence of robustness and ability to handle such scenarios

## Limitations
- Limited empirical validation of individual agent contributions - ablation studies don't clearly isolate each component's contribution
- Scalability concerns - relies on GPT-4, raising questions about computational efficiency and cost-effectiveness for production deployment
- Limited generalizability - experiments conducted primarily on BIRD and Spider datasets, which may not represent real-world database complexity

## Confidence
- High Confidence Claims: MAC-SQL achieves state-of-the-art execution accuracy on BIRD (59.59%) and Spider (86.75%) datasets; three-agent architecture provides structured approach to complex text-to-SQL tasks
- Medium Confidence Claims: Selector agent effectively reduces context window size and improves LLM performance; Decomposer agent's chain-of-thought reasoning improves SQL generation accuracy; Refiner agent successfully validates and corrects SQL queries
- Low Confidence Claims: Specific contribution of each agent to overall performance gains; framework's effectiveness on databases significantly larger or more complex than tested datasets; cost-benefit ratio of using multiple agents versus simpler approaches

## Next Checks
1. Conduct ablation study on individual agent contributions by sequentially removing each agent to quantify their specific contributions to overall performance
2. Evaluate MAC-SQL's performance on databases with significantly more tables and columns than those in BIRD and Spider to test framework's limits and validate Selector agent's effectiveness
3. Compare MAC-SQL's performance-to-cost ratio against simpler approaches using a single LLM call to determine whether multi-agent approach provides sufficient value to justify computational overhead