---
ver: rpa2
title: The distribution of discourse relations within and across turns in spontaneous
  conversation
arxiv_id: '2307.03645'
source_url: https://arxiv.org/abs/2307.03645
tags:
- discourse
- relations
- across
- annotation
- annotators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates discourse relations in spontaneous conversation,
  comparing their distribution across different contextual types: within and across
  turns, and within and across speakers. The authors adapt discourse relation annotation
  guidelines from written text to conversational data and have novice annotators label
  discourse relations in Switchboard corpus dialogues.'
---

# The distribution of discourse relations within and across turns in spontaneous conversation

## Quick Facts
- arXiv ID: 2307.03645
- Source URL: https://arxiv.org/abs/2307.03645
- Reference count: 6
- Primary result: Discourse relations vary significantly by conversational context, with single-turn annotations showing the greatest annotator uncertainty.

## Executive Summary
This paper investigates discourse relation annotation in spontaneous conversational data, comparing their distribution across different contextual types: within and across turns, and within and across speakers. The authors adapt discourse relation annotation guidelines from written text to conversational data and have novice annotators label discourse relations in Switchboard corpus dialogues. They find that discourse relations vary significantly by context, with single-turn annotations showing the greatest uncertainty for annotators. A logistic regression model shows that discourse context significantly predicts relation selection, and a BERT-based classifier achieves modest recall but poor precision, suggesting annotator uncertainty particularly for within-turn relations.

## Method Summary
The study adapts SDRT and STAC discourse relation annotation guidelines to conversational data from the Switchboard corpus. Novice annotators were recruited and trained to label discourse relations between Elementary Discourse Units (EDUs) in dialogue turns. The annotation task allowed multiple relations per EDU pair and included confidence ratings. The authors compared discourse relation distributions across four contextual types: within/across turns and speakers. They computed inter-annotator agreement using a multilabel-specific bootstrapping method, trained a logistic regression model to predict annotation patterns based on discourse context, and developed a BERT-based classifier to predict relations from EDU pairs using leave-one-conversation-out cross-validation.

## Key Results
- Discourse relations vary significantly by conversational context, with single-turn annotations creating the most uncertainty for annotators
- Logistic regression shows discourse context variables significantly improve prediction of discourse relation annotations
- BERT-based classifier achieves modest recall (0.76) but poor precision (0.21), with better performance when considering any matching label versus exact match

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discourse relations are harder to annotate within a single turn than across turns due to increased ambiguity and overlap of relation types.
- Mechanism: When multiple relation types could apply to a single-turn EDU pair, annotators experience uncertainty and tend to mark more relations per pair. This is less common in cross-turn or cross-speaker contexts, where discourse cues (e.g., acknowledgments, questions) provide clearer relational signals.
- Core assumption: The distribution of discourse relation types differs systematically by conversational context, with some relations more frequent or clearer in cross-turn settings.
- Evidence anchors:
  - [abstract]: "single-turn annotations creating the most uncertainty for annotators"
  - [section 4]: "Annotators generally selected more discourse relations per EDU pair in the single-turn case... We expect to find significant differences in discourse relation use across different discourse contexts"
- Break condition: If the discourse relation inventory does not align with conversational patterns, or if annotators are unfamiliar with conversational discourse cues, the ambiguity will persist regardless of context.

### Mechanism 2
- Claim: Including discourse context (within/across speakers and turns) in a logistic regression model significantly improves prediction of discourse relation annotations.
- Mechanism: Discourse context acts as a strong feature for predicting which relations annotators are likely to choose, as certain relations (e.g., Acknowledgment, Question-Answer Pair) are more probable in cross-speaker contexts, while others (e.g., Continuation, Explanation) are more likely within-speaker.
- Core assumption: Discourse relations are not uniformly distributed across conversational contexts; context-specific patterns exist.
- Evidence anchors:
  - [abstract]: "different discourse contexts produce distinct distributions of discourse relations"
  - [section 4.2]: "discourse context variables of interest, which significantly improved fit to the annotation data"
- Break condition: If context variables are not properly encoded or if the discourse relation inventory is too coarse, the model may not capture meaningful patterns.

### Mechanism 3
- Claim: A BERT-based classifier can leverage EDU pair embeddings to predict discourse relation annotations, with higher recall for any label than for exact label matching.
- Mechanism: By encoding EDU pairs as next-sentence pairs in BERT, the classifier learns contextual representations that capture implicit discourse signals. The model achieves better recall when evaluated against the set of all labels provided by annotators, indicating it captures uncertainty.
- Evidence anchors:
  - [abstract]: "a BERT-based classifier trained to predict relations from EDU pairs achieves modest recall (0.76) but poor precision (0.21)"
  - [section 4.3]: "Strict annotation-level accuracy to predict each selected label from all annotators was quite poor... recall was substantially higher when considering whether the top guess belonged to the set of all labels provided by annotators"
- Break condition: If the EDU segmentation or annotation guidelines do not align with BERT's pretraining objectives, the classifier may struggle to learn effective representations.

## Foundational Learning

- Concept: Elementary Discourse Units (EDUs) and their segmentation rules in conversational data.
  - Why needed here: EDUs are the basic units for discourse relation annotation; incorrect segmentation can lead to poor annotations and model performance.
  - Quick check question: How would you segment a turn containing a disfluency ("um, I think") or a clarification question ("Saginaw?")?

- Concept: Multilabel annotation and inter-annotator agreement metrics (e.g., Marchal et al., 2022 bootstrapping method).
  - Why needed here: The task allows multiple discourse relations per EDU pair, and agreement is measured using specialized metrics for multilabel data.
  - Quick check question: Why might soft-match and boot-match agreement differ in a multilabel setting?

- Concept: Logistic regression and classifier evaluation (precision, recall, F1) in the context of multilabel classification.
  - Why needed here: Model comparison and classifier evaluation are central to understanding the impact of discourse context and the quality of annotations.
  - Quick check question: What does it mean if recall is high but precision is low for a multilabel classifier?

## Architecture Onboarding

- Component map: Switchboard corpus -> EDU segmentation -> pair generation (within/across speaker/turn) -> annotation interface -> inter-annotator agreement -> logistic regression (context modeling) -> BERT classifier (embedding-based prediction)
- Critical path: EDU segmentation -> pair generation -> annotation -> agreement analysis -> context modeling -> classifier training -> evaluation
- Design tradeoffs:
  - Using novice annotators increases dataset size but may reduce agreement; using experts would increase agreement but limit scale.
  - Multilabel annotation captures uncertainty but complicates evaluation and model training.
  - BERT-based classification leverages contextual embeddings but requires careful handling of multilabel outputs.
- Failure signatures:
  - Low inter-annotator agreement across all contexts may indicate unclear guidelines or an inappropriate relation inventory.
  - High agreement but poor classifier performance may suggest that discourse relations are not easily predictable from EDU embeddings alone.
  - High recall but low precision may indicate annotator uncertainty and overlapping relation labels.
- First 3 experiments:
  1. Train and evaluate a logistic regression model with and without discourse context features to confirm their impact on annotation prediction.
  2. Compare classifier performance using different BERT-based architectures (e.g., with and without fine-tuning) to assess the impact of training strategy.
  3. Analyze the distribution of discourse relations across contexts to identify which relations are most ambiguous and require guideline refinement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do discourse relations vary systematically across different conversational genres or cultural contexts?
- Basis in paper: [inferred] The authors note that discourse relations may be "culturally, linguistically, and situationally specific" and that future work should validate the generality of the discourse relation system used.
- Why unresolved: The current study uses only Switchboard corpus data, which represents one specific type of spontaneous conversation between strangers on given topics. The annotation guidelines were adapted from written text and STAC corpus guidelines, but cultural and genre differences are not explored.
- What evidence would resolve it: Systematic comparison of discourse relation distributions and annotation patterns across multiple conversational corpora representing different genres (e.g., casual conversation, professional meetings, crisis negotiation) and cultural contexts.

### Open Question 2
- Question: What specific linguistic features or contextual cues make discourse relation annotation more or less certain for annotators?
- Basis in paper: [explicit] The authors find that single-turn annotations show the greatest uncertainty and that their classifier achieves poor precision but modest recall, suggesting annotators struggle particularly with within-turn relations.
- Why unresolved: While the paper identifies that uncertainty varies by context type, it doesn't investigate what specific features (e.g., lexical markers, syntactic patterns, speaker turn position) contribute to this uncertainty or how to mitigate it.
- What evidence would resolve it: Detailed analysis of linguistic features in high-uncertainty versus low-uncertainty annotations, and controlled experiments testing whether highlighting or providing additional context reduces annotation uncertainty.

### Open Question 3
- Question: How does the multi-label nature of discourse relation annotation affect both annotator behavior and model performance compared to single-label approaches?
- Basis in paper: [explicit] The authors explicitly allow multiple relations per EDU pair and note this creates analytical challenges, including that their multiclass logistic regression coefficients are not independent and should be interpreted as separate regressions.
- Why unresolved: The paper doesn't compare results to a single-label baseline or investigate how allowing multiple labels changes the cognitive task for annotators or the technical approach for modeling.
- What evidence would resolve it: Comparative study of annotation patterns, inter-annotator agreement, and model performance under both single-label and multi-label annotation schemes using the same data.

## Limitations

- The study relies on novice annotators and the Switchboard corpus, which may not fully capture the diversity of spontaneous conversation
- The observed uncertainty in single-turn annotations and poor classifier precision suggest that either the discourse relation inventory or the annotation guidelines may not be well-suited to conversational data
- The small sample size (19 dialogues) limits the generalizability of the findings

## Confidence

- **High confidence**: Discourse relations vary significantly by conversational context; logistic regression shows discourse context improves prediction of annotations
- **Medium confidence**: BERT-based classifier can leverage EDU embeddings to predict discourse relations, but the modest recall and poor precision indicate annotator uncertainty, especially for within-turn relations
- **Low confidence**: The specific causes of annotator uncertainty (e.g., guideline clarity, relation inventory appropriateness) are not fully explored, and the classifier's performance may be limited by the quality of EDU segmentation

## Next Checks

1. Analyze annotator behavior: Examine the distribution of selected discourse relations per EDU pair across contexts to identify which relations are most ambiguous and require guideline refinement
2. Evaluate EDU segmentation impact: Compare classifier performance using different EDU segmentation strategies (e.g., strict vs. relaxed rules) to assess their impact on discourse relation prediction
3. Test alternative relation inventories: Re-annotate a subset of data using a different discourse relation inventory (e.g., PDTB-style relations) to determine if the observed uncertainty is specific to the SDRT/STAC guidelines