---
ver: rpa2
title: 'Hypothesis Search: Inductive Reasoning with Language Models'
arxiv_id: '2309.05660'
source_url: https://arxiv.org/abs/2309.05660
tags:
- hypotheses
- grid
- programs
- language
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a pipeline for improving inductive reasoning
  in language models by decomposing the task into two stages: generating natural language
  hypotheses about the underlying transformation rules, and implementing these hypotheses
  as concrete Python programs. The pipeline is evaluated on three datasets: ARC, 1D-ARC,
  and SyGuS.'
---

# Hypothesis Search: Inductive Reasoning with Language Models

## Quick Facts
- **arXiv ID**: 2309.05660
- **Source URL**: https://arxiv.org/abs/2309.05660
- **Reference count**: 34
- **Primary result**: 27.5% accuracy on ARC with automated pipeline, 37.5% with human-selected hypotheses

## Executive Summary
This paper proposes a novel pipeline for improving inductive reasoning in language models by decomposing the task into hypothesis generation and program implementation. The approach uses LLMs to first generate natural language hypotheses about transformation rules, then implements these hypotheses as executable Python programs. Tested on ARC, 1D-ARC, and SyGuS datasets, the method achieves 27.5% accuracy on ARC with an automated pipeline and 37.5% with minimal human input to select from generated candidates. The results demonstrate that separating abstract hypothesis generation from concrete program verification can significantly improve LLM performance on inductive reasoning tasks.

## Method Summary
The pipeline works by first prompting an LLM (GPT-4) to generate multiple natural language hypotheses about the underlying transformation rules from input-output examples. To manage computational costs, these hypotheses are then filtered either through LLM summarization or human selection. The filtered hypotheses are implemented as Python programs, which are validated against training examples. If programs fail to pass all training examples, the LLM iteratively refines them using execution feedback. Finally, the best-performing program is selected to predict outputs for novel test inputs.

## Key Results
- Automated pipeline achieves 27.5% accuracy on a random 40-problem subset of ARC
- With human-selected hypotheses, performance increases to 37.5% accuracy
- The approach outperforms direct prompting baselines (12.5% accuracy)
- Both abstract hypothesis generation and concrete program implementation are shown to be beneficial for LLM inductive reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing inductive reasoning into hypothesis generation and program implementation improves performance by separating abstract reasoning from concrete verification.
- **Mechanism**: The pipeline first uses an LLM to generate natural language hypotheses about transformation rules, then implements these hypotheses as executable Python programs that can be verified against training examples.
- **Core assumption**: LLMs can generate useful hypotheses in natural language and can also translate these hypotheses into working Python code.
- **Evidence anchors**:
  - [abstract] "we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs."
  - [section] "Our pipeline thus disentangles inductive reasoning tasks primarily into two capabilities: the ability to propose accurate natural language hypotheses about the underlying rules, and the ability to formalize them as programs."
  - [corpus] FMR score 0.567 for "Improving In-Context Learning with Reasoning Distillation" suggests LLMs benefit from structured reasoning approaches.

### Mechanism 2
- **Claim**: Using execution feedback to iteratively refine program implementations improves accuracy by correcting errors through concrete validation.
- **Mechanism**: After generating programs from hypotheses, the system executes them on training examples and provides feedback to the LLM, which then revises the programs based on error messages and desired outputs.
- **Core assumption**: LLMs can understand error messages and desired outputs well enough to revise programs effectively.
- **Evidence anchors**:
  - [abstract] "These programs can be directly verified by running on the observed examples and generalized to novel inputs."
  - [section] "If we still cannot achieve a program that passes all the training examples after a preset number of feedback rounds, we select the program that passes most examples for generating the prediction."
  - [corpus] FMR score 0.567 for "Improving In-Context Learning with Reasoning Distillation" indicates iterative refinement helps with reasoning tasks.

### Mechanism 3
- **Claim**: Reducing the hypothesis search space through summarization or human selection improves efficiency without sacrificing accuracy.
- **Mechanism**: The system either uses an LLM to summarize multiple hypotheses into a smaller set or asks human annotators to select correct hypotheses from the generated candidates.
- **Core assumption**: The correct hypothesis will survive the filtering process and be included in the reduced set.
- **Evidence anchors**:
  - [abstract] "Because of the prohibitive cost of generation with state-of-the-art LLMs, we consider a middle step to filter the set of hypotheses that will be implemented into programs: we either ask the LLM to summarize into a smaller set of hypotheses, or ask human annotators to select a subset of the hypotheses."
  - [section] "To reduce the number of hypotheses that must be considered, we either ask the LLM to summarize them into a smaller set of hypotheses or ask human annotators to select a subset."
  - [corpus] FMR score 0.525 for "Hypothesis Generation via LLM-Automated Language Bias for ILP" suggests filtering approaches can be effective.

## Foundational Learning

- **Concept: Program synthesis from examples**
  - Why needed here: The system must generate Python programs that implement transformation rules from input-output examples.
  - Quick check question: Can you explain how to synthesize a program that transforms a grid by rotating it 90 degrees clockwise?

- **Concept: Natural language hypothesis generation**
  - Why needed here: The system relies on LLMs to generate natural language descriptions of transformation rules.
  - Quick check question: How would you describe in natural language the rule that "transforms each input grid by rotating it 90 degrees clockwise"?

- **Concept: Execution feedback and program debugging**
  - Why needed here: The system uses execution results to provide feedback for program refinement.
  - Quick check question: Given a program that fails on a specific test case, how would you modify it to pass that test?

## Architecture Onboarding

- **Component map**: Hypothesis Generator -> Hypothesis Filter -> Program Generator -> Program Executor -> Hypothesis Selection
- **Critical path**: Hypothesis generation → Hypothesis filtering → Program generation → Program execution → Program refinement (if needed) → Program selection → Test input prediction
- **Design tradeoffs**:
  - Cost vs. accuracy: Generating more hypotheses and programs improves accuracy but increases cost
  - Automation vs. human involvement: Human filtering provides better results but reduces automation
  - Abstraction vs. specificity: More abstract hypotheses are easier to generate but harder to implement precisely
- **Failure signatures**:
  - Low accuracy: Likely due to incorrect hypotheses or inability to implement them as programs
  - High cost: May indicate need for better hypothesis filtering or more efficient program generation
  - Inconsistent results: Could suggest instability in hypothesis generation or program implementation
- **First 3 experiments**:
  1. Test the hypothesis generation component alone by providing it with known ARC problems and evaluating the quality of generated hypotheses
  2. Test the program generation component by providing it with correct hypotheses and evaluating whether it can implement them as working Python programs
  3. Test the full pipeline on a small subset of ARC problems with human filtering to establish a performance baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can vision-language models close the gap in hypothesis generation for visual tasks like ARC?
- **Basis in paper**: [explicit] The paper states "we thus tentatively conclude that current LMs are quite capable of hypothesis generation for inductive learning and anticipate that vision-language models (Driess et al., 2023) may close the remaining gap for visual tasks like ARC."
- **Why unresolved**: The paper only mentions vision-language models as a future direction but does not test them on ARC or other visual tasks.
- **What evidence would resolve it**: Running experiments with vision-language models on ARC and comparing their hypothesis generation accuracy to text-only models like GPT-4.

### Open Question 2
- **Question**: How does the performance of the pipeline change when considering every candidate hypothesis instead of filtering?
- **Basis in paper**: [explicit] The paper notes "Currently, our pipeline does not consider many candidate hypotheses; we note that this is not a theoretical limitation of our method." and "Therefore, the performance of human-selected hypotheses can reasonably be treated as a lower bound for the performance if we consider every candidate hypothesis."
- **Why unresolved**: The paper filters hypotheses to reduce computational cost but does not evaluate the performance without filtering due to cost constraints.
- **What evidence would resolve it**: Running experiments with the pipeline on a subset of tasks without filtering hypotheses and comparing the accuracy to the filtered version.

### Open Question 3
- **Question**: What is the impact of using alternative geometric representations of ARC grids on hypothesis generation?
- **Basis in paper**: [explicit] The paper states "We observed that many for many tasks in ARC, it is easy for LLMs to come up with reasonable hypotheses if the grids are parsed into a useful geometric representation" but found that implementing an algorithm to represent grids as shape placements "harmed performance on more of them."
- **Why unresolved**: The paper only tried one alternative representation (shape placements) and found it did not improve performance, but did not explore other representations.
- **What evidence would resolve it**: Experimenting with different geometric representations of ARC grids (e.g. graphs, vectors) and evaluating their impact on hypothesis generation accuracy compared to the original text representation.

## Limitations

- Performance gains are based on a very small evaluation set (40 random ARC problems), limiting generalizability.
- Human filtering approach, while effective, doesn't scale to real-world applications.
- The paper doesn't analyze whether filtering processes correctly preserve the correct hypothesis or how often filtering removes the right answer.

## Confidence

- **Medium confidence**: The decomposition mechanism (hypothesis generation + program implementation) is well-specified and theoretically sound, but the empirical evidence is limited to a small subset of problems.
- **Medium confidence**: The execution feedback mechanism for program refinement is described but not extensively evaluated for effectiveness.
- **Low confidence**: The hypothesis filtering approach (LLM summarization vs human selection) lacks comparative analysis of when each succeeds or fails.

## Next Checks

1. Evaluate the full pipeline on a larger, more diverse set of ARC problems (ideally the complete test set) to establish robust performance estimates and identify failure patterns.
2. Test the hypothesis filtering mechanism by measuring what fraction of correct hypotheses survive LLM summarization versus human selection across different problem types.
3. Analyze failure cases where programs pass training examples but fail on test inputs to understand generalization gaps and whether this stems from hypothesis errors or implementation issues.