---
ver: rpa2
title: 'randomHAR: Improving Ensemble Deep Learners for Human Activity Recognition
  with Sensor Selection and Reinforcement Learning'
arxiv_id: '2307.07770'
source_url: https://arxiv.org/abs/2307.07770
tags:
- learning
- activity
- selection
- deep
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of improving Human Activity Recognition
  (HAR) performance by optimizing ensemble methods rather than model architectures.
  The core method, randomHAR, trains multiple deep learning models on randomly selected
  subsets of sensor data and uses reinforcement learning to select the optimal model
  combination for final prediction.
---

# randomHAR: Improving Ensemble Deep Learners for Human Activity Recognition with Sensor Selection and Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.07770
- Source URL: https://arxiv.org/abs/2307.07770
- Reference count: 29
- Key outcome: randomHAR significantly outperforms ensembleLSTM on six benchmark HAR datasets, achieving higher F1-scores across all tested datasets.

## Executive Summary
This paper addresses Human Activity Recognition (HAR) by proposing randomHAR, an ensemble approach that trains multiple deep learning models on randomly selected sensor subsets and uses reinforcement learning to select the optimal model combination. The method aims to improve HAR performance by addressing challenges like noisy data, intra-class variability, and inter-class similarity through sensor randomization and intelligent model selection. Experimental results demonstrate significant improvements over state-of-the-art ensemble methods across six benchmark datasets.

## Method Summary
The randomHAR framework trains multiple deep learning models with identical architectures on randomly selected subsets of sensor data, then uses a reinforcement learning agent to select the optimal subset of models for final prediction. The sensor subsets are generated using a Bernoulli distribution, creating diverse base models that focus on different sensor modalities. The RL agent learns to select model combinations that maximize validation performance, implicitly balancing accuracy with model diversity. The final prediction is made through majority voting of the selected models.

## Key Results
- randomHAR achieves higher F1-scores than ensembleLSTM on all six benchmark datasets (DSADS, HAPT, PAMAP2, RWHAR, SKODAR, OPPO)
- The approach demonstrates generality across different base model architectures, including both LSTM and CNN models
- Reinforcement learning-based model selection proves crucial for performance gains, outperforming static selection methods like TopK
- Sensor randomization approach reduces computational intensity while maintaining or improving performance

## Why This Works (Mechanism)

### Mechanism 1
Sensor randomization reduces inter-class similarity and intra-class variability by training models on different subsets of sensors, forcing them to learn complementary features. Randomly selecting sensor subsets creates diverse base models that focus on different sensor modalities, leading to more diverse and complementary predictions when combined. The core assumption is that different sensor subsets capture distinct aspects of the activity signal, and these differences improve ensemble performance when properly combined.

### Mechanism 2
Reinforcement learning-based model selection outperforms static selection methods like TopK by finding model combinations that are both accurate and diverse. The RL agent learns to select model subsets that maximize reward based on validation performance, implicitly balancing accuracy with model diversity. The core assumption is that the RL agent can effectively learn optimal model selection strategies through reward feedback without requiring explicit diversity constraints.

### Mechanism 3
The ensemble approach optimizes the ensemble process rather than individual model architectures, making it applicable across different base model types. By keeping base model architecture fixed and optimizing ensemble composition, the method can be applied to any HAR model that takes sensor signals as input. The core assumption is that the ensemble optimization process is independent of the specific base model architecture used.

## Foundational Learning

- **Reinforcement Learning**
  - Why needed here: RL is used to select the optimal subset of trained models for final prediction, replacing static selection methods
  - Quick check question: What is the key difference between policy gradient methods and value-based RL methods, and which would be more appropriate for this model selection task?

- **Ensemble Methods**
  - Why needed here: The core approach relies on combining multiple models to improve overall performance and robustness
  - Quick check question: How does diversity among ensemble members contribute to improved performance, and what techniques can be used to ensure diversity?

- **Sensor Data Processing**
  - Why needed here: Understanding how different sensors capture different aspects of human activities is crucial for effective sensor selection
  - Quick check question: What are the key differences between accelerometer, gyroscope, and magnetometer data in HAR applications, and how might these differences affect model performance?

## Architecture Onboarding

- **Component map**: Sensor selection module (Bernoulli distribution for random sensor subsets) -> Base model training pipeline (multiple instances trained on different sensor subsets) -> Reinforcement learning agent (selects optimal model combination) -> Aggregation module (majority voting of selected models)

- **Critical path**: 1. Generate sensor subsets using Bernoulli distribution 2. Train base models on each subset 3. Train RL agent to select optimal model combination 4. Use selected models for final prediction via majority voting

- **Design tradeoffs**: Number of sensor subsets vs. computational cost, Complexity of RL agent vs. performance gains, Size of sensor subsets vs. model performance

- **Failure signatures**: All models performing similarly (indicates lack of diversity), RL agent consistently selecting same subset (indicates convergence issues), Performance degradation with more models (indicates overfitting or poor selection)

- **First 3 experiments**: 1. Baseline test: Run with all sensors available to each model (no randomization) 2. Simple ensemble test: Use TopK selection instead of RL agent 3. Single model test: Compare ensemble performance against best individual model

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the reward function in the reinforcement learning model selection process be optimized to accelerate convergence? The paper mentions this as a future research direction: "can we accelerate the convergence by subtracting the average reward of the old agent?" This remains unresolved as the authors did not explore this optimization in their experiments.

- **Open Question 2**: Can meta-features be used to enhance the targeted performance of trained models instead of random sensor selection? The paper suggests this as a future direction: "instead of randomly selecting sensors, can meta-features be utilized to enhance the targeted performance of the trained models?" The current approach uses purely random sensor selection.

- **Open Question 3**: Can the original HAR problem be split into two consecutive sub-problems, combining multiple models trained on biased datasets for final prediction? The paper poses this as a future research question: "can the original HAR problem be split into two consecutive sub-problems, and multiple models trained on biased datasets be combined to make predictions?"

## Limitations
- The approach relies heavily on the effectiveness of random sensor selection and RL-based model combination, with limited ablation studies to isolate the contribution of each component
- The generality across different base model architectures is claimed but only demonstrated with CNN and LSTM models
- Performance sensitivity to the number of sensor subsets and size of each subset is not thoroughly explored

## Confidence
- **High**: The overall methodology is well-defined and the experimental setup (six benchmark datasets, LOSO cross-validation) is rigorous
- **Medium**: The improvement over baseline ensembleLSTM is statistically significant, but the exact contribution of sensor randomization vs. RL model selection is unclear
- **Low**: The claim about generality across any HAR model architecture lacks comprehensive validation beyond the two tested architectures

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of sensor randomization and RL model selection to the overall performance improvement
2. Test the approach with different base model architectures (e.g., Transformer-based models, attention mechanisms) to verify claimed generality
3. Evaluate performance sensitivity to the number of sensor subsets and size of each subset to identify optimal configurations