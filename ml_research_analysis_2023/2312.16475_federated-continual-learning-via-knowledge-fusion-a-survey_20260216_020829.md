---
ver: rpa2
title: 'Federated Continual Learning via Knowledge Fusion: A Survey'
arxiv_id: '2312.16475'
source_url: https://arxiv.org/abs/2312.16475
tags:
- learning
- knowledge
- data
- clients
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenges of federated continual learning
  (FCL), which combines federated learning (FL) and continual learning (CL) to enable
  decentralized model training on streaming data across multiple clients while preserving
  privacy. The authors identify a fundamental problem called "spatial-temporal catastrophic
  forgetting," where models suffer from forgetting both previous tasks (temporal)
  and local knowledge (spatial) due to data heterogeneity and non-IID distributions.
---

# Federated Continual Learning via Knowledge Fusion: A Survey

## Quick Facts
- arXiv ID: 2312.16475
- Source URL: https://arxiv.org/abs/2312.16475
- Authors: [Not specified in source]
- Reference count: 40
- Key outcome: This paper addresses the challenges of federated continual learning (FCL), which combines federated learning (FL) and continual learning (CL) to enable decentralized model training on streaming data across multiple clients while preserving privacy.

## Executive Summary
This paper presents a comprehensive survey of federated continual learning (FCL), identifying the fundamental challenge of "spatial-temporal catastrophic forgetting" where models forget both previous tasks and local knowledge due to data heterogeneity and non-IID distributions. The authors propose two unified frameworks for FCL - synchronous and asynchronous - that can integrate most existing methods and lay pipelines for future research. They categorize existing FCL methods based on knowledge fusion mechanisms including rehearsal, clustering, regularization, parameter isolation, dynamic architecture, prototype, and knowledge distillation. The paper conducts extensive experiments using FedAvg on CIFAR-100 to demonstrate the impact of spatial-temporal catastrophic forgetting on model performance and highlight the need for effective knowledge fusion algorithms.

## Method Summary
The paper proposes two unified frameworks for federated continual learning: synchronous FCL where all clients share the same task sequence and focus on aggregation, and asynchronous FCL where clients train on distinct task sequences and aim to turn aggregation into knowledge fusion. The authors conduct experiments using FedAvg on CIFAR-100 with 3 clients and 1 server, creating 12 different data distribution scenarios combining client class overlap (Same/Different) and task distribution patterns (IID/Non-IID/Class-Incremental). They track temporal knowledge retention (KR_t) and spatial knowledge retention (KR_s) to measure forgetting, using cross-entropy loss between models as an overall metric. The experiments demonstrate how spatial-temporal catastrophic forgetting affects model performance across different data heterogeneity scenarios.

## Key Results
- Spatial-temporal catastrophic forgetting occurs when federated models forget both previous tasks (temporal) and local knowledge (spatial) due to data heterogeneity and non-IID distributions
- Two unified frameworks (synchronous and asynchronous FCL) can integrate most existing methods and provide pipelines for future research
- Knowledge fusion in FCL can be categorized into seven mechanisms: rehearsal, clustering, regularization, parameter isolation, dynamic architecture, prototype, and knowledge distillation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Federated continual learning (FCL) suffers from "spatial-temporal catastrophic forgetting" where models forget both previous tasks (temporal) and local knowledge (spatial) due to data heterogeneity and non-IID distributions.
- Mechanism: The problem arises because traditional federated learning methods assume static data environments and use simple weighted averaging of model parameters, which overwrites crucial parameters for local tasks when aggregating heterogeneous models.
- Core assumption: Knowledge of a model is represented by parameters, and the more different the data, the greater the parameter divergence.
- Evidence anchors:
  - [abstract] "The key objective of FCL is to fuse heterogeneous knowledge from different clients and retain knowledge of previous tasks while learning on new ones."
  - [section] "Data heterogeneity results in diverse knowledge extracted by local models. Knowledge of a model is often represented by parameters. The more different the data, the greater the parameter divergence."
  - [corpus] Found 25 related papers with average neighbor FMR=0.429, indicating moderate relatedness to the topic.
- Break condition: If data distributions among clients become identical and task sequences are perfectly synchronized, spatial-temporal catastrophic forgetting would not occur.

### Mechanism 2
- Claim: Two unified frameworks for FCL - synchronous and asynchronous - can integrate most existing methods and lay pipelines for future research.
- Mechanism: Synchronous FCL assumes all clients share the same task sequence and focuses on aggregation, while asynchronous FCL allows clients to train on distinct task sequences and aims to turn aggregation into knowledge fusion.
- Core assumption: Knowledge fusion can be achieved through various mechanisms including rehearsal, clustering, regularization, parameter isolation, dynamic architecture, prototype, and knowledge distillation.
- Evidence anchors:
  - [abstract] "We propose two unified frameworks for FCL (i.e., synchronous FCL and asynchronous FCL). The two frameworks can integrate most existing methods and lay two pipelines for future research."
  - [section] "Based on whether the task sequence of the client is the same, FCL can be preliminarily divided into two types: synchronous FCL and asynchronous FCL."
  - [corpus] Papers like "Federated Continual Learning for Edge-AI: A Comprehensive Survey" and "Handling Spatial-Temporal Data Heterogeneity for Federated Continual Learning via Tail Anchor" support the need for these frameworks.
- Break condition: If clients cannot communicate at all or if the server cannot perform any aggregation, neither framework would work.

### Mechanism 3
- Claim: Knowledge fusion in FCL can be categorized into seven forms based on the expression forms of knowledge in different stages of model training.
- Mechanism: Local knowledge is hidden in three parts: data, models, and outputs. Existing methods are summarized with seven parts: rehearsal, clustering, all parameters, parameter isolation, dynamic architecture, prototype, and knowledge distillation.
- Core assumption: Different knowledge fusion mechanisms address different aspects of spatial-temporal catastrophic forgetting and can be combined for better performance.
- Evidence anchors:
  - [abstract] "We categorize a large number of methods according to the mechanism involved in knowledge fusion, namely rehearsal, clustering, all gradients/parameters, parameter isolation, dynamic architecture, prototype and knowledge distillation."
  - [section] "We find that in the FCL setting, local knowledge is hidden in three parts: data, models and outputs. We divide existing knowledge fusion methods into seven classes as shown in Fig. 4."
  - [corpus] Papers like "Accurate Forgetting for Heterogeneous Federated Continual Learning" and "Improving Generalization in Heterogeneous Federated Continual Learning via Spatio-Temporal Gradient Matching with Prototypical Coreset" demonstrate the effectiveness of these fusion mechanisms.
- Break condition: If the knowledge representation becomes too complex or if the communication overhead becomes prohibitive, simpler fusion methods may be necessary.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: FCL builds upon FL by adding the continual learning aspect, so understanding FL fundamentals is essential.
  - Quick check question: What is the main difference between traditional centralized training and federated learning?

- Concept: Continual Learning (CL)
  - Why needed here: FCL combines FL with CL to address the challenges of learning from streaming data across multiple clients while preserving privacy.
  - Quick check question: What is catastrophic forgetting in the context of continual learning?

- Concept: Non-IID Data Distribution
  - Why needed here: Data heterogeneity among clients is a fundamental challenge in FCL that leads to spatial catastrophic forgetting.
  - Quick check question: How does non-IID data distribution affect the performance of federated learning models?

## Architecture Onboarding

- Component map: Server (central coordinator) -> Clients (local devices) -> Global Model (aggregated knowledge) -> Local Models (client-specific knowledge) -> Communication Protocol (knowledge transfer mechanism)
- Critical path: Client local training → Parameter/gradient upload → Server aggregation → Global model distribution → Client local training on next task
- Design tradeoffs: Privacy vs. performance (uploading full gradients vs. knowledge distillation), Communication efficiency vs. model accuracy (parameter isolation vs. full parameter upload), Local adaptation vs. global generalization
- Failure signatures: Performance degradation on local tasks (spatial forgetting), Performance degradation on previous tasks (temporal forgetting), Communication overhead, Privacy breaches
- First 3 experiments:
  1. Implement FedAvg on CIFAR-100 with three clients to observe spatial-temporal catastrophic forgetting
  2. Apply regularization-based method (e.g., FedCurv) to mitigate forgetting and compare performance
  3. Implement knowledge distillation-based method (e.g., FedKL) and evaluate its effectiveness in preserving knowledge across tasks and clients

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between privacy protection and model utility in federated continual learning (FCL) systems?
- Basis in paper: [explicit] The paper discusses the challenges of balancing privacy protection and model utility in FCL, citing the work of Yang et al. on "Trustworthy Federated Learning" which reveals the incompatibility of privacy and utility in FL.
- Why unresolved: The paper mentions that enhancing privacy often comes at the cost of decreased model performance, and suggests seeking the Pareto Front of multi-objective optimization to determine the equilibrium point based on user requirements. However, it does not provide a specific solution or methodology for achieving this balance.
- What evidence would resolve it: A comprehensive study comparing different privacy-preserving mechanisms (e.g., differential privacy, secure multi-party computation, homomorphic encryption) in the context of FCL, along with their impact on model performance and convergence time.

### Open Question 2
- Question: How can delayed decision-making, such as the three-way decision (3WD) approach, be effectively incorporated into FCL to improve model performance?
- Basis in paper: [explicit] The paper mentions that the three-way decision approach can significantly improve average accuracy by delaying decisions on uncertain samples until the learner has sufficient capacity for confident prediction. It also notes that some scholars have incorporated 3WD into continual learning, but there is no study on incorporating it into FCL.
- Why unresolved: While the potential benefits of 3WD in FCL are mentioned, the paper does not provide any concrete methods or algorithms for integrating this approach into FCL frameworks.
- What evidence would resolve it: A novel FCL algorithm that incorporates the three-way decision approach, along with experimental results demonstrating its effectiveness in improving model performance compared to traditional FCL methods.

### Open Question 3
- Question: How can federated continual learning be effectively applied to large language models (LLMs) to address the challenges of privacy leakage and catastrophic forgetting?
- Basis in paper: [explicit] The paper discusses the potential of combining FCL and LLMs, mentioning recent works that explore using LLMs to address challenges in continual learning and post-training techniques to reduce bias and fuse domain knowledge into LLMs. However, it also notes that research on combining FCL and LLM is still limited.
- Why unresolved: While the paper highlights the potential benefits of applying FCL to LLMs, it does not provide any specific methods or frameworks for achieving this integration or addressing the challenges of privacy leakage and catastrophic forgetting in LLMs.
- What evidence would resolve it: A comprehensive framework for federated continual learning of large language models, along with experimental results demonstrating its effectiveness in preserving privacy, mitigating catastrophic forgetting, and improving model performance on downstream tasks.

## Limitations

- Lack of empirical validation beyond baseline FedAvg experiments for the proposed knowledge fusion mechanisms
- Limited testing scope confined to CIFAR-100 dataset without evaluation on more complex or diverse datasets
- No systematic benchmarking across diverse FCL methods to validate the claim that frameworks can integrate most existing approaches

## Confidence

- Claim that spatial-temporal catastrophic forgetting is a fundamental problem in FCL: High confidence based on established literature in both federated learning and continual learning
- Claim that the synchronous and asynchronous frameworks can integrate most existing methods: Low confidence without systematic benchmarking
- Claim that the seven-category taxonomy comprehensively covers knowledge fusion mechanisms: Medium confidence, appears reasonable but lacks experimental verification of completeness

## Next Checks

1. Implement and compare at least three different knowledge fusion mechanisms (e.g., regularization, knowledge distillation, and rehearsal) across the 12 experimental scenarios to empirically validate their effectiveness in addressing spatial-temporal catastrophic forgetting.

2. Conduct ablation studies to determine which components of the synchronous and asynchronous frameworks contribute most significantly to knowledge preservation, providing quantitative evidence for the claimed benefits of each approach.

3. Test the proposed frameworks on more complex datasets (e.g., CORe50 or mini-Imagenet) and larger client populations to assess scalability and generalizability beyond the CIFAR-100 experiments presented.