---
ver: rpa2
title: Memory Efficient Mixed-Precision Optimizers
arxiv_id: '2309.12381'
source_url: https://arxiv.org/abs/2309.12381
tags:
- training
- memory
- bits
- precision
- optimizer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes methods to reduce memory usage in mixed-precision
  training of deep learning models. The key ideas are: (1) storing only the difference
  between full-precision and half-precision parameter values instead of both, and
  (2) fusing the backward pass and optimizer step to avoid storing gradients.'
---

# Memory Efficient Mixed-Precision Optimizers
## Quick Facts
- arXiv ID: 2309.12381
- Source URL: https://arxiv.org/abs/2309.12381
- Reference count: 11
- Key outcome: Achieves 25% lower peak memory usage and 15% faster training while maintaining accuracy

## Executive Summary
This paper proposes methods to reduce memory usage in mixed-precision training of deep learning models. The key innovations are storing only the difference between full-precision and half-precision parameter values, and fusing the backward pass with the optimizer step to avoid storing gradients. Experiments on ResNet-18, DLRM, T5, and DCGAN models demonstrate significant memory savings (up to 54% in synthetic experiments) while maintaining or improving model accuracy compared to standard mixed-precision training.

## Method Summary
The paper introduces a memory-efficient mixed-precision optimizer that reduces memory usage by storing only the difference between full-precision (fp32) and half-precision (fp16) parameter values, rather than storing both copies. This difference is stored with a small number of extra bits (8-16) to maintain accuracy. Additionally, the optimizer fuses the backward pass with the optimization step, computing and applying gradients immediately without storing them. Custom CUDA kernels handle the extra precision arithmetic and the fused operations. The approach is implemented as a PyTorch-compatible wrapper that intercepts gradients and applies updates.

## Key Results
- 25% lower peak memory usage compared to standard mixed-precision training
- 15% faster training throughput on tested models
- Maintained or improved model accuracy across ResNet-18, DLRM, T5, and DCGAN
- Up to 54% memory reduction in synthetic experiments, close to theoretical maximum

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Storing only the difference between full-precision and half-precision parameter values reduces memory usage without accuracy loss.
- Mechanism: Instead of storing both fp32 and fp16 copies of parameters, the algorithm stores the difference (fp32 - fp16) and reconstructs full precision during updates using custom CUDA kernels that handle extra bits.
- Core assumption: The difference between fp32 and fp16 can be stored in fewer bits while preserving sufficient precision for accurate training updates.
- Evidence anchors:
  - [abstract] "getting rid of the floating point copy of the parameters, virtually keeping only half-precision numbers"
  - [section 3.1] "our approach consists of storing only the difference between the two formats"
  - [corpus] Weak - no direct corpus evidence found for this specific difference-based storage mechanism
- Break condition: If the stored difference requires more bits than the theoretical minimum, or if reconstruction errors accumulate significantly over training steps.

### Mechanism 2
- Claim: Fusing backward pass and optimizer step eliminates the need to store gradients, reducing memory pressure.
- Mechanism: The optimizer step is executed immediately during the backward pass as soon as gradients are computed, preventing the need to store all gradients simultaneously.
- Core assumption: Gradients can be applied immediately without requiring accumulation or access after backward pass completion.
- Evidence anchors:
  - [abstract] "getting rid of the gradient's value by executing the optimizer step during the back-propagation"
  - [section 3.2] "operate the optimization step as soon as the gradient is computed"
  - [corpus] Weak - no direct corpus evidence found for this specific fusion approach
- Break condition: If training requires gradient accumulation across multiple batches, or if gradient clipping/loss scaling must be applied after backward pass.

### Mechanism 3
- Claim: Stochastic rounding improves accuracy compared to basic truncation when saving extra bits.
- Mechanism: Instead of always rounding down when converting fp32 to lower precision, the algorithm randomly rounds up or down based on proximity, with an extra bit tracking the rounding direction for later reconstruction.
- Core assumption: Random rounding errors will average out over many operations, leading to lower overall error than systematic truncation.
- Evidence anchors:
  - [section 3.1] "we have developed the option to use stochastic rounding when splitting the full precision value"
  - [section 4] "Stochastic rounding provides a better accuracy as compared to classic 16bits formats"
  - [corpus] Weak - no direct corpus evidence found for this specific stochastic rounding application
- Break condition: If the overhead of tracking rounding direction outweighs the accuracy benefits, or if deterministic behavior is required.

## Foundational Learning

- Concept: IEEE-754 floating point representation
  - Why needed here: Understanding how floating point numbers are stored is crucial for designing memory-efficient representations and knowing what precision is actually available
  - Quick check question: How many bits are typically used for the exponent and significand in fp16 format?

- Concept: Mixed precision training principles
  - Why needed here: The algorithm builds on existing mixed precision techniques, so understanding the standard approach (fp16 for forward pass, fp32 for updates) is essential
  - Quick check question: What problem does keeping a master fp32 copy of parameters solve in mixed precision training?

- Concept: CUDA memory management and kernel design
  - Why needed here: The custom CUDA kernels for handling extra bits and fused operations require understanding of GPU memory layout and thread organization
  - Quick check question: Why does the implementation use 32-bit chunks to access arbitrarily sized bit strings?

## Architecture Onboarding

- Component map:
  Parameter storage layer -> Custom arithmetic operators -> Fused optimizer -> Trainer interface

- Critical path: Forward pass → Gradient computation → Immediate optimizer step → Parameter update
  The fused nature means gradients never leave the backward pass context

- Design tradeoffs:
  - Memory vs. accuracy: Fewer extra bits saved means more memory reduction but potential accuracy loss
  - Performance vs. flexibility: Fused optimizers are faster but may not support all optimizer types
  - Complexity vs. compatibility: Custom CUDA kernels provide efficiency but require CUDA expertise

- Failure signatures:
  - Training divergence: May indicate insufficient extra bits or issues with stochastic rounding
  - Slow performance: Could suggest inefficient shared memory usage in custom kernels
  - Memory leaks: Might occur if the fused implementation doesn't properly clean up intermediate values

- First 3 experiments:
  1. Verify memory reduction: Compare peak memory usage between standard mixed precision and difference-based storage on a small ResNet-18 model
  2. Test fused backward pass: Implement simple SGD with fused backward pass and verify it matches non-fused results
  3. Measure accuracy impact: Train a model with different numbers of extra bits to find the minimum needed for stable training

## Open Questions the Paper Calls Out
No explicit open questions are called out in the paper.

## Limitations
- Custom CUDA kernel implementation details are not fully specified, making faithful reproduction challenging
- Accuracy benefits of stochastic rounding versus basic truncation need more rigorous validation across diverse model architectures
- Fused backward pass approach may not be compatible with all optimizer types or training scenarios requiring gradient accumulation

## Confidence
- Memory reduction claim (25% lower peak memory): Medium confidence
- Training speed improvement (15% faster): Medium confidence
- Accuracy maintenance: Low-Medium confidence

## Next Checks
1. Systematically vary the number of extra bits stored and measure the resulting memory savings versus accuracy degradation across multiple epochs of training to establish the optimal tradeoff point.

2. Test the proposed techniques on different GPU architectures (e.g., both NVIDIA Ampere and Ada Lovelace) to verify the 15% training speed improvement is consistent and not platform-dependent.

3. Evaluate whether the fused backward pass approach can be adapted to support gradient accumulation across multiple micro-batches, which is crucial for training with larger effective batch sizes.