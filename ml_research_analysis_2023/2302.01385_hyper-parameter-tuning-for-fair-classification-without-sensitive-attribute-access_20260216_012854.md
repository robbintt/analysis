---
ver: rpa2
title: Hyper-parameter Tuning for Fair Classification without Sensitive Attribute
  Access
arxiv_id: '2302.01385'
source_url: https://arxiv.org/abs/2302.01385
tags:
- antigone
- sensitive
- george
- noisy
- attribute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles hyper-parameter tuning for fair classification
  without access to sensitive attribute labels. The core idea is to generate pseudo-sensitive
  attribute labels on validation data by training a biased ERM classifier and using
  its correctly (incorrectly) classified examples as proxies for majority (minority)
  groups.
---

# Hyper-parameter Tuning for Fair Classification without Sensitive Attribute Access

## Quick Facts
- **arXiv ID**: 2302.01385
- **Source URL**: https://arxiv.org/abs/2302.01385
- **Reference count**: 29
- **Primary result**: Achieves DP Gap of 11.1% and WGA of 68.1% on CelebA vs 18.6% DP Gap and 38.7% WGA for standard ERM

## Executive Summary
This paper addresses the challenge of hyper-parameter tuning for fair classification when sensitive attribute labels are unavailable. The proposed method, Antigone, generates pseudo-sensitive attribute labels by leveraging the natural bias of standard ERM classifiers - using correctly classified examples as proxies for majority groups and misclassified examples as proxies for minority groups. A key innovation is using the Euclidean distance between means (EDM) of these sets to select the most biased classifier in a completely unsupervised manner. Theoretical justification is provided using the mutually contaminated noise model, showing that fairness metrics can be estimated up to a proportionality constant. Empirical results demonstrate significant improvements over standard ERM and competitive performance with state-of-the-art methods.

## Method Summary
Antigone works by first training multiple ERM classifiers with different hyperparameters on training data lacking sensitive attribute labels. For each classifier, it computes the EDM between correctly and incorrectly classified examples on validation data, selecting the classifier with maximum EDM as the most biased. This biased classifier's predictions are then used as pseudo-sensitive attribute labels to tune fairness-aware methods like JTT or GEORGE. The approach leverages the observation that standard ERM models naturally achieve higher accuracy on majority groups, causing their misclassifications to disproportionately represent minority groups. The EDM metric effectively captures this bias pattern, allowing unsupervised selection of the best hyperparameter configuration for fairness optimization.

## Key Results
- On CelebA, Antigone+JTT achieves DP Gap of 11.1% and WGA of 68.1% compared to 18.6% DP Gap and 38.7% WGA for standard ERM
- Antigone produces more accurate pseudo-sensitive attribute labels than GEORGE on all three benchmark datasets
- The EDM-based selection mechanism successfully identifies biased classifiers that improve fairness when used for hyper-parameter tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using incorrectly classified examples as proxies for minority groups works because the model's own bias naturally clusters these misclassifications around the minority group.
- Mechanism: When a model is trained with standard ERM, it tends to achieve high accuracy on the majority group while underperforming on minority groups. This creates a natural partitioning where misclassifications are disproportionately drawn from minority groups.
- Core assumption: The training data contains bias that causes the ERM model to systematically misclassify minority group examples.
- Evidence anchors:
  - [abstract] "mis-classified examples of a model trained with standard ERM loss serve as an effective proxy for minority sub-groups"
  - [section 2.2] "mis-classiﬁed examples of a standard empirical risk minimization (ERM) model as a proxy for minority sub-groups"
  - [corpus] Weak evidence - related papers discuss fairness in recommendations and text classification but don't directly validate this mechanism
- Break condition: If the training data is perfectly balanced or the model achieves equal accuracy across all groups, this mechanism fails.

### Mechanism 2
- Claim: The EDM metric selects the most biased ERM model because bias manifests as greater separation between the means of correctly and incorrectly classified examples.
- Mechanism: A biased model will have clearer separation between the data distributions of its correct and incorrect classifications, as the model systematically performs better on one group than another.
- Core assumption: The distance between means of correctly and incorrectly classified examples correlates with the model's bias toward certain groups.
- Evidence anchors:
  - [section 2.2] "Antigone uses the Euclidean distance between the means (EDM) of the two sets as a distance measure"
  - [section 2.3] "maximizing the EDM between XA=0,noisy and XA=1,noisy, i.e.,∥µ(XA=0,noisy)−µ(XA=1,noisy)∥2 maximizes 1−α−β"
  - [corpus] Weak evidence - no direct corpus support for this specific metric selection
- Break condition: If the bias doesn't manifest as mean separation (e.g., if it's variance-based or involves complex interaction effects), EDM may not correlate with bias.

### Mechanism 3
- Claim: The mutually contaminated noise model justifies using noisy sensitive attribute labels because fairness metrics can be estimated up to a proportionality constant even with contaminated labels.
- Mechanism: Under the MC noise model, demographic parity and equal opportunity can be estimated up to a proportionality constant, meaning that even with noisy labels, the relative ordering of fairness across hyperparameter settings remains valid.
- Core assumption: The contamination pattern in pseudo sensitive attribute labels follows the mutually contaminated noise model.
- Evidence anchors:
  - [section 2.3] "fairness metrics like demographic parity, equal opportunity and subgroup accuracy can be estimated to within a proportionality constant even with noisy sensitive attribute information"
  - [section 2.3] "Under the ideal MC noise model in Equation 8, demographic parity and equal opportunity gaps measured on the noisy datsets are proportional to the true DP and EO gaps"
  - [corpus] Weak evidence - related papers discuss fairness but don't specifically validate the MC noise model application
- Break condition: If the contamination pattern deviates significantly from the MC noise model assumptions, the proportionality constant may not hold.

## Foundational Learning

- Concept: Mutually Contaminated Noise Model
  - Why needed here: Provides theoretical justification for using noisy sensitive attribute labels to estimate fairness metrics
  - Quick check question: If α=0.2 and β=0.3 in the MC model, what is the proportionality constant for DP estimation?

- Concept: Euclidean Distance Between Means
  - Why needed here: Core metric for selecting the most biased ERM model without access to ground truth sensitive attributes
  - Quick check question: Given two distributions with means (2,3) and (5,7), what is their EDM?

- Concept: Group Fairness Metrics (DP, EO, WGA)
  - Why needed here: The fairness metrics being optimized through the pseudo-sensitive attribute labeling process
  - Quick check question: Which fairness metric focuses on equalizing true positive rates across groups?

## Architecture Onboarding

- Component map: ERM model training loop with multiple hyperparameter settings -> EDM calculation and selection mechanism -> Pseudo-sensitive attribute label generation -> Fair training algorithm (JTT/GEORGE) with validation using pseudo labels -> Evaluation pipeline with true sensitive attributes for ground truth comparison

- Critical path: ERM model training → EDM-based selection → Pseudo label generation → Fair training hyperparameter tuning → Evaluation

- Design tradeoffs:
  - Using EDM vs more sophisticated distance metrics (simpler but may miss complex bias patterns)
  - Number of ERM models to train (more models = better selection but higher computational cost)
  - Early stopping criteria (earlier stopping may capture bias better but reduce target accuracy)

- Failure signatures:
  - Low pseudo-label accuracy despite high EDM (indicates EDM may not correlate with label quality)
  - Fair training results worse than baseline ERM (indicates poor hyper-parameter selection)
  - Sensitivity to minority group representation (indicates mechanism breakdown on balanced datasets)

- First 3 experiments:
  1. Verify EDM correlation with 1-α-β on a simple synthetic dataset with known bias
  2. Compare pseudo-label F1 scores against ground truth on validation data
  3. Run fair training with ground truth vs pseudo labels to measure performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is Antigone's performance to the choice of distance metric for comparing correct and incorrect sets?
- Basis in paper: [explicit] The paper uses Euclidean distance between means (EDM) but acknowledges this is the "simplest distance metric" and compares it against alternatives in ablation studies.
- Why unresolved: The paper only compares EDM to a baseline without EDM, not to other potential distance metrics like Wasserstein distance or KL divergence.
- What evidence would resolve it: Experiments comparing EDM against multiple alternative distance metrics on the same datasets.

### Open Question 2
- Question: Can Antigone be extended to handle multi-class sensitive attributes effectively?
- Basis in paper: [explicit] The paper states "for now Antigone is limited to binary sensitive attributes, but can be extended to multiple target labels."
- Why unresolved: The paper doesn't explore or demonstrate how the approach would work with more than two sensitive groups.
- What evidence would resolve it: Results showing Antigone's performance on datasets with 3+ sensitive attribute classes.

### Open Question 3
- Question: How does Antigone perform when the minority group is severely underrepresented in the training data?
- Basis in paper: [explicit] The paper varies minority representation from 5% to 50% and shows performance degradation, but doesn't explore even lower percentages.
- Why unresolved: The paper doesn't test scenarios with less than 5% minority representation or explore the theoretical limits of performance.
- What evidence would resolve it: Results showing performance trends as minority group representation approaches 0%.

## Limitations

- The core mechanism depends on existing bias in training data and may fail when data is balanced or models achieve uniform accuracy across groups
- The EDM metric may not capture complex forms of bias that don't manifest as mean separation between correctly and incorrectly classified examples
- The mutually contaminated noise model assumptions may not hold in real-world scenarios, potentially breaking the proportionality constant needed for reliable fairness estimation

## Confidence

- **High Confidence**: The EDM-based selection mechanism and its theoretical connection to the MC noise model (when assumptions hold)
- **Medium Confidence**: The effectiveness of pseudo-sensitive attribute labels for hyper-parameter tuning, based on empirical results
- **Medium Confidence**: The claim that this approach works without sensitive attribute access, though it requires validation data with ground truth sensitive attributes

## Next Checks

1. Test EDM correlation with bias on synthetic datasets where the relationship between misclassification patterns and group membership is known and controllable
2. Evaluate performance on balanced datasets to determine break conditions for the mechanism
3. Compare results using alternative distance metrics (e.g., Wasserstein distance, KL divergence) to assess sensitivity to the choice of EDM