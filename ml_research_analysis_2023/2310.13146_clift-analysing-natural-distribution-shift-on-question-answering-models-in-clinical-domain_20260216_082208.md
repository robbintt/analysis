---
ver: rpa2
title: 'CLIFT: Analysing Natural Distribution Shift on Question Answering Models in
  Clinical Domain'
arxiv_id: '2310.13146'
source_url: https://arxiv.org/abs/2310.13146
tags:
- dataset
- test
- clinical
- emrqa
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLIFT introduces a new benchmark for evaluating robustness of clinical
  domain QA models under natural distribution shifts. The testbed includes 7.5k high-quality
  question-answering samples across 5 diseases (heart, medication, obesity, smoking,
  cancer) derived from MIMIC-III.
---

# CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain

## Quick Facts
- arXiv ID: 2310.13146
- Source URL: https://arxiv.org/abs/2310.13146
- Reference count: 40
- Models fine-tuned on emrQA show 43.82% to 73.07% F1 score drops when evaluated on new test sets, despite strong original performance.

## Executive Summary
CLIFT introduces a benchmark for evaluating robustness of clinical domain QA models under natural distribution shifts. The testbed includes 7.5k high-quality question-answering samples across 5 diseases (heart, medication, obesity, smoking, cancer) derived from MIMIC-III. When evaluated on these new test sets, models fine-tuned on emrQA showed substantial performance drops ranging from 43.82% to 73.07% in F1 score, despite strong results on the original test set. This highlights the need for models that generalize across diverse clinical data sources and institutions. The benchmark enables tracking progress toward more robust clinical QA systems and emphasizes evaluation metrics that consider distributional shifts. The testbed is publicly available at github.com/openlifescience-ai/clift.

## Method Summary
The method involves fine-tuning 12 Transformer-based models (including BioBERT, BlueBERT, PubMedBERT) on emrQA training data (Medication and Relation subsets) using a 7:1:2 train/dev/test split ratio. Models are trained with batch size 8, learning rate 2e-5, for 4 epochs using HuggingFace Transformers. The trained models are then evaluated on five new test sets derived from MIMIC-III, each focusing on a different disease category (heart, medication, obesity, smoking, cancer). Performance is measured using F1 score, and the degradation across test sets is analyzed to quantify distribution shift effects.

## Key Results
- Models fine-tuned on emrQA achieved strong performance (high F1 scores) on the original test set
- When evaluated on CLIFT test sets, performance dropped by 43.82% to 73.07% in F1 score
- PubMedBERT showed the best performance among the 12 evaluated models but still exhibited significant degradation
- The Obesity & Heart subset showed more extended questions and broader vocabulary compared to other test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distribution shift causes significant performance degradation in clinical QA models
- Mechanism: Models trained on emrQA data overfit to that specific distribution and fail to generalize when tested on new data sources with different linguistic patterns, document structures, and clinical terminology usage
- Core assumption: The test data from MIMIC-III has naturally different distributions compared to emrQA training data
- Evidence anchors:
  - [abstract] "Despite impressive results on the original test set, the performance degrades when applied to new test sets, which shows the distribution shift"
  - [section 2.1] "This difference may be broken down into three parts: adaptivity gap, distribution gap, and generalization gap"
  - [corpus] Weak evidence - related papers focus on VQA and audio adaptation rather than clinical QA distribution shifts

### Mechanism 2
- Claim: Answer diversity and syntactic divergence are valid metrics for measuring dataset difficulty
- Mechanism: Datasets with more varied answer formats and different syntactic structures are harder for QA models to handle, requiring more robust language understanding
- Core assumption: The proposed CLIFT datasets have higher answer diversity and syntactic divergence than emrQA
- Evidence anchors:
  - [section 4.2] "We use the two difficulty metrics provided by [19] to compare the original emrQA test set to our five new test sets"
  - [section 6.2] "Obesity & Heart subset was observed to include more extended questions and a broader vocabulary than the others"
  - [corpus] No direct evidence - related papers don't address syntactic divergence metrics

### Mechanism 3
- Claim: Pre-trained clinical language models (BioBERT, BlueBERT, etc.) can be fine-tuned for clinical QA but remain sensitive to distribution shifts
- Mechanism: These models leverage domain-specific knowledge from biomedical corpora, but fine-tuning on emrQA creates specialization that doesn't transfer well to new clinical contexts
- Core assumption: The pre-training provides general clinical knowledge while fine-tuning creates task-specific but brittle models
- Evidence anchors:
  - [section 5.1] "We consider 12 existing models as a baseline, which have shown strong performance in biomedical NLP tasks"
  - [section 6.1] "PubMedBERT performed better than other models" but still showed performance degradation on new test sets
  - [corpus] Weak evidence - related papers focus on VQA and audio adaptation rather than clinical QA model performance

## Foundational Learning

- Concept: Distribution shift in machine learning
  - Why needed here: Understanding how and why model performance degrades when test data differs from training data is central to this work
  - Quick check question: What are the three components of performance degradation when distribution shifts occur?

- Concept: Question answering evaluation metrics
  - Why needed here: F1 score is the primary metric used, and understanding how it's calculated is crucial for interpreting results
  - Quick check question: How does F1 score differ from simple accuracy in question answering tasks?

- Concept: Clinical NLP domain challenges
  - Why needed here: The paper deals specifically with clinical texts, which have unique characteristics like abbreviations, jargon, and document structures
  - Quick check question: What makes clinical text processing more challenging than general domain text processing?

## Architecture Onboarding

- Component map: Data preprocessing (cleaning, tokenization) -> Model training (fine-tuning pre-trained transformers) -> Evaluation (F1 score calculation) -> Analysis (interpretability, difficulty metrics)
- Critical path: Data preprocessing → Model fine-tuning → Evaluation on test sets → Analysis of distribution shift effects
- Design tradeoffs: Large pre-trained models offer better initial performance but are more resource-intensive; simpler models are faster but may perform worse on complex clinical texts
- Failure signatures: Large F1 score gaps between emrQA and CLIFT test sets indicate distribution shift sensitivity; poor performance on specific disease categories may indicate vocabulary or domain gaps
- First 3 experiments:
  1. Fine-tune BioBERT on emrQA training data and evaluate on emrQA test set to establish baseline performance
  2. Evaluate the fine-tuned model on one CLIFT test set (e.g., Heart disease) to measure distribution shift impact
  3. Compare performance across different pre-trained models (BioBERT, BlueBERT, PubMedBERT) on the same test set to identify which handles distribution shifts best

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific model architecture modifications could reduce performance drops on distribution-shifted clinical QA datasets?
- Basis in paper: [inferred] The paper shows substantial performance degradation (43.82% to 73.07% F1 drop) when models trained on emrQA are evaluated on new test sets, indicating current architectures are insufficient for robust generalization.
- Why unresolved: The paper evaluates existing pre-trained models but does not propose or test architectural modifications specifically designed to handle distribution shifts in clinical domains.
- What evidence would resolve it: Comparative experiments showing performance of distribution-shift-aware architectures (e.g., domain adaptation techniques, robust fine-tuning strategies, or models with built-in generalization mechanisms) versus standard models on CLIFT benchmark.

### Open Question 2
- Question: How does dataset size and diversity of the training corpus impact robustness to natural distribution shifts in clinical QA?
- Basis in paper: [inferred] The paper mentions that future research should examine "datasets' size, quality, and quantity" but does not empirically investigate how these factors affect robustness to distribution shifts.
- Why unresolved: The study uses a fixed emrQA training set and does not systematically vary dataset characteristics to measure their impact on distribution shift performance.
- What evidence would resolve it: Controlled experiments training models on emrQA subsets of varying sizes and diversity levels, then measuring performance degradation on CLIFT test sets to establish correlations.

### Open Question 3
- Question: Which specific aspects of syntactic divergence most strongly correlate with performance degradation on distribution-shifted test sets?
- Basis in paper: [explicit] The paper uses syntactic divergence as a difficulty metric and observes differences between emrQA and new test sets, but does not analyze which syntactic features contribute most to performance drops.
- Why unresolved: While syntactic divergence is measured, the paper does not perform fine-grained analysis to identify specific syntactic patterns that cause model failures.
- What evidence would resolve it: Detailed analysis correlating specific syntactic features (e.g., dependency tree structures, phrase types) with model performance across different test sets to identify key failure modes.

## Limitations

- The testbed covers only 5 disease categories, which may not represent the full diversity of clinical QA scenarios
- The exact preprocessing pipeline for the CLIFT test sets is not fully specified, making exact reproduction challenging
- The paper doesn't validate whether answer diversity and syntactic divergence metrics actually predict model performance degradation

## Confidence

- **High confidence**: The fundamental observation that models trained on emrQA degrade significantly on new test sets - this is directly observable from the reported F1 scores.
- **Medium confidence**: The claim that CLIFT provides a useful benchmark for tracking robustness progress - supported by the methodology but limited by the small number of test categories.
- **Low confidence**: The effectiveness of answer diversity and syntactic divergence as difficulty metrics - the paper proposes these but doesn't demonstrate their predictive power for model performance.

## Next Checks

1. **Metric correlation validation**: Test whether answer diversity and syntactic divergence scores actually predict model performance drops across different disease categories, rather than just being descriptive statistics.

2. **Baseline comparison**: Implement and evaluate simpler QA models (e.g., without domain-specific pre-training) on the CLIFT benchmark to determine if the claimed advantages of clinical language models are statistically significant.

3. **Distribution analysis**: Quantify the actual distribution differences between emrQA and CLIFT test sets using KL divergence or other statistical measures to validate the claimed distribution shift mechanisms.