---
ver: rpa2
title: 'UT5: Pretraining Non autoregressive T5 with unrolled denoising'
arxiv_id: '2311.08552'
source_url: https://arxiv.org/abs/2311.08552
tags:
- language
- pretraining
- generation
- arxiv
- non-autoregressive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates pretraining for non-autoregressive (NAR)
  language models, which are faster than autoregressive models but often underperform
  them. The authors propose using unrolled denoising (SUNDAE) with prefix language
  modeling to pretrain NAR models, focusing on decoder-only architectures.
---

# UT5: Pretraining Non autoregressive T5 with unrolled denoising

## Quick Facts
- arXiv ID: 2311.08552
- Source URL: https://arxiv.org/abs/2311.08552
- Reference count: 5
- The paper investigates pretraining for non-autoregressive (NAR) language models, which are faster than autoregressive models but often underperform them.

## Executive Summary
This paper addresses the quality gap in non-autoregressive (NAR) language models by introducing a pretraining approach using unrolled denoising (SUNDAE) with prefix language modeling. The authors focus on decoder-only architectures and demonstrate significant performance improvements on XSum summarization and SQuAD question generation tasks. Their method achieves state-of-the-art results on XSum and competitive performance on SQuAD compared to autoregressive baselines, while maintaining the speed advantages of NAR models.

## Method Summary
The authors pretrain a decoder-only transformer (T5-base architecture with 12 layers, 768 hidden dimension) using SUNDAE unrolled denoising combined with prefix language modeling on the C4 dataset. The training involves 1 million steps with a batch size of 128 and sequence length of 512/114 tokens. During fine-tuning, the pretrained model is adapted to downstream tasks (XSum and SQuAD) for 50,000 steps. The inference uses a 10-step unrolling procedure to generate outputs efficiently while maintaining quality.

## Key Results
- Achieves state-of-the-art performance on XSum summarization with ROUGE-2 of 23.93 and ROUGE-L of 28.43
- Demonstrates competitive results on SQuAD question generation compared to autoregressive baselines
- Shows 1.17x speedup over autoregressive models while maintaining comparable quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining improves NAR model quality by exposing it to diverse linguistic patterns during the denoising process
- Mechanism: The SUNDAE unrolled denoising objective combined with prefix language modeling allows the NAR decoder to learn bidirectional context and robust denoising patterns during pretraining, which transfers to better downstream generation
- Core assumption: The pretraining task distribution (C4 corpus) is sufficiently diverse to provide meaningful linguistic signal that generalizes to downstream tasks
- Evidence anchors:
  - [abstract]: "unrolled denoising (SUNDAE) with prefix language modeling to pretrain NAR models"
  - [section 3]: "We adopted SUNDAE... The training process includes unrolled denoising, which involves starting the chain from corrupted data samples instead of the prior distribution"
  - [corpus]: Weak evidence - only 25 related papers found, average neighbor FMR=0.393, no citations
- Break condition: If pretraining data lacks sufficient linguistic diversity or if the downstream tasks have domain shift that doesn't match pretraining distribution

### Mechanism 2
- Claim: Pretraining enables sample efficiency improvements in NAR models
- Mechanism: The model learns general denoising and context modeling patterns during pretraining, requiring fewer fine-tuning steps to adapt to downstream tasks
- Core assumption: The pretraining objective (prefix LM with bidirectional attention) captures generalizable language patterns
- Evidence anchors:
  - [section 4.2]: "The pretraing helps the model to efficiently finetune on different downstream tasks with fewer number of steps"
  - [section 5.2]: "the pretrained model is more sample efficient, reaching higher number with the same fine-tune steps"
  - [corpus]: Weak evidence - no direct citations supporting this claim
- Break condition: If fine-tuning tasks require specialized knowledge not captured in pretraining, or if the learning rate schedule doesn't match pretraining dynamics

### Mechanism 3
- Claim: Decoder-only architecture is sufficient for NAR text generation tasks
- Mechanism: The bidirectional attention in the decoder-only model provides sufficient context modeling capability without needing a separate encoder
- Core assumption: The decoder-only bidirectional attention can effectively capture both left and right context needed for generation
- Evidence anchors:
  - [section 5.1]: "We conduct preliminary experiments on WMT14 using EN-DE on both encoder-decoder and decoder only model. The max BLEU number for encoder-decoder and decoder only model have negligible difference"
  - [section 3.1]: "Our baseline model utilizes a decoder-only transformer-based architecture with bidirectional self-attention"
  - [corpus]: Weak evidence - limited related work in corpus
- Break condition: If downstream tasks require explicit source-side encoding (e.g., translation with significant reordering), or if bidirectional context is insufficient for the generation task

## Foundational Learning

- Concept: Denoising autoencoders
  - Why needed here: SUNDAE is built on denoising autoencoder principles, understanding how noise injection and reconstruction works is fundamental
  - Quick check question: What's the difference between standard denoising and unrolled denoising in SUNDAE?

- Concept: Prefix language modeling
  - Why needed here: The pretraining uses prefix LM objective with bidirectional attention, understanding this helps grasp the pretraining approach
  - Quick check question: How does prefix LM differ from standard causal LM in terms of attention mask?

- Concept: Non-autoregressive generation
  - Why needed here: The entire work focuses on NAR models, understanding their limitations and advantages is crucial
  - Quick check question: What are the key quality gaps NAR models typically face compared to autoregressive models?

## Architecture Onboarding

- Component map: T5-base decoder-only transformer with 12 layers, hidden dimension 768, bidirectional self-attention, SUNDAE unrolled denoising pretraining, prefix LM objective
- Critical path: Pretraining (C4, 1M steps) → Fine-tuning (downstream tasks, 50k steps) → Inference (10-step unrolling)
- Design tradeoffs: Decoder-only vs encoder-decoder (simplicity vs potential performance), bidirectional attention vs causal (quality vs parallelism), unrolled denoising vs standard denoising (robustness vs complexity)
- Failure signatures: Poor performance on tasks requiring strong source-side conditioning, instability during fine-tuning, degradation when scaling to longer sequences
- First 3 experiments:
  1. Train decoder-only model with standard prefix LM on C4, evaluate on XSum to establish baseline
  2. Add unrolled denoising to pretraining, compare sample efficiency and final performance
  3. Test different unrolling step counts (5, 10, 15) to find optimal balance between quality and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of non-autoregressive models scale with increasing data size and model parameters compared to autoregressive models?
- Basis in paper: [explicit] The paper mentions this as a natural question for future work, noting that larger autoregressive models benefit from scaling.
- Why unresolved: The paper only investigates a decoder-only T5 base model (12 layers, 768 hidden dimension) and does not explore scaling effects.
- What evidence would resolve it: Experiments training larger NAR models (e.g., T5 large, T5 3B) on proportionally larger datasets, comparing scaling curves with equivalent autoregressive models.

### Open Question 2
- Question: Does pretraining NAR models on more diverse or specialized datasets (beyond C4) lead to further performance improvements on downstream tasks?
- Basis in paper: [inferred] The paper uses only the C4 dataset for pretraining and notes its diversity as an advantage, but doesn't explore alternative pretraining corpora.
- Why unresolved: The study is limited to a single pretraining dataset without ablation studies on dataset diversity or domain specificity.
- What evidence would resolve it: Comparative experiments pretraining on multiple datasets (e.g., specialized domain corpora, multilingual data) and measuring downstream task performance differences.

### Open Question 3
- Question: How does the inclusion of length prediction modules affect the performance of pretraining NAR models?
- Basis in paper: [explicit] The paper explicitly omits length prediction to focus on pretraining effects, noting it's used in other NAR techniques.
- Why unresolved: The study deliberately excludes this component to isolate pretraining effects, leaving the interaction between length prediction and pretraining unexplored.
- What evidence would resolve it: Experiments comparing NAR models with and without length prediction modules, both with and without pretraining, on the same downstream tasks.

## Limitations
- Limited empirical validation across tasks: The paper primarily evaluates on XSum and SQuAD question generation, with only preliminary results on WMT14, leaving questions about generalizability to other generation tasks.
- Implementation complexity of SUNDAE: The unrolled denoising algorithm is described but not fully detailed, with critical hyperparameters like the corruption function remaining unspecified, creating barriers to reproduction.
- Scalability and efficiency trade-offs: While claiming 1.17x speedup, the pretraining process involves 1M steps with unrolled denoising, which is computationally expensive and may offset runtime efficiency benefits.

## Confidence
- High confidence: The empirical improvements on XSum summarization are well-documented and substantial, with clear state-of-the-art performance.
- Medium confidence: The sample efficiency claims during fine-tuning are supported by experimental evidence but could benefit from more rigorous ablation studies.
- Low confidence: The claim that decoder-only architecture is sufficient for all NAR generation tasks is based on preliminary WMT14 experiments only, requiring broader task coverage for validation.

## Next Checks
1. **Task diversity validation**: Replicate the experiments on at least three additional NAR generation tasks including machine translation (with different language pairs), data-to-text generation (E2E dataset), and long-form content generation (WikiText-103) to verify the decoder-only architecture's generalizability.

2. **Ablation on pretraining objectives**: Systematically compare the SUNDAE unrolled denoising approach against standard denoising autoencoders and prefix LM pretraining in isolation to quantify the contribution of each component to final performance.

3. **Scaling analysis**: Evaluate the method's performance and efficiency at different model scales (T5-small, T5-large, T5-3B) to determine whether the pretraining benefits scale proportionally and to identify any architectural bottlenecks that emerge at larger scales.