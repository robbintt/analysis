---
ver: rpa2
title: Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve
  Better Generalization
arxiv_id: '2307.11007'
source_url: https://arxiv.org/abs/2307.11007
tags:
- lemma
- sharpness
- training
- will
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates whether sharpness of the loss landscape
  is a sufficient condition for generalization in over-parameterized neural networks.
  It identifies three scenarios for 2-layer ReLU networks: (1) all flattest minimizers
  provably generalize, (2) there exist non-generalizing flattest models and sharpness
  minimization algorithms fail to generalize, and (3) non-generalizing flattest models
  exist but sharpness minimization algorithms still find generalizing models.'
---

# Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization

## Quick Facts
- arXiv ID: 2307.11007
- Source URL: https://arxiv.org/abs/2307.11007
- Authors: 
- Reference count: 40
- Primary result: Sharpness minimization algorithms don't solely minimize sharpness to achieve better generalization; the relationship depends on data distributions and model architectures.

## Executive Summary
This paper investigates whether sharpness of the loss landscape is a sufficient condition for generalization in over-parameterized neural networks. Through theoretical analysis of 2-layer ReLU networks, the authors identify three distinct scenarios where the relationship between sharpness and generalization varies. They show that while sharpness minimization algorithms like SAM can find generalizing models, they don't do so solely through sharpness reduction. The results demonstrate that flatness alone is not a sufficient condition for generalization and that other mechanisms may be involved in the generalization performance of sharpness minimization algorithms.

## Method Summary
The authors analyze 2-layer ReLU networks with different architectures (with/without bias, with simplified BatchNorm/LayerNorm) on the 2 parity xor problem. They train models using Sharpness-Aware Minimization (SAM) with batch size 1, gradient descent with weight decay, and mean squared error loss. Generalization error is measured by zero-one loss, while sharpness is quantified by the trace of the Hessian of the training loss. Theoretical bounds for Rademacher complexity and uniform convergence are derived, and empirical results are validated across various architectures and datasets.

## Key Results
- All flattest minimizers provably generalize in 2-layer ReLU networks without bias (Scenario I)
- Non-generalizing flattest models can exist, and SAM can fail to generalize (Scenario II)
- Non-generalizing flattest models can exist, but SAM can still find generalizing models (Scenario III)
- Sharpness minimization algorithms don't only minimize sharpness to achieve better generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sharpness minimization algorithms do not always minimize sharpness to achieve better generalization.
- Mechanism: SAM and similar algorithms can find generalizing models even when non-generalizing flattest models exist, due to other implicit regularization mechanisms beyond sharpness reduction.
- Core assumption: The loss landscape contains both generalizing and non-generalizing flat minima, and SAM can navigate toward generalizing ones through mechanisms not solely related to sharpness.
- Evidence anchors:
  - [abstract] "sharpness minimization algorithms do not only minimize sharpness to achieve better generalization"
  - [section] "Scenario III: Both Flattest Non-generalizing and Generalizing Models Exist, and SAM Finds the Latter"
  - [corpus] Weak evidence - corpus neighbors discuss sharpness-aware minimization but don't directly address the claim about non-sharpness mechanisms.

### Mechanism 2
- Claim: The relationship between sharpness and generalization depends on network architecture and data distribution.
- Mechanism: Small architectural changes (e.g., adding bias, changing normalization layers) can shift the regime from one where flatness implies generalization to one where it does not, affecting how sharpness minimization algorithms perform.
- Core assumption: Different architectures create different loss landscapes where the correlation between flatness and generalization varies.
- Evidence anchors:
  - [section] "Intriguingly, we also find that the answer to Question 1 subtly depends on the architectures of neural networks"
  - [section] "Simply removing the bias in the first layer turns the aforementioned negative result into a positive result"
  - [corpus] Weak evidence - corpus neighbors focus on sharpness-aware minimization but don't discuss architectural dependencies in detail.

### Mechanism 3
- Claim: Memorizing solutions can be flattest minimizers with poor generalization, but SAM can avoid them.
- Mechanism: SAM can find interpolating models with minimal sharpness that do not memorize, even when memorizing solutions exist, due to implicit bias in the optimization process.
- Core assumption: The optimization dynamics of SAM favor solutions with certain properties (e.g., low-rank features) over memorizing solutions, even when both have similar sharpness.
- Evidence anchors:
  - [section] "we found that sharpness-minimization algorithms can still generalize well even when the answer to Question 1 is false"
  - [section] "Memorizing Solutions... there exists a width n layer D MLP-Bias that is a memorizing solution... and has minimal sharpness"
  - [corpus] Weak evidence - corpus neighbors discuss sharpness-aware minimization but don't specifically address memorizing solutions.

## Foundational Learning

- Concept: Rademacher Complexity
  - Why needed here: Used to derive generalization bounds that relate sharpness to model complexity and sample size.
  - Quick check question: How does Rademacher complexity provide a measure of model complexity in the context of generalization bounds?

- Concept: Sharpness of the Loss Landscape
  - Why needed here: Central to understanding the relationship between flatness of minima and generalization performance.
  - Quick check question: What are different ways to measure sharpness, and why is the trace of the Hessian used in this paper?

- Concept: Implicit Regularization
  - Why needed here: Explains how optimization algorithms like SAM can lead to generalizing models through mechanisms beyond explicit regularization.
  - Quick check question: How does implicit regularization differ from explicit regularization, and what are examples of implicit regularization in neural network training?

## Architecture Onboarding

- Component map:
  2-layer ReLU networks with and without bias -> 2-layer ReLU networks with simplified BatchNorm and LayerNorm -> Sharpness-Aware Minimization (SAM) algorithm -> Mean squared error and logistic loss with label smoothing -> Rademacher complexity and uniform convergence bounds

- Critical path:
  1. Define the network architecture and data distribution
  2. Implement SAM with appropriate learning rate and perturbation radius
  3. Train the model and compute sharpness (trace of Hessian)
  4. Evaluate generalization performance on test data
  5. Analyze whether flatness implies generalization for the given setup

- Design tradeoffs:
  - Using 2-layer networks simplifies analysis but may not capture all phenomena in deeper networks
  - Simplified normalization layers (without mean subtraction) are used for theoretical tractability but may differ from standard implementations
  - Focusing on specific data distributions (e.g., Pxor) allows for precise theoretical results but limits generalizability

- Failure signatures:
  - SAM finds flat minima with poor generalization (Scenario II)
  - Different architectures yield different generalization behaviors despite similar sharpness (Scenarios I, II, III)
  - Memorizing solutions exist with minimal sharpness but poor generalization

- First 3 experiments:
  1. Train 2-layer ReLU MLP without bias on Pxor using SAM; verify that flattest models generalize well (Scenario I)
  2. Train 2-layer ReLU MLP with bias on Pxor using SAM; observe that flattest models may not generalize (Scenario II)
  3. Train 2-layer ReLU MLP with simplified LayerNorm on Pxor using SAM; verify that SAM finds generalizing models even when non-generalizing flattest models exist (Scenario III)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what precise conditions does flatness provably imply generalization for neural networks with non-linear activations?
- Basis in paper: [explicit] The paper proves that for 2-layer ReLU networks without bias on the XOR problem, all flattest minimizers provably generalize (Theorem 3.1), but adding bias allows non-generalizing flattest models to exist (Theorem 4.1).
- Why unresolved: The paper only establishes these results for specific architectures (2-layer ReLU) and data distributions (XOR). The relationship between sharpness and generalization depends on subtle details of the architecture and distribution.
- What evidence would resolve it: Formal proofs or counterexamples showing when flatness implies generalization for various architectures (deeper networks, different activation functions) and data distributions beyond the XOR problem.

### Open Question 2
- Question: What mechanisms beyond sharpness minimization enable SAM to find generalizing models even when non-generalizing flattest models exist?
- Basis in paper: [explicit] The paper shows that SAM can find generalizing models for 2-layer ReLU networks with simplified LayerNorm, even though both generalizing and non-generalizing flattest models exist (Section 5).
- Why unresolved: The paper demonstrates this phenomenon empirically but does not identify the specific mechanisms beyond sharpness minimization that enable SAM to find generalizing solutions.
- What evidence would resolve it: Theoretical analysis or empirical experiments that isolate and identify the additional mechanisms (e.g., implicit regularization, optimization dynamics) that guide SAM to generalizing solutions.

### Open Question 3
- Question: Can sharpness-aware minimization algorithms be modified to guarantee finding generalizing solutions even when non-generalizing flattest models exist?
- Basis in paper: [inferred] The paper shows that SAM sometimes finds non-generalizing solutions (Section 4) and sometimes finds generalizing solutions despite the existence of non-generalizing flattest models (Section 5).
- Why unresolved: The paper demonstrates that SAM's behavior depends on the architecture and data distribution, but does not explore modifications to SAM that could guarantee finding generalizing solutions in all cases.
- What evidence would resolve it: Theoretical analysis or empirical experiments that propose and validate modifications to SAM (e.g., additional regularization terms, constrained optimization) that ensure generalization regardless of the existence of non-generalizing flattest models.

## Limitations
- Focuses exclusively on 2-layer ReLU networks, which may not capture phenomena in deeper architectures
- Simplified normalization layers may behave differently from standard implementations
- Specific data distribution (2 parity xor) may not generalize to other problems

## Confidence
- High: Sharpness minimization algorithms can find generalizing models even when non-generalizing flattest models exist (Scenario III)
- Medium: The relationship between sharpness and generalization depends on network architecture and data distribution
- Medium: Memorizing solutions can be flattest minimizers but SAM can avoid them through implicit regularization

## Next Checks
1. Test whether the three scenarios hold for deeper ReLU networks (3+ layers) on the same 2 parity xor task
2. Implement and evaluate standard BatchNorm/LayerNorm layers instead of simplified versions to check if results persist
3. Apply the analysis to a different data distribution (e.g., random parity or Gaussian mixtures) to test robustness of the findings