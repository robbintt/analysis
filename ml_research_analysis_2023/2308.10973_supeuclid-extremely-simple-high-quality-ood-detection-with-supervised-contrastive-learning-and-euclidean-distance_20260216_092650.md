---
ver: rpa2
title: 'SupEuclid: Extremely Simple, High Quality OoD Detection with Supervised Contrastive
  Learning and Euclidean Distance'
arxiv_id: '2308.10973'
source_url: https://arxiv.org/abs/2308.10973
tags:
- detection
- resnet18
- contrastive
- methods
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of Out-of-Distribution (OoD) detection
  in neural networks, where standard confidence scores can be unreliable for inputs
  outside the training distribution. The core method replaces standard cross-entropy
  loss with Supervised Contrastive Learning (SCL) during training and uses Euclidean
  distance from feature vectors to class means as the OoD detection score.
---

# SupEuclid: Extremely Simple, High Quality OoD Detection with Supervised Contrastive Learning and Euclidean Distance

## Quick Facts
- arXiv ID: 2308.10973
- Source URL: https://arxiv.org/abs/2308.10973
- Authors: 
- Reference count: 3
- One-line primary result: Achieves competitive OoD detection performance using only ResNet18 trained with SCL and Euclidean distance scoring, requiring no complex tuning or outlier exposure.

## Executive Summary
This work presents a remarkably simple approach to Out-of-Distribution (OoD) detection that combines Supervised Contrastive Learning (SCL) training with Euclidean distance to class means as the detection score. The method demonstrates that sophisticated OoD detection can be achieved without complex pipelines, outlier exposure, or hyperparameter tuning. When tested on CIFAR-10 with SVHN and CIFAR-100 as OoD datasets, the approach achieves state-of-the-art results while maintaining exceptional simplicity.

## Method Summary
The method trains a standard ResNet18 using Supervised Contrastive Loss instead of cross-entropy loss for 500 epochs with SGD optimizer. After training, it computes class means from the feature vectors of the training set. For OoD detection, it uses the minimum Euclidean distance from a test sample's feature vector to these class means as the detection score. This approach requires no outlier exposure, pretraining, or complex scoring rules, making it exceptionally simple to implement and deploy.

## Key Results
- Achieves AUROC of 99.6% for SVHN as OoD and 92.1% for CIFAR-100 as OoD when CIFAR-10 is in-distribution
- Attains FPR95 scores of 2.2% (SVHN) and 31.5% (CIFAR-100)
- Competes with or exceeds state-of-the-art methods while requiring no complex hyperparameter tuning or specialized scoring rules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Euclidean distance from feature vectors to class means creates effective OoD separation because SCL training produces more discriminative and well-separated feature embeddings.
- Mechanism: Supervised Contrastive Learning (SCL) explicitly optimizes for features where samples from the same class cluster together while being pushed away from samples of other classes. This creates compact class clusters in feature space with larger margins between them, making Euclidean distance to nearest class mean a reliable OoD score.
- Core assumption: SCL produces more separable feature clusters than standard cross-entropy training.
- Evidence anchors: [abstract] "we use Supervised Contrastive Loss" and "Euclidean distance to class means as a scoring rule"; [section] "Instead of the standard Cross Entropy (CE) loss, we use Supervised Contrastive Loss"

### Mechanism 2
- Claim: The simplicity of using only Euclidean distance and SCL loss eliminates the need for complex scoring rules and hyperparameter tuning.
- Mechanism: By relying on a single, geometrically interpretable metric (Euclidean distance) and a standard training loss (SCL), the method avoids the brittleness and complexity of multi-stage pipelines, ensemble methods, or specialized scoring functions that require careful calibration.
- Core assumption: Simpler methods can achieve competitive performance when the underlying training objective aligns well with the detection task.
- Evidence anchors: [abstract] "This may obviate the need in some cases for more sophisticated methods or larger models"; [section] "Our contribution is as follows: State-of-the-art OoD detection using only ResNet18 trained with out-of-the-box Supervised Contrastive Loss (SCL) loss... and requiring no outlier exposure, complicated hyperparameter tuning, pretraining, additional test-time computation, ensembling or complex scoring rules."

### Mechanism 3
- Claim: The combination of SCL training and Euclidean distance scoring creates a strong baseline that can exceed more complex methods.
- Mechanism: SCL's ability to create well-separated features, combined with the direct interpretability of Euclidean distance to class means, provides a powerful yet simple OoD detection framework that performs competitively without the overhead of more complex approaches.
- Core assumption: The alignment between SCL's optimization objective and the OoD detection task creates a synergistic effect.
- Evidence anchors: [abstract] "it is possible to achieve results that can exceed many of these state-of-the-art methods with a very simple method"; [section] "Our results in Table 1 demonstrate that an out-of-the-box ResNet18 trained with SCL and using Euclidean distance to class means as a scoring rule produces results that are competitive with or exceed many state-of-the-art baselines."

## Foundational Learning

- Concept: Supervised Contrastive Learning (SCL)
  - Why needed here: SCL is the core training method that creates well-separated feature clusters, which is essential for the Euclidean distance scoring to work effectively for OoD detection.
  - Quick check question: How does SCL differ from standard cross-entropy loss in terms of how it treats samples from different classes during training?

- Concept: Feature space and embeddings
  - Why needed here: Understanding that the model maps images to feature vectors (embeddings) in a high-dimensional space is crucial for grasping why Euclidean distance to class means serves as an OoD score.
  - Quick check question: What does it mean when we say an image's "feature vector" and how is it different from the final class prediction?

- Concept: OoD detection metrics (AUROC, FPR95)
  - Why needed here: These are the standard evaluation metrics used to quantify how well the method separates in-distribution from out-of-distribution examples.
  - Quick check question: If a method has an AUROC of 0.95, what does this tell us about its OoD detection performance?

## Architecture Onboarding

- Component map: Input Image -> ResNet18 Feature Extractor -> SCL Loss (Training) -> Class Means (Post-training) -> Euclidean Distance Scoring -> OoD Score

- Critical path:
  1. Forward pass through ResNet18 to get features
  2. Compute SCL loss during training
  3. After training, compute class means from training features
  4. For each test image, compute Euclidean distance to nearest class mean
  5. Use distance as OoD score and evaluate with AUROC/FPR95

- Design tradeoffs:
  - Simplicity vs. potential performance ceiling (more complex methods might capture subtler distinctions)
  - No outlier exposure means the model doesn't see OoD examples during training, which is simpler but might miss some nuances
  - Using only ResNet18 keeps the model lightweight but may limit representational capacity compared to larger models

- Failure signatures:
  - Poor AUROC/FPR95 scores indicating the method isn't separating ID and OoD well
  - Feature visualizations showing overlapping or poorly separated class clusters
  - High variance in performance across different random initializations

- First 3 experiments:
  1. Train the model with standard cross-entropy loss instead of SCL and compare OoD detection performance to establish the benefit of SCL.
  2. Visualize the feature space using t-SNE or UMAP to qualitatively assess how well SCL separates classes compared to cross-entropy.
  3. Test the method on a different OoD dataset (e.g., LSUN or iSUN) to evaluate generalization beyond SVHN and CIFAR-100.

## Open Questions the Paper Calls Out
1. How does SupEuclid's performance scale with larger models like WideResNet or Vision Transformers compared to its ResNet18 baseline?
2. What is the relationship between the number of training epochs (500 vs 1000) and OoD detection performance when using SCL?
3. How does SupEuclid's Euclidean distance scoring compare to Mahalanobis distance when applied to SCL-trained features?
4. Does the simplicity of SupEuclid (no pretraining, no outlier exposure, no hyperparameter tuning) come at the cost of robustness to different types of distribution shifts?

## Limitations
- Evaluation limited to specific dataset pairs (CIFAR-10/SVHN/CIFAR-100), leaving questions about performance on other OoD datasets or in different domains
- Ablation studies focus on different distance metrics but don't extensively explore the impact of architectural choices beyond ResNet18
- The claim that "many cases" can be addressed with this simple approach is qualitative and not rigorously quantified across diverse scenarios

## Confidence
- **High confidence**: The core mechanism of using SCL training with Euclidean distance scoring is clearly specified and well-supported by the results presented. The method's simplicity and effectiveness relative to baselines is well-demonstrated for the specific datasets tested.
- **Medium confidence**: The generalizability of the approach to other datasets, model architectures, and OoD detection scenarios is plausible but not extensively validated. The claim about obviating the need for more sophisticated methods is supported by the results but lacks comprehensive comparative analysis across diverse benchmarks.
- **Low confidence**: The specific reasons why this approach outperforms or underperforms certain baselines are not deeply analyzed. The interaction between SCL's clustering properties and the effectiveness of Euclidean distance as a scoring rule, while mechanistically sound, lacks direct empirical validation through feature space analysis.

## Next Checks
1. Test the method on additional OoD benchmarks (e.g., LSUN, iSUN, TinyImageNet) to assess generalizability beyond the CIFAR-10/SVHN/CIFAR-100 setup.
2. Systematically vary SCL hyperparameters (temperature, number of negative samples) and compare against different training objectives to isolate the contribution of SCL to performance.
3. Visualize and quantify the clustering properties of features learned with SCL vs. cross-entropy (e.g., using t-SNE, class separation metrics) to empirically validate the proposed mechanism.