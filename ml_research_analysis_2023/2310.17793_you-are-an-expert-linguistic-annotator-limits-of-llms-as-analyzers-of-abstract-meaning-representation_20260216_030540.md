---
ver: rpa2
title: '"You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract
  Meaning Representation'
arxiv_id: '2310.17793'
source_url: https://arxiv.org/abs/2310.17793
tags:
- event
- arg1
- parse
- arguments
- parses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (GPT-3, ChatGPT, GPT-4) were tested for their
  ability to generate Abstract Meaning Representation (AMR) parses directly and via
  natural language queries. In zero-shot settings, models produced basic AMR format
  but made frequent errors, with none generating fully accurate parses.
---

# "You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract Meaning Representation

## Quick Facts
- arXiv ID: 2310.17793
- Source URL: https://arxiv.org/abs/2310.17793
- Reference count: 25
- Large language models (GPT-3, ChatGPT, GPT-4) cannot reliably generate accurate AMR parses in zero-shot settings, achieving 0% full parse acceptability across 30 test sentences

## Executive Summary
This study evaluates the ability of large language models (GPT-3, ChatGPT, GPT-4) to generate Abstract Meaning Representation (AMR) parses directly and via natural language queries. In zero-shot settings, models can reproduce basic AMR format structure but make frequent semantic errors, with none achieving fully accurate parses. While few-shot demonstrations improve main event identification, overall accuracy remains limited, with models correctly capturing core event-argument structures in only 40-50% of cases. Natural language queries elicit similar error patterns, suggesting the limitations stem from fundamental semantic analysis capabilities rather than output format constraints.

## Method Summary
The study tested three LLM models (GPT-3, ChatGPT, GPT-4) on 30 test sentences from AMR 3.0, Little Prince, and 2023 web sources. Models were evaluated in zero-shot settings with basic AMR parsing prompts, few-shot demonstrations using example parses, and natural language query conditions asking for event-argument identification. Outputs were manually evaluated using two-tiered semantic criteria: Level 1 (basic format, top node, main relation, overall accept) and Level 2 (event arguments/modifiers, arg mods, extra mods). The evaluation focused on semantic accuracy rather than format compliance, with no model fine-tuning or additional training involved.

## Key Results
- Zero-shot AMR parsing: Models achieved 0% full parse acceptability with only basic format reproduction (>70% basic AMR structure)
- Few-shot improvement: Event identification improved but overall accuracy remained limited (40-50% core event-argument structure capture)
- Natural language queries: Produced similar error patterns to direct parse generation, indicating fundamental semantic analysis limitations
- Best performance: Core event-argument triplets (subject-verb-object) were most reliably identified, while modifiers and complex structures showed higher error rates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs can reproduce AMR format structure through surface-level pattern recognition without understanding semantic roles.
- **Mechanism**: Models learn to output graph-like structures with nodes and edges because AMR parses appear frequently in pre-training data, allowing them to mimic the visual format.
- **Core assumption**: The presence of AMR annotations in pre-training data enables zero-shot format reproduction.
- **Evidence anchors**:
  - [abstract] "models can reliably reproduce the basic format of AMR"
  - [section] "we see that for all models a majority of outputs (>70%) show basic AMR format"
  - [corpus] Weak - no direct corpus evidence that AMR annotations were in pre-training data
- **Break condition**: If prompts require deeper semantic reasoning beyond surface pattern matching, format reproduction fails.

### Mechanism 2
- **Claim**: Few-shot demonstrations improve event-argument structure recognition by providing explicit examples of predicate-argument mappings.
- **Mechanism**: Demonstrations show how verbs map to concept nodes and how syntactic arguments map to AMR ARG0, ARG1 roles, enabling models to replicate this structure.
- **Core assumption**: Models can generalize from few examples to new sentences with similar semantic structures.
- **Evidence anchors**:
  - [abstract] "few-shot demonstrations improved main event identification but not overall accuracy"
  - [section] "models have limited reliability in identifying a given event's arguments and modifiers (40-50%)"
  - [corpus] Weak - no evidence that few-shot examples specifically improve semantic understanding
- **Break condition**: When sentences contain complex argument structures or non-standard predicate-argument mappings not covered in demonstrations.

### Mechanism 3
- **Claim**: Natural language queries elicit similar error patterns to parse generation because both require semantic analysis capabilities.
- **Mechanism**: Both settings require identifying primary events, arguments, and modifiers, revealing fundamental limitations in semantic analysis rather than output format constraints.
- **Core assumption**: The semantic analysis required for both NL responses and parse generation is equally difficult for models.
- **Evidence anchors**:
  - [abstract] "Eliciting natural language responses produces similar patterns of errors"
  - [section] "we see that the overall patterns of accuracy are strikingly similar to those in the few-shot case"
  - [corpus] Weak - no corpus evidence supporting this claim about semantic analysis limitations
- **Break condition**: If models show differential performance between NL and parse settings on complex semantic structures.

## Foundational Learning

- **Concept: Semantic parsing vs surface parsing**
  - Why needed here: Understanding the difference between capturing meaning structure vs surface syntax is crucial for interpreting model limitations
  - Quick check question: Can you distinguish between a parse that captures "who did what to whom" versus one that just follows syntactic patterns?

- **Concept: Abstract Meaning Representation formalism**
  - Why needed here: AMR provides the target representation for semantic structure that models are being evaluated on
  - Quick check question: What are the key components of an AMR graph and how do they represent sentence meaning?

- **Concept: Zero-shot vs few-shot learning capabilities**
  - Why needed here: Understanding what models can do without training vs with minimal examples explains performance differences
  - Quick check question: What types of tasks typically show strong zero-shot performance in LLMs and why might AMR parsing be different?

## Architecture Onboarding

- **Component map**: Input sentence → prompt generation → LLM API call → parse output → format validation → semantic evaluation → result aggregation
- **Critical path**: Input sentence → prompt generation → LLM API call → parse output → format validation → semantic evaluation → result aggregation
- **Design tradeoffs**: Manual evaluation provides detailed insights but limits sample size; automated evaluation would enable larger samples but miss nuanced semantic errors
- **Failure signatures**: Common failures include missing ARG nodes, incorrect event identification, misplaced modifiers, and failure to capture clausal relations in top node
- **First 3 experiments**:
  1. Test zero-shot AMR generation on simple sentences with clear subject-verb-object structure
  2. Test few-shot performance using sentences similar to training examples but with different predicates
  3. Compare NL response performance vs parse generation on sentences with complex argument structures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific techniques could effectively improve LLMs' semantic analysis capabilities for AMR parsing beyond basic prompt engineering?
- Basis in paper: [explicit] The paper concludes that "additional fiddling and clever manipulations can further improve the outputs of these models" but notes that more involved techniques are needed to adapt models effectively for AMR parsing.
- Why unresolved: The paper focuses on out-of-the-box capabilities and doesn't explore advanced adaptation methods like fine-tuning on AMR datasets, specialized loss functions, or architectural modifications.
- What evidence would resolve it: Empirical comparison of different adaptation approaches (few-shot learning with optimal prompt engineering, fine-tuning on AMR data, domain-specific pretraining, specialized architectures) showing which yields the highest accuracy on AMR parsing benchmarks.

### Open Question 2
- Question: How do LLMs' semantic analysis limitations for AMR compare to their performance on other semantic parsing formalisms?
- Basis in paper: [inferred] The paper notes in Limitations that a "detailed comparison with models' success in other (likely simpler) semantic or syntactic parsing formalisms" would be valuable, suggesting this comparison hasn't been made.
- Why unresolved: The paper focuses exclusively on AMR without benchmarking against other semantic representations like UCCA, SDP, or even simpler domain-specific formalisms.
- What evidence would resolve it: Systematic evaluation of the same LLMs on multiple semantic parsing tasks showing relative strengths and weaknesses across different formalisms.

### Open Question 3
- Question: What underlying knowledge or capabilities do LLMs possess that enable them to correctly identify some semantic structures (like core event-argument triplets) while failing at others?
- Basis in paper: [explicit] The paper observes that "models show the most reliable performance with core event-argument triplets for individual verbs, most often corresponding to subject-verb-object triplets" while being less reliable elsewhere.
- Why unresolved: The paper identifies this pattern but doesn't investigate what linguistic knowledge or pretraining data enables this specific competence.
- What evidence would resolve it: Analysis of model internal representations (attention patterns, neuron activations) during successful versus unsuccessful semantic parsing, combined with analysis of pretraining data distribution to identify correlations.

## Limitations
- Manual evaluation introduces potential subjectivity despite Level 1/2 criteria framework
- Sample size of 30 sentences may not be representative of broader AMR parsing challenges
- Study does not explore fine-tuning or specialized training to improve LLM performance

## Confidence

- **High Confidence**: Models can reproduce basic AMR format structure but fail at semantic accuracy (supported by clear evidence of >70% basic format reproduction with 0% full parse acceptability)
- **Medium Confidence**: Few-shot demonstrations improve event identification but not overall accuracy (based on observed patterns, though the mechanism remains unclear)
- **Medium Confidence**: Natural language queries produce similar error patterns to parse generation (supported by similar accuracy patterns, but mechanism not fully explained)

## Next Checks

1. **Automated vs Manual Evaluation Comparison**: Run the same evaluation pipeline using automated AMR parsing metrics (Smatch, etc.) on a subset of outputs to assess alignment with manual expert judgments and determine if scale-up is feasible

2. **Fine-tuning Experiment**: Fine-tune a base LLM on AMR-annotated data using the same test sentences to determine whether current limitations are fundamental or can be overcome through training

3. **Cross-linguistic Validation**: Test the same LLM models on AMR parsing for non-English languages where semantic structures differ significantly from English to determine if observed limitations are language-specific or general across linguistic structures