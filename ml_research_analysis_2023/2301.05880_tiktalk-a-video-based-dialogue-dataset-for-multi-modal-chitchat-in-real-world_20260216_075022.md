---
ver: rpa2
title: 'TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World'
arxiv_id: '2301.05880'
source_url: https://arxiv.org/abs/2301.05880
tags:
- dialogue
- multi-modal
- responses
- arxiv
- tiktalk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TikTalk, a Chinese multi-modal dialogue dataset
  containing 38K videos and 367K associated conversations collected from a video social
  platform. The dataset captures real-world chitchat scenarios where users engage
  in spontaneous conversations after watching videos, providing multi-modal context
  including visual, audio, and textual information.
---

# TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World

## Quick Facts
- **arXiv ID**: 2301.05880
- **Source URL**: https://arxiv.org/abs/2301.05880
- **Reference count**: 12
- **Primary result**: Introduces TikTalk, a Chinese multi-modal dialogue dataset with 38K videos and 367K conversations collected from Douyin

## Executive Summary
This paper introduces TikTalk, a Chinese multi-modal dialogue dataset containing 38K videos and 367K associated conversations collected from Douyin (Chinese TikTok). The dataset captures real-world chitchat scenarios where users engage in spontaneous conversations after watching videos, providing multi-modal context including visual, audio, and textual information. The authors evaluate several baseline models including Livebot, DialoGPT, and Maria variants, finding that models incorporating common-sense conversational knowledge (Maria-C3KG) perform best. Results show that while models achieve BLEU-2 of 4.69 and BLEU-4 of 1.61, all models fall short of human performance, indicating significant room for improvement in multi-modal dialogue generation.

## Method Summary
The TikTalk dataset is constructed by collecting videos and associated conversations from Douyin, a Chinese video social platform. Videos are processed to extract visual features using ResNet101 from consecutive frames (max 50) and audio features using Viggish (max 180 frames), while textual context is tokenized (max 180 tokens). The dataset contains 38K videos with 367K conversations, where each video is associated with 1-12 conversations. The authors evaluate transformer-based models (Maria variants) with visual, audio, and knowledge inputs, training with MCP/MRP/MKP objectives and pre-trained on BERT-base-Chinese. Evaluation uses automatic metrics including BLEU-2/4, Meteor, Rouge-L, CIDEr for relevance; Recall@5/10, Mean Rank for ranking; and Dist-1/2 for diversity.

## Key Results
- Models incorporating common-sense conversational knowledge (Maria-C3KG) perform best with BLEU-2 of 4.69, BLEU-4 of 1.61, and diversity scores of 5.88 (Dist-1) and 32.82 (Dist-2)
- 42% of responses are related to visual context, 10% to audio context, and 34% require external knowledge
- All models fall short of human performance, indicating significant room for improvement in multi-modal dialogue generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal dialogue grounded in video provides richer context than image-only datasets, leading to more diverse conversations.
- Mechanism: Video contains temporal visual, audio, and textual information, enabling conversations that reference dynamic events, sounds, and multiple context types simultaneously.
- Core assumption: The richness of video content directly translates to richer conversational diversity and complexity.
- Evidence anchors:
  - [abstract] "The richer context types in TikTalk lead to more diverse conversations"
  - [section] "Various multi-modal information of video, such as dynamic images, audio, etc., provide context of diverse topics"
  - [corpus] Corpus analysis shows 42% visual context, 10% audio context, 34% external knowledge - significantly richer than image-only datasets
- Break condition: If video content becomes too homogeneous or if audio/visual information is not properly extracted and aligned, the multi-modal advantage diminishes.

### Mechanism 2
- Claim: Incorporating external knowledge graphs improves dialogue response quality by providing common-sense reasoning capabilities.
- Mechanism: Maria-C3KG model retrieves relevant conversational knowledge from C3KG knowledge graph and uses it as implicit context during response generation, enabling more informed and contextually appropriate responses.
- Core assumption: External knowledge graphs contain relevant conversational patterns that can be effectively retrieved and applied to specific dialogue contexts.
- Evidence anchors:
  - [abstract] "models incorporating common-sense conversational knowledge (Maria-C3KG) perform best"
  - [section] "Maria-C3KG introduces common-sense conversational knowledge as input based on Maria"
  - [corpus] 34% of responses require external knowledge, indicating knowledge is frequently needed
- Break condition: If knowledge retrieval is noisy or irrelevant, or if the knowledge base doesn't cover the conversation domains adequately.

### Mechanism 3
- Claim: Pre-training on large-scale conversational corpora improves ranking performance for dialogue response selection.
- Mechanism: DialoGPT's pre-training on conversational data enables it to better distinguish between appropriate and inappropriate responses during ranking tasks.
- Core assumption: Pre-training on relevant conversational data transfers to improved performance on ranking tasks in the target domain.
- Evidence anchors:
  - [section] "Compared with DialoGPT, Maria performs better in relevance and diversity metrics, while DialoGPT outperforms in the ranking metrics"
  - [section] "it is pre-trained on a large number of textual conversation corpora, and could rank various correct responses better"
  - [corpus] Weak - the corpus doesn't provide detailed evidence about pre-training effectiveness
- Break condition: If the pre-training data distribution significantly differs from the target domain, or if the ranking task requires domain-specific knowledge not covered in pre-training.

## Foundational Learning

- Concept: Multi-modal information fusion
  - Why needed here: The dataset contains visual, audio, and textual information that must be effectively combined for response generation
  - Quick check question: What are the key challenges in aligning temporal audio features with static visual features?

- Concept: Knowledge graph retrieval and integration
  - Why needed here: The dataset shows high reliance on external knowledge, requiring effective retrieval and incorporation of relevant knowledge triples
  - Quick check question: How does the model determine which knowledge triples are most relevant to a given conversation context?

- Concept: Dialogue evaluation metrics
  - Why needed here: Understanding the different evaluation perspectives (relevance, ranking, diversity) is crucial for proper model assessment
  - Quick check question: Why might a model perform well on diversity metrics but poorly on relevance metrics?

## Architecture Onboarding

- Component map: Video preprocessing -> Multi-modal feature extraction (video frames, audio features, text) -> Context encoding -> Knowledge retrieval (optional) -> Response generation -> Evaluation
- Critical path: Video/audio processing -> Feature alignment -> Context encoding -> Response generation
- Design tradeoffs: Richer multi-modal input vs. increased model complexity and training difficulty; knowledge integration vs. retrieval accuracy
- Failure signatures: Low diversity scores suggest generic responses; poor ranking performance indicates inability to distinguish appropriate responses; knowledge integration failures show up as irrelevant or noisy context incorporation
- First 3 experiments:
  1. Baseline: Run Maria model on TikTalk to establish performance baseline across all metrics
  2. Ablation: Remove video frames to assess impact of visual information
  3. Knowledge integration: Implement simple knowledge retrieval and measure impact on response quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can audio information be effectively extracted and aligned with visual and textual information for multi-modal dialogue generation?
- Basis in paper: [inferred] The paper shows that Maria-Audio performs worse than the original Maria model, suggesting that simply adding audio features leads to noise in the dialogues. The authors note that "How to introduce audio information needs to be carefully considered."
- Why unresolved: Current methods for incorporating audio in multi-modal dialogue systems appear to add noise rather than improve performance. The paper demonstrates that while audio is used in 10% of TikTalk responses, simply adding audio features degrades performance.
- What evidence would resolve it: Development of an audio extraction and alignment method that outperforms Maria-Audio on TikTalk, showing improved relevance metrics (BLEU, Meteor) while maintaining or improving diversity scores (Dist-1, Dist-2).

### Open Question 2
- Question: What is the optimal way to incorporate external knowledge (such as C3KG) into multi-modal dialogue models for real-world chitchat scenarios?
- Basis in paper: [explicit] The paper shows Maria-C3KG performs better than Maria-ATOMIC, suggesting that the dialogue flows in C3KG are more effective than social commonsense knowledge in ATOMIC for driving conversations.
- Why unresolved: While introducing C3KG improves performance, the paper notes that consecutive video frames may lack understanding of fine-grained visual contents, leading to less relevance of visuals than external knowledge when generating responses.
- What evidence would resolve it: A model that effectively balances visual context and external knowledge to outperform Maria-C3KG on all metrics, particularly showing improved performance on responses that require external knowledge (the 34% identified in the dataset analysis).

### Open Question 3
- Question: How can multi-modal dialogue models be improved to match human performance in generating personalized and contextually appropriate responses?
- Basis in paper: [explicit] The paper concludes that "all models fall short of human performance" and that "There is still a large room for improvement on TikTalk."
- Why unresolved: Current models, including those with LLM visual extensions, cannot solve all challenges well. The case studies show that while some models occasionally produce reasonable responses, they stay far from meeting expectations for real-world multi-modal dialogues.
- What evidence would resolve it: A model that significantly closes the gap between current performance and human-level responses on TikTalk, demonstrated through both automatic metrics and human evaluation showing responses comparable to or better than human-generated responses in the dataset.

## Limitations
- The dataset is collected from Douyin, a Chinese video social platform, which may introduce cultural and linguistic biases limiting generalizability to other regions or languages
- Automatic evaluation metrics may not fully capture the quality of multi-modal dialogue responses, particularly regarding context appropriateness and engagement
- Knowledge integration effectiveness depends heavily on the quality and coverage of the C3KG knowledge base, with uncertain performance in knowledge-poor scenarios

## Confidence
- **High Confidence**: Dataset construction methodology and basic statistics (38K videos, 367K conversations, context type distribution) are well-documented and verifiable
- **Medium Confidence**: Models incorporating common-sense conversational knowledge perform best, but evaluation could benefit from more comprehensive human studies
- **Low Confidence**: Generalizability of findings to other multi-modal dialogue scenarios and long-term effectiveness of knowledge graph integration require further validation across different datasets and cultural contexts

## Next Checks
1. **Cross-Cultural Validation**: Test the TikTalk-trained models on multi-modal dialogue datasets from other cultural contexts (e.g., Western social media platforms) to assess generalizability and identify potential cultural biases in the dataset and model performance.

2. **Knowledge Integration Robustness**: Conduct controlled experiments where knowledge retrieval is intentionally degraded or knowledge triples are replaced with irrelevant information to quantify the impact of knowledge quality on response generation and establish error bounds for the knowledge integration mechanism.

3. **Comprehensive Human Evaluation**: Implement a multi-dimensional human evaluation study that assesses not only relevance but also context appropriateness, engagement, coherence across multi-modal references, and naturalness of responses, comparing model outputs against human-generated responses across diverse conversation scenarios.