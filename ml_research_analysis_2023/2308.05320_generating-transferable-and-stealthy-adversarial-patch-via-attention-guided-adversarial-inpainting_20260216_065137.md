---
ver: rpa2
title: Generating Transferable and Stealthy Adversarial Patch via Attention-guided
  Adversarial Inpainting
arxiv_id: '2308.05320'
source_url: https://arxiv.org/abs/2308.05320
tags:
- adversarial
- image
- patch
- attacks
- patches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adv-Inpainting, a two-stage coarse-to-fine
  framework for generating natural-looking and highly transferable adversarial patches
  for face recognition attacks. The first stage employs an attention-guided StyleGAN
  (Att-StyleGAN) with a novel AAIN layer to adaptively fuse identity and texture features
  from source and target images, producing adversarial patches with improved visual
  quality and coherence.
---

# Generating Transferable and Stealthy Adversarial Patch via Attention-guided Adversarial Inpainting

## Quick Facts
- **arXiv ID**: 2308.05320
- **Source URL**: https://arxiv.org/abs/2308.05320
- **Reference count**: 40
- **Primary result**: Adv-Inpainting achieves higher attack success rates and better perceptual similarity than state-of-the-art adversarial patch methods.

## Executive Summary
This paper introduces Adv-Inpainting, a two-stage coarse-to-fine framework for generating natural-looking and highly transferable adversarial patches for face recognition attacks. The method uses an attention-guided StyleGAN with a novel AAIN layer to blend identity and texture features, producing patches that are both visually coherent and adversarial. A refinement network further enhances stealth and transferability. Experiments show significant improvements in attack success rates and perceptual similarity over existing methods.

## Method Summary
Adv-Inpainting is a two-stage framework for generating transferable and stealthy adversarial patches for face recognition. Stage 1 uses an attention-guided StyleGAN (Att-StyleGAN) with a novel AAIN layer to adaptively fuse identity and texture features from source and target images, generating a coarse adversarial patch. Stage 2 employs an Adversarial Patch Refinement Network (APR-Net) with a boundary variance loss to refine the patch, enhancing its visual coherence and transferability. The method is trained on datasets like LFW, CelebA, and FFHQ, and evaluated using attack success rate (ASR) and perceptual similarity (LPIPS distance).

## Key Results
- Adv-Inpainting achieves significantly higher attack success rates on black-box face recognition models compared to state-of-the-art adversarial patch attacks.
- The method produces adversarial patches with superior perceptual similarity, as measured by LPIPS distance, indicating better visual coherence.
- Notably outperforms other methods in transferability, especially on FaceNet, and generates patches that blend naturally with their surroundings.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Attention-guided adaptive instance normalization (AAIN) reduces stylistic gaps between source and target images.
- **Mechanism**: AAIN uses an attention map to blend texture embeddings from the source image with identity embeddings from the target image, ensuring the generated patch matches both the identity and the visual style of the background.
- **Core assumption**: Attention maps capture the relevant contextual information for blending styles and identities in the patch region.
- **Evidence anchors**:
  - [abstract] "AAIN layer to merge them via background-patch cross-attention maps"
  - [section 3.1] "We use the feature map Fp of source image, the normalized feature map Fin from the previous block and the mask M as the prior information to get the attention map Dh"
- **Break Condition**: If the attention map fails to align with semantic boundaries, the patch will appear incoherent.

### Mechanism 2
- **Claim**: Two-stage coarse-to-fine framework improves both transferability and visual coherence.
- **Mechanism**: The first stage generates a patch with plausible content via Att-StyleGAN, the second stage refines the patch boundaries using APR-Net to make it blend naturally with the surroundings.
- **Core assumption**: High-level adversarial perturbation can be generated first and then refined without losing its attack effectiveness.
- **Evidence anchors**:
  - [abstract] "two-stage coarse-to-fine framework... adversarial patch refinement network (APR-Net) with a boundary variance loss"
  - [section 3.2] "The second stage refines the adversarial patch to make it more consistent with the surrounding area"
- **Break Condition**: If refinement destroys the adversarial signal, transferability will drop.

### Mechanism 3
- **Claim**: Boundary variance loss enforces smooth transitions between patch and background.
- **Mechanism**: Computes pixel-wise variance differences along patch boundaries and penalizes abrupt changes, making the patch visually seamless.
- **Core assumption**: Human perception is sensitive to sharp transitions at patch edges.
- **Evidence anchors**:
  - [section 3.3] "computes the variance inside and outside the patch boundaries"
  - [section 4.2] "APR-Net to blend the generated patch with the original image while maintaining the adversarial attributes"
- **Break Condition**: If variance loss is too strong, adversarial efficacy may be lost.

## Foundational Learning

- **Concept**: Adversarial patch generation and transferability
  - **Why needed here**: This paper focuses on creating transferable adversarial patches that fool black-box models, a non-trivial task requiring understanding of both patch design and transferability mechanisms.
  - **Quick check question**: What makes a patch transferable across models?

- **Concept**: Style transfer and adaptive instance normalization
  - **Why needed here**: The proposed AAIN layer builds on AdaIN to blend styles; understanding how style transfer works is key to grasping the method.
  - **Quick check question**: How does AdaIN enable real-time style transfer?

- **Concept**: Image inpainting and contextual attention
  - **Why needed here**: The framework borrows from inpainting techniques to fill patches coherently; contextual attention helps ensure seamless blending.
  - **Quick check question**: What role does contextual attention play in image inpainting?

## Architecture Onboarding

- **Component map**:
  - Att-StyleGAN (Stage 1): pSp encoder → texture embeddings, Arcface encoder → identity embeddings, Generator with AAIN layers → adversarial patch
  - APR-Net (Stage 2): Unet generator → refined patch, PatchGAN discriminator → realism check
- **Critical path**:
  Source image + target image → pSp encoder + Arcface → Att-StyleGAN → xout → APR-Net → xrefine → adversarial patch
- **Design tradeoffs**:
  - More attention layers improve coherence but increase computation.
  - Stronger boundary loss improves stealth but may hurt attack success.
  - Balancing adversarial and perceptual losses is crucial for stable training.
- **Failure signatures**:
  - Patch boundaries remain visible → boundary variance loss insufficient.
  - Patch does not fool target model → adversarial loss too weak.
  - Patch style mismatched → AAIN blending failed.
- **First 3 experiments**:
  1. Train Att-StyleGAN alone and measure ASR and LPIPS.
  2. Add APR-Net and evaluate improvement in boundary coherence.
  3. Perform ablation: remove AAIN and compare ASR/LPIPS.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - **Question**: How does the Adv-Inpainting method perform against commercial face recognition systems in real-world conditions?
  - **Basis in paper**: [explicit] The paper mentions that the performance of the attack on commercial FR systems requires further evaluation.
  - **Why unresolved**: The authors acknowledge the need for evaluation on commercial systems but do not provide results.
  - **What evidence would resolve it**: Testing the Adv-Inpainting method on popular commercial face recognition APIs (e.g., AWS Rekognition, Microsoft Azure Face API) and reporting success rates under various conditions.

- **Open Question 2**
  - **Question**: What is the robustness of Adv-Inpainting-generated adversarial patches to variations in distance, angle, and lighting conditions in real-world scenarios?
  - **Basis in paper**: [explicit] The authors state that the robustness of the generated patches to different distances, angles, and illuminations in the real world needs to be measured.
  - **Why unresolved**: The paper does not provide empirical data on the method's performance under varying physical conditions.
  - **What evidence would resolve it**: Conducting experiments with physical adversarial patches under different distances, angles, and lighting conditions, and reporting the success rates of the attacks.

- **Open Question 3**
  - **Question**: How does the Adv-Inpainting method compare to other adversarial attacks in terms of transferability when targeting ensemble models or more diverse face recognition architectures?
  - **Basis in paper**: [inferred] The paper shows high transferability on several models but does not test against ensemble models or a broader range of architectures.
  - **Why unresolved**: The experiments focus on individual models, and ensemble defenses are common in practice.
  - **What evidence would resolve it**: Testing the Adv-Inpainting method against ensemble models (e.g., weighted combination of multiple FR models) and reporting the attack success rates compared to other adversarial methods.

## Limitations

- The method is limited to face recognition tasks and does not generalize to other domains.
- The attack is white-box during training, assuming knowledge of the target model's architecture.
- The computational cost of the two-stage pipeline may be prohibitive for real-time applications.

## Confidence

- **High confidence** in the two-stage framework design and the role of AAIN in improving style coherence. The architectural choices and loss functions are clearly specified and well-justified.
- **Medium confidence** in the transferability gains, as the evaluation is limited to four face recognition models. While results are strong, broader model diversity would strengthen claims.
- **Low confidence** in real-world stealth, since perceptual metrics (LPIPS) do not fully capture human judgment under varied lighting, pose, or occlusion conditions.

## Next Checks

1. Test transferability on additional face recognition models (e.g., DeepFace, InsightFace) and on non-face domains to assess generalization.
2. Conduct a user study to validate perceptual similarity scores with real human observers under varied conditions.
3. Evaluate the computational overhead and latency of the two-stage pipeline in a real-time attack scenario.