---
ver: rpa2
title: Milimili. Collecting Parallel Data via Crowdsourcing
arxiv_id: '2307.12282'
source_url: https://arxiv.org/abs/2307.12282
tags:
- language
- translation
- data
- sentences
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a crowdsourcing approach for creating parallel
  corpora for low-resource languages, demonstrating its feasibility for Chechen-Russian
  but encountering difficulties with Fula-English due to limited bilingual contributors.
  The method involves a two-stage pipeline where crowd workers first translate sentences
  and then verify translation quality through majority voting, with automatic filters
  applied for language detection and length checks.
---

# Milimili. Collecting Parallel Data via Crowdsourcing

## Quick Facts
- arXiv ID: 2307.12282
- Source URL: https://arxiv.org/abs/2307.12282
- Reference count: 3
- Key result: Successfully collected 1,078 high-quality parallel sentences for Chechen-Russian and 229 sentences for Fula-English using crowdsourced translation with majority voting

## Executive Summary
This paper presents a cost-effective crowdsourcing approach for creating parallel corpora for low-resource languages. The authors demonstrate their method on Chechen-Russian (achieving 1,078 high-quality parallel sentences) and Fula-English (229 sentences), using a two-stage pipeline of translation and verification with majority voting. The approach includes automatic filters for language detection and length checks to reduce verification workload. While the method is more affordable than professional translation, the authors acknowledge quality limitations and emphasize that crowdsourced translations are suitable only for training data, not evaluation.

## Method Summary
The method employs a two-stage crowdsourcing pipeline where workers first translate sentences and then verify translations through majority voting. Automatic filters (language detection via FastText and length ratio checks) are applied to reject clearly incorrect translations. The authors paid $0.02 per translation and $0.01 per verification set, completing the entire experiment for approximately $100. Quality control relies on having three workers verify each translation, with the majority vote determining acceptance.

## Key Results
- Collected 1,078 high-quality parallel sentences for Chechen-Russian (380 Chechen→Russian and 469 Russian→Chechen)
- Collected 229 sentences for Fula-English (53 Fula→English and 176 English→Fula)
- Total cost was approximately $100, making it more cost-effective than professional translation
- Quality control through majority voting and automatic filtering proved effective for Chechen-Russian but faced scalability issues with Fula-English due to limited bilingual contributors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage quality control via translation + verification with majority voting effectively filters low-quality translations
- Mechanism: Crowd workers first translate sentences, then separate workers verify translations through majority voting, creating a peer-review system
- Core assumption: Crowd workers are competent in both source and target languages and can reliably distinguish good from bad translations
- Evidence anchors:
  - [section]: "To ensure the accuracy of the evaluation, we gave the same translation to three workers. The majority vote was selected."
  - [abstract]: "verification quality through majority voting"
- Break condition: If bilingual contributors become scarce or if workers start using machine translation tools, quality degrades significantly

### Mechanism 2
- Claim: Automatic filters reduce human verification workload while maintaining quality
- Mechanism: Language detection and length ratio checks automatically reject translations that are clearly wrong (wrong language or unreasonable length differences)
- Core assumption: Language detection models are accurate enough to catch obvious errors and length anomalies reliably indicate poor translations
- Evidence anchors:
  - [section]: "We also conducted a length relation check between the source and translated sentences. If the difference exceeded three times, we rejected the translation."
  - [corpus]: Weak - no quantitative data provided on filter effectiveness
- Break condition: If source languages have complex morphology or free word order, length checks become unreliable

### Mechanism 3
- Claim: Low payment rates enable cost-effective data collection at scale
- Mechanism: Paying $0.02 per translation and $0.01 per verification set makes large-scale parallel corpus creation affordable compared to professional translators
- Core assumption: The quality trade-off is acceptable for training data, and workers accept low rates for contributing to language resources
- Evidence anchors:
  - [abstract]: "which is more cost-effective than hiring professional translators, albeit at the expense of quality"
  - [section]: "In fact, the total expenditure for all the experiments discussed in this context amounted to approximately $100."
- Break condition: If quality requirements increase or professional rates drop, cost advantage diminishes

## Foundational Learning

- Concept: Crowdsourcing quality control mechanisms
  - Why needed here: The system relies on non-professional workers for both translation and verification, requiring understanding of how to design effective quality control
  - Quick check question: What is the minimum number of verification votes needed to reliably filter poor translations?

- Concept: Language detection and filtering techniques
  - Why needed here: Automatic filtering reduces verification workload, requiring knowledge of language identification tools and heuristic filters
  - Quick check question: How does FastText language detection work and what are its accuracy limitations for low-resource languages?

- Concept: Cost-benefit analysis for data collection methods
  - Why needed here: The paper explicitly compares crowdsourced vs professional translation costs, requiring understanding of economic trade-offs
  - Quick check question: At what point does the quality reduction from crowdsourcing outweigh the cost savings?

## Architecture Onboarding

- Component map: Source sentences → Translation task → Automatic filtering → Verification task → Majority voting → Corpus inclusion
- Critical path: Source sentences → Translation → Language/Length check → Verification → Quality voting → Corpus
- Design tradeoffs: Cost vs quality (paying less means accepting lower quality), automation vs human oversight (filters reduce work but may miss subtle errors), verification redundancy vs efficiency (3 voters provides good reliability but increases cost)
- Failure signatures: High rejection rates from automatic filters (may indicate poor translator quality), low verification agreement (suggests ambiguous quality criteria), shortage of bilingual contributors (limits scalability)
- First 3 experiments:
  1. Run with a small set of source sentences and measure automatic filter rejection rate vs human verification agreement
  2. Test different payment rates to find the minimum acceptable compensation that maintains worker participation
  3. Pilot with a different language pair to validate scalability assumptions beyond Chechen-Russian

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the actual quality of crowdsourced translations compared to professional translations in terms of BLEU score or other MT evaluation metrics?
- Basis in paper: [explicit] The authors explicitly state that "the quality of the translated sentences cannot be compared with professional translation" and recommend using crowd-translated tasks only in training sets, but do not provide empirical quality measurements.
- Why unresolved: The authors did not conduct any MT model training or evaluation to empirically measure the impact of using crowdsourced data versus professional translations.
- What evidence would resolve it: Training NMT models using both crowdsourced and professionally translated parallel corpora for the same language pairs, then comparing their performance on standard MT evaluation metrics.

### Open Question 2
- Question: How can the scarcity of bilingual contributors for low-resource language pairs be addressed beyond simply expanding crowdsourcing platforms?
- Basis in paper: [explicit] The authors acknowledge that "scaling to other languages is exceedingly difficult due to the scarcity of users who are bilingual and willing to contribute" and suggest platform expansion as a solution, but recognize this may be insufficient.
- Why unresolved: The paper only identifies the problem and suggests one potential solution without exploring alternative strategies for recruiting or incentivizing bilingual contributors for rare language pairs.
- What evidence would resolve it: Empirical studies testing different recruitment strategies (social media outreach, university partnerships, diaspora community engagement) or incentive structures (higher pay, gamification, donation matching) for building bilingual contributor pools.

### Open Question 3
- Question: What is the optimal balance between automatic filtering and human verification to maximize translation quality while minimizing costs?
- Basis in paper: [inferred] The authors implemented automatic language detection and length checks to reduce human verification workload, but did not systematically evaluate how different filtering thresholds affect final corpus quality or cost efficiency.
- Why unresolved: The paper describes the automatic filtering approach but does not provide data on how many bad translations were caught by automatic means versus human verification, or how adjusting these parameters would impact results.
- What evidence would resolve it: Controlled experiments varying the strictness of automatic filters and the number of human verifiers per translation, measuring the trade-off between translation quality and total costs.

## Limitations
- Relies heavily on availability of bilingual contributors, making truly low-resource languages difficult to scale
- Quality control assumes workers can reliably distinguish good from bad translations without empirical validation
- Automatic filtering (language detection and length checks) may fail for morphologically rich languages or those with free word order

## Confidence
- Methodology confidence: Medium for languages with established bilingual speaker communities, Low for truly low-resource languages
- Quality control confidence: Medium, assuming workers can reliably verify translations
- Automatic filtering confidence: Medium for languages with standard morphology, may fail for complex languages

## Next Checks
1. **Quality validation study**: Conduct blind comparison between crowdsourced translations and professional translations for the same sentences to quantify quality degradation and establish acceptable quality thresholds for different use cases.

2. **Platform dependency analysis**: Test the pipeline across multiple crowdsourcing platforms and for different language pairs to measure the impact of worker availability on corpus size and quality, particularly for truly low-resource languages.

3. **Machine translation interference test**: Design experiments with controlled inputs to detect and measure the extent of machine translation tool usage among crowd workers, and assess its impact on overall corpus quality.