---
ver: rpa2
title: Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models
arxiv_id: '2305.01645'
source_url: https://arxiv.org/abs/2305.01645
tags:
- data
- distillation
- t5-small
- cost
- dist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the cost-efficiency of building compact
  NLP models by comparing two strategies: annotating more data to directly fine-tune
  a small model, versus fine-tuning a large model and distilling it to a compact model.
  The authors simulate realistic annotation and computation costs across six diverse
  NLP tasks.'
---

# Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models

## Quick Facts
- **arXiv ID:** 2305.01645
- **Source URL:** https://arxiv.org/abs/2305.01645
- **Reference count:** 27
- **Key outcome:** Distillation from T5-XXL (11B) to T5-Small (60M) is almost always more cost-efficient than annotating more data to directly train the compact model.

## Executive Summary
This paper investigates cost-efficient approaches for fine-tuning compact NLP models by comparing two strategies: annotating more data to directly fine-tune a small model versus fine-tuning a large model and distilling it to a compact model. The authors conduct extensive experiments across six diverse NLP tasks, simulating realistic annotation and computation costs. Their analysis reveals that knowledge distillation from a large T5-XXL model to a small T5-Small model consistently outperforms the annotation strategy in terms of cost-efficiency. The study also examines how optimal budget allocation between annotation and distillation shifts as more labeled data becomes available, and compares GPT-3 annotation against both strategies.

## Method Summary
The authors evaluate two strategies for building compact NLP models: Annotation (Ann.) where labeled data is collected and used to directly fine-tune T5-Small, and Distillation (Dist.) where T5-XXL is first fine-tuned on initial labeled data, then used to generate soft labels for unlabeled data which are used to train T5-Small. They conduct experiments across six diverse NLP tasks using T5 v1.1 models, varying budget levels and amounts of initial labeled data (N). Cost estimation includes per-label annotation costs and per-GPU-hour computation costs. GPT-3 annotation is also evaluated as an alternative annotation method. The analysis employs Pareto optimality to determine the most cost-effective strategy allocation under different budget constraints.

## Key Results
- Knowledge distillation from T5-XXL (11B) to T5-Small (60M) is consistently more cost-efficient than annotating additional data to directly train the compact model across all six tasks
- The optimal budget allocation shifts toward distillation as more labeled data becomes available and as the total budget increases
- GPT-3 annotation, while cheaper than human annotation, still underperforms distillation in terms of cost-efficiency despite GPT-3's larger parameter count (175B)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distillation from a large model to a compact model is almost always more cost-efficient than annotating additional data to directly fine-tune the compact model.
- **Mechanism:** The large teacher model leverages its superior knowledge representation to generate soft labels for unlabeled data, which when used to train the compact student model, achieves better performance per unit cost than manually annotating the same amount of data.
- **Core assumption:** The computational cost of distillation (fine-tuning large model + distillation) is less than the cost of human annotation for an equivalent performance gain.
- **Evidence anchors:**
  - [abstract] "distilling from T5-XXL (11B) to T5-Small (60M) is almost always a cost-efficient strategy compared to annotating more data to directly train a compact model"
  - [section 3.1] "the distillation (Dist.) strategy significantly outperforms the Ann. strategy across almost all cases for all tasks"
  - [corpus] Weak evidence - neighbor papers focus on different aspects of distillation but don't directly compare cost-efficiency against annotation
- **Break condition:** When the budget is extremely limited such that the cost of fine-tuning the large teacher model exceeds the available budget, making distillation infeasible.

### Mechanism 2
- **Claim:** The optimal allocation of budget towards computation (distillation) increases as more labeled data becomes available.
- **Mechanism:** With more initial labeled data, the large teacher model can be better fine-tuned, increasing the performance gap between teacher and student. This makes it more cost-effective to use larger amounts of unlabeled data for distillation as the teacher's superior performance justifies the additional distillation compute cost.
- **Core assumption:** The performance improvement from distillation scales with the quality of the teacher model, which depends on the amount of initial labeled data used to fine-tune it.
- **Evidence anchors:**
  - [abstract] "our analysis shows that the optimal allocation of budget towards computation (distillation) increases as more labeled data becomes available"
  - [section 4.1] "using a smaller amount of unlabeled data (U=10K) is Pareto optimal for smaller budgets, while larger unlabeled data (U=100K) maximizes utility as the budget increases"
  - [corpus] No direct evidence - neighbor papers don't address this specific budget allocation dynamic
- **Break condition:** When the teacher model's performance plateaus despite additional labeled data, or when the marginal cost of distillation exceeds the marginal performance gain.

### Mechanism 3
- **Claim:** GPT-3 annotation, while cheaper than human annotation, is still less cost-effective than distillation from a large model.
- **Mechanism:** Despite GPT-3's large parameter count (175B), it underperforms the distillation strategy because it lacks the task-specific fine-tuning and knowledge transfer that occurs when distilling from a large model trained on task-relevant data.
- **Core assumption:** The cost savings from using GPT-3 for annotation do not compensate for its inferior performance compared to distillation, when considering the total cost per performance unit.
- **Evidence anchors:**
  - [abstract] "we also investigate the cost efficiency of data annotation with GPT-3... We find that, although GPT-3 is cheaper than human annotators, fine-tuning T5-XXL and then distilling a small model is more cost-efficient than directly fine-tuning the small model with pseudo-labels from GPT-3"
  - [section 5] "it significantly underperforms the distillation (Dist.) strategy given the same budget despite its larger parameters (175B) than the teacher (11B)"
  - [corpus] Weak evidence - neighbor papers mention GPT-3 for annotation but don't compare its cost-efficiency against distillation
- **Break condition:** If GPT-3's performance improves significantly through better prompting or if the cost of distillation increases substantially relative to GPT-3's cost.

## Foundational Learning

- **Concept:** Knowledge distillation and its variants
  - **Why needed here:** Understanding how knowledge is transferred from large teacher models to compact student models is fundamental to evaluating the cost-efficiency of the distillation strategy
  - **Quick check question:** What is the difference between task-specific and general distillation, and why does this paper focus on task-specific distillation?

- **Concept:** Cost estimation and budget allocation
  - **Why needed here:** The core of this paper is comparing two strategies under fixed budgets, requiring understanding of how annotation costs and computational costs are estimated and compared
  - **Quick check question:** How are annotation costs estimated for different tasks, and what factors contribute to the computational cost of distillation?

- **Concept:** Pareto efficiency and optimization
  - **Why needed here:** The paper uses Pareto analysis to determine optimal budget allocation between annotation and computation, which requires understanding of Pareto optimality concepts
  - **Quick check question:** What does it mean for a strategy to be Pareto optimal in the context of budget allocation between annotation and distillation?

## Architecture Onboarding

- **Component map:** Data collection (annotation or unlabeled data) -> Model training (fine-tuning T5-Small directly or fine-tuning T5-XXL then distilling to T5-Small) -> Cost estimation modules
- **Critical path:** For distillation: collect initial labeled data → fine-tune large teacher model → generate soft labels on unlabeled data → distill to compact model. For annotation: collect labeled data → fine-tune compact model directly.
- **Design tradeoffs:** The main tradeoff is between the quality of supervision (human annotations vs. teacher-generated soft labels) and the cost per label/unit of supervision. Larger teacher models provide better supervision but cost more to fine-tune.
- **Failure signatures:** Distillation fails when the teacher model is undertrained due to insufficient initial labeled data, or when the unlabeled data is too dissimilar from the labeled data distribution. Annotation fails when the cost per label is too high relative to the performance gain.
- **First 3 experiments:**
  1. Replicate the main comparison on a single task (e.g., WLP) with different budget levels to verify the core finding that distillation is more cost-efficient.
  2. Test the impact of varying the initial labeled data size (N) on the optimal budget allocation between annotation and distillation.
  3. Compare different teacher model sizes (T5-Large, T5-XL, T5-XXL) to understand the impact of teacher scale on cost-efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the cost-efficiency of knowledge distillation compare when using different teacher model sizes or architectures beyond T5-XXL?
- Basis in paper: [explicit] The paper investigates varying teacher model sizes in section 4.3, finding that larger teachers generally perform better but are more expensive.
- Why unresolved: The paper only tests T5-Large, T5-XL, and T5-XXL as teacher models. Other architectures like GPT-3 or smaller models could potentially offer different cost-efficiency trade-offs.
- What evidence would resolve it: Comparative experiments measuring cost (annotation + compute) versus performance across multiple teacher architectures and sizes, including non-T5 models.

### Open Question 2
- Question: What is the impact of using unlabeled data from different domains or sources on distillation performance and cost-efficiency?
- Basis in paper: [explicit] Section D describes using unlabeled data from various sources (e.g., PROCEDURE corpus for WLP, MS MARCO for NATURAL QUESTIONS) but doesn't analyze the impact of domain mismatch.
- Why unresolved: The paper doesn't investigate whether domain-specific unlabeled data improves distillation compared to general-purpose unlabeled data, or how this affects cost-efficiency.
- What evidence would resolve it: Experiments comparing distillation performance and costs when using domain-matched versus general unlabeled data across multiple tasks.

### Open Question 3
- Question: How does the optimal budget allocation between annotation and distillation change when annotation costs vary significantly across different regions or economic contexts?
- Basis in paper: [inferred] The paper assumes fixed annotation costs from specific sources (Google Cloud, literature estimates) but doesn't explore how varying these costs would affect optimal strategies.
- Why unresolved: The cost estimates are based on specific pricing that may not reflect all economic contexts. The Pareto analysis in section 4.1 shows optimal allocations depend on budget, but doesn't explore cost variation sensitivity.
- What evidence would resolve it: Replicating the main experiments with varied annotation cost assumptions to see how optimal budget allocations shift across different economic scenarios.

## Limitations
- The findings are based on a specific set of tasks and model configurations, which may limit generalizability to other NLP tasks or model architectures.
- Cost estimates rely on fixed per-label and per-GPU-hour rates that may not reflect all deployment scenarios or economic contexts.
- The GPT-3 annotation experiments use a single prompting strategy (32-shot examples) without exploring parameter-efficient prompting techniques that could affect cost-effectiveness comparisons.

## Confidence
- **High Confidence:** The core finding that distillation is more cost-efficient than annotation for compact models, supported by extensive experiments across six diverse tasks
- **Medium Confidence:** The budget allocation dynamics (more distillation with larger budgets) are supported by analysis but would benefit from additional budget ranges
- **Low Confidence:** The GPT-3 annotation comparison, as it relies on a single prompting approach and doesn't explore alternative GPT-3 variants or prompting strategies

## Next Checks
1. **Generalization Test:** Replicate the distillation vs. annotation comparison on additional NLP tasks (e.g., sentiment analysis, summarization) to validate the findings beyond the six studied tasks.
2. **Cost Parameter Sensitivity:** Systematically vary the annotation cost per label and GPU compute cost to determine how robust the findings are to different economic scenarios.
3. **Teacher Model Exploration:** Compare distillation from multiple teacher model sizes (T5-Large, T5-XL, T5-XXL) to establish whether the 11B model is optimal or if smaller teachers could provide similar cost-efficiency.