---
ver: rpa2
title: 'Taxi1500: A Multilingual Dataset for Text Classification in 1500 Languages'
arxiv_id: '2305.08487'
source_url: https://arxiv.org/abs/2305.08487
tags:
- languages
- language
- data
- dataset
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Taxi1500, a multilingual dataset for text
  classification covering 1500+ languages. It leverages parallel Bible translations
  to create topic-based categories (recommendation, faith, description, sin, grace,
  violence) and uses crowdsourcing for English annotation, projecting labels onto
  other languages.
---

# Taxi1500: A Multilingual Dataset for Text Classification in 1500 Languages

## Quick Facts
- **arXiv ID**: 2305.08487
- **Source URL**: https://arxiv.org/abs/2305.08487
- **Reference count**: 13
- **Primary result**: Introduces Taxi1500, a multilingual dataset for text classification covering 1500+ languages using parallel Bible translations

## Executive Summary
Taxi1500 is a novel multilingual dataset for text classification that covers over 1500 languages by leveraging parallel translations of the Bible. The dataset categorizes verses into six topics (recommendation, faith, description, sin, grace, and violence) through crowdsourced English annotations, which are then projected to other languages via verse alignment. This approach enables large-scale evaluation of multilingual language models on extremely low-resource languages, addressing a critical gap in the field. The dataset is designed for in-language classification and zero-shot transfer experiments.

## Method Summary
The dataset creation process involves extracting verses from parallel Bible corpora, crowdsourcing English annotations with quality controls, and projecting labels to other languages through verse alignment. The English annotations are collected via Amazon Mechanical Turk with majority voting and performance thresholds to ensure quality. The dataset is split into train, development, and test sets for each language, enabling comprehensive evaluation of multilingual models across diverse linguistic contexts.

## Key Results
- XLM-R outperforms mBERT and Glot500 on training languages (head languages)
- mBERT shows superior performance on new languages not seen during pre-training (tail languages)
- The dataset enables evaluation of models on languages with extremely limited resources, including endangered languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel Bible translations enable consistent topic projection across languages.
- Mechanism: Shared verse identifiers in the Bible allow topic labels assigned in English to be directly transferred to aligned verses in other languages, ensuring consistent annotations.
- Core assumption: Verse alignment is exact and consistent across all translations in the dataset.
- Evidence anchors:
  - [abstract] "By annotating the English side of the data and projecting the labels onto other languages through aligned verses, we generate text classification datasets for more than 1500 languages."
  - [section 3] "Because the Bible is aligned at the verse level, we can easily project annotations from the English side to all other languages."
- Break Condition: Verse alignment is imperfect or inconsistent, leading to label misalignment across languages.

### Mechanism 2
- Claim: Crowdsourced annotations with quality controls ensure reliable English topic labels.
- Mechanism: Multiple annotators label each verse, with majority voting and quality thresholds to filter out low-quality annotations, resulting in a reliable English dataset for projection.
- Core assumption: Annotators can consistently interpret and apply the six defined topics to Bible verses.
- Evidence anchors:
  - [section 3] "Each verse is eventually annotated ten times in total. The final labels are selected by majority voting."
  - [section 3] "We implement quality control in the form of a performance threshold... if the worker obtains an F1 of <.4."
- Break Condition: Topics are too subjective or ambiguous, leading to inconsistent annotations even with quality controls.

### Mechanism 3
- Claim: Leveraging parallel data from the Bible enables evaluation of models on extremely low-resource languages.
- Mechanism: The Bible's wide translation coverage provides parallel text for over 1500 languages, many of which lack other annotated resources, enabling large-scale multilingual model evaluation.
- Core assumption: The Bible's translation coverage includes languages that are otherwise data-poor for NLP tasks.
- Evidence anchors:
  - [abstract] "We leverage parallel translations of the Bible to construct such a dataset by first developing applicable topics and employing a crowdsourcing tool to collect annotated data."
  - [section 2.3] "While there are other resources for parallel data available... we have chosen to use translations of the Bible due to the relatively larger number of supported languages."
- Break Condition: Parallel data quality is insufficient for the task, or the Bible domain is too restrictive for general language model evaluation.

## Foundational Learning

- Concept: Parallel corpora and alignment techniques
  - Why needed here: The dataset relies on aligning Bible verses across languages to transfer annotations, which requires understanding of parallel corpus construction and alignment methods.
  - Quick check question: How would you handle a case where a verse in one language is split into multiple verses in another language?

- Concept: Crowdsourcing and quality control for data annotation
  - Why needed here: The dataset is created through crowdsourcing, requiring knowledge of how to design annotation tasks, manage annotator quality, and aggregate results.
  - Quick check question: What methods could you use to measure and improve inter-annotator agreement in a subjective task?

- Concept: Text classification and topic modeling
  - Why needed here: The dataset is designed for text classification, requiring understanding of how to define meaningful topics, prepare data splits, and evaluate model performance.
  - Quick check question: How would you handle class imbalance in a dataset where some topics have significantly fewer examples than others?

## Architecture Onboarding

- Component map:
  - Parallel Bible Corpus (PBC) and additional web-crawled Bible translations -> English annotations (crowdsourcing) -> Majority voting and quality thresholds -> Verse-level alignment for label projection -> Train/dev/test splits for each language -> Multilingual model evaluation (mBERT, XLM-R, Glot500)

- Critical path:
  1. Extract verses from parallel Bible data
  2. Crowdsource English annotations with quality controls
  3. Project labels to other languages via verse alignment
  4. Split data into train/dev/test sets for each language
  5. Evaluate multilingual models on the resulting dataset

- Design tradeoffs:
  - Domain specificity vs. general applicability: Using Bible verses limits domain but enables coverage of many languages
  - Annotation cost vs. quality: Multiple annotators and quality controls improve reliability but increase cost
  - Label granularity vs. ease of annotation: Six topics balance expressiveness with annotator ability to consistently apply labels

- Failure signatures:
  - Low inter-annotator agreement or high rejection rates in crowdsourcing
  - Significant performance differences between languages with similar training data
  - Models overfit to Bible-specific language and perform poorly on general text

- First 3 experiments:
  1. Evaluate a multilingual model (e.g., mBERT) on the English portion of the dataset to establish a baseline
  2. Test label projection by training a model on English data and evaluating on a few other languages to verify consistency
  3. Compare model performance on head languages (in pre-training data) vs. tail languages (not in pre-training data) to assess the impact of pre-training language coverage

## Open Questions the Paper Calls Out

- **Open Question 1**: How do Taxi1500's classification results compare with other multilingual text classification datasets when using the same models?
  - Basis in paper: [inferred] The paper compares Taxi1500 results with existing multilingual benchmarks but does not directly compare Taxi1500's performance with other datasets using the same models.
  - Why unresolved: Direct comparison with other multilingual datasets using the same models is not provided in the paper.
  - What evidence would resolve it: Results of the same models (mBERT, XLM-R, Glot500) evaluated on Taxi1500 and other multilingual datasets side-by-side.

- **Open Question 2**: What is the impact of using a different source text (other than the Bible) on the quality and diversity of the Taxi1500 dataset?
  - Basis in paper: [explicit] The paper mentions the limitation of using the Bible as a religious text and suggests exploring other parallel corpora in the future.
  - Why unresolved: The paper does not experiment with different source texts to assess their impact on the dataset.
  - What evidence would resolve it: Creating and evaluating Taxi1500-like datasets using different parallel corpora and comparing their classification performance.

- **Open Question 3**: How does the performance of multilingual models on Taxi1500 vary across different language families, and what factors contribute to these differences?
  - Basis in paper: [explicit] The paper analyzes performance by language family but does not deeply investigate the factors contributing to performance variations.
  - Why unresolved: The paper does not provide a detailed analysis of the factors influencing model performance across language families.
  - What evidence would resolve it: A comprehensive study correlating model performance with linguistic features, script types, and typological similarities within language families.

## Limitations

- Domain specificity: Using Bible verses as source text limits the dataset's applicability to general language understanding tasks
- Alignment assumptions: Perfect verse-level alignment across all languages may not hold, particularly for structurally different languages
- Cultural context: English annotation topics may not transfer perfectly to other cultural contexts when projected to other languages

## Confidence

- **High Confidence**: The core mechanism of using parallel Bible data to create multilingual datasets is technically sound and well-documented. The experimental results showing XLM-R's superior performance on training languages and mBERT's better generalization to new languages are reproducible and clearly presented.
- **Medium Confidence**: The effectiveness of label projection through verse alignment across 1500+ languages assumes consistent annotation quality and alignment accuracy, which may vary in practice. The six-topic classification scheme may not capture all linguistic nuances across diverse language families.
- **Low Confidence**: Claims about the dataset's utility for endangered language preservation should be treated cautiously, as the Bible domain may not represent the full linguistic diversity needed for these applications.

## Next Checks

1. **Alignment Quality Verification**: Conduct a manual review of verse alignments for 10-15 randomly selected language pairs to assess the accuracy of label projection across structurally different languages.

2. **Domain Transfer Experiment**: Evaluate model performance when trained on Taxi1500 data and tested on general text from the same languages to measure domain adaptation requirements.

3. **Cross-Cultural Consistency Test**: Perform a small-scale annotation study with native speakers of 3-4 diverse languages to verify if the six topics are interpreted consistently across cultural contexts.