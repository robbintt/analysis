---
ver: rpa2
title: 'Towards Better Dynamic Graph Learning: New Architecture and Unified Library'
arxiv_id: '2303.13047'
source_url: https://arxiv.org/abs/2303.13047
tags:
- dynamic
- graph
- learning
- node
- dygformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limitations in existing dynamic graph learning
  methods, which struggle to capture correlations between nodes and long-term temporal
  dependencies. The authors propose DyGFormer, a Transformer-based architecture that
  incorporates a neighbor co-occurrence encoding scheme to explore node correlations
  and a patching technique to efficiently leverage longer histories.
---

# Towards Better Dynamic Graph Learning: New Architecture and Unified Library

## Quick Facts
- arXiv ID: 2303.13047
- Source URL: https://arxiv.org/abs/2303.13047
- Reference count: 40
- Primary result: DyGFormer outperforms existing methods on most datasets for dynamic graph learning tasks

## Executive Summary
The paper addresses key limitations in dynamic graph learning by introducing DyGFormer, a Transformer-based architecture that captures node correlations and long-term temporal dependencies through neighbor co-occurrence encoding and patching techniques. The authors also introduce DyGLib, a unified library with standardized training pipelines and evaluation protocols to promote reproducible research. Experiments on thirteen datasets demonstrate DyGFormer's effectiveness in dynamic link prediction and node classification tasks.

## Method Summary
DyGFormer uses a neighbor co-occurrence encoding scheme to capture correlations between source and destination nodes based on shared neighbor frequencies in their historical interaction sequences. The method divides interaction sequences into fixed-size patches to efficiently leverage longer histories while maintaining constant computational complexity. The architecture incorporates neighbor, link, time interval, and neighbor co-occurrence encodings as input to a Transformer encoder. The unified library DyGLib provides standardized data loaders, model trainers, and evaluation metrics across diverse datasets and algorithms, enabling fair comparison and reproducible research.

## Key Results
- DyGFormer outperforms existing methods on most datasets for both dynamic link prediction and node classification tasks
- The neighbor co-occurrence encoding scheme significantly improves performance by capturing node correlations
- The patching technique enables efficient learning from longer histories while maintaining constant computational complexity
- DyGLib facilitates reproducible research with standardized training pipelines and evaluation protocols across 13 diverse datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The neighbor co-occurrence encoding scheme captures node correlations by encoding the frequency of shared neighbors in the source and destination node sequences.
- Mechanism: For each interaction (u,v,t), the method counts how often each neighbor appears in both u's and v's historical interaction sequences. This co-occurrence frequency is encoded into a two-dimensional vector per neighbor, then transformed into a fixed-size embedding that reflects the strength of correlation between u and v.
- Core assumption: Nodes that share more common neighbors in their historical sequences are more likely to interact in the future.
- Evidence anchors:
  - [abstract]: "a neighbor co-occurrence encoding scheme that explores the correlations of the source node and destination node based on their sequences"
  - [section 4.1]: "the appearing frequency of a neighbor in a sequence indicates its importance, and the occurrences of a neighbor in the sequences of u and v... could reflect the correlations between u and v"

### Mechanism 2
- Claim: The patching technique enables efficient learning from long interaction histories by preserving local temporal proximity and limiting computational complexity.
- Mechanism: The historical interaction sequences for u and v are divided into fixed-size non-overlapping patches. Each patch contains P temporally adjacent interactions, flattened and fed into the Transformer. This keeps the number of patches constant regardless of sequence length, reducing computational cost and allowing the model to capture long-term dependencies.
- Core assumption: Local temporal proximity within patches is sufficient to preserve meaningful patterns, and dividing sequences into patches does not lose critical global context.
- Evidence anchors:
  - [abstract]: "a patching technique that divides each sequence into multiple patches and feeds them to Transformer, allowing the model to effectively and efficiently benefit from longer histories"
  - [section 4.1]: "The patching technique allows DyGFormer to capture long-term temporal dependencies since it can not only make DyGFormer effectively benefit from longer histories by preserving the local temporal proximities, but also efficiently reduce the computational complexity to a constant level"

### Mechanism 3
- Claim: The unified library DyGLib ensures reproducible and scalable research by standardizing training pipelines, interfaces, and evaluation protocols.
- Mechanism: DyGLib provides consistent data loaders, model trainers, and evaluation metrics across different dynamic graph learning methods. It integrates diverse datasets and algorithms, enabling fair comparison and easier experimentation.
- Core assumption: Inconsistent training pipelines and implementations in prior work are a major source of irreproducibility and hinder progress.
- Evidence anchors:
  - [abstract]: "We also introduce DyGLib, a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating protocols to promote reproducible, scalable, and credible dynamic graph learning research"
  - [section 4.2]: "We introduce a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating strategies to facilitate reproducible, scalable, and credible continuous-time dynamic graph learning research"

## Foundational Learning

- Concept: Transformer self-attention mechanism
  - Why needed here: DyGFormer uses Transformer encoders to model temporal dependencies in interaction sequences; understanding self-attention is essential to grasp how patches are processed.
  - Quick check question: How does multi-head self-attention enable the model to capture different types of relationships in the data?

- Concept: Graph neural networks and temporal graph modeling
  - Why needed here: DyGFormer operates on dynamic graphs where nodes and edges evolve over time; knowing how GNNs aggregate neighborhood information helps understand the input encoding.
  - Quick check question: What distinguishes discrete-time from continuous-time dynamic graph learning methods?

- Concept: Sequence modeling and patching in NLP/computer vision
  - Why needed here: The patching technique is inspired by methods like ViT (Vision Transformer) and PatchTST; understanding these precedents clarifies why patching works for dynamic graphs.
  - Quick check question: Why does dividing a sequence into patches preserve local temporal proximity while reducing computational cost?

## Architecture Onboarding

- Component map:
  Historical first-hop interactions -> Neighbor, link, time interval, and neighbor co-occurrence encodings -> Patching (fixed-size non-overlapping patches) -> Transformer encoder -> Averaged node representations -> Dynamic link prediction/node classification output

- Critical path:
  1. Retrieve and encode historical interactions for u and v
  2. Apply neighbor co-occurrence encoding
  3. Patch the encoded sequences
  4. Feed patches into Transformer encoder
  5. Average outputs and apply final layer for task-specific prediction

- Design tradeoffs:
  - Longer patch size reduces number of patches but may lose fine-grained temporal detail
  - Neighbor co-occurrence encoding adds parameter overhead but captures correlation signals
  - Mixing sequences of u and v increases interaction modeling at cost of complexity

- Failure signatures:
  - Poor performance when historical sequences are too short to form meaningful patches
  - Memory issues if patch size is too large relative to available GPU memory
  - Degraded results if neighbor co-occurrence encoding is not properly normalized

- First 3 experiments:
  1. Run DyGFormer on a small dataset (e.g., Wikipedia) with default settings to verify pipeline works
  2. Vary patch size (P=1, 4, 8) on a medium dataset to observe impact on performance and runtime
  3. Disable neighbor co-occurrence encoding and compare results to assess its contribution

## Open Questions the Paper Calls Out

- Question: How does DyGFormer's performance scale with extremely long input sequences (e.g., thousands of interactions) compared to baselines that use sampling or truncation?
  - Basis in paper: [explicit] The paper demonstrates that DyGFormer benefits from longer input sequences and maintains constant computational complexity through the patching technique, but only evaluates up to 4096 interactions.
  - Why unresolved: The paper does not explore the performance limits of DyGFormer with very long sequences or compare it to baselines using aggressive sampling strategies on datasets with extremely long node histories.
  - What evidence would resolve it: Experiments comparing DyGFormer's performance on datasets with nodes having thousands of interactions versus baselines using different sampling strategies, measuring both accuracy and computational efficiency.

- Question: What is the impact of the neighbor co-occurrence encoding scheme on model performance across different types of graph structures (e.g., bipartite vs. non-bipartite, dense vs. sparse)?
  - Basis in paper: [explicit] The paper shows that the neighbor co-occurrence encoding scheme is effective across multiple datasets but does not systematically analyze its performance across different graph structures or provide ablation studies on different graph types.
  - Why unresolved: The paper evaluates the scheme on a diverse set of datasets but does not isolate the effect of graph structure on the encoding scheme's effectiveness or compare it to alternative correlation-capturing methods.
  - What evidence would resolve it: Controlled experiments comparing DyGFormer with and without neighbor co-occurrence encoding on graphs with varying structures (bipartite/non-bipartite, dense/sparse), and comparisons to alternative correlation-capturing methods.

- Question: How sensitive is DyGFormer's performance to the patch size parameter, and is there an optimal patch size that varies by dataset characteristics?
  - Basis in paper: [explicit] The paper mentions that patch size is increased proportionally with input sequence length to maintain constant computational complexity, but does not provide systematic analysis of patch size sensitivity or optimization.
  - Why unresolved: The paper does not explore how different patch sizes affect model performance or whether there is an optimal patch size that depends on dataset characteristics such as interaction density or temporal patterns.
  - What evidence would resolve it: Sensitivity analysis varying patch sizes across different datasets while keeping other parameters constant, and identifying patterns in optimal patch sizes relative to dataset characteristics.

## Limitations

- The neighbor co-occurrence encoding assumes that shared neighbor frequency reliably indicates node correlation, which may not hold in all network topologies
- The patching technique's fixed size may lose important global temporal patterns in highly variable interaction sequences
- Performance may degrade when historical sequences are too short to form meaningful patches

## Confidence

- Mechanism 1 (neighbor co-occurrence encoding): Medium-High - supported by ablation studies showing performance drop when disabled
- Mechanism 2 (patching technique): Medium - theoretical justification is sound but empirical sensitivity to patch size varies
- Mechanism 3 (DyGLib standardization): High - directly demonstrated through reproducible results across 13 datasets

## Next Checks

1. Test neighbor co-occurrence encoding on networks with known adversarial or random interaction patterns to verify its assumptions hold
2. Perform sensitivity analysis on patch size (P) across different datasets to identify optimal ranges and failure points
3. Reproduce key results on at least two datasets using DyGLib's standardized pipeline to verify implementation correctness