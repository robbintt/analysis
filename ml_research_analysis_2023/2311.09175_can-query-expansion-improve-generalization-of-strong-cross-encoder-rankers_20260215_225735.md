---
ver: rpa2
title: Can Query Expansion Improve Generalization of Strong Cross-Encoder Rankers?
arxiv_id: '2311.09175'
source_url: https://arxiv.org/abs/2311.09175
tags:
- query
- keywords
- arxiv
- expansion
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the use of query expansion to improve the
  zero-shot performance of strong cross-encoder rankers. It shows that directly applying
  existing expansion techniques like PRF and LLMs to rankers like MonoT5 and RankT5
  deteriorates performance.
---

# Can Query Expansion Improve Generalization of Strong Cross-Encoder Rankers?

## Quick Facts
- arXiv ID: 2311.09175
- Source URL: https://arxiv.org/abs/2311.09175
- Reference count: 24
- Primary result: Query expansion using the GFF pipeline improves nDCG@10 scores for MonoT5 and RankT5 on BEIR and TREC DL datasets

## Executive Summary
This paper investigates whether query expansion can improve the zero-shot performance of strong cross-encoder rankers. The authors demonstrate that naive application of existing expansion techniques like pseudo-relevance feedback (PRF) and LLM-based methods actually deteriorates performance for cross-encoders like MonoT5 and RankT5. To address this, they propose the GFF (Generate, Filter, Fuse) pipeline that uses an LLM to generate keywords through a reasoning chain, selects top keywords via self-consistency, and combines ranking results using reciprocal rank weighting. Experiments on BEIR and TREC DL 2019/2020 show that GFF improves nDCG@10 scores for both rankers, demonstrating the effectiveness of this approach.

## Method Summary
The GFF pipeline addresses the challenge of query expansion for cross-encoders by first generating keywords through an LLM using either Q2D2K (query-to-document-to-keyword) or PRF+D2K (pseudo-relevance feedback plus document-to-keyword) methods. The generated keywords are filtered using self-consistency, where the LLM generates keywords three times and the top-3 most frequent keywords are selected. Each expanded query (original query plus one keyword) is then used to re-rank documents with RankT5-base, and the results are combined using reciprocal rank weighting based on how well each expansion ranks the original top-1 document. This approach mitigates the distributional shift that naive keyword concatenation causes in cross-encoders.

## Key Results
- GFF improves nDCG@10 scores for both MonoT5 and RankT5 on BEIR and TREC DL 2019/2020 datasets
- Improvements peak at 3 expanded keywords and diminish with more keywords
- Reciprocal rank weighting outperforms mean pooling and other fusion methods
- Q2D2K and PRF+D2K generation methods both show benefits over direct keyword generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-encoders are more sensitive to token distribution shifts than bi-encoders, making naive query expansion harmful
- Mechanism: Cross-encoders rely on token-level interactions between query and document, so adding noisy or semantically distant keywords disrupts the learned representations more severely than bi-encoders which encode query and document separately
- Core assumption: The performance degradation from query expansion in cross-encoders is primarily due to distributional shift in the query representation rather than semantic drift alone
- Evidence anchors:
  - [abstract] "cross-encoders are more heavily based on token interactions, making them more sensitive to the distribution of the textual representation of the query"
  - [section] "noisy keywords have more influence on precision-related metrics such as nDCG@10 which the cross-encoders aim to improve"
  - [corpus] Found 25 related papers; no direct evidence for this specific mechanism, but related work on query expansion and rankers exists
- Break condition: If the cross-encoder architecture is modified to be more robust to distributional shifts, or if the expansion terms are perfectly semantically aligned with the query

### Mechanism 2
- Claim: Generating documents first (Q2D2K) elicits more comprehensive knowledge from LLMs than direct keyword generation
- Mechanism: By prompting the LLM to generate a document answering the query, we tap into its broader knowledge base and reasoning capabilities, then extract relevant keywords from this generated context rather than relying on the LLM's immediate keyword generation
- Core assumption: LLMs can better organize and express relevant information in document form than in keyword form directly
- Evidence anchors:
  - [abstract] "Q2D2K... generates a detailed document given the query and then pick keywords in the document"
  - [section] "The intuition is similar to recitation-augmented language models where more knowledge can be elicited before fulfilling the task"
  - [corpus] No direct evidence for this mechanism in corpus
- Break condition: If the document generation step introduces irrelevant information or if the keyword extraction from documents is poor

### Mechanism 3
- Claim: Reciprocal rank weighting effectively combines expansion results by emphasizing expansions that improve ranking of relevant documents
- Mechanism: Each expanded query is used to re-rank candidates, and the weight assigned to each expansion is based on how well it ranks the original top-1 document, with better-performing expansions getting higher weights
- Core assumption: Expansions that improve the ranking of the original top document are likely to be more useful overall
- Evidence anchors:
  - [abstract] "we further combine the ranking results of each expanded query dynamically... using reciprocal rank weighting"
  - [section] "we follow previous work (Cormack et al., 2009; Dai et al., 2022) to weight the ranked lists using the reciprocal rank of the original top-1 document"
  - [corpus] Found related work on reciprocal rank fusion from corpus
- Break condition: If the original top-1 document is not a good proxy for overall ranking quality, or if expansions have different types of improvements

## Foundational Learning

- Concept: Cross-encoder architecture
  - Why needed here: Understanding why naive query expansion fails requires knowing how cross-encoders process input
  - Quick check question: How does a cross-encoder process query-document pairs differently from a bi-encoder?

- Concept: Pseudo-relevance feedback (PRF)
  - Why needed here: PRF is one of the classical approaches to query expansion that the paper compares against
  - Quick check question: What is the main difference between PRF-based expansion and LLM-based expansion?

- Concept: Reciprocal rank fusion
  - Why needed here: The paper uses this technique to combine rankings from different query expansions
  - Quick check question: How does reciprocal rank fusion differ from simple score averaging?

## Architecture Onboarding

- Component map: LLM keyword generator → Keyword filter (self-consistency) → Ranker (RankT5) → Fusion layer (reciprocal rank weighting) → Final ranking
- Critical path: Query → LLM generation → Filtering → Individual ranking → Weighted fusion → Final output
- Design tradeoffs: Using multiple expansions increases computational cost but improves robustness; simpler fusion methods are faster but less effective
- Failure signatures: Performance drops when keywords are too numerous or semantically distant; improvement plateaus after 3 keywords
- First 3 experiments:
  1. Test different numbers of keywords (1-5) to find optimal count
  2. Compare different fusion methods (mean pooling vs reciprocal rank weighting)
  3. Evaluate different keyword generation methods (Q2K vs Q2D2K vs PRF+D2K)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of keyword generation method (e.g., Q2D2K vs. PRF + D2K) impact the effectiveness of query expansion for cross-encoder rankers on datasets outside of Wikipedia and News domains?
- Basis in paper: [explicit] The paper shows that improvements on TREC DL and Wiki+News datasets are more significant compared to other datasets in BEIR, and mentions that the generated keywords are biased towards the data that the LLM pre-trained on.
- Why unresolved: The paper only provides a limited comparison of different keyword generation methods on a subset of BEIR datasets. It does not explore the impact of these methods on datasets with different domains, such as bio-medical datasets.
- What evidence would resolve it: A comprehensive evaluation of different keyword generation methods on a wider range of BEIR datasets, including those outside of Wikipedia and News domains, would provide insights into their generalizability and effectiveness.

### Open Question 2
- Question: What is the optimal number of keywords to generate and filter for maximizing the performance of cross-encoder rankers?
- Basis in paper: [explicit] The paper shows that the improvement of the proposed method peaks at 3 expanded keywords and gradually diminishes with the addition of more keywords.
- Why unresolved: The paper only explores the impact of keyword number up to 9. It is unclear whether there is an optimal number of keywords that maximizes the performance of cross-encoder rankers, and whether this number varies depending on the dataset or the specific ranker used.
- What evidence would resolve it: A systematic study of the impact of keyword number on the performance of cross-encoder rankers across different datasets and rankers would provide insights into the optimal number of keywords to use.

### Open Question 3
- Question: How does the choice of fusion method (e.g., reciprocal rank weighting vs. mean pooling) impact the effectiveness of query expansion for cross-encoder rankers?
- Basis in paper: [explicit] The paper compares reciprocal rank weighting with mean pooling and other fusion methods, and shows that reciprocal rank weighting is more effective.
- Why unresolved: The paper only explores a limited set of fusion methods. It is unclear whether there are other fusion methods that could be more effective for query expansion with cross-encoder rankers.
- What evidence would resolve it: A comprehensive evaluation of different fusion methods on a wide range of datasets and rankers would provide insights into their effectiveness and help identify the optimal fusion method for query expansion with cross-encoder rankers.

## Limitations

- Limited ablation studies on individual GFF components - the paper does not provide comprehensive ablation studies showing the individual contribution of each component (keyword generation method, self-consistency filtering, reciprocal rank weighting)
- Potential overfitting to specific datasets - the evaluation focuses on TREC DL 2019/2020 and BEIR benchmark datasets, with unclear effectiveness on other retrieval domains
- Computational cost considerations - the paper does not discuss the computational overhead of the GFF pipeline compared to baseline methods

## Confidence

- **High confidence** in the core finding that naive query expansion harms cross-encoder performance while GFF improves it
- **Medium confidence** in the specific mechanisms proposed (distributional shift sensitivity, document-first generation)
- **Low confidence** in the generalizability of results to domains beyond those tested and to scenarios with different computational constraints

## Next Checks

1. **Ablation study validation**: Conduct controlled experiments removing each GFF component (keyword generation method, filtering, fusion) to quantify individual contributions and identify which components are essential versus complementary

2. **Cross-domain robustness test**: Evaluate GFF on specialized retrieval datasets outside BEIR (e.g., biomedical literature, legal documents) to verify if improvements generalize beyond general web search scenarios

3. **Computational overhead analysis**: Measure and compare inference times and memory usage of GFF versus baseline methods across different hardware configurations to assess practical deployment viability