---
ver: rpa2
title: 'IMAD: IMage-Augmented multi-modal Dialogue'
arxiv_id: '2305.10512'
source_url: https://arxiv.org/abs/2305.10512
tags:
- image
- dataset
- dialogue
- utterance
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IMAD, a novel multi-modal dialogue dataset
  that aims to bridge the gap between text-based and image-based communication in
  dialogue systems. The dataset contains 4,864 dialogues where the last utterance
  is replaced with an image, making it the first validated English dataset of its
  kind.
---

# IMAD: IMage-Augmented multi-modal Dialogue

## Quick Facts
- arXiv ID: 2305.10512
- Source URL: https://arxiv.org/abs/2305.10512
- Reference count: 40
- Key outcome: Introduces IMAD dataset with 4,864 dialogues where last utterance replaced with image; baseline BLIP model achieves 23.73 BLEU score

## Executive Summary
This paper introduces IMAD, the first validated English dataset for multi-modal dialogue where text utterances are replaced with relevant images. The dataset is constructed using a two-stage approach: first filtering utterances suitable for image replacement using text-to-image similarity and sentence similarity, then selecting appropriate images using a visual question answering model. A baseline model based on BLIP is trained on this dataset and outperforms both text-only models and BlenderBot on the image-augmented dialogue task.

## Method Summary
The method uses a two-stage approach to construct the IMAD dataset. First, utterances are filtered using CLIP embeddings for text-to-image similarity and SentenceBERT for sentence similarity, with a Random Forest classifier predicting replaceability based on features including Image Score, Maximum Entity Score, Sentence Similarity, and BLEU Score. Second, relevant images are selected from candidates using BLIP's VQA confidence scores. The baseline model fine-tunes BLIP with both image and text inputs using a prefix LM paradigm, where the image and last utterance are inputs and the replaced utterance is the target.

## Key Results
- IMAD dataset contains 4,864 dialogues with image-replaced last utterances
- BLIP model with image+text input achieves BLEU score of 23.73 vs 10.63 for text-only model
- Inter-rater reliability scores of 0.34-0.47 demonstrate dataset quality
- Image+text model outperforms BlenderBot 400M zero-shot on test set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-to-image similarity and sentence similarity filters effectively identify replaceable utterances
- Mechanism: CLIP embeddings compute utterance-image cosine similarity combined with SentenceBERT for utterance-text comparison; Random Forest uses these as features
- Core assumption: Nouns and visual concepts indicate replaceability
- Evidence: Abstract states use of text-to-image and sentence similarity; Section 4.1.2 details feature engineering
- Break condition: If CLIP embeddings don't capture semantic relationships or nouns aren't primary indicators

### Mechanism 2
- Claim: Two-stage filtering improves dataset quality
- Mechanism: First stage uses Random Forest to filter replaceable utterances; second stage uses VQA confidence scores to select images
- Core assumption: Two-stage filtering produces cleaner data than simultaneous processing
- Evidence: Abstract describes two-stage replacement; Section 4.2.1 details VQA-based image selection showing only half of pairs match well
- Break condition: If VQA confidence doesn't correlate with matching quality or first-stage filtering is too restrictive

### Mechanism 3
- Claim: Fine-tuned BLIP with image+text outperforms text-only models
- Mechanism: ViT-B/16 for image encoding, BERT for text decoding, trained with prefix LM paradigm
- Core assumption: Visual information provides additional context for dialogue responses
- Evidence: Section 5.1 shows 23.73 vs 10.63 BLEU scores; Section 5.3 provides generation examples
- Break condition: If visual features don't add meaningful information beyond text context

## Foundational Learning

- Concept: Text-to-image similarity using CLIP embeddings
  - Why needed: To identify utterances that can be meaningfully replaced with images
  - Quick check: How would you compute similarity between utterance and image using CLIP embeddings?

- Concept: Visual Question Answering (VQA) confidence scoring
  - Why needed: To select most appropriate image from candidates by measuring image-question answering ability
  - Quick check: What does VQA confidence score represent in image selection for dialogue?

- Concept: Random Forest classification for feature-based filtering
  - Why needed: To predict whether utterances can be replaced with images using multiple similarity features
  - Quick check: Why might Random Forest be preferred over linear model for this classification task?

## Architecture Onboarding

- Component map: Dialogue corpus → utterance filtering → image selection → dataset creation → model training → evaluation
- Critical path: utterance filtering → image selection → model training → evaluation (each step depends on previous quality)
- Design tradeoffs: frozen image encoders for efficiency vs fine-tuning for alignment; two-stage filtering for quality vs single-stage for speed; human annotation for validation vs fully automated
- Failure signatures: low BLEU despite high VQA confidence suggests image selection issues; high variance in Random Forest predictions suggests feature engineering problems; poor inter-rater reliability indicates unclear guidelines
- First 3 experiments: 1) Test Random Forest with different feature combinations to optimize precision/recall tradeoff; 2) Compare VQA confidence threshold values for optimal image selection; 3) Run ablation study comparing image+text vs text-only model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper limit on quality of image-to-text matching achievable with proposed methodology?
- Basis: At best, only half of pairs with replaceable utterances found to have matching images
- Why unresolved: Paper lacks comprehensive analysis of limiting factors like language ambiguity, model limitations, or image diversity
- What evidence would resolve: Systematic evaluation on larger, more diverse dataset compared to alternative approaches

### Open Question 2
- Question: How does methodology generalize to other languages and cultural contexts?
- Basis: Paper focuses on English dialogues without addressing other languages/cultures where language/imagery nuances differ
- Why unresolved: No evidence or discussion of performance in other languages or what modifications might be necessary
- What evidence would resolve: Experiments with methodology on dialogues in other languages and cultural contexts

### Open Question 3
- Question: What is impact of methodology on dialogue system performance in real-world applications?
- Basis: Paper shows baseline model outperforms text-only models but doesn't demonstrate real-world application impact
- Why unresolved: No evidence or discussion of how methodology impacts customer service chatbots, virtual assistants, etc.
- What evidence would resolve: Experiments with methodology on real-world dialogue systems comparing performance to systems without image understanding

## Limitations

- The two-stage filtering approach relies heavily on the assumption that nouns and visual concepts are primary indicators of replaceability, with limited exploration of other linguistic features
- BLEU scores may not fully capture quality of image-augmented responses since images can convey information text cannot
- Comparison against BlenderBot is limited to zero-shot performance without extensive ablation studies on architectural choices
- Paper doesn't explore whether same approach would work for other languages or different dialogue domains beyond the seven conversational datasets used

## Confidence

**High Confidence**: Dataset construction methodology and two-stage filtering approach are well-documented and reproducible; BLEU score improvements (23.73 vs 10.63) and inter-rater reliability scores (0.34-0.47) provide strong evidence

**Medium Confidence**: Claim that visual information provides meaningful additional context is supported by performance gap but could be partially due to architectural differences rather than inherent value of visual information; utterance selection and image filtering thresholds are somewhat arbitrary

**Low Confidence**: Assertion that this is first validated English dataset of its kind is difficult to verify definitively; long-term utility for training robust multi-modal dialogue systems remains unproven as paper focuses on proof-of-concept

## Next Checks

1. Conduct feature importance analysis through ablation study on Random Forest classifier to determine which features contribute most to successful utterance filtering and test alternative features

2. Apply two-stage filtering approach to dialogue datasets from different domains (technical support, educational tutoring) to test generalization beyond conversational datasets

3. Conduct human evaluation comparing responses from image+text model versus text-only model on test dialogues, asking annotators to rate whether visual information improves response quality and relevance