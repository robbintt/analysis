---
ver: rpa2
title: Statistically Efficient Variance Reduction with Double Policy Estimation for
  Off-Policy Evaluation in Sequence-Modeled Reinforcement Learning
arxiv_id: '2308.14897'
source_url: https://arxiv.org/abs/2308.14897
tags:
- policy
- learning
- reinforcement
- behavior
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of importance sampling (IS) for
  offline reinforcement learning (RL) in sequence-modeled RL methods like Decision
  Transformer, where the behavior policy and target policy are typically unavailable
  or deterministic, making IS challenging. The authors propose Double Policy Estimation
  (DPE), a method that estimates both the behavior policy and target policy using
  maximum likelihood estimation, and uses these estimates to compute IS weights for
  off-policy evaluation.
---

# Statistically Efficient Variance Reduction with Double Policy Estimation for Off-Policy Evaluation in Sequence-Modeled Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2308.14897
- **Source URL**: https://arxiv.org/abs/2308.14897
- **Reference count**: 40
- **Key outcome**: Proposes Double Policy Estimation (DPE) for variance reduction in sequence-modeled RL, showing improved performance on D4RL benchmarks compared to DT, RvS, CQL, BEAR, UWAC, BC, and IQL.

## Executive Summary
This paper addresses the challenge of importance sampling (IS) for offline reinforcement learning (RL) in sequence-modeled methods like Decision Transformer, where both behavior and target policies are typically unavailable or deterministic. The authors propose Double Policy Estimation (DPE), which estimates both policies using maximum likelihood estimation and uses these estimates to compute IS weights for off-policy evaluation. The method provably reduces the asymptotic variance of policy gradient estimates compared to standard IS, and empirical results on D4RL benchmarks demonstrate performance improvements, particularly on medium-replay datasets.

## Method Summary
DPE extends sequence-modeled RL methods by estimating both behavior policy πb and target policy πe using maximum likelihood, enabling variance reduction through importance weighting. The method pre-trains a behavior policy estimator (using CQL) before training the Decision Transformer. During training, DPE computes weights as the ratio of estimated target policy to estimated behavior policy probabilities, clipped to prevent numerical instability. The loss function incorporates these DPE weights with a baseline prediction, aiming to reduce the mean squared error of policy evaluation compared to standard approaches.

## Key Results
- DPE reduces the asymptotic variance of importance sampling estimators compared to ordinary importance sampling
- Empirically improves performance on D4RL benchmarks (HalfCheetah, Hopper, Walker2d) compared to DT, RvS, CQL, BEAR, UWAC, BC, and IQL
- Shows particular advantage on medium-replay datasets
- Reduces mean squared error of policy evaluation compared to Decision Transformer baseline

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Estimating both behavior and target policies reduces variance in off-policy evaluation compared to using only the true behavior policy.
- **Mechanism**: The paper proves that when both behavior policy πb and evaluation policy πe are unknown, estimating them using maximum likelihood (DPE weights = πψe(at|st)/πηb(at|st)) produces an estimator with lower asymptotic variance than ordinary importance sampling (OIS) with ground truth policies.
- **Core assumption**: The maximum likelihood estimators of πb and πe converge to the true policies as sample size increases, and the baseline estimator bξ converges to the true return baseline.
- **Evidence anchors**:
  - [abstract] "We prove that DPE can statistically lower the mean squared error of importance sampling OPE with lower variance."
  - [section] "We prove that DPE can statistically lower the mean squared error of importance sampling OPE with lower variance. We implement the proposed DPE on D4RL environments and compare DPE with SOTA baselines..."
- **Break condition**: If either policy estimation is poor (e.g., insufficient data, model misspecification), the variance reduction may not materialize and could even increase variance.

### Mechanism 2
- **Claim**: Using estimated behavior policy corrects for sampling bias in deterministic evaluation policies common in sequence-modeling RL methods.
- **Mechanism**: In methods like Decision Transformer, the evaluation policy is deterministic (outputs single action), making standard IS impossible since πe(at|st) is undefined. By modeling πe as a Gaussian (mean=predicted action, variance=MSE), DPE enables importance weighting.
- **Core assumption**: Modeling the target policy as Gaussian with variance equal to training MSE is a valid approximation for computing importance weights.
- **Evidence anchors**:
  - [abstract] "importance sampling is not considered to correct the policy bias when dealing with off-policy data, mainly due to the absence of behavior policy and the use of deterministic evaluation policies."
  - [section] "sequence modeling-based RL usually are trained using a transformer structure to represent evaluation policy and to generate deterministic action outputs [17]. We need to extend them to stochastic policies to obtain πe in importance sampling."
- **Break condition**: If the MSE doesn't capture the true policy uncertainty (e.g., systematic errors, heteroscedastic noise), the importance weights may be inaccurate.

### Mechanism 3
- **Claim**: DPE reduces MSE by combining bias correction (importance weighting) with variance reduction (policy estimation).
- **Mechanism**: The DPE estimator combines three components: (1) importance weighting to correct for distribution shift, (2) behavior policy estimation to make IS possible, and (3) target policy estimation to make IS meaningful for deterministic methods. The theorem proves the combined effect reduces variance.
- **Core assumption**: The three components work synergistically rather than antagonistically.
- **Evidence anchors**:
  - [abstract] "Our method brings a performance improvements on selected methods which outperforms SOTA baselines in several tasks, demonstrating the advantages of enabling double policy estimation for sequence-modeled reinforcement learning."
  - [section] "We prove that DPE can statistically lower the mean squared error of importance sampling OPE with lower variance."
- **Break condition**: If the behavior policy estimation is poor, the importance weights could amplify errors rather than correct them.

## Foundational Learning

- **Concept**: Importance Sampling (IS) in reinforcement learning
  - **Why needed here**: The entire paper builds on using IS to correct for off-policy evaluation bias, but extends it to cases where both policies are unknown.
  - **Quick check question**: What is the formula for ordinary importance sampling in RL, and what problem does it solve?

- **Concept**: Maximum Likelihood Estimation (MLE)
  - **Why needed here**: DPE relies on MLE to estimate both behavior and target policies from data, which is the foundation for computing the importance weights.
  - **Quick check question**: What are the MLE estimates for Gaussian distributions with unknown mean and variance?

- **Concept**: Asymptotic Variance and Mean Squared Error
  - **Why needed here**: The paper's theoretical contribution centers on proving that DPE reduces asymptotic variance compared to OIS, which requires understanding these statistical concepts.
  - **Quick check question**: What is the relationship between asymptotic variance and MSE for consistent estimators?

## Architecture Onboarding

- **Component map**: Pre-trained CQL network (behavior policy) -> Decision Transformer (target policy) -> DPE weight calculator -> Weighted loss with baseline
- **Critical path**: Pre-train behavior policy estimator → Initialize DT → During DT training: estimate target policy → compute DPE weights → apply weighted gradients → update DT
- **Design tradeoffs**: 
  - Pre-training behavior policy adds computational overhead but enables IS
  - Using MSE as variance assumes homoscedastic noise, which may not hold
  - The DPE weight calculation can be numerically unstable if estimated probabilities are very small
- **Failure signatures**:
  - High variance in training despite DPE: suggests poor policy estimation
  - Training collapse or NaN gradients: suggests numerical instability in weight calculation
  - Performance worse than DT baseline: suggests the DPE weights are introducing harmful bias
- **First 3 experiments**:
  1. Verify behavior policy estimation: Compare CQL-trained behavior policy likelihood on held-out data vs. ground truth (if available)
  2. Validate target policy approximation: Check if MSE-based variance captures action uncertainty by comparing predicted vs. actual action distributions
  3. Test DPE weight stability: Monitor the distribution of DPE weights during training for extreme values or instability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the variance reduction property of DPE generalize to non-linear baseline estimators for return-to-go prediction?
- **Basis in paper**: [explicit] The paper proves variance reduction for linear baseline estimators b(ξ) in Theorem 1, but uses a linear baseline in practice.
- **Why unresolved**: The proof relies on the linearity of the baseline to obtain a closed-form expression for the estimator's asymptotic distribution. Extending this to non-linear baselines would require new theoretical analysis.
- **What evidence would resolve it**: Empirical evaluation of DPE with non-linear baselines (e.g., neural networks) on benchmark tasks, along with theoretical analysis of their asymptotic variance properties.

### Open Question 2
- **Question**: What is the impact of the warm-up phase for behavior policy estimation on the final performance of DPE?
- **Basis in paper**: [explicit] The paper mentions that DPE requires a warm-up phase to obtain the estimated behavior policy prior to training the Decision Transformer.
- **Why unresolved**: The paper does not investigate how the length or quality of the warm-up phase affects the final performance. It is unclear if a longer or more accurate warm-up phase would lead to better results.
- **What evidence would resolve it**: Ablation studies varying the warm-up phase length and quality, and measuring their impact on the final performance of DPE.

### Open Question 3
- **Question**: How does DPE perform in stochastic environments compared to deterministic ones?
- **Basis in paper**: [inferred] The paper mentions that RvS methods perform poorly in stochastic environments, and DPE is based on RvS. However, it does not explicitly evaluate DPE in stochastic environments.
- **Why unresolved**: The paper focuses on deterministic environments in the D4RL benchmark. It is unclear if the variance reduction property of DPE extends to stochastic environments.
- **What evidence would resolve it**: Empirical evaluation of DPE in stochastic environments, such as those with noisy transitions or rewards, and comparison with other methods.

## Limitations
- Theoretical proof assumes perfect convergence of MLE estimates, which may not hold in finite-sample regimes
- No empirical validation of asymptotic variance reduction claims with known ground truth
- Behavior policy estimation pre-training adds significant computational overhead
- MSE-based variance assumption for deterministic policies may not capture true uncertainty in all scenarios

## Confidence
- **High confidence**: The DPE method can be implemented and produces measurable differences in variance reduction on D4RL benchmarks
- **Medium confidence**: The theoretical variance reduction proof holds under stated assumptions
- **Medium confidence**: The improvement over baselines on medium-replay datasets represents a general trend rather than dataset-specific optimization

## Next Checks
1. Conduct a controlled experiment with synthetic data where true behavior and target policies are known to empirically verify the variance reduction claims
2. Test DPE sensitivity to the clipping bounds (0.05, 0.995) by varying them systematically and measuring impact on training stability and final performance
3. Compare DPE against a baseline that uses only behavior policy estimation without target policy estimation to isolate the contribution of each component to variance reduction