---
ver: rpa2
title: 'CLImage: Human-Annotated Datasets for Complementary-Label Learning'
arxiv_id: '2305.08295'
source_url: https://arxiv.org/abs/2305.08295
tags:
- labels
- complementary
- learning
- datasets
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first human-annotated real-world complementary-label
  learning datasets, addressing a critical gap in evaluating CLL algorithms under
  practical conditions. The authors collected complementary labels for CIFAR10 and
  CIFAR100-derived datasets (CLCIFAR10 and CLCIFAR20) using Amazon Mechanical Turk,
  revealing that human-annotated complementary labels exhibit significant noise (3.93%
  and 2.80% error rates) and bias compared to uniform synthetic labels.
---

# CLImage: Human-Annotated Datasets for Complementary-Label Learning

## Quick Facts
- arXiv ID: 2305.08295
- Source URL: https://arxiv.org/abs/2305.08295
- Authors: 
- Reference count: 11
- Key outcome: Introduces first human-annotated real-world CLL datasets showing noise (3.93% and 2.80% error rates) and bias significantly degrade state-of-the-art CLL algorithm performance compared to synthetic labels

## Executive Summary
This paper addresses the critical gap in evaluating complementary-label learning (CLL) algorithms under practical conditions by introducing human-annotated datasets (CLCIFAR10 and CLCIFAR20). The authors collected complementary labels using Amazon Mechanical Turk, revealing that real-world CLL suffers from significant noise and bias compared to uniform synthetic labels. Through extensive benchmarking, they demonstrate that state-of-the-art CLL algorithms perform substantially worse on human-annotated data, highlighting the need for algorithms robust to noisy and biased label distributions.

## Method Summary
The study collected complementary labels for CIFAR10 and CIFAR100-derived datasets using Amazon Mechanical Turk, where annotators selected from 4 randomly sampled incorrect labels per image. The authors implemented multiple CLL algorithms (forward-correction, unbiased risk estimator with gradient ascent, surrogate complementary loss, discriminative model, and probability estimates) using ResNet-18 architecture and Adam optimizer. They compared performance on human-annotated datasets versus synthetic uniform labels, analyzed transition matrix characteristics, and evaluated noise cleaning and validation scheme effectiveness.

## Key Results
- Human-annotated complementary labels exhibit 3.93% (CLCIFAR10) and 2.80% (CLCIFAR20) error rates
- State-of-the-art CLL algorithms show 10-15% accuracy degradation on real-world datasets compared to synthetic labels
- Noise cleaning (removing 50-75% of noisy data) can restore performance to synthetic label levels
- Forward-correction with uniform transition matrix assumption shows competitive performance despite uniform assumption being incorrect
- Models learning from practical complementary labels suffer severe overfitting, especially with true transition matrices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-annotated complementary labels contain significant noise and bias compared to synthetic uniform labels, degrading CLL algorithm performance.
- Mechanism: Human annotators make errors (3.93% for CLCIFAR10, 2.80% for CLCIFAR20) and exhibit systematic label preferences, creating a transition matrix that deviates from the uniform assumption used in most CLL algorithms.
- Core assumption: The transition matrix T represents the probability of generating complementary label j given true label i, and CLL algorithms assume either uniform T or accurate knowledge of T.
- Evidence anchors: [abstract] "human-annotated complementary labels exhibit significant noise (3.93% and 2.80% error rates) and bias compared to uniform synthetic labels"; [section] "The mean error rate made by the human annotators are 3.93% for CLCIFAR10 and 2.80% for CLCIFAR20"

### Mechanism 2
- Claim: The class-conditional assumption (P(ȳ|xi,yi) = P(ȳ|yi)) holds approximately even for human-annotated labels, as feature-dependent effects are small.
- Mechanism: Despite human biases in label selection, the approximation P(ȳ|Y,X) ≈ P(ȳ|Y) doesn't significantly affect classification accuracy when compared to i.i.d. synthetic labels sampled from the empirical transition matrix.
- Core assumption: The generation of complementary labels depends primarily on the true class rather than the specific features of the instance.
- Evidence anchors: [section] "we generated two synthetic complementary datasets clcifar10-iid and clcifar20-iid by i.i.d. sampling the CLs from the empirical transition matrix... The results show that the approximation does not significantly affect the classification accuracy"

### Mechanism 3
- Claim: CLL algorithms assuming uniform transition matrices are more robust to real-world noise than those using biased transition matrices.
- Mechanism: Algorithms like forward-correction with uniform T (fwd-u) maintain competitive accuracy (36.83% on CLCIFAR10) despite performance gaps, while those using true T (fwd-r) suffer severe overfitting (accuracy gap of ~10% between last epoch and early stopping).
- Core assumption: The uniform assumption, while incorrect, provides regularization that helps mitigate the effects of noise and bias in real-world data.
- Evidence anchors: [section] "we discover that by applying some naive data augmentation to CLCIFAR dataset, the performance would improve and become less overfitting... implicitly assuming a uniform transition matrix could be a robust choice, and yields competitive accuracy"; [section] "fwd-u with early stopping can reach 36.83%, which is close to fwd-r"

## Foundational Learning

- Concept: Weakly supervised learning paradigms
  - Why needed here: CLL is a specific type of weakly supervised learning where only negative (complementary) information is available instead of positive labels
  - Quick check question: What distinguishes CLL from other weakly supervised learning problems like positive-unlabeled learning or noisy-label learning?

- Concept: Transition matrix in multi-class classification
  - Why needed here: The transition matrix T captures how complementary labels are generated from true labels, and its properties (noise, bias, uniformity) are central to understanding CLL algorithm performance
  - Quick check question: How would you construct the empirical transition matrix from human-annotated complementary labels?

- Concept: Unbiased risk estimation in the presence of noisy labels
  - Why needed here: CLL algorithms rely on unbiased risk estimators to train classifiers without access to true labels, and the effectiveness of these estimators depends on the noise characteristics of the complementary labels
  - Quick check question: What is the key challenge in using unbiased risk estimators when the transition matrix is both noisy and biased?

## Architecture Onboarding

- Component map: Data collection → Transition matrix analysis → Algorithm benchmarking → Validation scheme evaluation
- Critical path: Human annotation collection → Empirical transition matrix estimation → CLL algorithm implementation → Performance comparison with synthetic labels
- Design tradeoffs: Uniform assumption simplicity vs. true transition matrix accuracy; noise cleaning vs. data retention; multiple complementary labels per instance vs. single label efficiency
- Failure signatures: Severe overfitting (accuracy gap between last epoch and early stopping), validation objective mismatch, performance degradation when transitioning from synthetic to real-world datasets
- First 3 experiments:
  1. Implement transition matrix estimation from human-annotated CLCIFAR10/CLCIFAR20 and compare empirical noise rates and bias patterns to synthetic uniform labels
  2. Benchmark fwd-u (uniform assumption) vs. fwd-r (true transition matrix) on CLCIFAR10 with and without data augmentation to isolate overfitting effects
  3. Test noise cleaning at different thresholds (25%, 50%, 75%) on CLCIFAR10 and measure performance recovery compared to uniform-CIFAR10 baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop validation schemes for CLL that do not require access to true labels, especially given the significant performance gap observed when comparing URE/SCEL validation to true label validation?
- Basis in paper: [explicit] The paper notes "Whether this gap could be further improved remains open" when discussing validation objectives, and shows non-negligible performance differences between models selected using URE/SCEL versus true labels
- Why unresolved: Current validation methods rely on either true labels (which defeats the purpose of CLL) or show suboptimal performance compared to true label validation
- What evidence would resolve it: A validation method that achieves comparable performance to true label validation without requiring true labels, demonstrated through extensive benchmarking on real-world CLL datasets

### Open Question 2
- Question: What regularization techniques beyond data augmentation are most effective at preventing overfitting in CLL algorithms when learning from biased transition matrices?
- Basis in paper: [explicit] The paper observes that "models learning from practical CL would suffer from overfitting, especially for fwd-r" and notes that "implicitly assuming a uniform transition matrix could be a robust choice" but doesn't explore other regularization approaches
- Why unresolved: The paper only tests data augmentation as a regularization method and doesn't systematically explore other regularization techniques for handling biased transition matrices
- What evidence would resolve it: Comparative experiments showing superior regularization techniques for biased CLL that outperform both uniform assumption methods and data augmentation approaches

### Open Question 3
- Question: Can we develop noise cleaning algorithms specifically designed for CLL that outperform simple percentage-based removal of noisy data?
- Basis in paper: [explicit] The paper shows that "by removing 50% and 75% noisy data, the performance of model learning from practical CL can achieve the one learning from synthetic CL" but questions remain about whether this is optimal
- Why unresolved: The paper only tests simple percentage-based removal of noisy data, leaving open whether more sophisticated noise cleaning approaches could achieve better results with less data loss
- What evidence would resolve it: A noise cleaning algorithm that achieves synthetic CL-level performance with less than 50% data removal, demonstrated through controlled experiments on the CLCIFAR datasets

### Open Question 4
- Question: How can we design CLL algorithms that are robust to feature-dependent complementary labels without requiring explicit modeling of the feature-label dependency?
- Basis in paper: [explicit] The paper's experiment 5.3 shows "the approximation does not significantly affect the classification accuracy" but also notes "there might be other reasons that lead to the performance gap"
- Why unresolved: While the paper shows class-conditional assumptions don't significantly hurt performance, it doesn't develop methods that can explicitly handle feature-dependent labels when they do matter
- What evidence would resolve it: An algorithm that explicitly handles feature-dependent labels and shows improved performance on datasets where such dependencies are known to exist, compared to class-conditional assumption methods

## Limitations
- The study assumes perfect knowledge of the transition matrix, which is unrealistic in practical applications
- Data augmentation techniques used are not specified, making it difficult to assess their impact on results
- Evaluation focuses on specific architectures (ResNet-18) and CIFAR-based datasets, limiting generalizability
- Severe overfitting with true transition matrices suggests current CLL algorithms may be fundamentally ill-suited for real-world noise patterns

## Confidence
- High confidence: Empirical findings about noise rates (3.93% and 2.80%) and bias patterns in human-annotated labels are directly measurable from collected data
- Medium confidence: Conclusion that uniform-assumption algorithms are more robust requires additional validation across diverse noise patterns and dataset characteristics
- Low confidence: Assertion that class-conditional assumption P(ȳ|xi,yi) = P(ȳ|yi) holds approximately is based on limited comparisons and may not generalize to more complex feature distributions

## Next Checks
1. Test the robustness of uniform-assumption vs. true-transition-matrix algorithms across synthetic noise patterns with varying correlation structures to isolate the source of performance differences
2. Implement and evaluate the proposed noise cleaning approach on a held-out validation set to determine optimal cleaning thresholds and assess potential information loss
3. Extend benchmarking to additional architectures (e.g., Vision Transformers) and real-world datasets with naturally occurring complementary labels to evaluate generalizability beyond CIFAR-based experiments