---
ver: rpa2
title: Characterizing Information Seeking Events in Health-Related Social Discourse
arxiv_id: '2308.09156'
source_url: https://arxiv.org/abs/2308.09156
tags:
- events
- chatgpt
- event
- treatment
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the challenge of analyzing health-related
  information-seeking behaviors in social media discourse, focusing on online discussions
  about medications for opioid use disorder (OUD). The authors develop a novel framework
  and dataset (TREAT-ISE) that characterizes five key event types in OUD recovery
  discussions: accessing MOUD, taking MOUD, experiencing psychophysical effects, relapse,
  and tapering MOUD.'
---

# Characterizing Information Seeking Events in Health-Related Social Discourse

## Quick Facts
- arXiv ID: 2308.09156
- Source URL: https://arxiv.org/abs/2308.09156
- Reference count: 18
- Primary result: Novel framework and dataset (TREAT-ISE) for characterizing information-seeking behaviors in OUD recovery discussions, achieving 77.4% weighted F1 with XLNet

## Executive Summary
This study develops a novel framework and dataset to analyze health-related information-seeking behaviors in social media discourse, focusing on online discussions about medications for opioid use disorder (OUD). Through collaboration with domain experts, the authors define five key event types in OUD recovery discussions and annotate 5,083 Reddit posts. The research establishes benchmark performance using various machine learning and deep learning models, while also evaluating ChatGPT's capabilities for this complex, knowledge-intensive task.

## Method Summary
The study defines five event categories (accessing MOUD, taking MOUD, experiencing psychophysical effects, relapse, and tapering MOUD) through expert collaboration, then annotates 5,083 Reddit posts from r/Suboxone. Ten models are evaluated including LR, NBSVM, FastText, BiGRU, BERT variants, XLNet, and ChatGPT using zero-shot, few-shot, and chain-of-thought prompting approaches. The dataset uses an 80/10/10 train/val/test split with weighted F1 as the primary metric.

## Key Results
- TREAT-ISE dataset contains 5,083 annotated Reddit posts with 5 event categories
- XLNet achieves best performance with 77.4% weighted F1 score
- ChatGPT underperforms compared to specialized fine-tuned models, tending to overpredict labels
- Average sample length (122-151 words) presents unique challenges for multilabel classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert-driven schema design captures real-world information needs
- Mechanism: Domain experts identify event types that reflect actual recovery trajectories and user concerns
- Core assumption: Expert guidance aligns with actual user discourse patterns
- Evidence anchors: "Collaborating with experts, we define information-seeking events..."; κ = 0.76 inter-annotator agreement
- Break condition: Schema becomes misaligned if expert guidance diverges from user discourse patterns

### Mechanism 2
- Claim: Transformer models handle domain-specific context better than non-transformers
- Mechanism: XLNet and RoBERTa capture long-range dependencies and domain terminology
- Core assumption: Average post length (122-151 words) requires contextual embeddings
- Evidence anchors: "TREAT-ISE stands apart... significantly longer average sample lengths"; XLNet achieves 77.4% F1
- Break condition: Performance degrades if posts are truncated or terms are out-of-vocabulary

### Mechanism 3
- Claim: ChatGPT struggles with domain-specific nuances
- Mechanism: General LLMs lack fine-grained domain adaptation for specialized terminology
- Core assumption: Zero/few-shot learning insufficient for domain-specific tasks
- Evidence anchors: "Surprisingly, all ChatGPT variants underperformed"; "struggles with domain-specific nuances"
- Break condition: Performance may improve with domain-specific prompts but still lag behind fine-tuned models

## Foundational Learning

- Concept: Multilabel classification vs multiclass classification
  - Why needed here: Posts can express multiple simultaneous information needs
  - Quick check question: If a post asks about both dosage and side effects, how many labels should it receive?

- Concept: Event schema design with domain experts
  - Why needed here: Ensures categories reflect actual recovery trajectories
  - Quick check question: Why is "Relapse or co-occurring substance usage" treated as a single event type?

- Concept: Inter-annotator agreement (Cohen's κ)
  - Why needed here: Validates annotation quality and dataset reliability
  - Quick check question: What does a κ-score of 0.76 indicate about annotator consistency?

## Architecture Onboarding

- Component map: Reddit scraping → filtering → expert annotation → model training → evaluation
- Critical path: 1) Define event schema with experts, 2) Annotate posts with 2+ annotators, 3) Train transformer models, 4) Benchmark against ChatGPT, 5) Analyze confusion matrices
- Design tradeoffs: Manual annotation (higher quality, slower) vs automated labeling; Transformer depth (better performance, slower) vs inference speed
- Failure signatures: Low precision on TM/EP classes; confusion between RL and TM/EP; performance drops on long posts
- First 3 experiments: 1) Compare non-transformer baselines on short vs long posts, 2) Fine-tune XLNet with different learning rates, 3) Test ChatGPT with/without Chain-of-Thought prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM performance (ChatGPT) compare to traditional models on TREAT-ISE?
- Basis: Paper explicitly compares ChatGPT performance to other models
- Why unresolved: Does not explore reasons behind performance differences or improvement potential
- What evidence would resolve it: Further analysis with different LLM architectures, prompting techniques, and fine-tuning strategies

### Open Question 2
- Question: How does classification accuracy vary across different event types?
- Basis: Paper presents classwise performance metrics showing accuracy variations
- Why unresolved: Does not explore underlying factors or challenges specific to each event type
- What evidence would resolve it: Detailed error analysis and investigation into linguistic/contextual nuances of each event type

### Open Question 3
- Question: How does ChatGPT performance on TREAT-ISE compare to other knowledge-intensive tasks?
- Basis: Paper compares performance to other tasks mentioned in related studies
- Why unresolved: Does not provide comprehensive comparison across diverse knowledge-intensive tasks
- What evidence would resolve it: Benchmarking ChatGPT on diverse knowledge-intensive tasks and analyzing performance patterns

## Limitations
- Single subreddit (r/Suboxone) focused on one medication limits generalizability
- ChatGPT evaluation only tested zero-shot and few-shot prompting without exploring intermediate fine-tuning
- Annotation process lacks detailed inter-annotator agreement reporting per event type
- Does not address potential temporal biases in Reddit discussions

## Confidence
- High confidence: Transformer vs non-transformer performance comparison, XLNet outperforming other models, ChatGPT struggling with domain nuances
- Medium confidence: Domain expert schema design capturing real-world needs, ChatGPT overprediction tendency
- Medium confidence: Class imbalance handling through weighted F1 score

## Next Checks
1. Cross-platform validation: Test trained models on OUD discussions from Twitter and health forums
2. Temporal robustness test: Evaluate model performance on posts from different time periods
3. ChatGPT optimization study: Systematically test with various prompt engineering techniques and intermediate fine-tuning approaches