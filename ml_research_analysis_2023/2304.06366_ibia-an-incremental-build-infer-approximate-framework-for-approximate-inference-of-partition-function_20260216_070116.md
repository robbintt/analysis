---
ver: rpa2
title: 'IBIA: An Incremental Build-Infer-Approximate Framework for Approximate Inference
  of Partition Function'
arxiv_id: '2304.06366'
source_url: https://arxiv.org/abs/2304.06366
tags:
- clique
- cliques
- factors
- variables
- obtained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents IBIA, a framework for approximate inference
  of partition functions in probabilistic graphical models. IBIA converts a PGM into
  a sequence of clique tree forests (SCTF) with bounded clique sizes, allowing efficient
  computation of the partition function.
---

# IBIA: An Incremental Build-Infer-Approximate Framework for Approximate Inference of Partition Function

## Quick Facts
- **arXiv ID:** 2304.06366
- **Source URL:** https://arxiv.org/abs/2304.06366
- **Reference count:** 22
- **Primary result:** IBIA achieves better accuracy than variational techniques and comparable or better accuracy than state-of-the-art sampling methods in significantly shorter time.

## Executive Summary
This paper introduces IBIA, a framework for approximate inference of partition functions in probabilistic graphical models. The key innovation is converting a PGM into a sequence of clique tree forests (SCTF) with bounded clique sizes, enabling efficient computation. The framework combines incremental CTF construction, belief propagation calibration, and approximation through marginalization to achieve a favorable accuracy-complexity trade-off. Experiments on 1717 instances from recent UAI competitions demonstrate that IBIA outperforms variational techniques and achieves comparable or better accuracy than sampling methods while running significantly faster.

## Method Summary
IBIA works by incrementally building clique tree forests (CTFs) with bounded clique sizes, calibrating them through belief propagation, and approximating them through marginalization. The framework starts with disjoint cliques and adds factors incrementally until clique size bounds are reached. After each CTF construction, the structure is calibrated to ensure all cliques agree on sepset marginals. The approximation step then reduces clique sizes by removing variables through exact and local marginalization while preserving calibration. This process repeats, creating a sequence of CTFs where each subsequent CTF incorporates more factors from the original PGM, ultimately converging to a single clique tree representing the complete model.

## Key Results
- IBIA achieves better accuracy than variational techniques on benchmark problems
- IBIA shows comparable or better accuracy than state-of-the-art sampling methods
- IBIA runs significantly faster than sampling methods while maintaining accuracy
- The framework successfully handles 1717 instances from recent UAI competitions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incremental CTF construction avoids loop formation and preserves validity
- Mechanism: The algorithm identifies the minimal subgraph (SGmin) impacted by new factors and replaces it with a triangulated subtree (ST'). By triangulating only the elimination graph (GE) rather than the entire modified chordal graph, it maintains chordality and avoids introducing loops.
- Core assumption: The elimination graph GE contains only the variables that need retriangulation (SE) and retained cliques can be reconnected without violating RIP.
- Evidence anchors:
  - [abstract] "The first is an algorithm for incremental construction of CTFs that is guaranteed to give a valid CTF with bounded clique sizes"
  - [section 4.1.3] "Based on Propositions 1 - 3, if the input is a valid CTF, the modified CTF is also a valid CTF"
- Break condition: If GE cannot be triangulated within clique size bounds, the algorithm fails to add the factors and defers them to the next CTF.

### Mechanism 2
- Claim: Local marginalization preserves calibration while reducing clique sizes
- Mechanism: Variables are removed from cliques by summing out their states locally, adjusting both clique and sepset beliefs. This maintains the calibration property (adjacent cliques agree on sepset marginals) while reducing computational complexity.
- Core assumption: The joint beliefs of interface variables (IV) are preserved even after removing non-interface variables (NIV) through marginalization.
- Evidence anchors:
  - [section 4.3] "We reduce clique sizes by removing variables from cliques with size greater than mcsim and locally marginalizing clique beliefs"
  - [section 4.3.2] "All CTs in the approximate CTF, CTFk,a, are calibrated" (Proposition 6)
- Break condition: If removing a variable disconnects a CT or removes an interface variable from all cliques, the local marginalization step must skip that variable.

### Mechanism 3
- Claim: The sequence of CTFs progressively incorporates all factors while maintaining tractable clique sizes
- Mechanism: Starting with CTF0 containing disjoint cliques, factors are added incrementally until clique size bounds are reached. Each CTF is calibrated, then approximated to reduce clique sizes before adding new factors. The final CTF contains a single CT representing the complete model.
- Core assumption: Each CTF in the sequence has bounded clique sizes and preserves the normalization constant of the previous CTF.
- Evidence anchors:
  - [abstract] "the probabilistic graphical model is converted into a sequence of clique tree forests (SCTF) with bounded clique sizes"
  - [section 5] "Theorem 2. Let the sequence {CTF 1, · · ·,CTF n} be the SCTF for a connected graph. Then, the last CTF, CTFn contains a single CT"
- Break condition: If the graph is disconnected after evidence-based simplification, the method constructs separate SCTFs for each connected component.

## Foundational Learning

- Concept: Clique Tree Calibration
  - Why needed here: The framework relies on calibrated CTFs where adjacent cliques agree on sepset marginals, enabling efficient inference and ensuring the normalization constant represents the partition function.
  - Quick check question: What property must be satisfied for two adjacent cliques C_i and C_j in a calibrated CTF?

- Concept: Variable Elimination and Triangulation
  - Why needed here: The incremental construction algorithm uses variable elimination to identify the elimination graph GE that needs retriangulation when adding new factors to maintain chordality.
  - Quick check question: When adding a factor whose scope overlaps with existing cliques, what graph structure must be retriangulated to preserve chordality?

- Concept: Marginalization in Junction Trees
  - Why needed here: Both exact and local marginalization steps in the approximation algorithm require understanding how to sum out variables from clique beliefs while maintaining calibration.
  - Quick check question: How does local marginalization of a variable v from adjacent cliques C_i and C_j affect their beliefs and the sepset belief?

## Architecture Onboarding

- Component map: Incremental Build -> Infer (Calibration) -> Approximate -> Next CTF Construction
- Critical path: Factor Addition → CTF Calibration → Clique Size Reduction → Next CTF Construction
  The partition function estimate is obtained from the normalization constant of the final CTF's single CT.
- Design tradeoffs:
  - Larger mcsp values improve accuracy but increase runtime exponentially due to inference complexity
  - Smaller mcsim values provide more aggressive approximation but may lose important information about interface variables
  - The choice between exact and local marginalization depends on computational cost versus preservation of joint beliefs
- Failure signatures:
  - Memory limit exceeded: Occurs when clique sizes cannot be reduced sufficiently in the approximation step
  - No solution found: Happens when the incremental build step cannot add any factors without violating clique size bounds
  - Poor accuracy: Results from aggressive approximation that removes interface variables or disconnects CTs
- First 3 experiments:
  1. Run IBIA on a small benchmark (e.g., BN_69) with mcsp=20, mcsim=15 and compare the error to LBP
  2. Test the incremental build algorithm by adding factors to CTF0 and verifying the resulting CTF is valid
  3. Evaluate the approximation step by applying exact marginalization to CTF1 and checking that the normalization constant is preserved

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal heuristic for choosing variables for local marginalization in the approximate CTF step?
- Basis in paper: [explicit] The paper proposes a heuristic based on Maximum Local Mutual Information (MLMI) and Maximum Mutual Information (maxMI) for selecting variables for local marginalization, but notes that in some cases random selection yielded better results.
- Why unresolved: The heuristic works well in most cases but not all, suggesting there may be better metrics or combinations of metrics that could improve the approximation quality.
- What evidence would resolve it: Comparative studies of various heuristics (including MLMI/maxMI) across diverse benchmark sets, measuring their impact on approximation accuracy and runtime.

### Open Question 2
- Question: How can the incremental build step be optimized to reduce runtime for large factor sets, particularly in Dynamic Bayesian Networks (DBNs)?
- Basis in paper: [explicit] The paper notes that repeated re-triangulations in the incremental build step dominate runtime for large factor sets, especially in DBNs with over 100,000 factors.
- Why unresolved: While the current algorithm is proven correct, its efficiency for very large networks is limited by the computational cost of repeated re-triangulations.
- What evidence would resolve it: Development and testing of alternative incremental construction strategies that minimize re-triangulation operations while maintaining correctness and clique size bounds.

### Open Question 3
- Question: Can the IBIA framework be extended to handle exact inference of marginals, max-marginals, and the most probable explanation (MPE) efficiently?
- Basis in paper: [inferred] The paper discusses potential extensions of the framework to handle other inference queries, but does not provide implementations or complexity analyses for these cases.
- Why unresolved: The current framework is designed for partition function estimation, and extending it to other inference tasks would require modifications to the approximation algorithms and calibration steps.
- What evidence would resolve it: Implementation and evaluation of IBIA-based algorithms for computing marginals, max-marginals, and MPE on benchmark problems, comparing their accuracy and efficiency to existing methods.

## Limitations
- The framework's theoretical guarantees rely on the correctness of triangulation and marginalization algorithms, requiring more rigorous empirical validation across diverse PGM structures.
- The choice of heuristics for variable selection in local marginalization is not fully specified, which could impact reproducibility.
- Empirical validation is limited to specific benchmark problems, raising questions about generalizability to other PGM structures.

## Confidence

- **High Confidence:** The framework's core approach of incremental CTF construction with bounded clique sizes is well-founded and addresses a clear need in approximate inference.
- **Medium Confidence:** The theoretical guarantees for validity preservation during incremental construction and approximation steps appear sound but require more rigorous empirical validation across diverse PGM structures.
- **Medium Confidence:** The experimental results showing improved accuracy over variational methods and competitive performance with sampling methods are promising, but the comparison could benefit from additional baselines and larger-scale experiments.

## Next Checks

1. **Structural Verification:** Implement the incremental build algorithm and verify that it maintains chordality and bounded clique sizes across a variety of PGM structures, particularly those with complex factor scopes and evidence configurations.

2. **Approximation Analysis:** Conduct ablation studies to quantify the impact of exact versus local marginalization choices on partition function accuracy, and validate that the calibration property is preserved across different levels of approximation.

3. **Scalability Assessment:** Test the framework's performance on larger-scale PGM benchmarks beyond the UAI competition instances, particularly focusing on models with thousands of variables and complex dependency structures to assess practical scalability limits.