---
ver: rpa2
title: 'WHAT, WHEN, and HOW to Ground: Designing User Persona-Aware Conversational
  Agents for Engaging Dialogue'
arxiv_id: '2306.03361'
source_url: https://arxiv.org/abs/2306.03361
tags:
- persona
- response
- personalized
- responses
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for building a personalized open-domain
  dialogue system to address the WWH (WHAT, WHEN, and HOW) problem for natural response
  generation in a commercial setting. The approach involves weighted dataset blending,
  negative persona information augmentation methods, and the design of personalized
  conversation datasets.
---

# WHAT, WHEN, and HOW to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue

## Quick Facts
- arXiv ID: 2306.03361
- Source URL: https://arxiv.org/abs/2306.03361
- Reference count: 11
- Primary result: A method for building personalized open-domain dialogue systems that balance fluency and persona-grounding through weighted dataset blending and negative persona augmentation

## Executive Summary
This paper presents a method for building personalized open-domain dialogue systems that address the WWH (WHAT, WHEN, and HOW) problem for natural response generation in commercial settings. The approach combines weighted dataset blending, negative persona information augmentation, and response-type labels to create a system that effectively balances dialogue fluency and grounding propensity. The method achieves improved performance in both subjective human evaluations and objective metrics compared to baseline models.

## Method Summary
The approach involves fine-tuning an 18B parameter LLM using three key techniques: weighted dataset blending of personalized and non-personalized responses, negative persona augmentation to teach when not to ground persona information, and response type labels to improve controllability and explainability. The system uses the Multi-Session Personalized Dialogue (MSPD) dataset combined with casual conversation data, and trains the model to generate both responses and response type labels (personalized or casual) simultaneously.

## Key Results
- The model effectively balances dialogue fluency and grounding propensity through weighted dataset blending
- Negative persona augmentation teaches the model when NOT to ground persona information
- Response Type Labels improve controllability and explainability of grounded responses
- Subjective human evaluations show improved sensibleness and specificity scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted dataset blending controls the model's grounding propensity by adjusting the proportion of personalized versus casual response data.
- Mechanism: The system divides the personalized dataset into personalized responses (DMSPD-PR) and non-personalized responses (DMSPD-NPR). By tuning blending weights for each dataset type, the model learns to ground persona information at the desired frequency, balancing fluency and personalization.
- Core assumption: Different dataset types have distinct grounding propensities, and the model can learn to balance these propensities through exposure to varied proportions during training.
- Evidence anchors:
  - [abstract]: "weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets"
  - [section]: "We control the model’s persona-grounding level by adjusting the blending weights of the conversational datasets"
  - [corpus]: Weak evidence - no direct corpus support for blending weights affecting grounding propensity

### Mechanism 2
- Claim: Negative persona augmentation teaches the model when NOT to ground persona information, preventing overly frequent personalization.
- Mechanism: For non-personalized responses, the model is given irrelevant persona subsets as input. This trains the model to recognize when persona information is not contextually appropriate and to generate casual responses instead.
- Core assumption: The model can learn from negative examples to suppress persona grounding when the context doesn't warrant it.
- Evidence anchors:
  - [abstract]: "negative persona information augmentation methods"
  - [section]: "We intentionally include a persona subset consisting of all contextually irrelevant persona attributes in the input for non-personalized responses"
  - [corpus]: No direct corpus evidence for negative persona augmentation improving WHEN decisions

### Mechanism 3
- Claim: Response Type Labels (RTL) provide explicit control over WHEN to ground and improve explainability for model decisions.
- Mechanism: The model is trained to generate both a response and an RTL token (<PRTL> for personalized, <CRTL> for casual). During inference, the RTL can be provided as input to explicitly control response type, and the generated RTL explains the model's decision.
- Core assumption: The model can learn to associate specific dialogue contexts with appropriate response types and that generating the label improves both control and transparency.
- Evidence anchors:
  - [abstract]: "introducing a response-type label to improve the controllability and explainability of the grounded responses"
  - [section]: "we also employ the Response Type Label (RTL) to improve the explainability of the model’s generated responses"
  - [corpus]: Weak evidence - no direct corpus support for RTL improving explainability

## Foundational Learning

- Concept: Dataset blending and augmentation
  - Why needed here: To create a balanced model that can generate both personalized and casual responses appropriately, avoiding the extremes of over-grounding or under-grounding.
  - Quick check question: What happens to the model's F1 score and PPL when you increase the weight of the negative persona subset dataset?

- Concept: Response type classification
  - Why needed here: To enable explicit control over when the model generates personalized versus casual responses, which is crucial for maintaining natural conversation flow in commercial applications.
  - Quick check question: How does providing the <PRTL> or <CRTL> token at inference time affect the type of response generated?

- Concept: Grounding evaluation metrics
  - Why needed here: To assess whether the model is appropriately grounding persona information (both frequency and relevance), which is the core objective of the personalized dialogue system.
  - Quick check question: What's the difference between the F1 score and P-Coverage metric in evaluating persona grounding?

## Architecture Onboarding

- Component map:
  - Input layer: User demographics, persona subset, dialogue context
  - Processing layer: 18B parameter LLM with dataset blending and augmentation training
  - Output layer: Response generation with optional RTL
  - Control mechanism: RTL token for explicit response type control
  - Evaluation layer: Objective metrics (PPL, F1, P-Coverage) and subjective human evaluation

- Critical path: Input → Dataset blending/augmentation → Model inference → RTL generation/control → Response output → Evaluation

- Design tradeoffs:
  - Grounding frequency vs. response fluency (controlled by blending weights)
  - Model complexity (18B parameters) vs. training/inference efficiency
  - Control explicitness (RTL) vs. model autonomy
  - Dataset curation effort vs. model performance

- Failure signatures:
  - Low PPL but poor grounding (model is fluent but doesn't personalize)
  - High grounding but unnatural responses (model personalizes at wrong times)
  - Inaccurate RTL predictions (control mechanism fails)
  - Poor performance on out-of-domain persona (model overfits to training personas)

- First 3 experiments:
  1. Train baseline model with only positive persona attributes, measure PPL, F1, and P-Coverage
  2. Add negative persona attribute augmentation, compare grounding propensity and fluency to baseline
  3. Add negative persona subset augmentation, measure changes in grounding propensity and response quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's grounding propensity change when varying the blending weights of the MSPD-PR and MSPD-NPR datasets?
- Basis in paper: [explicit] The paper discusses the trade-off between model fluency and grounding propensity, and how adjusting the blending weights can control this balance.
- Why unresolved: The paper provides some insights into this relationship, but does not offer a comprehensive analysis of how different blending weight combinations affect the model's grounding propensity.
- What evidence would resolve it: A detailed study analyzing the model's grounding propensity across a range of blending weight combinations, with quantitative metrics and qualitative analysis of the generated responses.

### Open Question 2
- Question: How does the model's performance vary across different grounding types (hard, soft, consistent, inconsistent) in terms of fluency and coherence?
- Basis in paper: [explicit] The paper introduces four grounding type categorizations and evaluates the model's performance using these categories in subjective evaluation.
- Why unresolved: While the paper provides some insights into the model's performance across different grounding types, it does not offer a comprehensive analysis of how each type affects fluency and coherence.
- What evidence would resolve it: A detailed study analyzing the model's performance across different grounding types, with quantitative metrics and qualitative analysis of the generated responses, focusing on fluency and coherence.

### Open Question 3
- Question: How does the model's performance vary across different domains (e.g., casual, empathetic, knowledge-based) in terms of grounding propensity and fluency?
- Basis in paper: [explicit] The paper mentions the use of different conversational datasets (Dcasual) to train a more balanced model, but does not provide a detailed analysis of the model's performance across different domains.
- Why unresolved: While the paper acknowledges the importance of training the model on diverse datasets, it does not offer a comprehensive analysis of how the model's performance varies across different domains.
- What evidence would resolve it: A detailed study analyzing the model's performance across different domains, with quantitative metrics and qualitative analysis of the generated responses, focusing on grounding propensity and fluency.

## Limitations
- The negative persona augmentation strategy's generalization capability to edge cases remains uncertain
- Optimal blending weight selection lacks systematic analysis across diverse conversational contexts
- The Response Type Label mechanism's contribution to explainability lacks rigorous validation

## Confidence
- High Confidence: Weighted dataset blending for controlling grounding propensity is well-established in literature
- Medium Confidence: Negative persona augmentation shows promise but lacks comprehensive validation
- Low Confidence: Response Type Label mechanism's explainability benefits are questionable without rigorous evaluation

## Next Checks
1. **Cross-domain generalization test**: Evaluate the model's performance when applied to personas and conversation topics not present in the training data.
2. **RTL prediction accuracy analysis**: Conduct detailed error analysis of the model's RTL predictions against human judgments.
3. **Ablation study on negative persona sampling strategy**: Systematically vary the number and selection criteria for negative persona samples to determine optimal balance.