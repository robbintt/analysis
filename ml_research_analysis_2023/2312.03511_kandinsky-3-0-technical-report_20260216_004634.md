---
ver: rpa2
title: Kandinsky 3.0 Technical Report
arxiv_id: '2312.03511'
source_url: https://arxiv.org/abs/2312.03511
tags:
- kandinsky
- image
- generation
- text
- report
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Kandinsky 3.0 is a large-scale text-to-image generation model based
  on latent diffusion. It uses a 11.9 billion parameter architecture with a large
  transformer-based U-Net backbone, an 8.6 billion parameter text encoder, and a modified
  VQGAN decoder.
---

# Kandinsky 3.0 Technical Report

## Quick Facts
- arXiv ID: 2312.03511
- Source URL: https://arxiv.org/abs/2312.03511
- Reference count: 40
- Key outcome: Large-scale text-to-image model with 11.9B parameters outperforming previous versions in human evaluations

## Executive Summary
Kandinsky 3.0 is a large-scale text-to-image generation model based on latent diffusion that achieves state-of-the-art performance through architectural innovations. The model uses a 11.9 billion parameter architecture with a large transformer-based U-Net backbone, an 8.6 billion parameter text encoder, and a modified VQGAN decoder. Trained on 150 million text-image pairs using a multi-stage training strategy across different resolutions, the model achieves superior text understanding and visual quality compared to previous versions while simplifying the architecture through a single-stage pipeline.

## Method Summary
Kandinsky 3.0 employs a latent diffusion architecture with a two-stage pipeline: text encoding using Flan-UL2 (8.6B parameters) followed by U-Net-based denoising (3.0B parameters) and image reconstruction via Sber-MoVQGAN (0.27B parameters). The model uses BigGAN-deep style residual blocks with bottlenecks to increase depth while controlling parameter growth, and removes intermediate diffusion mapping for simplified generation. Training proceeds through progressive resolutions from 256² to 1024² using a dataset of 150M filtered image-text pairs from LAION-5B, COYO-700M, and internal sources.

## Key Results
- Human evaluations show Kandinsky 3.0 outperforms Kandinsky 2.2 and SDXL in text understanding and visual quality
- The model achieves improved performance through ten times larger text encoder (8.6B vs 0.62B)
- Simplified single-stage architecture maintains quality while reducing complexity compared to previous versions

## Why This Works (Mechanism)

### Mechanism 1: Deep U-Net Architecture with Bottlenecks
The large-scale U-Net backbone with increased depth and bottleneck design directly improves generation quality by enabling deeper feature extraction while controlling parameter growth. Replacing standard residual blocks with BigGAN-deep style blocks that use bottlenecks allows the model to double the number of convolutional layers while keeping the parameter count similar. This deeper architecture processes visual information more effectively, improving both image fidelity and text alignment.

### Mechanism 2: Large Text Encoder for Better Semantic Understanding
The ten times larger text encoder (Flan-UL2 8.6B) significantly improves text understanding and alignment compared to previous models. A larger text encoder trained on diverse language tasks and fine-tuned with instruction data provides richer semantic representations, enabling better alignment between text prompts and generated images. This allows the model to understand complex prompts and generate images that more accurately reflect the described content.

### Mechanism 3: Simplified Single-Stage Pipeline
The removal of diffusion mapping and adoption of a single-stage pipeline simplifies the architecture while maintaining or improving quality. By eliminating the intermediate diffusion mapping step between latent spaces, the model directly generates images from text embeddings, reducing complexity and potential information loss. This streamlined approach focuses computational resources on the core generation task.

## Foundational Learning

- **Latent Diffusion Models**: Understanding the fundamentals of how diffusion models work in latent space is essential since the entire Kandinsky 3.0 model is built on latent diffusion. *Quick check*: What is the key difference between pixel-space and latent-space diffusion models, and why is latent diffusion more computationally efficient?

- **Transformer-based Architectures in Vision**: The U-Net backbone incorporates transformer elements, and the text encoder is a large transformer. Understanding how transformers process visual and textual data is crucial. *Quick check*: How do vision transformers handle spatial relationships differently from convolutional networks, and what are the implications for image generation?

- **Text Encoding and Semantic Representation**: The model's text understanding capability depends on how well the text encoder converts prompts into meaningful embeddings that guide image generation. *Quick check*: What are the key differences between standard language models and instruction-tuned models like Flan-UL2 in terms of their ability to understand and follow complex prompts?

## Architecture Onboarding

- **Component map**: Text prompt → Text Encoder (8.6B) → U-Net Backbone (3.0B) with BigGAN-deep residual blocks → Image Decoder (0.27B) → Final image
- **Critical path**: Text prompt → Text Encoder → U-Net denoising process → Image Decoder → Final image
- **Design tradeoffs**: Larger text encoder improves text understanding but increases computational cost; removing diffusion mapping simplifies architecture but requires robust direct text-to-image mapping; bottleneck blocks increase depth without proportional parameter growth but may introduce optimization challenges
- **Failure signatures**: Poor text-image alignment suggests issues with the text encoder or cross-attention; low image quality or artifacts may indicate problems with the U-Net architecture or decoder; training instability could result from the simplified single-stage approach
- **First 3 experiments**:
  1. Test text-to-image generation with simple prompts to verify basic functionality and text alignment
  2. Evaluate image quality and resolution handling across different input sizes
  3. Assess inpainting and outpainting capabilities to verify application-specific components

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific limitations of Kandinsky 3.0's semantic coherence between input text and generated images, and how could these be addressed through more efficient use of the Flan-UL2 text encoder?
- Basis in paper: The paper states that one of the limitations is the improvement of semantic coherence between the input text and the generated image due to more efficient use of the Flan-UL2 text encoder potential.
- Why unresolved: The paper mentions this as a limitation but does not provide specific details on the current shortcomings or potential solutions.
- What evidence would resolve it: Detailed analysis of the current semantic coherence performance, identification of specific areas where the text encoder could be used more effectively, and proposed architectural or training modifications to address these issues.

### Open Question 2
- Question: How does the performance of Kandinsky 3.0 compare to other state-of-the-art text-to-image models on specialized domains like medical imaging or technical diagrams, where precise visual details are crucial?
- Basis in paper: The paper focuses on general image generation and mentions specific domains in the context of style transfer, but does not provide a detailed comparison on specialized domains.
- Why unresolved: The paper does not present specific evaluations or comparisons on specialized domains that require high precision in visual details.
- What evidence would resolve it: Comprehensive benchmarking of Kandinsky 3.0 against other models on specialized datasets, with quantitative metrics and qualitative analysis of performance in these domains.

### Open Question 3
- Question: What are the potential improvements in image quality based on human evaluations, and what specific aspects of the model architecture or training process could be modified to achieve these improvements?
- Basis in paper: The paper states that it is possible to improve the image quality based on human evaluations significantly.
- Why unresolved: The paper mentions this as a potential area for improvement but does not provide specific details on the aspects of image quality that need enhancement or the modifications required.
- What evidence would resolve it: Detailed analysis of human evaluation results, identification of specific quality aspects that need improvement, and proposed architectural or training modifications to address these issues.

## Limitations

- Human evaluation methodology lacks transparency regarding evaluator selection and prompt specifications
- Architectural improvements lack ablation studies to isolate individual component contributions
- Dataset composition and quality filtering processes are not fully specified

## Confidence

**High Confidence Claims:**
- Technical architecture specifications (parameter counts, component sizes)
- Existence of multi-stage training pipeline and progressive resolution approach
- Applications mentioned (inpainting, outpainting, image-to-video)

**Medium Confidence Claims:**
- Outperformance of Kandinsky 2.2 and SDXL in human evaluations
- Simplified single-stage architecture maintaining quality
- Effectiveness of bottleneck block design

**Low Confidence Claims:**
- Comparison with DALL-E 3 performance
- Specific improvement claims from larger text encoder
- Simplified pipeline maintaining quality across all use cases

## Next Checks

1. **Ablation Study on Architectural Components**: Conduct controlled experiments isolating the impact of three key modifications - text encoder size, removal of diffusion mapping, and bottleneck block design. This would clarify which changes actually contribute to performance improvements versus which are neutral or potentially harmful.

2. **Transparent Human Evaluation Protocol**: Release the exact prompt sets, evaluator selection criteria, and scoring methodology used in human comparisons. This would allow independent verification of the claimed performance advantages and help identify potential biases in the evaluation process.

3. **Cross-Dataset Generalization Test**: Evaluate Kandinsky 3.0 on multiple independent test sets with varying domain distributions to assess whether performance claims hold across different types of visual content, not just the datasets used in training or the specific evaluation sets chosen for comparison.