---
ver: rpa2
title: Pruning at Initialization -- A Sketching Perspective
arxiv_id: '2305.17559'
source_url: https://arxiv.org/abs/2305.17559
tags:
- pruning
- data
- mask
- initialization
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the lottery ticket hypothesis (LTH) from the
  perspective of sketching, a technique from matrix multiplication. The authors establish
  a theoretical connection between finding sparse subnetworks at initialization (LTH)
  and sketching algorithms.
---

# Pruning at Initialization -- A Sketching Perspective

## Quick Facts
- arXiv ID: 2305.17559
- Source URL: https://arxiv.org/abs/2305.17559
- Reference count: 40
- Key outcome: Theoretical connection between lottery ticket pruning and matrix sketching, with randomized mask approach improving performance across architectures

## Executive Summary
This paper establishes a theoretical framework connecting pruning at initialization (lottery ticket hypothesis) with matrix sketching algorithms. The authors show that finding sparse subnetworks at initialization is mathematically equivalent to approximating matrix-vector multiplication through sketching. They derive error bounds for pruned linear models and analyze popular methods like SynFlow and SNIP through this lens. Empirically, they demonstrate that randomizing pruning masks based on method scores often improves performance while providing theoretical guarantees.

## Method Summary
The method involves computing pruning scores for all weights, converting these scores to probability distributions, and sampling sparse masks according to these probabilities (Algorithm 1). The approach can be applied to any pruning method by replacing deterministic thresholding with randomized sampling. Theoretical analysis derives bounds on approximation error that depend on the distance between initial and final weights, which is small under NTK assumptions. The method is validated across multiple architectures (VGG-19, ResNet-20, WideResNet) and datasets (CIFAR-10, CIFAR-100, Tiny-ImageNet).

## Key Results
- Sketching perspective provides theoretical justification for lottery ticket hypothesis through matrix multiplication approximation
- Randomized masks based on pruning scores improve performance in most cases compared to deterministic thresholding
- SynFlow's pruning strategy is mathematically equivalent to optimal sketching when using random data as input
- NTK-based error bounds show dependence on weight distance ∥w⋆ - w0∥, which is small in the NTK regime

## Why This Works (Mechanism)

### Mechanism 1
Pruning at initialization can be viewed as a matrix sketching problem where selecting a subset of weights is equivalent to choosing columns in matrix multiplication. The probability distribution for selecting weights (proportional to input data norm times weight absolute value) matches the optimal sketching strategy, allowing existing sketching error bounds to analyze pruned network approximation error.

### Mechanism 2
SynFlow's pruning strategy is mathematically equivalent to the optimal sketching algorithm when using random data as input. When applied with random input data instead of all-ones vector, SynFlow's score calculation becomes proportional to optimal sketching probability, meaning randomized masks based on SynFlow scores achieve the same theoretical guarantees as sketching.

### Mechanism 3
The approximation error of a pruned network depends on the distance between initial and final weights, which is small under NTK assumptions. The paper derives a bound on expected error proportional to 1/s (sparsity) that depends on ∥w⋆ - w0∥, which is small due to linearized neural network training dynamics in the NTK regime.

## Foundational Learning

- Matrix sketching and randomized algorithms for matrix multiplication approximation
  - Why needed here: The core insight is that pruning at initialization is equivalent to a well-studied sketching problem, allowing application of existing theoretical tools
  - Quick check question: What is the optimal probability distribution for selecting columns in matrix sketching, and how does it relate to pruning weights?

- Neural Tangent Kernel (NTK) and linearized neural network training dynamics
  - Why needed here: Theoretical analysis of pruning error relies on NTK assumptions about how little network weights change during training
  - Quick check question: Under NTK assumptions, what is the relationship between the distance of weights at initialization and after training?

- Sparse subnetwork identification and lottery ticket hypothesis
  - Why needed here: The paper builds on lottery ticket hypothesis but analyzes it from a new theoretical perspective
  - Quick check question: How does the sketching perspective provide new justification for why good sparse subnetworks might be data-independent?

## Architecture Onboarding

- Component map: Mask sampling procedure (Algorithm 1) -> any pruning method scores -> probability conversion -> mask sampling -> network training
- Critical path: 1) Compute pruning scores for all weights 2) Convert scores to probabilities 3) Sample mask entries according to these probabilities 4) Apply mask and train remaining network
- Design tradeoffs: Randomization introduces variance but provides theoretical guarantees and often improves results; more randomization (lower s) gives better guarantees but worse empirical performance
- Failure signatures: If NTK assumptions don't hold, theoretical bounds become meaningless; if pruning scores don't correlate with weight importance, randomization won't help; if network architecture is very shallow or narrow, NTK assumptions may break
- First 3 experiments:
  1. Apply randomized mask approach to SynFlow on CIFAR-10 with VGG-19 and compare to vanilla SynFlow
  2. Test sketching equivalence by comparing SynFlow scores with optimal sketching probabilities on linear data
  3. Validate NTK-based error bound by measuring ∥w⋆ - w0∥ for different architectures and datasets

## Open Questions the Paper Calls Out

### Open Question 1
How do the proposed sketching-based mask randomization techniques perform in structured pruning settings? The paper mentions structured pruning methods generally include more parameters than unstructured ones but are more beneficial for computational time on standard hardware. It states that a method originally presented for unstructured pruning that employs scoring has been extended for structured pruning at initialization, but empirical results are not provided.

### Open Question 2
How does the sketching-based approach for pruning at initialization perform when applied to deeper models beyond the linear features framework? The paper mentions future research may generalize the simple linear features framework to deeper models where the search space for the mask is even larger, noting that the search space in this case might be ambiguous, i.e., two different masks can lead to the same output.

### Open Question 3
How does the performance of sketching-based pruning methods compare to other data-independent pruning methods, such as SynFlow and IMP, when applied to a wider range of architectures and datasets? The paper presents empirical results comparing sketching-based methods to original SynFlow and SNIP on specific architectures and datasets but does not provide a comprehensive comparison with other data-independent pruning methods.

## Limitations

- Theoretical analysis heavily relies on NTK regime assumptions which may not hold for practical networks with finite width
- Sketching equivalence assumes linear features, but deep networks exhibit non-linear behavior that breaks the connection
- Randomized mask approach introduces additional variance that could hurt performance in some settings
- Connection between sketching and pruning requires careful validation across diverse architectures and tasks

## Confidence

- High confidence: The matrix sketching perspective as a mathematical framework for understanding pruning at initialization
- Medium confidence: The NTK-based theoretical bounds, as they depend on assumptions that may not hold in practice
- Low confidence: The claim that SynFlow is mathematically equivalent to optimal sketching in all settings, as this requires specific conditions on data distribution and network linearity

## Next Checks

1. NTK validity test: Measure the distance ∥w⋆ - w0∥ across different network widths and architectures to empirically validate when NTK assumptions break down

2. Randomization sensitivity: Systematically vary the randomization parameter (s) in the mask sampling algorithm across multiple runs to characterize the tradeoff between theoretical guarantees and empirical performance

3. Sketching equivalence verification: Implement the sketching algorithm with random data input to SynFlow and compare the resulting masks with optimal sketching masks on linear problems to verify the claimed mathematical equivalence