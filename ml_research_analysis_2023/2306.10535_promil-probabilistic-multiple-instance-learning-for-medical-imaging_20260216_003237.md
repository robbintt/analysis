---
ver: rpa2
title: 'ProMIL: Probabilistic Multiple Instance Learning for Medical Imaging'
arxiv_id: '2306.10535'
source_url: https://arxiv.org/abs/2306.10535
tags:
- instance
- learning
- promil
- instances
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProMIL introduces a novel percentage-based Multiple Instance Learning
  (MIL) assumption motivated by medical applications, where a bag is positive if a
  certain percentage of its instances are positive. The method uses Bernstein polynomial
  estimation to automatically determine the optimal threshold for classification.
---

# ProMIL: Probabilistic Multiple Instance Learning for Medical Imaging

## Quick Facts
- **arXiv ID**: 2306.10535
- **Source URL**: https://arxiv.org/abs/2306.10535
- **Reference count**: 40
- **Primary result**: ProMIL achieves 86.36% accuracy on ultrasound video classification for congenital heart defects, outperforming baseline MIL methods

## Executive Summary
ProMIL introduces a novel percentage-based Multiple Instance Learning (MIL) assumption for medical imaging, where a bag is positive if a certain percentage of its instances are positive. The method uses Bernstein polynomial estimation to automatically determine the optimal threshold for classification by learning the quantile parameter q during training. Experiments demonstrate ProMIL outperforms existing instance-based MIL methods on synthetic and real-world medical datasets, including histopathology images and ultrasound videos. The approach provides interpretable instance-level predictions, making it suitable for critical medical applications where understanding which instances contribute to the bag label is essential.

## Method Summary
ProMIL is an instance-based MIL method that classifies each instance in a bag and computes the q-quantile of their predictions. During training, the quantile parameter q is learned automatically using Bernstein polynomial estimation, which provides differentiable quantile computation. A bag is labeled positive if the q-quantile exceeds 0.5. The method uses binary cross-entropy loss and backpropagation to update both the instance classifier network and the quantile parameter. This percentage-based assumption is particularly well-suited for medical applications where the positive bag condition depends on the presence of a certain fraction of positive instances rather than just one.

## Key Results
- ProMIL achieves 86.36% accuracy on ultrasound video classification for congenital heart defects, outperforming AbMILP (79.77%) and instance-max (70.09%)
- On MNIST-bag with 40% threshold, ProMIL achieves 95.32% accuracy, exceeding instance-max (86.15%) and instance-mean (84.67%)
- Provides interpretable instance-level predictions through Grad-CAM heatmaps validated by medical experts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ProMIL automatically determines the optimal percentage threshold by learning the quantile parameter q
- Mechanism: Uses Bernstein polynomial estimation to compute q-quantile of instance predictions, with q as a trainable parameter optimized during training
- Core assumption: The optimal threshold corresponds to a specific quantile that generalizes well to unseen data
- Break condition: If learned q is too extreme (close to 0 or 1), model may overfit to specific instance prediction distributions

### Mechanism 2
- Claim: Instance-level predictions aggregated via q-quantile provide interpretable bag-level decisions
- Mechanism: After classifying each instance, predictions are sorted and q-quantile computed; bag is positive if quantile exceeds 0.5
- Core assumption: The relevant fraction of instances for bag label is captured by a single quantile of prediction distribution
- Break condition: If true relationship between instances and bag label is not monotonic in quantile, approach may fail

### Mechanism 3
- Claim: ProMIL outperforms baselines by adapting to actual percentage of positive instances rather than fixed thresholds
- Mechanism: Learns q to estimate true percentage threshold during training, handling datasets where positive condition depends on fraction of instances
- Core assumption: Percentage-based assumption is well-approximated by single quantile of instance prediction distribution
- Break condition: If percentage threshold varies widely between bags, single learned q may not generalize

## Foundational Learning

- **Concept**: Multiple Instance Learning (MIL) and standard assumption that bag is positive if at least one instance is positive
  - Why needed here: Understanding standard MIL assumption helps see why ProMIL's percentage-based approach is novel
  - Quick check question: In standard MIL, if bag has 10 instances and 2 are positive, is bag labeled positive? (Yes, because at least one instance is positive)

- **Concept**: Quantile estimation and Bernstein polynomials
  - Why needed here: ProMIL uses Bernstein polynomial estimation for differentiable quantiles, key to learning optimal q
  - Quick check question: What is 0.5-quantile of sorted list [0.1, 0.3, 0.5, 0.7, 0.9]? (0.5, the median)

- **Concept**: Instance-based vs. representation-based MIL methods
  - Why needed here: ProMIL is instance-based, so understanding distinction explains interpretability advantage
  - Quick check question: In instance-based MIL, do we classify bags directly or first classify instances then aggregate? (First classify instances, then aggregate)

## Architecture Onboarding

- **Component map**: Input bags -> Instance classifier f -> Bernstein quantile estimator -> BCE loss -> Updated f and q

- **Critical path**: 1) Pass each instance through f to get predictions 2) Sort predictions ascending 3) Compute q-quantile using Bernstein polynomial formula 4) Apply BCE loss and backpropagate to update f and q

- **Design tradeoffs**: ProMIL vs. Instance-MAX (learns threshold vs. assumes one positive suffices), ProMIL vs. Instance-MEAN (focuses on fraction vs. averages all), ProMIL vs. representation-based (more interpretable but potentially less accurate on complex patterns)

- **Failure signatures**: q collapsing to 0 or 1 (overfitting to extreme quantiles), predictions concentrated near 0 or 1 (numerical instability), training loss not decreasing (learning rate, batch size, or quantile computation issues)

- **First 3 experiments**: 1) Run ProMIL on MNIST-bag with known q (e.g., 0.4) and verify learned q matches 2) Compare ProMIL vs. Instance-MAX and Instance-MEAN on small histopathology dataset 3) Visualize Grad-CAM heatmaps on ultrasound dataset to confirm interpretability

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the percentage threshold q learned by ProMIL vary across different medical applications and what determines its optimal value for specific tasks?
  - Basis: Paper shows q is learned automatically but doesn't analyze variation across medical domains or factors influencing optimal value
  - Resolution needed: Systematic experiments across multiple medical domains showing relationship between task characteristics and learned q values

- **Open Question 2**: What is the impact of varying bag sizes on ProMIL's performance in real-world medical applications?
  - Basis: Method shown robust to bag size on MNIST-bag but medical datasets have vastly different instance counts
  - Resolution needed: Extensive experiments on medical datasets with controlled bag size variations showing performance degradation points

- **Open Question 3**: How does ProMIL's interpretability compare to traditional clinical assessment methods in terms of accuracy and workflow integration?
  - Basis: Paper discusses interpretability through Grad-CAM but doesn't compare to standard clinical practices
  - Resolution needed: Comparative studies measuring ProMIL's interpretability against traditional clinical methods using diagnostic accuracy, time efficiency, and workflow integration metrics

## Limitations
- Performance depends on stability of Bernstein polynomial quantile estimator and learned q parameter
- Small sample sizes in medical datasets raise questions about generalization to larger populations
- Computational cost of sorting instances may become prohibitive for bags with many instances

## Confidence
- **High confidence**: Core mechanism of using Bernstein polynomials for differentiable quantile estimation is well-established
- **Medium confidence**: Claim of outperforming baseline methods supported by experimental results but limited by small sample sizes
- **Low confidence**: Assertion of superior interpretability for medical applications not rigorously validated beyond visual inspection

## Next Checks
1. Test ProMIL on synthetic datasets with varying percentage thresholds to verify learned q matches ground truth across distributions
2. Conduct ablation studies comparing ProMIL with fixed quantile values (e.g., q=0.4 vs q=0.6) to assess benefit of automatic threshold learning
3. Evaluate ProMIL on larger medical imaging datasets (e.g., CheXpert or MIMIC-CXR) to assess scalability and robustness beyond current limited applications