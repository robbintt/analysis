---
ver: rpa2
title: Divide & Conquer for Entailment-aware Multi-hop Evidence Retrieval
arxiv_id: '2311.02616'
source_url: https://arxiv.org/abs/2311.02616
tags:
- question
- retrieval
- relevance
- arxiv
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that textual entailment is a key relevance
  signal for multi-hop question answering evidence retrieval, beyond semantic equivalence.
  It proposes two ensemble models, EAR and EARnest, that jointly consider semantic
  equivalence and textual entailment relevance signals.
---

# Divide & Conquer for Entailment-aware Multi-hop Evidence Retrieval

## Quick Facts
- arXiv ID: 2311.02616
- Source URL: https://arxiv.org/abs/2311.02616
- Authors: 
- Reference count: 13
- The ensemble models EAR and EARnest significantly outperform all single retrieval models and intuitive ensemble baselines on HotpotQA, achieving 10% higher MAP than the best single model.

## Executive Summary
This paper addresses the challenge of multi-hop question answering evidence retrieval by recognizing that textual entailment is a key relevance signal beyond semantic equivalence. The authors propose two ensemble models, EAR and EARnest, that jointly consider semantic equivalence and textual entailment relevance signals. EARnest further enhances retrieval by considering relatedness between evidence sentences through named entity similarity. These ensemble approaches significantly outperform single retrieval models and intuitive ensemble baselines on the HotpotQA dataset, demonstrating the importance of combining multiple relevance signals for complex multi-hop questions.

## Method Summary
The paper proposes two ensemble models for multi-hop evidence retrieval: EAR and EARnest. EAR forms sentence pairs by taking the Cartesian product of top-k sentences from semantic equivalence models (BM25 and MSMARCO CE) and entailment models (QNLI CE), then scores these pairs against the question. EARnest extends EAR by adding a named entity similarity term that promotes sentence pairs sharing named entities. The models are evaluated on bridge-type questions from the HotpotQA development set using precision at k, MAP, and recall at k metrics.

## Key Results
- EAR and EARnest significantly outperform all single retrieval models and intuitive ensemble baselines on HotpotQA
- EARnest achieves 10% higher MAP than the best single model
- The ensemble models demonstrate the importance of jointly considering semantic equivalence and textual entailment signals for multi-hop evidence retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jointly considering semantic equivalence and textual entailment relevance signals improves multi-hop evidence retrieval over single signal approaches.
- Mechanism: The ensemble models EAR and EARnest create pairs of candidate sentences that individually capture either semantic equivalence (via BM25 and MSMARCO CE) or textual entailment (via QNLI CE), then jointly re-rank them. This ensures evidence sentences that might be missed by one signal alone are still retrieved if they contain complementary signals.
- Core assumption: Different relevance signals (semantic equivalence vs. textual entailment) capture orthogonal aspects of relevance for multi-hop questions, and evidence for complex questions often requires both types of signals.
- Evidence anchors:
  - [abstract] "To retrieve evidences that are either semantically equivalent to or entailed by the question simultaneously, we divide the task of evidence retrieval for multi-hop question answering (QA) into two sub-tasks, i.e., semantic textual similarity and inference similarity retrieval."
  - [section] "It is common to utilize an inference model as a reader to infer the correct answer from a subset of retrieved context. However, most works of the evidence ranking only measure the semantic textual similarity between the question and candidate corpus to determine the relevance, and ignore the inference signals."
  - [corpus] Weak corpus support: only 1/5 related papers directly address entailment in multi-hop QA, suggesting this is an under-explored area.
- Break condition: If multi-hop questions rarely require textual entailment signals (e.g., if most evidence can be retrieved via semantic equivalence alone), the added complexity of EAR/EARnest would not provide benefits.

### Mechanism 2
- Claim: Using Cartesian product of top-ranked sentence sets from different base models captures complementary relevance signals that individual models miss.
- Mechanism: EAR forms sentence pairs by taking the Cartesian product of top-k sentences from semantic equivalence models (BM25 ∪ MSMARCO CE) and entailment models (QNLI CE), then scores these pairs against the question. This allows sentences that individually score low on one metric but high on another to be retrieved.
- Core assumption: Evidence sentences for multi-hop questions often form pairs where one sentence provides semantic equivalence and another provides entailment, and this compositional relevance is not captured by single-model ranking.
- Evidence anchors:
  - [section] "EAR and EARnest both jointly consider pairs of candidate sentences top ranked by the base models... The larger K is, the more exhaustive combination of candidate sentence pairs would be considered."
  - [section] "While BM25 and MSMARCO Cross-Encoder capture exact and semantic matches, respectively, they both aim for estimating STS. Thus, we take the union of top-ranked sentences by BM25 and MSMARCO Cross-Encoder as a unified set A = {Sa1, Sa2, Sa3, Sa4}, and top-ranked sentences by QNLI Cross-Encoder as another set B = {Sb1, Sb2, Sb3}. The pairs we consider are P = A × B = {(a, b) | a ∈ A ∧ b ∈ B}."
  - [corpus] Weak corpus support: no direct evidence in corpus papers about Cartesian product strategies for multi-hop QA.
- Break condition: If the top-k sentences from each model already capture all relevant evidence individually, the pairwise combination adds computational cost without benefit.

### Mechanism 3
- Claim: Considering named entity similarity between sentence pairs further improves retrieval by capturing relatedness signals beyond semantic equivalence and textual entailment.
- Mechanism: EARnest adds a binary named entity similarity term (NEST) to the scoring function, activating a promotion mechanism when sentence pairs share named entities. This captures logical connections between evidence sentences that might not be evident from semantic or entailment signals alone.
- Core assumption: Evidence sentences for multi-hop questions are often logically connected via shared named entities, and this connection indicates they can form coherent context for answering the question.
- Evidence anchors:
  - [section] "Evidences for a multi-hop question should be intuitively related, and often logically connected via a shared named entity that would allow a human reader to connect the information they contain. Thus, the presence of a shared named entity between two candidate sentences often indicates the likelihood that the sentence pairs relate to each other and, so that they can be connected to form a coherent context for the question."
  - [section] "When comparing whether two sentences share an entity, we apply basic normalization (i.e., lower case, removing articles and special punctuations) and fuzzy match to tolerate typos, variations, and inclusive match."
  - [corpus] Weak corpus support: no direct evidence in corpus papers about named entity similarity for multi-hop QA evidence retrieval.
- Break condition: If multi-hop questions rarely require evidence sentences to share named entities (e.g., if answers can be formed from unrelated evidence), the named entity similarity term adds unnecessary complexity.

## Foundational Learning

- Concept: Semantic textual similarity vs. textual entailment
  - Why needed here: The paper distinguishes between these two types of relevance signals, with semantic equivalence measuring "do these texts mean the same thing" and textual entailment measuring "can the meaning of one text be inferred from another."
  - Quick check question: Given premise "John bought a car" and hypothesis "John owns a car," would this be semantic equivalence or textual entailment?

- Concept: Dense vs. sparse retrieval models
  - Why needed here: The ensemble models combine both dense models (MSMARCO CE, QNLI CE) and sparse models (BM25) to capture different relevance signals, with dense models handling synonyms and paraphrases while sparse models handle exact matches.
  - Quick check question: Which model type (dense or sparse) would better match "password reset" with "account unlock" - and why?

- Concept: Ensemble learning and aggregation strategies
  - Why needed here: The paper compares different ensemble methods (average ranking, similarity combination, EAR, EARnest) to determine how best to combine multiple base models' outputs for improved retrieval performance.
  - Quick check question: If Model A ranks a sentence at position 1 and Model B ranks it at position 10, what would average ranking output for this sentence's final position?

## Architecture Onboarding

- Component map:
  - Base models: BM25 (sparse lexical match), MSMARCO CE (dense semantic match), QNLI CE (dense entailment match)
  - EAR component: Cartesian product formation, pair scoring, reranking
  - EARnest component: EAR functionality plus named entity similarity checking and promotion
  - Evaluation: P@k, MAP, R@k metrics on HotpotQA development set

- Critical path:
  1. Segment documents into sentences
  2. Apply base models to rank sentences individually
  3. Form Cartesian product of top-k sentences from different model sets
  4. Score sentence pairs against question
  5. (EARnest only) Check for shared named entities and apply promotion
  6. Output top-N sentences as supporting evidence

- Design tradeoffs:
  - K value vs. computational cost: larger K captures more combinations but increases quadratic complexity
  - Precision vs. recall: EARnest's named entity promotion may increase recall but potentially decrease precision
  - Model diversity vs. complexity: more base models capture more signals but increase implementation complexity

- Failure signatures:
  - Low MAP improvement despite high base model performance suggests poor ensemble aggregation
  - High computational cost with minimal performance gain suggests K is too large
  - Performance worse than best single model suggests ensemble strategy is harming rather than helping

- First 3 experiments:
  1. Run base models individually on a small sample to verify they capture complementary signals (check overlap in top-3 rankings)
  2. Test EAR with K=3 on sample to verify Cartesian product and pairwise scoring works correctly
  3. Compare EARnest vs. EAR on sample to verify named entity similarity promotion activates as expected when entities are shared

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the ensemble models (EAR and EARnest) perform on comparison-type questions in HotpotQA, given that this work only evaluates on bridge-type questions?
- Basis in paper: [explicit] The authors state they use "solely the 5918 bridge-type questions out of the 7,405 examples from the development partition in the distractor setting" for evaluation, leaving out comparison-type questions.
- Why unresolved: The performance of EAR and EARnest on comparison-type questions, which are said to be "easier to answer because the necessary information...tends to be present in the question," remains unknown.
- What evidence would resolve it: Experimental results showing the performance of EAR and EARnest on comparison-type questions in HotpotQA, including metrics like P@3, P@5, MAP, R@3, R@5, and R@10.

### Open Question 2
- Question: What is the impact of varying the cut-off parameter K in the EAR and EARnest models on their retrieval performance and computational cost?
- Basis in paper: [explicit] The authors mention that "The cut-off parameter K is used to partition sentences considered as top-ranked by individual base model or not" and discuss the trade-off between exhaustive combination and computational cost.
- Why unresolved: The paper only tests K values of 3 and 5, and the optimal value of K for balancing performance and computational efficiency is not determined.
- What evidence would resolve it: A comprehensive study varying K across a wider range of values, reporting the corresponding retrieval performance and computational costs, to identify the optimal K.

### Open Question 3
- Question: How does the performance of EARnest change when using different named entity recognition and linking tools, or when considering different types of entities beyond common ones like names of people, places, and organizations?
- Basis in paper: [explicit] The authors use SpaCy for named entity recognition and consider "titles of documents and phrases between a pair of single or double quotes" in addition to common entity types.
- Why unresolved: The impact of using different named entity recognition and linking tools, or considering different entity types, on the performance of EARnest is not explored.
- What evidence would resolve it: Experiments replacing SpaCy with other named entity recognition tools, or expanding the types of entities considered, and measuring the impact on EARnest's retrieval performance.

## Limitations

- The paper's core assumption that semantic equivalence and textual entailment are orthogonal signals for multi-hop evidence retrieval is supported by weak empirical evidence in the corpus.
- The effectiveness of named entity similarity in EARnest is particularly under-supported, with no comparative analysis showing whether this signal adds unique value beyond semantic and entailment signals.
- While the performance gains are substantial (10% MAP improvement), the underlying mechanism - that different relevance signals are truly complementary - lacks direct validation through ablation studies or signal correlation analysis.

## Confidence

- High confidence: The EAR ensemble model architecture is clearly specified and the performance improvements over baseline ensemble methods are measurable and significant.
- Medium confidence: The claim that textual entailment is a "key" relevance signal beyond semantic equivalence, as this relies on the assumption that EAR's performance gains directly reflect the importance of this signal rather than ensemble diversity effects.
- Low confidence: The specific contribution of named entity similarity in EARnest, as this mechanism lacks direct empirical validation and the paper doesn't analyze whether shared named entities actually correlate with better evidence coherence.

## Next Checks

1. Conduct ablation study removing QNLI CE from EAR to quantify the specific contribution of textual entailment signal vs. ensemble diversity effects.
2. Analyze correlation between named entity sharing and retrieval performance in EARnest to validate whether this signal adds unique value.
3. Test ensemble performance on non-bridge multi-hop questions in HotpotQA to determine if the observed benefits generalize across different multi-hop question types.