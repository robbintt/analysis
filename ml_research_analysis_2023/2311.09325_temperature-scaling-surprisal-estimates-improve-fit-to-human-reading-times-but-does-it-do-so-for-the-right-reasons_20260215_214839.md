---
ver: rpa2
title: Temperature-scaling surprisal estimates improve fit to human reading times
  -- but does it do so for the "right reasons"?
arxiv_id: '2311.09325'
source_url: https://arxiv.org/abs/2311.09325
tags:
- surprisal
- probability
- words
- reading
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Temperature-scaling surprisal estimates improve fit to human reading
  times -- but does it do so for the "right reasons"? This paper investigates whether
  temperature-scaling can improve the fit of surprisal estimates from large language
  models (LLMs) to human reading times.
---

# Temperature-scaling surprisal estimates improve fit to human reading times -- but does it do so for the "right reasons"?

## Quick Facts
- arXiv ID: 2311.09325
- Source URL: https://arxiv.org/abs/2311.09325
- Reference count: 12
- Key outcome: Temperature-scaling surprisal estimates improve fit to human reading times -- but does it do so for the "right reasons"?

## Executive Summary
This paper investigates whether temperature-scaling can improve the fit of surprisal estimates from large language models (LLMs) to human reading times. The authors propose temperature-scaled surprisal, calculated by scaling the probability distribution of LLMs, to address potential discrepancies between LLM predictions and human cognitive processing. The results show that temperature-scaling with an optimal value around 2.5 consistently improves the predictive power of surprisal estimates across three reading time corpora and four different LLMs, with up to 89% improvement in delta log-likelihood. However, this improvement comes at the cost of calibration, as temperature-scaled surprisal significantly worsens expected calibration error (ECE) and classwise-ECE (CECE) metrics. Further analysis reveals that the primary benefit of temperature-scaled surprisal is driven by a better fit to underpredicted open-class words, particularly named entities, nouns, and adjectives.

## Method Summary
The authors preprocess three reading time corpora (Natural Stories, Brown, and Dundee) and generate surprisal estimates for each word using four GPT-2 models (small, medium, large, and XL). They then apply temperature-scaling to the surprisal estimates using a range of temperatures (1-10) and evaluate the fit to human reading times using delta log-likelihood, ECE, and CECE. The optimal temperature value is found to be around 2.5 across all models and datasets.

## Key Results
- Temperature-scaling with optimal T ≈ 2.5 improves fit to reading times by up to 89% in delta log-likelihood
- Temperature-scaling significantly worsens calibration (ECE and CECE metrics)
- The primary benefit is driven by better fit to underpredicted open-class words, particularly named entities, nouns, and adjectives

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Temperature scaling improves fit by reducing overconfidence in LLM probability estimates
- **Mechanism**: Increasing temperature (>1) softens the probability distribution, making it more uniform and reducing the gap between LLM predictions and human processing uncertainty
- **Core assumption**: LLMs are overconfident in their predictions due to exposure to much larger training corpora than humans
- **Evidence anchors**:
  - [abstract] "modern language models are shown to be overconfident (i.e., higher than the ground-truth probabilities)"
  - [section] "temperature scaling (with T > 1) is also used to 'soften' the knowledge (i.e., probability distribution) provided by the teacher model"
  - [corpus] Weak - corpus evidence doesn't directly address calibration issues
- **Break condition**: If human processing uncertainty is not related to probability distribution calibration

### Mechanism 2
- **Claim**: Temperature scaling improves fit by better modeling entropy in human processing
- **Mechanism**: Temperature-scaled surprisal introduces uncertainty into the probability distribution, making it function partly as entropy, which influences reading times
- **Core assumption**: Human processing difficulty is partly related to entropy/uncertainty in word prediction
- **Evidence anchors**:
  - [section] "Our intuition of temperature-scaled surprisal is to directly introduce uncertainty into the probability distribution (i.e., into surprisal), thus making the surprisal partly function as the entropy"
  - [section] "Previous studies have shown that uncertainty (qualified as entropy) influences reading times"
  - [corpus] Weak - corpus evidence doesn't directly measure entropy effects
- **Break condition**: If human processing difficulty is not influenced by entropy/uncertainty

### Mechanism 3
- **Claim**: Temperature scaling improves fit by better matching underpredicted open-class words
- **Mechanism**: Temperature scaling increases surprisal estimates for open-class words (nouns, adjectives, named entities) which were underpredicted by LLMs, better aligning with human processing difficulty
- **Core assumption**: LLMs underpredict processing difficulty for open-class words due to extensive domain knowledge
- **Evidence anchors**:
  - [section] "we show that this improvement in fit is chiefly driven by words that are composed of multiple subword tokens"
  - [section] "Our results indicate that the enhancement of fit to reading times via temperature-scaled surprisal is primarily driven by a better fit of underpredicted open-class words for LLMs"
  - [corpus] Strong - analysis shows named entities, nouns, and adjectives are primary beneficiaries
- **Break condition**: If open-class words are not the primary source of fit improvement

## Foundational Learning

- **Surprisal Theory**: Why needed here: Understanding how surprisal quantifies word predictability and relates to processing difficulty is fundamental to the paper's approach
  - Quick check question: What is the mathematical definition of surprisal and how does it relate to word predictability?

- **Probability Calibration**: Why needed here: The paper's core contribution involves addressing miscalibration between LLM probabilities and human processing
  - Quick check question: How does temperature scaling affect the calibration of probability distributions?

- **Rényi Entropy**: Why needed here: The paper connects temperature-scaled surprisal to Rényi entropy, requiring understanding of this information-theoretic concept
  - Quick check question: What is the relationship between Rényi entropy with different α values and temperature-scaled surprisal?

## Architecture Onboarding

- **Component map**: Language model output → Temperature scaling → Surprisal calculation → Regression model → Reading time prediction
- **Critical path**: Language model output → Temperature scaling → Surprisal calculation → Regression model → Reading time prediction
- **Design tradeoffs**: Temperature scaling improves fit but worsens calibration; higher temperatures increase diversity but decrease generation quality
- **Failure signatures**: If temperature scaling doesn't improve fit, check if the issue is with overconfidence or entropy modeling; if calibration worsens significantly, reassess the temperature range
- **First 3 experiments**:
  1. Apply temperature scaling to a pre-trained LLM and measure changes in surprisal estimates
  2. Compare temperature-scaled surprisal predictions to human reading times across different corpora
  3. Analyze which word types benefit most from temperature scaling by examining residuals from regression models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal temperature value for temperature-scaling across different language models and corpora?
- Basis in paper: [explicit] The paper mentions that the optimal temperature value is around 2.5 across all models and datasets, but it also states that larger LMs typically require a higher value of T to reach their peak performance.
- Why unresolved: The paper does not provide a systematic method to determine the optimal temperature value for each specific model and corpus combination.
- What evidence would resolve it: Conducting a comprehensive study to find the optimal temperature value for each model and corpus combination, possibly through an automated method based on specific characteristics of the models or corpora.

### Open Question 2
- Question: How does the non-linear relationship between surprisal and reading times affect the performance of temperature-scaled surprisal?
- Basis in paper: [inferred] The paper mentions that previous research has shown that the relationship between surprisal and reading times is non-linear, but it does not explicitly discuss how this affects the performance of temperature-scaled surprisal.
- Why unresolved: The paper uses linear mixed-effects regression models, which assume a linear relationship between predictors and the response variable. The impact of non-linearity on the performance of temperature-scaled surprisal is not explored.
- What evidence would resolve it: Investigating the effectiveness of temperature-scaled surprisal using generalized additive models (GAMs) or other models that can capture non-linear relationships, and comparing the results with those obtained using linear models.

### Open Question 3
- Question: What is the underlying cause of the discrepancy between the calibration of large language models and human-like processing?
- Basis in paper: [explicit] The paper suggests that the discrepancy may be due to LLMs having extensive domain knowledge from large training corpora, leading to lower surprisal estimates for open-class words compared to human processing difficulty.
- Why unresolved: While the paper provides a hypothesis, it does not provide a definitive explanation for the discrepancy between LLM calibration and human-like processing.
- What evidence would resolve it: Conducting experiments to compare the performance of temperature-scaled surprisal on open-class words and closed-class words, and analyzing the impact of domain knowledge on the surprisal estimates for different word types. Additionally, investigating the relationship between model size and calibration error could provide insights into the underlying cause of the discrepancy.

## Limitations

- The improved fit may come at the cost of probabilistic interpretability due to calibration deterioration
- The primary benefit is driven by better fit to open-class words, raising questions about whether this reflects genuine cognitive processing or LLM biases
- The paper's conclusions about entropy and uncertainty effects on reading times rely on indirect evidence rather than direct measurements of cognitive processing uncertainty

## Confidence

**High Confidence**: Temperature-scaling with optimal T ≈ 2.5 consistently improves surprisal fit to reading times across multiple corpora and models. This is directly supported by the delta log-likelihood improvements (up to 89%) and replicated across three different datasets.

**Medium Confidence**: The improvement is primarily driven by better fit to underpredicted open-class words. This is supported by detailed word-type analysis but could reflect methodological artifacts in how these words are processed or represented.

**Low Confidence**: Temperature-scaled surprisal functions partly as entropy in human processing. While theoretically plausible and supported by prior entropy-reading time studies, the direct connection to temperature-scaling effects remains speculative.

## Next Checks

1. **Cross-linguistic validation**: Apply the temperature-scaling approach to reading time corpora in languages with different morphological complexity (e.g., Finnish, Turkish) to test whether the open-class word effect generalizes across typologically diverse languages.

2. **Domain transfer test**: Compare temperature-scaled surprisal performance on in-domain versus out-of-domain texts to determine whether the improvement reflects general cognitive processing or domain-specific LLM biases.

3. **Fine-tuning calibration experiment**: Fine-tune a subset of the LLMs on the reading time corpora themselves, then compare whether temperature-scaling remains beneficial or whether domain adaptation resolves the calibration issues more effectively.