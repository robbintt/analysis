---
ver: rpa2
title: Semi-Supervised Learning in the Few-Shot Zero-Shot Scenario
arxiv_id: '2308.14119'
source_url: https://arxiv.org/abs/2308.14119
tags:
- classes
- unseen
- seen
- labeled
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of semi-supervised learning
  (SSL) when the labeled training set contains only a few examples per class and is
  missing some classes entirely (few-shot zero-shot scenario). The authors propose
  a method to extend existing SSL techniques by adding an entropy loss term to the
  objective function.
---

# Semi-Supervised Learning in the Few-Shot Zero-Shot Scenario

## Quick Facts
- arXiv ID: 2308.14119
- Source URL: https://arxiv.org/abs/2308.14119
- Authors: [not specified]
- Reference count: 14
- Key outcome: This paper addresses the challenge of semi-supervised learning (SSL) when the labeled training set contains only a few examples per class and is missing some classes entirely (few-shot zero-shot scenario). The authors propose a method to extend existing SSL techniques by adding an entropy loss term to the objective function. This term penalizes the KL-divergence between the true and inferred class frequency distributions, helping the model learn to classify both seen and unseen classes. Experiments on CIFAR-100 and STL-10 show significant improvements over state-of-the-art SSL, open-set SSL, and open-world SSL methods, especially when the labeled data is severely limited (1-25 examples per class). The proposed method outperforms baselines by large margins in combined accuracy scores, primarily due to better performance on unseen classes.

## Executive Summary
This paper tackles the challenging few-shot zero-shot semi-supervised learning problem where labeled data is both limited and incomplete. The authors extend existing SSL methods by adding an entropy loss term that encourages the model to produce class probability distributions matching the true class frequency distribution in unlabeled data. By modifying SSL architectures to handle both seen and unseen classes, the proposed method significantly outperforms state-of-the-art approaches, particularly on unseen class classification, achieving up to 15% improvement in combined accuracy scores on CIFAR-100 and STL-10 datasets.

## Method Summary
The proposed method extends existing semi-supervised learning techniques to handle scenarios where the labeled training set contains only a few examples per class and is missing some classes entirely. The key innovation is adding an entropy loss term to the objective function that penalizes KL-divergence between true and inferred class frequency distributions. This is achieved by modifying the last layer of SSL architectures to incorporate both seen and unseen classes, allowing direct classification of all classes. The method is evaluated on CIFAR-100 and STL-10 datasets with varying numbers of unseen classes and labeled examples per class (1-25), showing substantial improvements over existing SSL, open-set SSL, and open-world SSL methods.

## Key Results
- Proposed method achieves up to 15% improvement in combined accuracy scores on CIFAR-100 and STL-10 datasets
- Significant gains primarily attributed to better performance on unseen classes compared to baseline methods
- Outperforms state-of-the-art SSL, open-set SSL, and open-world SSL methods especially when labeled data is severely limited (1-25 examples per class)
- Shows consistent improvement across different numbers of unseen classes and varying amounts of labeled data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The entropy loss term forces the model to produce class probability distributions that match the true class frequency distribution in the unlabeled data, thereby improving unseen class classification.
- Mechanism: The entropy loss is defined as the negative sum of p_i log(p_i) over all classes, where p_i is the empirical probability of class i in the batch. This penalizes the model when its predicted class distribution deviates from uniform (or a specified prior distribution).
- Core assumption: The unlabeled data accurately represents the true class distribution of the test data, including both seen and unseen classes.
- Evidence anchors:
  - [abstract] "This term penalizes the KL-divergence between the true and inferred class frequency distributions, helping the model learn to classify both seen and unseen classes."
  - [section] "Let pi denote the empirical probability of class i ∈ C across all the classes, in a specific batch: pi = 1/B sum_b f_i(x_b) Where f_i is the confidence of f on class i ∈ C. We define the following entropy loss: ℓe = -sum_i p_i log(p_i)"
- Break condition: If the unlabeled data does not accurately represent the true class distribution, the entropy loss will push the model in the wrong direction.

### Mechanism 2
- Claim: By modifying the last layer of the SSL architecture to include both seen and unseen classes, the model can directly output predictions for all classes, not just seen ones.
- Mechanism: The output layer is expanded from |C_seen| to |C_seen ∪ C_unseen| neurons. The model is then trained to minimize the supervised loss, unsupervised loss, and entropy loss over this expanded label space.
- Core assumption: The feature representations learned by the SSL backbone are discriminative enough to distinguish between seen and unseen classes.
- Evidence anchors:
  - [abstract] "The authors propose a method to extend existing SSL techniques by adding an entropy loss term to the objective function. This term penalizes the KL-divergence between the true and inferred class frequency distributions, helping the model learn to classify both seen and unseen classes."
  - [section] "Modify the last layer of a given SSL architecture to incorporate both seen and unseen classes. By doing so, the model is capable of handling and classifying instances from both types of classes."
- Break condition: If the feature space is not discriminative enough, the expanded output layer will not improve unseen class classification.

### Mechanism 3
- Claim: The entropy loss helps mitigate the overconfidence problem in deep neural networks, where the model assigns high confidence to incorrect predictions, especially for unseen classes.
- Mechanism: The entropy loss encourages the model to output lower confidence (higher entropy) predictions when the true class distribution is uncertain or unknown, reducing the impact of erroneous pseudo-labels for unseen classes.
- Core assumption: Deep neural networks tend to be overconfident in their predictions, which can lead to poor generalization on unseen classes.
- Evidence anchors:
  - [abstract] "The proposed method outperforms baselines by large margins in combined accuracy scores, primarily due to better performance on unseen classes."
  - [section] "Evidently, the improvement over other SSL algorithms is caused primarily by their poor performance on unseen classes, which in turn is caused by their overconfidence in their erroneous classification of seen classes."
- Break condition: If the model is not overconfident, the entropy loss may not provide significant benefits.

## Foundational Learning

- Concept: Semi-Supervised Learning (SSL)
  - Why needed here: The proposed method extends existing SSL techniques to handle the few-shot zero-shot scenario. Understanding SSL is crucial to grasp the problem setting and the proposed solution.
  - Quick check question: What are the main challenges in SSL, and how does the proposed method address them?

- Concept: Entropy and KL-divergence
  - Why needed here: The entropy loss term is based on the KL-divergence between the true and inferred class frequency distributions. A solid understanding of entropy and KL-divergence is necessary to interpret the proposed loss function and its effects.
  - Quick check question: How does the entropy loss encourage the model to produce class probability distributions that match the true class frequency distribution?

- Concept: Deep Neural Networks and Overconfidence
  - Why needed here: The proposed method aims to mitigate the overconfidence problem in deep neural networks, which can lead to poor generalization on unseen classes. Familiarity with this issue is important to understand the motivation behind the entropy loss.
  - Quick check question: Why are deep neural networks prone to overconfidence, and how does the entropy loss help address this issue?

## Architecture Onboarding

- Component map: Input images -> Wide-ResNet-28 or ResNet-18 backbone -> Expanded output layer (seen + unseen classes) -> Combined loss (supervised + unsupervised + entropy loss)

- Critical path:
  1. Load the labeled and unlabeled data
  2. Initialize the SSL model with the expanded output layer
  3. Train the model using the combined loss function
  4. Evaluate the model on the test set, measuring seen and unseen class accuracy

- Design tradeoffs:
  - Choosing the weight (λ) for the entropy loss term
  - Selecting the appropriate SSL method to extend (e.g., FlexMatch, FreeMatch)
  - Deciding on the size of the output layer (number of unseen classes)
  - Balancing the model's performance on seen and unseen classes

- Failure signatures:
  - Poor performance on unseen classes
  - Model collapse (sharp decline in entropy loss during training)
  - Overfitting to the labeled data
  - Slow convergence or failure to converge

- First 3 experiments:
  1. Train the proposed method on CIFAR-100 with 1-25 labeled examples per class and evaluate on both seen and unseen classes.
  2. Compare the performance of the proposed method with and without the entropy loss term to assess its impact.
  3. Investigate the effect of varying the weight (λ) of the entropy loss on the model's performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform when the unlabeled dataset is not representative of the true class distribution?
- Basis in paper: [explicit] The paper mentions that "the unlabeled set accurately represents the underlying class distribution" as a valuable constraint, but does not explore scenarios where this assumption is violated.
- Why unresolved: The authors only test their method under the assumption that the unlabeled dataset is representative of the true class distribution.
- What evidence would resolve it: Experiments showing the performance of the proposed method when the unlabeled dataset is not representative of the true class distribution, such as when the unlabeled dataset is heavily imbalanced or contains outliers.

### Open Question 2
- Question: Can the proposed method be extended to handle scenarios where the labeled dataset contains examples from all classes, but with very few examples per class (few-shot learning)?
- Basis in paper: [inferred] The paper focuses on the scenario where the labeled dataset contains examples from only a subset of classes (few-shot zero-shot learning). It does not explore the performance of the method in the more general few-shot learning scenario.
- Why unresolved: The authors do not test their method in the more general few-shot learning scenario, where the labeled dataset contains examples from all classes, but with very few examples per class.
- What evidence would resolve it: Experiments showing the performance of the proposed method in the few-shot learning scenario, comparing it to other state-of-the-art few-shot learning methods.

### Open Question 3
- Question: How does the proposed method perform when the test dataset contains examples from classes not present in either the labeled or unlabeled datasets (zero-shot learning)?
- Basis in paper: [inferred] The paper focuses on the scenario where the test dataset contains examples from both seen and unseen classes. It does not explore the performance of the method in the more challenging zero-shot learning scenario, where the test dataset contains examples from classes not present in either the labeled or unlabeled datasets.
- Why unresolved: The authors do not test their method in the zero-shot learning scenario, where the test dataset contains examples from classes not present in either the labeled or unlabeled datasets.
- What evidence would resolve it: Experiments showing the performance of the proposed method in the zero-shot learning scenario, comparing it to other state-of-the-art zero-shot learning methods.

## Limitations
- The method's effectiveness relies on the strong assumption that unlabeled data distribution matches the true test distribution, including unseen classes.
- Scalability concerns arise from expanding output layers to include large numbers of unseen classes.
- Potential model collapse scenarios where entropy loss drives predictions toward uniform distributions are not thoroughly addressed.

## Confidence
- **High Confidence**: Claims about improved combined accuracy scores and the basic mechanism of adding entropy loss to SSL objectives.
- **Medium Confidence**: Claims about outperforming state-of-the-art methods, particularly on unseen classes, due to overconfidence mitigation.
- **Low Confidence**: Claims about the entropy loss's effectiveness in all scenarios, given the strong distribution matching assumption.

## Next Checks
1. Test the proposed method on datasets where the unlabeled data distribution deliberately differs from the test distribution to assess robustness.
2. Conduct ablation studies varying the number of unseen classes to evaluate scalability limitations.
3. Implement additional regularization techniques to prevent potential model collapse from excessive entropy loss optimization.