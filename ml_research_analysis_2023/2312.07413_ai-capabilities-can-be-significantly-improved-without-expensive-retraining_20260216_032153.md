---
ver: rpa2
title: AI capabilities can be significantly improved without expensive retraining
arxiv_id: '2312.07413'
source_url: https://arxiv.org/abs/2312.07413
tags:
- enhancements
- post-training
- compute
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for quantifying the benefits
  and costs of post-training enhancements to AI models, measuring benefits via the
  compute-equivalent gain (CEG). The CEG is calculated as the additional compute required
  to match the performance improvement from an enhancement, assuming optimal training.
---

# AI capabilities can be significantly improved without expensive retraining

## Quick Facts
- arXiv ID: 2312.07413
- Source URL: https://arxiv.org/abs/2312.07413
- Reference count: 26
- Post-training enhancements can improve benchmark performance by more than 5x-20x the training compute equivalent

## Executive Summary
This paper introduces a framework for quantifying the benefits and costs of post-training enhancements to AI models, measuring benefits via the compute-equivalent gain (CEG). The CEG translates performance improvements from different enhancements into a common currency based on how much additional training compute would be needed to achieve the same improvement. The authors apply this framework to a representative set of post-training enhancements, finding that most surveyed enhancements improve benchmark performance by more than a 5x increase in training compute, some by more than 20x. The fine-tuning costs are typically less than 1% of the original training cost, but inference costs vary.

## Method Summary
The authors reviewed literature on post-training enhancements, categorizing them into tool-use, prompting methods, scaffolding, solution selection, and data generation. For each enhancement, they estimated the CEG by comparing enhanced and non-enhanced model performance on relevant benchmarks, while accounting for training compute requirements. They also estimated one-time fine-tuning compute costs and ongoing inference costs. The analysis focuses on estimating benefits and costs rather than proposing new enhancement techniques.

## Key Results
- Most surveyed post-training enhancements improve benchmark performance by more than 5x the training compute equivalent
- Some enhancements achieve more than 20x compute-equivalent gains, particularly in specialized domains like mathematics
- Fine-tuning costs are typically less than 1% of original training costs, though inference costs vary significantly by enhancement type

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Post-training enhancements improve model performance by applying domain-specific modifications after initial training
- **Mechanism:** The enhancement leverages existing model capabilities through techniques like fine-tuning, scaffolding, or tool integration to optimize performance for specific tasks without full retraining
- **Core assumption:** Pre-trained models have learned general representations that can be effectively adapted
- **Evidence anchors:** [abstract] "State-of-the-art AI systems can be significantly improved without expensive retraining via 'post-training enhancements'" [section] "The post-training enhancements in this section give models access to tools to improve their performance on downstream tasks."
- **Break condition:** If pre-trained models lack necessary general representations, post-training enhancements will fail to provide meaningful improvements

### Mechanism 2
- **Claim:** CEG metric translates performance improvements into training compute requirements
- **Mechanism:** Calculates how much additional training compute would be needed to achieve the same performance improvement, providing standardized comparison across enhancements
- **Core assumption:** Training compute is a reliable proxy for model capability with consistent scaling laws
- **Evidence anchors:** [abstract] "we translate improvements from different enhancements into a common currency, the compute-equivalent gain"
- **Break condition:** If scaling laws break down or training compute decouples from capability improvements

### Mechanism 3
- **Claim:** Multiple enhancements can be combined for multiplicative benefits with diminishing returns
- **Mechanism:** Enhancements targeting different aspects (tool use, prompting, scaffolding) can be layered together, each building on previous improvements
- **Core assumption:** Enhancements operate independently without interfering with each other's mechanisms
- **Evidence anchors:** [abstract] "Researchers have combined together multiple different post-training enhancements for increased benefit"
- **Break condition:** If enhancements interfere with each other or the model reaches performance ceilings

## Foundational Learning

- **Concept:** Scaling laws and compute-optimal training
  - Why needed here: Understanding training-compute relationships is essential for interpreting CEG and making enhancement decisions
  - Quick check question: If a 10B parameter model takes 1e23 FLOP to train, how much compute would a 100B parameter model need if it follows Chinchilla scaling?

- **Concept:** Fine-tuning and transfer learning
  - Why needed here: Post-training enhancements often rely on fine-tuning techniques to adapt pre-trained models
  - Quick check question: What is the typical compute cost of fine-tuning compared to pre-training, and why is this ratio important for evaluating post-training enhancements?

- **Concept:** Benchmarking and evaluation metrics
  - Why needed here: Different benchmarks measure different capabilities, making cross-domain comparison crucial
  - Quick check question: How would you normalize performance improvements across benchmarks that use different metrics?

## Architecture Onboarding

- **Component map:** Pre-trained model base (frozen weights) -> Enhancement modules (fine-tuning layers, tool interfaces, scaffolding code) -> Evaluation framework (benchmark runners, scaling law calculators) -> Cost tracking system (compute usage monitoring, inference cost analysis)

- **Critical path:** 1. Load pre-trained model and evaluate baseline performance 2. Apply post-training enhancement 3. Evaluate enhanced model on target benchmarks 4. Calculate CEG by comparing to scaled baseline 5. Analyze cost-benefit ratio and determine deployment feasibility

- **Design tradeoffs:**
  - Fine-tuning vs. scaffolding: Fine-tuning provides deeper integration but requires more compute; scaffolding is cheaper but may have performance limitations
  - General vs. specialized enhancements: General enhancements improve broad capabilities but may have lower CEG; specialized enhancements have high CEG for specific tasks but limited applicability
  - One-time vs. ongoing costs: Consider both fine-tuning compute costs and increased inference costs when evaluating enhancement viability

- **Failure signatures:**
  - CEG close to 1: Enhancement provides minimal benefit compared to scaling
  - Negative or decreasing CEG with scale: Enhancement benefits don't scale with model size
  - High inference cost increase: Enhancement makes model too expensive for practical deployment
  - Benchmark overfitting: Enhancement only improves performance on narrow, specific benchmarks

- **First 3 experiments:**
  1. Chain-of-thought prompting on GSM8K: Compare PaLM-8B with and without CoT, measure performance gain and inference cost increase
  2. Toolformer calculator integration: Fine-tune a 6B model to use a calculator, evaluate on MATH benchmark, calculate CEG vs. scaling
  3. WebGPT browser tool: Fine-tune GPT-3 to use a web browser, evaluate on ELI5, measure CEG and compare to baseline scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do diminishing returns from combining multiple post-training enhancements affect the total compute-equivalent gain (CEG)?
- Basis in paper: [inferred] The paper discusses combining multiple enhancements but notes it's unlikely that the combined CEG equals the product of individual CEGs
- Why unresolved: The paper lacks data on combinations of enhancements to study diminishing returns systematically
- What evidence would resolve it: Controlled experiments comparing CEG of combined enhancements versus individual ones

### Open Question 2
- Question: Is there a ceiling to the total improvement possible from post-training enhancements?
- Basis in paper: [inferred] The paper suggests future enhancements may continue improving capabilities but questions if there's a limit
- Why unresolved: Insufficient data to determine if there's an upper bound to CEG improvements
- What evidence would resolve it: Empirical studies showing saturation point of CEG gains from sequential enhancements

### Open Question 3
- Question: How does the rate of improvement from post-training enhancements compare to compute scaling over time?
- Basis in paper: [inferred] The paper suggests tracking CEG improvements over time to understand enhancement importance
- Why unresolved: No longitudinal data comparing CEG growth to compute scaling trends
- What evidence would resolve it: Historical analysis of CEG improvements versus compute scaling over multiple years

## Limitations

- Exact training compute values for frontier models like GPT-3.5 and GPT-4 are unknown, introducing uncertainty in CEG estimates
- The combined effects of multiple post-training enhancements are not fully explored, making it unclear how interaction effects affect total benefits
- Generalizability of enhancement benefits across different model architectures and scales is uncertain, particularly for emerging architectures

## Confidence

- **High Confidence**: The core claim that post-training enhancements can provide significant capability improvements at lower cost than additional training
- **Medium Confidence**: The CEG metric as a standardized measure for comparing enhancements, accuracy depends on precise training compute estimates
- **Medium Confidence**: The assertion that governance of post-training enhancements will be challenging, requires further empirical validation in policy contexts

## Next Checks

1. **Replication with Known Compute Values**: Calculate CEG for enhancements applied to models with fully documented training compute (e.g., open-source models like LLaMA) to establish baseline accuracy of the metric

2. **Combined Enhancement Study**: Systematically test the combined effects of multiple post-training enhancements (e.g., chain-of-thought prompting + tool use + scaffolding) on the same model and benchmarks to quantify interaction effects

3. **Cost-Benefit Analysis at Scale**: For each enhancement type, model how CEG changes as model scale increases from 1B to 100B+ parameters to determine which enhancements maintain their cost-effectiveness advantage over scaling