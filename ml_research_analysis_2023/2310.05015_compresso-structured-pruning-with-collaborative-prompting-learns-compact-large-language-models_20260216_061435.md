---
ver: rpa2
title: 'Compresso: Structured Pruning with Collaborative Prompting Learns Compact
  Large Language Models'
arxiv_id: '2310.05015'
source_url: https://arxiv.org/abs/2310.05015
tags:
- pruning
- arxiv
- compresso
- llms
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of compressing large language
  models (LLMs) while maintaining their performance, which is critical for deploying
  LLMs on resource-constrained hardware. The authors propose Compresso, a novel approach
  that combines resource-efficient training-based structured pruning with a collaborative
  prompting technique.
---

# Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models

## Quick Facts
- arXiv ID: 2310.05015
- Source URL: https://arxiv.org/abs/2310.05015
- Reference count: 20
- Primary result: Compresso reduces LLaMA-7B to 5.4B parameters while improving reading comprehension by 2.62% and achieving up to 11.43% higher scores on certain benchmarks compared to one-shot pruning baselines.

## Executive Summary
Compresso addresses the challenge of compressing large language models while maintaining performance through a novel approach that combines resource-efficient training-based structured pruning with collaborative prompting. The method integrates LoRA into L0 regularization during instruction tuning, enabling efficient learning of optimal pruning decisions. A key innovation is the collaborative pruning prompt that fosters cooperation between the LLM and pruning algorithm. Experiments on LLaMA-7B demonstrate that Compresso can significantly reduce model size while preserving or improving performance across various benchmarks.

## Method Summary
Compresso combines LoRA modules with L0 regularization during instruction tuning to enable efficient training-based structured pruning. The method uses a collaborative pruning prompt to guide the LLM's adaptation during pruning. LoRA parameters are initialized with rank r=2 and updated along with binary masks learned through L0 regularization. The pruning process involves three stages: 1) fine-tuning the base model, 2) pruning with a cubic sparsity schedule, and 3) post-pruning fine-tuning. GPT4-Alpaca dataset serves as the pruning data, and the final pruned model is converted to full precision weights for deployment.

## Key Results
- Compresso-5.4B model surpasses original LLaMA-7B in reading comprehension by 2.62%
- Achieves up to 11.43% higher scores on certain benchmarks compared to one-shot pruning baselines
- Successfully reduces LLaMA-7B from 7B to 5.4B parameters while maintaining or improving performance
- GPT4-Alpaca instruction tuning dataset outperforms C4 and LLM-QAT datasets for pruning purposes

## Why This Works (Mechanism)

### Mechanism 1
Collaborative prompting enables the LLM to actively participate in the pruning process, improving pruning decisions beyond algorithmic optimization alone. The pruning prompt informs the LLM about the pruning task and encourages it to adapt its internal representations to support the pruning process, creating a feedback loop where the LLM's responses guide mask optimization.

### Mechanism 2
LoRA integration with L0 regularization enables efficient training-based pruning without full model updates. LoRA injects low-rank adaptation matrices that capture gradient updates, while L0 regularization learns binary masks. This combination allows mask optimization without updating all original parameters.

### Mechanism 3
Instruction tuning datasets serve as effective proxies for pre-training data distribution in pruning tasks. The GPT4-Alpaca dataset provides instruction-following examples that maintain task diversity while being more accessible than pre-training data, with distributional similarity sufficient for pruning purposes.

## Foundational Learning

- Concept: Structured pruning vs unstructured pruning
  - Why needed here: The paper specifically targets structured pruning (removing entire heads, FFN dimensions) which directly reduces inference costs
  - Quick check question: What is the key difference between structured and unstructured pruning in terms of hardware efficiency?

- Concept: L0 regularization for sparsity learning
  - Why needed here: The method uses L0 regularization to learn optimal mask values during training
  - Quick check question: How does L0 regularization differ from L1 regularization in terms of sparsity learning?

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: LoRA is integrated to reduce memory and computation during training-based pruning
  - Quick check question: What is the mathematical form of LoRA's weight update approximation?

## Architecture Onboarding

- Component map: GPT4-Alpaca dataset -> LoRA modules + L0 regularization masks + collaborative prompt -> Pruned LLM with maintained performance
- Critical path:
  1. Load frozen LLM and initialize LoRA modules
  2. Initialize masks using hard concrete distribution
  3. Apply collaborative prompt to inputs
  4. Compute loss (next token prediction + L0 regularization)
  5. Update LoRA parameters and mask distributions
  6. Convert masks to binary and prune
- Design tradeoffs:
  - LoRA rank vs pruning quality: Higher rank provides better approximation but increases parameters
  - β in hard concrete: Controls mask differentiability vs discreteness
  - Dataset size vs generalization: Larger instruction datasets may improve pruning but increase training cost
- Failure signatures:
  - Poor pruning performance: Masks converge to trivial values (all zeros or ones)
  - Training instability: Loss divergence or mask oscillation
  - Memory issues: LoRA parameters exceeding available GPU memory
- First 3 experiments:
  1. Ablation study: Remove collaborative prompt to measure its impact on pruning quality
  2. Rank sensitivity: Vary LoRA rank (r=2,4,8) and measure performance vs efficiency tradeoff
  3. Dataset comparison: Test C4, LLM-QAT, and GPT4-Alpaca to validate dataset choice assumption

## Open Questions the Paper Calls Out

### Open Question 1
How does the collaborative prompting technique specifically influence the layer-wise sparsity distribution learned by Compresso compared to traditional uniform sparsity approaches? While the paper mentions the impact of the collaborative prompt on the sparsity distribution, it does not provide a detailed analysis of how the prompt specifically influences the decision-making process for layer-wise sparsity.

### Open Question 2
What are the potential long-term effects of using instruction tuning datasets, like GPT4-Alpaca, as pruning data on the generalization capabilities of pruned LLMs? The paper focuses on short-term performance metrics but does not explore the long-term effects of using instruction tuning datasets on the models' ability to generalize to new tasks or domains.

### Open Question 3
How does the integration of LoRA modules in the pruning process affect the convergence and stability of the training process compared to traditional training-based pruning methods? The paper introduces LoRA modules to reduce the training cost but does not delve into the convergence and stability aspects of this approach.

## Limitations
- Lack of ablation studies isolating the impact of the collaborative prompting mechanism
- Limited comparison of datasets, only testing three options for pruning data
- Method's generalization to different model architectures beyond LLaMA remains untested
- Hard concrete distribution parameters (particularly β) not thoroughly explored with only single value reported

## Confidence
- **High Confidence**: The core observation that structured pruning combined with LoRA and L0 regularization can reduce model size while maintaining performance
- **Medium Confidence**: The effectiveness of using instruction-tuning datasets as pruning data
- **Low Confidence**: The specific contribution of the collaborative prompting mechanism to overall performance

## Next Checks
1. Implement and compare Compresso with and without the collaborative prompt on the same model and dataset to quantify the prompt's specific contribution to performance gains
2. Systematically vary the hard concrete distribution parameter β (e.g., 0.5, 1.0, 2.0, 5.0) and measure its impact on mask convergence and final model performance
3. Apply Compresso to a different LLM architecture (e.g., OPT or Mistral) and evaluate whether the same performance preservation and collaborative prompting benefits extend beyond the LLaMA family