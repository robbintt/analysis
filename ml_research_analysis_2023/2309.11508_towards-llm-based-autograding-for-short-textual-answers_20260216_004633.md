---
ver: rpa2
title: Towards LLM-based Autograding for Short Textual Answers
arxiv_id: '2309.11508'
source_url: https://arxiv.org/abs/2309.11508
tags:
- answer
- grading
- educator
- chatgpt
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of large language models (LLMs)
  for autograding short textual answers, focusing on their potential to support educators
  in validating grading procedures. The research evaluates ChatGPT's performance across
  exams from two distinct courses, comparing LLM assessments with human grading.
---

# Towards LLM-based Autograding for Short Textual Answers

## Quick Facts
- arXiv ID: 2309.11508
- Source URL: https://arxiv.org/abs/2309.11508
- Reference count: 2
- Primary result: LLMs show near-zero correlation with human grading and tend to rate most answers as "good" or "very good" regardless of quality

## Executive Summary
This study investigates the use of large language models (LLMs) for autograding short textual answers, focusing on their potential to support educators in validating grading procedures. The research evaluates ChatGPT's performance across exams from two distinct courses, comparing LLM assessments with human grading. While LLMs provide a complementary perspective, the findings reveal significant discrepancies between LLM and educator judgments, with correlations near zero. LLMs tend to rate most responses as "good" or "very good," lacking differentiation, and are sensitive to minor changes in answers. They also favor vague content and struggle with contradictions. Although LLMs can assist in understanding complex answers, their readiness for independent automated grading remains limited, necessitating human oversight.

## Method Summary
The study uses ChatGPT (GPT 3.5) to grade short textual answers from two courses - one English master-level data science course with 21 participants and one German bachelor-level information systems course with 34 participants. Three prompting approaches are employed: assessing instructor answers, assessing student answers generally, and comparing student answers to instructor answers. Student answers are converted to numerical scores using a normalization scheme, and correlation analysis is performed between LLM and human grading. The LLM's categorical ratings (Good, Ok, Bad) are scaled to numerical scores for comparison.

## Key Results
- Correlation between human and LLM judgments is near zero across both courses
- LLMs rate most student responses as "good" or "very good" regardless of actual quality
- LLMs show high sensitivity to minor answer modifications, leading to inconsistent grading
- LLMs favor vague content and struggle to identify contradictions in student responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs provide a complementary grading perspective by leveraging world knowledge absent from course-specific context
- Mechanism: The LLM evaluates answers based on general linguistic patterns and factual content learned during pretraining, offering a broader viewpoint that contrasts with instructor biases toward lecture material
- Core assumption: The LLM's world knowledge contains relevant information to assess student answers meaningfully
- Evidence anchors:
  - [abstract] "out-of-the-box LLMs provide a valuable tool to provide a complementary perspective"
  - [section] "ChatGPT would rate the responses of these students positively, although the question explicitly restricts the factors to be discussed"
- Break condition: When the LLM's general knowledge conflicts with course-specific learning objectives or when answers require contextual understanding not present in pretraining data

### Mechanism 2
- Claim: LLMs help educators understand poorly written or ambiguous student answers
- Mechanism: The LLM processes and restructures difficult-to-parse responses into more coherent form, making underlying arguments clearer for human graders
- Core assumption: LLMs can accurately interpret grammatically poor or ambiguous responses without introducing significant errors
- Evidence anchors:
  - [section] "ChatGPT can help to make sense of hard to understand answers... LLM's interpretation of the answer made the student answer more understandable"
  - [section] "it was sometimes easier to make sense of a student reply after reading ChatGPT's assessment"
- Break condition: When the LLM's interpretation introduces incorrect assumptions or when the original answer is too ambiguous for reliable interpretation

### Mechanism 3
- Claim: LLM-based autograding reveals systematic differences between human and AI assessment criteria
- Mechanism: By comparing LLM and human grading, educators can identify biases in their own assessment patterns and understand where AI and human judgment diverge
- Core assumption: The comparison between LLM and human grading provides meaningful insights into assessment practices
- Evidence anchors:
  - [section] "The correlation between human and LLM's judgments is small"
  - [section] "The LLM tended to rate most student responses as 'good' or 'very good'... in stark contrast to the rating of the educator"
- Break condition: When the LLM's assessment criteria are so different from human standards that the comparison becomes uninformative

## Foundational Learning

- Concept: Prompt engineering for structured responses
  - Why needed here: The study shows that LLMs respond differently based on prompt structure, requiring careful prompt design to get consistent grading categories
  - Quick check question: What prompt element ensures the LLM provides both a categorical rating and an explanation?

- Concept: Grading rubric development and scaling
  - Why needed here: The paper scales LLM categorical responses (Good, Ok, Bad) to numerical scores for comparison with human grading, requiring clear mapping rules
  - Quick check question: How does the study map LLM categories like "Very close" to numerical scores?

- Concept: Understanding LLM limitations and biases
  - Why needed here: The research identifies specific LLM weaknesses (sensitivity to minor changes, favoring vague content, failing at contradictions) that affect grading reliability
  - Quick check question: What specific LLM behavior shows sensitivity to minor answer modifications?

## Architecture Onboarding

- Component map: Exam questions → Student answers → LLM service interface (ChatGPT API) → Prompt templates (3 assessment modes) → Scoring conversion module → Correlation analysis → Human review interface

- Critical path: Student answer → LLM assessment → score conversion → discrepancy analysis with human grading → report generation

- Design tradeoffs:
  - Prompt complexity vs. consistency: More detailed prompts improve structure but may reduce response reliability
  - Automation vs. oversight: Full automation increases efficiency but reduces quality control
  - Context inclusion vs. bias: Including course material helps relevance but may increase instructor bias

- Failure signatures:
  - Consistent middle-category ratings regardless of answer quality
  - Dramatic rating changes from minor answer modifications
  - Inability to recognize contradictions or irrelevant content
  - Grammar/spelling sensitivity when grading non-English responses

- First 3 experiments:
  1. Test prompt variations on a single answer to identify sensitivity to phrasing changes
  2. Compare LLM ratings of good vs. poor answers to establish baseline differentiation ability
  3. Run correlation analysis between LLM and human grading on a small dataset to quantify alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM-based autograding systems be optimized to better handle minor variations in student responses while maintaining consistency and fairness?
- Basis in paper: [explicit] The paper highlights that LLMs are sensitive to minor changes in answers, leading to dramatic changes in judgments.
- Why unresolved: The study demonstrates this sensitivity but does not provide solutions or methods to mitigate it.
- What evidence would resolve it: Experimental results showing improved consistency in grading when specific techniques (e.g., prompt engineering, fine-tuning) are applied to handle minor variations.

### Open Question 2
- Question: What are the specific ethical considerations and potential biases when using LLMs for grading, and how can they be mitigated to ensure fairness?
- Basis in paper: [explicit] The paper discusses ethical considerations, including biases and issues related to generating false information, and mentions the need for human oversight.
- Why unresolved: While the paper identifies these issues, it does not explore specific mitigation strategies or their effectiveness.
- What evidence would resolve it: A comprehensive study evaluating different mitigation strategies and their impact on reducing biases and ensuring fairness in LLM-based grading.

### Open Question 3
- Question: How can LLMs be effectively integrated into the grading process to complement human graders without compromising the depth of understanding and context provided by human judgment?
- Basis in paper: [explicit] The paper suggests that LLMs provide a complementary perspective but highlights the need for human oversight due to discrepancies in judgments.
- Why unresolved: The study identifies the potential for complementary use but does not explore methods for effective integration or the balance between LLM and human input.
- What evidence would resolve it: Case studies or experiments demonstrating successful integration of LLMs in grading processes, showing improved efficiency and accuracy without compromising human judgment.

## Limitations
- Narrow scope examining only two courses with 55 total responses limits generalizability
- Near-zero correlation between LLM and human grading questions the validity of LLM assessments
- LLM tendency to rate most answers as "good" or "very good" shows inability to differentiate quality
- Study relies on GPT-3.5, which may not represent current LLM capabilities

## Confidence

- Medium confidence in the complementary perspective claim: While the study demonstrates that LLMs offer different assessments from humans, the extreme sensitivity to minor changes and inability to differentiate between answer quality undermines confidence in this mechanism.
- Medium confidence in the interpretation assistance claim: The evidence of LLMs helping understand complex answers is supported, but this benefit must be weighed against the risk of LLM-introduced errors in interpretation.
- High confidence in identifying LLM limitations: The study clearly documents specific LLM weaknesses (vagueness preference, contradiction blindness, grammar sensitivity) through systematic testing.

## Next Checks

1. **Prompt Sensitivity Test**: Systematically vary single words in identical answers across multiple trials to quantify the LLM's grading volatility and establish acceptable thresholds for answer modification.

2. **Cross-Context Validation**: Test the LLM on course material from a different domain (e.g., STEM vs. humanities) to determine if the grading inconsistencies are course-specific or universal.

3. **Human-LLM Agreement Analysis**: Identify specific answer characteristics (length, vocabulary, structure) that correlate with maximum human-LLM disagreement to develop filtering criteria for LLM-assisted grading.