---
ver: rpa2
title: 'Personalized Tucker Decomposition: Modeling Commonality and Peculiarity on
  Tensor Data'
arxiv_id: '2309.03439'
source_url: https://arxiv.org/abs/2309.03439
tags:
- local
- global
- tensor
- data
- factor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces perTucker, a personalized Tucker decomposition
  method for modeling heterogeneity in tensor data across multiple sources. The method
  decomposes tensor data into shared global components and personalized local components,
  with a mode orthogonality constraint to distinguish global and local features.
---

# Personalized Tucker Decomposition: Modeling Commonality and Peculiarity on Tensor Data

## Quick Facts
- arXiv ID: 2309.03439
- Source URL: https://arxiv.org/abs/2309.03439
- Reference count: 40
- Key outcome: perTucker decomposes tensor data into shared global components and personalized local components, enabling improved anomaly detection, classification, and clustering across multiple data sources.

## Executive Summary
This paper introduces perTucker, a personalized Tucker decomposition method for modeling heterogeneity in tensor data across multiple sources. The method decomposes tensor data into shared global components and personalized local components, with a mode orthogonality constraint to distinguish global and local features. A proximal gradient regularized block coordinate descent algorithm is developed to estimate model parameters, with convergence guaranteed to a stationary point. The approach is shown to be effective in anomaly detection, client classification, and clustering through simulation studies and case studies on solar flare detection and tonnage signal classification. The method provides improved data reconstruction and classification accuracy compared to traditional tensor decomposition methods and allows for interpretable analysis of commonality and peculiarity across datasets.

## Method Summary
perTucker extends Tucker decomposition to multi-source tensor data by introducing global (shared across sources) and local (source-specific) components. The method uses a mode orthogonality constraint requiring at least one mode where global and local factor matrices are orthogonal, enabling simultaneous global and local decomposition without cross-talk. A proximal gradient regularized block coordinate descent algorithm is developed to update global and local core tensors (via closed-form projection) and factor matrices (with proximal regularization for convergence). The method is validated through simulation studies and real-world case studies on solar flare detection and tonnage signal classification.

## Key Results
- perTucker achieves simultaneous global and local decomposition without cross-talk through mode orthogonality constraints
- Closed-form updates for core tensors are possible due to the orthogonality constraint
- Proximal regularization stabilizes factor matrix updates and guarantees convergence to stationary points
- Method demonstrates improved data reconstruction and classification accuracy compared to globalTucker, localTucker, robustTucker, and perPCA baselines
- Case studies show effectiveness in solar flare detection and tonnage signal classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: perTucker achieves simultaneous global and local decomposition without cross-talk by enforcing mode orthogonality.
- Mechanism: By requiring at least one mode where global and local factor matrices are orthogonal, the cross-term in the reconstruction loss vanishes, enabling independent updates of global and local components.
- Core assumption: The orthogonality constraint ⟨Y^G,n, Y^L,n⟩ = 0 is equivalent to U^⊤_G,k V_n,k = 0 for at least one mode k.
- Evidence anchors:
  - [abstract] "introduce a mode orthogonality assumption and develop a proximal gradient regularized block coordinate descent algorithm"
  - [section 3.2] "we require the orthogonality of the global and local tensors...it turns out that this condition is equivalent to having the global and local factor matrices orthogonal in at least one dimension"
  - [corpus] Weak: no corpus entries explicitly discuss tensor mode orthogonality.

### Mechanism 2
- Claim: Closed-form updates for core tensors follow directly from the orthogonality constraint.
- Mechanism: With orthogonal global and local factors, the core tensors can be obtained by simple projection of residuals onto the corresponding factor matrices.
- Core assumption: Orthogonality allows the cross-term to cancel in the least-squares formulation.
- Evidence anchors:
  - [section 3.3.1] "the closed-form solution is the direct projection of the data to the global or local factor matrices"
  - [section 3.3.1] Proposition 2 provides the exact projection formulas.
  - [corpus] Weak: no corpus entry mentions closed-form core tensor updates.

### Mechanism 3
- Claim: Proximal regularization stabilizes factor matrix updates and guarantees convergence.
- Mechanism: The proximal term ρ||U U^⊤ - U_t U^⊤_t||^2_F limits subspace drift between iterations, enabling global convergence even with orthogonality constraints.
- Core assumption: Choosing ρ = O(B^2) (where B bounds tensor norms) is sufficient for convergence.
- Evidence anchors:
  - [section 3.3.2] "incorporate a proximal term into the optimization problem to regulate the update of the factor matrices"
  - [section 3.4] Theorem 5 proves convergence under ρ = O(B^2) condition.
  - [corpus] Weak: no corpus entry discusses proximal regularization in tensor decomposition.

## Foundational Learning

- Concept: Tucker decomposition and mode-n products
  - Why needed here: perTucker extends Tucker decomposition to multi-source data; understanding mode-n products is essential for interpreting the decomposition and updates.
  - Quick check question: Given X ∈ R^I1×I2×I3 and matrices U ∈ R^J1×I1, V ∈ R^J2×I2, what is the dimension of X ×1 U ×2 V?

- Concept: Orthogonal matrices and their properties
  - Why needed here: Orthogonality constraints are central to separating global and local components; factor matrices must be orthonormal.
  - Quick check question: If U is orthonormal (U^⊤U = I), what is U^⊤U^⊤?

- Concept: Block coordinate descent and proximal algorithms
  - Why needed here: perTucker uses BCD with proximal regularization to update factors; understanding convergence properties is critical for implementation.
  - Quick check question: In BCD, if each subproblem is solved optimally, does the objective always decrease?

## Architecture Onboarding

- Component map:
  - Input tensor data from N sources → Global core tensor CG,n and local core tensor CL,n for each source n → Global factor matrices UG,k (shared) and local factor matrices Vn,k (per-source) → Mode orthogonality set K (modes where UG,k ⊥ Vn,k) → Output: Reconstructed global and local components

- Critical path:
  1. Initialize factors (random or via global Tucker)
  2. Iterate: Update global factors → Update local factors (with/without orthogonality) → Update core tensors
  3. Monitor convergence via subspace error metrics
  4. Apply to downstream tasks (classification, anomaly detection, clustering)

- Design tradeoffs:
  - More orthogonal modes (|K| larger) → stricter separation but smaller feasible space
  - Larger local dimensions → better fit but risk overfitting
  - Larger proximal parameter ρ → more stable but slower adaptation

- Failure signatures:
  - Increasing reconstruction error → orthogonality constraint too restrictive or poor initialization
  - Slow convergence → ρ too small or inappropriate choice of K
  - Degenerate factor matrices (rank-deficient) → local dimensions too large relative to data

- First 3 experiments:
  1. Run perTucker on synthetic data with known global/local structure; verify component recovery accuracy.
  2. Vary |K| and observe effect on reconstruction error and convergence speed.
  3. Test classification performance on multi-source data versus baseline local Tucker.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of perTucker change when the mode orthogonality constraint is imposed on multiple modes (|K| > 1) versus a single mode (|K| = 1)?
- Basis in paper: [explicit] The paper mentions that the convergence analysis requires |K| ≥ 2, but also states that |K| = 1 works in practice. However, it does not provide a detailed comparison of performance between different values of |K|.
- Why unresolved: The paper does not conduct experiments to compare the performance of perTucker with different numbers of orthogonal modes.
- What evidence would resolve it: Experimental results comparing the reconstruction error, classification accuracy, and anomaly detection performance of perTucker for different values of |K|.

### Open Question 2
- Question: How does the choice of the proximal parameter ρ affect the convergence and performance of perTucker?
- Basis in paper: [explicit] The paper mentions that the convergence of Algorithm 2 is guaranteed when ρ = O(B2), but does not provide guidance on how to choose ρ in practice or investigate its impact on performance.
- Why unresolved: The paper does not conduct a sensitivity analysis on the choice of ρ or provide practical guidelines for selecting this parameter.
- What evidence would resolve it: Experimental results demonstrating the impact of different values of ρ on the convergence speed, reconstruction error, and downstream task performance of perTucker.

### Open Question 3
- Question: How does perTucker compare to other tensor decomposition methods, such as CP decomposition or robust tensor decomposition, in terms of performance and interpretability?
- Basis in paper: [explicit] The paper mentions that perTucker is compared to globalTucker, localTucker, robustTucker, and perPCA in the simulation study. However, the comparison is limited to a specific data generation scenario and does not provide a comprehensive evaluation across different types of data and tasks.
- Why unresolved: The paper does not conduct extensive experiments to compare perTucker with other tensor decomposition methods across various data types, tasks, and evaluation metrics.
- What evidence would resolve it: Comprehensive experimental results comparing the performance of perTucker with other tensor decomposition methods on diverse datasets and tasks, including reconstruction error, classification accuracy, anomaly detection performance, and interpretability of the results.

## Limitations

- The method's performance heavily depends on appropriate selection of the orthogonal mode set K and local tensor dimensions, but the paper provides limited guidance on choosing these hyperparameters in practice.
- The convergence guarantee requires ρ = O(B²), but B must be estimated from data, and inappropriate scaling could affect convergence rates.
- The claim of "improved data reconstruction and classification accuracy" is demonstrated only on two case studies, limiting generalizability to other domains.

## Confidence

- Mechanism 1 (mode orthogonality enabling independent decomposition): **High** - The mathematical derivation and convergence proof are rigorous and well-established.
- Mechanism 2 (closed-form core tensor updates): **Medium** - The projection formulas are derived but their optimality depends critically on maintaining orthogonality throughout optimization.
- Method performance claims: **Medium** - Simulation studies and case studies show promise, but the limited scope of validation studies warrants caution in broad application.

## Next Checks

1. Test perTucker on multi-source datasets with varying degrees of commonality to evaluate hyperparameter sensitivity and robustness across different heterogeneity scenarios.
2. Conduct ablation studies to quantify the contribution of the orthogonality constraint versus other design choices in the algorithm.
3. Evaluate the method's scalability and convergence behavior on larger tensor datasets with higher-order modes to verify the O(1/T) and O(1/√T) convergence rates hold in practice.