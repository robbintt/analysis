---
ver: rpa2
title: When hard negative sampling meets supervised contrastive learning
arxiv_id: '2308.14893'
source_url: https://arxiv.org/abs/2308.14893
tags:
- learning
- loss
- negative
- uni00000013
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new objective for supervised contrastive
  learning with hard negative sampling (SCHaNe) to address the limitations of cross-entropy
  loss in fine-tuning pre-trained image models. The proposed method explicitly leverages
  task labels to identify true positive and negative samples, and introduces a weighting
  scheme for hard negatives based on their dissimilarity to positive samples.
---

# When hard negative sampling meets supervised contrastive learning

## Quick Facts
- arXiv ID: 2308.14893
- Source URL: https://arxiv.org/abs/2308.14893
- Reference count: 22
- Key outcome: Introduces SCHaNe objective that outperforms BEiT-3 baseline with 86.14% Top-1 accuracy on ImageNet-1k

## Executive Summary
This paper addresses limitations of cross-entropy loss in fine-tuning pre-trained image models by introducing a supervised contrastive learning objective with hard negative sampling (SCHaNe). The method explicitly leverages task labels to identify true positive and negative samples while introducing a weighting scheme that emphasizes hard negatives based on their dissimilarity to positive samples. Experiments demonstrate that SCHaNe consistently outperforms the strong BEiT-3 baseline across multiple datasets in both few-shot learning and full dataset fine-tuning settings, achieving state-of-the-art performance on ImageNet-1k.

## Method Summary
The proposed SCHaNe method combines cross-entropy loss with a supervised contrastive learning objective that uses hard negative sampling. The loss function incorporates a hyperparameter λ to balance the contributions of the cross-entropy term and the proposed SCHaNe terms. The method uses label information to identify positive and negative samples, then applies a weighting scheme (β) that emphasizes hard negatives - those negative samples that have high dot product similarity with positive samples. This forces the model to create sharper decision boundaries between visually similar but semantically different classes. The encoder produces embeddings that are used for both classification and contrastive loss computation, with L2 normalization applied to outputs.

## Key Results
- SCHaNe outperforms BEiT-3 baseline across 8 datasets in full fine-tuning and 4 datasets in few-shot learning settings
- Achieves new state-of-the-art of 86.14% Top-1 accuracy on ImageNet-1k
- Produces more discriminative embeddings with better isotropy scores compared to cross-entropy alone
- Consistent performance improvements across different dataset sizes and few-shot scenarios

## Why This Works (Mechanism)

### Mechanism 1
The weighting scheme for hard negatives based on dissimilarity to positive samples improves discriminative power by forcing the model to focus on the most confusing examples. The β weight assigns higher importance to negative samples that have high dot product similarity with positive samples, making them "hard negatives". This pushes the model to create sharper decision boundaries between visually similar but semantically different classes. The core assumption is that embedding space similarity correlates with visual similarity that matters for classification.

### Mechanism 2
Combining cross-entropy loss with the supervised contrastive loss creates a complementary optimization that leverages both classification accuracy and representation learning. The weighted combination allows the model to maintain classification performance through cross-entropy while simultaneously learning better feature representations through contrastive learning. The λ hyperparameter balances these objectives, with higher values emphasizing representation learning.

### Mechanism 3
Label-aware positive/negative sample identification combined with hard negative mining produces more isotropic embedding spaces that generalize better. Using explicit labels to identify true positive and negative pairs while emphasizing hard negatives creates embeddings where classes are well-separated and uniformly distributed. This isotropy in embedding space correlates with generalization and classification performance.

## Foundational Learning

- **Concept**: Contrastive learning fundamentals (positive pairs should be close, negative pairs should be far)
  - Why needed here: The entire SCHaNe objective is built on contrastive learning principles, and understanding the core idea is essential to grasp why hard negative mining matters
  - Quick check question: In contrastive learning, what is the relationship between embeddings of positive pairs versus negative pairs?

- **Concept**: Supervised vs unsupervised contrastive learning
  - Why needed here: SCHaNe uses label information to identify true positives/negatives, which is different from unsupervised approaches like SimCLR
  - Quick check question: How does supervised contrastive learning differ from unsupervised contrastive learning in terms of sample selection?

- **Concept**: Hard negative mining and its benefits
  - Why needed here: The key innovation in SCHaNe is the explicit weighting of hard negatives based on their similarity to positives
  - Quick check question: Why are hard negatives (samples that are visually similar but from different classes) more informative for training than easy negatives?

## Architecture Onboarding

- **Component map**: Data augmentation → Encoder → Classification head → Loss computation (cross-entropy + SCHaNe)
- **Critical path**: The encoder produces embeddings that are used for both the classification head (for cross-entropy) and the contrastive loss computation (for SCHaNe). The quality of these embeddings directly determines performance.
- **Design tradeoffs**: The λ hyperparameter trades off between classification accuracy (cross-entropy) and representation learning (contrastive). Higher λ emphasizes representation learning but may reduce classification accuracy if taken to extremes.
- **Failure signatures**: If embeddings show poor class separation (measured by cosine similarity distributions or t-SNE visualizations), the hard negative weighting may be emphasizing wrong samples. If performance is similar to cross-entropy alone, the contrastive component may not be adding value.
- **First 3 experiments**:
  1. Run with λ=0 (pure cross-entropy) vs λ=0.9 (recommended) on CIFAR-FS 1-shot to verify performance improvement
  2. Visualize embedding spaces (t-SNE) for both settings to confirm better class separation with SCHaNe
  3. Measure isotropy scores for embeddings from both settings to confirm more uniform distribution with SCHaNe

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content there are several implicit areas for future research. The performance degradation when λ = 1 in few-shot learning tasks is observed but not explained. The method's effectiveness on extremely long-tailed class distributions is not explored, despite mentioning cross-entropy's sensitivity to class imbalance. The impact of different temperature values (τ) on the SCHaNe loss function's performance across various tasks and datasets is mentioned as optimized through grid search but lacks comprehensive analysis.

## Limitations

- Results are primarily validated on image classification tasks using BEiT-3 as the base model, limiting generalizability to other modalities or model architectures
- Hyperparameter λ was optimized via grid search, suggesting performance gains are sensitive to proper tuning
- Paper does not provide extensive ablation studies isolating the contribution of each component (label-awareness, hard negative weighting, and contrastive objective combination)

## Confidence

- **High confidence**: SCHaNe outperforms cross-entropy alone on the tested image classification datasets
- **Medium confidence**: The mechanism of hard negative weighting meaningfully improves discriminative embeddings (requires further ablation studies)
- **Medium confidence**: The isotropy improvements directly correlate with better generalization (correlation established but causation not definitively proven)

## Next Checks

1. **Ablation study**: Compare SCHaNe against variants: (a) label-aware contrastive without hard negative weighting, (b) hard negative weighting without label-awareness, to isolate each component's contribution.
2. **Architecture generalization**: Test SCHaNe on different backbone architectures (e.g., ViT, ConvNeXt) to verify the approach transfers beyond BEiT-3.
3. **Long-tail distribution test**: Evaluate SCHaNe on long-tail classification datasets to determine if hard negative mining remains beneficial when class frequencies are highly imbalanced.