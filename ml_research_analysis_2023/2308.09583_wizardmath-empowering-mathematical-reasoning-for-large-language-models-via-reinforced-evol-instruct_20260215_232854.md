---
ver: rpa2
title: 'WizardMath: Empowering Mathematical Reasoning for Large Language Models via
  Reinforced Evol-Instruct'
arxiv_id: '2308.09583'
source_url: https://arxiv.org/abs/2308.09583
tags:
- arxiv
- step
- articles
- language
- total
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "WizardMath improves open-source LLMs\u2019 mathematical reasoning\
  \ via a new RLEIF method combining Evol-Instruct and reinforced process supervision.\
  \ Applied to Llama-2, it achieves state-of-the-art performance on GSM8k (+24.8%\
  \ over Llama-2 70B) and MATH (+9.2% over Llama-2 70B), surpassing major closed-source\
  \ models like GPT-3.5 and Claude Instant on GSM8k."
---

# WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct

## Quick Facts
- **arXiv ID:** 2308.09583
- **Source URL:** https://arxiv.org/abs/2308.09583
- **Reference count:** 40
- **Key outcome:** WizardMath improves open-source LLMs' mathematical reasoning via a new RLEIF method combining Evol-Instruct and reinforced process supervision, achieving state-of-the-art performance on GSM8k (+24.8% over Llama-2 70B) and MATH (+9.2% over Llama-2 70B).

## Executive Summary
WizardMath is a new open-source language model for mathematical reasoning that significantly outperforms existing models on challenging benchmarks. The approach combines an innovative Evol-Instruct method with reinforced process supervision to fine-tune Llama-2 models. The method generates diverse, progressively harder math instructions through downward and upward evolution, then trains reward models to evaluate both instruction quality and step-by-step correctness. PPO training with composite rewards yields models that surpass major closed-source models like GPT-3.5 and Claude Instant on GSM8k.

## Method Summary
WizardMath employs a three-step process to enhance mathematical reasoning. First, it performs supervised fine-tuning on math datasets (GSM8k, MATH) and open-domain conversations. Second, it trains an Instruction Reward Model (IRM) to evaluate instruction quality and a Process-supervised Reward Model (PRM) to assess correctness of each solution step. Third, it applies 8 turns of Evol-Instruct to expand the dataset from 15k to 96k instructions, then performs PPO training using the product of IRM and PRM rewards as the final reward signal.

## Key Results
- Achieves 78.4% accuracy on GSM8k, a 24.8% absolute improvement over Llama-2 70B
- Achieves 44.8% accuracy on MATH, a 9.2% absolute improvement over Llama-2 70B
- Outperforms major closed-source models including GPT-3.5 and Claude Instant on GSM8k
- Demonstrates consistent performance improvements across Llama-2 model sizes (7B, 13B, 70B)

## Why This Works (Mechanism)

### Mechanism 1
The evol-instruct method generates diverse and progressively harder math instructions by applying downward and upward evolution steps, which improves model reasoning capacity. Downward evolution simplifies high-difficulty questions into easier versions, while upward evolution deepens and adds constraints to generate harder questions. This iterative process creates a spectrum of instruction complexities that expose the model to varied reasoning patterns. The core assumption is that diverse and progressively complex instruction data enhances model robustness and generalization in mathematical reasoning.

### Mechanism 2
The combination of Instruction Reward Model (IRM) and Process-supervised Reward Model (PRM) provides both instruction quality and step-by-step correctness feedback, enabling fine-tuning with multi-level supervision. IRM evaluates instruction quality on Definition, Precision, and Integrity; PRM checks each step in the solution. The product of IR and PR rewards forms the final reward signal used in PPO training. The core assumption is that step-by-step supervision captures logical errors better than outcome-only supervision, leading to more reliable reasoning.

### Mechanism 3
Reinforcement Learning via PPO with the composite reward (IRM Ã— PRM) fine-tunes Llama-2 to align with evolved instruction data and correct reasoning steps, yielding state-of-the-art performance. PPO optimizes policy parameters to maximize the product of instruction and process rewards over the evolved dataset, thereby reinforcing both instruction fidelity and reasoning accuracy. The core assumption is that PPO with well-calibrated reward models can effectively navigate the reward landscape without collapsing to trivial solutions.

## Foundational Learning

- **Chain-of-Thought (CoT) prompting**: Why needed here - CoT encourages the model to generate intermediate reasoning steps before producing the final answer, which is critical for complex multi-step math problems. Quick check question: What is the output format expected by the GSM8k and MATH evaluation prompts?
- **Reinforcement Learning from Human Feedback (RLHF)**: Why needed here - RLHF fine-tuning aligns model outputs with human preferences by optimizing rewards derived from instruction quality and reasoning correctness. Quick check question: How are the instruction and process rewards combined before being fed to PPO?
- **Reward Model Training**: Why needed here - Reward models (IRM and PRM) learn to score instruction quality and step correctness, providing differentiable feedback signals for RL optimization. Quick check question: What are the three aspects IRM evaluates in instruction quality?

## Architecture Onboarding

- **Component map**: Llama-2 (7B/13B/70B) -> Evol-Instruct pipeline (downward + upward evolution) -> Instruction Reward Model (IRM) -> Process-supervised Reward Model (PRM) -> PPO trainer
- **Critical path**: 1. Generate evolved instruction dataset via Evol-Instruct 2. Train IRM and PRM on instruction and step data 3. Run PPO to fine-tune Llama-2 with composite reward
- **Design tradeoffs**: Using ChatGPT for PRM supervision trades off open-source purity for accuracy; larger reward model size improves scoring but increases compute cost; more evolution turns increase data diversity but risk overfitting to synthetic patterns
- **Failure signatures**: Training collapse (reward model scores become uniform); Policy collapse (model outputs become repetitive or degenerate); Overfitting (performance drops on held-out math problems)
- **First 3 experiments**: 1. Train IRM and PRM separately and evaluate on a held-out validation set of instructions and step solutions 2. Run a small-scale PPO fine-tuning on a subset of evolved data to check reward signal stability 3. Evaluate the fine-tuned model on GSM8k dev set before full training to detect early overfitting

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of WizardMath scale with model size beyond 70B parameters? The paper evaluates WizardMath models of 7B, 13B, and 70B parameters, showing performance improvements with size, but does not explore larger scales. This remains unresolved as the paper does not provide experimental results for models larger than 70B.

### Open Question 2
What is the impact of using external tools like Python code execution on WizardMath's mathematical reasoning performance? The paper explicitly states that WizardMath enhances mathematical reasoning "without using external python tools," implying that the model's performance is evaluated without such aids. This remains unresolved as the paper does not explore or compare the performance with and without external computational tools.

### Open Question 3
How does WizardMath's performance compare to other open-source models when fine-tuned on similar instruction data? The paper demonstrates WizardMath's superior performance over other open-source models, but does not provide a direct comparison when these models are fine-tuned using the same instruction data and methodology. This remains unresolved as the paper does not conduct experiments where other open-source models are fine-tuned using the same RLEIF method and data as WizardMath.

## Limitations

- **Data Quality and Diversity**: The reliance on Evol-Instruct for generating math instructions introduces uncertainty around the diversity and difficulty calibration of the evolved dataset, without direct comparisons between evolved and non-evolved instruction sets.
- **Reward Model Calibration**: The product of IRM and PRM rewards as the final reward signal lacks ablation studies showing how each component contributes to performance, and sensitivity to reward scaling and variance is not explored.
- **Reproducibility Gaps**: Key implementation details are missing, including specific evolution turn parameters, reward model architectures, and PPO hyperparameters, limiting independent validation.

## Confidence

- **High Confidence**: The claim that WizardMath achieves state-of-the-art performance on GSM8k and MATH benchmarks is supported by specific accuracy numbers (+24.8% on GSM8k, +9.2% on MATH over Llama-2 70B) and direct comparisons to established models.
- **Medium Confidence**: The mechanism that diverse and progressively complex instruction data enhances model robustness is plausible given the evolution methodology description, but lacks direct experimental ablation to confirm this specific contribution versus other factors.
- **Low Confidence**: The claim that PPO with composite rewards reliably navigates the reward landscape without collapsing to trivial solutions lacks supporting evidence from reward model ablation studies or training stability analyses.

## Next Checks

1. **Reward Model Ablation Study**: Conduct experiments training WizardMath with only IRM reward, only PRM reward, and both rewards to quantify the individual and combined contributions to final performance.

2. **Evolution Turn Sensitivity Analysis**: Systematically vary the number of Evol-Instruct turns (e.g., 2, 4, 6, 8, 10) and measure the impact on instruction diversity metrics and downstream math performance to identify optimal evolution depth.

3. **Step-by-Step Solution Quality Audit**: Manually examine 100 randomly selected solutions from the fine-tuned model across different difficulty levels to assess logical consistency, correctness of intermediate steps, and identify systematic failure patterns.