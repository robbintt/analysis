---
ver: rpa2
title: Probabilistic inverse optimal control for non-linear partially observable systems
  disentangles perceptual uncertainty and behavioral costs
arxiv_id: '2303.16698'
source_url: https://arxiv.org/abs/2303.16698
tags:
- control
- optimal
- agent
- likelihood
- inverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a probabilistic approach to inverse optimal
  control (IOC) for non-linear partially observable systems that can infer cost functions
  and other model parameters from observed trajectories. The method addresses limitations
  of existing IOC techniques by modeling partial observability through explicit observation
  models and action uncertainty via maximum causal entropy policies.
---

# Probabilistic inverse optimal control for non-linear partially observable systems disentangles perceptual uncertainty and behavioral costs

## Quick Facts
- arXiv ID: 2303.16698
- Source URL: https://arxiv.org/abs/2303.16698
- Reference count: 26
- Key outcome: Method achieves median absolute relative errors of 0.11-0.41 across parameters, significantly outperforming baseline (0.93-1.99 errors)

## Executive Summary
This paper presents a probabilistic approach to inverse optimal control for non-linear partially observable systems that can infer cost functions and other model parameters from observed trajectories. The method addresses limitations of existing IOC techniques by modeling partial observability through explicit observation models and action uncertainty via maximum causal entropy policies. Using local linearization and extended Kalman filtering, the approach computes an approximate likelihood function in closed form within a single forward pass. Evaluations on classic control tasks (Pendulum, Cart Pole) and human behavioral tasks (reaching, navigation) demonstrate the method's ability to disentangle perceptual uncertainty from behavioral costs.

## Method Summary
The method computes an approximate likelihood for inverse optimal control in non-linear POMDPs using local linearization around observed trajectories and extended Kalman filtering for belief tracking. It combines maximum causal entropy policy formulation with iLQG control laws to enable inference of both behavioral costs and noise characteristics simultaneously. The approach linearizes dynamics once around the observed trajectory and propagates a Gaussian belief distribution through joint state-belief dynamics, allowing tractable marginalization over unobserved internal variables. Gradient-based optimization (L-BFGS) is then used to find maximum likelihood estimates of model parameters including cost function weights and noise characteristics.

## Key Results
- Median absolute relative errors of 0.11-0.41 across parameters for the proposed method
- Significantly outperforms maximum causal entropy baseline (0.93-1.99 errors)
- Successfully disentangles perceptual uncertainty from behavioral costs in sequential decision-making tasks
- Demonstrates accurate parameter recovery in both simulated control tasks and human behavioral experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local linearization around observed trajectories enables tractable likelihood computation in non-linear POMDPs.
- Mechanism: The method linearizes dynamics around the observed trajectory once, avoiding iterative iLQG forward-backward passes for each parameter update. This linearization allows Gaussian approximations of belief distributions through extended Kalman filtering, making marginalization tractable.
- Core assumption: The true trajectory is close enough to the linearization point that first-order Taylor expansion captures the essential dynamics.
- Evidence anchors:
  - [abstract]: "Using local linearization and extended Kalman filtering, the approach computes an approximate likelihood function in closed form within a single forward pass."
  - [section 3.3]: "we can simply linearize the dynamics once around the given trajectory and keep this linearization fixed... allows efficient computation of the gradient of the likelihood function."
  - [corpus]: Weak evidence - related papers focus on POMDP control but don't address the specific linearization-IOC combination.
- Break condition: If the observed trajectory deviates significantly from the linearization point, the first-order approximation becomes invalid and likelihood estimates degrade.

### Mechanism 2
- Claim: Gaussian belief tracking reconciles partial observability with inverse optimal control by maintaining a distribution over agent's internal state estimates.
- Mechanism: The method propagates a Gaussian belief distribution through the joint dynamics of states and beliefs, allowing marginalization over unobserved internal variables (beliefs, actions) while computing the likelihood.
- Core assumption: The belief distribution remains approximately Gaussian throughout the trajectory.
- Evidence anchors:
  - [abstract]: "Using an explicit model of the noise characteristics of the sensory and motor systems of the agent in conjunction with local linearization techniques, we derive an approximate likelihood for the model parameters"
  - [section 3.2]: "we model the agent's belief dynamics using the extended Kalman filter... this choice leads to an affine control law and belief dynamics given bt, making linearization of p(xt+1, bt+1|xt, bt) straight-forward."
  - [corpus]: Moderate evidence - POMDP filtering literature supports Gaussian approximations, but integration with IOC is novel.
- Break condition: If belief distributions become highly non-Gaussian (e.g., multimodal beliefs), the approximation breaks down and parameter estimates become biased.

### Mechanism 3
- Claim: Maximum causal entropy policy formulation allows inferring both behavioral costs and noise characteristics simultaneously.
- Mechanism: The MCE policy introduces stochasticity that can be separated from observation noise, enabling inference of both the cost function parameters and noise characteristics (σm, σo) from the same trajectory data.
- Core assumption: The agent's stochasticity can be decomposed into observation noise and policy entropy components.
- Evidence anchors:
  - [abstract]: "Importantly, we show that our method can disentangle perceptual factors and behavioral costs despite the fact that epistemic and pragmatic actions are intertwined in sequential decision-making under uncertainty"
  - [section 3.2]: "To allow for additional stochasticity in the agent's policy, we follow the common formulation of maximum causal entropy (MCE) reinforcement learning"
  - [corpus]: Strong evidence - MCE IRL literature extensively discusses entropy-based stochastic policies for behavior inference.
- Break condition: If the true policy stochasticity is non-Gaussian or highly structured, the MCE assumption may fail to capture the actual noise structure.

## Foundational Learning

- Concept: Extended Kalman Filter (EKF) for non-linear systems
  - Why needed here: EKF provides the Gaussian approximation of belief distributions needed for tractable marginalization in the IOC likelihood
  - Quick check question: How does EKF handle non-linear observation functions differently from the standard Kalman filter?

- Concept: Iterative Linear Quadratic Gaussian (iLQG) control
  - Why needed here: iLQG provides the locally optimal control law used for linearization points and belief dynamics modeling
  - Quick check question: What is the relationship between the iLQG control law and the MCE policy in the linearized system?

- Concept: Maximum Causal Entropy (MCE) reinforcement learning
  - Why needed here: MCE provides the stochastic policy formulation that separates behavioral stochasticity from observation noise
  - Quick check question: How does the MCE policy entropy term affect the shape of the IOC likelihood function?

## Architecture Onboarding

- Component map: Trajectory linearization module -> Belief tracking module -> Policy inference module -> Likelihood evaluation module -> Parameter optimization module
- Critical path: Trajectory linearization → Belief tracking → Policy inference → Likelihood evaluation → Parameter optimization
- Design tradeoffs:
  - Accuracy vs. computational efficiency: Iterative linearization vs. single linearization around observed trajectory
  - Gaussian assumption vs. flexibility: Closed-form tractability vs. potential bias for non-Gaussian beliefs
  - MCE stochasticity vs. deterministic policies: Ability to infer noise parameters vs. simplicity of deterministic control
- Failure signatures:
  - Poor parameter estimates with high variance: Likely indicates linearization is inadequate for the trajectory
  - Systematic bias in noise parameter estimates: May indicate Gaussian belief assumption is violated
  - Optimization getting stuck in local minima: Could suggest identifiability issues with the parameter set
- First 3 experiments:
  1. Pendulum task with known ground truth parameters: Verify that the method recovers parameters accurately in a simple, well-behaved system
  2. Partially observable reaching task with varying noise levels: Test the method's ability to disentangle motor and observation noise
  3. Cart pole with corrupted observations: Evaluate robustness to severe partial observability conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the method perform on tasks with multimodal belief distributions that cannot be well-approximated by a Gaussian?
- Basis in paper: [explicit] The authors acknowledge that the method may produce inaccurate results for more complex scenarios with belief distributions that are not well-approximated by a Gaussian.
- Why unresolved: The paper focuses on tasks that can be solved using controllers based on linearization and Gaussian approximation, and does not provide evidence on how the method would perform in scenarios with multimodal beliefs.
- What evidence would resolve it: Testing the method on tasks with known multimodal belief distributions and comparing the results to ground truth or other methods that can handle such distributions.

### Open Question 2
- Question: How does the method scale to problems with a large number of parameters, and what are the limitations in terms of identifiability?
- Basis in paper: [explicit] The authors mention that the method probably does not scale to a large number of parameters due to difficulties in optimization and potential identifiability issues.
- Why unresolved: The paper does not provide empirical evidence on the performance of the method with a large number of parameters or investigate the identifiability of model parameters in such cases.
- What evidence would resolve it: Conducting experiments with varying numbers of parameters and analyzing the identifiability of the parameters using techniques like Bayesian methods or sensitivity analysis.

### Open Question 3
- Question: How would the method perform in high-dimensional problems that cannot be solved forward using iLQG?
- Basis in paper: [inferred] The authors mention that high-dimensional problems that cannot be solved forward using iLQG are probably not directly solvable using the proposed method.
- Why unresolved: The paper focuses on tasks that can be solved using iLQG and does not provide evidence on how the method would perform in high-dimensional problems or with alternative forward control methods.
- What evidence would resolve it: Testing the method on high-dimensional problems using alternative forward control methods and comparing the results to ground truth or other methods that can handle such problems.

## Limitations
- Reliance on local linearization may fail for highly non-linear systems or trajectories that deviate significantly from the linearization point
- Gaussian belief assumption may not hold for tasks with multimodal beliefs or complex observation structures
- Assumes agent uses MCE policies with iLQG control laws, which may not capture all forms of human decision-making behavior

## Confidence
- **High**: The method's ability to outperform maximum causal entropy baselines in parameter estimation accuracy (median absolute relative errors of 0.11-0.41 vs 0.93-1.99)
- **Medium**: The claim that the method can disentangle perceptual uncertainty from behavioral costs in sequential decision-making tasks
- **Medium**: The computational efficiency gains from single-pass linearization versus iterative iLQG methods

## Next Checks
1. Test on a system with known multi-modal belief distributions to assess Gaussian approximation breakdown points
2. Compare parameter recovery accuracy across different trajectory lengths to establish sample efficiency
3. Evaluate robustness to observation noise levels that exceed the assumed noise model bounds