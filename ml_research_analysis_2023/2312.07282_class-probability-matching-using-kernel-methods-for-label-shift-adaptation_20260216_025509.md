---
ver: rpa2
title: Class Probability Matching Using Kernel Methods for Label Shift Adaptation
arxiv_id: '2312.07282'
source_url: https://arxiv.org/abs/2312.07282
tags:
- probability
- shift
- label
- class
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Class Probability Matching using Kernel Methods
  (CPMKM), a new approach for label shift adaptation in domain adaptation problems.
  Unlike existing methods that rely on feature probability matching (FPM) in high-dimensional
  feature spaces, CPMKM directly matches class probability functions in the one-dimensional
  label space.
---

# Class Probability Matching Using Kernel Methods for Label Shift Adaptation

## Quick Facts
- arXiv ID: 2312.07282
- Source URL: https://arxiv.org/abs/2312.07282
- Reference count: 40
- Key outcome: CPMKM outperforms existing FPM-based and maximum-likelihood-based algorithms on real datasets, particularly for datasets with a large number of classes.

## Executive Summary
This paper introduces Class Probability Matching using Kernel Methods (CPMKM), a new approach for label shift adaptation in domain adaptation problems. Unlike existing methods that rely on feature probability matching (FPM) in high-dimensional feature spaces, CPMKM directly matches class probability functions in the one-dimensional label space. The authors propose a novel framework called Class Probability Matching (CPM) and incorporate kernel logistic regression to estimate conditional probabilities. Theoretically, the paper establishes optimal convergence rates for CPMKM with respect to cross-entropy loss for multi-class label shift adaptation. Empirically, CPMKM outperforms existing FPM-based and maximum-likelihood-based algorithms on real datasets, particularly for datasets with a large number of classes. The method demonstrates superior performance in both class probability estimation error and classification accuracy in the target domain.

## Method Summary
CPMKM addresses label shift adaptation by matching class probability functions in the one-dimensional label space rather than feature probabilities in high-dimensional feature space. The method uses truncated kernel logistic regression to estimate conditional probabilities from source domain data, then applies a Class Probability Matching framework to estimate the class probability ratio between domains. This ratio is used to reweight source domain predictions for target domain classification. The approach avoids computational and estimation issues associated with FPM methods while establishing optimal convergence rates for cross-entropy loss.

## Key Results
- CPMKM achieves lower classification error than existing FPM-based and maximum-likelihood-based methods on multi-class datasets
- The method demonstrates superior performance in class probability estimation error compared to baseline approaches
- CPMKM establishes optimal convergence rates for label shift adaptation with respect to cross-entropy loss

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CPMKM matches class probabilities in the one-dimensional label space rather than feature probabilities in the high-dimensional feature space, avoiding computational and estimation issues in FPM methods.
- **Mechanism:** CPMKM constructs a matching equation on the source domain's class probability function p(y), which is represented as an expectation of a function involving the conditional probability p(y|x) and the class probability ratio q(y)/p(y) with respect to the target domain's feature distribution q(x). This allows estimation of the class probability ratio w*(y) by solving a system of equations in the discrete label space.
- **Core assumption:** The class-conditional probabilities p(x|y) and q(x|y) are equal (label shift assumption) and the class-conditional probability density functions {q(·|y) : y ∈ [M]} are linearly independent.
- **Evidence anchors:**
  - [abstract]: "CPM matches two class probability functions on the one-dimensional label space to estimate the class probability ratio, fundamentally different from FPM operating on the d-dimensional feature space."
  - [section 3.1]: "CPM only needs to solve an equation system due to the discreteness of the label space, which effectively avoids potential issues associated with FPM in the feature space."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.37, average citations=0.0. Weak corpus evidence for this specific mechanism.
- **Break condition:** If the linear independence assumption fails or if the conditional probability estimation is highly inaccurate, the matching equation may not have a unique solution or may produce erroneous estimates.

### Mechanism 2
- **Claim:** Truncated kernel logistic regression (KLR) provides stable conditional probability estimates by bounding the cross-entropy loss, enabling convergence rate analysis for label shift adaptation.
- **Mechanism:** KLR uses a truncated conditional probability estimator pt_f(m|x) that is always greater than or equal to a threshold t, ensuring the cross-entropy loss is bounded by -log t. This allows decomposition of the loss into upper and lower parts for analysis, establishing oracle inequalities and optimal convergence rates.
- **Core assumption:** The conditional probability function p(m|x) satisfies Hölder smoothness and small value bound conditions, and the truncation threshold t is chosen appropriately (t ≍ n^(-α/((1+β∧1)α+d))).
- **Evidence anchors:**
  - [section 3.2]: "To prevent the cross-entropy loss of pf(·|x), i.e. LCE(y, pf(·|x)) = - log pf(y|x) from exploding, we need to truncate pf(·|x)) downwards."
  - [section 5.1.2]: "We decompose the unbounded CE loss into an upper part and a lower part depending on whether the true conditional probability is greater or less than a certain value δ ∈ (0, 1)."
  - [corpus]: Weak corpus evidence for this specific mechanism.
- **Break condition:** If the truncation threshold t is set too high, the approximation error may become large; if set too low, the sample error bound may become large, degrading overall performance.

### Mechanism 3
- **Claim:** The excess risk of CPMKM in the target domain depends on both the excess risk of truncated KLR in the source domain and the estimation error of the class probability ratio, with optimal convergence rates achieved.
- **Mechanism:** The error decomposition shows that the excess CE risk of bq(y|x) is bounded by the sum of the excess CE risk of bp(y|x) in the source domain and the squared L2-norm error of the class probability ratio estimation ∥bw - w*∥². The convergence rates of bp(y|x) established via truncated KLR directly translate to rates for bq(y|x).
- **Core assumption:** The regularity assumption holds, ensuring that the weighted sums of estimated conditional probabilities are bounded away from zero, and the linear independence assumption holds for identifiability.
- **Evidence anchors:**
  - [section 5.3]: "To derive the excess risk of bq(y|x) in (22), let us define eq(m|x) = w*(m)bp(m|x)/Σj∈[M]w*(j)bp(j|x). Then we are able to make the error decomposition for the excess risk of bq(y|x) as RLCE,Q(bq(y|x)) - R*LCE,Q ≤ |RLCE,Q(bq(y|x)) - RLCE,Q(eq(y|x))| + RLCE,Q(eq(y|x)) - R*LCE,Q."
  - [section 4.2]: "Theorem 3 shows that up to the arbitrarily small constant ξ, we can see from (23) that the convergence rate of CPMKM depends on the larger term of n^(-(1+β∧1)α/((1+β∧1)α+d)) and n^(-1)q."
  - [corpus]: Weak corpus evidence for this specific mechanism.
- **Break condition:** If the class probability ratio estimation error ∥bw - w*∥² is large relative to the excess risk of bp(y|x), the overall performance may degrade significantly.

## Foundational Learning

- **Concept:** Domain adaptation and the distinction between covariate shift and label shift
  - **Why needed here:** CPMKM specifically addresses label shift adaptation, which requires understanding that the conditional probabilities p(y|x) and q(y|x) remain the same while the class probabilities p(y) and q(y) differ. This is fundamentally different from covariate shift where feature probabilities differ but class probabilities remain the same.
  - **Quick check question:** In label shift, which components of the joint distribution p(x,y) change between source and target domains?

- **Concept:** Kernel methods and reproducing kernel Hilbert spaces (RKHS)
  - **Why needed here:** CPMKM uses kernel logistic regression to estimate conditional probabilities in a high-dimensional feature space implicitly mapped by a kernel function. Understanding RKHS theory is essential for grasping how the kernel enables non-linear decision boundaries and how the kernel bandwidth affects convergence rates.
  - **Quick check question:** How does the choice of kernel bandwidth γ affect the approximation error and sample error in kernel logistic regression?

- **Concept:** Concentration inequalities and empirical process theory
  - **Why needed here:** The theoretical analysis of CPMKM relies on concentration inequalities (e.g., Bernstein's inequality) to bound deviations between empirical and expected risks, and empirical process theory to establish uniform convergence over function classes. These tools are crucial for proving the finite-sample error bounds and convergence rates.
  - **Quick check question:** Why is it necessary to decompose the cross-entropy loss into upper and lower parts when analyzing the truncated KLR estimator?

## Architecture Onboarding

- **Component map:** Source data -> KLR estimation -> Class probability estimation -> CPM ratio estimation -> Target prediction
- **Critical path:** Source domain labeled data → Truncated kernel logistic regression → Class probability estimation → CPM ratio estimation → Target domain predictions
- **Design tradeoffs:**
  - Kernel choice: Gaussian kernel provides smoothness but requires bandwidth selection; other kernels may be more appropriate for specific data structures
  - Truncation threshold t: Affects the balance between approximation error and sample error in KLR; too high causes large approximation error, too low causes large sample error
  - Regularization parameter λ: Controls the trade-off between fitting the training data and keeping the model complexity low; affects both estimation accuracy and generalization
- **Failure signatures:**
  - Poor performance on datasets with highly imbalanced classes or small sample sizes in some classes
  - Sensitivity to the choice of kernel bandwidth and regularization parameters
  - Degradation when the linear independence assumption is violated or when conditional probability estimation is highly inaccurate
- **First 3 experiments:**
  1. **Sanity check on synthetic data:** Generate synthetic data with known label shift (e.g., Dirichlet shift with varying α) and verify that CPMKM correctly estimates the class probability ratio and achieves lower error than baseline methods.
  2. **Ablation study on truncation:** Compare CPMKM with different truncation thresholds t to demonstrate the trade-off between approximation error and sample error, and identify the optimal choice.
  3. **Convergence rate validation:** Fix the source domain sample size np and vary the target domain sample size nq, plotting the classification accuracy and MSE against nq to verify the theoretical convergence rates (initially improving then stabilizing).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal mapping function h for transforming features in label shift adaptation when using moment matching methods?
- Basis in paper: [explicit] The paper discusses moment matching methods (like ELSA) that use a mapping function h to transform features into lower-dimensional spaces, noting that "the optimal choice of the transformation map h remains uncertain across different datasets."
- Why unresolved: While the paper demonstrates CPMKM's superiority without requiring feature space transformations, the optimal mapping function for moment matching approaches remains an open problem that could potentially improve existing FPM-based methods.
- What evidence would resolve it: Empirical studies comparing different transformation functions (linear, non-linear, learned) across diverse datasets showing consistent performance improvements or establishing theoretical bounds on optimal transformation properties.

### Open Question 2
- Question: How does the truncation threshold t affect the trade-off between sample error and approximation error in CPMKM?
- Basis in paper: [explicit] The paper mentions that "a sufficiently small t not only makes the CE loss of pt f0(y|x) bounded by −log t, but also ensures the upper bound of the approximation error of pt f0(y|x)" and discusses the trade-off between sample error and approximation error.
- Why unresolved: While the paper establishes theoretical convergence rates for specific t values, the optimal adaptive selection of t during training based on dataset characteristics and sample sizes remains unexplored.
- What evidence would resolve it: Experiments systematically varying t across datasets and sample sizes, combined with theoretical analysis of adaptive threshold selection strategies that minimize the overall error bound.

### Open Question 3
- Question: Can CPMKM be extended to handle cases where p(y) = 0 for some classes but q(y) > 0?
- Basis in paper: [inferred] The paper assumes Assumption 3(i) that p(y) > 0 for all y ∈ [M], but notes this "does not necessarily require that q(y) > 0, ∀y ∈ [M], which turns out to be more realistic."
- Why unresolved: The current theoretical framework and algorithm rely on p(y) > 0, yet practical scenarios may involve classes present in the target domain that were absent in the source domain.
- What evidence would resolve it: Theoretical extensions of the CPM framework to handle zero-probability source classes, along with experimental validation on datasets with class emergence in the target domain.

## Limitations

- The theoretical analysis relies on strong assumptions including linear independence of class-conditional probability density functions that may not hold in practice
- Method performance is sensitive to hyperparameter choices including kernel bandwidth and truncation threshold
- Limited empirical evaluation on diverse datasets beyond UCI and OpenML repositories

## Confidence

- **High confidence**: The fundamental advantage of CPMKM over FPM methods in avoiding high-dimensional feature space matching
- **Medium confidence**: The optimal convergence rates established for CPMKM
- **Medium confidence**: The empirical superiority of CPMKM over baseline methods

## Next Checks

1. **Stress test with violated assumptions**: Evaluate CPMKM performance when the linear independence assumption is violated by using synthetic data with overlapping class-conditional distributions to assess robustness to assumption violations.

2. **Hyperparameter sensitivity analysis**: Systematically vary kernel bandwidth and truncation threshold across a wide range of values to map the performance landscape and identify optimal settings for different dataset characteristics.

3. **Cross-dataset generalization**: Test CPMKM on additional datasets from diverse domains (e.g., text, image, and time series) to validate the method's generalizability beyond the UCI and OpenML datasets used in the paper.