---
ver: rpa2
title: Guided Distillation for Semi-Supervised Instance Segmentation
arxiv_id: '2308.02668'
source_url: https://arxiv.org/abs/2308.02668
tags:
- teacher
- data
- labeled
- training
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of training instance segmentation
  models with limited labeled data by leveraging unlabeled images through a semi-supervised
  learning approach. The authors propose a novel distillation strategy that improves
  upon existing teacher-student methods by introducing a "guided burn-in" stage.
---

# Guided Distillation for Semi-Supervised Instance Segmentation

## Quick Facts
- arXiv ID: 2308.02668
- Source URL: https://arxiv.org/abs/2308.02668
- Authors: 
- Reference count: 25
- Primary result: Improves semi-supervised instance segmentation by introducing guided burn-in and advanced backbones

## Executive Summary
This work addresses the challenge of training instance segmentation models with limited labeled data by leveraging unlabeled images through a semi-supervised learning approach. The authors propose a novel distillation strategy that improves upon existing teacher-student methods by introducing a "guided burn-in" stage. In this stage, the student model is pre-trained using pseudo labels from a fixed teacher, allowing it to benefit from unlabeled data from the start rather than relying solely on labeled data. The method uses a Mask2Former architecture with vision transformer backbones, including Swin and DINOv2, and employs EMA updates for the teacher. The approach is evaluated on Cityscapes and COCO datasets, showing substantial improvements over state-of-the-art methods.

## Method Summary
The method employs a teacher-student distillation framework with a Mask2Former architecture using vision transformer backbones (Swin and DINOv2). The key innovation is a "guided burn-in" stage where the student is pre-trained using pseudo labels from a fixed teacher on both labeled and unlabeled data. This is followed by a second stage where the teacher is updated with EMA of the student's weights. The approach uses strong data augmentations for the student and weaker ones for the teacher. Training involves computing supervised and unsupervised losses, with the latter using pseudo labels filtered and matched between teacher and student predictions.

## Key Results
- On Cityscapes with 10% labeled data, mask-AP improves from 23.7 to 33.9
- On COCO with 1% labeled data, mask-AP improves from 18.3 to 34.1
- Models with DINOv2 backbones show high performance and robustness at very low annotation regimes
- The guided burn-in stage significantly improves performance compared to standard teacher-student approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The guided burn-in stage improves the student model by allowing it to learn from both labeled and unlabeled data from the start, rather than waiting until after initial supervised training.
- Mechanism: By training the student model on both labeled data and unlabeled data using pseudo labels from a fixed teacher during the burn-in stage, the student can benefit from the larger pool of unlabeled data early in training. This reduces overfitting to the limited labeled data and allows the student to learn more generalizable features.
- Core assumption: The teacher model's pseudo labels are of sufficient quality to provide useful training signal to the student during the burn-in stage.
- Evidence anchors:
  - [abstract]: "Contrary to previous work which uses only supervised data for the burn-in period of the student model, we also use guidance of the teacher model to exploit unlabeled data in the burn-in period."
  - [section]: "During this initial training stage, which we refer as our revisited burn-in stage, the teacher model stays constant and is therefore not influenced by the student predictions. This enables the student model to learn from more data..."
  - [corpus]: Weak evidence. Corpus lacks direct discussion of burn-in mechanisms, but does show semi-supervised approaches are active research area.
- Break condition: If the teacher model is too noisy or inaccurate, its pseudo labels could mislead the student during burn-in, leading to worse performance than supervised-only training.

### Mechanism 2
- Claim: Using advanced backbone networks like Swin and DINOv2 improves performance, especially in low annotation regimes.
- Mechanism: These advanced backbones, particularly when pre-trained on large datasets, provide stronger feature representations that are more transferable to the instance segmentation task. DINOv2's self-supervised pre-training on 142M images seems especially effective when labeled data is scarce.
- Core assumption: The feature representations learned by these backbones during pre-training are relevant and useful for the instance segmentation task.
- Evidence anchors:
  - [abstract]: "...we leverage vision transformer architectures for the first time in the context of semi-supervised instance segmentation."
  - [section]: "We find that models trained with DINOv2 backbones show high performance and are on par with Swin backbones. Such models are also very robust at very low annotation regimes..."
  - [corpus]: Weak evidence. Corpus mentions other semi-supervised approaches but doesn't discuss backbone architectures in detail.
- Break condition: If the pre-training dataset is too different from the target dataset, or if the backbone architecture is not well-suited to the segmentation task, the benefits may not materialize.

### Mechanism 3
- Claim: The EMA (Exponential Moving Average) teacher updates provide stable and strong training signal to the student.
- Mechanism: By updating the teacher as an EMA of the student's weights, the teacher's predictions become more stable and consistent over time. This stability helps ensure that the pseudo labels provided to the student are of high quality and don't fluctuate wildly between training iterations.
- Core assumption: EMA updates create a smoother, more reliable teacher than using the student's weights directly or using a fixed teacher.
- Evidence anchors:
  - [abstract]: "...we adopt a student-teacher training with EMA, with several important differences w.r.t. the Polite Teacher approach..."
  - [section]: "During the second stage of training, the teacher model is updated with an exponential moving average of the student's weights. This approach has been proven to stabilize training by providing more regular pseudo labels during the student's training..."
  - [corpus]: No direct evidence. Corpus mentions teacher-student distillation but not EMA specifically.
- Break condition: If the EMA decay rate is set too high or too low, the teacher may either change too slowly to be useful or too quickly to provide stable guidance.

## Foundational Learning

- Concept: Semi-supervised learning
  - Why needed here: The entire approach relies on leveraging unlabeled data to improve performance when labeled data is scarce.
  - Quick check question: What are the main challenges in semi-supervised learning, and how does this approach address them?

- Concept: Teacher-student distillation
  - Why needed here: The method uses a teacher model to generate pseudo labels for training a student model, which is a core component of the approach.
  - Quick check question: How does teacher-student distillation work, and what are the key factors for its success?

- Concept: Vision transformer architectures
  - Why needed here: The paper explores using vision transformers (specifically Swin and ViT with DINOv2) as backbones for the first time in semi-supervised instance segmentation.
  - Quick check question: What are the advantages of vision transformers over traditional convolutional backbones for segmentation tasks?

## Architecture Onboarding

- Component map: Input -> Backbone (Swin/DINOv2) -> Pixel Decoder -> Transformer Decoder -> Segmentation Head
- Critical path: Student model's training loop: get predictions from student and teacher -> filter and match pseudo labels -> compute supervised and unsupervised losses -> update student weights
- Design tradeoffs: Using more advanced backbones (Swin-L, ViT-L) improves performance but increases computational cost. The choice of burn-in length and EMA decay rate involves balancing between learning speed and stability.
- Failure signatures: If the student diverges during training, it could indicate issues with the teacher's pseudo labels or the unsupervised loss weighting. Poor performance on the labeled data validation set could suggest overfitting to the unlabeled data.
- First 3 experiments:
  1. Run the supervised baseline with the chosen backbone architecture to establish a performance reference.
  2. Implement the guided burn-in stage and compare performance with and without it.
  3. Test different values for the unsupervised loss weight (Î»u) to find the optimal setting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the guided burn-in stage perform when applied to other vision tasks beyond instance segmentation, such as object detection or semantic segmentation?
- Basis in paper: [explicit] The authors suggest in their conclusions that testing their approach for other prediction tasks such as object detection would be valuable future work.
- Why unresolved: The paper focuses exclusively on instance segmentation and does not provide empirical results or theoretical analysis for other tasks.
- What evidence would resolve it: Experiments applying the guided burn-in method to object detection and semantic segmentation benchmarks, comparing performance against existing semi-supervised methods for those tasks.

### Open Question 2
- Question: What is the impact of using different backbone architectures beyond the ones tested (ResNet-50, Swin, and DINOv2) on the performance of the guided distillation approach?
- Basis in paper: [explicit] The authors mention that exploring transformer architectures better suited for segmentation on large scale datasets in an SSL setting is left for future work.
- Why unresolved: The paper only evaluates three backbone architectures and does not provide a comprehensive analysis of how different architectures affect the method's performance.
- What evidence would resolve it: Extensive experiments using a wide range of backbone architectures, including both convolutional and transformer-based models, across various datasets and label percentages.

### Open Question 3
- Question: How does the guided distillation approach perform in scenarios with extreme domain shifts between the labeled and unlabeled datasets?
- Basis in paper: [explicit] The authors mention in their conclusions that assessing the robustness of their approach to domain shifts would be interesting future work.
- Why unresolved: The paper evaluates the method on standard datasets (Cityscapes and COCO) but does not test its performance when the labeled and unlabeled data come from different distributions or domains.
- What evidence would resolve it: Experiments where the labeled dataset and unlabeled dataset are from different domains (e.g., using synthetic data for labels and real-world data for unlabeled images) and comparing the performance degradation against other semi-supervised methods.

## Limitations

- The exact implementation details of strong data augmentations for the student model are not fully specified, which could lead to variations in performance when attempting reproduction.
- The paper does not explore the optimal balance between the number of labeled samples and the amount of unlabeled data for maximizing performance.
- The method's performance in scenarios with extreme domain shifts between labeled and unlabeled datasets is not evaluated.

## Confidence

- High confidence in the core mechanism of guided burn-in improving performance over standard teacher-student approaches.
- Medium confidence in the benefits of using advanced backbones (Swin, DINOv2) specifically for semi-supervised instance segmentation.
- Medium confidence in the overall approach's robustness across different annotation budgets.

## Next Checks

1. **Ablation of Burn-in Length**: Systematically test different burn-in durations (e.g., 1K, 2K, 5K iterations) on a subset of data to determine the optimal length and verify the proportional relationship with labeled data percentage.

2. **Augmentation Intensity Sweep**: Perform a sensitivity analysis on the strength of data augmentations for the student model, varying parameters like color jitter magnitude, crop ratios, and blur kernel sizes to find the optimal configuration.

3. **Teacher Quality Analysis**: Evaluate the quality of pseudo labels generated by the teacher model at different training stages and their correlation with student performance to understand the limits of the guided burn-in approach.