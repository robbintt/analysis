---
ver: rpa2
title: Transformer-based interpretable multi-modal data fusion for skin lesion classification
arxiv_id: '2304.14505'
source_url: https://arxiv.org/abs/2304.14505
tags:
- classi
- metadata
- cation
- vitatt
- skin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose a transformer-based architecture for interpretable multi-modal
  data fusion in skin lesion classification. Our method combines dermoscopic image
  features and patient metadata via a multi-head self-attention layer for joint feature
  learning.
---

# Transformer-based interpretable multi-modal data fusion for skin lesion classification

## Quick Facts
- arXiv ID: 2304.14505
- Source URL: https://arxiv.org/abs/2304.14505
- Reference count: 40
- Key outcome: ViTAtt achieves state-of-the-art classification accuracy on PAD UFES 20 and ISIC 2019 datasets, outperforming single- and multi-modal baselines while providing interpretable attention-based explanations.

## Executive Summary
This paper proposes a transformer-based architecture for interpretable multi-modal data fusion in skin lesion classification. The method combines dermoscopic image features and patient metadata through a multi-head self-attention layer for joint feature learning. The architecture achieves state-of-the-art classification accuracy on standard datasets while enabling native interpretability through attention mechanisms that highlight relevant image regions and metadata features. This transparency helps dermatologists understand model decisions and builds trust in AI-assisted diagnosis.

## Method Summary
The ViTAtt architecture processes dermoscopic images using a Vision Transformer encoder to extract patch embeddings, which are combined with patient metadata embeddings through a multi-head self-attention fusion layer. The metadata is first embedded using fully connected layers with batch normalization. The fused representation passes through a classification head to produce class probabilities. The model is trained end-to-end using weighted cross-entropy loss with Adam optimizer and custom learning rates. Interpretability is achieved through attention maps and the TMME method, which provides saliency and relevancy visualizations for both image and metadata domains.

## Key Results
- Achieves state-of-the-art classification accuracy on PAD UFES 20 and ISIC 2019 datasets
- Single-stage attention-based fusion outperforms two-stage fusion architectures
- TMME provides superior interpretability compared to attention maps alone
- Architecture is robust to metadata sparsity and image quality variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-stage attention-based fusion outperforms two-stage fusion architectures
- Mechanism: Multi-head self-attention enables all-pairs interaction between image patches, metadata tokens, and class token in one forward pass
- Core assumption: Attention can learn meaningful cross-modal relationships when all modalities share the same embedding space
- Evidence anchors: [abstract] "We enable single-stage multi-modal data fusion via the attention mechanism of transformer-based architectures"
- Break condition: If embedding spaces of image and metadata are incompatible or attention cannot learn meaningful cross-modal dependencies

### Mechanism 2
- Claim: TMME provides superior interpretability compared to attention maps alone
- Mechanism: TMME integrates gradient backpropagation with attention maps to create model-wide relevancy maps
- Core assumption: Gradients flowing through network capture importance beyond what attention weights represent
- Evidence anchors: [abstract] "The choice of the architecture enables native interpretability support for the classification task both in image and metadata domain"
- Break condition: If gradient information does not correlate with feature importance or attention mechanism is insufficient

### Mechanism 3
- Claim: Metadata feature engineering and selective inclusion improve classification performance
- Mechanism: Correlation analysis between metadata labels and diagnostic labels allows selection of most informative metadata features
- Core assumption: Not all metadata features contribute equally to classification, removing weakly correlated features improves generalization
- Evidence anchors: [section] "Using the correlation coefficient computed on the training set between all metadata labels and the diagnostic label"
- Break condition: If correlation analysis does not capture true predictive relationships or metadata distribution changes significantly in deployment

## Foundational Learning

- Concept: Transformer encoder architecture and self-attention mechanism
  - Why needed here: Transformer's ability to learn long-range dependencies and native interpretability make it suitable for multi-modal medical image classification
  - Quick check question: What is the difference between self-attention and co-attention, and why does the paper prefer self-attention?

- Concept: Multi-modal data fusion strategies
  - Why needed here: Understanding feature-level vs decision-level fusion is crucial for evaluating the proposed single-stage approach
  - Quick check question: What are the limitations of concatenation-based feature-level fusion that the paper addresses?

- Concept: Interpretability techniques in deep learning
  - Why needed here: Paper's contribution includes native interpretability through attention and TMME, requiring understanding of existing interpretability methods
  - Quick check question: How does TMME differ from traditional attention visualization methods like Grad-CAM?

## Architecture Onboarding

- Component map:
  Input: Dermoscopic image patches + patient metadata → Transformer encoder → Image embeddings → Multi-head self-attention → Fused representation → Classification head → Output

- Critical path:
  1. Image patches → Transformer encoder → Image embeddings
  2. Raw metadata → Soft one-hot encoding → Metadata embeddings
  3. Concatenate embeddings → Multi-head self-attention → Fused representation
  4. Fused representation → Classification head → Class probabilities

- Design tradeoffs:
  - Single-stage fusion vs two-stage fusion (simplicity vs potential specialization)
  - Attention-based fusion vs concatenation-based fusion (learnability vs fixed combination)
  - TMME interpretability vs simpler attention visualization (accuracy vs complexity)

- Failure signatures:
  - Poor class separation in t-SNE visualization → Attention fusion not learning meaningful cross-modal relationships
  - Noisy saliency maps → Model relying on spurious correlations or overfitting
  - Performance degradation with more metadata → Overfitting to noise in metadata

- First 3 experiments:
  1. Train vitatt on image-only data to establish baseline performance
  2. Add metadata with all features to test impact of metadata inclusion
  3. Perform correlation analysis and train with HC-5 metadata subset to test feature selection effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed ViTAtt architecture compare to other state-of-the-art transformer-based models for multi-modal fusion beyond those evaluated?
- Basis in paper: [explicit] The authors evaluate against CaiNet and FusionM4Net but do not explore other recent transformer variants
- Why unresolved: Study focuses on ViT and its modifications, leaving open question of whether other transformer architectures might yield better performance
- What evidence would resolve it: Direct comparison of ViTAtt against other transformer-based multi-modal models on same datasets with identical training protocols

### Open Question 2
- Question: What is the impact of metadata quality and completeness on model performance, and how does model handle missing or noisy metadata values?
- Basis in paper: [inferred] Authors use PAD UFES 20 with variable metadata quality but do not systematically analyze robustness to metadata noise or sparsity
- Why unresolved: Study assumes complete metadata but real-world clinical data often contains missing or unreliable entries
- What evidence would resolve it: Experiments introducing controlled amounts of metadata noise or missing values to evaluate performance degradation and model resilience

### Open Question 3
- Question: How does interpretability provided by TMME saliency maps align with actual clinical decision-making processes across diverse dermatological expertise levels?
- Basis in paper: [explicit] Authors validate interpretability through medical expert feedback but do not quantify alignment with clinical reasoning
- Why unresolved: Study provides qualitative validation but lacks systematic assessment of how well model's explanations match expert reasoning
- What evidence would resolve it: Controlled studies where dermatologists with varying experience levels use TMME maps to diagnose cases, measuring agreement between model explanations and clinical reasoning

## Limitations
- Interpretability claims rely heavily on TMME methodology without sufficient validation against ground truth clinical reasoning
- Metadata correlation analysis and selective feature inclusion methodology lacks rigorous validation against alternative feature selection approaches
- Model's behavior with varying metadata quality and completeness is not systematically evaluated

## Confidence

- **High confidence**: Architectural design and implementation of transformer-based multi-modal fusion approach are technically sound and well-documented
- **Medium confidence**: Interpretability mechanisms (attention maps and TMME) appear technically feasible but practical utility for clinical decision support remains unproven
- **Low confidence**: Metadata correlation analysis and selective feature inclusion methodology, while intuitive, lacks rigorous validation

## Next Checks
1. **Clinical validation study**: Conduct user study with dermatologists to assess whether attention and saliency visualizations align with their diagnostic reasoning and improve trust or decision-making compared to black-box models
2. **Metadata robustness testing**: Systematically evaluate model performance when metadata contains varying degrees of noise, missing values, or distribution shifts to assess real-world deployment robustness
3. **Interpretability ablation study**: Compare TMME-based visualizations against simpler attention-based methods and ground truth feature importance (where available) to quantify added value of more complex interpretability approach