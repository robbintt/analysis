---
ver: rpa2
title: A framework for conditional diffusion modelling with applications in motif
  scaffolding for protein design
arxiv_id: '2312.09236'
source_url: https://arxiv.org/abs/2312.09236
tags:
- motif
- diffusion
- noise
- conditional
- amortised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work proposes a unified theoretical framework for conditional\
  \ diffusion modeling using Doob\u2019s h-transform, enabling principled conditioning\
  \ on both hard (equality) and soft (noisy) constraints. It introduces an amortised\
  \ training protocol where the conditioning mask is included during training, contrasting\
  \ with standard methods that condition only at inference."
---

# A framework for conditional diffusion modelling with applications in motif scaffolding for protein design

## Quick Facts
- arXiv ID: 2312.09236
- Source URL: https://arxiv.org/abs/2312.09236
- Reference count: 40
- One-line primary result: Amortised training with Doob's h-transform achieves superior motif scaffolding performance while using 10× fewer parameters and 100× less compute than RFDiffusion

## Executive Summary
This work presents a unified theoretical framework for conditional diffusion modeling using Doob's h-transform, enabling principled conditioning on both hard and soft constraints. The key innovation is an amortised training protocol where the conditioning mask is included during training, contrasting with standard methods that condition only at inference. Evaluated on image outpainting and protein motif scaffolding, the amortised approach outperforms reconstruction guidance and replacement methods, achieving competitive results with RFDiffusion despite significantly smaller model size and reduced training compute.

## Method Summary
The method extends diffusion models to conditional generation by incorporating the conditioning information (motif coordinates) directly into the noise predictor network during training. Using Doob's h-transform, the framework derives a modified drift term that ensures the reverse diffusion process hits the conditioning event at time zero. The amortised training objective minimizes the conditional score matching loss between the learned noise predictor and the true conditional score function, allowing a single network to handle arbitrary conditioning masks. At inference, the model iteratively denoises while enforcing motif constraints through guidance terms or replacement steps.

## Key Results
- Amortised training outperforms reconstruction guidance and replacement sampling on both image outpainting and protein motif scaffolding tasks
- Achieves competitive performance with RFDiffusion on protein design benchmarks despite being 10× smaller and trained with 100× less compute
- Successfully integrates structural motifs into protein scaffolds with motifRMSD < 1 Å on benchmark datasets
- Demonstrates the framework's effectiveness across both image and protein domains, validating its broad applicability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Amortised training learns the conditional score function directly, enabling principled hard and soft constraint satisfaction without heuristic guidance.
- **Mechanism:** The amortised objective (Eq. 9) minimizes the MSE between the learned noise predictor and the true conditional score ∇_h ln p_t|0(h|Y=y,A=A), effectively training the network to model the h-transform drift for arbitrary conditions.
- **Core assumption:** The measurement operator A is known and fixed, allowing its inclusion as an additional conditioning input during training.
- **Evidence anchors:**
  - [abstract] "We illustrate the effectiveness of this new protocol in both, image outpainting and motif scaffolding and find that it outperforms standard methods."
  - [section] "Proposition 2.5. The minimiser of ... is given by the conditional score f*_t(h,y,A) = ∇_h ln p_t|0(h|Y=y,A=A)."
  - [corpus] Weak corpus overlap; no direct mention of amortised training mechanism.

### Mechanism 2
- **Claim:** Doob's h-transform provides a principled way to condition diffusions on equality constraints by modifying the drift term to ensure hitting the target event in finite time.
- **Mechanism:** The transformed SDE (Eq. 2) adds a drift term ∇_H_t ln P^0|t(X_0∈B|H_t) that corrects the unconditional reverse process to hit the conditioning event X_0∈B at time 0.
- **Core assumption:** The conditioning event can be expressed as A(X_0)=y for a known measurement operator A.
- **Evidence anchors:**
  - [abstract] "unifying conditional training and conditional sampling procedures under one common framework based on the mathematically well-understood Doob's h-transform."
  - [section] "Proposition 2.1. ... the conditioned process X_t|X_0∈B is a solution of ... dH_t = (b_t(H_t) - σ^2_t ∇_H_t ln P^0|t(X_0∈B|H_t)) dt + σ_t dW_t."
  - [corpus] Limited direct evidence; neighboring papers focus on multi-motif scaffolding but not the h-transform mechanism.

### Mechanism 3
- **Claim:** Reconstruction guidance approximates the h-transform by matching the conditional and unconditional transition densities, acting as a Gaussian approximation of the Doob correction term.
- **Mechanism:** It adds a guidance term proportional to the gradient of the squared error between the observed condition y and the predicted mean of p_0|t(X_0|·), effectively performing moment matching.
- **Core assumption:** The posterior p_0|t(X_0|·) can be well approximated by a Gaussian with mean and variance derived from Tweedie's formula.
- **Evidence anchors:**
  - [abstract] "Several conditional generation protocols were proposed or imported from the Computer Vision literature... lack a unifying framework."
  - [section] "d H_t = (b_t(H_t) + σ^2_t ∇_H_t ||y - A E[X_0|X_t=H_t]||^2 / Γ_t) dt + σ_t dW_t, where Γ_t acts as a guidance scale."
  - [corpus] Weak evidence; reconstruction guidance mentioned but not the moment-matching interpretation.

## Foundational Learning

- **Concept:** Stochastic differential equations and their time reversal
  - Why needed here: Diffusion models are built on SDEs; understanding their reversal is essential to grasp conditioning via h-transform.
  - Quick check question: What is the relationship between the forward and reverse SDE drift terms for an Ornstein-Uhlenbeck process?

- **Concept:** Score matching and denoising score matching
  - Why needed here: Training the noise predictor f_θ requires minimizing the score matching loss; the amortised objective is a conditional extension.
  - Quick check question: How does the score matching loss change when conditioning on an additional variable?

- **Concept:** Measurement operators and inverse problems
  - Why needed here: Conditioning on A(X_0)=y is an inverse problem; understanding forward operators is key to implementing motif scaffolding.
  - Quick check question: If A selects a subset of coordinates, what is its matrix representation and how does it act on a protein structure?

## Architecture Onboarding

- **Component map:** Data loading -> Noise predictor network (f_θ) -> Training loop (samples, noising, forward pass, loss, update) -> Inference loop (denoising with condition enforcement)

- **Critical path:**
  1. Data loading and preprocessing (motif masking, conditioning).
  2. Forward noising step: x_t = √ᾱ_t x_0 + √(1-ᾱ_t) ε_t.
  3. Network forward pass: ε_θ = f_θ(x_t, t, condition).
  4. Loss computation and backpropagation.
  5. Parameter update.
  6. At inference: iterative denoising with condition enforcement.

- **Design tradeoffs:**
  - Amortised vs. separate networks per condition: amortised is more parameter-efficient but may require more training data to generalize across conditions.
  - Guidance scale γ_t: higher values enforce condition more strictly but can reduce sample diversity.
  - Replacement steps R: more steps improve condition adherence but increase sampling time.

- **Failure signatures:**
  - If motif coordinates drift significantly from target during sampling: likely insufficient guidance or poor network conditioning.
  - If unconditional samples are poor: base network not well trained or inappropriate noise schedule.
  - If training diverges: learning rate too high or conditioning signal too strong relative to reconstruction loss.

- **First 3 experiments:**
  1. Train unconditional model on protein data; verify generation quality via RMSD to native structures.
  2. Implement amortised training with motif masking; train and evaluate conditional generation quality via motifRMSD and scaffolding success metrics.
  3. Compare amortised vs. reconstruction guidance vs. replacement sampling on a held-out motif set; report success rates and designability scores.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the amortised training approach scale to larger protein structures beyond the 128 residue limit tested in this work?
- **Basis in paper:** [inferred] The authors note their model was trained only on protein generation up to 128 residues and had to shorten sampled scaffold ranges for some targets, suggesting limitations at larger scales.
- **Why unresolved:** The paper only tested on small to medium proteins, leaving the performance on larger proteins unknown.
- **What evidence would resolve it:** Training and testing the amortised model on larger protein datasets with varying scaffold lengths would demonstrate its scalability.

### Open Question 2
- **Question:** Can the amortised training framework be extended to handle non-contiguous motifs as effectively as contiguous ones?
- **Basis in paper:** [explicit] The authors state "We perform two sets of motif scaffolding experiments..." and note that "The implementation is therefore similar to the image case, where the motif features are presented as additional input and the model learns to use these for reconstructing the motif."
- **Why unresolved:** The paper only tested on contiguous motifs, and while the framework could theoretically extend to non-contiguous ones, its performance on such motifs is unknown.
- **What evidence would resolve it:** Testing the amortised model on datasets containing non-contiguous motifs and comparing its performance to other methods would show its effectiveness for this case.

### Open Question 3
- **Question:** How does the performance of the amortised training approach compare to other state-of-the-art protein design methods beyond RFDiffusion?
- **Basis in paper:** [explicit] The authors compare their amortised training implementation of Genie to RFDiffusion and find it achieves "competitive performance on several targets" despite being smaller and trained with less compute.
- **Why unresolved:** The paper only benchmarks against one specific method (RFDiffusion), leaving the relative performance to other methods unknown.
- **What evidence would resolve it:** Benchmarking the amortised model against other leading protein design methods like AF2Complex, OmegaFold, or ESMFold on the same datasets would provide a broader comparison.

## Limitations
- The framework's theoretical guarantees rely on exact measurement operators being known, which may not hold in real-world scenarios with structural uncertainty
- The 100× compute efficiency claim assumes direct comparability of training setups that may not account for architectural differences
- The empirical validation focuses primarily on structural metrics without deeper investigation into functional properties of generated proteins

## Confidence
- **High Confidence:** The theoretical foundation using Doob's h-transform is mathematically sound and well-established in stochastic analysis literature.
- **Medium Confidence:** The empirical results showing amortised training superiority are compelling but based on limited benchmark datasets.
- **Low Confidence:** The generalization of the amortised framework to arbitrary soft constraints beyond Gaussian noise models is not thoroughly validated.

## Next Checks
1. **Robustness to Measurement Uncertainty:** Test the framework when A is noisy or only partially known by adding Gaussian noise to motif coordinates during training and evaluating degradation in scaffolding performance.

2. **Cross-Domain Generalization:** Apply the amortised training protocol to a non-protein conditional generation task (e.g., image inpainting with multiple conditions) to verify the framework's broader applicability beyond the current domains.

3. **Guidance Scale Sensitivity Analysis:** Systematically vary γ_t across orders of magnitude and measure the trade-off between constraint satisfaction and sample diversity, particularly focusing on the Pareto frontier of motifRMSD vs. designability metrics.