---
ver: rpa2
title: Can gamification reduce the burden of self-reporting in mHealth applications?
  A feasibility study using machine learning from smartwatch data to estimate cognitive
  load
arxiv_id: '2302.03616'
source_url: https://arxiv.org/abs/2302.03616
tags:
- cognitive
- load
- gami
- survey
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a feasibility study on using gamification to
  reduce cognitive load during self-reporting in mHealth apps. The authors developed
  a cognitive load detector using PPG signals from smartwatches, leveraging transfer
  learning from a stress detection task.
---

# Can gamification reduce the burden of self-reporting in mHealth applications? A feasibility study using machine learning from smartwatch data to estimate cognitive load

## Quick Facts
- arXiv ID: 2302.03616
- Source URL: https://arxiv.org/abs/2302.03616
- Reference count: 18
- Pre-training on stress detection significantly improves cognitive load detection from PPG signals, with 30 seconds as the minimum required window length

## Executive Summary
This feasibility study investigates whether gamification can reduce cognitive load during self-reporting in mHealth applications by detecting cognitive load from smartwatch PPG signals. The authors developed a cognitive load detector using transfer learning from stress detection tasks and compared cognitive load during gamified versus non-gamified surveys. While participants showed a preference for the gamified version, no significant difference in cognitive load was found between the two formats. The study demonstrates that transfer learning improves cognitive load detection performance and identifies a minimum 30-second window requirement for adequate detection.

## Method Summary
The study employed a 1D CNN model for cognitive load detection from PPG signals, with two training protocols: vanilla training on pilot data and WESAD pre-training followed by fine-tuning. The model architecture consisted of two convolutional layers with max pooling and a fully connected layer. Cognitive load was induced using the Stroop test, and participants completed both gamified and non-gamified surveys while their PPG data was collected. Leave-one-out cross-validation was used with window sizes of 10, 30, and 60 seconds. The WESAD dataset (stress detection) served as the source task for transfer learning, and a pilot dataset with 11 participants provided initial cognitive load data before the main study with 13 participants.

## Key Results
- Pre-training on stress detection improved cognitive load detection performance compared to training from scratch
- A minimum of 30 seconds of PPG signal is required for adequate cognitive load detection
- Personalized cognitive load detectors achieved F1 scores above 0.7 for 10 out of 13 participants
- No significant difference in cognitive load between gamified and non-gamified surveys, though participants preferred the gamified version
- Average time per question was 5.5 seconds for gamified surveys versus 6 seconds for non-gamified surveys

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning from stress detection improves cognitive load detection performance
- Mechanism: Pretraining on WESAD dataset allows the model to learn general features from PPG signals that can be adapted to cognitive load detection
- Core assumption: Stress and cognitive load share similar physiological patterns detectable in PPG signals
- Evidence anchors: Enhanced CL detector performance via pre-training on stress detection tasks; Model pre-training on WESAD dataset has shown to be helpful for cognitive load detection
- Break condition: If stress and cognitive load have fundamentally different physiological signatures, transfer learning would not improve performance

### Mechanism 2
- Claim: A minimum of 30 seconds of PPG signal is required for adequate cognitive load detection
- Mechanism: The temporal resolution of the physiological response to cognitive load requires sufficient signal duration to capture meaningful patterns
- Core assumption: Cognitive load manifests as a sustained physiological response that requires longer observation windows to detect accurately
- Evidence anchors: Requires capturing of a minimum 30 seconds of PPG signal to work adequately; 10 seconds window seems to be insufficient to obtain promising cognitive load detection performance even when the model is pretrained
- Break condition: If cognitive load responses are faster or more transient than assumed, shorter windows might suffice

### Mechanism 3
- Claim: Personalized cognitive load detectors can achieve F1 scores above 0.7 for most participants
- Mechanism: Individual physiological differences in response to cognitive load can be captured by training models specific to each participant
- Core assumption: Individual differences in PPG signal patterns during cognitive load are significant enough to warrant personalized models
- Evidence anchors: For 10 out of 13 participants, a personalized cognitive load detector can achieve an F1 score above 0.7; For 10 out of 13 participants, we were able to find a model which could reliably distinguish between their high and low cognitive load
- Break condition: If individual physiological responses to cognitive load are too variable or idiosyncratic, personalized models may not generalize even within individuals

## Foundational Learning

- PPG signal processing
  - Why needed here: The cognitive load detector relies on PPG signals from smartwatches, so understanding how to process and extract features from these signals is essential
  - Quick check question: What are the key challenges in processing PPG signals for machine learning applications?

- Transfer learning in deep learning
  - Why needed here: The study uses transfer learning from stress detection to cognitive load detection, so understanding how and when transfer learning is effective is crucial
  - Quick check question: What factors determine whether transfer learning will improve performance on a target task?

- Cognitive load measurement techniques
  - Why needed here: The study uses the Stroop test to induce cognitive load, so understanding different methods for inducing and measuring cognitive load is important for interpreting results
  - Quick check question: What are the advantages and limitations of using the Stroop test to measure cognitive load?

## Architecture Onboarding

- Component map:
  - PPG signal capture (smartwatch) -> Signal preprocessing and feature extraction -> 1D CNN model architecture -> Transfer learning component (WESAD dataset) -> Personalized model selection -> Gamified and non-gamified survey applications

- Critical path:
  1. Collect PPG data during Stroop test and baseline conditions
  2. Train stress detection model on WESAD dataset
  3. Fine-tune stress detection model on cognitive load data
  4. Collect survey data with PPG signals
  5. Match participants to best-performing models
  6. Analyze time spent in high cognitive load during surveys

- Design tradeoffs:
  - Signal window length vs. model complexity: Longer windows may capture more information but require more data and computational resources
  - Personalization vs. generalization: Personalized models may perform better for individuals but require more data and don't generalize to new participants
  - Gamification complexity vs. user engagement: More complex gamification might increase engagement but could also increase cognitive load and affect survey responses

- Failure signatures:
  - Poor model performance across all participants: Indicates issues with the base model architecture or transfer learning approach
  - High variance in performance between participants: Suggests individual differences are too significant for the current approach
  - No difference in cognitive load between gamified and non-gamified surveys: Could indicate the gamification elements were not effective or the cognitive load detector is not sensitive enough

- First 3 experiments:
  1. Test different signal window lengths (10, 30, 60 seconds) on a subset of participants to determine optimal duration
  2. Compare transfer learning from WESAD dataset to training from scratch on the pilot dataset to quantify the benefit of pretraining
  3. Test different CNN architectures (e.g., number of layers, filter sizes) to optimize performance for cognitive load detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal window size for cognitive load detection from PPG signals in real-world mHealth applications?
- Basis in paper: The paper tested window sizes of 10, 30, and 60 seconds and found 30 seconds to be the minimum for adequate detection performance
- Why unresolved: While 30 seconds is identified as a minimum, the paper does not explore whether longer windows (e.g., 45 or 90 seconds) could provide better performance or if there's a point of diminishing returns
- What evidence would resolve it: Systematic testing of intermediate window sizes (e.g., 15, 20, 25, 35, 45 seconds) with corresponding performance metrics to identify the optimal balance between detection accuracy and practical usability

### Open Question 2
- Question: Does pre-training on stress detection tasks improve cognitive load detection beyond what could be achieved through other transfer learning approaches?
- Basis in paper: The paper found that pre-training on the WESAD stress detection dataset improved cognitive load detection performance compared to training from scratch
- Why unresolved: The paper only tested pre-training on one stress dataset and did not compare it to other potential transfer learning approaches (e.g., pre-training on different physiological states, general signal processing tasks, or using unsupervised pre-training methods)
- What evidence would resolve it: Comparative studies testing multiple pre-training strategies including different physiological datasets, unsupervised learning approaches, and domain-specific pre-training tasks to determine the most effective transfer learning methodology

### Open Question 3
- Question: What is the relationship between model performance on the source task (stress detection) and the target task (cognitive load detection), and can this relationship be used to predict transfer learning success?
- Basis in paper: The paper found only a weak correlation between source and target task performance, contrary to their expectations
- Why unresolved: The paper did not explore why this weak correlation exists or whether other metrics (beyond simple classification accuracy) might better predict transfer learning success
- What evidence would resolve it: Detailed analysis of model representations and feature spaces across tasks, testing whether intermediate metrics (e.g., feature similarity, domain adaptation scores) better predict transfer learning outcomes than end-task performance

## Limitations
- Small sample size (11 participants for pilot, 13 for survey study) and homogeneous population (only recruited through authors' university) limit generalizability
- Lack of significant difference between gamified and non-gamified surveys could be due to insufficient gamification design rather than the ineffectiveness of gamification itself
- 3 out of 13 participants had inadequate cognitive load detection (F1 scores below 0.7), which could bias the comparison between survey types

## Confidence
- High confidence: Transfer learning from stress detection improves cognitive load detection performance
- Medium confidence: 30-second minimum window requirement for adequate detection
- Low confidence: No significant difference in cognitive load between gamified and non-gamified surveys

## Next Checks
1. Conduct a larger-scale study with diverse participants across different age groups, educational backgrounds, and health conditions to assess generalizability
2. Test more sophisticated gamification designs with multiple engagement mechanics to determine if enhanced gamification can meaningfully reduce cognitive load
3. Implement a longitudinal study tracking cognitive load across multiple survey sessions over weeks to evaluate the long-term burden of self-reporting