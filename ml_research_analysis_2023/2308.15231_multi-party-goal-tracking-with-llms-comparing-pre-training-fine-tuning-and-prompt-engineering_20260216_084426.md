---
ver: rpa2
title: 'Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning,
  and Prompt Engineering'
arxiv_id: '2308.15231'
source_url: https://arxiv.org/abs/2308.15231
tags:
- goal
- turbo
- prompt
- mask
- multi-party
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new multi-party dialogue corpus, annotated
  for both goal-tracking and intent-slot recognition. The data collection occurred
  in a hospital memory clinic, involving patients, their companions, and a social
  robot.
---

# Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering

## Quick Facts
- arXiv ID: 2308.15231
- Source URL: https://arxiv.org/abs/2308.15231
- Reference count: 25
- Key outcome: GPT-3.5-turbo achieved 62.32% correct goal-tracking and 69.57% correct intent-slot recognition with reasoning-style prompts, significantly outperforming T5 and DialogLED in few-shot settings.

## Executive Summary
This paper introduces a new multi-party dialogue corpus collected in a hospital memory clinic and evaluates three model architectures (T5, DialogLM with LED, and GPT-3.5-turbo) for multi-party goal tracking and intent-slot recognition. The authors find that GPT-3.5-turbo significantly outperforms specialized models when trained with few-shot examples, particularly when using reasoning-style prompts that explain the logic behind annotations. However, the study also reveals that certain prompt styles like "story" can increase hallucination, which poses risks in safety-critical settings like hospitals.

## Method Summary
The authors collected 29 multi-party conversations between patients, companions, and a social robot in a hospital memory clinic, annotating them for goal tracking and intent-slot recognition. They compared three model architectures: T5, DialogLM with LED, and GPT-3.5-turbo, evaluating their performance across six different prompt styles in both zero-shot and few-shot settings. The few-shot setting used 7% of the corpus as training data. Performance was measured by exact, correct, and partial match rates for both goal tracking and intent-slot recognition tasks.

## Key Results
- GPT-3.5-turbo achieved 62.32% correct goal-tracking accuracy with reasoning-style prompts in few-shot settings
- GPT-3.5-turbo outperformed specialized models (T5, DialogLED) significantly in few-shot settings across all metrics
- Story-style prompts caused increased hallucination, with GPT-3.5-turbo generating no partially correct outputs in zero-shot settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reasoning-style prompts significantly outperform other prompt styles for multi-party goal tracking with limited data.
- Mechanism: The reasoning prompt explicitly explains the logic behind goal annotations, teaching the model the underlying structure of the task. This bridges the gap between the model's pre-training objective (next token prediction) and the complex reasoning required for multi-party goal tracking.
- Core assumption: GPT-3.5-turbo's pre-training included reasoning-oriented content that makes this style effective.
- Evidence anchors:
  - [abstract] "The 'reasoning' style prompt, when given 7% of the corpus as example annotated conversations, was the best performing method. It correctly annotated 62.32% of the goal tracking MPCs."
  - [section] "In the few-shot setting, GPT-3.5-turbo's results improved significantly compared to every other approach. The 'reasoning' prompt once again outperformed the others across all metrics, generating correct outputs 62.32% of the time."
- Break condition: If the model's pre-training corpus lacks reasoning-oriented examples, the reasoning prompt may not provide additional benefit.

### Mechanism 2
- Claim: GPT-3.5-turbo's few-shot performance significantly exceeds specialized models (T5, DialogLED) when data is limited.
- Mechanism: GPT-3.5-turbo leverages its massive pre-training on diverse web data to generalize from few examples, while T5 and DialogLED require more task-specific training data to perform well.
- Core assumption: The few-shot examples provided are sufficient to convey the task structure to GPT-3.5-turbo.
- Evidence anchors:
  - [abstract] "GPT-3.5-turbo significantly outperformed the others in a few-shot setting."
  - [section] "In the few-shot setting, GPT-3.5-turbo significantly outperformed all the other models. The difference was remarkable."
- Break condition: If the few-shot examples are too sparse or unrepresentative, GPT-3.5-turbo's performance advantage may diminish.

### Mechanism 3
- Claim: Story-style prompts increase hallucination in safety-critical settings.
- Mechanism: The story prompt style triggers GPT-3.5-turbo's narrative generation capabilities, leading it to fabricate dialogue turns rather than accurately annotate existing ones.
- Core assumption: GPT-3.5-turbo's pre-training included substantial story-like data that activates narrative generation when prompted.
- Evidence anchors:
  - [abstract] "A 'story' style prompt increased model hallucination, which could be detrimental if deployed in safety-critical settings."
  - [section] "The worst zero-shot GPT-3.5-turbo prompt was the 'story' style, not even generating one partially correct output. This was due to its increased hallucination."
- Break condition: If the model is fine-tuned on instruction-following data (like InstructGPT), the story prompt's negative effect may be reduced.

## Foundational Learning

- Concept: Multi-party conversation dynamics (shared goals, answering each other's goals, providing others' goals)
  - Why needed here: These phenomena don't occur in dyadic interactions, so models need to understand them to track goals correctly in multi-party settings.
  - Quick check question: Can you identify which utterances in a multi-party conversation represent shared goals versus individual goals?

- Concept: Few-shot learning capabilities of large language models
  - Why needed here: The paper compares models in few-shot settings with limited training data (7% of corpus for GPT-3.5-turbo).
  - Quick check question: What is the maximum context window size for GPT-3.5-turbo, and how does this limit the amount of training data that can be provided?

- Concept: Prompt engineering techniques and their impact on model behavior
  - Why needed here: The paper evaluates six different prompt styles (basic, specific, annotation, story, role-play, reasoning) and their effects on performance.
  - Quick check question: How does a reasoning-style prompt differ from a basic prompt in terms of the instructions provided to the model?

## Architecture Onboarding

- Component map: GPT-3.5-turbo -> Prompt formatting -> Masked conversation -> Token prediction -> Output annotations
- Critical path: Prompt engineering -> Few-shot examples -> Masked conversation -> Annotation prediction
- Design tradeoffs: Using GPT-3.5-turbo trades fine-tuning flexibility for superior few-shot performance but requires careful prompt design and external API calls
- Failure signatures: Hallucination (story prompts), failure to recognize shared goals (reasoning prompts), incorrect intent-slot recognition
- First 3 experiments:
  1. Test different prompt styles with the same few-shot examples to isolate prompt effects
  2. Vary the number of few-shot examples within GPT-3.5-turbo's context window to find the optimal training set size
  3. Compare zero-shot performance across prompt styles to establish a baseline for improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can prompt engineering techniques be effectively applied in a modular way for N users in multi-party conversations?
- Basis in paper: Inferred from the conclusion section, where the authors mention planning to evaluate whether prompt engineering can work modularly for N users.
- Why unresolved: The paper focuses on two-party conversations and does not explore the scalability of prompt engineering techniques for larger groups of users.
- What evidence would resolve it: Conducting experiments with multi-party conversations involving more than two users and evaluating the performance of prompt engineering techniques in such scenarios.

### Open Question 2
- Question: How can the issue of model hallucination be effectively addressed in safety-critical settings like hospitals when using language models for multi-party goal tracking and intent-slot recognition?
- Basis in paper: Explicit, as the authors mention that a 'story' style prompt increased model hallucination, which could be detrimental in safety-critical settings.
- Why unresolved: While the paper identifies the problem of hallucination, it does not provide a concrete solution for mitigating this issue in safety-critical environments.
- What evidence would resolve it: Developing and testing strategies to prevent or reduce hallucination in language models when applied to multi-party conversations in hospital settings.

### Open Question 3
- Question: Can the performance of prompt engineering techniques be further improved by incorporating knowledge graphs and other contextual information in multi-party conversations?
- Basis in paper: Inferred from the conclusion section, where the authors mention their plans to utilize knowledge graphs to ensure accessibility and transparency in spoken dialogue systems for user groups with specific needs.
- Why unresolved: The paper does not explore the potential benefits of integrating knowledge graphs and contextual information with prompt engineering techniques in multi-party conversations.
- What evidence would resolve it: Conducting experiments that combine prompt engineering with knowledge graphs and evaluating the impact on the performance of multi-party goal tracking and intent-slot recognition tasks.

## Limitations

- Dataset size is limited (29 conversations), which may affect statistical significance and generalizability
- The specific medical context may not transfer to other domains
- The study focuses on a single version of GPT-3.5-turbo without exploring parameter-efficient fine-tuning approaches

## Confidence

**Major Claim Confidence:**
- **High confidence**: GPT-3.5-turbo outperforms T5 and DialogLED in few-shot settings (supported by direct quantitative comparisons across multiple metrics)
- **Medium confidence**: Reasoning-style prompts are optimal for this task (based on significant performance differences, though effects could vary with different datasets)
- **Medium confidence**: Story-style prompts increase hallucination (observed pattern, but quantification of hallucination severity is limited)

## Next Checks

1. Test the reasoning prompt style on a larger, more diverse multi-party conversation dataset to verify generalizability across domains
2. Implement parameter-efficient fine-tuning (LoRA, adapters) for T5 and DialogLED to determine if the performance gap is bridgeable with modest training
3. Conduct ablation studies varying the number of few-shot examples to identify the minimum effective training set size for each model architecture