---
ver: rpa2
title: Evaluating multiple large language models in pediatric ophthalmology
arxiv_id: '2311.04368'
source_url: https://arxiv.org/abs/2311.04368
tags:
- children
- most
- years
- following
- infants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the performance of three large language models
  (LLMs) - ChatGPT (GPT-3.5), GPT-4, and PaLM2 - in answering highly specialized pediatric
  ophthalmology questions, comparing their results with medical students, graduate
  students, and attending physicians. The researchers created a 100-question multiple-choice
  exam covering various pediatric ophthalmology topics and administered it to the
  LLMs and human participants.
---

# Evaluating multiple large language models in pediatric ophthalmology

## Quick Facts
- arXiv ID: 2311.04368
- Source URL: https://arxiv.org/abs/2311.04368
- Reference count: 40
- Primary result: GPT-4 performed comparably to attending physicians on a 100-question pediatric ophthalmology exam, while ChatGPT (GPT-3.5) and PaLM2 outperformed medical students but slightly trailed postgraduate students.

## Executive Summary
This study evaluated the performance of three large language models (ChatGPT GPT-3.5, GPT-4, and PaLM2) on a specialized pediatric ophthalmology multiple-choice exam, comparing their results with medical students, graduate students, and attending physicians. The researchers created a 100-question exam covering ten pediatric ophthalmology topics and administered it to the LLMs and human participants. GPT-4's performance was comparable to attending physicians, while ChatGPT (GPT-3.5) and PaLM2 outperformed medical students but slightly trailed postgraduate students. The study also found that GPT-4 exhibited greater stability and confidence in answering questions compared to the other two LLMs, suggesting its potential to assist in pediatric ophthalmology consultations and guide medical education.

## Method Summary
The study used a 100-question pediatric ophthalmology multiple-choice exam from board and medical school question banks, administered to three LLMs (ChatGPT GPT-3.5, GPT-4, and PaLM2) and three groups of human participants (medical students, graduate students, and attending physicians). Each LLM was prompted with five different initialization/instructional prompts and tested five times. Performance was measured using mean test scores, answer stability (Pearson correlation between LLM answers across tests), and answer confidence (frequency of correct answers). Results were compared to human baseline scores to evaluate LLM capabilities in this specialized medical domain.

## Key Results
- GPT-4's performance was comparable to attending physicians on the pediatric ophthalmology exam.
- ChatGPT (GPT-3.5) and PaLM2 outperformed medical students but slightly trailed postgraduate students.
- GPT-4 exhibited greater stability and confidence in answering questions compared to ChatGPT (GPT-3.5) and PaLM2.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's superior performance is due to its larger model capacity and more advanced training techniques (RLHF) that enable better understanding of complex, specialized medical concepts.
- Mechanism: GPT-4 leverages 175 billion parameters and reinforcement learning from human feedback to refine responses, allowing it to integrate domain-specific knowledge more effectively than GPT-3.5 and PaLM2.
- Core assumption: The performance gap between GPT-4 and the other models is primarily due to architectural differences and training methods, not the specificity of the pediatric ophthalmology dataset.
- Evidence anchors:
  - [abstract]: "GPT-4 performed comparably to attending physicians, while ChatGPT (GPT-3.5) and PaLM2 outperformed medical students but slightly trailed behind postgraduate students."
  - [section]: "GPT-4 has proven to be exceptionally outstanding in various academic and professional domains... GPT-4 showcases exceptional reasoning abilities, creativity, image understanding, contextual comprehension, and multimodal capabilities."
  - [corpus]: Weak - related papers focus on ophthalmology but do not directly compare GPT-4 vs. GPT-3.5/PaLM2 in pediatric subspecialties.
- Break condition: If the dataset contains questions that were inadvertently included in the training data of GPT-4 but not the others, the apparent performance gap could be due to memorization rather than understanding.

### Mechanism 2
- Claim: The stability and confidence differences across models reflect the quality of their internal calibration and uncertainty estimation, not just raw accuracy.
- Mechanism: GPT-4's higher average correlation (>0.8) and better answer confidence distribution indicate more consistent internal representations and better calibration during inference.
- Core assumption: Correlation and confidence metrics are valid proxies for model stability and reliability in high-stakes medical contexts.
- Evidence anchors:
  - [section]: "ChatGPT-4 exhibited a high average correlation in the tests, consistently surpassing 0.8... GPT-4 performed better in determining the possibility of obtaining a definitive answer, with 45% of the questions answered correctly and 37% answered incorrectly in each test."
  - [abstract]: "GPT-4 exhibited greater stability and confidence when responding to inquiries compared to ChatGPT (GPT-3.5) and PaLM2."
  - [corpus]: Missing - no direct corpus evidence on model calibration or uncertainty estimation in medical QA tasks.
- Break condition: If the correlation metric is computed over too few test instances or if the scoring rubric is inconsistent, the stability claim may be artifactual.

### Mechanism 3
- Claim: The domain specificity of pediatric ophthalmology questions creates a more equitable testing ground for LLMs compared to general medical knowledge, exposing differences in their ability to handle niche concepts.
- Mechanism: Specialized question sets reduce the influence of memorized general knowledge and instead require deeper reasoning and integration of domain concepts, which better differentiates model capabilities.
- Core assumption: Pediatric ophthalmology is sufficiently specialized and underrepresented in general training corpora to serve as a fair benchmark for LLM capabilities.
- Evidence anchors:
  - [section]: "Pediatric ophthalmology, with its specialized barriers and need for extensive expertise, serves as a suitable field for fair evaluation of the performance of a LLM compared to highly accessible knowledge databases."
  - [abstract]: "This study evaluated the performance of three large language models... in answering highly specialized pediatric ophthalmology questions, comparing their results with medical students, graduate students, and attending physicians."
  - [corpus]: Assumption: related ophthalmology evaluations do not specifically focus on pediatric subspecialties, so direct comparison is limited.
- Break condition: If the pediatric ophthalmology question set is too small or biased toward certain subtopics, the fairness claim may not hold.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Explains how GPT-4 improves response quality beyond raw pretraining, which is central to its outperformance.
  - Quick check question: How does RLHF differ from standard supervised fine-tuning in aligning model outputs with human preferences?
- Concept: Domain-specific pretraining and fine-tuning
  - Why needed here: Shows why models like BioBERT outperform general models on biomedical tasks, and why similar specialization matters for pediatric ophthalmology.
  - Quick check question: What are the trade-offs between general-purpose LLMs and domain-specific models in medical applications?
- Concept: Calibration and uncertainty estimation in LLMs
  - Why needed here: Critical for interpreting stability and confidence metrics, especially in high-stakes medical decision support.
  - Quick check question: How can we quantify and compare the calibration quality of different LLMs on a specialized test set?

## Architecture Onboarding

- Component map: 100 pediatric ophthalmology questions -> LLMs (ChatGPT GPT-3.5, GPT-4, PaLM2) and human groups -> Performance metrics (mean scores, correlation, confidence)
- Critical path:
  1. Design and validate pediatric ophthalmology question set
  2. Administer questions to LLMs and humans under controlled conditions
  3. Compute performance metrics (mean, correlation, confidence)
  4. Compare LLM results to human baselines
  5. Analyze stability and confidence patterns
- Design tradeoffs:
  - Question specificity vs. generalizability of results
  - Number of test runs (5) vs. computational cost and time
  - Temperature setting (1.0) vs. variability in responses
- Failure signatures:
  - Low correlation or inconsistent confidence may indicate poor calibration or instability
  - LLM scores close to random guessing suggest the model lacks domain knowledge
  - Human scores inconsistent with expected knowledge levels may indicate flawed question design
- First 3 experiments:
  1. Vary temperature settings to study impact on answer stability
  2. Test with a mixed set of general and pediatric-specific questions to measure domain sensitivity
  3. Compare performance when questions are presented in batches vs. individually to assess context effects

## Open Questions the Paper Calls Out
None

## Limitations
- Small human sample sizes (only 2 attending physicians, 5 master's students, and 8 undergraduate students) raise concerns about statistical robustness and generalizability.
- The exact content and difficulty distribution of the pediatric ophthalmology question set are not fully specified, which may limit reproducibility and external validity.
- The claim that GPT-4 is more stable and better calibrated than other models is not supported by direct comparisons in the related literature.

## Confidence
- **High Confidence**: GPT-4 outperforms ChatGPT (GPT-3.5) and PaLM2 on pediatric ophthalmology questions; GPT-4's performance is comparable to attending physicians.
- **Medium Confidence**: The claim that GPT-4 is more stable and confident than the other models; the suitability of pediatric ophthalmology as a fair benchmark for LLM evaluation.
- **Low Confidence**: The generalizability of results to broader medical contexts; the robustness of conclusions given small human sample sizes and potential data leakage.

## Next Checks
1. Replicate with larger, independent human cohorts: Expand the human baseline to include more attending physicians, residents, and medical students from multiple institutions to improve statistical power and generalizability.
2. Verify question set independence: Confirm that the pediatric ophthalmology question set was not included in the training corpora of any of the LLMs to rule out memorization effects.
3. Analyze prompt and temperature sensitivity: Systematically vary initialization prompts and temperature settings across all models to assess the impact on answer stability and confidence, and to ensure observed differences are not artifacts of experimental design.