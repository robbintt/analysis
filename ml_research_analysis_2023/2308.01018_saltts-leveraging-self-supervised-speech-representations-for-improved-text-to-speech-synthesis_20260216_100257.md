---
ver: rpa2
title: 'SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech
  Synthesis'
arxiv_id: '2308.01018'
source_url: https://arxiv.org/abs/2308.01018
tags:
- speech
- representations
- fastspeech2
- baseline
- saltts-parallel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SALTTS, a text-to-speech (TTS) model that leverages
  self-supervised learning (SSL) representations to enhance speech synthesis quality.
  SALTTS builds upon the FastSpeech2 architecture and introduces an auxiliary SSL
  predictor to reconstruct SSL embeddings.
---

# SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis

## Quick Facts
- arXiv ID: 2308.01018
- Source URL: https://arxiv.org/abs/2308.01018
- Reference count: 0
- Primary result: SALTTS-parallel with HuBERT achieves 8.2% relative MOS improvement over FastSpeech2 baseline

## Executive Summary
This paper introduces SALTTS, a text-to-speech (TTS) model that leverages self-supervised learning (SSL) representations to enhance speech synthesis quality. SALTTS builds upon the FastSpeech2 architecture and introduces an auxiliary SSL predictor to reconstruct SSL embeddings. The paper proposes two variants: SALTTS-parallel, which maintains FastSpeech2's inference speed, and SALTTS-cascade, which passes SSL representations through the decoder. Experiments on the LJSpeech dataset show that SALTTS-parallel models outperform the baseline FastSpeech2 in terms of Mean Opinion Score (MOS), with SALTTS-parallel using HuBERT achieving an 8.2% relative improvement.

## Method Summary
SALTTS extends the FastSpeech2 architecture by adding an SSL predictor module that reconstructs SSL embeddings during training. The model uses an auxiliary L1-loss on SSL reconstruction to enrich variance adapter outputs without altering the decoder input during inference (SALTTS-parallel), or passes SSL representations through the decoder with a residual connection (SALTTS-cascade). The SSL embeddings are extracted from pre-trained models (HuBERT, wav2vec2.0, data2vec-aqc) and aligned with FastSpeech2 frames using a repeater module. The model is trained on the LJSpeech dataset and evaluated using MOS, MCD, and log-F0 RMSE metrics.

## Key Results
- SALTTS-parallel with HuBERT achieves 8.2% relative MOS improvement over FastSpeech2 baseline
- SALTTS-cascade models underperform the baseline, suggesting gradient path issues
- Both SALTTS variants maintain similar MCD and log-F0 RMSE scores compared to baseline

## Why This Works (Mechanism)

### Mechanism 1
SALTTS-parallel outperforms baseline by using auxiliary SSL reconstruction loss during training while maintaining FastSpeech2's inference efficiency. During training, SALTTS-parallel passes variance adapter outputs through an SSL predictor to reconstruct SSL embeddings. The auxiliary L1-loss on SSL reconstruction enriches variance adapter outputs without altering the decoder input during inference, preserving FastSpeech2's speed. Core assumption: SSL embeddings contain richer speech characteristics than pitch/energy features alone, and learning to reconstruct them improves variance adapter representations.

### Mechanism 2
SALTTS-cascade underperforms baseline because gradients traverse a longer path through SSL predictor before reaching variance adapter. In SALTTS-cascade, decoder receives 768-dim SSL predictor outputs instead of 384-dim variance adapter outputs. Gradients from mel-spectrogram loss must flow through SSL predictor → decoder → residual connection → variance adapter, potentially degrading gradient quality and slowing convergence. Core assumption: Longer gradient paths reduce training stability and representation quality compared to direct variance adapter usage.

### Mechanism 3
Repeater module is essential to align SSL embeddings' temporal resolution with FastSpeech2's, enabling meaningful auxiliary loss computation. SSL models (e.g., HuBERT) use 16kHz sampling with 20ms hop length; FastSpeech2 uses 22.05kHz with 11.6ms hop length. The repeater repeats or drops SSL frames to match FastSpeech2's frame count per time unit, ensuring frame-wise alignment for L1-loss. Core assumption: Temporal misalignment between SSL and FastSpeech2 embeddings would invalidate auxiliary loss, preventing SSL enrichment.

## Foundational Learning

- Concept: Self-supervised learning (SSL) speech representations
  - Why needed here: SALTTS leverages SSL embeddings as richer speech descriptors beyond pitch/energy, requiring understanding of SSL models (HuBERT, wav2vec2.0) and their embedding properties.
  - Quick check question: What are the typical dimensions and temporal resolutions of SSL speech embeddings, and how do they differ from FastSpeech2's variance adapter outputs?

- Concept: Multi-task learning with auxiliary losses
  - Why needed here: SALTTS adds an auxiliary L1-loss on SSL reconstruction to the primary mel-spectrogram loss; understanding gradient weighting and loss balance is critical.
  - Quick check question: How does the auxiliary SSL loss coefficient affect convergence and final MOS scores in SALTTS-parallel vs. SALTTS-cascade?

- Concept: Temporal alignment and frame-rate mismatch handling
  - Why needed here: Repeater module requires understanding of how to resample/align sequences with different sampling rates and hop lengths for frame-wise loss computation.
  - Quick check question: Given SSL hop length 20ms and FastSpeech2 hop length 11.6ms, how many SSL frames correspond to 31 FastSpeech2 frames, and what repetition pattern achieves alignment?

## Architecture Onboarding

- Component map: Text → FastSpeech2 encoder → variance adapter → multi-layer projector (384→768) → SSL predictor (4-layer) → (parallel: variance adapter → decoder; cascade: SSL predictor → decoder with residual) → HiFi-GAN vocoder

- Critical path:
  - For SALTTS-parallel: Text → FastSpeech2 encoder → variance adapter → projector → SSL predictor (auxiliary loss) → variance adapter → decoder → HiFi-GAN → audio
  - For SALTTS-cascade: Text → FastSpeech2 encoder → variance adapter → projector → SSL predictor → decoder (with residual) → HiFi-GAN → audio

- Design tradeoffs:
  - SALTTS-parallel: Maintains inference speed, but SSL enrichment only during training; SALTTS-cascade: Potentially richer decoder input, but slower inference and gradient path issues
  - SSL predictor depth and projector layers affect reconstruction quality vs. parameter overhead
  - Repeater design (repetition pattern, Gaussian noise) impacts alignment quality and auxiliary loss validity

- Failure signatures:
  - SALTTS-parallel: MOS close to baseline, MCD/ F0 RMSE similar; indicates auxiliary loss ineffective or gradient conflict
  - SALTTS-cascade: Worse MOS than baseline; suggests gradient degradation or decoder mismatch with 768-dim input
  - High auxiliary loss weight: Training instability, poor primary loss convergence
  - Poor repeater alignment: Auxiliary loss meaningless, SSL predictor fails to learn

- First 3 experiments:
  1. Train SALTTS-parallel with HuBERT, monitor auxiliary loss vs. primary loss trends, validate alignment by checking frame correspondence visually
  2. Compare SALTTS-parallel vs. SALTTS-cascade with identical SSL model and projector depth; measure inference speed and gradient norms
  3. Ablate repeater: train with mismatched frame rates (no alignment), observe auxiliary loss behavior and final MOS degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do SALTTS-parallel and SALTTS-cascade models perform on languages other than English, especially those with tonal characteristics?
- Basis in paper: [inferred] The paper evaluates models only on the English LJSpeech dataset and does not explore performance on other languages or tonal languages.
- Why unresolved: The paper does not provide any cross-lingual experiments or analysis of SALTTS performance on non-English or tonal languages, which could significantly impact model effectiveness.
- What evidence would resolve it: Comparative MOS and MCD scores of SALTTS models across multiple languages, particularly tonal languages like Mandarin Chinese, would demonstrate cross-lingual robustness.

### Open Question 2
- Question: What is the computational overhead of the SALTTS-parallel and SALTTS-cascade models compared to the baseline FastSpeech2 during training?
- Basis in paper: [explicit] The paper mentions that SALTTS-parallel and SALTTS-cascade models take 4.5 days to train for 1000 epochs on 4 GPUs, compared to 3 days for the baseline FastSpeech2.
- Why unresolved: While the paper provides training time comparisons, it does not detail the computational resources, memory usage, or inference speed differences between the models.
- What evidence would resolve it: Detailed profiling of GPU memory usage, FLOPs, and inference latency for each model variant would quantify the computational trade-offs.

### Open Question 3
- Question: How does the choice of SSL model layers affect the performance of SALTTS models?
- Basis in paper: [explicit] The paper uses averaged embeddings from layers 9, 10, and 11 of SSL models but does not explore the impact of using different layers or layer combinations.
- Why unresolved: The paper does not conduct ablation studies to determine the optimal SSL layer selection for SALTTS performance.
- What evidence would resolve it: MOS and MCD scores comparing SALTTS models using different SSL layer combinations (e.g., layers 7-9, 10-12, or individual layers) would identify the most effective layer selection strategy.

## Limitations
- Experimental scope limited to single English dataset (LJSpeech) without cross-lingual validation
- SALTTS-cascade variant underperforms baseline without clear explanation of architectural failure
- SSL predictor architectural decisions appear arbitrary without sensitivity analysis or justification

## Confidence
- **High Confidence**: SALTTS-parallel's auxiliary SSL reconstruction loss approach is well-defined and theoretically sound; temporal alignment problem is correctly identified and addressed; 8.2% MOS improvement with HuBERT is empirically demonstrated
- **Medium Confidence**: Hypothesis about SALTTS-cascade gradient degradation is plausible but not empirically validated; architectural explanation for cascade failure is reasonable but lacks gradient flow analysis
- **Low Confidence**: Claims about SSL representations being "richer" than pitch/energy features lack direct empirical support; effectiveness across different SSL models not thoroughly analyzed; scalability to larger, more diverse datasets remains untested

## Next Checks
1. **Gradient Flow Analysis**: Instrument SALTTS-cascade training to measure gradient norms and paths from mel-spectrogram loss to variance adapter layers. Compare with SALTTS-parallel to empirically validate the hypothesis about gradient degradation in cascade architecture.

2. **Repeater Ablation Study**: Train SALTTS-parallel variants with and without the repeater module on aligned and misaligned frame rates. Measure the impact on auxiliary loss convergence and final MOS to quantify the repeater's contribution to performance improvements.

3. **Cross-Dataset Generalization**: Evaluate SALTTS-parallel on at least two additional TTS datasets (e.g., VCTK for multi-speaker, LibriTTS for larger scale). Compare SSL model performance (HuBERT vs. wav2vec2.0) across datasets to assess the robustness and generalizability of SSL representation benefits.