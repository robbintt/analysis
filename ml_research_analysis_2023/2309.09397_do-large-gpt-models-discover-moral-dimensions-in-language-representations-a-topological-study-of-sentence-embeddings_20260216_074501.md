---
ver: rpa2
title: Do Large GPT Models Discover Moral Dimensions in Language Representations?
  A Topological Study Of Sentence Embeddings
arxiv_id: '2309.09397'
source_url: https://arxiv.org/abs/2309.09397
tags:
- language
- fairness
- moral
- representations
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to visualize the moral dimensions
  within language representations induced by large language models like GPT-3.5. The
  authors compute a fairness metric based on social psychology literature and use
  it to analyze the topological structure of sentence embeddings in GPT-3.5's high-dimensional
  vector space.
---

# Do Large GPT Models Discover Moral Dimensions in Language Representations? A Topological Study Of Sentence Embeddings

## Quick Facts
- arXiv ID: 2309.09397
- Source URL: https://arxiv.org/abs/2309.09397
- Reference count: 40
- One-line primary result: This paper proposes a novel approach to visualize the moral dimensions within language representations induced by large language models like GPT-3.5.

## Executive Summary
This paper proposes a novel approach to visualize the moral dimensions within language representations induced by large language models like GPT-3.5. The authors compute a fairness metric based on social psychology literature and use it to analyze the topological structure of sentence embeddings in GPT-3.5's high-dimensional vector space. By clustering sentences based on their projection onto the fairness dimension and visualizing the resulting simplicial complex, the authors show that the representation space is clearly separated into two submanifolds corresponding to fair and unfair moral judgments. This indicates that GPT-3.5 develops an understanding of fairness during training and encodes it in the shape of its sentence representation manifold.

## Method Summary
The authors propose a method to visualize moral dimensions in GPT-3.5's sentence embeddings by computing a fairness metric based on social psychology literature. They first define a fairness dimension as a linear combination of vectors representing responsibility, pleasure, benefit, reward, and harm factors. They then project sentence embeddings onto this fairness dimension and cluster them based on the projection values. Finally, they construct a simplicial complex from the clusters and visualize it, revealing a clear separation between fair and unfair moral judgments.

## Key Results
- GPT-3.5's sentence embeddings naturally separate into fair/unfair submanifolds when projected onto a learned fairness dimension.
- Topological visualization via simplicial complexes provides a human-readable summary of high-dimensional embedding structure.
- The fairness metric derived from social psychology literature captures human moral judgments in embedding space.

## Why This Works (Mechanism)
### Mechanism 1
- Claim: GPT-3.5's sentence embeddings naturally separate into fair/unfair submanifolds when projected onto a learned fairness dimension.
- Mechanism: The model's pretraining on massive text corpora causes its embedding space to encode moral associations. By defining a fairness dimension as a linear combination of responsibility, pleasure, benefit, reward, and harm vectors, the projection reveals clusters corresponding to moral judgments.
- Core assumption: The language model's embeddings capture implicit social biases present in the training data, and these biases align with human moral judgments.
- Evidence anchors:
  - [abstract] "Our results show that sentence embeddings based on GPT-3.5 can be decomposed into two submanifolds corresponding to fair and unfair moral judgments."
  - [section] "The two boxes contain fair and unfair sentences. We see that each box bounds a topologically complex manifold... but there are no edges connecting nodes from different boxes."
  - [corpus] Weak - related papers discuss fairness metrics but do not directly confirm the topological separation claim.
- Break condition: If the training corpus lacks sufficient moral content, or if the fairness metric does not align with human judgment, the separation would not appear.

### Mechanism 2
- Claim: Topological visualization via simplicial complexes provides a human-readable summary of high-dimensional embedding structure.
- Mechanism: Instead of directly visualizing thousands of dimensions, the method clusters sentence embeddings based on their projection onto the fairness dimension, then builds a graph (1D simplicial complex) where vertices are clusters and edges are intersections. This preserves topological features while reducing dimensionality.
- Core assumption: The nerve complex of the cluster cover approximates the topology of the original embedding manifold.
- Evidence anchors:
  - [section] "This procedure produces a sketch of the original high-dimensional manifold... with a topological object that can be visualized in human readable form."
  - [section] "Figure 4 shows a visualization of this process for a point cloud sampled from the circle (S2)."
  - [corpus] Weak - related work discusses fairness metrics but not topological methods for visualization.
- Break condition: If the clustering or cover selection is too coarse, important topological features could be lost, leading to misleading visualizations.

### Mechanism 3
- Claim: The fairness metric derived from social psychology literature captures human moral judgments in embedding space.
- Mechanism: The metric combines five factors (responsibility, pleasure, benefit, reward, harm) as linear combinations of opposite-polarity sentence pairs. The resulting vector defines a fairness dimension, and the inner product with sentence embeddings correlates with human fairness ratings.
- Core assumption: Human fairness assessments can be approximated by a linear combination of these five factors, and the language model has learned these associations during pretraining.
- Evidence anchors:
  - [section] "The authors of [15] propose to harness implicit social biases in language as a metric for an explainable assessment of sentences related to fairness."
  - [section] "In our computation, every factor is derived from pairs of sentences representing two polarities of each factor."
  - [corpus] Weak - related papers discuss fairness metrics but not the specific combination of factors used here.
- Break condition: If the factor combination does not align with human judgment, or if the model did not learn these associations, the metric would fail to capture fairness.

## Foundational Learning
- Concept: Vector space representations and cosine similarity
  - Why needed here: The paper relies on sentence embeddings in high-dimensional space and measures similarity via cosine similarity to define the fairness dimension.
  - Quick check question: If two sentence vectors have a cosine similarity of 0.8, are they considered more or less similar than vectors with a similarity of 0.3?
- Concept: Topological data analysis and simplicial complexes
  - Why needed here: The paper uses simplicial complexes to visualize the structure of high-dimensional embedding manifolds, which requires understanding concepts like nerve complexes and clustering.
  - Quick check question: In a simplicial complex, what do the vertices represent, and what do the edges connecting them signify?
- Concept: Linear combinations and basis vectors in embedding space
  - Why needed here: The fairness dimension is defined as a linear combination of basis vectors representing different moral factors, so understanding how to construct and use these combinations is crucial.
  - Quick check question: If you have basis vectors for "benefit" and "harm," how would you construct a vector that represents the concept of "fairness" as a combination of these?

## Architecture Onboarding
- Component map: GPT-3.5 -> Generate sentence embeddings -> Compute fairness metric -> Cluster embeddings based on fairness projection -> Build simplicial complex -> Visualize with heat map coloring
- Critical path: Generate sentence embeddings → Compute fairness metric → Cluster embeddings based on fairness projection → Build simplicial complex → Visualize with heat map coloring.
- Design tradeoffs: Using a linear fairness metric is simple but may not capture all nuances of moral judgment. Topological visualization preserves structure but may be harder to interpret than direct projection. The choice of clustering and cover granularity affects the balance between detail and readability.
- Failure signatures: If the simplicial complex shows no clear separation, the fairness metric may not align with human judgment. If the visualization is too complex or too simple, the clustering or cover selection may need adjustment. If the results are inconsistent across different runs, the randomness in clustering may need to be controlled.
- First 3 experiments:
  1. Generate embeddings for a small set of known fair/unfair sentences and manually verify the fairness metric values and simplicial complex separation.
  2. Vary the clustering granularity and observe how the simplicial complex structure changes to find the optimal balance between detail and readability.
  3. Test the robustness of the results by generating embeddings with different random seeds and checking for consistency in the simplicial complex structure.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the proposed topological method be generalized to analyze other moral dimensions beyond fairness, such as justice, responsibility, or harm?
- Basis in paper: [explicit] The authors suggest that their approach could be used to study other moral aspects of language representations.
- Why unresolved: The paper only demonstrates the method's effectiveness for fairness. Further research is needed to validate its applicability to other moral dimensions.
- What evidence would resolve it: Applying the method to datasets labeled for different moral dimensions and comparing the resulting topological structures to those obtained for fairness.

### Open Question 2
- Question: How do the topological structures of sentence embeddings change as language models are fine-tuned on specific tasks or domains?
- Basis in paper: [inferred] The paper analyzes the static topological structure of GPT-3.5's sentence embeddings. It does not explore how these structures evolve during fine-tuning.
- Why unresolved: Fine-tuning can significantly alter a model's representations. Investigating how topological structures change could provide insights into the fine-tuning process and its effects on model behavior.
- What evidence would resolve it: Conducting experiments that compare the topological structures of sentence embeddings before and after fine-tuning on various tasks or domains.

### Open Question 3
- Question: Can the proposed method be used to detect and analyze biases in language models beyond moral dimensions?
- Basis in paper: [explicit] The authors suggest that their approach could be useful for AI alignment research, which often involves detecting and mitigating biases.
- Why unresolved: While the paper focuses on moral dimensions, the method could potentially be adapted to study other types of biases, such as demographic or ideological biases.
- What evidence would resolve it: Applying the method to datasets designed to reveal various biases and analyzing the resulting topological structures for patterns indicative of bias.

## Limitations
- The topological separation of fair/unfair submanifolds lacks quantitative validation - no statistical measures confirm the separation is meaningful rather than an artifact of clustering parameters.
- The fairness metric derivation from five psychological factors is not validated against human judgments or alternative moral frameworks.
- The reliance on GPT-3.5's specific embedding space means results may not generalize to other models or even different versions of the same model.

## Confidence
- Topological separation of fair/unfair submanifolds: Medium - visually supported but not statistically validated
- Fairness metric captures moral dimensions: Low - theoretical derivation without empirical validation
- Simplicial complex visualization preserves topological structure: Medium - methodologically sound but parameter sensitivity untested

## Next Checks
1. **Statistical validation**: Apply quantitative measures (e.g., silhouette scores, cluster validity indices) to confirm the separation between fair and unfair submanifolds is statistically significant rather than clustering artifacts.
2. **Cross-model generalization**: Test whether similar topological separation appears in sentence embeddings from other language models (e.g., LLaMA, Claude) to verify the phenomenon isn't GPT-3.5-specific.
3. **Human judgment correlation**: Conduct a user study where human raters assess sentence fairness and compare their judgments to the model's fairness metric values to validate the metric's alignment with human moral intuitions.