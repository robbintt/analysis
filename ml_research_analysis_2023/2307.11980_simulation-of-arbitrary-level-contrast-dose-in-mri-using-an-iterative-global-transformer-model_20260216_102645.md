---
ver: rpa2
title: Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global
  Transformer Model
arxiv_id: '2307.11980'
source_url: https://arxiv.org/abs/2307.11980
tags:
- dose
- contrast
- low-dose
- gformer
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a transformer-based iterative learning model
  for simulating MRI images with arbitrary contrast enhancement levels. The approach
  uses a Global transformer (Gformer) with subsampling attention and rotational shift
  modules to capture long-range contrast uptake features.
---

# Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model

## Quick Facts
- **arXiv ID**: 2307.11980
- **Source URL**: https://arxiv.org/abs/2307.11980
- **Reference count**: 29
- **Key outcome**: Transformer-based iterative model achieves PSNR 42.29 dB, SSIM 0.98 for simulating MRI contrast dose levels

## Executive Summary
This paper proposes a transformer-based iterative learning model for simulating MRI images with arbitrary contrast enhancement levels. The approach uses a Global transformer (Gformer) with subsampling attention and rotational shift modules to capture long-range contrast uptake features. Trained on a highly imbalanced dataset of pre-contrast, 10% low-dose, and standard dose images, the model generates images at different dose levels by iteratively reducing contrast enhancement. Quantitative results show superior performance over state-of-the-art methods with an inference throughput of 0.65 Im/s, and the synthesized low-dose images demonstrate clinical utility in downstream tumor segmentation tasks.

## Method Summary
The method employs an iterative transformer model that takes post-contrast and pre-contrast images as input and applies a series of transformer blocks to gradually reduce the enhancement level. The Gformer block incorporates subsampling attention and rotational shift modules to capture global contextual information and shape irregularities in contrast uptake regions. The model is trained on paired T1w pre-contrast, 10% low-dose, and standard dose MRI images using L1, SSIM, adversarial, and perceptual losses. Six Gformer blocks with subsampling strides {4,8,16,16,8,4} and rotational shift angles {0,10,20,20,10,0} are used in the iterative process.

## Key Results
- Achieved PSNR of 42.29 dB, SSIM of 0.98, and RMSE of 0.13 on simulated low-dose images
- Outperformed state-of-the-art methods in image quality metrics and contrast uptake characteristics
- Demonstrated clinical utility with tumor segmentation Dice scores comparable to real low-dose images

## Why This Works (Mechanism)

### Mechanism 1
Iterative transformer learning enables dose-level interpolation by progressively removing contrast enhancement in a controlled manner. The model takes post-contrast and pre-contrast images as input and applies a series of transformer blocks to gradually reduce the enhancement level. At each iteration, the output image from the previous step and the pre-contrast image are fed into the model to predict the next lower dose image, effectively learning a dose reduction trajectory.

### Mechanism 2
Subsampling attention with global context captures long-range dependencies in contrast uptake patterns. The Gformer block uses a subsampling process that aggregates strided positions to create sub-feature maps. These sub-feature maps serve as attention windows, allowing the transformer to capture global contextual information about contrast uptake while maintaining computational efficiency.

### Mechanism 3
Rotational shift module captures shape irregularity of contrast uptake regions. The rotational shift operation rotates feature maps around the vertical axis of height/width by small angles (e.g., 10°, 20°). This allows the model to capture diverse contextual information and enhance the representation power of the Gformer by considering different orientations of contrast uptake regions.

## Foundational Learning

- **Vision Transformers and attention mechanisms**: Why needed here - The model relies on transformer blocks to capture global contextual information about contrast uptake, which is essential for simulating different dose levels. Quick check question - How does self-attention in transformers differ from convolutional operations in terms of capturing spatial relationships?

- **Iterative learning and model convergence**: Why needed here - The model uses an iterative approach to gradually reduce contrast enhancement, requiring careful consideration of loss functions and convergence strategies. Quick check question - What are the potential issues with using soft labels in iterative training, and how can they be mitigated?

- **Subsampling and feature map operations**: Why needed here - The subsampling process is crucial for creating attention windows that capture global context while maintaining computational efficiency. Quick check question - How does the choice of subsampling stride affect the balance between capturing global context and preserving local details?

## Architecture Onboarding

- **Component map**: Input: Post-contrast and pre-contrast images → Iterative loop: k iterations of Gformer blocks → Gformer block: Convolution layer, rotational shift, subsampling, transformer module → Output: Synthesized low-dose image at each iteration

- **Critical path**: Post-contrast image → Gformer block → Intermediate output → Gformer block (next iteration) → Final low-dose image

- **Design tradeoffs**: Number of iterations (k) vs. computational cost and model complexity; Subsampling stride vs. global context capture and local detail preservation; Rotation angles vs. shape irregularity capture and edge information loss

- **Failure signatures**: Blurry artifacts in synthesized images (insufficient local detail capture); Inconsistent contrast enhancement patterns across iterations (poor iterative learning); Loss of high-frequency texture information (inadequate perceptual loss weighting)

- **First 3 experiments**: 1) Test the effect of different subsampling strides on the quality of synthesized low-dose images and computational efficiency; 2) Evaluate the impact of varying rotation angles in the rotational shift module on the model's ability to capture shape irregularities in contrast uptake regions; 3) Compare the performance of the iterative model with different numbers of iterations (k) to find the optimal balance between image quality and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the proposed model generalize to different types of contrast agents and pathologies?
- Basis in paper: [explicit] The paper states that "different types of GBCAs and pathologies require different dose levels for the DL algorithms to work reliably."
- Why unresolved: The paper only evaluated the model on Gadoterate meglumine and Gadobenate dimeglumine contrast agents and did not assess its performance on other types of GBCAs or pathologies.
- What evidence would resolve it: Further experiments evaluating the model on a diverse set of GBCAs and pathologies would provide evidence for its generalizability.

### Open Question 2
- Question: Can the proposed model be extended to other anatomies beyond the brain?
- Basis in paper: [explicit] The paper mentions that "This simulation technique can easily be extended to other anatomies and contrast agents."
- Why unresolved: The paper only evaluated the model on brain MRI images and did not demonstrate its effectiveness on other anatomies.
- What evidence would resolve it: Experiments evaluating the model on MRI images of other anatomies would provide evidence for its potential extension.

### Open Question 3
- Question: How does the proposed model compare to physics-based models for MRI dose simulation?
- Basis in paper: [explicit] The paper mentions that "Currently MRI dose simulation is done using physics-based models[8]."
- Why unresolved: The paper does not provide a direct comparison between the proposed model and physics-based models for MRI dose simulation.
- What evidence would resolve it: A comparison between the proposed model and physics-based models in terms of accuracy, efficiency, and clinical utility would provide evidence for its superiority or limitations.

## Limitations

- The model's performance on datasets with different contrast agents and pathologies beyond the two sites studied remains untested
- The subsampling attention mechanism's effectiveness in capturing global context while preserving local contrast details requires further validation
- The iterative learning approach assumes a linear dose reduction trajectory that may not hold for all tissue types or contrast agents

## Confidence

- **High Confidence**: Quantitative metrics (PSNR, SSIM, RMSE) and clinical utility validation via tumor segmentation downstream task
- **Medium Confidence**: Architecture design choices (subsampling attention, rotational shift) based on theoretical justification
- **Low Confidence**: Generalization across different contrast agents and patient populations beyond the two sites studied

## Next Checks

1. **Dataset Generalization**: Test model performance on additional sites using different contrast agents (e.g., Gadopentetate dimeglumine, Gadoteridol) to assess robustness across varying pharmacokinetic profiles
2. **Mechanism Ablation**: Conduct controlled experiments isolating subsampling attention and rotational shift modules to quantify their individual contributions to final image quality
3. **Clinical Workflow Integration**: Evaluate model performance in a realistic clinical setting with radiologists assessing synthetic low-dose images for diagnostic accuracy and confidence compared to real low-dose acquisitions