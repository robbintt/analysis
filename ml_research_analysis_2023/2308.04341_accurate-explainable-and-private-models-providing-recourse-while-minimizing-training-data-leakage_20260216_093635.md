---
ver: rpa2
title: 'Accurate, Explainable, and Private Models: Providing Recourse While Minimizing
  Training Data Leakage'
arxiv_id: '2308.04341'
source_url: https://arxiv.org/abs/2308.04341
tags:
- recourse
- training
- data
- privacy
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces the first privacy-preserving recourse methods
  to mitigate membership inference attacks on algorithmic recourse. Two novel approaches
  are proposed: Differentially Private Model (DPM) and Laplace Recourse (LR).'
---

# Accurate, Explainable, and Private Models: Providing Recourse While Minimizing Training Data Leakage

## Quick Facts
- arXiv ID: 2308.04341
- Source URL: https://arxiv.org/abs/2308.04341
- Reference count: 6
- Key outcome: Novel DP methods (DPM and LR) significantly reduce membership inference attack efficacy on counterfactual explanations while maintaining model and recourse accuracy for sufficiently large training datasets

## Executive Summary
This paper addresses the privacy risks associated with algorithmic recourse through counterfactual explanations, which can inadvertently leak membership information in training data. The authors introduce two novel differentially private methods - Differentially Private Model (DPM) and Laplace Recourse (LR) - to generate private counterfactual explanations while preserving their utility. Through experiments on real-world and synthetic datasets, both methods demonstrate strong privacy protection by flattening ROC curves at low false positive rates, with LR showing particular success when training dataset size is large enough.

## Method Summary
The paper proposes two privacy-preserving approaches for counterfactual explanations. DPM trains logistic regression with differential privacy using diffprivlib, leveraging the post-processing property of DP to ensure private counterfactuals. LR adds Laplace noise to predicted probability scores before computing counterfactual distances, applying the Laplace mechanism to the probability query. Both methods are evaluated through membership inference attacks using counterfactual distance (CFD) and CFD likelihood ratio tests (LRT), with performance measured via ROC curves, AUC, and balanced accuracy.

## Key Results
- DPM and LR methods significantly reduce membership inference attack efficacy, particularly at low false positive rates
- LR method achieves strong privacy protection while maintaining model and recourse accuracy when training dataset size is sufficiently large
- Both methods flatten ROC curves from baseline, indicating reduced adversary capability to distinguish training from test data
- Privacy-accuracy tradeoff exists, with higher ε values providing better utility but weaker privacy guarantees

## Why This Works (Mechanism)

### Mechanism 1: Laplace Noise Addition
Laplace Recourse adds Laplace noise to predicted probability scores before computing counterfactual distances, obscuring membership information while preserving recourse accuracy when dataset size is large. The method applies the Laplace mechanism to the predicted probability Pr(y=1|x), clamps to [0,1], computes the noisy logistic regression score, and calculates counterfactual distance using this perturbed score. The global sensitivity of the predicted probability query is 1, ensuring bounded noise addition.

### Mechanism 2: DP Model Training
Differentially Private Model trains logistic regression with differential privacy, propagating privacy guarantees to counterfactual explanations through post-processing. The logistic regression model is trained using DP via diffprivlib, and the resulting model inherently produces private counterfactual explanations due to the post-processing property of differential privacy.

### Mechanism 3: Membership Inference Attack Mitigation
Both DPM and LR methods reduce the efficacy of counterfactual distance-based membership inference attacks by adding noise that disrupts the adversary's ability to distinguish training from test data. The controlled randomness through DP mechanisms makes counterfactual distances for training and test instances statistically similar, preventing adversaries from using distance thresholds or likelihood ratio tests to infer membership.

## Foundational Learning

- **Concept: Differential Privacy (DP)**
  - Why needed here: DP provides the mathematical framework for quantifying and limiting privacy loss, essential for protecting against membership inference attacks while maintaining utility
  - Quick check question: What is the relationship between the privacy parameter ε and the strength of privacy protection in differential privacy?

- **Concept: Counterfactual Explanations**
  - Why needed here: Counterfactual explanations are the basis for algorithmic recourse, and understanding how they can leak membership information is crucial for developing privacy-preserving methods
  - Quick check question: How does the counterfactual distance relate to the decision boundary in logistic regression, and why might this distance reveal membership information?

- **Concept: Membership Inference Attacks**
  - Why needed here: Understanding how adversaries can use counterfactual explanations to infer training data membership is the motivation for developing privacy-preserving recourse methods
  - Quick check question: What are the key differences between loss-based and distance-based membership inference attacks, and why might counterfactual distances be particularly informative?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Model training (DP/non-DP logistic regression) -> Recourse generation (counterfactual distance calculation with/without noise) -> Attack simulation (CFD/CFD LRT) -> Evaluation (ROC curves, AUC, BA, accuracy)

- **Critical path**: 
  1. Preprocess dataset (remove multicollinear features, standardize, normalize)
  2. Train logistic regression model (with or without DP)
  3. Generate counterfactual explanations for training and test instances
  4. Apply DP mechanism (DPM or LR) if privacy protection is desired
  5. Simulate membership inference attacks using CFD and CFD LRT methods
  6. Evaluate attack success and model/recourse accuracy

- **Design tradeoffs**: Privacy vs. accuracy (stronger privacy reduces model and recourse accuracy), computational cost vs. privacy strength (DP training is more expensive), noise type vs. interpretability (Laplace noise may be more interpretable)

- **Failure signatures**: ROC curves that don't flatten at low FPR indicate insufficient privacy protection, large discrepancies between train and test counterfactual distance distributions suggest privacy leakage, significant drops in model accuracy may indicate overly aggressive noise addition

- **First 3 experiments**:
  1. Compare baseline (no privacy) vs. DPM with ε=1.0 on Heloc dataset, measuring attack success and model accuracy
  2. Compare DPM vs. LR with ε=0.5 on MNIST dataset, focusing on recourse accuracy and attack resistance
  3. Test the privacy-accuracy tradeoff by varying ε from 0.1 to 10 on synthetic data with d=100, examining impact on both attack success and counterfactual distance distributions

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the accuracy of the Laplace Recourse (LR) method change with different values of epsilon (ε) beyond those tested in the paper? The paper only tests ε values up to 20, and the impact of even higher ε values on accuracy is unknown.

- **Open Question 2**: Can the privacy-accuracy tradeoff observed in the LR method be mitigated or eliminated through alternative approaches to clamping the noisy predicted probability scores? The paper attributes accuracy loss to clamping of noisy predicted probability scores that fall below 0.

- **Open Question 3**: How does the effectiveness of the DPM and LR methods vary across different types of machine learning models, such as neural networks or decision trees? The paper focuses on logistic regression classifiers and suggests further research is needed for other model types.

## Limitations

- Limited dataset diversity with focus on relatively small tabular datasets and MNIST
- Missing implementation details for hyperparameter optimization in DP logistic regression
- Unclear sensitivity to specific attack threshold choices in membership inference evaluation

## Confidence

- **High Confidence**: Theoretical framework connecting differential privacy to membership inference protection is sound
- **Medium Confidence**: Experimental results demonstrate effectiveness but limited dataset scale may not generalize
- **Low Confidence**: Some implementation details missing, particularly regarding hyperparameter optimization and exact attack thresholds

## Next Checks

1. Perform sensitivity analysis across a wider range of ε values (e.g., ε ∈ {0.1, 0.5, 1.0, 2.0, 5.0}) to better understand the privacy-accuracy tradeoff and identify optimal operating points

2. Evaluate the proposed methods against additional attack strategies beyond CFD and CFD LRT, such as gradient-based attacks or black-box attacks, to ensure comprehensive privacy protection

3. Apply the methods to larger, more complex datasets (e.g., ImageNet, larger tabular datasets) to assess scalability and performance in production-like environments