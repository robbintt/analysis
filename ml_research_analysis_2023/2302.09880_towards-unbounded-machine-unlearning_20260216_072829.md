---
ver: rpa2
title: Towards Unbounded Machine Unlearning
arxiv_id: '2302.09880'
source_url: https://arxiv.org/abs/2302.09880
tags:
- error
- unlearning
- mean
- scrub
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SCRUB, a new machine unlearning algorithm that
  removes the influence of a subset of the training data from a trained deep neural
  network. The key idea is to treat the original model as a teacher and train a student
  model that selectively obeys the teacher on retained data while disobeying on data
  to be forgotten.
---

# Towards Unbounded Machine Unlearning

## Quick Facts
- arXiv ID: 2302.09880
- Source URL: https://arxiv.org/abs/2302.09880
- Reference count: 14
- This paper proposes SCRUB, a new machine unlearning algorithm that removes the influence of a subset of the training data from a trained deep neural network

## Executive Summary
This paper introduces SCRUB, a novel machine unlearning algorithm that addresses the challenge of removing data influence from trained deep neural networks. SCRUB employs a teacher-student framework where the original model acts as a teacher, and a student model is trained to selectively obey the teacher on retained data while disobeying on data to be forgotten. The method demonstrates superior performance across three distinct evaluation metrics compared to previous state-of-the-art approaches, while also being more efficient and scalable.

## Method Summary
SCRUB treats the original trained model as a teacher and trains a student model that selectively obeys the teacher on retained data while disobeying on data to be forgotten. The student model is initialized with the teacher's weights and optimized using a contrastive objective: minimizing KL divergence to the teacher on retained data while maximizing it on forgotten data. The training alternates between epochs of updates on retain data (min-step) and forget data (max-step), followed by additional retain-only steps. For membership inference attack defense, SCRUB+R adds a rewinding procedure that stores checkpoints during training and selects the one where forget-set error matches a reference derived from identically-distributed validation data.

## Key Results
- SCRUB consistently outperforms previous state-of-the-art unlearning methods across three different metric sets
- Achieves high error on forget data while maintaining low error on retain data
- Effectively resolves class confusion issues when forgetting entire classes
- Provides strong defense against membership inference attacks
- More efficient and scalable than previous methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The teacher-student framework enables selective forgetting by training a student model to "disobey" the teacher on forget-set examples while staying close on retain-set examples.
- Mechanism: The student model is initialized with the teacher's weights and then optimized to minimize a contrastive objective: it stays close to the teacher on retained data (low KL divergence) while moving away on forgotten data (high KL divergence). This selectively removes influence from the forget set.
- Core assumption: The original teacher model performs well on both retain and forget sets initially, making it a valid "all-knowing" reference.
- Evidence anchors:
  - [abstract] "treat the original model as a teacher and train a student model that selectively obeys the teacher on retained data while disobeying on data to be forgotten"
  - [section] "SCRUB bears a novel teacher-student formulation, where the student model selectively disobeys an all-knowing teacher, to inherit from it only knowledge that does not pertain to the data to be deleted"
- Break condition: If the teacher performs poorly on either set, the initialization would not provide a reliable baseline for selective disobedience.

### Mechanism 2
- Claim: Alternating optimization between retain and forget sets stabilizes the unlearning process by preventing oscillations.
- Mechanism: SCRUB iterates between epochs of updates on retain data (minimizing KL divergence to the teacher) and epochs on forget data (maximizing KL divergence). This alternation, followed by additional retain-only steps, balances forgetting with preserving performance.
- Core assumption: The min-max optimization problem is inherently unstable without careful interleaving of update steps.
- Evidence anchors:
  - [section] "SCRUB provides a practical recipe for optimization, reminiscent of common 'tricks' used in other min-max-like problems like in Generative Adversarial Networks (GANs)"
  - [section] "iterates between performing an epoch of updates on the retain set (the min-step) followed by an epoch of updates on the forget set (the max-step), in an alternating fashion"
- Break condition: If the alternating schedule is too aggressive or too conservative, the model may either forget too much (hurting retain performance) or forget too little (failing to unlearn).

### Mechanism 3
- Claim: The "rewinding" procedure ensures the forget-set error matches a reference point, preventing membership inference attacks.
- Mechanism: SCRUB+R stores model checkpoints during training, then rewinds to the checkpoint where forget-set error matches a reference derived from identically-distributed validation data. This controls the forget error to be "just high enough" without being maximally high.
- Core assumption: The forget-set and validation-set distributions are sufficiently similar that their errors correlate meaningfully.
- Evidence anchors:
  - [section] "we propose a novel 'rewind-ing' procedure that pinpoints the best 'checkpoint' of the unlearning process to use, such that the error on the deleted data approximates the reference point"
  - [section] "the error on the identically-distributed validation set approximates the error of a model that never learned about that distribution from the forget set"
- Break condition: If the forget and validation distributions diverge significantly, the reference error becomes meaningless and rewinding may select inappropriate checkpoints.

## Foundational Learning

- Concept: KL divergence as a distance metric between probability distributions
  - Why needed here: SCRUB uses KL divergence to measure how much the student's predictions differ from the teacher's on individual examples
  - Quick check question: What does KL divergence measure between two probability distributions p and q?

- Concept: Contrastive learning objectives
  - Why needed here: The training objective pulls the student close to the teacher on retain data while pushing it away on forget data
  - Quick check question: How does a contrastive objective differ from a standard supervised loss?

- Concept: Membership inference attacks and their relationship to model performance on specific data subsets
  - Why needed here: The rewinding procedure specifically addresses the vulnerability to membership inference attacks by controlling forget-set error
  - Quick check question: Why would unusually high error on forgotten data make those examples identifiable?

## Architecture Onboarding

- Component map:
  - Teacher model -> Student model -> Optimizer -> Checkpoint manager -> Validation set

- Critical path:
  1. Load original model and forget/retain datasets
  2. Initialize student with teacher weights
  3. Run alternating optimization (retain epochs, then forget epochs)
  4. Store checkpoints throughout training
  5. Calculate reference error on validation set
  6. If needed, rewind to appropriate checkpoint
  7. Evaluate on all metrics

- Design tradeoffs:
  - Memory vs. accuracy: Storing more checkpoints enables better rewinding but increases memory usage
  - Speed vs. stability: More alternating steps improves stability but increases training time
  - Forget completeness vs. retain performance: Higher forget error may require sacrificing some retain accuracy

- Failure signatures:
  - Oscillating loss curves: Indicates need for learning rate adjustment or different alternating schedule
  - Retain error degradation: May require more min-steps or cross-entropy regularization
  - Forget error plateauing early: May indicate insufficient max-steps or inappropriate hyperparameters

- First 3 experiments:
  1. Class forgetting on CIFAR-10: Forget all examples from one class (10% of data), measure forget error and retain performance
  2. Selective forgetting on CIFAR-5: Forget 25 examples from class 0, measure class confusion metrics
  3. MIA defense on Lacuna-10: Forget examples from one class, measure membership inference attack success rate

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but it mentions investigating the interplay with other architectures and loss functions as future work.

## Limitations
- The teacher-student framework assumes the original model performs well on both retain and forget sets initially
- The alternating optimization approach introduces hyperparameters that require careful tuning
- The rewinding procedure's reliance on identically-distributed validation data is a significant assumption

## Confidence
- **High confidence**: The core mechanism of selective disobedience through contrastive objectives is theoretically sound
- **Medium confidence**: The alternating optimization approach is empirically validated but may require problem-specific tuning
- **Low confidence**: The rewinding procedure's effectiveness in defending against membership inference attacks depends heavily on distributional similarity

## Next Checks
1. **Distributional robustness test**: Systematically vary the similarity between forget and validation sets to quantify how the rewinding procedure's effectiveness degrades as distributional mismatch increases
2. **Cross-architecture generalization**: Apply SCRUB to architectures beyond All-CNN and ResNet-18 (e.g., Vision Transformers) to assess whether the alternating optimization schedule needs adaptation
3. **Hyperparameter sensitivity analysis**: Conduct a comprehensive grid search over learning rates, batch sizes, and alternating step counts to identify robust hyperparameter ranges