---
ver: rpa2
title: An Analysis of Untargeted Poisoning Attack and Defense Methods for Federated
  Online Learning to Rank Systems
arxiv_id: '2307.01565'
source_url: https://arxiv.org/abs/2307.01565
tags:
- poisoning
- learning
- data
- attack
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the vulnerability of federated online learning
  to rank (FOLTR) systems to poisoning attacks, where malicious participants aim to
  degrade the effectiveness of the globally-aggregated ranker. The authors investigate
  data poisoning, which corrupts click signals, and model poisoning, which directly
  manipulates local model updates before they are sent to the server.
---

# An Analysis of Untargeted Poisoning Attack and Defense Methods for Federated Online Learning to Rank Systems

## Quick Facts
- arXiv ID: 2307.01565
- Source URL: https://arxiv.org/abs/2307.01565
- Reference count: 40
- Key outcome: Model poisoning is more effective than data poisoning in degrading FOLTR ranker performance, especially with full attacker knowledge, while defense mechanisms can harm performance when no attack is present.

## Executive Summary
This paper investigates the vulnerability of federated online learning to rank (FOLTR) systems to poisoning attacks, where malicious participants aim to degrade the effectiveness of the globally-aggregated ranker. The authors explore both data poisoning (corrupting click signals) and model poisoning (manipulating local model updates) under different attack knowledge scenarios. They evaluate these attacks across various user behavior models and examine the effectiveness of defense mechanisms such as Krum, Multi-Krum, Trimmed Mean, and Median aggregation rules. The study reveals that model poisoning is significantly more effective than data poisoning, particularly when attackers have full system knowledge, and highlights the potential performance degradation caused by defense mechanisms when no attack is present.

## Method Summary
The paper implements a FOLTR system using the FPDGD algorithm with four LTR datasets (MQ2007, MSLR-WEB10k, Yahoo, Istella-S) and simulates user interactions through the SDBN click model. Attack modules include data poisoning via label flipping in click signals and model poisoning using both the LIE method and Fang's optimization-based attack. Defense mechanisms consist of Byzantine-robust aggregation rules (Krum, Multi-Krum, Trimmed Mean, Median). The experimental setup involves 10 clients with varying attacker percentages (0-50%) and evaluates performance degradation using nDCG@10 across different user behavior types (perfect, navigational, informational, poison).

## Key Results
- Model poisoning attacks achieve significantly higher effectiveness than data poisoning attacks in degrading FOLTR ranker performance
- Attack effectiveness increases substantially when attackers have full knowledge of the system compared to partial knowledge scenarios
- Deploying defense mechanisms without an ongoing attack can lead to decreased ranker performance, suggesting the need for attack detection before applying defenses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data poisoning attacks are effective because they directly manipulate user interaction signals, corrupting the training signal used by the ranker.
- Mechanism: Attackers simulate malicious clicks by reversing the click probabilities in the SDBN click model, causing the ranker to learn incorrect associations between documents and queries.
- Core assumption: The click model accurately reflects user behavior, and the ranker relies heavily on click signals for updates.
- Evidence anchors:
  - [abstract] "we experiment with and analyse data and model poisoning attack methods to showcase their impact on FOLTR search effectiveness."
  - [section] "Our data poisoning attack to FOLTR is inspired by the label flipping strategy...the attacker needs to intentionally flip the feedback by clicking on irrelevant documents to bring arbitrary noise thus poison the training."
  - [corpus] Weak evidence: The corpus does not mention data poisoning attacks specifically, but it does discuss poisoning attacks in federated learning contexts.
- Break condition: If the ranker uses additional signals beyond clicks (e.g., dwell time, explicit feedback), or if the click model is inaccurate, the attack's effectiveness diminishes.

### Mechanism 2
- Claim: Model poisoning attacks are more effective than data poisoning because they directly manipulate the model updates before aggregation.
- Mechanism: Attackers modify local model updates by adding noise or optimizing perturbations to bypass robust aggregation rules, directly corrupting the global model.
- Core assumption: The attacker can influence a significant portion of the clients (>20%) and the aggregation rules are vulnerable to well-crafted perturbations.
- Evidence anchors:
  - [abstract] "the paper highlights that model poisoning is more effective than data poisoning, especially when the attacker has full knowledge of the system."
  - [section] "Model poisoning, on the other hand, directly affects the local model updates...Fang et al. proposed an optimization-based model poisoning attack tailored to specific robust aggregation rules."
  - [corpus] Weak evidence: The corpus mentions model poisoning in federated learning but lacks specific details on FOLTR.
- Break condition: If the number of attackers is low (<20%) or if robust aggregation rules are highly effective, the attack's impact is reduced.

### Mechanism 3
- Claim: Defense mechanisms can be counterproductive when no attack is present, degrading ranker performance.
- Mechanism: Byzantine-robust aggregation rules exclude some local model updates to mitigate attacks, but this exclusion also removes valuable information when all updates are benign, leading to suboptimal aggregation.
- Core assumption: The FOLTR system operates under non-IID data distribution, and the aggregation rules assume the presence of malicious updates.
- Evidence anchors:
  - [abstract] "the paper highlights that deploying defense mechanisms without an ongoing attack can lead to a decrease in ranker performance."
  - [section] "It is essential to highlight that although Krum has proven effective in countering data poisoning...deploying these aggregation rules should be exercised with caution as they result in an overall decrease in search performance if the system is not exposed to attacks."
  - [corpus] Weak evidence: The corpus does not explicitly discuss the impact of defense mechanisms without attacks.
- Break condition: If the data distribution is IID or if the aggregation rules are adaptive to the presence of attacks, the negative impact may be mitigated.

## Foundational Learning

- Concept: Online Learning to Rank (OLTR)
  - Why needed here: FOLTR is built upon OLTR, and understanding how OLTR updates rankers based on user interactions is crucial for grasping the attack surface.
  - Quick check question: How does OLTR differ from traditional LTR in terms of training data and update frequency?

- Concept: Federated Learning
  - Why needed here: FOLTR uses federated learning to train models without sharing raw data, and understanding the aggregation process is key to understanding how attacks and defenses work.
  - Quick check question: What is the role of the server in federated learning, and how does it aggregate local model updates?

- Concept: Byzantine-robust Aggregation
  - Why needed here: Defense mechanisms in FOLTR rely on Byzantine-robust aggregation rules to mitigate poisoning attacks, and understanding these rules is essential for evaluating their effectiveness.
  - Quick check question: How do Krum and Trimmed Mean aggregation rules identify and exclude malicious model updates?

## Architecture Onboarding

- Component map: Clients -> Server -> Global ranker -> Clients
- Critical path:
  1. Clients receive the global ranker from the server.
  2. Clients perform local updates based on user interactions.
  3. Clients send local model updates to the server.
  4. Server aggregates local model updates using the chosen aggregation rule.
  5. Server broadcasts the updated global ranker to clients.
- Design tradeoffs:
  - Privacy vs. effectiveness: Federated learning preserves privacy but may be more vulnerable to attacks compared to centralized learning.
  - Robustness vs. performance: Robust aggregation rules can mitigate attacks but may degrade performance when no attack is present.
  - Complexity vs. efficiency: Sophisticated attack and defense mechanisms may be more effective but also more computationally expensive.
- Failure signatures:
  - Sudden drop in nDCG@10 scores during federated training.
  - Inconsistent performance across different clients or queries.
  - Failure of robust aggregation rules to improve performance when attacks are present.
- First 3 experiments:
  1. Implement a basic FOLTR system using FPDGD and evaluate its performance on a standard dataset (e.g., MSLR-WEB10k) without any attacks or defenses.
  2. Introduce a simple data poisoning attack (e.g., label flipping) and evaluate its impact on the FOLTR system's performance.
  3. Implement a basic defense mechanism (e.g., Krum aggregation) and evaluate its effectiveness in mitigating the data poisoning attack.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different user behavior models (e.g., perfect, navigational, informational, and poison) impact the effectiveness of poisoning attacks on FOLTR systems?
- Basis in paper: [explicit] The paper investigates the impact of different user behavior models on the effectiveness of poisoning attacks.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of different user behavior models on the effectiveness of poisoning attacks, especially under varying attack conditions.
- What evidence would resolve it: Conducting experiments with a broader range of user behavior models and analyzing the results to determine the impact of each model on the effectiveness of poisoning attacks under different attack conditions.

### Open Question 2
- Question: What is the trade-off between the effectiveness of defense mechanisms and the degradation of FOLTR system performance when no attack is present?
- Basis in paper: [explicit] The paper discusses the impact of deploying defense mechanisms on the performance of FOLTR systems when no attack is present.
- Why unresolved: The paper does not provide a detailed analysis of the trade-off between the effectiveness of defense mechanisms and the degradation of FOLTR system performance when no attack is present.
- What evidence would resolve it: Conducting experiments to measure the performance degradation of FOLTR systems when defense mechanisms are deployed under different conditions, such as varying numbers of attackers and types of user behavior, and comparing the results to the performance of systems without defense mechanisms.

### Open Question 3
- Question: How can attack detection methods be integrated into FOLTR systems to improve the deployment of defense mechanisms?
- Basis in paper: [inferred] The paper suggests that the deployment of defense mechanisms should be carefully considered, taking into account the specific context and risk of potential attacks.
- Why unresolved: The paper does not provide a detailed analysis of how attack detection methods can be integrated into FOLTR systems to improve the deployment of defense mechanisms.
- What evidence would resolve it: Developing and evaluating attack detection methods that can be integrated into FOLTR systems and analyzing their effectiveness in detecting and mitigating poisoning attacks.

## Limitations

- The experimental validation relies on simulated user behaviors rather than real user interactions, which may not capture the full complexity of actual search scenarios.
- The effectiveness of attacks and defenses could vary significantly with different data distributions and user behavior patterns not explored in the study.
- The computational overhead of Byzantine-robust aggregation rules in large-scale FOLTR systems is not addressed, potentially limiting practical deployment.

## Confidence

- **High Confidence**: The relative effectiveness of model poisoning versus data poisoning under different attack knowledge assumptions is well-supported by experimental results across multiple datasets.
- **Medium Confidence**: The observation that defense mechanisms can degrade performance when no attack is present is supported, but the specific thresholds for when this occurs need further validation.
- **Low Confidence**: The generalization of attack and defense effectiveness to real-world FOLTR systems with diverse user populations and complex ranking features remains uncertain.

## Next Checks

1. Conduct ablation studies to isolate the impact of different components of the SDBN click model on attack effectiveness, particularly examining how variations in click probability parameters affect poisoning success rates.

2. Test the robustness of defense mechanisms across different data distributions by evaluating performance when client data is highly non-IID versus moderately non-IID, to determine if current aggregation rules are universally applicable.

3. Implement a real-world pilot study using a small-scale FOLTR system with actual user interactions to validate whether simulated attack patterns and defense mechanisms translate effectively to production environments.