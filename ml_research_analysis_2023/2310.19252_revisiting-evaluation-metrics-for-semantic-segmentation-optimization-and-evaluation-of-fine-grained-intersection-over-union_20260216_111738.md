---
ver: rpa2
title: 'Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and
  Evaluation of Fine-grained Intersection over Union'
arxiv_id: '2310.19252'
source_url: https://arxiv.org/abs/2310.19252
tags:
- deeplabv3
- ioui
- mean
- kurtosis
- skew
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper advocates for the use of fine-grained mean intersection\
  \ over union (mIoU) metrics to address the limitations of traditional per-dataset\
  \ mIoU in semantic segmentation evaluation. The authors propose three fine-grained\
  \ metrics\u2014image-level (mIoUI), class-level (mIoUC), and instance-level (mIoUK)\u2014\
  which reduce bias towards large objects, provide richer statistical information,\
  \ and enable worst-case performance analysis."
---

# Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union

## Quick Facts
- arXiv ID: 2310.19252
- Source URL: https://arxiv.org/abs/2310.19252
- Reference count: 40
- Primary result: Fine-grained mIoU metrics (image-level, class-level, instance-level) reduce bias toward large objects and enable richer statistical analysis for semantic segmentation evaluation.

## Executive Summary
This paper advocates for fine-grained mean intersection over union (mIoU) metrics to address limitations of traditional per-dataset mIoU in semantic segmentation evaluation. The authors propose three fine-grained metrics—image-level (mIoUI), class-level (mIoUC), and instance-level (mIoUK)—which reduce bias towards large objects, provide richer statistical information, and enable worst-case performance analysis. Through an extensive benchmark study training 15 modern neural networks on 12 diverse datasets, they demonstrate that fine-grained metrics are less biased towards large objects and yield valuable insights for model and dataset auditing. The study also identifies crucial roles of architecture designs and loss functions, leading to best practices for optimizing fine-grained metrics.

## Method Summary
The paper introduces fine-grained mIoU metrics computed at different granularities: image-level (mIoUI), class-level (mIoUC), and instance-level (mIoUK). These metrics compute intersection over union at progressively finer levels rather than aggregating across entire datasets. The authors train 15 modern neural networks from scratch using AdamW optimizer with weight decay 0.01, learning rate warmup, poly learning rate decay, and cross-entropy loss combined with Jaccard metric loss (JML) with weights 0.25 and 0.75 respectively. They conduct extensive experiments across 12 diverse datasets and analyze the correlation between different mIoU metrics to identify bias patterns and architectural influences.

## Key Results
- Fine-grained metrics reduce bias towards large objects by computing TP/FP/FN at image or instance level rather than aggregating across whole datasets
- Class-level mIoUC and instance-level mIoUK show lower correlation with object size than per-dataset mIoUD, indicating reduced size bias
- DeepLabV3+-ResNet101 trained with JML loss variants shows significant improvements on mIoUC (nearly 3% on Cityscapes, 7% on ADE20K) compared to cross-entropy loss
- Statistical analysis reveals that different architecture designs (backbone, segmentation head, attention mechanisms) impact fine-grained metrics differently

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained mIoU metrics reduce bias towards large objects by computing TP/FP/FN at image or instance level rather than aggregating across the whole dataset.
- Mechanism: In traditional per-dataset mIoU, the denominator accumulates TP, FP, FN over all pixels, so large objects dominate the score. By computing per-image or per-instance IoU, the denominator resets for each image/instance, preventing large objects from overwhelming the metric.
- Core assumption: The size imbalance in the dataset is significant enough that per-dataset accumulation skews results, and per-image/instance aggregation restores fairness across object sizes.
- Evidence anchors:
  - [abstract] "These fine-grained metrics offer less bias towards large objects, richer statistical information, and valuable insights into model and dataset auditing."
  - [section] "mIoUI and mIoUC reduce size imbalance from dataset-level to image-level, and mIoUK further computes the score at an instance level."
  - [corpus] Weak/no direct evidence found; the corpus does not provide explicit experimental support for this claim.

### Mechanism 2
- Claim: Fine-grained metrics provide richer statistical information that enables worst-case performance analysis and model auditing.
- Mechanism: Per-image or per-instance scores yield a distribution of IoU values, from which statistics like min, max, mean, quantiles, and histograms can be computed. This distribution allows identification of worst-case images/instances and facilitates statistical significance testing.
- Core assumption: A distribution of scores per image/instance is more informative than a single per-dataset average, and worst-case scenarios are important for safety-critical applications.
- Evidence anchors:
  - [abstract] "These fine-grained metrics offer... richer statistical information, and valuable insights into model and dataset auditing."
  - [section] "Images that consistently yield a low image-level score across different models can be inspected. These low scores could potentially be attributed to mislabeling within the images."
  - [corpus] Weak/no direct evidence found; the corpus does not provide explicit experimental support for this claim.

### Mechanism 3
- Claim: Aligning loss functions with fine-grained evaluation metrics improves performance on those metrics.
- Mechanism: Traditional CE loss optimizes per-pixel accuracy, which does not directly optimize IoU. By using Jaccard metric loss (JML) variants that explicitly optimize mIoUI and mIoUC, the model learns to improve scores on those metrics.
- Core assumption: Loss functions that match evaluation metrics guide the model to learn features that directly improve those metrics, rather than indirectly through proxy losses.
- Evidence anchors:
  - [section] "We compare DeepLabV3+-ResNet101 trained with various loss functions on Cityscapes and ADE20K... Improvements on mIoUC over CE reaching nearly 3% on Cityscapes and 7% on ADE20K."
  - [corpus] Weak/no direct evidence found; the corpus does not provide explicit experimental support for this claim.

## Foundational Learning

- Concept: IoU (Intersection over Union) and its limitations in semantic segmentation evaluation
  - Why needed here: The paper builds its entire argument around improving IoU metrics to address class and size imbalance. Understanding IoU computation and bias is essential to grasp the motivation for fine-grained variants.
  - Quick check question: How does per-dataset IoU computation lead to bias towards large objects?

- Concept: Statistical significance testing and distribution analysis in model evaluation
  - Why needed here: The paper emphasizes that fine-grained metrics enable richer statistical information and worst-case analysis. Practitioners need to know how to interpret these distributions and conduct significance tests.
  - Quick check question: What statistical tests can be applied to compare model performance using per-image IoU distributions?

- Concept: Instance-level annotations and their role in semantic segmentation
  - Why needed here: The paper introduces instance-level IoU (mIoUK) which requires instance-level labels. Understanding the difference between "thing" and "stuff" classes and how instance annotations are used is crucial for applying this metric.
  - Quick check question: How does the presence of instance-level labels enable computation of per-instance IoU?

## Architecture Onboarding

- Component map: Data preprocessing → Model training with JML loss → Per-image/instance IoU computation → Statistical analysis → Worst-case identification → Model auditing
- Critical path: Training loop with JML loss → IoU aggregation at desired granularity → Evaluation pipeline that computes all three fine-grained metrics and worst-case variants
- Design tradeoffs: Per-dataset aggregation is computationally simple but biased; per-instance aggregation is most fine-grained but requires instance labels and is more complex
- Failure signatures: Low variance in per-image IoU histograms (indicating lack of granularity), high correlation between mIoUD and fine-grained metrics (indicating size imbalance is not a problem), NaN values in mIoUK (indicating label inconsistencies)
- First 3 experiments:
  1. Train a baseline model with CE loss and compute all three fine-grained metrics to establish baseline bias levels
  2. Switch to JML loss variant optimized for mIoUC and measure improvement on mIoUC vs mIoUD
  3. Analyze worst-case images/instances across different models to identify common failure modes and potential label errors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do fine-grained mIoUs perform on medical datasets compared to natural/aerial datasets?
- Basis in paper: [explicit] The authors acknowledge this as a limitation, stating their experiments primarily focus on natural and aerial datasets, excluding medical datasets from their analysis.
- Why unresolved: The paper does not include medical datasets in their benchmark study, leaving a gap in understanding how these metrics perform in medical imaging contexts where small lesion areas are common.
- What evidence would resolve it: A benchmark study applying the proposed fine-grained metrics to various medical segmentation datasets (e.g., KiTS19, LiTS, MoNuSAC) would provide direct evidence of their performance and effectiveness in this domain.

### Open Question 2
- Question: Can the concept of fine-grained metrics be extended to other evaluation metrics beyond IoU, such as Dice score or calibration error?
- Basis in paper: [inferred] The authors mention this as a future direction in their limitations section, suggesting that calculating segmentation metrics in a fine-grained manner could be applied to other metrics like mAcc, Dice score, or calibration error.
- Why unresolved: While the paper focuses on fine-grained mIoUs, it does not explore the application of this concept to other evaluation metrics, leaving uncertainty about its potential benefits in these areas.
- What evidence would resolve it: Developing and evaluating fine-grained variants of other common segmentation metrics (e.g., per-instance Dice score, image-level calibration error) would demonstrate the generalizability and advantages of this approach.

### Open Question 3
- Question: What is the optimal choice of q for worst-case metrics across different dataset sizes and characteristics?
- Basis in paper: [explicit] The authors discuss this in their suggestion section, noting that q=1 might be suitable for larger datasets like ADE20K but could introduce significant variance for smaller datasets like DeepGlobe Land, where q=5 or larger might be more appropriate.
- Why unresolved: The paper does not provide a definitive answer or methodology for choosing the optimal q value, leaving practitioners to make potentially suboptimal choices based on dataset characteristics.
- What evidence would resolve it: A systematic study analyzing the impact of different q values on worst-case metrics across datasets of varying sizes and characteristics would help establish guidelines for selecting the most appropriate q value in different scenarios.

## Limitations

- The corpus lacks direct experimental validation for key mechanisms, particularly the claims about bias reduction in fine-grained metrics and the statistical richness they provide
- Implementation details for Jaccard metric loss (JML) variants are not fully specified, making exact reproduction challenging
- The claim that instance-level metrics (mIoUK) are "least biased towards large objects" is based on correlation analysis rather than direct experimental evidence showing improved performance on small object segmentation

## Confidence

- **High**: Fine-grained metrics are computationally feasible and can be implemented as described. The correlation analysis showing different bias patterns across metrics is statistically sound.
- **Medium**: The mechanism that per-image/instance aggregation reduces size bias is plausible but not independently verified. The architectural findings about model designs are supported by the extensive benchmark but could be dataset-specific.
- **Low**: The statistical richness claims and worst-case analysis utility lack independent experimental validation. The effectiveness of JML loss alignment for optimizing fine-grained metrics needs more rigorous testing.

## Next Checks

1. **Independent Bias Analysis**: Conduct a controlled experiment training models on synthetic datasets with known size distributions to verify that mIoUI and mIoUC genuinely reduce bias toward large objects compared to mIoUD.

2. **Statistical Significance Testing**: Apply formal statistical tests (e.g., paired t-tests or Wilcoxon signed-rank) to per-image IoU distributions across multiple models to validate the claim that fine-grained metrics provide statistically significant insights.

3. **JML Loss Ablation Study**: Systematically vary the JML loss weights and compare performance on both aligned (e.g., JML for mIoUC evaluated on mIoUC) and misaligned metrics to quantify the actual benefit of loss-metric alignment.