---
ver: rpa2
title: Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot
  Summarization
arxiv_id: '2311.09344'
source_url: https://arxiv.org/abs/2311.09344
tags:
- language
- task
- data
- languages
- xlsum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for improving zero-shot cross-lingual
  transfer by composing language and task parameter-efficient fine-tuning (PEFT) modules
  using element-wise arithmetic operations. The approach leverages unlabeled data
  and labeled task data in source languages to create effective task vectors for unseen
  target languages.
---

# Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization

## Quick Facts
- arXiv ID: 2311.09344
- Source URL: https://arxiv.org/abs/2311.09344
- Reference count: 15
- This paper proposes composing language and task PEFT modules via element-wise arithmetic operations to leverage unlabeled data and English labeled data for zero-shot cross-lingual transfer.

## Executive Summary
This paper addresses zero-shot cross-lingual transfer for summarization by composing parameter-efficient fine-tuning (PEFT) modules using element-wise arithmetic operations. The method leverages unlabeled monolingual data and labeled task data in source languages to create effective task vectors for unseen target languages. By training separate language and task vectors with LoRA or Kronecker adapters, then combining them through addition and subtraction, the approach achieves consistent improvements over baselines on multilingual summarization tasks. The method particularly benefits from selective combination of task vectors from linguistically similar languages.

## Method Summary
The paper proposes composing language and task parameter-efficient fine-tuning modules through element-wise arithmetic operations to enable zero-shot cross-lingual transfer for summarization. The approach trains separate LoRA or Kronecker adapter parameters on unlabeled monolingual data (language vectors) and labeled task data in source languages (task vectors). For a target language, the model composes these vectors using addition and subtraction operations. The paper also introduces a selective composition strategy that combines task vectors from linguistically similar source languages identified using URIEL distance metrics, rather than uniformly averaging all available task vectors.

## Key Results
- Arithmetic composition of language and task vectors consistently outperforms monolingual and multilingual baselines on zero-shot summarization across 11 target languages.
- Selective combination of task vectors from linguistically similar languages (using URIEL) surpasses uniform averaging of all available task vectors.
- The addition-and-subtraction composition strategy (θtask;T = λθtask;S + (1-λ)(θLM;T - θLM;S)) provides superior performance compared to addition-only for summarization tasks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Element-wise arithmetic operations on parameter-efficient modules can effectively combine language and task knowledge for zero-shot cross-lingual transfer.
- **Mechanism**: The paper proposes using LoRA parameters trained on labeled task data in source languages and unlabeled language data to create composite parameters for target languages through addition and subtraction operations.
- **Core assumption**: Language and task parameters learned through PEFT follow linear trajectories in parameter space, allowing meaningful composition through arithmetic operations.
- **Evidence anchors**:
  - [abstract] The method composes language and task PEFT modules via element-wise arithmetic operations to leverage unlabeled data and English labeled data.
  - [section 2] We propose to compose the language and task vectors to better support summarization into the target language T.
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.398, average citations=0.0.
- **Break condition**: If language and task parameters do not follow linear trajectories, the arithmetic composition would not produce meaningful results and could degrade performance.

### Mechanism 2
- **Claim**: Selective combination of task vectors from linguistically similar languages outperforms uniform averaging of all available task vectors.
- **Mechanism**: The paper uses URIEL to identify source languages most similar to the target language based on syntactic and geographic distances, then averages only the task vectors from these related languages.
- **Core assumption**: Task vectors from linguistically similar languages provide more relevant information for the target language than task vectors from distant languages.
- **Evidence anchors**:
  - [section 2.2] When summarization articles are available in various languages, we combine representations from languages most relevant to the target.
  - [section 4.2] Our approach (task-only; Add related) is then presented... This selective composition of task vectors clearly surpasses the baselines.
  - [corpus] Weak corpus evidence - only general mention of parameter-efficient methods.
- **Break condition**: If URIEL's distance metrics do not accurately reflect linguistic similarity relevant to the task, the selection of related languages could be suboptimal.

### Mechanism 3
- **Claim**: Subtracting source language vectors from target language vectors helps the model generate in the target language while avoiding generation in the source language.
- **Mechanism**: The paper proposes subtracting the source language vector from the target language vector in addition to adding the task vector, which helps steer the model away from source language generation patterns.
- **Core assumption**: Subtraction of parameters can "unlearn" specific language generation patterns while preserving task knowledge.
- **Evidence anchors**:
  - [section 2.1] We propose subtracting the source language vector from the target language vector.
  - [section 4.1] Intuitively, negating the En language vector parameters likely reduces the bias of the model towards En and enhances the ability of the model to generate in the target language.
  - [corpus] No direct corpus evidence supporting this specific mechanism.
- **Break condition**: If the subtraction operation removes too much useful information along with the unwanted source language patterns, it could degrade overall performance.

## Foundational Learning

- **Concept**: Parameter-efficient fine-tuning (PEFT) methods like LoRA and Kronecker adapters
  - **Why needed here**: The paper's approach relies on training small, efficient parameter modules that can be composed through arithmetic operations rather than full model fine-tuning.
  - **Quick check question**: What is the key difference between LoRA and Kronecker adapters in terms of their parameter decomposition approach?

- **Concept**: Linear mode connectivity and the lottery ticket hypothesis
  - **Why needed here**: The paper's arithmetic composition approach is based on the observation that fine-tuned models follow linear trajectories in parameter space, allowing meaningful interpolation.
  - **Quick check question**: How does the lottery ticket hypothesis relate to the paper's approach of composing fine-tuned parameters?

- **Concept**: Cross-lingual transfer and zero-shot learning
  - **Why needed here**: The paper addresses the challenge of performing summarization in target languages without any labeled data by leveraging knowledge from source languages.
  - **Quick check question**: What is the main advantage of zero-shot cross-lingual transfer compared to traditional multilingual training approaches?

## Architecture Onboarding

- **Component map**: PaLM 2-S -> LoRA/Kronecker adapters (language vectors) -> LoRA/Kronecker adapters (task vectors) -> Arithmetic composition layer -> XLSumunseen evaluation

- **Critical path**:
  1. Train language vectors on unlabeled data for each language using LoRA
  2. Train task vectors on labeled data for source languages using LoRA
  3. Compose vectors for target languages using arithmetic operations
  4. Evaluate zero-shot performance on XLSumunseen

- **Design tradeoffs**:
  - LoRA vs. Kronecker adapters: LoRA is simpler but Kronecker offers more flexibility in decomposition
  - Addition vs. addition+subtraction: Subtraction helps avoid source language bias but may remove useful information
  - Related language selection: URIEL-based selection is data-efficient but may not capture all relevant linguistic features

- **Failure signatures**:
  - Performance drops when source and target languages are too dissimilar
  - Arithmetic composition creates negative interference between parameters
  - Language vectors trained on insufficient unlabeled data provide limited benefit

- **First 3 experiments**:
  1. Train LoRA parameters on English XLSum data (task vector) and English mC4 data (language vector), then compose for Portuguese summarization
  2. Train LoRA parameters on XLSum data from multiple source languages, select related languages using URIEL, and compose for a target language
  3. Compare LoRA vs. Kronecker adapter performance when composing language and task parameters for zero-shot transfer

## Open Questions the Paper Calls Out

- **Question**: How does the performance of language and task arithmetic scale with increasing numbers of source languages?
- **Question**: What is the optimal composition strategy when combining language and task vectors from different PEFT methods (e.g., LoRA and Kronecker)?
- **Question**: How does the language and task arithmetic approach perform on non-generative tasks like classification or structured prediction?
- **Question**: What is the computational overhead of computing language vectors during inference, and how does it compare to the benefits gained?

## Limitations

- The effectiveness depends on the assumption that language and task parameters follow linear trajectories in parameter space, which may not hold for all model architectures or tasks.
- URIEL-based language similarity metrics may not capture all relevant linguistic features needed for optimal task vector composition, particularly for languages with complex morphological or syntactic differences.
- The approach relies on the availability of sufficient unlabeled monolingual data for language vector training, which may not be available for low-resource languages.

## Confidence

**High confidence**: The paper's core methodology of using parameter-efficient fine-tuning with arithmetic composition is technically sound and well-implemented. The evaluation results showing consistent improvements over baselines on XLSum are reliable and reproducible.

**Medium confidence**: The claims about the effectiveness of selective task vector combination from related languages are supported by experimental results, but the URIEL-based language selection method may have limitations in capturing task-relevant linguistic similarity.

**Low confidence**: The specific mechanism by which subtraction of source language vectors improves performance is not well-validated. The paper provides intuitive explanations but lacks ablation studies to isolate the contribution of this operation.

## Next Checks

1. **Ablation study for subtraction operation**: Run experiments comparing addition-only vs. addition-and-subtraction composition methods across multiple language pairs to quantify the specific contribution of the subtraction operation and identify potential negative effects.

2. **Cross-task generalization**: Evaluate the arithmetic composition approach on additional multilingual tasks (e.g., machine translation, named entity recognition) using the same LoRA and Kronecker adapter framework to test the method's generalizability beyond summarization.

3. **Alternative language similarity metrics**: Replace URIEL with alternative language similarity measures (e.g., typological features, embedding-based similarity) to assess whether the selective combination of task vectors from related languages is robust to different similarity metrics.