---
ver: rpa2
title: What Makes ImageNet Look Unlike LAION
arxiv_id: '2306.15769'
source_url: https://arxiv.org/abs/2306.15769
tags:
- imagenet
- similarity
- laionet
- intra-class
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why ImageNet differs from LAION by recreating
  ImageNet using LAION image captions alone, creating a dataset called LAIONet. The
  authors find that LAIONet has significantly lower intra-class similarity than ImageNet,
  leading to 10 percentage point accuracy drops when evaluating ImageNet-trained models.
---

# What Makes ImageNet Look Unlike LAION

## Quick Facts
- arXiv ID: 2306.15769
- Source URL: https://arxiv.org/abs/2306.15769
- Reference count: 22
- Key outcome: ImageNet differs from LAION due to selection bias; text-only selection preserves natural data diversity

## Executive Summary
This paper investigates why ImageNet systematically differs from the naturally occurring LAION dataset by recreating ImageNet using only LAION image captions. The authors create LAIONet, a dataset matching ImageNet's class structure but selected purely through text-based filtering, and find it has significantly higher intra-class diversity. Models trained on ImageNet perform 10 percentage points worse on LAIONet, demonstrating that ImageNet's selection process creates an artificially narrow distribution within classes. The key insight is that using captions as an information bottleneck during dataset creation preserves natural diversity by reducing selection bias compared to image-based filtering.

## Method Summary
The authors recreate ImageNet using LAION-400M image-text pairs by applying text-based selection with CLIP similarity thresholds. They match ImageNet synsets using lemma substring matching, filter for high text-sytnset similarity (>0.82), remove multi-label samples and NSFW content, then evaluate pretrained ImageNet models on the resulting LAIONet dataset. The study compares intra-class similarity distributions between datasets, measures accuracy drops on LAIONet, and conducts ablation experiments with different selection frequencies and text-only recreation approaches.

## Key Results
- LAIONet has significantly lower intra-class similarity than ImageNet, leading to 10 percentage point accuracy drops when evaluating ImageNet-trained models
- Searching based on captions alone creates an information bottleneck that mitigates selection bias present in image-based filtering
- Varying annotation strictness shows more diversity with lower agreement, and text alone cannot fully explain ImageNet selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using captions as an information bottleneck reduces selection bias compared to image-based filtering
- Mechanism: When selection depends only on text embeddings, the conditional distribution of images given selection is closer to the natural class distribution, because text contains less information than the full image
- Core assumption: Text embeddings capture less visual information than full images, creating an effective information bottleneck
- Evidence anchors:
  - [abstract]: "searching based on an image caption alone creates an information bottleneck that mitigates the selection bias otherwise present in image-based filtering"
  - [section 2]: "a text caption, for example, generally carries much less information than the entire image"
  - [corpus]: Weak - neighboring papers discuss CLIP generalization and realism but don't directly test the information bottleneck claim
- Break condition: If text embeddings capture nearly all visual information relevant for class membership, the bottleneck effect disappears

### Mechanism 2
- Claim: Higher intra-class similarity in ImageNet reflects selection bias from human annotators
- Mechanism: Human annotators preferentially select stereotypical, clear examples, creating a narrower distribution within each class compared to natural web data
- Core assumption: Human selection systematically favors more prototypical examples over diverse ones
- Evidence anchors:
  - [abstract]: "ImageNet images are stereotypical, unnatural, and overly simple representations of the class category"
  - [section 3.2]: "LAIONet images are more diverse in each class" and "models trained on ImageNet perform significantly worse on LAIONet"
  - [corpus]: Weak - no direct evidence in corpus about human selection bias mechanisms
- Break condition: If annotator agreement is purely random or if they deliberately maximize diversity, the effect reverses

### Mechanism 3
- Claim: Modulating annotator agreement changes intra-class diversity, supporting the selection bias hypothesis
- Mechanism: Lower annotator agreement thresholds allow more diverse examples into the dataset, reducing intra-class similarity
- Core assumption: Different versions of ImageNetV2 with varying selection frequency thresholds create different levels of selection strictness
- Evidence anchors:
  - [section 4.1]: "as we decrease selection frequency, the resulting images come closer to LAIONet" and "the link from the image to the selection is contributing significantly to the divergence"
  - [corpus]: Weak - neighboring papers discuss CLIP robustness but not the relationship between selection frequency and diversity
- Break condition: If the relationship between selection frequency and diversity is non-monotonic or if other factors dominate

## Foundational Learning

- Concept: Causal graphs and selection bias
  - Why needed here: Understanding how selection mechanisms create bias in dataset distributions
  - Quick check question: In a causal graph where selection depends on both image and text, how does conditioning on selection change the image distribution compared to when selection depends only on text?

- Concept: Information bottleneck theory
  - Why needed here: Quantifying how much information is lost when selecting based on partial representations
  - Quick check question: If text embeddings capture 30% of image information relevant for classification, what fraction of the natural distribution variance remains after text-based selection?

- Concept: Intra-class similarity metrics
  - Why needed here: Measuring diversity within classes to quantify selection bias
  - Quick check question: If two datasets have the same number of classes but one has higher average pairwise similarity within classes, what does this tell you about their selection mechanisms?

## Architecture Onboarding

- Component map:
  - LAION-400M dataset -> Caption filtering -> CLIP similarity thresholding -> LAIONet creation
  - ImageNet models -> LAIONet evaluation -> Accuracy comparison
  - CLIP embeddings -> Intra-class similarity calculation -> Distribution comparison
  - ImageNetV2 versions -> Selection frequency analysis -> Diversity measurement

- Critical path:
  1. Extract LAION samples matching ImageNet synsets via lemma substring matching
  2. Filter by CLIP text-sytnset similarity threshold (0.82)
  3. Remove multi-label samples and NSFW content
  4. Evaluate pretrained models on LAIONet
  5. Calculate intra-class similarities and compare to ImageNet
  6. Run ablation experiments with ImageNetV2 versions

- Design tradeoffs:
  - Conservative similarity threshold (0.82) vs. class coverage
  - Text-only selection vs. image-based filtering for diversity
  - Uniform vs. frequency-weighted accuracy metrics
  - Sampling with replacement for distribution matching

- Failure signatures:
  - High intra-class similarity in LAIONet despite text-only selection
  - No accuracy drop when evaluating ImageNet models on LAIONet
  - Weak correlation between selection frequency and diversity in ImageNetV2
  - Similar performance trends across all model architectures

- First 3 experiments:
  1. Recreate LAIONet with varying similarity thresholds (0.7, 0.82, 0.9) and measure class coverage vs. intra-class similarity
  2. Compare CLIP zero-shot accuracy on LAIONet vs. ImageNet across individual classes to identify systematic differences
  3. Extract a text-matched dataset from LAION using ImageNet captions and measure how closely it matches ImageNet's intra-class similarity distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms beyond human annotation might cause the image-to-selection link in ImageNet's creation?
- Basis in paper: [explicit] The authors acknowledge that "Incorporation of visual features at the side of the search engine provider is another plausible mechanism" but do not investigate this further.
- Why unresolved: The paper focuses primarily on human annotation as the image-to-selection mechanism, leaving other potential mechanisms unexplored.
- What evidence would resolve it: Systematic analysis of how search engine algorithms might incorporate visual features in their ranking systems, or experimental recreation of ImageNet using automated image-based filtering.

### Open Question 2
- Question: How would LAIONet's characteristics change if created using LAION-5B instead of LAION-400M?
- Basis in paper: [explicit] The authors note that "for many cases, we were unable to find great matches for the ImageNet texts in LAION-400M. Scaling our analysis to LAION-5B might help here."
- Why unresolved: The current analysis is limited to LAION-400M due to computational constraints, leaving open questions about whether larger dataset size would change the findings.
- What evidence would resolve it: Creation of LAIONet using LAION-5B and comparison of its intra-class similarity and model accuracy drop with the current LAIONet.

### Open Question 3
- Question: What is the relationship between selection frequency thresholds and the preservation of natural data diversity across different types of datasets?
- Basis in paper: [explicit] The authors show that varying selection frequency in ImageNetV2 affects intra-class similarity, but this is limited to one dataset type.
- Why unresolved: The analysis focuses on ImageNet's selection mechanism but doesn't generalize to other dataset creation approaches.
- What evidence would resolve it: Comparative studies across multiple datasets with different selection mechanisms, measuring how selection frequency thresholds affect diversity preservation in each case.

## Limitations
- Reliance on CLIP embeddings as proxy for semantic similarity may not perfectly capture human perceptual judgments
- 0.82 similarity threshold chosen empirically without extensive sensitivity analysis
- Study focuses on ImageNet's specific characteristics and may not generalize to other curated datasets

## Confidence
- High confidence: Empirical findings (10 percentage point accuracy drops, measurable differences in intra-class similarity)
- Medium confidence: Causal interpretation linking selection bias to information bottlenecks
- Low confidence: Generalizability to other dataset types and selection mechanisms

## Next Checks
1. Recreate LAIONet with multiple similarity thresholds (0.7, 0.82, 0.9) and systematically measure how class coverage, dataset size, and intra-class similarity trade off against each other
2. Conduct human perceptual studies comparing LAIONet and ImageNet samples to validate whether CLIP's diversity metrics align with human judgments of "naturalness" and "stereotypicality"
3. Apply the text-only selection approach to other curated datasets (like CIFAR-100 or Food-101) to test whether the information bottleneck effect generalizes beyond ImageNet