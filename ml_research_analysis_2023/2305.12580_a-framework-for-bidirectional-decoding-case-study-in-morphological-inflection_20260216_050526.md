---
ver: rpa2
title: 'A Framework for Bidirectional Decoding: Case Study in Morphological Inflection'
arxiv_id: '2305.12580'
source_url: https://arxiv.org/abs/2305.12580
tags:
- medium
- each
- bidirectional
- small
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a bidirectional decoding framework for sequence-to-sequence
  tasks, where the model can generate tokens from either the left or right side of
  the sequence at each step. The framework supports various model architectures and
  includes training methods such as dynamic programming to marginalize over latent
  ordering variables.
---

# A Framework for Bidirectional Decoding: Case Study in Morphological Inflection

## Quick Facts
- arXiv ID: 2305.12580
- Source URL: https://arxiv.org/abs/2305.12580
- Reference count: 15
- Primary result: Bidirectional decoding framework achieves state-of-the-art results on 2022/2023 SIGMORPHON morphological inflection tasks, improving average accuracy by 4.7 and 2.7 points respectively over baselines.

## Executive Summary
This paper introduces a bidirectional decoding framework for sequence-to-sequence tasks that allows models to generate tokens from either the left or right side of the sequence at each step. The framework is demonstrated on morphological inflection, where it achieves state-of-the-art results on the SIGMORPHON 2022 and 2023 shared tasks. The key innovation is a dynamic programming algorithm that enables exact marginal likelihood computation over all possible generation orderings, combined with a carefully designed transformer architecture that satisfies the independence assumptions required for polynomial-time computation.

## Method Summary
The method implements a transformer-based encoder-decoder architecture where the decoder takes both prefix and suffix tokens as input, along with special classification tokens. At each generation step, the model can choose to generate a token from the left, generate from the right, or join the two sequences. The framework supports two training approaches: cross-entropy with fixed ordering probabilities and maximum marginal likelihood (MML) that learns ordering probabilities through marginalization. A dynamic programming algorithm computes the marginal likelihood P(y|x) by efficiently summing over all possible generation orderings, enabling exact training without enumeration.

## Key Results
- Achieves state-of-the-art results on SIGMORPHON 2022 and 2023 morphological inflection tasks
- Improves average accuracy by over 4.7 points on 2022 and 2.7 points on 2023 datasets compared to best baselines
- Particularly excels on long sequences and datasets with fewer unique lemmas but more examples per lemma
- Can implicitly learn word segmentation into stem and affix without supervision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dynamic programming algorithm enables exact marginal likelihood computation over all possible generation orderings.
- Mechanism: The DP algorithm computes f[i,j], the probability of having generated prefix y≤i and suffix y≤j, by combining results from smaller subproblems. Each cell depends only on its immediate neighbors, enabling polynomial-time computation of P(y|x) by summing f[i,j] where i+j=|y|.
- Core assumption: The local probabilities depend only on the current prefix and suffix, not on the path taken to reach them.
- Evidence anchors: The paper provides theoretical justification and experimental validation showing DP produces correct results when model architecture satisfies independence requirements.

### Mechanism 2
- Claim: Bidirectional decoding reduces "snowballing" errors by allowing the model to generate uncertain tokens from either direction.
- Mechanism: Instead of being forced to generate tokens left-to-right, the model can choose to generate from the right when it's uncertain about left-side tokens, or vice versa. This flexibility prevents error propagation that occurs when models are forced into a single generation direction.
- Core assumption: The model can effectively evaluate which generation direction is more reliable at each step.
- Evidence anchors: Results show particular improvement on long sequences where error propagation is most problematic.

### Mechanism 3
- Claim: The model architecture supports the independence assumption required for DP through careful design of the decoder inputs.
- Mechanism: The decoder takes prefix and suffix tokens concatenated with special classification tokens as input. This design ensures that the hidden representations for each prefix-suffix pair can be recomputed independently at each timestep, satisfying the DP requirement.
- Core assumption: Cross-attention between prefix and suffix does not violate independence because all hidden states must be recomputed at each timestep.
- Evidence anchors: The paper explicitly states that any model taking prefix and suffix as input and returning corresponding local probabilities is sufficient for DP.

## Foundational Learning

- Concept: Dynamic programming for sequence probability computation
  - Why needed here: The framework requires computing P(y|x) by marginalizing over all possible generation orderings, which is exponential in sequence length without DP.
  - Quick check question: Why can't we simply sum over all 2^|y| orderings directly instead of using dynamic programming?

- Concept: Conditional probability factorization
  - Why needed here: The model needs to compute P(y,o|x) as a product of local probabilities at each generation step, which forms the basis for both the DP algorithm and the training losses.
  - Quick check question: How does the probability factorization in Equation 3 enable the DP recurrence relation in Section 3.2?

- Concept: Cross-entropy vs maximum marginal likelihood training
  - Why needed here: The framework explores two training approaches - cross-entropy with fixed ordering probabilities versus MML that learns ordering probabilities through marginalization.
  - Quick check question: What's the key difference in how xH and MML losses handle the latent ordering variable during training?

## Architecture Onboarding

- Component map: Encoder processes lemma+tags → Decoder takes prefix+suffix+classification tokens → Produces token/join/order probabilities → Dynamic programming table f[i,j] for marginal likelihood
- Critical path: Input encoding → prefix/suffix token preparation → decoder forward pass → probability computation → beam search for P(y,o|x) or DP for P(y|x)
- Design tradeoffs: Cross-attention enables rich prefix-suffix interactions but requires recomputation at each timestep; MML learns ordering but may underperform on datasets with no clear morphological split
- Failure signatures: Model consistently generates in one direction only (degenerate MML behavior); DP algorithm produces NaN values (independence assumption violated); beam search fails to terminate (join probability issues)
- First 3 experiments:
  1. Implement unidirectional baseline (L2R) and verify it reproduces standard transformer results on simple sequence tasks
  2. Add prefix/suffix input mechanism to decoder and verify token generation works in both directions independently
  3. Implement DP algorithm for marginal likelihood computation and verify it matches brute-force enumeration on short sequences

## Open Questions the Paper Calls Out

1. What is the computational complexity of decoding the proposed bidirectional model using marginal probability (P(y|x)) rather than joint probability (P(y,o|x)), and would such a method improve performance?

2. How would the bidirectional framework perform on sequence generation tasks with multiple valid outputs per input, such as machine translation or abstractive summarization?

3. What specific architectural modifications could enable the use of dynamic programming with a causal mask in the bidirectional model?

## Limitations

- The model architecture cannot use causal masking during training because all hidden states must be recomputed at each timestep for dynamic programming to work
- The effectiveness of MML training can degrade on datasets without clear morphological structure where there is no natural split point
- Computational overhead of bidirectional decoding compared to standard approaches is not quantified, leaving questions about scalability

## Confidence

**High Confidence:** The core technical contributions of the bidirectional decoding framework and dynamic programming algorithm are well-defined and theoretically sound. The SIGMORPHON 2022 and 2023 results are clearly presented with specific accuracy improvements over baselines. The architectural requirements for enabling DP (recomputing hidden states at each timestep) are explicitly stated and justified.

**Medium Confidence:** The explanation for why bidirectional decoding outperforms unidirectional approaches (reduced "snowballing" errors) is plausible but not empirically validated. The claim that the model can "implicitly learn the split point of words composed of stem and affix" is supported by results on datasets with morphological structure, but this capability is not directly measured or analyzed.

**Low Confidence:** The comparative advantage of MML versus cross-entropy training is not thoroughly examined. The paper states that MML "can underperform cross-entropy on datasets with no clear morphological split" but provides limited evidence about when and why this occurs. The impact of different ordering distributions (uniform vs. learned) on final performance is not clearly characterized.

## Next Checks

1. **Ablation Study Implementation:** Implement controlled ablations to isolate the contributions of bidirectional decoding versus MML training. Compare L2R+MML, Bidirectional+CE, and other combinations on SIGMORPHON datasets to quantify each component's impact on accuracy.

2. **Generalization Testing:** Apply the bidirectional framework to non-morphological sequence-to-sequence tasks (e.g., machine translation or summarization) to evaluate whether the performance gains transfer to domains without clear morphological structure.

3. **Error Analysis on Long Sequences:** Conduct detailed analysis of model predictions on long sequences to verify the "snowballing" error reduction hypothesis. Compare error patterns between bidirectional and unidirectional models on sequences where errors are most likely to compound.