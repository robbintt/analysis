---
ver: rpa2
title: 'SIG: Speaker Identification in Literature via Prompt-Based Generation'
arxiv_id: '2312.14590'
source_url: https://arxiv.org/abs/2312.14590
tags:
- speaker
- evaluation
- quotation
- task
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SIG is a simple, generation-based approach for speaker identification
  in literary texts that addresses the challenge of non-explicit cases where no speaker
  mentions exist in the surrounding context. The method verbalizes the task and quotation
  input using designed prompt templates, allowing easy integration of auxiliary tasks
  and supporting out-of-domain evaluation.
---

# SIG: Speaker Identification in Literature via Prompt-Based Generation

## Quick Facts
- arXiv ID: 2312.14590
- Source URL: https://arxiv.org/abs/2312.14590
- Reference count: 15
- Primary result: SIG outperforms previous baselines and zero-shot ChatGPT on speaker identification in literary texts, achieving up to 17% improvement on non-explicit scenarios

## Executive Summary
SIG is a prompt-based generation approach for speaker identification in literary texts that addresses non-explicit cases where no speaker mentions exist in the surrounding context. The method verbalizes the task and quotation input using designed prompt templates, allowing easy integration of auxiliary tasks and supporting out-of-domain evaluation. SIG generates predictions either directly or by selecting the highest generation probability among candidate speakers. Empirical results show SIG outperforms previous baselines and zero-shot ChatGPT on the PDNC dataset, achieving up to 17% improvement on non-explicit scenarios, with strong performance also on the WP dataset.

## Method Summary
SIG uses prompt-based generation with BART to identify speakers in literary quotations. The approach converts speaker identification into an open-world classification task where the model generates speaker predictions either directly or by scoring candidate speakers. Prompt templates verbalize the task using natural language, allowing the model to leverage pretraining knowledge. The method supports auxiliary tasks like addressee prediction integrated into the prompt. Training uses teacher-forcing on explicit quotations, and evaluation includes both cross-domain and in-domain testing on PDNC and WP datasets with accuracy metrics.

## Key Results
- SIG achieves up to 17% improvement over previous baselines on non-explicit speaker identification cases
- Outperforms zero-shot ChatGPT on the PDNC dataset across multiple evaluation settings
- Demonstrates strong performance on both PDNC and WP datasets with top-k accuracy metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SIG leverages the internal knowledge of PLMs by aligning task verbalization with pretraining format, reducing the gap between finetuning and pretraining.
- Mechanism: By designing prompt templates that verbalize the speaker identification task using natural language, SIG allows BART to apply its pretraining knowledge of text completion to fill in speaker identities naturally.
- Core assumption: The PLM's pretraining involved similar language patterns (completing sentences, filling masks) that align with identifying speakers in quotations.
- Evidence anchors:
  - [abstract] "converts the task as open-world classification, choosing the speaker according to generation probability, and able to adopt labels in any forms."
  - [section] "minimizes the gap between the pretraining and finetuning for models such as BART, and helps to better leverage the internal knowledge of PLMs."
  - [corpus] No direct corpus evidence; assumption based on general PLM pretraining practices.
- Break condition: If the PLM was not trained on similar cloze-style tasks or the prompt templates do not align well with the model's pretraining, the performance gains would diminish.

### Mechanism 2
- Claim: SIG outperforms traditional pipeline approaches by avoiding error propagation from subtasks like named entity recognition and coreference resolution.
- Mechanism: Instead of using separate models for subtasks and chaining their outputs, SIG uses a single generative model to handle speaker identification directly, eliminating intermediate error accumulation.
- Core assumption: Errors in subtask models (e.g., coreference resolution) compound and degrade final speaker identification performance.
- Evidence anchors:
  - [abstract] "circumvents the inherent drawbacks associated with traditional pipeline approaches comprising multiple sub-tasks."
  - [section] "inherent challenges due to the inevitable error propagation from subtasks that undermines the final performance (Yu, Zhou, and Yu 2022a)."
  - [corpus] No corpus evidence; relies on general NLP pipeline literature.
- Break condition: If the subtasks themselves have very high accuracy, the benefit of avoiding error propagation might be minimal.

### Mechanism 3
- Claim: Integrating auxiliary tasks (e.g., addressee prediction) enhances speaker identification by focusing the model on relevant entity types.
- Mechanism: By extending the prompt template to include addressee prediction, the model learns to distinguish speaker and addressee entities, strengthening its attention to person names in context.
- Core assumption: Predicting addressees is complementary to predicting speakers and reinforces the model's ability to identify relevant person entities.
- Evidence anchors:
  - [abstract] "SIG also introduces auxiliary tasks to capture more implicit information related to speaker identification, such as predicting addressees."
  - [section] "we also investigate incorporating the auxiliary task for speaker identification... which is seamlessly integrated into the designed prompt template."
  - [corpus] No corpus evidence; empirical results in Table 4 show improved accuracy with addressee identification.
- Break condition: If the auxiliary task is not well-aligned with speaker identification, it could introduce noise or confusion rather than benefit.

## Foundational Learning

- Concept: Prompt engineering and task verbalization
  - Why needed here: SIG relies on designing prompts that naturally verbalize the speaker identification task to leverage the PLM's internal knowledge.
  - Quick check question: What is the difference between a prompt template that uses "<mask>" versus one that directly generates the answer without a mask?

- Concept: Open-world classification paradigm
  - Why needed here: SIG can handle speakers not seen during training by generating or scoring any candidate speaker, unlike traditional classification models limited to fixed classes.
  - Quick check question: How does SIG's classification-by-generation approach enable handling unseen speakers during inference?

- Concept: Generation-based vs. classification-based approaches
  - Why needed here: Understanding when to use direct generation versus classification-by-generation (scoring candidates) is crucial for applying SIG effectively.
  - Quick check question: In what scenarios would direct generation be preferable over classification-by-generation, and vice versa?

## Architecture Onboarding

- Component map: Input layer (quotation and context) -> Encoder (BART encoder) -> Decoder (BART decoder) -> Output layer (direct generation or classification-by-generation) -> Auxiliary task module (optional)

- Critical path:
  1. Format input quotation and context with prompt template
  2. Encode input through BART encoder
  3. Decoder generates output based on template (direct generation or classification-by-generation)
  4. Parse and output final speaker prediction

- Design tradeoffs:
  - Direct generation vs. classification-by-generation: Direct generation is simpler but may introduce ambiguity; classification-by-generation is more robust but requires enumerating candidates.
  - Single model vs. pipeline: SIG uses a single model to avoid error propagation but may require more compute during inference for candidate scoring.
  - Auxiliary tasks: Adding tasks like addressee prediction can improve performance but increases model complexity and training time.

- Failure signatures:
  - Direct generation fails: Output does not match any candidate speaker or introduces ambiguity (e.g., "Beaver" instead of "Mrs. Beaver").
  - Classification-by-generation fails: All candidate speakers receive low probabilities, indicating the model struggles with the context.
  - Auxiliary task integration fails: Adding an irrelevant auxiliary task degrades performance.

- First 3 experiments:
  1. Test direct generation on a small set of quotations with known speakers to assess output quality and ambiguity.
  2. Implement classification-by-generation with a fixed candidate list to evaluate scoring accuracy.
  3. Add addressee prediction as an auxiliary task and compare performance with and without it.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SIG's performance scale with increasing number of speaker candidates, particularly in novels with large casts of characters?
- Basis in paper: [inferred] The paper mentions SIG's ability to handle "any forms of candidate input" and evaluates on datasets with different numbers of speakers, but does not systematically study the effect of candidate set size on performance.
- Why unresolved: The paper does not report experiments varying the number of speaker candidates or analyze how performance degrades with larger candidate sets.
- What evidence would resolve it: Experiments showing accuracy as a function of candidate set size, with analysis of whether certain types of candidates (e.g., minor characters) disproportionately affect performance.

### Open Question 2
- Question: How would training SIG with large language models like GPT-3.5/4 compare to using smaller generative models like BART, both in terms of performance and computational efficiency?
- Basis in paper: [explicit] The paper explicitly states that "although this work does not use LLMs for training due to the computational resource constraints, our proposed approach could adopt LLMs in future for further enhancement."
- Why unresolved: The paper uses BART as the base model and only evaluates zero-shot ChatGPT performance, without comparing trained performance across different model scales.
- What evidence would resolve it: Experiments training SIG with various model sizes (BART, OPT, GPT models) measuring both accuracy and training/inference costs.

### Open Question 3
- Question: What is the impact of different prompt template designs on SIG's performance across different types of novels (e.g., detective fiction vs. children's literature)?
- Basis in paper: [explicit] The paper shows that "by judiciously selecting an appropriate pair of prompt templates, the performance receives a substantial increase" and presents ablation studies on prompt templates.
- Why unresolved: While the paper shows template variations improve performance generally, it doesn't analyze whether certain templates work better for specific genres or whether optimal templates vary by literary style.
- What evidence would resolve it: Genre-specific ablation studies comparing multiple prompt templates across different novel categories in the PDNC dataset.

## Limitations

- Generalizability across domains: SIG's effectiveness on non-literary domains or different text genres remains untested
- Prompt template sensitivity: Performance heavily depends on prompt design with small changes potentially causing significant performance variations
- Computational overhead: Classification-by-generation requires enumerating candidate speakers and calculating probabilities for each, which can be expensive for large candidate sets

## Confidence

- Mechanism 1 (Pretraining alignment via prompt verbalization): High
- Mechanism 2 (Avoiding error propagation): Medium
- Mechanism 3 (Auxiliary task integration): Low

## Next Checks

1. Cross-genre evaluation: Test SIG on a different literary corpus (e.g., classic literature, contemporary fiction) or non-literary domain (e.g., news articles, social media posts) to assess generalizability beyond the PDNC and WP datasets.

2. Prompt template ablation study: Systematically vary the prompt templates (e.g., different wording, inclusion/exclusion of masks) and measure the impact on speaker identification accuracy to quantify the sensitivity of SIG to template design.

3. Computational efficiency analysis: Measure and compare the inference time and memory usage of direct generation vs. classification-by-generation modes, especially for large candidate sets, to understand the practical scalability of SIG in real-world applications.