---
ver: rpa2
title: 'SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated Driving
  Systems'
arxiv_id: '2307.15786'
source_url: https://arxiv.org/abs/2307.15786
tags:
- explanations
- saliency
- image
- safe
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating counterfactual
  (CF) explanations for deep neural networks (DNNs) in safety-critical applications
  like automated driving systems. The authors propose a novel method, SAFE, that leverages
  saliency maps to guide a Generative Adversarial Network (GAN) in generating CF examples
  that lie near the decision boundary of the black-box model.
---

# SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated Driving Systems

## Quick Facts
- arXiv ID: 2307.15786
- Source URL: https://arxiv.org/abs/2307.15786
- Authors: 
- Reference count: 40
- One-line primary result: SAFE outperforms state-of-the-art CF explainers, achieving 53% improvement in LPIPS and 55% in FID while maintaining high validity and sparsity

## Executive Summary
This paper addresses the challenge of generating counterfactual (CF) explanations for deep neural networks (DNNs) in safety-critical applications like automated driving systems. The authors propose a novel method, SAFE, that leverages saliency maps to guide a Generative Adversarial Network (GAN) in generating CF examples that lie near the decision boundary of the black-box model. The key innovation is the introduction of a novel loss function term, Lfuse, that ensures the generative model focuses on the most influential features of the input as identified by the saliency map. This approach addresses the limitations of existing CF methods that often generate examples that do not lie near the decision boundary.

## Method Summary
SAFE uses an AttentionGAN architecture trained adversarially to generate counterfactual examples. The method takes a query image, its saliency map (computed via Grad-CAM from a ResNet50 black-box model), and a target label as inputs. The generator produces a counterfactual image while the discriminator distinguishes real from fake images and classifies labels. The novel Lfuse loss term penalizes changes to non-salient regions, forcing the model to focus modifications only where the black-box model is most sensitive. Cycle-consistency loss ensures minimal modifications while maintaining realism. The method is evaluated on the BDD100k dataset using metrics such as validity, sparsity, proximity, and generative metrics like FID and LPIPS.

## Key Results
- SAFE achieves a 53% improvement in LPIPS and a 55% improvement in FID metrics compared to state-of-the-art CF explainers
- The method maintains high validity and sparsity while generating more interpretable and clear CF examples
- Visual inspections demonstrate that SAFE generates counterfactuals that effectively convey explanations comprehensible to humans

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAFE's saliency-guided loss term Lfuse ensures counterfactuals lie near the decision boundary by constraining modifications to high-saliency pixels.
- Mechanism: The generator's loss function includes a term Lfuse = ||(x - x') ⊙ (1 - s)||₁, where s is the saliency map. This term penalizes changes to non-salient regions, forcing the model to focus modifications only where the black-box model is most sensitive.
- Core assumption: Saliency maps accurately identify the most influential features for the model's decision, and modifying only these features yields counterfactuals close to the decision boundary.
- Evidence anchors:
  - [abstract] "The key innovation is the introduction of a novel loss function term, Lfuse, that ensures the generative model focuses on the most influential features of the input as identified by the saliency map."
  - [section] "This term ensures that the saliency map information is fused into the generator model properly. Penalising the generator for modifying the non-salient features, (1 − s), leads the model to perceive the salient region and alter only that region's pixels towards the generation of the CF example."
- Break condition: If saliency maps poorly represent the true decision boundary (e.g., due to gradient saturation or model architecture limitations), then focusing modifications only on high-saliency regions may miss minimal changes that cross the boundary elsewhere.

### Mechanism 2
- Claim: The adversarial training with attention-based GAN architecture enables SAFE to generate diverse and realistic counterfactual examples without requiring paired data.
- Mechanism: AttentionGAN learns to translate between domains (original to counterfactual) by optimizing both generator and discriminator networks. The generator maps (x, s, y') → (x', s'), while the discriminator learns to distinguish real from fake images and classify labels. This unpaired setup avoids the need for expensive ground-truth counterfactuals.
- Core assumption: The attention mechanism effectively captures the spatial correspondence between salient regions and the required modifications for domain translation.
- Evidence anchors:
  - [abstract] "Our approach guides a Generative Adversarial Network based on the most influential features of the input of the black-box model to produce CFs near the decision boundary."
  - [section] "Following the conventional GAN architecture [43], AttentionGAN is composed of two competing modules, i.e., the generator G(·) and the discriminator D(·), iteratively trained to compete against each other in the manner of two-player minimax games."
- Break condition: If the attention mechanism fails to properly align salient regions with target domain features, the generator may produce unrealistic or semantically invalid counterfactuals.

### Mechanism 3
- Claim: The combination of saliency guidance and cycle-consistency loss ensures that generated counterfactuals are both minimal and realistic by constraining the generator to only make necessary modifications.
- Mechanism: The cycle-consistency loss Lrec = ||(x, s)ᵀ - G(G(x, s, y'), y)ᵀ||₁ enforces that transforming an image to a counterfactual and back should reconstruct the original. Combined with Lfuse, this prevents excessive modifications while maintaining realism.
- Core assumption: The reconstruction task forces the generator to learn minimal transformations that still achieve the desired label change, preventing it from making arbitrary or excessive changes.
- Evidence anchors:
  - [abstract] "Our approach guides a Generative Adversarial Network based on the most influential features of the input of the black-box model to produce CFs near the decision boundary."
  - [section] "Since the ground truth of CF examples is unavailable, following the SAcleGAN approach, we transfer back the generated CF example (fake image) into the initial domain and compare it with the query image. The following loss penalises the generator model for adding excessive artifacts to enforce forward and backward consistency."
- Break condition: If the cycle-consistency loss dominates training, the generator may prioritize reconstruction over achieving the target label, reducing validity of counterfactuals.

## Foundational Learning

- Concept: Saliency maps and gradient-based attribution methods
  - Why needed here: Understanding how saliency maps are generated and interpreted is crucial for grasping how SAFE identifies influential features for counterfactual generation.
  - Quick check question: How does Grad-CAM compute saliency maps, and what information do they capture about the model's decision process?

- Concept: Generative Adversarial Networks (GANs) and cycle-consistency
  - Why needed here: SAFE's architecture relies on GAN training with attention mechanisms and cycle-consistency losses to generate realistic counterfactuals without paired data.
  - Quick check question: What is the role of the discriminator in GAN training, and how does cycle-consistency ensure minimal modifications?

- Concept: Counterfactual explanations and decision boundaries
  - Why needed here: Understanding the formal definition of counterfactuals (minimal changes to cross decision boundary) is essential for evaluating SAFE's effectiveness.
  - Quick check question: Why is proximity to the decision boundary important for counterfactual explanations, and how does SAFE ensure this property?

## Architecture Onboarding

- Component map: Input image → ResNet50 black-box model → Grad-CAM saliency map → Generator (G) → Counterfactual output
- Critical path: Input image → black-box model → Grad-CAM saliency map → Generator → Counterfactual output
- Design tradeoffs:
  - Using saliency maps restricts modifications but may miss minimal changes outside salient regions
  - AttentionGAN enables unpaired training but may struggle with complex scene semantics
  - λfuse hyperparameter controls the balance between saliency guidance and other objectives
- Failure signatures:
  - Low validity but high proximity/sparsity: Generator focuses too much on saliency, missing necessary changes
  - High validity but low realism: Generator prioritizes label change over generating realistic images
  - Unstable training: GAN loss oscillates, possibly due to poor hyperparameter tuning or architecture mismatch
- First 3 experiments:
  1. Train with λfuse=0 (no saliency guidance) and compare validity/proximity to baseline to isolate saliency effect
  2. Vary λfuse (0.1, 1, 5, 10) and plot validity vs. proximity trade-off to find optimal balance
  3. Visualize saliency maps and corresponding counterfactual changes to verify that modifications align with high-saliency regions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SAFE compare when applied to other real-world datasets beyond BDD100k?
- Basis in paper: [explicit] The paper suggests validating SAFE's performance with other datasets in the future.
- Why unresolved: The current study only evaluates SAFE on the BDD100k dataset, limiting generalizability.
- What evidence would resolve it: Conducting experiments with SAFE on diverse datasets such as KITTI, Cityscapes, or ImageNet would provide insights into its robustness and applicability across different domains.

### Open Question 2
- Question: What is the impact of different saliency map generation techniques on the quality of counterfactual explanations produced by SAFE?
- Basis in paper: [inferred] The paper uses Grad-CAM for saliency map generation but does not explore alternative methods.
- Why unresolved: The choice of saliency map technique could influence the focus and effectiveness of the generated counterfactuals.
- What evidence would resolve it: Comparing SAFE's performance using different saliency map techniques (e.g., Score-CAM, Integrated Gradients) would reveal the optimal approach for generating high-quality counterfactual explanations.

### Open Question 3
- Question: How does SAFE perform in terms of computational efficiency compared to other state-of-the-art counterfactual explanation methods?
- Basis in paper: [inferred] The paper focuses on the quality of explanations but does not discuss computational efficiency.
- Why unresolved: Real-world applications require methods that are not only accurate but also computationally feasible.
- What evidence would resolve it: Measuring the runtime and resource consumption of SAFE against other methods during training and inference would provide insights into its practical viability.

### Open Question 4
- Question: Can SAFE be extended to handle multi-modal data (e.g., combining images with text or sensor data) for more comprehensive explanations?
- Basis in paper: [inferred] The current implementation focuses solely on image data without considering multi-modal inputs.
- Why unresolved: Many real-world systems, such as autonomous vehicles, utilize multiple data sources for decision-making.
- What evidence would resolve it: Developing and evaluating a multi-modal version of SAFE on datasets that include text descriptions or sensor readings alongside images would demonstrate its capability to generate explanations in complex scenarios.

## Limitations

- The approach's reliance on saliency maps assumes these maps accurately identify the most influential features, which may not always hold true in complex driving scenes.
- Evaluation is conducted on a single dataset (BDD100k) with a specific classification task, limiting generalizability to other driving scenarios or model architectures.
- Visual interpretability assessment relies heavily on qualitative inspection without systematic human evaluation studies to validate comprehensibility to end-users.

## Confidence

- High confidence: The mechanism of using saliency-guided loss (Lfuse) to focus modifications on influential features is technically sound and supported by the mathematical formulation.
- Medium confidence: The 53% improvement in LPIPS and 55% improvement in FID metrics are impressive but require independent verification due to potential hyperparameter sensitivity and the complexity of GAN training.
- Low confidence: The visual interpretability assessment relies heavily on qualitative inspection without systematic human evaluation studies to validate comprehensibility to end-users.

## Next Checks

1. Conduct ablation studies varying λfuse across a wider range (0.01, 0.1, 1, 5, 10) to quantify the sensitivity of validity and proximity metrics to saliency guidance strength.
2. Test the method on additional driving datasets (e.g., nuScenes, Waymo Open Dataset) and classification tasks to assess generalizability beyond the BDD100k "Move Forward"/"Stop/Slow down" scenario.
3. Implement a user study with domain experts to systematically evaluate whether the generated counterfactuals actually improve understanding and trust in the automated driving system compared to baseline methods.