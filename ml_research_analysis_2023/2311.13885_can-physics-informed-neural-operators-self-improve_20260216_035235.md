---
ver: rpa2
title: Can Physics Informed Neural Operators Self Improve?
arxiv_id: '2311.13885'
source_url: https://arxiv.org/abs/2311.13885
tags:
- data
- self-training
- physics
- time
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates self-training techniques for Fourier Neural
  Operators (FNO) in solving partial differential equations (PDEs) without labeled
  data. The authors explore self-training with physics-informed neural operators (PINO)
  to bridge the performance gap between PINO trained with physics loss only and those
  trained with both physics and data loss.
---

# Can Physics Informed Neural Operators Self Improve?

## Quick Facts
- arXiv ID: 2311.13885
- Source URL: https://arxiv.org/abs/2311.13885
- Reference count: 40
- Key outcome: Self-training with pseudo-labels significantly improves PINO performance trained on physics loss only, approaching 1.07x for 1D Burgers and 1.02x for 2D Darcy compared to PINO trained with both physics and data loss

## Executive Summary
This paper investigates self-training techniques for Fourier Neural Operators (FNO) in solving partial differential equations (PDEs) without labeled data. The authors explore self-training with physics-informed neural operators (PINO) to bridge the performance gap between PINO trained with physics loss only and those trained with both physics and data loss. They demonstrate that self-training with pseudo-labels can significantly improve the performance of PINO trained solely with physics loss, approaching 1.07x for the 1D Burgers equation and 1.02x for the 2D Darcy equation compared to PINO trained with both physics and data loss. Additionally, the authors find that pseudo-labels can be used for self-training without necessarily training to convergence in each iteration, enabling self-training schedules that improve upon the baseline performance of PINO in terms of accuracy and time.

## Method Summary
The method employs self-training for FNOs to solve PDEs without labeled data. The process involves training an initial PINO using physics loss only, generating pseudo-labels from this model, and then retraining using both physics loss and supervised loss on the pseudo-labels. This iterative process continues until convergence or a maximum number of iterations. The authors test this approach on 1D Burgers and 2D Darcy equations, using synthetic data generated via Finite Difference Method in spectral domain. Key aspects include early stopping based on loss plateau and exploring different self-training schedules to optimize the trade-off between accuracy and training time.

## Key Results
- Self-training with pseudo-labels significantly improves PINO performance trained on physics loss only
- Pseudo-labels can be used for self-training without necessarily training to convergence in each iteration
- Self-training schedules can be optimized to achieve better accuracy than PINO while using less training time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-training with pseudo-labels closes the performance gap between PINO trained with physics loss only and PINO trained with both physics and data loss.
- Mechanism: The iterative process of generating pseudo-labels from a PINO trained on physics loss and using them as supervised data improves the model's predictive accuracy. Each iteration refines the model by incorporating more accurate pseudo-labels, reducing the error progressively.
- Core assumption: Pseudo-labels generated by a PINO trained with physics loss contain enough correct information to guide the model towards better performance without introducing significant noise.
- Evidence anchors:
  - [abstract] "Specifically, FNOs, when trained exclusively with physics loss through self-training, approach 1.07× for Burgers and 1.02× for Darcy, compared to FNOs trained with both data and physics loss."
  - [section] "We show that self-training can be used to close this gap in performance."
- Break condition: If the pseudo-labels are too noisy due to poor initial training, the model may diverge instead of improving.

### Mechanism 2
- Claim: Self-training can be performed without training to full convergence in each iteration, saving computational time while maintaining or improving accuracy.
- Mechanism: By stopping training early in each iteration based on a fixed number of epochs or a threshold improvement, the model can still generate useful pseudo-labels. This early stopping prevents overfitting to noisy pseudo-labels and reduces overall training time.
- Core assumption: A partially trained model is still capable of generating pseudo-labels that are sufficiently accurate to guide the next iteration.
- Evidence anchors:
  - [abstract] "Furthermore, we discover that pseudo-labels can be used for self-training without necessarily training to convergence in each iteration."
  - [section] "We observe that one can follow an early stopping schedule while training a PINO with physics loss and subsequently with physics + pseudo-labels loss."
- Break condition: If the model is stopped too early, the pseudo-labels may be too inaccurate, leading to error amplification.

### Mechanism 3
- Claim: Self-training schedules can be optimized to achieve better accuracy than PINO while using less training time.
- Mechanism: By tuning the number of epochs per iteration and the number of self-training iterations, one can find a Pareto-optimal point where the model achieves higher accuracy with less computational cost.
- Core assumption: There exists a hyperparameter configuration (number of epochs per iteration) that balances the trade-off between accuracy and training time.
- Evidence anchors:
  - [abstract] "A consequence of this is that we are able to discover self-training schedules that improve upon the baseline performance of PINO in terms of accuracy as well as time."
  - [section] "We observe that some schedules are able to obtain an accuracy greater than PINO while utilizing time lesser than that required to train a PINO to convergence on the basis of physics loss."
- Break condition: If the self-training schedule is not optimized, the model may either take longer to train or fail to improve accuracy.

## Foundational Learning

- Concept: Fourier Neural Operators (FNO)
  - Why needed here: FNO is the base architecture used in this work for learning solution operators to PDEs.
  - Quick check question: What is the primary advantage of using FNO over traditional numerical solvers for PDEs?

- Concept: Physics-Informed Neural Operators (PINO)
  - Why needed here: PINO incorporates physics loss into the training of FNO, allowing it to learn from PDEs without requiring labeled data.
  - Quick check question: How does PINO differ from traditional FNO in terms of training objectives?

- Concept: Self-training
  - Why needed here: Self-training is the semi-supervised learning technique used to iteratively improve the PINO by generating and using pseudo-labels.
  - Quick check question: What is the main benefit of using self-training in the context of PINO?

## Architecture Onboarding

- Component map:
  - Fourier Neural Operator (FNO) -> Physics Loss -> Supervised Loss from Pseudo-labels -> Self-training Loop

- Critical path:
  1. Initialize FNO.
  2. Train FNO using physics loss.
  3. Generate pseudo-labels using the trained FNO.
  4. Retrain FNO using both physics loss and supervised loss on pseudo-labels.
  5. Repeat steps 3-4 until convergence or maximum iterations.

- Design tradeoffs:
  - Accuracy vs. Training Time: More iterations and epochs per iteration can improve accuracy but increase training time.
  - Noise in Pseudo-labels: Early stopping can reduce training time but may introduce noise in pseudo-labels, affecting model performance.

- Failure signatures:
  - Divergence: If pseudo-labels are too noisy, the model may diverge instead of improving.
  - Overfitting: If the model is trained for too many epochs per iteration, it may overfit to noisy pseudo-labels.

- First 3 experiments:
  1. Train PINO with physics loss only and measure the initial performance gap.
  2. Perform one iteration of self-training and observe the improvement in accuracy.
  3. Experiment with different early stopping schedules (e.g., 250, 100, 50 epochs per iteration) to find the optimal trade-off between accuracy and training time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does self-training with pseudo-labels consistently outperform training with ground truth labels across different types of PDEs and neural operator architectures?
- Basis in paper: [explicit] The paper demonstrates significant performance improvements using self-training for FNOs on Burgers and Darcy equations, approaching the performance of models trained with both physics and data loss.
- Why unresolved: The experiments were limited to specific PDEs (1D Burgers and 2D Darcy) and a single neural operator architecture (FNO). It remains unclear if these results generalize to other PDEs or different neural operator architectures.
- What evidence would resolve it: Testing self-training on a broader range of PDEs and neural operator architectures, comparing results against models trained with ground truth labels.

### Open Question 2
- Question: What is the optimal balance between physics loss and supervised loss from pseudo-labels in self-training for PINOs?
- Basis in paper: [inferred] The paper explores self-training schedules that improve accuracy and reduce training time, suggesting that the balance between physics loss and supervised loss is crucial for performance.
- Why unresolved: The paper does not explicitly investigate the impact of different weightings between physics loss and supervised loss from pseudo-labels. The optimal balance may vary depending on the specific PDE and neural operator architecture.
- What evidence would resolve it: Systematic experiments varying the weighting between physics loss and supervised loss from pseudo-labels, analyzing the impact on accuracy and training time for different PDEs and neural operator architectures.

### Open Question 3
- Question: How does the quality of pseudo-labels affect the performance of self-training in PINOs?
- Basis in paper: [explicit] The paper mentions that noisy pseudo-labels can lead to error amplification, highlighting the importance of label quality in self-training.
- Why unresolved: The paper does not explore the relationship between pseudo-label quality and self-training performance. It remains unclear how noisy pseudo-labels impact the convergence and final accuracy of PINOs.
- What evidence would resolve it: Experiments introducing controlled noise into pseudo-labels and analyzing the impact on self-training performance, comparing results against models trained with clean pseudo-labels.

## Limitations
- The self-training approach relies heavily on the quality of pseudo-labels generated by the initial PINO model trained on physics loss alone.
- The paper demonstrates results only on 1D Burgers and 2D Darcy equations, limiting generalizability to more complex PDEs or higher-dimensional problems.
- The early stopping schedules, while showing promise, are not systematically optimized across different PDE types.

## Confidence
- **High Confidence**: The core mechanism of self-training with pseudo-labels to improve PINO performance is well-supported by empirical results showing consistent error reduction across iterations for both test cases.
- **Medium Confidence**: The claim that early stopping during self-training iterations can maintain or improve accuracy while reducing training time is supported by specific examples but lacks systematic ablation studies across different stopping thresholds.
- **Medium Confidence**: The assertion that self-training schedules can outperform PINO in both accuracy and time is demonstrated on the tested PDEs but would benefit from broader validation across diverse problem types.

## Next Checks
1. Implement systematic ablation studies varying the early stopping thresholds across a wider range of values to identify optimal stopping criteria for different PDE types.
2. Test the self-training approach on additional PDE problems (e.g., Navier-Stokes, Allen-Cahn) to evaluate generalizability beyond the current 1D and 2D cases.
3. Conduct sensitivity analysis on the number of self-training iterations and the balance between physics loss and supervised loss to determine optimal training schedules for different problem complexities.