---
ver: rpa2
title: Dialogue Systems Can Generate Appropriate Responses without the Use of Question
  Marks? -- Investigation of the Effects of Question Marks on Dialogue Systems
arxiv_id: '2308.03293'
source_url: https://arxiv.org/abs/2308.03293
tags:
- question
- dialogue
- systems
- marks
- mark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the impact of question marks on dialogue
  systems' ability to generate appropriate responses to user queries. The researchers
  examined how modern dialogue systems, trained on written text, respond to questions
  uttered without a question mark.
---

# Dialogue Systems Can Generate Appropriate Responses without the Use of Question Marks? -- Investigation of the Effects of Question Marks on Dialogue Systems

## Quick Facts
- arXiv ID: 2308.03293
- Source URL: https://arxiv.org/abs/2308.03293
- Reference count: 8
- Primary result: Responses ending with question marks achieve significantly higher accuracy in dialogue systems

## Executive Summary
This study investigates how the presence or absence of question marks affects dialogue systems' ability to generate appropriate responses to user queries. Using Japanese dialogue datasets and four different dialogue systems (including GPT-based and Transformer-based models), researchers found that responses generated with question marks at the end achieved greater accuracy than those without. The study reveals that larger-parameter models can compensate for missing question marks by leveraging contextual information, while smaller models struggle without this explicit punctuation cue. The findings highlight the importance of question marks as linguistic markers in dialogue systems and suggest multimodal approaches may be needed to address the limitations of text-only systems.

## Method Summary
The researchers used three Japanese dialogue datasets (NUCC, DSLC, JED) and four dialogue systems (Transformer-based, GPT-3.5, 40B, and 80B) to generate responses to user utterances. They tested three conditions: utterances ending with question marks, periods, and no punctuation. Human evaluators assessed response appropriateness using a binary good/bad scale, with inter-rater agreement measured using Fleiss' Kappa coefficient. The study compared system performance across these conditions to determine the impact of question marks on response quality.

## Key Results
- Responses ending with question marks achieved significantly higher accuracy than those without punctuation or with periods
- Larger-parameter models (80B and 3.5) performed better at generating appropriate responses even without question marks
- The presence of interrogative particles (e.g., "masuka", "desuka") helped maintain performance when question marks were absent
- Smaller models showed substantial performance degradation when question marks were removed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Question marks serve as explicit linguistic markers that signal to dialogue systems that an utterance is a query requiring a response, and their absence forces the model to infer intent from context or prosodic cues.
- Mechanism: The model's attention mechanism uses the question mark as a strong contextual cue during decoding. Without it, the system must rely on weak contextual clues, which reduces response accuracy.
- Core assumption: The dialogue system's architecture treats punctuation as part of the semantic signal, not just formatting.
- Evidence anchors:
  - [abstract] The study shows that responses generated with a question mark at the end achieved greater accuracy, indicating the significant role of question marks in generating appropriate responses.
  - [section] "It is clear that responses generated with a question mark at the end achieved greater accuracy."
  - [corpus] Weak. No direct corpus evidence that punctuation tokens are attended to differently in the transformer's self-attention.

### Mechanism 2
- Claim: Larger-parameter models (e.g., 80B GPT) can compensate for missing question marks by leveraging richer contextual embeddings and more complex reasoning paths.
- Mechanism: Higher-capacity models have more parameters to model long-range dependencies and subtle contextual signals, allowing them to infer interrogative intent even without explicit punctuation.
- Core assumption: Model capacity correlates with ability to recover implicit meaning from sparse cues.
- Evidence anchors:
  - [section] "The '80B' and the '3.5' systems demonstrated better performance, suggesting that larger parameters allow for the generation of more appropriate responses using dialogue context, even without question marks."
  - [abstract] "The performance of larger-parameter systems was better, suggesting that they can generate more appropriate responses using dialogue context even without question marks."
  - [corpus] Weak. No direct evidence of how parameter scaling changes attention to context versus punctuation.

### Mechanism 3
- Claim: Utterances ending with interrogative particles (e.g., "masuka", "desuka") act as syntactic surrogates for question marks, partially mitigating the absence of explicit punctuation.
- Mechanism: The model's tokenizer and attention layers recognize these particles as markers of interrogative mood, allowing it to generate appropriate responses even without a question mark.
- Core assumption: Japanese interrogative particles are semantically equivalent to question marks in signaling intent to the model.
- Evidence anchors:
  - [section] "Table 2 shows the performance comparison predicated on the impact of interrogative particles... In instances where interrogative particles are present, high performance is maintained irrespective of the punctuation at the sentence’s end."
  - [abstract] "The '80B' and the '3.5' systems demonstrated better performance, suggesting that larger parameters allow for the generation of more appropriate responses using dialogue context, even without question marks."
  - [corpus] Weak. No corpus evidence that interrogative particles are attended to differently from other particles.

## Foundational Learning

- Concept: Self-attention in Transformers
  - Why needed here: The model must learn to attend to question marks and interrogative particles as distinct features for response generation.
  - Quick check question: In a Transformer, how does the model differentiate between a period and a question mark during decoding?

- Concept: Tokenization and subword units
  - Why needed here: Understanding how punctuation is tokenized (as separate tokens or merged) affects how the model learns to use them.
  - Quick check question: In the Japanese tokenizer used here, are question marks and interrogative particles separate tokens?

- Concept: Fine-tuning on dialogue data
  - Why needed here: The model must learn to map dialogue context and punctuation to appropriate response types.
  - Quick check question: What loss function encourages the model to generate questions when the input ends with a question mark?

## Architecture Onboarding

- Component map: Input tokenizer → Transformer encoder (context + user utterance) → Decoder (response generation) → Evaluation (human judgment). Punctuation tokens are embedded and fed into the transformer.
- Critical path: Tokenizer → Encoder attention → Decoder cross-attention → Output generation. Question mark tokens influence the decoder's next-token prediction distribution.
- Design tradeoffs: Use explicit punctuation tokens vs. infer intent from context; larger model size vs. inference cost; multi-modal inputs vs. pure text.
- Failure signatures: Responses that answer declaratively when the user asked a question; low agreement among evaluators; degraded performance on utterances without interrogative particles.
- First 3 experiments:
  1. Ablate question marks from training data and measure response quality drop.
  2. Train a model to predict punctuation from speech transcripts and measure downstream dialogue performance.
  3. Add prosody features to the input and measure if punctuation ablation effects reduce.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different languages, particularly those with distinct grammatical structures, affect the impact of question marks on dialogue system performance?
- Basis in paper: [explicit] The paper mentions that the study used Japanese dialogue data and suggests that findings may be useful for the broader research community, but does not investigate other languages.
- Why unresolved: The study focused exclusively on Japanese, and the impact of question marks on dialogue systems in other languages with different grammatical structures remains unexplored.
- What evidence would resolve it: Comparative studies using dialogue datasets from multiple languages, analyzing the performance of dialogue systems with and without question marks in each language.

### Open Question 2
- Question: What specific types of utterances or linguistic features make it more challenging for dialogue systems to generate appropriate responses without question marks?
- Basis in paper: [explicit] The paper discusses that sentences ending with interrogative particles or containing question-related words like "5W1H" set are more likely to be recognized as questions without question marks, while others may be perceived as declarative sentences.
- Why unresolved: The study provides examples but does not conduct a comprehensive analysis of linguistic features that influence the impact of question marks on response generation.
- What evidence would resolve it: A detailed linguistic analysis of dialogue datasets, identifying specific features or patterns in utterances that affect the system's ability to generate appropriate responses without question marks.

### Open Question 3
- Question: How effective are multimodal approaches, incorporating sound and facial expressions, in addressing the issue of generating appropriate responses without question marks?
- Basis in paper: [explicit] The paper suggests that the development of multimodal response generation using not only text but also sound and facial expressions is needed to address the issue of generating appropriate responses without question marks.
- Why unresolved: The study highlights the potential of multimodal approaches but does not explore their effectiveness in resolving the issue.
- What evidence would resolve it: Empirical studies comparing the performance of dialogue systems using multimodal inputs (text, sound, and facial expressions) versus text-only inputs in generating appropriate responses to utterances without question marks.

## Limitations
- Dataset & Domain Specificity: The study focuses exclusively on Japanese dialogue datasets, limiting generalizability to other languages with different punctuation norms or syntactic structures.
- Human Evaluation Reliability: While Fleiss' Kappa indicates substantial agreement, the evaluation criteria remain underspecified, raising questions about consistency and potential cultural bias.
- Architecture Specificity: The study doesn't isolate which architectural components are responsible for punctuation sensitivity or provide direct mechanistic evidence of how question marks influence attention and decoding.

## Confidence
- High Confidence: The empirical finding that responses ending with question marks achieve greater accuracy than those without, and that larger-parameter models perform better even without question marks.
- Medium Confidence: The proposed mechanisms (punctuation as explicit marker, parameter scaling compensating for missing punctuation, interrogative particles as syntactic surrogates).
- Low Confidence: The broader claim that multimodal response generation is needed to address the question mark issue, as this extends beyond the study's actual findings.

## Next Checks
1. Ablation Study on Punctuation Tokens: Remove question marks entirely from the training data and fine-tune models to measure degradation in response quality. This would directly test whether models learn to attend to punctuation or simply memorize that questions typically have marks.

2. Cross-Lingual Replication: Repeat the experiment with English and other language datasets to determine whether the question mark effect is language-specific or a general phenomenon in dialogue systems. This would test the domain specificity limitation.

3. Multi-Modal Input Experiment: Train models that take both text and prosody features as input, then compare performance on utterances with and without question marks. This would directly test whether multi-modal approaches can mitigate the question mark dependency identified in the study.