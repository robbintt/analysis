---
ver: rpa2
title: Hallucination Augmented Recitations for Language Models
arxiv_id: '2311.07424'
source_url: https://arxiv.org/abs/2311.07424
tags:
- answer
- question
- triviaqa
- document
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Hallucination Augmented Recitations (HAR),
  a pipeline that leverages LLM hallucination to generate counterfactual open book
  QA datasets. HAR prompts an LLM to generate multiple document-answer pairs for a
  given question, then filters out factual answers and non-grounded examples to retain
  high-quality counterfactuals.
---

# Hallucination Augmented Recitations for Language Models

## Quick Facts
- arXiv ID: 2311.07424
- Source URL: https://arxiv.org/abs/2311.07424
- Reference count: 21
- Key outcome: HAR pipeline generates 19K counterfactual examples that improve text grounding by up to 8.0% F1 score versus factual datasets, even with 4x less data and 4x smaller models.

## Executive Summary
This paper proposes Hallucination Augmented Recitations (HAR), a pipeline that leverages LLM hallucination to generate counterfactual open book QA datasets. HAR prompts an LLM to generate multiple document-answer pairs for a given question, then filters out factual answers and non-grounded examples to retain high-quality counterfactuals. The resulting CF-TriviaQA dataset significantly improves text grounding in models, achieving consistent gains across model sizes (60M to 11B) and diverse QA tasks, including multi-hop, biomedical, and adversarial settings.

## Method Summary
HAR uses LLMs to generate document-answer pairs via recitation prompts, then applies two filtering stages: factuality filtering removes pairs where the answer matches the gold answer (even with surface form differences), and attribution filtering removes pairs where the answer is not grounded in the generated document. The pipeline produces CF-TriviaQA with 19K counterfactual examples. T5 models are fine-tuned on TriviaQA, CF-TriviaQA, or combined datasets, then evaluated on out-of-domain open book QA tasks using F1 and exact match scores.

## Key Results
- CF-TriviaQA improves text grounding by up to 8.0% F1 score versus factual datasets
- Performance gains are consistent across model sizes from 60M to 11B parameters
- Smaller LLMs in HAR generate comparable counterfactuals after filtering, showing scalability
- Counterfactual training helps across diverse QA tasks including multi-hop, biomedical, and adversarial settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Counterfactual data improves text grounding by removing reliance on memorized facts.
- **Mechanism**: Factual open book QA datasets can reward models to recall answers from pretraining rather than grounding in the provided context. Counterfactuals conflict with memorized knowledge, forcing reliance on document attribution.
- **Core assumption**: The model has some memorized knowledge that overlaps with the factual dataset, and counterfactual examples can break this memorization pattern.
- **Evidence anchors**:
  - [abstract]: "factual datasets may reward language models to recall facts that they already know from their pretraining data, not attribution."
  - [section 1]: "Since pretrained language models already know facts through their pretraining data, finetuning them with factual open book QA datasets could reward the model to recall the facts from their memory without attribution instead of attributing to the document."
- **Break condition**: If the model lacks sufficient pretraining knowledge to overlap with factual examples, counterfactuals may not provide an advantage.

### Mechanism 2
- **Claim**: Hallucination-augmented recitations generate high-quality counterfactuals that are consistent and grounded.
- **Mechanism**: LLMs are prompted to generate document-answer pairs for given questions. Factuality filtering removes pairs where the answer matches the gold answer (even with surface form differences). Attribution filtering removes pairs where the answer is not grounded in the generated document.
- **Core assumption**: LLMs can be controlled via prompts to generate plausible counterfactual documents, and filtering can effectively remove non-grounded or factual examples.
- **Evidence anchors**:
  - [section 2.1]: Describes recitation generation with prompts and temperature sampling.
  - [section 2.2]: Describes factuality filtering to remove factual answers.
  - [section 2.3]: Describes attribution filtering to remove non-grounded examples.
- **Break condition**: If the LLM cannot generate consistent counterfactuals or the filtering steps are too aggressive, dataset quality degrades.

### Mechanism 3
- **Claim**: Smaller LLMs can generate counterfactuals as effectively as larger ones after filtering.
- **Mechanism**: Smaller LLMs have higher hallucination rates, producing more diverse counterfactuals. After applying the same factuality and attribution filters, the resulting dataset achieves similar performance.
- **Core assumption**: The hallucination rate difference between model sizes is sufficient to offset any quality gap, and filtering can normalize output quality.
- **Evidence anchors**:
  - [section 4]: "We observe that the hallucination rate in the smaller model is much higher than in the larger model...the counterfactual model with CF-TriviaQAPaLM 2-S achieves even slightly better performance than CF-TriviaQAPaLM 2-L."
- **Break condition**: If smaller models generate incoherent counterfactuals that cannot be salvaged by filtering, performance drops.

## Foundational Learning

- **Concept**: Text grounding in QA
  - **Why needed here**: The paper's goal is to improve a model's ability to ground answers in provided documents rather than recall from memory.
  - **Quick check question**: Given a question and a document, how would you determine if the answer is grounded in the document vs. recalled from memory?

- **Concept**: Counterfactual examples in NLP
  - **Why needed here**: Counterfactuals are used to break memorization patterns and force reliance on context, which is central to HAR's approach.
  - **Quick check question**: What distinguishes a counterfactual example from a factual one in the context of open book QA?

- **Concept**: LLM hallucination and filtering
  - **Why needed here**: HAR leverages hallucination to generate counterfactuals and uses filtering to ensure quality; understanding this pipeline is critical.
  - **Quick check question**: How would you design a filter to remove factual answers when surface forms differ (e.g., "10" vs. "ten")?

## Architecture Onboarding

- **Component map**: Recitation generation -> Factuality filter -> Attribution filter -> Dataset assembly
- **Critical path**: 1) Generate document-answer pairs via LLM, 2) Apply factuality filter to remove factual answers, 3) Apply attribution filter to remove non-grounded examples, 4) Assemble final dataset and evaluate
- **Design tradeoffs**: Using LLM-generated data vs. human-annotated (scalable but requires filtering), filtering aggressiveness (too aggressive loses data, too lenient keeps low-quality examples), model size for generation (larger models more coherent but generate fewer counterfactuals)
- **Failure signatures**: Low dataset size after filtering (recitation generation or filtering steps too restrictive), poor out-of-domain performance (counterfactuals not diverse or grounded enough), high factual contamination (factuality filter ineffective)
- **First 3 experiments**: 1) Generate 24 document-answer pairs per TriviaQA question and measure hallucination rate, 2) Apply factuality filter and measure reduction in factual examples, 3) Apply attribution filter and measure improvement in grounding scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of counterfactual data generated by smaller LLMs compare to that generated by larger LLMs when both undergo the same HAR pipeline?
- Basis in paper: [explicit] The paper compares PaLM 2-Small and PaLM 2-Large in generating counterfactual data, finding similar performance despite the smaller model's higher hallucination rate.
- Why unresolved: The study only compared two specific models (PaLM 2-Small vs. PaLM 2-Large). It's unclear how this generalizes to other model sizes or architectures.
- What evidence would resolve it: Systematic comparison of HAR-generated counterfactual data across a range of LLM sizes and architectures, measuring both the quality of generated examples and downstream task performance.

### Open Question 2
- Question: What is the optimal balance between factual and counterfactual data in a training mixture for maximizing text grounding performance?
- Basis in paper: [explicit] The paper shows that combining CF-TriviaQA with TriviaQA achieves good performance, but doesn't explore different mixing ratios.
- Why unresolved: The paper only tests one combination ratio (50/50 split) and doesn't investigate whether different proportions might yield better results.
- What evidence would resolve it: Ablation studies varying the ratio of factual to counterfactual examples in training data, measuring performance across multiple QA benchmarks.

### Open Question 3
- Question: How does the HAR pipeline perform when applied to datasets beyond TriviaQA, particularly in specialized domains like scientific literature or legal documents?
- Basis in paper: [inferred] The paper demonstrates HAR's effectiveness on TriviaQA but doesn't test its generalizability to other domains or document types.
- Why unresolved: The study focuses exclusively on TriviaQA, leaving open questions about HAR's effectiveness with different question styles, domain-specific language, or more complex document structures.
- What evidence would resolve it: Application of HAR to diverse QA datasets (e.g., scientific, legal, medical) followed by evaluation of generated counterfactuals' quality and downstream model performance.

## Limitations
- Core assumption that counterfactuals improve grounding may alternatively be explained by dataset diversity or size effects rather than true counterfactual benefits
- Filtering process relies heavily on LLM-based scoring, introducing potential biases and false positives/negatives that could systematically affect dataset quality
- Paper doesn't sufficiently isolate counterfactual effects from other dataset characteristics

## Confidence
**High confidence**: CF-TriviaQA improves text grounding across diverse QA tasks and model sizes (consistent F1 improvements up to 8.0% across 60M to 11B parameter models and multiple out-of-domain datasets)

**Medium confidence**: The hallucination-based pipeline generates quality counterfactuals (filtering appears effective but reliance on LLM scoring for both generation and validation creates potential circularity concerns)

**Low confidence**: Counterfactuals are the primary driver of improvements versus other factors like dataset diversity or size (paper doesn't sufficiently isolate counterfactual effects)

## Next Checks
1. Ablation on filtering thresholds: Systematically vary factuality and attribution filtering thresholds to quantify their impact on performance gains and determine if current settings are optimal
2. Human evaluation of counterfactual quality: Manually annotate a sample of CF-TriviaQA examples to verify they are truly counterfactual and properly grounded, assessing filter effectiveness
3. Memorization analysis: Compare model performance on factual vs. counterfactual examples from the same underlying questions to directly measure whether counterfactuals reduce reliance on memorization