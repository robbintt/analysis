---
ver: rpa2
title: Block-State Transformers
arxiv_id: '2306.09539'
source_url: https://arxiv.org/abs/2306.09539
tags:
- context
- transformer
- states
- sequence
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Block-State Transformer (BST), a hybrid
  layer that combines State Space Models (SSMs) and Block Transformers to improve
  language modeling on long sequences. The BST layer uses an SSM to provide long-range
  context and a Block Transformer to handle local attention within windows, eliminating
  the need for sequential recurrence and enabling full parallelization.
---

# Block-State Transformers

## Quick Facts
- arXiv ID: 2306.09539
- Source URL: https://arxiv.org/abs/2306.09539
- Reference count: 40
- This paper introduces the Block-State Transformer (BST), a hybrid layer that combines State Space Models (SSMs) and Block Transformers to improve language modeling on long sequences. The BST layer uses an SSM to provide long-range context and a Block Transformer to handle local attention within windows, eliminating the need for sequential recurrence and enabling full parallelization. Three variants (SH, MH, MF) are explored for integrating SSM states into attention. The BST achieves up to 10× speedup over Block-Recurrent Transformers at the layer level and shows improved perplexity on PG19, arXiv, and GitHub datasets compared to similar models. It also generalizes well to longer sequences, with the BST-SH:S4 variant demonstrating the best performance on very long sequences.

## Executive Summary
This paper introduces the Block-State Transformer (BST), a hybrid layer that combines State Space Models (SSMs) and Block Transformers to improve language modeling on long sequences. The BST layer uses an SSM to provide long-range context and a Block Transformer to handle local attention within windows, eliminating the need for sequential recurrence and enabling full parallelization. Three variants (SH, MH, MF) are explored for integrating SSM states into attention. The BST achieves up to 10× speedup over Block-Recurrent Transformers at the layer level and shows improved perplexity on PG19, arXiv, and GitHub datasets compared to similar models. It also generalizes well to longer sequences, with the BST-SH:S4 variant demonstrating the best performance on very long sequences.

## Method Summary
The Block-State Transformer (BST) is a hybrid layer that integrates an SSM sublayer for long-range context generation and a Block Transformer sublayer for local attention within windows. The SSM sublayer uses FFT-based convolutions to process the entire sequence in parallel, producing a context sequence that is split into blocks. The Block Transformer then applies local self-attention and cross-attention to the SSM context. Three variants (SH, MH, MF) are explored for integrating SSM states into attention. The BST achieves parallelization by replacing sequential recurrence with SSM-based context generation, allowing for full parallelization of the hybrid layer. The model is trained using Adam optimizer with specific hyperparameters and layer placements.

## Key Results
- BST achieves up to 10× speedup over Block-Recurrent Transformers at the layer level.
- Improved perplexity on PG19, arXiv, and GitHub datasets compared to similar models.
- Generalizes well to longer sequences, with BST-SH:S4 variant demonstrating the best performance on very long sequences.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Block-State Transformer (BST) achieves parallelization by replacing sequential recurrence with SSM-based context generation.
- Mechanism: SSMs process the full input sequence in parallel via FFT-based convolution, producing a context sequence. This context is split into blocks and fed to Block Transformers that run fully in parallel. By eliminating recurrence, BST avoids the O(LW) bottleneck of Block-Recurrent Transformers.
- Core assumption: The SSM can generate a useful, retrievable context for each block without requiring sequential updates.
- Evidence anchors:
  - [abstract] "completely remove the need for sequential recurrence and we are able to run our hybrid SSM-Transformer layer fully in parallel"
  - [section] "By introducing SSMs as a means of contextualization, we completely remove the need for sequential recurrences and we are able to run our hybrid SSM-Transformer layer fully in parallel."
- Break condition: If the SSM fails to capture long-range dependencies, or if the context states are not sufficiently retrievable, the attention mechanism will underperform and parallelization gains will not translate into quality improvements.

### Mechanism 2
- Claim: The hybrid design of BST leverages both local attention and long-range SSM context to improve language modeling perplexity.
- Mechanism: The SSM sublayer provides a compressed, long-range context representation for each block. The Block Transformer then applies local self-attention within the block and cross-attention to the SSM context, allowing the model to combine local detail with global coherence. This integration outperforms models that interleave SSM and Transformer layers.
- Core assumption: The context states generated by the SSM are informative enough for cross-attention to meaningfully improve token prediction.
- Evidence anchors:
  - [abstract] "we propose an architecture that integrates a strong local attention-based inductive bias with the long-term context modeling abilities of SSMs into a single layer"
  - [section] "In everyBST layer, an SSM takes the entire sequence as input and maps it into a 'context' sequence of the same length. The SSM sublayer takes advantage of FFT-based convolutions."
- Break condition: If the SSM output is too noisy or loses critical information, cross-attention will add little value and perplexity will not improve over pure Transformer or pure SSM baselines.

### Mechanism 3
- Claim: BST generalizes to longer sequences than it was trained on, thanks to the structured parameterization of SSM kernels.
- Mechanism: Structured SSMs like S4 have kernels K that depend only on the SSM parameters (A, B, C) and not on the sequence length L. This allows the same model trained on 4k sequences to be evaluated on 16k or 65k sequences without retraining, because the convolution can be extended.
- Core assumption: The structured SSM kernel remains valid and effective for sequence lengths beyond the training range.
- Evidence anchors:
  - [section] "When using a structured SSM, the computational complexity is closely tied to the internal memory state size of the SSM, N... we set N = 16 when reporting performance."
  - [section] "We show that the structured parametrization of K allows ourBST models to generalize to longer lengths."
- Break condition: If the structured SSM fails to capture dependencies beyond the training length, or if the kernel compilation is not truly length-agnostic, performance will degrade on longer sequences.

## Foundational Learning

- Concept: State Space Models (SSMs) as linear dynamical systems
  - Why needed here: BST relies on SSMs to generate long-range context efficiently; understanding their mathematical foundation is essential to tune and debug the model.
  - Quick check question: What is the form of the SSM recurrence relation and how is it unrolled into a convolution?

- Concept: Fast Fourier Transform (FFT) for parallel sequence processing
  - Why needed here: The speedup of BST over recurrence-based models comes from using FFT to compute convolutions in O(L log L) time.
  - Quick check question: How does FFT enable parallel computation of the SSM output across the entire sequence?

- Concept: Cross-attention and causal masking in Transformer blocks
  - Why needed here: BST uses cross-attention between token embeddings and SSM context; understanding masking ensures the model does not leak future information.
  - Quick check question: Why must future context states be masked out during cross-attention, and how is this implemented?

## Architecture Onboarding

- Component map:
  - Input embeddings → SSM sublayer → Context state collection (SH/MH/MF) → Block Transformer sublayer → Output
  - SSM sublayer: Generates full-sequence context via FFT-based convolution
  - Context state collection: Extracts S states per block for cross-attention
  - Block Transformer: Self-attention + cross-attention + FFN, window size W

- Critical path:
  1. Token embeddings enter SSM sublayer
  2. SSM produces context sequence
  3. Context states extracted per block
  4. Each block processed in parallel by Block Transformer with cross-attention to context
  5. Outputs concatenated and projected

- Design tradeoffs:
  - SH vs MH vs MF: Redundancy vs retrievability vs parameter efficiency
  - Structured (S4) vs unstructured (Hyena-style) SSM kernels: Length generalization vs richer representations
  - Window size W vs embedding dimension D: Local context granularity vs computational cost

- Failure signatures:
  - Perplexity does not improve over pure Transformer: SSM context may be uninformative or cross-attention not helping
  - Slow training: FFT operations or large D causing bottlenecks; check TPU/GPU utilization
  - Poor generalization to long sequences: Using unstructured SSM kernels or context states not capturing long-range info

- First 3 experiments:
  1. Compare BST-SH:S4 with and without the SSM sublayer to confirm cross-attention adds value
  2. Vary window size W (e.g., 128, 256) to find optimal local context size
  3. Test length generalization: evaluate a model trained on 4k on 16k and 65k sequences to confirm structured SSM benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Block-State Transformer perform on tasks beyond language modeling, such as long-range classification tasks like Long Range Arena?
- Basis in paper: [explicit] The authors mention that "despite the perplexity improvements on long-range language modeling tasks, this assumption needs to be tested on other long-range classification tasks such as Long Range Arena."
- Why unresolved: The paper focuses primarily on language modeling tasks and does not provide results on other long-range classification benchmarks.
- What evidence would resolve it: Empirical results comparing BST performance to other models on Long Range Arena or similar long-range classification tasks.

### Open Question 2
- Question: What is the impact of using non-differentiable sequence cache for very long documents in the Block-State Transformer?
- Basis in paper: [explicit] The authors state that "we believe that this is one advantage thatBRecT has over our method, especially for very long examples that split into ordered sequences of length L, since the cache carried from one sequence to the next can provide very useful long-range information and (weak) access to the whole past."
- Why unresolved: The paper does not explore the use of non-differentiable sequence cache in BST, leaving its potential benefits untested.
- What evidence would resolve it: Comparative experiments between BST with and without non-differentiable sequence cache on very long documents.

### Open Question 3
- Question: How does the Block-State Transformer's performance scale with increasing model depth and size?
- Basis in paper: [explicit] The authors mention that "the performance gains that are only due to parallelization made possible by our framework" and discuss the computational efficiency of BST, but do not explore scaling to very deep or large models.
- Why unresolved: The paper focuses on models with a fixed number of layers and parameters, without exploring the effects of scaling.
- What evidence would resolve it: Empirical results showing BST performance as a function of model depth and size, compared to other models.

## Limitations
- Limited ablation comparing different SSM variants (S4 vs unstructured) on the same tasks.
- No exploration of failure cases or scenarios where BST might underperform pure Transformer approaches.
- Incomplete computational complexity analysis - total training time and memory consumption comparisons are missing.

## Confidence
- High confidence: The parallelization mechanism and speedup claims are well-supported by the architecture description and empirical results
- Medium confidence: The perplexity improvements over Block-Recurrent Transformers, though results show consistent gains across datasets
- Medium confidence: The length generalization capability, based on the theoretical argument about structured SSM kernels and limited empirical validation

## Next Checks
1. **Ablation on SSM type**: Train BST models with both S4 and unstructured SSM variants on the same dataset and compare performance, particularly on length generalization tasks
2. **Efficiency benchmarking**: Measure total training time, memory usage, and throughput for BST versus Block-Recurrent Transformers and pure Transformers across different sequence lengths
3. **Failure mode analysis**: Systematically test BST performance degradation when SSM context states are corrupted or when attention mechanisms are disabled to quantify the contribution of each component