---
ver: rpa2
title: Combining pre-trained Vision Transformers and CIDER for Out Of Domain Detection
arxiv_id: '2309.03047'
source_url: https://arxiv.org/abs/2309.03047
tags:
- detection
- cider
- pre-trained
- performance
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically compares out-of-domain (OOD) detection performance
  between pre-trained Vision Transformers (ViTs) and Convolutional Neural Networks
  (CNNs). The study investigates whether pre-trained ViTs naturally excel at OOD detection
  compared to CNNs and whether refinement methods like CIDER can further improve their
  performance.
---

# Combining pre-trained Vision Transformers and CIDER for Out Of Domain Detection

## Quick Facts
- arXiv ID: 2309.03047
- Source URL: https://arxiv.org/abs/2309.03047
- Reference count: 27
- Pre-trained Vision Transformers consistently outperform CNNs in OOD detection tasks

## Executive Summary
This paper investigates the performance of pre-trained Vision Transformers (ViTs) versus Convolutional Neural Networks (CNNs) for Out-Of-Domain (OOD) detection tasks. Through comprehensive experiments on multiple OOD detection methods including Mahalanobis distance, MaxLogit, MaxSoftmax, ODIN, OpenMax, EnergyBased, and KLMatching, the study demonstrates that pre-trained ViTs achieve superior OOD detection performance compared to CNNs across various datasets. The research also evaluates the CIDER refinement method, showing it further improves OOD detection for both architectures, with more pronounced gains for ViTs. The findings suggest that pre-trained ViTs establish a stronger baseline for OOD detection tasks and that CIDER serves as a valuable refinement tool, particularly for transformer-based models.

## Method Summary
The study compares OOD detection performance between pre-trained Vision Transformers and CNNs using seven different detection methods. The approach involves fine-tuning pre-trained models (ViT-B/16 and ResNet18) on the in-domain dataset (CIFAR-10), then applying various OOD detection methods to identify out-of-domain samples from datasets like SVHN, LSUNResize, Textures, TinyImageNetCrop, and CIFAR-100. The CIDER method is applied by adding a projection head to map deep features into hyperspherical embeddings, training with dispersion and compactness losses to improve class separation. Performance is evaluated using AUROC and ACC95TPR metrics across all combinations of architectures, detection methods, and datasets.

## Key Results
- Pre-trained ViTs consistently outperform pre-trained CNNs in OOD detection across most methods and datasets
- Applying CIDER refinement improves OOD detection performance for both pre-trained CNNs and ViTs
- ViTs show more significant performance gains when combined with CIDER compared to CNNs
- The study establishes pre-trained ViTs as a strong baseline for OOD detection tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained Vision Transformers have superior OOD detection performance compared to CNNs due to their attention-based architecture capturing more relevant features for OOD detection.
- Mechanism: The self-attention mechanism in ViTs allows the model to capture global contextual relationships between different regions of an image, leading to richer feature representations that are more discriminative between in-domain and out-of-domain samples.
- Core assumption: The attention mechanism inherently learns features that are more generalizable and better at distinguishing distribution shifts.
- Evidence anchors:
  - [abstract] "Our experiments demonstrate that pre-trained transformers models achieve higher detection performance out of the box."
  - [section] "We observe that, on datasets like SVHN as reported, the OOD detection performance is increased on all metrics when using a pre-trained V iTL16 and further improved when using the CIDER method on it."
  - [corpus] Weak evidence. No direct corpus citations support the attention mechanism claim; only general performance comparisons are present.
- Break condition: If the dataset is simple or the distribution shift is subtle, CNNs with fewer parameters may perform equally well or better due to lower computational overhead and overfitting risk.

### Mechanism 2
- Claim: CIDER improves OOD detection by projecting features into a hyperspherical embedding space where classes are pushed apart while samples from the same class are grouped together.
- Mechanism: The hyperspherical geometry creates a natural margin between classes, making it easier to detect outliers that fall outside the cluster boundaries defined by the class prototypes.
- Core assumption: The geometry of the embedding space is the primary factor enabling better separation between ID and OOD samples.
- Evidence anchors:
  - [section] "CIDER is a recent method in the literature that uses hyperspherical embeddings on which it project the deep representation of samples. These samples are then pushed away from each other depending on their classes while the samples from a same class are regrouped together."
  - [abstract] "We show that pre-trained ViT and CNNs can be combined with refinement methods such as CIDER to improve their OOD detection performance even more."
  - [corpus] Weak evidence. No direct citations in the corpus explicitly validate the hyperspherical embedding claim for CIDER.
- Break condition: If the dataset has high intra-class variance or the OOD samples are semantically similar to in-domain classes, the hyperspherical separation may not be sufficient to distinguish them.

### Mechanism 3
- Claim: Pre-training improves OOD detection because the learned features are more robust and generalizable across different domains.
- Mechanism: During pre-training, models learn rich, general-purpose features that are not overfit to a specific dataset, making them better at recognizing when a sample falls outside the learned distribution.
- Core assumption: Pre-training inherently leads to better feature quality for OOD detection tasks.
- Evidence anchors:
  - [abstract] "Our experiments demonstrate that pre-trained transformers models achieve higher detection performance out of the box."
  - [section] "We observe that, on datasets like SVHN as reported, the OOD detection performance is increased on all metrics when using a pre-trained V iTL16 and further improved when using the CIDER method on it."
  - [corpus] Weak evidence. No direct citations in the corpus explicitly validate the pre-training claim for OOD detection.
- Break condition: If the pre-training dataset is too similar to the in-domain dataset, the model may overfit and lose generalizability, reducing OOD detection performance.

## Foundational Learning

- Concept: Vision Transformers vs. Convolutional Neural Networks
  - Why needed here: Understanding the architectural differences between ViTs and CNNs is crucial to interpreting why one performs better than the other in OOD detection tasks.
  - Quick check question: What is the key architectural difference between ViTs and CNNs that might influence their OOD detection performance?

- Concept: Hyperspherical Embeddings
  - Why needed here: CIDER relies on projecting features into a hyperspherical space, so understanding this geometry is essential to grasp how CIDER improves OOD detection.
  - Quick check question: How does the hyperspherical geometry help in separating in-domain and out-of-domain samples?

- Concept: Out-of-Distribution Detection Metrics
  - Why needed here: OOD detection relies on specific metrics like AUROC and ACC95TPR, so understanding these metrics is necessary to evaluate model performance.
  - Quick check question: What do AUROC and ACC95TPR measure in the context of OOD detection?

## Architecture Onboarding

- Component map:
  - Backbone: Pre-trained CNN (e.g., ResNet) or Vision Transformer (e.g., ViT)
  - Projection Head: Maps features to hyperspherical embeddings (used in CIDER)
  - CIDER Losses: Ldis (dispersion) and Lcomp (compactness) for training
  - OOD Detection Methods: Mahalanobis, MaxLogit, MaxSoftmax, ODIN, OpenMax, EnergyBased, KLMatching
- Critical path:
  1. Load pre-trained backbone
  2. Fine-tune on in-domain dataset (Cifar10)
  3. Apply CIDER (if using) to refine feature space
  4. Evaluate OOD performance using multiple detection methods
- Design tradeoffs:
  - Using pre-trained models vs. training from scratch: Pre-trained models offer better generalization but may require more memory.
  - CIDER vs. no CIDER: CIDER improves OOD detection but adds complexity and computational overhead.
  - CNN vs. ViT: ViTs perform better in OOD detection but are more computationally expensive.
- Failure signatures:
  - Poor OOD detection performance: May indicate insufficient fine-tuning, inappropriate CIDER hyperparameters, or dataset mismatch.
  - High computational cost: May indicate inefficient model choice or CIDER configuration.
- First 3 experiments:
  1. Compare OOD detection performance of pre-trained ResNet vs. ViT on SVHN dataset using MaxLogit method.
  2. Apply CIDER to pre-trained ResNet and evaluate OOD detection performance on LSUNResize dataset.
  3. Apply CIDER to pre-trained ViT and evaluate OOD detection performance on TinyImageNetCrop dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do ViT and CNN architectures differ in their internal feature representations that lead to better OOD detection performance?
- Basis in paper: [explicit] The paper notes that ViTs use self-attention mechanisms while CNNs use convolutional kernels, but does not explore the specific feature representation differences.
- Why unresolved: The study focuses on empirical performance comparison rather than architectural analysis of feature spaces.
- What evidence would resolve it: Detailed analysis of feature space geometry, attention patterns, and learned representations for both architectures on OOD detection tasks.

### Open Question 2
- Question: Can the CIDER method be effectively applied to more complex image datasets and tasks beyond classification?
- Basis in paper: [inferred] The paper mentions that current experiments use simple datasets and suggests future work should explore more complex tasks like image segmentation.
- Why unresolved: Current experiments are limited to simple classification tasks on relatively small datasets.
- What evidence would resolve it: Empirical testing of CIDER on large-scale datasets and diverse computer vision tasks.

### Open Question 3
- Question: What is the specific contribution of pre-training versus architecture to ViT's superior OOD detection performance?
- Basis in paper: [inferred] The paper suggests both pre-training and architecture differences could contribute to ViT's performance, but does not isolate these factors.
- Why unresolved: The study compares pre-trained models but does not include experiments with ViTs trained from scratch or with different pre-training approaches.
- What evidence would resolve it: Controlled experiments comparing pre-trained and randomly initialized ViTs, and ViTs with different pre-training objectives.

## Limitations

- Evaluation relies heavily on synthetic distribution shifts rather than real-world OOD scenarios
- Study only examines ViT-B/16 and ResNet18 architectures, limiting generalizability
- CIDER implementation details (hyperparameters, projection head architecture) are not fully specified
- Does not explore whether performance gains come from ViT architecture or larger parameter count

## Confidence

- High confidence: Pre-trained ViTs outperform pre-trained CNNs on the tested OOD detection methods and datasets
- Medium confidence: CIDER consistently improves OOD detection for both architectures
- Low confidence: The mechanisms proposed for why ViTs excel at OOD detection (attention mechanisms, pre-training effects)

## Next Checks

1. Replicate experiments using ViT-Base vs ResNet50 (matched parameter count) to isolate architecture effects from scale
2. Test on naturally occurring OOD datasets (real-world distribution shifts) rather than synthetic splits
3. Conduct ablation studies removing CIDER's hyperspherical constraints to validate this is the mechanism driving improvement