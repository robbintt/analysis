---
ver: rpa2
title: Conditionally Combining Robot Skills using Large Language Models
arxiv_id: '2310.17019'
source_url: https://arxiv.org/abs/2310.17019
tags:
- task
- plan
- learning
- language
- skills
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Language-World, an extension of the Meta-World
  benchmark that enables large language models (LLMs) to operate in a simulated robotic
  environment using natural language queries and scripted skills. The authors propose
  Plan Conditioned Behavioral Cloning (PCBC), a method that finetunes the behavior
  of high-level plans using end-to-end demonstrations.
---

# Conditionally Combining Robot Skills using Large Language Models

## Quick Facts
- arXiv ID: 2310.17019
- Source URL: https://arxiv.org/abs/2310.17019
- Reference count: 40
- Primary result: PCBC achieves strong few-shot generalization in simulated robotic tasks using LLM-generated plans

## Executive Summary
This paper introduces Language-World, an extension of Meta-World for evaluating large language models in robotic planning and execution. The authors propose Plan Conditioned Behavioral Cloning (PCBC), which uses LLM-generated conditional plans to guide skill execution through end-to-end finetuning from demonstrations. The method demonstrates strong performance in few-shot regimes, often generalizing to new tasks with minimal demonstrations by leveraging structured planning and cross-task co-learning.

## Method Summary
PCBC combines LLM-generated conditional plans with behavioral cloning to enable few-shot generalization in robotic manipulation tasks. The LLM generates a fixed plan mapping conditions to skills once per task, which is then encoded into skill latents. At each timestep, a query evaluation module (QAF) determines which conditions are true and activates corresponding skills. The action decoder blends these skill latents to produce actions. The method uses 1:1 data mixing across base and target tasks to enable single-demonstration generalization, regularizing the model through diverse task demonstrations.

## Key Results
- PCBC achieves strong few-shot generalization, often performing well with as few as 10 demonstrations per task
- The method outperforms descriptor conditioning baselines on MT50-language tasks
- Cross-task co-learning with 1:1 data mixing enables effective single-demonstration generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PCBC enables few-shot generalization by conditioning on language plans rather than raw state descriptors
- Mechanism: LLM generates conditional plan once per task, QAF evaluates conditions at each timestep, action decoder blends skill latents based on active conditions
- Core assumption: Generated plans are semantically coherent and QAF can reliably evaluate conditions
- Evidence anchors: Abstract mentions "strong performance in few-shot regimes, often generalizing to new tasks with as little as a single demonstration"
- Break condition: If LLM generates incoherent plans or QAF cannot evaluate conditions accurately

### Mechanism 2
- Claim: Cross-Task Co-Learning with 1:1 data mixing enables single-demonstration generalization
- Mechanism: Minibatches contain equal numbers of tuples from all base tasks and target task, forcing shared latent space while specializing for target
- Core assumption: Base tasks share sufficient structure with target tasks to regularize learning
- Evidence anchors: Abstract mentions "Using PCBC and Co-Learning are able to achieve strong few-shot generalization results"
- Break condition: If base and target tasks are too dissimilar, regularization may hurt rather than help

### Mechanism 3
- Claim: Natural language queries and scripted skills improve interpretability and control
- Mechanism: LLM produces human-readable plans that can be inspected before use, separating high-level planning from low-level execution
- Core assumption: Query set is expressive enough to cover conditions needed for successful task completion
- Evidence anchors: Abstract mentions "Plan Conditioned Behavioral Cloning (PCBC), which serves as a strong baseline imitation learning method"
- Break condition: If query set is too limited, plans will require unsupported conditions

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and indicator functions
  - Why needed here: Language-World frames tasks as infinite-horizon MDPs with non-Markovian success indicators
  - Quick check question: What distinguishes the success indicator function in Language-World from standard MDP reward functions?

- Concept: Behavioral Cloning and Imitation Learning
  - Why needed here: PCBC extends behavioral cloning by conditioning on plans
  - Quick check question: How does the loss function in PCBC differ from standard behavioral cloning when a plan is involved?

- Concept: Large Language Model prompting and plan formats
  - Why needed here: Generating effective conditional plans requires understanding chain-of-thought prompting
  - Quick check question: Why might markdown-formatted plan prompts work better for GPT-3.5 than plain text?

## Architecture Onboarding

- Component map: LLM -> Conditional Plan Generator -> QAF -> Query Evaluator -> Skill Encoder -> Action Decoder
- Critical path: LLM generates plan → encoded into skill latents → QAF evaluates conditions → attention weights over skills → action decoder blends latents → produces action
- Design tradeoffs:
  - Fixed plan vs. dynamic LLM calls: Fixed plans reduce runtime overhead but risk plan inadequacy
  - QAF vs. VQA: QAF is faster and deterministic but less general than learned VQA model
  - Skill granularity: More skills allow finer control but increase plan complexity and data needs
- Failure signatures:
  - Low success rate across many tasks → likely plan quality or QAF coverage issue
  - High variance between seeds → training instability or insufficient regularization
  - Sudden performance drop after few epochs → overfitting to base tasks
- First 3 experiments:
  1. Run scripted skills with QAF on MT10-language to verify baseline (~90% success)
  2. Generate plans with chain_py format using PaLM2; evaluate zero-shot success on MT50-language
  3. Train PCBC with few-shot data (10 demos per task) and compare to descriptor conditioning baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PCBC performance scale with number of demonstrations beyond few-shot regime?
- Basis in paper: [explicit] Authors mention strong performance at 100 demonstrations but don't explore upper limits
- Why unresolved: Paper focuses on few-shot generalization without performance data at higher demonstration counts
- What evidence would resolve it: Experiments comparing performance with 10, 100, 1000 demonstrations per task

### Open Question 2
- Question: Can PCBC be effectively combined with reinforcement learning methods?
- Basis in paper: [inferred] Authors note RL methods require more data than imitation learning and suggest exploring PCBC with RL
- Why unresolved: No experiments combining PCBC with RL algorithms included
- What evidence would resolve it: Experiments demonstrating PCBC performance when combined with RL methods

### Open Question 3
- Question: How sensitive is PCBC to quality and complexity of conditional plans?
- Basis in paper: [explicit] Authors discuss many generated plans were low quality and improvements could help
- Why unresolved: No systematic evaluation of how different plan qualities affect PCBC performance
- What evidence would resolve it: Experiments comparing PCBC with human-written vs. LLM-generated plans

## Limitations
- Method heavily depends on LLM-generated plan quality, which can be inconsistent
- 1:1 data mixing approach lacks comparison to other few-shot techniques
- "Strong few-shot generalization" claim needs qualification across different task types

## Confidence

**High**: Core architectural components (LLM plan generation, QAF evaluation, PCBC training) are well-defined and technically sound
**Medium**: Few-shot generalization results are promising but may not generalize across all task domains
**Medium**: Interpretability benefits are plausible given plan inspection capability but not empirically validated

## Next Checks

1. Conduct ablation studies removing plan conditioning to quantify PCBC's contribution versus standard behavioral cloning
2. Test plan quality systematically across different LLM models and prompt formats to identify failure modes
3. Evaluate cross-task generalization beyond base-to-target task structure, including completely unseen task types