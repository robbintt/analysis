---
ver: rpa2
title: 'DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness
  for Robust Fovea Localization'
arxiv_id: '2302.06961'
source_url: https://arxiv.org/abs/2302.06961
tags:
- fovea
- images
- features
- localization
- fundus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DualStreamFoveaNet, a dual-stream transformer-based
  architecture for robust fovea localization in retinal images. The method incorporates
  long-range connections and global features using retina and vessel distributions
  through a spatial attention mechanism in the dual-stream encoder.
---

# DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization

## Quick Facts
- arXiv ID: 2302.06961
- Source URL: https://arxiv.org/abs/2302.06961
- Reference count: 40
- Key outcome: Dual-stream transformer achieves state-of-the-art fovea localization with 100% accuracy on Messidor and 69-85% on PALM at standard thresholds

## Executive Summary
This paper proposes DualStreamFoveaNet, a dual-stream transformer-based architecture for robust fovea localization in retinal images. The method incorporates long-range connections and global features using retina and vessel distributions through a spatial attention mechanism in the dual-stream encoder. It reduces computational cost by decreasing token numbers while focusing on vessel-distributed features. Experiments on two public datasets and one large-scale private dataset show state-of-the-art performance, with the method achieving 100% accuracy at 1/2R to 2R evaluation thresholds on Messidor, and 69% and 85% at 1/4R and 1/2R on the more challenging PALM dataset. The architecture demonstrates robustness on both normal and diseased images and superior generalization in cross-dataset experiments.

## Method Summary
DualStreamFoveaNet uses a dual-stream architecture where the main stream processes fundus images through a ResNet34 backbone while the satellite stream processes vessel segmentation maps through a ResNet18 backbone. These streams are fused through four Bilateral Token Incorporation (BTI) modules that incorporate spatial attention mechanisms and token reduction from 1024 to 64 tokens per module. The fused features pass through four RSU blocks in the decoder to produce a segmentation map for fovea localization. The model is trained using Adam optimizer with dice loss and binary cross-entropy over 300 epochs with CosineAnnealingLR learning rate decay.

## Key Results
- Achieves 100% accuracy at 1/2R to 2R evaluation thresholds on Messidor dataset
- Achieves 69% accuracy at 1/4R and 85% at 1/2R on PALM dataset
- Demonstrates superior generalization in cross-dataset experiments compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-stream transformer encoder with TokenLearner reduces computational cost while maintaining accuracy by focusing attention on vessel-distributed features.
- Mechanism: The TokenLearner module uses spatial attention maps to adaptively select informative tokens along vessel distributions rather than processing all 1024 grid-based tokens. This reduces tokens from 1024 to 64 per BTI module, decreasing MHSA computational complexity quadratically.
- Core assumption: Vessel distributions contain sufficient anatomical information for fovea localization, and spatial attention can identify these relevant regions effectively.
- Evidence anchors:
  - [abstract] "We introduce a spatial attention mechanism in the dual-stream encoder to extract and fuse self-learned anatomical information, focusing more on features distributed along blood vessels and significantly reducing computational costs by decreasing token numbers."
  - [section III-B1] "Because of the spatial attention mechanism, the learned tokens are modeled using informative combination of corresponding spatial locations. Compared to the 1024 tokens of ViT and TransUNet [39], [40], we retain only 8× 8 tokens for each BTI module"
  - [corpus] Weak - no direct corpus evidence found for TokenLearner mechanism specifically

### Mechanism 2
- Claim: Multi-cue fusion of fundus images and vessel distribution maps enables robust fovea localization in diseased images where fundus-only approaches fail.
- Mechanism: The dual-stream architecture processes both raw fundus images and vessel segmentation maps in parallel, then fuses their features through BTI modules. This provides complementary information - fundus images capture overall retinal appearance while vessel maps highlight anatomical structure absent near fovea.
- Core assumption: Vessel distribution information remains reliable even when fundus appearance is altered by pathology, and fusion can effectively combine complementary cues.
- Evidence anchors:
  - [abstract] "This architecture explicitly incorporates long-range connections and global features using retina and vessel distributions for robust fovea localization."
  - [section I] "Since the OD and vessels play a role in fovea localization, many existing studies utilize these two anatomical features."
  - [section IV-E1] "Table V demonstrates the results using fundus as the second input (Fundus+Fundus)... Although fundus images contain more detailed information than vessel maps, feeding them into the satellite stream does not provide anatomical structure to guide TokenLeaner."
  - [corpus] Weak - no direct corpus evidence found for this specific dual-stream fusion approach

### Mechanism 3
- Claim: Global feature incorporation through transformer-based fusion outperforms local convolutional approaches for handling non-standard fovea positions and poor lighting conditions.
- Mechanism: The transformer-based BTI modules model long-range dependencies across the entire image rather than relying on fixed local receptive fields of convolutional layers. This enables the network to use global anatomical relationships for localization.
- Core assumption: Fovea location can be inferred from global retinal structure and vessel patterns rather than requiring local landmarks.
- Evidence anchors:
  - [abstract] "This architecture explicitly incorporates long-range connections and global features using retina and vessel distributions for robust fovea localization."
  - [section II-B] "These CNN-based architectures may fail when light conditions and fovea positions are abnormal, or when information on OD and vessels is lacking due to lesions."
  - [section III-A] "Unlike the main stream whose input is a fundus image, a vessel segmentation map generated by a pre-trained model is fed into the satellite stream."
  - [corpus] Weak - no direct corpus evidence found for this specific transformer-based global approach

## Foundational Learning

- Concept: Vision Transformers and Multi-Head Self-Attention (MHSA)
  - Why needed here: The paper relies on transformer architecture for global feature modeling and long-range connections, which is fundamentally different from convolutional approaches
  - Quick check question: How does the computational complexity of MHSA scale with the number of tokens, and why does reducing tokens from 1024 to 64 provide significant efficiency gains?

- Concept: Anatomical relationships in retinal imaging
  - Why needed here: The method exploits the relationship between fovea and blood vessel distribution, which is essential for understanding why the dual-stream approach works
  - Quick check question: What is the approximate anatomical relationship between fovea location and major blood vessel patterns in normal retinas?

- Concept: TokenLearner and spatial attention mechanisms
  - Why needed here: The paper introduces TokenLearner for adaptive token selection, which is central to the proposed efficiency improvements
  - Quick check question: How does the spatial attention mechanism in TokenLearner differ from standard self-attention, and what advantages does it provide for feature selection?

## Architecture Onboarding

- Component map: Input fundus images and vessel maps → Main stream (ResNet34) and Satellite stream (ResNet18) → Dual-stream encoder (4 BTI modules) → RSU decoder (4 blocks) → Output segmentation map
- Critical path: Input → Dual-stream encoder (with BTI fusion) → RSU decoder → Output segmentation map
- Design tradeoffs:
  - Dual-stream vs single-stream: Added complexity for improved robustness to pathology
  - Token reduction (1024→64): Significant computational savings but potential information loss risk
  - Transformer vs CNN: Global modeling capability vs established convolutional approaches
- Failure signatures:
  - Poor localization accuracy on severely diseased images where vessel information is corrupted
  - Reduced performance when TokenLearner fails to identify relevant anatomical features
  - Computational bottlenecks if MHSA layers are not properly optimized
- First 3 experiments:
  1. Validate vessel segmentation quality on test dataset to ensure satellite stream has reliable input
  2. Test fovea localization performance on normal vs diseased images to confirm robustness claims
  3. Compare computational cost (FLOPs) and accuracy against baseline transformer and CNN approaches

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several significant uncertainties remain based on the presented work. The method demonstrates strong performance on standard datasets but lacks detailed analysis of token behavior across different retinal conditions, systematic evaluation of token number optimization for different resolutions, and comprehensive testing on various retinal imaging modalities beyond traditional fundus photography.

## Limitations
- TokenLearner module implementation details are not fully specified, making exact replication difficult
- No comparison against state-of-the-art fovea localization methods that use both anatomical landmarks and deep learning
- Limited evaluation on extremely challenging cases with severe pathology or poor image quality

## Confidence
- **High confidence**: The architectural design and basic training methodology are clearly specified. The experimental results on Messidor dataset appear robust and well-documented.
- **Medium confidence**: The claimed computational efficiency gains through TokenLearner reduction are plausible but lack detailed ablation studies showing the impact of token reduction on accuracy.
- **Low confidence**: The cross-dataset generalization claims are supported by limited experiments, and the method's performance on severely diseased images with extensive lesions is not thoroughly evaluated.

## Next Checks
1. **Implement and test TokenLearner module**: Create a minimal working implementation of the spatial attention-based TokenLearner to verify the claimed computational efficiency gains and validate the mechanism for selecting informative tokens along vessel distributions.
2. **Conduct comprehensive ablation studies**: Systematically evaluate the impact of vessel stream inclusion, token reduction from 1024 to 64, and transformer-based fusion on both accuracy and computational cost across multiple datasets.
3. **Test on severely diseased images**: Evaluate performance on retinal images with extensive pathologies (severe diabetic retinopathy, age-related macular degeneration) where vessel information may be severely degraded or absent.