---
ver: rpa2
title: Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive
  Demonstrations
arxiv_id: '2311.09763'
source_url: https://arxiv.org/abs/2311.09763
tags:
- demonstrations
- defense
- backdoor
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of defending against backdoor
  attacks in black-box large language models (LLMs) at test time, where training-time
  defenses are not feasible. The proposed method, defensive demonstrations, leverages
  the in-context learning capabilities of LLMs by retrieving task-relevant, clean
  demonstrations from a demonstration pool and integrating them with user queries
  during testing.
---

# Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations

## Quick Facts
- arXiv ID: 2311.09763
- Source URL: https://arxiv.org/abs/2311.09763
- Reference count: 26
- Key outcome: Defensive demonstrations effectively mitigate backdoor attacks at test time with up to 99.8% reduction in attack success rate while maintaining high clean label accuracy.

## Executive Summary
This paper addresses the challenge of defending against backdoor attacks in black-box large language models (LLMs) at test time, where training-time defenses are not feasible. The proposed method, defensive demonstrations, leverages the in-context learning capabilities of LLMs by retrieving task-relevant, clean demonstrations from a demonstration pool and integrating them with user queries during testing. This approach does not require modifying the model or accessing its internal architecture. Experimental results show that defensive demonstrations effectively mitigate both instance-level and instruction-level backdoor attacks, achieving significant reductions in attack success rates (ASR) while maintaining high clean label accuracy (CACC).

## Method Summary
The defensive demonstrations method works by retrieving task-relevant, clean demonstrations from a demonstration pool and integrating them with user queries during testing. Three retrieval methods are evaluated: random sampling, similar sample retrieval using embedding models, and self-reasoning demonstrations that include explicit reasoning paths. The approach exploits LLMs' in-context learning capabilities to override backdoor triggers by providing correct task context during inference. The method is tested across three datasets (SST-2, Tweet Emotion, Trec-coarse) using two LLM backbones (7B Llama2, Flan-T5-large) and four attack methods (BadNet, AddSent, Style, Syntactic).

## Key Results
- Defensive demonstrations reduce attack success rates from 100% to as low as 0.2% in certain scenarios
- Self-reasoning demonstrations achieve the highest defense performance across all attack types
- The method maintains high clean label accuracy (>95%) while effectively mitigating backdoor attacks
- Demonstration shuffling helps mitigate recency bias in model predictions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Few-shot demonstrations provide correct context that overrides backdoor triggers during inference
- **Mechanism**: When a poisoned model encounters a trigger-containing query, defensive demonstrations provide clean, task-relevant examples that guide the model's attention toward the actual semantic content rather than the trigger. The in-context learning capability allows the model to align with the correct task understanding demonstrated in the examples.
- **Core assumption**: The model's in-context learning mechanism is strong enough to overcome the backdoor behavior learned during training when presented with clean demonstrations.
- **Evidence anchors**:
  - [abstract]: "The alignment properties inherent in in-context learning play a pivotal role in mitigating the impact of backdoor triggers, effectively recalibrating the behavior of compromised models"
  - [section]: "Learning from contextual demonstrations, models can produce faithful inferences and mitigate the negative impact of triggers, regardless of how concealed are the triggers"
  - [corpus]: Weak - related work focuses on training-time defenses rather than test-time mechanisms
- **Break condition**: If the model's in-context learning capability is insufficient relative to the strength of the backdoor poisoning, or if the demonstrations cannot be retrieved accurately for the task.

### Mechanism 2
- **Claim**: Self-reasoning demonstrations are most effective because they provide explicit reasoning paths that models can follow
- **Mechanism**: By including rationales in demonstrations (input, reasoning, label), models are prompted to articulate their reasoning process, which helps them access and utilize knowledge acquired during pretraining. This self-reasoning process empowers models to become more resilient against malicious triggers while retaining a strong grasp of the original meaning.
- **Core assumption**: LLMs can effectively leverage chain-of-thought reasoning when prompted, and this reasoning process helps counteract backdoor triggers.
- **Evidence anchors**:
  - [section]: "This addition enables the model to provide both results and reasons for its predictions, resulting in the highest level of defense performance"
  - [section]: "prompting the model to articulate its reasoning process enables it to tap into and utilize relevant knowledge acquired during pretraining"
  - [corpus]: Weak - related work doesn't directly address the specific mechanism of self-reasoning in backdoor defense
- **Break condition**: If the model cannot effectively generate or follow reasoning paths, or if the reasoning process itself becomes corrupted by the backdoor.

### Mechanism 3
- **Claim**: Demonstration ordering and variety prevent recency bias and model fixation on specific classes
- **Mechanism**: Shuffling demonstrations mitigates "recency bias" where models develop bias toward classes presented at the end. Random sampling provides variety that helps generalization, while similar samples retrieval aims to provide semantically close examples for better task understanding.
- **Core assumption**: The order and similarity of demonstrations significantly impact the model's ability to learn the correct task representation.
- **Evidence anchors**:
  - [section]: "shuffling helps mitigate 'recency bias' (Zhao et al., 2021), a phenomenon where a model develops a bias towards a particular class if it is repeatedly presented towards the end of the demonstrations"
  - [section]: "it was unexpected to find that the similar method underperforms compared to random in several instances"
  - [corpus]: Weak - limited discussion of demonstration ordering in related work
- **Break condition**: If demonstration ordering has minimal impact on the specific model architecture or if the model's attention mechanism is robust to ordering effects.

## Foundational Learning

- **Concept**: In-context learning
  - Why needed here: The defense mechanism relies entirely on the model's ability to learn from few-shot demonstrations without parameter updates
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and why is it particularly relevant for black-box LLM defense?

- **Concept**: Backdoor attack mechanisms
  - Why needed here: Understanding how backdoor triggers work (explicit vs. implicit, instance-level vs. instruction-level) is crucial for designing effective defenses
  - Quick check question: What are the key differences between explicit triggers (words, sentences) and implicit triggers (syntax, style), and how do they affect defense strategies?

- **Concept**: Retrieval methods and embeddings
  - Why needed here: The effectiveness of defensive demonstrations depends on retrieving relevant, clean examples from a demonstration pool
  - Quick check question: How do different embedding models (SimCSE, Sentence-BERT, etc.) affect the quality of demonstration retrieval?

## Architecture Onboarding

- **Component map**: User Query -> Demonstration Retrieval System -> Augmented Prompt -> Black-Box LLM -> Output
- **Critical path**: 
  1. Identify task from user query
  2. Retrieve task-relevant demonstrations from clean pool
  3. Augment query with demonstrations
  4. Forward to black-box LLM
  5. Return results to user
- **Design tradeoffs**:
  - Demonstration length vs. inference cost
  - Random vs. similar vs. self-reasoning retrieval methods
  - Number of shots (k) vs. defense effectiveness
  - Prompt engineering complexity vs. defense robustness
- **Failure signatures**:
  - ASR remains high despite defense (demonstrations ineffective)
  - CACC drops significantly (defense interferes with clean predictions)
  - Retrieval system fails to find relevant demonstrations
  - Model ignores demonstrations and follows backdoor triggers
- **First 3 experiments**:
  1. Test defense effectiveness on a simple binary classification task with known backdoor triggers
  2. Compare random, similar, and self-reasoning demonstration methods on the same poisoned model
  3. Evaluate impact of demonstration ordering (shuffled vs. class-ordered) on defense performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided text.

## Limitations

- The approach requires access to a clean demonstration pool, which may not always be available or may be domain-specific
- Performance depends on accurate task identification for demonstration retrieval, which could fail for ambiguous or novel queries
- The defense may introduce latency due to demonstration retrieval and prompt augmentation steps

## Confidence

- **High Confidence**: The basic mechanism of using clean demonstrations to override backdoor triggers during inference is well-supported by the empirical results showing significant ASR reduction across multiple attack types.
- **Medium Confidence**: The superiority of self-reasoning demonstrations is demonstrated but may be context-dependent, as effectiveness could vary based on task complexity and model architecture.
- **Low Confidence**: The generalizability of results across different poisoning rates, trigger types, and model sizes remains uncertain, particularly for extreme poisoning scenarios (>50%) or highly implicit triggers.

## Next Checks

1. Test defense effectiveness on models with varying in-context learning capabilities (different model sizes and architectures) to establish robustness bounds
2. Evaluate performance under different poisoning rates (5%, 25%, 50, 75%) to understand scalability limits
3. Assess defense against adaptive attacks where triggers are designed to evade demonstration-based mitigation