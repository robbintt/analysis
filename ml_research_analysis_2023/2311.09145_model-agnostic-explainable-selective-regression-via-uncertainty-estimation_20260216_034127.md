---
ver: rpa2
title: Model Agnostic Explainable Selective Regression via Uncertainty Estimation
arxiv_id: '2311.09145'
source_url: https://arxiv.org/abs/2311.09145
tags:
- selective
- https
- regression
- pmlb
- epistasislab
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses selective regression, allowing ML models to
  abstain from prediction when uncertain, focusing on tabular datasets. The core method,
  DOUBT INT and DOUBT VAR, uses model-agnostic bootstrap uncertainty estimation (variance
  or prediction interval width) as a confidence function, thresholding it to decide
  rejection.
---

# Model Agnostic Explainable Selective Regression via Uncertainty Estimation

## Quick Facts
- arXiv ID: 2311.09145
- Source URL: https://arxiv.org/abs/2311.09145
- Reference count: 13
- Key outcome: DOUBT INT and DOUBT VAR achieve ~45% MSE reduction at 50% coverage on 69 tabular datasets, outperforming baselines.

## Executive Summary
This paper introduces DOUBT INT and DOUBT VAR, model-agnostic selective regression methods that use bootstrap-based uncertainty estimation (variance or prediction interval width) as confidence functions to decide when to abstain from prediction. Evaluated on 69 tabular datasets with XGBoost, the methods achieve superior performance: average MSE reduction of ~45% at 50% coverage, significantly outperforming baselines like PLUG IN, SCROSS, and MAPIE. The paper also introduces an explainable selective regression framework using Shapley values to identify which features drive rejection decisions, validated on the House Prices dataset.

## Method Summary
The DOUBT framework generates multiple bootstrap samples, trains regression models on each, and uses the variance or prediction interval width of the bootstrap prediction distribution as a confidence function. A threshold is calibrated on a validation set to achieve target coverage levels. For explainability, a binary classifier is trained to predict the accept/reject decisions of the original regressor, and Shapley values are computed to attribute feature importance to rejection decisions.

## Key Results
- DOUBT INT and VAR achieve ~45% MSE reduction at 50% coverage across 69 tabular datasets
- Coverage constraints are satisfied in ~90% of experiments
- Shapley value analysis reveals that rejecting on certain features leads to significant MSE improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bootstrap-based variance estimation provides a model-agnostic confidence function for selective regression.
- Mechanism: By generating multiple bootstrap samples, we estimate the distribution of prediction errors, and use the variance of these errors as a proxy for uncertainty. This variance is then thresholded to decide rejection.
- Core assumption: The distribution of prediction errors converges to the true error distribution as the number of bootstrap samples increases.
- Evidence anchors:
  - [abstract] "Our proposed framework showcases superior performance compared to state-of-the-art selective regressors, as demonstrated through comprehensive benchmarking on 69 datasets."
  - [section 3.2] "we focus on applying uncertainty to selective regression...we extend existing model-agnostic uncertainty frameworks to calculate a confidence function for cases where we abstain from making predictions"
  - [corpus] No direct evidence in corpus; the corpus papers focus on conformal prediction or selective classification, not bootstrap-based variance estimation for regression.
- Break condition: If the underlying regression model is not stable across bootstrap samples, the variance estimate will be unreliable.

### Mechanism 2
- Claim: Using the variance of the bootstrap prediction distribution as a confidence function outperforms using the width of the prediction interval.
- Mechanism: The variance directly measures the spread of the prediction distribution, which is a more direct measure of uncertainty than the width of the interval, which can be influenced by the choice of quantile.
- Core assumption: The variance of the bootstrap prediction distribution is a better proxy for uncertainty than the width of the prediction interval.
- Evidence anchors:
  - [section 3.2] "we also consider a slight variation of DOUBT INT by directly using the variance of C(X) rather than the width of the intervals. We refer to this modification as DOUBT VAR."
  - [section 4.2] "DOUBT VAR and DOUBT INT are ranked first and second, respectively, with statistically significant differences from the other baselines"
  - [corpus] No direct evidence in corpus; the corpus papers do not compare variance vs. interval width for selective regression.
- Break condition: If the prediction distribution is highly skewed, the variance may not be the best measure of uncertainty.

### Mechanism 3
- Claim: Shapley values applied to a binary classifier predicting acceptance/rejection can explain which features drive rejection decisions.
- Mechanism: By training a classifier to predict the accept/reject decisions of the original regressor, we can use Shapley values to attribute the contribution of each feature to the rejection decision.
- Core assumption: The binary classifier trained to predict accept/reject decisions captures the relevant features that drive rejection in the original regressor.
- Evidence anchors:
  - [abstract] "Finally, we use explainable AI techniques to gain an understanding of the drivers behind selective regression."
  - [section 3.3] "we propose to solve this open issue by training a classifier to predict the accept/reject decisions of the original regressor model, after which we can perform a Shapley value analysis of the classifier"
  - [corpus] No direct evidence in corpus; the corpus papers do not discuss Shapley values for explaining rejection decisions in selective regression.
- Break condition: If the binary classifier does not accurately capture the decision boundary of the original regressor, the Shapley values will not be meaningful.

## Foundational Learning

- Concept: Bootstrap sampling and its convergence properties
  - Why needed here: The method relies on bootstrap sampling to estimate the distribution of prediction errors. Understanding the convergence properties of bootstrap is crucial to justify the use of variance as a confidence function.
  - Quick check question: What is the asymptotic distribution of the bootstrap variance estimator for a regression model?

- Concept: Conformal prediction and its relationship to selective regression
  - Why needed here: Conformal prediction is a related approach to uncertainty estimation, and understanding its strengths and weaknesses compared to the bootstrap approach is important for evaluating the proposed method.
  - Quick check question: How does the coverage guarantee of conformal prediction compare to the coverage guarantee of the proposed bootstrap-based method?

- Concept: Shapley values and their application to model interpretability
  - Why needed here: The method uses Shapley values to explain rejection decisions. Understanding the properties of Shapley values and their limitations is crucial for interpreting the results.
  - Quick check question: What are the axioms that Shapley values satisfy, and how do they relate to the concept of fairness in feature attribution?

## Architecture Onboarding

- Component map: Bootstrap sampling -> Variance estimation -> Threshold calibration -> Selective prediction
- Critical path: Bootstrap sampling -> Variance estimation -> Threshold calibration -> Selective prediction
- Design tradeoffs: Model-agnostic approach vs. model-specific uncertainty estimation; variance vs. interval width as confidence function; Shapley value analysis vs. other interpretability methods.
- Failure signatures: Poor coverage (over-rejection or under-rejection); high variance in bootstrap predictions; inaccurate Shapley value attributions.
- First 3 experiments:
  1. Evaluate the coverage of the proposed method on a synthetic dataset with known uncertainty.
  2. Compare the performance of DOUBT VAR and DOUBT INT on a real-world dataset.
  3. Analyze the Shapley values for a specific instance to understand which features drive rejection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DOUBT INT and DOUBT VAR compare when using deep learning models instead of tree-based methods on tabular data?
- Basis in paper: [inferred] The paper evaluates performance on tabular datasets using XGBoost and other tree-based methods, but does not explore deep learning models.
- Why unresolved: The paper's experiments are confined to non-deep learning methods to ensure faster training times, leaving the deep learning comparison unexplored.
- What evidence would resolve it: Conducting experiments using DOUBT INT and DOUBT VAR with deep learning models like neural networks on the same tabular datasets and comparing the results with the current tree-based methods.

### Open Question 2
- Question: What is the impact of different feature selection techniques on the performance of the selective regression methods?
- Basis in paper: [inferred] The paper uses one-hot encoding for categorical variables and Min-Max normalization but does not discuss feature selection techniques.
- Why unresolved: The paper does not address how different feature selection methods might affect the performance of selective regression methods.
- What evidence would resolve it: Running experiments with various feature selection techniques (e.g., Lasso, Ridge, mutual information) and analyzing their impact on the performance metrics like âˆ†MSE and CovSat for the selective regression methods.

### Open Question 3
- Question: How do DOUBT INT and DOUBT VAR perform on datasets with sizes outside the 100 < n < 100,000 range?
- Basis in paper: [explicit] The paper explicitly states that the analysis is confined to datasets with sizes ranging from 100 < n < 100,000.
- Why unresolved: The paper does not explore the performance of the methods on datasets smaller than 100 or larger than 100,000 instances.
- What evidence would resolve it: Extending the experiments to include datasets with fewer than 100 instances and more than 100,000 instances, and evaluating the performance of DOUBT INT and DOUBT VAR on these datasets.

## Limitations

- The method's effectiveness relies heavily on the stability of bootstrap samples and the assumption that variance adequately captures prediction uncertainty.
- The Shapley-based explanation framework depends on the binary classifier accurately capturing rejection patterns, which may not hold for complex decision boundaries.
- The current evaluation focuses exclusively on tabular data, limiting generalizability to other data modalities.

## Confidence

- High confidence: Coverage satisfaction metrics and MSE improvements are directly measured and consistently reported across multiple datasets.
- Medium confidence: Superiority claims over baselines are supported by statistical tests but depend on specific hyperparameter choices and dataset selection.
- Medium confidence: Explainability results are methodologically sound but rely on the validity of the audit classifier approximation.

## Next Checks

1. **Robustness to prediction distribution shape**: Systematically evaluate DOUBT VAR vs. DOUBT INT on synthetic datasets with varying skewness and kurtosis to determine when variance fails as an uncertainty proxy.

2. **Cross-dataset generalizability**: Apply the method to non-tabular datasets (image regression, time series) to assess whether the bootstrap-variance framework transfers beyond the evaluated domain.

3. **Ablation of bootstrap parameters**: Vary the number of bootstrap samples and replacement strategy to quantify their impact on coverage and MSE, particularly for small datasets where bootstrap instability is most pronounced.