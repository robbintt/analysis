---
ver: rpa2
title: 'Developing a Multilingual Dataset and Evaluation Metrics for Code-Switching:
  A Focus on Hong Kong''s Polylingual Dynamics'
arxiv_id: '2310.17953'
source_url: https://arxiv.org/abs/2310.17953
tags:
- speech
- recognition
- dataset
- audio
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of high-quality multilingual audio
  datasets for code-switching speech recognition, particularly for mixed Cantonese
  and English (MCE) audio in Hong Kong. The authors developed a 34.8-hour MCE dataset
  using their Multi-Agent Data Generation Framework (MADGF) and GPT-4 to generate
  text data, which was then recorded by human speakers.
---

# Developing a Multilingual Dataset and Evaluation Metrics for Code-Switching: A Focus on Hong Kong's Polylingual Dynamics

## Quick Facts
- arXiv ID: 2310.17953
- Source URL: https://arxiv.org/abs/2310.17953
- Reference count: 31
- Key outcome: 34.8-hour MCE dataset developed using MADGF framework with GPT-4, fine-tuned Whisper model, and novel FAL evaluation metric showing improved performance over baseline

## Executive Summary
This paper addresses the critical shortage of high-quality multilingual audio datasets for code-switching speech recognition, specifically focusing on mixed Cantonese and English (MCE) audio in Hong Kong. The authors developed a novel Multi-Agent Data Generation Framework (MADGF) that leverages GPT-4 to generate mixed-language text data, which is then recorded by human speakers to create a 34.8-hour MCE dataset. They fine-tuned the Whisper multilingual ASR model with this dataset and introduced a novel Fidelity to the Original Audio, Accuracy, and Latency (FAL) evaluation metric to address limitations of traditional metrics like WER.

The experimental results demonstrate that the Whisper-MCE model significantly outperforms the baseline Whisper-large-v2 model in terms of fidelity, accuracy, and latency. The FAL metric provides a more comprehensive assessment for code-switching scenarios by incorporating latency alongside fidelity and accuracy. This work represents a significant advancement in multilingual ASR research, particularly for low-resource code-switching scenarios where existing datasets and evaluation metrics fall short.

## Method Summary
The authors employed GPT-4 within their Multi-Agent Data Generation Framework (MADGF) to generate mixed Cantonese-English text data covering 18 daily topics. Human speakers recorded this text to create a 34.8-hour MCE audio dataset. The open-source Whisper-small model was fine-tuned on this dataset using standard procedures. A novel FAL metric was introduced to evaluate code-switching ASR performance, combining fidelity (0.4), accuracy (0.4), and latency (0.2) into a single weighted score. The fine-tuned model was compared against baseline Whisper-large-v2 and other ASR models trained on monolingual Cantonese datasets.

## Key Results
- Whisper-MCE model outperformed baseline Whisper-large-v2 in fidelity, accuracy, and latency metrics
- FAL metric provided more comprehensive evaluation than traditional WER for code-switching scenarios
- 34.8-hour MCE dataset successfully captured diverse code-switching patterns across 18 topics
- GPT-4 effectively generated linguistically diverse mixed-language text data for dataset creation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MADGF framework enables scalable generation of linguistically diverse code-switching text data
- Mechanism: GPT-4 generates mixed-language sentences using iterative topic-based prompts, then human speakers record them, creating a balanced dataset that captures real-world code-switching patterns
- Core assumption: GPT-4 can generate grammatically correct and contextually appropriate mixed Cantonese-English text when provided with topic-specific examples
- Evidence anchors: [abstract] "developed a 34.8-hour dataset of Mixed Cantonese and English (MCE) audio using our Multi-Agent Data Generation Framework (MADGF)"; [section 3.1] "we employed GPT-4 to generate a combined dataset consisting of text in both Cantonese and English"
- Break condition: GPT-4 fails to maintain linguistic coherence in mixed-language sentences or produces unnatural code-switching patterns

### Mechanism 2
- Claim: The FAL metric provides more comprehensive evaluation for code-switching ASR systems than traditional WER
- Mechanism: FAL combines fidelity to original audio, transcription accuracy, and latency into a single weighted score (0.4F + 0.4A + 0.2L) that better reflects real-world performance requirements
- Core assumption: Code-switching scenarios require evaluation beyond just word accuracy, specifically considering semantic fidelity and response time
- Evidence anchors: [abstract] "introduced a novel evaluation metric called Fidelity to the Original Audio, Accuracy, and Latency (FAL) to address limitations of traditional metrics like WER"; [section 3.3] "Rating score can be computed as: Rating score = 0.4F + 0.4A + 0.2L"
- Break condition: The weighted combination of F, A, and L doesn't correlate with actual user satisfaction or system effectiveness

### Mechanism 3
- Claim: Fine-tuning Whisper with MCE dataset improves performance specifically for mixed-language scenarios
- Mechanism: The MCE dataset contains 34.8 hours of mixed Cantonese-English audio that exposes the Whisper model to code-switching patterns during training, allowing it to learn language transition boundaries and mixed-language context
- Core assumption: A dataset containing actual mixed-language speech provides better training signals than monolingual datasets for code-switching recognition
- Evidence anchors: [abstract] "fine-tuned the open-source multilingual Automatic Speech Recognition (ASR) model, Whisper, with the MCE dataset, leading to impressive zero-shot performance"; [section 4.2] "we conducted a comparative analysis with several models... These models were fine-tuned on the common voice dataset, which exclusively comprises pure Cantonese audio samples"
- Break condition: The model overfits to the specific code-switching patterns in the MCE dataset and performs poorly on unseen code-switching patterns

## Foundational Learning

- Concept: Code-switching linguistic patterns
  - Why needed here: Understanding how and why speakers mix languages is crucial for designing datasets and evaluation metrics that capture real-world usage
  - Quick check question: What are the two main types of code-switching patterns (inter-sentential vs intra-sentential) and how do they differ?

- Concept: Semi-supervised learning and fine-tuning strategies
  - Why needed here: The approach relies on adapting a pre-trained model (Whisper) to a specific domain using limited labeled data
  - Quick check question: What are the key differences between full fine-tuning and parameter-efficient methods like LoRA when adapting large models to new tasks?

- Concept: Evaluation metric design for speech recognition
  - Why needed here: The FAL metric represents a novel approach to evaluating ASR systems in code-switching contexts, requiring understanding of traditional metrics and their limitations
  - Quick check question: How does WER fail to capture semantic meaning preservation in code-switching scenarios where word-for-word accuracy might miss the intended message?

## Architecture Onboarding

- Component map: GPT-4 text generation → Human audio recording → MCE dataset → Whisper model fine-tuning → FAL evaluation
- Critical path: MCE dataset quality → Whisper model fine-tuning effectiveness → FAL metric validity → Overall system performance
- Design tradeoffs: Larger models offer better accuracy but higher latency; more diverse training data improves generalization but increases collection costs; complex evaluation metrics provide better insights but are harder to implement
- Failure signatures: Poor code-switching recognition (model struggles with language transitions), high latency (FAL score drops due to latency component), evaluation inconsistencies (FAL scores don't align with user experience)
- First 3 experiments:
  1. Compare FAL scores on code-switching vs monolingual test sets to validate metric sensitivity
  2. Ablation study removing latency component from FAL to measure its impact on overall score
  3. Test model performance on different code-switching patterns (inter-sentential vs intra-sentential) to identify specific weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the MCE dataset perform when fine-tuned on models other than Whisper, such as Conformer or other E2E ASR architectures?
- Basis in paper: [inferred] The paper focuses on fine-tuning Whisper with the MCE dataset but doesn't explore performance on other model architectures
- Why unresolved: The authors chose Whisper for its multilingual capabilities and low latency but didn't conduct comparative experiments with other architectures
- What evidence would resolve it: Direct comparison of MCE dataset performance across multiple ASR architectures (Whisper, Conformer, etc.) would clarify if the dataset's benefits are model-agnostic

### Open Question 2
- Question: What is the optimal ratio of Cantonese to English content in the MCE dataset for maximizing recognition accuracy in mixed-language scenarios?
- Basis in paper: [explicit] The paper mentions generating 18 topics with mixed content but doesn't analyze optimal language mixing ratios
- Why unresolved: The dataset generation process doesn't systematically vary the Cantonese-to-English ratio to find optimal balance
- What evidence would resolve it: Experiments varying the proportion of Cantonese vs. English content in training data while measuring recognition accuracy would identify optimal ratios

### Open Question 3
- Question: How does the FAL metric compare to traditional WER in predicting real-world usability of ASR systems for code-switching applications?
- Basis in paper: [explicit] The authors introduce FAL as an alternative to WER specifically for code-switching scenarios but don't validate it against real-world usage metrics
- Why unresolved: While FAL addresses theoretical limitations of WER, its practical predictive value for user experience hasn't been established
- What evidence would resolve it: User studies measuring satisfaction and task completion rates using systems evaluated by both FAL and WER would demonstrate which metric better predicts real-world performance

## Limitations
- Dataset generalizability may be limited as GPT-4 generated text may not fully capture natural conversational code-switching patterns
- FAL metric validation is incomplete without extensive user studies or real-world deployment data to confirm the 0.4F + 0.4A + 0.2L weighting reflects actual user preferences
- Language pair specificity restricts the approach to Cantonese-English code-switching in Hong Kong, with uncertain generalizability to other language pairs

## Confidence
- High Confidence: The effectiveness of fine-tuning Whisper on domain-specific datasets for improved code-switching recognition
- Medium Confidence: The MADGF framework's ability to generate linguistically diverse code-switching text data
- Medium Confidence: The superiority of FAL over traditional WER metrics for code-switching evaluation

## Next Checks
1. Cross-dataset validation: Test the fine-tuned Whisper-MCE model on independently collected code-switching datasets to verify generalization beyond the training data distribution
2. Ablation study on FAL components: Systematically vary the weighting factors in FAL and conduct user preference studies to determine which combinations best predict actual user satisfaction
3. Alternative language pair testing: Apply the same MADGF framework and FAL metric to a different code-switching pair (e.g., Spanish-English or Mandarin-English) to evaluate the approach's generalizability to other multilingual contexts