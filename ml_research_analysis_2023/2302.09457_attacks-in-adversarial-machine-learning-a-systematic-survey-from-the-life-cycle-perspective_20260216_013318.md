---
ver: rpa2
title: 'Attacks in Adversarial Machine Learning: A Systematic Survey from the Life-cycle
  Perspective'
arxiv_id: '2302.09457'
source_url: https://arxiv.org/abs/2302.09457
tags:
- attack
- adversarial
- backdoor
- trigger
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a unified mathematical framework to comprehensively
  review the overall progress of adversarial machine learning (AML), which studies
  the adversarial phenomenon of machine learning models. The proposed framework covers
  three main attack paradigms: backdoor attack at the training stage, weight attack
  at the deployment stage, and adversarial attack at the testing stage.'
---

# Attacks in Adversarial Machine Learning: A Systematic Survey from the Life-cycle Perspective

## Quick Facts
- arXiv ID: 2302.09457
- Source URL: https://arxiv.org/abs/2302.09457
- Reference count: 40
- Primary result: Presents a unified mathematical framework covering backdoor attacks, weight attacks, and adversarial attacks across ML lifecycle stages

## Executive Summary
This paper provides a comprehensive survey of adversarial machine learning attacks through a unified mathematical framework that captures three main attack paradigms: backdoor attacks at training time, weight attacks at deployment time, and adversarial attacks at testing time. The authors develop a systematic taxonomy that categorizes existing methods across these paradigms using multiple classification dimensions. By establishing connections between different attack types within a common framework, the paper aims to advance the field by revealing shared principles and inspiring cross-paradigm innovation. The work is accompanied by an online resource (http://adversarial-ml.com) for continuous updates to the taxonomies and literature.

## Method Summary
The paper proposes a unified mathematical framework that abstracts adversarial machine learning attacks into six conditions: the set of attacks (S), constraints (C), and three variants of the inconsistency condition (I variants). This framework encompasses three attack paradigms across the ML lifecycle: data-poisoning based backdoor attacks during training, deployment-time weight attacks that modify model parameters, and testing-time adversarial attacks that manipulate inputs. The authors build a hierarchical taxonomy using classification axes such as trigger types, fusion strategies, and target classes to systematically organize existing attack methods. The framework enables systematic comparison and reveals connections between attack paradigms that could inform future research directions.

## Key Results
- Establishes a unified mathematical framework covering all three main attack paradigms in adversarial ML
- Develops a comprehensive taxonomy that categorizes existing attacks using multiple classification dimensions
- Reveals theoretical connections between training-time, deployment-time, and inference-time attacks
- Provides an online resource for continuously updating the taxonomies and literature

## Why This Works (Mechanism)

### Mechanism 1
The unified mathematical framework enables systematic comparison across attack paradigms by abstracting each attack into a shared formulation with six conditions (S, C, I variants). This creates a common vocabulary that allows direct mapping between training-time, deployment-time, and inference-time attacks. The framework assumes all adversarial ML attacks can be expressed using these six conditions regardless of implementation details. The paper demonstrates this by showing how diverse attacks from different paradigms can be mapped to the same framework conditions. The unification claim would fail if any attack paradigm cannot be expressed within this six-condition framework.

### Mechanism 2
The hierarchical taxonomy organizes existing methods into actionable categories through multiple classification axes (trigger type, fusion strategy, target classes, etc.). This multidimensional categorization system captures the diversity of existing approaches while revealing relationships between them. The assumption is that the chosen classification axes adequately capture the essential differences between attack methods. The paper demonstrates this by systematically categorizing representative methods from each paradigm using the proposed taxonomy structure. If new attack methods emerge that don't fit into the existing taxonomy structure, the categorization becomes incomplete.

### Mechanism 3
The systematic perspective reveals connections that enable cross-paradigm innovation by presenting all attack paradigms within the same framework. This allows researchers to identify shared principles and techniques that could be transferred between paradigms, potentially accelerating development of new attack methods. The assumption is that understanding connections between paradigms will lead to novel attack approaches. The paper highlights how techniques from one paradigm might be adapted for use in another. However, the cross-paradigm innovation claim remains largely theoretical without substantial empirical demonstrations of new attack methods arising from these connections.

## Foundational Learning

- Concept: Adversarial machine learning basics
  - Why needed here: Understanding what makes attacks "adversarial" is fundamental to grasping the unified framework
  - Quick check question: What distinguishes adversarial attacks from regular machine learning errors?

- Concept: Mathematical optimization in ML
  - Why needed here: The framework relies on optimization formulations for attack generation
  - Quick check question: How do constraint satisfaction problems differ from regular optimization in attack contexts?

- Concept: Attack vs defense taxonomy
  - Why needed here: The paper focuses on attacks but situates them within the broader security landscape
  - Quick check question: What are the key differences between attack methods and defense mechanisms?

## Architecture Onboarding

- Component map: Definition → Unified Framework → Taxonomy → Categorization → Discussion
- Critical path: Understanding the unified framework is prerequisite to making sense of the taxonomy and categorizations
- Design tradeoffs: Comprehensive coverage vs. readability - the unified framework sacrifices some simplicity for completeness
- Failure signatures: If readers cannot map specific attacks to the framework conditions, the framework is too abstract
- First 3 experiments:
  1. Map a known attack (e.g., FGSM) to the unified framework conditions
  2. Place an existing attack method into the taxonomy hierarchy
  3. Identify potential cross-paradigm techniques by comparing framework formulations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How many target classes can be embedded into a single dataset using multi-target backdoor attacks, and what is the relationship between the number of target classes and attack success rate?
- Basis in paper: [explicit] The paper discusses multi-target backdoor attacks in Section 6.1.1, noting that the relationship between the number of target classes and attack performance is underexplored
- Why unresolved: There is limited empirical research on the scalability of multi-target backdoor attacks and their effectiveness as the number of target classes increases
- What evidence would resolve it: Systematic experiments evaluating attack success rates with varying numbers of target classes and datasets, analyzing the trade-offs between attack complexity and performance

### Open Question 2
- Question: What are the key differences between single-target and multi-target backdoored models, and can these differences be identified to improve defense strategies?
- Basis in paper: [explicit] The paper mentions in Section 6.1.1 that understanding the differences between single-target and multi-target backdoored models could aid in developing defenses
- Why unresolved: While the paper highlights the potential for differences, it does not provide a detailed analysis or empirical evidence of such distinctions
- What evidence would resolve it: Comparative studies analyzing the structural and behavioral differences between single-target and multi-target backdoored models, potentially using techniques like model interpretability or anomaly detection

### Open Question 3
- Question: How can the adversarial transferability of attacks be theoretically understood in relation to data distribution, model architecture, and loss landscape?
- Basis in paper: [explicit] The paper discusses model-level adversarial transferability in Section 5.4.2, noting that the underlying reasons for transferability are not thoroughly explored
- Why unresolved: While heuristic strategies exist, there is a lack of theoretical frameworks that integrate factors like data distribution and model architecture to explain transferability
- What evidence would resolve it: Theoretical models or frameworks that formalize the relationship between data distribution, model architecture, and adversarial transferability, supported by empirical validation

## Limitations

- The six-condition framework may not capture all emerging attack paradigms as adversarial ML evolves
- The taxonomy's effectiveness depends heavily on chosen classification axes that may become inadequate for new attack methods
- Cross-paradigm innovation claims remain largely theoretical with limited empirical demonstrations of new attack discoveries

## Confidence

**High Confidence**: The mathematical framework's internal consistency and systematic categorization of existing attacks demonstrates strong technical rigor in mapping known attacks to the proposed framework.

**Medium Confidence**: The claim that the unified perspective enables cross-paradigm innovation shows theoretical merit but lacks substantial empirical evidence of new attack methods arising from these connections.

**Low Confidence**: The completeness of the taxonomy given the rapidly evolving nature of adversarial ML attacks, as new paradigms may not fit cleanly into the existing classification system.

## Next Checks

1. **Framework Completeness Test**: Systematically attempt to map all major attacks from the past two years to the six-condition framework. Document any attacks that cannot be expressed within the current formulation.

2. **Cross-Paradigm Experiment**: Select one technique from each attack paradigm and attempt to implement it in a different paradigm using the unified framework. Measure success rates and identify transfer limitations.

3. **Taxonomy Stress Test**: Apply the current taxonomy to emerging attack papers from top conferences (NeurIPS, ICML, ICLR) from the last 12 months. Document classification failures and propose taxonomy extensions.