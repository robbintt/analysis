---
ver: rpa2
title: Improving Prompt Tuning with Learned Prompting Layers
arxiv_id: '2310.20127'
source_url: https://arxiv.org/abs/2310.20127
tags:
- prompt
- tuning
- layers
- layer
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework called SPT (Selective Prompt Tuning)
  for improving prompt tuning by learning to select the best layers in the model to
  insert soft prompts. The core idea is to add a learnable gate at each intermediate
  layer to control whether to insert a prompt at that layer, and optimize the gates
  via a bi-level optimization framework called SPT-DARTS.
---

# Improving Prompt Tuning with Learned Prompting Layers

## Quick Facts
- arXiv ID: 2310.20127
- Source URL: https://arxiv.org/abs/2310.20127
- Reference count: 27
- Primary result: SPT improves prompt tuning by learning to insert soft prompts at optimal intermediate layers, outperforming baseline methods on 10 text classification benchmarks

## Executive Summary
This paper proposes SPT (Selective Prompt Tuning), a framework that learns to insert soft prompts at optimal intermediate layers of pre-trained language models rather than just at the input layer. By adding learnable probabilistic gates at each layer to control prompt insertion and optimizing these gates through a novel bi-level optimization framework called SPT-DARTS, the method achieves better performance than existing prompt tuning approaches with comparable or fewer tunable parameters. Extensive experiments on 10 text classification benchmarks demonstrate SPT's effectiveness across different model architectures including RoBERTa, DeBERTa, and GPT2.

## Method Summary
SPT introduces a prompt generator with parameterized hyper-complex multiplication (PHM) layers at each intermediate layer of a pre-trained model, controlled by learnable probabilistic gates. These gates determine whether to insert a new prompt or propagate the previous one at each layer. The method uses SPT-DARTS, a bi-level optimization framework that optimizes the gating parameters (α) conditioned on the prompt generator parameters, with an architectural consistency regularization term to ensure the hyper-network produces consistent outputs regardless of which prompts are pruned. This approach enables selective prompt insertion at optimal layers while maintaining parameter efficiency through PHM layer compression.

## Key Results
- SPT outperforms previous prompt tuning methods like PETuning on 10 text classification benchmarks
- The method achieves better performance with comparable or fewer tunable parameters
- Learned prompt layer settings show transferability across different datasets and backbone models
- SPT-DARTS optimization framework effectively learns meaningful prompt layer configurations

## Why This Works (Mechanism)

### Mechanism 1
Learning to insert prompts at intermediate layers improves performance by preserving task-related information longer in the model. SPT learns probabilistic gates at each layer that determine whether to insert a new prompt or propagate the previous one, effectively selecting optimal prompt insertion layers. The core assumption is that different layers capture different types of information, and task-relevant information is better preserved when prompts are inserted at appropriate intermediate layers rather than only at the input. This mechanism is supported by the paper's finding that simple modifications to prompt inserting strategies can result in better performances than baselines. A potential break condition is if optimal prompt layer configurations vary significantly across tasks, the learned gate values may not generalize well.

### Mechanism 2
The bi-level optimization framework (SPT-DARTS) with architectural consistency regularization improves the optimization of learnable gates, leading to better prompt layer selection. SPT-DARTS optimizes the gating parameters (α) in a bi-level fashion where the outer loop optimizes α conditioned on the optimized prompt generator parameters. The consistency regularization term encourages the hyper-network to produce consistent outputs regardless of which prompts are pruned. The core assumption is that bi-level optimization can effectively separate the learning of prompt generators from the learning of which layers to insert prompts. A potential break condition is if the bi-level optimization becomes unstable or if the consistency regularization term is set incorrectly, the optimization may fail to find good prompt layer configurations.

### Mechanism 3
Using a bottleneck architecture with parameterized hyper-complex multiplication (PHM) for the prompt generator reduces the number of tunable parameters while maintaining performance. The prompt generator uses PHM layers that substitute weight matrices with sums of Kronecker products, reducing parameter complexity from O(md) to O(md/n). The core assumption is that the PHM layer can effectively compress the prompt generator while still generating meaningful prompts for the downstream task. A potential break condition is if the compression ratio (1/n) is too high, the prompt generator may not have enough capacity to generate meaningful prompts.

## Foundational Learning

- **Concept**: Bi-level optimization
  - Why needed here: To separately optimize the prompt generator parameters and the architectural parameters (which layers to insert prompts), ensuring that the gate optimization is not biased by the generator optimization.
  - Quick check question: What is the difference between inner-level and outer-level optimization in bi-level optimization?

- **Concept**: Neural Architecture Search (NAS)
  - Why needed here: SPT-DARTS is essentially performing NAS over the space of prompt layer configurations, searching for the optimal architecture (which layers to insert prompts).
  - Quick check question: How does DARTS (Differentiable Architecture Search) work, and how is it applied to the prompt layer selection problem?

- **Concept**: Parameter-efficient fine-tuning
  - Why needed here: SPT is a parameter-efficient fine-tuning method that only tunes a small number of additional parameters (the prompts and gates) rather than all parameters of the pre-trained model.
  - Quick check question: What are the main differences between prompt tuning, adapter-based tuning, and full fine-tuning in terms of parameter efficiency?

## Architecture Onboarding

- **Component map**: Input → Pre-trained model layers → (Optional prompt insertion based on gate values) → Output
- **Critical path**: Input → Pre-trained model layers → (Optional prompt insertion based on gate values) → Output
- **Design tradeoffs**:
  - Number of prompt layers (K) vs. parameter efficiency: More prompt layers may improve performance but increase parameter count
  - Prompt length (l) vs. computation cost: Longer prompts may capture more information but increase computation quadratically
  - Bi-level optimization vs. single-level optimization: Bi-level optimization may find better prompt layer configurations but is more complex and time-consuming
- **Failure signatures**:
  - If all gates converge to 0 or 1, the optimization may have failed to find a good prompt layer configuration
  - If the consistency regularization term is too large, it may prevent the model from adapting to the task
  - If the prompt generators are not properly compressed, the method may lose its parameter efficiency advantage
- **First 3 experiments**:
  1. Implement a simple version of SPT with only one prompt layer (K=1) to verify that the basic mechanism works
  2. Run the full SPT method with K=4 on a small dataset (e.g., SST-2) to verify that the bi-level optimization finds meaningful prompt layer configurations
  3. Compare the performance of SPT with different numbers of prompt layers (K ∈ {1, 2, 4, 8}) to understand the tradeoff between performance and parameter efficiency

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the content and methodology, several natural open questions emerge:

1. How does the choice of the number of prompt layers (K) affect the performance of SPT across different downstream tasks and model architectures?
2. What is the impact of the architectural consistency learning regularization term on the optimization of learnable gates and final model performance?
3. How transferable are the learned prompt layer settings across different model architectures and task domains?
4. How does SPT perform on larger language models with tens of billions of parameters?

## Limitations
- The bi-level optimization framework (SPT-DARTS) introduces significant complexity and its effectiveness depends on proper tuning of hyperparameters like the consistency regularization coefficient.
- The paper provides limited ablation studies on critical hyperparameters, making it difficult to assess the robustness of the method.
- The learned prompt layer configurations may not generalize well across different tasks and backbone models.
- The parameter efficiency gains from using PHM layers are theoretically sound but lack sufficient evidence on how compression ratio affects prompt quality.

## Confidence

- **High confidence**: The basic mechanism of using learnable gates to control prompt insertion at intermediate layers is sound and likely to improve performance over baseline prompt tuning methods.
- **Medium confidence**: The bi-level optimization framework (SPT-DARTS) with architectural consistency regularization is likely to improve the optimization of the learnable gates, but its effectiveness may depend on careful hyperparameter tuning.
- **Low confidence**: The parameter efficiency gains from using PHM layers for prompt generation are theoretically sound, but the paper does not provide sufficient evidence on how the compression ratio affects prompt quality and overall performance.

## Next Checks

1. Perform a comprehensive ablation study on the key hyperparameters of SPT-DARTS, including the consistency regularization coefficient, the number of prompt layers, and the prompt length.
2. Conduct a thorough analysis of the learned prompt layer configurations across different tasks and backbone models, including visualizing gate values and evaluating transferability.
3. Implement a controlled experiment to isolate the effect of the PHM layer on prompt quality and overall performance by comparing SPT with and without PHM layers on a subset of tasks.