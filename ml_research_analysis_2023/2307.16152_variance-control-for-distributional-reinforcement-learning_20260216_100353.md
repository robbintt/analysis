---
ver: rpa2
title: Variance Control for Distributional Reinforcement Learning
arxiv_id: '2307.16152'
source_url: https://arxiv.org/abs/2307.16152
tags:
- variance
- learning
- quantile
- distribution
- distributional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new estimator, Quantiled Expansion Mean
  (QEM), to reduce variance in distributional reinforcement learning (DRL) algorithms.
  By analyzing the approximation errors in DRL, the authors propose using the Cornish-Fisher
  Expansion to model quantile heteroskedasticity and construct a weighted regression
  model for estimating the Q-function.
---

# Variance Control for Distributional Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2307.16152
- **Source URL:** https://arxiv.org/abs/2307.16152
- **Reference count:** 40
- **Primary result:** QEMRL achieves significant improvements in sample efficiency and convergence performance on Atari and MuJoCo benchmarks.

## Executive Summary
This paper introduces the Quantiled Expansion Mean (QEM) estimator to address variance control in distributional reinforcement learning (DRL). By modeling heteroskedasticity in quantile estimation errors and applying weighted least squares regression with Cornish-Fisher expansion, QEM reduces variance in Q-function estimates while improving distribution approximation accuracy. The method is applied to both DQN-style and SAC-style DRL algorithms and demonstrates substantial performance gains over baseline algorithms on standard benchmarks.

## Method Summary
QEM constructs a new estimator by modeling quantile heteroskedasticity and applying weighted least squares regression. The method uses the Cornish-Fisher expansion to relate quantiles to moments of the return distribution, then estimates these moments via regression rather than simple averaging. This approach assigns higher weights to tail quantiles where variance is largest, reducing overall estimation variance. The QEM framework is integrated into existing DRL algorithms by modifying the Q-function estimation process while maintaining the core Bellman update structure.

## Key Results
- Significant improvements in sample efficiency on Atari 2600 benchmark tasks
- Enhanced convergence performance compared to baseline quantile-based DRL algorithms
- Better distribution approximation accuracy through moment estimation
- Improved exploration efficiency when using QEM variance estimates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantiled Expansion Mean (QEM) reduces the variance of the Q-function estimate by modeling heteroskedasticity in quantile errors.
- Mechanism: The method assumes that the variance of quantile estimation errors depends on the quantile level (heteroskedasticity), and applies weighted least squares regression using the Cornish-Fisher expansion to estimate the Q-function, assigning higher weights to the tail quantiles where variance is largest.
- Core assumption: The quantile estimation errors are heteroskedastic, with larger variance in the tail regions (τ ∈ (0,0.1] ∪ [0.9,1)) than in the central region.
- Evidence anchors:
  - [abstract] "construct a new estimator Quantiled Expansion Mean (QEM) and introduce a new DRL algorithm (QEMRL) from the statistical perspective"
  - [section 4.2] "we may consider using the weighted ordinary least squares method (WLS) instead"
  - [corpus] No direct evidence found in corpus papers supporting heteroskedasticity assumption specifically for quantiles in DRL.

### Mechanism 2
- Claim: QEM improves distribution approximation accuracy by capturing higher-order moments of the return distribution.
- Mechanism: The Cornish-Fisher expansion expresses quantiles as functions of moments (mean, variance, skewness, kurtosis), and QEM uses a regression model to estimate these moments from quantile estimates, providing a more accurate representation of the return distribution than simple atom-based methods.
- Core assumption: The Cornish-Fisher expansion truncated at fourth order provides a good approximation of the quantile function for the return distributions encountered in reinforcement learning.
- Evidence anchors:
  - [section 4.2] "we plug in the estimate ˆq(τ) of the τ-th quantile to Equation (5) and expand it by the first order"
  - [section 4.2] "For this bivariate regression model (9), the traditional ordinary least squares method (OLS) can be used to estimate M2"
  - [corpus] No direct evidence found in corpus papers validating Cornish-Fisher expansion for return distributions in RL.

### Mechanism 3
- Claim: QEM enables more efficient exploration by providing a more accurate estimate of the distribution variance.
- Mechanism: Since QEM provides estimates of both the Q-function and its variance, these variance estimates can be used in exploration strategies (like DLTV) to encourage exploration in states with high uncertainty, leading to better sample efficiency.
- Core assumption: The variance estimates produced by QEM are more accurate than those obtained from standard quantile-based methods.
- Evidence anchors:
  - [section 5.3] "we follow the idea of DLTV and examine the model performance by using either the variance estimate obtained by QEM or the original DLTV estimation"
  - [section 5.3] "the exploration efficiency is significantly improved compared to QR-DQN+DLTV since QEM enhances the accuracy of the quantile estimates"
  - [corpus] No direct evidence found in corpus papers about using QEM variance estimates for exploration.

## Foundational Learning

- **Concept:** Quantile regression loss and its role in distributional reinforcement learning
  - Why needed here: The paper builds on quantile-based DRL methods like QR-DQN, which use quantile regression loss to estimate return distributions. Understanding this loss function is essential to grasp how QEM modifies the estimation process.
  - Quick check question: What is the form of the quantile regression loss used in QR-DQN, and how does it differ from standard regression losses?

- **Concept:** Cornish-Fisher expansion and its truncation properties
  - Why needed here: QEM uses the Cornish-Fisher expansion to relate quantiles to moments of the return distribution. Understanding this expansion and why it's truncated at fourth order is crucial for understanding the method's limitations.
  - Quick check question: What are the trade-offs between using higher-order terms in the Cornish-Fisher expansion versus computational efficiency and overfitting risk?

- **Concept:** Heteroskedasticity and weighted least squares regression
  - Why needed here: QEM's core innovation relies on modeling heteroskedasticity in quantile errors and using weighted least squares. Understanding these statistical concepts is essential for grasping why QEM works.
  - Quick check question: Under what conditions does weighted least squares regression provide better estimates than ordinary least squares?

## Architecture Onboarding

- **Component map:** Quantile estimation network -> Cornish-Fisher expansion transformation -> Weighted least squares regression for moment estimation -> Q-function reconstruction
- **Critical path:** Collect experience → Compute Bellman targets → Update quantile estimates via quantile regression loss → Apply Cornish-Fisher expansion → Perform weighted regression to estimate moments → Reconstruct Q-function from moments
- **Design tradeoffs:** QEM trades computational complexity (additional regression step) for improved variance reduction and distribution approximation accuracy
- **Failure signatures:** Poor performance when heteroskedasticity assumption is violated, Cornish-Fisher expansion poorly approximates true quantile function, or weighted regression overfits to noisy quantile estimates
- **First 3 experiments:**
  1. Verify heteroskedasticity: Plot the variance of quantile estimation errors across different quantile levels on a simple environment to confirm the heteroskedasticity assumption
  2. Ablation study: Compare QEMRL with and without the weighted regression component on a standard benchmark to isolate the contribution of variance reduction
  3. Moment accuracy: Measure the accuracy of estimated moments (mean, variance, skewness, kurtosis) from QEM against ground truth on environments with known return distributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we efficiently estimate the weight matrix V in the weighted least squares (WLS) regression for QEM?
- Basis in paper: [explicit] The paper discusses the challenge of estimating V and proposes treating it as a predefined hyperparameter.
- Why unresolved: Estimating V across all state-action pairs and time steps is computationally expensive and the optimal value likely varies depending on the specific task and stage of training.
- What evidence would resolve it: Developing a method to efficiently estimate V, either by using a decaying value over training or by assigning different values based on the novelty of state-action pairs, and demonstrating improved performance compared to using a fixed V.

### Open Question 2
- Question: What is the optimal order of the Cornish-Fisher Expansion to use in QEM?
- Basis in paper: [explicit] The paper uses a 4th order expansion and mentions a trade-off between model complexity and estimation accuracy.
- Why unresolved: While higher-order expansions can capture more information, they also increase the risk of overfitting and computational cost. The optimal order likely depends on the specific distribution and the amount of available data.
- What evidence would resolve it: Conducting a systematic study comparing the performance of QEM with different expansion orders across a variety of tasks and distributions.

### Open Question 3
- Question: How does QEM perform in continuous action spaces compared to discrete action spaces?
- Basis in paper: [inferred] The paper focuses on experiments in discrete action spaces (Atari and MuJoCo).
- Why unresolved: The performance of QEM may be affected by the choice of action space discretization in continuous control tasks.
- What evidence would resolve it: Implementing QEM in continuous control benchmarks like PyBullet and comparing its performance to existing methods.

### Open Question 4
- Question: Can QEM be extended to other distributional RL algorithms beyond quantile-based methods?
- Basis in paper: [explicit] The paper discusses extending QEM to IQN, which uses a continuous quantile function.
- Why unresolved: While QEM is developed for quantile-based methods, it is based on statistical principles that could potentially be applied to other distributional RL algorithms like categorical or expectile-based methods.
- What evidence would resolve it: Adapting the QEM framework to other distributional RL algorithms and demonstrating improved performance in terms of sample efficiency and convergence.

## Limitations
- The heteroskedasticity assumption for quantile errors lacks direct empirical validation in RL settings
- The Cornish-Fisher expansion's approximation quality for return distributions in RL environments is not rigorously evaluated
- The optimal weight matrix configuration for different environments is underspecified

## Confidence
- **High confidence**: QEM's variance reduction mechanism when heteroskedasticity assumption holds
- **Medium confidence**: Improved distribution approximation through moment estimation
- **Medium confidence**: Sample efficiency gains on evaluated benchmarks
- **Low confidence**: Generalization of heteroskedasticity assumption to diverse RL environments

## Next Checks
1. **Heteroskedasticity verification**: Empirically measure quantile error variance across different quantile levels on simple benchmark environments to validate the core assumption
2. **Expansion approximation quality**: Compare Cornish-Fisher approximated quantiles against empirical quantiles on environments with known return distributions
3. **Weight matrix sensitivity**: Systematically test different weight matrix configurations on a standard benchmark to determine optimal settings for different environment types