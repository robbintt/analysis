---
ver: rpa2
title: 'Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda
  and Disinformation Detection'
arxiv_id: '2311.03184'
source_url: https://arxiv.org/abs/2311.03184
tags:
- task
- detection
- content
- shared
- arabic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the participation of the Nexus team in the
  ArAIEval shared task for propaganda and disinformation detection in Arabic text.
  The team competed in subtasks 1A (persuasion technique detection) and 2A (disinformation
  detection) using fine-tuned transformer models and zero/few-shot learning with GPT-4.
---

# Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda and Disinformation Detection

## Quick Facts
- arXiv ID: 2311.03184
- Source URL: https://arxiv.org/abs/2311.03184
- Reference count: 15
- Achieved 9th and 10th place in subtasks 1A and 2A respectively, with Micro F1 scores of 0.740 (1A) and 0.893 (2A), and Macro F1 scores of 0.693 (1A) and 0.846 (2A).

## Executive Summary
This paper presents the participation of the Nexus team in the ArAIEval shared task for propaganda and disinformation detection in Arabic text. The team competed in subtasks 1A (persuasion technique detection) and 2A (disinformation detection) using fine-tuned transformer models and zero/few-shot learning with GPT-4. The submitted system achieved 9th and 10th place in subtasks 1A and 2A respectively. Key results include Micro F1 scores of 0.740 (1A) and 0.893 (2A), and Macro F1 scores of 0.693 (1A) and 0.846 (2A). The team explored various dropout rates and model architectures, finding that MarBERT performed well for subtask 1A while Qarib excelled in subtask 2A. Despite challenges with imbalanced data, the team optimized their models to achieve competitive results in the shared task.

## Method Summary
The study fine-tuned three Arabic transformer models (AraBERT, MarBERT, and Qarib) on propaganda and disinformation detection tasks using the ArAIEval shared task datasets. The models were trained with a maximum tokenization length of 128, batch size of 16, 3 epochs, and learning rate of 4e-5 using the AdamW optimizer. Cross-entropy loss with class weights was applied to handle imbalanced data, and dropout rates were varied to optimize performance. The models were evaluated using Micro F1 for subtask 1A and Macro F1 for subtask 2A.

## Key Results
- Achieved 9th place in subtask 1A with a Micro F1 score of 0.740
- Achieved 10th place in subtask 2A with a Macro F1 score of 0.846
- MarBERT performed well for subtask 1A, while Qarib excelled in subtask 2A

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning Arabic transformer models on imbalanced propaganda/disinformation datasets can achieve competitive detection performance. Pre-trained transformer models (AraBERT, MarBERT, Qarib) are adapted to the specific classification task using task-specific training data. The models learn to distinguish between propagandistic/disinformative and non-propagandistic/non-disinformative content through supervised learning with cross-entropy loss.

### Mechanism 2
Adjusting the class weights in the loss function can help mitigate the effects of imbalanced data in propaganda/disinformation detection. The cross-entropy loss function is modified to assign higher weights to the minority class ('false' or 'disinfo') compared to the majority class. This encourages the model to pay more attention to correctly classifying the minority class samples.

### Mechanism 3
Experimenting with different dropout rates can help find the optimal balance between underfitting and overfitting in propaganda/disinformation detection models. The model's performance is evaluated with varying dropout rates, and the rate that yields the best results on the validation set is selected.

## Foundational Learning

- Concept: Understanding the characteristics of propaganda and disinformation in Arabic text
  - Why needed here: The model needs to learn the specific patterns and features that distinguish propagandistic and disinformative content from non-propagandistic and non-disinformative content in the Arabic language.
  - Quick check question: Can you identify the key differences between propaganda and disinformation in the context of Arabic text?

- Concept: Fine-tuning pre-trained transformer models for specific classification tasks
  - Why needed here: The model needs to adapt the general language representations learned by the pre-trained transformer to the specific task of propaganda and disinformation detection.
  - Quick check question: How does fine-tuning a pre-trained transformer model differ from training a model from scratch?

- Concept: Handling imbalanced data in classification tasks
  - Why needed here: The dataset for propaganda and disinformation detection is highly imbalanced, with a large majority of non-propagandistic and non-disinformative samples. The model needs to learn to handle this imbalance effectively.
  - Quick check question: What are some common techniques for dealing with imbalanced data in classification tasks?

## Architecture Onboarding

- Component map: AraBERT preprocessor -> Transformer model (AraBERT/MarBERT/Qarib) -> Cross-entropy loss with class weights -> Dropout regularization -> Micro/Macro F1 evaluation

- Critical path: 1) Preprocess input text using AraBERT preprocessor 2) Fine-tune selected transformer model on training data 3) Evaluate model performance on validation set 4) Adjust hyperparameters based on validation performance 5) Evaluate final model on test set

- Design tradeoffs: Model selection (AraBERT, MarBERT, Qarib have different strengths/weaknesses), hyperparameter tuning (optimal values vary by dataset and architecture), regularization (dropout prevents overfitting but too much causes underfitting)

- Failure signatures: High validation loss but low training loss (overfitting), high training and validation loss (underfitting or insufficient model capacity), poor performance on minority class (imbalanced data or insufficient class weights)

- First 3 experiments: 1) Fine-tune AraBERT with default hyperparameters and evaluate on validation set 2) Adjust class weights in loss function and evaluate impact on validation performance 3) Experiment with different dropout rates and select optimal rate based on validation performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the performance of MarBERT decrease after applying the dropout layer?
- Basis in paper: [explicit] The paper mentions that MarBERT's performance dropped after applying the dropout layer.
- Why unresolved: The paper suggests that the model might be undertrained, but further investigation and experimentation are required to confirm this hypothesis.
- What evidence would resolve it: Conducting additional experiments with more epochs of training for MarBERT with and without dropout layers to compare the performance and determine if undertraining is the cause.

### Open Question 2
- Question: Why does Qarib outperform MarBERT in Subtask 2A, despite both models being trained on a variety of tweets?
- Basis in paper: [explicit] The paper notes that Qarib outperformed MarBERT in Subtask 2A, but does not provide a definitive explanation for this observation.
- Why unresolved: The paper suggests that the difference in performance could be due to a better/bigger training set or longer training duration, but further investigation and experimentation are required to understand the underlying reasons.
- What evidence would resolve it: Analyzing the training data, training duration, and model architectures of both Qarib and MarBERT to identify the factors contributing to the performance difference.

### Open Question 3
- Question: How can the performance of large language models (LLMs) like GPT-4 be improved for propaganda and disinformation detection tasks?
- Basis in paper: [inferred] The paper reports that the performances of LLMs in zero-shot and few-shot settings were significantly lower than fine-tuned models, indicating room for improvement.
- Why unresolved: The paper suggests that prompt engineering is the key factor to achieve desired results with LLMs, but does not provide specific strategies or techniques for improving their performance in this context.
- What evidence would resolve it: Experimenting with different prompt engineering techniques, model architectures, and fine-tuning approaches to enhance the performance of LLMs for propaganda and disinformation detection tasks.

## Limitations

- The study relies on a single evaluation metric (F1 scores) which may not capture the full complexity of propaganda and disinformation detection tasks.
- The analysis focuses on three specific Arabic transformer models without exploring potentially more effective architectures or ensemble methods.
- The study does not investigate the robustness of the models to adversarial attacks or domain shift, which are critical concerns in propaganda and disinformation detection.

## Confidence

- High Confidence: The reported F1 scores and ranking positions are directly verifiable from the ArAIEval shared task results.
- Medium Confidence: The impact of different dropout rates on model performance is supported by the reported experiments.
- Low Confidence: The effectiveness of the class weight adjustment strategy is based on limited experimentation.

## Next Checks

1. Conduct ablation studies to isolate the individual contributions of each model component (dropout, class weights, model architecture) to the final performance.
2. Evaluate the models on additional Arabic propaganda and disinformation datasets to assess their generalization capabilities beyond the ArAIEval shared task.
3. Perform robustness tests by introducing adversarial examples or domain-shifted data to measure the models' resilience to real-world challenges in propaganda and disinformation detection.