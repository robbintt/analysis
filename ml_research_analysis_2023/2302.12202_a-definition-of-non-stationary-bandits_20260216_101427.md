---
ver: rpa2
title: A Definition of Non-Stationary Bandits
arxiv_id: '2302.12202'
source_url: https://arxiv.org/abs/2302.12202
tags:
- bandit
- regret
- bandits
- stationary
- nition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal definition of non-stationary bandits
  that resolves ambiguity in existing informal definitions. The authors demonstrate
  that prior definitions, which rely on changing reward distributions, can inconsistently
  classify the same bandit as both stationary and non-stationary.
---

# A Definition of Non-Stationary Bandits

## Quick Facts
- **arXiv ID**: 2302.12202
- **Source URL**: https://arxiv.org/abs/2302.12202
- **Reference count**: 22
- **Primary result**: Introduces a formal definition of non-stationary bandits that resolves ambiguity in existing informal definitions

## Executive Summary
This paper addresses a fundamental gap in bandit learning theory by providing a rigorous mathematical definition of non-stationary bandits. The authors demonstrate that existing informal definitions, which rely on changing reward distributions, can inconsistently classify the same bandit as both stationary and non-stationary. They propose a new framework based on equivalence relations and predictive information that provides unambiguous classification and more meaningful regret measures for evaluating agent performance in non-stationary environments.

## Method Summary
The paper introduces a formal definition of non-stationary bandits based on an equivalence relation that classifies bandits according to the indistinguishability of their reward sequences from an agent's perspective. Rather than defining non-stationarity through changes in mean rewards, the authors establish that two bandits are equivalent if they produce identical experiences for agents. This approach resolves the ambiguity in existing definitions and provides a unified framework applicable to both Bayesian and frequentist formulations. The paper also develops an information-theoretic regret analysis that bounds regret in terms of predictive information, providing tighter bounds than previous approaches.

## Key Results
- The new definition resolves ambiguity by not relying on the ill-defined concept of "mean reward"
- A new regret framework better identifies capable agents by measuring performance against a stochastic process
- The predictive information measure provides a tighter characterization of non-stationarity complexity than existing metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The new definition resolves ambiguity by not relying on the ill-defined concept of "mean reward."
- Mechanism: Instead of defining non-stationarity through changes in reward distributions (which depend on arbitrary parametrization), the authors define it through the equivalence of reward sequences across different timesteps. If two bandits produce indistinguishable experiences for agents, they are either both stationary or both non-stationary.
- Core assumption: The equivalence relation on bandits based on agent experience is well-defined and partitions the space of bandits into meaningful classes.
- Evidence anchors:
  - [abstract]: "Our new definition provides a unified approach, applicable seamlessly to both Bayesian and frequentist formulations of bandits."
  - [section]: "Our definition desirably classifies the bandit in Example 6 as stationary, because for all T ∈ Z++, sequences of actions {ak}T k=1 and sequences of distinct timesteps {tk}T k=1, the reward sequence {Rtk,a k }T k=1 is a sequence of Bernoulli(θak) random variables that are conditionally independent conditioned on all θa."
- Break condition: If the agent experience does not fully capture the underlying reward distribution structure, the equivalence relation may fail to distinguish bandits with different properties.

### Mechanism 2
- Claim: The new regret framework better identifies capable agents by measuring performance against a stochastic process rather than fixed "mean rewards."
- Mechanism: The authors introduce a flexible regret definition that measures the gap between an agent's expected cumulative reward and that of an oracle who knows a stochastic process χt at each timestep. This allows regret to be defined in ways that give minimal regret to optimal agents in nearly stationary environments.
- Core assumption: The choice of stochastic process χt appropriately captures what constitutes good performance in the specific bandit environment.
- Evidence anchors:
  - [abstract]: "Our new definition provides a unified approach, applicable seamlessly to both Bayesian and frequentist formulations of bandits."
  - [section]: "In a modulated Bernoulli bandit with transition probability 1, an optimal agent always selects an action with the highest expected reward E[Rt+1,a]. The regret introduced by [12] corresponds to χt = R1:t, and thus Rχ t+1,⋆ = max a∈A E[Rt+1,a |R1:t] = max a∈A E[Rt+1,a]. In this case, an optimal agent incurs zero regret."
- Break condition: If the chosen stochastic process χt does not align with the agent's actual knowledge and decision-making process, the regret measure may not accurately reflect performance.

### Mechanism 3
- Claim: The predictive information measure provides a tighter characterization of non-stationarity complexity than existing metrics.
- Mechanism: Instead of measuring the number or magnitude of changes in mean rewards, the authors measure the rate at which information relevant for predicting future rewards accumulates. This rate is close to zero in nearly stationary bandits, allowing for tighter regret bounds.
- Core assumption: The amount of predictive information accurately reflects the difficulty an agent faces in adapting to the environment.
- Evidence anchors:
  - [abstract]: "The regret analysis applies to any bandit, stationary or non-stationary, and any agent."
  - [section]: "By Proposition 1, the cumulative predictive information in the k-th bandit converges to zero as k → +∞."
- Break condition: If the predictive information measure fails to capture all relevant aspects of non-stationarity (e.g., structural changes that don't affect immediate predictions), the regret bounds may be overly optimistic.

## Foundational Learning

- Concept: Exchangeability of random sequences
  - Why needed here: The paper relies on de Finetti's theorem and the concept of exchangeable sequences to establish the relationship between stationarity and equivalence classes of bandits.
  - Quick check question: If a sequence of random variables is exchangeable, what does de Finetti's theorem tell us about its structure?

- Concept: Information theory and mutual information
  - Why needed here: The regret analysis uses information-theoretic concepts like mutual information and information ratio to bound regret in terms of predictive information.
  - Quick check question: How does the chain rule of mutual information apply when decomposing I(αt+2:∞; Rt+1|R1:t)?

- Concept: Equivalence relations and partition theory
  - Why needed here: The paper introduces an equivalence relation on bandits based on agent experience and uses this to partition bandits into classes that are either all stationary or all non-stationary.
  - Quick check question: What are the three properties that must hold for a relation to be an equivalence relation?

## Architecture Onboarding

- Component map: Definition → Regret Framework → Information Measure → Regret Analysis
- Critical path: Definition → Regret Framework → Information Measure → Regret Analysis
- Design tradeoffs: The paper sacrifices some generality in the definition of non-stationarity to gain unambiguous classification and better regret measures
- Failure signatures: If the equivalence relation fails to partition bandits correctly, or if the predictive information measure is not tight, the entire framework breaks down
- First 3 experiments:
  1. Verify that the equivalence relation correctly classifies Example 2 as stationary
  2. Test the regret framework on a simple modulated Bernoulli bandit to confirm minimal regret for optimal agents
  3. Compare cumulative predictive information bounds with existing temporal variation bounds on a sequence of modulated Bernoulli bandits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between the proposed definition of non-stationarity and exchangeability of reward sequences?
- Basis in paper: [explicit] The paper discusses this relationship in Section 6.3 and Theorem 4
- Why unresolved: The paper states that a bandit is stationary if and only if it is equivalent to another bandit with exchangeable rewards, but does not fully characterize when non-stationary bandits have exchangeable reward sequences
- What evidence would resolve it: A complete characterization of when non-stationary bandits have exchangeable reward sequences, and what this implies about the relationship between the two definitions

### Open Question 2
- Question: How can we construct an optimal information ratio for non-stationary bandits that minimizes regret?
- Basis in paper: [explicit] Section 5.3 introduces an information ratio but notes it's not optimized
- Why unresolved: The paper introduces the concept but does not provide methods for finding the optimal information ratio
- What evidence would resolve it: Algorithms or analytical methods for computing the optimal information ratio for specific classes of non-stationary bandits

### Open Question 3
- Question: What is the fundamental trade-off between computational complexity and regret minimization in non-stationary bandits?
- Basis in paper: [inferred] The paper's regret analysis suggests computational challenges in tracking non-stationarity
- Why unresolved: While the paper provides regret bounds, it doesn't analyze the computational complexity of achieving these bounds
- What evidence would resolve it: A theoretical framework that quantifies the computational cost of maintaining low regret in non-stationary environments

## Limitations

- The paper's definition of non-stationarity relies on the assumption that agent experience fully captures the bandit's properties, but this may not hold in all scenarios.
- The equivalence relation's partitioning of bandits into stationary and non-stationary classes is only as good as the definition of "agent experience" itself.
- The predictive information measure, while theoretically elegant, requires careful calibration to ensure it captures all relevant aspects of non-stationarity.

## Confidence

- **High confidence**: The paper's identification of ambiguity in existing non-stationary bandit definitions and the proposed equivalence relation mechanism
- **Medium confidence**: The effectiveness of the new regret framework in identifying capable agents
- **Medium confidence**: The claim that predictive information provides tighter characterization of non-stationarity complexity

## Next Checks

1. **Equivalence Relation Validation**: Test the equivalence relation on a diverse set of bandit environments, including ones where the mean reward parametrization changes without affecting the actual reward distribution, to verify it correctly identifies stationary bandits.

2. **Regret Framework Empirical Testing**: Implement the new regret framework on a suite of non-stationary bandit problems with varying degrees of stationarity, comparing its ability to identify optimal agents against dynamic regret.

3. **Predictive Information Measure Accuracy**: Compare the predictive information measure against existing temporal variation metrics on synthetic and real-world non-stationary bandit problems to quantify its tightness and practical utility.