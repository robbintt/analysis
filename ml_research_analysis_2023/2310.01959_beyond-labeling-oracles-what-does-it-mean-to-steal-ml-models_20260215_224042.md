---
ver: rpa2
title: 'Beyond Labeling Oracles: What does it mean to steal ML models?'
arxiv_id: '2310.01959'
source_url: https://arxiv.org/abs/2310.01959
tags:
- data
- attacker
- queries
- knowledge
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Model extraction attacks claim attackers can steal trained models
  with lower cost than training from scratch. This work shows that current attacks
  often fail to achieve this because they implicitly assume access to in-distribution
  data.
---

# Beyond Labeling Oracles: What does it mean to steal ML models?

## Quick Facts
- arXiv ID: 2310.01959
- Source URL: https://arxiv.org/abs/2310.01959
- Reference count: 40
- Primary result: Model extraction attacks often fail to steal models cost-effectively because they implicitly assume access to in-distribution data

## Executive Summary
Model extraction attacks claim attackers can steal trained models with lower cost than training from scratch. This work demonstrates that current attacks often fail to achieve this goal because they implicitly assume access to in-distribution data. Without such access, attackers must either sample extensively from the entire input space or solve a hard out-of-distribution detection problem. The paper argues that even small amounts of in-distribution data render model extraction equivalent to a labeling oracle, fundamentally changing our understanding of what these attacks can accomplish.

The authors propose a new benchmark that explicitly controls for the informativeness of out-of-distribution queries, revealing that prior knowledge of the data distribution dominates other factors like attack policy. Their evaluation shows that the effectiveness of model extraction attacks is often overestimated due to implicit data access assumptions in current evaluation methods.

## Method Summary
The paper evaluates model extraction attacks across multiple datasets (CIFAR-10, Indoor67, CUBS200, Caltech256, and MNLI) using both vision and NLP tasks. The baseline attacker uses prior knowledge of the data distribution to train a surrogate model by querying the victim model. Additional experiments augment queries with data from different distributions and incorporate an out-of-distribution detection component to limit the informativeness of OOD queries. The attack performance is measured as the accuracy difference between victim and attacker models, with successful attacks producing models that perform nearly as well as the victim with lower or equivalent sample complexity.

## Key Results
- Prior knowledge of in-distribution data dominates other factors in model extraction success
- Without IND data access, attackers must either solve OOD detection (as hard as classification) or use extremely large query budgets
- Current evaluation methods overestimate extraction effectiveness by not properly controlling for IND data access
- Even 10% of training data enables attackers to achieve relatively high performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prior knowledge of in-distribution data dominates other factors in model extraction success.
- Mechanism: Attackers with access to IND data can train effective surrogate models without extensive querying, as the victim model acts primarily as a labeling oracle.
- Core assumption: The attacker's prior knowledge of the data distribution is sufficiently representative of the victim's training distribution.
- Evidence anchors:
  - [abstract]: "prior knowledge of the attacker, i.e., access to in-distribution data, dominates other factors like the attack policy"
  - [section 4.2]: "with only 10% of the data, the attacker already gets relatively high performance"
- Break condition: If attacker's prior knowledge is not representative of victim's training distribution, or if labeling costs become prohibitive relative to query costs.

### Mechanism 2
- Claim: Out-of-distribution queries provide limited information about in-distribution decision boundaries.
- Mechanism: Without prior knowledge of IND data, attackers must either solve OOD detection (as hard as classification) or use extremely large query budgets to explore the entire input space.
- Core assumption: The decision boundaries in OOD regions are not informative about IND decision boundaries.
- Evidence anchors:
  - [section 3]: "model extraction attacks implicitly assume that the adversary has prior knowledge of the distribution"
  - [section 5.2]: "robust OOD detection is reducible to robust classification"
- Break condition: If attacker can efficiently distinguish IND from OOD samples, or if OOD and IND decision boundaries are correlated.

### Mechanism 3
- Claim: The effectiveness of model extraction attacks is often overestimated due to implicit data access assumptions in evaluation.
- Mechanism: Many experiments assume attacker has access to surrogate datasets or portions of training data, but don't rigorously study how this contributes to attack efficiency.
- Core assumption: Current evaluation methods don't properly control for the informativeness of OOD queries.
- Evidence anchors:
  - [abstract]: "current evaluation methods misinterpret the ME performance"
  - [section 2.1]: "much of current research does not adequately account for how the adversary's prior knowledge contributes to the attacks' query efficiency"
- Break condition: If evaluation methods explicitly control for IND data access and OOD informativeness.

## Foundational Learning

- Concept: Active learning and query synthesis
  - Why needed here: Understanding the relationship between active learning and model extraction helps contextualize why prior knowledge is so important
  - Quick check question: How does the query complexity of active learning compare to model extraction when no prior data is available?

- Concept: Out-of-distribution detection
  - Why needed here: OOD detection is central to understanding why attackers need IND data or very large query budgets
  - Quick check question: What makes OOD detection as hard as classification, according to the paper?

- Concept: Information leakage in machine learning models
  - Why needed here: Understanding what information is leaked through soft labels vs hard labels helps explain why victim models act as labeling oracles
  - Quick check question: According to the paper, what is the main information leaked by victim models besides labels?

## Architecture Onboarding

- Component map:
  Victim model (Vo) -> Hybrid model (Vh) -> Decision rule R -> Fake model (Vf)

- Critical path:
  1. Create hybrid model by combining Vo with OOD detection
  2. For each query, use decision rule R to classify as IND or OOD
  3. If IND, use Vo for prediction; if OOD, use Vf
  4. Vf uses anchor points and permutations to generate uncorrelated predictions

- Design tradeoffs:
  - Higher softmax temperature improves OOD detection but reduces model utility
  - More anchor points and permutations increase attack difficulty but add computational overhead
  - Simple confidence-based OOD detection is easy to implement but may have higher false positive rates

- Failure signatures:
  - Attacker model achieves high accuracy despite OOD component
  - High false positive rate significantly degrades victim model utility
  - Attacker model agreement with Vf is near chance level

- First 3 experiments:
  1. Evaluate baseline attacker with varying levels of prior knowledge (0%, 10%, 50%, 100% of training data)
  2. Compare attack performance with and without OOD component for different threshold values
  3. Measure attacker model agreement with Vo and Vf to verify learning of both real and fake tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can model extraction attacks be accurately evaluated given the dependency on prior knowledge of the data distribution?
- Basis in paper: [explicit] The paper demonstrates that prior knowledge of in-distribution data dominates other factors like attack policy, making current evaluation methods inadequate.
- Why unresolved: The paper identifies the problem but does not propose a comprehensive solution for evaluation that decouples prior knowledge from attack policy.
- What evidence would resolve it: A new benchmark that explicitly controls for informativeness of out-of-distribution queries and evaluates attack policy directly, as suggested by the authors.

### Open Question 2
- Question: What is the practical threat posed by model extraction attacks in scenarios where labeling costs are low?
- Basis in paper: [explicit] The paper argues that with low labeling costs, model extraction attacks are not cost-effective compared to training from scratch.
- Why unresolved: The paper provides a cost analysis but does not explore real-world scenarios where labeling costs vary significantly.
- What evidence would resolve it: Empirical studies comparing the costs of model extraction versus training from scratch in different industries with varying labeling costs.

### Open Question 3
- Question: How does the complexity of the task and model size affect the success of model extraction attacks?
- Basis in paper: [inferred] The paper briefly touches on task and model complexity but does not provide a thorough analysis.
- Why unresolved: The paper mentions that simpler tasks and larger models have minimal impact on attack success but does not explore this relationship in depth.
- What evidence would resolve it: Systematic experiments varying task complexity and model sizes to determine their impact on model extraction success rates.

## Limitations
- The theoretical connection between OOD detection and classification hardness relies on specific assumptions about computational capabilities
- Experimental validation is more thorough for vision tasks than NLP tasks
- The OOD detection mechanism using GMMs and anchor points may not generalize well to all model architectures
- The paper doesn't provide a comprehensive solution for evaluation methodology

## Confidence
- High: IND data access dramatically improves extraction success
- Medium: Theoretical arguments about OOD detection hardness
- Low: Broader implications about fundamental limitations of model extraction without prior data

## Next Checks
1. **OOD Detection Equivalence Test**: Conduct a formal reduction proof showing that OOD detection is as hard as classification under the adversary's constraints, including computational limits and access to unlabeled data.

2. **Cross-Domain Transfer**: Evaluate whether models trained on one dataset (e.g., CIFAR-10) can be effectively extracted when attacker has prior knowledge from a related but different dataset (e.g., ImageNet), testing the robustness of the prior knowledge hypothesis.

3. **Label-Only Extraction**: Implement a pure label-only extraction attack without access to soft labels or IND data, and measure whether the claimed limitations hold under these stricter constraints.