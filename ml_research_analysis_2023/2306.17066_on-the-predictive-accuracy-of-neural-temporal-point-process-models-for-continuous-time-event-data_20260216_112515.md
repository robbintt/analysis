---
ver: rpa2
title: On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time
  Event Data
arxiv_id: '2306.17066'
source_url: https://arxiv.org/abs/2306.17066
tags:
- datasets
- event
- decoder
- history
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a comprehensive empirical study evaluating\
  \ the predictive accuracy of state-of-the-art neural Temporal Point Process (TPP)\
  \ models on multiple real-world and synthetic datasets. The authors investigate\
  \ the influence of major architectural components\u2014event encoding, history encoder,\
  \ and decoder parametrization\u2014on both time and mark prediction tasks."
---

# On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time Event Data

## Quick Facts
- arXiv ID: 2306.17066
- Source URL: https://arxiv.org/abs/2306.17066
- Reference count: 30
- Key outcome: Comprehensive empirical study evaluating state-of-the-art neural TPP models on 15 datasets, revealing architectural limitations and calibration issues

## Executive Summary
This paper presents the first comprehensive empirical evaluation of neural Temporal Point Process models, examining how architectural choices affect predictive accuracy for both time and mark prediction tasks. The study systematically investigates three major components—event encoding, history encoder, and decoder parametrization—across 15 real-world and synthetic datasets. A key finding is that neural TPP models often fail to leverage complete historical context, with truncated histories yielding comparable performance to full sequences. The authors also conduct the first assessment of probabilistic calibration in neural TPPs, finding that many models produce overconfident predictions despite achieving low log-likelihood scores.

## Method Summary
The study evaluates neural TPP architectures using a standardized experimental framework across 15 datasets (7 marked, 8 unmarked). Models are trained with 60%/20%/20% train/validation/test splits using mini-batch gradient descent with Adam optimizer and early stopping. The evaluation examines four event encoding schemes (raw inter-arrival times, log inter-arrival times, temporal encoding, learnable encoding), three history encoders (GRU, self-attention, constant), and eight decoder parametrizations. Performance is measured using negative log-likelihood metrics (NLL-T for time, NLL-M for marks), probabilistic calibration error (PCE), expected calibration error (ECE), and F1-score for mark prediction.

## Key Results
- Neural TPP models often fail to leverage complete historical context, with truncated histories yielding comparable performance to full sequences
- Vectorial time representations combined with self-attention encoders significantly outperform raw or log-inter-arrival time encodings
- Several commonly used event sequence datasets may not be suitable for benchmarking neural TPP baselines
- Neural TPP models frequently produce poorly calibrated predictions despite achieving low log-likelihood scores

## Why This Works (Mechanism)

### Mechanism 1
Neural TPP models often fail to leverage complete historical context, with a subset of recent events yielding comparable performance. When history encoders are given fixed-size windows instead of full history, performance on NLL-T stabilizes quickly without degradation, indicating limited dependency on deep history. This suggests event sequences exhibit Markovian properties or short-range dependencies dominate. Break condition: If future events depend critically on long-range dependencies, truncated history would sharply degrade performance.

### Mechanism 2
Vectorial representations of time combined with self-attention encoders improve time prediction over raw or log-inter-arrival times. Self-attention requires dense, continuous embeddings to compute meaningful similarity scores; raw or log-inter-arrival times are sparse, leading to poor attention weight distributions. Break condition: If event sequences have very regular, evenly spaced events, raw time values may suffice without harming attention.

### Mechanism 3
Inclusion of mark embeddings in event encoding significantly improves mark prediction but not necessarily time prediction. Mark embeddings provide explicit semantic context that decoders use to condition mark distributions; for time prediction, this extra information is less directly relevant or harder for the encoder to utilize effectively. Break condition: If marks are uncorrelated with timing or carry redundant timing information, including them could add noise and degrade performance.

## Foundational Learning

- **Temporal Point Processes (TPPs)**: Stochastic models for event sequences with conditional intensity functions. *Why needed*: All subsequent mechanisms depend on understanding how TPPs model arrival times and marks. *Quick check*: What is the difference between the conditional intensity function and the conditional density of arrival times?

- **Neural network parametrization of TPP components**: How event encoders, history encoders, and decoders interact. *Why needed*: Architectural choices directly impact model performance; mechanisms hinge on encoder-decoder interplay. *Quick check*: How does a GRU history encoder differ from a self-attention encoder in processing event sequences?

- **Calibration of probabilistic forecasts**: Measuring whether predicted probabilities match empirical frequencies. *Why needed*: Model evaluation requires understanding calibration beyond log-likelihood. *Quick check*: What does it mean for a predictive distribution to be "well-calibrated"?

## Architecture Onboarding

- **Component map**: Event encoder → History encoder → Decoder
- **Critical path**: Event encoding → History encoding → Decoder output. Failures in any stage propagate downstream.
- **Design tradeoffs**:
  - Event encoding: Raw time (simple, interpretable) vs. vectorial (richer, requires careful tuning)
  - History encoder: GRU (sequential, scalable) vs. SA (parallel, richer context but O(L²))
  - Decoder: Non-parametric (flexible, hard to train) vs. semi-parametric (inductive bias, easier optimization)
- **Failure signatures**:
  - High NLL-T but low NLL-M: Time distribution mis-specified or encoder cannot capture temporal patterns
  - Poor calibration (high PCE/ECE): Overconfident predictions; check reliability diagrams
  - Training instability: Check learning rate, batch size, and encoder-decoder compatibility
- **First 3 experiments**:
  1. Compare GRU vs. SA history encoder with TEM event encoding on a small dataset; measure NLL-T and training time
  2. Fix GRU encoder; test TO, LTO, TEM, LE event encodings; record NLL-T and calibration metrics
  3. Fix GRU + TEM; swap between EC, MLP/MC, and LNM decoders; compare NLL-T, NLL-M, and F1-score

## Open Questions the Paper Calls Out

### Open Question 1
What are the key architectural components that drive improvements in predictive accuracy for neural TPP models? While the paper conducts comprehensive studies on influence of event encoding, history encoder, and decoder parametrization, it does not definitively identify which combinations are most crucial for achieving high predictive accuracy.

### Open Question 2
How does the probabilistic calibration of neural TPP models compare to classical parametric baselines? The paper assesses calibration but does not provide direct comparison of neural TPP models against classical parametric baselines.

### Open Question 3
How much historical information is truly necessary for accurate predictions in neural TPP models? While the paper demonstrates that encoding a subset of recent events can be sufficient, it does not explore the optimal amount of historical information required across different datasets and model architectures.

## Limitations

- The study focuses on relatively short event sequences, potentially missing long-range dependency patterns
- All datasets are normalized to [0,10] time scale, which may mask important temporal dynamics
- Calibration analysis uses relatively coarse binning, potentially missing finer calibration patterns

## Confidence

- **High confidence**: Claims about architectural component performance comparisons (GRU vs SA, TEM vs LE encodings)
- **Medium confidence**: Claims about limited historical dependency leveraging, as these depend on dataset characteristics
- **Medium confidence**: Calibration findings, though methodology is sound, interpretation requires domain-specific context

## Next Checks

1. Test truncated history experiments on datasets known for long-range dependencies (e.g., financial market data) to verify limits of short-range dependency assumptions
2. Conduct scale sensitivity analysis by training on datasets with preserved natural time scales rather than normalized [0,10] intervals
3. Perform fine-grained calibration analysis with adaptive binning strategies to better understand calibration patterns across different probability regions