---
ver: rpa2
title: Leveraging Normalization Layer in Adapters With Progressive Learning and Adaptive
  Distillation for Cross-Domain Few-Shot Learning
arxiv_id: '2312.11260'
source_url: https://arxiv.org/abs/2312.11260
tags:
- domain
- learning
- domains
- distillation
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses cross-domain few-shot learning, where models
  must adapt to new domains with limited labeled samples. The key idea is to use two
  adapters with different normalization strategies: one without normalization for
  similar domains, and another with normalization for dissimilar domains.'
---

# Leveraging Normalization Layer in Adapters With Progressive Learning and Adaptive Distillation for Cross-Domain Few-Shot Learning

## Quick Facts
- arXiv ID: 2312.11260
- Source URL: https://arxiv.org/abs/2312.11260
- Reference count: 40
- Primary result: ProLAD achieves 74.7% average accuracy in multi-domain setting, outperforming state-of-the-art approaches with up to +7.9% improvements on highly dissimilar domains

## Executive Summary
This paper addresses the challenge of cross-domain few-shot learning by proposing a two-adapter architecture with progressive learning and adaptive distillation. The method uses Task Adapter (TA) without normalization for similar domains and Task Adapter with Normalization (TAN) for dissimilar domains, with an adaptive coefficient that dynamically controls which adapter is active based on domain similarity. The approach achieves state-of-the-art performance on the Meta-Dataset benchmark, particularly excelling on domains with high distributional shift from the source domain.

## Method Summary
The method employs a three-stage progressive learning approach using ResNet18 as feature extractor. Stage 1 pretrains the feature extractor on ImageNet. Stage 2 fine-tunes only the Task Adapter (TA) without normalization on the support set, saving features as teacher representations. Stage 3 fine-tunes both TA and Task Adapter with Normalization (TAN) using adaptive distillation, where the distillation strength is controlled by an adaptive coefficient computed from domain similarity metrics. The adaptive coefficient can be estimated either from feature similarities (difference between inter-class and feature similarities) or from support set performance metrics.

## Key Results
- Achieves 74.7% average accuracy on Meta-Dataset multi-domain setting
- Outperforms state-of-the-art methods by up to +7.9% on highly dissimilar domains
- Shows balanced performance across all 13 Meta-Dataset domains
- Demonstrates the effectiveness of progressive learning and adaptive distillation in few-shot scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using two adapters with different normalization strategies allows selective adaptation based on domain similarity.
- Mechanism: The Task Adapter without normalization (TA) is more effective for similar domains because it preserves well-represented batch statistics from the source domain. The Task Adapter with normalization (TAN) is better for dissimilar domains because it can adapt to the target domain's statistics. Adaptive distillation dynamically controls which adapter is active.
- Core assumption: Domain similarity can be reliably estimated from features, and the optimal adapter choice depends on this similarity.
- Evidence anchors:
  - [abstract] "our methodology utilizes two separate adapters: one devoid of a normalization layer, which is more effective for similar domains, and another embedded with a normalization layer, designed to leverage the batch statistics of the target domain, thus proving effective for dissimilar domains."
  - [section] "Our approach exhibits performance akin to TA in similar domains and mirrors TAN in dissimilar domains, implying that our method effectively increases the activation of optimal adapter based on the domain similarity."
  - [corpus] Weak - no direct evidence in corpus about this specific mechanism.

### Mechanism 2
- Claim: Progressive learning with adaptive distillation serves as regularization during adapter training.
- Mechanism: Training TA first establishes a baseline feature representation. Then training both adapters with adaptive distillation allows the model to learn domain-specific adaptations while preventing overfitting to few samples. The adaptive coefficient modulates distillation strength based on domain similarity.
- Core assumption: Distillation from a simpler adapter to a more complex one can regularize training and prevent overfitting in few-shot scenarios.
- Evidence anchors:
  - [abstract] "to address the pitfalls of noisy statistics, we deploy two strategies: a progressive training of the two adapters and an adaptive distillation technique derived from features determined by the model solely with the adapter devoid of a normalization layer."
  - [section] "Both progressive learning and adaptive distillation emerge as crucial for regularization during the concurrent training of both adapters; this is evidenced by the peak performance exhibited by ProLAD."
  - [corpus] Weak - no direct evidence in corpus about this specific mechanism.

### Mechanism 3
- Claim: The adaptive coefficient for distillation can be estimated from feature similarities or performance metrics.
- Mechanism: Two methods are proposed: (1) difference between inter-class and feature similarities, and (2) support set loss and accuracy. These metrics correlate with domain similarity and control distillation strength.
- Core assumption: Domain similarity correlates with either (a) how distinct class prototypes are from individual feature similarities, or (b) performance metrics on the support set.
- Evidence anchors:
  - [section] "Our method estimates domain similarity by comparing the distinctness of class prototypes to the similarity between pairs of individual samples" and "These metrics are especially relevant in understanding task distribution shift or task difficulty, both correlating with domain shift."
  - [section] "Coefficients calculated with Eq. (3) exhibit a high correlation with domain similarity, as illustrated in Figure 4, showcasing the efficacy of this metric as a reliable estimator of domain similarity."
  - [corpus] Weak - no direct evidence in corpus about these specific estimation methods.

## Foundational Learning

- Concept: Domain adaptation and the challenges of batch normalization in cross-domain scenarios
  - Why needed here: The paper addresses cross-domain few-shot learning, where batch normalization statistics from the source domain can be misleading for target domains.
  - Quick check question: Why might batch normalization layers trained on one domain perform poorly when applied to a different domain?

- Concept: Knowledge distillation and its application beyond model compression
  - Why needed here: The paper uses distillation as a regularization technique during adapter training, not just for model compression.
  - Quick check question: How can distillation be used as a regularizer rather than just a compression technique?

- Concept: Progressive learning and staged training approaches
  - Why needed here: The paper employs progressive learning to train adapters sequentially, which helps establish a baseline before introducing domain-specific adaptations.
  - Quick check question: What are the benefits of training components of a model in stages rather than all at once?

## Architecture Onboarding

- Component map: Feature extractor -> TA (1x1 conv, no normalization) -> TAN (SN + group conv) -> Classifier (cosine distance) -> Adaptive coefficient estimator

- Critical path:
  1. Pretrain feature extractor on source data
  2. Fine-tune TA on support set (Stage 2)
  3. Compute teacher features and adaptive coefficient
  4. Fine-tune both adapters with adaptive distillation (Stage 3)

- Design tradeoffs:
  - Normalization vs no normalization: normalization helps with dissimilar domains but can introduce noise for similar domains
  - Progressive learning vs joint training: progressive learning provides better regularization but requires more training steps
  - Adaptive distillation coefficient estimation: similarity-based vs performance-based approaches have different strengths

- Failure signatures:
  - Poor performance on similar domains: likely not relying enough on TA
  - Poor performance on dissimilar domains: likely not leveraging TAN enough
  - Unstable training: adaptive coefficient might be oscillating too much
  - Overfitting: distillation might be too weak or progressive learning not effective

- First 3 experiments:
  1. Compare TA-only vs TAN-only performance across domains to validate the domain similarity hypothesis
  2. Test different adaptive coefficient estimation methods (similarity-based vs performance-based)
  3. Ablate progressive learning by comparing staged training vs joint training of both adapters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ProLAD change when using different feature extractor architectures beyond ResNet18, such as ResNet50 or Vision Transformers?
- Basis in paper: [explicit] The paper states "We use the standard ResNet18 (He et al. 2016) model as the feature extractor, which is a commonly used architecture for CD-FSL."
- Why unresolved: The paper only evaluates ProLAD using ResNet18. Testing with other architectures could reveal whether the method's effectiveness generalizes to more complex models or different architectural paradigms.
- What evidence would resolve it: Comparative experiments showing ProLAD's performance across multiple feature extractor architectures (e.g., ResNet50, ViT) on the same benchmarks.

### Open Question 2
- Question: What is the impact of different domain similarity metrics on the adaptive coefficient calculation, and how robust is ProLAD to the choice of metric?
- Basis in paper: [explicit] The paper uses Earth-Mover Distance (EMD) for domain similarity and notes "We utilize the Earth-Mover Distance (EMD) to estimate the similarity between two domains."
- Why unresolved: While EMD is used, the paper doesn't explore alternative metrics like Maximum Mean Discrepancy (MMD) or Wasserstein distance, which could potentially offer better correlation with domain similarity for adaptive coefficient estimation.
- What evidence would resolve it: Systematic comparison of ProLAD's performance using different domain similarity metrics to determine which yields the most effective adaptive coefficient estimation.

### Open Question 3
- Question: How does ProLAD perform in settings where the test domains have overlapping classes with the source domain versus completely disjoint classes?
- Basis in paper: [explicit] The paper states "The core objective of this learning paradigm is to cultivate a model that can generalize effectively to new classes within the target domain using a limited set of examples."
- Why unresolved: The paper focuses on completely disjoint classes between source and target domains, but real-world scenarios might involve partial overlap. Understanding ProLAD's behavior in this intermediate case could reveal limitations or advantages.
- What evidence would resolve it: Experiments comparing ProLAD's performance on datasets with varying degrees of class overlap between source and target domains.

## Limitations
- The core hypothesis linking domain similarity to adapter effectiveness rests on empirical observations rather than theoretical guarantees
- The adaptive coefficient estimation methods need more rigorous validation across diverse domain pairs
- The progressive learning framework assumes staged training is superior without comprehensive ablation studies

## Confidence

- High confidence: The general approach of using domain-specific normalization strategies is well-supported by domain adaptation literature
- Medium confidence: The specific implementation of two-adapter architecture and progressive learning shows promising results but requires more ablation studies
- Low confidence: The adaptive coefficient estimation methods need more rigorous validation, particularly their robustness across diverse domain pairs

## Next Checks

1. **Ablation study of training strategies**: Compare progressive learning against joint training of both adapters to quantify the regularization benefit claimed by the authors.

2. **Coefficient estimation robustness**: Test the adaptive coefficient methods across all 13 Meta-Dataset domains to verify consistent correlation with domain similarity, particularly for domain pairs where one domain is highly dissimilar to source but similar to another target domain.

3. **Normalization layer analysis**: Conduct experiments isolating the effect of normalization in TAN by comparing against adapters with different normalization strategies (batch norm, layer norm, instance norm) to determine if the specific normalization choice is critical to performance.