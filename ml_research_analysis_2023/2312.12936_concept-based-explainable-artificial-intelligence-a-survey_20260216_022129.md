---
ver: rpa2
title: 'Concept-based Explainable Artificial Intelligence: A Survey'
arxiv_id: '2312.12936'
source_url: https://arxiv.org/abs/2312.12936
tags:
- concept
- concepts
- concept-based
- methods
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first systematic review of Concept-based
  eXplainable Artificial Intelligence (C-XAI) methods. The authors propose a taxonomy
  of C-XAI methods based on how concepts are employed (post-hoc vs.
---

# Concept-based Explainable Artificial Intelligence: A Survey

## Quick Facts
- arXiv ID: 2312.12936
- Source URL: https://arxiv.org/abs/2312.12936
- Reference count: 40
- Primary result: First systematic review of Concept-based eXplainable Artificial Intelligence (C-XAI) methods

## Executive Summary
This paper provides the first comprehensive systematic review of Concept-based eXplainable Artificial Intelligence (C-XAI) methods. The authors propose a taxonomy of C-XAI methods based on concept training approaches (post-hoc vs. by-design), concept types (symbolic, unsupervised basis, prototypes, textual), and explanation types (class-concept relation, node-concept association, concept visualization). They analyze over 30 methods across 13 dimensions and provide guidelines for selecting appropriate approaches based on user requirements. The survey also covers evaluation strategies, including metrics, human evaluations, and datasets, while identifying key challenges such as performance loss in concept-based models, information leakage issues, and vulnerability to adversarial attacks.

## Method Summary
The authors conducted a systematic review of C-XAI literature from 2017 to July 2023, collecting papers from major AI conferences (NeurIPS, ICLR, ICML, AAAI, IJCAI, ICCV, CVPR) and journals (Nature Machine Intelligence, TMLR, Artificial Intelligence). They defined key concepts and explanations, proposed a taxonomy categorizing methods based on training approach, concept type, and explanation type, and analyzed over 30 methods across 13 dimensions including concept characteristics, applicability, and evaluation resources. The paper provides guidelines for selecting C-XAI approaches based on user requirements and surveys existing evaluation strategies including metrics, human evaluations, and datasets.

## Key Results
- Proposed the first systematic taxonomy of C-XAI methods based on concept training, concept type, and explanation type
- Identified significant performance degradation (up to 20%) in concept-based models compared to black-box models
- Highlighted vulnerability of post-hoc methods to adversarial attacks through concept manipulation
- Identified gaps in the field including lack of node-concept association for unsupervised methods and limited generative approaches for post-hoc methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised concept-based explainability methods effectively assess the influence of symbolic concepts on model predictions by analyzing latent space projections.
- Mechanism: Methods like T-CAV and TCAR train linear probes in the model's latent space to quantify the sensitivity of class predictions to concept activations, using concept activation vectors (CAVs) or concept activation regions (CARs) to represent concepts and measure their directional influence.
- Core assumption: Symbolic concepts are linearly separable in the latent space of the model.
- Evidence anchors:
  - [abstract] The paper defines TCAV as computing the inner product between the directional derivatives of the class and the CAVs to quantify the models' conceptual sensitivity.
  - [section] CAR relaxes the linear separability assumption of concepts imposed by T-CAV, only requiring concept examples to be grouped in clusters in the model's latent space such that they are identifiable by a non-linear probe.
  - [corpus] The corpus contains related work on concept-based XAI, including a survey on concept-based approaches for model improvement, suggesting the broader relevance of this mechanism.
- Break condition: If concepts are not linearly separable or clusterable in the latent space, the linear probe or non-linear probe methods will fail to accurately represent and measure concept influence.

### Mechanism 2
- Claim: Unsupervised concept-based methods extract meaningful concepts from data without requiring labeled concept annotations by clustering similar samples in the latent space.
- Mechanism: Methods like ACE and ICE segment images at multiple resolutions, project these segments into the model's latent space, and cluster them to identify common patterns representing concepts, then analyze the correlation of these clusters with class predictions.
- Core assumption: The latent space of the model captures meaningful semantic representations of data that can be clustered to identify concepts.
- Evidence anchors:
  - [abstract] ACE involves segmenting images belonging to a specific class at multiple resolutions, encompassing textures, object parts, and complete objects, thus capturing a diverse spectrum of concepts.
  - [section] ICE replaces K-means with Non-Negative Matrix Factorization (NMF) for extracting concept vectors, using NF over the feature maps to uncover a concept for each reduced dimension.
  - [corpus] The corpus contains related work on concept discovery, suggesting the broader relevance of this mechanism.
- Break condition: If the latent space does not capture meaningful semantic representations, or if the clustering algorithm fails to identify coherent clusters, the extracted concepts will be meaningless or non-coherent.

### Mechanism 3
- Claim: Explainable-by-design concept-based models ensure that the network learns explicit concept representations by incorporating them into the model architecture, enabling concept intervention and counterfactual explanations.
- Mechanism: Methods like CBM and LEN introduce an intermediate concept bottleneck layer that predicts concept scores, which are then used to predict the final class, forcing the network to learn and represent the concepts explicitly.
- Core assumption: The model can learn to predict concepts accurately while maintaining task performance.
- Evidence anchors:
  - [abstract] CBM design enhances the model's transparency and enables interaction with the model itself, allowing a domain expert to intervene in the predicted concepts.
  - [section] CEM improves CBM performance by using an entire set of neurons to represent a concept (concept embedding), obtaining the same classification accuracy of black-box neural networks.
  - [corpus] The corpus contains related work on concept-based models, suggesting the broader relevance of this mechanism.
- Break condition: If the concept bottleneck layer significantly degrades task performance or if the model fails to learn accurate concept representations, the approach will not be viable.

## Foundational Learning

- Concept: Concept-based explanations
  - Why needed here: To understand the difference between standard XAI methods (focusing on feature importance) and concept-based methods (focusing on higher-level abstractions).
  - Quick check question: What is the key difference between a saliency map and a concept-based explanation?
- Concept: Latent space
  - Why needed here: To understand how concepts are represented and analyzed within the model's internal representations.
  - Quick check question: What is the role of the latent space in concept-based explainability methods?
- Concept: Concept activation vectors (CAVs)
  - Why needed here: To understand how supervised concept-based methods quantify the influence of concepts on class predictions.
  - Quick check question: How are CAVs used to measure the importance of a concept for a specific class?

## Architecture Onboarding

- Component map:
  - Input layer: Receives raw data (images, text, etc.)
  - Encoder: Transforms input into latent representations
  - Concept layer: (Explainable-by-design only) Explicitly represents concepts
  - Decoder: (Unsupervised methods only) Reconstructs input from latent representations
  - Classifier: Predicts final class labels
  - Explanation module: Generates concept-based explanations
- Critical path: Input → Encoder → Concept layer (if applicable) → Classifier → Output
- Design tradeoffs:
  - Performance vs. interpretability: Concept-based models may sacrifice some predictive accuracy for increased transparency.
  - Supervision vs. unsupervised learning: Supervised methods require labeled concept annotations, while unsupervised methods do not.
  - Post-hoc vs. explainable-by-design: Post-hoc methods analyze existing models, while explainable-by-design models incorporate concepts during training.
- Failure signatures:
  - Poor concept accuracy: The model fails to learn accurate concept representations.
  - Performance degradation: The concept bottleneck layer significantly reduces task performance.
  - Meaningless concepts: The extracted concepts are not coherent or interpretable.
- First 3 experiments:
  1. Train a simple CNN on a dataset with known concepts (e.g., CUB-200-2011) and apply TCAV to assess the influence of specific concepts on class predictions.
  2. Implement ACE on a standard image classification dataset (e.g., CIFAR-10) to extract unsupervised concepts and analyze their correlation with class labels.
  3. Build a CBM on a tabular dataset with interpretable features (e.g., COMPAS) and evaluate its performance and interpretability compared to a black-box model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generative approaches be effectively adapted for post-hoc concept-based explanation methods?
- Basis in paper: [explicit] The paper discusses generative concept-based models and suggests that generative approaches could be explored for post-hoc methods as well.
- Why unresolved: The paper does not provide any empirical evidence or detailed analysis on the effectiveness of generative approaches for post-hoc methods. It only mentions the potential of such an adaptation.
- What evidence would resolve it: Experimental results comparing the performance of post-hoc methods using generative approaches with traditional post-hoc methods would provide evidence on the effectiveness of this adaptation.

### Open Question 2
- Question: How can node-concept association explanations be effectively implemented for unsupervised concept-based methods?
- Basis in paper: [explicit] The paper identifies a gap in exploring node-concept association explanations for unsupervised methods, despite the challenges in associating nodes with extracted concepts.
- Why unresolved: The paper does not provide any solutions or detailed discussion on how to overcome the challenges of implementing node-concept association for unsupervised methods.
- What evidence would resolve it: A detailed methodology and empirical results demonstrating successful implementation of node-concept association for unsupervised methods would provide evidence on how to address this challenge.

### Open Question 3
- Question: What are the most effective metrics and datasets for evaluating concept-based explainable AI methods?
- Basis in paper: [explicit] The paper discusses various metrics and datasets used in the literature but highlights the lack of standardized benchmarks and datasets for concept-based XAI.
- Why unresolved: The paper does not provide a definitive answer on which metrics and datasets are most effective for evaluating concept-based methods, as it only reviews the existing ones.
- What evidence would resolve it: A comprehensive study comparing the effectiveness of different metrics and datasets in evaluating concept-based methods would provide evidence on the most effective evaluation tools.

## Limitations

- Specific papers analyzed are not listed, making verification of coverage completeness difficult
- 13 analysis dimensions are not fully specified beyond their categories
- Performance degradation claims rely on aggregated results across heterogeneous studies rather than systematic measurement

## Confidence

- High confidence: Taxonomy structure and definitions of concepts/explanations
- Medium confidence: Analysis of evaluation strategies and metrics
- Medium confidence: Performance claims due to aggregation across heterogeneous studies

## Next Checks

1. Compile and verify the complete list of analyzed papers to ensure systematic coverage of the field
2. Implement a subset of representative methods (e.g., TCAV, ACE, CBM) on standard datasets to empirically validate the claimed performance tradeoffs
3. Conduct controlled experiments to test the vulnerability of post-hoc methods to adversarial attacks using the proposed adversarial attack framework on benchmark datasets