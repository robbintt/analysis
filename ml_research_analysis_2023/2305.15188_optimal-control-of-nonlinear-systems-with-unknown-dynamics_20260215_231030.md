---
ver: rpa2
title: Optimal Control of Nonlinear Systems with Unknown Dynamics
arxiv_id: '2305.15188'
source_url: https://arxiv.org/abs/2305.15188
tags:
- policy
- learning
- dynamics
- control
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a data-driven method for finding a closed-loop
  optimal controller for systems with unknown dynamics. The method introduces a novel
  gradient estimation framework that approximates the gradient of the cost function
  with respect to policy parameters via integrating the Koopman operator with the
  classical concept of actor-critic.
---

# Optimal Control of Nonlinear Systems with Unknown Dynamics

## Quick Facts
- **arXiv ID**: 2305.15188
- **Source URL**: https://arxiv.org/abs/2305.15188
- **Reference count**: 40
- **Key outcome**: Presents a data-driven method for finding closed-loop optimal controllers for systems with unknown dynamics using Koopman operator theory and actor-critic methods

## Executive Summary
This paper introduces a novel framework for optimal control of nonlinear systems with unknown dynamics by integrating Koopman operator theory with actor-critic methods. The approach learns a linear approximation of nonlinear dynamics in a lifted feature space, enabling gradient-based policy optimization without requiring explicit system models. The method avoids accumulated prediction errors through temporal-difference learning and provides convergence guarantees under specific learning rate hierarchies. Performance is validated through simulations comparing against DDPG and PETS baselines.

## Method Summary
The method employs a three-component optimization framework: (1) Deep Koopman Representation (DKR) learns a linear approximation of nonlinear dynamics by mapping states to a lifted space via a DNN, (2) Temporal-difference learning estimates the value function to avoid error accumulation in long-horizon tasks, and (3) Policy gradient updates optimize control actions using the learned model and value function. The approach iteratively updates these components using experience batches collected from system-environment interactions, with convergence enforced through a learning rate hierarchy (αf > αJ > αµ).

## Key Results
- Demonstrates successful policy optimization for nonlinear systems without requiring explicit dynamics models
- Shows improved performance compared to DDPG and PETS methods in simulation experiments
- Provides theoretical convergence analysis for the proposed framework

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Deep Koopman representation enables linear approximation of nonlinear dynamics for gradient-based policy optimization
- **Core assumption**: Nonlinear dynamics can be well-approximated by a linear operator in a lifted space learned by a DNN
- **Evidence anchors**: Koopman operator integration mentioned in abstract; methodology references prior work on Koopman-based system approximation
- **Break condition**: Poor Koopman representation leads to large prediction errors and unstable policy updates

### Mechanism 2
- **Claim**: TD learning with approximate value function avoids accumulated prediction errors for long-horizon tasks
- **Core assumption**: TD error minimization converges to accurate cost-to-go estimates using Bellman's principle
- **Evidence anchors**: Abstract mentions avoiding accumulated errors through Bellman's principle; paper discusses TD learning benefits
- **Break condition**: Poor value function approximation or TD learning divergence causes suboptimal policy updates

### Mechanism 3
- **Claim**: Learning rate hierarchy (αf > αJ > αµ) ensures stable policy improvement
- **Core assumption**: Prescribed convergence order guarantees that policy updates are based on increasingly accurate models and value functions
- **Evidence anchors**: Paper explicitly assigns convergence order and discusses learning rate assignment
- **Break condition**: Improper learning rate tuning violates convergence order, leading to unstable updates

## Foundational Learning

- **Concept**: Koopman operator theory
  - Why needed here: Provides foundation for representing nonlinear dynamics as linear operators in lifted space
  - Quick check question: How does the Koopman operator transform a nonlinear dynamical system into a linear system in a higher-dimensional space?

- **Concept**: Temporal-difference learning
  - Why needed here: Enables value function learning without explicit dynamics model and avoids error accumulation
  - Quick check question: What is the key difference between temporal-difference learning and Monte Carlo methods in terms of value function estimation?

- **Concept**: Actor-critic methods
  - Why needed here: Combines policy and value function for gradient-based policy optimization
  - Quick check question: In actor-critic methods, how does the critic's estimate of the value function guide the actor's policy updates?

## Architecture Onboarding

- **Component map**: DNN g(·,θf) -> Linear dynamics (A,B,C) -> Value function DNN ˆJ(·,θJ) -> Policy DNN µ(·,θµ) -> Replay buffer
- **Critical path**: Collect experience -> Update Koopman model (θf) -> Update value function (θJ) -> Update policy (θµ) -> Iterate
- **Design tradeoffs**: Accuracy of Koopman model vs. DNN complexity; sample efficiency vs. value function bias; convergence speed vs. stability
- **Failure signatures**: Large prediction errors, high TD error, degrading policy performance
- **First 3 experiments**: 1) Test Koopman model prediction accuracy on known nonlinear system, 2) Verify TD learning convergence on simple task, 3) Evaluate policy gradient updates on system with known optimal policy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the convergence guarantees for PGDK algorithm when applied to high-dimensional nonlinear systems?
- Basis in paper: [explicit] Mentions capability to represent complex nonlinear models but lacks explicit convergence guarantees for high-dimensional systems
- Why unresolved: Theoretical analysis provided but doesn't address high-dimensional case specifically
- What evidence would resolve it: Theoretical proof or extensive numerical simulations demonstrating convergence for high-dimensional systems

### Open Question 2
- Question: How does PGDK performance compare to other model-based RL methods like Gaussian Processes or Neural Networks?
- Basis in paper: [inferred] Compares with DDPG and PETS but not other model-based RL methods
- Why unresolved: Focus on DDPG and PETS comparisons without exploring other model-based approaches
- What evidence would resolve it: Numerical simulations or theoretical analysis comparing PGDK with other model-based RL methods

### Open Question 3
- Question: What are the limitations of PGDK when applied to systems with high-dimensional state and control spaces?
- Basis in paper: [inferred] Doesn't explicitly discuss limitations for high-dimensional state/control spaces
- Why unresolved: Focuses on advantages without addressing limitations in high-dimensional settings
- What evidence would resolve it: Numerical simulations or theoretical analysis demonstrating performance on high-dimensional systems

## Limitations
- Accuracy heavily depends on quality of Koopman representation approximation
- Computational complexity of pseudoinverse operations for updating linear dynamics matrices not discussed
- Learning rate hierarchy sensitivity and hyperparameter tuning requirements not extensively validated

## Confidence

- **High confidence**: Integration of Koopman operator theory with actor-critic methods for policy optimization
- **Medium confidence**: Convergence analysis based on learning rate hierarchy appears reasonable but practically sensitive
- **Low confidence**: TD learning preventing error accumulation claims lack empirical validation and corpus support

## Next Checks
1. Test Koopman representation accuracy on known nonlinear system with verifiable eigenvalues to quantify approximation error
2. Implement algorithm with varying learning rate hierarchies to empirically test convergence requirements and sensitivity
3. Evaluate policy performance on significantly longer time horizons than presented to verify TD learning prevents error accumulation