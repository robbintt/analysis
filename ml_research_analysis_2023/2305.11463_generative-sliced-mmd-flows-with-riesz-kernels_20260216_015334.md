---
ver: rpa2
title: Generative Sliced MMD Flows with Riesz Kernels
arxiv_id: '2305.11463'
source_url: https://arxiv.org/abs/2305.11463
tags:
- gradient
- generative
- learning
- sliced
- riesz
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents an efficient method for computing maximum
  mean discrepancy (MMD) flows using Riesz kernels. The key insight is that MMD with
  Riesz kernels has a special property: its MMD coincides with the sliced MMD, allowing
  reduction of the computation to a one-dimensional setting.'
---

# Generative Sliced MMD Flows with Riesz Kernels

## Quick Facts
- arXiv ID: 2305.11463
- Source URL: https://arxiv.org/abs/2305.11463
- Reference count: 40
- Key outcome: Efficient method for computing MMD flows using Riesz kernels, achieving competitive FID scores on MNIST, FashionMNIST, and CIFAR10 datasets

## Executive Summary
This paper introduces an efficient method for computing maximum mean discrepancy (MMD) flows using Riesz kernels, which enables fast sorting algorithms for gradient computation with complexity O((M+N) log(M+N)). The key insight is that MMD with Riesz kernels coincides with sliced MMD, allowing reduction to a one-dimensional setting. The method is applied to generative modeling, where neural networks are trained to approximate MMD particle flows. Experiments demonstrate the efficiency of the approach for image generation on standard datasets, achieving competitive Fréchet Inception Distance (FID) scores.

## Method Summary
The method uses Riesz kernels to compute MMD flows efficiently by leveraging the property that MMD with Riesz kernels coincides with sliced MMD. This enables a sorting-based algorithm for computing gradients with complexity O((M+N) log(M+N)). The approach involves simulating momentum MMD flows using stochastic approximation of sliced MMD gradients, and training a sequence of UNet networks to approximate these flows. The method is applied to generative modeling tasks on MNIST, FashionMNIST, and CIFAR10 datasets.

## Key Results
- Achieved competitive FID scores on MNIST, FashionMNIST, and CIFAR10 datasets
- Reduced computational complexity from O(MN+N²) to O((M+N) log(M+N)) for MMD gradient computation
- Demonstrated faster convergence using momentum MMD flows compared to standard MMD flows

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MMD with Riesz kernels coincides with the sliced MMD, enabling efficient computation.
- Mechanism: The special property of Riesz kernels (K(x,y) = -||x-y||^r) allows the MMD to be expressed as an expectation over one-dimensional projections, reducing the computational complexity from O(MN+N²) to O((M+N)log(M+N)).
- Core assumption: The kernel is a Riesz kernel with r ∈ (0,2).
- Evidence anchors:
  - [abstract]: "the MMD of Riesz kernels, which is also known as energy distance, coincides with the MMD of their sliced version."
  - [section 2]: "we show that for Riesz kernels we have the outstanding property that their MMD coincides with the sliced MMD."
  - [corpus]: Strong, multiple papers discuss the property of Riesz kernels in MMD flows.

### Mechanism 2
- Claim: Stochastic approximation of sliced MMD gradients converges with error O(sqrt(d/P)).
- Mechanism: By using a finite number P of random projections and concentration inequalities, the error in the gradient estimation decreases as the square root of the ratio of data dimension to the number of projections.
- Core assumption: The projections are independent and uniformly distributed on the sphere.
- Evidence anchors:
  - [abstract]: "we prove that the error induced by this approximation behaves asymptotically as O(sqrt(d/P))".
  - [section 3]: "Theorem 4 (Error Bound for Stochastic MMD Gradients). Let x1,...,x N,y 1,...,y M ∈ Rd. Then, it holds that E[||~∇P_Fd(x1,...,x N|y1,...,y M) - ∇Fd(x1,...,x N|y1,...,y M)||] ∈ O(sqrt(d/P))."
  - [corpus]: Weak, the corpus does not provide direct evidence for this specific error bound.

### Mechanism 3
- Claim: Momentum MMD flows converge faster than standard MMD flows.
- Mechanism: Introducing a momentum term in the gradient flow iteration accelerates convergence by incorporating previous gradient information.
- Core assumption: The momentum parameter m is chosen appropriately (0 ≤ m < 1).
- Evidence anchors:
  - [section 4.1]: "To reduce the required number of steps in (9), we introduce a momentum parameter."
  - [section 4.1]: "We observe that the momentum MMD flow (10) converges indeed faster than the MMD flow (9) without momentum."
  - [corpus]: Weak, the corpus does not provide direct evidence for the effectiveness of momentum in MMD flows.

## Foundational Learning

- Concept: Maximum Mean Discrepancy (MMD)
  - Why needed here: MMD is the core metric used to measure the distance between the generated samples and the target distribution.
  - Quick check question: What is the definition of MMD and how is it computed for empirical measures?

- Concept: Riesz Kernels
  - Why needed here: Riesz kernels have the special property that their MMD coincides with the sliced MMD, enabling efficient computation.
  - Quick check question: What is the definition of a Riesz kernel and why are they used in this paper?

- Concept: Sliced Wasserstein Distance
  - Why needed here: The sliced MMD is a variant of the sliced Wasserstein distance, which is used to reduce the computational complexity in high dimensions.
  - Quick check question: How does the sliced Wasserstein distance differ from the standard Wasserstein distance and why is it useful in high-dimensional settings?

## Architecture Onboarding

- Component map:
  - Input samples from uniform distribution -> MMD Flow simulation -> Sliced MMD computation -> UNet network training -> Generated samples

- Critical path:
  - Draw initial samples from the uniform distribution
  - Simulate the momentum MMD flow using the sliced MMD computation
  - Train the neural networks to approximate the MMD flow
  - Generate new samples by applying the trained neural networks to the initial samples

- Design tradeoffs:
  - Number of projections P: Increasing P improves the accuracy of the sliced MMD computation but increases computational cost
  - Momentum parameter m: Choosing m too high can lead to instability, while choosing it too low slows down convergence
  - Number of neural networks L: Increasing L allows for a more accurate approximation of the MMD flow but increases the computational cost and memory requirements

- Failure signatures:
  - High FID scores: Indicates that the generated samples are not similar enough to the target distribution
  - Unstable training: Can be caused by an inappropriate choice of the momentum parameter or a too high learning rate
  - Slow convergence: Can be caused by a too low momentum parameter or a too small number of neural networks

- First 3 experiments:
  1. Generate samples from a simple 2D Gaussian distribution using a small number of projections (P=10) and a small number of neural networks (L=5). Measure the FID score and visualize the generated samples.
  2. Increase the number of projections (P=100) and the number of neural networks (L=10) and compare the FID score and visual quality of the generated samples.
  3. Experiment with different momentum parameters (m=0.5, 0.7, 0.9) and observe the effect on the convergence speed and the quality of the generated samples.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sorting algorithm for computing MMD gradients be extended to non-Euclidean domains like the sphere?
- Basis in paper: [explicit] The authors mention that sliced probability metrics are closely related to the Radon transform and are of interest for non-Euclidean domains like the sphere
- Why unresolved: The paper only demonstrates the method for Euclidean domains and does not explore non-Euclidean applications
- What evidence would resolve it: A proof that the sorting algorithm can be adapted for spherical domains, or experimental results showing successful application to spherical data

### Open Question 2
- Question: How does the choice of momentum parameter affect convergence speed and final sample quality in different dimensional spaces?
- Basis in paper: [explicit] The authors use different momentum parameters (m = 0.9 for MNIST, m = 0.6 for CIFAR10) but don't provide a systematic analysis of momentum effects
- Why unresolved: The paper only tests two specific momentum values without exploring the parameter space or providing theoretical guidance on momentum selection
- What evidence would resolve it: A comprehensive study comparing different momentum values across various datasets and dimensionalities, showing convergence rates and FID scores

### Open Question 3
- Question: Can the computational complexity of O((M+N) log(M+N)) be further improved for even larger scale applications?
- Basis in paper: [inferred] While the paper significantly improves complexity from O(MN+N²) to O((M+N) log(M+N)), it doesn't explore whether further optimization is possible
- Why unresolved: The paper focuses on demonstrating the current complexity improvement but doesn't investigate potential additional optimizations
- What evidence would resolve it: Development of a more efficient algorithm with lower computational complexity, or theoretical proof that O((M+N) log(M+N)) is optimal for this problem

## Limitations

- Limited empirical validation on relatively small-scale image generation tasks (MNIST, FashionMNIST, CIFAR10)
- Computational advantages for large-scale applications need further verification on more challenging datasets
- Effectiveness of momentum MMD flows and stochastic approximation needs more extensive empirical validation across diverse datasets and scenarios

## Confidence

*High confidence*: The theoretical framework connecting Riesz kernels to sliced MMD computation is well-established in the literature and the sorting algorithm for gradient computation is mathematically sound.

*Medium confidence*: The claimed computational complexity improvements (O((M+N)log(M+N)) versus O(MN+N²)) are theoretically valid, but practical performance may vary depending on implementation details and hardware constraints.

*Low confidence*: The effectiveness of the proposed momentum MMD flows and the stochastic approximation of sliced MMD gradients needs more extensive empirical validation across diverse datasets and scenarios.

## Next Checks

1. **Scale test**: Evaluate the method on high-resolution image datasets (e.g., CelebA, LSUN) to verify the claimed computational advantages for large-scale applications.

2. **Robustness test**: Systematically vary the number of projections P, momentum parameters, and network architectures to assess the method's sensitivity and identify optimal configurations.

3. **Theoretical validation**: Rigorously verify the error bound O(sqrt(d/P)) for stochastic MMD gradients through extensive experiments across different data dimensions and projection counts.