---
ver: rpa2
title: Long Horizon Temperature Scaling
arxiv_id: '2302.03686'
source_url: https://arxiv.org/abs/2302.03686
tags:
- temperature
- lhts
- scaling
- myopic
- horizon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes long horizon temperature scaling (LHTS), a
  general method to temperature scale the joint distribution of likelihood based models.
  LHTS requires finetuning a likelihood-based model on a temperature-dependent objective,
  after which the model can sample long-horizon temperature-scaled outputs without
  any additional cost over standard sampling.
---

# Long Horizon Temperature Scaling

## Quick Facts
- arXiv ID: 2302.03686
- Source URL: https://arxiv.org/abs/2302.03686
- Reference count: 5
- Primary result: LHTS improves sample quality and accuracy on multiple choice analogy tasks by 10% over myopic temperature scaling

## Executive Summary
Long Horizon Temperature Scaling (LHTS) is a method for temperature scaling the joint distribution of likelihood-based generative models. Unlike myopic scaling which adjusts per-token probabilities, LHTS learns to rescale the full sequence likelihood through fine-tuning on a temperature-dependent objective. This enables generation of high-quality samples at any temperature, including extrapolation beyond the training range, without additional cost during sampling. Experiments demonstrate improvements in likelihood, sample quality, and task performance across both image diffusion models and autoregressive language models.

## Method Summary
LHTS works by fine-tuning a pretrained likelihood-based model on a KL divergence objective that approximates the temperature-scaled joint distribution. The method uses importance sampling with variance reduction through baseline subtraction and horizon-limited suffixes for autoregressive models. A continuous temperature embedding allows smooth temperature control and extrapolation. After fine-tuning, the model can generate samples at any temperature using standard decoding algorithms, achieving better trade-offs between likelihood and diversity compared to myopic scaling.

## Key Results
- 10% accuracy improvement on multiple choice analogy task using GPT-2 models
- Better trade-off between sample likelihood and diversity compared to myopic scaling
- Successful extrapolation to temperatures outside training range while maintaining sample quality
- Improved sample quality measured by FID score for diffusion models and reduced repetition for language models

## Why This Works (Mechanism)

### Mechanism 1: Joint Distribution Rescaling
- Claim: LHTS corrects myopic temperature scaling bias by learning to jointly rescale full sequence likelihood instead of per-token rescaling
- Mechanism: Trains qT to approximate temperature-scaled joint distribution pT through KL(pT || qT) minimization using importance weighting with baseline subtraction for variance reduction
- Core assumption: Base model p(x) provides good approximation of data distribution, enabling reliable importance sampling
- Break condition: Poor base model fit causes unreliable importance weights and failed convergence

### Mechanism 2: Continuous Temperature Embedding
- Claim: Enables extrapolation to unseen temperatures through continuous temperature embedding learning
- Mechanism: Learns temperature token embedding that maps scalar temperatures to model's embedding space during training on range [0.9, 1.1], enabling smooth extrapolation
- Core assumption: Temperature-to-sharpness relationship is smooth and locally linear within training range
- Break condition: Highly nonlinear temperature relationship prevents meaningful extrapolation

### Mechanism 3: Diversity-Likelihood Balance
- Claim: Improves sample quality by better balancing likelihood and diversity through joint scaling
- Mechanism: Encourages selection of high joint likelihood sequences rather than greedy per-token choices, maintaining diversity in generated text
- Core assumption: High joint likelihood sequences are more likely to be diverse and coherent
- Break condition: Base model already lacks diversity, limiting LHTS effectiveness

## Foundational Learning

- **Importance sampling and variance reduction**: Required for stable training with importance weights; quick check: Why does subtracting baseline b help? (Answer: Keeps weights near 1, reducing variance without changing expectation)
- **KL divergence and strict properness**: Ensures theoretical soundness of LHTS objective; quick check: What is "strictly proper"? (Answer: Loss uniquely minimized when model matches target distribution)
- **Autoregressive factorization**: Enables efficient variance-reduced training through per-prefix decomposition; quick check: How does factorization enable decomposition? (Answer: Allows rewriting expectations over sequences into prefix/suffix components)

## Architecture Onboarding

- **Component map**: Base model pφ -> LHTS finetuning module -> Temperature embedding r -> Sampling interface
- **Critical path**: Compute log-likelihoods → Calculate importance weights with baseline → Apply variance-reduced loss → Update parameters → Deploy for sampling
- **Design tradeoffs**: Joint vs per-token scaling (quality vs cost), continuous vs discrete temperatures (flexibility vs complexity), horizon length (variance vs context)
- **Failure signatures**: Training instability (large weights → NaN), poor extrapolation (fails beyond training range), loss of diversity (repetitive samples)
- **First 3 experiments**: 1) Verify importance weight computation and visualization, 2) Test per-prefix decomposition matches full-sequence loss, 3) Validate extrapolation by sampling at extreme temperatures

## Open Questions the Paper Calls Out
- How variance of LHTS objective changes with different temperature ranges and optimal range for stability
- Impact of baseline choice on performance and stability
- Effect of horizon length on performance and efficiency for autoregressive models
- Comparison with top-k and nucleus sampling techniques
- Scalability to larger and more complex models beyond GPT-2

## Limitations
- Requires computationally expensive fine-tuning that may lead to overfitting
- Extrapolation capability relies on smoothness assumptions that may not hold universally
- Does not fully explore impact on long-horizon dependencies and coherence

## Confidence
- **High**: Theoretical foundation (importance sampling, KL divergence) is well-established and correctly applied; experimental results are convincing
- **Medium**: Extrapolation capability demonstrated but not thoroughly validated; comparison with myopic scaling limited to specific tasks
- **Low**: Impact on long-horizon coherence not fully explored; no explicit measurement of complex reasoning capabilities

## Next Checks
1. Test robustness to temperature range by training on varying ranges and evaluating extrapolation stability
2. Design long-horizon coherence task to compare LHTS vs myopic scaling on maintaining coherence over long sequences
3. Analyze computational overhead of LHTS vs myopic scaling and explore techniques to reduce overhead while maintaining quality