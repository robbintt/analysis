---
ver: rpa2
title: One-Shot Pruning for Fast-adapting Pre-trained Models on Devices
arxiv_id: '2307.04365'
source_url: https://arxiv.org/abs/2307.04365
tags:
- tasks
- pruning
- task
- smsp
- pruned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a one-shot pruning method for fast adaptation
  of pre-trained models on devices with limited resources. The method, called Scalable
  Mask Selection Pruning (SMSP), leverages knowledge from pruned models of similar
  tasks to generate a pruning mask for new tasks, reducing computational costs while
  maintaining high accuracy.
---

# One-Shot Pruning for Fast-adapting Pre-trained Models on Devices

## Quick Facts
- arXiv ID: 2307.04365
- Source URL: https://arxiv.org/abs/2307.04365
- Reference count: 32
- Key outcome: SMSP achieves 88.55% accuracy on ResNet-18 with 100 training iterations vs. 87.63% with 1000 iterations for AMP

## Executive Summary
This paper introduces Scalable Mask Selection Pruning (SMSP), a one-shot pruning method that leverages knowledge from pruned models of similar tasks to generate pruning masks for new tasks. By identifying task-specific filters/nodes through mask score aggregation from similar tasks, SMSP enables fast adaptation of pre-trained models with significantly fewer training iterations while maintaining high accuracy. The method demonstrates scalability across different memory constraints and achieves state-of-the-art performance on both CNNs and Vision Transformers across multiple datasets.

## Method Summary
SMSP creates pruning masks by leveraging knowledge from pruned models of similar tasks, identified using LEEP similarity scores. The method aggregates mask scores from these similar tasks to identify task-specific filters/nodes in the pre-trained model, then extracts a sub-network using a single round of pruning. This sub-network serves as initialization for fine-tuning the new task, requiring only a few training iterations compared to traditional pruning methods. The approach is scalable because the same pre-trained model can be adapted to different memory constraints by adjusting the pruning ratio.

## Key Results
- SMSP achieves 88.55% accuracy on ResNet-18 with 100 training iterations, outperforming AMP's 87.63% with 1000 iterations
- Consistently outperforms state-of-the-art pruning methods across CNNs and ViTs on CIFAR-100, ImageNet, and Caltech-256
- Maintains high accuracy while significantly reducing computational costs through fewer training iterations
- Demonstrates scalability by adapting to tasks with different memory constraints using the same pre-trained model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pruning mask generated from similar tasks preserves more task-specific filters/nodes, leading to faster adaptation with fewer iterations.
- Mechanism: The method leverages knowledge from pruned models of similar tasks by summing their mask scores to identify task-specific filters/nodes in the pre-trained model. This process creates a mask that retains more relevant parameters for the new task.
- Core assumption: Tasks with high similarities share more task-specific filters/nodes in their pruned models, making their pruned results useful for identifying task-specific parameters in new tasks.
- Evidence anchors:
  - [abstract] "we create a score mask using the pruned models of similar tasks to identify task-specific filters/nodes in the pre-trained model for the new task."
  - [section 3.2] "Similar tasks tend to share more task-specific filters/nodes. We observe that similar tasks tend to share more task-specific filters/nodes."
  - [corpus] Weak. No direct evidence in the corpus for this mechanism. The corpus shows related pruning methods but doesn't validate the similarity-based mask sharing.
- Break condition: If the similarity measure fails to accurately identify truly similar tasks, the mask will include irrelevant filters/nodes, reducing pruning effectiveness.

### Mechanism 2
- Claim: SMSP achieves comparable accuracy with significantly fewer training iterations by starting from a well-initialized sub-network.
- Mechanism: By leveraging the pruned knowledge of similar tasks, SMSP creates an initial sub-network that already contains task-specific parameters, reducing the need for extensive fine-tuning.
- Core assumption: The sub-network extracted using similar task knowledge is already close to optimal for the new task, requiring only minimal fine-tuning.
- Evidence anchors:
  - [abstract] "Based on this mask, we conduct a single round of pruning to extract a suitably-sized sub-network that can quickly adapt to the new task with only a few training iterations."
  - [section 4.2] "SMSP leverages a sub-network created by similar tasks as an initialization, hence, only a few training iterations are necessary to construct a well-performing pruned model."
  - [corpus] Weak. The corpus shows that one-shot pruning methods exist but doesn't provide evidence for the specific initialization advantage of SMSP.
- Break condition: If the initial sub-network is poorly initialized due to inaccurate mask generation, more training iterations will be required, negating the efficiency benefit.

### Mechanism 3
- Claim: SMSP is scalable because the same pre-trained model can be used across different memory constraints and task sizes.
- Mechanism: The method can generate masks for any pruning ratio, allowing the same pre-trained model to be adapted to different devices with varying memory constraints.
- Core assumption: The pre-trained model contains sufficient redundancy that different pruning ratios can be achieved without significant accuracy loss.
- Evidence anchors:
  - [abstract] "SMSP is scalable because the created mask can be used to extract a sub-network of any pruning ratio from the pre-trained model to adapt to different devices."
  - [section 3.3] "SMSP leverages the knowledge of pruned models for similar tasks to create a pruning mask of the pre-trained model for a new task."
  - [corpus] Weak. The corpus mentions scalability in pruning but doesn't specifically validate SMSP's approach to handling different memory constraints.
- Break condition: If the pre-trained model lacks sufficient redundancy or the pruning ratio is too extreme, the method will fail to maintain accuracy across different constraints.

## Foundational Learning

- Concept: LEEP (Log Expected Empirical Prediction) similarity measure
  - Why needed here: SMSP uses LEEP to identify similar tasks from which to extract pruning knowledge. Understanding this similarity measure is crucial for implementing the method correctly.
  - Quick check question: What does LEEP measure and why is it particularly useful for this pruning approach?

- Concept: Mask-based pruning and filter/node importance
  - Why needed here: The method relies on mask scores to identify and preserve important filters/nodes. Understanding how masks work in pruning is essential for implementing SMSP.
  - Quick check question: How do mask scores differ from traditional weight-based pruning importance measures?

- Concept: Vision Transformer architecture and attention mechanisms
  - Why needed here: SMSP is applied to both CNNs and ViTs, with specific attention to pruning attention heads and feed-forward network nodes in ViTs.
  - Quick check question: Why might pruning attention heads in ViTs be more effective than traditional filter pruning?

## Architecture Onboarding

- Component map: Pre-trained model pool (ResNet-18, ResNet-50, DeiT-S) -> Pruned model database with mask scores -> LEEP similarity calculator -> Mask generation module (sums mask scores from similar tasks) -> Pruning module (removes filters/nodes below threshold) -> Fine-tuning module (trains sub-network for few iterations)

- Critical path: Task similarity calculation → Mask score aggregation → Sub-network extraction → Fine-tuning → Final pruned model

- Design tradeoffs:
  - Number of similar tasks vs. mask quality (more similar tasks generally improve mask quality but increase computation)
  - Pruning ratio vs. accuracy (higher pruning ratios save more resources but may reduce accuracy)
  - Training iterations vs. adaptation quality (more iterations improve adaptation but reduce efficiency)

- Failure signatures:
  - Low accuracy despite high similarity scores (indicates mask generation issues)
  - Converging to similar accuracy regardless of pruning ratio (suggests insufficient redundancy in pre-trained model)
  - High variance in results across similar tasks (indicates unstable mask generation)

- First 3 experiments:
  1. Verify LEEP similarity calculation produces meaningful rankings by checking overlap ratios of top-k filters/nodes for tasks with different similarity scores.
  2. Test mask generation by comparing accuracy of models using SMSP masks versus randomly generated masks with the same pruning ratio.
  3. Validate scalability by applying SMSP to tasks with varying memory constraints and measuring accuracy degradation across different pruning ratios.

## Open Questions the Paper Calls Out
None explicitly stated in the provided document.

## Limitations
- The core assumption that similar tasks share meaningful pruning patterns remains largely unverified in the corpus
- Scalability claims lack rigorous testing across extreme memory constraints
- Performance on tasks with vastly different domain distributions (e.g., medical images vs. natural images) remains unexplored

## Confidence
- High confidence: Computational efficiency claims (fewer iterations) are well-supported by ablation studies comparing SMSP to AMP
- Medium confidence: Accuracy claims are supported by benchmark results but lack comparison to other one-shot pruning methods
- Low confidence: The fundamental mechanism of task similarity driving pruning effectiveness lacks empirical validation

## Next Checks
1. Verify that LEEP similarity scores correlate with actual overlap of task-specific filters/nodes by measuring intersection-over-union of top-k masks across tasks with different similarity scores
2. Test whether random task selection (versus similarity-based) degrades SMSP performance to baseline levels, confirming the importance of task similarity
3. Validate that mask scores from similar tasks consistently outperform masks from dissimilar tasks across multiple pruning ratios and model architectures