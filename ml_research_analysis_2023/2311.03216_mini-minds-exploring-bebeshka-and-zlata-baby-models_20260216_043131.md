---
ver: rpa2
title: 'Mini Minds: Exploring Bebeshka and Zlata Baby Models'
arxiv_id: '2311.03216'
source_url: https://arxiv.org/abs/2311.03216
tags:
- language
- bebeshka
- tasks
- task
- zlata
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the University of Lyon 2 submission to the
  STRICT-SMALL track of the BabyLM competition, which focuses on small-scale language
  modeling from limited data. The authors perform an architecture search to find an
  optimal configuration for encoder-based LMs, minimizing masked language modeling
  loss.
---

# Mini Minds: Exploring Bebeshka and Zlata Baby Models

## Quick Facts
- arXiv ID: 2311.03216
- Source URL: https://arxiv.org/abs/2311.03216
- Reference count: 14
- Primary result: Introduces Bebeshka (4-layer encoder, 8 heads) and Zlata (6-layer decoder, 12 heads) achieving comparable performance to larger models on downstream tasks despite being half the size.

## Executive Summary
This paper presents two small-scale language models developed for the STRICT-SMALL track of the BabyLM competition. Through systematic architecture search, the authors identify that encoder LMs perform optimally with an attention-heads-to-layers ratio around 2. The resulting models, Bebeshka and Zlata, demonstrate comparable performance to larger baselines on GLUE and SuperGLUE benchmarks despite having half the parameters. Additionally, the paper explores these small models' capabilities in moral judgment tasks, finding they perform on par with base models in common-sense morality scenarios and even outperform existing baselines in virtue and justice assessment tasks.

## Method Summary
The authors employ a systematic architecture search using Optuna's Tree-structured Parzen Estimator algorithm to optimize encoder configurations for masked language modeling on a 10M word corpus. Two final models are pre-trained: Bebeshka (4-layer encoder, 8 attention heads, 8K vocabulary) and Zlata (6-layer decoder, 12 heads, 30K vocabulary). Models are trained for 10 epochs each on Graphcore IPUs with mixed precision. Downstream evaluation includes linguistic minimal pairs (BLiMP), GLUE/SuperGLUE benchmarks, and moral judgment tasks from the ETHICS benchmark, with additional grid search for fine-tuning hyperparameters on moral scenarios.

## Key Results
- Bebeshka achieves 76.8 average score on GLUE benchmark compared to 79.8 for larger RoBERTa models
- Zlata performs comparably to larger decoder models despite having half the parameters
- Both models demonstrate strong performance on morphological and syntactic tasks in BLiMP
- Small-scale models show comparable performance to base LMs on ETHICS moral judgment tasks, with improvements in virtue and justice assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ratio of attention heads to layers being around 2 is optimal for encoder LMs in small-scale training.
- Mechanism: A higher attention-to-layer ratio increases model capacity for capturing dependencies without excessive depth, which is particularly beneficial when training data is limited.
- Core assumption: Attention heads can effectively capture linguistic patterns even with fewer layers, and the increased parallelism improves learning efficiency.
- Evidence anchors: [section] "Our parameter search results suggest that optimal LMs have a ratio of attention heads to layers around 2" and [section] "we find that they all have an attention-heads-to-layers ratio of two"

### Mechanism 2
- Claim: Small-scale LMs trained on developmentally plausible corpora can perform comparably to larger models on downstream tasks.
- Mechanism: Training on a corpus similar in size to a child's vocabulary forces the model to learn more generalizable patterns rather than memorizing specific instances.
- Core assumption: Limited data encourages the model to focus on essential linguistic features rather than overfitting to frequent patterns.
- Evidence anchors: [abstract] "Despite being half the scale of the baseline LMs, our proposed models achieve comparable performance" and [section] "we evaluate LMs at a small scale trained on a 10M size dataset of BabyLM shared tasks"

### Mechanism 3
- Claim: Small LMs can align their predictions with human values in moral judgment tasks.
- Mechanism: The structure of the training data, including transcribed speech with single-word reactions and imperatives, may naturally encode moral contexts that the model learns to associate with ethical principles.
- Core assumption: Moral scenarios in the training data are represented in a way that the model can learn to recognize and apply consistently.
- Evidence anchors: [abstract] "We further explore the applicability of small-scale language models in tasks involving moral judgment, aligning their predictions with human values" and [section] "we evaluate small-scale LM's understanding of fundamental moral principles in various scenarios covered by ETHICS benchmark"

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is used as the pre-training objective for Bebeshka, allowing it to learn bidirectional context representations.
  - Quick check question: What is the primary difference between MLM and Causal Language Modeling in terms of context learning?

- Concept: Causal Language Modeling (CLM)
  - Why needed here: CLM is used for Zlata, making it suitable for autoregressive tasks and maintaining the directionality of natural language.
  - Quick check question: How does the unidirectional nature of CLM affect the types of downstream tasks the model is best suited for?

- Concept: Tokenization and Vocabulary Size
  - Why needed here: The choice of vocabulary size (8K for Bebeshka, 30K for Zlata) directly impacts the model's ability to generalize and its computational efficiency.
  - Quick check question: How does reducing vocabulary size affect the model's ability to handle out-of-vocabulary words?

## Architecture Onboarding

- Component map: Bebeshka: 4 layers → 8 attention heads → relative positional embeddings; Zlata: 6 layers → 12 attention heads → absolute positional embeddings
- Critical path: Data preprocessing → Tokenizer training → Architecture search → Model pre-training → Fine-tuning on downstream tasks → Evaluation
- Design tradeoffs: Smaller models are more efficient but may lack the capacity for complex reasoning; larger vocabularies increase coverage but also computational cost.
- Failure signatures: High perplexity on validation data indicates overfitting or underfitting; poor performance on GLUE tasks may suggest issues with fine-tuning hyperparameters.
- First 3 experiments:
  1. Train Bebeshka on a small subset of the training data to verify that the architecture search results are reproducible.
  2. Fine-tune Bebeshka on the CoLA task to test its ability to capture grammatical acceptability judgments.
  3. Evaluate Bebeshka's performance on the ETHICS benchmark to assess its alignment with human values in moral scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural attributes beyond the attention-heads-to-layers ratio of 2 are crucial for optimizing small-scale language models?
- Basis in paper: [explicit] The paper mentions that parameters other than positional embeddings type, dropout ratio, and the number of layers/heads vary significantly across the top 10% runs, and the best-performing encoder LMs are smaller than the base configuration of RoBERTa.
- Why unresolved: While the paper identifies the importance of the attention-heads-to-layers ratio, it does not provide a comprehensive analysis of other critical architectural attributes that contribute to optimal performance.
- What evidence would resolve it: A detailed study that systematically varies and tests other architectural parameters such as hidden size per head, feed-forward layer size, and activation functions to determine their impact on model performance.

### Open Question 2
- Question: How do the moral judgment capabilities of small-scale language models trained on developmentally plausible corpora compare to those of large-scale models in more diverse ethical scenarios?
- Basis in paper: [explicit] The paper explores the applicability of small-scale language models in moral judgment tasks, finding that they perform on par with base LMs in common-sense morality scenarios and even outperform existing baselines in virtue and justice assessment tasks.
- Why unresolved: The study focuses on a limited set of moral judgment tasks and does not explore the models' performance across a broader range of ethical scenarios or compare them directly with large-scale models in more diverse contexts.
- What evidence would resolve it: Additional experiments that evaluate small-scale models on a wider variety of moral judgment tasks and directly compare their performance with large-scale models in diverse ethical scenarios.

### Open Question 3
- Question: What is the relationship between the syntactic and morphological understanding of small-scale language models and their performance on downstream tasks?
- Basis in paper: [explicit] The paper reports that the introduced models perform well on tasks involving morphological phenomena and syntactic understanding, suggesting that these capabilities influence their behavior on downstream tasks.
- Why unresolved: The paper does not provide a detailed analysis of how specific syntactic and morphological understanding contributes to performance on various downstream tasks, nor does it explore the potential trade-offs between these linguistic capabilities and other factors.
- What evidence would resolve it: A comprehensive analysis that correlates specific linguistic understanding (e.g., syntactic, morphological) with performance on a wide range of downstream tasks, potentially using ablation studies or controlled experiments.

## Limitations

- Evaluation scope is limited to specific downstream tasks, with potential performance degradation on tasks requiring longer sequence understanding due to 128-token maximum length
- Moral judgment experiments lack extensive human validation to confirm genuine ethical reasoning rather than pattern matching
- Architecture search methodology explores a limited hyperparameter space that may not capture optimal configurations for all applications

## Confidence

- **High Confidence**: The architectural findings regarding the 2:1 attention-heads-to-layers ratio for encoder LMs are well-supported by systematic parameter search results across multiple trials.
- **Medium Confidence**: The claim that small-scale LMs can perform comparably to larger models is supported by the reported results, but the evaluation scope is limited and may not generalize to all applications.
- **Low Confidence**: The assertion that the training data's "developmentally plausible" nature inherently improves generalization lacks direct empirical support.

## Next Checks

1. **Extended Task Evaluation**: Evaluate Bebeshka and Zlata on additional reasoning and comprehension tasks that require longer sequence understanding, such as Quoref, DROP, or long-document QA tasks, to assess whether the 128-token limitation significantly impacts performance on real-world applications.

2. **Human Evaluation of Moral Judgments**: Conduct human studies comparing model predictions on ETHICS benchmark tasks with human moral judgments to quantify the alignment beyond automated metrics. This should include qualitative analysis of cases where models succeed or fail to capture nuanced ethical reasoning.

3. **Ablation Study on Architecture Components**: Perform systematic ablation studies removing key architectural features (relative positional embeddings, specific dropout ratios, attention-head configurations) to determine which components are essential for the reported performance and which are incidental to the specific experimental setup.