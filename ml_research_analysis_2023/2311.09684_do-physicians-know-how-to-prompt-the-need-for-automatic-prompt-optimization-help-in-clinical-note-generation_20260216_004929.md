---
ver: rpa2
title: Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization
  Help in Clinical Note Generation
arxiv_id: '2311.09684'
source_url: https://arxiv.org/abs/2311.09684
tags:
- prompt
- arxiv
- gpt-4
- gpt-3
- turbo-0613
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores how prompt engineering affects the performance
  of Large Language Models (LLMs) in clinical note generation. An Automatic Prompt
  Optimization (APO) framework was introduced to refine initial prompts provided by
  medical experts, non-medical experts, and APO-enhanced GPT3.5 and GPT4 models.
---

# Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation

## Quick Facts
- arXiv ID: 2311.09684
- Source URL: https://arxiv.org/abs/2311.09684
- Reference count: 17
- Key outcome: GPT4 APO significantly improves prompt quality and consistency across clinical note sections compared to human and GPT3.5 modifications

## Executive Summary
This study introduces an Automatic Prompt Optimization (APO) framework to enhance Large Language Models' performance in clinical note generation. The research compares APO-enhanced GPT3.5 and GPT4 models against medical experts and non-medical experts in refining initial prompts. Results demonstrate that GPT4 APO achieves superior standardization of prompt quality across clinical note sections, while human experts maintain high content standards when refining APO-generated prompts. The study recommends a two-phase optimization process combining APO-GPT4's consistency with expert personalization for optimal clinical note generation.

## Method Summary
The study employs the MTS-Dialog dataset containing 1.7k doctor-patient dialogues organized into 20 SOAP note sections. Researchers implement an APO framework that iteratively refines prompts through forward (generating summaries) and backward (suggesting improvements) passes. The method is evaluated using ROUGE, METEOR, and UMLS-F1 metrics, comparing APO-GPT3.5 and GPT4 against human expert and non-expert modifications. The two-phase optimization process leverages APO-GPT4 for consistency followed by expert input for personalization.

## Key Results
- GPT4 APO significantly outperforms both human modifications and APO-GPT3.5 in standardizing prompt quality across clinical note sections
- Expert modifications maintain high content quality while preserving APO-established standards
- The two-phase optimization process (APO-GPT4 + expert refinement) effectively balances AI efficiency with human expertise

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** APO-GPT4's iterative refinement process produces prompts that consistently outperform human experts and non-experts across clinical note sections.
- **Mechanism:** The backward pass in APO uses GPT4 to self-critique and iteratively improve the initial prompt by analyzing the gap between generated and reference summaries, producing increasingly targeted prompts.
- **Core assumption:** GPT4 can effectively identify and articulate prompt weaknesses through its self-feedback mechanism.
- **Evidence anchors:** [abstract] "Results highlight GPT4 APO's superior performance in standardizing prompt quality across clinical note sections." [section] "Compared to human modifications, APO-GPT4 enhanced summary quality, a feat APO-GPT3.5 did not achieve."
- **Break condition:** If GPT4 cannot accurately identify prompt deficiencies or if the self-feedback loop introduces noise instead of refinement.

### Mechanism 2
- **Claim:** Human experts maintain high content quality when refining APO-generated prompts, with preference for their own modifications indicating personalized customization value.
- **Mechanism:** Experts review APO-GPT4 prompts and make adjustments based on clinical knowledge and personal style preferences, preserving content integrity while adding customization.
- **Core assumption:** Expert knowledge allows meaningful refinement without degrading the quality established by APO.
- **Evidence anchors:** [abstract] "expert modifications maintained the high standards set by APO" and "experts showing a preference for their own revisions." [section] "Exp-APO modifications did not significantly differ from APO-GPT4 in terms of ROUGE, METEOR, and UMLS-f1 scores."
- **Break condition:** If expert modifications introduce subjective bias that reduces objective quality metrics or if experts lack sufficient understanding of APO's optimization goals.

### Mechanism 3
- **Claim:** The two-phase optimization process (APO-GPT4 for consistency + expert input for personalization) balances AI efficiency with human expertise.
- **Mechanism:** First, APO-GPT4 establishes a high-quality baseline prompt through systematic optimization. Then, human experts refine these prompts to match personal preferences while maintaining quality standards.
- **Core assumption:** The combination of systematic AI optimization followed by human customization produces superior results compared to either approach alone.
- **Evidence anchors:** [abstract] "We recommend a two-phase optimization process, leveraging APO-GPT4 for consistency and expert input for personalization." [section] "This strategy offers a pragmatic balance, effectively harnessing the power of AI while respecting the nuances of human expertise."
- **Break condition:** If the two-phase process creates conflicts between AI-optimized structure and human preferences that cannot be resolved.

## Foundational Learning

- **Concept:** Large Language Model prompt engineering
  - Why needed here: Understanding how prompt formulation affects LLM output quality is fundamental to both APO's automated refinement and human expert modifications.
  - Quick check question: What are the key components of an effective clinical note generation prompt?

- **Concept:** Clinical note structure (SOAP format)
  - Why needed here: The study specifically targets optimization across different SOAP note sections, requiring understanding of clinical documentation standards.
  - Quick check question: What distinguishes the content requirements for subjective vs. objective sections in SOAP notes?

- **Concept:** Evaluation metrics for clinical text generation
  - Why needed here: ROUGE, METEOR, and UMLS-F1 scores are used to assess prompt quality, requiring understanding of their strengths and limitations.
  - Quick check question: How does UMLS-F1 specifically capture medical concept accuracy compared to general text similarity metrics?

## Architecture Onboarding

- **Component map:** Generic prompt → Forward pass → Backward pass → Iterative refinement → Evaluation → Expert modification (if applicable) → Final evaluation

- **Critical path:** Generic prompt → Forward pass → Backward pass → Iterative refinement → Evaluation → Expert modification (if applicable) → Final evaluation

- **Design tradeoffs:**
  - Model selection: GPT4 vs GPT3.5 for APO implementation
  - Iteration count: Balancing refinement depth vs. computational cost
  - Human expertise level: Medical experts vs. non-experts for post-APO refinement
  - Evaluation metrics: Objective metrics vs. subjective expert preferences

- **Failure signatures:**
  - Performance degradation after human modification
  - Inconsistent improvements across different clinical note sections
  - Excessive computational cost relative to quality gains
  - Human preference conflict with objective quality metrics

- **First 3 experiments:**
  1. Run APO with GPT4 on a single clinical note section to verify the backward pass improvement mechanism
  2. Compare GPT4 vs GPT3.5 APO performance on the same section to confirm model superiority
  3. Test human expert modification of APO-optimized prompts to validate the two-phase approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Automatic Prompt Optimization (APO) framework perform when applied to different medical domains beyond clinical note generation, such as medical imaging analysis or drug discovery?
- Basis in paper: [inferred] The study focuses on clinical note generation, but the authors mention the potential for applying APO to other domains within NLP.
- Why unresolved: The study only evaluates APO's effectiveness in clinical note generation, leaving its performance in other medical applications unexplored.
- What evidence would resolve it: Conducting experiments applying APO to various medical domains, such as medical imaging analysis or drug discovery, and comparing its performance to traditional methods would provide insights into its broader applicability.

### Open Question 2
- Question: What is the optimal balance between APO-generated prompts and expert-modified prompts to achieve the best performance in clinical note generation?
- Basis in paper: [explicit] The study recommends a two-phase optimization process: using APO-GPT4 for consistency and expert input for personalization.
- Why unresolved: While the study suggests a two-phase approach, it does not provide specific guidelines on the optimal balance between APO-generated and expert-modified prompts for different clinical note sections or medical specialties.
- What evidence would resolve it: Conducting a systematic study to determine the optimal ratio of APO-generated to expert-modified prompts for various clinical note sections and medical specialties would help establish best practices for prompt optimization.

### Open Question 3
- Question: How does the performance of APO compare to other prompt optimization techniques, such as soft prompts or reinforcement learning-based approaches?
- Basis in paper: [inferred] The study introduces APO but does not compare its performance to other prompt optimization methods mentioned in the related work section, such as soft prompts or reinforcement learning.
- Why unresolved: The study focuses on evaluating APO's performance but does not provide a comparative analysis with other prompt optimization techniques.
- What evidence would resolve it: Conducting a comprehensive comparison of APO with other prompt optimization methods, such as soft prompts or reinforcement learning, on the same clinical note generation task would provide insights into APO's relative strengths and weaknesses.

## Limitations

- The study relies entirely on automated metrics and a limited number of human raters, which may not capture the full complexity of clinical note quality and usability
- The research uses a relatively small dataset (1.7k dialogues) with only 14 SOAP sections selected for experimentation, potentially limiting generalizability
- Human-in-the-loop validation only tested one round of expert modifications without exploring whether multiple refinement cycles could yield further improvements or degradation

## Confidence

- **High Confidence:** The APO-GPT4 framework's ability to improve prompt consistency across clinical note sections
- **Medium Confidence:** The superiority of expert modifications over non-expert modifications
- **Medium Confidence:** The recommendation for a two-phase optimization process

## Next Checks

1. Expand Human Evaluation Scope: Conduct a larger-scale human evaluation study with more raters and multiple rounds of prompt refinement to better understand the long-term effects of human modifications on APO-optimized prompts and to validate whether expert preferences align with actual clinical utility.

2. Cross-Dataset Validation: Test the APO framework on additional clinical datasets with different note structures and medical specialties to assess generalizability beyond the MTS-Dialog dataset and verify that improvements in prompt quality translate across diverse clinical contexts.

3. Cost-Benefit Analysis: Measure the computational cost and time investment required for APO-GPT4 optimization versus the quality improvements achieved, particularly comparing the resource requirements of the two-phase approach (APO + expert refinement) against simpler alternatives to determine if the quality gains justify the additional complexity.