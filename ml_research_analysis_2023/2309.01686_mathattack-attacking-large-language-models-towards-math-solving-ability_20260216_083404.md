---
ver: rpa2
title: 'MathAttack: Attacking Large Language Models Towards Math Solving Ability'
arxiv_id: '2309.01686'
source_url: https://arxiv.org/abs/2309.01686
tags:
- math
- llms
- adversarial
- attack
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MathAttack, a novel method to attack large
  language models (LLMs) on their math solving ability. MathAttack uses logical entity
  recognition to identify and freeze key elements like numbers, roles, and scenarios
  in math word problems, then applies word-level attacks to the remaining text to
  generate adversarial samples.
---

# MathAttack: Attacking Large Language Models Towards Math Solving Ability

## Quick Facts
- arXiv ID: 2309.01686
- Source URL: https://arxiv.org/abs/2309.01686
- Reference count: 13
- Key outcome: Novel method to attack LLMs on math solving ability using logical entity recognition and word-level attacks

## Executive Summary
MathAttack introduces a novel adversarial attack methodology targeting large language models' math word problem solving capabilities. The approach uses logical entity recognition to identify and freeze critical elements (numbers, roles, scenarios) while applying word-level attacks to non-logical text. This preserves the mathematical logic while creating semantically similar but problematic samples. The authors create RobustMath, a new dataset of 300 high-quality adversarial samples, demonstrating that the attack successfully breaks LLMs with an average 40% success rate on ChatGPT. The paper also shows that adversarial samples from more capable models can transfer to less capable ones, and that using these samples in few-shot prompts improves model robustness.

## Method Summary
MathAttack operates by first recognizing logical entities in math word problems using named entity recognition, then freezing these entities to preserve mathematical logic. A word-level attacker (BertAttack) modifies the remaining text to create adversarial samples that maintain semantic similarity while breaking model reasoning. The method is evaluated on GSM8K and MultiArith datasets, with results validated on multiple LLMs including Flan-T5 variants, ChatGLM2, and ChatGPT. The attack preserves mathematical logic by only modifying non-logical text, achieving high semantic similarity while successfully breaking model performance.

## Key Results
- MathAttack achieves an average 40% attack success rate on ChatGPT
- Complex problems (more solving steps, longer text, more numbers) are more vulnerable to attack
- Adversarial samples from higher-accuracy LLMs successfully attack lower-accuracy LLMs
- Using adversarial samples in few-shot prompts improves model robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing logical entities preserves mathematical logic while allowing word-level attacks on non-logical text
- Mechanism: NER identifies Role, Number, and Scenario entities which are frozen, preventing changes to mathematical logic while allowing modifications to other text
- Core assumption: Mathematical logic is entirely contained within identified logical entity types
- Evidence anchors:
  - [abstract] "it is essential to preserve the mathematical logic of original MWPs during the attacking. To this end, we propose logical entity recognition to identify logical entries which are then frozen."
  - [section] "We first recognize logical entities, altering these logical entities easily leads to changing the logic of math word problems. Then we freeze the logical entities, preventing the attacker from modifying logical entities."
  - [corpus] Weak - corpus focuses on jailbreak attacks and prompt manipulation rather than MWP-specific entity-based attacks
- Break condition: If mathematical logic depends on relationships between frozen entities or if logical entities interact with non-logical text in ways that create emergent logic

### Mechanism 2
- Claim: More complex MWPs are more vulnerable to attack
- Mechanism: Increased complexity creates more opportunities for semantic perturbations that can disrupt reasoning without changing core mathematical logic
- Core assumption: Complexity introduces more points of failure in the reasoning chain that can be exploited through subtle text modifications
- Evidence anchors:
  - [abstract] "(2) Complex MWPs (such as more solving steps, longer text, more numbers) are more vulnerable to attack"
  - [section] "we can observe that the ASR will increase when the reasoning steps from 2 to 3... with the increase in problem length... with the increase in the quantity of numbers"
  - [corpus] Weak - corpus neighbors focus on jailbreak attacks, not complexity-based vulnerability analysis
- Break condition: If model robustness scales with complexity or if complex problems have inherent redundancy that protects against attacks

### Mechanism 3
- Claim: Adversarial samples from higher-accuracy LLMs can successfully attack lower-accuracy LLMs
- Mechanism: Higher-accuracy models generate more subtle and sophisticated adversarial examples that exploit weaknesses in less capable models
- Core assumption: There is a transferable vulnerability pattern between different LLMs based on their relative capabilities
- Evidence anchors:
  - [abstract] "(1) Our adversarial samples from higher-accuracy LLMs are also effective for attacking LLMs with lower accuracy (e.g., transfer from larger to smaller-size LLMs, or from few-shot to zero-shot prompts)"
  - [section] "the adversarial samples of larger-size models can attack smaller-size models too. ChatGPT could get 94.44% TSR to Flan-T5-large"
  - [corpus] Weak - corpus neighbors don't discuss transferability between different model sizes or capabilities
- Break condition: If models of similar capability have different vulnerability profiles or if the attack mechanism is highly model-specific

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: Essential for identifying and freezing logical entities (Role, Number, Scenario) that must remain unchanged during attacks
  - Quick check question: Can you explain how NER models distinguish between entity types like PERSON, NUMBER, and TIME in text?

- Concept: Semantic similarity metrics
  - Why needed here: Used to measure effectiveness of attacks by ensuring generated adversarial samples remain semantically close to originals
  - Quick check question: What is the difference between semantic similarity and lexical similarity, and why is semantic similarity more important for MWP attacks?

- Concept: Black-box adversarial attack methodology
  - Why needed here: The attack operates without access to model internals, requiring iterative generation and testing of adversarial samples
  - Quick check question: How does a black-box attack differ from a white-box attack in terms of available information and attack strategy?

## Architecture Onboarding

- Component map: NER -> Freeze entities -> Word-level attack -> Evaluate -> Manual verification
- Critical path: NER → Freeze entities → Word-level attack → Evaluate → Manual verification
- Design tradeoffs:
  - Freezing too many entities makes attacks ineffective; freezing too few risks changing mathematical logic
  - Word-level attacks preserve semantics better than char-level but are more computationally expensive
  - Manual verification ensures quality but doesn't scale well
- Failure signatures:
  - Low attack success rate despite multiple iterations
  - High semantic similarity but failed attacks (indicates frozen entities are too restrictive)
  - Low semantic similarity (indicates attack is making overly aggressive changes)
- First 3 experiments:
  1. Test attack on simple MWPs with 2-3 entities to establish baseline effectiveness
  2. Vary the number of frozen entities to find optimal balance between attack success and logic preservation
  3. Test transferability from one model to another with similar architecture but different sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact impact of using adversarial samples as few-shot prompts on the robustness of LLMs across different math problem types (e.g., arithmetic, algebra, geometry)?
- Basis in paper: [explicit] The paper mentions that using adversarial samples in few-shot prompts can improve model robustness, but does not explore this across different math problem types.
- Why unresolved: The paper only provides a general observation about improved robustness without delving into specific math problem types or providing detailed analysis.
- What evidence would resolve it: Conduct experiments using adversarial samples as few-shot prompts across a diverse set of math problem types and compare the robustness improvements for each type.

### Open Question 2
- Question: How does the transferability of adversarial samples vary when attacking LLMs with different underlying architectures (e.g., transformers vs. recurrent neural networks)?
- Basis in paper: [inferred] The paper discusses transferability between different LLMs but does not consider the impact of underlying architectures.
- Why unresolved: The paper focuses on transferability between models of similar architectures (e.g., different sizes of transformers) but does not explore how transferability might differ across fundamentally different architectures.
- What evidence would resolve it: Generate adversarial samples from one type of architecture (e.g., transformers) and test their effectiveness against LLMs with different architectures (e.g., recurrent neural networks).

### Open Question 3
- Question: What are the long-term effects of using adversarial training on the overall performance and generalization ability of LLMs in math problem solving?
- Basis in paper: [inferred] The paper mentions improving robustness by using adversarial samples in few-shot prompts but does not discuss long-term effects or generalization.
- Why unresolved: The paper only provides short-term observations of robustness improvement without considering potential long-term consequences or impacts on generalization to unseen problems.
- What evidence would resolve it: Conduct longitudinal studies on LLMs trained with adversarial samples, monitoring their performance on both seen and unseen math problems over time.

## Limitations
- Manual verification of 300 RobustMath samples introduces potential bias and prevents independent validation
- Assumption that mathematical logic can be fully captured by freezing Role, Number, and Scenario entities may oversimplify complex semantic relationships
- Evaluation limited to specific transformer-based LLMs without exploring other architectures or training paradigms

## Confidence

**High Confidence (5/5):** Basic attack methodology works as described. ASR results (average 40% on ChatGPT) and semantic similarity preservation (>0.7) are empirically demonstrated through multiple experiments.

**Medium Confidence (3/5):** Relationship between problem complexity and attack vulnerability. While trends are shown, analysis doesn't control for confounding factors like problem type or mathematical operations.

**Low Confidence (2/5):** Generalizability of findings to other model families or training approaches. Experiments limited to specific transformer-based LLMs without exploring models with different architectures.

## Next Checks

1. **Automated Verification Protocol:** Develop an automated evaluation framework to validate the manual verification process for RobustMath samples, testing whether identified adversarial samples actually break model reasoning in consistent ways across multiple model instances.

2. **Cross-Architecture Transferability:** Test the MathAttack methodology on non-transformer architectures (RNNs, CNNs) and models with different pretraining objectives to assess whether the logical entity freezing approach generalizes beyond the current model set.

3. **Mathematical Logic Boundary Analysis:** Systematically vary the number and types of frozen entities to identify the minimum set required to preserve mathematical logic, testing whether the current three-entity approach (Role, Number, Scenario) is optimal or overly restrictive.