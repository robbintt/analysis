---
ver: rpa2
title: 'Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative
  Decoding'
arxiv_id: '2309.08168'
source_url: https://arxiv.org/abs/2309.08168
tags:
- decoding
- draft
- inference
- tokens
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the high inference cost of Transformer-based
  Large Language Models (LLMs) due to autoregressive decoding. It proposes a novel
  method called self-speculative decoding, which leverages the LLM itself for both
  drafting and verification stages, avoiding the need for an auxiliary draft model
  and extra memory overhead.
---

# Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding

## Quick Facts
- arXiv ID: 2309.08168
- Source URL: https://arxiv.org/abs/2309.08168
- Reference count: 13
- Key outcome: Self-speculative decoding achieves up to 1.73× speedup on LLaMA-2 models with minimal quality loss

## Executive Summary
This paper introduces self-speculative decoding, a novel method for accelerating inference in large language models by leveraging the model itself for both drafting and verification stages. Unlike previous approaches that require auxiliary draft models, this technique selectively skips intermediate layers during drafting and adaptively determines the number of draft tokens to generate. The method maintains output quality while achieving significant speedups by using the original LLM to verify draft tokens in a single forward pass.

## Method Summary
The self-speculative decoding method operates in two stages: drafting and verification. During drafting, the model generates tokens with certain intermediate layers selectively skipped to accelerate generation while maintaining reasonable quality. The verification stage uses the original LLM to validate each draft token in a single forward pass, ensuring the final output matches what the unaltered model would produce. An adaptive draft-exiting mechanism stops generation when confidence falls below a threshold, preventing wasted computation on tokens likely to be rejected. Bayesian optimization is used to select which layers to skip during drafting.

## Key Results
- Achieved up to 1.73× speedup on LLaMA-2-13B and LLaMA-2-70B models
- Maintained ROUGE-2 scores within 0.2% of autoregressive baseline for text summarization
- Preserved pass@1 and pass@10 scores for code generation tasks
- No additional memory overhead compared to auxiliary model approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Skipping intermediate layers in LLMs during drafting stage significantly accelerates token generation while preserving output quality.
- Mechanism: Selectively bypassing certain layers reduces computational load and memory bandwidth usage, enabling faster draft token generation.
- Core assumption: Quality degradation from skipping layers is minimal and doesn't significantly impact acceptance rate.
- Evidence anchors: Abstract states "drafting stage generates draft tokens at a slightly lower quality but more quickly, which is achieved by selectively skipping certain intermediate layers during drafting"; observation that skipping some layers doesn't significantly compromise generation quality.
- Break condition: If acceptance rate drops below critical threshold, speedup benefits are negated by frequent verification and rejection.

### Mechanism 2
- Claim: The two-stage drafting and verification process ensures final output remains identical to that produced by the unaltered LLM.
- Mechanism: Draft tokens are quickly generated with layers skipped, then original LLM verifies each token in single forward pass, ensuring consistency with original model's probability distribution.
- Core assumption: Verification stage can effectively validate draft tokens and maintain output quality.
- Evidence anchors: Abstract states "This process ensures the final output remains identical to that produced by the unaltered LLM, thereby maintaining output quality"; original LLM verifies draft tokens in single forward pass.
- Break condition: If verification stage fails to reject invalid draft tokens, output quality will degrade.

### Mechanism 3
- Claim: The adaptive draft-exiting mechanism prevents unnecessary computation and potential discard of additional draft tokens, thereby enhancing efficiency.
- Mechanism: Mechanism evaluates predicted probability of each draft token against threshold, stopping generation if confidence falls below threshold to avoid wasting resources on tokens likely to be rejected.
- Core assumption: Confidence threshold accurately reflects likelihood of token acceptance.
- Evidence anchors: Abstract mentions "adaptively determining the number of draft tokens"; adaptive draft-exiting mechanism stops generating draft tokens once confidence level drops below threshold.
- Break condition: If threshold is set too high or too low, it may either stop too early or continue generating tokens that are likely to be rejected.

## Foundational Learning

- Concept: Transformer architecture and autoregressive decoding
  - Why needed here: Understanding how transformers generate tokens sequentially is crucial for grasping the inefficiency the paper addresses.
  - Quick check question: How does autoregressive decoding in transformers work, and why is it memory bandwidth-bound?

- Concept: Speculative execution in computer architecture
  - Why needed here: The paper's approach is based on speculative execution principles, so understanding these concepts is essential.
  - Quick check question: What is speculative execution, and how does it improve computational efficiency?

- Concept: Bayesian optimization
  - Why needed here: The paper uses Bayesian optimization to select which layers to skip during drafting, so understanding this technique is important.
  - Quick check question: What is Bayesian optimization, and how does it balance exploration and exploitation in finding optimal solutions?

## Architecture Onboarding

- Component map: LLM -> Drafting stage (with layer skipping) -> Verification stage (original LLM) -> Output generation

- Critical path: 1) Prompt input 2) Drafting stage (with selected layers skipped) 3) Verification stage (original LLM) 4) Output generation

- Design tradeoffs:
  - Speed vs. output quality: Skipping more layers increases speed but may reduce quality
  - Memory usage: No extra memory overhead compared to training an auxiliary draft model
  - Complexity: Implementing adaptive draft-exiting adds complexity but improves efficiency

- Failure signatures:
  - Low acceptance rate: Indicates too many layers are being skipped or draft-exiting threshold is incorrect
  - No speedup: Suggests layer skipping is not effective or adaptive mechanism is not working properly
  - Output quality degradation: Could indicate issues with verification stage or inappropriate layer skipping

- First 3 experiments:
  1. Test basic self-speculative decoding with simple LLM and small dataset to verify core concept
  2. Experiment with different numbers of skipped layers to find optimal balance between speed and quality
  3. Implement and test adaptive draft-exiting mechanism to ensure it effectively prevents unnecessary computation

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Hardware dependency uncertainty: Speedup effectiveness may vary significantly across different hardware architectures
- Generalization to diverse model architectures: Method effectiveness on non-LLaMA architectures remains unverified
- Dataset representation: Evaluation focuses on text summarization and code generation, with limited evidence for other task types

## Confidence
- High Confidence: Fundamental concept of self-speculative decoding is sound; layer skipping can reduce computational load without catastrophic quality degradation; method achieves speedups on tested LLaMA-2 models with minimal quality loss
- Medium Confidence: Adaptive draft-exiting mechanism effectively prevents wasted computation; Bayesian optimization successfully identifies optimal layer-skipping configurations; speedup claims are representative across diverse scenarios
- Low Confidence: Method generalizes to non-LLaMA architectures without modification; performance claims hold across all hardware configurations; technique works equally well for all types of LLM tasks

## Next Checks
1. Hardware architecture testing: Evaluate method across diverse hardware platforms (consumer GPUs, specialized AI accelerators, different CPU architectures) to quantify hardware dependency and establish minimum hardware requirements

2. Architecture generalization: Test approach on non-LLaMA models (GPT-3/4 variants, Mistral, custom architectures) to determine if layer-skipping strategy requires architecture-specific tuning or can generalize

3. Task diversity assessment: Evaluate method on additional task categories including reasoning tasks, multi-turn dialogue, and multi-modal generation to establish bounds on task generalizability