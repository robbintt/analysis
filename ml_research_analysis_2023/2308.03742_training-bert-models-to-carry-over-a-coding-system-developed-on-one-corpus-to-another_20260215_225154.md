---
ver: rpa2
title: Training BERT Models to Carry Over a Coding System Developed on One Corpus
  to Another
arxiv_id: '2308.03742'
source_url: https://arxiv.org/abs/2308.03742
tags:
- label
- translation
- labels
- training
- train
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of transferring a manual coding\
  \ system for analyzing the perception of literary translation in Hungary (1980\u2013\
  1999) from one literary journal to another. The authors employ BERT models, leveraging\
  \ extensive hyperparameter tuning, domain adaptation, and robust loss functions\
  \ to handle label imbalance."
---

# Training BERT Models to Carry Over a Coding System Developed on One Corpus to Another

## Quick Facts
- arXiv ID: 2308.03742
- Source URL: https://arxiv.org/abs/2308.03742
- Reference count: 39
- BERT models successfully transfer manual coding systems between related Hungarian literary journals using domain adaptation and robust loss functions

## Executive Summary
This paper addresses the challenge of transferring a manual coding system for analyzing the perception of literary translation in Hungary (1980–1999) from one literary journal (Alföld) to another (Nagyvilág). The authors employ BERT models with extensive hyperparameter tuning, domain adaptation pretraining, and robust loss functions to handle severe label imbalance. Using 10-fold cross-validation and model ensembles, they achieve successful transfer of the annotation system while introducing novel calibration methods for temporal analysis and label relation networks to understand label interactions.

## Method Summary
The methodology involves OCR preprocessing to extract paragraphs from scanned journal pages, followed by domain adaptation pretraining using masked language modeling on the combined Alföld-Nagyvilág corpus. The authors then employ 10-fold cross-validation with focal loss and Population-Based Training for hyperparameter optimization. Model ensembles are created from the cross-validation folds, and a custom probability truncation method calibrates predictions for temporal analysis. Manual validation via importance sampling confirms model performance, while label relation networks analyze label interactions using conditional probabilities.

## Key Results
- Domain adaptation pretraining reduces perplexity from 43.07 to 2.88 on the combined corpus
- Focal loss achieves average ROC AUC of 0.9524 ± 0.0114 for multilabel classification
- Model ensemble with probability truncation (plow = 0.2, phigh = 0.54) provides reliable predictions for temporal analysis

## Why This Works (Mechanism)

### Mechanism 1
BERT models can successfully transfer a manually developed coding system from one literary journal corpus to another despite domain shift. Extensive hyperparameter tuning combined with domain adaptation pretraining (masked language modeling on the combined Alföld-Nagyvilág corpus) creates a BERT model that generalizes better to the target domain's stylistic and topical characteristics. The linguistic and topical similarities between the two Hungarian literary journals from the same time period are sufficient for knowledge transfer.

### Mechanism 2
Robust loss functions and metrics handle the severe label imbalance in the dataset, enabling effective learning for rare categories. Focal loss is used during training to focus learning on hard-to-classify examples, while ROC AUC and balanced accuracy metrics are used during evaluation to properly assess performance across all label frequencies. The model can learn to recognize rare categories even with few examples if the training process is appropriately weighted.

### Mechanism 3
Model ensemble and calibration techniques improve prediction reliability for downstream analysis tasks. 10-fold cross-validation creates multiple models that are averaged for predictions, while a custom probability truncation method calibrates outputs to better match true label counts in temporal analysis. Aggregating predictions from multiple models reduces variance and improves overall reliability for analysis purposes.

## Foundational Learning

- Concept: Domain adaptation through pretraining on target corpus
  - Why needed here: The Alföld and Nagyvilág journals have different editorial focuses and writing styles, requiring adaptation beyond general Hungarian language understanding
  - Quick check question: What would happen to performance if we skipped the domain adaptation pretraining step?

- Concept: Handling multilabel classification with imbalanced data
  - Why needed here: The content labels are multilabel (each paragraph can have multiple codes) and highly imbalanced, requiring specialized loss functions and evaluation metrics
  - Quick check question: Why is ROC AUC preferred over simple accuracy for multilabel imbalanced classification?

- Concept: Ensemble methods for reliable predictions
  - Why needed here: Small training set size and domain shift require multiple models to achieve consistent predictions for downstream analysis
  - Quick check question: How does 10-fold cross-validation help both with evaluation and with creating an ensemble?

## Architecture Onboarding

- Component map: OCR preprocessing → domain adaptation pretraining → 10-fold training with hyperparameter optimization → ensemble prediction → calibration and analysis
- Critical path: OCR preprocessing → domain adaptation pretraining → 10-fold training with focal loss and PBT optimization → ensemble prediction → calibration and analysis
- Design tradeoffs: Larger models perform better but require more resources; simpler calibration (thresholding at 50%) is faster but less accurate for label count prediction
- Failure signatures: Poor domain adaptation shows as degraded performance on the target corpus; inadequate calibration shows as predicted label counts that don't match actual distributions
- First 3 experiments:
  1. Train a single BERT model without domain adaptation on the Alföld corpus and evaluate on Nagyvilág to establish baseline performance
  2. Implement the OCR preprocessing pipeline and verify paragraph separation quality by comparing to manual annotation
  3. Run a small-scale 3-fold training with focal loss to verify the loss function implementation before scaling to 10-fold

## Open Questions the Paper Calls Out

### Open Question 1
How do the performance metrics of the model trained on the Alföld corpus compare to a model trained on the Nagyvilág corpus when both are tested on the same target domain? The authors mention using domain adaptation on the Alföld-Nagyvilág dataset and compare it to a model without domain adaptation, but do not compare models trained exclusively on either Alföld or Nagyvilág. Training and evaluating separate models on Alföld and Nagyvilág, then testing both on the same target domain, would provide direct performance metrics for comparison.

### Open Question 2
What is the impact of different OCR quality levels on the model's performance in text classification tasks? The paper discusses preprocessing challenges, including OCR quality issues with the Alföld and Nagyvilág journals, but does not explore how varying OCR quality affects model performance. Conducting experiments with datasets of varying OCR quality and measuring the corresponding changes in model performance would clarify the impact of OCR quality.

### Open Question 3
How does the inclusion of context labels affect the model's ability to classify content labels in multilabel scenarios? The paper discusses the use of both content and context labels but notes that context classification was less successful, particularly after truncation. While the paper mentions the use of context labels, it does not fully explore how their inclusion influences the accuracy of content label classification in multilabel scenarios.

## Limitations

- Domain transfer validity is untested on more distant domains beyond Hungarian literary journals from the same time period
- Manual validation process relies on importance sampling which may miss systematic errors in rare categories
- Calibration method's probability thresholds (0.2 and 0.54) lack theoretical justification for why these specific values are optimal

## Confidence

**High Confidence**: The core claim that BERT models can transfer manual coding systems between related domains is well-supported by experimental results and manual validation. The methodology for handling label imbalance through focal loss and robust metrics is sound and well-established.

**Medium Confidence**: The specific performance improvements from domain adaptation pretraining are convincing within this study's context, but the mechanism explaining why OCR domain adaptation works "almost to the same extent" as adaptation on the original corpus needs more theoretical grounding.

**Low Confidence**: The calibration method's effectiveness is demonstrated empirically but lacks theoretical explanation for why the chosen thresholds (0.2 and 0.54) are optimal. The relationship between these thresholds and the underlying label distribution remains unclear.

## Next Checks

1. Apply the same methodology to transfer the coding system between more dissimilar domains (e.g., literary journals vs. academic publications) to test the limits of domain adaptation effectiveness.

2. Systematically vary the calibration thresholds (plow and phigh) across a wider range to determine their sensitivity and establish whether the chosen values are truly optimal or simply locally effective.

3. Implement a stratified manual validation covering all label categories with sufficient sample sizes to verify that the importance sampling approach doesn't systematically miss errors in rare categories.