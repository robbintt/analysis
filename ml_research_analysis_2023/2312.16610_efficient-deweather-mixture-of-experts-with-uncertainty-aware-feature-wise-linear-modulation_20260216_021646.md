---
ver: rpa2
title: Efficient Deweather Mixture-of-Experts with Uncertainty-aware Feature-wise
  Linear Modulation
arxiv_id: '2312.16610'
source_url: https://arxiv.org/abs/2312.16610
tags:
- experts
- mofme
- router
- image
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Mixture-of-Feature-Modulation-Experts (MoFME)
  framework to improve upstream deweathering performance while saving significant
  model parameters. The core idea is to replace parallel FFN experts in traditional
  MoE with lightweight feature modulation modules and a shared FFN, effectively reducing
  parameter overhead.
---

# Efficient Deweather Mixture-of-Experts with Uncertainty-aware Feature-wise Linear Modulation

## Quick Facts
- arXiv ID: 2312.16610
- Source URL: https://arxiv.org/abs/2312.16610
- Reference count: 9
- This paper introduces MoFME, achieving 0.1-0.2 dB PSNR improvement over prior MoE methods while saving over 72% parameters

## Executive Summary
This paper proposes a Mixture-of-Feature-Modulation-Experts (MoFME) framework for efficient deweathering that replaces parallel FFN experts with lightweight feature modulation modules and a shared FFN. The approach achieves significant parameter savings (>72%) while maintaining or improving image restoration quality by 0.1-0.2 dB PSNR compared to state-of-the-art methods. The framework also demonstrates strong generalization to downstream segmentation and classification tasks, making it practical for real-world applications.

## Method Summary
MoFME modifies traditional MoE by replacing parallel FFN experts with feature modulation modules (γ, β parameters) that perform input-dependent affine transformations on intermediate features before a shared FFN processes them. An Uncertainty-aware Router (UaR) assigns task-specific features to experts using Monte Carlo dropout to estimate uncertainty, then normalizes router weights to emphasize confident assignments. The framework is trained with task-specific loss plus load balance and uncertainty-aware loss components, achieving parameter efficiency through shared FFN while maintaining expressivity through multiple modulation experts.

## Key Results
- MoFME achieves 0.1-0.2 dB PSNR improvement over prior MoE-based models on deweathering tasks
- Saves over 72% of parameters compared to traditional MoE architectures
- Reduces inference time by 39% while maintaining state-of-the-art performance
- Demonstrates strong generalization to downstream segmentation and classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature Modulation Expert (FME) reduces parameter overhead by replacing parallel FFN experts with lightweight affine transformations and a shared FFN
- Mechanism: FME applies input-dependent feature-wise affine transformations (via learned γ and β parameters) to tokens before feeding them into a single shared FFN
- Core assumption: A shared FFN can process diverse task-specific features if those features are first modulated appropriately by FM modules
- Evidence anchors: "replace parallel FFN experts in traditional MoE with lightweight feature modulation modules and a shared FFN"

### Mechanism 2
- Claim: Uncertainty-aware Router (UaR) improves expert assignment by weighting modulated features according to estimated model uncertainty
- Mechanism: UaR uses MC dropout to estimate uncertainty as the inverse covariance of router outputs, then normalizes router weights to emphasize more confident assignments
- Core assumption: Uncertainty estimates can reliably distinguish between task-specific features and guide expert routing more effectively than naive linear routers
- Evidence anchors: "assign task-specific features to experts using well-calibrated weights"

### Mechanism 3
- Claim: Combining FME and UaR yields both parameter efficiency and performance gains over prior MoE methods
- Mechanism: FME reduces model size by sharing FFN parameters across experts while UaR improves feature-to-expert matching
- Core assumption: The parameter savings from FME are sufficient to allow adding more experts, and UaR's routing improvements are complementary to FME's parameter efficiency
- Evidence anchors: "MoFME achieves 0.1-0.2 dB PSNR improvement compared to prior MoE-based models while saving over 72% of parameters"

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding MoE is essential because MoFME is a variant that modifies how experts are instantiated and routed
  - Quick check question: What is the main computational advantage of MoE compared to dense models, and what is its typical bottleneck?

- Concept: Feature-wise linear modulation (FiLM)
  - Why needed here: FME is directly inspired by FiLM, using learned affine transformations to condition features on input
  - Quick check question: How does FiLM modulate features, and what are its typical parameter requirements compared to full expert networks?

- Concept: Monte Carlo dropout for uncertainty estimation
  - Why needed here: UaR relies on MC dropout to estimate predictive uncertainty for router calibration
  - Quick check question: How does MC dropout provide uncertainty estimates, and what is the difference between epistemic and aleatoric uncertainty?

## Architecture Onboarding

- Component map: Input tokens → Multi-head Attention layer → MoFME layer (FME + UaR) → Output
- Critical path:
  1. Token embedding from previous transformer block
  2. Router computes task-specific weights for each FM module
  3. Each FM module applies affine transformation to token
  4. Modulated features are summed and passed through shared FFN
  5. Output tokens proceed to next transformer block

- Design tradeoffs:
  - Parameter efficiency vs. expressivity: FME trades individual expert parameters for shared FFN plus lightweight FM modules
  - Routing complexity vs. performance: UaR adds computation for uncertainty estimation but aims to improve expert assignment quality
  - Model scalability: MoFME can scale expert count without proportional parameter growth, but router complexity may increase

- Failure signatures:
  - Performance degradation with too few experts relative to task diversity
  - Router collapse (all tokens routed to same expert) if uncertainty estimation fails
  - Parameter inefficiency if FM modules become too large relative to shared FFN

- First 3 experiments:
  1. Compare MoFME with varying numbers of FM modules (e.g., 4, 8, 16) on All-weather dataset to find optimal expert count
  2. Ablation: MoFME with and without UaR to quantify routing improvement contribution
  3. Efficiency test: Measure parameter count and inference time for MoFME vs. traditional MoE with same number of experts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MoFME compare to conventional MoE when the number of experts scales beyond 128, and what is the theoretical limit for the parameter savings?
- Basis in paper: [inferred] The paper demonstrates parameter savings and performance improvements up to 128 experts, but does not explore the scalability limits
- Why unresolved: The scalability of MoFME with a larger number of experts has not been tested, and the theoretical limit for parameter savings is unknown
- What evidence would resolve it: Additional experiments with varying numbers of experts beyond 128, including theoretical analysis of parameter scaling

### Open Question 2
- Question: What is the impact of the Uncertainty-aware Router (UaR) on the performance of MoFME when applied to tasks other than deweathering, such as medical image segmentation or autonomous driving?
- Basis in paper: [inferred] The paper shows the effectiveness of UaR on deweathering tasks, but its applicability to other domains is not explored
- Why unresolved: The paper does not provide evidence of UaR's effectiveness on tasks outside of deweathering, and its performance in other domains is unknown
- What evidence would resolve it: Experiments applying MoFME with UaR to a variety of tasks in different domains, such as medical imaging or autonomous driving

### Open Question 3
- Question: How does the proposed MoFME framework perform when dealing with real-time constraints, such as those found in autonomous vehicles or live video streaming?
- Basis in paper: [explicit] The paper mentions the importance of efficient deployment for practical applications but does not provide specific results for real-time scenarios
- Why unresolved: The paper does not evaluate the performance of MoFME under real-time constraints, and its suitability for time-sensitive applications is unclear
- What evidence would resolve it: Experiments measuring the latency and throughput of MoFME in real-time applications, such as autonomous vehicles or live video streaming

## Limitations

- The specific implementation details of FM modules (g and b functions) and UaR (MC dropout configuration) are not fully specified, making exact reproduction challenging
- Performance improvements of 0.1-0.2 dB PSNR may be dataset-specific and require validation on additional deweathering datasets
- The paper does not evaluate MoFME's behavior with highly correlated or redundant input features, which is common in many applications

## Confidence

**High Confidence**: The general approach of using feature modulation to reduce MoE parameter overhead is sound and aligns with established FiLM principles. The reported parameter savings (~72%) and inference time reduction (~39%) are likely achievable given the architectural changes described.

**Medium Confidence**: The performance improvements of 0.1-0.2 dB PSNR are plausible but may be dataset-specific. The generalization to downstream tasks (segmentation and classification) is supported by the experimental results but would benefit from testing on additional datasets to confirm robustness.

**Low Confidence**: The specific implementation details of the FM modules (g and b functions) and UaR (MC dropout configuration) are not fully specified, making exact reproduction challenging. The claimed superiority over MoE, M3ViT, and MoWE baselines should be verified with identical training protocols.

## Next Checks

1. **Ablation study on expert count**: Systematically vary the number of FM modules (e.g., 4, 8, 16) to determine the optimal expert count for different task complexities and verify that parameter efficiency gains scale as claimed

2. **Uncertainty calibration analysis**: Evaluate the quality of UaR's uncertainty estimates by comparing predicted uncertainty with actual model performance variance across different task types and weather conditions

3. **Cross-dataset generalization**: Test MoFME on additional deweathering datasets (e.g., Rain100L, SPA-Data) and downstream tasks to confirm that the reported performance gains are not dataset-specific artifacts