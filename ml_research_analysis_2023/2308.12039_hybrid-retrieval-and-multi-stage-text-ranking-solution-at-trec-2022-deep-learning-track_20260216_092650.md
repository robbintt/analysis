---
ver: rpa2
title: Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning
  Track
arxiv_id: '2308.12039'
source_url: https://arxiv.org/abs/2308.12039
tags:
- retrieval
- ranking
- stage
- document
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes a hybrid retrieval and multi-stage text ranking
  solution for TREC 2022 Deep Learning Track. The approach combines traditional sparse
  retrieval (BM25, Doc2query, SPLADE) with neural dense retrieval (CoCondenser, ROM)
  in the retrieval stage, followed by multi-stage ranking using various backbone models
  (BERT, RoBERTa, ERNIE, ELECTRA, DeBERTa) and an HLATR re-ranking module.
---

# Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track

## Quick Facts
- **arXiv ID**: 2308.12039
- **Source URL**: https://arxiv.org/abs/2308.12039
- **Reference count**: 5
- **Primary result**: 1st place in passage ranking (NDCG@10: 0.7184) and 4th place in document ranking (NDCG@10: 0.7533) on TREC 2022 Deep Learning Track test set

## Executive Summary
This paper presents a hybrid retrieval and multi-stage text ranking solution that achieved top performance in the TREC 2022 Deep Learning Track. The approach combines traditional sparse retrieval methods (BM25, Doc2query, SPLADE) with neural dense retrieval (CoCondenser, ROM) in the retrieval stage, followed by multi-stage ranking using various backbone models (BERT, RoBERTa, ERNIE, ELECTRA, DeBERTa) and an HLATR re-ranking module. The system demonstrates how hybrid approaches can leverage complementary strengths of different retrieval paradigms to achieve superior performance in both passage and document ranking tasks.

## Method Summary
The method employs a two-stage architecture: hybrid retrieval combining sparse (BM25, Doc2query, SPLADE) and dense (CoCondenser, ROM) methods, followed by multi-stage ranking using multiple backbone models (BERT, RoBERTa, ERNIE, ELECTRA, DeBERTa) with negative sampling and R-drop loss. The ROM model optimizes random masking in MLM to weighted masking based on term weighting, reducing stop word masking. HLATR re-ranking fuses retrieval and ranking results using a Transformer architecture. The system achieved state-of-the-art results on TREC 2022 Deep Learning Track datasets.

## Key Results
- Achieved 1st place in passage ranking with NDCG@10: 0.7184
- Achieved 4th place in document ranking with NDCG@10: 0.7533
- Demonstrated effectiveness of hybrid sparse and dense retrieval ensemble
- Showed multi-stage ranking with multiple backbone models improves performance

## Why This Works (Mechanism)

### Mechanism 1
Hybrid retrieval combining sparse and dense methods achieves better performance than either alone. Sparse methods capture exact term matching and lexical expansion, while dense methods capture semantic similarity. Their weighted ensemble leverages complementary strengths.

Core assumption: Sparse and dense methods capture orthogonal information about relevance.
Evidence anchors:
- [abstract] "The retrieval stage combined the two structures of traditional sparse retrieval and neural dense retrieval."
- [section 2.1] "Although the performance of the dense retrieval method has far exceeded that of the sparse retrieval method, we found that the results of the two methods can be further improved by weighted ensemble."
Break condition: If sparse and dense methods capture highly overlapping information, the ensemble provides little benefit.

### Mechanism 2
Multi-stage ranking with multiple backbone models improves final ranking performance. Initial retrieval narrows candidates, then ranking models perform fine-grained relevance scoring on reduced set. HLATR further refines by fusing stage outputs.

Core assumption: Complex interaction-based ranking is feasible only after retrieval reduces candidate set size.
Evidence anchors:
- [abstract] "In the ranking stage, in addition to the full interaction-based ranking model built on large pre-trained language model, we also proposes a lightweight sub-ranking module to further enhance the final text ranking performance."
- [section 2.2] "Benefit from the excellent performance of large-scale pre-trained language model (e,g. BERT [DCLT19]) on various natural language processing tasks, the ranking model is also gradually turning to the BERT model as the relevance scoring model."
Break condition: If retrieval stage fails to sufficiently reduce candidates, ranking stage becomes computationally infeasible.

### Mechanism 3
ROM model's retrieval-oriented masking improves dense retrieval performance. ROM optimizes random mask in MLM task to weighted mask based on term weighting, reducing probability of masking stop words that have weak role in retrieval.

Core assumption: Stop words have weak role in retrieval tasks.
Evidence anchors:
- [section 2.1] "The ROM model we use is optimized for the random mask in the MLM task to a weighted mask based on term weighting, which reduces the probability of the stop words that have a weak role in retrieval being masked."
- [section 2.1] "Such adjustments can make the final trained model more suitable for retrieval tasks."
Break condition: If stop words contribute significantly to retrieval relevance, ROM's masking strategy would harm performance.

## Foundational Learning

- Concept: Dense retrieval vs sparse retrieval differences
  - Why needed here: Understanding why hybrid approach works requires knowing complementary strengths of each method
  - Quick check question: What is the fundamental difference between how BM25 and dense retrieval models like CoCondenser measure relevance?

- Concept: Pre-trained language models in ranking
  - Why needed here: The ranking stage relies on BERT-based models for fine-grained relevance scoring
  - Quick check question: How does a BERT-based ranking model use query-doc interaction differently from a dense retriever's dual-encoder approach?

- Concept: Multi-stage information retrieval pipeline
  - Why needed here: The entire system architecture is built on sequential retrieval and ranking stages
  - Quick check question: Why is it computationally necessary to have a separate retrieval stage before ranking, rather than ranking all documents?

## Architecture Onboarding

- Component map: Retrieval stage (sparse: BM25, Doc2query, SPLADE; dense: CoCondenser, ROM) → Ranking stage (BERT, RoBERTa, ERNIE, ELECTRA, DeBERTa) → HLATR re-ranking module

- Critical path: Query → Hybrid retrieval ensemble → Top-k candidates → Multiple ranking models → HLATR fusion → Final ranked results

- Design tradeoffs: Sparse methods are faster but less semantically rich; dense methods are more accurate but computationally heavier; multi-stage ranking increases accuracy but adds complexity

- Failure signatures: Retrieval stage too narrow (misses relevant docs), ranking stage overfits to training data, HLATR fails to properly fuse stage outputs

- First 3 experiments:
  1. Compare hybrid retrieval ensemble vs individual sparse/dense methods on development set
  2. Test single ranking backbone vs ensemble of multiple models
  3. Evaluate HLATR re-ranking impact by comparing with and without this final stage

## Open Questions the Paper Calls Out

### Open Question 1
Question: How does the ROM model's weighted masking approach compare to traditional random masking in terms of long-term retrieval effectiveness on diverse corpora?
Basis in paper: [explicit] The paper states that ROM "optimizes for the random mask in the MLM task to a weighted mask based on term weighting, which reduces the probability of the stop words that have a weak role in retrieval being masked."
Why unresolved: The paper only mentions this optimization but doesn't provide comparative results showing long-term effectiveness on diverse corpora.
What evidence would resolve it: A comprehensive study comparing ROM with standard MLM pre-training on multiple retrieval benchmarks over extended periods, showing performance trends and robustness across different domains.

### Open Question 2
Question: What is the optimal combination ratio for hybrid retrieval that maximizes performance across different dataset characteristics?
Basis in paper: [explicit] "we found that the results of the two methods can be further improved by weighted ensemble... The sparse retrieval method pays more attention to the text literal matching information, and the dense retrieval method pays more attention to the semantic matching information, and the two can complement each other."
Why unresolved: The paper demonstrates that hybrid retrieval works but doesn't explore the parameter space to find optimal combination ratios or how these ratios vary with dataset characteristics.
What evidence would resolve it: Extensive ablation studies showing performance across different weight combinations and dataset types, identifying optimal ratios and patterns in how they should vary.

### Open Question 3
Question: How does the performance of the HLATR re-ranking module scale with increasing candidate set sizes beyond what was tested?
Basis in paper: [inferred] The paper mentions "we found that as the candidate set participating in the ranking stage increases, the final ranking performance will decrease" but doesn't explore performance beyond the tested ranges.
Why unresolved: The paper identifies this limitation but doesn't test the boundaries of this relationship or propose solutions for handling larger candidate sets.
What evidence would resolve it: Systematic testing of HLATR performance across a wide range of candidate set sizes, identifying the breaking point and potential architectural modifications to handle larger sets effectively.

## Limitations

- Hybrid retrieval assumes sparse and dense methods capture orthogonal information, which may not hold across all domains
- ROM's masking strategy effectiveness depends on the assumption that stop words have weak retrieval roles, which could vary by domain
- Multi-stage ranking approach requires significant computational resources, limiting applicability to resource-constrained settings

## Confidence

- Hybrid retrieval effectiveness: High - supported by strong empirical results (1st place in passage ranking)
- Multi-stage ranking improvement: Medium - mechanism is sound but specific gains depend on dataset characteristics
- ROM masking strategy: Low - mechanism described but effectiveness evidence is limited

## Next Checks

1. Test the hybrid retrieval ensemble's performance degradation when removing either sparse or dense components to quantify their individual contributions
2. Evaluate ROM model performance with different term weighting schemes to validate the retrieval-oriented masking hypothesis
3. Measure computational overhead of multi-stage ranking versus single-stage approaches across different dataset sizes to assess practical scalability