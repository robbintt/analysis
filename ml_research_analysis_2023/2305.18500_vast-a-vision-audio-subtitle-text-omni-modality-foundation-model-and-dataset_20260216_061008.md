---
ver: rpa2
title: 'VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset'
arxiv_id: '2305.18500'
source_url: https://arxiv.org/abs/2305.18500
tags:
- video
- vision
- audio
- arxiv
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VAST is a new foundation model that processes vision, audio, subtitle,
  and text modalities together for video understanding. It uses a large, automatically
  generated dataset (VAST-27M) of 27 million video clips with 11 captions each, including
  vision, audio, and integrated omni-modality captions produced by a large language
  model.
---

# VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset

## Quick Facts
- arXiv ID: 2305.18500
- Source URL: https://arxiv.org/abs/2305.18500
- Reference count: 40
- VAST outperforms existing methods on 22 different benchmarks, setting new state-of-the-art results in video retrieval, captioning, and question answering across vision-text, audio-text, and multi-modal video-text tasks.

## Executive Summary
VAST is a new foundation model that processes vision, audio, subtitle, and text modalities together for video understanding. It uses a large, automatically generated dataset (VAST-27M) of 27 million video clips with 11 captions each, including vision, audio, and integrated omni-modality captions produced by a large language model. The model employs separate encoders for each modality and is trained with contrastive, matching, and generation objectives to strengthen multi-modal connections.

## Method Summary
VAST uses a two-stage automatic pipeline for caption generation, first training vision and audio captioners on existing datasets, then using an LLM to integrate these with subtitles into omni-modality captions. The model architecture consists of three single-modality encoders (ViT for vision, BEATs for audio, BERT for text) with cross-attention layers for fusion. Training employs three objectives: OM-VCC (contrastive loss), OM-VCM (matching loss), and OM-VCG (generation loss) on the VAST-27M dataset. The modality grouping strategy allows handling cases where not all modalities are available at test time.

## Key Results
- VAST establishes new state-of-the-art results on 22 benchmarks across retrieval, captioning, and QA tasks
- Sets new records on popular benchmarks including MSR-VTT, VATEX, and ActivityNet
- Demonstrates superior performance in zero-shot and few-shot settings compared to uni-modal and bi-modal baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model's performance gains come from jointly modeling vision, audio, and subtitle modalities rather than treating them separately.
- Mechanism: VAST uses three separate encoders (vision, audio, text) that are combined through cross-attention layers, allowing the model to learn rich cross-modal representations that capture the complementary information across modalities.
- Core assumption: The different modalities contain non-redundant, complementary information that can be better leveraged when processed together.
- Evidence anchors:
  - [abstract]: "VAST consists of three single modality encoders while text encoder can fulfill cross-modality fusion through cross-attention layers."
  - [section 4.1]: "This framework can accommodate multi-modal inputs such as images, videos, audios, subtitles, and captions. The text encoder is responsible for encoding single-modality captions or subtitles, as well as performing multi-modal encoding/decoding through cross-attention layers."
- Break condition: If the modalities contain mostly redundant information or if the cross-attention mechanism introduces too much noise, the performance gains could diminish or reverse.

### Mechanism 2
- Claim: The automated dataset creation pipeline allows VAST to scale to a much larger dataset than manual annotation would permit.
- Mechanism: Vision and audio captioners are trained on existing datasets, then used to generate captions for 27 million video clips. An LLM integrates these with subtitles to create omni-modal captions.
- Core assumption: The automatically generated captions are of sufficient quality to train a strong foundation model.
- Evidence anchors:
  - [abstract]: "We first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions."
  - [section 3.1]: "We employ the off-the-shelf Vicuna-13b [8] model as the omni-modality captioner...For each video clip, we randomly select 3 vision captions and 3 audio captions, and feed them, along with the raw subtitle and designed instructional prompts, into the LLM."
- Break condition: If the automatically generated captions contain too much noise or factual errors, the foundation model would learn incorrect associations.

### Mechanism 3
- Claim: The modality grouping strategy allows the model to handle cases where not all modalities are available at test time.
- Mechanism: During pretraining, the model learns to handle different combinations of modalities (V-T, A-T, V-A-T, V-S-T, V-A-S-T) through separate training objectives.
- Core assumption: The model can learn modality-agnostic representations that generalize to different combinations of available modalities.
- Evidence anchors:
  - [section 4.3]: "Inspired by the modality grouping strategy proposed by V ALOR [7], we uniformly model the relations of V-T, A-T, V A-T, VS-T, and V AS-T...Specifically, vision and audio captions are used in V-T and A-T modeling, respectively, while omni-modality captions are employed in V A-T, VS-T, and V AS-T modeling."
- Break condition: If the modality grouping introduces too much complexity or if the model overfits to specific modality combinations, generalization could suffer.

## Foundational Learning

- Concept: Cross-modal representation learning
  - Why needed here: The model needs to learn representations that capture the relationships between vision, audio, and text modalities.
  - Quick check question: Can you explain how cross-attention layers help fuse information from different modalities?

- Concept: Contrastive learning
  - Why needed here: The OM-VCC objective uses contrastive learning to align video and caption representations in a shared embedding space.
  - Quick check question: What is the purpose of using a contrastive loss in the OM-VCC objective?

- Concept: Masked language modeling
  - Why needed here: The OM-VCG objective uses masked language modeling to improve the model's ability to generate coherent captions.
  - Quick check question: How does masked language modeling help improve the model's generation capabilities?

## Architecture Onboarding

- Component map: Video/audio/text input → individual encoders (ViT/BEATs/BERT) → cross-attention fusion → pretraining objectives → learned representations

- Critical path: Video/audio/text input → individual encoders → cross-attention fusion → pretraining objectives → learned representations

- Design tradeoffs:
  - Separate encoders allow specialized processing but require careful fusion
  - Large-scale automated dataset creation vs. potential noise in generated captions
  - Complex pretraining objectives vs. model capacity

- Failure signatures:
  - Poor performance on downstream tasks could indicate issues with any of the three encoders
  - Mode collapse in generated captions could indicate problems with the OM-VCG objective
  - Inconsistent performance across different modality combinations could indicate issues with the modality grouping strategy

- First 3 experiments:
  1. Ablation study removing one modality at a time to understand their individual contributions
  2. Comparison of automatically generated captions vs. human-written captions on a small validation set
  3. Analysis of cross-attention weights to understand how the model is fusing information from different modalities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we further improve the quality and diversity of the omni-modality captions in VAST-27M?
- Basis in paper: [explicit] The paper mentions that the omni-modality captions are generated using an LLM (Vicuna-13b) with vision, audio, and subtitle inputs. It also discusses the importance of instructional prompts in guiding the LLM.
- Why unresolved: While the paper shows that the omni-modality captions outperform other corpora, it does not explore alternative methods for generating these captions or investigate the impact of different LLM architectures or training strategies.
- What evidence would resolve it: Comparative experiments using different LLM architectures, training strategies, or alternative caption generation methods (e.g., human-in-the-loop approaches) to assess their impact on caption quality and downstream task performance.

### Open Question 2
- Question: How does the performance of VAST scale with the size of the training dataset?
- Basis in paper: [inferred] The paper mentions that VAST is trained on a subset of 27 million video clips from a larger corpus (HD_VILA_100M). However, it does not explore the impact of using different dataset sizes on model performance.
- Why unresolved: The paper focuses on the effectiveness of VAST with the proposed dataset size, but it does not investigate the potential benefits or limitations of scaling up the training data.
- What evidence would resolve it: Experiments training VAST on different dataset sizes (e.g., 10M, 50M, 100M clips) and evaluating its performance on downstream tasks to determine the relationship between dataset size and model effectiveness.

### Open Question 3
- Question: Can VAST be effectively adapted to handle new modalities beyond vision, audio, and subtitles?
- Basis in paper: [inferred] The paper presents VAST as an omni-modality foundation model, but it does not explicitly discuss its ability to handle additional modalities or explore the potential for extending its capabilities.
- Why unresolved: While the paper demonstrates the effectiveness of VAST for the four modalities it was trained on, it does not investigate its adaptability to new modalities or the challenges and opportunities associated with such extensions.
- What evidence would resolve it: Experiments integrating new modalities (e.g., text transcripts, 3D point clouds) into the VAST architecture and evaluating its performance on downstream tasks that leverage these additional modalities.

## Limitations
- Automated caption generation pipeline introduces uncertainty about caption quality and potential noise in training data
- Model's performance gains from modality fusion lack deeper analysis of when and why cross-modal connections improve performance
- Heavy reliance on Vicuna-13b LLM for omni-modality caption generation creates potential single point of failure

## Confidence

**High Confidence:** The VAST architecture design and training objectives are clearly specified, and the empirical results showing state-of-the-art performance on 22 benchmarks are well-documented with appropriate metrics. The methodology for handling missing modalities through the modality grouping strategy is sound and directly addresses a practical concern.

**Medium Confidence:** The automated dataset creation pipeline appears methodologically sound, but the quality of the generated captions is difficult to verify without access to the full dataset or human evaluation studies. The claim that the omni-modality captions effectively capture cross-modal relationships depends heavily on the LLM's performance, which is not directly evaluated.

**Low Confidence:** The paper does not provide ablation studies showing the relative contribution of each modality to the final performance, making it difficult to quantify the true value of the multi-modal approach versus improvements that might come from scale alone. The computational requirements for training VAST are not specified, limiting understanding of the approach's practical accessibility.

## Next Checks

1. **Caption Quality Validation:** Select a random sample of 100 videos from VAST-27M and have human annotators rate the quality of vision, audio, and omni-modality captions against ground truth. Compare these ratings with model performance on downstream tasks to establish correlation between caption quality and task performance.

2. **Modality Ablation Study:** Retrain VAST variants with one modality removed at a time (vision-only, audio-only, text-only) and evaluate on the same 22 benchmarks. This would quantify the marginal contribution of each modality and help determine if the multi-modal approach provides additive benefits or if certain modalities dominate performance.

3. **Cross-Attention Analysis:** Extract and analyze the cross-attention weights from VAST's text encoder during inference on a held-out validation set. Visualize which modality contributes most to different types of queries (retrieval vs. captioning vs. QA) to understand the model's decision-making process and verify that the fusion mechanism is working as intended.