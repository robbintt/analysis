---
ver: rpa2
title: 'Garbage in, garbage out: Zero-shot detection of crime using Large Language
  Models'
arxiv_id: '2307.06844'
source_url: https://arxiv.org/abs/2307.06844
tags: []
core_contribution: This paper shows that GPT-4 can detect and classify crimes in surveillance
  videos with state-of-the-art performance using only zero-shot reasoning, when provided
  with high-quality human-written textual descriptions of the videos. However, automated
  video-to-text approaches failed to generate descriptions of sufficient quality for
  accurate reasoning, highlighting a significant gap between the potential and current
  capabilities of such models for crime detection.
---

# Garbage in, garbage out: Zero-shot detection of crime using Large Language Models

## Quick Facts
- arXiv ID: 2307.06844
- Source URL: https://arxiv.org/abs/2307.06844
- Reference count: 11
- Key outcome: GPT-4 achieves state-of-the-art crime detection accuracy with human-written descriptions but automated captioning fails to provide sufficient detail for accurate reasoning.

## Executive Summary
This paper demonstrates that GPT-4 can detect and classify crimes in surveillance videos with state-of-the-art performance using zero-shot reasoning, but only when provided with high-quality human-written textual descriptions. Automated video-to-text conversion methods (GIT, LLaVA, YOLO-v8+ByteTrack) fail to generate descriptions of sufficient quality for accurate crime detection, primarily due to loss of critical details like identity tracking and objective action descriptions. The study reveals a significant gap between the potential of LLMs for crime detection and the current capabilities of automated caption generation systems.

## Method Summary
The authors evaluate GPT-4's ability to classify crimes from surveillance videos by converting videos to text descriptions at 10-second intervals, then prompting GPT-4 to reason about the most likely crime category. They compare three automated captioning approaches (GIT, LLaVA, YOLO-v8+ByteTrack) against human-written descriptions across 168 videos from the UCF-Crime dataset. GPT-4 is prompted with a fixed task description to perform zero-shot reasoning, and performance is measured by classification accuracy against ground truth labels.

## Key Results
- GPT-4 achieves state-of-the-art accuracy (85.7%) when provided human-written descriptions of surveillance videos
- Automated captioning methods fail to capture sufficient detail for accurate crime detection, with accuracy dropping to 40-60% depending on the method
- Object tracking fails to maintain identity persistence across frames, and image captioning lacks detail about actor-object interactions necessary for crime reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can perform zero-shot crime detection when given high-quality textual descriptions
- Mechanism: The LLM leverages its pre-trained common sense knowledge to reason about contextual relationships between actors, objects, and actions described in the text, allowing it to classify crime categories without explicit training examples
- Core assumption: The textual description contains sufficient objective detail about actors, objects, and their interactions for the LLM to reason about crime
- Evidence anchors:
  - [abstract]: "when video is (manually) converted to high quality textual descriptions, large language models are capable of detecting and classifying crimes with state-of-the-art performance using only zero-shot reasoning"
  - [section]: "Our results show that while GPT-4 was able to determine the crime category with state of the art performance when provided a human generated caption of the video"
  - [corpus]: Weak evidence - related papers focus on video anomaly detection but don't directly support the zero-shot reasoning mechanism described
- Break condition: If textual descriptions lack objective detail about who did what, when, and where, the LLM cannot reason accurately about crime

### Mechanism 2
- Claim: Automated video-to-text conversion fails for crime detection due to quality gaps
- Mechanism: Current image captioning and object tracking models cannot preserve critical details needed for crime reasoning - specifically identity tracking over time and objective action descriptions
- Core assumption: Crime detection requires linking actors across time and understanding precise interactions between objects and people
- Evidence anchors:
  - [section]: "we observed that object tracking was unable to maintain a constant identity for people and objects over long time periods" and "image captioning models only describe the scene at a high level"
  - [section]: "Reasoning about crime requires details of who did what" and "without this detail, there is insufficient information to reason about whether the man is sitting on the ground to repair their own car, or is sitting on the ground to steal something from someone else's car"
  - [corpus]: No direct evidence - the paper identifies this as an open problem
- Break condition: If automated captioning systems could preserve identity tracking and generate objective, detailed descriptions of actions

### Mechanism 3
- Claim: Censoring sensitive attributes from text inputs improves fairness in crime detection
- Mechanism: By removing race and gender information from textual descriptions, the LLM cannot use these protected attributes in its decision-making process, reducing bias
- Core assumption: LLMs may use demographic information in ways that introduce bias, and textual representations allow easier censorship than images
- Evidence anchors:
  - [section]: "textual descriptions provide a way to restrict which information the LLM has access to by censoring details the LLM should not use in its decision, such as race and gender"
  - [section]: "Inspecting the chain-of-thought produced by the LLM for bias is insufficient, as LLMs may produce unfaithful explanations that do not reveal the underlying factors that influenced the decision"
  - [corpus]: Weak evidence - fairness considerations are mentioned but not empirically tested in this paper
- Break condition: If LLM reasoning doesn't actually depend on censored attributes, or if other proxy features can be used to infer the same information

## Foundational Learning

- Concept: Zero-shot reasoning in LLMs
  - Why needed here: Understanding how GPT-4 can classify crime categories without explicit training examples is central to the paper's contribution
  - Quick check question: How does the "let's think step by step" prompt enable zero-shot reasoning in LLMs?

- Concept: Video-to-text conversion quality requirements
  - Why needed here: The paper shows automated methods fail because they cannot generate descriptions of sufficient quality for crime reasoning
  - Quick check question: What specific types of information loss occur when converting surveillance video to automated text descriptions?

- Concept: Chain-of-thought reasoning limitations
  - Why needed here: The paper notes that LLM explanations may be unfaithful and not reveal true decision factors
  - Quick check question: Why might inspecting chain-of-thought reasoning be insufficient for detecting bias in LLM crime detection?

## Architecture Onboarding

- Component map:
  Video input → Frame sampling (every 10 seconds) → Caption generation (GIT, LLaVA, YOLO-v8+ByteTrack) → Text prompt construction → GPT-4 inference → Crime category classification
  Human-written descriptions serve as ground truth for evaluating automated approaches

- Critical path:
  1. High-quality textual description generation (human or automated)
  2. GPT-4 prompt formatting with zero-shot reasoning instruction
  3. GPT-4 inference and chain-of-thought output parsing
  4. Ground truth comparison for accuracy calculation

- Design tradeoffs:
  - Frame sampling rate (10 seconds) vs. token limit constraints - more frequent sampling captures more detail but risks exceeding GPT-4 input limits
  - Automated captioning method selection - image captioning (GIT) provides objective descriptions but lacks detail, while LLM-based vision models (LLaVA) provide detail but may hallucinate
  - Token limit management - long descriptions may need truncation, risking loss of critical information for crime reasoning

- Failure signatures:
  - GPT-4 outputs invalid categories or exceeds token limits
  - Automated captions fail to capture identity persistence across frames
  - Image captioning models miss critical interactions between actors and objects
  - LLM-based vision models hallucinate objects or actions not present in video

- First 3 experiments:
  1. Compare GPT-4 accuracy on human-written descriptions vs. automated captions across all 168 videos
  2. Test different frame sampling rates (5s, 10s, 15s) to find optimal balance between detail and token limits
  3. Evaluate impact of censoring race/gender information on GPT-4 accuracy to measure potential bias reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-based vision models be fine-tuned to reduce hallucination and bias when describing crime-related scenarios?
- Basis in paper: [inferred] The paper notes that LLaVA descriptions are biased towards a particular interpretation and may hallucinate about objects and actions, highlighting the need for more objective descriptions
- Why unresolved: The study only tested pre-trained models without exploring fine-tuning or domain adaptation approaches to improve objectivity and reduce hallucination
- What evidence would resolve it: Experiments comparing hallucination rates and bias in descriptions from fine-tuned LLM-based vision models versus pre-trained models on crime-related datasets

### Open Question 2
- Question: What is the minimum level of detail required in video descriptions for LLMs to accurately detect and classify crimes?
- Basis in paper: [inferred] The paper suggests that current automated captioning models lack sufficient detail for reasoning about crime, but doesn't quantify the exact information needed
- Why unresolved: The study didn't systematically vary the level of detail in descriptions to determine the threshold for accurate crime detection
- What evidence would resolve it: Controlled experiments varying the granularity of information in video descriptions and measuring the impact on crime detection accuracy

### Open Question 3
- Question: Can object tracking algorithms be improved to maintain identity consistency over longer time periods in surveillance videos?
- Basis in paper: [explicit] The paper observes that object tracking failed to maintain constant identities for people and objects over long time periods, which is crucial for crime reasoning
- Why unresolved: The study only tested existing object tracking methods without exploring architectural improvements or alternative approaches to identity persistence
- What evidence would resolve it: Comparative analysis of different object tracking architectures and their performance on maintaining identity consistency in extended surveillance scenarios

## Limitations

- Reliance on manually written descriptions as ground truth may not represent real-world surveillance video complexity
- Evaluation only measures classification accuracy without precision-recall tradeoffs or false positive rates
- Automated captioning methods have fundamental limitations but hybrid approaches combining their strengths were not explored
- Claim about fairness improvements through attribute censorship is asserted but not empirically validated

## Confidence

- **High confidence**: GPT-4 can perform zero-shot crime detection with state-of-the-art accuracy when provided with high-quality human-written descriptions
- **Medium confidence**: Automated video-to-text conversion currently fails to generate descriptions of sufficient quality for crime detection
- **Low confidence**: Censoring sensitive attributes from text inputs will significantly improve fairness in crime detection

## Next Checks

1. Implement systematic bias measurements comparing GPT-4 performance on censored vs. uncensored descriptions across demographic groups to quantify the actual impact of attribute censorship on fairness outcomes

2. Develop and test hybrid automated captioning approaches that combine object tracking identity persistence with detailed action description generation to determine if the quality gap can be bridged without human intervention

3. Evaluate the human-written description approach on a held-out set of surveillance videos from different domains (e.g., public transportation, retail) to assess whether the textual description methodology generalizes beyond the UCF-Crime dataset