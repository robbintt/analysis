---
ver: rpa2
title: Importance of negative sampling in weak label learning
arxiv_id: '2309.13227'
source_url: https://arxiv.org/abs/2309.13227
tags:
- sampling
- negative
- learning
- data
- weak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the importance of negative sampling in
  weak-label learning, where only bag-level labels are available for training. The
  authors propose several sampling strategies including gradient embedding, entropy-based,
  SVM-based margin, and BADGE to select informative negative instances from each bag.
---

# Importance of negative sampling in weak label learning

## Quick Facts
- arXiv ID: 2309.13227
- Source URL: https://arxiv.org/abs/2309.13227
- Reference count: 0
- Key outcome: Gradient embedding and BADGE negative sampling strategies improve weak-label learning performance compared to random sampling, with gradient embedding improving 28 out of 40 AudioSet classes.

## Executive Summary
This paper investigates how different negative sampling strategies can improve weak-label learning where only bag-level labels are available. The authors propose five sampling methods - random, margin, entropy, gradient embedding, and BADGE - to select informative negative instances from bags. Through experiments on CIFAR-10 and AudioSet datasets, they demonstrate that informed sampling strategies consistently outperform random sampling, with gradient-based methods showing the most significant improvements. The study reveals that negative instances are not equally irrelevant, and selecting them wisely based on their informativeness and diversity can substantially benefit weak-label classification.

## Method Summary
The method employs a bag-level classification framework where bags contain multiple instances but only bag-level labels are available. For images, a Simple CNN with mean-pooling and softmax is used, while PSLA models handle audio spectrograms. Five sampling strategies are implemented: Random (uniform selection), Margin (uncertainty-based selection using probability differences), Entropy (uncertainty-based using probability distribution entropy), Gradient Embedding (selecting instances with large gradient magnitudes), and BADGE (combining uncertainty with K-MEANS++ clustering for diversity). Models are trained using Adam optimizer with learning rate 1e-3, incorporating MixUp and Time/Frequency masking data augmentation, with weighted batch sampling maintaining a 0.5 positive-negative ratio.

## Key Results
- Gradient embedding and BADGE methods consistently outperform random sampling across both CIFAR-10 and AudioSet datasets
- Gradient embedding improves performance in 28 out of 40 AudioSet classes tested
- Not all negative instances are equally irrelevant - informed selection strategies can significantly improve weak-label classification
- The performance gains are highly class-dependent, with different strategies working better for different classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient embedding and BADGE sampling strategies improve weak-label learning by selecting negative instances that are both informative and diverse.
- Mechanism: These strategies compute gradients of the loss with respect to the model parameters for each negative instance. Instances with large gradient magnitudes are considered more informative because they contribute more to the loss, indicating they are harder to classify correctly. BADGE further clusters these instances to ensure diversity in the selected negative set.
- Core assumption: Not all negative instances are equally irrelevant; some provide more useful information for training than others.
- Evidence anchors:
  - [abstract] "Our work reveals that negative instances are not all equally irrelevant, and selecting them wisely can benefit weak-label learning."
  - [section] "Gradient embedding and BADGE methods consistently outperform others, with gradient embedding improving performance in 28 out of 40 AudioSet classes."
  - [corpus] Weak evidence; no direct corpus papers on gradient-based sampling for weak labels found.

### Mechanism 2
- Claim: Entropy-based and margin-based sampling strategies select negative instances that the model is most uncertain about.
- Mechanism: Entropy-based sampling selects instances with highest entropy in the predicted probability distribution, indicating uncertainty. Margin-based sampling selects instances with smallest difference between positive and negative probabilities, also indicating uncertainty.
- Core assumption: Instances that the model is uncertain about are more informative for training because they lie near the decision boundary.
- Evidence anchors:
  - [section] "Entropy: An uncertainty-based strategy that samples k bags with the highest entropy scores."
  - [section] "Margin: An uncertainty-based strategy that samples k bags with the lowest margin, which is defined as the difference between positive probability and negative probability in each output result."
  - [corpus] Weak evidence; no direct corpus papers on entropy/margin sampling for weak labels found.

### Mechanism 3
- Claim: Random sampling serves as a baseline but is outperformed by informed sampling strategies.
- Mechanism: Random sampling selects negative instances uniformly at random, without considering their potential informativeness or diversity.
- Core assumption: Without any guidance, random selection may miss informative negative instances or include many uninformative ones.
- Evidence anchors:
  - [section] "Random: Randomly select k bags with uniform distribution at each epoch. Each negative bag has an equal chance of being selected. This strategy is the baseline for all the sampling strategies."
  - [section] "Table 1 shows the results of 2 classes in the CIFAR dataset. In the experiments, the KL-Divergence method is implemented in two ways... As we can see, Random i.e. our baseline sampling strategy, performs relatively well for test data."
  - [corpus] Weak evidence; no direct corpus papers comparing random vs informed sampling for weak labels found.

## Foundational Learning

- Concept: Weak-label learning
  - Why needed here: The paper focuses on weak-label learning where only bag-level labels are available, not instance-level labels.
  - Quick check question: In weak-label learning, do we know the labels of individual instances within a bag, or only the label of the entire bag?

- Concept: Multiple Instance Learning (MIL)
  - Why needed here: Weak-label learning is a form of MIL where each bag is labeled, but instances within the bag are not individually labeled.
  - Quick check question: In MIL, what determines the label of a bag?

- Concept: Negative sampling
  - Why needed here: The paper investigates different strategies for selecting negative instances from bags to improve training.
  - Quick check question: Why might some negative instances be more informative for training than others?

## Architecture Onboarding

- Component map: Simple CNN/ResNet (images) or PSLA (audio) -> Aggregation layer (mean-pooling) -> Softmax output -> Sampling module (selects negative bags)
- Critical path: Forward pass through base model and aggregation layer → Sampling module runs before each epoch to select negative bags → Backward pass with sampled negatives
- Design tradeoffs: Gradient-based sampling (BADGE, gradient embedding) is more computationally expensive but potentially more effective. Random sampling is cheap but may be less effective. Entropy/margin sampling is a middle ground.
- Failure signatures: If the model overfits to the training data, or if certain classes are consistently misclassified, it may indicate issues with the sampling strategy or model architecture.
- First 3 experiments:
  1. Run the baseline model with random sampling on a small subset of the data to establish performance.
  2. Implement and test one informed sampling strategy (e.g., gradient embedding) on the same subset to compare performance.
  3. Gradually scale up to the full dataset and compare all sampling strategies, analyzing which ones perform best for different classes or data types.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal negative sampling strategies for different types of weakly-labeled data (e.g., audio vs. image) and how do they vary with data characteristics like class imbalance or label density?
- Basis in paper: [explicit] The authors state "We demonstrate the applicability of sampling strategy for image and audio classification tasks" and find different strategies work better for different classes, but don't provide a comprehensive analysis across data types.
- Why unresolved: The paper only tests on two datasets (CIFAR-10 and AudioSet) with limited variations in data characteristics. A systematic study across diverse weakly-labeled data types is needed.
- What evidence would resolve it: Experiments comparing multiple sampling strategies on a wide range of weakly-labeled datasets with varying characteristics (class imbalance, label density, etc.) to identify optimal strategies for each data type.

### Open Question 2
- Question: How can negative sampling strategies be adapted to handle more complex weakly-labeled learning scenarios beyond binary classification, such as multi-label or multi-class problems?
- Basis in paper: [inferred] The paper focuses on binary weak-label classification, but mentions "In a multiclass weak classification problem, Y = [K]". This suggests the need to extend strategies to more complex scenarios.
- Why unresolved: The proposed strategies are primarily evaluated on binary classification tasks. Their effectiveness in multi-label or multi-class settings remains unexplored.
- What evidence would resolve it: Experiments testing the sampling strategies on diverse multi-label and multi-class weakly-labeled learning problems, with performance comparisons to current methods.

### Open Question 3
- Question: What is the theoretical foundation behind the effectiveness of gradient-based sampling strategies in weak-label learning, and how can it be further improved?
- Basis in paper: [explicit] The authors find gradient embedding and BADGE methods perform best, but don't provide a deep theoretical explanation for their success.
- Why unresolved: While the paper shows empirical results, it lacks a theoretical framework explaining why gradient-based methods excel in weak-label learning.
- What evidence would resolve it: A theoretical analysis connecting gradient-based sampling to the unique challenges of weak-label learning, potentially leading to improved sampling strategies based on this understanding.

## Limitations

- Performance gains are highly class-dependent, with gradient embedding improving only 28 out of 40 AudioSet classes tested
- Computational overhead of gradient-based methods (BADGE and gradient embedding) is not quantified
- Limited exploration of how sampling strategies interact with different MIL architectures beyond the tested CNN and PSLA models

## Confidence

- High confidence: The observation that not all negative instances are equally informative is well-supported by experimental results across both datasets
- Medium confidence: The superiority of gradient embedding and BADGE strategies is demonstrated but requires further validation on more diverse MIL architectures and datasets
- Low confidence: The mechanism by which entropy and margin-based strategies select informative negatives is less rigorously tested, with limited ablation studies

## Next Checks

1. Conduct computational efficiency analysis comparing training time per epoch across all sampling strategies, particularly for gradient-based methods
2. Test sampling strategies on additional MIL architectures (e.g., attention-based models, transformer-based MIL) to verify generalizability
3. Perform ablation studies isolating the impact of diversity (via clustering in BADGE) versus pure informativeness to understand which component drives performance gains