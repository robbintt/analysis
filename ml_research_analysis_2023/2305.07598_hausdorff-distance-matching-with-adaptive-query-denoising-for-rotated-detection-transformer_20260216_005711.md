---
ver: rpa2
title: Hausdorff Distance Matching with Adaptive Query Denoising for Rotated Detection
  Transformer
arxiv_id: '2305.07598'
source_url: https://arxiv.org/abs/2305.07598
tags:
- object
- detection
- rotated
- training
- r-50
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of detecting rotated objects
  using Detection Transformers (DETRs). The authors identify two key issues: duplicate
  low-confidence predictions and unstable training in rotated DETR models.'
---

# Hausdorff Distance Matching with Adaptive Query Denoising for Rotated Detection Transformer

## Quick Facts
- arXiv ID: 2305.07598
- Source URL: https://arxiv.org/abs/2305.07598
- Reference count: 40
- This paper introduces RHINO, a rotated detection transformer that achieves state-of-the-art performance on DOTA-v1.0/v1.5/v2.0 and DIOR-R benchmarks, with improvements of +4.18 AP50, +4.59 AP50, and +4.99 AP50 respectively compared to models using a ResNet-50 backbone.

## Executive Summary
This paper addresses the challenge of detecting rotated objects using Detection Transformers (DETRs). The authors identify two key issues in rotated DETR models: duplicate low-confidence predictions and unstable training. To address these problems, they introduce a Hausdorff distance-based cost for bipartite matching and an adaptive query denoising method that uses Hungarian matching to filter out noisy queries. Their proposed model, RHINO, achieves state-of-the-art performance on DOTA-v1.0/v1.5/v2.0 and DIOR-R benchmarks.

## Method Summary
The RHINO model addresses rotated object detection challenges through three key innovations: CenterL1 distance for bipartite matching to reduce duplicate low-confidence predictions, dynamic denoising via Hungarian matching to filter noisy queries during later training stages, and query alignment to maintain consistent matching assignments across decoder layers. The model uses a 5D coordinate system for rotated box representation and builds upon existing denoising training strategies while adding these stability improvements.

## Key Results
- RHINO achieves +4.18 AP50 improvement on DOTA-v1.0 compared to ResNet-50 baseline
- RHINO achieves +4.59 AP50 improvement on DOTA-v1.5 compared to ResNet-50 baseline
- RHINO achieves +4.99 AP50 improvement on DIOR-R compared to ResNet-50 baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CenterL1 distance reduces duplicate low-confidence predictions by decoupling angle from spatial matching cost
- Mechanism: Standard L1 cost in bipartite matching conflates angular misalignment with positional distance, causing perpendicular boxes to match incorrectly. CenterL1 uses only center coordinates for matching, eliminating angle-induced false matches
- Core assumption: Angular errors in rotated boxes are the primary cause of duplicate predictions, and positional matching can be isolated from angular errors
- Evidence anchors: [abstract] "the boundary discontinuity and square-like problem in bipartite matching poses an issue with assigning appropriate ground truths to predictions, leading to duplicate low-confidence predictions." [section 3.3] "we have observed a persistent occurrence of duplicate predictions under specific conditions, as depicted in the left image of Figure 1. These duplicate predictions consistently manifest themselves with lower confidence scores, most notably in objects resembling a square shape."

### Mechanism 2
- Claim: Dynamic denoising via Hungarian matching filters noisy positive queries during later training stages when predictions exceed noisy ground truths
- Mechanism: Instead of using all noised queries for denoising training, Hungarian matching selects only those noised queries whose matched prediction index equals their ground-truth index, filtering out overly noisy queries
- Core assumption: During later training, model predictions become more accurate than noised ground truths, and some positive noised queries become detrimental to training
- Evidence anchors: [abstract] "we propose an adaptive query denoising method that employs bipartite matching to selectively eliminate noised queries that detract from model improvement." [section 3.4] "we observed that a large proportion of positive noised queries for denoising training is too noisy to guide the model in the later training phase since the model already proposes more accurate predictions."

### Mechanism 3
- Claim: Query alignment maintains consistent matching assignments across decoder layers, preventing unstable optimization from layer-to-layer assignment variation
- Mechanism: Instead of performing bipartite matching independently in each decoder layer, query alignment uses the assignment indices from the first bipartite matching (between encoder proposals and ground-truths) across all decoder layers
- Core assumption: Inconsistent matching assignments across decoder layers create unstable optimization paths that contradict DETR's one-to-one matching strategy
- Evidence anchors: [abstract] "we propose a query alignment method that applies the same assignment from the first bipartite matching results to all decoder layers to mitigate the inconsistent matching issue across the decoder layers mentioned by Chen et al. [33] and Liu et al. [34]."

## Foundational Learning

- Concept: Hungarian matching algorithm for bipartite graph optimization
  - Why needed here: Core to both bipartite matching cost calculation and dynamic denoising query filtering
  - Quick check question: What is the computational complexity of the Hungarian algorithm for N predictions and M ground truths?

- Concept: Intersection over Union (IoU) and its limitations for rotated boxes
  - Why needed here: Understanding why GIoU cannot be used directly for rotated objects and why alternatives like KLD are needed
  - Quick check question: Why is calculating IoU between rotated boxes more complex than axis-aligned boxes?

- Concept: Query denoising training strategy in DETR variants
  - Why needed here: Understanding the DN-DETR and DINO approaches that this work builds upon
  - Quick check question: How does denoising training accelerate convergence in DETR compared to standard training?

## Architecture Onboarding

- Component map: Input image -> Backbone features -> Encoder -> Decoder queries -> Regression head -> Hungarian matching -> Loss computation -> Model update
- Critical path: Input image → Backbone features → Encoder → Decoder queries → Regression head → Hungarian matching → Loss computation → Model update
- Design tradeoffs: Using CenterL1 instead of L1 for matching reduces duplicates but may slightly decrease precision on angle-sensitive objects; dynamic denoising improves final performance but slows early training convergence; query alignment stabilizes training but may reduce decoder layer specialization
- Failure signatures: Duplicate low-confidence predictions clustered around square-like objects; performance degradation when training beyond 12 epochs without dynamic denoising; inconsistent predictions across decoder layers when query alignment is absent
- First 3 experiments: 1) Replace L1 matching cost with CenterL1 and measure duplicate prediction reduction on DOTA validation set; 2) Implement basic dynamic denoising without query alignment and compare 12 vs 36 epoch performance; 3) Add query alignment to the dynamic denoising model and verify consistent assignments across decoder layers using debugging visualization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CenterL1 distance metric compare to other distance metrics (like GWD or KLD) for rotated object detection in terms of training stability and final performance?
- Basis in paper: [explicit] The paper mentions that CenterL1 significantly improves performance over L1 and reduces duplicate predictions, but doesn't compare it to other rotated object distance metrics like GWD or KLD
- Why unresolved: The authors only tested CenterL1 against L1 and showed it outperforms L1, but didn't benchmark against other state-of-the-art distance metrics designed specifically for rotated objects
- What evidence would resolve it: A controlled experiment comparing CenterL1, GWD, and KLD as both matching costs and training losses on the same rotated object detection benchmarks, measuring both convergence speed and final AP50/AP75

### Open Question 2
- Question: What is the impact of dynamic denoising on training convergence speed, particularly in the early training phases, and can this be optimized?
- Basis in paper: [explicit] The paper explicitly states that dynamic denoising appears to slow down convergence, especially in early training phases, with the model trained for 36 epochs outperforming the 12-epoch model, but the 12-epoch model with denoising underperforming the one without
- Why unresolved: The authors only observe this phenomenon but don't investigate optimization strategies or the underlying causes beyond noting the variance in noised queries during early training
- What evidence would resolve it: Experiments varying the amount of noise added, the timing of when dynamic denoising is applied, or alternative denoising strategies that could accelerate early training while maintaining the benefits

### Open Question 3
- Question: How would incorporating specialized rotated object detection architectures (like ReDet or S2A-Net) affect the performance of the RHINO model?
- Basis in paper: [explicit] The discussion section explicitly states that the study focused on simplicity and didn't explore specialized architectures, acknowledging this as a limitation and suggesting it as future work
- Why unresolved: The authors deliberately chose to prioritize simplicity over architectural innovations, leaving the question of whether these specialized designs would provide additional benefits unanswered
- What evidence would resolve it: Experiments integrating architectural features from ReDet (rotation-equivariant features) or S2A-Net (self-attention alignment) into the RHINO framework and measuring the impact on detection accuracy

## Limitations
- The effectiveness of CenterL1 matching cost depends heavily on the square-like nature of target objects, with unclear performance on highly elongated rotated objects
- Dynamic denoising introduces additional computational overhead during training without clear ablation on training speed impact
- Query alignment may reduce decoder layer specialization, potentially limiting the model's ability to learn hierarchical feature representations

## Confidence
- **High Confidence**: The mechanism for duplicate reduction using CenterL1 matching is well-supported by observed failure cases and has clear theoretical justification
- **Medium Confidence**: The dynamic denoising approach builds on established DN-DETR methodology but lacks extensive ablation studies on optimal timing and filtering thresholds
- **Medium Confidence**: Query alignment addresses documented inconsistencies in DETR decoder layers but may trade off against layer specialization benefits

## Next Checks
1. Test CenterL1 matching on non-square rotated objects (e.g., ships, airplanes in DOTA) to verify generalization beyond the reported square-like objects
2. Perform ablation study comparing training convergence speed and final performance with and without dynamic denoising at different training stages
3. Visualize decoder layer outputs with and without query alignment to quantify the trade-off between matching consistency and layer specialization