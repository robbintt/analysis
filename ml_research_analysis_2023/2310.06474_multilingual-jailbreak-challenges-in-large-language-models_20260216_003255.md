---
ver: rpa2
title: Multilingual Jailbreak Challenges in Large Language Models
arxiv_id: '2310.06474'
source_url: https://arxiv.org/abs/2310.06474
tags:
- unsafe
- language
- multilingual
- languages
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines multilingual safety risks in large language
  models (LLMs), revealing that safety mechanisms perform worse in low-resource languages,
  leading to higher rates of harmful content generation. Two risk scenarios are considered:
  unintentional (non-English users bypassing safety mechanisms) and intentional (malicious
  users exploiting multilingual prompts with harmful instructions).'
---

# Multilingual Jailbreak Challenges in Large Language Models

## Quick Facts
- arXiv ID: 2310.06474
- Source URL: https://arxiv.org/abs/2310.06474
- Reference count: 28
- One-line primary result: Multilingual jailbreaks expose up to three times higher unsafe rates in low-resource languages; SELF-DEFENCE fine-tuning reduces unsafe outputs by up to 20.92%.

## Executive Summary
This paper investigates safety vulnerabilities in large language models when faced with multilingual jailbreak attempts. It identifies two scenarios: unintentional (non-English users bypassing safety mechanisms) and intentional (malicious users exploiting multilingual prompts with harmful instructions). Experiments reveal that low-resource languages face up to three times higher rates of unsafe content generation compared to high-resource languages, and combining malicious instructions with multilingual prompts can push unsafe outputs to 80.92% for ChatGPT and 40.71% for GPT-4. To mitigate these risks, the authors propose the SELF-DEFENCE framework, which automatically generates multilingual safety training data without human annotation. Fine-tuning ChatGPT with this data reduces unsafe outputs by 6.24% in unintentional scenarios and 20.92% in intentional scenarios, highlighting the need for robust multilingual safety measures.

## Method Summary
The study constructs the MultiJail benchmark with 3,150 samples across 10 languages (3 resource tiers) to evaluate multilingual jailbreak risks. Two scenarios are tested: unintentional (harmful prompts without malicious intent) and intentional (harmful prompts combined with strong malicious instructions). The SELF-DEFENCE framework is introduced to automatically generate multilingual safety training data using seed examples and LLM-based expansion, without human annotation. ChatGPT is fine-tuned on this dataset for 3 epochs, and performance is measured via unsafe rates and safeness metrics across both scenarios. General multilingual capabilities are validated using XNLI and X-CSQA datasets.

## Key Results
- Low-resource languages exhibit up to three times higher unsafe rates than high-resource languages.
- Malicious multilingual prompts boost unsafe outputs to 80.92% for ChatGPT and 40.71% for GPT-4.
- SELF-DEFENCE fine-tuning reduces unsafe outputs by 6.24% in unintentional and 20.92% in intentional scenarios.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual jailbreak works because safety fine-tuning is predominantly in English, leaving non-English prompts poorly defended.
- Mechanism: LLMs receive safety training on English prompts, creating a safety boundary that does not generalize to other languages. Low-resource languages are even less protected because their training data is sparse.
- Core assumption: Safety mechanisms are language-dependent and rely on exposure to safety examples in that language.
- Evidence anchors:
  - [abstract] "Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English data."
  - [section] "In the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases."
  - [corpus] "A Cross-Language Investigation into Jailbreak Attacks in Large Language Models" supports this mechanism with multilingual safety analysis.
- Break condition: If safety fine-tuning data is expanded to include all target languages with comparable breadth and depth, the jailbreak advantage of non-English prompts should diminish.

### Mechanism 2
- Claim: Malicious instructions amplify jailbreak effectiveness when combined with multilingual prompts.
- Mechanism: The presence of a malicious instruction (e.g., "AIM") provides a strong override signal that dominates the safety layer, and coupling this with a multilingual prompt bypasses language-specific safety checks.
- Core assumption: The model prioritizes instruction-following over safety alignment when the instruction is strongly phrased, and language mismatch weakens safety enforcement.
- Evidence anchors:
  - [abstract] "In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92% for ChatGPT and 40.71% for GPT-4."
  - [section] "Malicious actors may take advantage of the vulnerabilities in these models to intentionally map their harmful prompts into low-resource languages."
  - [corpus] "Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression" discusses instruction-based jailbreak strategies.
- Break condition: If the model's instruction-following behavior is decoupled from safety-critical contexts or if multilingual safety data includes adversarial instruction combinations, the amplification effect should be reduced.

### Mechanism 3
- Claim: Low-resource languages present higher jailbreak risk because the model's multilingual capability is unevenly distributed across languages.
- Mechanism: During pre-training, high-resource languages receive more exposure, leading to stronger linguistic and safety alignment. Low-resource languages have weaker representation, making safety alignment incomplete.
- Core assumption: The model's ability to understand and apply safety constraints is proportional to the quality and quantity of multilingual training data for each language.
- Evidence anchors:
  - [abstract] "Specifically, low-resource languages exhibit about three times the likelihood of encountering harmful content compared to high-resource languages."
  - [section] "When examining the language categories, we notice a consistent pattern similar to our preliminary experiments, where the presence of unsafe content increases as language availability decreases."
  - [corpus] "LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges" explores the relationship between training data diversity and security vulnerabilities.
- Break condition: If balanced multilingual data is provided or if safety fine-tuning explicitly targets low-resource languages, the disparity in unsafe rates should decrease.

## Foundational Learning

- Concept: Language resource availability in pre-training corpora
  - Why needed here: Explains why certain languages are more vulnerable to jailbreaks due to less training data.
  - Quick check question: If a language has a data ratio of 0.05% in CommonCrawl, is it high-resource or low-resource?

- Concept: Safety alignment vs. instruction following
  - Why needed here: Shows the tension between following user instructions and adhering to safety constraints.
  - Quick check question: If a model receives a malicious instruction and a safe prompt in different languages, which factor dominates behavior?

- Concept: Fine-tuning with synthetic data
  - Why needed here: The SELF-DEFENCE framework relies on auto-generated multilingual safety data without human annotation.
  - Quick check question: Why might synthetic data be more scalable for multilingual safety training than human-annotated data?

## Architecture Onboarding

- Component map:
  - Input layer: User prompt (potentially multilingual + malicious instruction)
  - Safety filter: English-trained safety mechanisms
  - Language model core: Multilingual understanding with uneven safety alignment
  - Output layer: Generated response (safe/unsafe/invalid)

- Critical path:
  1. Receive prompt in any language
  2. Safety layer checks against English-trained patterns
  3. If no safety trigger, model generates response
  4. Output is evaluated for safety content

- Design tradeoffs:
  - Tradeoff between safety coverage and model usability: Broad safety fine-tuning in many languages may reduce performance or fluency.
  - Tradeoff between manual and synthetic data: Synthetic data scales but may miss nuanced safety failures; manual data is precise but expensive.

- Failure signatures:
  - Higher unsafe rates in low-resource languages indicate incomplete safety alignment.
  - Dramatic increases in unsafe outputs when combining multilingual prompts with malicious instructions indicate instruction dominance over safety.
  - Invalid outputs in non-English languages may indicate model comprehension issues.

- First 3 experiments:
  1. Measure unsafe rates across 10 languages (3 resource tiers) with identical harmful prompts.
  2. Combine each prompt with a strong malicious instruction and measure unsafe rates.
  3. Apply SELF-DEFENCE fine-tuning and compare unsafe rates before/after for both unintentional and intentional scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the SELF-DEFENCE framework vary across different low-resource languages, and what factors contribute to these differences?
- Basis in paper: [inferred] The paper discusses the challenges of multilingual jailbreak in low-resource languages and the effectiveness of the SELF-DEFENCE framework in reducing unsafe outputs, but does not provide a detailed analysis of its effectiveness across different low-resource languages.
- Why unresolved: The paper does not provide a detailed breakdown of the effectiveness of the SELF-DEFENCE framework across different low-resource languages, making it difficult to understand the specific factors that contribute to variations in its effectiveness.
- What evidence would resolve it: A detailed analysis of the effectiveness of the SELF-DEFENCE framework across different low-resource languages, including factors such as language similarity, cultural context, and the nature of harmful queries, would help to understand the variations in its effectiveness.

### Open Question 2
- Question: What are the long-term implications of using the SELF-DEFENCE framework on the overall capabilities of LLMs, particularly in terms of their ability to handle complex tasks and maintain general knowledge?
- Basis in paper: [explicit] The paper mentions a trade-off between safety and usefulness when using the SELF-DEFENCE framework, with an increase in safeness leading to a decrease in usefulness.
- Why unresolved: The paper does not provide a detailed analysis of the long-term implications of using the SELF-DEFENCE framework on the overall capabilities of LLMs, particularly in terms of their ability to handle complex tasks and maintain general knowledge.
- What evidence would resolve it: A long-term study of the impact of the SELF-DEFENCE framework on the overall capabilities of LLMs, including their performance on complex tasks and their ability to maintain general knowledge, would help to understand the potential trade-offs and benefits of using this framework.

### Open Question 3
- Question: How can the SELF-DEFENCE framework be adapted to address the unique challenges posed by different types of harmful content, such as misinformation, hate speech, and explicit content?
- Basis in paper: [inferred] The paper discusses the challenges of multilingual jailbreak and the effectiveness of the SELF-DEFENCE framework in reducing unsafe outputs, but does not provide a detailed analysis of how the framework can be adapted to address different types of harmful content.
- Why unresolved: The paper does not provide a detailed analysis of how the SELF-DEFENCE framework can be adapted to address different types of harmful content, making it difficult to understand the specific challenges and solutions for each type of content.
- What evidence would resolve it: A detailed analysis of how the SELF-DEFENCE framework can be adapted to address different types of harmful content, including the specific challenges and solutions for each type of content, would help to understand the potential applications and limitations of the framework.

## Limitations

- The SELF-DEFENCE framework relies on synthetic data, which may not capture all real-world safety nuances.
- Evaluation depends on GPT-4 as a safety judge, introducing potential subjectivity.
- Findings are limited to ChatGPT and GPT-4, reducing generalizability to other models.

## Confidence

- Confidence in observed disparities in unsafe rates: High
- Confidence in effectiveness of SELF-DEFENCE framework: High
- Confidence in long-term robustness of synthetic data approach: Medium
- Confidence in scalability across diverse LLM architectures: Medium

## Next Checks

1. Test the framework on additional LLM families beyond ChatGPT and GPT-4.
2. Evaluate the impact of synthetic vs. human-annotated safety data on fine-tuning outcomes.
3. Assess the framework's performance in low-resource languages with minimal pre-training data.