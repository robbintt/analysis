---
ver: rpa2
title: 'Generative Large Language Models Are All-purpose Text Analytics Engines: Text-to-text
  Learning Is All Your Need'
arxiv_id: '2312.06099'
source_url: https://arxiv.org/abs/2312.06099
tags:
- clinical
- tasks
- extraction
- concept
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explored using a unified generative large language model,
  GatorTronGPT, with prompt tuning to solve seven major clinical NLP tasks. The approach
  reformulated tasks like concept extraction, relation extraction, abbreviation disambiguation,
  and natural language inference as text-to-text problems, applying soft prompts while
  keeping LLM parameters frozen.
---

# Generative Large Language Models Are All-purpose Text Analytics Engines: Text-to-text Learning Is All Your Need

## Quick Facts
- arXiv ID: 2312.06099
- Source URL: https://arxiv.org/abs/2312.06099
- Authors: 
- Reference count: 0
- Unified generative LLM (GatorTronGPT) with prompt tuning achieves state-of-the-art performance on 7 clinical NLP tasks

## Executive Summary
This study demonstrates that a single generative large language model can serve as an all-purpose text analytics engine for clinical NLP by reformulating diverse tasks as text-to-text learning problems. Using GatorTronGPT with prompt tuning (frozen LLM parameters, trainable soft prompts), the authors solve seven major clinical NLP tasks including concept extraction, relation extraction, abbreviation disambiguation, and natural language inference. The approach achieves state-of-the-art performance on five tasks, outperforming task-specific transformer models by 3-10% across various metrics.

## Method Summary
The method reformulates seven clinical NLP tasks as text-to-text learning problems where both inputs and outputs are textual sequences. A unified generative LLM (GatorTronGPT) with 5B or 20B parameters is adapted to each task using prompt tuning - training only soft prompt vectors while keeping the LLM parameters frozen. Soft prompts are initialized using bidirectional LSTM and concatenated with input embeddings. The model is trained on 277 billion words of clinical and general English text, and evaluated on seven benchmark datasets for clinical concept extraction, relation extraction, abbreviation disambiguation, and natural language inference.

## Key Results
- State-of-the-art performance on 5 out of 7 clinical NLP tasks
- ~3% improvement in concept and relation extraction F1 scores
- 3.4% improvement in concept normalization accuracy
- 3.4-10% improvement in abbreviation disambiguation accuracy
- 5.5-9% improvement in natural language inference accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative LLMs can solve diverse clinical NLP tasks through unified text-to-text learning without task-specific architectures
- Mechanism: Reformulating each task as a text generation problem where both input and output are textual sequences, enabling a single model to handle all tasks via prompt tuning
- Core assumption: Text-to-text reformulation preserves task semantics and does not lose critical structural information needed for accurate predictions
- Evidence anchors:
  - [abstract] "We formulated 7 key clinical NLP tasks as text-to-text learning and solved them using one unified generative clinical LLM"
  - [section] "To solve all 7 clinical NLP tasks using a unified LLM, we reformatted all benchmark datasets into a text-to-text learning format, where both the input and output are in text form."
  - [corpus] Weak evidence; corpus contains general NLP applications but lacks direct clinical task reformulation studies
- Break condition: Tasks requiring structured outputs (e.g., spans or discrete labels) cannot be accurately represented or parsed from generated text without ambiguity

### Mechanism 2
- Claim: Prompt tuning with frozen LLM parameters achieves competitive performance while avoiding task-specific fine-tuning
- Mechanism: Trainable soft prompts (learnable vectors) are optimized to condition the frozen LLM for each task, keeping original model parameters unchanged
- Core assumption: Soft prompts can encode task-specific instructions sufficiently without updating the LLM weights
- Evidence anchors:
  - [abstract] "We adopted soft prompts (i.e., trainable vectors) with frozen LLM, where the LLM parameters were not updated (i.e., frozen) and only the vectors of soft prompts were updated, known as prompt tuning."
  - [section] "We used soft prompts initialized by a bidirectional Long Short Term Memory (LSTM) to instruct GatorTronGPT to generate accurate responses for different tasks."
  - [corpus] Moderate evidence; prompt tuning literature shows effectiveness, but specific clinical adaptation studies are sparse
- Break condition: Complex tasks requiring deep task-specific knowledge adaptation may not be captured by prompt vectors alone

### Mechanism 3
- Claim: Scaling LLM size improves text-to-text learning performance across clinical NLP tasks
- Mechanism: Larger models with more parameters capture richer linguistic patterns and domain knowledge, enhancing generalization
- Core assumption: Increased model capacity directly translates to better task performance without overfitting
- Evidence anchors:
  - [abstract] "We explored GatorTronGPT, a generative clinical LLM developed in our previous study... trained on 277 billion words of clinical and general English text."
  - [section] "We observed performance improvements in both datasets when scaling up from the GatorTronGPT-5B to the GatorTronGPT-20B model."
  - [corpus] Weak evidence; corpus lacks detailed scaling studies comparing different model sizes in clinical NLP
- Break condition: Diminishing returns or overfitting if model size increases without corresponding data or task complexity

## Foundational Learning

- Concept: Text-to-text learning reformulation
  - Why needed here: Enables diverse NLP tasks to be mapped into a unified generative framework, avoiding task-specific architectures
  - Quick check question: How would you convert a relation extraction task into a text generation prompt?

- Concept: Prompt tuning with frozen parameters
  - Why needed here: Allows adaptation to multiple tasks without modifying the core LLM, preserving generalization and reducing computational cost
  - Quick check question: What is the difference between prompt tuning and full fine-tuning in terms of parameter updates?

- Concept: Clinical NLP task diversity
  - Why needed here: Understanding task types (extraction, classification, normalization) is crucial for proper reformulation and evaluation
  - Quick check question: Which clinical NLP tasks are most challenging to represent as text generation?

## Architecture Onboarding

- Component map: Unified LLM (GatorTronGPT) → Soft prompt encoder → Text generation module → Task-specific output parser
- Critical path: Input text → Soft prompt conditioning → LLM forward pass → Generated text → Post-processing to structured output
- Design tradeoffs: Single model simplicity vs. potential performance gap vs. task-specific models; prompt expressiveness vs. model parameter freeze
- Failure signatures: Hallucinations in generated text, ambiguous outputs, task performance degradation when scaling to more diverse tasks
- First 3 experiments:
  1. Test text-to-text reformulation on a simple clinical concept extraction task and compare F1 score to baseline NER model
  2. Evaluate prompt tuning on two tasks simultaneously to check for cross-task interference
  3. Compare GatorTronGPT-5B vs. GatorTronGPT-20B performance on a subset of tasks to observe scaling effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GatorTronGPT scale with increasing parameter size beyond 20 billion parameters?
- Basis in paper: [explicit] The paper notes that performance improvements were observed when scaling up from the 5 billion to 20 billion parameter models, indicating that larger models may further enhance performance
- Why unresolved: The study only evaluated GatorTronGPT models with 5 billion and 20 billion parameters, leaving the impact of even larger models untested
- What evidence would resolve it: Evaluation of GatorTronGPT with parameter sizes larger than 20 billion on the same clinical NLP tasks to determine if further scaling continues to improve performance

### Open Question 2
- Question: What are the specific causes and patterns of hallucinations in GatorTronGPT, and how can they be systematically reduced?
- Basis in paper: [explicit] The paper identifies three types of hallucinations (nonlogical, irrelevant, interpretable) but does not provide detailed analysis or solutions for reducing them
- Why unresolved: While hallucinations are noted as a challenge, the study does not delve into their underlying causes or propose methods for mitigation
- What evidence would resolve it: Detailed error analysis identifying the root causes of hallucinations and development of techniques or fine-tuning strategies to minimize their occurrence

### Open Question 3
- Question: How does GatorTronGPT perform on clinical NLP tasks in cross-institutional or multi-domain settings?
- Basis in paper: [inferred] The study evaluates GatorTronGPT on benchmark datasets, but real-world deployment may involve diverse data sources and institutions
- Why unresolved: The experiments were conducted using specific datasets, and the model's generalizability to different clinical settings is not addressed
- What evidence would resolve it: Testing GatorTronGPT on clinical NLP tasks using data from multiple institutions or domains to assess its adaptability and performance consistency

### Open Question 4
- Question: How does multi-task instruction learning compare to the current soft prompting approach in improving GatorTronGPT's performance?
- Basis in paper: [inferred] The paper mentions that fine-tuning on many different tasks could improve performance, but does not explore this approach
- Why unresolved: The study focuses on soft prompting with frozen LLMs, leaving the potential benefits of multi-task instruction learning unexplored
- What evidence would resolve it: Comparative experiments between soft prompting and multi-task instruction learning on the same clinical NLP tasks to evaluate performance differences

## Limitations
- Task reformulation ambiguity may introduce parsing errors when converting structured outputs to text
- Prompt tuning scalability is uncertain when scaling to dozens or hundreds of tasks
- Model size dependency makes it unclear whether improvements come from the unified approach or larger models

## Confidence

**High Confidence**:
- Generative LLMs can be effectively used for clinical text analytics through text-to-text reformulation
- Prompt tuning with frozen parameters is a viable alternative to full fine-tuning for task adaptation
- The GatorTronGPT model demonstrates state-of-the-art performance on the tested benchmark datasets

**Medium Confidence**:
- The text-to-text approach provides consistent improvements across all seven clinical NLP tasks
- Soft prompts can sufficiently encode task-specific instructions without updating LLM parameters
- Scaling model size improves performance in this unified framework

**Low Confidence**:
- A single generative LLM can serve as a universal "all-purpose" text analytics engine for clinical NLP without architectural specialization

## Next Checks

1. **Cross-task interference test**: Train soft prompts for all seven tasks simultaneously in a multi-task learning setup, then evaluate whether performance on individual tasks degrades compared to single-task training, measuring task interference through performance variance.

2. **Structured output fidelity evaluation**: For tasks requiring structured outputs (relation extraction, concept extraction), implement automated evaluation comparing the precision of extracted spans/labels from generated text against ground truth, measuring information loss during text-to-text conversion.

3. **Parameter efficiency comparison**: Replicate the experiments using a smaller LLM (e.g., 1B parameters) with full fine-tuning, comparing both performance and computational cost against the prompt-tuned 20B parameter model to isolate the contribution of model size versus methodology.