---
ver: rpa2
title: Knowledge Corpus Error in Question Answering
arxiv_id: '2310.18076'
source_url: https://arxiv.org/abs/2310.18076
tags:
- knowledge
- corpus
- context
- question
- gold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores why generating context passages from LLMs may
  be more effective than retrieving them from a knowledge corpus in open-domain question
  answering. The authors introduce the concept of "knowledge corpus error," which
  arises when the knowledge corpus used for retrieval is only a subset of the entire
  string space, potentially excluding more helpful passages that exist outside the
  corpus.
---

# Knowledge Corpus Error in Question Answering

## Quick Facts
- arXiv ID: 2310.18076
- Source URL: https://arxiv.org/abs/2310.18076
- Reference count: 35
- Primary result: Generating context passages from LLMs outperforms retrieving them from knowledge corpora in open-domain QA due to "knowledge corpus error"

## Executive Summary
This paper introduces the concept of "knowledge corpus error" in open-domain question answering, where the knowledge corpus used for retrieval is only a subset of the entire string space, potentially excluding more helpful passages. The authors demonstrate that generating context passages from large language models (LLMs) can outperform traditional retrieval-based methods. Through paraphrasing gold contexts from three QA benchmarks (NQ, HotPotQA, StrategyQA), they show a 10-13% performance improvement, suggesting that LLM-generated passages may access information beyond what's available in standard knowledge corpora.

## Method Summary
The paper explores whether generating context passages from LLMs is more effective than retrieving them from a knowledge corpus in open-domain question answering. The authors introduce "knowledge corpus error" - the idea that relevant context may exist outside the finite knowledge corpus. To observe this empirically, they design an experiment where human-annotated gold contexts are paraphrased using LLMs. They compare reader performance on original versus paraphrased contexts across three QA benchmarks, measuring exact match for NQ and HotPotQA, and accuracy for StrategyQA and QASC.

## Key Results
- Paraphrased passages achieve 10-13% higher performance compared to original gold contexts
- Results observed across three QA benchmarks: NQ, HotPotQA, and StrategyQA
- Evidence suggests knowledge corpus error exists when using finite knowledge corpora for retrieval

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Corpus Error
- Claim: Retrieval-based QA is limited by the finite knowledge corpus, which excludes potentially more helpful passages that exist outside the corpus.
- Core assumption: The knowledge corpus Z is a strict subset of the entire string space V*.
- Evidence anchors:
  - [abstract]: "This error arises when the knowledge corpus used for retrieval is only a subset of the entire string space, potentially excluding more helpful passages that exist outside the corpus."
  - [section]: "The gap in this formulation is that relevant context c may exist outside of the knowledge corpus Z. This makes the sum of marginal probabilities over Z only an approximation."
  - [corpus]: Limited - the paper does not provide direct evidence of the existence of more helpful passages outside the knowledge corpus. The evidence is indirect, based on the improved performance when using paraphrased passages.
- Break condition: If the knowledge corpus Z is expanded to include all possible passages, or if LLMs are unable to generate helpful passages outside the corpus.

### Mechanism 2: Paraphrasing as Information Filtering
- Claim: Paraphrasing gold context with LLMs can filter out irrelevant information and focus on the question, resulting in more helpful passages.
- Core assumption: LLMs have the capability to understand the question and filter relevant information from the gold context.
- Evidence anchors:
  - [abstract]: "LLMs may mitigate this shortcoming by generating passages in a larger space."
  - [section]: "LLMs can only filter the helpful information from the gold passage during the paraphrasing."
  - [corpus]: Limited - the paper does not provide direct evidence of the filtering capability of LLMs. The evidence is based on the improved performance when using paraphrased passages.
- Break condition: If the LLMs are unable to understand the question or filter relevant information from the gold context.

### Mechanism 3: Commonsense Knowledge Incorporation
- Claim: LLMs can incorporate commonsense knowledge during paraphrasing, which is not explicitly stated in the knowledge corpus.
- Core assumption: LLMs possess a degree of tacit knowledge that can be utilized during paraphrasing.
- Evidence anchors:
  - [abstract]: "LLMs may mitigate this shortcoming by generating passages in a larger space."
  - [section]: "Commonsense knowledge plays a crucial role in understanding the world, but not often explicitly stated, especially in a corpus such as Wikipedia."
  - [corpus]: Limited - the paper does not provide direct evidence of the commonsense knowledge incorporation capability of LLMs. The evidence is based on the improved performance when using paraphrased passages.
- Break condition: If the LLMs are unable to incorporate commonsense knowledge during paraphrasing.

## Foundational Learning

- Concept: Open-domain question answering
  - Why needed here: The paper focuses on improving the performance of open-domain question answering by generating context passages from LLMs.
  - Quick check question: What is the difference between open-domain and closed-book question answering?

- Concept: Knowledge corpus error
  - Why needed here: The paper introduces the concept of knowledge corpus error to explain why generating context passages from LLMs may be more effective than retrieving them from a knowledge corpus.
  - Quick check question: How does knowledge corpus error arise in the context of question answering?

- Concept: Paraphrasing
  - Why needed here: The paper uses paraphrasing as a method to observe knowledge corpus error empirically.
  - Quick check question: What is the purpose of paraphrasing the gold context in the experiment?

## Architecture Onboarding

- Component map: Question -> Context generation (LLM) -> Reader (LLM) -> Answer
- Critical path: Question -> Context generation (LLM) -> Reader (LLM) -> Answer
- Design tradeoffs:
  - Using generated contexts vs. retrieved contexts
  - Paraphrasing vs. using original gold context
  - Different LLM models for context generation and reading
- Failure signatures:
  - Degradation in performance when using paraphrased contexts
  - Inability of LLMs to generate helpful contexts
  - Overfitting to specific question-answer pairs
- First 3 experiments:
  1. Compare the performance of the reader when given original gold context vs. paraphrased context.
  2. Vary the LLM model used for context generation and observe the impact on reader performance.
  3. Experiment with different paraphrasing techniques and evaluate their effectiveness in improving reader performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does knowledge corpus error manifest in retrieval settings when using paraphrased contexts?
- Basis in paper: [explicit] The paper mentions that it did not employ the method in a retrieval setting and acknowledges that retrieved contexts may deviate significantly from the gold context.
- Why unresolved: The paper only tested paraphrasing gold contexts, which represent the best-case scenario in retrieval. It did not address how paraphrased contexts would perform when used in actual retrieval settings with noisy or irrelevant contexts.
- What evidence would resolve it: Experiments comparing the performance of paraphrased contexts in both gold context and retrieval settings would provide insights into how knowledge corpus error manifests in real-world scenarios.

### Open Question 2
- Question: What is the practical way to combine retrieval and generation using LLMs to leverage the benefits of both approaches?
- Basis in paper: [explicit] The paper acknowledges the limitations of generated contexts, such as information staleness and hallucination, and mentions that contemporaneous works explore various methods to combine retrieval and generation.
- Why unresolved: The paper primarily focuses on understanding the advantages of generation over retrieval but does not provide a concrete solution for integrating the two approaches effectively.
- What evidence would resolve it: Developing and evaluating a model that combines retrieval and generation, addressing issues like information staleness and hallucination, would provide a practical solution to leverage the benefits of both approaches.

### Open Question 3
- Question: How does knowledge corpus error manifest in language modeling tasks beyond question answering?
- Basis in paper: [explicit] The paper mentions that conditioning generation on retrieved context is a well-studied approach in language modeling and suggests exploring how knowledge corpus error manifests in this domain.
- Why unresolved: The paper's scope is limited to question answering, and it does not investigate how knowledge corpus error affects other language modeling tasks.
- What evidence would resolve it: Conducting experiments on various language modeling tasks, such as text completion, summarization, and dialogue generation, would reveal how knowledge corpus error impacts these tasks and provide insights into potential mitigation strategies.

## Limitations

- The experiments only test paraphrasing gold contexts, not generating contexts from scratch
- The paper doesn't explore whether improvements come from information loss in paraphrasing or other factors
- Reader models used are the same LLMs that generated the paraphrased contexts, raising potential concerns about model bias

## Confidence

The paper's central claim about "knowledge corpus error" is **Medium confidence**. While the empirical evidence of improved performance with paraphrased contexts is compelling, the interpretation that this demonstrates knowledge corpus error is indirect. The experiments show that paraphrased contexts can be more effective, but don't directly prove that the original knowledge corpus was missing passages that would have been more helpful.

- **Low confidence** for knowledge corpus error as the primary mechanism - the evidence is circumstantial rather than direct
- **Medium confidence** for information filtering - this is a plausible effect but not explicitly validated
- **Low confidence** for commonsense incorporation - this mechanism is mentioned but not empirically tested

## Next Checks

1. Generate contexts from scratch (not based on gold contexts) and test reader performance to better isolate knowledge corpus error effects
2. Use different reader models than those used for context generation to control for model-specific biases
3. Conduct ablation studies comparing paraphrased contexts with information-reduced versions to quantify the information filtering effect