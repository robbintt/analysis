---
ver: rpa2
title: 'MONGOOSE: Path-wise Smooth Bayesian Optimisation via Meta-learning'
arxiv_id: '2302.11533'
source_url: https://arxiv.org/abs/2302.11533
tags:
- function
- regret
- mongoose
- figure
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of optimizing black-box functions
  where movement costs between evaluations are significant. Standard Bayesian optimization
  methods, which focus on minimizing the number of evaluations, often produce inefficient
  trajectories due to large jumps between successive evaluations.
---

# MONGOOSE: Path-wise Smooth Bayesian Optimisation via Meta-learning

## Quick Facts
- **arXiv ID**: 2302.11533
- **Source URL**: https://arxiv.org/abs/2302.11533
- **Reference count**: 40
- **Key outcome**: MONGOOSE achieves lower regret with lower movement costs by learning smooth trajectories through memory-based meta-learning

## Executive Summary
MONGOOSE addresses the challenge of optimizing black-box functions where movement costs between evaluations are significant. Standard Bayesian optimization methods often produce inefficient trajectories with large jumps between successive evaluations. The authors propose a memory-based meta-learning approach that directly learns a non-myopic policy to generate smooth optimization trajectories. The key innovation is a training objective that encourages smooth paths and a new prior that generates more realistic training functions with global structure. Experiments demonstrate that MONGOOSE outperforms existing methods, especially in higher dimensions, by achieving lower regret with lower movement costs.

## Method Summary
MONGOOSE uses a meta-learnt LSTM policy to generate smooth optimization trajectories by incorporating movement costs into the training objective. The method samples training functions from Gaussian Processes enhanced with random quadratic bowls to provide global structure, then trains the LSTM to optimize these functions while penalizing large movements. The non-myopic training objective allows the policy to plan multiple steps ahead, accepting short-term suboptimality to access distant promising regions. The approach uses Random Fourier Features for efficient GP sampling and includes a curriculum learning strategy that gradually increases the planning horizon from short to long trajectories.

## Key Results
- MONGOOSE outperforms standard BO methods (EI, EIpu, SnAKe) on COCO test suite functions across 2D-6D dimensions
- The method achieves lower regret with lower movement costs, demonstrating the effectiveness of smooth trajectories
- Performance improvements are most pronounced in higher dimensions where traditional methods struggle with exploration-exploitation tradeoffs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MONGOOSE learns a non-myopic policy that inherently favors smooth trajectories due to the memory architecture's inductive bias.
- Mechanism: The LSTM-based meta-optimizer stores historical evaluations and uses them to generate new query points. Because the hidden state retains more information from recent evaluations, the model naturally produces sequential points that are close to each other in the input space.
- Core assumption: The LSTM's hidden state dynamics inherently bias the policy toward local continuity in the trajectory.
- Evidence anchors:
  - [abstract] "Our chosen network architecture enjoys an inductive bias for smooth paths"
  - [section] "Interestingly, even without any moving penalty (i.e. setting α = 0) MONGOOSE still generates relatively smooth trajectories"
- Break condition: If the hidden state becomes too diluted over time, the inductive bias weakens and trajectories may become jumpy.

### Mechanism 2
- Claim: The training objective with movement cost encourages exploration of global optima by penalizing large jumps.
- Mechanism: The training loss includes a term that divides the improvement by (1 + α × cumulative movement cost), creating a tradeoff between immediate improvement and movement efficiency. This forces the policy to accept short-term suboptimality to access distant promising regions.
- Core assumption: The division-based cost penalty effectively modulates exploration vs exploitation without requiring manual tuning for each problem.
- Evidence anchors:
  - [abstract] "Our algorithm, MONGOOSE, uses a meta-learnt parametric policy to generate smooth optimisation trajectories"
  - [section] "This is due to the myopic nature of such an approach: it takes into account only the immediate benefit provided by making an evaluation"
- Break condition: If α is too large, the optimizer may become overly conservative and fail to escape local optima.

### Mechanism 3
- Claim: Training on functions with injected global structure improves generalization to real-world objectives with single central optima.
- Mechanism: By adding a random convex quadratic to GP samples, the training distribution mimics real-world functions that have one dominant minimum. This structural bias helps the policy learn to navigate toward central optima rather than getting stuck at multiple comparable local minima.
- Core assumption: Real-world objective functions often have single central optima, unlike standard GP samples.
- Evidence anchors:
  - [section] "To alleviate the shortcomings described above, we deviate from standard training function priors... we sample a quadratic bowl and add this to the training functions"
  - [section] "we found that including this global structure gives an improvement in optimisation performance downstream"
- Break condition: If the added quadratic structure is too dominant, the model may overfit to bowl-shaped functions and perform poorly on other shapes.

## Foundational Learning

- Concept: Gaussian Processes and kernel functions
  - Why needed here: MONGOOSE's meta-training uses GP samples to create training functions, so understanding how kernels generate function samples is essential for debugging and extending the method.
  - Quick check question: What is the effect of the lengthscale parameter on GP sample smoothness?

- Concept: Recurrent neural networks and LSTM mechanics
  - Why needed here: The core optimizer is an LSTM, so understanding hidden states, gates, and backpropagation through time is necessary for implementation and troubleshooting.
  - Quick check question: How does the LSTM's forget gate influence the memory of past evaluations?

- Concept: Meta-learning and non-myopic optimization
  - Why needed here: MONGOOSE uses meta-training to learn a policy that plans multiple steps ahead, so understanding the difference between myopic and non-myopic objectives is crucial for proper training.
  - Quick check question: What is the key difference between the observed improvement objective with and without gradient detachment?

## Architecture Onboarding

- Component map: LSTM policy network → RFF-based GP sampling → Quadratic bowl injection → Non-myopic training objective with movement cost
- Critical path: Sample training functions → Roll out policy for H steps → Calculate loss with movement cost → Backprop through time → Update LSTM weights
- Design tradeoffs: Memory-based vs. GP-based approaches (speed vs. flexibility), RFF approximation vs. exact sampling (computational efficiency vs. accuracy), additive vs. multiplicative movement cost (parameter sensitivity vs. stability)
- Failure signatures: Jump discontinuities in trajectories (insufficient movement cost penalty), convergence to local optima (α too small), poor performance on high dimensions (inadequate global structure)
- First 3 experiments:
  1. Train MONGOOSE on 2D Branin function with α = 0 to verify smooth trajectories emerge from LSTM inductive bias
  2. Test trained MONGOOSE on a 3D Ackley function with varying α values to observe exploration vs exploitation tradeoff
  3. Compare performance with and without global structure injection on a 4D Hartmann function to verify improvement claim

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the analysis, several areas warrant further investigation: the theoretical justification for the LSTM's inductive bias toward smooth trajectories, the scalability of MONGOOSE to extremely high-dimensional problems, and the sensitivity of the method to different cost function choices beyond L1 and L2 distances.

## Limitations

- The performance gains are most pronounced in higher dimensions, with less dramatic improvements in 2D problems where standard BO methods already perform well.
- The method requires careful tuning of the α hyperparameter to balance exploration and exploitation, which may be problem-dependent.
- The quadratic bowl injection assumes real-world functions have single central optima, which may not hold for all optimization problems with complex multi-modal landscapes.

## Confidence

- The core claim about LSTM inductive bias toward smooth trajectories is supported by empirical evidence but lacks rigorous theoretical justification. Confidence: Medium.
- The training objective's effectiveness depends heavily on the α hyperparameter, requiring tuning for different problem domains. Confidence: Medium.
- The assumption that real-world functions have single central optima may not hold for all problems, potentially limiting the method's applicability. Confidence: Low.

## Next Checks

1. Test MONGOOSE on benchmark functions with known multi-modal structure (e.g., Rastrigin, Griewank) to verify performance degradation when the quadratic structure assumption fails.

2. Conduct ablation studies removing the LSTM component to isolate whether the smoothness truly comes from the memory architecture versus other design choices.

3. Perform sensitivity analysis across a wider range of α values (including very small and very large) to better characterize the exploration-exploitation tradeoff and identify failure modes.