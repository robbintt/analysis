---
ver: rpa2
title: Variations and Relaxations of Normalizing Flows
arxiv_id: '2309.04433'
source_url: https://arxiv.org/abs/2309.04433
tags:
- normalizing
- flows
- distribution
- diffusion
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey reviews variations and relaxations of Normalizing
  Flows (NFs) that aim to improve expressivity, training speed, sample efficiency,
  and likelihood tractability compared to vanilla NFs. It covers four main categories:
  Variational Autoencoders (VAEs), Diffusion models, Score-based methods, and models
  that relax NF constraints such as SurVAE Flows, Stochastic NFs, and Diffusion NFs.'
---

# Variations and Relaxations of Normalizing Flows

## Quick Facts
- arXiv ID: 2309.04433
- Source URL: https://arxiv.org/abs/2309.04433
- Authors: 
- Reference count: 12
- Primary result: Survey of Normalizing Flow variations that trade exact likelihood for improved expressivity and sampling efficiency

## Executive Summary
This survey comprehensively reviews methods that relax the strict bijectivity constraints of Normalizing Flows to improve their expressivity and sampling efficiency. The paper categorizes these approaches into four main groups: VAEs, Diffusion models, Score-based methods, and constraint-relaxing variants including SurVAE Flows, Stochastic NFs, and Diffusion NFs. Key findings show that introducing stochasticity and surjective transformations can bypass topological constraints and improve model performance on complex distributions, though often at the cost of exact likelihood computation. The survey provides a unified framework for understanding how these various approaches relate to and extend the foundational Normalizing Flow framework.

## Method Summary
The paper systematically surveys variations and relaxations of Normalizing Flows that aim to improve upon vanilla NFs' limitations in expressivity, training speed, and sampling efficiency. It categorizes these approaches into four main groups: Variational Autoencoders (VAEs), Diffusion models, Score-based methods, and models that relax NF constraints (SurVAE Flows, Stochastic NFs, Diffusion NFs). The survey provides theoretical foundations for each approach, explains how they modify or extend the basic NF framework, and discusses their respective trade-offs in terms of likelihood tractability versus expressivity. Implementation details are generally described at a conceptual level rather than providing specific code or algorithms.

## Key Results
- Stochasticity improves expressivity and bypasses topological constraints by adding controlled noise to help models represent multiple modes
- SurVAE Flows enable combining bijective and surjective transformations for better expressivity and exact likelihoods in some cases
- Stochastic NFs improve training speed and sample efficiency through controlled stochastic sampling blocks
- Diffusion NFs improve expressivity and can learn sharper boundaries by adding noise only to targeted areas
- Score-based methods can achieve state-of-the-art likelihoods on some tasks while maintaining high expressivity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Surjective transformations in SurVAE Flows bypass the topological constraints of NFs by allowing changes in dimensionality and support topology.
- Mechanism: By introducing surjective layers (absolute value, max-value, stochastic permutation), the model can map from a lower-dimensional or simpler base distribution to a higher-dimensional or more complex target distribution without requiring invertibility.
- Core assumption: The target distribution has inherent symmetries or lower-dimensional support that can be modeled by surjective transformations.
- Evidence anchors:
  - [abstract]: "SurVAE Flows enable combining bijective and surjective transformations for better expressivity and exact likelihoods in some cases"
  - [section 6.1]: "Through the use of surjective, non-injective layers, the authors present constructions that allow for inference surjections...and generative surjections"
  - [corpus]: Weak - no corpus evidence directly supporting this mechanism
- Break condition: If the target distribution lacks exploitable symmetries or the surjective transformation introduces significant density leakage outside the target support.

### Mechanism 2
- Claim: Stochastic layers in Stochastic Normalizing Flows (SNF) improve expressivity by adding controlled noise that helps the model explore and represent multiple modes.
- Mechanism: By incorporating stochastic sampling blocks (Langevin, Metropolis-Hastings, VAE, diffusion layers) into the NF framework, the model can overcome the topological constraints and avoid mode collapse.
- Core assumption: The target distribution has multiple modes or complex topology that cannot be represented by a purely deterministic NF.
- Evidence anchors:
  - [abstract]: "stochasticity improves expressivity and bypasses topological constraints"
  - [section 6.2]: "Stochastic Normalizing Flows (SNF) are a generalization of the Normalizing Flow framework...offering certain benefits...by introducing noise to relax Normalizing Flow's bijectivity constraints"
  - [section 7.1]: "Stochastic normalizing flows overcome some of these limitations by incorporating stochastic sampling blocks into the normalizing flow framework, thus improving representational capacity over deterministic flow architectures by overcoming topological constraints"
- Break condition: If the added stochasticity is too high, leading to loss of control over the generated distribution or if the computational cost outweighs the expressivity gains.

### Mechanism 3
- Claim: Diffusion Normalizing Flows (DiffFlow) improve expressivity by adding noise only to targeted areas of the data, rather than indiscriminately, allowing for sharper boundaries.
- Mechanism: By making the forward stochastic differential equation (SDE) trainable and adding noise only to areas that need it, DiffFlow can learn distributions with sharper boundaries than vanilla NFs or diffusion models.
- Core assumption: The target distribution has sharp boundaries or complex topology that requires targeted noise injection.
- Evidence anchors:
  - [abstract]: "Diffusion NFs improve expressivity and can learn sharper boundaries"
  - [section 6.3]: "DiffFlow adds noise only to targeted areas...can diffuse noise more efficiently and retain topological details that might have been blurred out in other diffusion processes"
  - [section 7.1]: "DiffFlow enjoys better expressivity than vanilla normalizing flows by overcoming their bijectivity constraints by adding noise...DiffFlow can learn distributions with sharper boundaries"
- Break condition: If the trainable noise injection fails to identify the correct areas to add noise or if the computational cost of the trainable forward SDE is too high.

## Foundational Learning

- Concept: Normalizing Flows (NFs)
  - Why needed here: NFs form the foundation for understanding the variations and relaxations discussed in the paper. They provide the baseline for comparing the expressivity, training speed, and likelihood tractability of the relaxed models.
  - Quick check question: What is the key constraint that limits the expressivity of vanilla NFs?

- Concept: Topological Constraints
  - Why needed here: Topological constraints are the primary motivation for the variations and relaxations of NFs. Understanding these constraints is crucial for appreciating the need for models that can bypass them.
  - Quick check question: How do topological constraints manifest in the context of NFs?

- Concept: Stochasticity and Surjectivity
  - Why needed here: Stochasticity and surjectivity are the key mechanisms used to relax the constraints of NFs. Understanding how these concepts are applied in the context of generative modeling is essential for grasping the proposed variations.
  - Quick check question: How do stochastic and surjective transformations differ from the bijective transformations used in vanilla NFs?

## Architecture Onboarding

- Component map: Base distribution -> Transformation layers (bijective/surjective/stochastic) -> Target distribution
- Critical path:
  1. Define the base distribution and target distribution
  2. Choose the appropriate variation of NF based on the target distribution's characteristics
  3. Construct the transformation layers, incorporating stochastic blocks or surjective layers as needed
  4. Train the model using the appropriate objective function
  5. Evaluate the model's expressivity, training speed, and likelihood tractability

- Design tradeoffs:
  - Expressivity vs. likelihood tractability: Relaxing the bijectivity constraint improves expressivity but often at the cost of exact likelihood computation
  - Training speed vs. sample efficiency: Models with stochastic components may train slower but can generate higher-quality samples
  - Computational cost vs. model complexity: More complex models may be more expressive but also more computationally expensive

- Failure signatures:
  - Mode collapse: The model fails to capture all modes of the target distribution
  - Density leakage: The model generates samples outside the support of the target distribution
  - Slow convergence: The model takes too long to train or fails to converge

- First 3 experiments:
  1. Implement a simple NF on a synthetic multimodal distribution and observe mode collapse
  2. Implement a SurVAE Flow with surjective layers on a synthetic distribution with symmetries and compare its expressivity to the vanilla NF
  3. Implement an SNF with stochastic layers on a synthetic distribution with sharp boundaries and compare its sampling efficiency to the vanilla NF

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which stochasticity improves expressivity in normalizing flow models, and is noise truly necessary for this improvement?
- Basis in paper: [explicit] The paper discusses that both Stochastic Normalizing Flows and Diffusion Normalizing Flows introduce stochasticity to improve expressivity, but the exact mechanism is not fully elucidated. It also mentions experimental evidence showing successful models trained using deterministic, non-Gaussian forward processes, calling into question the necessity of noise.
- Why unresolved: The paper does not provide a clear explanation of the mechanism by which stochasticity improves expressivity, and there is conflicting evidence regarding the necessity of noise.
- What evidence would resolve it: Further theoretical analysis and experimental studies comparing the expressivity of stochastic and deterministic models, as well as the impact of different types of noise or stochastic processes on model performance, would help clarify the role of stochasticity in improving expressivity.

### Open Question 2
- Question: How can we effectively balance the trade-off between likelihood tractability and expressivity in generative models, and are there any models that can achieve both high expressivity and exact likelihood computation?
- Basis in paper: [explicit] The paper discusses the trade-off between likelihood tractability and expressivity in various generative models, such as Normalizing Flows, VAEs, and Diffusion models. It mentions that models like Stochastic Normalizing Flows and Diffusion Normalizing Flows introduce stochasticity to improve expressivity but at the cost of exact likelihood computation.
- Why unresolved: The paper does not provide a definitive answer on how to balance this trade-off or identify models that can achieve both high expressivity and exact likelihood computation.
- What evidence would resolve it: Further research into developing new model architectures or training techniques that can maintain exact likelihood computation while improving expressivity, as well as empirical studies comparing the performance of such models, would help address this open question.

### Open Question 3
- Question: What are the most effective ways to improve the sampling efficiency of generative models, particularly for models that require iterative sampling processes like Diffusion models and Score-based methods?
- Basis in paper: [explicit] The paper discusses the sampling efficiency of various generative models, noting that Diffusion models and Score-based methods tend to be slow due to their iterative nature. It mentions that these models can produce high-quality samples but may struggle with sampling efficiency.
- Why unresolved: The paper does not provide specific solutions or techniques to improve the sampling efficiency of these models, and further research is needed to address this issue.
- What evidence would resolve it: Development and evaluation of new sampling techniques, such as parallelized sampling or more efficient optimization algorithms, as well as empirical studies comparing the sampling efficiency of different models and techniques, would help improve our understanding of how to enhance sampling efficiency in generative models.

## Limitations

- Most proposed models lack comprehensive empirical validation across diverse datasets
- Theoretical claims about expressivity improvements are not always supported by extensive benchmarking
- Computational efficiency trade-offs of stochastic and diffusion-based approaches are not fully characterized
- Limited discussion of how these approaches perform on high-dimensional, real-world data beyond synthetic examples

## Confidence

- Expressivity improvements through stochasticity: Medium - theoretically sound but limited empirical validation
- Surjective transformations bypassing topological constraints: Medium - mechanism is clear but practical effectiveness needs verification
- Sharp boundary learning via DiffFlow: Low-Medium - specific claims about boundary sharpness need more empirical support

## Next Checks

1. Implement a comprehensive benchmark comparing SurVAE Flows against both vanilla NFs and diffusion models on high-dimensional image datasets to verify expressivity claims.

2. Conduct ablation studies on the stochastic components in SNFs to determine the minimal amount of stochasticity needed for performance gains versus computational cost.

3. Test the boundary learning capabilities of DiffFlow on datasets with known sharp boundaries (like concentric circles or spirals) to verify the claimed improvements over both vanilla NFs and standard diffusion models.