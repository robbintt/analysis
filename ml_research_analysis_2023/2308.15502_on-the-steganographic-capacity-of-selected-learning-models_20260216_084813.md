---
ver: rpa2
title: On the Steganographic Capacity of Selected Learning Models
arxiv_id: '2308.15502'
source_url: https://arxiv.org/abs/2308.15502
tags:
- capacity
- weights
- accuracy
- bits
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the steganographic capacity of various
  machine learning and deep learning models by determining how many low-order bits
  of trained parameters can be overwritten without degrading model performance. For
  a wide range of models including Logistic Regression, SVM, MLP, CNN, LSTM, VGG16,
  DenseNet121, InceptionV3, Xception, and ACGAN, the authors embed information in
  the low-order bits of model weights and measure accuracy degradation.
---

# On the Steganographic Capacity of Selected Learning Models

## Quick Facts
- arXiv ID: 2308.15502
- Source URL: https://arxiv.org/abs/2308.15502
- Reference count: 40
- Key outcome: Machine learning models can tolerate overwriting 20+ low-order bits per weight without significant accuracy loss, enabling high-capacity steganography

## Executive Summary
This paper systematically investigates the steganographic capacity of various machine learning and deep learning models by determining how many low-order bits of trained parameters can be overwritten without degrading model performance. Through experiments on 10 different model architectures including Logistic Regression, SVM, MLP, CNN, LSTM, and several pre-trained models, the authors demonstrate that most models can tolerate overwriting 20+ bits per weight. The results reveal surprisingly high steganographic capacities ranging from 7.04 KB for simple models to 44.74 MB for complex pre-trained architectures like InceptionV3. This work raises important security concerns about the potential for embedding malicious information within trained ML models.

## Method Summary
The researchers train each model on a malware classification dataset, establish baseline accuracy, then systematically overwrite low-order bits of weights (1-32 bits per weight) and measure resulting accuracy degradation. The steganographic capacity is defined as the maximum number of bits that can be overwritten before accuracy drops by more than 1% compared to baseline. Experiments include testing all weights versus individual layers, and comparing pre-trained versus trained weights in transfer learning models. The methodology is applied across diverse model architectures to establish comparative capacity measurements.

## Key Results
- Most models tolerate overwriting 20+ low-order bits per weight without significant accuracy loss
- Steganographic capacity ranges from 7.04 KB (Logistic Regression) to 44.74 MB (InceptionV3)
- Pre-trained models show higher capacity than models trained from scratch
- SVM models demonstrate the highest per-weight capacity at 25 bits
- Individual layers within models show varying sensitivity to bit overwrites

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-order bits in trained weights can be overwritten without significantly degrading model accuracy.
- Mechanism: Deep learning models have high redundancy in their weights; many weights contribute little to final performance, allowing information hiding in low-significance bits without perceptible loss of accuracy.
- Core assumption: The low-order bits of floating-point weights are sufficiently imprecise that overwriting them does not materially affect model predictions.
- Evidence anchors:
  - [abstract] "For each model considered, we graph the accuracy as a function of the number of low-order bits that have been overwritten, and for selected models, we also analyze the steganographic capacity of individual layers."
  - [section] "Machine learning models do not typically require high precision in their trained parameters."
- Break condition: If the model is highly sensitive to small weight perturbations (e.g., certain transfer learning models), overwriting even a few low-order bits could cause noticeable accuracy loss.

### Mechanism 2
- Claim: Pre-trained transfer learning models have different steganographic capacities in trained vs. pre-trained weights.
- Mechanism: Pre-trained weights are optimized for general tasks and are more sensitive to bit changes, while trained weights for a specific task are less sensitive due to lower precision requirements.
- Core assumption: Pre-trained weights are more precisely tuned for the source task, making them more vulnerable to corruption than task-specific weights.
- Evidence anchors:
  - [abstract] "In Section 4.8, we found that the per-weight capacity for the pre-trained layers of the InceptionV3 model was just 14 bits, as compared to 25 bits for its trained weights."
- Break condition: If the model is trained from scratch with high-precision requirements, the distinction between trained and pre-trained weights may not apply.

### Mechanism 3
- Claim: Steganographic capacity is inversely related to the precision required by the training algorithm.
- Mechanism: Training algorithms like SMO (for SVMs) are designed to work with low-precision weights, so these models inherently tolerate more bit overwriting.
- Core assumption: The design of the training algorithm determines the minimum precision required for acceptable model performance.
- Evidence anchors:
  - [abstract] "The SMO algorithm specifically takes advantage of the fact that the weights of a trained SVM do not require great accuracy."
  - [section] "SVM models have the highest capacity per weight, which implies that this particular model requires the least precision in its weights."
- Break condition: If a model is trained with high-precision optimization (e.g., some neural network training regimes), the capacity may decrease significantly.

## Foundational Learning

- Concept: Bit precision in floating-point numbers
  - Why needed here: Understanding how low-order bits contribute minimally to numerical precision helps explain why they can be overwritten without degrading model performance.
  - Quick check question: If a 32-bit float has 23 bits of mantissa, what fraction of its precision is contained in the lowest 8 bits?

- Concept: Overfitting and model generalization
  - Why needed here: Models with high redundancy or overfitting are more tolerant of weight perturbations, which directly affects steganographic capacity.
  - Quick check question: How would you determine if a model is overfitting, and why would that impact its ability to tolerate bit overwrites?

- Concept: Transfer learning vs. from-scratch training
  - Why needed here: The difference in steganographic capacity between pre-trained and trained weights depends on understanding how transfer learning works and why pre-trained weights are more sensitive.
  - Quick check question: Why might a pre-trained weight be more sensitive to bit changes than a weight trained from scratch on a specific task?

## Architecture Onboarding

- Component map:
  Dataset preprocessing -> Model training -> Bit-overwrite experiments -> Accuracy evaluation -> Capacity calculation

- Critical path:
  1. Load and preprocess dataset
  2. Train baseline model
  3. Overwrite n low-order bits of weights
  4. Evaluate accuracy drop
  5. Determine capacity (n-1 bits per weight)

- Design tradeoffs:
  - Higher n increases capacity but risks accuracy loss
  - Per-layer vs. all-weights overwrite balances granularity and simplicity
  - Transfer learning models require separate treatment of pre-trained and trained weights

- Failure signatures:
  - Sudden accuracy drop indicates too many bits overwritten
  - Inconsistent accuracy across layers suggests uneven weight sensitivity
  - No capacity (n=0) suggests model requires high precision

- First 3 experiments:
  1. Overwrite 1-8 bits in a simple model (e.g., Logistic Regression) to establish baseline tolerance
  2. Overwrite all weights vs. output layer only in a CNN to compare capacity
  3. Test pre-trained vs. trained weights in InceptionV3 to quantify sensitivity difference

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided text. However, based on the limitations section, some implicit questions that emerge include:
- How does steganographic capacity vary across different types of datasets beyond malware classification?
- What is the relationship between model architecture complexity and steganographic capacity?
- How does the use of different precision levels (e.g., 16-bit vs 32-bit weights) affect steganographic capacity?

## Limitations
- The findings are based primarily on a malware classification dataset, which may not generalize to other domains or tasks
- The security implications of steganographic capacity in real-world deployment scenarios are not fully explored
- The analysis focuses on bit-overwrite capacity without considering other steganographic techniques or detection methods

## Confidence
- **High Confidence**: The experimental methodology for measuring accuracy degradation as a function of overwritten bits is well-defined and reproducible across most model types.
- **Medium Confidence**: The comparative analysis between pre-trained and trained weights in transfer learning models shows consistent patterns, but the underlying mechanism explaining sensitivity differences could benefit from deeper investigation.
- **Low Confidence**: The generalizability of findings to non-malware datasets and the security implications of steganographic capacity in production systems require additional validation.

## Next Checks
1. Test the same bit-overwrite methodology on non-malware datasets (e.g., CIFAR-10, ImageNet) to assess whether capacity findings generalize beyond the specific domain studied.
2. Conduct sensitivity analysis on pre-trained weights alone (without fine-tuning) to isolate whether observed capacity differences stem from optimization level or architecture-specific properties.
3. Implement bit-overwrite attacks on deployed models in simulated production environments to measure real-world security implications and detection feasibility.