---
ver: rpa2
title: 'EarthPT: a time series foundation model for Earth Observation'
arxiv_id: '2309.07207'
source_url: https://arxiv.org/abs/2309.07207
tags:
- earthpt
- time
- data
- arxiv
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EarthPT is a 700M parameter decoding transformer foundation model
  trained on 14B tokens of Earth Observation (EO) data in an autoregressive self-supervised
  manner. The model accurately forecasts future pixel-level surface reflectances across
  the 400-2300 nm range, achieving a median L1 error of 0.05 for NDVI predictions
  five months into the future, outperforming simple phase-folded models.
---

# EarthPT: a time series foundation model for Earth Observation

## Quick Facts
- arXiv ID: 2309.07207
- Source URL: https://arxiv.org/abs/2309.07207
- Authors: 
- Reference count: 38
- Key outcome: EarthPT achieves median L1 error of 0.05 for NDVI predictions five months ahead using 700M parameters trained on 14B tokens

## Executive Summary
EarthPT is a foundation model for Earth Observation that forecasts multispectral reflectance time series across the 400-2300 nm range. The 700 million parameter decoding transformer is trained autoregressively on 14 billion tokens of EO data, enabling it to predict future observations and generate semantically meaningful embeddings. The model outperforms simple phase-folded baselines for medium-term forecasting and demonstrates potential for downstream land cover classification tasks.

## Method Summary
EarthPT uses a decoding transformer architecture trained in an autoregressive self-supervised manner on multispectral time series data converted from Sentinel-1 SAR to Sentinel-2 equivalent reflectances. The model processes input sequences of 256 time steps with 20 spectral channels, embedding both spectral values and date information before passing through 36 transformer layers. Training uses Huber loss with Adam optimizer and cosine learning rate decay over 90k steps on 8 A100 GPUs, producing forecasts up to five months ahead with median L1 error of 0.05 for NDVI predictions.

## Key Results
- Achieves median L1 error of 0.05 for NDVI predictions five months into the future
- Outperforms simple phase-folded models for medium-term forecasting
- Generates semantically meaningful embeddings that capture land cover information for downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The transformer's autoregressive training on long multispectral time series allows it to learn temporally coherent patterns that generalize to forecasting unseen future observations.
- Mechanism: The model treats each temporal observation as a token and predicts the next one using self-attention over the sequence, capturing both short-term dependencies and long-term seasonal cycles in the reflectance data.
- Core assumption: The spectral reflectance time series are sufficiently stationary and predictable across seasons to enable autoregressive forecasting.
- Evidence anchors: [abstract] "EarthPT is a 700 million parameter decoding transformer foundation model trained in an autoregressive self-supervised manner"; [section] "We train EarthPT in the usual autoregressive way, by repeatedly predicting the next observation in a given set of time series."

### Mechanism 2
- Claim: The large volume of EO data (≈10^15 potential tokens) allows the model to scale without hitting data bottlenecks, enabling continued performance gains.
- Mechanism: By following Chinchilla scaling laws (N ∝ D), EarthPT can grow parameters proportionally to dataset size, leveraging quadrillions of tokens without overfitting.
- Core assumption: EO spectral reflectance time series can be treated as discrete tokens in the same way natural language tokens are, and scaling laws apply across modalities.
- Evidence anchors: [abstract] "the abundance of EO data provides us with – in theory – quadrillions of training tokens"; [section] "the abundance of EO data provides us with – in theory – quadrillions of training tokens."

### Mechanism 3
- Claim: The learned embeddings encode semantically meaningful spatial-temporal features that can be reused for downstream land cover classification.
- Mechanism: The penultimate layer outputs a high-dimensional representation that, when averaged across time, preserves discriminative patterns (e.g., NDVI, BSI, RGB) that cluster by land cover type.
- Core assumption: The self-supervised training objective induces representations that align with semantically relevant EO features.
- Evidence anchors: [abstract] "embeddings learnt by EarthPT hold semantically meaningful information and could be exploited for downstream tasks such as highly granular, dynamic land use classification"; [section] "we can visualise them by projecting onto a two-dimensional manifold... By colour-coding the projected embedding space we see that it has a physically meaningful organisation."

## Foundational Learning

- Concept: Autoregressive sequence modeling
  - Why needed here: EarthPT predicts future multispectral observations, requiring it to model temporal dependencies step-by-step.
  - Quick check question: Can you explain how the model generates a forecast 5 months ahead starting from a fixed context window?

- Concept: Self-supervised pretraining
  - Why needed here: Labeled EO datasets are scarce and expensive; self-supervision leverages abundant unlabeled time series.
  - Quick check question: What is the training objective used to pretrain EarthPT without labels?

- Concept: Multimodal token embedding
  - Why needed here: Spectral bands and temporal features must be mapped into a common vector space for self-attention.
  - Quick check question: How does EarthPT embed both spectral reflectances and date embeddings before feeding them to the transformer?

## Architecture Onboarding

- Component map: Input arrays [index, time, channel] + date embeddings -> MLP + positional encoding -> 36 decoder transformer layers -> Final projection to predict next observation -> Downstream embedding extraction from penultimate layer

- Critical path: 1. Load and memory-map preprocessed arrays; 2. Embed inputs (MLP + date); 3. Run through decoder layers; 4. Apply final projection to predict next observation; 5. Compute Huber loss and backprop

- Design tradeoffs: Fixed block size (256) vs variable-length sequences; Autoregressive decoding vs bidirectional for embeddings; Large embedding dimension (1280) vs parameter count

- Failure signatures: Training loss plateaus early -> insufficient model capacity or learning rate too low; Forecast error grows linearly with horizon -> model failing to capture long-term patterns; Embeddings not visually coherent -> embedding space not semantically organized

- First 3 experiments: 1. Verify autoregressive training by generating a short forecast and comparing to ground truth for a single pixel; 2. Test embedding coherence by visualizing PCA projections colored by NDVI and checking for spatial structure; 3. Scale model size from 10M to 300M parameters and plot training loss to confirm Chinchilla-like scaling behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal dataset size and model size for EarthPT to maximize performance while minimizing computational cost?
- Basis in paper: [explicit] The paper mentions that the Chinchilla scaling law suggests a compute-optimal decoding transformer model should be trained roughly following the scaling N ∼ 20D, where N is the number of parameters in the model, and D is the number of tokens in the training set. However, it also notes that this scaling law may not be suitable for EO datasets.
- Why unresolved: The paper acknowledges that the Chinchilla scaling law may not be directly applicable to EO datasets, indicating a need for a specific scaling law for EarthPT.
- What evidence would resolve it: Derivation of a specific scaling law for EarthPT based on extensive experiments with different model sizes and dataset sizes, and validation of this law's effectiveness in maximizing performance while minimizing computational cost.

### Open Question 2
- Question: How can EarthPT embeddings be effectively utilized for downstream tasks such as land cover classification and what are the limitations of this approach?
- Basis in paper: [explicit] The paper demonstrates that EarthPT can produce semantically meaningful embeddings for input time series, which could be exploited for land cover classification and other downstream tasks.
- Why unresolved: While the paper shows the potential of EarthPT embeddings, it does not explore the full range of downstream tasks or the limitations of using these embeddings in practical applications.
- What evidence would resolve it: Extensive testing of EarthPT embeddings across various downstream tasks, including land cover classification, and a thorough analysis of the limitations and challenges in applying these embeddings in real-world scenarios.

### Open Question 3
- Question: What are the potential applications of EarthPT in other domains beyond Earth Observation, and how can it be adapted for these applications?
- Basis in paper: [inferred] The paper mentions that EarthPT could be trained to produce a single embedding space for all EO (and other) multi-modal data types, suggesting potential applications in other domains.
- Why unresolved: The paper does not explore the specific applications of EarthPT in other domains or provide a roadmap for adapting the model for these applications.
- What evidence would resolve it: Identification of specific domains where EarthPT could be applied, development of strategies for adapting the model to these domains, and demonstration of the model's effectiveness in these new applications.

## Limitations

- Geographic generalization: The model is trained on UK data and may not generalize to other regions with different land cover patterns and climate regimes
- Temporal stationarity assumption: The autoregressive mechanism assumes stationarity across seasons, which may be violated by climate change and land use transitions
- Downstream validation: Claims about embedding utility for land cover classification lack rigorous benchmarking against established datasets

## Confidence

**High confidence** in technical implementation: Transformer architecture, training procedure, and evaluation metrics are clearly specified with sufficient detail for reproduction.

**Medium confidence** in scaling claims: Quadrillions of tokens assertion follows logically from EO data volumes, but practical accessibility and quality varies significantly across regions and time periods.

**Low confidence** in downstream utility claims: Visualization evidence for embedding coherence exists, but lacks quantitative validation on established downstream tasks.

## Next Checks

1. Geographic transfer experiment: Fine-tune EarthPT on a different geographic region (e.g., North American agricultural areas) and evaluate forecast accuracy and embedding coherence. Compare performance degradation against training on the same region to quantify geographic generalization limits.

2. Temporal robustness test: Evaluate forecast accuracy across different seasonal patterns and climate regimes by testing on years with known anomalies (droughts, floods, unusual temperatures). Plot error growth against forecast horizon for normal vs. anomalous years to identify temporal generalization boundaries.

3. Downstream benchmark validation: Extract EarthPT embeddings and train simple classifiers for established land cover datasets (e.g., EuroSAT, BigEarthNet). Compare classification accuracy against baselines using raw spectral features and established EO-specific architectures to quantify practical downstream utility.