---
ver: rpa2
title: Self-Expanding Neural Networks
arxiv_id: '2307.04526'
source_url: https://arxiv.org/abs/2307.04526
tags:
- which
- natural
- neurons
- expansion
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to dynamically adjust neural network
  size during training without disrupting previous optimization. The key idea is to
  monitor a natural expansion score, which measures how much a potential network modification
  could accelerate loss reduction according to natural gradient descent.
---

# Self-Expanding Neural Networks

## Quick Facts
- arXiv ID: 2307.04526
- Source URL: https://arxiv.org/abs/2307.04526
- Reference count: 40
- Primary result: Neural networks can dynamically adjust size during training using natural gradient-based expansion scores, achieving good anytime performance without sacrificing accuracy.

## Executive Summary
This paper presents Self-Expanding Neural Networks (SENN), a method for dynamically adjusting neural network size during training without disrupting previous optimization. The approach monitors a natural expansion score based on natural gradient descent to determine when and where to add neurons or layers. SENN adds capacity when it would substantially reduce converged training loss, initializing new parameters to preserve the existing function. Theoretical analysis bounds the number of additions, and experiments demonstrate networks self-regulate their size based on task complexity and available data.

## Method Summary
SENN uses natural gradient descent to optimize parameters while monitoring a natural expansion score (η = gᵀF⁻¹g) that measures how much new capacity would accelerate loss reduction. When η exceeds a threshold, the algorithm adds neurons or layers where they maximize the score increase, initializing them to preserve the existing function. The method employs Kronecker-factored Fisher approximations for computational efficiency and uses Metropolis-adjusted Langevin algorithm (MALA) to find good initializations for new neurons. SENN dynamically adjusts both width and depth, with the network size adapting to task complexity and dataset size.

## Key Results
- Networks self-regulate their size based on task complexity, growing wider for simple tasks and deeper for complex tasks
- SENN achieves good anytime performance, maintaining accuracy while reducing training time through adaptive capacity
- On MNIST classification, SENN uses 2-5× fewer neurons than fixed-width networks while achieving comparable accuracy
- The number of additions is bounded theoretically, preventing unbounded network growth

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Natural expansion score η measures the reduction in loss under natural gradient descent that would result from adding capacity.
- **Mechanism:** η = gᵀF⁻¹g is the inner product of the gradient with the natural gradient, projecting loss reduction direction onto achievable changes in function space.
- **Core assumption:** The Fisher information matrix F accurately captures local geometry of the loss surface in function space.
- **Evidence anchors:**
  - [abstract] "We introduce a natural gradient based approach which intuitively expands both the width and depth of a neural network when this is likely to substantially reduce the hypothetical converged training loss."
  - [section 3.3] "We are now able to rewrite the squared norm ||PΘgy||2 in the familiar form of definition 1: ||PΘgy||2 = t*T J T J t* = gT F⁻¹J T J F⁻¹g = N gT F⁻¹g = N η"
- **Break condition:** If Fisher matrix F becomes ill-conditioned or fails to represent true curvature, η loses meaning as rate of loss reduction.

### Mechanism 2
- **Claim:** Adding neurons at zero-initialized weights preserves the existing function, avoiding interference with previous optimization.
- **Mechanism:** New neurons are concatenated with existing layers with output weights initialized to zero, creating identity mappings that don't change network function until trained.
- **Core assumption:** The identity function can be represented by the chosen activation function when properly parameterized.
- **Evidence anchors:**
  - [section 3.1] "We add proposed neurons p to layer i by concatenation along the ith hidden dimension (0 ⊎ Wi+1) ◦(σp ⊎ σi) ◦(Wp ⊎ Wi) = Wi+1 ◦ σi ◦ Wi, and initialize the output weights of p to zero."
- **Break condition:** If activation function cannot represent identity (e.g., ReLU), mechanism fails to preserve original function.

### Mechanism 3
- **Claim:** Lower bound ∆η′ on natural expansion score increase can be computed efficiently using only cached activations and gradients.
- **Mechanism:** ∆η′ = Tr[A⁻¹pE[apgT]Gl⁻¹E[grat]p] uses correlation between proposed neuron activations and residual gradients, avoiding full Fisher matrix inversion for each proposal.
- **Core assumption:** Kronecker-factored approximation of Fisher matrix is sufficiently accurate for layer-local decisions.
- **Evidence anchors:**
  - [section 3.6] "The bound ∆η′ can be computed for an arbitrary proposal p of additional neurons using only those intermediate activations and gradients which it would be necessary to cache in order to calculate the gradient g and (Kronecker factored approximate) natural gradient F̃⁻¹g via backpropagation."
- **Break condition:** If Kronecker approximation poorly represents true Fisher matrix, ∆η′ may not be reliable indicator of true η improvement.

## Foundational Learning

- **Concept: Natural Gradient Descent**
  - Why needed here: SENN uses natural gradient F⁻¹g instead of vanilla gradient g to measure how much new capacity would help optimization in function space.
  - Quick check question: Why does natural gradient account for redundancy better than vanilla gradient when deciding where to add neurons?

- **Concept: Fisher Information Matrix**
  - Why needed here: The Fisher matrix F is used to compute the natural gradient and expansion score, capturing local geometry of the loss surface.
  - Quick check question: How does the Fisher matrix relate to the Jacobian of the network output with respect to parameters?

- **Concept: Kronecker-Factored Approximation**
  - Why needed here: SENN uses this approximation to efficiently compute layer-local natural gradients without inverting full Fisher matrix.
  - Quick check question: Why is Kronecker factorization computationally advantageous when computing natural gradients for individual layers?

## Architecture Onboarding

- **Component map:** Core network -> Expansion module -> Proposal optimizer -> Fisher approximator
- **Critical path:** Forward pass to compute activations and loss → Backward pass to compute gradients → Compute Fisher approximations and natural gradients → Evaluate expansion score η and proposals → If threshold met, add new neurons/layers → Continue training with expanded network
- **Design tradeoffs:**
  - Accuracy vs speed: Exact Fisher matrices give better expansion decisions but are slower than Kronecker approximations
  - Exploration vs exploitation: More proposal samples improve initialization quality but increase computational cost
  - Granularity vs overhead: Adding single neurons gives finer control but requires more expansion decisions than adding entire layers
- **Failure signatures:**
  - Network grows too slowly: Threshold τ is too high or α is too large
  - Network grows too quickly: Threshold τ is too low or α is too small
  - Poor performance: Fisher approximation quality is insufficient or expansion decisions are not being made effectively
  - Training instability: Identity initialization of new layers is not working correctly
- **First 3 experiments:**
  1. Single-layer regression on a simple function (like figure 2) to visualize expansion decisions
  2. Binary classification on a simple 2D dataset (like figure 3) to observe depth expansion
  3. MNIST classification with varying dataset sizes to test adaptive sizing (like figure 4)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the natural expansion score's performance compare to other neural network growth methods like ActiveNAS, DyNC, and DENNs when applied to more complex architectures like transformers or graph neural networks?
- Basis in paper: [inferred] The paper acknowledges that experiments focus on multilayer perceptrons and suggests potential extensions to other architectures like CNNs or normalizing flows, but does not provide experimental comparisons.
- Why unresolved: Paper only provides experimental results for MLP architectures, leaving gap in understanding how SENN performs on more complex and widely used architectures.
- What evidence would resolve it: Direct experimental comparisons of SENN against existing methods on transformer and GNN architectures would provide concrete evidence of relative performance and scalability.

### Open Question 2
- Question: What is the theoretical relationship between the natural expansion score and the Hessian matrix, and how does this affect the bound on number of additions in non-constant curvature scenarios?
- Basis in paper: [explicit] Paper discusses relationship between Fisher matrix and Hessian matrix, and how deviations from baseline case where H = F affect bound on number of additions.
- Why unresolved: Paper provides theoretical bounds but does not offer empirical evidence or deeper exploration of how these bounds behave in practical, non-ideal scenarios.
- What evidence would resolve it: Empirical studies on behavior of natural expansion score under varying curvature conditions, along with more rigorous theoretical analysis of bounds, would clarify this relationship.

### Open Question 3
- Question: How sensitive is SENN's performance to choice of hyperparameters such as expansion threshold τ and stopping criterion α, and what are optimal strategies for tuning these parameters?
- Basis in paper: [explicit] Paper mentions specific values for τ and α in different experiments but does not provide comprehensive analysis of how these hyperparameters affect performance or guidelines for tuning them.
- Why unresolved: Paper does not explore sensitivity of SENN's performance to hyperparameter choices, which is crucial for practical applications.
- What evidence would resolve it: Systematic hyperparameter sweep across different datasets and architectures would reveal sensitivity and provide insights into optimal tuning strategies.

## Limitations

- Computational overhead of maintaining Fisher approximations during training
- Sensitivity to hyperparameters (τ, α, damping) that aren't thoroughly explored
- Limited evaluation on large-scale, complex tasks beyond MNIST
- No comparison with concurrent neural architecture search methods

## Confidence

The theoretical foundations of SENN have **High confidence** due to rigorous derivations of natural expansion score and its computational bounds. The empirical evaluation has **Medium confidence** - while experimental results demonstrate the approach works on standard benchmarks, the paper relies heavily on synthetic datasets and small-scale real-world experiments. The practical implementation has **Medium-Low confidence** due to several unspecified details including incomplete rational activation function parameterization and varying stopping criteria without explanation.

## Next Checks

1. Reproduce the 1D regression experiment to verify expansion decision process and ∆η′ computation matches theoretical expectations
2. Test SENN on a more challenging image classification task (e.g., CIFAR-10) with multiple runs to assess variance in final architectures
3. Compare against a standard fixed-architecture baseline with optimal width/depth tuned separately to quantify the anytime performance benefit