---
ver: rpa2
title: 'MELA: Multilingual Evaluation of Linguistic Acceptability'
arxiv_id: '2311.09033'
source_url: https://arxiv.org/abs/2311.09033
tags:
- languages
- language
- training
- acceptability
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MELA, the first large-scale multilingual
  benchmark for linguistic acceptability with 48K samples across 10 languages from
  diverse language families. The authors establish baselines using LLMs and XLM-R,
  and investigate cross-lingual transfer and multi-task learning.
---

# MELA: Multilingual Evaluation of Linguistic Acceptability

## Quick Facts
- **arXiv ID:** 2311.09033
- **Source URL:** https://arxiv.org/abs/2311.09033
- **Reference count:** 34
- **Primary result:** First large-scale multilingual benchmark for linguistic acceptability with 48K samples across 10 languages

## Executive Summary
This paper introduces MELA, a groundbreaking multilingual benchmark for evaluating linguistic acceptability across 10 languages from diverse language families. The authors establish baselines using large language models and XLM-R, investigating cross-lingual transfer and multi-task learning dynamics. Their findings reveal that in-language training data is crucial for acceptability judgments, particularly for low-resource languages, while also identifying potential challenges in cross-lingual transfer indicated by conflicting LayerNorm weights.

## Method Summary
The study employs XLM-RoBERTa as the base model, fine-tuned with specific hyperparameters (learning rate 7.5e-6, weight decay 0.075, batch size 32, 15k steps with 750 warmup steps and cosine decay) on the MELA benchmark. The evaluation uses Matthews Correlation Coefficient (MCC) as the primary metric. The research includes cross-lingual transfer experiments, probing studies to analyze model layers, and LayerNorm weight analysis to understand transfer difficulties between languages.

## Key Results
- XLM-R performance on acceptability judgments improves by 15+ points when adding 500 in-language samples for low-resource languages
- Upper layers of XLM-R become task-specific but language-agnostic for multilingual acceptability judgment
- Conflicting weights in LayerNorm matrices correlate with difficulty of cross-lingual transfer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-language training data is crucial for acceptability judgments in low-resource languages
- Mechanism: When training on 500 in-language samples for languages like Arabic or Icelandic, performance improves significantly compared to using only high-resource language data, suggesting syntactic structures are language-specific and require direct exposure
- Core assumption: Acceptability judgments depend heavily on language-specific syntactic rules rather than universal patterns
- Evidence anchors:
  - [section] "Arabic, which comes from a different language family from all four base languages, always achieves an average of 15-points improvement whenever in-language data is added to the training set"
  - [section] "For the six low-resource languages, adding 500 in-domain training samples significantly improves performance"
  - [corpus] Weak - the corpus doesn't directly show mechanism but supports the claim through performance data
- Break condition: If acceptability judgments were more universal across languages, we would see less improvement from in-language data

### Mechanism 2
- Claim: Upper layers of XLM-R become task-specific but language-agnostic for multilingual acceptability judgment
- Mechanism: Probing experiments show that while language classification accuracy drops in upper layers after finetuning on MELA, acceptability judgment performance increases sharply, indicating these layers have specialized for the task regardless of language
- Core assumption: The model can separate task-specific processing from language-specific processing
- Evidence anchors:
  - [section] "Results of our probing experiments indicate that the upper layers of XLM-R become a task-specific but language-agnostic region for multilingual acceptability judgment"
  - [section] "When using the output of the first token to represent the sentence, the accuracy of language classification drops dramatically in the lower layers of XLM-R"
  - [corpus] Weak - probing results are the primary evidence
- Break condition: If language-specific information were still needed in upper layers for acceptability judgments

### Mechanism 3
- Claim: Conflicting weights in LayerNorm matrices may indicate difficulty of cross-lingual transfer
- Mechanism: When comparing LayerNorm weights from models finetuned on different languages, conflicting weights (weights that increase in one language but decrease in another) correlate with lower cross-lingual transfer performance
- Core assumption: LayerNorm serves a language-specific function that can conflict across languages
- Evidence anchors:
  - [section] "conflicting weight refers to one weight, value of which increases during finetuning on one language but decreases on the other"
  - [section] "We use ∆MCC to measure the performance of bidirectional language-transfer between two languages"
  - [corpus] Weak - correlation data is from the paper's analysis
- Break condition: If conflicting weights were not correlated with transfer difficulty or if they appeared equally across all language pairs

## Foundational Learning

- Concept: Linguistic acceptability judgments
  - Why needed here: The entire benchmark and experiments are based on determining whether sentences are grammatically acceptable to native speakers
  - Quick check question: Can you explain the difference between semantic acceptability and syntactic acceptability in this context?

- Concept: Cross-lingual transfer learning
  - Why needed here: The experiments specifically test how well models trained on one language can perform on others without direct training data
  - Quick check question: What factors might make cross-lingual transfer easier for semantic tasks than for syntactic tasks like acceptability?

- Concept: Multi-task fine-tuning and curse of multilinguality
  - Why needed here: The paper explores how adding more languages to training affects performance, finding that too many languages can hurt performance due to conflicting syntax
  - Quick check question: Why might adding training data from one language hurt performance on another language in this task?

## Architecture Onboarding

- Component map: XLM-R (multilingual RoBERTa) -> Probing modules (language classification, acceptability judgment) -> LayerNorm weight analysis
- Critical path: Data collection → model training on MELA → evaluation on all languages → probing experiments → weight analysis → conclusion about transfer difficulty
- Design tradeoffs: Using existing datasets vs collecting new data, choosing between in-language vs English prompts for evaluation, balancing training data across languages
- Failure signatures: Poor performance on low-resource languages despite in-language training, conflicting weight patterns that don't correlate with transfer difficulty, or probing results that don't show task-specific upper layers
- First 3 experiments:
  1. Train XLM-R on one high-resource language and evaluate on all 10 languages to establish baseline transfer patterns
  2. Add 500 samples from one low-resource language to the training set and measure improvement
  3. Compare conflicting weight patterns between language pairs with different transfer difficulty scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of XLM-R on linguistic acceptability tasks compare to human performance across different languages?
- Basis in paper: [explicit] The paper mentions that XLM-R's performance is compared to human performance in the introduction, but does not provide specific numbers or comparisons.
- Why unresolved: The paper focuses on establishing baselines and comparing the performance of different models, but does not provide a direct comparison to human performance on the MELA benchmark.
- What evidence would resolve it: Conducting a human evaluation study on the MELA benchmark and comparing the results to XLM-R's performance would provide a direct comparison to human performance.

### Open Question 2
- Question: What are the specific syntactic phenomena that are most challenging for language models to handle in acceptability judgments?
- Basis in paper: [inferred] The paper mentions that acceptability judgments involve identifying whether a sentence is grammatically correct and has a naturalistic text, but does not delve into the specific syntactic phenomena that are most challenging.
- Why unresolved: The paper focuses on the overall performance of language models on the task, but does not analyze the specific types of errors or difficulties encountered by the models.
- What evidence would resolve it: Analyzing the errors made by language models on the MELA benchmark and identifying the specific syntactic phenomena that lead to incorrect judgments would provide insights into the challenges faced by the models.

### Open Question 3
- Question: How does the performance of language models on linguistic acceptability tasks vary across different language families?
- Basis in paper: [explicit] The paper introduces MELA, a benchmark covering 10 languages from diverse language families, and reports the performance of different models on these languages.
- Why unresolved: While the paper provides performance results for individual languages, it does not explicitly analyze the variation in performance across different language families.
- What evidence would resolve it: Conducting a detailed analysis of the performance of language models on different language families in the MELA benchmark and identifying patterns or trends would provide insights into the challenges faced by models across different language families.

## Limitations

- Limited corpus evidence for LayerNorm weight conflict hypothesis, showing correlation without establishing causation
- Probing experiments rely on specific architectural choices that may not generalize to other methodologies
- Analysis focuses primarily on syntactic acceptability, leaving unclear how findings extend to semantic acceptability tasks

## Confidence

**High Confidence:** The core finding that in-language training data significantly improves acceptability judgments for low-resource languages is well-supported by consistent experimental results across multiple language pairs.

**Medium Confidence:** The claim about upper layers becoming task-specific but language-agnostic is supported by probing experiments, though alternative explanations could account for some findings.

**Low Confidence:** The LayerNorm weight conflict hypothesis, while intriguing, has limited corpus evidence and requires more rigorous testing to establish whether conflicting weights are truly indicative of transfer difficulty or merely correlated with it.

## Next Checks

1. **Cross-validation of LayerNorm hypothesis:** Conduct ablation studies where LayerNorm weights are frozen vs. fine-tuned to determine if conflicting weights are necessary for cross-lingual transfer or merely a byproduct of the training process.

2. **Alternative probing methodologies:** Repeat the probing experiments using different token representations (CLS token vs. mean pooling) and probing classifiers to verify that the observed patterns are robust across different methodological choices.

3. **Semantic vs. syntactic task comparison:** Design a parallel study using a multilingual semantic acceptability benchmark to determine whether the observed patterns of in-language data importance and LayerNorm conflicts are specific to syntactic tasks or generalize across different types of linguistic judgments.