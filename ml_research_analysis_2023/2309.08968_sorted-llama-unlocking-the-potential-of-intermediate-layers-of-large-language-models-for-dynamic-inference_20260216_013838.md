---
ver: rpa2
title: 'Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language
  Models for Dynamic Inference'
arxiv_id: '2309.08968'
source_url: https://arxiv.org/abs/2309.08968
tags:
- sorted
- llama
- layer
- crime
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the SortedNet training method to generative
  language modeling by sharing a single LLM head across depth-wise sub-networks. The
  authors apply Sorted Fine-Tuning (SoFT) to LLaMA 2 13B on Stanford Alpaca and evaluate
  on PandaLM.
---

# Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference

## Quick Facts
- **arXiv ID:** 2309.08968
- **Source URL:** https://arxiv.org/abs/2309.08968
- **Reference count:** 40
- **Key outcome:** Sub-models (12-40 layers) outperform standard fine-tuning counterparts and early-exit baselines, achieving roughly twice the speed of the full model while maintaining or exceeding performance.

## Executive Summary
This paper extends the SortedNet training method to generative language modeling by sharing a single LLM head across depth-wise sub-networks. The authors apply Sorted Fine-Tuning (SoFT) to LLaMA 2 13B on Stanford Alpaca and evaluate on PandaLM. Their sub-models achieve roughly twice the speed of the full model while maintaining or exceeding performance. Analysis shows SoFT preserves probability distributions and hidden state representations more closely across layers than standard fine-tuning, enabling intermediate layers to generate meaningful, task-relevant outputs earlier in the model depth.

## Method Summary
The authors apply Sorted Fine-Tuning (SoFT) to LLaMA 2 13B by forming sub-networks at layers {12, 16, 20, 24, 28, 32, 36, 40}, sharing a single LLM head across all sub-models. Loss is computed at each sub-model boundary and summed for backpropagation. The method is trained on Stanford Alpaca dataset for instruction following and evaluated on PandaLM benchmark, comparing sub-model performance against standard fine-tuning and early-exit baselines.

## Key Results
- SoFT sub-models (12-40 layers) outperform both standard fine-tuning and SFT+ICT early-exit baselines on PandaLM
- Intermediate layers preserve probability distributions and hidden state representations more closely than standard fine-tuning
- Achieved roughly twice the speed of the full model while maintaining or exceeding performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SoFT enables intermediate transformer layers to generate meaningful, task-specific outputs earlier in model depth.
- Mechanism: Multi-layer supervision forces each sub-network to produce outputs that align with the full model's distribution, improving intermediate embedding quality.
- Core assumption: Intermediate layers can meaningfully contribute if trained with outputs from multiple depths rather than only the final layer.
- Evidence anchors: Abstract states sub-models are "integral components" minimizing storage/transition costs; section 3 describes sub-network fn(x; θn) and shared output prediction head.
- Break condition: If shared head cannot normalize across depths (e.g., without RMSNorm), sub-models diverge rapidly.

### Mechanism 2
- Claim: SoFT preserves probability distribution and hidden state representations across layers more closely than standard fine-tuning.
- Mechanism: Multi-loss supervision at intermediate layers constrains internal representations to remain task-relevant and stable across depth.
- Core assumption: Multi-loss supervision at intermediate layers maintains alignment between intermediate and final layer outputs.
- Evidence anchors: Section 4.2.1 shows SoFT preserves output distribution close to SFT full-size model even in lower layers; section 4.2.2 shows hidden state similarity to Sorted last layer after generating multiple tokens.
- Break condition: If intermediate layers overfit to early token distributions, generalization to longer sequences may degrade.

### Mechanism 3
- Claim: SoFT yields sub-models twice as fast as full model while matching or exceeding performance.
- Mechanism: Nested, sorted training with shared head creates efficient, task-tuned sub-models deployable without distillation or compression.
- Core assumption: Storage and transition cost savings outweigh minor performance trade-offs.
- Evidence anchors: Abstract mentions "efficient tuning and without additional memory usage during inference"; section 4 states SoFT delivers models twice as fast while maintaining or exceeding performance.
- Break condition: If inference hardware cannot exploit reduced depth efficiently, speed gains may not materialize.

## Foundational Learning

- Concept: Nested sub-network formation in depth-wise transformer architectures.
  - Why needed here: Enables SoFT to train multiple effective sub-models within one forward/backward pass, sharing the LLM head.
  - Quick check question: If you remove the sharing of the LLM head across sub-models, how does training cost and storage change?

- Concept: KL-divergence and cosine similarity as distribution and representation alignment metrics.
  - Why needed here: Used to quantify how well intermediate layers match full model's output distribution and hidden states.
  - Quick check question: What happens to KL-divergence if intermediate layers generate unrelated text compared to final layer?

- Concept: Early-exit vs. shared-head multi-output training.
  - Why needed here: Clarifies why SoFT outperforms naive early-exit approaches (separate heads, no joint loss).
  - Quick check question: How does training a separate head for each intermediate layer differ from SoFT's shared-head approach in terms of parameter count and output consistency?

## Architecture Onboarding

- Component map: Input embedding → Transformer layers (12-40) → RMSNorm → Shared LLM head → Output logits
- Critical path: Forward pass through all layers up to desired sub-model depth → Apply RMSNorm + shared head → Compute loss and backpropagate to update all preceding layers simultaneously
- Design tradeoffs:
  - Pros: Single model serves multiple latency/compute budgets; no storage overhead; no distillation needed
  - Cons: Larger memory footprint during training (multiple losses); requires careful normalization to align distributions
- Failure signatures:
  - Rapidly increasing KL-divergence in early layers → normalization or head-sharing issue
  - Inconsistent outputs across sub-models → gradient interference or loss weighting problem
  - Sub-models not faster in practice → hardware cannot exploit reduced depth effectively
- First 3 experiments:
  1. Train SoFT on small Alpaca-like dataset; measure KL-divergence across layers for seed prompts
  2. Compare zero-shot outputs of sub-models vs. full model on held-out instruction set; check semantic coherence
  3. Profile inference latency and memory for sub-models vs. baseline full model; verify speed-up claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Sorted Fine-Tuning preserve learned representations more effectively than Standard Fine-Tuning across all layers, or is this effect limited to specific ranges?
- Basis in paper: Explicit - paper presents comparisons of hidden state cosine similarity and KL-divergence between SFT and Sorted LLaMA sub-models across different layers
- Why unresolved: Paper lacks comprehensive analysis of representation preservation consistency across all layers for various tasks and model sizes
- What evidence would resolve it: Detailed analysis of representation preservation across all layers for various tasks and model sizes, comparing Sorted LLaMA and SFT

### Open Question 2
- Question: How does choice of architecture impact effectiveness of Sorted Fine-Tuning in enhancing intermediate layer performance?
- Basis in paper: Inferred - paper applies SoFT to LLaMA 2 13B but does not explore impact of different architectures or model sizes
- Why unresolved: Paper focuses on specific model and does not investigate generalizability to other architectures or sizes
- What evidence would resolve it: Experiments applying SoFT to various transformer architectures and model sizes, comparing their performance

### Open Question 3
- Question: Can Sorted Fine-Tuning be effectively applied during pre-training phase, and how does it compare to applying during fine-tuning?
- Basis in paper: Inferred - paper mentions possibility of applying SoFT during pre-training as future research direction but provides no empirical evidence
- Why unresolved: Paper does not explore potential benefits or challenges of applying SoFT during pre-training, nor compare to applying during fine-tuning
- What evidence would resolve it: Comparative experiments applying SoFT during pre-training and fine-tuning, evaluating respective impacts on model performance

## Limitations

- Lack of ablation studies isolating effects of shared vs. separate LLM heads or comparing multi-loss supervision to single-loss baselines
- Hardware-dependent speed claims may not materialize on GPUs with parallel layer execution
- Uncertainty about normalization strategy and its critical role in distribution preservation

## Confidence

**High Confidence:** Core technical contribution of extending SortedNet to generative language modeling with shared heads is well-defined and reproducible; PandaLM benchmark results are directly reported and verifiable.

**Medium Confidence:** Claim that SoFT preserves probability distributions and hidden state representations more closely than SFT is supported by metrics but lacks external validation or ablation studies.

**Low Confidence:** General scalability to larger models or longer instruction sequences is untested; paper only evaluates up to 40 layers without extended context length reporting.

## Next Checks

1. **Normalization Ablation:** Implement and compare SoFT with and without RMSNorm before shared LLM head; measure KL-divergence across layers for seed prompts to confirm normalization is critical for distribution preservation.

2. **Shared vs. Separate Head Experiment:** Train SoFT with separate LLM heads for each sub-model (increasing parameter count) and compare to shared-head approach; evaluate both on PandaLM to isolate effect of head sharing on output consistency and speed.

3. **Hardware Profiling:** Profile inference latency and memory usage of SoFT sub-models (12-40 layers) on both GPU and CPU to verify "twice as fast" claim under realistic deployment scenarios; check whether speedup scales linearly with depth reduction.