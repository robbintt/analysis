---
ver: rpa2
title: How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations
  Shows their Vulnerabilities
arxiv_id: '2311.09447'
source_url: https://arxiv.org/abs/2311.09447
tags:
- llms
- arxiv
- trustworthiness
- red-lm
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive assessment of the trustworthiness
  of open-source large language models (LLMs) across eight aspects: toxicity, stereotypes,
  ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial
  demonstrations. The authors propose an enhanced Chain of Utterances (CoU)-based
  prompting strategy, called advCoU, which incorporates malicious demonstrations and
  misleading internal thoughts to conduct trustworthiness attacks.'
---

# How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities

## Quick Facts
- arXiv ID: 2311.09447
- Source URL: https://arxiv.org/abs/2311.09447
- Reference count: 10
- Open-source LLMs show significant vulnerabilities to adversarial attacks across multiple trustworthiness dimensions

## Executive Summary
This paper presents a comprehensive assessment of trustworthiness vulnerabilities in open-source large language models (LLMs) through an enhanced adversarial attack methodology. The authors propose advCoU, an extension of Chain of Utterances prompting that incorporates malicious demonstrations and misleading internal thoughts to systematically evaluate LLM robustness across eight trustworthiness aspects: toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and adversarial robustness. Through extensive experiments on five prominent LLM series (Vicuna, MPT, Falcon, Mistral, and Llama 2), the study reveals that larger models are paradoxically more vulnerable to attacks despite superior general performance, and that instruction-tuned models without explicit safety alignment are particularly susceptible to manipulation.

## Method Summary
The study employs advCoU, an enhanced Chain of Utterances-based prompting strategy that integrates malicious demonstrations and misleading internal thoughts to conduct trustworthiness attacks. The methodology involves constructing adversarial prompts with carefully crafted in-context examples that model harmful behavior, then evaluating model outputs across eight trustworthiness dimensions using standardized datasets. Experiments are conducted on five LLM series with varying sizes, comparing attack success rates (ASR) across different model configurations. The evaluation framework measures the percentage of successful attacks where models generate harmful or undesirable outputs when prompted with malicious demonstrations.

## Key Results
- Larger LLMs demonstrate higher vulnerability to adversarial attacks despite superior general NLP performance
- Instruction-tuned models without safety alignment show significantly higher susceptibility to attacks compared to their base counterparts
- Safety alignment fine-tuning proves effective in mitigating adversarial trustworthiness attacks, particularly for larger models
- The advCoU attack strategy achieves consistent success rates across diverse trustworthiness aspects, revealing systematic vulnerabilities in open-source LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Malicious demonstrations combined with internal thoughts in CoU prompting significantly increase attack success rates across multiple trustworthiness aspects.
- Mechanism: The attack exploits LLMs' in-context learning capability by providing carefully crafted demonstrations that model the desired (malicious) behavior, while internal thoughts guide the model's reasoning toward producing harmful outputs.
- Core assumption: LLMs are susceptible to influence from in-context examples and can be directed by explicit internal reasoning prompts to override their safety alignment.
- Evidence anchors:
  - [abstract]: "We propose advCoU, an extended Chain of Utterances-based (CoU) prompting strategy by incorporating carefully crafted malicious demonstrations for trustworthiness attack."
  - [section 2.2]: "We expand their scope and introduce to mislead LLMs through the design of malicious demonstrations as in-context examples. This approach allows us to manipulate only the demonstrations without changing the input to perform trustworthiness attacks."
  - [corpus]: Weak evidence - no direct mentions of CoU or advCoU mechanisms in corpus neighbors, suggesting this is a novel approach.
- Break condition: If the model's safety alignment mechanisms can effectively detect and filter out malicious demonstrations or internal thoughts, or if the model's context window is insufficient to process the crafted prompts.

### Mechanism 2
- Claim: Larger LLMs are more vulnerable to adversarial attacks despite their superior performance on general NLP tasks.
- Mechanism: As model size increases, the model's capacity to generate diverse and contextually appropriate responses also increases, making it more susceptible to manipulation through crafted prompts that exploit this capability.
- Core assumption: Model size correlates with both capability and vulnerability, as larger models have more parameters to be influenced by in-context demonstrations.
- Evidence anchors:
  - [abstract]: "Our result analysis reveals that models with superior performance in general NLP tasks do not always have greater trustworthiness; in fact, larger models can be more vulnerable to attacks."
  - [section 3.3.1]: "For each model series, as the base model grows larger, the average ASR across different aspects becomes higher."
  - [corpus]: Weak evidence - no direct mentions of size-vulnerability relationship in corpus neighbors, though related work on adversarial attacks exists.
- Break condition: If safety alignment techniques are specifically designed and proven effective for larger models, or if the relationship between size and vulnerability is not linear.

### Mechanism 3
- Claim: Instruction-tuned models are more susceptible to attacks unless they include safety alignment fine-tuning.
- Mechanism: Instruction tuning focuses on improving the model's ability to follow instructions, which can be exploited by adversarial prompts that instruct the model to produce harmful content. Safety alignment fine-tuning mitigates this by incorporating additional training to resist such manipulation.
- Core assumption: Instruction tuning without safety alignment creates a vulnerability where the model prioritizes instruction following over content safety.
- Evidence anchors:
  - [abstract]: "models that have undergone instruction tuning, focusing on instruction following, tend to be more susceptible, although fine-tuning LLMs for safety alignment proves effective in mitigating adversarial trustworthiness attacks."
  - [section 3.3.2]: "FALCON and MISTRAL experience higher ASR scores for their instruct versions compared to their respective base versions. This outcome is in line with our expectations, because MISTRAL-instruct focuses on fine-tuning models for better performance on NLP tasks without additional moderation mechanisms."
  - [corpus]: Weak evidence - no direct mentions of instruction tuning vulnerability in corpus neighbors, though safety alignment is discussed in related work.
- Break condition: If instruction tuning inherently includes safety considerations, or if models can distinguish between benign and malicious instructions.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: The attack strategy relies on manipulating the model's behavior through carefully crafted in-context examples rather than modifying the model weights.
  - Quick check question: How does providing demonstrations in the prompt context influence the model's output generation?

- Concept: Chain of Utterances (CoU) prompting
  - Why needed here: The attack builds upon the CoU framework, which simulates conversations with internal thoughts to elicit specific responses from the model.
  - Quick check question: What role do internal thoughts play in guiding the model's response generation in the CoU framework?

- Concept: Adversarial demonstrations
  - Why needed here: The core of the attack involves designing malicious demonstrations that mislead the model into producing harmful outputs across various trustworthiness aspects.
  - Quick check question: How can demonstration examples be crafted to exploit the model's in-context learning capabilities for malicious purposes?

## Architecture Onboarding

- Component map: Input prompt construction -> Model inference -> Output evaluation -> Dataset management
- Critical path:
  1. Construct adversarial prompt with demonstrations and internal thoughts
  2. Feed prompt to target LLM
  3. Capture model output
  4. Evaluate output against ground truth or safety criteria
  5. Calculate attack success rate

- Design tradeoffs:
  - Manual vs. automated prompt generation: Manual allows for targeted attacks but is labor-intensive; automated could scale but may be less effective
  - Single vs. multiple demonstrations: More demonstrations may increase attack success but also increase prompt length and complexity
  - Specificity vs. generalizability: Highly specific prompts may work better on individual models but may not transfer across different model architectures

- Failure signatures:
  - Low attack success rates across all models and aspects
  - Model refusal to complete prompts containing harmful content
  - Output that deviates from the desired malicious behavior but doesn't clearly indicate safety intervention
  - Inconsistent results across different model sizes or instruction-tuned variants

- First 3 experiments:
  1. Replicate baseline attack on toxicity using straightforward adversarial prompt to establish reference ASR
  2. Test enhanced CoU attack on stereotype aspect with targeted prompts to compare against baseline
  3. Evaluate size vulnerability by running attacks on different model sizes within the same series (e.g., Llama 2 7B, 13B, 70B)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of adversarial attacks on open-source LLMs vary across different instruction-tuning datasets and fine-tuning methods?
- Basis in paper: [explicit] The paper discusses the impact of instruction tuning and alignment processes on LLM trustworthiness, noting that models with instruction tuning tend to be more susceptible to attacks, but fine-tuning for safety alignment can mitigate this.
- Why unresolved: The paper does not provide a detailed analysis of how different instruction-tuning datasets and fine-tuning methods specifically affect the vulnerability of LLMs to adversarial attacks.
- What evidence would resolve it: Conducting experiments that compare the attack success rates of LLMs fine-tuned with various instruction-tuning datasets and methods would provide insights into the relationship between fine-tuning approaches and adversarial attack effectiveness.

### Open Question 2
- Question: Can automated prompt generation and optimization techniques improve the effectiveness of adversarial attacks on open-source LLMs?
- Basis in paper: [inferred] The paper mentions that manual design of malicious demonstrations and internal thoughts is used for attacks, but suggests that automating prompt generation and optimization could be interesting future work.
- Why unresolved: The paper does not explore automated prompt generation techniques, leaving open the question of whether such methods could enhance attack effectiveness.
- What evidence would resolve it: Implementing and testing automated prompt generation and optimization algorithms to generate more effective adversarial prompts would provide evidence of their impact on attack success rates.

### Open Question 3
- Question: How do different model architectures and sizes influence the susceptibility of open-source LLMs to adversarial attacks across various trustworthiness aspects?
- Basis in paper: [explicit] The paper discusses the relationship between model size and trustworthiness, finding that larger models tend to be more vulnerable to attacks, but does not delve into the influence of different model architectures.
- Why unresolved: The paper focuses on model size as a factor in trustworthiness but does not investigate the role of different model architectures in susceptibility to adversarial attacks.
- What evidence would resolve it: Conducting experiments that compare the attack success rates of LLMs with different architectures (e.g., transformer variants) and sizes would provide insights into the impact of architecture on adversarial attack effectiveness.

## Limitations
- Manual prompt engineering approach may not generalize well across different model architectures or new datasets
- Study focuses exclusively on open-source LLMs, limiting generalizability to proprietary models with potentially stronger safety mechanisms
- Evaluation framework measures attack success through ASR metrics without considering severity or practical impact of harmful outputs

## Confidence
**High Confidence**: The observation that larger models show increased vulnerability to attacks is well-supported by systematic experiments across multiple model series.

**Medium Confidence**: The claim that instruction-tuned models without safety alignment are more vulnerable has reasonable support but may be model-specific.

**Low Confidence**: The effectiveness of advCoU as a general attack methodology against real-world deployments is questionable, as the paper demonstrates controlled laboratory success but doesn't address practical deployment scenarios.

## Next Checks
1. **Cross-model transferability test**: Apply the same advCoU prompts to a completely different set of LLMs (including proprietary models if possible) to verify if the attack strategy generalizes beyond the five tested model families.

2. **Safety mechanism interaction analysis**: Test whether models with different safety alignment techniques (RLHF, constitutional AI, etc.) show varying vulnerability patterns when subjected to the same advCoU attacks, particularly focusing on models that combine instruction tuning with explicit safety training.

3. **Real-world deployment simulation**: Create a pipeline that simulates actual deployment conditions by adding content filtering layers, rate limiting, and human oversight to assess whether advCoU attacks remain effective when models operate in production environments rather than controlled testing scenarios.