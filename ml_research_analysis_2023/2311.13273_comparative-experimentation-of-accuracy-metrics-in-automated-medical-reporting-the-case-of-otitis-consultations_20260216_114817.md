---
ver: rpa2
title: 'Comparative Experimentation of Accuracy Metrics in Automated Medical Reporting:
  The Case of Otitis Consultations'
arxiv_id: '2311.13273'
source_url: https://arxiv.org/abs/2311.13273
tags:
- metrics
- medical
- report
- accuracy
- reports
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates 10 metrics for measuring the accuracy of AI-generated
  medical reports against General Practitioner reports on Otitis consultations. The
  metrics are compared by correlating their scores with the number of missing, incorrect,
  and additional statements in the AI reports, and by introducing a Composite Accuracy
  Score.
---

# Comparative Experimentation of Accuracy Metrics in Automated Medical Reporting: The Case of Otitis Consultations

## Quick Facts
- arXiv ID: 2311.13273
- Source URL: https://arxiv.org/abs/2311.13273
- Reference count: 16
- Primary result: ROUGE-L and Word Mover's Distance emerge as preferred metrics for evaluating AI-generated medical reports, contrary to previous work

## Executive Summary
This study evaluates 10 accuracy metrics for measuring the performance of AI-generated medical reports against General Practitioner reports on Otitis consultations. The authors introduce a Composite Accuracy Score that combines correlations with different types of errors to produce a single comparable score for each metric. Their findings show that ROUGE-L and Word Mover's Distance are the preferred metrics for this domain, offering a practical framework for evaluating AI medical reporting systems that could reduce administrative burden for GPs.

## Method Summary
The study uses 7 Dutch medical consultation transcripts (4 Otitis Externa, 3 Otitis Media Acuta) with corresponding GP SOEP reports. AI reports are generated using GPT-4 with a Dutch prompt. Ten accuracy metrics are computed comparing AI reports to GP reports. Human evaluators manually count missing, incorrect, and additional statements in AI reports, along with post-edit time. Pearson correlations between metric scores and human evaluation aspects are calculated, and a Composite Accuracy Score (CAS) is computed to rank metrics.

## Key Results
- ROUGE-L and Word Mover's Distance are identified as preferred metrics for medical report accuracy
- The Composite Accuracy Score provides a single comparable score across different metrics
- Strong negative correlation between preferred metrics and post-edit time indicates practical usability
- Edit distance metrics (Levenshtein, WER) show inverted correlations requiring manual adjustment
- The study challenges previous findings that favored different metric sets for medical reporting

## Why This Works (Mechanism)

### Mechanism 1
Human evaluation provides a domain-specific accuracy baseline that captures missing, incorrect, and off-topic statements in AI-generated medical reports. By manually comparing AI reports with GP reports and counting specific error types, the authors establish a reference score against which metric correlations are calculated. The core assumption is that human evaluators can reliably identify these statements in Dutch medical reports.

### Mechanism 2
Metrics with stronger negative correlation to post-edit time are preferred for measuring medical report accuracy. Post-edit time represents the practical burden on GPs to correct AI reports, so metrics that correlate strongly negative with this time are better indicators of real-world usefulness. The core assumption is that correction time is a meaningful proxy for report accuracy and usability.

### Mechanism 3
Composite Accuracy Score (CAS) enables direct comparison of metrics by normalizing correlations with different types of errors. The CAS formula weights correlations with missing, incorrect, and added statements to produce a single comparable score for each metric. The core assumption is that the formula weights appropriately reflect the severity of each error type.

## Foundational Learning

- **SOEP (Dutch SOAP) reporting structure**: Understanding the report structure is crucial for interpreting what constitutes a missing or incorrect statement in AI reports. *Quick check*: What are the four categories of a SOEP report and what type of information belongs in each?

- **Metric categories (edit distance, embedding, text overlap)**: The study compares metrics from different categories, so understanding their underlying mechanisms is important for interpreting results. *Quick check*: How does ROUGE-L differ from BLEU in terms of what text features they measure?

- **Pearson correlation coefficient**: The study uses Pearson correlation to compare metric scores with human evaluation aspects. *Quick check*: What does a strong negative Pearson correlation indicate about the relationship between two variables?

## Architecture Onboarding

- **Component map**: Transcripts -> GPT-4 AI reports -> 10 metrics scores -> Human evaluation (7 aspects) -> Correlation analysis -> CAS calculation -> Preferred metrics identification

- **Critical path**: 1) Generate AI reports from transcripts, 2) Run all 10 metrics on AI vs GP reports, 3) Perform human evaluation on AI reports, 4) Calculate correlations between metrics and human evaluation aspects, 5) Compute Composite Accuracy Scores, 6) Determine preferred metrics based on CAS and post-edit time correlation

- **Design tradeoffs**: Using Dutch-specific embeddings vs multilingual models, manual human evaluation vs automated evaluation methods, broad metric selection vs focus on domain-specific metrics, simple prompt formulation vs complex, tailored prompts

- **Failure signatures**: Metrics showing positive correlation with incorrect statements, large discrepancies between metric rankings and human evaluation preferences, post-edit time not correlating with any metrics, Composite Accuracy Scores being too similar across metrics

- **First 3 experiments**: 1) Compare metric rankings using different prompt formulations, 2) Test metric performance on reports generated for different medical conditions, 3) Evaluate metric robustness by introducing controlled errors into reports

## Open Questions the Paper Calls Out

### Open Question 1
How do the preferred metrics (ROUGE-L and WMD) perform on medical reports from other pathologies beyond Otitis consultations? The study's findings are based on a narrow medical scope of only otitis cases, and it is unclear if these metrics generalize to other medical domains.

### Open Question 2
How would the accuracy metrics perform if the human evaluation was conducted by healthcare professionals with medical expertise? The current study's human evaluation was performed by researchers without prior experience in writing medical reports, and healthcare professionals would likely provide more accurate assessments.

### Open Question 3
How does the use of abbreviations in AI-generated medical reports affect the accuracy metrics' performance? The study mentions that abbreviations were changed to full expressions in GP reports but does not explore the impact of abbreviations in AI-generated reports.

## Limitations

- Small sample size (7 consultation transcripts) limits generalizability to other medical domains
- Manual human evaluation lacks inter-rater reliability measures and detailed documentation
- Focus exclusively on otitis consultations prevents broader medical domain validation
- Unknown impact of prompt variations on metric performance rankings

## Confidence

- **High confidence**: The methodology for comparing metrics using correlation with human evaluation aspects is sound and well-documented
- **Medium confidence**: The conclusion that ROUGE-L and WMD are preferred metrics is supported by the data but would benefit from validation on a larger dataset
- **Medium confidence**: The claim that post-edit time is a meaningful proxy for report accuracy and usability is reasonable but not directly validated within the study

## Next Checks

1. **Expand dataset validation**: Replicate the analysis with at least 50 consultation transcripts across multiple medical conditions to verify metric preferences are consistent beyond otitis consultations.

2. **Human evaluation reliability assessment**: Implement a formal inter-rater reliability study with multiple annotators performing the same evaluation task to establish confidence intervals for the human evaluation baseline.

3. **Metric stability testing**: Perform cross-validation by splitting the dataset into training and test subsets to verify that metric rankings remain consistent across different data partitions.