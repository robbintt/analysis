---
ver: rpa2
title: On Privileged and Convergent Bases in Neural Network Representations
arxiv_id: '2307.12941'
source_url: https://arxiv.org/abs/2307.12941
tags:
- blocks
- frozen
- network
- networks
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether neural networks learn privileged
  and convergent bases in their internal representations. The authors examine the
  significance of feature directions represented by individual neurons and explore
  the possibility of multiple bases achieving identical performance.
---

# On Privileged and Convergent Bases in Neural Network Representations

## Quick Facts
- arXiv ID: 2307.12941
- Source URL: https://arxiv.org/abs/2307.12941
- Authors: 
- Reference count: 40
- Key outcome: Neural networks don't converge to unique bases even at high width, but basis correlation increases significantly when early layers are frozen identically

## Executive Summary
This paper investigates whether neural networks learn privileged and convergent bases in their internal representations. The authors examine the significance of feature directions represented by individual neurons and explore the possibility of multiple bases achieving identical performance. They find that even in wide networks, neural networks do not converge to a unique basis. However, basis correlation increases significantly when a few early layers of the network are frozen identically. Additionally, while Linear Mode Connectivity improves with increased network width, this improvement is not due to an increase in basis correlation.

## Method Summary
The authors train multiple neural network architectures (ResNet-20, ConvNeXt, Vision Transformer) on CIFAR-10 and ImageNet datasets. They compute permutation correlation between activations from different training runs to measure basis similarity. To test rotation invariance, they apply random orthonormal rotations to layer activations and retrain subsequent layers. They also freeze early layers in some networks to study effects on basis convergence and identity stitching performance.

## Key Results
- Neural networks do not converge to a unique basis, even at high width (perm-corr remains almost constant with width)
- Basis correlation increases significantly when a few early layers of the network are frozen identically
- Neural networks cannot learn rotationally invariant representations (accuracy degrades significantly when applying random rotations)
- Linear Mode Connectivity improves with increased network width, but this is not due to increased basis correlation

## Why This Works (Mechanism)

### Mechanism 1
Neural network representations do not converge to a unique basis, even at high width. The neuron activations across different training runs remain permuted but not aligned, meaning that while the overall functional output may be similar, the internal feature directions are not consistent across models.

### Mechanism 2
Neural networks cannot learn rotationally invariant representations. Arbitrary orthonormal rotations applied to layer activations cannot be undone by subsequent layers through retraining, indicating that the directions represented by individual neurons are functionally significant and not interchangeable.

### Mechanism 3
Freezing early layers enforces a convergent basis across different training runs. By freezing the first few layers identically in two networks, the subsequent layers must adapt to the same input representation, leading to higher permutation correlation and better identity stitching.

## Foundational Learning

- **Permutation symmetry in neural networks**: Understanding that neurons can be permuted without changing function output is key to interpreting basis correlation and linear mode connectivity metrics.
  - Quick check: If two networks have the same function but different neuron orders, what property must we consider when comparing their representations?

- **Linear mode connectivity (LMC)**: LMC measures how smoothly one model can interpolate to another, which relates to basis similarity but is not solely determined by it.
  - Quick check: If perm-LMC barrier decreases with width but basis correlation stays constant, what other factor might be improving?

- **Rotational invariance**: Distinguishing between linear networks (which can be rotationally invariant) and non-linear networks (which cannot) is central to understanding why individual neuron directions matter.
  - Quick check: In a two-layer linear network, can you rotate the first layer's representation and still recover the same output? Why or why not?

## Architecture Onboarding

- **Component map**: ConvNeXt/ViT/ResNet-20 -> Early layers (to freeze) -> Middle layers (activations for correlation) -> Late layers (output)
- **Critical path**: Train two networks with different initializations → Extract activations from each layer → Compute permutation correlation between activations → Test rotation invariance by freezing layers and retraining → Evaluate identity stitching performance
- **Design tradeoffs**: Freezing early layers increases basis convergence but reduces flexibility in learning; Higher width improves noise robustness but does not guarantee basis alignment; Linear mode connectivity is easier at high width but may not reflect true basis similarity
- **Failure signatures**: perm-corr stays near zero even after freezing early layers → freezing not effective; LMC barrier decreases but basis correlation unchanged → width improves robustness, not alignment; Rotation experiment succeeds → model can learn rotationally invariant representations (contrary to claim)
- **First 3 experiments**:
  1. Train two ResNet-20s, compute perm-corr across all layers, verify it stays constant with width
  2. Apply random rotation to first layer activations, freeze and retrain rest, measure accuracy drop
  3. Freeze first 2 layers of two networks, retrain, recompute perm-corr and identity stitching penalty

## Open Questions the Paper Calls Out

### Open Question 1
Does the proposed freezing procedure for early layers generalize across different architectures (e.g., transformers, LSTMs) and tasks (e.g., language modeling, reinforcement learning)? The paper primarily demonstrates this on vision models and CIFAR-10/ImageNet.

### Open Question 2
What is the theoretical explanation for why width improves linear mode connectivity but not basis correlation? The paper provides empirical evidence but does not offer a theoretical explanation for this phenomenon.

### Open Question 3
Are there alternative methods to achieve convergent bases without freezing layers, and how do they compare in terms of performance and interpretability? The authors propose freezing early layers as one approach, but this is not necessarily the only method to achieve convergent bases.

## Limitations

- The study focuses primarily on image classification tasks and may not generalize to other domains or architectures
- The random rotation experiments assume perfect orthonormal matrices and may not reflect practical scenarios where basis misalignment occurs gradually during training
- Permutation correlation and linear mode connectivity may not capture all aspects of representational alignment

## Confidence

- High confidence: Neural networks do not converge to a unique basis even at high width
- Medium confidence: Basis correlation increases significantly when early layers are frozen
- Low confidence: Neural networks cannot learn rotationally invariant representations

## Next Checks

1. Replicate the permutation correlation analysis across additional architectures (MLP-Mixer, ConvNeXt) to verify the width-independence of basis correlation
2. Test the rotation invariance claim with different rotation strategies (random orthogonal matrices vs. structured rotations) and verify that the accuracy degradation is consistent
3. Investigate whether freezing middle/late layers instead of early layers produces similar increases in basis correlation to isolate whether the effect is specific to early layer features