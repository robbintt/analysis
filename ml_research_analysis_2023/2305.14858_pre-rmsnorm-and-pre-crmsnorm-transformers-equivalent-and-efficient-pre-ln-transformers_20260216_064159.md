---
ver: rpa2
title: 'Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN
  Transformers'
arxiv_id: '2305.14858'
source_url: https://arxiv.org/abs/2305.14858
tags:
- pre-ln
- rmsnorm
- transformer
- training
- layernorm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes two variants of Transformer architectures,
  Pre-RMSNorm and Pre-CRMSNorm, which are equivalent to the widely used Pre-LN Transformer.
  The key idea is to remove the inherent redundancy in Pre-LN Transformers by maintaining
  zero-mean on the main branch vectors, allowing for the simplification of LayerNorm
  to RMSNorm.
---

# Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers

## Quick Facts
- arXiv ID: 2305.14858
- Source URL: https://arxiv.org/abs/2305.14858
- Authors: 
- Reference count: 40
- Pre-(C)RMSNorm Transformers achieve up to 10% reduction in training and inference time compared to Pre-LN Transformers without loss in functionality

## Executive Summary
This paper proposes two variants of Transformer architectures, Pre-RMSNorm and Pre-CRMSNorm, which are mathematically equivalent to the widely used Pre-LayerNorm (Pre-LN) Transformer. The key insight is that LayerNorm applied to main branch vectors in Pre-LN Transformers is redundant because these vectors are always normalized before use. By maintaining zero-mean on main branch vectors, LayerNorm can be simplified to RMSNorm, which only scales by root mean square without shifting. The proposed variants achieve up to 10% reduction in training and inference time on Vision Transformers and GPT3 models while preserving identical functionality.

## Method Summary
The paper establishes that Pre-LN Transformers can be converted to equivalent Pre-RMSNorm Transformers by recentering input vectors to zero mean, modifying linear layers to preserve zero-mean outputs, and replacing LayerNorm with RMSNorm. For further memory efficiency, Compressed RMSNorm (CRMSNorm) compresses zero-mean vectors by discarding the last element and reconstructing it during decompression. This allows Pre-CRMSNorm Transformers to operate on Rd-1 dimensional vectors. The conversion process involves three main modifications: recentering inputs, replacing linear layer weights and biases with zero-mean preserving versions, and substituting LayerNorm with RMSNorm or CRMSNorm at appropriate positions in the architecture.

## Key Results
- Pre-RMSNorm and Pre-CRMSNorm Transformers are mathematically equivalent to Pre-LN Transformers
- Up to 10% reduction in training and inference time compared to Pre-LN Transformers
- First unification of LayerNorm and RMSNorm in pre-normalization Transformers with proven arithmetic equivalence
- Effective on both ViT (ImageNet) and GPT3 models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing mean information from main branch vectors in Pre-LN Transformers allows substitution of LayerNorm with RMSNorm without affecting functionality.
- Mechanism: Pre-LN applies LayerNorm to main branch vectors before they are used. Since LayerNorm is invariant to constant shifts (LayerNorm(x + k1) = LayerNorm(x)), we can recenter these vectors to zero mean, making LayerNorm equivalent to RMSNorm.
- Core assumption: The mean of main branch vectors is redundant information because these vectors are always normalized before use.
- Evidence anchors:
  - [abstract]: "By removing the inherent redundant mean information in the main branch of Pre-LN Transformers, we can reduce LayerNorm to RMSNorm"
  - [section 3.1]: "LayerNorm is invariant to the shifting LN(x + k1) = LN(x), ∀k ∈ R. We observe that LayerNorm is applied to the main branch vectors before they are used in either residual branches or the final postprocessing in Pre-LN Transformers"
- Break condition: If main branch vectors are used in their unnormalized form somewhere in the architecture, the equivalence would break.

### Mechanism 2
- Claim: CRMSNorm provides lossless compression of zero-mean vectors by discarding the last element and reconstructing it from the others.
- Mechanism: For a zero-mean vector x ∈ Rd, we can compress it to Rd-1 by discarding xd = -Σi=1 to d-1 xi. During decompression, we reconstruct the full vector and apply RMSNorm.
- Core assumption: Zero-mean constraint allows lossless dimensionality reduction since the last element is determined by the others.
- Evidence anchors:
  - [section 2.3]: "For a zero-mean vector x ∈ Rd, we can compress it losslessly by discarding its last element. In the decompression, we recover the discarded element with xd = -Σi=1 to d-1 xi"
  - [section 3.2]: "We define Compressed Root Mean Square Normalization (CRMSNorm), which takes a vector x ∈ Rd-1 as input. CRMSNorm first decompresses the vector x to obtain a zero-mean vector in Rd, then applies RMSNorm on the zero-mean vector"
- Break condition: If the zero-mean constraint is violated at any point, the compression would become lossy.

### Mechanism 3
- Claim: Linear layers with zero-mean inputs can be simplified without affecting output.
- Mechanism: Given a linear transformation y = Ax + b where x has zero mean, we can decompose the output into a zero-mean part and a mean part. The zero-mean part can be computed using a simplified weight matrix A' = A - (1/m)11T A.
- Core assumption: The mean component of the output can be handled separately or is redundant.
- Evidence anchors:
  - [section 3.1]: "Lemma 3.1 Given a linear transformation y = Ax + b, x ∈ Rn, A ∈ Rm×n, b, y ∈ Rm, we can decompose the output with two parts y = Ax + b + μ(y)1"
  - [section 3.4.1]: "The linear layer with zero-mean output. The modified linear layer will not induce extra computation for inference since we can replace the weight and bias in advance"
- Break condition: If the mean component of the output is required for subsequent computations, this simplification would break functionality.

## Foundational Learning

- Concept: LayerNorm normalization (zero-mean and unit-variance transformation)
  - Why needed here: Understanding the difference between LayerNorm and RMSNorm is fundamental to grasping why removing mean information enables the equivalence
  - Quick check question: What is the key mathematical difference between LayerNorm and RMSNorm?

- Concept: Transformer architecture (residual connections and normalization placement)
  - Why needed here: The equivalence relies on understanding how normalization is applied in Pre-LN versus Pre-RMSNorm architectures
  - Quick check question: Where is LayerNorm applied in Pre-LN Transformers compared to Post-LN Transformers?

- Concept: Linear algebra (matrix decomposition and mean properties)
  - Why needed here: The proof of equivalence relies on decomposing linear transformations and understanding how mean information propagates
  - Quick check question: How can you decompose a linear transformation y = Ax + b into mean and zero-mean components?

## Architecture Onboarding

- Component map:
  - Embeddings -> Pre-RMSNorm (or Pre-CRMSNorm) -> Residual blocks with normalization -> Output
  - Key change: Replace LayerNorm with RMSNorm/CRMSNorm and recenter vectors

- Critical path: Input → Recentering → CRMSNorm → Residual blocks → CRMSNorm → Output
  - The critical path involves the main branch through the network where normalization occurs

- Design tradeoffs:
  - Pre-RMSNorm: Minimal changes, requires recentering of inputs and linear layer modifications
  - Pre-CRMSNorm: Additional compression saves memory but may be slower on some accelerators
  - Tradeoff: Memory efficiency vs. computational efficiency depending on hardware capabilities

- Failure signatures:
  - Loss of zero-mean property: If vectors are modified outside the controlled flow, equivalence breaks
  - Numerical instability: Low-precision arithmetic may accumulate small errors in mean calculations
  - Hardware limitations: Some accelerators cannot efficiently handle non-power-of-2 dimensions

- First 3 experiments:
  1. Verify equivalence by training identical models with Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm and comparing outputs
  2. Benchmark inference speed on different batch sizes to measure practical efficiency gains
  3. Test on different hardware platforms (GPU, CPU, specialized accelerators) to identify performance bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which the numerical discrepancies introduced by the conversion from Pre-LN to Pre-(C)RMSNorm Transformers accumulate in very large models, and under what conditions do they lead to significant performance degradation?
- Basis in paper: [explicit] The paper mentions that "it is possible that these small discrepancies may be accumulated and enlarged in the large models, further degrading the numerical stability."
- Why unresolved: The paper acknowledges the potential for numerical issues but does not provide a detailed analysis of the accumulation mechanism or specific conditions leading to significant degradation.
- What evidence would resolve it: A detailed numerical analysis of the discrepancies introduced by the conversion process in various large models, showing the accumulation patterns and their impact on performance under different conditions (e.g., model size, precision, training data).

### Open Question 2
- Question: How does the efficiency improvement from Pre-(C)RMSNorm Transformers vary across different hardware accelerators, especially those not optimized for handling vectors of dimension Rd-1?
- Basis in paper: [explicit] The paper discusses that "most accelerators can not efficiently handle the vectors in Rd−1 when d is a large even number, especially a power of two" and suggests alternatives for inference and training.
- Why unresolved: While the paper provides alternatives, it does not present a comprehensive evaluation of efficiency improvements across a wide range of hardware accelerators.
- What evidence would resolve it: Experimental results showing the efficiency gains of Pre-(C)RMSNorm Transformers on various hardware accelerators, including those not optimized for Rd-1 vectors, and a comparison of the performance of the proposed alternatives (e.g., zero padding, decompression).

### Open Question 3
- Question: What is the impact of the Pre-(C)RMSNorm Transformer conversion on the training dynamics and convergence of large language models, particularly in terms of stability and speed?
- Basis in paper: [inferred] The paper claims that the conversion process is "almost no cost" and does not require fine-tuning, but it does not provide a detailed analysis of the impact on training dynamics and convergence.
- Why unresolved: The paper focuses on the equivalence and efficiency of the proposed variants but does not explore the training dynamics and convergence aspects in depth.
- What evidence would resolve it: A comprehensive study comparing the training dynamics and convergence of Pre-LN and Pre-(C)RMSNorm Transformers on large language models, including metrics such as stability, speed of convergence, and final performance.

## Limitations

- The paper relies on the assumption that zero-mean vectors can be maintained throughout computation without numerical drift, which may not hold in low-precision training regimes
- The compression/decompression operations in CRMSNorm introduce computational overhead that may offset memory savings on certain hardware accelerators
- The paper only validates on Vision Transformers and GPT3 inference, leaving open questions about performance on encoder-decoder architectures

## Confidence

- **High Confidence**: The mathematical equivalence between Pre-LN and Pre-RMSNorm is rigorously proven and the fundamental insight about LayerNorm's invariance to shifts is well-established
- **Medium Confidence**: The claimed 10% efficiency improvements are demonstrated empirically but may be highly dependent on specific hardware and implementation details
- **Low Confidence**: The generalization of these findings to other transformer variants (Post-LN, Pre-Norm, etc.) and different tasks beyond vision and language modeling

## Next Checks

1. **Numerical Stability Analysis**: Implement the Pre-RMSNorm architecture using mixed-precision training (FP16/FP8) and measure mean drift accumulation across layers to verify the zero-mean assumption holds under realistic conditions
2. **Hardware-Agnostic Benchmarking**: Compare performance across multiple hardware platforms (CPU, GPU, TPU, specialized ML accelerators) with varying memory bandwidth and compute characteristics to identify where the efficiency gains materialize
3. **Architecture Generalization**: Test the equivalence and efficiency claims on encoder-decoder transformer variants (like T5) and in settings with different normalization patterns (Post-LN, Pre-Norm) to assess broader applicability