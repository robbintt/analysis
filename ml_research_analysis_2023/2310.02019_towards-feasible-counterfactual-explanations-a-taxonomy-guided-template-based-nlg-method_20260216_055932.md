---
ver: rpa2
title: 'Towards Feasible Counterfactual Explanations: A Taxonomy Guided Template-based
  NLG Method'
arxiv_id: '2310.02019'
source_url: https://arxiv.org/abs/2310.02019
tags:
- feature
- counterfactual
- n-xai
- user
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of generating feasible and actionable
  counterfactual explanations in natural language. It introduces a taxonomy-guided
  template-based Natural Language Generation (NLG) method, n-XAIT, which categorizes
  features based on their mutability and employs appropriate sentence constructs for
  explanation.
---

# Towards Feasible Counterfactual Explanations: A Taxonomy Guided Template-based NLG Method

## Quick Facts
- arXiv ID: 2310.02019
- Source URL: https://arxiv.org/abs/2310.02019
- Reference count: 28
- Primary result: Taxonomy-guided template-based NLG method significantly improves feasibility and acceptability of counterfactual explanations across three domains.

## Executive Summary
This paper addresses the challenge of generating feasible and actionable counterfactual explanations in natural language for black-box AI models. The authors propose n-XAIT, a taxonomy-guided template-based Natural Language Generation (NLG) method that categorizes features based on their mutability and employs appropriate sentence constructs for explanation. The method was evaluated through a user study across three domains (health, education, finance), comparing it to a baseline approach. Results demonstrate significant improvements in explanation feasibility and acceptability, with n-XAIT receiving higher user ratings across all dimensions including articulation, acceptability, feasibility, and sensitivity.

## Method Summary
The proposed method, n-XAIT, is a three-stage template-based NLG pipeline that generates counterfactual explanations. It begins with a Feature Actionability Taxonomy (FAT) that categorizes features into four groups: Mutable Directly, Mutable Indirectly, Immutable Non-sensitive, and Immutable Sensitive. The pipeline then selects appropriate sentence templates based on these categories, fills template slots with query and counterfactual values, and orders the resulting sentences by FAT category and SHAP feature importance weights. This approach is designed to be compatible with existing counterfactual explainers like DICE, NICE, and DisCERN, and was evaluated through a user study comparing it to a baseline method across three domains.

## Key Results
- n-XAIT significantly improved explanation feasibility across all three domains (health, education, finance)
- The method enhanced explanation acceptability in two of three domains
- User ratings for n-XAIT were higher across all dimensions: articulation, acceptability, feasibility, and sensitivity
- The taxonomy-guided approach outperformed baseline methods in generating actionable counterfactual recommendations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Taxonomy-guided template selection improves the alignment between counterfactual recommendations and user-perceived actionability
- Mechanism: FAT categorizes features into Mutable Directly, Mutable Indirectly, Immutable Non-sensitive, and Immutable Sensitive. For each category, the system selects a sentence template that uses content and structure themes appropriate to that category. This ensures that mutable features are described with actionable verbs and counterfactual values, while immutable features are presented as factual explanations or sensitivity warnings.
- Core assumption: The template chosen for each category matches the user's mental model of what is actionable and appropriate
- Evidence anchors:
  - [abstract] "Using insights from the user study and our taxonomy, we created a generalisable template-based NLG method compatible with existing explainers like DICE, NICE, and DisCERN, to produce counterfactuals that address the aforementioned limitations of existing approaches."
  - [section] "FAT was defined using a data-driven methodology that relied on examining features extracted from six datasets...The resulting categories and distributions are summarised in Table 3."
  - [corpus] Found 25 related papers; 0.42 average FMR, but no direct citation overlap. Corpus does not contradict the claim
- Break condition: If the taxonomy categories do not map cleanly to user expectations (e.g., a feature considered actionable by users is categorized as immutable), the alignment will break and user satisfaction will drop

### Mechanism 2
- Claim: Sentence templates that use counterfactual values (rather than query values) for mutable features increase the plausibility of explanations
- Mechanism: The NLG pipeline fills template slots with counterfactual values and feature names (C2, C3 themes from user study), avoiding mention of original query values. This creates a forward-looking, action-oriented statement that focuses on what to do rather than what is wrong
- Core assumption: Users find explanations that only mention desired end states more motivating than those that emphasize the gap between current and desired states
- Evidence anchors:
  - [section] "Results show that actionable changes are more frequently referenced with counterfactual values and feature names, rather than using query values"
  - [section] "Cohort2 outperformed Cohort1 suggesting that basic Natural-XAI formats of n-XAI B1 and n-XAI B2 had influenced the cohort to write more grammatically correct explanations"
  - [corpus] Weak overlap with corpus; no direct evidence
- Break condition: If users require context about the magnitude of change to feel confident, omitting query values may reduce understanding

### Mechanism 3
- Claim: Grouping mutable features by actionability verb before ordering by feature importance increases user preference for the explanation structure
- Mechanism: The template pipeline first clusters sentences by the FAT category, then orders features within each category using SHAP importance. This creates blocks of related actions (e.g., all "increase" actions together), which the user study showed was preferred over pure SHAP ordering
- Core assumption: Users process action blocks more easily than interleaved features
- Evidence anchors:
  - [section] "The analysis indicated that Cohort2 preferred grouping their text by actionability verbs before ordering features by SHAP order"
  - [abstract] "Our findings show that the taxonomy-guided Natural-XAI approach (n-XAIT ) received higher user ratings across all dimensions..."
  - [corpus] No corpus evidence directly supporting this claim
- Break condition: If the number of actions is small, grouping by verb may add unnecessary complexity and reduce clarity

## Foundational Learning

- Concept: Counterfactual explanations (cf-XAI)
  - Why needed here: The paper builds a method to improve the presentation of cf-XAI; understanding what cf-XAI is and its goals is foundational
  - Quick check question: What is the primary goal of a counterfactual explanation in XAI?
    - Answer: To show the smallest change in feature values needed to change a model's prediction to the desired outcome

- Concept: Feature actionability taxonomy
  - Why needed here: The FAT drives template selection; without it the method collapses to generic NLG
  - Quick check question: What are the four categories in the FAT and how do they differ?
    - Answer: Mutable Directly (can be changed directly), Mutable Indirectly (change via another feature), Immutable Non-sensitive (cannot be changed, no ethical concern), Immutable Sensitive (cannot be changed, ethical concern)

- Concept: Template-based NLG pipeline stages
  - Why needed here: The method is explicitly a three-stage pipeline; understanding each stage is critical for implementation
  - Quick check question: What are the three stages of the NLG pipeline in n-XAIT and their roles?
    - Answer: Sentence planning (choose template based on FAT), surface realisation (fill template slots with values and verbs), discourse planning (order sentences by FAT category and SHAP importance)

## Architecture Onboarding

- Component map:
  - Input: (query, counterfactual) feature pairs from any cf-XAI system
  - FAT categorizer: assigns each feature to one of four FAT categories
  - Template selector: picks a sentence template per feature based on FAT category and content/structure themes
  - Slot filler: substitutes feature name, query value, counterfactual value, and action verb into the template
  - Discourse orderer: groups by FAT category, sorts within by SHAP, concatenates sentences
  - Output: Natural-XAI explanation string

- Critical path:
  1. Receive (query, counterfactual) from cf-XAI system
  2. For each feature: categorize via FAT
  3. Select template and fill slots
  4. Order sentences by FAT group and SHAP
  5. Return explanation

- Design tradeoffs:
  - FAT vs. on-the-fly feasibility check: FAT is static and dataset-driven; feasibility could be dynamic but requires runtime knowledge
  - Template variety vs. simplicity: More templates allow finer nuance but increase maintenance; current design uses 8 templates for 4 FAT categories
  - SHAP ordering vs. user preference: SHAP is automatic; user study suggests grouping by verb is preferred, so the design trades pure SHAP for hybrid ordering

- Failure signatures:
  - All features categorized as Immutable Sensitive → no actionable recommendations
  - FAT categorization mismatches user perception → low feasibility scores
  - Slot filling fails (missing values) → incomplete sentences or errors
  - Discourse ordering yields long, unstructured blocks → poor readability

- First 3 experiments:
  1. Unit test FAT categorizer on a small hand-crafted feature set to verify correct mapping
  2. Integration test template pipeline with a mock cf-XAI output to ensure templates render correctly
  3. A/B test SHAP vs. verb-group ordering on a small user sample to confirm preference before full deployment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed taxonomy-based approach be extended to handle features with complex, non-linear relationships?
- Basis in paper: [inferred] The paper mentions that the current taxonomy is based on analyzing features from six datasets, but does not explicitly address how to handle complex, non-linear relationships between features
- Why unresolved: The paper focuses on the development and evaluation of the taxonomy-based approach, but does not delve into the complexities of handling non-linear relationships between features
- What evidence would resolve it: Empirical studies demonstrating the effectiveness of the taxonomy-based approach in handling features with complex, non-linear relationships, and comparisons with existing methods

### Open Question 2
- Question: How can the proposed approach be adapted to provide personalized explanations based on individual user preferences and backgrounds?
- Basis in paper: [explicit] The paper mentions that user-dependent nature of applying taxonomy definitions and the need for an interactive iterative process with the user to enable adaptive feature actionability categories based on individual circumstances
- Why unresolved: The paper acknowledges the need for personalization but does not provide a concrete method for achieving it
- What evidence would resolve it: A framework or algorithm for adapting the proposed approach to provide personalized explanations based on individual user preferences and backgrounds, along with empirical evaluations demonstrating its effectiveness

### Open Question 3
- Question: How can the proposed approach be extended to handle real-time counterfactual explanations in dynamic environments?
- Basis in paper: [inferred] The paper does not explicitly address the issue of real-time counterfactual explanations in dynamic environments, but it is a relevant and important aspect of XAI systems
- Why unresolved: The paper focuses on the development and evaluation of the taxonomy-based approach, but does not consider the challenges of real-time counterfactual explanations in dynamic environments
- What evidence would resolve it: A method for extending the proposed approach to handle real-time counterfactual explanations in dynamic environments, along with empirical evaluations demonstrating its effectiveness and scalability

## Limitations

- The method's effectiveness depends heavily on the accuracy of the Feature Actionability Taxonomy (FAT) categorization, which may not generalize to all domains
- The template-based approach may sacrifice some flexibility in handling complex or nuanced counterfactual scenarios
- The user study was limited to three specific domains and may not capture the full range of user needs across different applications

## Confidence

- High confidence: The core mechanism of using FAT to guide template selection and improve explanation feasibility is well-supported by user study results across all three domains
- Medium confidence: The specific template structures and FAT categories will generalize well to new domains, as this was validated on a limited set of datasets
- Low confidence: The relative importance of template structure versus content themes in driving user preference, as the user study design confounds these factors

## Next Checks

1. **Cross-domain validation**: Apply n-XAIT to 2-3 additional domains not represented in the original study to test FAT generalizability and identify any domain-specific template adjustments needed
2. **A/B testing with professional users**: Conduct a controlled experiment with domain experts (e.g., financial advisors, medical professionals) to validate that feasibility improvements translate to real-world decision-making contexts
3. **Dynamic FAT refinement**: Implement a feedback mechanism to update FAT categories based on user interaction data, testing whether adaptive categorization improves explanation quality over time compared to the static taxonomy