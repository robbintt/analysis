---
ver: rpa2
title: A Systematic Review for Transformer-based Long-term Series Forecasting
arxiv_id: '2310.20218'
source_url: https://arxiv.org/abs/2310.20218
tags:
- forecasting
- time
- transformer
- series
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive systematic review of transformer-based
  architectures for long-term time series forecasting (LTSF). The authors examine
  various transformer variants designed to address key challenges in LTSF including
  computational complexity, long-term dependencies, and non-stationarity.
---

# A Systematic Review for Transformer-based Long-term Series Forecasting

## Quick Facts
- **arXiv ID**: 2310.20218
- **Source URL**: https://arxiv.org/abs/2310.20218
- **Reference count**: 0
- **Primary result**: Comprehensive systematic review of transformer architectures for long-term time series forecasting covering recent variants, training strategies, and evaluation metrics

## Executive Summary
This paper presents a systematic review of transformer-based architectures for long-term time series forecasting (LTSF), examining how transformers address key challenges including computational complexity, long-term dependencies, and non-stationarity. The authors analyze various transformer variants such as LogSparse, Informer, Autoformer, Pyraformer, and FEDformer, along with specialized models for domains like finance, energy, and transportation. The review covers training strategies, preprocessing techniques, and evaluation metrics while highlighting both the effectiveness of transformers for LTSF and ongoing challenges with computational complexity and data requirements.

## Method Summary
The review systematically examines transformer-based architectures for LTSF by analyzing their architectural components, training procedures, and performance characteristics. The method involves categorizing transformer variants based on their attention mechanisms (sparse attention, convolutional self-attention, auto-correlation), positional encoding schemes, and domain-specific adaptations. The review evaluates these models across multiple time series domains including finance, energy, transportation, meteorology, and medicine, using both scale-dependent metrics (MSE, RMSE, MAE) and scale-independent measures (MAPE, MASE).

## Key Results
- Transformers effectively capture long-term dependencies through self-attention mechanisms, avoiding sequential processing bottlenecks of traditional RNNs
- Transformer variants reduce computational complexity from O(L²) to O(L log L) or O(L) through specialized attention mechanisms like ProbSparse and convolutional self-attention
- Multi-head attention enables simultaneous learning of different temporal patterns (seasonal, trend, local fluctuations) improving forecasting accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers effectively capture long-term dependencies in time series data due to their self-attention mechanism
- Mechanism: Self-attention computes query-key-value interactions for each position, allowing direct modeling of relationships across all time steps
- Core assumption: Quadratic complexity is manageable or can be mitigated through architectural optimizations
- Evidence anchors: Abstract states transformers "extract semantic correlations among elements within a long sequence"; self-attention described as allowing "adaptive learning of short-term and long-term dependencies"
- Break condition: When sequence lengths become extremely large (thousands of time steps) or when temporal dependencies are actually short-range

### Mechanism 2
- Claim: Transformer variants reduce computational complexity while maintaining accuracy through specialized attention mechanisms
- Mechanism: Variants like Informer and LogSparse introduce probabilistic sparse attention or convolutional mechanisms to reduce complexity from O(L²) to O(L log L)
- Core assumption: Sparse or approximated attention retains sufficient information about temporal dependencies
- Evidence anchors: Informer uses "ProbSparse self-attention mechanism...reducing computational complexity to O(L(logL))"; LogSparse "proposed convolutional self-attention...reducing memory utilization from O(L²) to O(L(logL))"
- Break condition: When sparsity patterns miss critical temporal dependencies or approximation error accumulates over long horizons

### Mechanism 3
- Claim: Multi-head attention allows learning different types of temporal patterns simultaneously
- Mechanism: Each attention head learns different relationships (seasonal patterns, trend components, local fluctuations)
- Core assumption: Time series contains multiple types of patterns benefiting from parallel attention mechanisms
- Evidence anchors: Multi-head attention described as "processing multiple groups of input sequences...spliced together to perform linear transformation"
- Break condition: When time series is relatively simple with uniform patterns or when head count is poorly tuned

## Foundational Learning

- **Concept: Attention mechanisms** - Why needed: Fundamental to understanding how transformers process sequences and why they're effective for LTSF; Quick check: What are the three matrices (Q, K, V) in self-attention, and how are they used to compute attention weights?
- **Concept: Sequence modeling challenges** - Why needed: Recognizing limitations of traditional approaches (RNNs, statistical models) and why transformers address these challenges; Quick check: What are main limitations of RNNs for LTSF, and how do transformers address these limitations?
- **Concept: Time series decomposition** - Why needed: Understanding seasonal-trend decomposition important as many variants incorporate this for better forecasting; Quick check: What are typical components in time series decomposition, and why is this decomposition useful for forecasting?

## Architecture Onboarding

- **Component map**: Input → Positional encoding → Encoder layers (self-attention + FFN) → Decoder layers (masked self-attention + encoder-decoder attention + FFN) → Output projection
- **Critical path**: Input → Positional encoding → Encoder layers (self-attention + FFN) → Decoder layers (masked self-attention + encoder-decoder attention + FFN) → Output projection
- **Design tradeoffs**: Complexity vs. accuracy (simpler models may be more efficient but less accurate), Global vs. local attention (capturing long-range vs. short-range dependencies), Pre-training vs. task-specific training (transfer learning benefits vs. task adaptation)
- **Failure signatures**: Poor long-term forecasting accuracy (attention may not capture distant dependencies), High computational cost (inefficient attention mechanisms), Overfitting on small datasets (too many parameters for limited data)
- **First 3 experiments**:
  1. Implement basic transformer encoder-only model for univariate time series forecasting on simple dataset (electricity consumption) to validate core mechanism
  2. Compare different attention variants (full vs. sparse attention) on moderate-length time series to assess computational efficiency vs. accuracy tradeoffs
  3. Test impact of positional encoding schemes (sinusoidal vs. learned) on forecasting accuracy for periodic time series data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can transformer-based models be effectively integrated with large language models (LLMs) for time series forecasting, particularly for capturing complex temporal patterns and providing interpretable explanations?
- Basis in paper: The paper discusses recent efforts to explore integration of LLMs in time series forecasting and highlights potential of LLMs to generate forecasts while offering human-readable explanations
- Why unresolved: While acknowledging LLM potential, paper does not provide specific details on effective integration or addressing challenges of capturing complex temporal patterns and providing interpretable explanations
- What evidence would resolve it: Research demonstrating successful integration of transformer-based models with LLMs for time series forecasting, along with quantitative and qualitative analysis of performance and interpretability

### Open Question 2
- Question: What are the most effective strategies for handling long-term dependencies and non-stationarity in time series data when using transformer-based models?
- Basis in paper: The paper discusses challenges of long-term dependencies and non-stationarity and mentions various transformer variants designed to address these issues
- Why unresolved: While introducing variants that aim to handle these issues, paper does not provide comprehensive comparison or evaluation of their effectiveness
- What evidence would resolve it: Empirical studies comparing performance of different transformer variants on wide range of time series datasets, along with detailed analysis of strengths and weaknesses in handling long-term dependencies and non-stationarity

### Open Question 3
- Question: How can transformer-based models be optimized for computational efficiency and memory usage when dealing with extremely long time series data?
- Basis in paper: The paper discusses computational complexity and memory usage challenges and mentions various techniques proposed to address these issues
- Why unresolved: While introducing several techniques aimed at improving efficiency and memory usage, paper does not provide thorough analysis of their relative effectiveness
- What evidence would resolve it: Empirical studies comparing computational efficiency and memory usage of different transformer variants and optimization techniques on extremely long time series datasets, along with detailed analysis of trade-offs and limitations

## Limitations
- Lack of direct corpus evidence supporting specific effectiveness claims about transformer variants
- Computational complexity claims (e.g., O(L log L) for sparse attention) are asserted but not empirically verified across different time series characteristics
- Review does not address potential domain-specific limitations where transformers may underperform traditional statistical methods

## Confidence
- **Mechanism 1 (Self-attention effectiveness)**: Medium confidence - theoretically well-established but lacks comprehensive empirical validation across diverse time series types
- **Mechanism 2 (Computational complexity reduction)**: Medium confidence - algorithmic improvements documented but real-world performance gains depend heavily on implementation details
- **Mechanism 3 (Multi-head attention benefits)**: Low confidence - theoretical rationale is sound but empirical evidence for time series specifically is sparse

## Next Checks
1. **Cross-domain benchmarking**: Implement and compare multiple transformer variants (Informer, Autoformer, LogSparse) on at least five diverse time series datasets (energy, finance, transportation, meteorology, and medical) to validate generalization claims across different data characteristics

2. **Complexity-performance tradeoff analysis**: Conduct controlled experiments varying sequence lengths from 100 to 10,000 time steps, measuring both computational efficiency (training time, memory usage) and forecasting accuracy (MASE, RMSE) to empirically validate complexity claims

3. **Attention mechanism ablation study**: Systematically disable or modify attention components (sparse patterns, multi-head configurations, positional encodings) in a transformer forecasting model to isolate which architectural elements contribute most significantly to performance improvements