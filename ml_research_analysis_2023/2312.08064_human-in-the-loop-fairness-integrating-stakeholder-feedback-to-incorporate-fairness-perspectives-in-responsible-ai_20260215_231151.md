---
ver: rpa2
title: 'Human-in-the-loop Fairness: Integrating Stakeholder Feedback to Incorporate
  Fairness Perspectives in Responsible AI'
arxiv_id: '2312.08064'
source_url: https://arxiv.org/abs/2312.08064
tags:
- fairness
- feedback
- participants
- perc
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to integrate user feedback on AI fairness
  into an XGBoost model and the effects of doing so. In a study with 58 lay users,
  participants provided feedback on loan application decisions marked as fair or unfair,
  and adjusted feature weights.
---

# Human-in-the-loop Fairness: Integrating Stakeholder Feedback to Incorporate Fairness Perspectives in Responsible AI

## Quick Facts
- arXiv ID: 2312.08064
- Source URL: https://arxiv.org/abs/2312.08064
- Reference count: 26
- Key outcome: Integrating lay user feedback into XGBoost via weight adjustments and label flipping can improve group fairness metrics when aggregated, but individual feedback often worsens fairness for certain attributes

## Executive Summary
This paper investigates how to integrate user feedback on AI fairness into an XGBoost model and the effects of doing so. In a study with 58 lay users, participants provided feedback on loan application decisions marked as fair or unfair, and adjusted feature weights. The authors then conducted offline experiments to evaluate the effects on accuracy and multiple group and individual fairness metrics when integrating user feedback. They found that when all participants' feedback was combined, fairness metrics like demographic parity ratio and equal opportunity difference improved for protected attributes. However, when analyzing individual participants' feedback, most actually made fairness worse for certain attributes. The paper provides baseline results for integrating user feedback in XGBoost and highlights challenges in leveraging fairness feedback from lay users.

## Method Summary
The study used the Home Credit dataset with 120 original attributes reduced to 49 features including 3 protected attributes (gender, marital status, age). 58 lay users provided feedback on 1,000 loan application predictions through a UI where they could mark decisions as fair/unfair and adjust feature weights. Three feedback integration approaches were tested: Labels Fair+Unfair, Labels Unfair, and Labels+Weights Fair+Unfair, in both OneOff and IML settings. XGBoost was retrained with adjusted sample weights and flipped labels for "unfair" instances. Fairness metrics including DPR, AOD, EOD, CDD, PPD, consistency, and Theil index were computed on baseline and retrained models.

## Key Results
- Aggregated user feedback improved group fairness metrics (DPR, AOD) across protected attributes when all participants' feedback was combined
- Individual user feedback often deteriorated fairness metrics for certain protected attributes, with most participants worsening fairness in some cases
- Less than half of participants interacted with protected attributes, with many focusing on non-protected attributes like FLAG OWN CAR and YEARS EMPLOYED

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating lay user feedback into XGBoost via weight adjustments and label flipping can improve group fairness metrics (DPR, AOD) across protected attributes when feedback from all participants is aggregated.
- Mechanism: Feedback instances labeled as "unfair" are integrated into the training set after flipping their target values. Feature weights are adjusted according to user-provided feedback weights. XGBoost retraining is performed with adjusted sample weights to balance feedback and original training data.
- Core assumption: Aggregated feedback from multiple users captures a representative view of fairness that aligns with fairness metrics, and the XGBoost model can learn from these adjustments without severe degradation in accuracy.
- Evidence anchors:
  - [abstract] "when all participants' feedback was combined, fairness metrics like demographic parity ratio and equal opportunity difference improved for protected attributes."
  - [section] "We found that using OneOff Labels Unfair obtains good improvements for all attributes on specific fairness metrics, DPR and AOD."
- Break condition: If individual user feedback consistently worsens fairness metrics, or if accuracy drops below an acceptable threshold.

### Mechanism 2
- Claim: Lay users' feedback, when analyzed individually, often deteriorates fairness metrics for certain protected attributes, indicating conflicting fairness notions.
- Mechanism: Each participant's feedback is integrated separately into individual XGBoost models, allowing analysis of feedback effects on fairness metrics per user. Clustering analysis based on metric changes reveals patterns of fairness perception.
- Core assumption: Individual feedback can be meaningfully analyzed to reveal distinct fairness notions, and clustering can identify groups of users with similar fairness perspectives.
- Evidence anchors:
  - [abstract] "when analyzing individual participants' feedback, most actually made fairness worse for certain attributes."
  - [section] "Participants in cluster 0, the largest cluster, improve DPR and AOD on gender by a large amount, while there are also a few who improve DPR and AOD on marital status and age."
- Break condition: If clustering does not reveal meaningful patterns, or if individual feedback consistently improves fairness metrics.

### Mechanism 3
- Claim: Participants often interact with non-protected attributes, suggesting they may use fairness notions beyond standard group fairness metrics.
- Mechanism: Analysis of user interactions with attributes (feature weight adjustments) reveals which attributes participants consider important for fairness, potentially extending fairness metrics beyond protected attributes.
- Core assumption: User interactions with attributes reflect their fairness notions, and these notions can be used to inform or extend existing fairness metrics.
- Evidence anchors:
  - [section] "We found that less than half of the participants interacted with the protected attributes, while there were other, non-protected attributes participants might have deemed as irrelevant or unfairly used."
  - [section] "Similar to previous work, participants might have considered affordability as a fairness notion."
- Break condition: If attribute interactions do not correlate with fairness metric improvements, or if user notions of fairness cannot be quantified.

## Foundational Learning

- Concept: Fairness metrics (group, subgroup, individual)
  - Why needed here: Understanding the different types of fairness metrics is crucial for evaluating the impact of user feedback on model fairness and for selecting appropriate metrics for integration.
  - Quick check question: What is the difference between demographic parity ratio and equal opportunity difference?

- Concept: Interactive Machine Learning (IML) and human-in-the-loop approaches
  - Why needed here: The study employs an IML framework where users provide feedback on model predictions, requiring understanding of how to integrate this feedback into model retraining.
  - Quick check question: How does interactive machine learning differ from traditional machine learning in terms of user involvement?

- Concept: XGBoost model and its feature weight mechanism
  - Why needed here: The study uses XGBoost as the underlying model, and understanding how feature weights influence predictions is essential for interpreting user feedback on feature importance.
  - Quick check question: How do feature weights in XGBoost influence the model's predictions?

## Architecture Onboarding

- Component map:
  User Interface -> Data Preprocessing -> XGBoost Model -> Feedback Integration -> Fairness Evaluation

- Critical path:
  1. User interacts with the UI to provide feedback on loan application decisions.
  2. Feedback is logged and preprocessed for integration.
  3. XGBoost model is retrained with adjusted feature weights and flipped labels based on user feedback.
  4. Fairness metrics are computed to evaluate the impact of feedback integration.

- Design tradeoffs:
  - Accuracy vs. Fairness: Integrating user feedback may improve fairness metrics but often leads to a drop in accuracy.
  - Individual vs. Aggregated Feedback: Analyzing individual feedback reveals conflicting fairness notions, while aggregated feedback may improve overall fairness.
  - Protected vs. Non-protected Attributes: Users may focus on non-protected attributes, suggesting fairness notions beyond standard metrics.

- Failure signatures:
  - Accuracy drops below an acceptable threshold after feedback integration.
  - Fairness metrics deteriorate for certain protected attributes.
  - Clustering analysis does not reveal meaningful patterns in user feedback.

- First 3 experiments:
  1. Integrate feedback from all participants and evaluate the impact on accuracy and fairness metrics.
  2. Analyze individual participant feedback and cluster users based on their effects on fairness metrics.
  3. Investigate user interactions with non-protected attributes to understand alternative fairness notions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective approaches for integrating user feedback into AI fairness metrics that go beyond the protected attributes typically used in group fairness definitions?
- Basis in paper: [explicit] The paper notes that participants interacted with many non-protected attributes like FLAG OWN CAR and YEARS EMPLOYED, suggesting they may have different notions of fairness than standard metrics capture. The authors suggest leveraging user feedback to extend fairness metrics beyond protected attributes.
- Why unresolved: The paper found that participants' feedback improved group fairness metrics for protected attributes when they marked predictions as unfair, but this may not capture all the fairness notions participants had. The authors suggest this is an area for future investigation.
- What evidence would resolve it: Experiments that collect user feedback on a wider range of attributes and test different fairness metrics that incorporate those attributes. Analysis of which attributes participants focus on and how feedback on those attributes affects model fairness.

### Open Question 2
- Question: How can we develop methods to identify and filter out "poor" user feedback that decreases model fairness when integrating feedback into AI models?
- Basis in paper: [explicit] The paper notes that some participants' feedback actually made fairness worse for certain attributes. The authors suggest developing a mechanism to identify and deal with this "poor" feedback.
- Why unresolved: The paper does not provide a solution for how to identify or filter out poor feedback. This is left as an open question for future research.
- What evidence would resolve it: Experiments that test different methods for identifying poor feedback, such as analyzing the effect of each participant's feedback on fairness metrics. Development of algorithms that can automatically filter out poor feedback while retaining useful feedback.

### Open Question 3
- Question: How does the effect of user feedback on model fairness differ between one-shot retraining from all participants versus incremental retraining using each participant's feedback?
- Basis in paper: [explicit] The paper compared one-shot retraining (integrating all feedback at once) to incremental retraining (integrating each participant's feedback one at a time). They found differences in the effect on fairness metrics between the two approaches.
- Why unresolved: The paper does not fully explain why these differences occurred or which approach is better. This is left as an open question for future research.
- What evidence would resolve it: More experiments comparing the two approaches on different datasets and with different numbers of participants. Analysis of the patterns of feedback and their effects on fairness metrics in each approach.

## Limitations
- Participant sample (58 lay users) may not represent diverse stakeholder perspectives on fairness, particularly from affected communities
- The Home Credit dataset's specific characteristics and the 1,000-instance subset used for feedback may constrain generalizability to other domains
- The weight adjustment mechanism (setting non-adjusted feature weights to 1.0) represents an arbitrary choice that may not reflect actual user intent
- The study does not validate whether participants' fairness notions align with affected communities' actual preferences or legal/ethical standards

## Confidence
- High Confidence: The empirical finding that aggregated user feedback improves group fairness metrics (DPR, AOD) while individual feedback often worsens fairness for specific attributes
- Medium Confidence: The observation that users frequently interact with non-protected attributes, suggesting alternative fairness notions
- Low Confidence: Claims about the mechanism by which user feedback improves fairness, given the arbitrary weight adjustment implementation

## Next Checks
1. **Sample Representativeness Test:** Conduct a demographic analysis of participants to assess whether the sample represents affected communities and test if fairness improvements persist with a more diverse participant pool
2. **Cross-Dataset Validation:** Replicate the feedback integration approach on a different lending dataset (e.g., FICO) to verify generalizability of the fairness improvements
3. **Longitudinal Feedback Stability:** Implement a multi-session IML study to assess whether users' fairness notions and feedback patterns remain consistent over time or with repeated exposure to model decisions