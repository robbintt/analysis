---
ver: rpa2
title: 'Exploring Methods for Cross-lingual Text Style Transfer: The Case of Text
  Detoxification'
arxiv_id: '2311.13937'
source_url: https://arxiv.org/abs/2311.13937
tags:
- detoxification
- language
- translation
- text
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a comprehensive study of cross-lingual text
  detoxification methods, where the goal is to transfer the ability to convert toxic
  text into neutral text from one language to another using parallel corpora. We propose
  several approaches including backtranslation, training data translation, multitask
  learning, and adapter training.
---

# Exploring Methods for Cross-lingual Text Style Transfer: The Case of Text Detoxification

## Quick Facts
- arXiv ID: 2311.13937
- Source URL: https://arxiv.org/abs/2311.13937
- Authors: 
- Reference count: 27
- This work presents a comprehensive study of cross-lingual text detoxification methods, where the goal is to transfer the ability to convert toxic text into neutral text from one language to another using parallel corpora.

## Executive Summary
This paper explores cross-lingual text detoxification - transferring the ability to convert toxic text into neutral text from one language to another. The authors propose several approaches including backtranslation, training data translation, multitask learning, and adapter training. They also introduce a new task of simultaneous detoxification and translation, providing strong baselines for it. To evaluate their methods, they introduce new automatic detoxification metrics with higher correlations to human judgments than previous benchmarks.

## Method Summary
The paper presents four main approaches for cross-lingual text detoxification: backtranslation (translating input to a language with detox data, detoxifying, then translating back), training data translation (translating monolingual detox corpora and training on them), multitask learning (combining detoxification, translation, and paraphrasing tasks), and adapter training (inserting small adapter layers into multilingual models and training only those). The authors also introduce a simultaneous detoxification and translation task using synthetic cross-lingual corpora. They evaluate their methods using new automatic metrics (STA, SIM, FL) and manual evaluation for Russian detoxification.

## Key Results
- Backtranslation achieves the highest performance across automatic metrics
- Adapter training is the most resource-efficient method while maintaining strong performance
- Manual evaluation confirms adapter training is the best approach for transferring detoxification knowledge from English to Russian
- New automatic detoxification metrics show higher correlation with human judgments than previous benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Backtranslation leverages monolingual detox models via bilingual bridging
- Backtranslation achieves highest performance by using existing monolingual detox models through translation pipelines
- Core assumption: Translation preserves toxic elements and monolingual detox models generalize to translated content
- Break condition: Translation fails to preserve subtle toxic cues or monolingual models don't generalize

### Mechanism 2: Adapter training freezes multilingual base and learns task-specific parameters
- Adapter training is most resource-efficient by inserting small adapter layers and fine-tuning only those
- Core assumption: Multilingual base encodes language-agnostic semantic knowledge that adapters can build upon
- Break condition: Base lacks sufficient target language representation or adapters overfit

### Mechanism 3: Synthetic cross-lingual corpora enable simultaneous detox and translation
- Simultaneous detox+translation achieved by training on NMT-generated parallel toxic-neutral pairs
- Core assumption: NMT preserves toxic-neutral correspondence for model learning
- Break condition: Synthetic data introduces noise or mismatches between toxic and neutral forms

## Foundational Learning

- **Multilingual transformer architectures (mT5, mBART, M2M100)**: Serve as base for cross-lingual detoxification experiments; Quick check: What are key differences between mT5, mBART, and M2M100 regarding pre-training tasks and language coverage?
- **Adapter layers and parameter-efficient fine-tuning**: Central to adapter training approach; Quick check: How does inserting adapter between encoder and decoder differ from fine-tuning entire model?
- **Automatic evaluation metrics for style transfer (STA, SIM, FL)**: Used throughout to rank and compare models; Quick check: How is joint metric J calculated from STA, SIM, and FL, and why is each component important?

## Architecture Onboarding

- **Component map**: Multilingual models (mT5, mBART, M2M100) -> Detox models (ruT5-detox, BART-detox) -> Translation models (Helsinki OPUS-MT, YandexTranslate, FSMT, Google Translate) -> Evaluation (toxicity classifiers, paraphrase classifiers, fluency classifiers) -> Adapter layers
- **Critical path**: 1. Prepare datasets (ParaDetox, translations, paraphrasing corpora) 2. Load base multilingual model 3. For adapter: insert adapter, train on monolingual detox data 4. For backtranslation: translation→detox→translation pipeline 5. Evaluate using STA, SIM, FL, and J
- **Design tradeoffs**: Adapter training (lower resource use, potentially lower STA) vs Backtranslation (higher STA, requires three inference steps) vs Synthetic data (enables end-to-end models, quality depends on NMT)
- **Failure signatures**: Low STA (model fails to neutralize), Low SIM (content lost/altered), Low FL (ungrammatical output)
- **First 3 experiments**: 1. Adapter training on English ParaDetox with mBART, evaluate on Russian test set 2. Backtranslation using ruT5-detox + YandexTranslate on Russian toxic input 3. Simultaneous detox+translation with mT5 on synthetic cross-lingual data

## Open Questions the Paper Calls Out

- **Open Question 1**: How does effectiveness vary across different language pairs beyond English and Russian?
- **Open Question 2**: What are specific types of toxic expressions most challenging to translate and detoxify across languages?
- **Open Question 3**: How do proposed automatic evaluation metrics correlate with human judgments across different languages and domains?
- **Open Question 4**: What is impact of different types of paraphrasing and translation corpora on multilingual detoxification model performance?
- **Open Question 5**: How does size and quality of parallel detoxification corpus affect cross-lingual transfer performance?

## Limitations

- Evaluation relies heavily on automatic metrics without extensive human validation across all methods
- Adapter training results only tested for English-to-Russian transfer, limiting generalization claims
- Synthetic cross-lingual approach lacks evaluation on truly unseen parallel toxic-neutral pairs

## Confidence

- **High confidence**: Adapter training effectiveness (supported by manual evaluation results)
- **Medium confidence**: Backtranslation performance (strong automatic metrics but limited manual validation)
- **Medium confidence**: Synthetic cross-lingual approach (novelty established but limited evaluation scope)
- **Low confidence**: Generalization claims to other language pairs (only English-to-Russian validated)

## Next Checks

1. Conduct manual evaluation for backtranslation and multitask learning approaches to verify automatic metric correlations
2. Test adapter training framework on additional language pairs (e.g., English-to-Spanish, English-to-French)
3. Evaluate synthetic cross-lingual detoxification models on truly unseen parallel toxic-neutral pairs to assess real-world applicability