---
ver: rpa2
title: Enhancing Sharpness-Aware Optimization Through Variance Suppression
arxiv_id: '2309.15639'
source_url: https://arxiv.org/abs/2309.15639
tags:
- vasso
- adversary
- generalization
- variance
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a key limitation in Sharpness-Aware Minimization
  (SAM) called the "friendly adversary" problem, where the adversarial perturbation
  computed on one minibatch does not generalize well to other data, undermining SAM's
  theoretical benefits. To address this, the authors propose Variance-Suppressed Sharpness-Aware
  Optimization (VaSSO), which uses an exponentially moving average of gradients to
  compute a more stable adversarial perturbation.
---

# Enhancing Sharpness-Aware Optimization Through Variance Suppression

## Quick Facts
- arXiv ID: 2309.15639
- Source URL: https://arxiv.org/abs/2309.15639
- Reference count: 40
- Key outcome: VaSSO improves SAM's generalization by 0.2-0.5% accuracy through variance-suppressed adversarial perturbations

## Executive Summary
This paper addresses a critical limitation in Sharpness-Aware Minimization (SAM) called the "friendly adversary" problem, where adversarial perturbations computed from one minibatch fail to generalize to other data, undermining SAM's theoretical benefits. The authors propose Variance-Suppressed Sharpness-Aware Optimization (VaSSO), which uses exponentially moving average of gradients to compute more stable adversarial perturbations. Theoretical analysis shows VaSSO achieves O(√θ) improvement in stability and tighter convergence bounds compared to SAM. Empirically, VaSSO consistently improves generalization across vision and language tasks while maintaining computational efficiency.

## Method Summary
VaSSO modifies SAM's adversarial perturbation computation by replacing the direct gradient gt(xt) with an exponentially moving average (EMA) of gradients: dt = (1-θ)dt-1 + θgt(xt). The adversarial perturbation becomes ϵt = ρdt/∥dt∥, where ρ is the perturbation radius. This variance suppression stabilizes the adversary direction, making it more representative of global sharpness rather than batch-specific noise. The method maintains SAM's core philosophy of minimizing worst-case loss in a neighborhood while addressing the instability caused by gradient variance across minibatches.

## Key Results
- VaSSO achieves 0.2-0.5% accuracy improvements over SAM on CIFAR-10/100 and ImageNet classification tasks
- Hessian spectrum analysis shows flatter minima with VaSSO compared to SAM, confirming improved stability
- Label noise robustness improves with VaSSO, with gains increasing as noise levels rise (50% to 75% corruption)
- VaSSO is compatible with other SAM variants like GSAM and maintains computational efficiency
- Theoretical bounds show O(√θ) improvement in mean squared error compared to SAM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The "friendly adversary" in SAM arises because the adversarial perturbation computed from one minibatch's gradient does not generalize to other data, leading to unstable sharpness estimates.
- Mechanism: SAM's adversary ϵt is computed via stochastic linearization using gt(xt). When gradients from different minibatches are misaligned (⟨gt(xt), gB(xt)⟩ ≤ 0), the adversary fails to accurately capture global sharpness, causing poor generalization.
- Core assumption: Gradient variance across minibatches is significant enough to misalign adversaries.
- Evidence anchors: [abstract] "such an 'over-friendly adversary' can curtail the outmost level of generalization"; [section] "SAM's adversary only captures the sharpness for a particular minibatch of data, and can become a friend on other data samples"

### Mechanism 2
- Claim: VaSSO stabilizes the adversarial perturbation by suppressing gradient variance through exponentially moving average (EMA).
- Mechanism: Instead of using gt(xt) directly, VaSSO computes dt = (1-θ)dt-1 + θgt(xt), creating a smoothed gradient estimate that reduces variance and improves stability of the adversary direction.
- Core assumption: Gradient noise follows patterns that EMA can effectively suppress without losing critical information.
- Evidence anchors: [abstract] "VaSSO's provable stability safeguards its numerical improvement over SAM"; [section] "VaSSO guarantees that the MSE of dt is bounded by E[∥dt − ∇f (xt)∥2] ≤ θσ2 + O((1 − θ)2σ2/θ2√T)"

### Mechanism 3
- Claim: Stabilized adversaries in VaSSO provide more accurate sharpness estimates, leading to better generalization.
- Mechanism: The smoothed dt captures global sharpness more reliably than single-batch gt, ensuring the adversary reflects the true loss landscape geometry rather than batch-specific noise.
- Core assumption: The EMA of gradients preserves the direction information needed for sharpness estimation while reducing variance.
- Evidence anchors: [abstract] "VaSSO's provable stability safeguards its numerical improvement over SAM in model-agnostic tasks"; [section] "Because the global sharpness is not approached accurately, the friendly adversary precludes SAM from attaining its utmost generalizability"

## Foundational Learning

- Concept: Stochastic Linearization in Optimization
  - Why needed here: SAM and VaSSO both rely on linearizing the loss function to compute adversarial perturbations efficiently. Understanding this approximation is crucial for grasping why variance matters.
  - Quick check question: Why can't we compute the true maximum loss in the neighborhood directly in SAM?

- Concept: Exponential Moving Average (EMA) for Variance Reduction
  - Why needed here: VaSSO uses EMA to smooth gradient estimates. Understanding how EMA balances between responsiveness and noise reduction is key to choosing θ appropriately.
  - Quick check question: What happens to the EMA if θ → 0 versus θ → 1?

- Concept: Sharpness and Generalization Relationship
  - Why needed here: The entire paper builds on the premise that flatter minima generalize better. Understanding this connection helps appreciate why stabilizing sharpness estimates matters.
  - Quick check question: How does the Hessian spectrum relate to the sharpness of a minimum?

## Architecture Onboarding

- Component map: Base optimizer (SGD/AdamW) -> Gradient computation -> EMA update (dt) -> Adversary computation (ϵt) -> Gradient at perturbed point -> Parameter update
- Critical path: For each training iteration: (1) sample minibatch, (2) compute gradient, (3) update EMA slope dt, (4) compute adversary ϵt = ρdt/∥dt∥, (5) compute gradient at xt + ϵt, (6) update parameters. The EMA update is the critical path element that distinguishes VaSSO from SAM.
- Design tradeoffs: The θ hyperparameter trades off between variance suppression (smaller θ) and responsiveness to loss landscape changes (larger θ). Smaller θ provides better stability but may over-smooth; larger θ is more responsive but less stable. The perturbation radius ρ must be tuned carefully as it affects both the adversary's effectiveness and computational stability.
- Failure signatures: (1) Poor generalization despite training accuracy suggests the adversary isn't capturing true sharpness (check if θ is too large). (2) Training instability or divergence indicates the EMA is over-smoothing (check if θ is too small). (3) Minimal performance difference from SAM suggests the variance suppression isn't effective (check if θ is poorly tuned or gradient variance is already low).
- First 3 experiments:
  1. Run SAM and VaSSO with θ=0.9 on CIFAR-10 with ResNet-18, comparing test accuracy and Hessian spectrum to verify stability improvement.
  2. Test VaSSO with varying θ (0.2, 0.4, 0.9) on CIFAR-100 to observe the tradeoff between stability and responsiveness.
  3. Evaluate label noise robustness by training with 50% and 75% label corruption, comparing SAM vs VaSSO performance to validate theoretical predictions about stability benefits under noisy conditions.

## Open Questions the Paper Calls Out

- Question: Does the variance-suppressed linearization technique generalize beyond the SAM framework to other optimization methods that rely on adversarial perturbations or stochastic linearization?
- Basis in paper: [explicit] The authors mention in Section 3.2 that "VaSSO has the potential to boost the performance of other SAM family approaches by stabilizing their stochastic linearization through variance suppression" and specifically mention GSAM as a tested extension.
- Why unresolved: The paper only demonstrates integration with GSAM and leaves broader investigation for future work. The theoretical analysis is specific to SAM's adversarial perturbation framework.
- What evidence would resolve it: Systematic testing of VaSSO with other SAM variants (ASAM, FisherSAM, multi-step ascent SAM) and potentially non-SAM adversarial methods, showing consistent generalization improvements.

- Question: What is the optimal choice of θ in VaSSO across different network architectures and datasets, and can this be automated?
- Basis in paper: [explicit] The authors test θ = {0.4, 0.9} and find different values work best for different models (θ = 0.4 for ResNet-18 and WRN-28-10, θ = 0.9 for others), but don't provide a systematic approach for selecting θ.
- Why unresolved: The paper uses manual tuning of θ, which is impractical for large-scale deployment. The theoretical analysis (Theorem 3) only suggests θ should be "small but not too small" without providing specific guidance.
- What evidence would resolve it: Development of an adaptive or automated method for selecting θ based on training dynamics, or empirical studies showing consistent patterns in optimal θ selection across diverse architectures and tasks.

- Question: How does the friendly adversary problem manifest in optimization methods beyond SAM that use stochastic approximations of gradients or Hessians?
- Basis in paper: [explicit] The authors explicitly draw parallels between SAM's stochastic linearization and stochastic Frank-Wolfe methods in Appendix A, noting that both suffer from stability issues due to gradient variance.
- Why unresolved: While the paper identifies the friendly adversary issue in SAM and suggests it's related to gradient variance, it doesn't systematically explore whether similar problems exist in other optimization methods that rely on stochastic approximations.
- What evidence would resolve it: Theoretical analysis and empirical studies showing friendly adversary-like behavior in other optimization methods (e.g., SGD with momentum, adaptive methods, or second-order methods with stochastic Hessian approximations), along with potential solutions analogous to VaSSO's variance suppression.

## Limitations

- The optimal θ parameter selection appears problem-dependent, requiring manual tuning across different architectures and datasets
- The "friendly adversary" concept lacks direct experimental validation with measurements of gradient alignment between batches
- Computational overhead comparisons with other SAM variants are not explicitly addressed
- The paper focuses on standard CNNs and RNNs without exploring transformer-based architectures

## Confidence

- Theoretical analysis: High confidence - mathematically rigorous bounds with O(√θ) improvement
- Empirical validation: Medium confidence - consistent improvements across multiple tasks but limited architecture diversity
- Friendly adversary concept: Medium confidence - intuitively compelling but lacks direct experimental measurement
- Generalizability claims: Low confidence - limited testing beyond SAM framework and standard architectures

## Next Checks

1. **Gradient Alignment Analysis**: Measure the cosine similarity between gt(xt) and gB(xt) across multiple minibatches during SAM training to quantify how frequently the "friendly adversary" problem occurs and verify it decreases with VaSSO.

2. **Convergence Speed Comparison**: Beyond final accuracy, measure and compare the wall-clock training time and convergence speed of VaSSO versus SAM across different batch sizes to assess practical efficiency gains.

3. **Generalization to Non-Standard Architectures**: Test VaSSO on transformer-based architectures (e.g., ViT, BERT) and reinforcement learning tasks to evaluate whether the variance suppression benefits extend beyond standard CNNs and RNNs used in the experiments.