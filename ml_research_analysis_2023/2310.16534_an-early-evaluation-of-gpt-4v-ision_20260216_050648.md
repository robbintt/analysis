---
ver: rpa2
title: An Early Evaluation of GPT-4V(ision)
arxiv_id: '2310.16534'
source_url: https://arxiv.org/abs/2310.16534
tags:
- gpt-4v
- image
- visual
- figure
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates GPT-4V on various tasks including visual understanding,
  language understanding, visual puzzle solving, and understanding of other modalities
  such as depth, thermal, video, and audio. The authors manually construct 656 test
  instances to assess GPT-4V's performance.
---

# An Early Evaluation of GPT-4V(ision)

## Quick Facts
- arXiv ID: 2310.16534
- Source URL: https://arxiv.org/abs/2310.16534
- Reference count: 3
- Primary result: GPT-4V shows strong English visual performance but struggles with Chinese OCR, inconsistent safety refusals, and visual math puzzles

## Executive Summary
This paper presents an early evaluation of GPT-4V's multimodal capabilities across visual understanding, language understanding, visual puzzle solving, and other modalities like depth, thermal, video, and audio. The authors manually construct 656 test instances to assess performance across diverse tasks. Results reveal impressive English visual understanding but significant limitations including failure on Chinese text recognition, inconsistent refusal behavior for sensitive attributes, poor performance on visual math puzzles, and struggles with depth and audio understanding. The study provides valuable insights into GPT-4V's strengths and limitations while highlighting important areas for future research.

## Method Summary
The evaluation employs a manually constructed test set of 656 instances covering diverse tasks including image captioning, visual question answering, visual reasoning, language understanding, and multimodal tasks. The authors use both zero-shot and few-shot prompting strategies, with manual human evaluation to assess responses due to GPT-4V's tendency to generate verbose outputs that break automated metrics. Tasks span standard benchmarks like Nocaps, Flickr30K, VQAv2, and custom-created tests for specialized capabilities. The evaluation methodology emphasizes human judgment over automated metrics to capture the quality of multimodal responses.

## Key Results
- GPT-4V achieves strong performance on English visual benchmarks but fails to recognize simple Chinese texts in images
- The model exhibits inconsistent refusal behavior when answering questions about sensitive traits like gender, race, and age
- GPT-4V performs worse than GPT-4 API on language understanding tasks despite strong visual capabilities
- Few-shot prompting significantly improves performance on both visual and language understanding tasks
- The model struggles with visual math puzzles and finding nuances between similar images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4V's visual understanding ability is robust for English text in images but fails on Chinese text due to training data imbalance.
- Mechanism: The model's OCR capability is strongly tuned for English character recognition patterns learned during pretraining, while Chinese text recognition requires different stroke-based pattern learning not sufficiently represented in the training corpus.
- Core assumption: GPT-4V's training data contained predominantly English text samples versus Chinese text samples.
- Evidence anchors:
  - [abstract] "GPT-4V exhibits impressive performance on English visual-centric benchmarks but fails to recognize simple Chinese texts in the images"
  - [section 2] "GPT-4V performs very well with English text recognition, yet it cannot recognize Chinese texts in images"
  - [corpus] No corpus evidence specifically addressing training data composition for OCR; assumption based on benchmark performance gap.
- Break condition: If GPT-4V were fine-tuned on balanced multilingual OCR datasets or if the model architecture includes separate language-specific OCR modules that were not properly initialized for Chinese.

### Mechanism 2
- Claim: GPT-4V shows inconsistent refusal behavior for sensitive attribute questions due to ambiguous safety fine-tuning boundaries.
- Mechanism: The refusal policy was trained on examples with unclear boundaries between acceptable and sensitive queries, leading to inconsistent classification of similar questions.
- Core assumption: Safety fine-tuning data included ambiguous examples that created overlapping decision regions in the refusal classifier.
- Evidence anchors:
  - [abstract] "GPT-4V shows inconsistent refusal behavior when answering questions related to sensitive traits such as gender, race, and age"
  - [section 2] Multiple examples where similar questions receive different refusal decisions (e.g., gender-related questions with varying formulations)
  - [corpus] No corpus evidence about safety training data specifics; inference based on observed behavior patterns.
- Break condition: If the refusal system were replaced with rule-based filtering or if fine-tuning data were expanded to include clearer boundary examples.

### Mechanism 3
- Claim: GPT-4V struggles with math picture puzzles despite strong individual capabilities due to domain generalization limitations.
- Mechanism: The model's visual and mathematical reasoning capabilities are learned separately and do not automatically transfer to the specific domain of visual math puzzles without targeted fine-tuning.
- Core assumption: Visual understanding and mathematical reasoning are represented in separate latent subspaces that require explicit bridging for domain-specific tasks.
- Evidence anchors:
  - [abstract] "GPT-4V struggles to solve the easy math picture puzzles" despite strong performance on "much harder textual math datasets such as SAT math"
  - [section 4] "GPT-4V only solves one problem correctly" out of 10 math picture puzzles
  - [corpus] No corpus evidence about internal representation separation; inference based on capability mismatch.
- Break condition: If the model were fine-tuned on visual math puzzle datasets or if the architecture included explicit domain adaptation mechanisms.

## Foundational Learning

- Concept: Few-shot prompting and in-context learning
  - Why needed here: The paper demonstrates that GPT-4V's performance improves with exemplars, showing it has in-context learning capabilities that can be leveraged for better results
  - Quick check question: How many exemplars can GPT-4V process given its maximum input constraints, and how does this limit the effectiveness of few-shot prompting?

- Concept: Multimodal model evaluation metrics
  - Why needed here: Standard automatic metrics like CIDEr and EM Accuracy are inadequate for GPT-4V due to its verbose responses, requiring human evaluation
  - Quick check question: What alternative evaluation metrics could better capture the quality of GPT-4V's multimodal responses while accounting for its tendency to generate detailed explanations?

- Concept: Model refusal behavior and safety alignment
  - Why needed here: GPT-4V exhibits inconsistent refusal patterns for sensitive attribute questions, indicating complex safety alignment challenges in multimodal models
  - Quick check question: What specific patterns in question phrasing trigger GPT-4V's refusal behavior, and how do these patterns differ from human judgment of sensitivity?

## Architecture Onboarding

- Component map: Image → Visual Encoder → Feature Extraction → Multimodal Fusion → Language Model → Response Generation, with parallel safety analysis that can interrupt the path with refusals.
- Critical path: The system processes input images through a visual encoder, combines features with text prompts via multimodal fusion, passes through the language model decoder, and generates responses while safety modules analyze content for potential refusals.
- Design tradeoffs: The model trades off detailed visual understanding for processing constraints (max 4 images, no interleaved text/images), and balances capability with safety through refusal mechanisms that sometimes produce inconsistent results.
- Failure signatures: Performance degradation on Chinese OCR, inconsistent refusal behavior, inability to solve visual math puzzles, and verbose responses that break automatic evaluation metrics.
- First 3 experiments:
  1. Test GPT-4V's OCR performance on a balanced dataset of English and Chinese text in images to quantify the language-specific capability gap
  2. Systematically vary the phrasing of sensitive attribute questions to map the boundaries of GPT-4V's refusal behavior and identify patterns in inconsistent responses
  3. Create a curriculum of visual math puzzles with increasing complexity to determine whether the failure is due to domain generalization or specific puzzle-solving limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does GPT-4V exhibit inconsistent refusal behavior when answering questions related to sensitive traits such as gender, race, and age?
- Basis in paper: [explicit] The paper observes that GPT-4V refuses some requests for sensitive traits but approves others that seem similar, suggesting inconsistent behavior.
- Why unresolved: The paper does not provide a definitive explanation for this inconsistent behavior. It suggests that this issue is vital for future research and should be systematically studied.
- What evidence would resolve it: A comprehensive study that systematically examines the factors influencing GPT-4V's refusal behavior, such as the specific wording of questions, the context, and the perceived sensitivity of the topic.

### Open Question 2
- Question: Can GPT-4V generalize well to the domain of math picture puzzles, given its strong performance on textual math datasets like SAT math?
- Basis in paper: [explicit] The paper notes that GPT-4V struggles to solve easy math picture puzzles, even though it exhibits strong performance on much harder textual math datasets such as SAT math.
- Why unresolved: The paper suggests that GPT-4V may not generalize well to this domain or may rely on additional modules that do not work well in this domain. However, it does not provide a definitive explanation for this limitation.
- What evidence would resolve it: A study that systematically evaluates GPT-4V's performance on a wide range of math picture puzzles and investigates the factors influencing its performance, such as the visual complexity of the puzzles and the type of mathematical operations involved.

### Open Question 3
- Question: How can the performance of GPT-4V on other modalities such as depth, thermal, video, and audio be improved?
- Basis in paper: [explicit] The paper finds that GPT-4V struggles to understand depth images and audio signals, and only shows non-trivial performance on tasks of similar modalities to image, such as video and thermal.
- Why unresolved: The paper does not provide specific strategies for improving GPT-4V's performance on these modalities. It suggests that further training could be necessary before the application, but does not elaborate on the specific training methods or data required.
- What evidence would resolve it: A study that systematically investigates different training strategies and data augmentation techniques for improving GPT-4V's performance on these modalities, and evaluates the effectiveness of these approaches.

## Limitations

- The study relies on a manually constructed test set of 656 instances without public access to the full dataset, making independent verification difficult
- The evaluation only compares GPT-4V against GPT-4 API without benchmarking against other multimodal models under identical conditions
- Manual evaluation methodology introduces potential subjectivity and inter-rater reliability concerns that are not quantified in the paper

## Confidence

**High Confidence**: Claims about GPT-4V's strong performance on English visual benchmarks are well-supported by multiple test instances across different datasets (Nocaps, Flickr30K, VQAv2). The demonstration of failure on Chinese text recognition is directly observable and reproducible. The observation of inconsistent refusal behavior is empirically validated through multiple examples with varying question formulations.

**Medium Confidence**: Claims about GPT-4V's struggles with math picture puzzles are supported by test results but may be task-specific rather than indicating fundamental limitations in visual-mathematical reasoning. The assertion that few-shot prompting improves performance is demonstrated but the magnitude and consistency of improvement across tasks is not fully characterized.

**Low Confidence**: Claims about the mechanisms underlying specific failures (such as training data imbalance for Chinese OCR or domain generalization limitations for math puzzles) are speculative and not directly supported by evidence about GPT-4V's training process or internal representations.

## Next Checks

1. **OCR Capability Gap Quantification**: Conduct a systematic evaluation of GPT-4V's OCR performance on a balanced multilingual dataset containing equal numbers of English and Chinese text samples across varying complexity levels (simple words, complex sentences, different fonts, noisy backgrounds). This would provide quantitative measures of the performance gap and help identify whether the limitation is fundamental or could be addressed through targeted fine-tuning.

2. **Safety Refusal Boundary Mapping**: Design a comprehensive experiment that systematically varies the phrasing, context, and framing of sensitive attribute questions to create a detailed map of GPT-4V's refusal behavior boundaries. This would include questions with different levels of directness, contextual framing, and implied intent to understand the decision boundaries and identify patterns in inconsistent responses.

3. **Visual Math Puzzle Curriculum Testing**: Develop a structured curriculum of visual math puzzles ranging from basic arithmetic with visual elements to complex multi-step reasoning problems. Test GPT-4V's performance across this spectrum while varying the visual complexity and mathematical difficulty independently to determine whether failures stem from visual understanding limitations, mathematical reasoning gaps, or domain-specific generalization challenges.