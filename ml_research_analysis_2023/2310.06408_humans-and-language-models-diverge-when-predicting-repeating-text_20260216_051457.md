---
ver: rpa2
title: Humans and language models diverge when predicting repeating text
arxiv_id: '2310.06408'
source_url: https://arxiv.org/abs/2310.06408
tags:
- human
- attention
- memory
- each
- stimulus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares human and language model performance on a next-word
  prediction task with repeated text stimuli. Human and GPT-2 LM predictions are strongly
  aligned in the first presentation of a text span, but their performance quickly
  diverges when memory begins to play a role.
---

# Humans and language models diverge when predicting repeating text

## Quick Facts
- arXiv ID: 2310.06408
- Source URL: https://arxiv.org/abs/2310.06408
- Reference count: 10
- Key outcome: Humans and GPT-2 show strong alignment in first presentation of repeated text, but diverge as humans improve modestly while models achieve near-perfect recall, traced to induction heads that copy words with superhuman accuracy.

## Executive Summary
This paper investigates how humans and language models differ in predicting words in repeating text stimuli. Through experiments with repeated story segments, the authors found that while human and GPT-2 performance align well on initial presentations, they quickly diverge as repetition increases. Humans show modest improvement with each repetition due to lossy short-term memory, while the model achieves near-perfect recall after just one presentation. The authors identified specific attention heads in middle layers that enable the model to copy exact words from previous presentations, creating superhuman memorization. By adding a power-law recency bias to these attention heads, the model's predictions become more human-like, suggesting a path toward more human-aligned language models.

## Method Summary
The study used five stimuli from spoken story transcripts, each consisting of 40-100 word spans repeated 1-3 times. Human predictions were collected via online RSVP experiments with 100 participants, while GPT-2 Small was fine-tuned for word-level tokenization. The model's attention matrices were analyzed to identify induction heads responsible for copying from previous presentations. A power-law recency bias was then added to these attention heads and optimized via gradient descent to minimize MSE with human predictions. Performance was evaluated through accuracy metrics, correlation between human and model predictions, and perplexity on unseen text.

## Key Results
- Human and GPT-2 predictions strongly align on first presentation of repeated text
- Model reaches near-perfect recall after one presentation while humans improve modestly
- Specific middle-layer attention heads enable superhuman copying of previous presentations
- Adding power-law recency bias to attention heads makes model predictions more human-like

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models use induction heads to copy exact tokens from previous repetitions, leading to superhuman recall accuracy.
- Mechanism: Attention heads in middle layers detect the token after the previous instance of the current token and attend to it, effectively copying the next word from earlier presentations.
- Core assumption: Induction heads emerge in layers 6-12 and are responsible for pattern completion across repeated spans.
- Evidence anchors:
  - [abstract]: "We traced this divergence to specific attention heads in a middle layer that enable the model to copy words from previous presentations with superhuman accuracy."
  - [section]: "We found multiple heads across many layers that exhibit induction behavior...attention heads should attend to the token k − 1 tokens in the past."
  - [corpus]: Weak. No direct corpus evidence linking induction heads to memorization behavior.
- Break condition: If attention patterns do not show strong diagonals or if induction heads are disrupted without affecting performance.

### Mechanism 2
- Claim: Human next-word prediction improves modestly with repetition due to lossy short-term memory, while models achieve near-perfect recall via in-context learning.
- Mechanism: Humans rely on decaying short-term memory with recency bias, whereas models can attend to exact token identities across hundreds of tokens at no additional cost.
- Core assumption: Human memory decay follows a power law and is fundamentally limited compared to model attention.
- Evidence anchors:
  - [abstract]: "While human performance improves modestly with each repetition, the model reaches near-perfect performance after just one presentation."
  - [section]: "Humans must rely on lossy short-term memory, while the model can leverage in-context learning to provide superhuman, near-perfect recall."
  - [corpus]: Weak. No direct corpus evidence on human memory decay curves.
- Break condition: If models are modified to have recency bias similar to humans, their performance will align more closely with human predictions.

### Mechanism 3
- Claim: Adding a power-law recency bias to attention heads reduces the model's long-distance copying behavior, making predictions more human-like.
- Mechanism: Optimizing attention biases with αh · k^(-exp(βh)) form reduces attention to distant tokens while increasing focus on recent and current tokens.
- Core assumption: Reducing induction-like attention patterns will decrease superhuman memorization without severely harming overall model performance.
- Evidence anchors:
  - [abstract]: "Adding a power-law recency bias to these attention heads yielded a model that performs much more similarly to humans."
  - [section]: "We parameterized Bh with αh, βh ∈ R: Bh = Σ diagk(αh · k^(-exp(βh)))"
  - [corpus]: Weak. No direct corpus evidence on the effectiveness of recency bias optimization.
- Break condition: If the recency bias increases perplexity significantly or fails to improve human-model correlation.

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: Understanding how models "remember" previous tokens is crucial for explaining the divergence between human and model performance.
  - Quick check question: What is the difference between content-based and position-based attention in transformers?

- Concept: Human memory decay and recency bias
  - Why needed here: The paper contrasts human memory limitations with model capabilities, requiring knowledge of how human recall degrades over time.
  - Quick check question: How does the serial position effect relate to recency and primacy biases in human memory?

- Concept: In-context learning and pattern completion
  - Why needed here: Models use in-context examples to complete patterns, which is central to understanding their superhuman performance on repeated text.
  - Quick check question: What is the difference between in-context learning and traditional fine-tuning in language models?

## Architecture Onboarding

- Component map:
  - Input: Repeated text spans (40-100 words, 2-4 presentations)
  - Model: GPT-2 Small (12 layers, 12 attention heads per layer)
  - Output: Next-word prediction accuracy and attention matrices
  - Optimization: Additive bias Bh for attention heads using power-law recency bias

- Critical path:
  1. Feed repeated text into model
  2. Extract attention matrices for each head
  3. Identify induction heads via attention pattern analysis
  4. Optimize attention biases to match human performance
  5. Evaluate changes in human-model correlation and perplexity

- Design tradeoffs:
  - Memory vs. runtime: Quadratic scaling of attention limits context length
  - Accuracy vs. generalization: Superhuman recall hurts human-like behavior
  - Layer selection: Modifying layer 6-12 affects induction heads but may harm other abilities

- Failure signatures:
  - If attention patterns do not show induction behavior, the divergence mechanism is incorrect
  - If recency bias optimization fails to improve human-model correlation, the assumption about memory decay is wrong
  - If perplexity increases significantly, the optimization harms general language understanding

- First 3 experiments:
  1. Run the same repeated text stimuli through a different model (e.g., GPT-2 Medium) to test if induction heads are model-specific
  2. Apply the recency bias optimization to earlier layers (1-5) to see if induction heads are necessary for the effect
  3. Test the optimized model on non-repeating text to measure the cost of human-like behavior in terms of perplexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms in the attention heads of layers 6-12 enable the model to copy words from previous presentations with superhuman accuracy?
- Basis in paper: [explicit] The authors trace the cause of divergence to specific attention heads in a middle layer and hypothesize that these layers contain induction heads that enable the model to identify and recall patterns with superhuman accuracy.
- Why unresolved: The paper identifies the involvement of specific attention heads and layers but does not provide a detailed explanation of the exact mechanisms within these attention heads that allow for such precise copying behavior.
- What evidence would resolve it: Detailed analysis of the attention matrices and weight distributions within these specific heads, possibly through ablation studies or visualization techniques that reveal how information is being copied across presentations.

### Open Question 2
- Question: How does the power-law recency bias learned by the optimization procedure affect the model's performance on other memory tasks beyond the repeating text stimuli?
- Basis in paper: [explicit] The authors add a power-law recency bias to attention heads and show it makes the model more human-like on the repeating text task, but question whether this improvement generalizes.
- Why unresolved: The study only tests the bias on the specific repeating text task and does not investigate its effects on other memory-related tasks or general language modeling capabilities.
- What evidence would resolve it: Testing the optimized model on a variety of memory tasks (e.g., sequence recall, pattern completion, long-term dependencies) and comparing its performance to both the original model and human baselines.

### Open Question 3
- Question: Is the divergence in human and LM performance on repeating text stimuli a result of fundamental differences in memory mechanisms, or could it be attributed to the model's training data and architecture?
- Basis in paper: [inferred] The paper suggests that humans and LMs use substantially different memory mechanisms, but also acknowledges that the model's superhuman performance might stem from its ability to exploit in-context learning rather than genuine memory processes.
- Why unresolved: The study does not control for the model's training data or architecture, making it difficult to determine whether the observed divergence is due to inherent differences in memory mechanisms or simply a consequence of how the model was trained and designed.
- What evidence would resolve it: Comparing the performance of different language models with varying architectures and training data on the same repeating text task, and analyzing whether the divergence persists across models or is specific to GPT-2. Additionally, investigating the effects of modifying the model's training objective or architecture to prioritize human-like memory processes.

## Limitations
- Small dataset of only five stimuli limits generalizability to other text types
- Fine-tuning procedure for GPT-2 not fully specified, raising potential confounds
- Evaluation focuses solely on next-word prediction accuracy, not other aspects of language understanding
- Causal link between induction heads and superhuman memorization not definitively established

## Confidence
- High confidence: The observation that human and model performance diverge on repeated text stimuli, with humans showing modest improvement and models achieving near-perfect recall after one presentation.
- Medium confidence: The identification of induction heads as the mechanism for superhuman memorization, though causal evidence is not definitive.
- Low confidence: The claim that adding power-law recency bias produces a model that is "more human-like" across all aspects of language processing.

## Next Checks
1. Conduct ablation studies by selectively disabling induction heads to provide stronger causal evidence for their role in superhuman memorization
2. Test the recency-biased model on a broader range of text types and repetition patterns to assess generalizability
3. Compare the optimized model's behavior to human behavioral and neural data beyond next-word prediction to evaluate comprehensive alignment with human language processing