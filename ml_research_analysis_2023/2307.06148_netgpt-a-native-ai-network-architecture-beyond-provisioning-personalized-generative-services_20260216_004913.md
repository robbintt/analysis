---
ver: rpa2
title: 'NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized
  Generative Services'
arxiv_id: '2307.06148'
source_url: https://arxiv.org/abs/2307.06148
tags:
- edge
- llms
- netgpt
- network
- cloud
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes NetGPT, a native-AI network architecture that
  leverages collaborative cloud-edge computing to provision personalized generative
  services based on large language models (LLMs). The key idea is to deploy different
  sizes of LLMs at the edge and cloud, with edge LLMs leveraging location-based information
  for personalized prompt completion and interaction with cloud LLMs.
---

# NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services

## Quick Facts
- arXiv ID: 2307.06148
- Source URL: https://arxiv.org/abs/2307.06148
- Reference count: 16
- Key outcome: Proposes a native-AI network architecture leveraging cloud-edge collaboration for personalized generative services using different-sized LLMs at edge and cloud

## Executive Summary
This paper introduces NetGPT, a native-AI network architecture that provisions personalized generative services by deploying smaller LLMs at the edge and larger LLMs at the cloud. The architecture leverages collaborative cloud-edge computing where edge LLMs handle prompt completion using location-based information while cloud LLMs perform complex generation tasks. The authors demonstrate the feasibility of this approach using low-rank adaptation (LoRA) for efficient fine-tuning of open-source models like GPT-2-base and LLaMA-7B. Comprehensive numerical comparisons show NetGPT's superiority in personalization, customization, and reduced latency compared to alternative approaches.

## Method Summary
The paper proposes deploying GPT-2-base (0.1B parameters) at the edge and LLaMA-7B at the cloud, with LoRA-based lightweight fine-tuning for edge adaptation. The method involves a collaborative workflow where the edge LLM completes prompts using local information before sending them to the cloud LLM for generation. Fine-tuning is performed using the Stanford Alpaca dataset for LLaMA and various datasets for popularity prediction and intent inference. The architecture aims to reduce communication overhead and latency while maintaining personalization capabilities through location-based information processing at the edge.

## Key Results
- Demonstrates personalized prompt completion and generative response generation using edge-cloud collaboration
- Achieves reduced latency and communication overhead compared to cloud-only approaches
- Shows edge LLMs' capability for popularity prediction and intent inference with promising accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deploying smaller LLMs at the edge and larger LLMs at the cloud reduces latency and communication overhead for personalized generative services.
- Mechanism: Edge LLMs handle prompt completion using local location-based information, reducing the amount of data sent to the cloud. Cloud LLMs handle more complex generation tasks. This division leverages the edge's proximity to users and the cloud's superior computational capacity.
- Core assumption: Edge servers have sufficient computational capacity to run smaller LLMs (e.g., GPT-2-base) effectively for prompt completion.
- Evidence anchors:
  - [abstract] "leverage location-based information for personalized prompt completion and interaction with cloud LLMs"
  - [section] "we select and deploy the LLaMA-7B model [7] and the GPT-2-base model, which consist of approximately 6.7 and 0.1 billion parameters, at the cloud and the edge respectively"
  - [corpus] Weak evidence - no specific mention of latency reduction in corpus.
- Break condition: If edge servers lack the computational capacity to run even smaller LLMs effectively, the proposed architecture fails to deliver the promised latency benefits.

### Mechanism 2
- Claim: Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large LLMs on resource-constrained devices.
- Mechanism: LoRA approximates weight updates using low-rank matrices, significantly reducing the number of parameters that need to be trained and stored. This allows fine-tuning of large models like LLaMA-7B on consumer-level hardware.
- Core assumption: The low-rank approximation introduced by LoRA does not significantly degrade the model's performance after fine-tuning.
- Evidence anchors:
  - [section] "we leverage a low-rank adaptation (LoRA) technique [11] to achieve parameter-efficient fine-tuning on a consumer-level hardware"
  - [section] "Our experiment shows that it only costs 28 GB VRAM to fine-tune the LLaMA-7B model, without significantly elongating the training duration"
  - [corpus] No specific mention of LoRA in corpus.
- Break condition: If the low-rank approximation significantly degrades model performance, the benefit of using LoRA is negated.

### Mechanism 3
- Claim: Edge LLMs can predict user preferences and infer user intent, enabling proactive network optimization and personalized service delivery.
- Mechanism: By fine-tuning edge LLMs on location-specific data and user behavior patterns, they can learn to anticipate user needs and translate natural language requests into network configurations.
- Core assumption: Sufficient training data is available to fine-tune edge LLMs for accurate prediction and intent inference in the target domain.
- Evidence anchors:
  - [abstract] "the edge LLMs' capability to predict trends and infer intents promises a unified solution for intelligent network management & orchestration"
  - [section] "the RAN can gather information from the affiliated terminals and interprets user' preference from historical visiting records by fine-tuning edge LLMs"
  - [corpus] No specific mention of intent inference in corpus.
- Break condition: If insufficient or poor-quality training data is available, the edge LLMs will not be able to accurately predict user preferences or infer user intent.

## Foundational Learning

- Concept: Transformer architecture and its components (self-attention, FNN, layer normalization)
  - Why needed here: Understanding the transformer architecture is crucial for grasping how LLMs process information and why certain architectural choices (like RoPE in LLaMA) are made.
  - Quick check question: What is the primary function of self-attention in a transformer model?

- Concept: Low-Rank Adaptation (LoRA) and its application to fine-tuning large models
  - Why needed here: LoRA is a key technique used in this paper to enable efficient fine-tuning of LLMs on resource-constrained devices, which is essential for the edge deployment strategy.
  - Quick check question: How does LoRA reduce the computational cost of fine-tuning compared to full fine-tuning?

- Concept: Prompt engineering and its role in controlling LLM output
  - Why needed here: Prompt completion at the edge is a core component of the NetGPT architecture, and understanding how to craft effective prompts is crucial for achieving desired results.
  - Quick check question: What is the difference between a "concise prompt" and a "comprehensive prompt" in the context of this paper?

## Architecture Onboarding

- Component map: User -> Edge LLM (GPT-2-base) -> Cloud LLM (LLaMA-7B) -> Response
- Critical path: User prompt → Edge LLM (completion) → Cloud LLM (generation) → Response
- Design tradeoffs:
  - Model size vs. computational cost at the edge
  - Communication overhead vs. processing power at the cloud
  - Privacy concerns vs. personalization capabilities
- Failure signatures:
  - High latency in prompt completion or generation
  - Inaccurate or irrelevant responses
  - Excessive resource consumption at the edge or cloud
- First 3 experiments:
  1. Deploy a small LLM (e.g., GPT-2-small) at the edge and measure its performance on prompt completion tasks with location-based information.
  2. Fine-tune a larger LLM (e.g., LLaMA-7B) using LoRA on a consumer-level GPU and evaluate the trade-off between parameter efficiency and model performance.
  3. Implement a simple end-to-end system with edge and cloud components and measure the latency and communication overhead for personalized generative services.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the feasibility of implementing LLM inference and fine-tuning at terminal devices, given the limited computing capability and cost constraints?
- Basis in paper: [inferred] The paper discusses the potential of edge LLMs for personalized generative services but does not explicitly explore the possibility of running LLMs directly on terminal devices.
- Why unresolved: The paper focuses on the feasibility of deploying LLMs at the edge and cloud, but does not address the challenges and opportunities of implementing LLMs on resource-constrained terminal devices.
- What evidence would resolve it: Empirical studies comparing the performance, energy consumption, and cost of running LLMs on terminals versus edge devices would provide insights into the feasibility of terminal-based LLM deployment.

### Open Question 2
- Question: How can online learning-based LLMs be developed to adapt to the dynamicity of wireless environments at the edge?
- Basis in paper: [explicit] The paper mentions the need for continual evolution of knowledge in cellular networks and the potential of online learning-based LLMs to adapt to dynamic wireless environments.
- Why unresolved: The paper highlights the importance of online learning but does not provide specific methods or techniques for implementing it in the context of edge LLMs.
- What evidence would resolve it: Research demonstrating the effectiveness of online learning algorithms in updating LLMs based on real-time data from wireless environments would provide a solution to this open question.

### Open Question 3
- Question: How can the sensitivity for numerical inference and deception effects in LLMs be improved to enhance their rigorousness?
- Basis in paper: [explicit] The paper acknowledges the limitations of LLMs in numerical inference and the possibility of deception effects, but does not propose specific solutions to address these issues.
- Why unresolved: The paper identifies the problem but does not provide concrete methods or techniques to improve the rigorousness of LLMs in handling numerical data and avoiding deceptive outputs.
- What evidence would resolve it: Studies presenting novel architectures, training techniques, or post-processing methods that enhance the numerical inference capabilities and robustness of LLMs would resolve this open question.

## Limitations
- Computational resource requirements may limit deployment on typical edge devices
- Evaluation relies on specific datasets that may not generalize to all use cases
- Communication overhead analysis lacks comprehensive empirical validation

## Confidence
- **High Confidence**: The fundamental architectural design of deploying different-sized LLMs at edge and cloud, and the use of LoRA for efficient fine-tuning are well-supported by established techniques in the field.
- **Medium Confidence**: The performance claims regarding latency reduction and communication efficiency are plausible but lack comprehensive empirical validation across diverse scenarios.
- **Low Confidence**: The claims about edge LLMs' capability to predict trends and infer user intent are promising but minimally validated, with limited discussion of the training data requirements and accuracy metrics.

## Next Checks
1. Conduct detailed profiling of computational and memory requirements for running GPT-2-base at the edge under various load conditions, including both idle and peak usage scenarios.
2. Implement comprehensive monitoring of actual data exchange between edge and cloud components during typical user interactions, measuring both bandwidth usage and latency across different network conditions.
3. Evaluate the architecture's performance on multiple diverse datasets beyond the Stanford Alpaca dataset, including datasets with different characteristics (e.g., technical support queries, creative writing prompts) to assess robustness and generalization capabilities.