---
ver: rpa2
title: Active Membership Inference Attack under Local Differential Privacy in Federated
  Learning
arxiv_id: '2302.12685'
source_url: https://arxiv.org/abs/2302.12685
tags:
- attack
- privacy
- data
- success
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a new active membership inference (AMI) attack
  against federated learning (FL) systems, carried out by a dishonest server. The
  attack exploits the correlation among data features through a non-linear decision
  boundary to infer whether a target data sample is included in a client's private
  training data.
---

# Active Membership Inference Attack under Local Differential Privacy in Federated Learning

## Quick Facts
- arXiv ID: 2302.12685
- Source URL: https://arxiv.org/abs/2302.12685
- Reference count: 40
- One-line primary result: A dishonest server can achieve severely high success rates for membership inference under LDP protection by exploiting feature correlations through malicious model parameter manipulation.

## Executive Summary
This paper presents an active membership inference (AMI) attack where a dishonest server in federated learning (FL) can infer whether a target data sample is included in a client's private training data, even under rigorous local differential privacy (LDP) protection. The attack exploits correlations among data features through a non-linear decision boundary by manipulating model parameters and embedding malicious weights. Theoretical analysis and experiments on benchmark datasets demonstrate that the attack can maintain strong success rates even when LDP mechanisms like BitRand and OME are applied, highlighting significant privacy risks in FL systems.

## Method Summary
The attack works by having the server craft malicious weights for a chosen neuron in the global model such that it activates only for the target sample and its LDP-preserving perturbations. The server generates multiple perturbations of the target sample under LDP noise, labels them as positive, and samples other data as negative. Training the neuron to distinguish these groups exploits the remaining correlation after LDP noise is applied. The server then observes whether the neuron's gradient is zero or non-zero during client training to infer membership. The attack provides a certified guarantee of success by computing (1-δ)-confidence bounds on the neuron's output distribution using Hoeffding's inequality and Monte Carlo sampling.

## Key Results
- The attack achieves severely high success rates (up to 85%) under LDP protection with reasonable privacy budgets (ε = 5-10)
- Model accuracy under LDP protection remains relatively high (>90%) while attack success rates stay significant
- Theoretical bounds using Hoeffding's inequality match empirical results, providing certified guarantees of attack success
- The attack is effective across multiple datasets including CIFAR-10, ImageNet, and CelebA with feature embeddings from pre-trained Resnet-18

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attack exploits correlation among data features through a non-linear decision boundary to infer membership.
- Mechanism: The server crafts malicious parameters for a chosen neuron such that it activates only for the target sample. By observing whether the neuron's gradient is zero or non-zero, the server infers membership.
- Core assumption: The chosen neuron can be trained to distinguish the target sample from all others under the given LDP noise level.
- Evidence anchors:
  - [abstract] "By exploiting the correlation among data features through a non-linear decision boundary, AMI attacks with a certified guarantee of success can achieve severely high success rates under rigorous local differential privacy (LDP) protection."
  - [section] "The key idea is that, given a target data sample, the server carefully crafts malicious weights of the global model such that the model updates from the clients would expose the membership information of the target data sample through the behavior of a chosen neuron."
- Break condition: If LDP noise is large enough to destroy the feature correlation, the neuron cannot be trained to distinguish the target sample.

### Mechanism 2
- Claim: Under LDP, the attack generates multiple perturbations of the target sample and trains the neuron to activate only for these perturbations.
- Mechanism: The server samples l perturbations M(t,ε) of the target sample, labels them as positive, and samples other data as negative. Training the neuron to distinguish these groups exploits the remaining correlation.
- Core assumption: The LDP mechanism's noise is insufficient to make all perturbations indistinguishable from other samples.
- Evidence anchors:
  - [section] "To achieve our goal, we strengthen our attack by generating a set T of l perturbations M(t,ε), that is, invoking M(t,ε)l times with independent draws of LDP-preserving noise."
  - [section] "The key idea is to simultaneously ensure that the lower bound ˆElb[v(t)] is larger than 0 and the upper bound ˆEub[v(x)] is smaller than or equal to 0 with a broken probability δ."
- Break condition: If privacy budget ε is very small, the noise destroys correlation and the neuron cannot distinguish groups.

### Mechanism 3
- Claim: The attack provides a certified guarantee of success by bounding the neuron's output distribution.
- Mechanism: Using Hoeffding's inequality, the server computes (1-δ)-confidence lower and upper bounds on the neuron's expected output for target and non-target samples. Success is guaranteed when these bounds don't overlap.
- Core assumption: The neuron's output is bounded and the LDP mechanism's noise has known statistical properties.
- Evidence anchors:
  - [section] "By leveraging the Monte Carlo sampling for the expectation estimation, we can replace E[v(t)] with ˆE[v(t)] = 1/p ∑p vp(t) and replace E[v(x)] with ˆE[v(x)] = 1/q ∑q vq(x)."
  - [section] "The key idea