---
ver: rpa2
title: 'Aligning Large Language Models with Human: A Survey'
arxiv_id: '2307.12966'
source_url: https://arxiv.org/abs/2307.12966
tags:
- llms
- instructions
- arxiv
- human
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of recent research
  efforts on aligning large language models (LLMs) with human expectations. It discusses
  three main aspects of LLM alignment: data collection, training methodologies, and
  model evaluation.'
---

# Aligning Large Language Models with Human: A Survey

## Quick Facts
- arXiv ID: 2307.12966
- Source URL: https://arxiv.org/abs/2307.12966
- Reference count: 20
- Key outcome: This paper provides a comprehensive survey of recent research efforts on aligning large language models (LLMs) with human expectations, discussing data collection, training methodologies, and model evaluation.

## Executive Summary
This survey provides a comprehensive overview of alignment technologies for large language models, focusing on three key aspects: data collection, training methodologies, and model evaluation. The paper examines methods for collecting high-quality instruction data from diverse sources, including NLP benchmarks, human annotations, and leveraging strong LLMs. It reviews various training approaches such as supervised fine-tuning, reinforcement learning from human feedback, and parameter-efficient fine-tuning methods. The survey also presents benchmarks and protocols for evaluating human-aligned LLMs, covering both closed-set and open-set evaluation paradigms. Finally, it discusses future research directions including fine-grained instruction data management, non-English language alignment, and human-LLM joint evaluation frameworks.

## Method Summary
The survey synthesizes research on LLM alignment by examining three main components: data collection methods (using NLP benchmarks, human annotations, and strong LLMs), training methodologies (including SFT, RLHF, and parameter-efficient approaches like LoRA), and evaluation frameworks (covering both automatic protocols and human-based assessment). The training process typically involves three stages: supervised fine-tuning on high-quality instruction data, training a reward model using ranked human preference data, and reinforcement learning optimization under frameworks like PPO. The paper also explores offline human preference training techniques and discusses the challenges of incorporating human preferences into LLMs while maintaining computational efficiency.

## Key Results
- Alignment requires diverse instruction collection from multiple sources including NLP benchmarks, human annotations, and strong LLMs
- Parameter-efficient fine-tuning methods like LoRA can reduce computational costs while maintaining alignment quality
- Human-LLM joint evaluation frameworks offer potential benefits by combining human judgment with LLM efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse instruction collection from multiple sources improves LLM alignment across varied tasks and domains
- Mechanism: By aggregating instructions from NLP benchmarks, human annotations, and strong LLMs, the model is exposed to a wider range of instruction types, complexities, and linguistic styles, enhancing its generalization and task performance
- Core assumption: Instruction diversity directly correlates with model capability to handle unseen, complex tasks
- Evidence anchors:
  - [abstract]: "This survey presents a comprehensive overview of these alignment technologies, including the following aspects. (1) Data collection: the methods for effectively collecting high-quality instructions for LLM alignment, including the use of NLP benchmarks, human annotations, and leveraging strong LLMs."
  - [section]: "Researchers propose leveraging the power of existing NLP benchmarks, human annotators, and state-of-the-art LLMs (e.g., ChatGPT and GPT-4) to generate training instructions."
  - [corpus]: Weak - corpus evidence is missing or minimal
- Break condition: If instruction diversity leads to conflicting or noisy training signals that degrade model performance, or if the model overfits to a specific instruction style

### Mechanism 2
- Claim: Parameter-efficient fine-tuning methods (e.g., LoRA) reduce computational cost while maintaining alignment quality
- Mechanism: By freezing most model parameters and only training a small set of additional parameters (e.g., update matrices), LoRA achieves efficient adaptation to new tasks with reduced memory and compute requirements
- Core assumption: The pretrained LLM's knowledge can be effectively leveraged with minimal parameter updates for task-specific alignment
- Evidence anchors:
  - [abstract]: "Additionally, some researchers consider human preference as ranking-based training signals or replace scalar rewards with language-based feedback to enhance training stability and performance."
  - [section]: "Low-Rank Adaptation (LoRA) (Hu et al., 2022) suggests the addition of pairs of rank-decomposition trainable weight matrices (i.e., update matrices) to the existing weights, which are kept frozen."
  - [corpus]: Weak - corpus evidence is missing or minimal
- Break condition: If the rank decomposition is insufficient to capture the necessary parameter updates, leading to underfitting or degraded alignment performance

### Mechanism 3
- Claim: Human-LLM joint evaluation frameworks combine the strengths of both human judgment and LLM efficiency for comprehensive alignment assessment
- Mechanism: By leveraging LLMs for initial evaluation and human judgment for nuanced assessment, this approach balances efficiency with high-quality, context-aware evaluation
- Core assumption: LLMs can provide reliable, scalable evaluation while humans offer critical judgment for complex, context-dependent cases
- Evidence anchors:
  - [abstract]: "The paper also discusses human-based and LLM-based evaluation paradigms, highlighting the strengths and limitations of each approach."
  - [section]: "Human-LLM Joint Evaluation Framework Existing LLM evaluation frameworks either use LLMs for effective evaluation or leverage crowd-sourcing for high-quality evaluation."
  - [corpus]: Weak - corpus evidence is missing or minimal
- Break condition: If LLM evaluators introduce significant bias or if human evaluation becomes a bottleneck, negating the efficiency gains

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT is the foundational method for aligning LLMs with human instructions by training on instruction-response pairs
  - Quick check question: What is the primary loss function used in SFT for aligning LLMs with human instructions?
- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is a key training methodology that incorporates human preferences into LLM alignment through reward modeling and policy optimization
  - Quick check question: What are the three main stages of RLHF as described in the survey?
- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: PEFT methods like LoRA enable efficient alignment of LLMs with reduced computational cost and memory requirements
  - Quick check question: How does LoRA modify the forward pass of a neural layer to achieve parameter-efficient fine-tuning?

## Architecture Onboarding

- Component map: Data collection -> Training methodologies (SFT, RLHF, PEFT) -> Model evaluation (benchmarks and automatic protocols)
- Critical path: The most critical path is the integration of high-quality instruction data with effective training methodologies to achieve robust alignment, followed by comprehensive evaluation
- Design tradeoffs: Balancing instruction diversity with data quality, computational efficiency with alignment performance, and human evaluation with LLM-based evaluation
- Failure signatures: Poor alignment quality (e.g., misunderstanding instructions, generating biased or hallucinated content), computational inefficiency, and biased or unreliable evaluation results
- First 3 experiments:
  1. Evaluate the impact of instruction diversity on model performance by training on instructions from different sources (e.g., NLP benchmarks, human annotations, strong LLMs)
  2. Compare the alignment performance and computational efficiency of full fine-tuning vs. parameter-efficient methods (e.g., LoRA) on a given instruction dataset
  3. Assess the effectiveness of human-LLM joint evaluation by comparing evaluation results and efficiency against human-only or LLM-only evaluation frameworks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively manage instruction data to improve the performance of aligned LLMs?
- Basis in paper: [explicit] The paper discusses the importance of fine-grained instruction data management, highlighting the need for optimal quality control, optimal instruction training sequence, and effective mixing of different instructions.
- Why unresolved: Despite some findings on the implications of specific instruction datasets, there are still many issues in other aspects of instruction data management that remain unclear. These include the optimal quality control towards instruction data, optimal instruction training sequence, and how to effectively mix up different instructions.
- What evidence would resolve it: Further research and experiments that explore different instruction data management strategies and their impact on LLM performance would provide evidence to resolve this question.

### Open Question 2
- Question: How can we adapt alignment technologies to improve the performance of LLMs in non-English languages?
- Basis in paper: [explicit] The paper mentions that most research in LLM alignment is English-dominated and explores the potential of adapting alignment technologies to other languages, including low-resource languages.
- Why unresolved: While some alignment technologies may be language-agnostic, their effectiveness in non-English languages is unclear. The paper suggests exploring how well these prompts perform when adapted to other languages and how to effectively transfer the effect of LLM alignment across different languages.
- What evidence would resolve it: Empirical studies and experiments that evaluate the performance of alignment technologies in various non-English languages, particularly low-resource languages, would provide evidence to address this question.

### Open Question 3
- Question: What is the optimal training framework for aligning LLMs with human preferences?
- Basis in paper: [explicit] The paper discusses the challenges of incorporating human preferences into LLMs and mentions the need for resource-constrained LLM alignment training frameworks.
- Why unresolved: The paper suggests that most aligned LLMs are based on simple supervised fine-tuning (SFT) technology, which does not explicitly incorporate human preferences. It highlights the lack of comprehensive investigation over the effectiveness of various training technologies to incorporate human preferences into LLMs.
- What evidence would resolve it: Comparative studies and experiments that evaluate the performance of different training technologies, such as online and offline human preference training, parameter-efficient training, and their combinations, would provide evidence to address this question.

### Open Question 4
- Question: How can we develop a human-in-the-loop solution for LLM alignment data generation?
- Basis in paper: [explicit] The paper discusses the potential of human-in-the-loop data generation solutions in LLM alignment, highlighting the success of ShareGPT data in improving alignment quality.
- Why unresolved: While the paper mentions the potential of human-in-the-loop solutions, it does not explore other types of human-in-the-loop solutions for LLM alignment. It suggests exploring other types of human-in-the-loop solutions to further facilitate LLM alignment.
- What evidence would resolve it: Research and experiments that explore different human-in-the-loop data generation solutions, such as interactive dialogue generation, human feedback integration, and collaborative annotation, would provide evidence to address this question.

### Open Question 5
- Question: How can we develop a human-LLM joint evaluation framework for LLM alignment?
- Basis in paper: [explicit] The paper discusses the use of LLM-based evaluation paradigms and mentions the potential of developing a human-LLM joint evaluation framework where LLMs and humans are assigned different evaluation tasks based on their strengths.
- Why unresolved: While the paper suggests the potential of human-LLM joint evaluation frameworks, it does not provide specific details on how to develop such frameworks or the benefits they may offer.
- What evidence would resolve it: Research and experiments that explore different human-LLM joint evaluation frameworks, their effectiveness in evaluating LLM alignment, and the benefits they offer compared to purely human or LLM-based evaluation would provide evidence to address this question.

## Limitations

- The survey primarily focuses on English-language instruction data and evaluation benchmarks, with limited discussion of alignment challenges for non-English languages
- The evaluation section mentions both automatic and human-based assessment methods but lacks detailed comparative analysis of their effectiveness and trade-offs
- The paper does not provide extensive empirical evidence or experimental results to validate the discussed methodologies and approaches

## Confidence

- High confidence in the description of training methodologies (SFT, RLHF, PEFT) as these are well-established approaches with extensive literature
- Medium confidence in the completeness of data collection methods, as the survey covers major approaches but may not capture all emerging techniques for instruction generation
- Medium confidence in the evaluation framework coverage, as new benchmarks continue to emerge rapidly in this fast-moving field

## Next Checks

1. Replicate the effectiveness of instruction diversity by training models on data from different sources (NLP benchmarks vs. human annotations vs. LLM-generated instructions) and measuring performance differences on downstream tasks

2. Implement and compare LoRA-based parameter-efficient fine-tuning against full fine-tuning on a standard instruction dataset, measuring both alignment quality and computational efficiency metrics

3. Design and conduct a small-scale human-LLM joint evaluation study to validate the claimed benefits of combining human judgment with LLM-based assessment, measuring both evaluation quality and efficiency gains compared to pure human evaluation