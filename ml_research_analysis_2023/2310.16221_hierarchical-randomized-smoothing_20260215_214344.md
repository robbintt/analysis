---
ver: rpa2
title: Hierarchical Randomized Smoothing
arxiv_id: '2310.16221'
source_url: https://arxiv.org/abs/2310.16221
tags:
- smoothing
- hierarchical
- certificates
- robustness
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hierarchical randomized smoothing introduces a novel framework
  for certifying robustness in complex data where adversaries perturb only a subset
  of entities. The key idea is to first select a subset of entities randomly, then
  add noise only to the selected ones.
---

# Hierarchical Randomized Smoothing

## Quick Facts
- arXiv ID: 2310.16221
- Source URL: https://arxiv.org/abs/2310.16221
- Reference count: 40
- Key outcome: Introduces a framework for certifying robustness when adversaries perturb only subsets of entities in complex data

## Executive Summary
Hierarchical randomized smoothing introduces a novel framework for certifying robustness in complex data where adversaries perturb only a subset of entities. The key innovation is to first select entities randomly, then add noise only to the selected ones, yielding stronger robustness guarantees while maintaining high accuracy. This framework generalizes both additive noise and ablation smoothing, allowing integration of existing smoothing distributions. Experiments demonstrate significant improvements in the robustness-accuracy trade-off compared to existing methods across image and node classification tasks.

## Method Summary
The method extends randomized smoothing by introducing a two-level hierarchical approach. First, an upper-level Bernoulli distribution selects which entities to smooth. Then, a lower-level smoothing distribution (Gaussian, sparse, or ablation-based) adds noise only to the selected entities. This targeted noise addition is achieved by extending the data matrix with an indicator column showing which entities were smoothed. Certificates are derived using the Neyman-Pearson lemma twice - first on the upper-level selection and then on the lower-level distribution. The framework can integrate existing smoothing distributions provided their certificates exist.

## Key Results
- Hierarchical smoothing achieves stronger robustness guarantees while maintaining higher accuracy than standard smoothing methods
- The framework generalizes both additive noise and ablation smoothing, allowing integration of various existing smoothing distributions
- Experiments show significant improvements in the robustness-accuracy trade-off on both image and node classification tasks
- The approach effectively expands the Pareto front under various threat models

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical smoothing achieves stronger robustness guarantees by adding noise in a targeted manner. The framework first selects a subset of entities to smooth (via upper-level Bernoulli distribution) and then applies lower-level smoothing only to those selected entities. This targeted approach preserves accuracy while maintaining strong robustness guarantees. The upper-level selection probability p can be tuned to balance robustness and accuracy trade-offs. [abstract, section 5]

### Mechanism 2
The framework generalizes both additive noise and ablation certificates into a novel hierarchical approach. By appending an indicator variable τ to the data matrix, the framework creates a higher-dimensional space where the upper-level certificate can separate clean from perturbed rows and delegate robustness guarantees to the lower-level distribution. The base classifier can be trained to operate on the extended matrix space while ignoring the indicator dimension. [abstract, section 5]

### Mechanism 3
The framework efficiently integrates existing smoothing distributions without deriving certificates from scratch. The Neyman-Pearson lemma is applied twice - first on the upper-level distribution to handle entity selection, then on the lower-level distribution to handle perturbations within selected entities. This allows leveraging certificates from the lower-level distribution. The certificates apply to a wide range of models, domains and tasks and are at least as efficient as certificates for the underlying smoothing distribution. [abstract, section 5]

## Foundational Learning

- Concept: Randomized smoothing framework
  - Why needed here: Hierarchical smoothing builds upon randomized smoothing as its foundation, extending it to handle entity-specific perturbations
  - Quick check question: What is the main idea behind randomized smoothing and how does it provide robustness guarantees?

- Concept: Neyman-Pearson lemma
  - Why needed here: The framework uses the Neyman-Pearson lemma twice - once for the upper-level entity selection and once for the lower-level perturbation handling
  - Quick check question: How does the Neyman-Pearson lemma help derive robustness certificates in randomized smoothing?

- Concept: Threat models and perturbation spaces
  - Why needed here: Understanding how adversaries can perturb subsets of entities is crucial for designing appropriate certificates
  - Quick check question: What is the difference between perturbing entire objects versus subsets of entities, and why does this matter for certification?

## Architecture Onboarding

- Component map: Upper-level Bernoulli selection -> Lower-level smoothing distribution (Gaussian/sparse/ablation) -> Extended data matrix with indicator column -> Classifier training -> Certification via Neyman-Pearson lemma
- Critical path: Select entities to smooth → Apply noise to selected entities → Estimate smoothed classifier probabilities → Compute upper-level and lower-level certificates → Combine results for final robustness guarantees
- Design tradeoffs: Main tradeoff is between robustness and accuracy, controlled by selection probability p. Higher p increases robustness but decreases accuracy. Framework trades computational efficiency for flexibility by leveraging existing certificates
- Failure signatures: Common failure modes include: insufficient smoothing leading to no certificates, over-smoothing causing accuracy degradation, classifier unable to handle extended matrix space, computational inefficiency when integrating complex lower-level distributions
- First 3 experiments:
  1. Implement hierarchical smoothing with Gaussian noise on MNIST for image classification with varying selection probabilities
  2. Compare hierarchical smoothing against standard Gaussian smoothing on Cora dataset for node classification
  3. Test hierarchical smoothing with sparse smoothing distribution on binary feature graphs to validate discrete domain certificates

## Open Questions the Paper Calls Out

- Can the hierarchical smoothing framework be extended to incorporate non-uniform entity-selection probabilities for improved robustness guarantees? [explicit] The paper mentions this possibility in Section H without providing experimental results or theoretical guarantees.
- Is it possible to derive tight and efficient certificates for hierarchical randomized smoothing that do not rely on the worst-case assumption of selecting the least robust classifier? [inferred] The paper acknowledges limitations of the worst-case assumption and mentions need for tighter certificates as future work.
- Can the hierarchical smoothing framework be applied to data-dependent smoothing distributions, such as those proposed by Súkeník et al. (2022)? [explicit] The paper mentions this as a potential direction for future work without exploration.
- How does the choice of smoothing parameters impact the trade-off between robustness and accuracy in hierarchical smoothing? [explicit] The paper discusses importance of finding optimal parameters but doesn't provide systematic analysis.
- Can the framework be extended to certify robustness against adversaries using generative methods rather than ℓp-norm bounded perturbations? [explicit] The paper acknowledges this limitation without providing a solution.

## Limitations
- Practical implementation details remain underspecified, particularly regarding indicator column handling during training and exact threat model specifications
- Theoretical framework appears sound but lacks comprehensive empirical validation across diverse domains and threat models
- The framework's flexibility introduces complexity in implementation and optimization procedures

## Confidence
- High confidence in the core theoretical framework and its ability to generalize existing methods
- Medium confidence in practical implementation details and optimization procedures
- Low confidence in comprehensive empirical validation across all claimed scenarios

## Next Checks
1. Implement hierarchical smoothing with ablation-based lower-level distribution on graph datasets to verify discrete domain certificates and compare against standard ablation smoothing
2. Conduct ablation studies varying selection probability p to quantify the robustness-accuracy tradeoff and identify optimal parameter settings for different threat models
3. Test hierarchical smoothing framework on a novel domain (e.g., 3D point clouds) to validate generalization claims beyond the presented image and graph classification tasks