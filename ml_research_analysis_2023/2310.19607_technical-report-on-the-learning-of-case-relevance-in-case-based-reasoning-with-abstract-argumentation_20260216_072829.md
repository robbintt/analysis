---
ver: rpa2
title: Technical Report on the Learning of Case Relevance in Case-Based Reasoning
  with Abstract Argumentation
arxiv_id: '2310.19607'
source_url: https://arxiv.org/abs/2310.19607
tags:
- aa-cbr
- decision
- cases
- case
- trees
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to learn relevance in case-based reasoning
  (CBR) with abstract argumentation (AA-CBR) for legal datasets. The method extracts
  binary features from decision trees to represent cases, allowing AA-CBR to handle
  tabular data.
---

# Technical Report on the Learning of Case Relevance in Case-Based Reasoning with Abstract Argumentation

## Quick Facts
- arXiv ID: 2310.19607
- Source URL: https://arxiv.org/abs/2310.19607
- Reference count: 25
- Key outcome: AA-CBR with learned relevance achieves competitive accuracy with decision trees while being up to 94.2% smaller in model size

## Executive Summary
This paper proposes a method to learn case relevance for Abstract Argumentation Case-Based Reasoning (AA-CBR) using decision trees on tabular legal datasets. The approach extracts binary features from decision tree splits to represent cases, enabling AA-CBR to handle tabular data. The method is evaluated on COMPAS and Welfare Benefits datasets, showing that AA-CBR with learned relevance performs competitively with decision trees in terms of accuracy while producing significantly more compact models (up to 94.2% reduction in size). The authors also demonstrate that majority voting is the most effective strategy for handling incoherence in the case base.

## Method Summary
The method extracts binary features from CART decision trees to represent cases for AA-CBR. Decision tree splits act as binary features, allowing tabular data handling in the abstract argumentation framework. The approach uses pre-pruning (limited depth and leaves) to control overfitting, extracts binary features from splits for case representation, and applies majority voting to handle incoherence. The evaluation compares AA-CBR with learned relevance to decision trees using 5-fold cross-validation on accuracy and model size metrics.

## Key Results
- AA-CBR with learned relevance achieves competitive accuracy with decision trees on COMPAS and Welfare Benefits datasets
- AA-CBR models are up to 94.2% smaller than decision trees, potentially enhancing cognitive tractability of explanations
- Majority voting is the most effective strategy for handling incoherence in the case base

## Why This Works (Mechanism)

### Mechanism 1
- Decision tree splits act as binary features for AA-CBR, enabling tabular data handling
- CART decision tree greedily selects feature-split thresholds to minimize loss; each split becomes a binary feature
- Core assumption: Decision tree splits preserve sufficient information to represent cases for AA-CBR relevance reasoning
- Break condition: If decision tree splits discard discriminative features or if the subset-based relevance ordering no longer captures case specificity

### Mechanism 2
- Majority voting strategy mitigates incoherence in casebases
- When multiple cases share the same binary feature representation but differ in outcome, majority voting selects the most frequent outcome
- Core assumption: The majority outcome is a good approximation of the "true" label for that case representation
- Break condition: If class imbalance is severe or the majority label is systematically wrong

### Mechanism 3
- Compact AA-CBR models yield cognitively tractable explanations
- AA-CBR models built from binary splits are smaller than decision trees, leading to shorter explanation trees
- Core assumption: Smaller models are easier for humans to understand and trace reasoning
- Break condition: If explanation size is dominated by the number of cases rather than model structure

## Foundational Learning

- Concept: Abstract Argumentation Frameworks (AFs)
  - Why needed here: AA-CBR models cases as arguments and uses attack relations derived from relevance to determine outcomes
  - Quick check question: In an AF, if argument A attacks B and B has no attackers, is B in the grounded extension?

- Concept: Partial order and relevance in CBR
  - Why needed here: Specificity between cases is captured by a partial order, which defines relevance and irrelevance for retrieving past cases
  - Quick check question: If case X is more specific than Y, does that mean X ⪰ Y in the AA-CBR ordering?

- Concept: Decision Tree feature binarization
  - Why needed here: CART splits are converted into binary features so AA-CBR can reason about case similarity via subset inclusion
  - Quick check question: If a split threshold is 5, what are the two binary feature values for values 3 and 7?

## Architecture Onboarding

- Component map: Data → CART Decision Tree (binary splits extraction) → Binary feature case representation → AA-CBR framework → AF mining → Grounded extension → Outcome prediction
- Critical path:
  1. Train CART decision tree with max depth/leaf constraints
  2. Extract binary features from split tests for each example
  3. Build casebase D from binary features and labels
  4. Handle incoherence (prefer majority)
  5. Construct AF from D and new case
  6. Compute grounded extension to determine outcome
- Design tradeoffs:
  - Tree depth vs. overfitting: deeper trees capture more nuance but risk noise
  - Incoherence handling: keep preserves all data but risks unstable AFs; removal reduces noise but may lose information; majority is stable but can bias toward majority class
  - Explanation size vs. detail: smaller AFs are easier to read but may omit nuanced distinctions
- Failure signatures:
  - Low accuracy despite good tree performance → feature binarization losing critical distinctions
  - AF with very few nodes → splits too coarse or dataset too small
  - Incoherent AF despite majority voting → majority label itself is noisy or wrong
- First 3 experiments:
  1. Train CART on COMPAS with varying max depth; plot training/test accuracy vs. depth to find sweet spot
  2. Compare three incoherence strategies (keep/remove/majority) on same trained tree; measure AF size and prediction accuracy
  3. Generate ADTs for a few COMPAS cases; compare node counts and depths against decision paths from the original tree

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AA-CBR with learned relevance compare to other CBR approaches in legal domains?
- Basis in paper: The paper mentions comparing AA-CBR with decision trees but does not compare it to other CBR approaches like precedential constraint or factor-based methods
- Why unresolved: The paper focuses on demonstrating the viability of learning relevance in AA-CBR but does not extend the comparison to other established CBR methods in the legal domain
- What evidence would resolve it: Conducting experiments comparing AA-CBR with learned relevance to other CBR approaches like precedential constraint or factor-based methods on the same legal datasets

### Open Question 2
- Question: How can case relevance be learned for unstructured data like images and text?
- Basis in paper: The authors mention this as a promising avenue for future work, noting that their current approach is limited to tabular data
- Why unresolved: The paper focuses on tabular data and does not explore methods for learning relevance from unstructured data
- What evidence would resolve it: Developing and evaluating methods for learning case relevance from unstructured data like images and text, and testing their performance in AA-CBR

### Open Question 3
- Question: How do users perceive the explanations generated by AA-CBR compared to decision trees in terms of cognitive tractability?
- Basis in paper: The paper suggests that AA-CBR models are more compact and potentially more cognitively tractable, but does not empirically test this with users
- Why unresolved: The paper provides theoretical arguments for the cognitive benefits of AA-CBR explanations but lacks empirical validation through user studies
- What evidence would resolve it: Conducting user studies to compare the perceived cognitive tractability of AA-CBR explanations (e.g., ADTs) with decision tree explanations

## Limitations

- The 94.2% size reduction metric isn't standardized across different model types
- The superiority of majority voting for incoherence handling is based on limited empirical comparison without theoretical justification
- The cognitive tractability claim lacks user studies or human evaluation metrics

## Confidence

- High confidence: Decision trees can be converted to binary features for AA-CBR representation
- Medium confidence: AA-CBR with learned relevance achieves competitive accuracy with decision trees
- Low confidence: The 94.2% size reduction translates to meaningfully improved human interpretability

## Next Checks

1. Replicate the experiment on an additional tabular dataset to test generalizability
2. Conduct a human evaluation study measuring time-to-understanding for AA-CBR vs decision tree explanations of equal prediction accuracy
3. Test AA-CBR performance when decision tree depth is constrained to match the number of relevant features, isolating the impact of feature selection from model compression