---
ver: rpa2
title: Fine-tuning can cripple your foundation model; preserving features may be the
  solution
arxiv_id: '2308.13320'
source_url: https://arxiv.org/abs/2308.13320
tags:
- training
- completion
- fine-tuning
- fine-tuned
- eurosat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Fine-tuning foundation models often causes concept forgetting,\
  \ degrading performance on tasks outside the fine-tuning domain. To address this,\
  \ we propose LDIFS, which minimizes the \u21132 distance between features of the\
  \ pre-trained and fine-tuned models."
---

# Fine-tuning can cripple your foundation model; preserving features may be the solution

## Quick Facts
- arXiv ID: 2308.13320
- Source URL: https://arxiv.org/abs/2308.13320
- Authors: 
- Reference count: 40
- Key outcome: Fine-tuning foundation models often causes concept forgetting, degrading performance on tasks outside the fine-tuning domain.

## Executive Summary
Fine-tuning foundation models often degrades their ability to recognize concepts learned during pre-training, a phenomenon called concept forgetting. The authors propose LDIFS, which minimizes the ℓ2 distance between features of pre-trained and fine-tuned models across multiple layers. This approach significantly reduces concept forgetting compared to standard fine-tuning methods while maintaining high downstream task accuracy. Experiments on 9 diverse datasets demonstrate LDIFS's effectiveness in both single-task and continual fine-tuning scenarios.

## Method Summary
The authors propose LDIFS (Layer-wise Distance-based Initialization and Fine-tuning), which adds a regularization term to the fine-tuning objective that minimizes the ℓ2 distance between features of the pre-trained and fine-tuned models. Unlike previous methods that only preserve last-layer features, LDIFS concatenates features from multiple internal representations to capture a more comprehensive representation of the model's knowledge. The method is evaluated against six baselines including standard fine-tuning, L2 regularization, and prompt tuning across 9 image classification datasets.

## Key Results
- LDIFS significantly reduces concept forgetting compared to standard fine-tuning methods
- LP-init-LDIFS achieves high downstream task accuracy while maintaining performance on unrelated tasks
- LDIFS supports continual fine-tuning across multiple tasks without losing prior knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LDIFS reduces concept forgetting by preserving the feature space similarity between the pre-trained and fine-tuned models.
- Mechanism: The LDIFS regularizer minimizes the ℓ2 distance between the feature representations of the pre-trained and fine-tuned models, ensuring that the fine-tuned model's input-output behavior remains close to the pre-trained one.
- Core assumption: The feature space captures the model's true input-output behavior and knowledge about real-world concepts.
- Evidence anchors:
  - [abstract]: "LDIFS significantly reduces concept forgetting compared to standard fine-tuning methods."
  - [section]: "we find that it is much effective to define it in terms of distance in the model's feature space which captures its true input-output behaviour."
  - [corpus]: Weak/no direct evidence on this specific claim. The related works focus on fine-tuning generalization but not on preserving feature space similarity.
- Break condition: If the feature space does not accurately represent the model's knowledge or if the distance metric does not correlate with concept preservation.

### Mechanism 2
- Claim: Preserving features across multiple layers is more effective than preserving only the last layer features.
- Mechanism: By concatenating features from various internal representations (not just the last layer), LDIFS captures a more comprehensive representation of the model's knowledge, leading to better preservation of concepts.
- Core assumption: Intermediate layer features contribute significantly to the model's overall knowledge and concept representation.
- Evidence anchors:
  - [abstract]: "we observe that simply preserving the last-layer features is not effective in reducing concept forgetting, therefore, motivated by [36], we preserve features extracted from different internal representations."
  - [section]: "the feature vector Φθv(t)(xi) is obtained by concatenating various internal representations (not just the last layer features) of the network architecture, similar to the perceptual features presented in [36]."
  - [corpus]: Weak/no direct evidence on this specific claim. The related works focus on fine-tuning methods but not on multi-layer feature preservation.
- Break condition: If intermediate layer features do not contribute significantly to concept preservation or if concatenating them does not improve performance.

### Mechanism 3
- Claim: LDIFS allows for continual fine-tuning across multiple tasks without losing prior knowledge.
- Mechanism: By preserving the feature space similarity during fine-tuning, LDIFS enables the model to accumulate knowledge from multiple tasks while maintaining its ability to recognize concepts learned during pre-training.
- Core assumption: The feature space preservation during fine-tuning translates to knowledge preservation across tasks.
- Evidence anchors:
  - [abstract]: "LDIFS is highly effective in performing continual fine-tuning on a sequence of tasks as well, in comparison with both fine-tuning as well as continual learning baselines."
  - [section]: "we look at a sequence of 2 fine-tuning tasks instead of 1 and find a desirable performance from LDIFS compared to other fine-tuning methods even in this setting."
  - [corpus]: Weak/no direct evidence on this specific claim. The related works focus on fine-tuning and continual learning but not on the combination with feature space preservation.
- Break condition: If feature space preservation during fine-tuning does not translate to knowledge preservation across tasks or if the model still experiences catastrophic forgetting.

## Foundational Learning

- Concept: Concept Forgetting
  - Why needed here: Understanding concept forgetting is crucial to grasp the problem LDIFS aims to solve and why it is important to preserve the model's knowledge about real-world concepts.
  - Quick check question: What is concept forgetting, and why is it undesirable in the context of fine-tuning foundation models?

- Concept: Feature Space
  - Why needed here: The feature space is the key component that LDIFS operates on to preserve the model's knowledge and reduce concept forgetting.
  - Quick check question: What is the feature space, and how does it relate to the model's input-output behavior and knowledge representation?

- Concept: Regularization
  - Why needed here: LDIFS uses a regularization term to enforce feature space similarity between the pre-trained and fine-tuned models, making it essential to understand the concept of regularization in machine learning.
  - Quick check question: What is regularization, and how does it help in controlling the model's behavior during fine-tuning?

## Architecture Onboarding

- Component map:
  Pre-trained foundation model -> Fine-tuning dataset -> LDIFS regularizer -> Feature extraction module -> Optimization loop -> Updated model

- Critical path:
  1. Load pre-trained foundation model and fine-tuning dataset
  2. Initialize fine-tuning with LDIFS regularizer
  3. Extract features from pre-trained and fine-tuned models at each training step
  4. Compute ℓ2 distance between feature representations
  5. Update model parameters using the LDIFS objective
  6. Evaluate model performance on downstream tasks and concept preservation

- Design tradeoffs:
  - Feature extraction vs. computational cost: Extracting features from multiple layers increases computational overhead but may improve concept preservation.
  - Regularization strength (λLDIFS): Higher values enforce stronger feature space similarity but may hinder task-specific learning.
  - Feature space distance metric: ℓ2 distance is used in LDIFS, but other metrics (e.g., cosine similarity) may be explored.

- Failure signatures:
  - Increased concept forgetting: If LDIFS does not effectively preserve the model's knowledge about real-world concepts.
  - Degraded downstream task performance: If LDIFS hinders the model's ability to learn task-specific information.
  - Computational overhead: If feature extraction and distance computation become too expensive for large models or datasets.

- First 3 experiments:
  1. Evaluate concept forgetting on a simple fine-tuning task with and without LDIFS regularization.
  2. Compare LDIFS performance with other fine-tuning methods (e.g., L2SP, prompt tuning) on a set of diverse downstream tasks.
  3. Assess the impact of different feature extraction strategies (e.g., number of layers, distance metrics) on LDIFS performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does preserving features across all layers of a foundation model provide better concept retention than preserving only a subset of layers?
- Basis in paper: [explicit] The authors mention that preserving features from different internal representations (not just last-layer features) is more effective in reducing concept forgetting.
- Why unresolved: The paper does not compare feature preservation across different numbers of layers or identify the optimal subset of layers for preservation.
- What evidence would resolve it: Controlled experiments comparing concept forgetting when preserving features from 1, 3, 5, and all layers during fine-tuning.

### Open Question 2
- Question: How does the ordering of fine-tuning tasks affect knowledge retention and forward transfer in foundation models?
- Basis in paper: [explicit] The authors note that task ordering impacts forward transfer, with SVHN → EuroSAT showing positive transfer but EuroSAT → SVHN showing negative transfer.
- Why unresolved: The paper only examines two-task sequences and does not systematically investigate how different task orders or longer sequences affect concept retention.
- What evidence would resolve it: Extensive experiments testing multiple task orderings and sequences to identify patterns in knowledge retention and transfer.

### Open Question 3
- Question: What is the optimal regularization strength (λ) for LDIFS across different types of foundation models and downstream tasks?
- Basis in paper: [explicit] The authors perform a grid search for λ and find λ=10 works best generally, but note this could vary.
- Why unresolved: The paper only tests λ values on CLIP models and doesn't explore whether optimal λ differs across model architectures or task types.
- What evidence would resolve it: Systematic testing of LDIFS across different foundation models (LLMs, multimodal models) and task categories to determine optimal λ ranges.

## Limitations

- Implementation details for feature concatenation across layers are not fully specified
- Computational overhead of extracting features from multiple layers for large models is not addressed
- Experiments focus only on image classification tasks, limiting generalizability to other domains

## Confidence

- **High Confidence**: The observation that preserving features across layers reduces concept forgetting compared to preserving only last-layer features
- **Medium Confidence**: The claim that LDIFS enables continual fine-tuning without catastrophic forgetting
- **Medium Confidence**: The assertion that the feature space captures the model's true input-output behavior

## Next Checks

1. Perform an ablation study varying which layers are included in the feature concatenation to determine the optimal combination for concept preservation

2. Evaluate LDIFS on a longer sequence of fine-tuning tasks (5+ tasks) to better assess its ability to prevent catastrophic forgetting in more challenging continual learning scenarios

3. Test LDIFS on tasks outside the image classification domain (e.g., natural language understanding or speech tasks) to evaluate its effectiveness across different types of foundation models and task families