---
ver: rpa2
title: 'BELB: a Biomedical Entity Linking Benchmark'
arxiv_id: '2308.11537'
source_url: https://arxiv.org/abs/2308.11537
tags:
- entity
- belb
- biomedical
- corpora
- ncbi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BELB provides a standardized benchmark for biomedical entity linking,
  covering 11 corpora and 7 knowledge bases across six entity types. It addresses
  the lack of a unified evaluation setup, which has led to inconsistent experimental
  designs and limited comparability across studies.
---

# BELB: a Biomedical Entity Linking Benchmark

## Quick Facts
- **arXiv ID:** 2308.11537
- **Source URL:** https://arxiv.org/abs/2308.11537
- **Reference count:** 40
- **Primary result:** Provides a standardized benchmark for biomedical entity linking with 11 corpora, 7 knowledge bases, and six entity types, enabling fair comparisons of rule-based and neural approaches.

## Executive Summary
BELB addresses the lack of a unified evaluation setup in biomedical entity linking (BEL) by providing a standardized benchmark covering 11 corpora and 7 knowledge bases across six entity types. The benchmark reduces preprocessing overhead and enables reproducible experiments through uniformly preprocessed data and a standardized evaluation protocol. Experiments reveal that rule-based entity-specific systems still outperform neural models for genes and variants, highlighting the need for more robust, entity-agnostic neural approaches that can handle homonyms and scale to large knowledge bases.

## Method Summary
BELB provides access to 11 biomedical corpora (NCBI Disease, BC5CDR, NLM-Chem, Linnaeus, S800, BioID, GNormPlus, NLM-Gene, SNP, Osiris v1.2, tmVar v3, MedMentions) linked to 7 knowledge bases (CTD Diseases, CTD Chemicals, Cellosaurus, NCBI Taxonomy, NCBI Gene, dbSNP, UMLS). The benchmark includes a unified schema for representing knowledge bases and a standardized evaluation protocol that handles unseen entities, synonyms, and homonyms. The study compares six rule-based entity-specific systems (GNormPlus, TaggerOne, BC7T2W, SR4GN, tmVar, SciSpacy) and three neural approaches (BioSyn, GenBioEL, arboEL) using mention-level recall@1 as the primary metric.

## Key Results
- Rule-based entity-specific systems consistently outperform neural models for genes and variants, despite neural approaches showing promise for other entity types.
- Neural models struggle to scale to large knowledge bases like NCBI Gene and dbSNP, requiring the use of subsets for experiments.
- The benchmark reveals that homonym handling remains a significant challenge for neural models, which lack the ad-hoc components (like abbreviation resolution) used by rule-based systems.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BELB reduces preprocessing overhead and maximizes comparability across biomedical entity linking studies.
- Mechanism: By providing uniformly preprocessed corpora and knowledge bases in a unified format, BELB eliminates the need for custom input-output routines and quality assurance steps for each new dataset.
- Core assumption: Consistent preprocessing and formatting of biomedical text mining datasets will lead to more reliable and reproducible experimental results.
- Evidence anchors:
  - [abstract]: "BELB greatly reduces preprocessing overhead in testing BEL systems on multiple corpora offering a standardized testbed for reproducible experiments."
  - [section 2.1.1]: "In BELB we cover six entity types... represented by 11 corpora linked to 7 specialized KBs... This allows to test a model’s ability to preserve performance across multiple pairs of corpus and KB with minimal preprocessing overhead."

### Mechanism 2
- Claim: BELB facilitates testing on specialized knowledge bases and addresses scalability and homonym challenges.
- Mechanism: By including a diverse set of specialized knowledge bases (e.g., NCBI Gene, dbSNP) alongside the more general UMLS, BELB allows researchers to evaluate the performance of entity linking models on different scales and types of entities.
- Core assumption: Performance on a broad-coverage knowledge base like UMLS may not generalize well to specialized knowledge bases, and testing on both types is necessary to assess the robustness of entity linking models.
- Evidence anchors:
  - [abstract]: "By introducing entity types such as genes and variants, BELB allows to probe a model’s ability to (i) identify contextual information and (ii) handle highly ambiguous search spaces (KB)."
  - [section 2.1.1]: "As mentioned in Section 2.1.1 studies on neural methods have primarily targeted UMLS, which, as shown in Table 2, is one and three order of magnitude(s) smaller than NCBI Gene and dbSNP, respectively."

### Mechanism 3
- Claim: BELB enables fair comparisons of rule-based and neural approaches by providing a standardized evaluation protocol and accounting for unseen entities and synonyms.
- Mechanism: BELB's standardized evaluation protocol, which includes handling of unseen entities, synonyms, and homonyms, allows for a fair assessment of the strengths and weaknesses of different entity linking approaches.
- Core assumption: A standardized evaluation protocol that accounts for the complexities of biomedical entity linking, such as unseen entities and synonyms, will provide a more accurate comparison of different approaches than ad-hoc evaluation setups.
- Evidence anchors:
  - [abstract]: "Using BELB we perform an extensive evaluation of six rule-based entity-specific systems and three recent neural approaches... Our results reveal a mixed picture showing that neural approaches fail to perform consistently across entity types."
  - [section 2.1.2]: "In BELB we assign a unique identifier to each mention and provide lists of mentions of (i) unseen entities... and (ii) train entities occurring in the test set but with different (case-insensitive) surface forms... This allows to easily report a model’s performance in (i) generalizing to new entities and (ii) recognizing known ones appearing in different forms."

## Foundational Learning

- Concept: Biomedical entity linking (BEL) and its role in information extraction pipelines
  - Why needed here: Understanding the task of BEL and its importance in biomedical text mining is crucial for appreciating the value of BELB and the need for a standardized benchmark.
  - Quick check question: What is the main goal of biomedical entity linking, and why is it important for downstream applications like indexing PubMed?

- Concept: Knowledge bases and their role in entity linking
  - Why needed here: Knowledge bases provide the target entities that mentions in text need to be linked to. Understanding the different types of knowledge bases (e.g., specialized vs. broad-coverage) and their characteristics is essential for interpreting the results of entity linking experiments.
  - Quick check question: What is the difference between a specialized knowledge base (e.g., NCBI Gene) and a broad-coverage knowledge base (e.g., UMLS), and why might performance on one not generalize to the other?

- Concept: Homonyms and their impact on entity linking
  - Why needed here: Homonyms, where the same surface form refers to different entities, are a key challenge in entity linking. Understanding how they are handled by different approaches is crucial for interpreting the results of the benchmarking study.
  - Quick check question: How do homonyms affect the performance of entity linking models, and what strategies can be used to address this challenge?

## Architecture Onboarding

- Component map:
  - BELB -> 11 corpora -> 7 knowledge bases -> unified schema -> standardized evaluation protocol
  - Rule-based systems (GNormPlus, TaggerOne, BC7T2W, SR4GN, tmVar, SciSpacy) and neural models (BioSyn, GenBioEL, arboEL) -> BELB benchmark

- Critical path:
  - Select a corpus and its corresponding knowledge base from BELB
  - Preprocess the corpus and knowledge base using BELB's unified schema
  - Train and evaluate the entity linking model on the preprocessed data
  - Use BELB's standardized evaluation protocol to assess the model's performance, including handling of unseen entities, synonyms, and homonyms

- Design tradeoffs:
  - BELB prioritizes coverage of diverse entity types and knowledge bases over depth in any single domain
  - The use of a unified schema may not capture all the nuances of individual knowledge bases, potentially introducing some loss of information
  - The benchmarking study focuses on comparing rule-based and neural approaches, but other types of entity linking models (e.g., hybrid approaches) are not included

- Failure signatures:
  - Poor performance on unseen entities or synonyms may indicate that the model is overfitting to the training data and not generalizing well
  - Significant performance differences between knowledge bases may suggest that the model is not robust to variations in scale and entity ambiguity
  - Inability to handle homonyms effectively may point to a lack of contextual understanding or insufficient training data

- First 3 experiments:
  1. Evaluate a simple exact-match approach on BELB to establish a baseline for comparison with more sophisticated models
  2. Compare the performance of a rule-based entity-specific system (e.g., GNormPlus for genes) and a neural model (e.g., BioSyn) on a subset of BELB corpora to identify strengths and weaknesses of each approach
  3. Investigate the impact of handling homonyms by comparing the performance of a model with and without an abbreviation resolution component (e.g., Ab3P) on a corpus with high homonym density (e.g., NCBI Disease)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can neural models be improved to handle homonyms in biomedical entity linking more effectively?
- Basis in paper: [explicit] The paper discusses that homonyms are a significant challenge for neural models, which lack ad-hoc components like those used by rule-based systems (e.g., abbreviation resolution tools). It suggests that introducing similar approaches for neural models could improve performance.
- Why unresolved: Neural models are designed to learn from data, and the paper emphasizes the need for entity-agnostic models rather than relying on hand-crafted heuristics. This creates a tension between improving performance and maintaining the principles of neural learning.
- What evidence would resolve it: Experiments comparing neural models with and without homonym-handling components, and studies on developing neural architectures that inherently handle ambiguity without custom heuristics.

### Open Question 2
- Question: Can neural models be optimized to scale effectively to large knowledge bases like NCBI Gene and dbSNP?
- Basis in paper: [explicit] The paper notes that neural models fail to scale to large KBs, requiring the use of subsets for experiments. It raises the question of how to handle the full scale of these databases.
- Why unresolved: The computational and memory requirements for scaling neural models to large KBs are significant, and the paper does not provide a solution or explore potential optimizations.
- What evidence would resolve it: Studies on efficient neural architectures, memory optimization techniques, or distributed training methods that enable scaling to large KBs without compromising performance.

### Open Question 3
- Question: How does the performance of neural models change with hyperparameter optimization and pre-training strategies?
- Basis in paper: [explicit] The paper mentions that no hyperparameter exploration was conducted due to computational constraints, and it suggests that optimizing hyperparameters or using pre-training strategies could improve results.
- Why unresolved: The lack of hyperparameter tuning and pre-training exploration leaves open the question of whether neural models could perform better with these optimizations.
- What evidence would resolve it: Comprehensive hyperparameter searches and experiments comparing models with and without pre-training on KBs, showing the impact on performance across different entity types.

### Open Question 4
- Question: Can entity-agnostic neural models be developed that match the performance of entity-specific rule-based systems?
- Basis in paper: [explicit] The paper highlights that rule-based entity-specific systems outperform neural models, especially for genes and variants, and suggests the need for further studies towards entity-agnostic models.
- Why unresolved: While the paper identifies the performance gap, it does not propose or test specific approaches to develop entity-agnostic neural models that could close this gap.
- What evidence would resolve it: Development and evaluation of neural architectures designed to be entity-agnostic, demonstrating comparable or superior performance to entity-specific rule-based systems across multiple entity types.

## Limitations

- BELB focuses on entity linking accuracy without addressing broader challenges like temporal or contextual dependencies in biomedical text.
- The diversity of entity types and knowledge bases, while extensive, may still be insufficient to capture all real-world variations in biomedical entity linking.
- The evaluation protocol's reliance on mention-level recall@1 may not fully capture the complexity of entity linking tasks, particularly for ambiguous or context-dependent mentions.

## Confidence

- **High:** Need for a standardized benchmark in biomedical entity linking and reduction of preprocessing overhead
- **Medium:** Claim that rule-based systems still outperform neural models for certain entity types, given the limited scope of neural approaches tested
- **Low:** Generalizability of results to all biomedical entity linking scenarios due to benchmark's specific focus and potential domain-specific variations

## Next Checks

1. Test the benchmark's scalability by evaluating models on additional knowledge bases not included in the original study.
2. Assess the impact of preprocessing choices by comparing results with alternative preprocessing pipelines.
3. Investigate the robustness of entity linking models to temporal changes in biomedical knowledge by evaluating performance on corpora spanning different time periods.