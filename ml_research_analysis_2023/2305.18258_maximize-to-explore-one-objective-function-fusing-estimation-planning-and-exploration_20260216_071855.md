---
ver: rpa2
title: 'Maximize to Explore: One Objective Function Fusing Estimation, Planning, and
  Exploration'
arxiv_id: '2305.18258'
source_url: https://arxiv.org/abs/2305.18258
tags:
- function
- model-free
- learning
- hypothesis
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Maximize to Explore (MEX), a novel framework
  for balancing exploration and exploitation in online reinforcement learning with
  general function approximations. The key idea is to optimize a single objective
  that combines planning (maximizing expected return) and estimation (minimizing prediction
  error), automatically balancing exploration and exploitation without requiring constrained
  optimization or complex sampling.
---

# Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration

## Quick Facts
- arXiv ID: 2305.18258
- Source URL: https://arxiv.org/abs/2305.18258
- Reference count: 40
- Key outcome: MEX achieves sublinear regret O(Poly(H)·dGEC(1/√HK)^(1/2)·K^(1/2)) in online RL with general function approximations

## Executive Summary
This paper introduces Maximize to Explore (MEX), a novel framework that unifies exploration and exploitation in online reinforcement learning through a single objective function. By maximizing a weighted sum of optimal expected return and negative estimation error, MEX automatically balances the exploration-exploitation tradeoff without requiring constrained optimization or complex sampling strategies. The framework extends to two-player zero-sum Markov games and demonstrates strong empirical performance in sparse reward MuJoCo environments compared to baseline methods.

## Method Summary
MEX operates by maximizing a composite objective V1,f - η·ΣhLk-1h(f) at each episode, where V1,f is the optimal expected return under hypothesis f and Lk-1h(f) measures estimation error. The algorithm alternates between solving for the best hypothesis in a function class and executing an exploration policy derived from this hypothesis. Two implementations are provided: MEX-MF uses squared Bellman errors for model-free learning, while MEX-MB uses negative log-likelihood for model-based learning. The framework naturally extends to two-player zero-sum games by maintaining separate hypotheses for each player with asymmetric objectives.

## Key Results
- Achieves sublinear regret bounds scaling with √(dGEC(1/√HK)·K) under mild assumptions
- Extends naturally to two-player zero-sum Markov games with appropriate modifications to the objective
- Outperforms baseline methods in sparse reward MuJoCo environments including hopper-vel-sparse and ant-goal-sparse
- Demonstrates sample efficiency through effective generalization enabled by the Generalized Eluder Coefficient

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MEX unifies estimation and planning into a single objective, enabling automatic exploration-exploitation balance without requiring constrained optimization.
- Mechanism: The algorithm maximizes V1,f - η·ΣhLk-1h(f), where V1,f is the optimal expected return under hypothesis f and Lk-1h(f) is the estimation error. By maximizing this composite objective, the agent simultaneously improves its value estimates (exploration) and exploits current knowledge (planning).
- Core assumption: The hypothesis class H contains the true model f*, and the loss functions Lk-1h(f) provide meaningful estimates of prediction error that correlate with out-of-sample performance.
- Evidence anchors:
  - [abstract]: "MEX propose to maximize a weighted sum of two objectives: (a) the optimal expected total return associated with a given hypothesis, and (b) the negative estimation error of that hypothesis"
  - [section]: "MEX propose to maximize a weighted sum of two objectives: (a) the optimal expected total return associated with a given hypothesis, and (b) the negative estimation error of that hypothesis"
- Break condition: If the loss function Lk-1h(f) becomes a poor proxy for generalization error (e.g., due to distribution shift or model misspecification), the exploration incentive breaks down.

### Mechanism 2
- Claim: MEX achieves sample efficiency by leveraging the Generalized Eluder Coefficient (GEC) to quantify learning complexity in function approximation settings.
- Mechanism: The regret bound scales with √(dGEC(1/√HK)·K), where GEC measures how well past data enables prediction of future outcomes. Low GEC means the agent can generalize effectively from limited samples, enabling sample-efficient learning.
- Core assumption: The MDP has low GEC, which implies that minimizing in-sample prediction error effectively reduces out-of-sample error.
- Evidence anchors:
  - [abstract]: "Under mild structural assumptions, we prove that MEX achieves a sublinear regret O(Poly(H)·dGEC(1/√HK)^(1/2)·K^(1/2) with general function approximators"
  - [section]: "We prove that MEX achieves a sublinear eO(Poly(H)d^(1/2)GEC(1/√HK)K^(1/2) regret under mild assumptions"
- Break condition: If the underlying MDP has high GEC (e.g., requires memorization rather than generalization), the sample efficiency guarantee fails.

### Mechanism 3
- Claim: MEX naturally extends to two-player zero-sum Markov games by maintaining separate hypotheses for max-player and min-player with asymmetric objectives.
- Mechanism: The algorithm estimates f^k for the max-player (maximizing V1,f) and gk for the min-player (minimizing Vμk,†1,g), where the min-player assists the max-player's regret minimization. This self-play structure enables learning near-Nash policies.
- Core assumption: The hypothesis class contains the true Nash equilibrium policies, and the two-player GEC (TGEC) characterizes learning complexity appropriately.
- Evidence anchors:
  - [abstract]: "the paper also extends MEX to two-player zero-sum Markov games"
  - [section]: "we further extend the definition of generalized eluder coefficient (GEC) to two-player zero-sum MGs"
- Break condition: If the two-player game has high TGEC or the hypothesis class lacks sufficient capacity, the algorithm cannot learn near-optimal policies.

## Foundational Learning

- Concept: Bellman equations and value function iteration
  - Why needed here: MEX relies on estimating value functions under different hypotheses, and understanding Bellman optimality is crucial for interpreting the planning component
  - Quick check question: Can you derive the Bellman optimality equation Q*(x,a) = r(x,a) + E[VQ(x')] and explain how it relates to planning?

- Concept: Function approximation and generalization
  - Why needed here: MEX operates in general function approximation settings where the agent must generalize from limited samples using hypothesis classes like neural networks
  - Quick check question: What's the difference between model-free and model-based function approximation, and when would each be preferable?

- Concept: Exploration-exploitation tradeoff and optimism
  - Why needed here: MEX's core innovation is balancing exploration and exploitation through a single objective, requiring understanding of why exploration is needed and how optimism principles work
  - Quick check question: Why does adding an exploration bonus to the reward function encourage the agent to visit uncertain states?

## Architecture Onboarding

- Component map:
  - Hypothesis class H -> Contains approximators for optimal value functions (model-free) or transition kernels (model-based)
  - Loss function Lk-1h(f) -> Measures estimation error on historical data (Bellman residual or log-likelihood)
  - Composite objective V1,f - η·ΣhLk-1h(f) -> Balances exploitation and exploration
  - Policy extraction πf^k for max-player, νgk,μk for min-player -> Extracts policies from hypotheses
  - Data collection execute exploration policy πexp(f^k) -> Gathers new samples for next iteration

- Critical path:
  1. At each episode k, solve f^k = argmaxf∈H[V1,f - η·ΣhLk-1h(f)]
  2. Set max-player policy μk = μf^k
  3. Solve gk = argmaxg∈H[-Vμk,†1,g - η·ΣhLk-1h,μk(g)]
  4. Set min-player policy νk = νgk,μk
  5. Execute joint policy (μk, νk) and collect data
  6. Update loss functions Lk h(f) for next iteration

- Design tradeoffs:
  - Single objective vs. separate exploration/exploitation: MEX trades off modularity for simplicity and easier implementation
  - Fixed η vs. adaptive: Fixed η simplifies analysis but may not be optimal across all environments
  - Model-free vs. model-based: Model-free requires less memory but model-based can be more sample efficient when dynamics are learnable

- Failure signatures:
  - High variance in value estimates → η too large, causing excessive exploration
  - Premature convergence to suboptimal policies → η too small, insufficient exploration
  - Computational bottlenecks in optimization → hypothesis class H too large or complex
  - Poor performance in sparse reward tasks → loss function not capturing relevant structure

- First 3 experiments:
  1. Implement MEX-MF on a simple tabular MDP with known optimal policy to verify basic functionality
  2. Test MEX-MF vs. TD3 on a continuous control task (e.g., HalfCheetah) with dense rewards to compare sample efficiency
  3. Evaluate MEX-MB on a sparse reward navigation task (e.g., Ant-goal) to test exploration capabilities in challenging scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MEX's performance scale with increasing horizon length H in terms of regret bounds and computational cost?
- Basis in paper: [explicit] The paper states regret bounds scale as Poly(H) · dGEC(1/√HK)^(1/2) · K^(1/2), but doesn't analyze scaling with large H.
- Why unresolved: The paper only provides theoretical bounds for fixed H and doesn't empirically test scaling with horizon length.
- What evidence would resolve it: Experiments varying H and analyzing both regret growth and computational overhead, or theoretical analysis of Poly(H) term growth rate.

### Open Question 2
- Question: Can MEX be extended to multi-agent settings beyond two-player zero-sum games, such as general-sum games or cooperative multi-agent RL?
- Basis in paper: [inferred] The paper extends MEX to two-player zero-sum games but doesn't explore other multi-agent scenarios.
- Why unresolved: The paper's extensions are limited to two-player zero-sum games, and it's unclear how MEX would perform in more complex multi-agent settings.
- What evidence would resolve it: Theoretical analysis and experiments applying MEX to general-sum games or cooperative multi-agent RL problems.

### Open Question 3
- Question: How sensitive is MEX's performance to the choice of the exploration policy πexp(f) in practice?
- Basis in paper: [explicit] The paper mentions that the choice of πexp(f) depends on the specific MDP structure but doesn't provide detailed analysis.
- Why unresolved: The paper only briefly mentions the importance of πexp(f) but doesn't investigate its impact on performance.
- What evidence would resolve it: Experiments comparing different exploration policies (e.g., uniform, greedy, Boltzmann) and analyzing their effect on MEX's performance across various environments.

## Limitations

- Theoretical guarantees depend on the Generalized Eluder Coefficient being small, but computing or estimating GEC in practice remains challenging
- Performance is sensitive to the choice of exploration parameter η, which is fixed rather than adaptive across environments
- Model-based implementation assumes accurate environment modeling, which may not hold in complex real-world scenarios with stochastic dynamics

## Confidence

- **High confidence**: The core algorithmic framework and its theoretical regret bounds (proven under stated assumptions) - the mathematical analysis is rigorous and follows established online RL techniques.
- **Medium confidence**: The empirical results showing MEX outperforming baselines in sparse reward environments - while the experiments are well-designed, the ablation studies are limited and the comparison methods are not state-of-the-art RL algorithms.
- **Low confidence**: The extension to two-player zero-sum Markov games - this is presented more conceptually with less empirical validation and the theoretical analysis is less detailed.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary η across multiple orders of magnitude and different environments to understand how critical this parameter is for balancing exploration and exploitation.

2. **Benchmark against modern RL algorithms**: Compare MEX against contemporary algorithms like SAC, MPO, or Dreamer that use entropy regularization and achieve state-of-the-art performance, rather than older baselines like TD3.

3. **Real-world complexity test**: Evaluate MEX in environments with partial observability, stochastic dynamics, or non-stationary rewards to assess robustness beyond the controlled MuJoCo settings with sparse rewards.