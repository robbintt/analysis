---
ver: rpa2
title: Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological
  Carbon Storage
arxiv_id: '2308.09113'
source_url: https://arxiv.org/abs/2308.09113
tags:
- pressure
- data
- high-fidelity
- multi-fidelity
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-fidelity Fourier neural operators can predict large-scale
  geological carbon storage with 81% less data generation cost while maintaining accuracy.
  The grid-invariant property of FNO simplifies transfer learning between different
  discretizations, enabling accurate predictions even when high-fidelity data are
  limited.
---

# Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological Carbon Storage

## Quick Facts
- arXiv ID: 2308.09113
- Source URL: https://arxiv.org/abs/2308.09113
- Reference count: 3
- Primary result: Multi-fidelity FNO achieves 81% less data generation cost while maintaining accuracy for large-scale geological carbon storage modeling

## Executive Summary
This study presents a multi-fidelity Fourier Neural Operator (FNO) approach for fast and accurate modeling of large-scale geological carbon storage (GCS). By leveraging the grid-invariant property of FNO, the method enables transfer learning between low-fidelity (coarse grid) and high-fidelity (fine grid) data, achieving comparable accuracy to high-fidelity models with significantly reduced data generation costs. The approach demonstrates promise for accelerating surrogate modeling in 3D GCS problems, particularly for pressure field predictions, though CO2 saturation predictions are less accurate due to the discontinuous nature of saturation fronts.

## Method Summary
The method involves training a low-fidelity FNO model on abundant coarse-grid data (32×32×28), then fine-tuning with limited high-fidelity data (64×64×28) to capture discretization-specific effects. The FNO's grid-invariant property allows the same network structure to process both discretizations without modification. The model learns the solution operator of PDEs governing CO2 injection into saline aquifers, with input features including porosity, permeability, injection rates, and time. Training uses Adam optimizer with learning rate 0.01 for pre-training and 0.005 for fine-tuning, spanning 400 and 200 epochs respectively.

## Key Results
- Achieves 81% less data generation cost while maintaining accuracy comparable to high-fidelity models
- Grid-invariant property enables transfer learning between different discretizations without architectural changes
- Generalizes to indirectly correlated datasets from different simulators, though saturation predictions are less accurate than pressure predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-fidelity FNO achieves high accuracy with 81% less data generation cost by leveraging grid-invariant property for transfer learning between discretizations.
- Mechanism: FNO's grid-invariant property allows the same network to process both low-fidelity and high-fidelity data without architectural modification. The model learns general physics from abundant low-fidelity data, then fine-tunes with limited high-fidelity data to capture discretization-specific effects.
- Core assumption: Physical processes learned from coarse discretization contain sufficient transferable features to the fine discretization problem, despite differences in pressure magnitude and plume sharpness.
- Evidence anchors:
  - [abstract]: "The multi-fidelity model can predict with accuracy comparable to a high-fidelity model trained with the same amount of high-fidelity data with 81% less data generation costs"
  - [section 2.2]: "The FNO has desirable grid-invariant features, which means that the model can be evaluated at any query points that are not necessarily members of the set of training grids"
  - [corpus]: Weak evidence - no direct corpus examples of this specific grid-invariant transfer learning approach

### Mechanism 2
- Claim: Multi-fidelity training works even when low-fidelity and high-fidelity datasets are indirectly correlated (different geostatistical models, simulators).
- Mechanism: FNO learns general physical relationships from low-fidelity data, then adapts to specific discretization and modeling differences through fine-tuning. The model extracts common physics while adjusting for simulator-specific artifacts.
- Core assumption: Despite different data sources, both datasets describe the same underlying physical process with sufficient shared features for transfer learning.
- Evidence anchors:
  - [abstract]: "This approach shows promise for accelerating surrogate modeling in large-scale 3D GCS problems" and "The findings of this study can help for better understanding of the transferability of multi-fidelity deep learning surrogate models"
  - [section 5]: "The model can predict pressure fields with acceptable accuracy after training using a limited amount of high-fidelity data (less than 20)"
  - [corpus]: Weak evidence - corpus neighbors discuss multi-fidelity approaches but not this specific indirect correlation scenario

### Mechanism 3
- Claim: FNO learns pressure dynamics better than CO2 saturation dynamics due to the nature of physical processes involved.
- Mechanism: Pressure diffusion is a smoother, more continuous process that FNO can approximate well from limited data. CO2 saturation involves sharp interfaces and discontinuities that are harder for data-driven models to capture accurately.
- Core assumption: Mathematical structure of FNO and its training procedure is better suited to continuous field predictions than discontinuous interface predictions.
- Evidence anchors:
  - [abstract]: "saturation predictions are less accurate than pressure predictions"
  - [section 5]: "The results indicate that the FNO model can learn pressure diffusion better than CO2 plume migration. The later one forms a contact discontinuity which is a challenging scenario for data-driven deep learning-based models to learn in general"
  - [corpus]: Weak evidence - corpus neighbors don't specifically address this accuracy differential

## Foundational Learning

- Concept: Fourier Neural Operator architecture and grid-invariant property
  - Why needed here: Understanding why FNO can transfer between different discretizations without architectural changes is crucial for implementing the multi-fidelity approach
  - Quick check question: What mathematical property of FNO enables it to process data at arbitrary grid resolutions?

- Concept: Transfer learning and fine-tuning procedures
  - Why needed here: The two-step training process (pre-training on low-fidelity, fine-tuning on high-fidelity) is central to the approach's success
  - Quick check question: Why is the learning rate typically lower during fine-tuning than during pre-training?

- Concept: Partial differential equation solutions as operator learning
  - Why needed here: FNO learns the solution operator of PDEs rather than specific solutions, enabling generalization to new inputs
  - Quick check question: How does learning an operator differ from learning a specific solution in terms of model generalization?

## Architecture Onboarding

- Component map: Input layer (porosity, permeability, injection rates, time) → Lifting layer → 4 Fourier layers with spectral operations → Projection layer → Output (pressure or saturation field)
- Critical path: Data generation → Low-fidelity pre-training → High-fidelity fine-tuning → Model evaluation
- Design tradeoffs: More Fourier modes improve accuracy but increase computational cost; larger low-fidelity dataset improves pre-training but increases data generation cost
- Failure signatures: Large pressure RMSE (>0.1 MPa) after fine-tuning indicates insufficient high-fidelity data or poor low-fidelity pre-training; non-physical saturation patterns suggest model instability
- First 3 experiments:
  1. Train LF model only and evaluate on high-fidelity data to establish baseline performance without transfer learning
  2. Fine-tune with minimum high-fidelity data (100 samples) and evaluate accuracy improvement
  3. Vary the ratio of low-fidelity to high-fidelity data to find optimal balance between data generation cost and prediction accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-fidelity FNO model performance scale with reservoir model complexity (e.g., inclusion of faults, varying permeability distributions)?
- Basis in paper: [inferred] The authors note the need to explore more complex geological features and control strategies, suggesting current testing is limited to relatively simple models.
- Why unresolved: The current study uses synthetic models without geological complexities like faults or highly heterogeneous permeability distributions that are common in real reservoirs.
- What evidence would resolve it: Testing the multi-fidelity FNO model on reservoir models with complex geological features (faults, stratigraphic layers, varying permeability distributions) and comparing performance metrics (RMSE, PME) against simpler models.

### Open Question 2
- Question: What is the optimal ratio of low-fidelity to high-fidelity data points for achieving maximum model accuracy with minimum computational cost?
- Basis in paper: [explicit] The authors explore different numbers of high-fidelity data points (100, 300, 500) but do not systematically optimize the ratio of low to high-fidelity data.
- Why unresolved: The study varies the amount of high-fidelity data but does not explore the impact of varying the amount of low-fidelity data or systematically optimizing the ratio between the two.
- What evidence would resolve it: A parametric study varying both the amount of low-fidelity and high-fidelity data to determine the optimal ratio that minimizes computational cost while maintaining accuracy.

### Open Question 3
- Question: How does the transferability of multi-fidelity FNO models perform when low-fidelity and high-fidelity data come from different physical simulators with varying levels of physical process representation?
- Basis in paper: [explicit] The authors test transferability between datasets from different simulators (GEOS vs. CMG-GEM) but note that this is a more challenging case.
- Why unresolved: While the authors demonstrate transferability between different simulators, they do not explore the limits of this transferability or how differences in physical process representation affect model performance.
- What evidence would resolve it: Testing the multi-fidelity FNO model on datasets from simulators with varying levels of physical process representation (e.g., black oil vs. compositional models) and quantifying the impact on prediction accuracy.

## Limitations
- Limited validation on real-world field data, relying entirely on synthetic datasets
- Struggles with CO2 saturation predictions due to discontinuous nature of saturation fronts
- Indirect correlation between datasets may not generalize to all GCS scenarios

## Confidence
- **High confidence**: The grid-invariant property of FNO enabling transfer learning between discretizations (supported by strong mathematical foundation and consistent experimental results)
- **Medium confidence**: The 81% data cost reduction claim (based on synthetic data generation costs but not validated on real-world data)
- **Low confidence**: The generalizability to indirectly correlated datasets (limited evidence and weak corpus support)

## Next Checks
1. Test the multi-fidelity approach on real field data from actual GCS sites to validate transferability beyond synthetic datasets
2. Systematically vary the correlation strength between low-fidelity and high-fidelity datasets to identify the minimum correlation threshold for effective transfer learning
3. Evaluate alternative architectures (e.g., U-Net, DeepONet) on the same GCS problem to benchmark FNO's performance advantages and limitations