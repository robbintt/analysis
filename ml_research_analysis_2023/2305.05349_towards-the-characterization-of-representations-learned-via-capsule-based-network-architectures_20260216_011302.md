---
ver: rpa2
title: Towards the Characterization of Representations Learned via Capsule-based Network
  Architectures
arxiv_id: '2305.05349'
source_url: https://arxiv.org/abs/2305.05349
tags:
- units
- layer
- capsule
- relevant
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Capsule networks (CapsNets) were introduced as a more compact and
  interpretable alternative to standard deep neural networks. However, their interpretability
  properties have not been fully assessed.
---

# Towards the Characterization of Representations Learned via Capsule-based Network Architectures

## Quick Facts
- arXiv ID: 2305.05349
- Source URL: https://arxiv.org/abs/2305.05349
- Reference count: 40
- Key outcome: Capsule networks may not encode truly disentangled part-whole relationships despite claims of interpretability

## Executive Summary
This paper proposes a principled methodology to systematically assess the interpretability of Capsule Networks (CapsNets). The approach combines perturbation analysis to understand how internal representations affect performance with layer-wise relevant unit selection to identify important capsules and filters. Applied to datasets including MNIST, SVHN, PASCAL-Part, and CelebA, the results suggest that while CapsNets may be interpretable, their representations are not necessarily disentangled nor strictly related to parts-whole relationships as commonly claimed in the literature.

## Method Summary
The methodology involves training CapsNet architectures on various datasets, then applying two complementary analysis techniques. First, perturbation analysis systematically modifies capsule vector dimensions and measures reconstruction and classification impacts to identify encoded features. Second, layer-wise relevant unit selection uses ablation to identify critical capsules and filters, combined with relevance mass accuracy to quantify part-whole overlap. The approach was validated on MNIST, SVHN, PASCAL-Part, and CelebA datasets using a CapsNet architecture based on Sabour et al. (2017) with specific training parameters and evaluation metrics.

## Key Results
- CapsNets achieve reasonable classification performance across tested datasets (MNIST, SVHN, CelebA)
- Perturbation analysis reveals feature encodings but also indicates limited disentanglement in capsule representations
- Relevance Mass Accuracy (RMA) scores suggest weak part-whole relationships between capsule layers
- Results challenge common claims about CapsNet interpretability and part-whole modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Capsule networks encode part-whole relationships through dynamic routing, where coupling coefficients route information from lower-layer capsules to higher-layer capsules based on agreement with predicted vectors.
- Mechanism: The dynamic routing algorithm computes coupling coefficients that indicate how strongly a lower-layer capsule (part) should be routed to a higher-layer capsule (whole). These coefficients are iteratively refined based on the agreement between predicted vectors and actual outputs, creating an interpretable flow of information.
- Core assumption: The dynamic routing procedure effectively captures meaningful part-whole relationships in the data, and these relationships are reflected in the learned representations.
- Evidence anchors:
  - [abstract] "Capsule Networks (CapsNets) have been re-introduced as a more compact and interpretable alternative to standard deep neural networks... part-whole relationships are modelled."
  - [section] "A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. Then, by explicitly providing a mechanism to link specific capsules at a lower layer with those at a higher layer, part-whole relationships are modelled."
  - [corpus] No direct evidence found in corpus; this is based on the paper's claims about CapsNet design principles.
- Break condition: If the dynamic routing procedure fails to capture meaningful part-whole relationships, or if the coupling coefficients do not accurately reflect the agreement between predicted vectors and actual outputs, the interpretability of the network would be compromised.

### Mechanism 2
- Claim: Perturbation analysis reveals which dimensions of capsule vectors encode specific visual features by measuring changes in reconstruction quality and classification performance when individual dimensions are systematically modified.
- Mechanism: By perturbing each dimension of the capsule vector output and observing how this affects both the reconstructed image and the classification accuracy, we can identify which dimensions encode which visual features (e.g., thickness, rotation, scale).
- Core assumption: The dimensions of capsule vectors have interpretable meaning, and perturbations along specific dimensions will produce predictable changes in the output.
- Evidence anchors:
  - [abstract] "The approach involves a perturbation analysis to understand how variations in the internal representation affect reconstruction and classification performance."
  - [section] "We modify the value of each dimension in v j producing the perturbed vector v′ j. This vector is pushed through the decoder in order to produce reconstructions xs for each perturbed v′ j."
  - [corpus] No direct evidence found in corpus; this mechanism is described in the paper itself.
- Break condition: If the perturbations do not produce interpretable changes in the output, or if multiple dimensions encode overlapping features (feature entanglement), the interpretability of the perturbation analysis would be limited.

### Mechanism 3
- Claim: Layer-wise relevant unit selection identifies the most important capsules and filters by measuring their contribution to classification performance when ablated, revealing the critical path of information flow.
- Mechanism: By systematically removing capsules and filters and measuring the impact on classification accuracy, we can identify which units are most critical for the task. The routing procedure then reveals how information flows through the network along this critical path.
- Core assumption: The contribution of each unit to overall performance is measurable through ablation, and the routing procedure accurately reflects the importance of different units.
- Evidence anchors:
  - [abstract] "Additionally, layer-wise relevant unit selection is used to identify the most important capsules and filters within the network."
  - [section] "This method is aimed at detecting the relevant features/units that define activation paths in a given network. This is achieved by probing one layer at a time and verifying how the selection/suppression of internal units in such layers affects classification performance."
  - [corpus] No direct evidence found in corpus; this mechanism is described in the paper itself.
- Break condition: If the ablation method does not accurately capture unit importance, or if the routing procedure does not reflect true information flow, the layer-wise selection would fail to identify truly relevant units.

## Foundational Learning

- Concept: Dynamic routing between capsules
  - Why needed here: Understanding how CapsNets route information between layers is fundamental to interpreting their behavior and assessing whether they truly capture part-whole relationships.
  - Quick check question: How does the iterative agreement-based routing in CapsNets differ from simple weighted connections in CNNs, and why is this important for interpretability?

- Concept: Perturbation analysis for interpretability
  - Why needed here: The perturbation methodology is central to the paper's approach for assessing interpretability, requiring understanding of how systematic modifications to internal representations can reveal feature encodings.
  - Quick check question: What does it mean when perturbing a single dimension of a capsule vector affects multiple visual features simultaneously, and how does this relate to feature disentanglement?

- Concept: Relevance mass accuracy (RMA) for measuring part-whole relationships
  - Why needed here: The RMA metric is used to quantify the overlap between lower-layer (parts) and higher-layer (wholes) responses, providing a numerical assessment of whether CapsNets actually encode part-whole relationships.
  - Quick check question: Why might the RMA scores be low even when CapsNets are working as intended, and what does this tell us about the limitations of this metric?

## Architecture Onboarding

- Component map: Conv layer -> Primary Capsule (PC) layer -> Dynamic routing -> Class Capsule (CC) layer -> Decoder
- Critical path: Input → Conv → Primary Capsules → Dynamic Routing → Class Capsules → Decoder (for reconstruction)
- Design tradeoffs:
  - Pros: More interpretable than CNNs due to explicit part-whole modeling, potentially more robust to viewpoint changes
  - Cons: Higher computational complexity due to routing procedure, potentially lower performance on complex datasets
- Failure signatures:
  - Poor classification accuracy on test set despite good training performance
  - Reconstructions that don't preserve input features
  - Low relevance mass accuracy scores indicating poor part-whole overlap
  - Feature entanglement revealed by perturbation analysis
- First 3 experiments:
  1. Train a basic CapsNet on MNIST and verify it achieves reasonable accuracy (>99%)
  2. Apply perturbation analysis to a trained model and visualize how individual dimensions affect reconstruction
  3. Perform layer-wise relevant unit selection and verify that a sparse model maintains comparable performance to the full model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do capsule networks truly encode part-whole relationships in a disentangled manner?
- Basis in paper: [explicit] The paper's results suggest that the representations learned by CapsNets might not be as disentangled nor strictly related to parts-whole relationships as commonly stated in the literature.
- Why unresolved: The study only analyzed the basic CapsNet architecture and found low relevance mass accuracy scores, but it's unclear if this limitation extends to more complex CapsNet variants or if it's a fundamental property of the approach.
- What evidence would resolve it: Comprehensive testing of various CapsNet architectures on diverse datasets, comparing their performance in encoding part-whole relationships against other network architectures.

### Open Question 2
- Question: How do the interpretability properties of CapsNets compare to those of other deep learning models like CNNs?
- Basis in paper: [inferred] The paper compares the interpretability of CapsNets to CNNs but doesn't provide a direct comparison of their interpretability properties.
- Why unresolved: While the paper suggests that CapsNets might not be as interpretable as claimed, it doesn't offer a comparative analysis with other models to determine if they are more or less interpretable.
- What evidence would resolve it: A systematic study comparing the interpretability of CapsNets and CNNs using the same evaluation metrics and datasets.

### Open Question 3
- Question: What is the impact of the choice of datasets on the interpretability of CapsNets?
- Basis in paper: [explicit] The paper tests CapsNets on MNIST, SVHN, PASCAL-Part, and CelebA datasets but doesn't explore how the choice of datasets affects the interpretability of the learned representations.
- Why unresolved: The study uses a diverse set of datasets but doesn't analyze how the complexity or nature of the data influences the interpretability of CapsNets.
- What evidence would resolve it: An analysis of CapsNets' interpretability on a wider range of datasets, including synthetic and real-world data, to determine if certain types of data lead to more interpretable representations.

## Limitations
- Perturbation analysis methodology assumes linear relationships between capsule dimensions and visual features
- Relevance Mass Accuracy metric may undercount overlapping regions due to pixel-level aggregation approach
- Study only examines one specific CapsNet architecture from Sabour et al. (2017)

## Confidence

- **High Confidence**: CapsNets can achieve reasonable classification performance on standard datasets (MNIST, SVHN, CelebA)
- **Medium Confidence**: Perturbation analysis can reveal feature encodings in capsule dimensions, though disentanglement is limited
- **Low Confidence**: CapsNets do not encode strictly part-whole relationships as claimed in the literature, based on qualitative observations and RMA scores

## Next Checks
1. **Cross-Architecture Validation**: Repeat the perturbation analysis and RMA computation on multiple CapsNet architectures (e.g., Hinton's EM routing, Matrix Capsules) to verify if the findings generalize beyond the Sabour architecture.

2. **Feature Correlation Analysis**: For datasets where perturbation produces entangled features, compute pairwise correlation matrices between capsule dimensions to quantify the degree of feature entanglement and compare against baseline CNN architectures.

3. **Ablation Study with RMA Variants**: Perform an ablation study where capsules are removed based on RMA scores versus random removal, and compare the resulting classification performance to determine if RMA effectively identifies truly important units.