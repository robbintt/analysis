---
ver: rpa2
title: 'Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility
  Study'
arxiv_id: '2301.05174'
source_url: https://arxiv.org/abs/2301.05174
tags:
- datasets
- scene-centric
- object-centric
- retrieval
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the reproducibility of image-text cross-modal
  retrieval (CMR) methods across object-centric and scene-centric datasets. The authors
  evaluate two state-of-the-art models, CLIP and X-VLM, on two scene-centric datasets
  (MS COCO, Flickr30k) and three object-centric datasets (CUB-200, Fashion200k, ABO).
---

# Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study

## Quick Facts
- arXiv ID: 2301.05174
- Source URL: https://arxiv.org/abs/2301.05174
- Reference count: 0
- Key outcome: Absolute scores on object-centric datasets are much lower than on scene-centric datasets, with partially reproducible relative performance

## Executive Summary
This paper investigates the reproducibility of image-text cross-modal retrieval (CMR) methods across different dataset types. The authors evaluate two state-of-the-art models, CLIP and X-VLM, on both scene-centric datasets (MS COCO, Flickr30k) and object-centric datasets (CUB-200, Fashion200k, ABO). They find that while absolute scores on object-centric datasets are significantly lower, relative performance patterns are partially reproducible on scene-centric datasets and partially replicable when transferring between dataset types. The results suggest that pre-training on one dataset type does not necessarily improve performance on the other, and the authors recommend training CMR models on both types to improve generalizability.

## Method Summary
The study evaluates CLIP and X-VLM models in a zero-shot setting across five datasets: two scene-centric (MS COCO, Flickr30k) and three object-centric (CUB-200, Fashion200k, ABO). For each dataset, one caption per image is randomly selected. Both models extract image and text features, compute cosine similarities between all pairs, and rank results for text-to-image and image-to-text retrieval tasks. Performance is measured using R@1, R@5, R@10, and rsum metrics. The study focuses on reproducing original scores and analyzing relative performance patterns across different dataset types.

## Key Results
- Absolute retrieval scores on object-centric datasets (CUB-200, Fashion200k, ABO) are significantly lower than on scene-centric datasets (MS COCO, Flickr30k)
- Relative performance between CLIP and X-VLM is partially reproducible on scene-centric datasets
- Relative performance is partially replicable when transferring models from scene-centric to object-centric datasets
- Pre-training on scene-centric datasets does not necessarily improve CMR performance on object-centric datasets, and vice versa

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP's architecture enables zero-shot cross-modal retrieval without fine-tuning
- Mechanism: CLIP uses a dual encoder (image + text) trained with contrastive loss on 400M image-text pairs. The symmetric loss aligns visual and textual embeddings in a shared latent space where cosine similarity measures relevance
- Core assumption: Pre-training on large-scale diverse image-text pairs creates generalizable representations for retrieval
- Evidence anchors:
  - [abstract] "We select two pre-trained models which demonstrate state-of-the-art performance on CMR task and evaluate them in a zero-shot setting."
  - [section] "CLIP uses a dual encoder that comprises an image encoder, and a text encoder. The model was pre-trained in a contrastive manner using a symmetric loss function."
- Break condition: If the pre-training data distribution differs significantly from the target domain, zero-shot performance degrades

### Mechanism 2
- Claim: X-VLM's cross-attention fusion improves cross-modal alignment over simple dual encoders
- Mechanism: X-VLM adds a cross-modal encoder that fuses visual and textual representations via cross-attention. This allows the model to capture fine-grained inter-modal relationships beyond what symmetric contrastive loss alone provides
- Core assumption: Cross-attention can model complex dependencies between visual and textual features better than independent encoding
- Evidence anchors:
  - [abstract] "X-VLM. This model consists of three encoders: an image encoder, a text encoder, and a cross-modal encoder."
  - [section] "The cross-modal encoder fuses the output of the image encoder and the output of the text encoder. The fusion is done via a cross-attention mechanism."
- Break condition: If the cross-modal encoder overfits to scene-centric data, it may not generalize to object-centric datasets

### Mechanism 3
- Claim: Scene-centric and object-centric datasets require different feature representations for optimal retrieval
- Mechanism: Scene-centric datasets contain complex scenes with multiple objects and relations, while object-centric datasets focus on single objects with fine-grained attributes. Models pre-trained on one type may not capture the distinctive features needed for the other
- Core assumption: The visual and textual features that define relevance differ between scene-centric and object-centric retrieval tasks
- Evidence anchors:
  - [abstract] "Most approaches to cross-modal retrieval (CMR) focus either on object-centric datasets... or on scene-centric datasets..."
  - [section] "Scene-centric images depict complex scenes that typically feature multiple objects and relations between them... Images in object-centric image datasets are usually focused on a single object of interest..."
- Break condition: If a model can learn domain-agnostic features that generalize across both dataset types

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: The paper relies on contrastive pre-training to align image and text embeddings in a shared space
  - Quick check question: What loss function is typically used in contrastive learning for cross-modal retrieval?

- Concept: Zero-shot evaluation
  - Why needed here: Both CLIP and X-VLM are evaluated without fine-tuning on the target datasets
  - Quick check question: What does "zero-shot" mean in the context of cross-modal retrieval?

- Concept: Dataset bias and domain adaptation
  - Why needed here: The paper shows that performance differs significantly between scene-centric and object-centric datasets
  - Quick check question: Why might a model trained on scene-centric data perform worse on object-centric data?

## Architecture Onboarding

- Component map:
  - CLIP: Image encoder (ViT-L/14) → Text encoder (transformer) → Cosine similarity for retrieval
  - X-VLM: Image encoder (ViT-base) → Text encoder (BERT-6L) → Cross-modal encoder (BERT-6L) → Cosine similarity for retrieval
  - Datasets: MS COCO, Flickr30k (scene-centric); CUB-200, Fashion200k, ABO (object-centric)

- Critical path: Data preprocessing → Feature extraction → Embedding alignment → Retrieval ranking → Evaluation (R@1, R@5, R@10, Rsum)

- Design tradeoffs:
  - CLIP: Simpler architecture, more pre-training data, potentially better generalization
  - X-VLM: More complex with cross-attention, potentially better fine-grained alignment but risk of overfitting

- Failure signatures:
  - Large performance gap between original and reproduced scores indicates data preprocessing differences
  - Different relative performance on scene vs object-centric datasets indicates domain-specific feature learning

- First 3 experiments:
  1. Reproduce CLIP and X-VLM scores on MS COCO and Flickr30k using the exact preprocessing from the original papers
  2. Evaluate both models on CUB-200, Fashion200k, and ABO to test domain generalization
  3. Compare relative performance ordering across scene and object-centric datasets to identify domain-specific strengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does pre-training on scene-centric datasets improve cross-modal retrieval performance on object-centric datasets, and vice versa?
- Basis in paper: [explicit] The authors state that "pre-training on scene-centric datasets does not necessarily improve CMR performance on object-centric datasets, and vice versa."
- Why unresolved: The paper shows that CLIP and X-VLM models perform differently on scene-centric and object-centric datasets, but it does not conclusively determine whether pre-training on one type of dataset improves performance on the other.
- What evidence would resolve it: Experiments comparing models pre-trained on scene-centric datasets and then fine-tuned on object-centric datasets, and vice versa, would provide evidence of transfer learning benefits.

### Open Question 2
- Question: How do scene-centric and object-centric datasets differ in terms of their impact on cross-modal retrieval model generalization?
- Basis in paper: [explicit] The authors discuss the differences between scene-centric and object-centric datasets, stating that scene-centric datasets depict complex scenes with multiple objects and relations, while object-centric datasets focus on a single object of interest.
- Why unresolved: While the paper highlights the differences between the two dataset types, it does not provide a comprehensive analysis of how these differences impact model generalization across datasets.
- What evidence would resolve it: Experiments evaluating model performance across various scene-centric and object-centric datasets, and analyzing the impact of dataset characteristics on generalization, would provide insights into this question.

### Open Question 3
- Question: What factors contribute to the reproducibility issues observed in cross-modal retrieval experiments?
- Basis in paper: [explicit] The authors note that the reproducibility of CMR results is an open problem, with some scores being reproducible and others not.
- Why unresolved: The paper identifies reproducibility as an issue but does not delve into the specific factors contributing to this problem.
- What evidence would resolve it: A systematic investigation of experimental setup variations, data preprocessing techniques, and implementation details across different studies would help identify the factors affecting reproducibility in CMR experiments.

## Limitations

- Significant performance gaps between reproduced and reported scores suggest preprocessing differences
- Cross-attention mechanism in X-VLM remains incompletely understood regarding domain generalization
- Study focuses only on zero-shot evaluation without fine-tuning experiments

## Confidence

- Absolute performance claims: Low (due to preprocessing uncertainties)
- Relative performance patterns: Medium
- Domain transfer observations: Medium

## Next Checks

1. Contact original authors to obtain exact preprocessing specifications and validate against their implementations
2. Conduct ablation studies comparing CLIP and X-VLM performance when trained on mixed scene-object datasets
3. Evaluate model robustness to caption variations by testing multiple captions per image rather than random single caption selection