---
ver: rpa2
title: 'AutoConv: Automatically Generating Information-seeking Conversations with
  Large Language Models'
arxiv_id: '2308.06507'
source_url: https://arxiv.org/abs/2308.06507
tags:
- dialogues
- human
- synthetic
- autoconv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes AutoConv, a method for automatically generating
  information-seeking conversations using large language models (LLMs). The core idea
  is to finetune an LLM with a few human conversations to capture the characteristics
  of the information-seeking process, then use it to generate high-quality synthetic
  conversations grounded on external documents.
---

# AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models

## Quick Facts
- arXiv ID: 2308.06507
- Source URL: https://arxiv.org/abs/2308.06507
- Reference count: 40
- Achieves 5.06 F1 score improvement on QuAC dataset

## Executive Summary
AutoConv presents a method for automatically generating information-seeking conversations using large language models. The approach finetunes an LLM with a few human dialogues to capture conversation characteristics, then generates synthetic dialogues grounded on external documents. These synthetic conversations are used to pre-train a smaller task-specific model, which is subsequently finetuned on human dialogues. Experimental results demonstrate significant performance improvements over strong baselines while reducing the need for human annotation.

## Method Summary
The method formulates conversation generation as a language modeling task. First, an LLM (OPT-13B) is finetuned on a small set of human dialogues to capture information-seeking patterns. The finetuned model then generates synthetic dialogues using document inputs, employing nucleus sampling for user questions and greedy search for system answers. Generated dialogues are filtered based on diversity scores to avoid repetitive patterns. A small task model (UnifiedQA-V2-base) is pre-trained on synthetic dialogues, then fine-tuned on human dialogues. The approach enables knowledge distillation from large to small models while maintaining efficiency.

## Key Results
- AutoConv achieves 5.06 F1 score improvement on QuAC dataset compared to direct finetuning with same human dialogues
- Synthetic dialogues enable small models to outperform their larger teacher models
- Method significantly outperforms strong baselines while reducing human annotation requirements

## Why This Works (Mechanism)

### Mechanism 1
Finetuning LLM with few human dialogues captures information-seeking conversation characteristics. The few-shot finetuning allows LLM to learn grounding, question answering, and contextual dependencies from limited examples. Core assumption: LLM pretraining provides sufficient general language understanding to adapt quickly to conversation patterns. Evidence anchors: [abstract] "finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process", [section 2.2] "finetune an LLM with a few human dialogues (e.g., 50 from QuAC) to capture the characteristics of information-seeking conversations". Break condition: If human dialogues lack diversity or represent atypical conversation patterns, the captured characteristics may be misleading.

### Mechanism 2
Greedy search for answer generation reduces hallucination compared to sampling. Greedy decoding selects highest probability next token, constraining generation to stay grounded in document content. Core assumption: Document-grounded answers benefit more from deterministic decoding than creative, open-ended generation. Evidence anchors: [section 2.2] "we find it results in the 'hallucination' problem... we utilize greedy search for answer generation", [section 3.5] "maximization-based decoding strategies for answer generation" performs better. Break condition: If documents contain ambiguous or contradictory information, greedy decoding may produce overly conservative or incorrect answers.

### Mechanism 3
Synthetic dialogues enable knowledge distillation from large to small models. LLM generates high-quality training data that transfers learned conversation patterns to more efficient task-specific models. Core assumption: Quality synthetic data can compensate for parameter difference between teacher and student models. Evidence anchors: [section 3.4] "AutoConv not only keeps the efficiency of STM, but also boosts the performance" and "AutoConv even outperforms its teacher model", [section 3.2] "AutoConv has substantial improvements over several strong baselines". Break condition: If synthetic data quality degrades or distribution mismatch grows, knowledge transfer efficiency drops.

## Foundational Learning

- Concept: Few-shot learning adaptation
  - Why needed here: Enables effective finetuning with limited human dialogue examples
  - Quick check question: How many examples are typically needed for few-shot learning to stabilize on conversation patterns?

- Concept: Decoding strategy selection
  - Why needed here: Different generation tasks require different trade-offs between creativity and faithfulness
  - Quick check question: What distinguishes tasks that benefit from sampling versus greedy decoding?

- Concept: Knowledge distillation principles
  - Why needed here: Understanding how synthetic data can effectively transfer knowledge from large to small models
  - Quick check question: What factors determine whether student models can outperform their teachers?

## Architecture Onboarding

- Component map: LLM finetuning module (OPT-13B) -> Document grounding engine -> User question generator (nucleus sampling) -> System answer generator (greedy search) -> Diversity filtering system -> Small task model trainer (UnifiedQA-V2-base)

- Critical path: 1. Finetune LLM with human dialogues, 2. Generate synthetic dialogues using document inputs, 3. Filter low-diversity synthetic conversations, 4. Pre-train small task model on synthetic data, 5. Finetune small task model on human dialogues

- Design tradeoffs: Sampling vs greedy decoding for different utterance types, Number of synthetic dialogues vs human dialogue quality, Model size vs computational efficiency, Diversity filtering threshold vs data quantity

- Failure signatures: Poor answer grounding → check greedy decoding implementation, Repetitive conversations → increase diversity filtering stringency, Degraded performance with more data → examine synthetic data quality

- First 3 experiments: 1. Compare nucleus sampling vs greedy decoding on a small validation set, 2. Test finetuning LLM with varying numbers of human dialogues (10, 50, 100), 3. Evaluate synthetic dialogue quality by training small models with different diversity thresholds

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal decoding strategy for generating system answers in AutoConv, and how does it vary with different types of documents or datasets? Basis in paper: [explicit] The paper discusses the impact of decoding strategy on system answer generation, finding that greedy search performs better than nucleus sampling for avoiding "hallucination" issues. However, it does not explore how this might vary with document complexity or dataset characteristics. Why unresolved: The paper only tests a few decoding strategies on the QuAC dataset. There is no exploration of how decoding strategy might need to be adjusted for different types of documents (e.g., longer, more complex documents) or different datasets with varying characteristics. What evidence would resolve it: Experiments comparing different decoding strategies (e.g., greedy search, beam search, nucleus sampling with different parameters) on various datasets and document types, measuring both answer quality and computational efficiency.

### Open Question 2
How does the performance of AutoConv scale with increasingly larger language models, and what are the computational trade-offs involved? Basis in paper: [explicit] The paper briefly investigates scaling laws by testing AutoConv with different sizes of OPT models (350M to 13B parameters) on the QuAC dataset. It finds that larger models generally perform better, but the investigation is limited to 13B parameters due to computational constraints. Why unresolved: The paper does not explore the full range of available large language models (e.g., GPT-3, OPT-175B, BLOOM-176B, GLM-130B) and does not provide a detailed analysis of the computational trade-offs (e.g., memory usage, inference time) associated with scaling up the model size. What evidence would resolve it: Comprehensive experiments using a wide range of large language models, comparing their performance on multiple datasets while also measuring and analyzing computational resources required (e.g., GPU memory, inference time, training time).

### Open Question 3
What specific aspects of the information-seeking conversation process does AutoConv capture, and how can these be further improved to reduce the gap between synthetic and human dialogues? Basis in paper: [explicit] The paper mentions that AutoConv captures "characteristics of the information-seeking process" such as grounding and question answering, but does not provide a detailed breakdown of which specific conversational elements are captured or how they compare to human conversations. Why unresolved: While the paper provides some error analysis showing that 40% of system answers are imperfect, it does not offer a detailed analysis of what specific conversational elements (e.g., context understanding, question variety, answer relevance) are well-captured versus those that need improvement. What evidence would resolve it: A detailed analysis breaking down the performance of AutoConv on specific conversational elements (e.g., context understanding, question variety, answer relevance, coherence), comparing these elements between synthetic and human dialogues, and identifying specific areas for improvement.

## Limitations
- Reliance on quality of few-shot finetuning with no clear guidance on required examples for different domains
- Diversity filtering mechanism lacks specification of thresholds and metrics
- Knowledge distillation effectiveness depends on maintaining consistent data distribution between synthetic and human dialogues

## Confidence
- High confidence: The core approach of using LLM-generated synthetic dialogues for data augmentation is technically sound and aligns with established knowledge distillation principles. The experimental results showing improved performance over baselines are likely reproducible.
- Medium confidence: The specific claim that 50 human dialogues are sufficient for effective finetuning requires domain-specific validation. The assertion that greedy decoding prevents hallucination better than sampling lacks quantitative evidence and may vary by document type.
- Low confidence: The scalability claims regarding computational efficiency improvements are based on limited comparisons and don't account for generation costs or diverse deployment scenarios.

## Next Checks
1. **Decoding strategy ablation**: Systematically compare nucleus sampling vs greedy search across multiple document types to quantify hallucination differences and identify optimal decoding for different conversation segments.
2. **Few-shot scaling study**: Evaluate performance as a function of human dialogue quantity (5, 10, 50, 100, 200) to determine the minimum effective training set size and identify diminishing returns.
3. **Synthetic data quality audit**: Implement automated metrics to assess grounding accuracy, diversity, and coherence of generated dialogues, then correlate these metrics with downstream task performance to validate the diversity filtering approach.