---
ver: rpa2
title: Intentional Biases in LLM Responses
arxiv_id: '2311.07611'
source_url: https://arxiv.org/abs/2311.07611
tags:
- question
- context
- answer
- arxiv
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study found that GPT-4\u2019s mixture-of-experts architecture\
  \ with a supervisor makes it less prone to hallucinations and more resistant to\
  \ contextual bias than the dense Falcon-7B model. In high-coverage questions, Falcon-7B\
  \ was 44% less accurate and 33% more likely to produce hallucinations when given\
  \ counter-context, while GPT-4\u2019s accuracy dropped only 17%."
---

# Intentional Biases in LLM Responses

## Quick Facts
- arXiv ID: 2311.07611
- Source URL: https://arxiv.org/abs/2311.07611
- Reference count: 16
- Key outcome: GPT-4's mixture-of-experts architecture with supervisor reduces hallucinations and contextual bias compared to dense Falcon-7B model

## Executive Summary
This study examines how different LLM architectures handle intentional biasing through contextual information. The research compares GPT-4's mixture-of-experts architecture against Falcon-7B's dense architecture, testing their susceptibility to hallucinations and contextual bias when presented with supporting and counter-context. The findings reveal significant architectural differences in how these models process and reconcile conflicting information, with important implications for both factual accuracy and creative applications requiring specific persona development.

## Method Summary
The study generated climate-related question and context pairs, classified as high or low coverage, with supporting and counter-context versions. Researchers used LangChain to interface with GPT-4 and Falcon-7B models at temperature 0.9, employing human evaluators to rate responses on three metrics: hallucination likelihood, incorrect statement probability, and accuracy. The evaluation used a 0-1 scale with 0.5 increments, averaging scores across different context conditions.

## Key Results
- Falcon-7B showed 44% lower accuracy and 33% higher hallucination rates than GPT-4 when given counter-context on high-coverage questions
- GPT-4's accuracy dropped only 17% under counter-context conditions compared to Falcon-7B's 44% drop
- In low-coverage scenarios, Falcon hallucinated 33% of the time without context, while GPT-4 produced no hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixture-of-experts (MoE) architecture with a supervisor reduces hallucination susceptibility compared to dense models when given conflicting contextual information.
- Mechanism: The supervisor selects among specialized expert sub-models, each trained on different data subsets. When presented with counter-context that contradicts general training data, the supervisor can filter out expert responses that don't align with the broader knowledge base, reducing hallucination probability.
- Core assumption: The supervisor has access to reliable training data that can override contradictory context signals.
- Evidence anchors:
  - [abstract] "GPT-4's mixture-of-experts architecture with a supervisor makes it less prone to hallucinations and more resistant to contextual bias than the dense Falcon-7B model"
  - [section] "The guardrail s contained in a mixture-of-experts model like GPT make it less likely to generate an answer that is out of concordance with its background training material, which reduces its likelihood of hallucinating"
- Break condition: If the supervisor lacks sufficient training data to resolve contradictions, or if the counter-context is too compelling relative to general training data.

### Mechanism 2
- Claim: Dense models without supervisor mechanisms are more susceptible to contextual biasing because they lack a filtering layer to reconcile conflicting information.
- Mechanism: Without a supervisor to mediate between context and general knowledge, dense models must reconcile contradictory information directly, leading to higher hallucination rates when context contradicts training data.
- Core assumption: The model treats all input context as equally valid for response generation without an additional validation layer.
- Evidence anchors:
  - [abstract] "Falcon hallucinated 33% of the time without context, whereas GPT-4 produced no hallucinations"
  - [section] "Due to the OpenAI GPT models having significant guardrails, we hypothesized that an open source model without guardrails would be more easily biased towards a less common viewpoint"
- Break condition: If the dense model has exceptionally strong internal consistency checks or if the context is sufficiently weak relative to general training.

### Mechanism 3
- Claim: Temperature parameter settings influence hallucination propensity, with higher values increasing the likelihood of creative but potentially incorrect responses.
- Mechanism: Temperature controls the randomness of token selection during generation. Higher temperatures (0.9 in this study) make the model more likely to choose less probable tokens, increasing novelty but also the risk of hallucinations.
- Core assumption: The relationship between temperature and hallucination probability is linear or monotonically increasing within the tested range.
- Evidence anchors:
  - [section] "We kept the temperature at the value of 0.9 for the entirety of testing. To reduce the amount of hidden variables we used only the large language model chain functions from the framework."
- Break condition: If temperature effects plateau at high values or if the model has temperature-independent hallucination safeguards.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding how MoE differs from dense models is crucial for interpreting why GPT-4 shows different hallucination patterns than Falcon-7B
  - Quick check question: What is the key architectural difference between MoE and dense models, and how does this difference impact guardrail effectiveness?

- Concept: Hallucination detection and classification
  - Why needed here: The study uses specific metrics to evaluate hallucinations (coherence, correctness, harmful misinformation) that require clear understanding
  - Quick check question: How does the study distinguish between a hallucination and an incorrect statement, and what are the three evaluation metrics used?

- Concept: Contextual biasing techniques
  - Why needed here: The experiment intentionally introduces biases through supporting and counter-context to test model responses
  - Quick check question: What is the difference between supporting context and counter context in the experimental design, and how do they affect model outputs?

## Architecture Onboarding

- Component map: LLM backend (GPT-4 or Falcon-7B) -> LangChain prompt template system -> Temperature controller -> Human evaluation framework
- Critical path: Question/context generation → Model inference via LangChain → Output evaluation by human raters → Aggregation of results by coverage type and context condition
- Design tradeoffs: MoE models provide better guardrails against hallucinations but limit persona creation flexibility; dense models allow more creative persona development but risk higher hallucination rates; temperature settings balance novelty against accuracy
- Failure signatures: When the supervisor in MoE models fails to filter contradictory context, you'll see hallucination rates similar to dense models; when dense models hallucinate, outputs will be completely disconnected from both context and general knowledge; when temperature is too high, outputs will be creative but frequently incorrect
- First 3 experiments:
  1. Test the same question with supporting context, no context, and counter context on both models to establish baseline differences in hallucination susceptibility
  2. Vary the temperature parameter systematically (0.1, 0.5, 0.9) on both models with counter-context questions to measure the relationship between temperature and hallucination rates
  3. Use questions with known answers but contradictory context to test the supervisor's ability to override context versus the dense model's susceptibility to contextual biasing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively measure and control intentional biases in LLM-generated personas for creative applications?
- Basis in paper: Explicit
- Why unresolved: The paper acknowledges the potential benefits of intentional biases for creative applications but doesn't provide a concrete methodology for measuring or controlling these biases.
- What evidence would resolve it: A study that develops and validates a metric for measuring intentional biases in LLM-generated personas, and demonstrates its effectiveness in controlling these biases for creative applications.

### Open Question 2
- Question: Can we develop techniques to steer LLM responses towards specific personas without compromising their general knowledge and factual accuracy?
- Basis in paper: Explicit
- Why unresolved: The paper shows that while GPT-4 is less prone to hallucinations, it's also harder to intentionally bias towards uncommon viewpoints. However, it doesn't explore techniques to balance persona steering with factual accuracy.
- What evidence would resolve it: A study that develops and evaluates techniques for steering LLM responses towards specific personas while maintaining factual accuracy, and demonstrates their effectiveness in creative applications.

### Open Question 3
- Question: How do different LLM architectures (dense vs. mixture-of-experts) impact their ability to generate biased responses while maintaining factual accuracy?
- Basis in paper: Explicit
- Why unresolved: The paper compares the performance of GPT-4 (mixture-of-experts) and Falcon-7B (dense) in terms of bias and factual accuracy, but doesn't explore the underlying architectural differences that contribute to these differences.
- What evidence would resolve it: A study that analyzes the architectural differences between dense and mixture-of-experts models, and demonstrates how these differences impact their ability to generate biased responses while maintaining factual accuracy.

## Limitations
- The classification threshold for high vs. low coverage questions remains unspecified
- The exact question/context pairs used in the experiment are not fully detailed
- The human evaluation process lacks explicit inter-rater reliability metrics

## Confidence
- High confidence: GPT-4 demonstrates lower hallucination rates than Falcon-7B under counter-context conditions
- Medium confidence: The mixture-of-experts architecture with supervisor is the primary driver of hallucination resistance
- Medium confidence: Temperature settings at 0.9 significantly influence hallucination propensity
- Low confidence: The generalizability of these findings to non-climate-related domains

## Next Checks
1. Replicate the coverage classification methodology using GPT-3.5 Turbo as a baseline to establish consistent high/low coverage thresholds for questions
2. Test the temperature-hallucination relationship by systematically varying temperature (0.1, 0.5, 0.9) on both models with identical counter-context questions to validate the monotonic relationship
3. Cross-domain validation by applying the same experimental design to a different topic area (e.g., medical information) to assess whether the MoE architecture consistently provides hallucination resistance across domains