---
ver: rpa2
title: "Composite Diffusion | whole >= \u03A3parts"
arxiv_id: '2307.13720'
source_url: https://arxiv.org/abs/2307.13720
tags:
- image
- diffusion
- composite
- segment
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Composite Diffusion, a method for artists
  to generate high-quality images by composing from sub-scenes. The approach allows
  flexible free-form segment layouts and uses natural text, reference images, and
  control inputs like line art and human pose to describe sub-scene content.
---

# Composite Diffusion | whole >= Σparts

## Quick Facts
- arXiv ID: 2307.13720
- Source URL: https://arxiv.org/abs/2307.13720
- Authors: 
- Reference count: 40
- Key outcome: This paper introduces Composite Diffusion, a method for artists to generate high-quality images by composing from sub-scenes.

## Executive Summary
This paper presents Composite Diffusion, a novel method enabling artists to generate high-quality composite images by composing sub-scenes with flexible free-form layouts. The approach leverages natural text descriptions, reference images, and control inputs like line art and human pose to describe sub-scene content. A comprehensive modular method allows alternative ways of generating, composing, and harmonizing sub-scenes without requiring retraining or modifying base diffusion models. The method works plug-and-play with fine-tuned models and demonstrates greater spatial, semantic, and creative control over image generation.

## Method Summary
Composite Diffusion divides the generative process of a diffusion model into two successive temporal stages: scaffolding and harmonization. The scaffolding stage generates each segment independently using text descriptions, reference images, or control conditions like line art and pose. These segments are then composed into an intermediate composite. The harmonization stage denoises this composite with either global text conditions or segment-specific conditions to ensure context-aware refinement. The method uses classifier-free guidance in latent diffusion models to integrate multiple conditioning modalities at inference time, and employs segment masks for spatial control without requiring object-level segmentation training.

## Key Results
- Demonstrates greater spatial, semantic, and creative control over image generation through extensive user surveys and quantitative/qualitative analysis
- Achieves high-quality composite image generation without requiring retraining or modification of base diffusion models
- Works plug-and-play with fine-tuned models while maintaining superior performance across five proposed quality criteria

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage scaffolding + harmonization process enables independent segment generation followed by context-aware blending.
- Mechanism: Scaffolding stage generates each segment in isolation (using reference images, scaffolding image, or control conditions), then composes them into an intermediate composite. Harmonization stage denoises this composite with either global text or segment-specific conditions, ensuring context-aware refinement.
- Core assumption: Diffusion processes are inherently harmonizing; context from other segments improves blending when introduced after initial segment anchoring.
- Evidence anchors:
  - [abstract] "Our method divides the generative process of a diffusion model into two successive temporal stages: (a) the Scaffolding stage and (b) the Harmonization stage."
  - [section 3.3] "We can further develop the intermediate composite from the previous stage in the following ways: (i) by direct denoising the composite image latent via a global prompt... or (ii) by denoising the intermediate composite latent separately with each segment specific conditioning..."
  - [corpus] No direct evidence in corpus about this two-stage design; likely novel to this work.
- Break condition: If scaffolding factor κ is set too low, segments may not anchor properly; if too high, blending suffers.

### Mechanism 2
- Claim: Classifier-free guidance in latent diffusion models enables effective control via natural text and image conditions without retraining.
- Mechanism: The model is trained to act both conditionally and unconditionally; guidance scale s amplifies conditioning signal. CLIP embeddings provide text-image alignment; ControlNet provides spatial control conditioning.
- Core assumption: Conditional diffusion with classifier-free guidance can integrate multiple conditioning modalities (text, reference images, control inputs) at inference time.
- Evidence anchors:
  - [abstract] "Our methods do not need to retrain or modify the architecture of the base diffusion models and can work in a plug-and-play manner with the fine-tuned models."
  - [section B.2.3] "A conditional generative model is trained to act as both conditional and unconditional... For s = 0, we get an unconditional model, for s = 1, we get a conditional model, and for s >1 we strengthen the conditioning signal."
  - [corpus] No direct corpus evidence about this specific guidance mechanism.
- Break condition: If guidance scale is too high, generation may overfit to conditioning and lose diversity.

### Mechanism 3
- Claim: Segment masks enable spatial control without requiring object-level segmentation training.
- Mechanism: Free-form segment layouts are converted to one-hot mask encodings. Each segment is generated independently using its mask to isolate the relevant region in latent space. This coarse spatial control avoids need for dense segmentation training.
- Core assumption: Spatial control via binary masks is sufficient for composite generation without object-level segmentation.
- Evidence anchors:
  - [abstract] "The artists can specify the arrangement of these sub-scenes through a flexible free-form segment layout."
  - [section 3.1] "We convert the segment layout to segment-specific masks, M = [ m1,m 2,...,mn], as one-hot encoding vectors."
  - [corpus] No corpus evidence about mask-based spatial control in this specific context.
- Break condition: If mask boundaries are ambiguous, generation may produce artifacts at segment edges.

## Foundational Learning

- Concept: Diffusion models and DDIM sampling
  - Why needed here: Core generative process relies on denoising latents via DDIM; understanding timesteps and noise schedules is critical for scaffolding/harmonization stages.
  - Quick check question: What is the difference between DDPM and DDIM sampling in terms of timesteps required?

- Concept: Classifier-free guidance and CLIP embeddings
  - Why needed here: Enables integration of text and image conditions without external classifiers; CLIP provides text-image similarity scoring for quality evaluation.
  - Quick check question: How does the guidance scale s affect the balance between unconditional and conditional generation?

- Concept: ControlNet architecture and conditioning
  - Why needed here: Provides spatial control inputs (lineart, pose, scribbles) that can be embedded alongside text conditions for segment-specific generation.
  - Quick check question: What is the role of the control feature map cf in ControlNet's conditioning mechanism?

## Architecture Onboarding

- Component map: Base latent diffusion model (Stable Diffusion 1.5) -> ControlNet for control conditions -> Segment layout parser → mask generator -> CLIP text encoder -> VAE encoder/decoder for latent ↔ pixel space conversion -> DDIM scheduler for sampling -> Quality evaluation pipeline (human + automated)

- Critical path: 1. Parse segment layout → generate masks 2. For each segment, generate latents using appropriate conditioning (text, reference image, control) 3. Compose intermediate composite 4. Harmonize composite using global or segment-specific denoising 5. Decode final image

- Design tradeoffs:
  - Fine-grained vs coarse spatial control (object segments vs sub-scenes)
  - Scaffolding factor κ vs blending quality
  - Text-only vs text+control conditioning complexity
  - Human evaluation vs automated metrics for quality assessment

- Failure signatures:
  - Poor segment anchoring → visible seams or artifacts at boundaries
  - Over-guidance → loss of diversity or overfitting to conditioning
  - Mask misalignment → generation spills outside intended regions
  - Low scaffolding κ → lack of spatial fidelity

- First 3 experiments:
  1. Generate composite with text-only conditioning, κ=30, evaluate spatial fidelity
  2. Add reference images for one segment, keep others text-only, compare content fidelity
  3. Use ControlNet lineart for one segment, compare technical quality vs text-only

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of scaffolding factor κ affect the trade-off between spatial layout fidelity and blending/harmony in Composite Diffusion?
- Basis in paper: [explicit] Section 3.4 discusses the scaffolding factor κ and its impact on spatial conformance versus blending, with lower values increasing harmony but reducing spatial adherence.
- Why unresolved: The paper provides qualitative examples and a single table showing automated evaluation results across different κ values, but does not conduct a systematic study isolating the trade-off effects or determining optimal κ ranges for different use cases.
- What evidence would resolve it: A comprehensive ablation study varying κ across multiple images and measuring both spatial fidelity and blending metrics quantitatively, with user preference studies to identify sweet spots for different artistic applications.

### Open Question 2
- Question: Can Composite Diffusion be effectively extended to support finer-grained object-level control within segments beyond what current control conditions (lineart, pose, etc.) provide?
- Basis in paper: [inferred] Section 2.4 mentions that the paper's approach is "complementary" to other diffusion-based composition methods, and Section 3.2.3 discusses control conditions for segments, suggesting potential for expansion.
- Why unresolved: The paper focuses on segment-level control and mentions that "strict object conformance" is difficult with text-only conditioning, but does not explore whether more sophisticated control conditions or multi-scale approaches could achieve finer-grained control.
- What evidence would resolve it: Implementation and evaluation of Composite Diffusion using more granular control conditions (e.g., part segmentation, detailed sketches) and comparison with object-level control methods like null-text inversion or prompt-to-prompt editing.

### Open Question 3
- Question: How does Composite Diffusion perform when applied to base models trained on different datasets or domains (e.g., artistic vs. photographic)?
- Basis in paper: [explicit] Section 5.1 mentions that "the strength of the base model heavily influences the quality of generated composite" and that evaluation should be "in relation to the base model image quality," but does not systematically compare different base models.
- Why unresolved: The paper uses Stable Diffusion v1.5 as the base model for all experiments and only briefly mentions plug-and-play compatibility with fine-tuned models in Figure 18, without rigorous comparison of Composite Diffusion performance across different base model architectures or training domains.
- What evidence would resolve it: A controlled experiment applying Composite Diffusion to multiple base models (e.g., DALL-E, Imagen, domain-specific fine-tuned models) and measuring composite quality relative to each base model's text-to-image performance across the established quality criteria.

## Limitations

- Implementation details for the composite generation steps and harmonization process remain underspecified, particularly how segments are merged.
- Automated quality metrics (CF, SF, BH, QT, QA) are conceptually sound but their exact computational implementation is unclear.
- Human evaluation methodology lacks detail on rater selection, training, and potential biases.

## Confidence

- High Confidence: The general approach of using diffusion models for composite generation with spatial masks is technically sound and well-grounded in existing literature.
- Medium Confidence: The two-stage scaffolding + harmonization framework appears promising but lacks sufficient empirical validation across diverse scenarios.
- Low Confidence: The paper claims state-of-the-art performance and superior user preference, but the user study methodology lacks detail on sample size, demographic diversity, and statistical significance testing.

## Next Checks

1. Parameter Sensitivity Analysis: Systematically vary the scaffolding factor κ (e.g., 10, 20, 30, 40) and guidance scales across different segment types to establish robust parameter ranges and identify failure modes.

2. Cross-Dataset Evaluation: Test the method on datasets beyond those used in the paper, particularly with complex layouts involving many segments or challenging content to assess generalization and identify edge cases.

3. Ablation Study of Quality Metrics: Implement the proposed automated metrics (CF, SF, BH, QT, QA) and conduct an ablation study comparing them against established alternatives to validate their effectiveness and identify potential correlations or redundancies.