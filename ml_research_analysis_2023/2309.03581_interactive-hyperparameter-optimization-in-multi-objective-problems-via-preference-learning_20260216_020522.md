---
ver: rpa2
title: Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference
  Learning
arxiv_id: '2309.03581'
source_url: https://arxiv.org/abs/2309.03581
tags:
- pareto
- front
- learning
- user
- indicator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an interactive human-centered hyperparameter
  optimization approach for multi-objective machine learning (MO-ML) algorithms. The
  key idea is to leverage preference learning to extract user desiderata from pairwise
  comparisons of Pareto fronts, instead of relying on users to select an appropriate
  quality indicator.
---

# Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning

## Quick Facts
- arXiv ID: 2309.03581
- Source URL: https://arxiv.org/abs/2309.03581
- Reference count: 40
- This paper introduces an interactive human-centered hyperparameter optimization approach for multi-objective machine learning (MO-ML) algorithms that learns user preferences from pairwise comparisons of Pareto fronts.

## Executive Summary
This paper presents a novel approach to hyperparameter optimization for multi-objective machine learning algorithms that replaces the need for users to select quality indicators with an interactive preference learning system. The method leverages pairwise comparisons of Pareto fronts to learn a utility function that captures user preferences, then uses this utility function to guide hyperparameter optimization. Experiments on the environmental impact of ML models show that this approach substantially outperforms traditional HPO when users select incorrect indicators, and performs comparably to advanced users who know the correct indicator. This makes HPO for MO-ML substantially more easily and robustly applicable in practice.

## Method Summary
The approach consists of three phases: preliminary sampling to obtain diverse Pareto fronts using random hyperparameter configurations, interactive preference learning where users compare pairs of Pareto fronts and RankSVM learns a utility function from these comparisons, and utility-driven HPO using SMAC to optimize hyperparameters based on the learned utility function. The feature representation flattens and normalizes loss matrices from Pareto fronts, and the utility function maps these features to scalar scores representing user preferences.

## Key Results
- The preference-based (PB) approach outperforms indicator-based (IB) optimization when users select wrong indicators (11/16 datasets)
- PB performs comparably to IB when users know the correct indicator
- Kendall's Tau correlation increases with more pairwise comparisons, showing improved ranking performance
- The approach reduces user burden by replacing indicator selection with interactive preference elicitation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The approach reduces user burden by replacing the need to choose a Pareto front quality indicator with an interactive preference elicitation phase.
- Mechanism: Users compare pairs of Pareto fronts, and the system learns a utility function that maps each Pareto front to a scalar score matching user preferences.
- Core assumption: Users can reliably rank Pareto fronts by visual inspection without knowing the underlying indicator.
- Evidence anchors:
  - [abstract]: "Instead of relying on the user guessing the most suitable indicator for their needs, our approach automatically learns an appropriate indicator."
  - [section 4.2]: "we leverage pairwise comparisons of distinct Pareto fronts to learn such an appropriate quality indicator."
- Break condition: If users cannot consistently distinguish between Pareto fronts (e.g., high-dimensional fronts or too many models), learned utility function may not reflect true preferences.

### Mechanism 2
- Claim: The RankSVM-based utility function generalizes well from a small number of pairwise comparisons.
- Mechanism: Each user comparison provides two training examples (one positive, one negative) to train a linear SVM, whose weight vector becomes the utility function.
- Core assumption: The linear RankSVM can capture the user's implicit ranking preferences given enough diverse pairwise examples.
- Evidence anchors:
  - [section 4.2]: "we leverage the RankSVM approach by Joachims (2002) which generalizes a standard SVM to the case of object ranking."
  - [section 5.2]: Shows Kendall's Tau correlation increasing with more comparisons, indicating improved ranking performance.
- Break condition: If user preferences are non-linear or complex, the linear utility function may fail to capture them.

### Mechanism 3
- Claim: Combining learned utility with SMAC yields better hyperparameter optimization than using a mismatched indicator.
- Mechanism: SMAC optimizes hyperparameters using the learned utility function as the loss, aligning search with user preferences.
- Core assumption: SMAC can effectively optimize with any scalar utility function provided as loss.
- Evidence anchors:
  - [abstract]: "our approach leads to substantially better Pareto fronts compared to optimizing based on a wrong indicator pre-selected by the user."
  - [section 5.3]: Table 6 shows PB outperforms IB in 11/16 cases when wrong indicator is chosen.
- Break condition: If the learned utility is noisy or poorly calibrated, SMAC may converge to suboptimal hyperparameters.

## Foundational Learning

- Concept: Multi-objective optimization and Pareto dominance
  - Why needed here: Understanding how MO-ML algorithms generate Pareto fronts and why users need to select among them.
  - Quick check question: What does it mean for one ML model to dominate another in a multi-objective setting?

- Concept: Preference learning and pairwise ranking
  - Why needed here: Core method to infer user utility from comparisons without explicit indicator selection.
  - Quick check question: How does RankSVM convert pairwise preferences into a utility function?

- Concept: Hyperparameter optimization with surrogate models
  - Why needed here: SMAC uses Bayesian optimization; understanding this helps debug and extend the pipeline.
  - Quick check question: What role does the utility function play in guiding SMAC's search?

## Architecture Onboarding

- Component map:
  Data generator (LCBench surrogate) -> Pareto front collection -> Preference collector (user interface) -> Ranker (RankSVM training) -> Optimizer (SMAC with learned utility) -> Evaluator (metrics computation)

- Critical path:
  1. Preliminary sampling → Pareto front collection
  2. Interactive preference elicitation → RankSVM training
  3. Utility-driven HPO → Final Pareto front evaluation

- Design tradeoffs:
  - Linear vs. non-linear RankSVM: Simplicity and interpretability vs. expressive power
  - Number of comparisons: More data → better ranking, but higher user burden
  - Feature representation: Flattened loss matrix captures trade-offs but assumes fixed number of models

- Failure signatures:
  - Low Kendall's Tau during preference learning → User comparisons inconsistent or too few
  - SMAC stuck in local optima → Utility function poorly aligned or too noisy
  - Pareto fronts dominated by one objective → Utility function overemphasizes that objective

- First 3 experiments:
  1. Vary number of pairwise comparisons (2, 28, 140) and measure Kendall's Tau to confirm learning curve.
  2. Replace linear RankSVM with RBF kernel RankSVM to test non-linear preference capture.
  3. Swap SMAC with random search to quantify impact of BO optimization with learned utility.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the approach be generalized to multi-objective HPO for single-objective ML algorithms?
- Basis in paper: [explicit] The paper mentions this as a limitation and a potential direction for future work, stating that the approach is tailored towards MO-ML but single-objective ML is more common in practice.
- Why unresolved: The paper does not provide any concrete ideas or methods for generalizing the approach to single-objective ML.
- What evidence would resolve it: A concrete method or algorithm that can adapt the preference learning and interactive HPO approach to the single-objective ML setting.

### Open Question 2
- Question: How can the feature representation of Pareto fronts be improved to be independent of the number of loss functions?
- Basis in paper: [explicit] The paper mentions this as a potential improvement to the current approach, which relies on a feature representation that depends on the number of loss functions.
- Why unresolved: The paper does not propose any specific alternative feature representations that are independent of the number of loss functions.
- What evidence would resolve it: A new feature representation for Pareto fronts that does not depend on the number of loss functions and can be used effectively with the preference learning approach.

### Open Question 3
- Question: How can the preliminary sampling phase be improved to ensure better coverage of the Pareto front space?
- Basis in paper: [explicit] The paper mentions this as a potential limitation and suggests using rejection sampling with a model mapping from hyperparameter configurations to Pareto front features.
- Why unresolved: The paper does not provide a concrete implementation or evaluation of this idea.
- What evidence would resolve it: An experimental comparison showing that the proposed meta-learning approach leads to better coverage of the Pareto front space and improved performance of the overall HPO approach.

## Limitations

- Scalability concerns with high-dimensional Pareto fronts or many models per front where pairwise visual comparison becomes impractical
- Linear RankSVM assumption may fail to capture complex, non-linear user preferences
- Feature representation depends on the number of loss functions, limiting generalizability

## Confidence

**High Confidence**: The claim that PB outperforms IB when users select wrong indicators is well-supported by empirical evidence (11/16 datasets showing PB superiority). The mechanism of replacing indicator selection with preference elicitation is clearly demonstrated.

**Medium Confidence**: The assertion that PB performs comparably to advanced users who know the correct indicator. While results show similar performance in many cases, the comparison is limited to a small number of datasets and doesn't account for user variability in selecting indicators.

**Medium Confidence**: The RankSVM-based utility function generalizes well from pairwise comparisons. The learning curve evidence is promising, but the linear assumption and limited comparison to non-linear alternatives reduces confidence.

## Next Checks

1. **Scalability Test**: Evaluate the approach with Pareto fronts containing 100+ models to verify if the pairwise comparison mechanism remains effective and if the utility function continues to generalize well.

2. **Non-linear Preference Capture**: Replace the linear RankSVM with a non-linear alternative (e.g., RBF kernel RankSVM or neural network-based ranker) to test if complex user preferences can be better captured, and compare performance against the linear baseline.

3. **User Variability Analysis**: Conduct a user study with multiple participants having different levels of ML expertise to assess how consistently the preference learning approach works across different users and whether the learned utilities are stable across users.