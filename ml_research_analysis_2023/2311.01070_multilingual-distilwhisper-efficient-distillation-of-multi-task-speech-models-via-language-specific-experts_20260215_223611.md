---
ver: rpa2
title: 'Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech Models
  via Language-Specific Experts'
arxiv_id: '2311.01070'
source_url: https://arxiv.org/abs/2311.01070
tags:
- speech
- languages
- clsr
- distilwhisper
- whisper-small
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DistilWhisper, a knowledge distillation approach
  to improve the multilingual ASR performance of the Whisper model for low-resource
  languages. The method uses language-specific experts and a gating mechanism to route
  tokens through shared or language-specific feed-forward layers, allowing efficient
  distillation of knowledge from the larger Whisper-large-v2 model.
---

# Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts

## Quick Facts
- arXiv ID: 2311.01070
- Source URL: https://arxiv.org/abs/2311.01070
- Reference count: 0
- Key outcome: DistilWhisper reduces the WER gap between Whisper-large-v2 and Whisper-small by 39% on out-of-domain data with only 10% parameter overhead.

## Executive Summary
This paper introduces DistilWhisper, a parameter-efficient approach for improving multilingual ASR performance on low-resource languages. The method combines language-specific experts with knowledge distillation from Whisper-large-v2 to boost performance while preserving robustness. Through conditional routing mechanisms and targeted adaptation, DistilWhisper achieves significant improvements over standard fine-tuning and LoRA adapters, particularly in out-of-domain settings.

## Method Summary
DistilWhisper extends the Whisper-small model with Conditional Language-Specific Routing (CLSR) modules that insert language-specific feed-forward layers with binary gating mechanisms. During training, each token is routed either through a shared multilingual feed-forward layer or a language-specific expert layer based on learned gating decisions. The model is trained using a combination of ASR fine-tuning loss, gate budget loss, and knowledge distillation loss from Whisper-large-v2. At inference, the model loads shared layers along with the language-specific modules and gates for the target languages.

## Key Results
- Reduces WER gap between Whisper-large-v2 and Whisper-small by 39% on out-of-domain FLEURS data
- Achieves 36.7% gap reduction with only 4 hours of training data
- Outperforms standard fine-tuning and LoRA adapters while adding only 10% parameter overhead
- Demonstrates effectiveness across eight languages from five different language sub-families

## Why This Works (Mechanism)

### Mechanism 1: Conditional Routing Through Language-Specific Experts
The CLSR modules use binary gates to route tokens either through shared multilingual feed-forward layers or language-specific expert layers. This selective routing reduces interference between languages while preserving the robust multilingual representation from Whisper's pre-training. The gating decision is made per-token based on hidden embeddings, allowing dynamic adaptation to language-specific patterns.

### Mechanism 2: Knowledge Distillation From Whisper-large-v2
The JS-divergence-based distillation loss aligns the student's output distribution with the teacher's, encouraging the student to mimic Whisper-large-v2's generalization patterns. This transfers robustness to out-of-domain data even without access to the full training corpus, addressing a key weakness of standard fine-tuning approaches.

### Mechanism 3: Combined Gating and Distillation Preservation
The gated routing mechanism selectively activates language-specific experts when beneficial, while the shared frozen backbone preserves multilingual features. Distillation enforces alignment with the robust teacher, maintaining generalization while boosting low-resource language performance. This dual approach effectively balances specialization with robustness.

## Foundational Learning

- **Transformer self-attention and feed-forward sublayers**: Essential for understanding how CLSR modules modify the standard architecture by replacing feed-forward layers with gated routing mechanisms. Quick check: In a standard Transformer feed-forward layer, what is the dimensionality of the intermediate representation relative to the hidden size?

- **Knowledge distillation and symmetric divergences**: Critical for grasping why JS divergence is used instead of KL divergence for distillation. Quick check: What is the key difference between KL-divergence and JS-divergence in terms of how they handle distributions that are far apart?

- **Parameter-efficient fine-tuning methods (LoRA, adapters)**: Necessary for evaluating DistilWhisper's claimed efficiency advantages. Quick check: How does LoRA achieve parameter efficiency compared to full fine-tuning?

## Architecture Onboarding

- **Component map**: Token embedding → Encoder self-attention → CLSR gating → Either shared or LS FFN → Decoder cross-attention → CLSR gating → Either shared or LS FFN → Final linear projection → CTC loss + KD loss

- **Critical path**: Token embedding flows through encoder self-attention, encounters CLSR gating decision, routes to either shared or language-specific FFN, continues through decoder cross-attention with similar gating, and produces final output via linear projection with combined CTC and KD losses.

- **Design tradeoffs**: 10% parameter overhead vs. full fine-tuning (100%); minimal routing overhead vs. extra LS weights; token-level gating flexibility vs. static adapters; KD loss for robustness vs. pure adaptation.

- **Failure signatures**: All tokens routed to shared layers (no LS benefit); LS gates stuck at extreme values (loss of shared knowledge); KD loss dominates (student diverges from ASR objective); overfitting on small datasets (high in-domain, low out-of-domain scores).

- **First 3 experiments**: 1) Train DistilWhisper on 10k utterances per language and evaluate in-domain vs. out-of-domain gap reduction. 2) Compare gating activation patterns with and without KD across in/out-of-domain test sets. 3) Train with varying dataset sizes (3k, 10k, 28k) and measure WER improvement per size.

## Open Questions the Paper Calls Out

- **How does performance vary across different language families?** The paper mentions languages from five sub-families but lacks detailed analysis of performance differences across these families, which could reveal insights about the model's adaptation to different linguistic structures.

- **What is the impact of different knowledge distillation techniques?** The paper focuses on JS divergence but does not explore alternative distillation methods or compare their effectiveness, leaving open questions about whether other techniques might yield better robustness and accuracy.

- **How does scalability affect performance with more languages or larger models?** While the paper demonstrates effectiveness on eight languages, it does not explore how the architecture scales to additional languages or larger model sizes, raising questions about adaptability in more diverse settings.

## Limitations

- Implementation details for critical components like the CLSR module and JS divergence formulation are not fully specified, making exact replication challenging.
- Experimental results are based on CommonVoice and FLEURS datasets, which may not fully represent the diversity of low-resource languages in real-world applications.
- The claimed minimal inference overhead from routing decisions may become significant at scale, particularly for real-time applications with multiple language-specific experts.

## Confidence

- **High Confidence**: The core claim that language-specific experts combined with knowledge distillation can improve low-resource ASR performance, supported by ablation studies and direct comparisons.
- **Medium Confidence**: The 39% WER gap reduction on out-of-domain data, as results may not generalize beyond the specific datasets and experimental conditions.
- **Medium Confidence**: The efficiency claims regarding 10% parameter overhead, as practical impact depends on implementation details not fully disclosed.

## Next Checks

1. **Ablation of Routing Mechanism**: Conduct controlled experiments removing the gating mechanism to verify that language-specific experts alone provide similar benefits, isolating the contribution of dynamic routing versus static adaptation.

2. **Teacher Model Sensitivity**: Test the approach with different teacher models (e.g., whisper-medium-v2) to determine whether improvements are primarily due to the distillation objective or specific properties of whisper-large-v2.

3. **Cross-Lingual Transfer Analysis**: Evaluate the model's ability to handle code-switching and mixed-language input by creating synthetic code-switched test sets, verifying that the gating mechanism effectively routes between languages in realistic multilingual scenarios.