---
ver: rpa2
title: 'Magnushammer: A Transformer-Based Approach to Premise Selection'
arxiv_id: '2303.04488'
source_url: https://arxiv.org/abs/2303.04488
tags:
- proof
- premise
- premises
- magnushammer
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses premise selection in formal theorem proving,
  where the goal is to automatically identify relevant mathematical facts (premises)
  to advance a proof from a given proof state. The proposed method, Magnushammer,
  uses a transformer-based model trained with contrastive learning to retrieve and
  re-rank premises based on their textual representations, without relying on domain-specific
  heuristics.
---

# Magnushammer: A Transformer-Based Approach to Premise Selection

## Quick Facts
- arXiv ID: 2303.04488
- Source URL: https://arxiv.org/abs/2303.04488
- Reference count: 40
- Primary result: Achieves 59.5% proof success rate on PISA benchmark, outperforming Sledgehammer (38.3%)

## Executive Summary
Magnushammer addresses the challenge of premise selection in formal theorem proving by using a transformer-based model trained with contrastive learning. The method treats proof states and premises as text, making no assumptions about their formal structure, and can be applied to any theorem-proving environment with sufficient training data. The model consists of two stages: a fast SELECT stage that retrieves relevant premises using cosine similarity of embeddings, and a RERANK stage that refines the selection using transformer attention over full proof state-premise pairs. This approach achieves state-of-the-art performance on the PISA benchmark while being data-efficient and scalable across different computational budgets.

## Method Summary
Magnushammer uses a two-stage transformer-based approach for premise selection in formal theorem proving. The method begins with a pre-trained transformer backbone (86M parameters) that is fine-tuned on the MAPL dataset using alternating SELECT and RERANK tasks. In the SELECT stage, the model learns to produce embeddings for proof states and premises that are close in embedding space when the premise is relevant. The RERANK stage uses the transformer to attend over full textual representations of (proof state, premise) pairs to produce relevance scores. The model is trained using contrastive learning with in-batch and sampled negatives, and evaluates using a computational budget of 12 seconds per proof step.

## Key Results
- Achieves 59.5% proof success rate on PISA benchmark, substantially outperforming Sledgehammer's 38.3%
- When combined with TacTok, improves proof success rate from 57.0% to 71.0% using 4x fewer parameters
- Data-efficient: fine-tuned on only 0.1% of MAPL dataset (approximately 4K samples) while still outperforming Sledgehammer
- Scalable: performance improves with larger computational budgets and shows promise for further improvements with more computing resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Magnushammer improves premise selection by combining fast similarity-based retrieval with fine-grained re-ranking using transformer attention over full proof state-premise pairs.
- Mechanism: SELECT stage uses cosine similarity in a shared embedding space to quickly retrieve the top 1024 most relevant premises from a large database. RERANK stage then uses a transformer to attend over the full textual representation of each (proof state, premise) pair to refine the ranking, producing a relevance score for each candidate.
- Core assumption: Textual representations of proof states and premises capture enough semantic information to allow a transformer to distinguish relevant from irrelevant premises.
- Evidence anchors:
  - [abstract] "Magnushammer consists of two stages of retrieval, both trained using contrastive learning. Given a proof state, in the SELECT stage, we retrieve the most relevant 1024 premises (measured by the cosine similarity of their embeddings) from hundreds of thousands of premises in the theorem (database up to 433K). In the second stage, RERANK, we re-rank the retrieved premises with more fine-grained but expensive processing: in a transformer architecture, we let the tokens from the proof state directly attend to the tokens of the retrieved premise, outputting a relevance score."
  - [section] "Magnushammer achieves a 59.5% proof rate on the PISA benchmark (Jiang et al., 2021), a substantial improvement over Sledgehammer’s 38.3% proof rate."
- Break condition: If the textual representation fails to capture the necessary semantic content (e.g., if proof states rely heavily on internal proof object structure not reflected in text), the transformer will not be able to learn effective relevance scoring.

### Mechanism 2
- Claim: Contrastive learning with in-batch and sampled negatives improves embedding quality for premise retrieval.
- Mechanism: SELECT stage is trained with a modified InfoNCE loss using both in-batch negatives (other premises in the same batch) and additional negatives sampled from premises not used in the proof state. This provides richer negative examples and forces the model to learn tighter clusters for relevant premises.
- Core assumption: Additional negatives beyond in-batch examples provide more informative training signals for learning discriminative embeddings.
- Evidence anchors:
  - [abstract] "SELECT leverages representation similarity and is based on batch-contrastive learning similar to the methods of (Alemi et al., 2016; Bansal et al., 2019; Han et al., 2021; Radford et al., 2021)."
  - [section] "We typically use M = 3N, which differs from standard batch-contrastive learning (Radford et al., 2021), in which M = 0 and negatives are only the other N − 1 premises in the batch."
- Break condition: If the negative sampling strategy fails to provide diverse and challenging examples, the embeddings may not learn to distinguish subtle relevance differences.

### Mechanism 3
- Claim: Pre-training on large language corpora improves downstream premise selection performance, especially when training data is limited.
- Mechanism: The transformer backbone is pre-trained on GitHub and arXiv subsets of The Pile dataset using language modeling, then fine-tuned on premise selection tasks. This pre-training provides general language understanding that transfers to the mathematical domain.
- Core assumption: General language understanding from pre-training is useful for understanding the specific language of mathematical proofs and premises.
- Evidence anchors:
  - [abstract] "The backbone is pre-trained with a language modeling task on the GitHub and arXiv subsets of The Pile dataset (Gao et al., 2021)."
  - [section] "We observe that Magnushammer fine-tuned on only 0.1% of MAPL, equivalent to approximately 4K samples, is already able to outperform Sledgehammer. This indicates that when starting from a pre-trained model, Magnushammer is a promising approach for addressing premise selection in theorem-proving environments with limited training data."
- Break condition: If the pre-training corpus does not contain relevant mathematical or logical content, the transfer may be limited.

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: To learn discriminative embeddings where relevant premises are close together and irrelevant premises are far apart in the embedding space.
  - Quick check question: What is the difference between using only in-batch negatives versus adding sampled negatives in contrastive learning?

- Concept: Transformer attention mechanisms for text pairing
  - Why needed here: To allow the model to focus on relevant parts of both proof states and premises when determining relevance.
  - Quick check question: How does the transformer in the RERANK stage differ from a standard text classification transformer?

- Concept: Two-stage retrieval (coarse + fine)
  - Why needed here: To balance speed and accuracy - SELECT provides fast retrieval from a large database, RERANK provides more accurate scoring on a smaller candidate set.
  - Quick check question: Why is it computationally prohibitive to use the transformer RERANK stage on all premises in the database?

## Architecture Onboarding

- Component map:
  Pre-trained transformer backbone -> SELECT stage (embedding projections + cosine similarity) -> RERANK stage (attention over pairs + sigmoid scoring)

- Critical path:
  1. Input proof state text → SELECT embedding projection → cosine similarity with cached premise embeddings → top K premises
  2. Top K premises + proof state → RERANK transformer → relevance scores → final top KR premises

- Design tradeoffs:
  - Speed vs. accuracy: SELECT is fast but less accurate; RERANK is accurate but slower
  - Memory vs. retrieval quality: Larger premise databases improve coverage but increase memory for cached embeddings
  - Model size vs. performance: Larger models show better performance but increase computational cost

- Failure signatures:
  - Low precision: SELECT retrieves irrelevant premises (embedding space not discriminative enough)
  - Low recall: SELECT misses relevant premises (embedding space too sparse)
  - RERANK fails to improve ranking: Transformer not learning useful attention patterns
  - Poor generalization: Pre-training corpus not representative of proof assistant language

- First 3 experiments:
  1. Ablation: Run Magnushammer with SELECT only (no RERANK) to measure the contribution of the re-ranking stage
  2. Dataset size scaling: Train on increasing fractions of MAPL dataset to measure data efficiency
  3. Model size scaling: Train with varying numbers of transformer layers to measure impact of model capacity

## Open Questions the Paper Calls Out

- How would Magnushammer's performance scale with significantly larger models, datasets, and computational budgets beyond those tested?
- Can the textual representation of proof states and premises be further improved by incorporating more structural information from the Isabelle proof objects?
- Can Magnushammer be successfully adapted to other proof assistants beyond Isabelle, and how would its performance compare across different formal systems?

## Limitations
- Reliance on textual representations may not capture all semantic information present in formal proof objects
- Evaluation limited to Isabelle/HOL ecosystem and PISA benchmark, raising questions about generalizability
- Computational budget used for evaluation (12 seconds per step) is relatively generous compared to typical automated theorem proving scenarios

## Confidence
- High confidence: The core claim that Magnushammer outperforms Sledgehammer on PISA (59.5% vs 38.3%) is well-supported by the experimental results presented in the paper
- Medium confidence: The claim about data efficiency (achieving strong performance with only 0.1% of MAPL) is supported by results but depends on the quality and representativeness of the pre-training corpus
- Medium confidence: The claim about combining with TacTok to improve proof success rate from 57.0% to 71.0% is supported but represents a specific combination that may not generalize to all theorem provers

## Next Checks
1. Evaluate Magnushammer on different proof assistant ecosystems (e.g., Lean, Coq) to verify if the performance gains transfer beyond Isabelle/HOL
2. Compare Magnushammer's performance when using full formal proof objects versus textual representations to quantify the impact of the text-only approach
3. Systematically vary the computational budget and measure how proof success rate changes, particularly focusing on the regime where Magnushammer would be most practical in real-world theorem proving