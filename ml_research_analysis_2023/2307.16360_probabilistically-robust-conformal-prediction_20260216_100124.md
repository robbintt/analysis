---
ver: rpa2
title: Probabilistically robust conformal prediction
arxiv_id: '2307.16360'
source_url: https://arxiv.org/abs/2307.16360
tags:
- coverage
- robust
- aprcp
- prediction
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces probabilistically robust conformal prediction\
  \ (PRCP) to improve uncertainty quantification of deep classifiers under adversarial\
  \ perturbations. The key idea is to use a quantile-of-quantile design that finds\
  \ two parallel thresholds\u2014one for clean data and another for most perturbations\u2014\
  achieving probabilistically robust coverage without requiring special score functions\
  \ or test-time overhead."
---

# Probabilistically robust conformal prediction

## Quick Facts
- arXiv ID: 2307.16360
- Source URL: https://arxiv.org/abs/2307.16360
- Reference count: 25
- One-line primary result: Introduces probabilistically robust conformal prediction (PRCP) to improve uncertainty quantification under adversarial perturbations

## Executive Summary
This paper introduces probabilistically robust conformal prediction (PRCP) to improve uncertainty quantification of deep classifiers under adversarial perturbations. The key idea is to use a quantile-of-quantile design that finds two parallel thresholds—one for clean data and another for most perturbations—achieving probabilistically robust coverage without requiring special score functions or test-time overhead. The adaptive PRCP (aPRCP) algorithm is theoretically analyzed and shown to guarantee robust coverage. Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that aPRCP achieves tighter empirical coverage closer to the target (90%) and smaller prediction set sizes compared to vanilla CP and adversarially robust CP (RSCP), balancing nominal and robust performance more effectively.

## Method Summary
The paper proposes a probabilistically robust conformal prediction (PRCP) algorithm called aPRCP that achieves robust coverage guarantees under adversarial perturbations. The method uses a "quantile-of-quantile" design where it computes robust quantiles for each data point considering perturbations, then derives a threshold from these robust quantiles to construct prediction sets. The algorithm employs non-conformity scores (HPS and APS) and uses a conservativeness parameter s to balance between nominal performance and robust coverage. Calibration is performed with noisy samples, and the method provides theoretical guarantees on the approximation error of empirical quantiles.

## Key Results
- aPRCP achieves better trade-offs than state-of-the-art CP and adversarially robust CP algorithms on CIFAR-10, CIFAR-100, and ImageNet
- aPRCP maintains tighter empirical coverage closer to the target (90%) compared to vanilla CP and RSCP
- aPRCP produces smaller prediction set sizes while maintaining robust coverage under adversarial perturbations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adaptive PRCP algorithm achieves probabilistically robust coverage by computing two parallel quantiles—one for clean data and another for most perturbations.
- Mechanism: The algorithm uses a "quantile-of-quantile" design where it first computes robust quantiles for each data point considering perturbations, then derives a threshold from these robust quantiles to construct prediction sets.
- Core assumption: The score function is Mr,η-probabilistically inflated, meaning the probability that the score on perturbed data exceeds the clean score by Mr,η is at least 1-η.
- Evidence anchors:
  - [abstract] "The key idea behind aPRCP is to determine two parallel thresholds, one for data samples and another one for the perturbations on data (aka 'quantile-of-quantile' design)."
  - [section] "Qrob(X, Y ; ˜α) := min{t : Pϵ{S(eX, Y ) ≤ t} ≥ 1 − ˜α}" - Definition of robust quantile
  - [corpus] Weak evidence - related works focus on adversarial robustness but don't explicitly describe quantile-of-quantile mechanisms
- Break condition: If the score function doesn't satisfy the probabilistically inflated condition, the theoretical guarantees fail.

### Mechanism 2
- Claim: The adaptive PRCP algorithm can achieve improved efficiency compared to adversarially robust CP methods while maintaining similar robust coverage.
- Mechanism: By tuning the conservativeness parameter s, the algorithm can find a balance between nominal performance and robust performance, potentially achieving smaller prediction set sizes than methods that ensure worst-case robustness.
- Core assumption: There exists a value of s that provides better efficiency than worst-case approaches while still achieving the target coverage.
- Evidence anchors:
  - [abstract] "aPRCP achieves better trade-offs than state-of-the-art CP and adversarially robust CP algorithms"
  - [section] "Corollary 3. To achieve the same (1 − α)-probabilistically robust coverage on Z, the following inequalities hold: minη∈[0,α] τ iPR(α; η) ≤ τ AR(α), mins∈[0,α] τ aPR(α; s) ≤ τ AR(α)"
  - [corpus] Weak evidence - related works discuss adversarial robustness but don't provide quantitative efficiency comparisons
- Break condition: If the underlying data distribution doesn't allow for meaningful trade-offs between nominal and robust performance.

### Mechanism 3
- Claim: The adaptive PRCP algorithm provides theoretical guarantees on the approximation error of empirical quantiles.
- Mechanism: The algorithm's performance improves with more calibration samples, with the approximation error decreasing at a rate of O(1/√n).
- Core assumption: The empirical quantiles converge to population quantiles as the number of samples increases.
- Evidence anchors:
  - [abstract] "We provide theoretical analysis to show that aPRCP algorithm achieves robust coverage"
  - [section] "Proposition 3. (Concentration inequality for quantiles)... with probability at least 1 − δ, we have bQn(α + ˜O(1/√n)) ≤ Q(α) ≤ bQn(α − ˜O(1/√n))"
  - [corpus] Weak evidence - related works mention concentration inequalities but don't provide specific convergence rates
- Break condition: If the number of calibration samples is too small, the empirical quantiles may not provide reliable coverage guarantees.

## Foundational Learning

- Concept: Conformal Prediction (CP) Framework
  - Why needed here: Understanding CP is essential as aPRCP builds upon and extends the standard CP framework to handle adversarial perturbations.
  - Quick check question: What are the two key steps in standard CP and how does aPRCP modify these steps?

- Concept: Quantile-Based Threshold Selection
  - Why needed here: aPRCP's core mechanism relies on computing and using quantiles of conformity scores to determine prediction set thresholds.
  - Quick check question: How does the "quantile-of-quantile" design in aPRCP differ from standard quantile-based threshold selection in CP?

- Concept: Adversarial Robustness in Machine Learning
  - Why needed here: aPRCP specifically addresses the challenge of maintaining coverage guarantees under adversarial perturbations.
  - Quick check question: What is the difference between probabilistically robust coverage and adversarially robust coverage in the context of CP?

## Architecture Onboarding

- Component map:
  Input Processing -> Perturbation Sampling -> Score Computation -> Quantile Estimation -> Threshold Determination -> Prediction Set Construction

- Critical path: Clean data → Perturbation sampling → Score computation → Quantile estimation → Threshold determination → Prediction set construction

- Design tradeoffs:
  - Sampling Strategy: Trade-off between computational cost (number of perturbations) and robustness guarantees
  - Conservativeness Parameter (s): Balancing between nominal performance and robust coverage
  - Quantile Probability (˜α): Determining the level of robustness against perturbations

- Failure signatures:
  - Coverage violation: Empirical coverage significantly below target coverage
  - Overly conservative prediction sets: Prediction sets containing most or all possible labels
  - High variance in prediction set sizes: Inconsistent performance across different inputs

- First 3 experiments:
  1. Implement aPRCP on a simple binary classification dataset (e.g., MNIST) with controlled perturbations to verify coverage guarantees.
  2. Compare prediction set sizes of aPRCP with vanilla CP and RSCP on CIFAR-10 under both clean and adversarial conditions.
  3. Analyze the effect of varying the conservativeness parameter s on the trade-off between nominal and robust performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the hyper-parameter s in aPRCP affect the trade-off between nominal and robust performance across different datasets and model architectures?
- Basis in paper: [explicit] The paper discusses the role of the s hyper-parameter in achieving probabilistically robust coverage and mentions varying s to achieve the target marginal coverage.
- Why unresolved: The paper provides some empirical results but does not conduct a systematic analysis of how different s values impact performance across various datasets and model architectures.
- What evidence would resolve it: Comprehensive experiments evaluating aPRCP with a wide range of s values on multiple datasets and model architectures, analyzing the impact on coverage and prediction set size.

### Open Question 2
- Question: Can aPRCP be extended to handle other types of perturbations beyond l2-norm bounded noise, such as adversarial examples generated by different attack algorithms or natural distribution shifts?
- Basis in paper: [inferred] The paper focuses on l2-norm bounded noise and adversarial examples but does not explore other types of perturbations or distribution shifts.
- Why unresolved: The theoretical analysis and experimental evaluation are limited to specific types of perturbations, leaving open the question of generalizability.
- What evidence would resolve it: Theoretical analysis and empirical evaluation of aPRCP on various perturbation types and distribution shifts, demonstrating its effectiveness and limitations.

### Open Question 3
- Question: How does the choice of the noise distribution during calibration (e.g., uniform vs. Gaussian) impact the performance of aPRCP in terms of coverage and prediction set size?
- Basis in paper: [explicit] The paper mentions using different noise distributions for calibration and testing and provides some empirical results.
- Why unresolved: The paper does not provide a systematic comparison of the impact of different noise distributions on aPRCP's performance.
- What evidence would resolve it: Experiments comparing aPRCP's performance using different noise distributions for calibration and testing, analyzing the impact on coverage and prediction set size.

## Limitations

- The experimental evaluation focuses on specific datasets (CIFAR and ImageNet) and perturbation types (l2-bounded adversarial perturbations), limiting generalizability.
- The paper does not conduct a systematic analysis of how the conservativeness parameter s affects performance across different datasets and model architectures.
- Limited comparison with other recent robust CP methods raises questions about the claim of "state-of-the-art" performance.

## Confidence

High confidence: Theoretical guarantees on probabilistically robust coverage and efficiency improvements under Mr,η-probabilistically inflated score assumption.

Medium confidence: Experimental results demonstrate practical improvements on CIFAR-10, CIFAR-100, and ImageNet, but generalizability to other datasets and perturbation types remains uncertain.

Low confidence: Claims of "state-of-the-art" performance due to limited comparison with other recent robust CP methods.

## Next Checks

1. Test aPRCP on additional datasets (e.g., medical imaging or NLP) and perturbation types (e.g., rotation, occlusion) to assess robustness across diverse scenarios.

2. Evaluate computational overhead by measuring wall-clock time for calibration and prediction phases compared to vanilla CP and RSCP.

3. Investigate the sensitivity of the conservativeness parameter s through ablation studies to determine optimal settings for different data distributions and perturbation intensities.