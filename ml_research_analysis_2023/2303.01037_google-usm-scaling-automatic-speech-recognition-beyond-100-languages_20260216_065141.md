---
ver: rpa2
title: 'Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages'
arxiv_id: '2303.01037'
source_url: https://arxiv.org/abs/2303.01037
tags:
- speech
- data
- languages
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors develop a Universal Speech Model (USM) for ASR across
  100+ languages, leveraging a large multilingual dataset of 12M hours from YouTube.
  They use unsupervised pre-training (BEST-RQ) followed by multi-objective supervised
  training (MOST) with text-injection to improve robustness.
---

# Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages

## Quick Facts
- **arXiv ID**: 2303.01037
- **Source URL**: https://arxiv.org/abs/2303.01037
- **Reference count**: 40
- **Key outcome**: USM achieves <30% WER on 73 languages and outperforms Whisper on YouTube ASR despite using 1/7th the labeled data

## Executive Summary
This paper presents Universal Speech Model (USM), a multilingual ASR system trained on 12M hours of YouTube audio spanning 300+ languages. The model uses BEST-RQ unsupervised pre-training followed by multi-objective supervised training (MOST) with text-injection to achieve state-of-the-art performance across 100+ languages. USM demonstrates superior performance on YouTube ASR benchmarks compared to Whisper while using significantly less labeled data, and sets new records on FLEURS and CoV oST speech translation tasks.

## Method Summary
USM uses a Conformer encoder (600M or 2B parameters) pre-trained with BEST-RQ on unlabeled multilingual speech. The model is then fine-tuned using multi-objective supervised training that combines unlabeled speech, unlabeled text, and paired speech-text data through text-injection. For long-form audio, chunk-wise attention is applied, and residual adapters enable efficient adaptation to new languages and domains. The system leverages a massive dataset including 12M hours of YouTube audio and 28B sentences of multilingual text.

## Key Results
- Achieves <30% WER on 73 languages in the FLEURS benchmark
- Outperforms Whisper on YouTube ASR benchmarks using 1/7th the labeled data
- Sets new state-of-the-art on CoV oST speech translation task
- Supports long-form audio via chunk-wise attention mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BEST-RQ with multi-softmax improves convergence and stability over single softmax
- Mechanism: Multiple independent softmax layers provide parallel gradient signals, reducing instability from single codebook collapse
- Core assumption: Codebook collapse is a bottleneck in scaling unsupervised speech pre-training
- Evidence anchors: Abstract mentions "random-projection quantization"; section 2.2.1 describes multi-softmax approach; Table 5 shows internal experiment results
- Break condition: If gradient variance becomes too high or model capacity is insufficient

### Mechanism 2
- Claim: MOST (BEST-RQ + text-injection) learns joint speech-text representations that improve ASR and AST
- Mechanism: Training on unlabeled speech, unlabeled text, and paired data aligns speech and text embeddings in shared space
- Core assumption: Joint embedding space allows cross-modal transfer from text to speech
- Evidence anchors: Abstract mentions "speech-text modality matching"; section 2.5 describes text-injection; limited external citation
- Break condition: If modality alignment loss dominates and causes speech quality degradation

### Mechanism 3
- Claim: Chunk-wise attention solves long-form ASR degradation by avoiding train-test receptive field mismatch
- Mechanism: Restricting attention to fixed-size audio chunks prevents context leakage across layers
- Core assumption: Local self-attention with accumulated context creates harmful train-test mismatch
- Evidence anchors: Abstract mentions "chunk-wise attention"; section 2.4 describes the mechanism; Fig. 7 and Table 7 show internal results
- Break condition: If chunk size is too small to capture linguistic context or too large to be efficient

## Foundational Learning

- **Concept**: Self-supervised speech representation learning (wav2vec 2.0, BEST-RQ)
  - Why needed here: Enables training on massive unlabeled multilingual audio without manual transcription
  - Quick check question: What is the key difference between contrastive loss (wav2vec 2.0) and masked prediction (BEST-RQ)?

- **Concept**: Modality matching and text-injection for multimodal pre-training
  - Why needed here: Leverages large text-only corpora to improve speech representations for low-resource languages
  - Quick check question: How does the consistency regularization loss in text-injection help align speech and text embeddings?

- **Concept**: Residual adapter modules for parameter-efficient adaptation
  - Why needed here: Allows fine-tuning on many languages/tasks without re-training entire model
  - Quick check question: What is the approximate parameter overhead when adding residual adapters to the Conformer-2B?

## Architecture Onboarding

- **Component map**: Conformer encoder -> BEST-RQ pre-training -> MOST (speech encoder + shared encoder + text encoder + decoder) -> Downstream transducers (CTC, LAS, RNN-T) -> Chunk-wise attention (for long-form) -> Residual adapters (for adaptation)

- **Critical path**:
  1. Pre-train Conformer encoder with BEST-RQ on YT-NTL-U (12M hrs)
  2. (Optional) Continue with MOST on YT-NTL-U + Pub-U + Web-NTL + Pub-S
  3. Fine-tune with downstream transducer on task-specific data
  4. For long-form: apply chunk-wise attention
  5. For new languages: attach and train residual adapters

- **Design tradeoffs**:
  - BEST-RQ vs wav2vec 2.0: Simpler, no codebook collapse, better scaling
  - Chunk-wise vs local self-attention: Robustness to long-form degradation vs. computational efficiency
  - MOST vs speech-only: Better low-resource performance vs. increased training complexity
  - Residual adapters vs full fine-tuning: Parameter efficiency vs. potential performance gap

- **Failure signatures**:
  - Training instability: Check BEST-RQ loss variance, consider reducing learning rate or increasing softmax groups
  - Poor low-resource performance: Verify text-injection losses are active, check language sampling
  - Long-form degradation: Confirm chunk size matches receptive field needs, check attention mask
  - Hallucinations on unseen languages: Validate pseudo-label quality, check adapter initialization

- **First 3 experiments**:
  1. Pre-train Conformer-0.6B with BEST-RQ on YT-55 subset, measure CoV oST BLEU
  2. Apply chunk-wise attention (8s chunks) to Conformer-0.6B, test on YouTube long-form set
  3. Add 16-group multi-softmax to BEST-RQ, compare convergence speed and final CoV oST BLEU

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does USM performance scale with model size beyond 2B parameters, particularly for low-resource languages?
- Basis in paper: The study focuses on 600M and 2B parameter models without exploring larger sizes
- Why unresolved: Scaling behavior for larger models and impact on low-resource languages remains unexplored
- What evidence would resolve it: Experimental results comparing USM performance with models larger than 2B parameters across different resource levels

### Open Question 2
- Question: What is the optimal trade-off between chunk size and recognition accuracy for long-form audio in chunk-wise attention?
- Basis in paper: The paper mentions using 8-second chunks but doesn't systematically explore different sizes
- Why unresolved: Only reports results for single chunk size without investigating effects of varying chunk sizes
- What evidence would resolve it: Ablation studies testing multiple chunk sizes (4s, 8s, 16s) on long-form audio benchmarks

### Open Question 3
- Question: How does USM's performance on CoV oST compare when using speech-only vs joint speech-text fine-tuning across different language resource levels?
- Basis in paper: USM-M outperforms USM with joint fine-tuning but doesn't analyze performance across resource categories
- Why unresolved: While joint fine-tuning benefits are shown, performance differences by language resource level aren't broken down
- What evidence would resolve it: Detailed performance analysis of speech-only vs joint fine-tuning on high, mid, and low resource languages

### Open Question 4
- Question: What is the impact of using different tokenization strategies (graphemes vs word-pieces) on USM's performance across different language families?
- Basis in paper: Mentions using graphemes for FLEURS and word-pieces for other tasks without analyzing differential effects
- Why unresolved: Study doesn't investigate whether tokenization strategy affects language families with different morphological characteristics
- What evidence would resolve it: Comparative analysis of USM performance using different tokenization strategies across various language families

## Limitations
- Relies heavily on pseudo-labeled data quality for low-resource language performance
- Limited ablation studies on individual components' contributions to overall performance
- Evaluation primarily focused on YouTube-centric benchmarks without extensive cross-dataset validation

## Confidence
- **High Confidence**: General framework of unsupervised pre-training followed by supervised fine-tuning; Conformer architecture effectiveness; Whisper comparison on YouTube benchmarks
- **Medium Confidence**: BEST-RQ with multi-softmax implementation details; text-injection mechanism effectiveness; <30% WER claim on 73 languages
- **Low Confidence**: Claims about robustness to domain shifts; residual adapter effectiveness for rapid adaptation; chunk-wise attention superiority without comprehensive ablation

## Next Checks
1. Conduct ablation study removing text-injection while keeping BEST-RQ, and vice versa, to quantify individual contributions across different language groups
2. Evaluate USM on publicly available multilingual ASR benchmarks (CommonVoice, VoxPopuli) not part of training data to verify domain robustness
3. Compare USM with residual adapters against full fine-tuning baselines for both high-resource and low-resource languages to quantify parameter efficiency benefits