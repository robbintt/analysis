---
ver: rpa2
title: 'Positive Unlabeled Learning Selected Not At Random (PULSNAR): class proportion
  estimation when the SCAR assumption does not hold'
arxiv_id: '2303.08269'
source_url: https://arxiv.org/abs/2303.08269
tags:
- unlabeled
- positive
- examples
- positives
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PULSNAR, a novel algorithm for estimating
  the proportion of positive examples among unlabeled data when the SCAR assumption
  fails. The method clusters labeled positives into subtypes, applies PULSCAR to each
  cluster, and combines the estimates.
---

# Positive Unlabeled Learning Selected Not At Random (PULSNAR): class proportion estimation when the SCAR assumption does not hold

## Quick Facts
- arXiv ID: 2303.08269
- Source URL: https://arxiv.org/abs/2303.08269
- Reference count: 40
- Mean absolute errors below 5% across varying class imbalances and proportions

## Executive Summary
PULSNAR addresses the challenge of estimating the proportion of positive examples among unlabeled data when the standard SCAR (Selected Completely At Random) assumption fails. The method clusters labeled positives into homogeneous subtypes based on feature importance, applies PULSCAR to each cluster, and combines the estimates. This divide-and-conquer approach effectively handles SNAR (Selected Not At Random) scenarios where positive examples are labeled based on their features. Experiments demonstrate that PULSNAR outperforms state-of-the-art methods in α estimation and improves classification metrics including accuracy, AUC-ROC, and F1-score.

## Method Summary
PULSNAR is a two-stage algorithm for PU learning that first clusters labeled positive examples into homogeneous subtypes using Gaussian mixture models on scaled important features, then applies PULSCAR to each cluster to estimate the positive proportion α. The final α is obtained by summing subtype proportions. PULSCAR uses beta kernel density estimation on ML-predicted probabilities to estimate positive and unlabeled densities, finding α where the maximum deviation between scaled positive density and unlabeled density is minimized. The method employs XGBoost as the base classifier and uses 5-fold cross-validation for evaluation. Probability calibration is performed using isotonic or sigmoid regression to improve classification performance.

## Key Results
- Mean absolute errors below 5% in α estimation across varying class imbalances
- Outperforms state-of-the-art methods (DEDPUL, KM1, KM2, TICE) in α estimation accuracy
- Improves classification metrics including accuracy, AUC-ROC, and F1-score
- Scales to large datasets while maintaining estimation accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PULSNAR improves α estimation by decomposing SNAR problems into SCAR-like subproblems via clustering.
- Mechanism: The algorithm uses Gaussian mixture models to cluster labeled positives into homogeneous subtypes based on feature importance scores. Each cluster is then treated as a SCAR problem and solved with PULSCAR, with final α estimated by summing subtype proportions.
- Core assumption: Selection bias (SNAR) is driven by subclass membership, so clustering can isolate subclasses with more homogeneous selection probabilities.
- Evidence anchors:
  - [section] "The PULSNAR algorithm divides the labeled positive examples into k clusters. For each cluster, after estimating αj for j in 1...k , the class 1 predicted probabilities of only unlabeled examples are calibrated using Algorithm 4."
  - [abstract] "PULSNAR employs a divide-and-conquer approach to cluster SNAR positives into subtypes and estimates α for each subtype by applying PULSCAR to positives from each cluster and all unlabeled."
- Break condition: If selection bias is not driven by subclass membership (e.g., bias depends on a continuous covariate not captured by clustering), the SCAR assumption will not hold in any cluster, and PULSNAR will underperform.

### Mechanism 2
- Claim: PULSCAR accurately estimates α in SCAR data using a density-based error function.
- Mechanism: It estimates probability densities of positive and unlabeled examples using beta kernel density estimation. α is chosen to minimize the maximum deviation between the scaled positive density and the unlabeled density, found via the maximum of the derivative of an error function.
- Core assumption: The true negative density can be represented as fu(x) - αfp(x) everywhere positive, and the beta kernel provides smooth, boundary-respecting density estimates.
- Evidence anchors:
  - [section] "A key observation is that fp(x) should not exceed fu(x) anywhere, allowing one to place an upper bound on α. PULSCAR estimates α by finding the value α where the following error function maximally changes: f(α) = log(| min(fu(x) - αfp(x))| + ϵ)"
  - [section] "We use beta kernel density estimates on ML-predicted class 1 probabilities of positives and unlabeled to estimate fp(x) and fu(x)."
- Break condition: If the ML model is poorly calibrated or the densities are multimodal with overlapping regions, the density subtraction may not yield a clean separation, causing α to be poorly estimated.

### Mechanism 3
- Claim: PULSNAR scales well to large, imbalanced datasets by avoiding computationally expensive density matching.
- Mechanism: Instead of global density matching over the full dataset, PULSNAR reduces the problem size by clustering positives into smaller, more homogeneous groups and applying PULSCAR locally. This avoids the high computational cost of global methods like DEDPUL.
- Core assumption: The computational complexity of PULSCAR on smaller clustered subproblems is much lower than on the full dataset, and clustering does not introduce significant information loss.
- Evidence anchors:
  - [section] "We compared our algorithm with DEDPUL on synthetic SNAR datasets with different fractions... As the class imbalance increased, the performance of DEDPUL dropped, especially for larger true fractions. The estimated α by the PULSNAR method was close to the true α for all fractions and sample sizes."
  - [abstract] "It also improves classification metrics like accuracy, AUC-ROC, and F1-score."
- Break condition: If clustering produces too many small clusters, PULSCAR must be run many times, increasing computational cost; if too few clusters are formed, the SCAR assumption may still fail within clusters.

## Foundational Learning

- Concept: Selected Completely At Random (SCAR) assumption
  - Why needed here: Many PU learning algorithms assume positives are labeled independently of their features; understanding this is critical to see why PULSNAR is needed for SNAR data.
  - Quick check question: If p(s=1|x,y=1) is constant across all x, what assumption about labeling is being made?

- Concept: Beta kernel density estimation
  - Why needed here: PULSCAR uses beta kernels to estimate probability densities over [0,1], avoiding boundary issues of Gaussian kernels.
  - Quick check question: Why is a beta kernel preferred over a Gaussian kernel when estimating densities of predicted probabilities?

- Concept: Gaussian mixture model clustering
  - Why needed here: PULSNAR uses GMM clustering on scaled important features to identify homogeneous subtypes of labeled positives.
  - Quick check question: What property of GMM makes it suitable for identifying subtypes when the number of clusters is unknown?

## Architecture Onboarding

- Component map:
  XGBoost classifier -> Beta kernel density estimator -> GMM clustering -> PULSCAR core -> Probability calibrator -> Performance evaluator

- Critical path:
  1. Train XGBoost on P+U to get predicted probabilities
  2. In PULSCAR: Estimate densities, compute error function, find α
  3. In PULSNAR: Cluster positives, apply PULSCAR to each cluster, sum α
  4. Calibrate probabilities using estimated α
  5. Improve classification by flipping top α|U| unlabeled examples
  6. Evaluate performance

- Design tradeoffs:
  - Using beta kernels increases estimation stability but slows computation vs. Gaussian kernels.
  - Clustering reduces problem size but may lose information if subtypes are not cleanly separable.
  - 5-fold CV improves robustness but increases runtime.

- Failure signatures:
  - α estimate far from true value → check if SCAR assumption holds or if clustering failed.
  - Calibration curves deviate from y=x → check if ML probabilities are poorly calibrated or α is misestimated.
  - Performance degrades on SNAR data → check if clustering produced too few or too many clusters.

- First 3 experiments:
  1. Run PULSCAR on a small SCAR synthetic dataset with known α and verify estimation accuracy.
  2. Run PULSNAR on a small SNAR synthetic dataset, check if clustering produces expected subtypes.
  3. Compare PULSNAR vs. DEDPUL on a medium SNAR dataset to confirm scalability and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is PULSNAR to overestimating the number of clusters when dividing labeled positives into subtypes?
- Basis in paper: [explicit] The paper mentions "Preliminary work (not shown) suggests PULSNARα estimation is robust to overestimating the number of clusters."
- Why unresolved: The paper states that preliminary work suggests robustness but does not provide the actual results or methodology of this work.
- What evidence would resolve it: Experimental results showing PULSNAR performance with different numbers of clusters, including cases where the number of clusters is deliberately overestimated, would resolve this question.

### Open Question 2
- Question: How does the performance of PULSNAR change when applied to datasets with different levels of class separability (class sep parameter)?
- Basis in paper: [inferred] The paper notes that "For SNAR data, with true α = 1%, when we increased class sep from 0.3 to 0.5 the PULSNARα estimate improved from 1.6% (Figure 3) to 0.98% (data not shown)."
- Why unresolved: The paper only provides a single comparison between two class sep values for one specific case, and does not explore the full range of class separability.
- What evidence would resolve it: A comprehensive study of PULSNAR performance across a wide range of class sep values would provide insights into its behavior with varying class separability.

### Open Question 3
- Question: How does PULSNAR's performance compare to other state-of-the-art PU learning methods when applied to very large datasets with high class imbalance?
- Basis in paper: [explicit] The paper states that "KM1, KM2, and TICE algorithms were not scalable and failed to execute on large datasets, so we used smaller synthetic datasets to compare our method with these methods."
- Why unresolved: The paper does not provide a direct comparison of PULSNAR with other methods on large datasets due to scalability issues with the other methods.
- What evidence would resolve it: A comparison of PULSNAR with other methods on very large datasets, possibly using distributed computing or other scalability techniques, would provide insights into its relative performance in challenging scenarios.

## Limitations
- Dependence on clustering effectiveness to handle SNAR scenarios may fail if bias depends on continuous or high-dimensional factors
- No theoretical guarantees for α estimation accuracy under various SNAR mechanisms
- Performance degrades if clustering produces too few or too many clusters

## Confidence
- SCAR scenarios: High confidence (PULSCAR performs as expected)
- SNAR scenarios: Medium confidence (clustering effectiveness varies)
- Classification improvements: Medium confidence (depends on accurate α estimation)

## Next Checks
1. Test PULSNAR on SNAR datasets where selection probability varies continuously with a feature (not captured by clustering) to evaluate breakdown conditions
2. Compare clustering stability and α estimation consistency across multiple random seeds on the same SNAR dataset
3. Evaluate PULSNAR's performance when applied to subsets of features that are not informative for clustering but are relevant for classification