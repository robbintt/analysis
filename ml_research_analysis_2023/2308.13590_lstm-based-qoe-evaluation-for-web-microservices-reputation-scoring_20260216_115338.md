---
ver: rpa2
title: LSTM-based QoE Evaluation for Web Microservices' Reputation Scoring
arxiv_id: '2308.13590'
source_url: https://arxiv.org/abs/2308.13590
tags:
- reputation
- reviews
- microservices
- sentiment
- services
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes an LSTM-based approach to classify web microservices'
  user reviews and assess their reputation. The method employs LSTM for sentiment
  classification and NBR algorithm for reputation scoring.
---

# LSTM-based QoE Evaluation for Web Microservices' Reputation Scoring

## Quick Facts
- arXiv ID: 2308.13590
- Source URL: https://arxiv.org/abs/2308.13590
- Reference count: 15
- Primary result: 93% accuracy and precision achieved on sentiment classification of microservice reviews

## Executive Summary
This study proposes an LSTM-based approach for evaluating web microservices' reputation through sentiment analysis of user reviews. The method combines LSTM for sentiment classification with the NBR algorithm for reputation scoring, achieving 93% accuracy and precision on a dataset of 10,000+ Amazon Web Services microservice reviews. The approach demonstrates significant improvements over traditional deep learning architectures and provides a reliable framework for automated reputation assessment in microservice ecosystems.

## Method Summary
The method employs LSTM neural networks for binary sentiment classification (positive/negative) of microservice reviews, addressing the challenge of highly imbalanced datasets through oversampling strategies. Word embeddings are generated using GloVe, and the NBR (Net Promoter-like Reputation) algorithm calculates reputation scores based on the ratio of positive to negative reviews. The entire pipeline includes text preprocessing, model training with Adam optimizer and cross-entropy loss, and reputation score calculation that compares favorably with ground truth values.

## Key Results
- Achieved 93% accuracy and precision after oversampling on highly imbalanced dataset
- LSTM outperformed RNN, GRU, and CNN architectures in classification tasks
- NBR score of 89.58% closely matched actual reputation score of 90.25%
- 15 Amazon Web Services microservices analyzed with 10,676 total reviews

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LSTM model outperforms other deep learning architectures (RNN, GRU, CNN) in classifying microservice reviews
- Mechanism: LSTM's gating mechanisms (input, forget, output gates) enable selective memory retention, allowing it to capture long-term dependencies in sequential text data that other models struggle with
- Core assumption: Microservice reviews contain meaningful long-range dependencies that affect sentiment classification
- Evidence anchors:
  - [abstract]: "The LSTM model outperformed other deep learning architectures (RNN, GRU, CNN) in classification tasks"
  - [section]: "Compared to traditional RNNs, LSTM can selectively forget or remember previous inputs and outputs, allowing it to capture more complex patterns in sequential data"
- Break condition: If microservice reviews are extremely short or context-independent, the advantage of LSTM's long-term dependency handling would diminish

### Mechanism 2
- Claim: Oversampling strategy improves classification accuracy from 91% to 93% on imbalanced dataset
- Mechanism: By balancing the positive and negative review samples during training, the model learns to better recognize minority class patterns, reducing bias toward the majority class
- Core assumption: The minority class (negative reviews) contains meaningful patterns that the model can learn to identify
- Evidence anchors:
  - [section]: "Only 521 negative reviews are included in the whole dataset... To address the imbalance problem, various resampling strategies were tested"
  - [section]: "The results confirmed the oversampling strategy's effectiveness since it provided considerable improvements in performance compared to testing results without a resampling strategy"
- Break condition: If negative reviews are truly random noise rather than meaningful patterns, oversampling would amplify noise rather than improve classification

### Mechanism 3
- Claim: NBR algorithm combined with LSTM sentiment classification produces reputation scores (89.58%) closely matching actual reputation scores (90.25%)
- Mechanism: The NBR formula weights positive feedback more heavily than negative feedback, and when fed accurate LSTM sentiment predictions, produces reputation scores that align with ground truth
- Core assumption: The LSTM sentiment classification is sufficiently accurate that its output can be trusted as input to the NBR calculation
- Evidence anchors:
  - [section]: "Comparing the NBR score generated using LSTM-based techniques for reputation assessment with the score obtained from the original dataset revealed close similarity between the two values"
  - [section]: "These results imply that the LSTM-based approach can be a reliable and effective technique for assessing the reputation of microservices providers"
- Break condition: If LSTM sentiment classification accuracy drops below a threshold (likely around 85-90%), the NBR scores would diverge significantly from actual reputation scores

## Foundational Learning

- Concept: Sentiment analysis and polarity classification
  - Why needed here: The entire approach depends on accurately classifying reviews as positive or negative before reputation scoring can occur
  - Quick check question: How does the model handle neutral reviews, and what happens to them in the NBR calculation?

- Concept: Imbalanced dataset handling techniques
  - Why needed here: The dataset contains 95% positive and 5% negative reviews, making standard training approaches ineffective
  - Quick check question: Why did oversampling work better than SMOTE or ADASYN in this specific case?

- Concept: Long Short-Term Memory (LSTM) architecture and gating mechanisms
  - Why needed here: Understanding how LSTM selectively remembers and forgets information is crucial to grasping why it outperforms other architectures
  - Quick check question: What specific problems with RNNs (vanishing gradients, short-term memory) does LSTM solve in this application?

## Architecture Onboarding

- Component map: Data collection → Preprocessing pipeline (tokenization, stemming, POS tagging) → Word embeddings (GloVe) → LSTM classification → NBR reputation scoring
- Key components: Text preprocessing module, embedding generation layer, LSTM model with configurable parameters, NBR calculation engine

- Critical path:
  1. Review collection and validation
  2. Text preprocessing and feature extraction
  3. LSTM model training and validation
  4. Sentiment classification on test set
  5. NBR reputation score calculation

- Design tradeoffs:
  - LSTM vs. Transformer models: LSTM chosen for proven effectiveness and lower computational requirements
  - Oversampling vs. class weighting: Oversampling selected to address severe imbalance while maintaining training stability
  - Fixed sequence length (50) vs. variable length: Fixed length chosen for computational efficiency despite potential information loss

- Failure signatures:
  - Low precision but high recall: Model over-predicts positive sentiment
  - High training accuracy but low validation accuracy: Overfitting to training data
  - NBR scores consistently deviating from ground truth: LSTM classification accuracy insufficient

- First 3 experiments:
  1. Train LSTM with default parameters on unbalanced data to establish baseline performance
  2. Apply oversampling and retrain to measure improvement in minority class detection
  3. Compare LSTM performance against GRU and CNN using identical preprocessing and evaluation metrics

## Open Questions the Paper Calls Out

- Question: How does the proposed LSTM-based approach perform on microservices with more balanced review distributions (i.e., closer to 50/50 positive/negative ratio) compared to the highly imbalanced dataset used in the study?
  - Basis in paper: [explicit] The paper mentions that the dataset was highly imbalanced with 95% positive reviews and only 5% negative reviews, and that oversampling was applied to address this issue.
  - Why unresolved: The study only tested the approach on one specific imbalanced dataset. Performance on balanced or differently imbalanced datasets is unknown.
  - What evidence would resolve it: Testing the LSTM approach on multiple datasets with varying degrees of class imbalance and comparing accuracy/precision/recall/F1 scores across these datasets.

- Question: How would the reputation scores change if the LSTM model incorporated aspect-based sentiment analysis rather than just binary positive/negative classification?
  - Basis in paper: [explicit] The paper mentions aspect-level SA as one of three classification levels, but only implements sentence-level binary classification.
  - Why unresolved: The study only performs binary sentiment classification without considering specific aspects of the microservices being reviewed.
  - What evidence would resolve it: Implementing aspect-based sentiment analysis with the LSTM model and comparing the resulting reputation scores to those obtained from binary classification.

- Question: What is the impact of incorporating temporal dynamics into the reputation scoring model, considering that microservice quality and user perceptions may change over time?
  - Basis in paper: [inferred] The current NBR formula treats all reviews equally regardless of when they were posted, which may not reflect evolving microservice quality.
  - Why unresolved: The study uses a static NBR formula that doesn't account for temporal patterns in reviews or changing microservice performance over time.
  - What evidence would resolve it: Implementing a time-weighted reputation scoring model and comparing reputation scores and their predictive validity over different time periods.

## Limitations
- Reliance on Amazon Web Services data introduces platform-specific biases
- 95:5 positive-to-negative review ratio suggests potential selection bias
- Two-stage labeling approach lacks detailed description, raising concerns about label quality
- NBR algorithm implementation details are not fully specified

## Confidence
- **High Confidence**: LSTM outperforming other deep learning architectures, oversampling strategy effectiveness, overall approach feasibility
- **Medium Confidence**: Specific accuracy improvement from 91% to 93%, NBR score alignment with ground truth (limited to single dataset validation)

## Next Checks
1. **Cross-platform validation**: Test the approach on microservice reviews from different platforms (Google Cloud, Azure) to assess generalizability and identify platform-specific biases.

2. **Ablation study**: Systematically remove individual components (oversampling, LSTM architecture, NBR calculation) to quantify each component's contribution to final performance.

3. **Neutral review handling**: Implement and test strategies for processing neutral reviews, examining how different handling approaches affect final reputation scores and classification metrics.