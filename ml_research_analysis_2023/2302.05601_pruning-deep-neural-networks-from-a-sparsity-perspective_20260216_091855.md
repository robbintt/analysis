---
ver: rpa2
title: Pruning Deep Neural Networks from a Sparsity Perspective
arxiv_id: '2302.05601'
source_url: https://arxiv.org/abs/2302.05601
tags:
- pruning
- sparsity
- index
- pruned
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new sparsity measure called PQ Index (PQI)
  for deep neural networks, designed to quantify the compressibility of a model during
  pruning. The PQI is a function of two norms (p and q) of the model parameters and
  satisfies six properties that an ideal sparsity measure should have.
---

# Pruning Deep Neural Networks from a Sparsity Perspective

## Quick Facts
- arXiv ID: 2302.05601
- Source URL: https://arxiv.org/abs/2302.05601
- Reference count: 33
- Introduces PQ Index (PQI) - a new sparsity measure for deep neural networks that quantifies compressibility during pruning

## Executive Summary
This paper proposes a new sparsity measure called PQ Index (PQI) for deep neural networks, designed to quantify the compressibility of a model during pruning. The PQI is a function of two norms (p and q) of the model parameters and satisfies six properties that an ideal sparsity measure should have. Based on the PQI, the authors propose a Sparsity-informed Adaptive Pruning (SAP) algorithm that adaptively determines the pruning ratio at each iteration. The SAP algorithm is shown to outperform iterative pruning methods such as the lottery ticket-based pruning methods in terms of both compression efficiency and robustness.

## Method Summary
The paper introduces PQ Index (PQI) as a new sparsity measure that quantifies the compressibility of deep neural networks during pruning. PQI is defined as the ratio of ℓp-norm to ℓq-norm of model parameters (where p<q). The authors prove PQI satisfies six properties of an ideal sparsity measure including scale invariance, Robin Hood property, and targeting property. Based on PQI, they propose the SAP (Sparsity-informed Adaptive Pruning) algorithm that adaptively determines pruning ratios at each iteration by computing a PQI-bound on retained parameters. The method is evaluated on multiple datasets (FashionMNIST, CIFAR10, CIFAR100, TinyImageNet) and architectures (Linear, MLP, CNN, ResNet18/50, WResNet28x8).

## Key Results
- SAP with neuron-wise pruning achieves better performance than both one-shot and lottery ticket pruning methods
- PQI shows non-monotonic behavior during pruning: decreases (regularization), increases (compressibility limit), decreases again (collapse)
- SAP adapts pruning ratios effectively, achieving similar performance to lottery ticket methods in fewer iterations (T=10 vs T=25)

## Why This Works (Mechanism)

### Mechanism 1
PQI (p,q)-norm ratio detects model compressibility better than Gini index because it captures scale-invariant sparsity across different model sizes. The ratio of two different norms (p<q) creates a measure sensitive to the distribution of parameter magnitudes. As pruning removes small weights, the ℓp-norm decreases faster than ℓq-norm, causing PQI to first decrease (regularization phase) then increase (compressibility limit) then decrease again (collapse).

### Mechanism 2
SAP algorithm achieves better compression efficiency by adaptively adjusting pruning ratio based on PQI-bound rather than fixed ratios. SAP computes lower bound rt on retained parameters using PQI and ηr, then applies scaling factor γ and max pruning ratio β to determine actual pruning. This prevents both under-pruning (insufficient compression) and over-pruning (performance collapse).

### Mechanism 3
Neuron-wise pruning scope works better with SAP than global pruning because it computes PQI per neuron rather than across all parameters. Neuron-wise pruning computes separate PQI for each neuron's parameters, preventing scale mismatches between layers. Global pruning suffers when initial layers have larger magnitude parameters that dominate the combined PQI.

## Foundational Learning

- **Concept**: Vector norms and Hölder's inequality
  - Why needed here: PQI relies on ratio of ℓp and ℓq norms, requiring understanding of norm properties and relationships
  - Quick check question: What happens to ∥w∥p/∥w∥q ratio as vector becomes sparser when p<q?

- **Concept**: Iterative pruning algorithms (lottery ticket hypothesis)
  - Why needed here: SAP builds on lottery ticket framework, so understanding iterative prune-retrain cycles is essential
  - Quick check question: How does lottery ticket pruning differ from one-shot pruning in terms of retraining?

- **Concept**: Sparsity measures and their properties
  - Why needed here: PQI must satisfy six properties (D1-D4, P1-P2) to be a valid sparsity measure, requiring knowledge of existing measures like Gini index
  - Quick check question: Which property ensures sparsity measure is scale-invariant?

## Architecture Onboarding

- **Component map**: winit → mt → train(˜wt, mt) → wt → PQI(wt) → rt → ct → mt+1 → wT
- **Critical path**: winit → mt → train(˜wt, mt) → wt → PQI(wt) → rt → ct → mt+1 → wT
  - Bottleneck: Training step (E epochs per iteration)
  - Data dependency: PQI computation requires complete wt
- **Design tradeoffs**:
  - p,q selection: Higher p,q difference → more aggressive pruning but less stable PQI
  - ηr vs γ: Higher ηr allows more compression but risks underfitting; γ accelerates pruning but may overshoot
  - Scope choice: Neuron-wise (stable PQI, layer protection) vs Global (aggressive, scale-sensitive)
- **Failure signatures**:
  - PQI increasing too early → model compressible but SAP prunes too aggressively
  - PQI plateauing → model incompressible but SAP continues pruning
  - Performance drop before PQI increase → ηr overestimated
- **First 3 experiments**:
  1. MLP on FashionMNIST with p=0.5,q=1.0, ηr=0, γ=1.0 - verify PQI correlates with compressibility
  2. ResNet18 on CIFAR10 with neuron-wise pruning - test layer-wise PQI stability
  3. Wide ResNet28x8 on CIFAR100 with varying γ - tune pruning acceleration factor

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal choice of p and q in the PQ Index for different model architectures and pruning tasks? The paper demonstrates that different combinations of p and q can lead to varying levels of pruning aggressiveness, but it does not provide a clear guideline for selecting the optimal values.

### Open Question 2
How does the dynamic relationship between sparsity and compressibility evolve throughout the pruning process? The paper hypothesizes a relationship between sparsity and compressibility but does not provide a detailed analysis of how this relationship changes during pruning.

### Open Question 3
How does the PQ Index perform compared to other sparsity measures, such as Gini Index, in terms of guiding the pruning process? The paper compares the PQ Index with the Gini Index and shows that the PQ Index can effectively measure sparsity, but it does not provide a direct comparison of their performance in guiding the pruning process.

## Limitations

- Limited empirical validation against a broader range of sparsity measures beyond Gini index
- No clear guidance on how to estimate the critical ηr hyperparameter for different architectures
- The underlying reasons for neuron-wise vs global pruning performance differences aren't fully explored

## Confidence

- **PQI as superior sparsity measure**: Medium - Theoretical properties are sound, but limited empirical comparison with state-of-the-art measures
- **SAP algorithm effectiveness**: High - Strong experimental results across multiple datasets and architectures
- **Non-monotonic PQI behavior correlation**: Medium - Observed pattern documented but causation not rigorously established

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary ηr, γ, and β across different model families to establish robust hyperparameter ranges
2. **Extended sparsity measure comparison**: Benchmark PQI against recent sparsity measures (e.g., synaptic saliency, layer-wise importance) on identical pruning tasks
3. **Transferability test**: Apply SAP-trained sparse models to downstream tasks to verify compression benefits extend beyond original training domain