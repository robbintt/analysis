---
ver: rpa2
title: User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue
arxiv_id: '2309.13233'
source_url: https://arxiv.org/abs/2309.13233
tags:
- user
- system
- simulator
- systems
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a user simulator for task-oriented dialogue
  systems built using in-context learning with large language models (LLMs) rather
  than fine-tuning on domain-specific data. The simulator aims to generate linguistically
  diverse, human-like utterances for testing TOD systems across various domains.
---

# User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue

## Quick Facts
- arXiv ID: 2309.13233
- Source URL: https://arxiv.org/abs/2309.13233
- Authors: 
- Reference count: 7
- Single-intent GSR: 25% on MultiWOZ

## Executive Summary
This paper introduces a user simulator for task-oriented dialogue systems built using in-context learning with large language models (LLMs) rather than fine-tuning on domain-specific data. The simulator aims to generate linguistically diverse, human-like utterances for testing TOD systems across various domains. The system achieves competitive performance on single-intent goals, with a Goal Success Rate (GSR) of 25% on MultiWOZ, and demonstrates near-human level lexical and syntactic diversity. Human evaluation found 73% of generated utterances to be reasonable given dialogue context.

## Method Summary
The approach uses in-context learning with LLMs to generate user-side utterances without fine-tuning on domain-specific data. A context accumulator stores dialogue history and current goal, while a simulator module formats prompts with example dialogues and queries an LLM to generate user utterances. The system primarily uses GPTNeoX-20B through Huggingface Transformers, though other models are also evaluated. The simulator interacts with target TOD systems (Google Assistant in experiments) until an end-of-dialogue token is generated. Performance is evaluated using GSR, lexical diversity (MTLD), and syntactic diversity (dependency distance metrics) against human-level baselines.

## Key Results
- Achieves 25% GSR on MultiWOZ, compared to 64% for human interactions
- Demonstrates near-human MTLD scores (58.23 vs 57.99) and dependency distance metrics
- 73% of generated utterances judged reasonable by human evaluators
- Shows competitive performance on single-intent goals with better domain adaptability than fine-tuned alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning with LLMs can generate linguistically diverse user utterances without domain-specific fine-tuning.
- Mechanism: The user simulator leverages a small number of example dialogues as in-context prompts to guide LLM generation, allowing it to produce varied responses aligned with conversational goals.
- Core assumption: LLMs pretrained on broad web text retain sufficient knowledge of task-oriented dialogue patterns to generate appropriate utterances when given structured examples.
- Evidence anchors:
  - [abstract] "we use in-context learning to prompt the LLMs to generate robust and linguistically diverse output"
  - [section 2] "recent work, such as Kojima et al. (2022) and Wei et al. (2022) to build a user simulator which is able to effectively engage with extant TOD systems"
  - [corpus] Weak - no direct corpus comparison for in-context learning effectiveness
- Break condition: If the LLM lacks sufficient dialogue pattern knowledge, generated utterances may become incoherent or off-topic despite in-context prompts.

### Mechanism 2
- Claim: Goal Success Rate (GSR) is not the optimal metric for user simulator evaluation; human-level GSR is more appropriate.
- Mechanism: The system is evaluated against human performance on the same TOD systems rather than maximizing GSR, ensuring more realistic simulation of user behavior.
- Core assumption: Real users do not always successfully complete tasks when interacting with TOD systems, so a perfect GSR would indicate artificial behavior.
- Evidence anchors:
  - [abstract] "our goal is a system which achieves a GSR similar to that observed in human interactions with TOD systems"
  - [section 3.1] "we argue that GSR must be considered relative to the goal success seen in human interaction"
  - [section 7] "our proposed system is promising as a platform for testing in-development TOD systems"
- Break condition: If human data is not representative of real-world interactions, using human-level GSR as a target could lead to underperformance in practical scenarios.

### Mechanism 3
- Claim: Lexical and syntactic diversity metrics (MTLD, dependency distance) provide better evaluation of simulator output than traditional measures.
- Mechanism: The system uses MTLD to measure lexical diversity and dependency distance metrics to assess syntactic complexity, capturing linguistic richness that simple n-gram measures miss.
- Core assumption: Traditional diversity measures like type-token ratio are inadequate for short dialogues, while MTLD and dependency metrics better capture linguistic complexity.
- Evidence anchors:
  - [section 3.2] "we adopt a set of metrics frequently used in the language acquisition and linguistic analysis research"
  - [section 5] "we use these collected dialogues to calculate the human-level GSR, lexical diversity, and syntactic diversity"
  - [corpus] Weak - no direct corpus comparison for diversity metric effectiveness
- Break condition: If the metrics are not sensitive enough to detect meaningful differences in linguistic diversity, they may fail to distinguish between truly diverse and superficially varied output.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The approach relies on providing examples within prompts rather than fine-tuning, requiring understanding of how LLMs use context to generate responses.
  - Quick check question: How does the number and quality of example dialogues in a prompt affect LLM output generation?

- Concept: Task-oriented dialogue evaluation metrics
  - Why needed here: The system uses GSR, BLEU, and custom diversity metrics, requiring knowledge of how these measures capture different aspects of dialogue quality.
  - Quick check question: What are the limitations of using only GSR to evaluate user simulator performance?

- Concept: Lexical and syntactic diversity measurement
  - Why needed here: The system employs MTLD and dependency distance metrics, requiring understanding of how these measures capture linguistic complexity.
  - Quick check question: Why might traditional type-token ratio be inadequate for measuring diversity in short dialogue utterances?

## Architecture Onboarding

- Component map:
  Context accumulator -> Simulator module -> LLM backend -> TOD system interface

- Critical path:
  1. Load dialogue history and goal into context accumulator
  2. Simulator module formats prompt with examples and current goal
  3. LLM generates user utterance based on prompt
  4. Utterance sent to TOD system for response
  5. TOD response and user utterance added to context accumulator
  6. Repeat until end-of-dialogue token

- Design tradeoffs:
  - Model choice: GPTNeoX-20B offers better performance but requires more resources than smaller models
  - Prompt design: More examples improve grounding but are limited by token constraints
  - Temperature setting: Higher values increase diversity but may reduce coherence

- Failure signatures:
  - Premature dialogue termination: User simulator generates end token before completing goals
  - Conversational loops: Both systems repeatedly respond with previous turns
  - LLM hallucination: Simulator attempts to achieve goals not in the grounding

- First 3 experiments:
  1. Test different temperature settings (0.3, 0.5, 0.7) to balance diversity and coherence
  2. Compare performance using different numbers of example dialogues in prompts (1, 2, 3)
  3. Evaluate impact of goal formatting (logical, natural language, parsed logical) on GSR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of in-context learning-based user simulators compare to fine-tuned models when evaluated on new, unseen domains?
- Basis in paper: [explicit] The paper discusses the limitations of fine-tuned models for new domains and the potential advantages of in-context learning, but does not provide direct comparative evaluation on truly unseen domains.
- Why unresolved: The paper primarily evaluates on MultiWOZ data and collected human interactions with existing TOD systems, rather than testing the simulator's ability to generalize to entirely new domains.
- What evidence would resolve it: Direct comparison of in-context learning and fine-tuned models on tasks from domains not represented in their training data, measuring GSR, diversity metrics, and human evaluation scores.

### Open Question 2
- Question: What is the optimal number of in-context examples to provide for achieving the best balance between performance and computational efficiency?
- Basis in paper: [explicit] The paper mentions being constrained by token input limits and using two examples, but acknowledges that additional examples may improve performance.
- Why unresolved: The paper does not systematically explore the relationship between the number of in-context examples and simulator performance, nor does it address the computational cost implications.
- What evidence would resolve it: Systematic experiments varying the number of in-context examples (e.g., 1, 2, 4, 8) while measuring GSR, diversity metrics, and generation time to identify an optimal point.

### Open Question 3
- Question: How do different prompting strategies (e.g., chain-of-thought, goal repetition) affect the performance of in-context learning-based user simulators?
- Basis in paper: [explicit] The paper suggests potential improvements like chain-of-thought prompting and goal repetition but does not implement or evaluate them.
- Why unresolved: The paper only uses a basic in-context learning setup and does not explore more sophisticated prompting techniques that could enhance performance.
- What evidence would resolve it: Comparative experiments testing various prompting strategies (chain-of-thought, goal repetition, belief state inclusion) on the same tasks, measuring GSR, diversity metrics, and qualitative analysis of generated utterances.

## Limitations
- Achieves only 25% GSR on MultiWOZ compared to 64% for humans, indicating substantial performance gaps
- Performance degrades substantially on multi-intent goals, suggesting limitations with complex real-world scenarios
- Relies on a single dataset (MultiWOZ 2.2) and evaluates against only one commercial TOD system (Google Assistant)

## Confidence
**High Confidence**: The demonstration that in-context learning with LLMs can generate linguistically diverse utterances without fine-tuning is well-supported by empirical results showing near-human MTLD scores (58.23 vs 57.99) and dependency distance metrics.

**Medium Confidence**: The claim that GSR should be evaluated relative to human performance rather than maximized is logically sound but relies on the assumption that human data represents optimal interaction patterns.

**Low Confidence**: The assertion that lexical and syntactic diversity metrics provide superior evaluation compared to traditional measures lacks strong empirical validation.

## Next Checks
1. **Cross-domain validation**: Test the simulator on non-tourism domains (restaurant booking, scheduling, customer service) to verify domain adaptability claims and identify any domain-specific limitations.

2. **Multi-intent goal evaluation**: Design controlled experiments with increasing complexity of multi-intent goals to quantify the performance degradation observed and identify specific failure patterns in complex scenarios.

3. **Human evaluation expansion**: Conduct larger-scale human evaluations comparing simulator output with human-generated dialogues across multiple interaction dimensions (relevance, coherence, goal alignment) to validate the claimed near-human linguistic diversity.