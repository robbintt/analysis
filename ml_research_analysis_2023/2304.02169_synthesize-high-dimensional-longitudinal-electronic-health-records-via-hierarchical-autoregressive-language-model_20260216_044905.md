---
ver: rpa2
title: Synthesize High-dimensional Longitudinal Electronic Health Records via Hierarchical
  Autoregressive Language Model
arxiv_id: '2304.02169'
source_url: https://arxiv.org/abs/2304.02169
tags:
- data
- halo
- synthetic
- dataset
- visit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HALO, a hierarchical autoregressive language
  model that generates realistic synthetic electronic health records (EHRs) in their
  original high-dimensional form without aggregation or variable selection. The method
  learns a probability distribution over medical codes, visits, and patient records
  using a multi-granularity approach that models both visit-level and code-level dependencies.
---

# Synthesize High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model

## Quick Facts
- arXiv ID: 2304.02169
- Source URL: https://arxiv.org/abs/2304.02169
- Reference count: 40
- Primary result: Hierarchical autoregressive model generates realistic synthetic EHRs with R2 > 0.9 correlations and enables ML models to achieve comparable accuracy to real data

## Executive Summary
This paper presents HALO, a hierarchical autoregressive language model for generating high-dimensional longitudinal synthetic electronic health records without aggregation or variable selection. The method learns a probability distribution over medical codes, visits, and patient records using a multi-granularity approach that models both visit-level and code-level dependencies. Experiments on outpatient and inpatient datasets demonstrate HALO outperforms baseline methods in capturing statistical properties of real EHRs while preserving patient privacy.

## Method Summary
HALO uses a hierarchical autoregressive framework that generates patient records through two modules: a visit-level transformer decoder that captures dependencies across visits, and a code-level module that models within-visit medical code generation. The model represents each visit as a multi-hot binary vector encoding medical codes, processes them through masked transformer layers to generate visit history embeddings, then conditions code predictions on both this history and previously generated codes. The approach enables efficient learning over extremely high-dimensional medical code spaces while maintaining autoregressive properties.

## Key Results
- R2 correlation above 0.9 for code probabilities across different granularities (d > 10,000 for disease codes, d > 1,000,000 for within-visit co-occurrences, d > 5,000,000 for across-visit dependencies)
- Synthetic data enables ML models to achieve comparable accuracy to real data (0.938 vs 0.943 AUROC)
- Privacy evaluations confirm HALO preserves patient privacy while generating realistic records

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical decomposition into visit-level and code-level autoregressive modeling enables efficient learning over extremely high-dimensional medical code spaces. The model first compresses patient history into a fixed-length visit embedding sequence (coarse granularity), then conditions code predictions on both this history and previously generated codes in the current visit (fine granularity). This avoids quadratic scaling in code interactions while preserving full autoregressive properties.

### Mechanism 2
Multi-hot encoding of medical codes enables direct modeling of code presence/absence probabilities without aggregation. Each visit is represented as a binary vector where each dimension indicates the presence (1) or absence (0) of a specific medical code. This allows the model to learn exact code occurrence probabilities and co-occurrence patterns.

### Mechanism 3
Transformer decoder blocks with masking enable parallel training while preserving autoregressive dependencies. The visit-level module uses masked self-attention to prevent information flow from future visits during training, while the code-level module uses masked linear layers to maintain autoregressive property for code generation.

## Foundational Learning

- **Autoregressive modeling and probability factorization**: The model needs to learn the joint probability distribution of sequential medical visits and codes, which requires decomposing it into conditional probabilities. *Quick check*: Can you explain why P(R) = P(v_s) * P(v_l|v_s) * P(v_1|v_s,v_l) * ... * P(v_e|v_s,v_l,...,v_T) allows efficient learning compared to modeling P(R) directly?

- **Transformer architecture and self-attention**: The model uses transformer decoder blocks to capture complex dependencies between visits while maintaining the autoregressive property through masking. *Quick check*: How does masked self-attention in transformer decoders prevent the model from "seeing" future visits during training?

- **Multi-hot encoding and binary classification**: The model represents medical codes as binary vectors and treats code presence prediction as multi-label classification, requiring understanding of how to handle sparse high-dimensional binary data. *Quick check*: What are the advantages and disadvantages of using multi-hot encoding versus one-hot encoding for this application?

## Architecture Onboarding

- **Component map**: Multi-hot patient record matrix R (T+3) × |C| -> 12 transformer decoder blocks -> visit history embeddings H^(M) ∈ R(T+3)×n_emb -> Masked linear layers + sigmoid -> code probability matrix O ∈ R(T+2)×|C|

- **Critical path**: 1. Convert multi-hot visits to embeddings (code + positional) 2. Generate visit history embeddings through transformer decoder stack 3. Combine history embeddings with current visit codes 4. Generate code probabilities through masked linear layers 5. Apply sigmoid to obtain final probabilities

- **Design tradeoffs**: Hierarchical vs flat autoregressive (hierarchical reduces computation but may miss some cross-visit code dependencies), Transformer vs LSTM (transformers allow parallel training and better long-range dependencies, but require more memory), Multi-hot vs one-hot (multi-hot preserves exact code presence information but increases dimensionality)

- **Failure signatures**: Poor performance on visit-level tasks but good on code-level (transformer decoder blocks not capturing visit dependencies), Good performance on short records but poor on long records (masking not properly preventing information leakage), Generated records with unrealistic code combinations (code-level module not properly conditioning on visit history)

- **First 3 experiments**: 1. Train HALO − Coarse (visit-level only) and compare to LSTM baseline on outpatient dataset to verify transformer effectiveness 2. Add code-level module and evaluate on inpatient dataset to verify multi-granularity benefits 3. Test privacy evaluation (membership inference attack) on both datasets to verify no memorization occurred

## Open Questions the Paper Calls Out
- The paper mentions this as a limitation in the conclusion: "However, other crucial data modalities, such as clinical notes and medical images, are not yet covered by the model." The current HALO architecture is designed specifically for structured, multi-hot encoded medical codes and continuous variables. Clinical notes require natural language processing and medical images need computer vision approaches, which are fundamentally different data types.

## Limitations
- The hierarchical decomposition's effectiveness for capturing extremely long-range dependencies across visits remains uncertain given the autoregressive constraint, with limited ablation studies on information loss.
- Privacy evaluation focuses on membership inference attacks but doesn't address potential vulnerabilities to other attack types (e.g., attribute inference, linkage attacks).
- Discretization of continuous variables using fixed bucket ranges may introduce artifacts that affect model performance, but sensitivity to different bucketing strategies is not explored.

## Confidence
- **High Confidence**: The hierarchical autoregressive framework design and the overall approach of generating high-dimensional EHRs without aggregation. The paper clearly articulates the method and demonstrates strong performance on standard metrics (R² > 0.9, AUROC ~0.94).
- **Medium Confidence**: The claim that HALO preserves patient privacy while generating realistic records. While membership inference attacks show no memorization, the evaluation scope is limited and doesn't address all potential privacy threats.
- **Low Confidence**: The specific mechanisms by which the hierarchical decomposition enables efficient learning over extremely high-dimensional code spaces. The paper asserts this benefit but provides limited empirical evidence about the information loss from compression.

## Next Checks
1. **Ablation on Hierarchical Decomposition**: Train HALO without the coarse granularity (visit-level module only) and compare performance on long patient histories (>50 visits) to quantify information loss from the hierarchical approach.
2. **Extended Privacy Evaluation**: Conduct attribute inference and linkage attacks on the synthetic data to comprehensively assess privacy preservation beyond membership inference.
3. **Sensitivity to Discretization**: Vary the bucket ranges for continuous variable discretization and measure the impact on model performance and synthetic data quality to understand the robustness to this preprocessing choice.