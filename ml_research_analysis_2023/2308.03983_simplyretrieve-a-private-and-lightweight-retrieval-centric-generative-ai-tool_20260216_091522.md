---
ver: rpa2
title: 'SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI
  Tool'
arxiv_id: '2308.03983'
source_url: https://arxiv.org/abs/2308.03983
tags:
- knowledge
- tool
- retrieval-centric
- simplyretrieve
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SimplyRetrieve is a tool designed to facilitate Retrieval-Centric
  Generation (RCG) in large language models (LLMs) by integrating a knowledge retrieval
  architecture. It allows seamless integration of private data into publicly available
  generative AI systems without requiring additional model fine-tuning.
---

# SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool

## Quick Facts
- arXiv ID: 2308.03983
- Source URL: https://arxiv.org/abs/2308.03983
- Reference count: 6
- Key outcome: SimplyRetrieve achieves Rouge-L score of 0.413 and average response time of 11.67 seconds per query using a 13B parameter LLM for retrieval-centric generation.

## Executive Summary
SimplyRetrieve is an open-source tool designed to integrate private data into publicly available generative AI systems without requiring model fine-tuning. It implements Retrieval-Centric Generation (RCG), which explicitly separates the roles of large language models (LLMs) and retrievers for context interpretation and knowledge memorization respectively. The tool features a GUI and API-based platform with a Private Knowledge Base Constructor and Retrieval Tuning Module, allowing users to explore RCG's potential for improving generative AI performance while maintaining privacy standards.

## Method Summary
SimplyRetrieve uses a knowledge retrieval architecture to integrate private data into LLMs without fine-tuning. The system employs a dense passage retriever with ANNS indexing (HNSW algorithm) to efficiently search knowledge bases, then constructs prompts that explicitly instruct the LLM to use retrieved knowledge for response generation. The tool includes Explicit Prompt-Weighting (EPW) to control the balance between retriever and LLM contributions, and evaluates performance using Rouge-L scores and response times across RCG, RAG, and ROG approaches.

## Key Results
- RCG achieves Rouge-L score of 0.413 with average response time of 11.67 seconds per query
- Explicit Prompt-Weighting at 50% yields highest improvement in Rouge-L scores compared to baseline
- 13B parameter LLM demonstrates satisfactory performance in retrieval-centric generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating LLM context interpretation from knowledge memorization improves response accuracy and efficiency.
- Mechanism: The retriever handles knowledge memorization while the LLM focuses on context interpretation and response generation, reducing hallucinations and improving precision.
- Core assumption: LLMs are better at context interpretation than memorization of factual knowledge.
- Evidence anchors: [abstract] "explicitly separates roles of LLMs and retrievers in context interpretation and knowledge memorization"; [section] "RCG significantly improves the Rouge-L score compared to the baseline approach"
- Break condition: If the retriever cannot accurately retrieve relevant knowledge, the LLM will lack necessary information for context interpretation.

### Mechanism 2
- Claim: Explicit Prompt-Weighting allows fine-tuning the balance between retriever and LLM contributions to the response.
- Mechanism: By adjusting the percentage of retrieved knowledge tokens used in the prompt, users can control how much influence the retriever has on the LLM's response, optimizing for accuracy or creativity as needed.
- Core assumption: The optimal balance between retriever and LLM contributions varies depending on the task and knowledge base.
- Evidence anchors: [section] "Explicit Prompt-Weighting of retrieval to adjust the level of influence exerted by the retriever"; [section] "RCG-EPW, where only the first 50% of retrieved knowledge base are injected into the prompt, the model generated partial responses but still maintained factual accuracy"
- Break condition: If EPW is set too high, the LLM may become too constrained; if too low, it may rely too much on its own knowledge and hallucinate.

### Mechanism 3
- Claim: Using a smaller LLM for context interpretation in RCG reduces computational requirements while maintaining performance.
- Mechanism: By offloading knowledge memorization to the retriever, the LLM can be smaller (e.g., 13B parameters) while still achieving high accuracy on never-seen-before factual knowledge.
- Core assumption: Context interpretation requires less model capacity than knowledge memorization.
- Evidence anchors: [section] "This efficient performance may facilitate broader adoption within the community" and "our findings suggest that even a modestly sized LLM of 13B parameters can demonstrate satisfactory performance"; [section] "by separating context interpretation from knowledge memorization, this approach has the potential to reduce the scale of the LLM required for generative tasks"
- Break condition: If the knowledge base is too large or complex, a larger LLM may be needed for effective context interpretation.

## Foundational Learning

- Concept: Dense passage retrieval using ANNS (Approximate Nearest Neighbor Search)
  - Why needed here: To efficiently retrieve relevant knowledge from large knowledge bases without requiring exhaustive search.
  - Quick check question: What is the advantage of using HNSW (Hierarchical Navigable Small Worlds) algorithm for indexing knowledge bases in SimplyRetrieve?

- Concept: Prompt engineering for controlling LLM behavior
  - Why needed here: To guide the LLM to follow the retrieval-centric approach and use retrieved knowledge appropriately.
  - Quick check question: How does the "answer the following question with the provided knowledge" prompt suffix encourage retrieval-centric behavior in the LLM?

- Concept: Rouge-L score for evaluating text generation quality
  - Why needed here: To quantitatively measure the accuracy of generated responses compared to reference answers.
  - Quick check question: What does a higher Rouge-L score indicate about the quality of generated responses in SimplyRetrieve's evaluations?

## Architecture Onboarding

- Component map: User query -> Retriever (dense encoder + ANNS index) -> Retrieved knowledge -> LLM prompt construction -> LLM generation -> Response
- Critical path: User query → Retriever → Retrieved knowledge → LLM prompt construction → LLM generation → Response
- Design tradeoffs:
  - Smaller LLM reduces computational cost but may limit context interpretation ability
  - More sophisticated retrieval methods improve accuracy but increase latency
  - Granular control over prompts and weighting increases flexibility but adds complexity
- Failure signatures:
  - Poor retrieval results → LLM lacks necessary context for accurate response
  - Incorrect prompt weighting → LLM either ignores retriever or is too constrained
  - Large knowledge base → Slow retrieval or memory issues
- First 3 experiments:
  1. Verify basic functionality: Run a simple query with a small knowledge base and confirm retrieval and response generation.
  2. Test prompt variations: Modify the prompt suffixes and observe changes in LLM behavior (retrieval-centric vs. retrieval-augmented).
  3. Evaluate EPW impact: Experiment with different Explicit Prompt-Weighting values and measure changes in response accuracy and length.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between context interpretation and knowledge memorization in Retrieval-Centric Generation (RCG) for maximizing both performance and interpretability?
- Basis in paper: [inferred] The paper introduces RCG as a promising approach that emphasizes separating roles between LLMs and retrievers, potentially leading to more efficient implementation. The authors suggest that increased clarity in role-separation could boost performance and interpretability of generative AI systems.
- Why unresolved: The paper presents initial experiments and qualitative evaluations, but does not provide a comprehensive analysis of the optimal balance between context interpretation and knowledge memorization. Further research is needed to determine the ideal configuration for different use cases and knowledge bases.
- What evidence would resolve it: A systematic study comparing various configurations of RCG in terms of performance metrics (e.g., accuracy, response time, and interpretability) across diverse datasets and knowledge bases would provide insights into the optimal balance.

### Open Question 2
- Question: How can Explicit Prompt-Weighting be further optimized to improve the performance of RCG in various domains and knowledge bases?
- Basis in paper: [explicit] The paper mentions that Explicit Prompt-Weighting allows manual adjustment of the degree of influence of retrievers on the language model. The authors conducted an ablation study showing that adjusting the weightage to 50% yielded the highest improvement in Rouge-L scores.
- Why unresolved: The ablation study only explored a limited range of weightage values (10% to 90%). It is unclear whether these findings generalize to all datasets or knowledge bases, and further investigation may be necessary to determine optimal weightages for specific use cases.
- What evidence would resolve it: A comprehensive analysis of the impact of Explicit Prompt-Weighting on RCG performance across various domains, knowledge bases, and weightage values would help identify optimal configurations for different scenarios.

### Open Question 3
- Question: What are the potential limitations and risks associated with using Retrieval-Centric Generation (RCG) in real-world applications, and how can they be mitigated?
- Basis in paper: [explicit] The paper acknowledges that RCG does not provide a foolproof solution for ensuring a completely safe and responsible response from generative AI models. The authors also mention that generated texts may exhibit variations even when only slightly modifying prompts or queries.
- Why unresolved: The paper does not delve into the potential limitations and risks of RCG in real-world applications, nor does it provide guidance on how to mitigate these risks. Further research is needed to identify potential issues and develop strategies to address them.
- What evidence would resolve it: A thorough analysis of the limitations and risks associated with RCG in various real-world applications, along with proposed mitigation strategies and their effectiveness, would help users make informed decisions about deploying RCG in their specific contexts.

## Limitations
- Dependency on quality and comprehensiveness of constructed knowledge base
- Limited evaluation across diverse domains and knowledge base structures
- Basic prompt engineering approaches may not optimize retrieval-centric generation across all use cases

## Confidence
- High Confidence: Core retrieval-centric generation architecture and separation of LLM context interpretation from knowledge memorization is well-supported by improved Rouge-L scores (0.413) and maintained factual accuracy.
- Medium Confidence: Explicit Prompt-Weighting mechanism shows promise but optimal weighting across different knowledge bases and tasks needs validation.
- Medium Confidence: Smaller LLMs can achieve satisfactory performance through RCG, but this needs validation across broader model sizes and knowledge base complexities.

## Next Checks
1. Cross-Domain Knowledge Base Testing: Evaluate SimplyRetrieve with knowledge bases from different domains (medical, legal, technical documentation) to assess performance consistency and identify domain-specific limitations.
2. Prompt Weighting Optimization Study: Systematically test a range of Explicit Prompt-Weighting values (10%, 30%, 50%, 70%, 90%) across multiple query types to determine optimal weighting strategies for different use cases.
3. Model Size Scalability Analysis: Compare RCG performance across different LLM sizes (7B, 13B, 30B, 70B parameters) using identical knowledge bases to quantify the relationship between model capacity and retrieval-centric generation effectiveness.