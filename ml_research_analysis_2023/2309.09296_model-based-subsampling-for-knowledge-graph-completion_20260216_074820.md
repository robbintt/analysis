---
ver: rpa2
title: Model-based Subsampling for Knowledge Graph Completion
arxiv_id: '2309.09296'
source_url: https://arxiv.org/abs/2309.09296
tags:
- subsampling
- none
- base
- complex
- wn18rr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Model-based Subsampling (MBS) and Mixed Subsampling (MIX) were
  proposed to address the limitation of existing count-based subsampling (CBS) methods
  in Knowledge Graph Embedding (KGE). CBS only considers frequencies of queries, potentially
  underestimating the appearance probabilities of infrequent queries even if the frequencies
  of their entities or relations are high.
---

# Model-based Subsampling for Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2309.09296
- Source URL: https://arxiv.org/abs/2309.09296
- Reference count: 4
- Primary result: Model-based Subsampling (MBS) and Mixed Subsampling (MIX) improve KGE performance on sparse knowledge graphs by better estimating query appearance probabilities

## Executive Summary
This paper addresses the limitation of count-based subsampling (CBS) methods in Knowledge Graph Embedding (KGE) that underestimate the appearance probabilities of infrequent queries. The authors propose Model-based Subsampling (MBS) which estimates these probabilities through predictions of KGE models, and Mixed Subsampling (MIX) which combines CBS and MBS frequencies. Experiments on FB15k-237, WN18RR, and YAGO3-10 datasets show that MBS and MIX improve MRR, H@1, H@3, and H@10 compared to CBS for popular KGE models including RotatE, TransE, HAKE, ComplEx, and DistMult.

## Method Summary
The paper proposes two subsampling methods for KGE: Model-based Subsampling (MBS) and Mixed Subsampling (MIX). MBS uses a pre-trained KGE model to estimate frequencies for each triplet and query, addressing CBS's limitation of underestimating infrequent queries. MIX combines the frequencies of both CBS and MBS using a mixing ratio λ. The evaluation was conducted on three Knowledge Graph datasets (FB15k-237, WN18RR, YAGO3-10) using five popular KGE models (RotatE, TransE, HAKE, ComplEx, DistMult) and standard metrics (MRR, H@1, H@3, H@10).

## Key Results
- MBS and MIX significantly improve performance on sparse datasets (FB15k-237, WN18RR) compared to CBS
- MIX consistently outperforms both CBS and MBS across different KGE models and datasets
- MBS is particularly effective when combined with ComplEx as the sub-model
- On denser datasets like YAGO3-10, the improvements are less pronounced

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MBS improves KGE performance by leveraging sub-model predictions to estimate query appearance probabilities.
- **Mechanism**: MBS estimates frequencies using a pre-trained sub-model's predictions rather than relying solely on observed query frequencies. This allows it to infer probabilities for infrequent queries that CBS underestimates.
- **Core assumption**: The sub-model can learn useful entity and relation representations that capture their importance beyond raw frequency counts.
- **Evidence anchors**:
  - [abstract] "MBS estimates these probabilities through predictions of KGE models"
  - [section] "scoreθ′(x, y) in Eq. (9) estimates them by sub-model inference regardless of their actual frequencies"
- **Break condition**: The sub-model fails to learn meaningful entity/relation embeddings, causing MBS estimates to be inaccurate.

### Mechanism 2
- **Claim**: MIX combines CBS and MBS strengths to improve performance across diverse datasets.
- **Mechanism**: MIX interpolates between CBS (count-based) and MBS (model-based) frequency estimates using a mixing ratio λ, allowing it to benefit from both approaches' advantages.
- **Core assumption**: CBS and MBS capture complementary information about query importance, and their combination provides better estimates than either alone.
- **Evidence anchors**:
  - [abstract] "MIX combines the frequencies of both CBS and MBS"
  - [section] "Amix(θ′) = λAmbs(θ′) + (1 − λ)Acbs" and "Bmix(θ′) = λBmbs(θ′) + (1 − λ)Bcbs"
- **Break condition**: If CBS and MBS estimates are highly correlated or one approach dominates, MIX provides no benefit.

### Mechanism 3
- **Claim**: Sub-model selection is critical for MBS effectiveness, as different models capture different aspects of entity/relation importance.
- **Mechanism**: The choice of sub-model affects how well MBS can estimate frequencies for sparse queries. Models like ComplEx may perform better on certain datasets due to their smoothing properties.
- **Core assumption**: Different KGE architectures capture different aspects of knowledge graph structure, making some more suitable as sub-models for specific datasets.
- **Evidence anchors**:
  - [section] "Figure 3. In FB15k-237, we can see several spikes of frequencies in TransE, RotatE, and HAKE that do not exist in ComplEx"
  - [section] "these are along with the fact that FB15k-237 has larger training data than WN18RR"
- **Break condition**: If no suitable sub-model exists for a given dataset, MBS cannot improve upon CBS.

## Foundational Learning

- **Concept**: Negative Sampling in KGE
  - Why needed here: Understanding how subsampling modifies negative sampling loss is fundamental to grasping MBS and MIX.
  - Quick check question: How does the negative sampling loss in KGE differ from standard cross-entropy loss?

- **Concept**: Subsampling for Word Embeddings
  - Why needed here: MBS and MIX build upon subsampling techniques originally developed for word2vec, adapted for KGE.
  - Quick check question: What problem does subsampling solve in word2vec training?

- **Concept**: Knowledge Graph Sparsity
  - Why needed here: The motivation for MBS and MIX stems from the sparsity problem in knowledge graphs where many entities and relations have low frequencies.
  - Quick check question: Why is sparsity a particular challenge for traditional count-based subsampling methods?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Base KGE models (RotatE, TransE, HAKE, ComplEx, DistMult) -> Subsampling module (CBS, MBS, MIX) -> Training pipeline with NS loss -> Evaluation (MRR, H@1, H@3, H@10)

- **Critical path**: For MBS - train sub-model → use sub-model predictions to estimate frequencies → apply estimated frequencies in NS loss. For MIX - combine CBS and MBS frequencies using mixing ratio λ.

- **Design tradeoffs**: 
  - MBS requires pre-training a sub-model, increasing computational cost but potentially improving performance on sparse data.
  - MIX adds a hyperparameter (λ) that needs tuning but can adapt to different datasets.
  - Choice of sub-model significantly impacts MBS performance, requiring careful selection.

- **Failure signatures**: 
  - Poor performance on dense datasets (like YAGO3-10) suggests MBS/MIX may not be beneficial when sparsity is low.
  - Inconsistent results across runs may indicate sensitivity to initialization or sub-model choice.
  - No improvement over CBS suggests the sub-model isn't capturing useful information.

- **First 3 experiments**:
  1. Implement CBS and compare performance on FB15k-237 vs YAGO3-10 to observe sparsity effects.
  2. Add MBS with ComplEx as sub-model on FB15k-237 to test if model-based estimation helps.
  3. Implement MIX and tune λ to find optimal combination of CBS and MBS for different datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is the performance of MIX and MBS to the choice of the sub-model, and can this be automated?
- Basis in paper: [explicit] The paper notes that the performance of MBS and MIX depends on the choice of sub-model, with ComplEx often being selected for FB15k-237 and WN18RR datasets.
- Why unresolved: The selection of an appropriate sub-model for different datasets is crucial for optimizing the performance of MIX and MBS. However, the process of choosing the right sub-model is currently manual and based on validation MRR.
- What evidence would resolve it: Systematic experimentation with various sub-models across different datasets to identify patterns or develop an automated method for sub-model selection could provide insights into optimizing the performance of MIX and MBS.

### Open Question 2
- Question: What is the impact of using MIX and MBS on computational efficiency, especially in larger and noisier datasets?
- Basis in paper: [inferred] The paper mentions that utilizing model-based subsampling requires pre-training for choosing a suitable sub-model, which may increase computational costs.
- Why unresolved: While the paper acknowledges the potential increase in computational budget, it does not provide detailed analysis on how MIX and MBS affect computational efficiency, particularly with larger and noisier datasets.
- What evidence would resolve it: Conducting experiments that measure the computational efficiency of MIX and MBS on various dataset sizes and noise levels would help determine their scalability and practical applicability.

### Open Question 3
- Question: Can the MIX and MBS methods be generalized to improve performance in other deep learning models beyond KGE?
- Basis in paper: [explicit] The paper discusses the potential for generalizing MIX and MBS to deep learning models but notes that the current work is limited to KGE models.
- Why unresolved: The effectiveness of MIX and MBS in enhancing the performance of other deep learning models, such as natural language processing or computer vision, remains unexplored.
- What evidence would resolve it: Extending the application of MIX and MBS to different types of deep learning models and evaluating their impact on performance metrics would provide insights into their broader applicability and effectiveness.

## Limitations

- Evaluation is limited to three knowledge graph datasets, which may not represent all knowledge graph domains
- Computational overhead of training additional sub-models for MBS is not thoroughly analyzed
- Sensitivity to hyperparameter λ in MIX requires further investigation
- Impact of different negative sampling strategies on MBS and MIX effectiveness remains unexplored

## Confidence

- **High Confidence**: The core claim that MBS and MIX improve performance over CBS on sparse datasets is well-supported by experimental results.
- **Medium Confidence**: The mechanism by which MBS estimates frequencies through sub-model predictions is plausible but could benefit from more detailed analysis of sub-model selection.
- **Low Confidence**: The generalization of results to other knowledge graph domains and the impact of different negative sampling strategies remain uncertain.

## Next Checks

1. Evaluate MBS and MIX on additional knowledge graph datasets with varying levels of sparsity to assess generalization.
2. Conduct ablation studies to quantify the impact of sub-model selection on MBS performance.
3. Analyze the computational overhead of MBS compared to CBS and MIX to understand the trade-offs involved.