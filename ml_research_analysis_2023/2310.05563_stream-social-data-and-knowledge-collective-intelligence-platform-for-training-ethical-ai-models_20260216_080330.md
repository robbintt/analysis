---
ver: rpa2
title: 'STREAM: Social data and knowledge collective intelligence platform for TRaining
  Ethical AI Models'
arxiv_id: '2310.05563'
source_url: https://arxiv.org/abs/2310.05563
tags:
- moral
- stream
- ethical
- platform
- judgments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STREAM is a social data and knowledge collective intelligence platform
  for training ethical AI models, addressing the challenge of aligning AI with human
  moral values. The platform collects ethical scenarios and moral judgment data from
  diverse groups, including humans and AI models, to establish, evaluate, embed, embody,
  ensemble, and evolve AI's moral capabilities.
---

# STREAM: Social data and knowledge collective intelligence platform for TRaining Ethical AI Models

## Quick Facts
- arXiv ID: 2310.05563
- Source URL: https://arxiv.org/abs/2310.05563
- Reference count: 20
- Key outcome: Social platform collecting ethical scenarios and moral judgments from diverse groups to align AI models with human values, showing current LLMs have gaps in moral judgment compared to humans

## Executive Summary
STREAM is a platform designed to train ethical AI models by collecting and analyzing moral judgments from diverse human and AI sources. The system addresses the challenge of aligning AI moral reasoning with human values by creating a comprehensive dataset of ethical scenarios annotated across demographic groups. Through systematic collection of moral judgments, the platform enables the construction of demographic-aligned classifiers that can guide AI decision-making in culturally appropriate ways.

## Method Summary
The STREAM platform implements a crowd-sourcing approach where volunteers and large language models annotate moral scenarios, creating a rich dataset of ethical judgments. The system collects demographic metadata alongside judgments, enabling analysis of how moral reasoning varies across different groups. The platform employs Cramer's V correlation analysis to compare LLM judgments with human judgments and measures stability through repeated trials to assess consistency. These data feed into classifier training pipelines that can be used to align AI systems with specific community standards and values.

## Key Results
- Current LLMs show significant gaps in moral judgment compared to human populations, with GPT-4 and Claude2 showing highest correlation with human judgments
- Moral judgments by LLMs are less consistent than humans, with stability increasing with model size
- The platform successfully collects diverse moral judgment data across demographic groups for training ethical AI classifiers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: STREAM aligns AI moral judgments with diverse human populations by collecting demographic-specific ethical annotations.
- Mechanism: The platform recruits volunteers across varied demographics, annotates moral scenarios, and uses this data to fine-tune AI models to reflect group-specific moral norms.
- Core assumption: Moral judgments vary meaningfully by demographic group and can be captured through structured annotations.
- Evidence anchors:
  - [abstract] "By creating a comprehensive and representative platform that accurately mirrors the moral judgments of diverse groups including humans and AIs..."
  - [section] "This is done in the hope that the future deployment of AI models, with the aid of STREAM, can align with the values of specific demographic groups accordingly."
  - [corpus] FMR analysis shows 0.439 average neighbor score, indicating moderate relevance of related moral judgment studies.

### Mechanism 2
- Claim: LLM moral judgments are unstable across repeated trials, unlike human judgments.
- Mechanism: By having models evaluate the same scenario multiple times, STREAM quantifies inconsistency and correlates it with model size.
- Core assumption: Stability is a meaningful proxy for moral reasoning quality and can be measured via repeated trials.
- Evidence anchors:
  - [section] "It can be clearly seen that with the increase of parameters for a specific model, the stability of moral judgment also increases."
  - [section] "Considering the very high stability of human moral judgments in general, it can be assumed that current large language models still lack stable moral values..."
  - [corpus] Studies on LLM moral reasoning show emerging frameworks but lack systematic stability metrics.

### Mechanism 3
- Claim: STREAM enables construction of demographic-aligned moral classifiers for downstream AI systems.
- Mechanism: Collects group-specific moral judgments, uses them as supervised labels to train classifiers that guide AI decisions in context.
- Core assumption: Group-specific moral judgments can be generalized into classifier rules without overfitting to narrow samples.
- Evidence anchors:
  - [section] "STREAM offers a wealth of moral judgment scenarios and associated behaviors, alongside a cross-cultural, diverse dataset of moral judgments from various demographic groups."
  - [section] "The collective moral judgments of these groups are then utilized as supervised signals, optimizing the classifier’s fine-tuning to specific community standards and values."
  - [corpus] Limited citations in corpus suggest this demographic classifier approach is novel and under-validated.

## Foundational Learning

- Concept: Moral Foundations Theory
  - Why needed here: Provides a structured framework to categorize moral judgments across cultures, critical for interpreting STREAM annotations.
  - Quick check question: What are the five core moral foundations proposed by Haidt and Graham?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: STREAM data feeds into RLHF pipelines to align model behavior with human moral preferences.
  - Quick check question: How does RLHF differ from standard supervised fine-tuning in terms of data flow and objective?

- Concept: Cross-validation in demographic sampling
  - Why needed here: Ensures that moral judgment classifiers generalize across subgroups and avoid demographic bias.
  - Quick check question: What validation strategy would detect overfitting when training on one cultural group's moral data?

## Architecture Onboarding

- Component map: Frontend (Scenario submission, judgment annotation UI, user dashboards) -> Backend (Scenario database, demographic metadata store, LLM integration API) -> ML pipeline (Annotation aggregation, stability analysis, classifier training) -> Evaluation (Human baseline alignment, model stability tracking, cross-cultural comparison)

- Critical path: 1. Scenario ingestion → 2. Annotation collection → 3. Demographic grouping → 4. Judgment aggregation → 5. Classifier training → 6. AI alignment

- Design tradeoffs:
  - Open vs. curated scenario sources: Open increases scale but risks noise; curated ensures quality but limits coverage.
  - Demographic granularity: Fine-grained groups improve alignment but reduce statistical power per group.
  - Stability measurement frequency: High frequency reveals inconsistency but increases computational cost.

- Failure signatures:
  - Annotation convergence: If all groups judge similarly, demographic differentiation fails.
  - Stability plateaus: If LLM stability stops improving with size, other factors dominate.
  - Classifier collapse: If models misalign when applied outside training demographics, overfitting occurred.

- First 3 experiments:
  1. Measure moral judgment agreement rates within and across demographic groups for identical scenarios.
  2. Test LLM stability by repeating judgments five times per scenario and track distribution shifts.
  3. Train a simple demographic classifier and evaluate performance on held-out scenarios from the same culture.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can STREAM effectively handle the dynamic nature of moral judgments over time?
- Basis in paper: [explicit] The paper discusses the importance of capturing the dynamic evolution of moral judgments over time, but does not detail how this can be achieved.
- Why unresolved: The paper mentions the need for an ongoing platform but does not provide a clear methodology for continuously updating and evolving the moral judgments dataset.
- What evidence would resolve it: A detailed methodology or framework for continuously updating the moral judgments dataset, including mechanisms for incorporating new scenarios and evolving societal norms.

### Open Question 2
- Question: How can STREAM ensure the representation of diverse cultural backgrounds in its moral judgments?
- Basis in paper: [explicit] The paper highlights the challenge of cultural sensitivity and the need for diverse cultural backgrounds in the annotators.
- Why unresolved: The paper does not provide specific strategies or methods for ensuring diverse cultural representation in the moral judgments dataset.
- What evidence would resolve it: Implementation of strategies such as targeted recruitment of annotators from various cultural backgrounds, partnerships with international organizations, or the use of culturally diverse scenario datasets.

### Open Question 3
- Question: What are the specific mechanisms for incentivizing user participation on the STREAM platform?
- Basis in paper: [inferred] The paper mentions the need for incentive mechanisms for user participation but does not detail what these mechanisms are.
- Why unresolved: The paper does not provide information on how users will be motivated to participate and contribute to the platform.
- What evidence would resolve it: Details on incentive structures such as gamification, rewards, recognition, or other motivational strategies to encourage user engagement on the platform.

## Limitations

- The platform's demographic differentiation mechanism assumes meaningful variation in moral judgments across groups, but this requires empirical validation across diverse cultural contexts
- Stability measurements rely on repeated trials without clear optimization of repetition frequency versus computational cost
- The effectiveness of demographic classifiers for real-world AI alignment remains theoretical without comprehensive deployment validation

## Confidence

- **High Confidence**: The platform's core architecture for collecting ethical scenarios and annotations is well-specified. The correlation analysis between LLM and human judgments using Cramer's V is methodologically sound.
- **Medium Confidence**: The stability analysis showing increased consistency with model size is supported by experimental data, though the generalizability across different LLM families needs further testing.
- **Low Confidence**: The claim about demographic classifier effectiveness for downstream AI alignment is based on theoretical potential rather than demonstrated results.

## Next Checks

1. Conduct cross-cultural validation by testing STREAM's demographic differentiation across at least three distinct cultural regions to verify the platform's generalizability.
2. Perform ablation studies on stability measurement frequency to determine the optimal balance between computational cost and measurement accuracy.
3. Implement a deployment simulation where demographic classifiers guide AI decisions in mixed-population scenarios to test real-world effectiveness and potential biases.