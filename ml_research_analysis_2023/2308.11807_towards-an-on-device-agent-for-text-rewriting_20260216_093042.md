---
ver: rpa2
title: Towards an On-device Agent for Text Rewriting
arxiv_id: '2308.11807'
source_url: https://arxiv.org/abs/2308.11807
tags:
- text
- arxiv
- data
- rewrite
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to create a smaller on-device language
  model for text rewriting that balances model size with performance. The approach
  uses synthetic data generation via LLM hallucinations and a heuristic-based reinforcement
  learning framework to enhance the model without human labeling.
---

# Towards an On-device Agent for Text Rewriting

## Quick Facts
- arXiv ID: 2308.11807
- Source URL: https://arxiv.org/abs/2308.11807
- Reference count: 17
- This paper proposes a method to create a smaller on-device language model for text rewriting that balances model size with performance.

## Executive Summary
This paper introduces an approach for creating compact on-device language models capable of text rewriting tasks while preserving the emergent capabilities of larger LLMs. The method leverages synthetic data generation through LLM hallucinations, heuristic-based reinforcement learning, and a cascading approach that combines the on-device model with a larger server model. The authors develop a new benchmark, MESSAGE REWRITE EVAL, specifically for evaluating text rewriting in message-like contexts, and demonstrate that their on-device model outperforms state-of-the-art LLMs on text rewriting tasks while being significantly smaller.

## Method Summary
The approach begins with generating synthetic instruction-tuning datasets using few-shot prompting to elicit LLM hallucinations of diverse rewrite examples. The model is then fine-tuned using supervised learning on this synthetic data, followed by heuristic reinforcement learning that combines multiple signals (NLI, reversed NLI, length ratio, edit distance, n-gram frequency) into a weighted reward function. A critique distillation phase adds self-critique capability via suffix-based confidence scoring, enabling the cascading approach where the on-device model can decide when to invoke the server model based on its confidence in the output.

## Key Results
- On-device model outperforms state-of-the-art LLMs on text rewriting tasks while being significantly smaller (XXS model)
- Cascading approach improves success rates across different rewrite tasks by intelligently routing to server model when needed
- Achieves strong results on both EDIT EVAL and MESSAGE REWRITE EVAL benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data generation via LLM hallucinations can create high-quality instruction tuning datasets without human labeling.
- Mechanism: The approach uses few-shot prompting to make LLMs "hallucinate" diverse rewrite examples by continuing from a small seed query set. This generates paired data (source, rewrite, instruction) that captures the desired rewriting patterns.
- Core assumption: LLMs can generate realistic and diverse rewrite examples when prompted appropriately with few-shot examples.
- Evidence anchors:
  - [abstract] "Our strategies enable the generation of high quality training data without any human labeling."
  - [section] "To generate in-domain data efficiently, we propose a data generation approach based on 'hallucinations' of off-the-shelf LLMs"
- Break condition: If the LLM cannot generate diverse enough examples or hallucinates too much irrelevant content, the synthetic data quality degrades.

### Mechanism 2
- Claim: Heuristic reinforcement learning can improve model performance without preference data or reward model training.
- Mechanism: The approach combines multiple heuristic signals (NLI score, reversed NLI, length ratio, edit distance ratio, n-gram frequency) into a weighted reward function that guides reinforcement learning. This eliminates the need for human preference data or training a separate reward model.
- Core assumption: Simple heuristics can effectively capture the quality of text rewrites without complex preference modeling.
- Evidence anchors:
  - [abstract] "we propose a heuristic reinforcement learning framework which substantially enhances performance without requiring preference data."
  - [section] "We propose to use the following heuristics as reward signals" followed by detailed heuristic list
- Break condition: If the heuristic signals don't correlate well with actual rewrite quality, the RL process won't improve performance.

### Mechanism 3
- Claim: Cascading with critique distillation enables smaller models to achieve near-server performance with fewer server calls.
- Mechanism: The approach adds a confidence suffix to on-device model outputs, learned via distillation from larger LLM critiques. This allows the on-device model to self-critique and decide when to cascade to the server model, reducing server calls while maintaining quality.
- Core assumption: The distilled confidence signal can accurately predict when the on-device model's output is insufficient.
- Evidence anchors:
  - [abstract] "we propose an effective approach that combines the mobile rewrite agent with the server model using a cascade"
  - [section] "We distill the critiquing ability of the server LLM to the smaller model using discriminative training"
- Break condition: If the confidence signal is poorly calibrated, the cascade either makes too many unnecessary server calls or fails to cascade when needed.

## Foundational Learning

- Concept: Few-shot prompting
  - Why needed here: Used for both synthetic data generation and LLM filtering/critique
  - Quick check question: What are the key elements that make few-shot prompts effective for generating diverse rewrite examples?

- Concept: Reinforcement learning with heuristics
  - Why needed here: Provides performance improvement without costly preference data collection
  - Quick check question: How do the different heuristic signals (NLI, length ratio, etc.) complement each other in the reward function?

- Concept: Knowledge distillation
  - Why needed here: Transfers the critique ability from large LLM to small on-device model
  - Quick check question: What's the difference between generative and discriminative fine-tuning in the context of adding suffix-based critique signals?

## Architecture Onboarding

- Component map: Pre-trained LLM → Synthetic Data Generator → Supervised Fine-tuning → Heuristic RL → Critique Distillation → Cascading System
- Critical path: Data generation → Supervised fine-tuning → Heuristic RL → Model evaluation
- Design tradeoffs: Model size vs performance (balancing XXS size with state-of-the-art results), synthetic data quality vs human labeling cost, server calls vs on-device inference speed
- Failure signatures: Poor synthetic data quality (hallucinations too unrealistic), RL not improving performance (heuristics poorly weighted), cascade making wrong decisions (confidence scores miscalibrated)
- First 3 experiments:
  1. Generate synthetic data with different few-shot prompt variations and measure diversity/quality
  2. Test different heuristic reward weight combinations on a small dataset
  3. Evaluate cascading performance with different confidence thresholds on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the cascading approach's performance scale with different types of text rewriting tasks?
- Basis in paper: Explicit - The paper discusses the cascading approach and evaluates it across different rewrite tasks like formalize, shorten, elaborate, paraphrase, and proofread.
- Why unresolved: The paper provides a general overview of the cascading approach's effectiveness but does not delve into detailed performance metrics for each specific type of text rewriting task. It would be beneficial to understand how well the cascading approach performs for each task type to optimize its application.
- What evidence would resolve it: Detailed performance metrics for each rewrite task type when using the cascading approach, showing success rates and on-device inference ratios for each.

### Open Question 2
- Question: What are the limitations of using heuristic-based reinforcement learning for text rewriting tasks?
- Basis in paper: Inferred - The paper introduces a heuristic-based reinforcement learning framework as an alternative to human-labeled data and preference data, but it does not extensively discuss potential limitations or scenarios where this approach might underperform.
- Why unresolved: While the paper demonstrates the effectiveness of heuristic-based reinforcement learning, it does not explore the boundaries or limitations of this approach, such as its applicability to more complex or nuanced rewriting tasks.
- What evidence would resolve it: Empirical studies comparing heuristic-based reinforcement learning with other methods on a variety of text rewriting tasks, highlighting scenarios where it excels or falls short.

### Open Question 3
- Question: How does the quality of synthetic data generated through LLM hallucinations impact the overall performance of the on-device model?
- Basis in paper: Explicit - The paper describes the use of synthetic data generation via LLM hallucinations to enhance the model without human labeling.
- Why unresolved: The paper mentions the generation of synthetic data but does not provide a detailed analysis of how the quality of this data affects the model's performance. Understanding this relationship could help in refining the data generation process.
- What evidence would resolve it: Comparative studies analyzing the impact of synthetic data quality on model performance, including metrics like accuracy, fluency, and coherence, to determine the correlation between data quality and model effectiveness.

### Open Question 4
- Question: What are the potential privacy implications of using LLM hallucinations for synthetic data generation?
- Basis in paper: Inferred - The paper discusses the use of synthetic data generation to avoid human labeling, which implies a focus on privacy, but does not explicitly address potential privacy concerns related to LLM hallucinations.
- Why unresolved: While the paper highlights the privacy benefits of avoiding human labeling, it does not explore whether the use of LLM hallucinations for data generation introduces any privacy risks, such as the inadvertent inclusion of sensitive information in the synthetic data.
- What evidence would resolve it: An analysis of the synthetic data for potential privacy risks, including audits for sensitive information and assessments of data anonymization effectiveness.

### Open Question 5
- Question: How does the on-device model's performance compare to other compact models that use different architectures or training methodologies?
- Basis in paper: Explicit - The paper compares the on-device model to other instruction-tuned LLMs and pre-trained models, highlighting its superior performance despite being smaller.
- Why unresolved: While the paper demonstrates the on-device model's superiority over certain baselines, it does not compare it to other compact models that may use different architectures or training methodologies, which could provide insights into alternative approaches to achieving similar performance.
- What evidence would resolve it: Comparative studies evaluating the on-device model against other compact models with different architectures or training methodologies, focusing on performance metrics like accuracy, latency, and memory usage.

## Limitations
- The approach relies heavily on LLM-generated synthetic data without human validation, creating potential quality risks that aren't directly measured
- The heuristic RL framework assumes simple metrics can capture rewrite quality without preference data, but the evidence for this assumption is largely based on downstream performance rather than direct validation of heuristic effectiveness
- The cascading mechanism's success depends on the distilled confidence signal being well-calibrated, but the paper doesn't provide analysis of false positive/negative rates for cascade decisions

## Confidence
- High confidence: The basic architecture (SFT → RL → distillation) is clearly specified and the cascading approach shows measurable improvements in success rates
- Medium confidence: The synthetic data generation approach will work in practice, though quality may vary with different LLM choices
- Low confidence: The heuristic RL framework will generalize well to tasks beyond the MESSAGE REWRITE EVAL benchmark

## Next Checks
1. Measure actual cascade decision accuracy by comparing the model's self-critique against human judgments on a validation set, computing precision/recall for cascade decisions
2. Conduct ablation studies removing individual heuristic signals from the RL reward function to quantify their contribution to final performance
3. Test the synthetic data generation pipeline with different LLM models (not just PaLM 2) to assess robustness of hallucination quality across model choices