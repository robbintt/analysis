---
ver: rpa2
title: 'TaskWeb: Selecting Better Source Tasks for Multi-task NLP'
arxiv_id: '2305.13256'
source_url: https://arxiv.org/abs/2305.13256
tags:
- task
- tasks
- source
- transfer
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates task selection for multi-task learning
  in NLP by building a large-scale benchmark of pairwise task transfers across 22
  tasks, three model types, and three adaptation methods (about 25,000 experiments).
  It proposes TaskShop, a method that uses transitive behavior in positive task transfers
  to estimate source-to-target transferability for unseen tasks.
---

# TaskWeb: Selecting Better Source Tasks for Multi-task NLP

## Quick Facts
- arXiv ID: 2305.13256
- Source URL: https://arxiv.org/abs/2305.13256
- Authors: 
- Reference count: 40
- This paper investigates task selection for multi-task learning in NLP by building a large-scale benchmark of pairwise task transfers across 22 tasks, three model types, and three adaptation methods (about 25,000 experiments). It proposes TaskShop, a method that uses transitive behavior in positive task transfers to estimate source-to-target transferability for unseen tasks. TaskShop improves overall rankings and top-k precision of source tasks by 10% and 38%, respectively. It also enables building small multi-task training sets that improve zero-shot performance across 11 target tasks by at least 4.3%.

## Executive Summary
This paper addresses the challenge of selecting beneficial source tasks for multi-task learning in NLP. The authors construct TaskWeb, a comprehensive repository of pairwise transfer scores across 22 diverse NLP tasks, three model architectures, and three adaptation methods. Building on this data, they propose TaskShop, a novel method that leverages transitive relationships in positive task transfers to predict source-to-target transferability, even for unseen tasks. The approach significantly improves task selection quality and enables effective multi-task learning with small, carefully curated training sets.

## Method Summary
The authors build TaskWeb by computing pairwise transfer scores across 22 NLP tasks, three model types (T5, GPT-2, RoBERTa), and three adaptation methods through fine-tuning experiments. TaskShop uses these scores to predict transferability by averaging scores from source→pivot and pivot→target paths, combined with direct task selection scores. For multi-task learning, the method selects top-k source tasks based on these predictions, builds a small training set from their examples, and trains a multi-task model to achieve improved zero-shot performance on target tasks.

## Key Results
- TaskShop improves overall rankings and top-k precision of source tasks by 10% and 38%, respectively
- Small multi-task training sets built from top-k source tasks improve zero-shot performance across 11 target tasks by at least 4.3%
- Performance typically peaks with 3-5 source tasks; using ten or more source tasks results in worse performance than using five

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pairwise transfer scores capture transferable knowledge that generalizes transitively to unseen tasks.
- **Mechanism**: The method uses the transitive property of positive transfers—if task A → B and B → C are both positive, then A → C is more likely to be positive. By chaining these predictions through pivot tasks, the system estimates source→target transferability even when the target is unseen.
- **Core assumption**: Positive pairwise task transfers exhibit a transitive relationship that can be leveraged to predict unseen task performance.
- **Evidence anchors**:
  - [abstract] "We discover a transitive property where having strong, positive transfers A → B and B → C for tasks A, B and C makes it more likely that A → C is also a positive transfer."
  - [section 3.3] "Results are shown in Figure 5. We observe that as a stricter criteria is imposed for the pair of source → pivot and pivot→ target transfers, the likelihood of observing a positive transfer steadily increases across all training setups."
  - [corpus] Weak corpus evidence; related works focus on prompt tuning and task grouping, but none directly validate the transitive property for task transfer prediction.
- **Break condition**: If the transitive assumption fails for a target task domain, the predicted scores will degrade, especially when pivot tasks are not truly representative of the source→target relationship.

### Mechanism 2
- **Claim**: Directional pairwise transfer scores improve task selection precision over symmetric similarity measures.
- **Mechanism**: Unlike symmetric cosine similarity embeddings, the method incorporates directional transfer scores, which reflect asymmetric knowledge transfer. This allows more accurate identification of helpful source tasks.
- **Core assumption**: Task transfer is directional; knowledge gained from A to B does not necessarily imply B to A is beneficial.
- **Evidence anchors**:
  - [section 3.3] "We uniquely observe from our experiments that pairwise transfer does not display strong signs of commutativity."
  - [section 4.1] "One interesting feature of our method is its directionality due to the usage of pairwise transfer scores. This implies that our transfer predictions for A → B differs from those for B → A."
  - [corpus] No direct corpus evidence for directionality benefits; related works use symmetric embeddings, suggesting this is a novel contribution.
- **Break condition**: If commutativity unexpectedly holds for a new task category, symmetric methods may outperform the directional approach.

### Mechanism 3
- **Claim**: Small, targeted multi-task training sets built from top-k source tasks outperform large indiscriminate sets.
- **Mechanism**: By selecting only the top-scoring source tasks for a target and combining their examples into a small training set, the model avoids negative interference and focuses on relevant knowledge.
- **Core assumption**: Including unrelated tasks in multi-task training can degrade performance due to interference.
- **Evidence anchors**:
  - [section 5.2] "Our results support previous observations that it can be useful to build smaller multi-task training sets with a more careful task selection strategy."
  - [section 5.3] "Surprisingly, using ten or more source tasks results in an overall worse performance than using five source tasks."
  - [corpus] Related works discuss task grouping but lack empirical validation that small curated sets beat large sets in this context.
- **Break condition**: If interference effects are negligible or beneficial for the target domain, larger sets may perform better.

## Foundational Learning

- **Concept**: Pairwise task transfer and transfer learning metrics.
  - **Why needed here**: Understanding how transfer scores are computed and interpreted is critical for designing and debugging the selection algorithm.
  - **Quick check question**: Given a source model trained on task A and evaluated on task B, how is the transfer score calculated and what does a positive score signify?

- **Concept**: Transitivity in task relationships and its mathematical formulation.
  - **Why needed here**: The core algorithm relies on chaining positive transfers; without this, the method cannot generalize to unseen tasks.
  - **Quick check question**: If A→B and B→C are positive, under what conditions can we infer A→C will also be positive?

- **Concept**: Multi-task learning interference and negative transfer.
  - **Why needed here**: Knowing when and why combining tasks helps or hurts is key to selecting the right number of source tasks.
  - **Quick check question**: What factors cause negative transfer in multi-task learning, and how does task selection mitigate it?

## Architecture Onboarding

- **Component map**:
  - TASK WEB -> TASK SHOP -> Task selection engine -> Multi-task builder -> Zero-shot evaluation

- **Critical path**:
  1. Load TASK WEB and precompute pivot-based transfer scores.
  2. For a new target, sample ≤32 examples.
  3. Use TASK SHOP to score all sources via pivot averaging.
  4. Select top-k sources, sample examples, train multi-task model.
  5. Evaluate zero-shot on target.

- **Design tradeoffs**:
  - **Pivot selection**: Using all pivots maximizes coverage but increases computation; limiting pivots trades accuracy for speed.
  - **Score interpolation**: Mixing direct F(s→t) and pivot-based estimates balances local and global knowledge.
  - **Model choice**: T5-large is used for transfer scores to balance expressiveness and cost; smaller models are faster but less accurate.

- **Failure signatures**:
  - **Low NDCG/Regret@k**: Indicates pivot paths are not capturing true transferability.
  - **Degraded zero-shot performance**: Suggests negative interference or poor source selection.
  - **High variance across seeds**: May signal instability in transfer scores or task selection.

- **First 3 experiments**:
  1. **Baseline sanity check**: Run TASK SHOP with F=RoE on a held-out task; verify NDCG > baseline.
  2. **Pivot ablation**: Limit pivot tasks to top-5 most similar; compare performance vs. full pivot set.
  3. **k-selection sweep**: Vary k (1, 3, 5, 10) in multi-task setup; plot target accuracy vs. k to find sweet spot.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do negative task transfers affect multi-task learning performance, and can we predict or mitigate these negative effects?
- Basis in paper: [explicit] The paper demonstrates that negative pairwise transfers between source and target tasks consistently decrease target performance during multi-task learning, and that mixing helpful and unhelpful source tasks leads to performance degradation.
- Why unresolved: While the paper shows that negative transfers exist and harm performance, it does not investigate the mechanisms behind these negative effects or develop methods to predict or mitigate them.
- What evidence would resolve it: Experiments that systematically analyze the impact of negative transfers on multi-task learning, develop methods to predict negative transfers before training, and evaluate mitigation strategies to counteract negative transfer effects.

### Open Question 2
- Question: How does task selection scale to larger numbers of tasks (100+) and larger model sizes (1B+ parameters)?
- Basis in paper: [inferred] The paper evaluates task selection methods on 22 tasks and models up to 770M parameters, but does not investigate how these methods perform as the number of tasks and model sizes increase significantly.
- Why unresolved: The computational cost of computing pairwise transfer scores increases quadratically with the number of tasks, and larger models may exhibit different transfer patterns that could affect task selection methods.
- What evidence would resolve it: Experiments that evaluate task selection methods on datasets with 100+ tasks and models with 1B+ parameters, along with analyses of computational efficiency and transfer patterns at scale.

### Open Question 3
- Question: What is the optimal number of source tasks to include in multi-task training for different target tasks and model sizes?
- Basis in paper: [explicit] The paper finds that performance typically peaks with 3-5 source tasks but varies by target task, and that using too many tasks can decrease performance.
- Why unresolved: The paper provides a general guideline but does not develop a method to determine the optimal number of source tasks for specific target tasks or model sizes, nor does it explain the mechanisms behind the optimal number.
- What evidence would resolve it: A systematic study that identifies factors influencing the optimal number of source tasks for different target tasks and model sizes, and develops a method to predict the optimal number for new target tasks.

## Limitations

- The core method relies on the transitive property of positive task transfers, which may not generalize beyond the 22 tasks studied
- The approach depends on computing pairwise transfer scores across all task pairs, which becomes computationally expensive as task count increases
- The method's effectiveness with emerging task types (e.g., multimodal tasks) or newer model architectures remains untested

## Confidence

**High Confidence**: The empirical improvements in NDCG (10%) and top-k precision (38%) are directly measured from the TaskWeb benchmark experiments and are well-supported by the data. The observation that small, curated multi-task training sets outperform larger indiscriminate sets is also strongly evidenced by the zero-shot performance gains reported.

**Medium Confidence**: The proposed TaskShop method's general applicability beyond the specific 22 tasks and three model types tested is supported by the transitive property discovery but requires further validation. The mechanism by which directional transfer scores improve precision over symmetric measures is observed but not fully explained theoretically.

**Low Confidence**: The long-term stability of the TaskWeb repository and its maintenance for future research applications is not addressed. The method's performance on emerging task types (e.g., multimodal tasks) or with newer model architectures remains unknown.

## Next Checks

1. **Cross-domain validation**: Apply TaskShop to a held-out set of tasks from different domains (e.g., biomedical, legal, or code-related NLP tasks) to test the robustness of the transitive transfer assumption and directional transfer benefits.

2. **Pivot coverage analysis**: Systematically vary the pivot task set size and composition to quantify the tradeoff between computational efficiency and prediction accuracy, identifying optimal pivot selection strategies.

3. **Interference effect measurement**: Conduct controlled experiments to isolate and quantify negative transfer effects across different task combinations, validating whether the small multi-task training sets truly avoid interference or simply benefit from other factors.