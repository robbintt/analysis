---
ver: rpa2
title: Effective Robustness against Natural Distribution Shifts for Models with Different
  Training Data
arxiv_id: '2302.01381'
source_url: https://arxiv.org/abs/2302.01381
tags:
- imagenet
- accuracy
- robustness
- ective
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses a limitation in the existing evaluation of
  effective robustness when comparing models trained on different datasets. The proposed
  method uses multiple ID test sets to provide a more precise estimate of effective
  robustness.
---

# Effective Robustness against Natural Distribution Shifts for Models with Different Training Data

## Quick Facts
- arXiv ID: 2302.01381
- Source URL: https://arxiv.org/abs/2302.01381
- Authors: 
- Reference count: 35
- One-line primary result: Multi-ID evaluation provides more accurate effective robustness estimates by using multiple ID test sets to predict OOD accuracy through multi-dimensional linear regression

## Executive Summary
This paper addresses a fundamental limitation in evaluating effective robustness when comparing models trained on different datasets. Traditional single-ID evaluation (e.g., using only ImageNet as the ID test set) biases results toward models trained on that specific dataset. The authors propose using multiple ID test sets that cover all training distributions, then applying multi-dimensional linear regression in logit space to predict OOD accuracy more accurately. This approach provides a more precise estimate of effective robustness gains, particularly when comparing models with different training data distributions.

## Method Summary
The method uses multi-dimensional linear regression to predict OOD accuracy from accuracies on multiple ID test sets. Instead of controlling for a single ID accuracy that may bias towards models from a particular training distribution, the approach uses multiple ID test sets that cover the training distributions of all evaluated models. The regression is performed in logit space, where accuracies are transformed using logit transformation before fitting a linear plane. The effective robustness is then calculated as the difference between actual OOD accuracy and the predicted OOD accuracy from the multi-ID regression model.

## Key Results
- Multi-ID evaluation achieves better fitting quality (higher R² and lower MAE) compared to single-ID evaluation
- CLIP models' apparent effective robustness gains diminish when evaluated with multiple ID test sets, revealing these gains were artifacts of training data mismatch
- The proposed method provides more precise estimates of effective robustness when comparing models trained on different datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-dimensional linear regression on multiple ID test sets reduces bias from single-ID evaluation when comparing models trained on different data
- Mechanism: Using multiple ID test sets that cover all training distributions allows the regression plane to more accurately predict OOD accuracy regardless of training data, avoiding bias toward any single dataset
- Core assumption: The relationship between ID accuracies across multiple test sets and OOD accuracy can be modeled as a linear plane in logit space
- Evidence anchors:
  - [abstract] "The proposed method uses multiple ID test sets to provide a more precise estimate of effective robustness"
  - [section] "Instead of controlling for a single ID accuracy that may bias towards models from a particular training distribution, we propose to use multiple ID test sets that cover the training distributions of all the evaluated models"
- Break condition: If the linear relationship between ID accuracies and OOD accuracy breaks down for certain distribution shifts (e.g., adversarial attacks), the multi-ID approach would not work

### Mechanism 2
- Claim: CLIP models' apparent effective robustness gains diminish when evaluated with multiple ID test sets
- Mechanism: Prior single-ID evaluation showed CLIP models had significant effective robustness gains due to training data mismatch (LAION/YFCC vs ImageNet), but multi-ID evaluation reveals these gains were artifacts
- Core assumption: The effective robustness gains observed in single-ID evaluation were primarily due to training data mismatch rather than true robustness
- Evidence anchors:
  - [abstract] "Our results provide new understandings on the effective robustness gains of CLIP-like models observed in prior works only using ImageNet as the ID test set, while the gains diminish under our new evaluation"
  - [section] "In contrast, under our multi-ID evaluation, the average effective robustness becomes 0.77±0.85 (%) for YFCC models, and -0.00 ±0.52 (%) for LAION models, much closer to 0"
- Break condition: If CLIP models have genuinely different effective robustness that is not captured by the linear relationship, the multi-ID approach might underestimate their true gains

### Mechanism 3
- Claim: Multi-ID evaluation improves the accuracy of OOD accuracy prediction through better fitting quality (higher R², lower MAE)
- Mechanism: By using multiple ID test sets, the regression model captures more variance in the data, resulting in a better fit to the OOD accuracy and more precise effective robustness estimates
- Core assumption: The additional ID test sets provide complementary information that improves the regression model's predictive power
- Evidence anchors:
  - [section] "Compared to single-ID evaluation, our multi-ID evaluation achieves a better fitting quality and predicts the OOD accuracy from the ID accuracies more precisely (higher R2 and lower MAE)"
  - [section] "Consistent with results in Section 5.3, our multi-ID evaluation improves the fitting quality over the single-ID evaluation to better predict and understand the OOD accuracies from ID accuracies"
- Break condition: If the additional ID test sets are highly correlated or redundant, they may not improve the fitting quality and could even introduce noise

## Foundational Learning

- Concept: Linear regression and logit transformation
  - Why needed here: The core method relies on fitting a linear plane to predict OOD accuracy from multiple ID accuracies in logit space
  - Quick check question: Why do we apply a logit transformation to the accuracies before performing linear regression?

- Concept: Effective robustness and its limitations
  - Why needed here: Understanding the concept of effective robustness and why single-ID evaluation is problematic when comparing models with different training data is crucial
  - Quick check question: What is the main limitation of using a single ID test set when comparing models trained on different datasets?

- Concept: Distribution shifts and robustness evaluation
  - Why needed here: The paper focuses on natural distribution shifts and evaluating model robustness under these shifts
  - Quick check question: What is the difference between in-distribution (ID) accuracy and out-of-distribution (OOD) accuracy?

## Architecture Onboarding

- Component map:
  Data preparation -> Model training -> Evaluation -> Visualization

- Critical path:
  1. Prepare diverse training data and labeled test sets
  2. Train models with various architectures and data distributions
  3. Compute ID and OOD accuracies for all models
  4. Perform multi-dimensional linear regression on baseline models
  5. Calculate effective robustness for all models
  6. Visualize results and compare with single-ID evaluation

- Design tradeoffs:
  - Using more ID test sets improves evaluation precision but increases computational cost and data requirements
  - Combining datasets at various ratios creates diverse training distributions but may introduce distribution shift artifacts
  - Automatically generating labels from image-text pairs is efficient but may be less accurate than human labeling

- Failure signatures:
  - Low R2 values in regression indicate poor linear relationship between ID accuracies and OOD accuracy
  - High variance in effective robustness values suggests the evaluation is not precise
  - Contradictory conclusions between single-ID and multi-ID evaluation indicate potential bias in single-ID approach

- First 3 experiments:
  1. Compare single-ID vs multi-ID effective robustness on a small set of models (e.g., ResNet-18 on CIFAR-10 and ImageNet)
  2. Evaluate the impact of adding a third ID test set on fitting quality and effective robustness estimates
  3. Test the method on models with more significant distribution shift (e.g., models trained on completely different datasets)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the multi-ID effective robustness method be generalized to compare models trained on more than two datasets?
- Basis in paper: [explicit] The paper states "Our method can be generalizable to more than two datasets in principal, by defining a baseline function based on multiple ID accuracies acc1(·),..., acck(·)." However, the authors note this could be costly and leave it as future work.
- Why unresolved: The paper does not explore the practical implementation or efficiency of this generalization
- What evidence would resolve it: Experiments demonstrating the method's effectiveness and efficiency when comparing models trained on three or more datasets

### Open Question 2
- Question: Does the training data distribution significantly impact the evaluation of effective robustness when using multiple ID test sets?
- Basis in paper: [inferred] The paper discusses the limitation of using a single ID test set when models are trained on different datasets and proposes using multiple ID test sets to provide a more precise estimate of effective robustness. However, it does not explore if the training data distribution itself could still impact the evaluation
- Why unresolved: The paper focuses on the number of ID test sets rather than the diversity or distribution of the training data
- What evidence would resolve it: Experiments comparing the effective robustness of models trained on datasets with similar vs. different distributions, using the proposed multi-ID method

### Open Question 3
- Question: How does the quality of automatically generated ID test sets compare to human-labeled test sets in evaluating effective robustness?
- Basis in paper: [explicit] The paper mentions that for datasets without original labels, they automatically generate class labels by matching captions with ImageNet classes. They acknowledge that human labeling could yield higher-quality test sets but show that automatically labeled test sets can already produce reasonable results
- Why unresolved: The paper does not provide a direct comparison between the quality of automatically generated and human-labeled test sets
- What evidence would resolve it: A study comparing the effective robustness evaluation results using automatically generated ID test sets versus human-labeled test sets for the same models

## Limitations

- The method's effectiveness depends on having representative ID test sets for all training distributions, which may not be available for all datasets
- The linear assumption in logit space may not hold for all types of distribution shifts, particularly adversarial perturbations or synthetic corruptions
- The approach increases computational cost and data requirements compared to single-ID evaluation

## Confidence

- High confidence in the core observation that single-ID evaluation biases effective robustness estimates when comparing models with different training data
- Medium confidence in the generalizability of the multi-ID approach to other domains beyond natural distribution shifts
- Medium confidence in the claim that CLIP models' apparent robustness gains are primarily artifacts of single-ID evaluation

## Next Checks

1. Test the multi-ID evaluation approach on synthetic distribution shifts (e.g., noise, blur, adversarial examples) to assess its applicability beyond natural distribution shifts
2. Evaluate the method on a more diverse set of architectures and training paradigms to verify robustness of the approach across different model families
3. Conduct ablation studies to determine the minimum number of ID test sets needed for reliable effective robustness estimates and identify which combinations provide the most complementary information