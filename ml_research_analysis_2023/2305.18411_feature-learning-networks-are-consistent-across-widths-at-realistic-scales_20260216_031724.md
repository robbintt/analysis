---
ver: rpa2
title: Feature-Learning Networks Are Consistent Across Widths At Realistic Scales
arxiv_id: '2305.18411'
source_url: https://arxiv.org/abs/2305.18411
tags:
- networks
- widths
- learning
- training
- width
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We study the effect of width on the dynamics of feature-learning
  neural networks across a variety of architectures and datasets. Early in training,
  wide neural networks trained on online data have not only identical loss curves
  but also agree in their point-wise test predictions throughout training.
---

# Feature-Learning Networks Are Consistent Across Widths At Realistic Scales

## Quick Facts
- arXiv ID: 2305.18411
- Source URL: https://arxiv.org/abs/2305.18411
- Reference count: 40
- Key outcome: Early in training, wide neural networks trained on online data have identical loss curves and point-wise test predictions across widths, with consistency holding throughout training for simple tasks like CIFAR-5m.

## Executive Summary
This paper investigates how network width affects the training dynamics of feature-learning neural networks across various architectures and datasets. The authors demonstrate that early in training, wide neural networks exhibit identical loss curves and consistent point-wise test predictions across different widths. For simple tasks like CIFAR-5m, this width consistency persists throughout training for networks of realistic widths. The study also shows that structural properties such as internal representations, preactivation distributions, edge-of-stability phenomena, and large learning rate effects remain consistent across large widths. However, for harder tasks like ImageNet and language modeling, finite-width deviations grow systematically over time.

## Method Summary
The authors conduct experiments across multiple architectures (ResNet, MLP, Transformer) and datasets (CIFAR-5m, ImageNet, Wikitext-103, C4) using online training with SGD or Adam optimizers. Networks are parameterized using maximal update parameterization (µP) and trained with varying widths. The study measures consistency across widths through loss curves, point-wise predictions, feature kernels, preactivation distributions, and dynamical phenomena. Ensembling experiments are performed to study variance reduction effects.

## Key Results
- Early in training, wide neural networks exhibit identical loss curves and point-wise test predictions across different widths
- For simple tasks like CIFAR-5m, width consistency holds throughout training for networks of realistic widths
- Finite-width deviations for harder tasks arise from both initialization-dependent variance (scaling as 1/N) and spectral bias in narrower networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Width consistency emerges because preactivation distributions and feature kernels become deterministic at large width
- Mechanism: At infinite width, neurons behave as independent draws from a single-site distribution; finite-width deviations are O(1/√N) fluctuations around this deterministic limit
- Core assumption: The mean-field theory of neural networks applies, i.e., the infinite-width limit is well-defined and captures the essential dynamics of finite-width networks
- Evidence anchors: Internal representations and preactivation distributions become consistent across widths; supported by mean-field theory framework
- Break condition: If finite-width corrections accumulate over time or if the dataset complexity exceeds the width-dependent threshold for consistency

### Mechanism 2
- Claim: The variance of network outputs due to initialization scales inversely with width
- Mechanism: Initialization-dependent variance in the learned function is O(1/N), so ensembling reduces variance while bias remains the dominant source of discrepancy from infinite-width behavior
- Core assumption: The variance term dominates finite-width effects early in training, and ensembling effectively reduces this variance
- Evidence anchors: Network outputs have initialization-dependent variance scaling inversely with width; ensembling reduces this variance
- Break condition: If bias becomes comparable to or larger than variance at narrow widths, or if ensembling does not sufficiently reduce variance

### Mechanism 3
- Claim: The spectral decomposition of the neural tangent kernel explains finite-width bias
- Mechanism: Narrower networks put more of the task power into higher spectral modes of the NTK, leading to slower learning and larger bias; wider networks align better with lower modes
- Core assumption: The task can be decomposed into eigenfunctions of the NTK, and the alignment of these eigenfunctions with the target function determines learning speed
- Evidence anchors: Spectral perspective on finite-width bias; narrower networks spread task power into slower modes
- Break condition: If the NTK spectrum changes significantly with width or if the task alignment cannot be captured by the spectral decomposition

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: The NTK describes the training dynamics of infinitely wide networks and serves as a reference point for understanding finite-width deviations
  - Quick check question: What is the relationship between the NTK and the feature learning capability of a neural network?

- Concept: Mean-field theory of neural networks
  - Why needed here: Mean-field theory provides the framework for understanding the infinite-width limit and the finite-width corrections to network dynamics
  - Quick check question: How does the mean-field approximation simplify the analysis of neural network training dynamics?

- Concept: Feature learning vs. lazy training
  - Why needed here: Understanding the distinction between feature learning and lazy training is crucial for interpreting the results on width consistency and the role of the µP parameterization
  - Quick check question: What is the key difference between feature learning and lazy training in the context of neural network training?

## Architecture Onboarding

- Component map: Input layer -> Hidden layers (feature learning) -> Output layer -> Loss function -> Optimizer (SGD/Adam) -> Weight updates

- Critical path: 1. Data preprocessing and augmentation 2. Forward pass through the network to compute predictions 3. Calculation of the loss function 4. Backward pass to compute gradients 5. Weight update using the optimizer 6. Repeat steps 2-5 for multiple epochs or until convergence

- Design tradeoffs:
  - Width vs. depth: Wider networks may achieve better width consistency but require more computational resources
  - Learning rate scheduling: Different learning rate schedules may affect the consistency of loss curves across widths
  - Batch size: Larger batch sizes may improve the stability of training dynamics but require more memory

- Failure signatures: Inconsistent loss curves across widths, large variance in network outputs due to initialization, slow convergence or poor generalization performance

- First 3 experiments:
  1. Train a ResNet on CIFAR-5m with varying widths and compare loss curves and predictions
  2. Analyze the preactivation distributions and feature kernels across different widths to assess consistency
  3. Perform ensembling experiments to study the effect of variance reduction on width consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise scaling relationship between the finite-width bias and the complexity of the learning task?
- Basis in paper: The paper notes that "harder tasks induce larger bias gaps" but does not quantify this relationship
- Why unresolved: The paper does not provide a mathematical framework to predict the bias based on task complexity metrics
- What evidence would resolve it: Experiments systematically varying task complexity (e.g., input dimension, number of classes, noise level) while measuring the bias across widths

### Open Question 2
- Question: How does the finite-width bias manifest in the loss landscape geometry of realistic neural networks?
- Basis in paper: The paper discusses convergence of loss curves and predictions but does not analyze the underlying loss landscape structure
- Why unresolved: The paper focuses on observable quantities (loss, predictions, kernels) rather than the geometric properties of the loss surface
- What evidence would resolve it: Analysis of loss landscape curvature, barrier heights, and basin geometry across widths, potentially using tools like Hessian analysis or barrier finding algorithms

### Open Question 3
- Question: What is the mechanism by which finite-width corrections deform the eigenfunctions of the neural tangent kernel?
- Basis in paper: The paper shows that "finite width corrections do not substantially effect the spectrum but spread out target function power into slower modes in narrower networks"
- Why unresolved: The paper identifies the effect but does not provide a theoretical explanation for why this spectral redistribution occurs
- What evidence would resolve it: A theoretical framework connecting finite-width fluctuations in the NTK to changes in the eigenfunction structure, potentially through perturbation theory or random matrix theory

## Limitations

- Width consistency breaks down for harder tasks like ImageNet and language modeling, with finite-width deviations growing systematically over time
- The spectral decomposition mechanism explaining finite-width bias is described but lacks comprehensive empirical validation
- Evidence for internal representations and dynamical phenomena consistency is primarily based on CIFAR-5m results, with weaker support for more challenging datasets

## Confidence

- High confidence: Claims about variance scaling inversely with width and the effectiveness of ensembling to reduce this variance
- Medium confidence: Claims about width consistency on CIFAR-5m and simple tasks throughout training
- Low confidence: Claims about spectral decomposition explaining finite-width bias and the general applicability of infinite-width limits to realistic models

## Next Checks

1. **Spectral Analysis Validation**: Replicate the spectral decomposition analysis on CIFAR-5m with varying widths to confirm that narrower networks indeed concentrate task power in higher NTK modes, and test whether this pattern holds across different architectures.

2. **Ensemble Performance Gap**: Systematically measure the performance gap between ensembles of narrow networks versus single wide networks across a broader range of widths and datasets to better understand the "bias of narrower width" phenomenon.

3. **Cross-Dataset Consistency**: Test width consistency claims on additional challenging datasets (e.g., CIFAR-10/100, language modeling tasks) with multiple architectures to determine the boundaries of where width consistency breaks down and whether the identified mechanisms explain these failures.