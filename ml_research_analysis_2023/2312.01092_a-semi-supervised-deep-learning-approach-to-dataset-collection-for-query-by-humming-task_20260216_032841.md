---
ver: rpa2
title: A Semi-Supervised Deep Learning Approach to Dataset Collection for Query-By-Humming
  Task
arxiv_id: '2312.01092'
source_url: https://arxiv.org/abs/2312.01092
tags:
- fragments
- song
- cover
- dataset
- songs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training Query-by-Humming
  (QbH) systems, which traditionally struggle due to the lack of large, high-quality
  datasets. The authors propose a semi-supervised deep learning approach to dataset
  collection, introducing the Covers and Hummings Aligned Dataset (CHAD).
---

# A Semi-Supervised Deep Learning Approach to Dataset Collection for Query-By-Humming Task

## Quick Facts
- arXiv ID: 2312.01092
- Source URL: https://arxiv.org/abs/2312.01092
- Authors: 
- Reference count: 0
- One-line primary result: Semi-supervised approach expands humming dataset from 18h to 308h, achieving competitive QbH performance on benchmark datasets.

## Executive Summary
This paper addresses the challenge of training Query-by-Humming (QbH) systems, which traditionally struggle due to the lack of large, high-quality datasets. The authors propose a semi-supervised deep learning approach to dataset collection, introducing the Covers and Hummings Aligned Dataset (CHAD). CHAD contains 18 hours of time-aligned music fragments paired with hummed versions. Using a semi-supervised training pipeline, the authors iteratively expand the dataset to over 308 hours by leveraging cover song identification (CSI) as a related task. The final model, trained on this expanded dataset, achieves competitive results on benchmark QbH datasets, demonstrating the effectiveness of their approach in facilitating QbH system implementation.

## Method Summary
The authors introduce a semi-supervised deep learning approach to expand the Query-by-Humming (QbH) dataset. They start with a small labeled dataset (CHAD) containing 18 hours of time-aligned music fragments and hummed versions. Using a semi-supervised training pipeline that leverages cover song identification (CSI) as a related task, they iteratively expand the dataset to over 308 hours. The method involves extracting vocal parts from songs, identifying cover song groups, and aligning fragments based on melodic similarity using correlation matrices. The expanded dataset is then used to train a deep learning model for matching audio fragments with similar melodies using metric learning.

## Key Results
- The semi-supervised approach expands the QbH dataset from 18 hours to over 308 hours.
- The final model achieves competitive results on benchmark QbH datasets, including MIREX, MTG-QBH, and DB90K.
- The use of cover song identification as a surrogate task enables effective dataset expansion without manual annotation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semi-supervised training with cover song identification as a surrogate task allows expansion of labeled data without manual annotation.
- Mechanism: The system first trains on a small labeled humming dataset (H). Then it uses the trained model to identify cover song groups in a large unlabeled corpus (C). Time-aligned fragments are extracted from these groups and added back to the training set, iteratively improving the model.
- Core assumption: Cover song identification is a sufficiently similar task to QbH that features learned on one transfer well to the other.
- Evidence anchors:
  - [abstract] "we employ a semi-supervised model training pipeline that leverages the QbH task as a specialized case of cover song identification (CSI) task."
  - [section 4.1] "We use this dataset to train our deep learning model for matching audio fragments with similar melodies using metric learning paradigm. We demonstrate that these techniques can also be successfully applied in the QbH task..."
- Break condition: If cover song groups contain significant structural differences (e.g., tempo, instrumentation) that are not relevant to humming similarity, the learned embeddings may not generalize to QbH queries.

### Mechanism 2
- Claim: Time-alignment extraction algorithm maximizes unique vocal fragments while preserving melodic content for metric learning.
- Mechanism: Algorithm 1 iterates over different silence detection thresholds and pause lengths to segment original songs into fragments, filters out highly correlated duplicates, and then finds matching fragments in cover songs using cross-correlation. Only fragments with correlation above βrel are retained.
- Core assumption: Vocal segments in cover songs contain sufficient melodic similarity to original fragments to be useful for training, even with variations in arrangement or performance.
- Evidence anchors:
  - [section 4.1] "the algorithm takes the vocal part of the original song yo and a group of cover songs Y c... finds the best combination of Dp and Ldb to yield Fo of maximal size."
  - [section 4.3] "the number of covers is limited, as there are usually fewer cover versions for non-popular songs."
- Break condition: If silence detection parameters are poorly chosen, either too many or too few fragments are extracted, degrading the quality of training data.

### Mechanism 3
- Claim: Metric learning with multi-similarity mining and NT-XENT loss enables fine-grained melodic similarity matching across versions.
- Mechanism: The model encodes vocal fragments into embeddings using ResNet18, then applies NT-XENT loss over groups of time-aligned fragments (positive pairs) while pushing apart embeddings from different groups (negative pairs). Multi-similarity mining selects informative pairs for training.
- Core assumption: Melodic content is preserved sufficiently in embeddings to allow cosine similarity to distinguish same-song fragments from different-song fragments.
- Evidence anchors:
  - [section 3] "Our loss is defined as follows: ℓ = −KXk=1Xzi k,zj k∈Zk log exp(sim(zi k,zj k)τ )Pzl̸∈Zk exp(sim(zi k,zl)τ )"
  - [section 5.1] "We employed the Multi-similarity miner [28] and an adaptive batch sampler to improve convergence speed."
- Break condition: If embeddings collapse or become too spread, similarity scores lose discriminative power, causing retrieval failures.

## Foundational Learning

- Concept: Metric learning and contrastive loss functions
  - Why needed here: The task requires learning a similarity space where fragments from the same song are close and from different songs are far apart.
  - Quick check question: What is the difference between triplet loss and NT-XENT loss in terms of positive/negative pair sampling?

- Concept: Audio feature extraction (CQT, f0/melody)
  - Why needed here: The model needs compact, noise-robust representations of melodic content to match hummed queries to original songs.
  - Quick check question: Why might CQT be preferred over raw waveforms for this task?

- Concept: Semi-supervised learning via self-training
  - Why needed here: Manual annotation of humming data is expensive; using cover songs as a proxy task enables large-scale data expansion.
  - Quick check question: How does the iterative retraining loop in Algorithm 1 differ from standard self-training?

## Architecture Onboarding

- Component map:
  - Vocal extraction (Spleeter) -> Feature extractor (CREPE for f0 or CQT) -> Encoder (ResNet18 + L2 normalization) -> Loss module (NT-XENT + multi-similarity miner) -> Data pipeline (silence detection, fragment matching, correlation filtering)

- Critical path:
  1. Input waveform → vocal separation
  2. Feature extraction → overlapping windows
  3. Encoding → normalized embeddings
  4. Loss computation over groups
  5. Backward pass → parameter update

- Design tradeoffs:
  - f0 vs CQT: f0 is more melody-focused but slower; CQT is faster but includes more harmonic content.
  - Fragment length: shorter fragments are more common in humming queries but may lose context; longer fragments capture more melody but risk tempo mismatches.
  - Thresholds (βrel, βirrel): stricter thresholds yield cleaner data but reduce dataset size.

- Failure signatures:
  - High correlation in unrelated fragments → thresholds too loose
  - Low retrieval accuracy → embeddings not discriminative enough or model underfit
  - Slow training → feature extraction bottleneck or inefficient data loading

- First 3 experiments:
  1. Train on H only, evaluate on validation split of H to confirm baseline performance.
  2. Run Algorithm 1 on a small subset of C, inspect fragment quality and correlation distributions.
  3. Train on H + small C subset, compare retrieval metrics to H-only baseline to validate semi-supervised gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would generative networks be in overcoming the limitations of the semi-supervised pipeline for extracting instrumental segments from songs?
- Basis in paper: [explicit] "Future research could explore using generative networks [22] to overcome these limitations."
- Why unresolved: The paper only suggests this as a potential future direction without providing any experimental results or comparative analysis.
- What evidence would resolve it: Empirical studies comparing the performance of generative networks with the current approach for extracting instrumental segments, along with a quantitative analysis of the improvement in dataset quality and model performance.

### Open Question 2
- Question: What is the impact of using different types of audio features (e.g., Mel-frequency cepstral coefficients, spectrograms) on the performance of the Query-by-Humming system?
- Basis in paper: [explicit] "We used CREPE [20] activations as f 0 features... Additionally, we downscaled this representation... To address this, we incorporated CQT features into our model..."
- Why unresolved: The paper only compares CREPE and CQT features, leaving the potential benefits of other audio features unexplored.
- What evidence would resolve it: Systematic experiments comparing the performance of the Query-by-Humming system using various audio features, including Mel-frequency cepstral coefficients, spectrograms, and other relevant representations.

### Open Question 3
- Question: How does the proposed semi-supervised approach compare to fully supervised methods in terms of performance and data efficiency for the Query-by-Humming task?
- Basis in paper: [inferred] The paper proposes a semi-supervised approach due to the lack of large, high-quality datasets for training QbH systems, implying that fully supervised methods may not be feasible.
- Why unresolved: The paper does not provide a direct comparison between the semi-supervised approach and fully supervised methods, as the latter may not be applicable due to data limitations.
- What evidence would resolve it: Comparative experiments between the semi-supervised approach and fully supervised methods (if applicable) using the same dataset and evaluation metrics, along with an analysis of data efficiency and performance trade-offs.

## Limitations

- The semi-supervised expansion mechanism relies heavily on the assumption that cover song groups provide sufficient melodic alignment for effective QbH training, with sensitivity to threshold selection not thoroughly analyzed.
- Claims about f0 outperforming CQT are based on limited ablation on one dataset, making generalization claims uncertain.
- The model comparison between Mshort and Mlong shows performance differences, but the impact of feature type (f0 vs CQT) on different query lengths is not fully explored.

## Confidence

- **High Confidence:** The core architecture (ResNet18 encoder + NT-XENT loss + multi-similarity mining) is well-established in metric learning literature and correctly implemented.
- **Medium Confidence:** The semi-supervised pipeline for dataset expansion is methodologically sound, but performance gains depend on data quality and threshold selection.
- **Low Confidence:** Claims about f0 outperforming CQT are based on limited ablation; the superiority may be dataset-specific.

## Next Checks

1. **Threshold Sensitivity Analysis:** Systematically vary βrel and βirrel to quantify their impact on fragment quality and downstream QbH performance.
2. **Cross-Dataset Ablation:** Re-run the f0 vs CQT comparison on MIREX and MTG-QBH to confirm robustness across benchmarks.
3. **Cover Song Quality Audit:** Manually inspect a random sample of aligned fragments from the expanded dataset to verify melodic consistency and identify potential noise sources.