---
ver: rpa2
title: A Corrected Expected Improvement Acquisition Function Under Noisy Observations
arxiv_id: '2310.05166'
source_url: https://arxiv.org/abs/2310.05166
tags:
- function
- acquisition
- noisy
- improvement
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a modified Expected Improvement (EI) acquisition
  function for Bayesian optimization under noisy observations. The key innovation
  is correcting the closed-form expression of EI to account for the uncertainty in
  the incumbent solution, which is typically ignored in existing methods.
---

# A Corrected Expected Improvement Acquisition Function Under Noisy Observations

## Quick Facts
- arXiv ID: 2310.05166
- Source URL: https://arxiv.org/abs/2310.05166
- Authors: 
- Reference count: 7
- One-line primary result: Corrected EI improves Bayesian optimization under noisy observations by incorporating covariance information, achieving sublinear regret bounds

## Executive Summary
This paper addresses a critical limitation in Bayesian optimization under noisy observations: the standard Expected Improvement (EI) acquisition function ignores the uncertainty in the incumbent solution. The authors propose a corrected EI formula that incorporates the covariance structure from the Gaussian Process model, generalizing the noise-free result and providing better exploration-exploitation trade-offs. The method is theoretically justified with a regret bound under heteroscedastic noise and empirically validated on benchmark functions and neural network compression tasks.

## Method Summary
The paper proposes a modified Expected Improvement (EI) acquisition function for Bayesian optimization under noisy observations. The key innovation is correcting the closed-form expression of EI to account for the uncertainty in the incumbent solution, which is typically ignored in existing methods. This correction incorporates the covariance information provided by the Gaussian Process (GP) model. The proposed method, called corrected EI, generalizes the noise-free result and should replace the classical formula in Bayesian optimization software. The authors prove a regret bound for this acquisition function under heteroscedastic observation noise, achieving a sublinear convergence rate. Empirical results demonstrate that corrected EI outperforms standard EI in the presence of noisy observations on benchmark functions and neural network model compression tasks.

## Key Results
- Corrected EI incorporates covariance between incumbent and query points, reducing local search behavior
- The method generalizes the noise-free EI result and achieves sublinear regret bounds under heteroscedastic noise
- Empirical results show improved performance on benchmark functions and neural network compression tasks compared to standard EI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Correcting EI by incorporating covariance between the incumbent and other points reduces local search behavior.
- Mechanism: In noisy settings, the best posterior mean is used as the incumbent, but its uncertainty is ignored. The corrected EI uses the full covariance matrix to adjust the improvement calculation, making points near the incumbent less attractive unless strongly justified by their own mean and variance.
- Core assumption: The Gaussian Process model accurately captures covariance structure, especially for kernels like Matérn or Squared Exponential with large length scales.
- Evidence anchors:
  - [abstract] "corrects its closed-form expression by incorporating the covariance information provided by the Gaussian Process (GP) model"
  - [section 4] "we derive an analytical expression for this acquisition function in the presence of noisy observations, which also generalizes that of the noiseless case"
  - [corpus] Weak - no direct citation supporting this mechanism; only mentions "unexpected improvements to expected improvement" without specifics
- Break condition: If kernel covariance structure is poorly estimated (e.g., wrong length scale), the correction may over- or under-explore.

### Mechanism 2
- Claim: The corrected EI generalizes the noise-free EI result, allowing seamless use in both noisy and noiseless settings.
- Mechanism: The modified variance term ˜σ²t-1(x) reduces to the noise-free case when observation noise is zero, ensuring backward compatibility and consistent behavior.
- Core assumption: The mathematical form of the corrected EI reduces exactly to the classical EI when noise vanishes.
- Evidence anchors:
  - [abstract] "This acquisition function specializes to the classical noise-free result"
  - [section 4.1] "When ˜σt-1(x) = 0, we set αC t(x) = 0. In the noiseless case where ∀x ≠ x+t, σt-1(x+t) = σt-1(xx+t) = 0, we recover the expression for EI"
  - [corpus] Weak - no corpus evidence directly confirming generalization property
- Break condition: Numerical instability when ˜σt-1(x) is very small but non-zero could cause deviations from ideal behavior.

### Mechanism 3
- Claim: The corrected EI achieves sublinear regret bounds under heteroscedastic noise similar to standard EI.
- Mechanism: By properly accounting for the uncertainty in the incumbent solution, the theoretical analysis shows the cumulative regret grows sublinearly with iteration count, matching the convergence rate of standard EI.
- Core assumption: The heteroscedastic noise model and GP assumptions (bounded kernel, smooth function) hold.
- Evidence anchors:
  - [abstract] "We show that our method achieves a sublinear convergence rate on the cumulative regret bound under heteroscedastic observation noise"
  - [section 4.3.3] "when choosing a squared exponential kernel for the GP model, we achieve a sublinear rate RT ~ O(√T (log T)d+4)"
  - [corpus] Weak - no direct citations supporting the regret bound claim in related works
- Break condition: If noise variance estimates are inaccurate or the kernel assumptions are violated, the regret bound may not hold.

## Foundational Learning

- Concept: Gaussian Process regression and covariance kernels
  - Why needed here: The correction relies on accurate covariance estimates between points, especially the incumbent and query points
  - Quick check question: How does the choice of kernel (e.g., Matérn vs Squared Exponential) affect the covariance structure used in the corrected EI?

- Concept: Expected Improvement acquisition function derivation
  - Why needed here: Understanding the standard EI formula is essential to see how the correction modifies it
  - Quick check question: What is the closed-form expression for standard EI, and how does it differ from the corrected version?

- Concept: Heteroscedastic noise modeling
  - Why needed here: The theoretical analysis assumes noise variance varies across input space, which is more realistic than homoscedastic noise
  - Quick check question: How does the heteroscedastic noise model differ from homoscedastic in GP regression, and why is it important for the corrected EI?

## Architecture Onboarding

- Component map: GP model fitting -> Mean and covariance prediction -> Corrected EI calculation -> Optimization of acquisition function -> Regret monitoring
- Critical path:
  1. Fit GP model with noisy observations and noise variances
  2. Compute predictive mean and variance at candidate points
  3. Calculate covariance between candidate and incumbent
  4. Apply corrected EI formula
  5. Optimize acquisition to select next evaluation point
  6. Update GP model with new observation

- Design tradeoffs:
  - Accuracy vs. computational cost: Corrected EI requires full covariance matrix computation, which is more expensive than standard EI
  - Model complexity: Heteroscedastic noise model adds parameters but better captures real-world noise patterns
  - Exploration vs. exploitation: The correction may promote more global search, potentially at the cost of slower local refinement

- Failure signatures:
  - Poor performance on functions with isotropic noise or when kernel length scale is small (covariance between points is negligible)
  - Numerical instability when ˜σt-1(x) approaches zero
  - Over-exploration in early iterations if κ threshold is set too high

- First 3 experiments:
  1. Compare corrected EI vs. standard EI on a simple 1D function with known noise structure, varying noise levels
  2. Test performance on benchmark functions (e.g., Hartmann3d, Griewank(d=6), Levy(d=4), Powell(d=5)) with different kernel length scales
  3. Evaluate compression task performance on pre-trained neural networks with varying noise levels in objective evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed corrected EI method compare to other state-of-the-art acquisition functions like Predictive Entropy Search (PES) or Upper Confidence Bound (UCB) in high-dimensional spaces (d > 10)?
- Basis in paper: [inferred] The paper only tests on benchmark functions with d up to 6 and mentions that differences in performance among acquisitions might be negligible for functions in high dimensions.
- Why unresolved: The authors only tested on benchmark functions with relatively low dimensionality. High-dimensional spaces present unique challenges for Bayesian optimization that were not explored.
- What evidence would resolve it: Empirical results comparing corrected EI to PES, UCB, and other methods on benchmark functions with d > 10, and on real-world high-dimensional problems like hyperparameter tuning for deep learning models.

### Open Question 2
- Question: Can the regret bound for corrected EI be improved by using more sophisticated kernels or by incorporating additional prior information about the objective function?
- Basis in paper: [explicit] The paper derives a regret bound for corrected EI under the squared exponential kernel, achieving a sublinear convergence rate. However, the bound relies on the choice of kernel and the authors mention that the regret bound is equivalent to that of standard EI.
- Why unresolved: The regret bound is only proven for the squared exponential kernel, and it's unclear if more advanced kernels or prior information could lead to tighter bounds.
- What evidence would resolve it: Theoretical analysis proving regret bounds for corrected EI under different kernels, or empirical results showing improved performance with more sophisticated kernels on problems where the objective function has specific structure.

### Open Question 3
- Question: How does the performance of corrected EI scale with increasing levels of observation noise beyond 20% of the objective function range?
- Basis in paper: [explicit] The paper tests corrected EI on benchmark functions with noise levels up to 20% of the objective function range, and shows that it outperforms standard EI in this regime.
- Why unresolved: The paper does not explore the behavior of corrected EI under extremely high noise levels. It's unclear if the method remains effective when the noise dominates the signal.
- What evidence would resolve it: Empirical results testing corrected EI on benchmark functions with noise levels exceeding 20% of the objective function range, and theoretical analysis of the method's robustness to noise.

## Limitations

- The empirical evaluation is limited to only 4 benchmark functions, which may not be representative of all optimization problems
- The regret bound analysis is only proven for the squared exponential kernel, leaving open the question of performance under other kernels
- The method's effectiveness under extremely high noise levels (beyond 20% of objective range) remains untested

## Confidence

- Mechanism 1 (local search reduction): Low
- Mechanism 2 (generalization property): Medium
- Mechanism 3 (regret bound): Low
- Empirical performance claims: Medium

## Next Checks

1. Implement and compare against "Unexpected Improvements to Expected Improvement" to validate the local search mechanism claim
2. Test the corrected EI on additional benchmark functions (at least 10) with varying noise levels and kernel parameters
3. Verify the numerical stability of the corrected formula when ˜σt-1(x) approaches zero through controlled experiments