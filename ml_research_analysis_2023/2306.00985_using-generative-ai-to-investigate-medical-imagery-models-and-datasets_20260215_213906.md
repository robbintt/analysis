---
ver: rpa2
title: Using generative AI to investigate medical imagery models and datasets
arxiv_id: '2306.00985'
source_url: https://arxiv.org/abs/2306.00985
tags:
- page
- race
- panel
- what
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to automatically explain what visual
  attributes a classifier has learned in medical imaging. The approach uses a StyleGAN-based
  generator to create counterfactual visualizations that show what the classifier
  is sensitive to, then presents these to an interdisciplinary panel of experts to
  interpret and form hypotheses.
---

# Using generative AI to investigate medical imagery models and datasets

## Quick Facts
- arXiv ID: 2306.00985
- Source URL: https://arxiv.org/abs/2306.00985
- Reference count: 40
- This paper presents a method to automatically explain what visual attributes a classifier has learned in medical imaging using generative AI to create counterfactual visualizations.

## Executive Summary
This paper introduces a novel approach to understanding what visual attributes medical imaging classifiers have learned by using a StyleGAN-based generator to create counterfactual visualizations. The method combines classifier-guided GAN training with interdisciplinary expert interpretation to identify clinically known features, confounders from factors beyond physiology, and novel attributes. Applied across eight prediction tasks in three imaging modalities, the approach reveals important insights about model biases and generates hypotheses about structural determinants of health. The method represents a significant advance in explainable AI for medical imaging, moving beyond traditional feature importance methods to provide interpretable visualizations of classifier behavior.

## Method Summary
The method trains a classifier on a medical imaging task, then trains a StyleGAN2-based generator guided by the classifier to create realistic counterfactual images. The generator is trained with an encoder to reconstruct input images while maintaining classifier probability through a cross-entropy loss. Visual attributes are extracted from the StyleSpace of the generator, and the top attributes affecting classifier decisions are identified. These attributes are then used to generate counterfactual visualizations that show how modifying each attribute changes the classification. Finally, an interdisciplinary panel of experts interprets these visualizations to identify potential biases and generate research hypotheses, considering both clinical and social determinants of health.

## Key Results
- The method uncovered attributes capturing clinically known features, confounders from factors beyond physiology, and novel attributes across three imaging modalities
- Expert panel interpretation identified areas of bias and generated research hypotheses using socio-ecological theory and structural determinants of health framework
- Code will be released to enable others to apply the method to their own medical imaging datasets and tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The StyleGAN-based generator can disentangle fine-grained visual attributes that affect classifier decisions.
- Mechanism: By training a StyleGAN2-like generator with a reconstruction loss and a cross-entropy loss on the classifier, the generator is forced to represent visual attributes important for classification in its StyleSpace. This disentangled latent space allows individual attributes to be manipulated to visualize their effect on classification.
- Core assumption: The StyleSpace of StyleGAN2 contains semantically meaningful attributes that can be linearly mapped to classifier-relevant features.
- Evidence anchors:
  - [abstract] "Train a StyleGAN-based image generator with an architecture that enables guidance by the classifier ("StylEx")"
  - [section] "StylEx relies on the fact that StyleGAN2 contains a disentangled latent space called "StyleSpace" which contains semantically meaningful attributes of the images."
- Break condition: If the StyleSpace does not contain interpretable attributes relevant to the classifier task, or if the reconstruction loss fails to preserve classifier-relevant details.

### Mechanism 2
- Claim: The interdisciplinary expert panel can identify potential biases and generate research hypotheses from the attribute visualizations.
- Mechanism: By presenting counterfactual visualizations of the top attributes to a diverse panel of experts (clinical, social science, human factors, ML engineers), the panel can critically assess whether attributes represent known clinical features, confounders from factors beyond physiology, or novel associations. This collaborative interpretation helps identify areas of bias and generates testable hypotheses.
- Core assumption: Expert interpretation of attribute visualizations can reveal social, structural, and environmental factors that may confound the model predictions.
- Evidence anchors:
  - [abstract] "Specifically, present the discovered attributes and corresponding counterfactual visualizations to an interdisciplinary panel of experts so that hypotheses can account for social and structural determinants of health"
- Break condition: If the expert panel cannot reach consensus on attribute interpretation, or if panel biases prevent identification of confounding factors.

### Mechanism 3
- Claim: The classifier-guided GAN training ensures generated images preserve the decision-relevant visual details for the specific classification task.
- Mechanism: By adding a cross-entropy classification loss to the GAN training, the generator is encouraged to produce images that maintain the same classifier probability as the input image. This ensures subtle visual details important for the classifier (such as those used to identify medical conditions) are included in the generated image.
- Core assumption: The classifier can provide meaningful guidance to the generator about which visual features are important for the classification task.
- Evidence anchors:
  - [section] "We further add a cross-entropy classification-loss to the GAN training, which forces the classifier probability of the generated image to be the same as the classifier probability of the input image."
- Break condition: If the classifier guidance leads the generator to emphasize spurious correlations rather than true task-relevant features.

## Foundational Learning

- Concept: Disentangled latent spaces in generative models
  - Why needed here: The method relies on manipulating individual attributes in a disentangled latent space to visualize their effect on classification.
  - Quick check question: What is the difference between entangled and disentangled representations in generative models?

- Concept: Counterfactual explanations in machine learning
  - Why needed here: The method generates counterfactual visualizations by modifying specific attributes to show how they affect classification.
  - Quick check question: How do counterfactual explanations differ from traditional feature importance methods?

- Concept: Social determinants of health and structural bias
  - Why needed here: Expert interpretation of attributes must consider social, environmental, and structural factors that may confound model predictions.
  - Quick check question: What are some examples of structural determinants of health that could affect medical imaging datasets?

## Architecture Onboarding

- Component map: Classifier (C) -> StyleGAN2-based generator (G) -> Encoder (E) -> Expert panel

- Critical path:
  1. Train classifier on medical imaging task
  2. Train StyleGAN2-based generator with classifier guidance
  3. Extract top attributes that affect classifier decisions
  4. Generate counterfactual visualizations
  5. Present to expert panel for interpretation

- Design tradeoffs:
  - Using a classifier-guided GAN vs. traditional explainability methods (saliency maps)
  - Balancing reconstruction quality with classifier guidance during training
  - Selecting appropriate confounding variables to include in multi-headed classifier

- Failure signatures:
  - Classifier performance too low to extract meaningful attributes
  - Generated images fail to preserve classifier-relevant details
  - Expert panel cannot interpret attribute visualizations
  - Attributes reveal only known clinical features with no novel insights

- First 3 experiments:
  1. Train classifier on a simple medical imaging task (e.g., cataract detection) and verify performance meets threshold
  2. Train StyleGAN2-based generator with classifier guidance and generate sample counterfactuals
  3. Apply attribute extraction method to a small set of images and visualize top attributes for manual inspection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we determine whether the observed associations between race and skeletal conspicuity in chest X-rays are due to biological differences or social/environmental factors?
- Basis in paper: [explicit] The paper discusses that increased bone mineral density may explain the association, but acknowledges it cannot conclude whether this is biological or due to environmental exposures, nutrition, or structural artifacts.
- Why unresolved: The paper highlights that race is socially, not biologically, defined and that there is greater genetic variation within racial groups than between them. It also notes that environmental exposures and structural artifacts are not measured in the dataset.
- What evidence would resolve it: Studies measuring environmental exposures, nutrition, and structural artifacts alongside bone density in diverse populations could help disentangle biological from social factors.

### Open Question 2
- Question: What specific mechanisms link eyelid margin pallor to elevated HbA1c levels?
- Basis in paper: [explicit] The paper suggests this may be a manifestation of meibomian gland disease, which is associated with diabetes, but notes the mechanism is not well understood.
- Why unresolved: While the association is observed, the paper acknowledges that the specific pathophysiological mechanisms connecting meibomian gland dysfunction to elevated blood sugar are unclear.
- What evidence would resolve it: Detailed studies examining meibomian gland morphology, function, and biochemical markers in diabetic and non-diabetic populations could clarify the underlying mechanisms.

### Open Question 3
- Question: How can we mitigate the impact of confounding variables like sex and age on model predictions in medical imaging?
- Basis in paper: [explicit] The paper identifies several attributes that appear to be confounded by sex and age, such as eyeliner thickness correlating with low hemoglobin and choroid vasculature visibility correlating with sex.
- Why unresolved: The paper demonstrates that current models pick up on these confounders but does not provide a definitive solution for mitigating their impact on predictions.
- What evidence would resolve it: Development and validation of techniques to explicitly account for and remove the influence of known confounders during model training and evaluation would help address this issue.

## Limitations
- The method relies heavily on the assumption that StyleGAN2's StyleSpace contains semantically meaningful attributes relevant to the classification task, which has not been explicitly validated for the specific medical imaging domains examined
- The approach was validated across eight tasks but only three imaging modalities, which may not be sufficient to generalize to all medical imaging applications
- The expert panel composition, while interdisciplinary, may still have inherent biases that could affect the interpretation of attributes

## Confidence

**High Confidence**: The core mechanism of using a classifier-guided StyleGAN to generate counterfactual visualizations is well-established and technically sound. The mathematical formulation of the training objective (combining reconstruction loss with classifier guidance) is clearly specified.

**Medium Confidence**: The claim that this method can uncover both clinically known features and novel attributes is supported by the results, but the interpretation of what constitutes "novel" versus "confounding" attributes relies heavily on expert judgment, which introduces uncertainty.

**Low Confidence**: The paper's assertion that the method can reliably identify structural determinants of health through visual attribute analysis is the most speculative claim. While the expert panel framework is sound, the connection between visual attributes and social/structural factors is indirect and requires further validation.

## Next Checks
1. **Quantitative Validation of Attribute Interpretability**: Conduct a controlled study where the same attribute visualizations are presented to multiple expert panels to measure inter-rater reliability and quantify the subjectivity in attribute interpretation.

2. **Cross-Modality Generalization Test**: Apply the method to a new imaging modality (e.g., MRI or ultrasound) not covered in the original study to assess whether the approach generalizes beyond the three modalities examined.

3. **Temporal Stability Analysis**: Train classifiers on medical images from different time periods and apply the method to determine whether the extracted attributes remain stable or change over time, which would indicate whether they capture persistent features or temporal artifacts.