---
ver: rpa2
title: Prediction and Control in Continual Reinforcement Learning
arxiv_id: '2312.11669'
source_url: https://arxiv.org/abs/2312.11669
tags:
- value
- function
- learning
- task
- eseeds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel approach to continual reinforcement
  learning inspired by the complementary learning systems (CLS) theory from neuroscience.
  The authors propose decomposing the value function into two components: a permanent
  value function that slowly accumulates general knowledge, and a transient value
  function that rapidly adapts to new situations.'
---

# Prediction and Control in Continual Reinforcement Learning

## Quick Facts
- arXiv ID: 2312.11669
- Source URL: https://arxiv.org/abs/2312.11669
- Reference count: 40
- Primary result: Decomposes value function into permanent (slow) and transient (fast) components inspired by neuroscience, achieving better stability-plasticity balance in continual RL

## Executive Summary
This paper introduces a novel approach to continual reinforcement learning inspired by the complementary learning systems (CLS) theory from neuroscience. The authors propose decomposing the value function into two components: a permanent value function that slowly accumulates general knowledge, and a transient value function that rapidly adapts to new situations. This decomposition allows for better balance between stability and plasticity, addressing the stability-plasticity dilemma in continual learning. The authors provide theoretical results and empirical case studies demonstrating the effectiveness of their approach in both prediction and control problems.

## Method Summary
The method decomposes the value function into permanent and transient components that update at different timescales. The permanent component (V(P)) learns slowly over the entire task distribution using accumulated experience, providing a stable baseline. The transient component (V(T)) learns quickly on top of this baseline, adapting to current task specifics. The additive combination V(P) + V(T) allows both stability and plasticity. Updates follow semi-gradient TD rules, with the transient component learning residuals not captured by the permanent component. The permanent component is updated at task boundaries using experience buffers, while the transient component updates every timestep and can be reset or decayed across tasks.

## Key Results
- The permanent component optimizes the jumpstart objective, enabling zero-shot transfer to new tasks
- The transient component learns only the portion of the value function not captured by the permanent component
- The method shows significant improvements in performance compared to traditional TD-learning and Q-learning baselines, particularly in non-stationary environments with changing rewards, transitions, and observations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing the value function into permanent and transient components enables simultaneous learning of general knowledge and rapid adaptation to new tasks.
- **Mechanism**: The permanent component (V(P)) learns slowly over the entire task distribution using accumulated experience, providing a stable baseline. The transient component (V(T)) learns quickly on top of this baseline, adapting to current task specifics. The additive combination V(P) + V(T) allows both stability and plasticity.
- **Core assumption**: The task distribution has sufficient overlap in value function structure that a common permanent component can provide useful baseline estimates across tasks.
- **Evidence anchors**:
  - [abstract] "We propose to decompose the value function into two components which update at different timescales: a permanent value function, which holds general knowledge that persists over time, and a transient value function, which allows quick adaptation to new situations."
  - [section 3] "Inspired by CLS theory, we decompose the value function estimated by the agent into two, independently parameterized components: one which slowly acquires general knowledge, the other which quickly learns local nuances but then forgets."
- **Break condition**: If the task distribution has minimal overlap in value function structure, the permanent component provides little useful baseline and the decomposition offers minimal advantage over single-value-function approaches.

### Mechanism 2
- **Claim**: The transient value function learns only the portion of the value function not captured by the permanent component.
- **Mechanism**: The transient update rule uses the combined V(P) + V(T) estimate as the target, meaning the transient component learns residuals. The fixed point of the transient operator is (I - γPπ)^(-1)rπ - V(P), which is exactly the difference between the true value function and the permanent estimate.
- **Core assumption**: The permanent and transient components can be additively decomposed such that V(P) captures shared structure and V(T) captures task-specific deviations.
- **Evidence anchors**:
  - [section 4.1, theorem 3] "The unique fixed point of the transient operator is (I - γPπ)^(-1)rπ - V(P)."
  - [section 3.2] "Since the permanent and the transient value functions are independently parameterized, the semi-gradient ends up being just ∇wV(T)(St). This results in learning only a part of the value function that is not captured by the permanent value function."
- **Break condition**: If the permanent component cannot capture shared structure effectively, the transient component must learn the entire value function, defeating the purpose of decomposition.

### Mechanism 3
- **Claim**: The permanent component optimizes the jumpstart objective, enabling zero-shot transfer to new tasks.
- **Mechanism**: The permanent component's fixed point is Eτ[vτ], which minimizes Eτ[∥u - vτ∥²]. This means the permanent estimate is optimal for immediate performance on new tasks without any adaptation.
- **Core assumption**: The task distribution is stationary and the agent can accumulate sufficient experience across tasks to estimate Eτ[vτ].
- **Evidence anchors**:
  - [section 4.2, theorem 6] "The fixed point of the permanent value function optimizes the jumpstart objective, J = arg min u∈R|S| 1/2 Eτ[∥u - vτ∥²]."
  - [section 3.1] "To achieve this in a scenario where the agent goes through a sequence of tasks and knows when the task changes, we might store all the states visited during the current task in a buffer and using them when the task finishes, to update the permanent value function."
- **Break condition**: If the task distribution is non-stationary or the agent cannot accumulate sufficient cross-task experience, the permanent component may converge to a suboptimal estimate that doesn't generalize.

## Foundational Learning

- **Concept**: Temporal Difference (TD) Learning
  - **Why needed here**: The proposed method builds directly on TD-learning as a special case (when V(P) = V(TD)₀ and V(T)₀ = 0), and understanding TD-learning is essential for grasping how the decomposition generalizes and improves upon standard approaches.
  - **Quick check question**: What is the fixed point of TD-learning with linear function approximation under the on-policy distribution?

- **Concept**: Complementary Learning Systems (CLS) Theory
  - **Why needed here**: The paper explicitly draws inspiration from CLS theory from neuroscience, which posits two learning systems: one for slow acquisition of structured knowledge (neocortex) and one for rapid learning of specifics (hippocampus). Understanding this framework is crucial for understanding the motivation behind the decomposition.
  - **Quick check question**: How does the hippocampus function in CLS theory compare to the transient value function in this work?

- **Concept**: Bellman Operators and Fixed Points
  - **Why needed here**: The theoretical analysis relies heavily on properties of Bellman operators (contraction mappings, fixed points) to characterize both permanent and transient components. Understanding these concepts is essential for following the convergence proofs and understanding why the decomposition works.
  - **Quick check question**: Why is the Bellman operator a contraction mapping, and what is its significance for convergence?

## Architecture Onboarding

- **Component map**:
  - Experience Buffer -> Permanent Value Function (V(P)) -> Additive combination with Transient Value Function (V(T)) -> Action Selection
  - Environment -> Transient Value Function (V(T)) -> Additive combination with Permanent Value Function (V(P)) -> Action Selection

- **Critical path**:
  1. At each timestep: update V(T) using semi-gradient TD rule with target based on V(P+T)
  2. At task boundaries: update V(P) using accumulated experience from buffer
  3. Reset V(T) and clear buffer after permanent update
  4. Use V(P+T) for action selection and bootstrapping

- **Design tradeoffs**:
  - **Update frequency**: More frequent permanent updates (smaller k) increases plasticity but reduces stability; less frequent updates increase stability but slow adaptation.
  - **Transient decay rate (λ)**: Higher λ preserves more transient knowledge across task boundaries but may reduce adaptability; lower λ increases plasticity but may lose useful transient information.
  - **Component capacity**: Larger capacity for permanent component improves generalization but increases computational cost; larger transient capacity improves adaptation but may reduce stability.

- **Failure signatures**:
  - **High error on new tasks**: Permanent component hasn't captured sufficient shared structure; consider increasing cross-task experience or adjusting update rules.
  - **High error on old tasks**: Transient component not forgetting quickly enough; consider increasing decay rate or resetting more aggressively.
  - **Slow convergence**: Learning rates too small; consider increasing α for appropriate component.
  - **Oscillations**: Learning rates too large or update frequencies misaligned with task boundaries.

- **First 3 experiments**:
  1. **Tabular gridworld with known task boundaries**: Implement both PT-TD and standard TD-learning on a simple 5x5 grid with reward changes. Compare RMSVE on current and past tasks to demonstrate stability-plasticity tradeoff.
  2. **Linear function approximation on continuous grid**: Use radial basis functions on a continuous state space with changing goal rewards. Compare MSE on other tasks to show generalization benefits.
  3. **Deep RL on Minigrid**: Implement PT-DQN and DQN on a 4-room environment with changing item rewards. Compare episodic returns to demonstrate performance improvements in control setting.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the frequency parameter k and the decay parameter λ be unified into a single parameter?
- **Basis in paper**: [explicit] The paper mentions that the two parameters could be unified into a single one in the discussion section.
- **Why unresolved**: The paper suggests this as a potential future direction but does not provide any theoretical or empirical evidence to support the unification.
- **What evidence would resolve it**: Empirical studies comparing the performance of the algorithm with a unified parameter against the current two-parameter version, or theoretical analysis deriving a unified parameter from the existing ones.

### Open Question 2
- **Question**: Can the complementary learning systems approach be extended to policy gradient methods?
- **Basis in paper**: [explicit] The discussion section mentions that the approach is general and could in principle be used with policy gradient approaches.
- **Why unresolved**: The paper only demonstrates the approach with value-based algorithms and discrete action spaces, leaving the application to policy gradient methods as an open question.
- **What evidence would resolve it**: Implementation and evaluation of the approach with policy gradient methods (e.g., actor-critic) on continuous control tasks, comparing performance against standard policy gradient baselines.

### Open Question 3
- **Question**: How does the approach perform in non-stationary environments with continuous changes (e.g., gradual wear and tear)?
- **Basis in paper**: [inferred] The theoretical analysis is limited to piece-wise non-stationarity, and the discussion section mentions extending the approach to other types of non-stationarity as a future direction.
- **Why unresolved**: The current theoretical results and experiments focus on environments with discrete task changes, not continuous gradual changes.
- **What evidence would resolve it**: Empirical studies in simulated environments with continuous non-stationarity (e.g., gradually degrading robot joints) comparing the approach against other continual RL methods, or theoretical analysis extending the current results to continuous non-stationarity.

## Limitations

- Theoretical analysis is primarily conducted in the tabular setting, with extensions to linear function approximation relying on assumptions about approximation quality
- Empirical validation focuses on relatively small-scale problems, leaving performance in high-dimensional domains uncertain
- The method assumes tasks share sufficient structural similarity to benefit from a common permanent component, which may not hold in highly diverse task distributions

## Confidence

**Theoretical foundations (High)**: The mathematical analysis of Bellman operators, contraction properties, and fixed points is rigorous and well-established. The decomposition framework is theoretically sound in the tabular case.

**Empirical validation (Medium)**: The experimental results demonstrate consistent improvements across multiple domains, but the evaluation is limited to relatively simple environments. The magnitude of improvements in more challenging settings is uncertain.

**Neuroscientific motivation (Medium)**: While the CLS theory provides an elegant conceptual framework, the direct applicability of neuroscience findings to reinforcement learning algorithms involves interpretation. The empirical connection between the proposed decomposition and biological learning systems remains largely conceptual.

## Next Checks

1. **Scaling to high-dimensional domains**: Implement and evaluate the method on standard deep RL benchmarks like Atari or continuous control tasks to assess whether the stability-plasticity benefits persist in more complex settings.

2. **Ablation studies on component capacity**: Systematically vary the capacity and learning rates of permanent vs transient components to determine the sensitivity of performance to architectural choices and identify optimal configurations.

3. **Non-stationary task distributions**: Evaluate performance when task distributions change over time (e.g., concept drift) to test whether the permanent component can adapt to evolving environments or becomes a liability.