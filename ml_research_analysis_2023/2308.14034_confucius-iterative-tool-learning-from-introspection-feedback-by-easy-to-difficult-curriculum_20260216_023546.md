---
ver: rpa2
title: 'Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult
  Curriculum'
arxiv_id: '2308.14034'
source_url: https://arxiv.org/abs/2308.14034
tags:
- tools
- uni00000013
- tool
- string
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework called Confucius for teaching large
  language models (LLMs) to use external tools in real-world scenarios. The framework
  includes a multi-stage learning method to train LLMs from an easy-to-difficult curriculum,
  and an iterative self-instruct method with introspective feedback to dynamically
  update the training dataset.
---

# Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum

## Quick Facts
- arXiv ID: 2308.14034
- Source URL: https://arxiv.org/abs/2308.14034
- Reference count: 8
- One-line primary result: Confucius achieves superior tool selection and usage performance through iterative self-instruct with introspective feedback

## Executive Summary
Confucius introduces a framework for teaching large language models (LLMs) to effectively use external tools in real-world scenarios. The framework combines a multi-stage curriculum learning approach with an iterative self-instruct mechanism guided by introspective feedback. By progressively increasing task difficulty and continuously updating the training dataset based on model perplexity, Confucius demonstrates significant improvements in tool selection accuracy, parameter correctness, compositional reasoning, and interaction fluency across both seen and unseen tool sets.

## Method Summary
Confucius employs a two-pronged approach: (1) a multi-stage learning curriculum that trains the model from easy to difficult tool usage scenarios, and (2) an Iterative Self-instruct from Introspective Feedback (ISIF) mechanism that dynamically updates the training dataset. The multi-stage curriculum begins with warm-up training on a minimal toolset, progresses through in-category and cross-category stages with retrieved candidate tools, and culminates in training on the full toolset. ISIF computes perplexity on generated responses, filters high-perplexity instances, uses them as prompts to generate new similar instances, and adds these to the training set for iterative retraining.

## Key Results
- Confucius outperforms both tuning-free and tuning-based baselines in tool selection, parameter correctness, compositional reasoning, and interaction fluency
- The framework demonstrates strong generalization to unseen toolsets while maintaining high performance on seen toolsets
- Iterative self-instruct with introspective feedback prevents overfitting to simple tools and improves mastery of complex tool interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative dataset updates guided by introspective feedback improve model performance on complex tools.
- Mechanism: The model generates responses using tools, perplexity is computed for each response, high-perplexity instances are filtered and used as prompts to generate new similar instances, these are added to the training set, and the model is retrained. This targets training data toward areas where the model struggles.
- Core assumption: Perplexity is a reliable proxy for identifying instances that are difficult for the current model to handle correctly.
- Evidence anchors:
  - [abstract] "we propose the Iterative Self-instruct from Introspective Feedback (ISIF) to dynamically construct the dataset to improve the ability to use the complicated tool."
  - [section] "We then filter the generated instances D = {d1, d2, ..., d|D|} with high perplexity instances D* which should be trained more."
- Break condition: If perplexity does not correlate with actual error rate or if the model overfits to the specific instances added via this process.

### Mechanism 2
- Claim: Multi-stage training from easy to difficult curriculum improves model ability to select tools from large candidate sets.
- Mechanism: The model is first trained on a minimal toolset to learn basic tool usage, then on a mixed set of ground truth and related tools to learn selection among candidates, and finally on a retrieved candidate set to simulate real-world tool selection.
- Core assumption: Gradually increasing task difficulty during training leads to better generalization than training on the final difficult setting from the start.
- Evidence anchors:
  - [abstract] "We first propose a multi-stage learning method to teach the LLM to use various tools from an easy-to-difficult curriculum"
  - [section] "In real-world applications, the tool-use model should select appropriate tools from the retrieved tools and schedule them correctly (a.k.a., difficult mode), instead of directly using human given candidate toolset (a.k.a., easy mode)."
- Break condition: If the staged curriculum does not improve performance compared to direct training on the final stage.

### Mechanism 3
- Claim: Self-instruct with introspective feedback prevents overfitting to simple tools and improves generalization to unseen tools.
- Mechanism: By continuously updating the training dataset based on the model's own introspective feedback (high perplexity instances), the model is exposed to more diverse and complex tool usage patterns, preventing it from specializing only on simple, well-understood tools.
- Core assumption: The introspective feedback process generates diverse enough instances to cover the complexity of real-world tool usage.
- Evidence anchors:
  - [abstract] "Compared to previous works, ISIF facilitates the LLM to master more intricate tools and prevents it from overfitting to a subset of simple tools."
  - [section] "Since intricate tools require more training data for LLM to fully master, the pre-created dataset is out of sync with the up-to-date LLM."
- Break condition: If the introspective feedback process generates too many similar or redundant instances, leading to overfitting on a narrow subset of tool usage patterns.

## Foundational Learning

- Concept: Perplexity as a measure of model uncertainty
  - Why needed here: Used to identify instances where the model is uncertain or likely to make errors, guiding the dataset update process.
  - Quick check question: What does a high perplexity score indicate about a model's confidence in its generated output?

- Concept: Curriculum learning
  - Why needed here: The multi-stage training approach gradually increases task difficulty, which is shown to improve learning efficiency and generalization.
  - Quick check question: How does curriculum learning differ from standard training approaches, and what is its theoretical basis?

- Concept: Chain-of-thought reasoning
  - Why needed here: The tool usage instances are constructed in a chain-of-thought format, requiring the model to break down complex tasks into sub-tasks and schedule tools accordingly.
  - Quick check question: What is the purpose of chain-of-thought prompting in the context of tool learning?

## Architecture Onboarding

- Component map:
  Base LLM -> Tool Store -> Dense Tool Retriever -> Multi-stage Training Pipeline -> ISIF Mechanism -> Evaluation Pipeline

- Critical path:
  1. Initialize tool store and dataset
  2. Multi-stage training on initial dataset
  3. Compute perplexity on generated responses
  4. Filter high-perplexity instances
  5. Generate new instances using filtered instances as prompts
  6. Update dataset and retrain
  7. Repeat steps 3-6 for multiple epochs
  8. Evaluate on seen and unseen toolsets

- Design tradeoffs:
  - Perplexity threshold for filtering: Too low may add noisy instances, too high may miss important training signals
  - Update percentage (σ): Higher values increase dataset size but risk overfitting, lower values may not provide enough targeted training
  - Tool retriever recall: Higher recall increases candidate set size but may introduce more noise, lower recall may miss relevant tools

- Failure signatures:
  - Performance degradation on unseen toolsets despite good performance on seen toolsets (overfitting)
  - Slow convergence or no improvement over iterations (perplexity not a good signal or update percentage too low)
  - High variance in tool selection scores across runs (instability in the introspective feedback process)

- First 3 experiments:
  1. Run baseline multi-stage training without introspective feedback updates (σ=0) and compare performance to full Confucius
  2. Vary the update percentage σ (e.g., 10%, 20%, 30%) and analyze performance trade-offs
  3. Test the model on a held-out subset of the training tools (seen tools) vs. completely new tools (unseen tools) to measure generalization

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the methodology and results presented, several questions emerge:

1. How does Confucius handle tool selection when multiple tools from different categories are equally relevant to the query?
2. How does the introspective feedback mechanism in ISIF ensure that the generated instances are not just similar to the filtered high perplexity instances but also diverse and cover a wide range of tool usage scenarios?
3. How does Confucius perform when the toolset contains tools that are very similar in functionality, making it challenging to distinguish between them?

## Limitations
- Reliance on perplexity as a proxy for identifying difficult instances without direct validation of this correlation
- Potential for closed-loop reinforcement of existing biases through the introspective feedback mechanism
- Lack of ablation studies on the relative importance of the multi-stage curriculum versus the iterative updates

## Confidence
- **High confidence**: The multi-stage curriculum approach is well-grounded in established curriculum learning theory and the experimental design comparing different training stages is robust
- **Medium confidence**: The ISIF mechanism's effectiveness in preventing overfitting and improving generalization, as this depends heavily on the validity of perplexity as a signal and the diversity of generated instances
- **Low confidence**: Claims about superior performance on unseen toolsets, as the paper does not provide sufficient analysis of whether improvements come from better generalization or memorization of patterns

## Next Checks
1. **Perplexity-signal validation**: Conduct a controlled experiment comparing ISIF performance when perplexity is replaced with an oracle signal (actual error rates) to determine if perplexity is indeed the limiting factor in the approach
2. **Diversity analysis of generated instances**: Measure the semantic and syntactic diversity of instances generated through the introspective feedback process to verify that the model is not overfitting to a narrow distribution of tool usage patterns
3. **Ablation on training stages**: Systematically remove each training stage (warm-up, in-category, cross-category) to quantify their individual contributions to final performance, particularly on unseen toolsets