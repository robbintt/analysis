---
ver: rpa2
title: When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge
  Sparkle Dust
arxiv_id: '2305.07230'
source_url: https://arxiv.org/abs/2305.07230
tags:
- llms
- knowledge
- language
- insurance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) excel in many natural language processing
  tasks but often lack domain-specific knowledge required for practical business scenarios,
  especially those involving complex reasoning. This paper addresses this gap by proposing
  a knowledge enhancement approach that integrates domain-specific knowledge sources
  with LLMs for question answering in the insurance domain.
---

# When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust

## Quick Facts
- arXiv ID: 2305.07230
- Source URL: https://arxiv.org/abs/2305.07230
- Reference count: 23
- Key outcome: Knowledge enhancement significantly improves GPT-3.5's accuracy on insurance questions from 9.6% to 55.8-57.8%

## Executive Summary
Large language models excel at general language tasks but struggle with domain-specific knowledge requirements in practical business applications. This paper addresses this gap by proposing a knowledge enhancement approach that integrates domain-specific knowledge sources with LLMs for insurance question answering. The method combines policy rulebooks and knowledge graphs to provide LLMs with necessary context, significantly improving reasoning ability and accuracy. The approach demonstrates how external knowledge resources can effectively supplement LLM capabilities in specialized domains.

## Method Summary
The study implements a knowledge enhancement framework for insurance question answering using GPT-3.5-Turbo. It processes seven insurance policy rulebooks into structured formats, creates a knowledge graph from these rulebooks, and uses cosine similarity for context retrieval. Three experimental settings are tested: direct GPT usage without context, GPT with rulebooks context, and GPT with knowledge graph context. Prompt engineering techniques, including chain-of-thought prompting, guide the LLM to utilize retrieved context effectively. Human evaluation measures accuracy across 104 insurance-related questions.

## Key Results
- Knowledge enhancement improves GPT-3.5 accuracy from 9.6% to 55.8-57.8% on insurance questions
- Table processing increases correct answers from 7 to 16 questions (80% accuracy)
- Chain-of-thought prompting significantly improves performance on open-domain questions
- Domain-specific knowledge integration enables effective multi-step reasoning in specialized domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific knowledge enhancement significantly improves GPT-3.5's reasoning ability for insurance question answering.
- Mechanism: The model retrieves relevant context from policy rulebooks and knowledge graphs, then incorporates this context into prompts to guide the LLM's response generation.
- Core assumption: LLMs lack sufficient domain knowledge for specialized tasks and can benefit from external knowledge integration.
- Evidence anchors:
  - [abstract] "knowledge enhancement significantly improves the reasoning ability of GPT-3.5 (55.80% and 57.83% in terms of accuracy)"
  - [section] "domain-specific knowledge enhancement allows the GPT model to learn from a given context, such as a rulebook or KG, and provide more correct answers"
  - [corpus] Weak - corpus lacks insurance-specific studies, but general LLM adaptation papers exist
- Break condition: If the knowledge base contains inaccurate or contradictory information, or if the LLM cannot effectively integrate the provided context.

### Mechanism 2
- Claim: Using structured table data from policy rulebooks improves GPT-3.5's performance on insurance questions.
- Mechanism: Tables are preprocessed into a machine-readable format and included as context in prompts, allowing the LLM to understand structured information.
- Core assumption: GPT-3.5 cannot effectively process structured data in its native format.
- Evidence anchors:
  - [section] "the model GPT+RB using table processing can answer 16 correct questions (80%) while the model without using the processing can only answer 7 correct questions"
  - [corpus] Weak - corpus lacks specific studies on LLM table processing, but general NLP research exists
- Break condition: If table preprocessing fails to capture essential information or if the LLM still cannot interpret the structured data.

### Mechanism 3
- Claim: External knowledge bases like DBPedia and chain-of-thought prompting can supplement LLM knowledge for open-domain questions.
- Mechanism: For questions involving concepts not covered in rulebooks, relevant information is retrieved from external knowledge bases and incorporated into prompts. Chain-of-thought prompting guides the LLM through intermediate reasoning steps.
- Core assumption: LLMs can effectively utilize external knowledge and reasoning chains when provided in prompts.
- Evidence anchors:
  - [section] "complementing GPT with external DBPedia or extending its knowledge with CoT can significantly improve its ability to answer open-domain questions"
  - [corpus] Weak - corpus lacks specific studies on LLM external knowledge integration, but general knowledge graph research exists
- Break condition: If external knowledge is irrelevant, contradictory, or if the LLM cannot effectively follow the reasoning chain.

## Foundational Learning

- Concept: Prompt engineering
  - Why needed here: Effective prompts are crucial for guiding the LLM to use the provided context and generate accurate responses.
  - Quick check question: Can you identify the three main components of a prompt used in this study (indicator, question, context)?

- Concept: Information retrieval
  - Why needed here: The adapter module retrieves relevant context from knowledge bases based on the input question.
  - Quick check question: What similarity metric is used for context retrieval in this study?

- Concept: Chain-of-thought reasoning
  - Why needed here: This technique helps the LLM break down complex questions into intermediate steps, improving its reasoning ability.
  - Quick check question: How many steps are included in the CoT prompt designed for this study?

## Architecture Onboarding

- Component map:
  Knowledge Base -> Adapter -> LLM (GPT-3.5-Turbo) -> Evaluation

- Critical path:
  1. User question input
  2. Context retrieval from knowledge base
  3. Prompt creation with retrieved context
  4. LLM response generation
  5. Human evaluation of response accuracy

- Design tradeoffs:
  - Knowledge base selection: Domain-specific vs. general knowledge
  - Context retrieval method: Simple similarity vs. complex semantic matching
  - Prompt structure: Natural language vs. structured format

- Failure signatures:
  - Incorrect context retrieval leading to wrong answers
  - LLM generating hallucinations or irrelevant responses
  - Poor performance on questions requiring multi-step reasoning

- First 3 experiments:
  1. Compare GPT-3.5 performance with and without domain knowledge enhancement on a small set of insurance questions.
  2. Test different context retrieval methods (e.g., cosine similarity vs. semantic search) to optimize relevant context selection.
  3. Evaluate the impact of chain-of-thought prompting on complex reasoning tasks not covered by rulebooks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4 compare to GPT-3.5 in the insurance domain knowledge enhancement task?
- Basis in paper: [explicit] The paper mentions that GPT-4 testing is ongoing and results will be available soon, but no comparative results are provided.
- Why unresolved: The paper does not provide any results comparing GPT-4 to GPT-3.5, despite mentioning that GPT-4 testing is ongoing.
- What evidence would resolve it: Experimental results comparing the performance of GPT-4 and GPT-3.5 on the insurance domain knowledge enhancement task, including accuracy metrics and error analysis.

### Open Question 2
- Question: What is the optimal size of the domain-specific knowledge base for effective knowledge enhancement in LLMs?
- Basis in paper: [inferred] The paper discusses using policy rulebooks and knowledge graphs for knowledge enhancement, but does not explore the optimal size or granularity of these knowledge bases.
- Why unresolved: The paper does not investigate how the size or granularity of the knowledge base affects the performance of LLMs in domain-specific tasks.
- What evidence would resolve it: Experiments varying the size and granularity of the knowledge base (e.g., using different numbers of policy rulebooks or varying the depth of the knowledge graph) and measuring the impact on LLM performance.

### Open Question 3
- Question: How does the performance of knowledge enhancement techniques vary across different insurance domains or subdomains?
- Basis in paper: [inferred] The paper focuses on a general insurance domain, but does not explore how knowledge enhancement techniques might perform in specific subdomains (e.g., health insurance, auto insurance).
- Why unresolved: The paper does not investigate whether knowledge enhancement techniques are equally effective across different insurance subdomains or if domain-specific optimizations are needed.
- What evidence would resolve it: Experiments applying knowledge enhancement techniques to specific insurance subdomains and comparing performance across domains, including error analysis to identify domain-specific challenges.

## Limitations
- Small evaluation dataset (104 questions) may not capture full complexity of real-world insurance scenarios
- Limited testing to GPT-3.5-Turbo only, preventing insights into other LLM architectures
- Heavy reliance on quality and completeness of policy rulebooks and knowledge graphs
- Human evaluation introduces potential subjectivity and domain expertise requirements

## Confidence
**High Confidence Claims:**
- LLMs benefit from domain-specific knowledge enhancement for specialized tasks (supported by significant accuracy improvements from 9.6% to 55.8-57.8%)
- Structured data preprocessing improves LLM performance on table-based questions (demonstrated by 80% accuracy with vs. 35% without preprocessing)
- Chain-of-thought prompting aids complex reasoning tasks (shown to help with open-domain questions)

**Medium Confidence Claims:**
- The specific knowledge enhancement approach is optimal for insurance QA (limited by single-domain focus and single-LLM testing)
- Human evaluation provides reliable accuracy measurements (constrained by small sample size and potential evaluator bias)

**Low Confidence Claims:**
- The approach will generalize to other domains without modification (not tested across different domains)
- The performance gap will remain consistent with larger or more diverse datasets (limited by dataset size and composition)

## Next Checks
1. **Cross-domain generalization test**: Apply the same knowledge enhancement approach to a different domain (e.g., healthcare or legal) using the same LLM to assess whether the methodology transfers effectively across specialized fields.

2. **Scalability and performance evaluation**: Measure the computational overhead and response latency of the knowledge retrieval and prompt engineering pipeline at scale, testing with larger knowledge bases and concurrent user requests.

3. **Multi-LLM comparative analysis**: Replicate the experiments with different LLM architectures (GPT-4, Claude, Llama) to determine whether the knowledge enhancement benefits are consistent across model families and parameter scales.