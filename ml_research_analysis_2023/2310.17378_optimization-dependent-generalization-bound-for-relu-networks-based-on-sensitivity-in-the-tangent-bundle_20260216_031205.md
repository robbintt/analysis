---
ver: rpa2
title: Optimization dependent generalization bound for ReLU networks based on sensitivity
  in the tangent bundle
arxiv_id: '2310.17378'
source_url: https://arxiv.org/abs/2310.17378
tags:
- networks
- generalization
- bound
- sensitivity
- relu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a PAC bound on the generalization error
  of feedforward ReLU networks trained by gradient descent, which crucially depends
  on the sensitivity of the network's gradient to input perturbations along the optimization
  trajectory. The key idea is to bound the Rademacher complexity of the function class
  by analyzing the network's behavior via a sensitivity measure called Tangent Sensitivity.
---

# Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle

## Quick Facts
- arXiv ID: 2310.17378
- Source URL: https://arxiv.org/abs/2310.17378
- Reference count: 21
- Primary result: PAC bound on generalization error for ReLU networks that depends on Tangent Sensitivity rather than depth

## Executive Summary
This paper introduces a novel PAC generalization bound for ReLU networks trained by gradient descent that depends on Tangent Sensitivity rather than network depth. The key insight is to measure how the network's gradient mapping changes when input data is perturbed by Gaussian noise along the optimization trajectory. By bounding the Rademacher complexity through this sensitivity measure, the bound avoids explicit depth dependence while still providing meaningful generalization guarantees. Experiments on MNIST and CIFAR-10 verify that Tangent Sensitivity correlates with the empirical generalization gap.

## Method Summary
The method computes a PAC bound by analyzing the gradient mapping's sensitivity to input perturbations using Tangent Sensitivity matrices. For each training iteration, the Tangent Sensitivity matrix is calculated as the Jacobian of the gradient with respect to inputs, and its Frobenius norm is accumulated along the optimization trajectory. The Rademacher complexity is then bounded using these sensitivity measures, leading to a generalization bound that applies to wide ReLU networks trained by gradient descent.

## Key Results
- Established PAC bound on generalization error depending on Tangent Sensitivity rather than depth
- Verified correlation between Tangent Sensitivity norm and empirical generalization gap on MNIST and CIFAR-10
- Bound applies to feedforward ReLU networks trained by gradient descent with gradient flow assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tangent Sensitivity measures the gradient map's change under input perturbations, enabling PAC bounds without explicit depth dependence.
- Mechanism: The Frobenius norm of the Tangent Sensitivity matrix captures how gradients change when inputs are perturbed by Gaussian noise. By bounding this norm along the optimization trajectory, the Rademacher complexity of the network class is controlled.
- Core assumption: The Frobenius norm of Tangent Sensitivity is bounded for all parameter vectors in a ball around initialization, and the Taylor approximation error is small for wide networks.

### Mechanism 2
- Claim: The generalization bound is achieved by combining Taylor approximation of ReLU networks with gradient descent trajectory analysis.
- Mechanism: The network function is approximated by its linearization around the initialization plus a small error term. The update direction is expressed via Tangent Sensitivity, enabling a linear classifier bound on Rademacher complexity.
- Core assumption: The Taylor error term decays with network width; gradients evolve smoothly under gradient descent.

### Mechanism 3
- Claim: Tangent Sensitivity correlates empirically with the generalization gap, justifying its use in the theoretical bound.
- Mechanism: Higher sensitivity to input perturbations implies the model is more responsive to small changes, which may correlate with better generalization due to smoother decision boundaries.
- Core assumption: The correlation between Tangent Sensitivity norm and generalization gap holds across architectures and datasets.

## Foundational Learning

- Concept: Rademacher complexity
  - Why needed here: The PAC bound is established by bounding the Rademacher complexity of the function class, which controls generalization.
  - Quick check question: What does Rademacher complexity measure in the context of generalization bounds?

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: The proof relies on the fact that for wide networks, the gradient w.r.t. parameters is approximately constant, linking to NTK behavior.
  - Quick check question: How does the NTK regime simplify the analysis of deep network training?

- Concept: PAC-Bayesian framework
  - Why needed here: The generalization bound is derived within the PAC framework, which provides probabilistic guarantees.
  - Quick check question: How does a PAC bound differ from a uniform convergence bound?

## Architecture Onboarding

- Component map: ReLU network -> Tangent Sensitivity computation -> Frobenius norm accumulation -> Rademacher complexity bound -> PAC generalization bound
- Critical path: Compute Tangent Sensitivity at each step → accumulate along trajectory → bound Rademacher complexity → derive generalization bound
- Design tradeoffs: High width reduces Taylor error but increases computation cost. Tangent Sensitivity computation is expensive; approximations may be needed.
- Failure signatures: If Tangent Sensitivity norm is unbounded or the Taylor error is large, the bound will be vacuous. Poor correlation between Tangent Sensitivity and generalization gap suggests the mechanism fails.
- First 3 experiments:
  1. Compute Tangent Sensitivity norm during training on a small MNIST MLP and plot against validation loss.
  2. Vary network width and verify the Taylor error term decreases as predicted.
  3. Replace ReLU with another activation and check if the correlation and bound still hold.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Tangent Sensitivity norm change during training for networks that generalize well versus those that overfit?
- Basis in paper: The paper states that empirical evidence suggests the Tangent Sensitivity norm is correlated with the empirical generalization gap, and shows correlation plots in Figures 1-5.
- Why unresolved: The paper does not analyze how the Tangent Sensitivity norm evolves during training or compare networks with different generalization performance.
- What evidence would resolve it: Tracking Tangent Sensitivity norm over training epochs for both generalizing and overfitting networks would show if it increases/decreases differently and correlates with generalization gap changes.

### Open Question 2
- Question: Can Tangent Sensitivity be efficiently approximated for practical use in training deep networks?
- Basis in paper: The discussion section mentions that calculating Tangent Sensitivity norm is computationally expensive and requires finding an efficient approximation for practical use.
- Why unresolved: The paper does not propose or evaluate any approximation methods for Tangent Sensitivity.
- What evidence would resolve it: Developing and benchmarking efficient approximation methods that maintain correlation with generalization gap, then testing them during actual network training.

### Open Question 3
- Question: How does Tangent Sensitivity relate to other measures of network smoothness or complexity?
- Basis in paper: The discussion suggests Tangent Sensitivity might be connected to smoothness in some function space, but this is not rigorously explored.
- Why unresolved: The paper does not formally relate Tangent Sensitivity to other smoothness/complexity measures like Lipschitz constants or path norms.
- What evidence would resolve it: Mathematical proofs or empirical studies showing how Tangent Sensitivity correlates with or bounds other complexity measures across different network architectures and tasks.

## Limitations
- The bound relies on the infinite-width regime where Taylor approximation errors become negligible
- Experimental validation focuses on correlation rather than direct validation of the bound's tightness
- Tangent Sensitivity computation is computationally expensive for practical networks

## Confidence

- **High Confidence**: The theoretical framework linking Tangent Sensitivity to Rademacher complexity is mathematically sound within the stated assumptions
- **Medium Confidence**: The experimental correlation between Tangent Sensitivity and generalization gap, though the correlation strength is not quantified
- **Low Confidence**: The practical applicability of the bound for networks of realistic width where Taylor approximation errors may dominate

## Next Checks
1. Quantify the Taylor approximation error as a function of network width and test on networks with varying widths to verify error decay
2. Implement the full bound calculation on a small network and compare predicted vs. actual generalization gap on held-out data
3. Test whether the correlation between Tangent Sensitivity and generalization gap holds across different activation functions and architectures (CNNs, ResNets)