---
ver: rpa2
title: 'CMOSE: Comprehensive Multi-Modality Online Student Engagement Dataset with
  High-Quality Labels'
arxiv_id: '2312.09066'
source_url: https://arxiv.org/abs/2312.09066
tags:
- features
- engagement
- loss
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CMOSE, a comprehensive multi-modal online
  student engagement dataset with high-quality labels annotated by psychology experts.
  It proposes MocoRank, a training mechanism using contrastive learning to handle
  data imbalance, intra-class variation, and ordinal relationships in engagement detection.
---

# CMOSE: Comprehensive Multi-Modality Online Student Engagement Dataset with High-Quality Labels

## Quick Facts
- **arXiv ID**: 2312.09066
- **Source URL**: https://arxiv.org/abs/2312.09066
- **Reference count**: 6
- **Key outcome**: Introduces CMOSE dataset and MocoRank framework, achieving 1.32% higher overall accuracy and 5.05% higher average accuracy in engagement detection.

## Executive Summary
This paper introduces CMOSE, a comprehensive multi-modal dataset for online student engagement detection with high-quality psychology expert annotations. The dataset addresses key challenges in engagement detection including data imbalance, intra-class variation, and ordinal relationships between engagement levels. The authors propose MocoRank, a novel training mechanism using contrastive learning that outperforms previous methods by 1.32% overall accuracy and 5.05% average accuracy. The study demonstrates the effectiveness of combining pre-trained visual features (OpenFace and I3D) with audio features for engagement detection, showing that multi-modality significantly improves performance.

## Method Summary
The method combines multi-level visual features (high-level facial features from OpenFace and spatiotemporal features from I3D) with audio features (speech content from BERT and acoustics from Parselmouth) to detect student engagement levels. Features are processed through a Temporal Convolutional Network and attention mechanism, then combined and mapped to engagement scores using MocoRank. The MocoRank framework employs a momentum encoder and score pool to handle data imbalance, intra-class variation, and ordinal relationships through contrastive learning with multi-margin loss. Features are extracted separately and combined to avoid performance degradation from imbalanced feature quantities.

## Key Results
- MocoRank achieves 1.32% higher overall accuracy compared to previous methods
- MocoRank achieves 5.05% higher average accuracy across engagement levels
- Multi-modality improves performance, with audio features contributing 3.18% to overall accuracy and 3.47% to average accuracy

## Why This Works (Mechanism)

### Mechanism 1
MocoRank addresses intra-class variation by using relative comparisons rather than fixed ground truth values. Instead of assigning a fixed scalar value to each engagement class, MocoRank uses a score pool containing predicted scores from a momentum encoder. During training, it compares a sample's predicted score against scores from the pool, enforcing that a sample should score higher than less engaged samples and lower than more engaged ones. The margin between scores is dynamically adjusted based on cosine similarity between feature embeddings.

### Mechanism 2
MocoRank handles data imbalance by using a score pool that includes minority class samples and emphasizes relative assessment over absolute class labels. The score pool is maintained as a queue that cycles through all classes, ensuring minority class samples are regularly included in comparisons. The multi-margin loss compares each training sample against a diverse set of reference scores, preventing the model from overfitting to majority classes.

### Mechanism 3
Combining multi-level visual features (high-level from OpenFace and low-level from I3D) with audio features improves engagement detection accuracy. High-level features capture interpretable engagement indicators like gaze and head pose, while I3D features capture subtle temporal and spatial patterns. Attention is used to weight the importance of I3D features over time. Audio features from speech content and acoustics provide additional context, especially for segments where participants are speaking.

## Foundational Learning

- **Concept**: Ordinal regression and its difference from standard classification
  - Why needed here: Engagement levels have a natural order (HD < DE < EG < HE), and treating them as unordered classes loses this information. The model needs to predict a scalar engagement score rather than discrete class probabilities.
  - Quick check question: What is the key difference between predicting engagement as a scalar versus predicting class probabilities, and why is this important for ordinal data?

- **Concept**: Contrastive learning and its application to engagement detection
  - Why needed here: Contrastive learning helps the model learn discriminative features by comparing similar and dissimilar samples. In engagement detection, it helps handle intra-class variation and ordinal relationships by focusing on relative engagement levels rather than absolute labels.
  - Quick check question: How does contrastive learning help with intra-class variation in engagement detection compared to standard supervised learning?

- **Concept**: Multi-modal feature fusion strategies
  - Why needed here: Combining visual and audio features requires careful integration to leverage complementary information. The paper uses separate training stages and concatenation with attention mechanisms to combine features effectively.
  - Quick check question: Why does the paper train visual and audio feature extractors separately before combining them, and what problem does this approach solve?

## Architecture Onboarding

- **Component map**: Video segments → OpenFace (high-level features) + I3D (visual features) + BERT/Parselmouth (audio features) → TCN processing → Attention fusion → Concatenation → Score prediction → MocoRank loss computation

- **Critical path**: Feature extraction → TCN processing → Attention fusion → Score prediction → MocoRank loss computation

- **Design tradeoffs**:
  - Separate training of modalities vs. joint training: Chosen to avoid performance degradation from imbalanced feature quantities
  - Scalar prediction vs. class probabilities: Chosen to preserve ordinal relationships and handle intra-class variation
  - Momentum encoder with slow update vs. direct comparisons: Chosen for stable reference scores and smooth learning

- **Failure signatures**:
  - Performance drops on minority classes: May indicate score pool imbalance or momentum encoder instability
  - Overfitting to majority classes: May indicate need for stronger contrastive learning or data augmentation
  - Poor generalization to new datasets: May indicate domain shift or need for better feature transferability

- **First 3 experiments**:
  1. Train with only OpenFace features and standard MSE loss to establish baseline
  2. Add I3D features with attention mechanism but keep standard loss to measure feature contribution
  3. Implement MocoRank with only visual features to isolate the impact of the training mechanism

## Open Questions the Paper Calls Out

### Open Question 1
How do the proposed audio features (acoustics and speech content) specifically contribute to engagement detection performance in different learning contexts? The paper suggests audio features could add information not contained in visual features but does not provide a detailed analysis of how different audio features contribute in various learning contexts.

### Open Question 2
How does the MocoRank framework perform compared to other state-of-the-art methods on other engagement detection datasets? While the paper demonstrates effectiveness on CMOSE, it does not evaluate MocoRank's performance on other engagement detection datasets.

### Open Question 3
How does the proposed MocoRank framework handle the ordinal relationship between different engagement levels compared to other methods? The paper states that MocoRank is specifically designed to handle ordinal relationships but does not detail the specific mechanism compared to other approaches.

## Limitations
- Performance claims rely heavily on the quality and representativeness of the CMOSE dataset (12,193 video segments)
- Limited ablation studies on critical components like score pool size and momentum encoder update strategy
- Specific implementation details of attention mechanism and TCN architecture are not fully specified

## Confidence
- **High Confidence**: The general framework of combining multi-level visual features with audio features is well-established and the reported accuracy improvements are plausible
- **Medium Confidence**: The MocoRank mechanism's effectiveness in handling data imbalance and intra-class variation, though limited empirical validation exists for edge cases
- **Low Confidence**: The exact implementation details necessary for faithful reproduction, particularly attention mechanism architecture and momentum encoder update strategy

## Next Checks
1. Conduct ablation studies to isolate the contribution of each feature type (OpenFace, I3D, audio) to verify the claimed 5.05% average accuracy improvement
2. Test MocoRank's robustness to varying score pool sizes and momentum encoder update rates to identify failure conditions
3. Evaluate model performance across different engagement levels to confirm that the 1.32% overall accuracy improvement is not driven primarily by majority classes