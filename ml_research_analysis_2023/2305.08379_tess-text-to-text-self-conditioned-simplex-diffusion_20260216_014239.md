---
ver: rpa2
title: 'TESS: Text-to-Text Self-Conditioned Simplex Diffusion'
arxiv_id: '2305.08379'
source_url: https://arxiv.org/abs/2305.08379
tags:
- diffusion
- generation
- tess
- text
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TESS introduces a fully non-autoregressive text diffusion model
  that performs diffusion in the vocabulary logit simplex space, incorporates an efficient
  self-conditioning method, and generates sequences of arbitrary length. By combining
  simplex diffusion with self-conditioning, TESS achieves state-of-the-art performance
  among non-autoregressive models on natural language generation tasks like summarization
  (39.7 ROUGE-L), paraphrase generation (62.2 ROUGE-L), and question generation (38.9
  BERTScore), while being competitive with autoregressive baselines such as BART.
---

# TESS: Text-to-Text Self-Conditioned Simplex Diffusion

## Quick Facts
- arXiv ID: 2305.08379
- Source URL: https://arxiv.org/abs/2305.08379
- Reference count: 20
- Key outcome: Achieves state-of-the-art performance among non-autoregressive models on text generation tasks (e.g., 39.7 ROUGE-L on summarization, 62.2 ROUGE-L on paraphrase generation) and matches RoBERTa on GLUE.

## Executive Summary
TESS introduces a fully non-autoregressive text diffusion model that performs diffusion directly in the vocabulary logit simplex space and incorporates an efficient self-conditioning method. By diffusing in the simplex space, TESS avoids the need for extra mapping steps and leverages the probabilistic structure of text tokens. The self-conditioning method exploits this simplex semantics to improve generation quality. TESS achieves state-of-the-art results among non-autoregressive models on natural language generation tasks and is competitive with autoregressive baselines like BART, while also being effective for natural language understanding tasks on GLUE.

## Method Summary
TESS is a fully non-autoregressive diffusion model for text generation that performs diffusion in the vocabulary logit simplex space rather than the learned embedding space. It uses a transformer encoder to denoise the entire sequence in parallel, enabling variable-length output generation. The model incorporates a novel self-conditioning method that averages probability distributions from the current noisy simplex and the previous prediction, exploiting the simplex's probabilistic structure. TESS is trained with cross-entropy loss and can generate sequences in as few as 100 sampling steps while maintaining quality.

## Key Results
- Achieves state-of-the-art performance among non-autoregressive models on summarization (39.7 ROUGE-L), paraphrase generation (62.2 ROUGE-L), and question generation (38.9 BERTScore).
- Matches RoBERTa performance on GLUE benchmark for natural language understanding tasks.
- Demonstrates robust performance with as few as 100 sampling steps and generates variable-length outputs that match the target distribution.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusing in the vocabulary logit simplex space improves stability and avoids the need for extra mapping steps.
- Mechanism: By adding Gaussian noise directly to the k-logit simplex representation of tokens, the model operates in a discrete, categorical space. The simplex structure ensures that the noise remains bounded and interpretable as probabilities, avoiding instability from diffusing in continuous embedding space.
- Core assumption: The k-logit simplex representation preserves semantic relationships between tokens and is suitable for Gaussian diffusion.
- Evidence anchors:
  - [abstract] "applies the diffusion process on the logit simplex space rather than the typical learned embedding space."
  - [section] "Direct diffusion on the probability simplex is desirable (Richemond et al., 2022) as it eliminates the need for an extra step to map diffused embeddings to actual discrete inputs..."
  - [corpus] Missing direct empirical comparison to embedding-space diffusion; only theoretical claim provided.
- Break condition: If the k-logit simplex fails to preserve semantic similarity or becomes unstable under high noise, performance would degrade.

### Mechanism 2
- Claim: Self-conditioning exploits the simplex semantics and improves generation quality.
- Mechanism: Instead of concatenating the previous prediction with the current noisy simplex, TESS averages the probability distributions from both the current noisy simplex and the previous prediction, then computes the weighted embedding. This leverages the simplex's probabilistic structure and avoids extra projection steps.
- Core assumption: Averaging probability distributions is a valid way to combine information in the simplex space.
- Evidence anchors:
  - [abstract] "employs a new form of self-conditioning... that exploits the simplex semantics of the diffusion space."
  - [section] "Let st ∈ R|V| be a noised k-logit simplex... we ﬁrst compute the average of simplex probabilities as pw avg = 1/2 (softmax(st) + softmax(ˆst+1 0))."
  - [corpus] Missing ablation comparing this averaging method to the original concatenation-based self-conditioning.
- Break condition: If the averaged distribution becomes too smooth or loses discriminative power, the model's generation quality could suffer.

### Mechanism 3
- Claim: Fully non-autoregressive modeling allows generation of arbitrary-length sequences in parallel, improving efficiency and flexibility.
- Mechanism: TESS feeds the entire sequence's latent vector into a transformer encoder and diffuses the whole sequence simultaneously, unlike SSD-LM which generates in blocks of 25 tokens. This allows variable-length output matching the target distribution.
- Core assumption: The transformer can effectively denoise the entire sequence in parallel without loss of context.
- Evidence anchors:
  - [abstract] "performs diffusion on the entire sequence" and "generates sequences of arbitrary length."
  - [section] "Unlike SSD-LM, which feeds small blocks of text to semi-autoregressively generate sequences of text, we feed the entire latent vector into an encoder transformer model."
  - [corpus] Missing direct timing or efficiency comparison for long sequences.
- Break condition: If the transformer's attention mechanism cannot maintain coherence over long sequences, or if variable-length outputs become too noisy, the model's quality could degrade.

## Foundational Learning

- Concept: Gaussian diffusion in continuous spaces
  - Why needed here: Understanding how adding Gaussian noise and reversing it works in continuous domains provides the basis for adapting it to discrete text.
  - Quick check question: What is the role of the noise schedule in diffusion models, and how does it affect the quality of generation?

- Concept: Simplex (probability) space
  - Why needed here: The vocabulary logit simplex is the space where TESS performs diffusion; knowing its properties is key to understanding why it's chosen over embeddings.
  - Quick check question: How does the k-logit simplex representation map discrete tokens to a continuous space suitable for diffusion?

- Concept: Self-conditioning in generative models
  - Why needed here: TESS uses a novel self-conditioning method; understanding the original idea and its benefits helps grasp the improvement.
  - Quick check question: What is the difference between standard conditioning and self-conditioning in diffusion models, and why does it help?

## Architecture Onboarding

- Component map:
  Tokenizer -> k-logit simplex mapper -> Noise adder -> Transformer encoder -> Self-conditioning module -> Argmax projector -> Output tokens

- Critical path:
  1. Input text -> k-logit simplex
  2. Add noise (forward diffusion)
  3. Transformer encoder with self-conditioning
  4. Argmax to get next step's simplex
  5. Repeat for T steps (reverse diffusion)

- Design tradeoffs:
  - Using simplex space avoids extra mapping steps but requires careful handling of probability distributions.
  - Fully non-autoregressive generation is faster but may lose some coherence compared to autoregressive models.
  - Self-conditioning improves quality but adds complexity and requires averaging distributions.

- Failure signatures:
  - If the k-logit simplex becomes too noisy or loses semantic structure, generation quality drops.
  - If the transformer cannot denoise long sequences effectively, outputs become incoherent.
  - If self-conditioning averaging is too aggressive, diversity and quality may suffer.

- First 3 experiments:
  1. Ablation: Compare TESS with and without self-conditioning on a small text generation task (e.g., paraphrase).
  2. Ablation: Compare simplex-space diffusion to embedding-space diffusion on a short sequence task.
  3. Scaling: Vary the number of diffusion steps (T) and measure generation quality and speed on a summarization task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TESS compare to autoregressive models when scaled up to larger model sizes?
- Basis in paper: [inferred] The paper mentions that TESS uses RoBERTa as its underlying model and suggests that scaling up to larger models could potentially improve performance. However, the paper does not provide experimental results comparing TESS with larger autoregressive models.
- Why unresolved: The paper does not include experiments with larger model sizes or direct comparisons with larger autoregressive models.
- What evidence would resolve it: Experimental results comparing TESS with larger autoregressive models, such as GPT-3 or T5, would provide evidence for the performance of TESS at larger scales.

### Open Question 2
- Question: How does the self-conditioning method proposed in TESS compare to other self-conditioning methods in terms of performance and efficiency?
- Basis in paper: [explicit] The paper introduces a new self-conditioning method that exploits the simplex nature of the diffusion space and claims that it is more efficient than the original method. However, the paper does not provide a detailed comparison with other self-conditioning methods.
- Why unresolved: The paper only compares the proposed self-conditioning method with the original method and does not include comparisons with other existing self-conditioning methods.
- What evidence would resolve it: A comprehensive comparison of the proposed self-conditioning method with other existing self-conditioning methods, including their performance and computational efficiency, would provide evidence for the effectiveness of the proposed method.

### Open Question 3
- Question: How does the performance of TESS vary with different noise schedules during the diffusion process?
- Basis in paper: [inferred] The paper uses a cosine noise schedule for the diffusion process, but it does not explore the impact of different noise schedules on the performance of TESS.
- Why unresolved: The paper does not include experiments with different noise schedules or an analysis of how the choice of noise schedule affects the performance of TESS.
- What evidence would resolve it: Experimental results comparing the performance of TESS with different noise schedules, such as linear or exponential schedules, would provide evidence for the impact of noise schedules on the model's performance.

## Limitations

- Lack of direct ablation comparing TESS to variants without self-conditioning or to embedding-space diffusion models, limiting isolation of each proposed mechanism's impact.
- Missing empirical validation of how well the k-logit simplex mapping preserves semantic relationships under high noise levels.
- No timing or efficiency comparisons for long sequences, nor analysis of potential coherence issues from non-autoregressive generation.

## Confidence

- **High Confidence**: Claims about achieving SOTA among non-autoregressive models on generation tasks (e.g., ROUGE-L of 39.7 on summarization, 62.2 on paraphrase generation) are supported by reported metrics and comparisons to baselines like SSD-LM and Smooth-ed-LM.
- **Medium Confidence**: Claims about competitive performance with autoregressive models (e.g., BART) and matching RoBERTa on GLUE tasks are based on reported results, but lack direct ablations or comparisons to state-of-the-art autoregressive models in all cases.
- **Low Confidence**: Claims about the superiority of simplex-space diffusion over embedding-space diffusion and the effectiveness of the self-conditioning method are primarily theoretical, with missing empirical ablations to support these claims.

## Next Checks

1. Conduct an ablation comparing TESS with and without self-conditioning on a small text generation task (e.g., paraphrase generation) to isolate the impact of the self-conditioning mechanism.

2. Compare simplex-space diffusion to embedding-space diffusion on a short sequence task to empirically validate the claimed stability and efficiency benefits of the simplex space.

3. Vary the number of diffusion steps (T) and measure generation quality and speed on a summarization task to assess the robustness of TESS with fewer steps and validate the claimed efficiency.