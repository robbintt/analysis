---
ver: rpa2
title: 'I-WAS: a Data Augmentation Method with GPT-2 for Simile Detection'
arxiv_id: '2308.04109'
source_url: https://arxiv.org/abs/2308.04109
tags:
- simile
- data
- augmentation
- detection
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of limited and biased datasets
  for simile detection in natural language processing. The authors propose a data
  augmentation method called I-WAS, which combines word replacement and sentence completion
  using the GPT-2 language model.
---

# I-WAS: a Data Augmentation Method with GPT-2 for Simile Detection

## Quick Facts
- arXiv ID: 2308.04109
- Source URL: https://arxiv.org/abs/2308.04109
- Reference count: 40
- Primary result: I-WAS method significantly improves simile detection accuracy on a diverse test set compared to baseline methods.

## Executive Summary
This paper tackles the challenge of limited and biased datasets for simile detection in natural language processing. The authors introduce I-WAS, an iterative data augmentation method combining word replacement and sentence completion using GPT-2. By expanding simile sentences with diverse comparative words and filtering for label consistency, the method improves model robustness to real-world simile forms. Experiments show substantial gains in accuracy on a newly compiled diverse test set (DTS), demonstrating the effectiveness of the approach.

## Method Summary
The I-WAS method starts with a base simile detection dataset and applies iterative augmentation. First, comparative words in sentences are replaced with alternatives from a predefined set. GPT-2 then generates multiple candidate sentences using sentence completion. A base simile detection model filters these candidates to retain only those with consistent labels. The augmented dataset is used to retrain the model, and the process repeats. This iterative loop gradually shifts the training distribution toward diverse simile patterns, improving real-world generalization.

## Key Results
- I-WAS improves simile detection accuracy on the diverse DTS test set compared to baseline methods.
- Augmented sentences maintain high content fluency and label consistency.
- Performance on the biased test set decreases with more iterations, while DTS performance increases, indicating effective adaptation to diverse simile forms.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative word replacement and sentence completion improves simile detection accuracy by expanding data diversity while preserving label consistency.
- Mechanism: The I-WAS method replaces comparative words randomly from a predefined set, then uses GPT-2 to generate 10 candidate sentences via sentence completion. A base simile detection model filters candidates, keeping only those with consistent labels. Iterating this process further aligns training data with real-world simile forms.
- Core assumption: GPT-2 can generate grammatically fluent and contextually relevant simile sentences that maintain the original label when guided by a pre-trained detection model.
- Evidence anchors:
  - [abstract] "Our iterative process called I-WAS, is designed to improve the quality of the augmented sentences."
  - [section] "We apply GPT-2 to generate 10 data samples for each sentence using Top-K sampling."
- Break condition: If GPT-2's generated sentences lose label consistency or become too diverse, the filtering step fails to maintain quality, reducing detection performance.

### Mechanism 2
- Claim: Using diverse comparative words in training data improves model robustness to real-world simile forms.
- Mechanism: The method replaces the single comparative word "like" with seven alternatives (e.g., "similar to," "same as"), expanding the training distribution to cover multiple simile patterns. This better matches the test set that contains varied comparative words.
- Core assumption: Simile detection performance improves when training data distribution closely matches test data distribution in terms of comparative word variety.
- Evidence anchors:
  - [abstract] "Our dataset is more diverse than existing datasets, with a total of 606 simile sentences that cover 7 different comparative words."
  - [section] "We replace the comparative word "like" with a randomly selected comparative word from a predefined set."
- Break condition: If the predefined comparative word set is too small or not representative, the model will not generalize well to unseen simile forms.

### Mechanism 3
- Claim: Iterative training with augmented data reduces catastrophic forgetting of rare simile patterns.
- Mechanism: Each iteration augments the training set with new simile variants and retrains the detection model. This gradually shifts the training distribution toward the diverse test set, improving accuracy despite some performance loss on the biased test set.
- Core assumption: Retraining on a gradually shifting data distribution helps the model adapt without severe performance degradation on either dataset.
- Evidence anchors:
  - [section] "As the number of iterations increases, the performance of the model on the BTS gradually decreases, while the performance on the DTS improves."
  - [section] "This can be attributed to the data distribution drift [45]."
- Break condition: If too many iterations cause excessive drift, the model catastrophically forgets the original patterns, harming overall performance.

## Foundational Learning

- Concept: Simile detection as a binary classification task.
  - Why needed here: The method depends on training a classifier to identify simile sentences and filter augmented candidates.
  - Quick check question: What is the output label of the simile detection model used for filtering?
- Concept: GPT-2 sentence completion via top-k sampling.
  - Why needed here: Sentence completion generates diverse simile candidates from masked prompts.
  - Quick check question: How many candidates does GPT-2 generate per sentence in this method?
- Concept: Label consistency in data augmentation.
  - Why needed here: Ensures that augmented sentences maintain the same semantic class as originals, crucial for effective training.
  - Quick check question: How does the method select label-consistent augmented sentences from GPT-2 candidates?

## Architecture Onboarding

- Component map: Original dataset -> Word replacement -> GPT-2 sentence completion -> Label filtering -> Dataset augmentation -> Model retraining -> (loop back to Word replacement)
- Critical path: Word replacement → GPT-2 generation → Label filtering → Dataset augmentation → Model retraining.
- Design tradeoffs:
  - Higher GPT-2 candidate count increases diversity but also noise.
  - More iterations improve real-world generalization but risk catastrophic forgetting.
  - Fixed comparative word set limits unseen simile pattern coverage.
- Failure signatures:
  - Generated sentences lose label consistency → poor filtering performance.
  - Model accuracy drops on biased test set → excessive data drift.
  - Low fluency in GPT-2 outputs → reduced usability of augmented data.
- First 3 experiments:
  1. Run I-WAS with 1 iteration, compare accuracy on DTS vs BTS.
  2. Vary GPT-2 candidate count (5, 10, 15) and measure diversity vs noise trade-off.
  3. Replace predefined comparative word set with a larger set and observe generalization gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of I-WAS scale with different numbers of augmentation iterations, and is there an optimal number of iterations beyond which performance plateaus or degrades?
- Basis in paper: [explicit] The paper discusses the impact of iterative processes on model performance in Figure 1(a), but does not provide a definitive optimal number of iterations.
- Why unresolved: The paper indicates that performance on the BTS decreases while DTS performance improves with more iterations, but does not specify an optimal stopping point or explain the underlying reasons for this trend.
- What evidence would resolve it: Conducting experiments with a wider range of iteration counts and analyzing the trade-off between performance gains and computational costs could identify an optimal number of iterations.

### Open Question 2
- Question: How does the noise level in the augmented data affect the performance of simile detection models, and what strategies can be employed to mitigate the impact of noisy samples?
- Basis in paper: [inferred] The paper mentions that adding different proportions of augmented samples impacts model performance, with noise becoming more significant when the number of augmented samples is smaller.
- Why unresolved: The paper does not provide a detailed analysis of the noise level in the augmented data or propose specific strategies to handle noisy samples.
- What evidence would resolve it: Analyzing the noise level in the augmented data and comparing the performance of simile detection models with and without noise filtering techniques would provide insights into the impact of noise and potential mitigation strategies.

### Open Question 3
- Question: How can the catastrophic forgetting problem in iterative learning for simile detection be addressed to maintain performance on the original training data while improving generalization to diverse simile patterns?
- Basis in paper: [explicit] The paper discusses the issue of catastrophic forgetting in iterative learning and mentions that the performance on the BTS decreases as the number of iterations increases.
- Why unresolved: The paper acknowledges the problem of catastrophic forgetting but does not propose specific solutions to address it.
- What evidence would resolve it: Experimenting with different techniques, such as regularization methods or episodic memory, to mitigate catastrophic forgetting in iterative learning for simile detection would provide insights into potential solutions.

## Limitations

- The method's effectiveness depends heavily on the quality of GPT-2's sentence completion and the robustness of the filtering model.
- The predefined comparative word set may not cover all possible simile forms, limiting generalization.
- The paper does not provide detailed performance metrics or extensive comparative analysis with baseline methods.

## Confidence

- **High Confidence**: The claim that data augmentation using GPT-2 can improve simile detection accuracy is well-supported by the experimental results, particularly the performance improvements on the DTS.
- **Medium Confidence**: The assertion that iterative training with augmented data reduces catastrophic forgetting is plausible but requires further validation, as the experimental setup and results are not fully detailed.
- **Low Confidence**: The claim that the I-WAS method significantly outperforms baseline methods is based on limited comparative analysis, and the specific performance metrics are not provided in the abstract.

## Next Checks

1. Evaluate Label Consistency: Conduct a detailed analysis of the label consistency of GPT-2 generated sentences to ensure they maintain the original simile meaning and are not introducing noise.
2. Test with Expanded Comparative Word Sets: Experiment with larger and more diverse comparative word sets to assess if the method's generalization improves and if it can handle a broader range of simile patterns.
3. Analyze Catastrophic Forgetting: Perform ablation studies to quantify the extent of catastrophic forgetting across iterations and identify the optimal number of iterations to balance adaptation and retention of original patterns.