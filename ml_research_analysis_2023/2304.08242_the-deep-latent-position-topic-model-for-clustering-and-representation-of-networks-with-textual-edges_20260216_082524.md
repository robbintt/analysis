---
ver: rpa2
title: The Deep Latent Position Topic Model for Clustering and Representation of Networks
  with Textual Edges
arxiv_id: '2304.08242'
source_url: https://arxiv.org/abs/2304.08242
tags:
- topic
- node
- cluster
- deep-lptm
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Deep-LPTM, a novel deep probabilistic model
  for jointly clustering nodes and modeling topics in networks with textual edges.
  The model embeds nodes and edges into a latent space using graph convolutional networks
  and topic modeling, enabling simultaneous clustering and visualization.
---

# The Deep Latent Position Topic Model for Clustering and Representation of Networks with Textual Edges

## Quick Facts
- arXiv ID: 2304.08242
- Source URL: https://arxiv.org/abs/2304.08242
- Reference count: 34
- Primary result: Deep-LPTM outperforms state-of-the-art methods (ETSBM, STBM) in node clustering accuracy on synthetic data with ARI improvements up to 0.15

## Executive Summary
This paper introduces Deep-LPTM, a novel deep probabilistic model for jointly clustering nodes and modeling topics in networks with textual edges. The model embeds nodes and edges into a latent space using graph convolutional networks and topic modeling, enabling simultaneous clustering and visualization. Inference is performed via a two-stage variational EM algorithm, with analytical updates for cluster parameters and stochastic gradient descent for deep learning components. A new IC2L criterion is proposed for model selection. Experiments on synthetic data show Deep-LPTM outperforms state-of-the-art methods (ETSBM, STBM) in node clustering accuracy, with ARI improvements up to 0.15. Application to the Enron email dataset demonstrates meaningful topic discovery and interpretable cluster structures. The model effectively leverages both network topology and textual content for improved clustering and representation.

## Method Summary
Deep-LPTM combines variational graph auto-encoders with topic modeling to jointly cluster nodes and discover topics in networks with textual edges. The model represents nodes and edges in separate latent spaces, with node positions capturing network topology through GCNs and edge positions capturing textual content through Embedded Topic Models. A two-stage variational EM algorithm alternates between analytical updates of cluster parameters and SGD updates of deep learning components. The IC2L criterion enables principled model selection for the number of clusters, topics, and latent space dimensions. The model assumes node cluster memberships and topic proportions can be inferred simultaneously from both the adjacency matrix and textual documents on edges.

## Key Results
- Deep-LPTM achieves ARI improvements up to 0.15 compared to ETSBM and STBM on synthetic data
- The model successfully discovers meaningful topics and interpretable cluster structures in the Enron email dataset
- IC2L criterion effectively selects appropriate model complexity parameters (Q, K, P) across synthetic scenarios
- Joint modeling of graph structure and textual edges provides superior clustering performance versus separate modeling approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint modeling of graph structure and textual edges enables simultaneous clustering and visualization
- Mechanism: Deep-LPTM represents nodes and edges in separate latent spaces, then models their distributions as mixtures of Gaussians conditioned on cluster memberships. This allows both network topology and text content to inform cluster assignments and embeddings.
- Core assumption: Node cluster membership and topic proportions can be inferred simultaneously from both the adjacency matrix and textual documents on edges.
- Evidence anchors:
  - [abstract] "enables simultaneous clustering and visualization"
  - [section 2] "Assuming that the number of clusters Q is fixed beforehand, each node i is assumed to belong to a cluster, represented by the cluster membership variable Ci."
  - [corpus] Weak - corpus does not contain papers that explicitly compare joint modeling vs separate modeling
- Break condition: If textual content provides no additional clustering signal beyond network structure, the benefit of joint modeling diminishes.

### Mechanism 2
- Claim: Graph Convolutional Networks (GCNs) effectively encode node position information from network topology
- Mechanism: GCN layers transform the adjacency matrix into node embeddings that capture local neighborhood structure. The encoder µφZ maps the normalized adjacency matrix to posterior mean positions, while σ2φZ provides uncertainty estimates.
- Core assumption: Local connectivity patterns contain sufficient information to infer meaningful latent positions for clustering.
- Evidence anchors:
  - [section 3.1] "The two mappings µφZ and σ2φZ rely on a GCN parametrized by φZ"
  - [section 2.1] "the connection between two nodes is assumed to depend on the closeness of the node representations in the latent space"
  - [corpus] Weak - corpus lacks direct comparisons of GCN-based vs non-GCN node position models
- Break condition: In networks with weak community structure or random topology, GCN embeddings may not capture meaningful positions.

### Mechanism 3
- Claim: Embedded Topic Models (ETM) with pre-trained word embeddings improve topic coherence
- Mechanism: ETM uses word embeddings to represent topics in the same vector space as documents. Pre-trained embeddings (e.g., skip-gram) provide semantic information that helps distinguish topics even with limited training data.
- Core assumption: Word embeddings capture semantic relationships that are useful for distinguishing topics in document collections.
- Evidence anchors:
  - [section 3.1] "In practice, in all the experiments we carried out, we used a feed-forward neural network to encode the documents, with three layers and 800 units on each layer"
  - [section 4.3] "The improvement provided by the pre-trained embeddings depends on the scenario"
  - [corpus] Weak - corpus does not provide quantitative comparisons of ETM with/without pre-trained embeddings
- Break condition: If vocabulary is domain-specific or embeddings are not pre-trained on relevant data, pre-trained embeddings may introduce noise.

## Foundational Learning

- Variational Inference and ELBO optimization
  - Why needed here: The marginal likelihood involves intractable integrals over latent variables; variational inference provides a tractable approximation through the Evidence Lower Bound (ELBO).
  - Quick check question: What is the relationship between the ELBO and the Kullback-Leibler divergence to the true posterior?

- Graph Neural Networks (specifically GCNs)
  - Why needed here: GCNs encode node position information from network topology into low-dimensional embeddings that respect graph structure.
  - Quick check question: How does the normalized adjacency matrix D^(-1/2)AD^(-1/2) help with message passing in GCNs?

- Topic Modeling with Neural Networks
  - Why needed here: Traditional topic models struggle with large vocabularies; neural topic models like ETM use embeddings to represent words and topics in a shared space.
  - Quick check question: What is the role of the softmax function in mapping latent topic proportions to word distributions?

## Architecture Onboarding

- Component map:
  - Input: Binary adjacency matrix A (N×N), document word count matrices W_ij (V-dimensional)
  - Node encoder: GCN with parameters φZ → node posterior means µφZ(A) and variances σ2φZ(A)
  - Edge encoder: Feed-forward network with parameters φY → edge posterior means µφY(W_ij) and variances σ2φY(W_ij)
  - Clustering model: Mixture of Gaussians with parameters π, µ, σ for node clusters; m, s for edge clusters
  - Topic model: Embedded topic model with parameters α, ρ for topic-word distributions
  - Output: Node positions Z, edge positions Y, cluster assignments C, topic proportions θ

- Critical path:
  1. Pre-train ETM on documents to initialize φY, α, ρ
  2. Initialize node cluster memberships using similarity-based method
  3. Alternate between analytical updates (π, µ, σ, m, s) and SGD updates (κ, φZ, φY)
  4. Monitor ELBO and ARI during training

- Design tradeoffs:
  - Joint vs separate modeling: Joint modeling captures interactions but increases complexity
  - GCN depth: Deeper GCNs capture longer-range dependencies but may oversmooth
  - Embedding dimension: Higher dimensions capture more information but risk overfitting
  - Pre-trained embeddings: Provide semantic information but may introduce domain mismatch

- Failure signatures:
  - ELBO plateaus early: Check learning rates, initialization, or model identifiability
  - Poor clustering (low ARI): Insufficient training, inappropriate number of clusters/topics, or weak signal in data
  - Node positions collapse: Check GCN architecture or normalization of adjacency matrix

- First 3 experiments:
  1. Train Deep-LPTM on synthetic Scenario A (easy setting) with pre-trained embeddings; verify ARI ≈ 1
  2. Remove GCN component and use random node positions; compare clustering performance
  3. Train with varying numbers of clusters Q; verify IC2L selects true Q on synthetic data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Deep-LPTM compare to other state-of-the-art methods on real-world networks with textual edges beyond the Enron dataset?
- Basis in paper: [explicit] The paper mentions that Deep-LPTM is evaluated on the Enron email dataset, but does not provide comparisons with other methods on different real-world datasets.
- Why unresolved: The paper only provides results on one real-world dataset, limiting the generalizability of the findings.
- What evidence would resolve it: Additional experiments on other real-world networks with textual edges, such as social media networks or collaboration networks, comparing Deep-LPTM's performance to other state-of-the-art methods.

### Open Question 2
- Question: What is the impact of the dimensionality of the latent space (P) on the performance of Deep-LPTM, and is there an optimal value for different types of networks?
- Basis in paper: [explicit] The paper mentions that the dimension of the node latent space (P) is a parameter that can be selected using the IC2L criterion, but does not provide a detailed analysis of its impact on performance.
- Why unresolved: The paper only provides results for a fixed value of P (P=2) and does not explore the sensitivity of Deep-LPTM to this parameter.
- What evidence would resolve it: A systematic study of the impact of P on the performance of Deep-LPTM across different network types and sizes, identifying optimal values or ranges for different scenarios.

### Open Question 3
- Question: How does Deep-LPTM handle dynamic networks where the structure and content evolve over time?
- Basis in paper: [inferred] The paper focuses on static networks and does not address the issue of dynamic networks. However, the paper mentions that the STBM (a baseline method) has been adapted for dynamic networks, suggesting that this is a relevant extension for Deep-LPTM.
- Why unresolved: The paper does not provide any insights into how Deep-LPTM can be extended or modified to handle dynamic networks.
- What evidence would resolve it: A study of Deep-LPTM's performance on dynamic