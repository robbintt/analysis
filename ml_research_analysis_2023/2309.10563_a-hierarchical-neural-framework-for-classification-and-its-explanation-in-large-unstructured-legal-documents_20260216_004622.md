---
ver: rpa2
title: A Hierarchical Neural Framework for Classification and its Explanation in Large
  Unstructured Legal Documents
arxiv_id: '2309.10563'
source_url: https://arxiv.org/abs/2309.10563
tags:
- legal
- documents
- document
- mesc
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of legal judgment prediction from
  long, unstructured legal documents lacking structural annotations. The proposed
  method, MESc (Multi-stage Encoder-based Supervised with-clustering), tackles this
  by dividing documents into chunks, extracting embeddings from fine-tuned transformer
  encoders, approximating structure labels through clustering, and processing them
  hierarchically for classification.
---

# A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents

## Quick Facts
- arXiv ID: 2309.10563
- Source URL: https://arxiv.org/abs/2309.10563
- Reference count: 40
- Key outcome: MESc improves classification performance by at least 2 points over previous methods; ORSE improves explainability scores by 50%.

## Executive Summary
This paper tackles the challenge of predicting legal judgments from long, unstructured legal documents that lack explicit structural annotations. The authors propose MESc, a multi-stage encoder-based framework that uses clustering to approximate document structure from chunk embeddings, combined with a hierarchical transformer model for classification. Additionally, ORSE is introduced as an occlusion sensitivity-based method to extract relevant sentences as explanations for model predictions. Experiments on ILDC and LexGLUE datasets demonstrate improved classification accuracy and explanation quality compared to prior methods.

## Method Summary
The method proceeds in stages: (1) Document chunks are extracted (512 tokens, 90-token overlap) and processed by a fine-tuned transformer encoder (BERT, GPT-Neo, GPT-J). (2) Embeddings from the last four layers are concatenated. (3) HDBSCAN clustering on these embeddings approximates structural labels. (4) A hierarchical transformer encoder processes the chunk embeddings with the approximated labels, followed by classification via feed-forward neural networks. ORSE explains predictions by ranking sentences according to their occlusion sensitivity: sentences are occluded one-by-one, and their influence on the model loss is measured to produce a weighted relevance score.

## Key Results
- MESc achieves a minimum performance gain of 2 points over previous methods on classification benchmarks.
- ORSE improves explainability scores by 50% compared to baselines.
- Hierarchical modeling with structure approximation outperforms flat classification on long legal documents.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dividing documents into overlapping chunks and fine-tuning an LLM on them captures domain-specific structure for legal documents.
- Mechanism: Fine-tuning forces the model to learn legal domain representations, and clustering chunk embeddings recovers latent structure.
- Core assumption: Legal documents have consistent latent chunk-level similarity that can be recovered by clustering.
- Evidence anchors: Abstract, section 3.1.
- Break condition: If legal documents lack consistent latent similarity, clustering fails and provides noisy labels.

### Mechanism 2
- Claim: Combining embeddings from multiple transformer layers improves representation quality.
- Mechanism: Concatenating last four layer embeddings gives richer features than a single layer.
- Core assumption: Different layers encode different aspects; combining them captures more nuance.
- Evidence anchors: Abstract, section 3.1.
- Break condition: If representations are highly redundant, concatenation adds noise.

### Mechanism 3
- Claim: Occlusion-based sensitivity scoring ranks sentences by their impact on prediction, providing interpretable explanations.
- Mechanism: Model loss change upon sentence occlusion is used to rank sentence importance.
- Core assumption: Sentence importance for prediction correlates with change in model loss when occluded.
- Evidence anchors: Abstract, section 3.2.
- Break condition: If predictions are not strongly sentence-dependent, occlusion sensitivity yields meaningless rankings.

## Foundational Learning

- Concept: Hierarchical modeling for long documents
  - Why needed here: Legal documents exceed typical transformer limits; hierarchical models break them into manageable chunks.
  - Quick check question: What is the maximum sequence length a standard BERT model can process, and why is chunking necessary for legal documents?

- Concept: Unsupervised clustering for structure approximation
  - Why needed here: Legal documents lack explicit structural annotations, so clustering approximates structure without manual labeling.
  - Quick check question: How does HDBSCAN differ from K-means clustering, and why might it be preferable for legal chunk embeddings?

- Concept: Occlusion sensitivity for explanation
  - Why needed here: Without human-annotated explanations, occlusion sensitivity provides a model-agnostic way to rank sentence importance.
  - Quick check question: What does it mean for a sentence to have high occlusion sensitivity, and how does that relate to its influence on the final prediction?

## Architecture Onboarding

- Component map:
  Document → Tokenizer → Chunking → Fine-tuned LLM (BERT/GPT variants) → Layer embedding extraction → Clustering (UMAP+HDBSCAN) → Transformer encoder layers → FFNN → Classification output
  ORSE: Fine-tuned LLM → Chunk embeddings → Sentence-level occlusion → Sensitivity scoring → Ranked sentences

- Critical path:
  1. Chunk document → 2. Fine-tune LLM on chunks → 3. Extract embeddings from last layers → 4. Cluster embeddings for structure labels → 5. Process with transformer encoders → 6. Final classification → 7. ORSE for explanation

- Design tradeoffs:
  - Chunk size vs. overlap: Larger chunks preserve context but increase memory; overlap mitigates boundary effects but raises compute cost.
  - Number of layers combined: More layers may add diversity but risk noise; fewer layers are cleaner but less expressive.
  - Clustering granularity: Smaller clusters capture finer structure but risk overfitting; larger clusters are robust but may miss nuance.

- Failure signatures:
  - Classification drops sharply when structure labels are omitted (confirms clustering dependency).
  - ORSE produces uniform scores across sentences (suggests occlusion sensitivity is ineffective).
  - Training loss plateaus early (indicates embeddings are too noisy or redundant).

- First 3 experiments:
  1. Run MESc with and without structure labels on a small legal dataset to confirm their impact.
  2. Compare performance using embeddings from last 1, 2, and 4 layers to find optimal layer combination.
  3. Apply ORSE to a simple classification model to validate occlusion sensitivity ranking quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal chunk size for balancing document representation quality and computational efficiency in MESc?
- Basis in paper: The paper experiments with different chunk sizes (512 and 2048 tokens) and discusses the trade-offs between fragmenting important information versus computational efficiency.
- Why unresolved: The paper shows that for GPT-J, larger chunk sizes (2048) work better, but doesn't systematically explore the optimal size across different model architectures and document types.
- What evidence would resolve it: Systematic experiments varying chunk sizes from 256 to 4096 tokens across different document types and LLM architectures, measuring both performance and computational costs.

### Open Question 2
- Question: How does the choice of clustering algorithm and dimensionality reduction technique affect the quality of approximated structure labels?
- Basis in paper: The paper uses HDBSCAN with pUMAP for clustering, but acknowledges that performance decreases with higher dimensions and doesn't compare alternative methods.
- Why unresolved: Different clustering algorithms and dimensionality reduction techniques could yield different structure approximations, potentially affecting classification performance.
- What evidence would resolve it: Comparative experiments using alternative clustering methods (e.g., K-means, DBSCAN) and dimensionality reduction techniques (e.g., PCA, t-SNE) to evaluate their impact on classification accuracy.

### Open Question 3
- Question: Can ORSE be generalized to work with non-hierarchical classification models?
- Basis in paper: ORSE is designed specifically for hierarchical models, but the paper suggests it "can also be adapted to shorter documents."
- Why unresolved: The paper only tests ORSE on hierarchical models (MESc) and doesn't explore its applicability to flat classification architectures.
- What evidence would resolve it: Experiments applying ORSE to non-hierarchical transformer models and comparing explanation quality with alternative methods like LIME or SHAP.

## Limitations
- Clustering-based structure approximation may not reliably generalize across different legal systems or document types.
- The optimal number of transformer layers and layer combination strategy are not fully explored.
- Occlusion sensitivity's robustness and independence from model architecture are not fully validated.

## Confidence
- **High Confidence**: The hierarchical approach (MESc) improves classification performance, as evidenced by consistent gains over previous methods (minimum 2 points) and corroborated by internal ablation studies.
- **Medium Confidence**: Occlusion sensitivity (ORSE) effectively ranks sentences for explanation, but its robustness and independence from model architecture are less certain.
- **Low Confidence**: The clustering step reliably approximates legal document structure; the evidence is largely internal and lacks external validation.

## Next Checks
1. Validate that clustering consistently approximates meaningful legal document structure across multiple datasets and legal domains.
2. Compare MESc performance when using embeddings from different layer combinations (e.g., last 1, 2, 4 layers).
3. Test ORSE on models trained with and without structure labels to determine if explanation quality is robust to classification architecture.