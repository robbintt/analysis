---
ver: rpa2
title: 'TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference
  Cost Reduction'
arxiv_id: '2310.15556'
source_url: https://arxiv.org/abs/2310.15556
tags:
- compression
- arxiv
- retrieval
- token
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a token compression scheme for retrieval-augmented
  large language models to reduce inference cost. The approach includes two methods:
  summarization compression using fine-tuned T5 models and semantic compression removing
  low-impact words.'
---

# TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction

## Quick Facts
- arXiv ID: 2310.15556
- Source URL: https://arxiv.org/abs/2310.15556
- Authors: 
- Reference count: 19
- Key outcome: 30% cost reduction for commercial LLM API calls while maintaining high accuracy

## Executive Summary
This paper addresses the high inference costs of retrieval-augmented large language models (LLMs) by proposing a token compression scheme that reduces the size of retrieved context before feeding it to the LLM. The approach introduces two methods: summarization compression using fine-tuned mT5 models and semantic compression that removes low-impact words. Evaluated on a food recommendation dataset, the methods achieve significant token reduction (65% for summarization, 20% for semantic) with minimal accuracy loss (0.3% improvement for summarization, 1.6% drop for semantic), resulting in 30% overall cost reduction for commercial LLM API calls.

## Method Summary
The proposed method compresses retrieved context tokens before they are sent to commercial LLMs to reduce inference costs. Two compression approaches are introduced: summarization compression that uses fine-tuned mT5 models to generate concise summaries of retrieved context, and semantic compression that iteratively removes words with low semantic impact based on sentence-transformer embeddings. The approach is evaluated on a new food recommendation dataset (FRDB) containing 1,000 multiple-choice QA pairs and 7,588 knowledge entries, using accuracy as the primary metric alongside token reduction and cost analysis.

## Key Results
- Summarization compression reduces token size by 65% with 0.3% accuracy improvement
- Semantic compression reduces token size by 20% with only 1.6% accuracy drop
- Overall cost reduction of 30% for commercial LLM API calls achieved
- Fine-tuned mT5 models achieve 90.6% accuracy compared to 60% with pre-trained mT5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Summarization compression can reduce token size while improving accuracy.
- Mechanism: Fine-tuned mT5 models generate concise summaries that retain critical information, reducing input tokens by 65% with a 0.3% accuracy improvement.
- Core assumption: Domain-specific fine-tuning with self-instruct generated datasets helps the summarization model better capture and preserve essential information in the target domain.
- Evidence anchors: [abstract] "Our summarization compression can reduce 65% of the retrieval token size with further 0.3% improvement on the accuracy"

### Mechanism 2
- Claim: Semantic compression removes less important words while maintaining semantic meaning, offering flexible token reduction with minimal accuracy loss.
- Mechanism: Uses sentence-transformer embeddings to measure semantic impact of word removal; iteratively removes words with lower semantic importance, achieving 20% token reduction with only 1.6% accuracy drop.
- Core assumption: Words with lower semantic impact can be removed without drastically altering the sentence meaning, and embedding distance correlates with semantic importance.
- Evidence anchors: [abstract] "semantic compression provides a more flexible way to trade-off the token size with performance, for which we can reduce the token size by 20% with only 1.6% of accuracy drop"

### Mechanism 3
- Claim: Retrieval-augmented LLMs with token compression maintain high accuracy while significantly reducing inference costs.
- Mechanism: Compresses retrieved context before feeding it to the LLM, reducing input tokens and thus API costs by ~30% while maintaining accuracy close to uncompressed retrieval.
- Core assumption: Commercial LLMs can generate accurate answers with compressed context as long as critical information is preserved, and retrieval provides sufficient domain knowledge.
- Evidence anchors: [abstract] "Our method achieves 30% overall cost reduction for commercial LLM API calls while maintaining high accuracy"

## Foundational Learning

- Concept: Self-instruct data generation
  - Why needed here: Enables creation of summarization datasets with varying lengths without manual annotation, allowing fine-tuning of mT5 for domain-specific summarization.
  - Quick check question: How does the self-instruct loop ensure the generated summaries meet the desired length constraints?

- Concept: Sentence embedding similarity and semantic importance
  - Why needed here: Forms the basis for semantic compression by quantifying the impact of word removal on sentence meaning.
  - Quick check question: What distance metric is used to measure semantic deviation between original and perturbed embeddings?

- Concept: Retrieval-augmented generation (RAG) pipeline
  - Why needed here: The context in which token compression is applied; understanding RAG is crucial for integrating compression methods.
  - Quick check question: What are the three main components of a typical RAG system, and how does token compression fit into this pipeline?

## Architecture Onboarding

- Component map: User query -> Dense/Sparse Retriever -> NSP Filtering -> Token Compressor -> LLM -> Answer
- Critical path: User query → Retriever → NSP → Token Compressor → LLM → Answer
- Design tradeoffs:
  - Summarization vs. semantic compression: Summarization offers higher accuracy but less flexibility; semantic compression offers variable length control but lower accuracy.
  - Retrieval method: Dense retrieval (GPT-embedding + FAISS) vs. sparse retrieval (BM25); dense retrieval showed better performance in experiments.
  - Compression level: Higher compression reduces cost but may impact accuracy; need to find optimal balance.

- Failure signatures:
  - Accuracy drops significantly: Likely due to over-compression removing critical information
  - Retrieval returns irrelevant context: Retriever or NSP may be failing
  - Token limit errors: Compression not aggressive enough or retrieved context too long
  - High latency: Token compressor or retriever may be bottlenecks

- First 3 experiments:
  1. Compare accuracy of GPT-3.5-turbo with and without retrieval on FRDB dataset to establish baseline performance gain from retrieval.
  2. Evaluate summarization compression with different fine-tuning lengths (30%, 50%, 70%) to find optimal balance between compression and accuracy.
  3. Test semantic compression with different word removal thresholds to assess trade-off between token reduction and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the summarization compression method vary across different domains beyond food recommendation (e.g., medical, legal, technical domains)?
- Basis in paper: [explicit] The paper focuses on evaluating the summarization compression method on a food recommendation dataset and shows promising results with 65% token reduction and 0.3% accuracy improvement.
- Why unresolved: The paper does not explore the generalizability of the summarization compression method to other domains. The effectiveness of the method might depend on the specific characteristics of the food recommendation domain.
- What evidence would resolve it: Conducting experiments using the summarization compression method on datasets from various domains (medical, legal, technical, etc.) and comparing the results in terms of token reduction and accuracy impact.

### Open Question 2
- Question: What is the optimal trade-off between token compression rate and accuracy loss for the semantic compression method across different types of input text (e.g., short vs. long sentences, simple vs. complex sentences)?
- Basis in paper: [explicit] The paper evaluates the semantic compression method on the food recommendation dataset and shows that it can reduce token size by 20% with only a 1.6% accuracy drop. However, the impact of different input text characteristics on this trade-off is not explored.
- Why unresolved: The paper does not investigate how the characteristics of the input text (e.g., sentence length, complexity) affect the optimal balance between compression rate and accuracy loss for the semantic compression method.
- What evidence would resolve it: Analyzing the performance of the semantic compression method on a diverse set of input texts with varying characteristics (e.g., short vs. long sentences, simple vs. complex sentences) and determining the optimal compression rate for each type to minimize accuracy loss.

### Open Question 3
- Question: How does the cost reduction achieved by the proposed token compression methods scale with the frequency of API calls and the size of the input data?
- Basis in paper: [explicit] The paper provides a cost reduction analysis based on a specific scenario with a certain number of API calls and input data size, showing a 30% overall cost reduction.
- Why unresolved: The paper does not explore how the cost reduction scales with different frequencies of API calls and input data sizes. The cost savings might vary significantly depending on these factors.
- What evidence would resolve it: Conducting a comprehensive cost analysis by varying the frequency of API calls and the size of the input data, and measuring the resulting cost reduction achieved by the proposed token compression methods in each scenario.

## Limitations

- Evaluation is constrained to a single domain-specific dataset (FRDB), limiting generalizability to other domains or tasks
- Accuracy measurements only report final QA accuracy without providing intermediate metrics like retrieval precision/recall or embedding similarity scores
- Self-instruct data generation process relies on GPT-3.5-turbo prompts without detailed prompt templates or quality control measures specified

## Confidence

**High Confidence**: Cost reduction claims (30% overall reduction) are well-supported by explicit calculations in Section 5.3 and align with standard pricing models for commercial LLM APIs. The retrieval-augmented pipeline showing accuracy improvement from 51% to 90.2% is also highly confident as it uses standard RAG components.

**Medium Confidence**: Accuracy preservation claims for both compression methods (0.3% improvement for summarization, 1.6% drop for semantic) are moderately confident but limited by the single dataset evaluation.

**Low Confidence**: Generalizability claims about applying these methods to other domains and languages are low confidence due to lack of cross-domain validation.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate both compression methods on a non-food domain dataset (e.g., biomedical QA or legal document retrieval) to assess whether the 65% compression with 0.3% accuracy improvement holds across different knowledge domains.

2. **Intermediate Metric Analysis**: Track and report retrieval precision/recall, embedding similarity scores (cosine similarity between original and compressed contexts), and information entropy changes during compression to identify at what point accuracy degradation begins.

3. **Prompt Template Sensitivity Analysis**: Systematically vary the self-instruct prompts used for mT5 fine-tuning and measure the impact on summarization quality and downstream accuracy to determine how sensitive the approach is to prompt engineering quality.