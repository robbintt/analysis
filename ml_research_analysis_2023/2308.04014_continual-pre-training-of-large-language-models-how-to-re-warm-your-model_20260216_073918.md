---
ver: rpa2
title: 'Continual Pre-Training of Large Language Models: How to (re)warm your model?'
arxiv_id: '2308.04014'
source_url: https://arxiv.org/abs/2308.04014
tags:
- learning
- data
- arxiv
- language
- pile
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the effectiveness of different warm-up strategies
  for continual pre-training of large language models (LLMs) on new datasets. The
  authors explore how varying the length and maximum learning rate of the warm-up
  phase impacts performance on both the original (upstream) and new (downstream) datasets.
---

# Continual Pre-Training of Large Language Models: How to (re)warm your model?

## Quick Facts
- arXiv ID: 2308.04014
- Source URL: https://arxiv.org/abs/2308.04014
- Authors: 
- Reference count: 26
- Key outcome: Warm-up length doesn't significantly affect downstream performance, but higher maximum learning rates improve downstream results at the cost of upstream forgetting.

## Executive Summary
This paper investigates warm-up strategies for continual pre-training of LLMs on new datasets. Through experiments on a Pythia 410M model, the authors find that while warm-up length doesn't significantly impact downstream performance, using higher maximum learning rates during warm-up improves adaptation to new data. However, this comes at the cost of increased forgetting of upstream data. The study reveals a stability gap phenomenon where re-warming initially increases both upstream and downstream loss, but ultimately improves downstream performance.

## Method Summary
The authors conduct experiments on a Pythia 410M model pre-trained on the Pile dataset, continuing pre-training on SlimPajama. They use AdamW optimizer with β1=0.9, β2=0.95, ε=1e-8, weight decay=0.1, and train with FP16 precision. The experiments vary maximum learning rates (1.5e-4, 3e-4, 6e-4) and warm-up lengths (0%, 0.5%, 1%, 2%) while monitoring validation perplexity on both Pile and SlimPajama datasets. Gradient clipping at 1.0 is applied, and no dropout is used.

## Key Results
- Warm-up length (0%, 0.5%, 1%, 2%) doesn't significantly affect downstream performance.
- Higher maximum learning rates (up to 6e-4) improve downstream performance but increase upstream forgetting.
- Continual pre-training outperforms training from scratch on downstream data, even with significant overlap between upstream and downstream datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Re-warming the learning rate helps adapt the model to new data distributions even when the new data partially overlaps with the original training set.
- Mechanism: The initial spike in loss caused by re-warming (stability gap) pushes the model out of a local minimum in the loss landscape, enabling better exploration of parameter space and adaptation to the downstream dataset.
- Core assumption: The model can escape suboptimal minima without catastrophic forgetting if the learning rate is later decayed.
- Evidence anchors:
  - [abstract] "while rewarming models first increases the loss on upstream and downstream data, in the longer run it improves the downstream performance"
  - [section] "These findings show that altering the maximum learning rate can be an effective way to tradeoff between downstream and upstream performance."
- Break condition: If the downstream dataset is drastically different in distribution, the stability gap may cause permanent forgetting that cannot be recovered by later decay.

### Mechanism 2
- Claim: Continual pre-training outperforms training from scratch on the downstream dataset, even with significant overlap between upstream and downstream data.
- Mechanism: Pre-trained models retain general linguistic knowledge and world understanding that transfers positively to new data, even when fine-tuning on overlapping data.
- Core assumption: Knowledge learned during initial pre-training is not simply duplicated but enhances the model's ability to process new data efficiently.
- Evidence anchors:
  - [abstract] "Continual pre-training with the latest pre-trained checkpoint improves performance"
  - [section] "all the fine-tuned models with a warm-up perform better than the model trained from scratch"
- Break condition: If the downstream dataset is extremely large relative to the upstream, the benefits of pre-training may be outweighed by the need to fully adapt to the new distribution.

### Mechanism 3
- Claim: The length of the warm-up phase does not significantly affect downstream performance in large-scale continual pre-training.
- Mechanism: Once the learning rate reaches its maximum value, the specific path taken to get there has minimal impact on final performance because the cosine decay dominates the training dynamics.
- Core assumption: The downstream task is sufficiently complex that small variations in early training dynamics are negligible compared to the overall learning process.
- Evidence anchors:
  - [section] "The amount of data used for warming up the learning rate does not significantly influence the perplexity on the downstream task"
  - [section] "These results invalidate our hypothesis that using more tokens for warm-up can smooth the transition"
- Break condition: In smaller models or with less complex downstream tasks, the warm-up phase might have a more significant impact on final performance.

## Foundational Learning

- Concept: Learning rate schedules and their impact on optimization dynamics
  - Why needed here: The entire study revolves around how different learning rate strategies affect continual pre-training outcomes
  - Quick check question: What is the purpose of a warm-up phase in learning rate schedules, and how does it differ from the main training phase?

- Concept: Catastrophic forgetting and its relationship to learning rate
  - Why needed here: Understanding how learning rate affects the model's ability to retain knowledge from upstream data while learning new information
  - Quick check question: How does a high learning rate during fine-tuning potentially lead to forgetting of upstream data?

- Concept: Loss landscape and local minima in neural network optimization
  - Why needed here: The study's findings about stability gaps and escape from local minima require understanding of optimization geometry
  - Quick check question: Why might temporarily increasing the learning rate help a model escape a local minimum in the loss landscape?

## Architecture Onboarding

- Component map: Pythia 410M model architecture -> AdamW optimizer with β1=0.9, β2=0.95, ε=1e-8 -> linear warmup + cosine decay learning rate schedule -> Pile (upstream) and SlimPajama (downstream) datasets -> validation using perplexity on both datasets
- Critical path: Pre-training on Pile -> checkpoint selection -> continual pre-training on SlimPajama with various learning rate strategies -> evaluation on both datasets
- Design tradeoffs: Higher maximum learning rate improves downstream performance but increases upstream forgetting; longer warm-up doesn't significantly improve performance but may smooth initial training dynamics
- Failure signatures: Large spike in both upstream and downstream loss at the beginning of re-warming; inability to recover from initial performance degradation when fine-tuning on the same dataset
- First 3 experiments:
  1. Test different warm-up lengths (0%, 0.5%, 1%, 2%) on the same model and dataset to verify the finding that warm-up length doesn't significantly impact performance
  2. Vary the maximum learning rate (1.5e-4, 3e-4, 6e-4) to observe the tradeoff between upstream preservation and downstream adaptation
  3. Compare continual pre-training with training from scratch on the downstream dataset to validate the performance benefits of continual learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of warm-up strategies in continual pre-training of LLMs scale with model size (e.g., 3B, 7B parameters)?
- Basis in paper: [inferred] The paper discusses experiments on a 410M parameter model and mentions plans to verify conclusions at different model scales.
- Why unresolved: The current study only examines a 410M parameter model, leaving uncertainty about whether the observed effects of warm-up length and maximum learning rate would hold for larger models.
- What evidence would resolve it: Conducting experiments with models of 3B and 7B parameters using the same warm-up strategies and comparing performance on upstream and downstream datasets.

### Open Question 2
- Question: How do different distribution shifts between upstream and downstream datasets affect the optimal warm-up strategy?
- Basis in paper: [explicit] The paper acknowledges that their experimental setup involves datasets with high similarity and data overlap, and notes that results may not generalize to setups with different distribution shifts.
- Why unresolved: The study focuses on datasets with significant overlap, so the impact of warm-up strategies on datasets with more diverse distribution shifts remains unknown.
- What evidence would resolve it: Testing warm-up strategies on upstream and downstream datasets with varying degrees of distribution shift, such as domain adaptation scenarios or datasets from different time periods.

### Open Question 3
- Question: Can the negative effects of rewarming on upstream performance be mitigated while maintaining improved downstream performance?
- Basis in paper: [explicit] The paper observes that increasing the maximum learning rate during warm-up improves downstream performance but increases forgetting of upstream data.
- Why unresolved: While the trade-off between upstream and downstream performance is demonstrated, the paper does not explore strategies to mitigate the forgetting of upstream data.
- What evidence would resolve it: Investigating techniques such as regularization methods, replay mechanisms, or adaptive learning rate schedules that could preserve upstream performance while allowing effective adaptation to downstream data.

## Limitations
- Experiments limited to a single model size (410M parameters), raising questions about scalability to larger models.
- High similarity between Pile and SlimPajama datasets may limit generalizability to more diverse distribution shifts.
- Focus on perplexity as primary metric may not capture all aspects of model quality or task-specific performance.

## Confidence
**High Confidence**: Continual pre-training outperforms training from scratch on downstream data; tradeoff between upstream preservation and downstream adaptation through learning rate manipulation.

**Medium Confidence**: Warm-up length doesn't significantly affect downstream performance; mechanism of stability gap and escape from local minima.

**Low Confidence**: Generalizability to much larger models or drastically different dataset distributions; specific mechanisms by which learning rate affects loss landscape.

## Next Checks
- Validation Check 1: Replicate experiments with larger model sizes (1B, 3B, 8B parameters) to test scalability of findings.
- Validation Check 2: Conduct ablation studies with varying degrees of dataset overlap to quantify impact on stability gap phenomenon.
- Validation Check 3: Test different learning rate schedules (warmup + cosine decay vs warmup + linear decay vs constant learning rate) to isolate specific contributions.