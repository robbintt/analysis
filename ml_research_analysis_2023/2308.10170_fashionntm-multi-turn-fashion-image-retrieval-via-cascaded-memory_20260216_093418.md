---
ver: rpa2
title: 'FashionNTM: Multi-turn Fashion Image Retrieval via Cascaded Memory'
arxiv_id: '2308.10170'
source_url: https://arxiv.org/abs/2308.10170
tags:
- multi-turn
- turn
- memory
- image
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FashionNTM, a novel memory-based approach for
  multi-turn feedback-based fashion image retrieval. The method incorporates a Cascaded
  Memory Neural Turing Machine (CM-NTM) to learn complex relationships across multiple
  turns of user feedback.
---

# FashionNTM: Multi-turn Fashion Image Retrieval via Cascaded Memory

## Quick Facts
- arXiv ID: 2308.10170
- Source URL: https://arxiv.org/abs/2308.10170
- Reference count: 40
- Key outcome: FashionNTM outperforms previous state-of-the-art by 50.5% on Multi-turn FashionIQ and 12.6% on Multi-turn Shoes dataset

## Executive Summary
This paper presents FashionNTM, a novel memory-based approach for multi-turn feedback-based fashion image retrieval. The method incorporates a Cascaded Memory Neural Turing Machine (CM-NTM) to learn complex relationships across multiple turns of user feedback. By using multiple controllers and read/write heads to interact with a cascade of memory banks, FashionNTM can retain and integrate information from past turns more effectively than traditional approaches. Extensive experiments demonstrate significant performance improvements over state-of-the-art methods, with the model showing strong memory retention capabilities and robustness to turn order for non-contradictory feedback.

## Method Summary
FashionNTM uses a cascaded memory architecture based on Neural Turing Machines to process multi-turn fashion feedback. The model first extracts single-turn features using FashionVLP, then processes these features through C cascaded memory blocks where each controller receives input from the previous controller's output and current query features. The memory-modified features are compared to target features using cosine similarity, and the model is trained using batch cross-entropy loss. The cascaded design allows the model to preserve turn-order information and feedback content that would be lost in simple aggregation methods, enabling better performance on multi-turn retrieval tasks.

## Key Results
- Achieves 50.5% improvement over previous state-of-the-art on Multi-turn FashionIQ dataset
- Demonstrates 12.6% improvement on newly created Multi-turn Shoes dataset
- User study shows FashionNTM images were favored 83.1% more than other multi-turn models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cascaded Memory Neural Turing Machine (CM-NTM) outperforms vanilla NTM by enabling multiple controllers and read/write heads to interact with a sequence of memory banks, allowing the model to retain and integrate information from past turns.
- Mechanism: The CM-NTM architecture uses C cascaded memory blocks where each controller receives input from the previous controller's output and the current query feature, enabling hierarchical information flow and complex relationship learning across turns.
- Core assumption: Multiple memory stages allow for better retention and integration of historical information than a single memory block.
- Evidence anchors:
  - [abstract] "Unlike vanilla Neural Turing Machine (NTM), our CM-NTM operates on multiple inputs, which interact with their respective memories via individual read and write heads, to learn complex relationships."
  - [section 3.1] "In our cascaded multi-memory setup, the controllers are linked in a chain, such that the memory-modified features are sequentially propagated."
- Break condition: Performance degrades when C becomes too large (e.g., C=16 in Table 5) due to overfitting or increased inference time.

### Mechanism 2
- Claim: FashionNTM demonstrates memory retention across turns, enabling it to handle non-contradictory feedback regardless of turn order.
- Mechanism: The memory network stores relevant features from each turn and retrieves them when needed, allowing the model to maintain context across multiple feedback iterations.
- Core assumption: Stored features in memory are sufficiently discriminative to retrieve relevant information across different turn orders.
- Evidence anchors:
  - [abstract] "Further analysis of the model in a real-world interactive setting demonstrates two important capabilities of our model – memory retention across turns, and agnosticity to turn order for non-contradictory feedback."
  - [section 4.5] "Our proposed approach on the right can learn to retain data from both the turns, and therefore retrieves desirable product in 2 out of 3 cases."
- Break condition: Memory retention fails when feedback becomes contradictory or when the memory size is insufficient to store relevant features.

### Mechanism 3
- Claim: CM-NTM with multiple memories achieves better performance than single-turn aggregation methods (mean or concatenation) by preserving turn-order information and feedback content.
- Mechanism: Instead of aggregating all turns into a single feature vector, CM-NTM processes each turn sequentially through cascaded memory blocks, preserving temporal relationships and allowing the model to weigh recent versus historical information appropriately.
- Core assumption: Turn-order information is crucial for accurate retrieval and cannot be adequately captured by simple aggregation methods.
- Evidence anchors:
  - [section 4.4] "The multi-turn baselines generally perform better than single-turn methods. This is expected as aggregating data by naïvely averaging/concatenating loses feedback content and turn-order information and hence is likely to miss out on important cues."
  - [section 4.4] "Our memory-based approach outperforms all the other multi-turn baselines by a large margin."
- Break condition: Performance plateaus or degrades when the number of turns becomes very large relative to memory capacity.

## Foundational Learning

- Concept: Neural Turing Machine (NTM) architecture
  - Why needed here: FashionNTM builds directly on NTM by extending it with cascaded memory blocks and multiple controllers to handle multi-turn fashion retrieval.
  - Quick check question: What are the three main components of a vanilla NTM and how do they interact?

- Concept: Multi-modal feature fusion
  - Why needed here: The model must combine visual features from query images with textual features from feedback to create joint representations for retrieval.
  - Quick check question: How does FashionVLP extract and fuse multi-modal features for the query and target images?

- Concept: Cascaded attention mechanisms
  - Why needed here: The cascaded design allows each memory block to attend to different aspects of the input, creating hierarchical representations that capture complex relationships across turns.
  - Quick check question: How does the cascaded design differ from standard multi-head attention in transformers?

## Architecture Onboarding

- Component map: FashionVLP feature extraction -> Cascaded Memory NTM (C controllers, C memory blocks, read/write heads) -> Cosine similarity matching -> Batch cross-entropy loss

- Critical path: Input (Image, Text) pairs for each turn -> Feature extraction -> Cascaded memory processing -> Final feature comparison -> Retrieval

- Design tradeoffs: Number of memory stages (C) vs. performance and inference time; Memory size vs. capacity to store complex relationships; Cascaded vs. parallel memory architecture

- Failure signatures: Performance degrades with too many memory stages (overfitting); Memory retention fails with contradictory feedback; Single-turn aggregation methods fail to capture turn-order information

- First 3 experiments:
  1. Vary number of memory stages (C=1,2,4,8,16) while keeping memory size fixed to find optimal configuration
  2. Compare cascaded vs. parallel memory architecture on the same dataset
  3. Test memory retention capability by evaluating performance when varying number of past turns included in input

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FashionNTM scale with increasing numbers of turns in a transaction beyond 4 turns?
- Basis in paper: [explicit] The paper evaluates performance on transactions of 2, 3, and 4 turns but does not explore longer sequences.
- Why unresolved: The datasets used only contain up to 4-turn transactions, limiting experimental scope.
- What evidence would resolve it: Creating or using datasets with longer transaction sequences (5+ turns) and evaluating FashionNTM's performance and memory retention capabilities on these would provide insights into scalability and limitations.

### Open Question 2
- Question: How does FashionNTM's performance compare to transformer-based models with external memory modules for multi-turn retrieval?
- Basis in paper: [inferred] The paper focuses on Neural Turing Machines but does not directly compare against modern transformer architectures with memory augmentation.
- Why unresolved: The field of transformer-based models with external memory is rapidly evolving, and FashionNTM's relative performance is unknown.
- What evidence would resolve it: Implementing transformer models with memory modules (e.g., Memformer, ∞-former) and conducting head-to-head comparisons with FashionNTM on the same datasets would clarify relative strengths and weaknesses.

### Open Question 3
- Question: What is the impact of varying the memory size (N × M dimensions) on FashionNTM's performance and computational efficiency?
- Basis in paper: [explicit] The paper varies the number of memory stages (C) but keeps the memory size fixed at 4 × 768 or 8 × 768 in ablation studies.
- Why unresolved: The interplay between memory size, number of stages, and overall performance is not fully explored.
- What evidence would resolve it: Conducting a comprehensive ablation study varying both the number of memory stages and the memory size dimensions (N and M) would reveal optimal configurations and trade-offs between performance and computational cost.

### Open Question 4
- Question: How does FashionNTM handle contradictory feedback across turns, and can it learn to resolve or prioritize conflicting information?
- Basis in paper: [inferred] The paper demonstrates agnosticity to turn order for non-contradictory feedback but does not address contradictory cases.
- Why unresolved: Real-world user interactions may include conflicting requirements, and the model's behavior in such scenarios is unknown.
- What evidence would resolve it: Creating a dataset or simulation with contradictory feedback across turns and evaluating FashionNTM's ability to handle, resolve, or prioritize conflicting information would provide insights into its robustness and potential areas for improvement.

## Limitations
- Cascaded memory architecture introduces significant computational overhead that scales linearly with the number of stages
- Performance is bounded by the quality of pre-extracted FashionVLP features
- Choice of 8 memory stages for FashionIQ and 4 for Shoes appears somewhat arbitrary without clear justification
- User study methodology with only 10 users and 200 examples provides limited statistical power

## Confidence
- High confidence: Core architectural claims about cascaded memory improving over single-memory NTM are well-supported by quantitative results (50.5% improvement on FashionIQ, 12.6% on Shoes) and ablation studies on memory stages
- Medium confidence: Memory retention and turn-order agnosticity claims are supported by qualitative examples and some quantitative analysis, but would benefit from more rigorous testing across diverse feedback patterns
- Low confidence: User study conclusions about user preference (83.1% favoring FashionNTM) have limited statistical validity due to small sample size

## Next Checks
1. Perform paired t-tests or Wilcoxon signed-rank tests on R@K metrics across all compared methods to establish whether the 50.5% and 12.6% improvements are statistically significant rather than due to random variation

2. Conduct a more comprehensive ablation study varying memory stages (C=1,2,4,8,16) while also varying memory size to identify optimal configurations and determine if the chosen values (C=8 for FashionIQ, C=4 for Shoes) are truly optimal or just reasonable defaults

3. Replicate the user preference study with a larger, more diverse user pool (minimum 30-50 users) and stratified sampling of examples to ensure robust statistical power, including confidence intervals for the preference percentages and analysis of individual user consistency