---
ver: rpa2
title: A Refutation of Shapley Values for Explainability
arxiv_id: '2309.03041'
source_url: https://arxiv.org/abs/2309.03041
tags:
- feature
- shapley
- features
- values
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper refutes the adequacy of Shapley values for feature attribution
  in machine learning explainability. It proves that for any number of features greater
  than 2 or 3, there exist Boolean functions where Shapley values provide misleading
  information about feature importance.
---

# A Refutation of Shapley Values for Explainability

## Quick Facts
- arXiv ID: 2309.03041
- Source URL: https://arxiv.org/abs/2309.03041
- Reference count: 40
- Key outcome: Proves Shapley values can provide misleading feature importance information in Boolean functions for n > 2-3 features

## Executive Summary
This paper presents a theoretical refutation of Shapley values as a foundation for feature attribution in machine learning explainability. Through mathematical proofs, it demonstrates that for Boolean functions with more than 2-3 features, Shapley values can produce misleading importance scores where irrelevant features may receive higher values than relevant ones, and relevant features may receive zero values. The paper constructs specific Boolean functions exhibiting various pathologies of the Shapley value approach, undermining its use as a theoretical basis for explainability methods in high-stakes applications.

## Method Summary
The paper employs theoretical proofs and constructions of Boolean functions to demonstrate specific issues with Shapley values for explainability. The approach involves creating Boolean functions that exhibit pathological behavior when Shapley values are computed, then proving these constructions are valid for any number of features beyond a certain threshold. The proofs analyze how Shapley values calculate feature importance through marginal contributions across all possible coalitions, revealing scenarios where this calculation produces misleading results relative to feature relevance determined through minimal explanation sets.

## Key Results
- Proves that for n > 2 or 3 features, Boolean functions exist where Shapley values mislead about feature importance
- Demonstrates that irrelevant features can receive higher Shapley values than relevant features
- Shows that relevant features can receive zero Shapley values despite being necessary for predictions
- Provides constructions for Boolean functions exhibiting various Shapley value pathologies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shapley values can be misleading when a feature is irrelevant but has non-zero value
- Mechanism: The paper constructs Boolean functions where an irrelevant feature (not needed for any explanation) still receives a non-zero Shapley value through the game-theoretic calculation that considers all possible coalitions
- Core assumption: Feature irrelevance means the feature never appears in any minimal explanation set, yet Shapley values compute importance based on marginal contributions across all feature subsets
- Evidence anchors:
  - [abstract] "irrelevant features can be assigned higher Shapley values than relevant ones, and that relevant features can receive zero Shapley values"
  - [section 3] "I1 issue is such that the feature is irrelevant, but its Shapley value is non-zero"
  - [corpus] Weak - no direct corpus evidence supporting this specific construction
- Break condition: If feature selection methods that only consider necessary features were used instead of the full Shapley coalition framework

### Mechanism 2
- Claim: Irrelevant features can receive higher Shapley values than relevant features
- Mechanism: By constructing Boolean functions where the irrelevant feature's marginal contributions across different coalitions outweigh the relevant feature's contributions, the Shapley calculation assigns higher importance to the irrelevant feature
- Core assumption: The Shapley value formula weights marginal contributions by the number of possible coalitions, allowing irrelevant features to accumulate higher scores through their distribution across subsets
- Evidence anchors:
  - [abstract] "irrelevant features can be assigned higher Shapley values than relevant ones"
  - [section 3] "I2 issue is such that there is at least one irrelevant feature exhibiting a Shapley value larger (in absolute value) than the Shapley of a relevant feature"
  - [corpus] Weak - no direct corpus evidence showing this specific phenomenon
- Break condition: If the Shapley calculation were modified to only consider coalitions that actually matter for the prediction

### Mechanism 3
- Claim: Relevant features can receive zero Shapley values
- Mechanism: The paper constructs functions where a relevant feature's marginal contributions perfectly cancel out across different coalitions, resulting in a net Shapley value of zero despite the feature being necessary for some explanations
- Core assumption: The symmetry and linearity properties of Shapley values can cause positive and negative marginal contributions to exactly offset, even when the feature is fundamentally important
- Evidence anchors:
  - [abstract] "relevant features can receive zero Shapley values"
  - [section 3] "I3 issue is such that the feature is relevant, but its Shapley value is zero"
  - [corpus] Weak - no direct corpus evidence demonstrating this exact cancellation effect
- Break condition: If Shapley values were computed using a different weighting scheme that prevents perfect cancellation

## Foundational Learning

- Concept: Feature relevance vs feature importance distinction
  - Why needed here: The paper hinges on showing that features deemed irrelevant by explanation theory can still receive high Shapley values, requiring understanding of what makes a feature relevant versus important
  - Quick check question: What's the difference between a feature being relevant for a prediction and being assigned high importance by Shapley values?

- Concept: Boolean function construction techniques
  - Why needed here: The proofs rely on carefully constructing Boolean functions that exhibit specific Shapley value pathologies, requiring understanding of how to build functions with desired properties
  - Quick check question: How would you construct a Boolean function where feature x3 is irrelevant but still influences the Shapley value calculation?

- Concept: Game-theoretic Shapley value calculation
  - Why needed here: Understanding the exact formula and mechanics of Shapley value computation is essential to grasp why the constructed examples produce misleading results
  - Quick check question: What are the key steps in computing the Shapley value for a feature in a Boolean function?

## Architecture Onboarding

- Component map:
  Boolean function generator -> Shapley value calculator -> Relevance analyzer -> Issue detector -> Proof verifier

- Critical path:
  1. Generate Boolean function with desired properties
  2. Compute Shapley values for all features
  3. Determine feature relevance through minimal explanation sets
  4. Check for mismatches between Shapley values and relevance
  5. Verify the issue exists and meets proof conditions

- Design tradeoffs:
  - Exhaustive truth-table enumeration ensures exact results but limits scalability to small feature counts
  - Constructive proofs provide generality but require careful mathematical verification
  - Using Boolean functions simplifies analysis but may not capture all real-world complexities

- Failure signatures:
  - Incorrect Shapley values due to implementation errors in the marginal contribution calculation
  - Wrong relevance determination from flawed minimal explanation set computation
  - Failed proofs from incorrect assumptions about feature interactions in constructed functions

- First 3 experiments:
  1. Verify the I1 example function (ÎºI1) produces the claimed Shapley values and relevance results
  2. Test the constructive proof for Proposition 3 by building a 4-variable function and checking the irrelevant feature's non-zero Shapley value
  3. Implement the I5 example and confirm the irrelevant feature has the highest absolute Shapley value across all features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact distribution of Boolean functions exhibiting issues I1, I3, I4, and I5 for different numbers of features?
- Basis in paper: [explicit] The paper concludes with "One direction of research is to develop a better understanding of the distributions of functions exhibiting one or more of the issues of Shapley values."
- Why unresolved: The paper provides lower bounds on the number of functions exhibiting each issue but does not provide the exact distribution.
- What evidence would resolve it: A comprehensive enumeration and analysis of all Boolean functions for various numbers of features, showing the frequency of each issue type.

### Open Question 2
- Question: How do the identified issues with Shapley values for explainability manifest in non-Boolean classification functions?
- Basis in paper: [inferred] The paper focuses exclusively on Boolean functions but acknowledges that "domains could be categorical or ordinal" in the classification context.
- Why unresolved: The paper's proofs and examples are restricted to Boolean functions, leaving the applicability to non-Boolean functions unexplored.
- What evidence would resolve it: Proofs or counterexamples demonstrating whether the identified issues with Shapley values persist in non-Boolean classification functions.

### Open Question 3
- Question: Are there alternative feature attribution methods that avoid the identified issues while maintaining the desirable properties of Shapley values?
- Basis in paper: [explicit] The paper argues against the use of Shapley values as the theoretical underpinning of feature-attribution methods in explainability.
- Why unresolved: The paper focuses on refuting Shapley values but does not propose or analyze alternative methods.
- What evidence would resolve it: Development and rigorous analysis of alternative feature attribution methods that are theoretically sound and practically effective, demonstrating superiority over Shapley values in avoiding the identified issues.

## Limitations
- Theoretical focus on Boolean functions may not generalize to continuous or complex real-world ML models
- Mathematical proofs provide existence guarantees but don't quantify how often these issues occur in practice
- Limited to deterministic Boolean functions, excluding stochastic or probabilistic prediction settings

## Confidence
- High: Mathematical correctness of proofs and existence of constructed pathologies
- Medium: Practical significance for real-world explainability systems and complex ML models
- Low: Quantitative assessment of how frequently these issues appear in practical applications

## Next Checks
1. Replicate the I1-I3 issue constructions with numerical verification on small Boolean functions (3-4 features) to confirm the exact Shapley value calculations
2. Test whether similar Shapley value pathologies appear in simple real ML models (e.g., decision trees on synthetic Boolean datasets) when using approximate Shapley methods
3. Investigate if feature relevance definitions based on model behavior (rather than theoretical minimal explanations) would avoid the identified issues