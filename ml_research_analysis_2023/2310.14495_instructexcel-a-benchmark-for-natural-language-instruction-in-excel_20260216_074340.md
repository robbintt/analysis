---
ver: rpa2
title: 'InstructExcel: A Benchmark for Natural Language Instruction in Excel'
arxiv_id: '2310.14495'
source_url: https://arxiv.org/abs/2310.14495
tags:
- excel
- language
- code
- data
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces INSTRUCT EXCEL, a benchmark for evaluating
  large language models' ability to generate Excel OfficeScripts from natural language
  instructions. The benchmark was constructed by leveraging Excel's Automate feature
  to generate code from user actions on publicly available spreadsheets, resulting
  in over 10k samples across 170+ Excel operations.
---

# InstructExcel: A Benchmark for Natural Language Instruction in Excel

## Quick Facts
- **arXiv ID**: 2310.14495
- **Source URL**: https://arxiv.org/abs/2310.14495
- **Reference count**: 25
- **Key outcome**: Benchmark construction using Excel's Automate feature generated over 10k samples across 170+ Excel operations

## Executive Summary
InstructExcel introduces a benchmark for evaluating large language models' ability to generate Excel OfficeScripts from natural language instructions. The benchmark was constructed by leveraging Excel's Automate feature to generate code from user actions on publicly available spreadsheets, resulting in over 10k samples across 170+ Excel operations. Experiments with state-of-the-art models like GPT-4 and T5 showed that the benchmark is challenging, with performance improving when using GPT-4 over GPT-3.5, providing more in-context examples, and employing dynamic prompting. The paper also demonstrated the utility of InstructExcel through a case study on conditional formatting, showing it can be used to derive specific benchmarks for individual Excel tasks.

## Method Summary
The benchmark construction leveraged Excel's Automate feature to generate OfficeScript code from user actions on publicly available spreadsheets, creating a dataset of over 10k samples across 170+ Excel operations. The evaluation involved testing various models including GPT-3.5 Turbo, GPT-4, and a finetuned T5 model using zero-shot, few-shot, and max-shot in-context learning setups. The experiments explored the impact of including the API in context and dynamic prompting, with performance measured using Exact Match (EM), ROUGE, F1, and SacreBLEU scores, along with a function-based evaluation for semantic equivalence.

## Key Results
- GPT-4 significantly outperforms GPT-3.5 Turbo on the benchmark
- Providing more in-context examples consistently improves model performance
- Dynamic prompting through semantic similarity search of training examples enhances performance
- Including the full OfficeScripts API in context sometimes harms performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic prompting improves performance by exposing relevant API elements in context
- Mechanism: The model searches for semantically similar natural language inputs in the training set and appends the top examples, increasing the likelihood that correct API elements are directly exposed in the prompt
- Core assumption: Semantically similar examples contain relevant API patterns that help the model generate correct code
- Evidence anchors:
  - [abstract] "dynamic prompting can help improve performance on this benchmark"
  - [section 4.5] "For dynamic prompting, given a natural language input, we search for the most similar natural language inputs in the training set, measured according to F1 score of the normalized text"
  - [corpus] "Investigating Instruction Tuning Large Language Models on Graphs" shows related work on instruction tuning with structured data
- Break condition: When the most similar examples do not contain relevant API patterns or when the model fails to recognize the patterns despite their presence

### Mechanism 2
- Claim: Including more in-context examples improves few-shot learning capabilities
- Mechanism: Additional examples provide the model with more patterns and templates for mapping natural language to OfficeScript code, reducing the search space for correct solutions
- Core assumption: The model can effectively extract patterns from multiple examples and apply them to new inputs
- Evidence anchors:
  - [abstract] "providing more in-context examples... can help improve performance on this benchmark"
  - [section 5] "We observe that... (2) providing more in-context examples... help improve performance"
  - [corpus] "Investigating Instruction Tuning Large Language Models on Graphs" suggests instruction tuning benefits from diverse examples
- Break condition: When the examples are too diverse or contain conflicting patterns that confuse the model

### Mechanism 3
- Claim: GPT-4 outperforms GPT-3.5 Turbo due to superior reasoning and pattern recognition
- Mechanism: GPT-4's larger architecture and training allows it to better understand complex natural language instructions and generate more accurate code mappings
- Core assumption: The performance difference is due to architectural improvements rather than domain-specific knowledge
- Evidence anchors:
  - [abstract] "We observe that (1) using GPT-4 over GPT-3.5... can help improve performance on this benchmark"
  - [section 5] "We find that GPT-4 outperforms GPT-3.5 Turbo"
  - [corpus] "The Instruction Gap: LLMs get lost in Following Instruction" suggests model size affects instruction following capability
- Break condition: When the task complexity exceeds GPT-4's reasoning capabilities or when domain-specific knowledge is required that neither model possesses

## Foundational Learning

- Concept: Natural language to code mapping
  - Why needed here: The core task is converting user instructions into executable OfficeScript code
  - Quick check question: Can you explain how "highlight the first row" translates to OfficeScript API calls?

- Concept: Few-shot learning with in-context examples
  - Why needed here: Models learn to generate correct code by seeing examples of similar instruction-to-code mappings
  - Quick check question: What information do in-context examples provide that helps the model generate correct output?

- Concept: Dynamic programming and API understanding
  - Why needed here: OfficeScripts are TypeScript programs that require understanding of the Excel API structure and programming concepts
  - Quick check question: How would you describe the difference between "setFormulaLocal" and "setRule" methods in OfficeScript?

## Architecture Onboarding

- Component map: Data collection → Prompt engineering → Model inference → Evaluation → Manual annotation
- Critical path: Natural language instruction → In-context examples → Model generation → Code evaluation
- Design tradeoffs: Context window size vs. number of examples, automated metrics vs. manual evaluation, API inclusion vs. prompt size
- Failure signatures: Hallucinated API elements, incorrect cell ranges, misunderstood user intentions, overwriting important data
- First 3 experiments:
  1. Compare zero-shot performance across GPT-3.5 Turbo and GPT-4 to establish baseline differences
  2. Test different numbers of in-context examples (1, 3, 10) to find optimal few-shot learning performance
  3. Implement dynamic prompting and compare against static prompting with same number of examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would increasing the context window size beyond the current 32k limit impact the performance of GPT-4 on INSTRUCT EXCEL?
- Basis in paper: Inferred from the discussion on data truncation and context size limitations.
- Why unresolved: The paper hypothesizes that increasing context window size could improve performance, but this has not been empirically tested.
- What evidence would resolve it: Conducting experiments with models that have larger context windows and comparing their performance to the current results.

### Open Question 2
- Question: Would incorporating local language support in INSTRUCT EXCEL significantly improve its utility for non-English speaking users?
- Basis in paper: Explicitly stated as a limitation of the current work.
- Why unresolved: The benchmark currently focuses on English instructions, and the impact of local language support is not explored.
- What evidence would resolve it: Expanding the dataset to include non-English instructions and evaluating model performance on this expanded dataset.

### Open Question 3
- Question: How does the performance of instruction-tuned models on INSTRUCT EXCEL compare to models trained specifically on Excel-related data?
- Basis in paper: Inferred from the discussion on finetuning and the use of general-purpose language models.
- Why unresolved: The paper focuses on finetuning general-purpose models and does not explore models trained on Excel-specific data.
- What evidence would resolve it: Training models on a dataset specifically focused on Excel operations and comparing their performance to the current models on INSTRUCT EXCEL.

## Limitations
- Benchmark only captures actions that Excel's Automate feature can recognize, missing complex or nuanced user operations
- Dataset is limited to publicly available spreadsheets, potentially not representing full diversity of real-world Excel usage
- Performance improvements may be due to architectural differences rather than domain-specific knowledge

## Confidence
- **High Confidence**: GPT-4 outperforms GPT-3.5 Turbo and providing more in-context examples improves performance
- **Medium Confidence**: Effectiveness of dynamic prompting and benchmark's representativeness of real-world Excel usage
- **Medium Confidence**: Performance improvements are due to architectural differences rather than domain-specific knowledge

## Next Checks
1. Test the benchmark's models on Excel operations not captured by the Automate feature to assess their ability to generalize beyond the benchmark's scope
2. Evaluate the models using a diverse set of user-created Excel files and instructions that weren't publicly available to verify the benchmark's applicability to actual user scenarios
3. Conduct a detailed analysis of the semantic similarity approach in dynamic prompting to verify that the most similar examples consistently contain relevant API patterns across different instruction types