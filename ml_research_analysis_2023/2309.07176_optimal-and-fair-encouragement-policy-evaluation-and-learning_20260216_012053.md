---
ver: rpa2
title: Optimal and Fair Encouragement Policy Evaluation and Learning
arxiv_id: '2309.07176'
source_url: https://arxiv.org/abs/2309.07176
tags:
- treatment
- policy
- fairness
- constraints
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper develops a method for optimizing encouragement policies
  that are constrained by fairness, addressing the gap in take-up of beneficial services
  among those who could benefit the most. It introduces a two-stage algorithm for
  solving constrained optimization over policy classes, ensuring variance-sensitive
  regret bounds.
---

# Optimal and Fair Encouragement Policy Evaluation and Learning

## Quick Facts
- arXiv ID: 2309.07176
- Source URL: https://arxiv.org/abs/2309.07176
- Reference count: 40
- One-line primary result: A two-stage algorithm with variance-sensitive regret bounds for optimizing encouragement policies under fairness constraints, validated on SNAP benefits and health insurance enrollment case studies.

## Executive Summary
This paper addresses the challenge of optimizing encouragement policies (e.g., notifications, subsidies) that are constrained by fairness requirements, particularly when certain populations systematically underutilize beneficial services. The authors develop a two-stage algorithm that learns nuisance models, identifies binding fairness constraints, and re-optimizes with augmented constraints and variance regularization. Through case studies on SNAP benefits and health insurance enrollment, they demonstrate that optimal policies can reduce disparities in access with minimal impact on overall utility. The method uses doubly-robust estimation for improved statistical properties and provides robustness checks for violations of overlap in algorithmic recommendations.

## Method Summary
The method involves a two-stage algorithm for constrained policy optimization. In Stage 1, nuisance models (propensity for treatment and outcome models) are estimated on one data split, and an initial policy distribution is obtained using weighted classification reduction and saddle-point optimization. The algorithm identifies binding fairness constraints and estimates their variances. In Stage 2, the policy is re-optimized on a second data split using augmented constraints that enforce variance regularization and tighter slacks based on the estimated variances. This focuses optimization on policies close to the first-stage optimum. The approach uses doubly-robust estimation to protect against misspecification of nuisance models and includes robust extrapolation methods for handling overlap violations in algorithmic recommendations.

## Key Results
- Two-stage algorithm achieves variance-sensitive regret bounds by focusing optimization on low-regret policy slices
- Doubly-robust estimators provide statistical efficiency when nuisance models are consistent and robustness when models are misspecified
- Case studies show optimal policies can reduce disparities in SNAP benefits access and health insurance enrollment with minimal utility loss
- Method handles deterministic algorithmic recommendations through robust extrapolation using human treatment randomness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The two-stage algorithm reduces variance-sensitive regret bounds by conditioning optimization on low-regret policy slices.
- **Mechanism:** The method first learns nuisance models and an initial policy distribution on a first data split, identifies binding fairness constraints and their variances, then re-optimizes on a second data split using augmented constraints and tighter slacks based on estimated variances. This focuses the second-stage optimization on policies close to the first-stage optimum rather than the full policy space.
- **Core assumption:** The first-stage estimate is within a small regret radius of the true optimum, and nuisance model estimation error is bounded.
- **Evidence anchors:**
  - [abstract]: "develop a two-stage algorithm for solving over parametrized policy classes under general constraints to obtain variance-sensitive regret bounds"
  - [section]: "split the data into two subsets D1, D2, and first learn nuisance estimators... We run Algorithm 1... on data from D1 to obtain an estimate of the optimal policy distribution Q1, and the constraint variances at Q1... Next, we augment the constraint matrix with additional constraints that require feasible policies for the second-stage policy distribution to achieve ϵn close policy value and constraint moment values relative to Q1"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.373, average citations=0.0. Top related titles: Optimal Transport Learning: Balancing Value Optimization and Fairness in Individualized Treatment Rules, Deep Attention Q-Network for Personalized Treatment Recommendation, Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy Learning.

### Mechanism 2
- **Claim:** Doubly-robust estimation protects against misspecification of either the propensity or outcome models while achieving faster statistical convergence when both are consistent.
- **Mechanism:** The policy value and constraint estimators use both inverse probability weighting and outcome regression terms, with the latter reweighted by the inverse propensity. This orthogonalization means that if either the propensity or outcome model is consistent, the estimator remains consistent, and if both are consistent, the estimator achieves √n-rate convergence.
- **Core assumption:** Either the propensity model or the outcome model is consistently estimated.
- **Evidence anchors:**
  - [abstract]: "develop statistically improved estimators and robustness checks for the setting of algorithmic recommendations with sufficiently randomized decisions"
  - [section]: "improve statistical properties of estimation by developing doubly robust estimators which can achieve faster statistical convergence when both the probability of recommendation assignment... and the probability of outcome are consistently estimated; or otherwise protect against misspecification of either model"
  - [corpus]: Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy Learning.

### Mechanism 3
- **Claim:** Robust extrapolation under violations of overlap in algorithmic recommendations allows the method to handle deterministic classifiers by leveraging randomness in human treatment decisions.
- **Mechanism:** When algorithmic recommendations do not satisfy overlap, the method uses parametric extrapolation for treatment response probabilities and bounds the policy value using uncertainty sets (e.g., Lipschitz smoothness, uniform bounds). This allows the method to optimize over the space of treatment probabilities even when recommendation probabilities are degenerate.
- **Core assumption:** There is sufficient randomness in human treatment decisions to satisfy overlap in treatment probabilities, even if recommendations are deterministic.
- **Evidence anchors:**
  - [abstract]: "Our framework can be extended to handle algorithmic recommendations under an often-reasonable covariate-conditional exclusion restriction, using our robustness checks for lack of positivity in the recommendation"
  - [section]: "we derive robustness checks in this setting... we derive the following alternative identification based on marginal control variates... On the other hand, parametric extrapolation is generally unsatisfactory because conclusions will be driven by model specification rather than observed data"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.373, average citations=0.0.

## Foundational Learning

- **Concept:** Neyman-Rubin potential outcomes framework
  - Why needed here: The paper's causal identification and estimation rely on potential outcomes notation (Y(T(R)), T(R)) and assumptions like consistency and unconfoundedness. Understanding this framework is essential to follow the identification arguments.
  - Quick check question: What does Y(t(r)) represent in the notation used in the paper?

- **Concept:** Doubly-robust estimation
  - Why needed here: The method uses doubly-robust estimators for both policy value and fairness constraints. Understanding how these estimators combine inverse probability weighting and outcome regression is crucial for grasping the statistical properties.
  - Quick check question: Why does doubly-robust estimation protect against misspecification of either the propensity or outcome model?

- **Concept:** Constrained optimization and Lagrangian duality
  - Why needed here: The optimal policy is characterized as the solution to a constrained optimization problem with fairness constraints. The threshold structure of the optimal policy follows from duality arguments in infinite-dimensional linear programming.
  - Quick check question: How does the Lagrangian relaxation allow the constrained problem to be solved via a weighted classification reduction?

## Architecture Onboarding

- **Component map:** Nuisance model estimation (propensity, outcome) -> Two-stage algorithm (Stage 1: estimate optimal policy and variances; Stage 2: re-optimize with augmented constraints) -> Fairness constraint formulation (linear constraints in reductions framework) -> Robust extrapolation module (for overlap violations) -> Weighted classification reduction (for solving saddle point problem)

- **Critical path:** Nuisance model estimation → First-stage optimization → Variance estimation → Second-stage optimization with augmented constraints → Output policy distribution

- **Design tradeoffs:**
  - Two-stage vs. single-stage: Two-stage reduces variance at the cost of sample splitting and computational complexity
  - Doubly-robust vs. regression adjustment: Doubly-robust protects against model misspecification but requires estimating both models
  - Parametric vs. non-parametric extrapolation: Parametric is simpler but relies on model assumptions; non-parametric is more robust but may require stronger assumptions

- **Failure signatures:**
  - First-stage estimate far from true optimum → Second-stage optimization ineffective
  - Both propensity and outcome models misspecified → Doubly-robust estimator biased
  - Insufficient randomness in human decisions → Overlap violation not handled properly

- **First 3 experiments:**
  1. Validate the two-stage algorithm on a synthetic dataset with known optimal policy and compare variance of regret bounds to single-stage method
  2. Test doubly-robust vs. regression adjustment estimators on a semi-synthetic dataset with simulated model misspecification
  3. Evaluate robust extrapolation under overlap violations on a dataset with deterministic recommendations and varying levels of human randomness

## Open Questions the Paper Calls Out
- **Open Question 1:** How robust are the proposed methods to violations of the conditional exclusion restriction assumption (Assumption 2)?
- **Open Question 2:** How do the proposed methods perform in settings with high-dimensional covariates and complex treatment effect heterogeneity?
- **Open Question 3:** How can the proposed methods be extended to handle continuous treatments or outcomes?

## Limitations
- Strong overlap assumption in recommendation space may not hold for deterministic algorithmic policies
- Doubly-robust estimators require consistent nuisance model estimation, with untested performance under model misspecification
- Variance-sensitive regret bounds rely on correctly identifying binding constraints in first stage, which may fail if initial estimate is far from optimal

## Confidence
- **High confidence:** The two-stage algorithmic framework and its connection to constrained optimization are well-specified and theoretically sound.
- **Medium confidence:** The doubly-robust estimation approach should improve statistical efficiency when nuisance models are consistent, but empirical validation is needed.
- **Medium confidence:** The robust extrapolation methods for overlap violations are theoretically justified, but their practical performance depends heavily on the quality of parametric assumptions or the tightness of uncertainty sets.

## Next Checks
1. Conduct simulation studies varying the degree of overlap violation in recommendation space to evaluate the performance of robust extrapolation methods compared to standard IPW approaches.
2. Test the two-stage algorithm on semi-synthetic datasets where the true optimal policy is known, measuring how often the first-stage identifies the correct binding constraints.
3. Implement sensitivity analysis for unobserved confounding by introducing hidden bias parameters and measuring how policy recommendations change under different confounding scenarios.