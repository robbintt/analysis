---
ver: rpa2
title: On the Robustness of Neural Collapse and the Neural Collapse of Robustness
arxiv_id: '2311.07444'
source_url: https://arxiv.org/abs/2311.07444
tags:
- neural
- collapse
- epoch
- robustness
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the stability and prevalence of Neural
  Collapse (NC) under adversarial attacks and in adversarially trained models. The
  authors find that standard neural networks trained on clean data exhibit NC, but
  small adversarial perturbations destroy this structure.
---

# On the Robustness of Neural Collapse and the Neural Collapse of Robustness

## Quick Facts
- arXiv ID: 2311.07444
- Source URL: https://arxiv.org/abs/2311.07444
- Reference count: 40
- This paper investigates the stability and prevalence of Neural Collapse (NC) under adversarial attacks and in adversarially trained models.

## Executive Summary
This paper investigates the stability and prevalence of Neural Collapse (NC) under adversarial attacks and in adversarially trained models. The authors find that standard neural networks trained on clean data exhibit NC, but small adversarial perturbations destroy this structure. In contrast, adversarially trained models show NC for both clean and perturbed data, forming aligned simplices. Early layers of standard models maintain some NC on adversarial examples, leading to surprisingly robust nearest-neighbor classifiers. The results suggest that NC is not inherently robust to adversarial perturbations, but can be induced through robust training.

## Method Summary
The paper trains VGG and ResNet18 models on CIFAR-10 and CIFAR-100 using standard training, adversarial training (AT), and TRADES for 400 epochs. Models are evaluated on clean and adversarially perturbed data using Neural Collapse metrics (NC1-NC4), accuracy, loss, and geometric measures. The analysis focuses on the terminal phase of training where NC emerges, comparing the simplex structure of class-means across different training regimes and network layers.

## Key Results
- Standard training leads to NC for clean data but small adversarial perturbations destroy the simplex structure
- Adversarially trained models exhibit NC for both clean and perturbed data, forming aligned simplices
- Early layers of standard models maintain some NC on adversarial examples, enabling robust nearest-neighbor classifiers
- TRADES does not induce NC-like behavior for perturbed data despite being a robust training method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural Collapse (NC) emerges when class-mean feature vectors align to form a simplex equiangular tight frame (ETF) at convergence.
- Mechanism: During terminal phase of training (post zero-error), within-class variability collapses and class-means maximize mutual angular separation, creating a robust geometric arrangement.
- Core assumption: Training continues past zero training error and SGD implicitly biases toward maximum-separation solutions.
- Evidence anchors:
  - [abstract] "Neural Collapse refers to the curious phenomenon in the end of training of a neural network, where feature vectors and classification weights converge to a very simple geometrical arrangement (a simplex)."
  - [section] "While it has been observed empirically in various cases and has been theoretically motivated..."
- Break condition: Adversarial perturbations or alternative training objectives (e.g., TRADES) disrupt simplex formation.

### Mechanism 2
- Claim: Standard training leads to fragile NC under adversarial perturbations; AT restores NC for both clean and perturbed data.
- Mechanism: Standard training yields simplex only for clean data; small adversarial perturbations destroy simplex structure. AT forces network to maintain simplex ETF for perturbed examples by incorporating worst-case loss.
- Core assumption: Adversarially trained networks learn representations that remain within-class concentrated even under ℓ∞ perturbations.
- Evidence anchors:
  - [abstract] "We find that the simplex structure disappears under small adversarial attacks... Moving forward, we pose the question of whether Neural Collapse can appear in other settings... In particular in robust training settings... we find that Neural Collapse also happens during adversarial training..."
  - [section] "Interestingly, we find that Neural Collapse also happens during adversarial training... and, in fact, simplices form both for the 'clean' original samples and the adversarially perturbed training data."
- Break condition: Non-robust training methods (e.g., TRADES) do not induce simplex formation for perturbed data.

### Mechanism 3
- Claim: Early layers exhibit more robust NC on adversarial examples, enabling simple nearest-neighbor classifiers to retain accuracy.
- Mechanism: Earlier layers maintain lower variability collapse on perturbed data, preserving some simplex-like geometry; nearest-neighbor classifiers based on early-layer centers are surprisingly robust.
- Core assumption: Feature propagation through network layers degrades simplex structure for adversarial data; earlier layers are less affected.
- Evidence anchors:
  - [abstract] "By studying the propagation of the amount of collapse inside the network, we identify novel properties of both robust and non-robust machine learning models, and show that earlier, unlike later layers maintain reliable simplices on perturbed data."
  - [section] "Analyzing NC metrics in the representations of the inner layers, we observe that initial layers exhibit a higher degree of collapse on adversarial data... This phenomenon disappears in later layers."
- Break condition: Very deep networks or aggressive adversarial training may reduce early-layer collapse advantage.

## Foundational Learning

- Concept: Simplex equiangular tight frame (ETF)
  - Why needed here: NC geometry is defined by class-means forming an ETF—maximally separated unit vectors.
  - Quick check question: What angular separation do vertices of a C-class simplex ETF have?
    - Answer: arccos(-1/(C-1))

- Concept: Adversarial training (AT)
  - Why needed here: AT is the method that induces NC on both clean and perturbed data; contrast with standard training.
  - Quick check question: How does AT differ from standard cross-entropy training in terms of loss computation?
    - Answer: AT uses worst-case loss over ℓ∞ perturbations generated by PGD during training.

- Concept: Terminal Phase of Training (TPT)
  - Why needed here: NC emerges after zero training loss; TPT is the regime where collapse and simplex formation are observed.
  - Quick check question: At what point in training does NC typically become measurable?
    - Answer: Well beyond the point of (effectively) zero training loss.

## Architecture Onboarding

- Component map: Data → Model (CNN backbone) → Penultimate layer features → Linear classifier → Loss (CE/AT/TRADES)
- Critical path: Feature extraction → class-mean computation → NC metrics (NC1-4) → analysis of simplex stability
- Design tradeoffs: Standard training: high clean accuracy, fragile NC; AT: lower clean accuracy, robust NC; TRADES: balanced clean/robust, no NC
- Failure signatures: NC1/2/3/4 metrics plateau or diverge under adversarial perturbations; Simplex Similarity near zero for targeted attacks
- First 3 experiments:
  1. Train standard VGG/ResNet18 on CIFAR-10, measure NC metrics on clean vs. PGD-perturbed data.
  2. Train same architecture with AT (ℓ∞ radius 8/255), compare NC metrics for clean and perturbed data.
  3. Measure layerwise NC collapse for ST vs. AT models; test nearest-neighbor classifier accuracy on perturbed data at each layer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the emergence of Neural Collapse in adversarially trained models contribute to their improved robustness, or is it merely a side effect of the training process?
- Basis in paper: [inferred] The paper observes that adversarially trained models exhibit Neural Collapse for both clean and perturbed data, and suggests this may contribute to robustness, but does not conclusively establish causality.
- Why unresolved: The paper establishes correlation between Neural Collapse and robustness in AT models but does not perform controlled experiments to isolate the effect of Neural Collapse on robustness.
- What evidence would resolve it: Experiments comparing the robustness of AT models with and without induced Neural Collapse (e.g., through architectural constraints or targeted interventions during training) would help establish causality.

### Open Question 2
- Question: Can the "cluster leaping" phenomenon observed in standardly trained models be leveraged to develop more effective adversarial attack strategies?
- Basis in paper: [explicit] The paper describes how adversarial perturbations in standard models cause representations to "leap" to the vicinity of target class means, but does not explore the implications for attack design.
- Why unresolved: The paper only characterizes this phenomenon qualitatively and does not investigate whether this behavior can be exploited for more efficient or transferable attacks.
- What evidence would resolve it: Developing attack algorithms that explicitly target this "cluster leaping" behavior and evaluating their effectiveness compared to standard attacks would provide insights into its exploitability.

### Open Question 3
- Question: What is the relationship between the power-law decrease of Neural Collapse across layers and the phenomenon of adversarial robustness in early layers of standard models?
- Basis in paper: [explicit] The paper observes that early layers in standard models show some Neural Collapse on adversarial data and produce surprisingly robust nearest-neighbor classifiers, but does not explain the underlying mechanism.
- Why unresolved: The paper describes the phenomenon but does not provide a theoretical explanation for why early layers exhibit this behavior or how it relates to the observed robustness.
- What evidence would resolve it: Theoretical analysis of how adversarial perturbations propagate through network layers, combined with empirical studies of feature representations at different depths, could elucidate the relationship between layer-wise Neural Collapse and robustness.

## Limitations

- The analysis is limited to CIFAR-10/100 datasets with specific network architectures (VGG and ResNet18), which may not generalize to other datasets or deeper architectures.
- The study focuses on ℓ∞ bounded perturbations, but other attack types or threat models may yield different NC behaviors.
- The theoretical justification for why AT induces NC on perturbed data remains incomplete - the mechanism appears empirical rather than analytically proven.

## Confidence

- **High confidence**: Standard training leads to fragile NC under adversarial perturbations; AT restores NC for both clean and perturbed data.
- **Medium confidence**: Early layers maintain more robust NC on adversarial examples, enabling simple nearest-neighbor classifiers to retain accuracy.
- **Medium confidence**: TRADES does not induce NC-like behavior for perturbed data despite being a robust training method.

## Next Checks

1. **Cross-architecture validation**: Test NC behavior on different architectures (e.g., WideResNet, EfficientNet) and datasets (e.g., ImageNet, SVHN) to verify generalization.
2. **Alternative attack types**: Evaluate NC stability under ℓ2-bounded attacks, Carlini-Wagner attacks, and adaptive attacks to understand perturbation sensitivity.
3. **Theoretical analysis**: Develop mathematical framework explaining why AT induces NC on perturbed data while TRADES does not, focusing on loss landscape geometry.