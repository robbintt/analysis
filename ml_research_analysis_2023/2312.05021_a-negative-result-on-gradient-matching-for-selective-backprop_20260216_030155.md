---
ver: rpa2
title: A Negative Result on Gradient Matching for Selective Backprop
arxiv_id: '2312.05021'
source_url: https://arxiv.org/abs/2312.05021
tags:
- batch
- random
- loss-based
- subsampling
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Selective Backprop, a method to speed up
  neural network training by dropping low-loss examples after the forward pass. The
  authors propose a gradient-matching approach to select a subset of data that best
  approximates the mean gradient over the entire minibatch, using last-layer gradients
  as a cheap proxy.
---

# A Negative Result on Gradient Matching for Selective Backprop

## Quick Facts
- arXiv ID: 2312.05021
- Source URL: https://arxiv.org/abs/2312.05021
- Reference count: 40
- Key outcome: Gradient matching for selective backprop reduces gradient error but fails to improve test accuracy over random selection.

## Executive Summary
This paper investigates Selective Backprop, a method to speed up neural network training by dropping low-loss examples after the forward pass. The authors propose a gradient-matching approach using last-layer gradients as a cheap proxy to select subsets of data that best approximate the mean gradient over the entire minibatch. Surprisingly, all selection strategies tested (gradient-matching, loss-based, and random) fail to consistently outperform random selection, despite the gradient-matching approach reducing gradient error compared to random sampling. The authors tune learning rates separately for each method but still find no consistent gains from selective backprop.

## Method Summary
The method performs a forward pass on the full minibatch, computes last-layer gradient Gram matrices, and uses orthogonal matching pursuit (OMP) to select a weighted subset that best approximates the mean gradient. This subset is then used for the backward pass, avoiding the more expensive backward computation on low-contribution examples. The approach is compared against loss-based and random selection baselines across multiple datasets (CIFAR-10, CIFAR-100, SVHN, ImageNet-32, IMDB) and models (ResNet-18, WideResNet variants, pretrained BERT).

## Key Results
- Gradient matching reduces gradient error compared to random sampling but does not improve test accuracy
- Loss-based selection performs worse than random selection, especially with label noise
- Separate learning rate tuning per method does not resolve the performance gap
- Selective backprop shows no consistent advantage over simply using smaller batch sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient matching reduces gradient error compared to random sampling
- Mechanism: The method selects a subset of data points whose weighted combination of last-layer gradients best approximates the mean gradient of the full minibatch, using orthogonal matching pursuit (OMP) on the Gram matrix of last-layer gradients
- Core assumption: Last-layer gradients are a cheap and accurate proxy for full gradients, and matching their mean preserves the overall gradient direction
- Evidence anchors:
  - [abstract]: "the gradient-matching approach does reduce gradient error compared to random sampling"
  - [section 4.1]: Explains that last-layer gradients are computed implicitly at virtually no overhead using the formula K = HH^T â—¦ PP^T + PP^T
  - [corpus]: Weak/no direct evidence in related papers for gradient-matching in selective backprop specifically
- Break condition: If last-layer gradients become a poor proxy for full gradients (e.g., very deep networks with weak connections to output), the approximation error could dominate

### Mechanism 2
- Claim: Selective backprop reduces computational cost by avoiding backward pass on low-contribution examples
- Mechanism: Forward pass is performed on full minibatch, losses are computed, then only high-loss examples are selected for backward pass, avoiding the more expensive backward computation
- Core assumption: Low-loss examples contribute negligibly to gradient updates and can be safely dropped
- Evidence anchors:
  - [abstract]: "Since, according to commonly accepted estimates... the backward pass constitutes the roughly two thirds of the computational effort"
  - [section 2]: "The intuition is that the gradient contributions of low-loss examples are small and therefore do not significantly influence the training process"
  - [corpus]: Bonsai paper shows gradient-free pruning works, suggesting gradients from some examples may be less critical
- Break condition: If loss is not a good proxy for gradient magnitude (e.g., flat loss landscapes or pathological loss distributions)

### Mechanism 3
- Claim: OMP-based subset selection can identify and downweight redundant examples
- Mechanism: The matching pursuit algorithm assigns weights to selected examples based on their gradient contribution to matching the mean, potentially assigning lower weights to near-duplicates
- Core assumption: Redundant examples (near-duplicates with similar gradients) can be identified through their inner products in the Gram matrix
- Evidence anchors:
  - [abstract]: "Our matching approach will be aware of the redundancy and select only one of them. It can also assign that instance a higher weight to account for the gradient contribution of its duplicate."
  - [section 4.2]: Describes OMP selection returning indices and weights, with normalization applied
  - [corpus]: Weak/no direct evidence in related papers for redundancy detection in selective backprop
- Break condition: If the Gram matrix computation becomes inaccurate due to numerical precision issues or if OMP fails to converge to meaningful sparse solutions

## Foundational Learning

- Concept: Orthogonal Matching Pursuit (OMP) for sparse approximation
  - Why needed here: OMP solves the NP-hard cardinality-constrained quadratic optimization problem of finding the weighted subset that best matches the mean gradient
  - Quick check question: How does OMP greedily select elements, and why does it require only the Gram matrix and vector t for computation?

- Concept: Gradient computation complexity (forward vs backward pass)
  - Why needed here: Understanding that backward pass is ~2x more expensive than forward pass motivates selective backprop and explains cost savings
  - Quick check question: If backward pass is twice as expensive as forward, what is the break-even point for subset size m in selective backprop?

- Concept: Last-layer gradient as proxy for full gradient
  - Why needed here: The method relies on last-layer gradients being a computationally cheap and accurate approximation of full gradients for subset selection
  - Quick check question: What mathematical relationship allows computing the Gram matrix of last-layer gradients implicitly without computing individual gradients?

## Architecture Onboarding

- Component map:
  Forward pass -> Cache last-layer activations -> Compute losses -> Calculate Gram matrix K -> Run OMP on K and t -> Select subset with weights -> Weighted backward pass

- Critical path:
  1. Forward pass with activation caching
  2. Loss computation
  3. Gram matrix calculation
  4. OMP subset selection
  5. Weighted backward pass

- Design tradeoffs:
  - Memory vs computation: Caching last-layer activations vs recomputing them
  - Accuracy vs speed: Using last-layer gradients vs full gradients for selection
  - Weight normalization: Normalizing OMP weights vs allowing arbitrary scaling
  - Negative weights: Clipping weights at zero vs allowing negative weights

- Failure signatures:
  - High gradient error despite subset selection (indicates poor proxy gradients)
  - No improvement over random selection (indicates selection strategy not capturing useful signal)
  - Performance degradation with label noise (indicates sensitivity to outliers)
  - Memory overflow from caching activations (indicates batch size too large)

- First 3 experiments:
  1. Verify Gram matrix computation matches analytical expectations on small synthetic data
  2. Compare gradient error of OMP selection vs random selection on fixed minibatch
  3. Profile runtime overhead of selective backprop vs full backpropagation on small model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would gradient matching selective backprop outperform random selection when using very large batch sizes or extreme subsampling fractions?
- Basis in paper: [explicit] The authors note that random selection corresponds to using smaller batch sizes or fewer steps with accelerated learning rate decay, and observe that all methods achieve lower accuracy on average when using scaled base batch sizes.
- Why unresolved: The experiments only tested a limited range of batch sizes and subsampling fractions. The authors found that random selection was competitive, but did not test the extremes of the design space.
- What evidence would resolve it: Additional experiments testing very large batch sizes (e.g. 10x base) and extreme subsampling fractions (e.g. 0.1 or 0.9) would determine if gradient matching could provide benefits in those regimes.

### Open Question 2
- Question: Would incorporating non-negativity constraints or regularization on the weights in the gradient matching optimization improve performance?
- Basis in paper: [explicit] The authors mention that the weights in their optimization problem could be clipped at zero or regularized, but did not pursue these directions in their experiments.
- Why unresolved: The authors used unconstrained weights and found that gradient matching improved over loss-based selection, but did not explore if further constraints could provide additional gains.
- What evidence would resolve it: Experiments comparing the unconstrained, non-negative, and regularized versions of the gradient matching method would determine if these modifications provide benefits.

### Open Question 3
- Question: Could gradient matching selective backprop provide benefits for other model architectures beyond those tested (CNNs and BERT)?
- Basis in paper: [explicit] The authors tested ResNet, WideResNet, and BERT models, but did not explore a broader range of architectures.
- Why unresolved: The performance of gradient matching may depend on the specific model architecture. The authors only tested a few common architectures.
- What evidence would resolve it: Experiments applying gradient matching selective backprop to a diverse set of model architectures (e.g. RNNs, Transformers, Graph Neural Networks) would determine if the benefits generalize beyond the tested architectures.

## Limitations
- The negative result suggests fundamental limitations in the selective backprop framework rather than implementation issues
- The paper does not explore extreme batch sizes or subsampling fractions where gradient matching might provide benefits
- Only tested on CNNs and BERT architectures, limiting generalizability to other model types

## Confidence
- **Medium confidence**: The negative result - that selective backprop does not improve performance over random selection - is supported by extensive experiments across multiple datasets and models.
- **High confidence**: The gradient-matching approach does reduce gradient error compared to random sampling, as this is a mathematical property of the OMP algorithm.
- **Medium confidence**: The claim that last-layer gradients serve as a cheap proxy requires validation, as the paper does not compare against full gradient computation.

## Next Checks
1. Test selective backprop with fixed learning rate across all methods to isolate whether learning rate tuning is masking fundamental issues.
2. Measure the correlation between gradient error reduction and actual training performance across different subset sizes to understand if gradient matching is targeting the right objective.
3. Implement a variant using full gradients (not just last-layer) for subset selection to quantify the cost-benefit tradeoff of the proxy approximation.