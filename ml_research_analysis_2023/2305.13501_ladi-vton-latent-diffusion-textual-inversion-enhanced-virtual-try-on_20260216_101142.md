---
ver: rpa2
title: 'LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On'
arxiv_id: '2305.13501'
source_url: https://arxiv.org/abs/2305.13501
tags:
- image
- diffusion
- try-on
- virtual
- emasc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LaDI-VTON, the first Latent Diffusion Model
  (LDM)-based approach for virtual try-on, addressing the limitations of GAN-based
  methods in preserving garment details and high-frequency image features. The core
  method involves extending the Stable Diffusion inpainting pipeline with a novel
  textual inversion module that maps in-shop garment visual features to CLIP token
  embeddings, conditioning the generation process.
---

# LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On

## Quick Facts
- arXiv ID: 2305.13501
- Source URL: https://arxiv.org/abs/2305.13501
- Reference count: 40
- One-line primary result: First LDM-based virtual try-on method achieving state-of-the-art results on Dress Code and VITON-HD datasets

## Executive Summary
LaDI-VTON introduces a novel latent diffusion model approach for virtual try-on that addresses the limitations of GAN-based methods in preserving garment details and high-frequency image features. The method extends the Stable Diffusion inpainting pipeline with a textual inversion module that maps garment visual features to CLIP token embeddings, enabling conditioning on garment textures during generation. Enhanced Mask-Aware Skip Connection (EMASC) modules are also proposed to improve the autoencoder's reconstruction capabilities, particularly for preserving fine details like hands, faces, and feet.

## Method Summary
LaDI-VTON extends the Stable Diffusion inpainting pipeline to accept additional spatial inputs: pose maps, warped garment encodings, and a novel textual inversion adapter that maps garment visual features to CLIP token embeddings. The approach includes EMASC modules to enhance the image reconstruction autoencoder, allowing intermediate features to bypass the latent compression bottleneck and improve high-frequency detail preservation. The method is trained in three stages: first EMASC modules, then textual inversion adapter, and finally the full model with classifier-free guidance.

## Key Results
- Achieves FID scores of 4.14 and 6.48 for paired and unpaired settings on Dress Code dataset
- Significantly outperforms competitors in terms of realism and coherence
- Generates images at 512x384 resolution with state-of-the-art results on Dress Code and VITON-HD datasets

## Why This Works (Mechanism)

### Mechanism 1
The textual inversion adapter improves garment detail preservation by mapping visual features to CLIP token embeddings, conditioning the generation process without losing texture information. The visual features of the in-shop garment are encoded using CLIP's visual encoder and then mapped to the CLIP token embedding space via a learned adapter network. These pseudo-word token embeddings (PTEs) are concatenated with the original prompt and fed into the CLIP text encoder to condition the denoising U-Net during generation.

### Mechanism 2
EMASC modules improve reconstruction of high-frequency details by learning to propagate intermediate encoder features to corresponding decoder layers, masked by the inverted inpainting mask. This allows relevant information outside the inpainting area to bypass the latent compression bottleneck, improving reconstruction quality for fine details like hands, faces, and feet.

### Mechanism 3
Extending the spatial input of the denoising U-Net to include pose maps and warped garment encodings enables better pose preservation and garment fitting. The spatial input is expanded to concatenate the encoded masked image, inpainting mask, resized pose map, and encoded warped garment, guiding the generation process to preserve the model's pose and ensure the garment fits the body shape.

## Foundational Learning

- Concept: Diffusion models and the denoising process
  - Why needed here: Understanding how the latent diffusion model iteratively denoises a latent variable to generate an image is crucial for grasping how conditioning inputs guide the generation process.
  - Quick check question: What is the role of the denoising U-Net in a latent diffusion model, and how does it use conditioning inputs during the reverse diffusion process?

- Concept: CLIP model and its embedding space
  - Why needed here: The textual inversion component relies on CLIP's visual and text encoders to map garment images to token embeddings and back, enabling conditioning on garment details.
  - Quick check question: How does CLIP align visual and textual inputs in a shared embedding space, and why is this useful for conditioning a diffusion model on garment images?

- Concept: Autoencoder reconstruction and information loss
  - Why needed here: The EMASC modules aim to mitigate information loss during the autoencoder's compression of the image into the latent space, which is critical for preserving high-frequency details.
  - Quick check question: Why does the autoencoder's compression of the image into a lower-dimensional latent space lead to loss of high-frequency details, and how can skip connections help recover this information?

## Architecture Onboarding

- Component map: Target model image -> Garment warping (TPS + refinement U-Net) -> CLIP visual encoder -> Textual inversion adapter -> Stable Diffusion inpainting pipeline (with extended spatial input) -> EMASC modules in autoencoder -> Generated image

- Critical path:
  1. Warp in-shop garment to fit model body shape
  2. Encode masked model image, pose map, and warped garment
  3. Use CLIP to encode garment and generate PTEs via textual inversion adapter
  4. Concatenate all conditioning inputs and feed to extended Stable Diffusion inpainting model
  5. Generate final image with EMASC modules improving reconstruction

- Design tradeoffs:
  - Using CLIP for garment conditioning enables fine-grained detail preservation but adds computational overhead for the textual inversion step
  - EMASC modules improve high-frequency detail reconstruction but require additional training and increase model complexity
  - Extending the spatial input to include pose and warped garment encodings improves fitting but may introduce artifacts if the conditioning signals are inaccurate

- Failure signatures:
  - Poor garment fitting: inaccurate warped garment encoding or pose map misalignment
  - Loss of garment details: textual inversion adapter fails to capture texture information or CLIP embedding space is not well-aligned with garment features
  - Artifacts in high-frequency regions (hands, faces): EMASC modules not trained properly or skip connections not masking correctly

- First 3 experiments:
  1. Train and evaluate the garment warping module (TPS + refinement U-Net) on a subset of paired images to ensure accurate fitting
  2. Train the textual inversion adapter on the inpainting pipeline with frozen Stable Diffusion weights, then evaluate generated PTEs' ability to reconstruct garment images
  3. Train EMASC modules on the Stable Diffusion autoencoder with L1 + VGG loss, then evaluate reconstruction quality on masked model images compared to baseline autoencoder

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed LaDI-VTON approach scale to high-resolution images (e.g., 1024x768 or higher) in terms of computational efficiency and image quality?

### Open Question 2
How does the proposed LaDI-VTON approach perform on diverse garment categories beyond the ones covered in the Dress Code and VITON-HD datasets?

### Open Question 3
How does the proposed LaDI-VTON approach handle complex scenarios such as occlusions, extreme poses, and diverse body shapes?

## Limitations
- Performance on real-world, diverse fashion images with varying poses, backgrounds, and lighting remains untested
- Computational overhead due to training three separate components (EMASC, textual inversion adapter, full model)
- Dependency on accurate garment warping that may not generalize to complex scenarios

## Confidence

**High Confidence (8/10)**: The core diffusion-based framework with extended spatial conditioning is well-established, with substantial quantitative improvements over baselines (FID scores of 4.14 and 6.48).

**Medium Confidence (6/10)**: The textual inversion mechanism for garment conditioning is novel and shows promise, but evidence is primarily from the paper's own experiments.

**Medium-Low Confidence (5/10)**: The EMASC modules' effectiveness in preserving high-frequency details is demonstrated through quantitative metrics, but qualitative improvements may be dataset-specific.

## Next Checks

**Check 1: Cross-Dataset Generalization** - Evaluate LaDI-VTON on a third, independent dataset (e.g., VITON or DeepFashion) with different characteristics. Measure performance degradation and identify which components are most sensitive to dataset changes.

**Check 2: Ablation Study on Conditioning Components** - Systematically remove each conditioning input (pose map, warped garment, textual inversion) and measure the impact on FID, LPIPS, and qualitative results.

**Check 3: Real-World Robustness Test** - Test the method on real-world fashion images with varied poses, occlusions, and complex backgrounds. Document failure modes and quantify the gap between synthetic dataset performance and real-world applicability.