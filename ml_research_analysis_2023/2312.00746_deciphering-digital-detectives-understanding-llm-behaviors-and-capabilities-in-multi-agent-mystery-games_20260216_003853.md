---
ver: rpa2
title: 'Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities
  in Multi-Agent Mystery Games'
arxiv_id: '2312.00746'
source_url: https://arxiv.org/abs/2312.00746
tags:
- yang
- boss
- agents
- game
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the application of Large Language Models
  (LLMs) in the Chinese detective role-playing game "Jubensha". The authors construct
  a dataset tailored for Jubensha games and propose a multi-agent interaction framework
  for autonomous LLM-based agents.
---

# Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games

## Quick Facts
- arXiv ID: 2312.00746
- Source URL: https://arxiv.org/abs/2312.00746
- Reference count: 40
- Key outcome: Multi-agent LLM framework significantly improves information gathering and reasoning in Chinese detective role-playing games

## Executive Summary
This study investigates the application of Large Language Models (LLMs) in the Chinese detective role-playing game "Jubensha." The authors construct a specialized dataset and propose a multi-agent interaction framework for autonomous LLM-based agents. By implementing advanced in-context learning techniques through their "ThinkThrice" framework, they demonstrate significant improvements in agents' abilities to gather information, detect murderers, and perform logical reasoning within complex narrative environments.

## Method Summary
The researchers developed a multi-agent interaction framework where LLM-based agents autonomously engage in Jubensha games. Their "ThinkThrice" framework implements a staged reasoning process: memory retrieval using vector databases, self-refinement through question decomposition, and self-verification against character scripts. Agents interact synchronously in turn-based dialogue, accumulating information about other characters' roles and actions. The framework is evaluated using GPT-3.5 and GPT-4 models across two tasks measuring information mastery and reasoning skills.

## Key Results
- The ThinkThrice framework significantly enhances agents' factual question answering accuracy through staged reasoning
- Multi-agent dialogue interactions improve information gathering, with more game rounds correlating to better reasoning performance
- Self-verification modules effectively reduce hallucinations while maintaining information completeness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ThinkThrice improves reasoning through staged memory retrieval, self-refinement, and self-verification
- Core assumption: Staged approach reduces hallucinations and improves information completeness
- Evidence: Experimental improvements in factual question accuracy after implementing verification module

### Mechanism 2
- Claim: Multi-agent dialogue enables information gathering about other characters
- Core assumption: Dialogue exchanges provide reliable information for storage and retrieval
- Evidence: Significant improvement in answering questions about others after memory retriever introduction

### Mechanism 3
- Claim: Staged reasoning improves inferential question performance
- Core assumption: Reasoning accuracy scales with integrated information across character scripts
- Evidence: Correlation between game rounds (information gathered) and reasoning accuracy

## Foundational Learning

- **In-context learning**: Technique for guiding agents through reasoning stages without fine-tuning; needed because ThinkThrice relies on prompt engineering rather than model training
- **Vector database retrieval**: Stores and retrieves historical dialogue records; needed for memory retrieval module using Faiss for similarity search
- **Theory of mind in multi-agent systems**: Modeling other agents' beliefs and intentions; needed because agents must infer hidden roles based on dialogue and actions

## Architecture Onboarding

- **Component map**: Character script loader → Memory retriever (vector DB + Faiss) → Self-refinement module → Self-verification module → Dialogue generator → Host controller (non-LLM) → Game state manager → Voting module
- **Critical path**: Character script → Memory retrieval → Self-refinement → Self-verification → Dialogue/action
- **Design tradeoffs**: Memory vs. cost (more storage improves accuracy but increases token usage); self-verification strictness vs. fluency (higher thresholds reduce hallucinations but may cause repetition); LLM model choice (GPT-4 better reasoning but higher cost)
- **Failure signatures**: High hallucination rate → verification too lenient or retrieval missing facts; poor performance on others' questions → insufficient dialogue capture; low reasoning accuracy → inadequate information integration
- **First 3 experiments**: 1) Baseline with only memory retrieval, measuring factual accuracy; 2) Add self-refinement, compare factual accuracy and completeness; 3) Add self-verification, measure hallucination reduction and accuracy with different authenticity thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Impact of different in-context learning techniques (Chain-of-Thought, self-consistency) on reasoning performance
- Basis: Authors mention using advanced techniques but don't evaluate their impact
- Resolution: Experiments comparing different in-context learning approaches

### Open Question 2
- How script complexity (number of characters, relationship intricacy) affects agent performance
- Basis: Dataset includes varying player counts but lacks complexity analysis
- Resolution: Experiments with scripts of different complexities

### Open Question 3
- LLM agents' effectiveness with multi-modal Jubensha games including audio/video clues
- Basis: Current dataset focuses on text-modal games despite multi-modal data collection
- Resolution: Development and evaluation of agents processing multi-modal game data

## Limitations
- Limited external validation beyond constructed dataset
- Unexplored tradeoff between conversation naturalness and accuracy accuracy
- Effectiveness of staged reasoning not tested across diverse case complexities

## Confidence

- **High confidence**: Baseline observation that LLM agents struggle without specialized frameworks
- **Medium confidence**: ThinkThrice module effectiveness in improving reasoning
- **Medium confidence**: Correlation between information gathering and reasoning accuracy

## Next Checks

1. **Cross-dataset validation**: Test ThinkThrice framework on Jubensha games from different sources to assess generalizability
2. **Human-in-the-loop evaluation**: Compare LLM agent reasoning accuracy against human players in identical scenarios
3. **Longitudinal stability test**: Run extended game sessions (10+ rounds) to evaluate whether information accumulation benefits persist or degrade due to memory interference