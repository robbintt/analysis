---
ver: rpa2
title: 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions
  in Text Games'
arxiv_id: '2311.07687'
source_url: https://arxiv.org/abs/2311.07687
tags:
- games
- language
- transitions
- action
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We explore the feasibility of updating Large Language Models (LLMs)
  used for action recommendation during learning in text games. We find that updating
  the LLM during learning using carefully selected in-game transitions reduces the
  dependency on human annotated gameplays and accelerates convergence.
---

# Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games

## Quick Facts
- arXiv ID: 2311.07687
- Source URL: https://arxiv.org/abs/2311.07687
- Authors: 
- Reference count: 19
- Key outcome: Updating LLMs during learning using carefully selected in-game transitions reduces dependency on human annotated gameplays and accelerates convergence, with state feature-based selection showing greatest gains.

## Executive Summary
This paper explores updating Large Language Models (LLMs) during learning in text games to reduce reliance on human-annotated gameplay data. The approach, called Language Model-in-the-Loop (LM-in-the-Loop), trains GPT-2 using in-game generated transitions rather than relying solely on static human gameplay data. The method shows promise in reducing the need for extensive human annotations while maintaining or improving performance compared to traditional approaches that use frozen pre-trained models.

## Method Summary
The method builds upon the CALM framework by Yao et al. (2020) and introduces LM-in-the-Loop, where GPT-2 is updated during learning using in-game transitions. The approach involves fine-tuning GPT-2 on ClubFloyd dataset initially, then periodically updating it with transitions categorized by state features or reward during gameplay. The DRRN agent selects actions from GPT-2's candidates, and after k steps, the LM is updated with sampled transitions from categorized buffers. Different transition selection strategies are evaluated including Uncategorized Transitions (UT), State Feature Categorized (OC), and Reward Trajectories (RT).

## Key Results
- State feature-based transition selection provided greater performance gains than other approaches
- LM-in-the-Loop reduced dependency on human annotated gameplays
- Transferring in-game trained models to other games did not result in consistent improvements
- Using GPT-2 as a policy network without a separate DRRN agent fared poorly across games

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training GPT-2 on in-game transitions improves action relevance compared to static human-annotated data
- Mechanism: The language model updates its action-generation probabilities to reflect game-specific patterns discovered during RL episodes
- Core assumption: In-game transitions contain information that improves the alignment between GPT-2's action candidates and the actual state transitions in the target game
- Evidence anchors: [abstract] "by updating the LLM during learning using carefully selected in-game transitions, we can reduce the dependency on using human annotated game plays"

### Mechanism 2
- Claim: State-feature-based transition selection yields greater performance gains than reward-based selection
- Mechanism: Transitions are categorized by whether they change location or yield positive reward, biasing LM updates toward exploratory actions that reach new game states
- Core assumption: Transitions that change location represent meaningful game progress and should be prioritized for LM training to encourage broader exploration
- Evidence anchors: [abstract] "State feature-based transitions selection provided greater gains than other approaches"

### Mechanism 3
- Claim: LM-in-the-Loop accelerates convergence by reducing reliance on human annotations
- Mechanism: By continuously adapting the LM with in-game data, the agent needs fewer external annotations to achieve competitive performance
- Core assumption: Human annotations provide diminishing returns once the LM has adapted to the specific game dynamics through in-game learning
- Evidence anchors: [abstract] "LM-in-the-Loop reduces emphasis on human annotated transitions and enables accelerated convergence"

## Foundational Learning

- Concept: Temporal Difference (TD) Learning
  - Why needed here: DRRN uses TD learning to estimate Q-values for action selection
  - Quick check question: What is the TD error formula used to update the DRRN's Q-value estimates?

- Concept: Advantage Function
  - Why needed here: Advantage-based weighting of transitions guides LM training toward actions that yield higher-than-expected returns
  - Quick check question: How does the advantage function differ from the raw Q-value in the context of action selection?

- Concept: Cross-Entropy Loss for LM Training
  - Why needed here: The weighted cross-entropy loss updates GPT-2's action-generation probabilities based on transition importance
  - Quick check question: Why is a weighted cross-entropy loss preferred over unweighted loss when training the LM on in-game transitions?

## Architecture Onboarding

- Component map: Pre-trained GPT-2 → Finetuned on ClubFloyd → Updated with in-game transitions → Action candidate generator -> DRRN → Q-value estimator → Action selector -> Jericho environment → Game state and reward provider -> Transition buffers (D+, D-) → Stores categorized transitions for LM updates

- Critical path: DRRN selects action from GPT-2 candidates → Game environment returns new state and reward → Transition stored in appropriate buffer (D+ or D-) → After k steps, LM updated with sampled transitions → Updated LM generates new candidates

- Design tradeoffs: Using state features vs. reward for transition selection: State features encourage exploration but may miss reward-dense paths; reward-based selection focuses on exploitation but risks local optima. Frequency of LM updates (k): Frequent updates provide more adaptation but increase computational cost and risk overfitting to recent transitions.

- Failure signatures: LM generates irrelevant or nonsensical actions → Check transition selection and LM update frequency. DRRN ignores LM candidates and defaults to random actions → Check Q-value estimation and action filtering. Performance degrades over time → Check buffer overflow and transition diversity.

- First 3 experiments: Replace state-feature categorization with random categorization to confirm its importance. Vary LM update frequency (k) to find optimal balance between adaptation and stability. Test weighted cross-entropy loss with different advantage weighting schemes to optimize transition importance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LM-in-the-Loop scale with different sizes of human-annotated gameplay data?
- Basis in paper: [explicit] The paper discusses reducing reliance on human annotated gameplays and shows that using 10% of ClubFloyd data with State Features (OC) achieved better performance than CALM using 100% of the data.
- Why unresolved: The paper only compares 10% and 100% of the adaptation data. A more comprehensive study across a range of data sizes would provide a clearer understanding of the scaling relationship.
- What evidence would resolve it: Experiments comparing LM-in-the-Loop performance with varying percentages of human-annotated gameplay data, ideally including a plot of performance vs. data size.

### Open Question 2
- Question: Can the improvements from LM-in-the-Loop be attributed solely to the updated language model, or does the DRRN agent also play a significant role?
- Basis in paper: [explicit] The ablation experiment where GPT-2 was used as a policy network without DRRN showed poor performance, suggesting that DRRN contributes significantly to the observed improvements.
- Why unresolved: The ablation experiment only tested GPT-2 as a policy network. It would be valuable to also test the performance of a frozen GPT-2 model (without in-game training) to isolate the contribution of the language model updates.
- What evidence would resolve it: Experiments comparing the performance of DRRN with a frozen GPT-2 model vs. DRRN with LM-in-the-Loop trained GPT-2.

### Open Question 3
- Question: What are the most effective strategies for selecting transitions for LM-in-the-Loop training?
- Basis in paper: [explicit] The paper explores different strategies including Uncategorized Transitions (UT), State Feature Categorized (OC), and Reward Trajectories (RT), with OC showing the best performance.
- Why unresolved: The paper only tests a limited set of transition selection strategies. Other strategies, such as using the advantage function or more complex heuristics, could potentially yield even better results.
- What evidence would resolve it: Experiments comparing the performance of LM-in-the-Loop with a wider range of transition selection strategies, including those based on the advantage function or other game-specific heuristics.

### Open Question 4
- Question: How can the interpretability of LM-in-the-Loop be improved, particularly in terms of understanding the model's decision-making process?
- Basis in paper: [inferred] The paper notes that interpreting the performance of LM-in-the-Loop through conventional metrics like semantic and syntactic scores is less effective, suggesting a need for more interpretable metrics.
- Why unresolved: The paper does not propose any specific solutions for improving interpretability. Developing interpretable metrics that identify important transitions for LM-in-the-Loop training would be a valuable contribution.
- What evidence would resolve it: The development and validation of interpretable metrics for LM-in-the-Loop, along with experiments demonstrating their effectiveness in understanding the model's decision-making process.

## Limitations

- Transfer learning between games yielded inconsistent results, suggesting learned patterns are highly game-specific
- Exact implementation details of state feature categorization remain underspecified
- Weighted cross-entropy loss formulation for negative advantages needs clarification

## Confidence

- High confidence: The core finding that in-game updates can reduce human annotation needs is well-supported by experimental results across multiple games
- Medium confidence: The claim about state-feature-based selection being superior needs additional validation across more diverse game types
- Low confidence: The transfer learning results are too inconsistent to draw firm conclusions about cross-game applicability

## Next Checks

1. Implement ablation studies comparing state-feature categorization against random categorization to quantify the specific contribution of this method.

2. Test the approach across a broader set of Jericho games with varying state feature structures to assess generalizability.

3. Conduct transfer learning experiments using games with similar mechanics but different themes to determine if the approach can capture transferable patterns.