---
ver: rpa2
title: Estimating Uncertainty in Multimodal Foundation Models using Public Internet
  Data
arxiv_id: '2310.09926'
source_url: https://arxiv.org/abs/2310.09926
tags:
- image
- calibration
- data
- prediction
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a heuristic approach to estimate uncertainty
  in zero-shot predictions of multimodal foundation models using conformal prediction
  with web data. The core idea is to use the internet as a source of calibration data
  for conformal prediction, given a set of user-specified classes.
---

# Estimating Uncertainty in Multimodal Foundation Models using Public Internet Data

## Quick Facts
- arXiv ID: 2310.09926
- Source URL: https://arxiv.org/abs/2310.09926
- Reference count: 12
- Primary result: Web-based conformal prediction achieves target coverage on biomedical datasets while outperforming standard CP on scraped data

## Executive Summary
This paper proposes a heuristic approach to estimate uncertainty in zero-shot predictions of multimodal foundation models using conformal prediction with web data. The core idea is to use the internet as a source of calibration data for conformal prediction, given a set of user-specified classes. The method involves data mining from the web, generating plausibility scores to quantify alignment between the mined data and user queries, and applying a Monte Carlo-based conformal prediction procedure. Results show that WebCP achieves the target coverage with satisfactory efficiency on biomedical datasets, outperforming standard conformal prediction applied to scraped calibration data.

## Method Summary
The proposed method, WebCP, uses the internet as a universal source of calibration data for zero-shot conformal prediction. It involves three main steps: (1) web scraping to collect images and metadata for user-specified classes, (2) generating plausibility scores that combine context alignment (measuring metadata relevance) and content alignment (verifying visual correspondence), and (3) applying Monte Carlo conformal prediction that handles ambiguous ground truth labels by sampling from plausibility distributions. The method is evaluated on biomedical datasets including Fitzpatrick17k and MedMNIST subsets using CLIP-style foundation models.

## Key Results
- WebCP achieves the user-specified target coverage (1-α) on biomedical test datasets
- Compared to standard conformal prediction on scraped data, WebCP shows significantly better coverage across all α values
- WebCP efficiency (average prediction set size) is comparable to an oracle procedure using target data for calibration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Web-based conformal prediction works because internet data approximates the test distribution well enough for zero-shot calibration
- Mechanism: The open web contains diverse images and contextual metadata for arbitrary user-specified classes, providing a universal calibration set without requiring labeled examples from the target domain
- Core assumption: The web data distribution Pweb is sufficiently similar to the test distribution Pπ that exchangeability assumptions approximately hold for conformal prediction coverage guarantees
- Evidence anchors:
  - [abstract] "we would like to test if web-scraped calibration data eCweb ~ Pweb is a good approximation of oracle calibration sets drawn from the test distribution eC* ~ Pπ"
  - [section] "In almost all experiments, our WebCP procedure results in prediction sets that consistently achieves the targeted 1 − α coverage on the test datasets"
- Break condition: If the web data becomes too biased or unrepresentative of the test domain, coverage guarantees will fail and prediction sets will be inaccurate

### Mechanism 2
- Claim: Plausibility scores effectively handle ambiguity in web-scraped labels by combining context and content alignment measures
- Mechanism: Context alignment measures relevance of web page metadata to search queries using text embeddings, while content alignment filters invalid images and verifies visual correspondence to classes using CLIP similarity scores
- Core assumption: The combination of context and content plausibility scores can accurately identify and downweight mislabeled or irrelevant web images in the calibration set
- Evidence anchors:
  - [section] "We show that web-based CP empirically achieves target coverage while retaining efficiency comparable to that of an 'oracle' CP procedure that uses data from target datasets for calibration"
  - [section] "Compared to the Standard CP procedure applies to the scraped calibration data significantly under-covers in the test dataset across all values of α"
- Break condition: If either context or content alignment metrics fail to accurately identify misaligned examples, the plausibility scores will be unreliable and calibration will suffer

### Mechanism 3
- Claim: Monte Carlo conformal prediction with ambiguous ground truth handles probabilistic labels effectively
- Mechanism: The procedure randomly samples labels from plausibility score distributions to create multiple calibration sets, then finds conformity score thresholds that achieve target coverage across all sampled sets
- Core assumption: Sampling from plausibility distributions and aggregating results across multiple calibration sets provides robust coverage guarantees despite label ambiguity
- Evidence anchors:
  - [section] "We apply the Monte Carlo CP procedure proposed in (Stutz et al. (2023)), which accounts for ambiguity in ground-truth labels"
  - [section] "WebCP could be a promising approach to zero-shot calibration—in biomedical datasets, it achieves the user-specified target coverage"
- Break condition: If plausibility distributions are too uncertain or multimodal, sampling may not adequately capture the true label distribution, leading to coverage failures

## Foundational Learning

- Concept: Conformal prediction provides distribution-free coverage guarantees by constructing prediction sets based on conformity scores from calibration data
  - Why needed here: Standard CP requires labeled calibration data from the target distribution, which is unavailable in zero-shot settings
  - Quick check question: What are the key assumptions required for conformal prediction to provide valid coverage guarantees?

- Concept: Zero-shot learning enables classification of unseen categories without task-specific training data
  - Why needed here: The foundation models must classify arbitrary user-specified categories at test time without labeled examples
  - Quick check question: How do CLIP-style models enable zero-shot classification through text-image alignment?

- Concept: Web scraping and data mining techniques for acquiring large-scale labeled datasets
  - Why needed here: The method requires automated collection of web images and metadata for arbitrary user-specified classes
  - Quick check question: What are the main challenges in ensuring web-scraped data quality and relevance?

## Architecture Onboarding

- Component map: Web scraper -> Plausibility scorer -> Monte Carlo CP engine -> Foundation model interface
- Critical path: Search query -> Web data collection -> Context/content alignment -> Plausibility generation -> Calibration set creation -> CP prediction
- Design tradeoffs: Accuracy vs. efficiency in web scraping, complexity of plausibility scoring vs. coverage guarantees, conservative vs. aggressive prediction sets
- Failure signatures: Under-coverage (prediction sets too small), over-coverage (prediction sets too large), slow calibration due to web scraping delays
- First 3 experiments:
  1. Test web scraping pipeline with simple categories to verify data collection works
  2. Validate plausibility scoring on manually labeled web data to check accuracy
  3. Compare WebCP coverage vs. oracle CP on small test dataset to measure performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the WebCP method perform when applied to non-biomedical domains outside the tested datasets?
- Basis in paper: [explicit] The paper focuses on biomedical datasets and mentions that "web-scraped data can provide useful calibration sets that can help calibrate the zero-shot predictions of foundation models" for the tested datasets, but does not explore other domains
- Why unresolved: The experiments are limited to biomedical datasets, leaving uncertainty about generalizability to other domains
- What evidence would resolve it: Experiments applying WebCP to diverse non-biomedical datasets (e.g., natural images, satellite imagery, etc.) showing comparable coverage and efficiency

### Open Question 2
- Question: What is the impact of varying the number of web-scraped images per category (ny) on the performance of WebCP?
- Basis in paper: [inferred] The paper uses a fixed number of web-scraped images (top 50 search results) but does not analyze how performance scales with different numbers of calibration samples
- Why unresolved: The effect of calibration set size on coverage and efficiency is not explored, which is critical for understanding practical deployment
- What evidence would resolve it: A systematic study varying ny (e.g., 10, 50, 100, 200) and measuring changes in coverage and efficiency

### Open Question 3
- Question: How sensitive is WebCP to the choice of search engine and query formulation?
- Basis in paper: [explicit] The paper uses Google Custom Image Search Engine and specific prompt templates, but does not analyze robustness to different search engines or query variations
- Why unresolved: Different search engines may provide different quality or diversity of calibration data, affecting WebCP performance
- What evidence would resolve it: Experiments comparing WebCP performance using different search engines (e.g., Bing, DuckDuckGo) and alternative query formulations

## Limitations
- The web data distribution's similarity to target domains is assumed but not empirically validated beyond biomedical datasets
- Plausibility score accuracy depends heavily on CLIP's zero-shot performance, which may vary across domains
- The Monte Carlo sampling approach's effectiveness with highly ambiguous web labels remains unproven

## Confidence

- **High**: WebCP achieves target coverage on biomedical test datasets as claimed
- **Medium**: WebCP's efficiency (prediction set size) is comparable to oracle CP procedures
- **Low**: The method generalizes to non-biomedical domains and real-world applications without further validation

## Next Checks
1. Test WebCP coverage and efficiency on diverse domain datasets (e.g., natural images, satellite imagery) to assess generalization beyond biomedical applications
2. Benchmark WebCP computational overhead and latency compared to standard CP to evaluate practical deployment feasibility
3. Conduct ablation studies removing either context or content plausibility scoring to quantify their individual contributions to calibration quality