---
ver: rpa2
title: On Wasserstein distances for affine transformations of random vectors
arxiv_id: '2310.03945'
source_url: https://arxiv.org/abs/2310.03945
tags:
- bound
- wasserstein
- random
- lower
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides theoretical insights and practical bounds for
  the quadratic Wasserstein distance between random vectors in R^n under affine transformations,
  motivated by manifold learning applications in Wasserstein space. The authors establish
  explicit lower bounds for rotations of random vectors with uncorrelated components
  in R^2 using the Bures metric between covariance matrices, and upper bounds for
  compositions of translations, dilations, and rotations.
---

# On Wasserstein distances for affine transformations of random vectors

## Quick Facts
- arXiv ID: 2310.03945
- Source URL: https://arxiv.org/abs/2310.03945
- Reference count: 21
- Primary result: Provides theoretical bounds for quadratic Wasserstein distances under affine transformations, with applications to synthetic handwriting dataset generation

## Executive Summary
This paper establishes theoretical bounds for the quadratic Wasserstein distance between random vectors in R^n under affine transformations. The authors focus on random vectors with uncorrelated components and derive explicit lower bounds using the Bures metric between covariance matrices for rotations, and upper bounds for compositions of translations, dilations, and rotations. The work is motivated by applications in manifold learning where Wasserstein geometry provides a natural framework for analyzing data lying on low-dimensional manifolds embedded in high-dimensional spaces.

## Method Summary
The paper develops theoretical bounds for Wasserstein distances under affine transformations by leveraging properties of optimal transport and covariance matrices. For lower bounds, it computes the Bures metric between covariance matrices of rotated random vectors with uncorrelated components. For upper bounds, it applies the triangle inequality to decompose compositions of transformations into sequential Wasserstein distances. The framework is then applied to generate synthetic handwriting datasets by constructing uniform distributions over curves representing letters, enabling controlled studies of manifold learning algorithms in Wasserstein space.

## Key Results
- Explicit lower bounds for rotated copies of random vectors with uncorrelated components using the Bures metric
- Upper bounds for compositions of translations, dilations, and rotations via triangle inequality
- Framework for generating synthetic handwriting datasets based on uniform distributions over curves
- Validation of bound quality through comparison with actual Wasserstein distances for various distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The quadratic Wasserstein distance between rotated random vectors with uncorrelated components can be effectively bounded using the Bures metric between covariance matrices.
- Mechanism: The paper leverages the relationship between Wasserstein distance and optimal transport, where the quadratic Wasserstein distance between two random vectors can be decomposed into a mean difference term and a covariance-based trace term. For uncorrelated components, the rotation operation preserves certain structural properties that allow explicit computation of the covariance matrix transformations, enabling calculation of the Bures metric as a lower bound.
- Core assumption: Random vectors have uncorrelated components, allowing the covariance matrix to be diagonal and simplifying the Bures metric computation.
- Evidence anchors:
  - [abstract]: "we give concrete lower bounds for rotated copies of random vectors in $\mathbb{R}^2$ with uncorrelated components by computing the Bures metric between the covariance matrices"
  - [section]: "Theorem 2.8. Let $X=\begin{pmatrix}X_1\\X_2\end{pmatrix}\in \mathbb{R}^2$ have uncorrelated components. Then for any $\theta, \varphi \in [0,2\pi)$..."
- Break condition: The bound becomes less meaningful or trivial when the covariance matrices become singular or when the components are correlated, as the trace term computation becomes more complex.

### Mechanism 2
- Claim: Compositions of affine transformations (translation, dilation, rotation) can be bounded by decomposing into sequential Wasserstein distances.
- Mechanism: The triangle inequality for Wasserstein distance allows bounding the distance between a composition of transformations by summing the distances of individual transformations. The paper derives explicit upper bounds for these components based on the properties of each transformation type.
- Core assumption: The triangle inequality holds for Wasserstein distance, and individual transformation bounds are additive.
- Evidence anchors:
  - [section]: "Theorem 2.15. Let $X=\begin{pmatrix}X_1\\X_2\end{pmatrix}$ be a random vector in $\mathbb{R}^2$ with uncorrelated components..."
  - [section]: "Wasserstein distance between a composition of transformations by summing the distances of individual transformations"
- Break condition: The bound becomes loose when transformations interact in non-additive ways or when the individual bounds are not tight, particularly for large transformation parameters.

### Mechanism 3
- Claim: 1-dimensional manifold structures in $\mathbb{R}^2$ can be used to generate synthetic handwriting datasets with controlled Wasserstein geometry.
- Mechanism: By constructing random vectors that trace out 1-dimensional curves (letters), and applying affine transformations with bounded Wasserstein distances, one can create synthetic datasets where the geometric relationships between samples are known and controllable. This enables controlled testing of manifold learning algorithms in Wasserstein space.
- Core assumption: The curves representing letters can be parameterized as random vectors with uncorrelated components, and affine transformations preserve the useful structure for learning.
- Evidence anchors:
  - [abstract]: "Finally, we give a framework for mimicking handwritten digit or alphabet datasets that can be applied in a manifold learning framework."
  - [section]: "3.1. Toward handwriting analysis. In the following examples we illustrate construction of uniform distributions over shapes represented by random curves with uncorrelated components..."
- Break condition: The assumption of uncorrelated components may not hold for more complex letter shapes, or the affine transformations may distort the manifold structure in ways that break the learning algorithm's assumptions.

## Foundational Learning

- Concept: Wasserstein distance and optimal transport theory
  - Why needed here: The entire paper builds on the mathematical properties of Wasserstein distance as a metric on probability measures, and how it relates to optimal transport problems.
  - Quick check question: Can you explain the difference between the quadratic Wasserstein distance and other metrics like Euclidean distance when comparing probability distributions?

- Concept: Covariance matrices and the Bures metric
  - Why needed here: The lower bounds for rotated random vectors rely on computing the Bures metric between covariance matrices, which requires understanding positive semi-definite matrices and their properties.
  - Quick check question: Given two covariance matrices Σ₁ and Σ₂, can you write out the formula for the Bures metric between them?

- Concept: Affine transformations and their effect on random vectors
  - Why needed here: The paper extensively analyzes how translations, dilations, and rotations affect the mean and covariance of random vectors, which directly impacts the Wasserstein distance.
  - Quick check question: If X is a random vector with mean m and covariance Σ, what are the mean and covariance of AX + b for matrix A and vector b?

## Architecture Onboarding

- Component map: Random Vector Generation -> Affine Transformation Application -> Wasserstein Distance Computation -> Bound Calculation
- Critical path: For a new user, the typical workflow would be: (1) Define the random vector structure (uncorrelated components), (2) Choose the transformation(s) of interest, (3) Apply the appropriate theoretical bound (lower for rotation, upper for composition), (4) Generate synthetic data if needed for testing.
- Design tradeoffs: The paper prioritizes analytical tractability (uncorrelated components, specific transformation types) over generality. This allows for explicit formulas but limits applicability to cases that don't satisfy these assumptions. The synthetic dataset generation trades realism for control and interpretability.
- Failure signatures: The bounds become trivial (zero) when the covariance structure is too simple (e.g., one component has zero variance), or when the transformation parameters are too large relative to the data scale. The synthetic dataset approach may not capture all the variability present in real handwriting data.
- First 3 experiments:
  1. Verify the rotation bound: Generate a random vector with uncorrelated components, compute the actual Wasserstein distance between rotated copies using numerical methods, and compare with the theoretical lower bound from Theorem 2.8.
  2. Test composition bounds: Apply a sequence of translation, dilation, and rotation to a random vector, compute the actual Wasserstein distance, and compare with the upper bound from Theorem 2.15.
  3. Synthetic dataset validation: Generate a synthetic alphabet dataset using the framework from Section 3, apply manifold learning algorithms, and verify that the learned structure matches the known transformation parameters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what precise conditions does the lower bound of the Bures metric exactly equal the Wasserstein distance for rotated copies of random vectors with uncorrelated components?
- Basis in paper: [explicit] The paper discusses when the lower bound is attained, citing Gelbrich's work on elliptically contoured distributions and noting that Gaussians satisfy equality, but states that for many distributions the bound is only an approximation.
- Why unresolved: The paper provides examples showing the bound is close but not exact for uniform distributions on squares and rectangles, yet doesn't characterize the full class of distributions for which equality holds.
- What evidence would resolve it: A complete characterization theorem proving necessary and sufficient conditions for equality, along with counterexamples for distributions that fail to achieve equality.

### Open Question 2
- Question: Can the upper bounds for compositions of affine transformations be tightened for specific classes of random vectors beyond the general bounds provided?
- Basis in paper: [explicit] The paper derives upper bounds using the Dowson-Landau upper bound, which it notes is typically far from optimal based on examples showing the lower bound provides a good approximation.
- Why unresolved: The paper acknowledges the upper bound is often loose but doesn't explore whether tighter bounds exist for particular distributions or transformation sequences.
- What evidence would resolve it: Derivation of tighter upper bounds for specific distribution families (e.g., uniform distributions on convex sets) or proof that the general bounds cannot be improved beyond a certain factor.

### Open Question 3
- Question: How do the affine transformation bounds extend to non-Euclidean Wasserstein spaces or alternative cost functions?
- Basis in paper: [inferred] The paper focuses exclusively on quadratic Wasserstein distances in Euclidean spaces, though the introduction mentions optimal transport's applications in diverse fields, suggesting broader applicability.
- Why unresolved: The paper doesn't explore whether the techniques for deriving lower and upper bounds can be generalized to non-quadratic costs or non-Euclidean domains.
- What evidence would resolve it: Extension of the Bures metric and affine transformation analysis to Riemannian manifolds or demonstration of specific cases where the Euclidean approach fails in non-Euclidean settings.

## Limitations

- The assumption of uncorrelated components significantly restricts applicability to real-world data where correlation is typically present
- The bounds become increasingly loose for large transformation parameters, with the upper bound potentially becoming orders of magnitude larger than the actual Wasserstein distance
- The synthetic handwriting dataset generation relies heavily on the assumption that letter shapes can be adequately represented as 1-dimensional manifolds with uncorrelated components

## Confidence

- **High Confidence**: The theoretical framework for Wasserstein distances and the application of the triangle inequality for composed transformations
- **Medium Confidence**: The explicit computation of lower bounds using the Bures metric for rotated vectors with uncorrelated components
- **Low Confidence**: The effectiveness of the synthetic dataset generation framework for real-world manifold learning applications

## Next Checks

1. **Correlation Structure Validation**: Test the bounds with random vectors having varying degrees of correlation between components. Measure how the bound quality degrades as correlation increases, and identify the threshold beyond which the bounds become practically useless.

2. **Large Transformation Parameter Study**: Systematically vary the magnitude of affine transformation parameters (rotation angles, translation distances, dilation factors) and measure the gap between theoretical bounds and actual Wasserstein distances. This will quantify the practical limits of the bounds.

3. **Synthetic vs Real Data Comparison**: Generate synthetic handwriting datasets using the proposed framework and compare their Wasserstein geometry with real handwriting datasets. Measure the discrepancy in manifold learning performance when algorithms are trained on synthetic versus real data.