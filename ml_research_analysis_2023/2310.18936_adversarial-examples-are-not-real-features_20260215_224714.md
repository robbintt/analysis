---
ver: rpa2
title: Adversarial Examples Are Not Real Features
arxiv_id: '2310.18936'
source_url: https://arxiv.org/abs/2310.18936
tags:
- features
- robust
- non-robust
- learning
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper re-examines the robust and non-robust feature theory
  from a cross-paradigm learning perspective. The authors find that while non-robust
  features are useful for supervised classification, they are much less useful when
  transferred to self-supervised learning paradigms like contrastive learning, masked
  image modeling, and diffusion models.
---

# Adversarial Examples Are Not Real Features

## Quick Facts
- arXiv ID: 2310.18936
- Source URL: https://arxiv.org/abs/2310.18936
- Authors: 
- Reference count: 40
- Primary result: Non-robust features are paradigm-wise shortcuts that fail when transferred to self-supervised learning paradigms

## Executive Summary
This paper re-examines the robust and non-robust feature theory through a cross-paradigm learning lens. The authors demonstrate that non-robust features, while useful for supervised classification, lose effectiveness when transferred to self-supervised paradigms like contrastive learning, masked image modeling, and diffusion models. This suggests these features are paradigm-specific shortcuts rather than truly useful features. The study also reveals that robust features extracted from supervised models lose their robustness when evaluated under more reliable attacks (AutoAttack) across different learning paradigms, and that adversarial examples have poor cross-paradigm transferability.

## Method Summary
The authors construct robust and non-robust datasets using iterative optimization with PGD attacks on CIFAR-10 and Tiny-ImageNet-200. They train encoders using four learning paradigms: supervised learning, contrastive learning, masked image modeling, and diffusion models. The learned representations are evaluated using linear probing heads across paradigms, with cross-paradigm relative usefulness (CRU) and robustness (CRR) metrics measuring minimum performance across paradigms. The study compares reliability of robustness evaluation using PGD versus AutoAttack and examines adversarial example transferability between paradigms.

## Key Results
- Non-robust features show poor cross-paradigm usefulness, working well only in supervised learning but failing in self-supervised paradigms
- Robust features extracted from supervised models lose robustness when evaluated under reliable attacks (AutoAttack) and across paradigms
- Adversarial examples have limited transferability between different learning paradigms, even with the same architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-robust features are paradigm-wise shortcuts rather than truly useful features.
- Mechanism: Non-robust features that work well under supervised learning fail to transfer to self-supervised paradigms like contrastive learning, masked image modeling, and diffusion models. This indicates they are only useful within their original learning context.
- Core assumption: Truly useful features should maintain usefulness across different learning paradigms, not just within one.
- Evidence anchors:
  - [abstract] "non-robust features attain poor usefulness when transferred to other self-supervised learning paradigms"
  - [section 3.2] "models pretrained from the non-robust datasets... differ a lot across different paradigms: they work well for learning from the supervised task (ResNet-50), but much worse on all the other SSL paradigms"
- Break condition: If non-robust features show consistent usefulness across multiple paradigms, this mechanism fails.

### Mechanism 2
- Claim: Adversarial examples have poor cross-paradigm transferability due to paradigm-wise non-robust features.
- Mechanism: Adversarial examples crafted using one learning paradigm's objective (e.g., supervised cross-entropy) have limited effect when evaluated under different paradigms' objectives (e.g., contrastive InfoNCE loss).
- Core assumption: The effectiveness of adversarial perturbations depends on the specific learning paradigm's objective function.
- Evidence anchors:
  - [section 5.1] "there is a significant difference between the changes of different objectives, and maximizing one attack objective has a limited effect on the other objective"
  - [section 5.2] "adversarial examples transfer poorly between different paradigms, even under the same architecture"
- Break condition: If adversarial examples show strong transferability across paradigms, this mechanism fails.

### Mechanism 3
- Claim: Robust features extracted from supervised models are not truly robust when evaluated under more reliable attacks and across paradigms.
- Mechanism: Robustness evaluations using PGD attacks overestimate true robustness, and robust features lose effectiveness when transferred to self-supervised learning paradigms.
- Core assumption: Robustness should be evaluated using reliable attacks (like AutoAttack) and across multiple learning paradigms to avoid false positives.
- Evidence anchors:
  - [section 4.2] "contrary to the findings in Ilyas et al. [19] that natural training on the robust dataset produces a robust classifier, we find these so-called robust features hardly show robustness when learned with other learning paradigms"
- Break condition: If robust features maintain effectiveness under reliable attacks and across paradigms, this mechanism fails.

## Foundational Learning

- Concept: Cross-paradigm feature evaluation
  - Why needed here: To determine if features are truly useful or just paradigm-specific shortcuts
  - Quick check question: Can a feature that works well for supervised learning also work well for self-supervised learning tasks like contrastive learning or masked image modeling?

- Concept: Reliable robustness evaluation methods
  - Why needed here: To avoid false conclusions about model robustness that come from using unreliable attack methods
  - Quick check question: Does using AutoAttack instead of PGD significantly change the measured robustness of a model?

- Concept: Paradigm-wise feature extraction and transfer
  - Why needed here: To understand how features learned in one context perform when applied to different learning paradigms
  - Quick check question: If you extract features from a supervised model, will they retain their usefulness when used for generative tasks like diffusion models?

## Architecture Onboarding

- Component map: Data construction module -> Multi-paradigm training system -> Linear probing evaluation -> Cross-paradigm robustness testing
- Critical path:
  1. Construct robust and non-robust datasets from supervised models
  2. Train encoders on these datasets using four different learning paradigms
  3. Evaluate cross-paradigm usefulness via linear probing
  4. Evaluate cross-paradigm robustness using reliable attacks
  5. Analyze transferability of adversarial examples across paradigms
- Design tradeoffs:
  - Dataset construction vs. computational cost: More iterations yield better feature separation but increase computation time
  - Attack reliability vs. computational efficiency: AutoAttack is more reliable but slower than PGD
  - Paradigm diversity vs. implementation complexity: More paradigms provide better cross-paradigm insights but increase implementation complexity
- Failure signatures:
  - Poor cross-paradigm transferability suggests features are paradigm-specific shortcuts
  - Robustness overestimation with PGD but not AutoAttack indicates unreliable evaluation
  - Inconsistent results across random seeds may indicate unstable feature extraction
- First 3 experiments:
  1. Replicate the cross-paradigm usefulness evaluation using CIFAR-10 to verify non-robust features lose effectiveness in self-supervised paradigms
  2. Test adversarial example transferability by generating attacks with one paradigm's objective and evaluating with another paradigm's encoder
  3. Compare robustness evaluation using PGD vs AutoAttack on robust features to demonstrate the reliability difference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically identify truly useful features that generalize across learning paradigms, beyond just supervised learning?
- Basis in paper: [explicit] The authors note that non-robust features are paradigm-wise shortcuts that work well for supervised classification but fail in self-supervised paradigms like contrastive learning, masked image modeling, and diffusion models. They define cross-paradigm usefulness as a measure of true feature informativeness.
- Why unresolved: The paper identifies the gap between paradigm-wise and cross-paradigm usefulness but doesn't provide a systematic method to identify features that would be truly useful across paradigms. Current methods like adversarial training focus on single paradigms.
- What evidence would resolve it: Developing and testing feature selection/engineering methods that explicitly optimize for cross-paradigm usefulness, then validating these features' performance across diverse learning paradigms including supervised, self-supervised, and potentially reinforcement learning tasks.

### Open Question 2
- Question: What is the minimum set of learning paradigms needed to reliably evaluate cross-paradigm feature usefulness and robustness?
- Basis in paper: [inferred] The authors use four paradigms (supervised, contrastive learning, masked image modeling, and diffusion models) but acknowledge that different paradigms yield representations at different levels. They note that if one paradigm performs poorly even on natural images, it could dominate cross-paradigm metrics.
- Why unresolved: The paper uses a specific set of four paradigms but doesn't systematically analyze whether this is sufficient or optimal for evaluating cross-paradigm properties. Some paradigms might be more informative than others.
- What evidence would resolve it: Experimental analysis showing how the choice and number of paradigms affects the measurement of cross-paradigm usefulness and robustness, potentially identifying a minimal set that provides reliable evaluation.

### Open Question 3
- Question: Can adversarial training be effectively extended to multiple learning paradigms simultaneously to improve robustness?
- Basis in paper: [explicit] The authors suggest that "a mixture of adversarial training on multiple paradigms may come to the aid" and note that adversarial vulnerability may not come from data alone but requires joint training against both data-level and model-level vulnerabilities.
- Why unresolved: While the authors propose this direction, they don't provide experimental validation or theoretical framework for multi-paradigm adversarial training. The interaction between different paradigms during adversarial training is unexplored.
- What evidence would resolve it: Experimental results comparing single-paradigm adversarial training versus multi-paradigm adversarial training on both in-distribution and out-of-distribution robustness, along with analysis of how different paradigms complement each other during training.

## Limitations
- The evaluation of non-robust features across self-supervised paradigms may not capture all potential use cases
- The robustness evaluation using AutoAttack, while more reliable than PGD, may still not capture all real-world attack scenarios
- The feature extraction process through iterative optimization could introduce biases that affect the generalizability of results

## Confidence

- High confidence: The core finding that non-robust features show poor cross-paradigm transferability is well-supported by experimental evidence and aligns with established principles of feature generalization.
- Medium confidence: The claim about robust features losing effectiveness across paradigms is supported but could benefit from additional attack scenarios and larger-scale evaluations.
- Medium confidence: The adversarial example transferability results are compelling but limited by the specific attack methods and paradigms evaluated.

## Next Checks

1. **Cross-paradigm generalization test**: Evaluate non-robust features across additional self-supervised paradigms (e.g., masked autoencoding, vision transformers) to verify if the observed poor transferability is paradigm-specific or a general phenomenon.

2. **Alternative attack evaluation**: Test the robustness of features extracted from different paradigms using a broader range of attack methods beyond AutoAttack, including black-box attacks and adaptive adversaries, to validate the reliability of robustness assessments.

3. **Transferability under diverse conditions**: Generate adversarial examples using various attack objectives and evaluate their effectiveness across paradigms with different model architectures to determine if the observed poor transferability holds under different experimental conditions.