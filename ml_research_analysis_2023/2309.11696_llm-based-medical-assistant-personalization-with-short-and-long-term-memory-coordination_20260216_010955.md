---
ver: rpa2
title: LLM-based Medical Assistant Personalization with Short- and Long-Term Memory
  Coordination
arxiv_id: '2309.11696'
source_url: https://arxiv.org/abs/2309.11696
tags:
- memory
- knowledge
- user
- language
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of personalizing LLM-based medical
  assistants by proposing a novel memory-augmented framework called MaLP. The core
  idea involves a Dual-Process enhanced Memory (DPeM) mechanism inspired by neuroscience,
  consisting of working memory, short-term memory, and long-term memory, which collaboratively
  store and retrieve user-specific and common-sense knowledge.
---

# LLM-based Medical Assistant Personalization with Short- and Long-Term Memory Coordination

## Quick Facts
- **arXiv ID**: 2309.11696
- **Source URL**: https://arxiv.org/abs/2309.11696
- **Reference count**: 5
- **Primary result**: MaLP framework with Dual-Process enhanced Memory (DPeM) and PEFT improves medical assistant personalization across QA, preference classification, and response generation tasks.

## Executive Summary
This paper presents MaLP, a memory-augmented framework for personalizing LLM-based medical assistants. The core innovation is a Dual-Process enhanced Memory (DPeM) mechanism inspired by neuroscience, which coordinates working memory, short-term memory, and long-term memory to store and retrieve user-specific and common-sense knowledge. Combined with parameter-efficient fine-tuning (PEFT) using LoRA, the framework adapts LLaMA-7B to user preferences without full fine-tuning. Experimental results demonstrate significant improvements over standard LLM settings and dictionary-based memory methods across three medical dialogue tasks, validated through both automatic metrics and human evaluation.

## Method Summary
MaLP personalizes medical assistants through a dual-component system: DPeM memory architecture and PEFT fine-tuning. DPeM implements three memory tiers—working memory (buffer), short-term memory (recent knowledge), and long-term memory (frequently accessed knowledge)—coordinated by a natural language processing component. The system uses two distinct retrievers: closest-match (Levenshtein distance) for STM and semantic-match for LTM. PEFT employs LoRA to update only a small subset of parameters, enabling efficient adaptation to user preferences while preserving base LLM knowledge. The framework was evaluated on a newly constructed medical dialogue dataset using LLaMA-7B and GPT-3.5 models.

## Key Results
- MaLP significantly outperforms standard LLM settings and dictionary-based memory methods on medical dialogue tasks
- Notable improvements in ROUGE scores (1.2-3.7% increase), classification accuracy (6.2% increase), and win rates for response quality (65% win rate)
- Human evaluation confirms effectiveness of the proposed approach for response personalization
- Parameter-efficient fine-tuning enables adaptation without computational costs of full fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPeM provides more flexible memory management than dictionary-based methods.
- Mechanism: Dual-process schema separates information filtering (rehearsal) from long-term storage decisions (executive), allowing context-specific memory retention.
- Core assumption: Not all learned information should be stored permanently; some can be transient while others need long-term retention.
- Evidence anchors:
  - [abstract] "We contend that a mere memory module is inadequate and fully training an LLM can be excessively costly"
  - [section] "Drawing inspirations from this, we propose a novel Dual-Process enhanced Memory (DPeM) mechanism"
  - [corpus] Weak - corpus papers discuss memory systems but don't provide direct evidence for dual-process advantages
- Break condition: If the threshold θ for LTM transfer is poorly tuned, the system may store irrelevant information or discard important context.

### Mechanism 2
- Claim: PEFT enables efficient personalization without full fine-tuning.
- Mechanism: Low-Rank Adaptation (LoRA) updates only a small subset of parameters, preserving base LLM knowledge while adapting to user preferences.
- Core assumption: A small number of low-rank matrices can capture user-specific adaptation patterns effectively.
- Evidence anchors:
  - [abstract] "equipped with a parameter-efficient fine-tuning (PEFT) schema"
  - [section] "PEFT focuses on updating a small subset of parameters, ensuring that the trained LLM achieves promising performance"
  - [corpus] Weak - corpus papers discuss memory but not PEFT mechanisms specifically
- Break condition: If the LoRA rank r is too low, adaptation may be insufficient; if too high, computational savings diminish.

### Mechanism 3
- Claim: Memory utilization through retrieval enhances response personalization.
- Mechanism: Different retrieval strategies for STM (closest-match) and LTM (semantic-match) leverage memory structure for appropriate knowledge access.
- Core assumption: STM should be accessed by proximity while LTM should be accessed by semantic similarity.
- Evidence anchors:
  - [section] "we have designed two retrievers: a closest-match retriever, Rc, for STM retrieval, and a semantic-match retriever, Rs, for LTM retrieval"
  - [section] "Rc aims to find the knowledge stored in STM that is closest to the query in terms of Levenshtein distance"
  - [corpus] Moderate - some corpus papers discuss retrieval but not the dual-retrieval strategy
- Break condition: If the retriever cannot distinguish between similar contexts, it may retrieve irrelevant knowledge.

## Foundational Learning

- **Concept**: Dual-process theory in neuroscience
  - Why needed here: Provides the theoretical foundation for separating information processing into automatic/unconscious and effortful/analytical components
  - Quick check question: How does dual-process theory explain the difference between learning a new skill versus performing an automatic task?

- **Concept**: Parameter-efficient fine-tuning
  - Why needed here: Enables adaptation to user preferences without the computational cost of full fine-tuning
  - Quick check question: What is the key mathematical operation that makes LoRA parameter-efficient?

- **Concept**: Memory hierarchy (working, short-term, long-term)
  - Why needed here: Mirrors human cognitive architecture to manage different types of information with appropriate retention strategies
  - Quick check question: Why would some information be better stored in STM versus LTM in a medical assistant context?

## Architecture Onboarding

- **Component map**: User query → Retriever (Rc for STM, Rs for LTM) → PEFT-tuned LLM → Response generation
- **Critical path**: User query → Coordinator C → DPeM memory system → PEFT module → LLM base model → Response
- **Design tradeoffs**:
  - Memory complexity vs. retrieval accuracy: More complex memory structure may improve relevance but increase retrieval complexity
  - PEFT rank vs. adaptation quality: Higher rank allows better personalization but reduces computational efficiency
  - Threshold θ for LTM transfer: Too low wastes memory; too high loses valuable context
- **Failure signatures**:
  - Degradation in response quality despite sufficient historical data
  - Inconsistent responses to similar queries across different sessions
  - Memory retrieval returning irrelevant information
- **First 3 experiments**:
  1. Test memory retrieval accuracy with synthetic dialogues of known patterns
  2. Validate PEFT adaptation by comparing preference classification before/after fine-tuning
  3. Measure response quality improvement with increasing dialogue history length

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the DPeM mechanism be effectively scaled to handle larger and more complex datasets without significantly increasing computational costs?
- Basis in paper: [explicit] The paper mentions the need for future work on injecting expert knowledge into memory and enabling online-learning fashion.
- Why unresolved: The current implementation of DPeM and MaLP is in the prototype stage, and the paper does not provide a detailed plan for scaling the mechanism to larger datasets or integrating expert knowledge.
- What evidence would resolve it: A comprehensive study demonstrating the performance of DPeM and MaLP on larger datasets, along with a detailed analysis of computational costs and the integration of expert knowledge, would provide insights into the scalability of the proposed approach.

### Open Question 2
- Question: How can the safety of generated responses be further improved to ensure that the LLM-based medical assistant does not provide incorrect or harmful suggestions?
- Basis in paper: [explicit] The paper acknowledges the potential risks of incorrect medicine suggestions and mentions the use of explicit prompts to guide generation behaviors, but it also notes that this approach can be risky if the guidance is missed.
- Why unresolved: The paper does not provide a robust solution for ensuring the safety of generated responses, and the reliance on explicit prompts may not be sufficient to prevent all potential safety issues.
- What evidence would resolve it: A thorough evaluation of the safety of generated responses using a diverse set of test cases and scenarios, along with the development of additional safety mechanisms or filters, would help address this concern.

### Open Question 3
- Question: How can the DPeM mechanism be adapted to handle other domains beyond healthcare, such as legal consultation or customer service, while maintaining its effectiveness?
- Basis in paper: [explicit] The paper mentions the potential applicability of the data construction method to other scenarios, such as legal consultation, but does not provide specific details on how the DPeM mechanism would be adapted for these domains.
- Why unresolved: The effectiveness of DPeM in other domains is not explored in the paper, and the unique characteristics of different domains may require modifications to the memory structure and retrieval processes.
- What evidence would resolve it: A comparative study of DPeM's performance across multiple domains, along with an analysis of the necessary adaptations for each domain, would provide insights into the generalizability of the proposed approach.

## Limitations

- Implementation details of the coordinator component are not fully specified, particularly how it processes dialogue contexts into purified notes for memory storage
- The threshold θ for transferring information from STM to LTM lacks concrete guidance for medical dialogue scenarios
- Synthetic dataset generation process doesn't provide sufficient detail on prompt engineering or validation of generated profiles and preferences
- Computational overhead of maintaining three memory tiers with different retrieval mechanisms raises scalability concerns

## Confidence

**High confidence**: The core mechanism of combining memory-augmented LLM with PEFT is sound and well-grounded in existing literature. The improvements over baseline methods on the reported metrics are statistically significant and consistent across multiple tasks.

**Medium confidence**: The specific dual-process memory architecture and its neuroscience inspiration are theoretically compelling, but the direct translation to practical implementation details remains partially opaque. The choice of LoRA rank r=8 is reasonable but not empirically justified for this specific medical dialogue context.

**Low confidence**: The scalability of the approach to real-world medical settings with diverse patient populations and complex medical knowledge. The paper doesn't address potential privacy concerns or regulatory compliance for medical applications.

## Next Checks

1. **Memory retrieval validation**: Conduct controlled experiments with synthetic dialogues where the expected knowledge retrieval is known, measuring precision and recall of the dual-retrieval system. Test edge cases where similar contexts should trigger different memory responses based on user preferences.

2. **PEFT adaptation robustness**: Systematically vary the LoRA rank (r=2, 4, 8, 16) and measure the trade-off between personalization quality and computational efficiency. Include stress tests where the base LLM knowledge is deliberately contradicted by user preferences.

3. **Long-term memory stability**: Implement a longitudinal evaluation where the model is tested on consistency of responses to recurring queries over extended dialogue histories (50+ turns). Measure for memory degradation or drift in personalization over time.