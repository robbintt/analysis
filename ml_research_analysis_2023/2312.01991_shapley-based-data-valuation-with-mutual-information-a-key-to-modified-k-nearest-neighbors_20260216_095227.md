---
ver: rpa2
title: 'Shapley-Based Data Valuation with Mutual Information: A Key to Modified K-Nearest
  Neighbors'
arxiv_id: '2312.01991'
source_url: https://arxiv.org/abs/2312.01991
tags:
- data
- dataset
- neighbors
- algorithm
- imknn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Information-Modified KNN (IM-KNN), a method
  that improves the K-Nearest Neighbors (KNN) algorithm by leveraging Mutual Information
  (MI) and Shapley values to assign weighted values to neighbors. The authors address
  the limitation of traditional KNN that treats all samples equally by introducing
  a weighted approach based on MI and distance.
---

# Shapley-Based Data Valuation with Mutual Information: A Key to Modified K-Nearest Neighbors

## Quick Facts
- arXiv ID: 2312.01991
- Source URL: https://arxiv.org/abs/2312.01991
- Reference count: 40
- Information-Modified KNN (IM-KNN) improves accuracy, precision, and recall by 16.80%, 17.08%, and 16.98% respectively

## Executive Summary
This paper proposes Information-Modified KNN (IM-KNN), a method that improves the K-Nearest Neighbors (KNN) algorithm by leveraging Mutual Information (MI) and Shapley values to assign weighted values to neighbors. The authors address the limitation of traditional KNN that treats all samples equally by introducing a weighted approach based on MI and distance. IM-KNN achieves significant improvements in accuracy, precision, and recall compared to traditional KNN and other variants, with an average improvement of 16.80%, 17.08%, and 16.98%, respectively, across 12 benchmark datasets. The method also demonstrates robustness to noise, imbalanced data, and skewed distributions in experiments on four large-scale datasets.

## Method Summary
IM-KNN preprocesses the training dataset by standardizing it to have a mean of 0 and a variance of 1 (N(0,1)). The algorithm then evaluates the validity of each sample based on Mutual Information (MI) and Shapley values. It performs weighted KNN using validity assessment, calculating Normalized Mutual Information (NMI) and information value for each sample. The final classification prediction is determined using a combination of distance weights and MI-based weights through a parameter α.

## Key Results
- IM-KNN improves accuracy, precision, and recall by 16.80%, 17.08%, and 16.98% respectively compared to traditional KNN
- Significant improvements observed across 12 benchmark datasets
- Demonstrates robustness to noise, imbalanced data, and skewed distributions on four large-scale datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mutual Information (MI) improves neighbor selection by capturing non-linear dependencies that Euclidean distance alone misses
- Mechanism: MI measures statistical dependence between features and class labels, identifying neighbors that share more informative patterns beyond mere proximity
- Core assumption: Higher MI between a neighbor and the target class indicates more useful information for classification
- Evidence anchors:
  - [abstract] "Our approach leverages Mutual Information (MI) to enhance the significance of weights"
  - [section II] "NMI using the equation 2" and "we calculate the Normalized Mutual Information (NMI)"
  - [corpus] Weak - no direct neighbor mentions MI-based neighbor selection
- Break condition: If MI values become uniform across neighbors (e.g., in highly correlated datasets), the weighting advantage disappears

### Mechanism 2
- Claim: Shapley value-based validity assessment weights neighbors according to their marginal contribution to classification coalitions
- Mechanism: Each training sample's validity is computed based on its contribution to different subsets of neighbors, following cooperative game theory principles
- Core assumption: A neighbor's contribution to classification accuracy varies based on which other neighbors are present
- Evidence anchors:
  - [section II] "The validity of each data point is computed based on MI of all points" and "Equation PK k=1 |S|!(K−|S|−1)! k! calculates all conceivable coalitions"
  - [abstract] "Shapley values, a concept originating from cooperative game theory, to refine value allocation"
  - [corpus] Weak - no direct neighbor mentions Shapley-based weighting
- Break condition: If all neighbors contribute equally (symmetric problem), Shapley values become uniform and provide no advantage

### Mechanism 3
- Claim: Distance-weighted KNN with MI-based weights creates a hybrid that captures both proximity and information content
- Mechanism: The final classification combines traditional distance weights with information-based weights through a parameter α
- Core assumption: Both distance proximity and information relevance matter for optimal neighbor influence
- Evidence anchors:
  - [section II] "Wik ← (1 − distance(Ui, Di)P k distance(Ui, Di) )/(k − 1)" for distance weights and "α(PK k=1 WikYk) + (1 − α)(PK k=1 XiYk)" for final combination
  - [abstract] "assign weighted values to neighbors, thereby bridging the gap in treating all samples with the same value and weight"
  - [corpus] Weak - no direct neighbor mentions hybrid weighting schemes
- Break condition: If distance and MI weights are perfectly correlated, the hybrid approach adds no value over either method alone

## Foundational Learning

- Concept: Mutual Information and its calculation
  - Why needed here: MI quantifies the dependency between features and class labels, which is central to the IM-KNN weighting scheme
  - Quick check question: What does MI measure that correlation does not capture?

- Concept: Shapley values and cooperative game theory
  - Why needed here: Shapley values determine fair contribution weights for each neighbor based on marginal contributions to different coalitions
  - Quick check question: How does the Shapley value formula account for all possible coalitions of neighbors?

- Concept: Distance-weighted KNN mechanics
  - Why needed here: Understanding traditional distance weighting is essential before appreciating how IM-KNN modifies it with MI
  - Quick check question: How do distance weights typically decay with increasing neighbor distance?

## Architecture Onboarding

- Component map: Distance calculation module → Weight assignment module → MI computation module → Shapley value validation module → Hybrid classification module
- Critical path: Neighbor distance computation → Weight calculation → MI computation → Validity assessment → Final classification prediction
- Design tradeoffs:
  - MI computation is expensive (O(n²)) but improves neighbor quality
  - Shapley value calculation adds computational overhead but provides principled weighting
  - The α parameter balances distance vs information influence but requires tuning
- Failure signatures:
  - Uniform MI values across neighbors → degraded performance
  - High computational cost on large datasets → scalability issues
  - Poor results on highly correlated features → MI becomes less discriminative
- First 3 experiments:
  1. Run traditional KNN vs IM-KNN on Iris dataset with varying K values
  2. Test sensitivity to α parameter by sweeping from 0 to 1
  3. Evaluate performance degradation when MI computation is disabled

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Information-Modified KNN (IM-KNN) algorithm perform when applied to real-time data streams or online learning scenarios?
- Basis in paper: [inferred] The paper discusses the adaptability of KNN to changing data without retraining, making it suitable for online learning scenarios, but does not specifically address the performance of IM-KNN in such contexts.
- Why unresolved: The paper focuses on the performance of IM-KNN in static datasets and does not explore its behavior in dynamic, real-time environments.
- What evidence would resolve it: Experimental results comparing the performance of IM-KNN in real-time data streams or online learning scenarios against other algorithms would provide insights into its effectiveness in such contexts.

### Open Question 2
- Question: How does the Information-Modified KNN (IM-KNN) algorithm handle high-dimensional data, and what impact does dimensionality have on its performance?
- Basis in paper: [explicit] The paper mentions that the curse of dimensionality can cause KNN's efficacy to decline in high-dimensional domains, but it does not specifically address how IM-KNN performs in such scenarios.
- Why unresolved: The paper does not provide experimental results or analysis of IM-KNN's performance in high-dimensional datasets.
- What evidence would resolve it: Experimental results evaluating IM-KNN's performance on high-dimensional datasets, along with comparisons to other algorithms, would shed light on its effectiveness in handling the curse of dimensionality.

### Open Question 3
- Question: How does the Information-Modified KNN (IM-KNN) algorithm perform when dealing with missing or incomplete data, and what preprocessing steps are required?
- Basis in paper: [inferred] The paper does not explicitly discuss the handling of missing or incomplete data, but it mentions the use of preprocessing steps such as standardization, suggesting that some form of data cleaning or imputation may be necessary.
- Why unresolved: The paper does not provide details on how IM-KNN handles missing data or what specific preprocessing steps are required.
- What evidence would resolve it: Experimental results evaluating IM-KNN's performance on datasets with missing values, along with a discussion of the preprocessing steps used, would provide insights into its robustness to incomplete data.

## Limitations
- Computational complexity of MI and Shapley value calculations raises scalability concerns for large datasets
- Empirical evaluation limited to relatively small benchmark datasets (maximum 2190 samples)
- Robustness claims for noise, imbalanced data, and skewed distributions lack detailed methodology and comprehensive experimental validation

## Confidence
- **High Confidence**: The mathematical framework for combining distance and MI weights is correctly specified, and the cooperative game theory foundation for Shapley values is valid.
- **Medium Confidence**: The claimed performance improvements (16.80% accuracy, 17.08% precision, 16.98% recall) are well-supported by the experimental results on the 12 benchmark datasets.
- **Low Confidence**: The robustness claims for noise, imbalanced data, and skewed distributions lack detailed methodology and comprehensive experimental validation.

## Next Checks
1. **Scalability Test**: Implement IM-KNN on a large dataset (100K+ samples) to empirically measure computational overhead and verify whether the performance gains justify the increased complexity.

2. **Hyperparameter Sensitivity**: Systematically vary the α parameter and K values across all datasets to determine optimal settings and assess whether the reported improvements are consistent across different parameter configurations.

3. **Robustness Validation**: Design controlled experiments with synthetic noise injection, class imbalance creation, and feature correlation manipulation to directly test the claimed robustness properties.