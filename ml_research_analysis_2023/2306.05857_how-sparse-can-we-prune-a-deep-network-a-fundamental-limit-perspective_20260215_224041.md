---
ver: rpa2
title: 'How Sparse Can We Prune A Deep Network: A Fundamental Limit Perspective'
arxiv_id: '2306.05857'
source_url: https://arxiv.org/abs/2306.05857
tags:
- pruning
- network
- algorithm
- ratio
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the fundamental question of how sparse a deep
  neural network can be pruned without hurting its performance. It takes a first-principles
  approach by formulating the pruning problem as minimizing an l1-regularized loss
  function and leverages tools from high-dimensional geometry to characterize the
  sharp phase transition point of pruning ratio, which is determined by the squared
  Gaussian width of a convex body normalized by the parameter dimension.
---

# How Sparse Can We Prune A Deep Network: A Fundamental Limit Perspective

## Quick Facts
- **arXiv ID**: 2306.05857
- **Source URL**: https://arxiv.org/abs/2306.05857
- **Reference count**: 40
- **Primary result**: Characterizes fundamental limit of pruning ratio using Gaussian width of convex sublevel sets, achieving competitive performance with global one-shot pruning algorithm

## Executive Summary
This paper establishes a theoretical framework for understanding the fundamental limits of neural network pruning by formulating the problem as minimizing an l1-regularized loss function. The key insight is that the maximum pruning ratio is determined by the squared Gaussian width of the convex sublevel set normalized by parameter dimension, which depends on both the weight magnitude and loss landscape sharpness. The authors develop an efficient spectrum estimation algorithm for large Hessian matrices and propose a convexification approach for non-positive definite cases, enabling practical computation of the pruning limit. Extensive experiments on CIFAR-10 and CIFAR-100 validate the theoretical predictions and demonstrate competitive performance of the proposed global one-shot pruning algorithm.

## Method Summary
The method formulates pruning as minimizing an l1-regularized loss function, creating a convex sublevel set whose Gaussian width determines the fundamental pruning limit. The approach leverages Gordon's Escape Theorem from high-dimensional geometry to characterize the sharp phase transition point. To compute the Gaussian width, the authors develop an improved spectrum estimation algorithm (SLQ) for large Hessian matrices and propose convexifying the deformed ellipsoid by taking absolute values of negative eigenvalues. The pruning algorithm sorts weights by magnitude after l1-regularization training and removes the smallest weights in a single step, achieving competitive performance with existing iterative methods.

## Key Results
- The pruning ratio threshold can be predicted using Gaussian width of convex sublevel sets with high accuracy
- Weight magnitude and loss landscape flatness are the two key factors determining pruning capacity
- The proposed one-shot pruning algorithm achieves competitive or better performance than iterative baselines
- Theoretical predictions align closely with experimental results on multiple network architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pruning ratio limit is determined by the squared Gaussian width of the convex body normalized by parameter dimension.
- Mechanism: The paper formulates pruning as an l1-regularized loss minimization problem, creating a convex sublevel set. Using high-dimensional geometry, specifically Gordon's Escape Theorem, they relate the phase transition point (maximum pruning ratio) to the Gaussian width of this convex body.
- Core assumption: The loss function sublevel set can be accurately approximated by an ellipsoid, and the Gaussian width of this ellipsoid characterizes the pruning limit.
- Evidence anchors:
  - [abstract]: "characterize the sharp phase transition point, which can be regarded as the fundamental limit of the pruning ratio... determined by the squared Gaussian width of a convex body normalized by the parameter dimension"
  - [section 3.1]: "The pruning ratio threshold of the network M can be expressed as: T(M, wp) = w(P(wp, S))²/D"
  - [corpus]: Weak - only 5/8 neighbors mention "sparse" or "prune" in titles, suggesting limited direct support for this specific geometric approach
- Break condition: If the loss landscape deviates significantly from ellipsoidal geometry, or if the network operates far from the optimal point where the Hessian approximation fails.

### Mechanism 2
- Claim: Loss function flatness and weight magnitude are the two key factors determining pruning ratio limit.
- Mechanism: Flatter loss landscapes (smaller eigenvalues in Hessian) and smaller weight magnitudes both increase the Gaussian width of the sublevel set, allowing for higher pruning ratios without performance degradation.
- Core assumption: The relationship between eigenvalue magnitude, weight magnitude, and Gaussian width is linear and predictable.
- Evidence anchors:
  - [abstract]: "key factors that influence the pruning ratio limit are identified as the weight magnitude and the sharpness of the loss landscape"
  - [section 5.4]: "as the magnitude of network weights increases, the capacity of the network to tolerate pruning decreases"
  - [corpus]: Weak - neighbors don't explicitly discuss loss landscape properties or their relationship to pruning limits
- Break condition: If the loss landscape becomes highly non-convex or if weight distributions are extremely skewed, breaking the assumed linear relationships.

### Mechanism 3
- Claim: The proposed global one-shot pruning algorithm based on l1-regularization achieves competitive or better performance than existing pruning algorithms.
- Mechanism: By incorporating l1-regularization during training and pruning based on weight magnitude, the algorithm directly optimizes for sparsity while maintaining performance, avoiding iterative pruning cycles.
- Core assumption: l1-regularization effectively identifies and preserves important weights while eliminating redundant ones in a single step.
- Evidence anchors:
  - [abstract]: "proposed global one-shot pruning algorithm achieves competitive or even better performance than existing pruning algorithms"
  - [section 5.1]: "our pruning algorithm is the fastest and best performing than other algorithms" with quantitative comparisons
  - [corpus]: Weak - no direct neighbor support for the specific one-shot approach or its performance claims
- Break condition: If the weight importance cannot be accurately captured by magnitude alone, or if the network requires fine-grained pruning strategies that cannot be captured in a single step.

## Foundational Learning

- Concept: High-dimensional convex geometry and Gaussian width
  - Why needed here: The core theoretical framework relies on understanding how Gaussian width characterizes the complexity of convex bodies and relates to the maximum pruning ratio
  - Quick check question: How does Gordon's Escape Theorem connect the Gaussian width of a set to the probability of a random subspace intersecting that set?

- Concept: Convex relaxation and l1-norm approximation
  - Why needed here: The transition from the non-convex l0-norm to the convex l1-norm is fundamental to making the pruning optimization tractable
  - Quick check question: Why is the l1-norm a good convex approximation of the l0-norm for sparsity-inducing regularization?

- Concept: Hessian matrix properties and spectrum estimation
  - Why needed here: The eigenvalues of the Hessian matrix determine the geometry of the loss sublevel set, which directly impacts the pruning ratio limit calculation
  - Quick check question: Why does the paper need to convexify the deformed ellipsoid when the Hessian matrix is non-positive definite?

## Architecture Onboarding

- Component map: Train with l1-regularization -> Compute Hessian spectrum -> Calculate Gaussian width -> Predict pruning ratio -> Apply one-shot pruning -> Validate performance
- Critical path: Train with l1-regularization → Calculate Hessian spectrum → Compute Gaussian width → Predict pruning ratio → Apply one-shot pruning → Validate performance
- Design tradeoffs: Global one-shot vs. iterative pruning (speed vs. potential precision), theoretical accuracy vs. computational feasibility (exact vs. approximate Gaussian width)
- Failure signatures: Large discrepancy between predicted and actual pruning ratios, poor performance of pruned networks, computational infeasibility of spectrum estimation
- First 3 experiments:
  1. Run the one-shot pruning algorithm on FC-5 model with CIFAR-10 and compare test accuracy at different sparsity levels
  2. Validate the predicted pruning ratio threshold by finding where accuracy begins to degrade significantly
  3. Test the improved SLQ spectrum estimation algorithm on AlexNet to verify eigenvalue computation accuracy and pruning ratio prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise impact of the network's loss landscape flatness on the maximum pruning ratio?
- Basis in paper: [explicit] The paper states that "the flatter the loss landscape or the smaller the weight magnitude, the smaller pruning ratio."
- Why unresolved: While the paper establishes a correlation between flatness and pruning ratio, it does not provide a quantitative relationship or threshold.
- What evidence would resolve it: Experimental data showing the relationship between loss landscape flatness (measured by Hessian eigenvalues) and the maximum achievable pruning ratio across various network architectures and datasets.

### Open Question 2
- Question: How does the proposed improved spectrum estimation algorithm compare to other methods for large-scale Hessian matrices in terms of accuracy and computational efficiency?
- Basis in paper: [explicit] The paper introduces an improved spectrum estimation algorithm to address the challenge of computing eigenvalues for large Hessian matrices in deep neural networks.
- Why unresolved: The paper does not provide a comparative analysis of the proposed algorithm with other existing methods.
- What evidence would resolve it: Benchmarking results comparing the proposed algorithm with other state-of-the-art spectrum estimation methods on large-scale Hessian matrices.

### Open Question 3
- Question: What is the theoretical justification for using the convex hull of the deformed ellipsoid (by taking absolute values of negative eigenvalues) to approximate the Gaussian width?
- Basis in paper: [explicit] The paper proposes convexifying the deformed ellipsoid by replacing negative eigenvalues with their absolute values to approximate the Gaussian width.
- Why unresolved: The paper provides a lemma stating that the Gaussian width of the deformed ellipsoid can be approximated by the convex hull, but does not offer a rigorous proof or explanation for why this approximation is valid.
- What evidence would resolve it: A theoretical proof or a set of experiments demonstrating the accuracy of this approximation across different network architectures and loss functions.

## Limitations
- The theoretical framework assumes the loss landscape can be accurately approximated by an ellipsoid, which may not hold for highly non-convex loss surfaces or during training dynamics
- The relationship between Gaussian width and pruning ratio is derived in the infinite-dimensional limit and may have finite-sample corrections
- Empirical validation is limited to CIFAR-10/100 datasets and relatively small networks

## Confidence

**High**: The core mathematical framework connecting Gaussian width to pruning limits (Gordon's Escape Theorem application)

**Medium**: The practical algorithm implementation and its performance claims relative to baselines

**Medium**: The identification of weight magnitude and loss sharpness as key factors

## Next Checks

1. **Theoretical verification**: Test the Gaussian width-pruning ratio relationship on synthetic convex loss landscapes with known analytical solutions

2. **Algorithm robustness**: Apply the pruning algorithm to more diverse architectures (Transformer-based models, recurrent networks) and datasets (ImageNet, speech data)

3. **Numerical stability**: Compare the improved SLQ spectrum estimation against exact eigenvalue computation on small Hessian matrices to verify accuracy