---
ver: rpa2
title: 'Unsupervised Adversarial Detection without Extra Model: Training Loss Should
  Change'
arxiv_id: '2308.03243'
source_url: https://arxiv.org/abs/2308.03243
tags:
- outputs
- adversarial
- output
- 'false'
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles unsupervised adversarial detection without requiring
  extra models or prior attack knowledge. It observes that common cross-entropy loss
  teaches the model unnecessary features in false outputs, making them vulnerable
  to attacks.
---

# Unsupervised Adversarial Detection without Extra Model: Training Loss Should Change

## Quick Facts
- arXiv ID: 2308.03243
- Source URL: https://arxiv.org/abs/2308.03243
- Reference count: 7
- Primary result: Achieves over 93.9% TPR with only 2.5% FPR against white-box attacks on CIFAR-10 without extra models

## Executive Summary
This paper proposes an unsupervised adversarial detection method that eliminates the need for extra detection models by modifying training loss functions. The key insight is that common cross-entropy loss teaches models unnecessary features in false outputs, making them vulnerable to adversarial attacks. The authors introduce two novel training losses: one that constrains false raw outputs toward uniform distributions using KL divergence, and another that pulls true raw outputs higher using multi-hot KL divergence. Detection is then performed using simple threshold-based checks on raw outputs. The method achieves strong results against various white-box attacks with minimal false positives.

## Method Summary
The method modifies standard training by introducing two custom loss functions. First, it uses KL divergence to constrain false raw outputs toward uniform distributions, removing distinguishing features that attacks can exploit. Second, it employs multi-hot KL divergence for true outputs to adjust learning rates based on class accuracy. During training, 10% adversarial samples (PGD with eps=8, 5 iterations) are injected to prevent the model from developing one-hot output tendencies. Detection is performed by thresholding raw outputs: if the maximum or minimum raw output falls below certain values, the input is flagged as adversarial. No extra detection models or layers are added - detection is purely based on these thresholds.

## Key Results
- Achieves over 93.9% true positive rate (TPR) with only 2.5% false positive rate (FPR) on CIFAR-10
- Effective against all tested white-box attacks (FGSM, BIM, PGD, DeepFool, AutoAttack) except DF(∞)
- Maintains similar classification accuracy (~86%) compared to standard training
- No extra models or detection layers required

## Why This Works (Mechanism)

### Mechanism 1
Common cross-entropy loss encourages the model to learn unnecessary features for false outputs, making them vulnerable to adversarial attacks. By forcing false raw outputs toward a uniform distribution using KL divergence, the model learns to treat all false classes equally, removing distinguishing features that attacks can exploit.

### Mechanism 2
Cross-entropy continues to update already-accurate classes at the same rate as inaccurate classes, leading to suboptimal learning distribution. The multi-hot KL divergence approach adjusts the learning rate for each class based on its accuracy, giving more gradient updates to classes that need improvement.

### Mechanism 3
Models naturally develop a preference for one-hot outputs during standard training, which attackers can exploit. Adding adversarial samples during training removes this tendency by suppressing true outputs and forcing the model to avoid relying on a single dominant output.

## Foundational Learning

- **KL divergence and cross-entropy relationship**: Understanding KL divergence is crucial because the proposed method uses it to constrain false outputs and replace cross-entropy for true outputs. Quick check: What is the mathematical relationship between KL divergence and cross-entropy, and why does this matter for gradient computation?

- **Adversarial attack mechanics and white-box attacks**: The method must understand how attacks work to design defenses that specifically target attackable outputs while protecting non-attackable ones. Quick check: How do white-box attacks compute gradients to modify inputs, and why does this make raw outputs before softmax vulnerable?

- **Multi-hot classification and label representation**: The multi-hot KL divergence approach requires understanding how to represent labels when multiple classes can be "true" in a mini-batch. Quick check: How does multi-hot labeling differ from one-hot labeling, and why is this important for computing gradients when some classes are less accurate?

## Architecture Onboarding

- **Component map**: Standard classification model (e.g., ResNet18) with modified training loss functions → Detection based on raw output thresholds
- **Critical path**: Training with proposed loss → Model learns to protect false outputs → Only true outputs remain attackable → Detection based on raw output thresholds
- **Design tradeoffs**: The method trades some classification accuracy for adversarial robustness, though the paper claims accuracy remains similar (~86%). The key tradeoff is between false positive rate and true positive rate.
- **Failure signatures**: Detection fails when attacks successfully modify false outputs, or when normal inputs occasionally have low maximum raw outputs due to model uncertainty.
- **First 3 experiments**:
  1. Implement the false output uniform distribution constraint using KL divergence and verify that false outputs become resistant to L1 attacks.
  2. Replace cross-entropy with multi-hot KL divergence for true outputs and measure the impact on class accuracy distribution.
  3. Add adversarial samples during training and verify that the one-hot output tendency is reduced.

## Open Questions the Paper Calls Out

### Open Question 1
How can we further improve the robustness of false raw outputs against adversarial attacks, beyond the current KL divergence-based approach? The paper acknowledges that improving the resistance ability of false outputs is still a key research point, and the current method is not perfect.

### Open Question 2
What is the underlying mechanism that causes true raw outputs to be pressed down when training with adversarial samples? The paper states that the authors do not fully understand why this happens, and solving this problem could lead to even lower false positive rates.

### Open Question 3
Can the proposed training losses be effectively applied to other tasks beyond image classification, such as object detection or natural language processing? The paper focuses on image classification with CIFAR-10, but suggests the method may be useful in other scenarios.

## Limitations
- Results may not generalize well to black-box attacks or non-CIFAR datasets
- The claim that false outputs become "attack-resistant" relies heavily on KL divergence constraints without exploring potential bypass strategies
- Limited ablation studies to isolate the specific contribution of each proposed loss component

## Confidence

- **High**: The observation that cross-entropy encourages unnecessary features in false outputs, leading to vulnerability
- **Medium**: The effectiveness of KL divergence in constraining false outputs to uniform distributions
- **Low**: The generalizability of results to other datasets and attack types beyond white-box scenarios

## Next Checks

1. Test the method on CIFAR-100 or ImageNet to verify scalability and performance on more complex datasets
2. Evaluate black-box attack resistance, particularly transfer-based attacks where the adversary doesn't have access to model gradients
3. Conduct ablation studies to isolate the contribution of each proposed loss component (false output constraint vs. true output enhancement)