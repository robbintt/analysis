---
ver: rpa2
title: Large Language Models and Prompt Engineering for Biomedical Query Focused Multi-Document
  Summarisation
arxiv_id: '2311.05169'
source_url: https://arxiv.org/abs/2311.05169
tags:
- context
- bioasq
- snippets
- extract
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates prompt engineering and retrieval-augmented
  generation (RAG) with GPT-3.5 for biomedical query-focused multi-document summarization
  in the BioASQ Challenge. Using manually curated relevant snippets as context, their
  RAG prompts achieve the highest human evaluation scores in 3 of 4 BioASQ 11b test
  batches, ranking second overall.
---

# Large Language Models and Prompt Engineering for Biomedical Query Focused Multi-Document Summarisation

## Quick Facts
- arXiv ID: 2311.05169
- Source URL: https://arxiv.org/abs/2311.05169
- Reference count: 14
- Authors investigated prompt engineering and retrieval-augmented generation (RAG) with GPT-3.5 for biomedical query-focused multi-document summarization in the BioASQ Challenge, achieving the highest human evaluation scores in 3 of 4 BioASQ 11b test batches and ranking second overall.

## Executive Summary
This paper investigates prompt engineering and retrieval-augmented generation (RAG) with GPT-3.5 for biomedical query-focused multi-document summarization in the BioASQ Challenge. Using manually curated relevant snippets as context, their RAG prompts achieve the highest human evaluation scores in 3 of 4 BioASQ 11b test batches, ranking second overall. The results demonstrate the effectiveness of prompt engineering and RAG for biomedical question answering, confirming improvements from few-shot learning and context incorporation.

## Method Summary
The paper uses GPT-3.5 to generate answers to biomedical questions from the BioASQ 11b dataset. They experiment with various prompt strategies: zero-shot, few-shot (with 10 training QA pairs), and RAG (using manually curated relevant snippets as context). The model is evaluated using human evaluation metrics (IR, IP, IRep, Read) and ROUGE-SU4 F1. The prompts are designed to guide the model in generating concise, factual answers while minimizing hallucinations.

## Key Results
- RAG prompts with manually curated snippets achieved the highest human evaluation scores in 3 of 4 BioASQ 11b test batches, ranking second overall.
- Few-shot prompting with 10 training QA pairs improved performance over zero-shot variants.
- The largest improvement was achieved by retrieval-augmented generation, confirming the effectiveness of providing relevant context to GPT-3.5.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation with manually curated biomedical snippets significantly improves answer quality over zero-shot or few-shot without context.
- Mechanism: GPT-3.5 is provided with a list of relevant snippets from PubMed, which acts as a retrieved knowledge base. The model can then synthesize the answer using only the provided context, reducing hallucinations.
- Core assumption: The curated snippets contain the correct answer and are of higher quality than automatically retrieved snippets.
- Evidence anchors:
  - [abstract]: "The largest improvement was achieved by retrieval augmented generation."
  - [section]: "In a Zero-shot setting, we experimented with two variants. In the Snippets variant...we inserted all the snippets as a list."
  - [corpus]: Found 25 related papers; none directly compare RAG vs. no-context in biomedical QA, so this is weakly supported externally.
- Break condition: If the curated snippets are incomplete or do not contain the correct answer, the model cannot hallucinate beyond them.

### Mechanism 2
- Claim: Few-shot prompting with relevant question-answer pairs from the training set improves performance over zero-shot variants.
- Mechanism: By including 10 prior QA pairs as examples in the prompt, the model learns the expected format and style of the ideal answer for the given question type.
- Core assumption: The training samples are representative and the model can generalize the pattern to the new question.
- Evidence anchors:
  - [abstract]: "Prompts that incorporated few-shot samples generally improved on their counterpart zero-shot variants."
  - [section]: "A Few-shot variant includes a short introductory text, plus the last n = 10 question-answer pairs from the BioASQ 11b training data."
  - [corpus]: Weak; corpus shows some related work but no direct evidence of few-shot vs zero-shot in biomedical domain.
- Break condition: If the few-shot samples are not representative or too few, the improvement may not materialize.

### Mechanism 3
- Claim: Using an extractive summarizer's output as context for GPT-3.5 yields better results than using all snippets directly.
- Mechanism: The extractive summarizer (DistilBERT-based) filters the relevant snippets down to the most salient sentences, reducing noise and improving GPT-3.5's focus.
- Core assumption: The extractive summarizer is accurate enough that its output contains the answer while removing irrelevant content.
- Evidence anchors:
  - [abstract]: "Using the output of an extractive summariser does not appear to improve much."
  - [section]: "In the Ex- tract variant...we inserted an extract of the snippets. The extract is the unedited output of the DistilBERT QA system."
  - [corpus]: No direct evidence; this is a novel comparison in the paper.
- Break condition: If the extractive summarizer removes content containing the answer, the final answer quality degrades.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: BioASQ tasks involve answering questions from biomedical literature; RAG grounds the LLM in retrieved evidence to reduce hallucinations.
  - Quick check question: What is the difference between using curated snippets vs automatically retrieved snippets in RAG?
- Concept: Few-shot learning in prompt engineering
  - Why needed here: Demonstrates how including examples in the prompt can guide the model to generate better answers without fine-tuning.
  - Quick check question: How does the number of examples in a few-shot prompt affect output quality?
- Concept: Evaluation metrics for QA (ROUGE-SU4 F1 vs. human evaluation)
  - Why needed here: Human evaluation in BioASQ is high-stakes; understanding the difference between automatic metrics and human scores is critical for interpreting results.
  - Quick check question: Why might ROUGE-SU4 F1 be low even when human evaluation scores are high in BioASQ?

## Architecture Onboarding

- Component map: BioASQ question -> Retrieve snippets -> Generate answer -> Evaluate
- Critical path: Question -> Retrieve snippets -> Generate answer -> Evaluate
- Design tradeoffs:
  - Using all snippets vs. extractive summary: More context vs. less noise
  - Few-shot vs. zero-shot: More guidance vs. simplicity
  - Prompt wording: Small changes may significantly impact output quality
- Failure signatures:
  - Low human scores: Likely due to hallucinations or irrelevant content
  - Low ROUGE: Possible mismatch between golden answers and generated text
  - Inconsistent answers across batches: May indicate sensitivity to prompt wording
- First 3 experiments:
  1. Run zero-shot with no context as baseline
  2. Run zero-shot with all snippets as context
  3. Run few-shot with extractive summary as context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt engineering techniques impact the performance of LLMs in biomedical question answering compared to traditional methods?
- Basis in paper: [explicit] The paper investigates the impact of prompt engineering techniques on LLMs for biomedical question answering.
- Why unresolved: The paper presents results from a specific set of experiments, but does not explore the full range of potential prompt engineering techniques or compare them to all traditional methods.
- What evidence would resolve it: Comprehensive experiments comparing various prompt engineering techniques with traditional methods across different biomedical question answering tasks.

### Open Question 2
- Question: How does the quality of automatically-generated snippets compare to manually-curated snippets in terms of improving LLM performance in biomedical question answering?
- Basis in paper: [inferred] The paper mentions that manually-curated snippets are likely of better quality than automatically-generated ones, but does not directly compare their impact on LLM performance.
- Why unresolved: The paper does not conduct experiments using automatically-generated snippets as context for LLM prompts.
- What evidence would resolve it: Experiments using both manually-curated and automatically-generated snippets as context in LLM prompts, comparing their impact on performance.

### Open Question 3
- Question: What are the optimal parameters for fine-tuning LLMs specifically for biomedical question answering tasks?
- Basis in paper: [inferred] The paper uses GPT-3.5 with specific parameters, but does not explore the impact of different parameter settings on performance.
- Why unresolved: The paper does not experiment with different parameter settings or fine-tuning approaches for LLMs in the context of biomedical question answering.
- What evidence would resolve it: Experiments varying LLM parameters and fine-tuning approaches, measuring their impact on performance in biomedical question answering tasks.

## Limitations
- The paper relies on manually curated snippets, which may not always contain the correct answer or may introduce bias.
- The results may be sensitive to prompt wording and few-shot example selection, as evidenced by inconsistent performance across test batches.
- The lack of comparison to automatically retrieved snippets in the RAG experiments leaves open the possibility that the improvements are due to the quality of the curated snippets rather than the RAG approach itself.

## Confidence

- High: The effectiveness of few-shot prompting over zero-shot prompting is well-supported by the results and aligns with established knowledge in prompt engineering.
- Medium: The superiority of RAG with manually curated snippets over zero-shot or few-shot without context is supported by human evaluation scores but not by the automatic ROUGE metric.
- Low: The claim that using an extractive summarizer's output as context yields better results than using all snippets directly is not well-supported by the results and contradicts the paper's own findings.

## Next Checks

1. Conduct a controlled experiment comparing RAG with manually curated snippets vs. automatically retrieved snippets to isolate the effect of snippet quality on performance.
2. Perform an ablation study on the number and selection of few-shot examples to determine the optimal number and quality of examples for guiding the model.
3. Evaluate the robustness of the prompt engineering strategies across different question types and domains to assess the generalizability of the findings.