---
ver: rpa2
title: On the Interplay of Convolutional Padding and Adversarial Robustness
arxiv_id: '2308.06612'
source_url: https://arxiv.org/abs/2308.06612
tags:
- padding
- adversarial
- attacks
- training
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first in-depth analysis of the impact of
  padding-related architectural design choices on adversarial CNN robustness. The
  authors investigate how different padding modes (zeros, reflect, replicate, circular)
  and kernel sizes affect the robustness of ResNet-20 models trained on CIFAR-10.
---

# On the Interplay of Convolutional Padding and Adversarial Robustness

## Quick Facts
- arXiv ID: 2308.06612
- Source URL: https://arxiv.org/abs/2308.06612
- Reference count: 40
- Primary result: Zero padding does not always yield optimal adversarial robustness; reflect/replicate padding with larger kernels often perform better

## Executive Summary
This paper provides the first in-depth analysis of how padding-related architectural design choices affect the adversarial robustness of convolutional neural networks. Through extensive experiments on ResNet-20 models trained on CIFAR-10, the authors investigate the impact of different padding modes (zero, reflect, replicate, circular) and kernel sizes on both clean and robust accuracy under various adversarial attacks. Their findings challenge the common assumption that zero padding is always optimal, revealing that boundary artifacts introduced by zero padding can make models more vulnerable to adversarial perturbations.

## Method Summary
The authors conduct experiments using ResNet-20 models on CIFAR-10 with configurable padding modes (zero, reflect, replicate, circular) and kernel sizes (3, 5, 7, 9). They train models both with and without adversarial training using FGSM and PGD attacks. Robustness is evaluated using multiple attack methods including APGD-CE, FAB, and Square with varying budgets. The study also investigates padding-free architectures by using up-scaling or out-painting to compensate for the down-scaling effect of non-padded convolutions. Perturbation distributions and LayerCAM explanations are analyzed to understand padding effects on attack locations and model decisions.

## Key Results
- Zero padding does not always result in the best performance, especially when combined with adversarial training
- Circular and reflect padding modes show improved robustness compared to zero padding under certain conditions
- Increasing kernel size can diminish the gap between zero padding and other padding modes in terms of robustness
- Padding-free architectures generally result in worse performance and are not recommended

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero padding introduces artificial discontinuities at image boundaries, causing adversarial perturbations to concentrate at those edges, thereby reducing robustness
- Mechanism: The zero-padding operation creates a sharp contrast between image content and the padded border. During adversarial attacks, gradients flow differently at this discontinuity, leading to higher perturbation magnitudes at the edges
- Core assumption: The model learns to rely on features at the boundary, making it vulnerable to edge-specific perturbations
- Evidence anchors:
  - [abstract] "We show that adversarial attacks often result in perturbation anomalies at the image boundaries, which are the areas where padding is used."
  - [section] "Zero padding introduces artificial discontinuities at image boundaries, causing adversarial perturbations to concentrate at those edges, thereby reducing robustness."
  - [corpus] No direct evidence in corpus; assumption based on analysis
- Break condition: If the model architecture or training method does not rely on boundary features, this mechanism may not apply

### Mechanism 2
- Claim: Circular padding distributes perturbations more evenly across the image, reducing the concentration of adversarial attacks at boundaries
- Mechanism: Circular padding wraps the image around its edges, eliminating the artificial discontinuity introduced by zero padding. This allows adversarial perturbations to spread more uniformly
- Core assumption: The model's reliance on boundary features is reduced with circular padding
- Evidence anchors:
  - [abstract] "We investigate the role of padding and its parameters (like type and size) for the robustness of CNNs."
  - [section] "Circular padding wraps the image around its edges, eliminating the artificial discontinuity introduced by zero padding."
  - [corpus] No direct evidence in corpus; assumption based on analysis
- Break condition: If the model architecture inherently processes the entire image uniformly, circular padding may not provide additional robustness

### Mechanism 3
- Claim: Increasing kernel size improves robustness by allowing the model to capture more global features, reducing reliance on boundary artifacts
- Mechanism: Larger kernels have a broader receptive field, which helps the model focus on more central, relevant features rather than boundary-specific artifacts
- Core assumption: The model's robustness improves with a larger receptive field, reducing sensitivity to edge perturbations
- Evidence anchors:
  - [abstract] "Our empirical evaluations on CIFAR-10 show that the commonly applied same sized zero padding does not always result in the best performance, especially in combination with adversarial training."
  - [section] "We observe that increasing k seems to diminish the gap between zero padding and other modes."
  - [corpus] No direct evidence in corpus; assumption based on analysis
- Break condition: If the dataset or task inherently requires boundary-specific features, increasing kernel size may not improve robustness

## Foundational Learning

- Concept: Adversarial attacks
  - Why needed here: Understanding how adversarial attacks exploit model vulnerabilities is crucial for analyzing the impact of padding on robustness
  - Quick check question: What is the primary goal of an adversarial attack in the context of CNNs?

- Concept: Padding modes
  - Why needed here: Different padding modes (zero, reflect, replicate, circular) affect how the model processes image boundaries, impacting robustness
  - Quick check question: How does zero padding differ from circular padding in terms of boundary handling?

- Concept: Kernel size
  - Why needed here: Kernel size affects the receptive field and feature extraction, influencing the model's robustness to adversarial perturbations
  - Quick check question: How does increasing kernel size affect the model's reliance on boundary features?

## Architecture Onboarding

- Component map: ResNet-20 model with configurable padding modes (zero, reflect, replicate, circular) and kernel sizes (3, 5, 7, 9)
- Critical path: Training the model on CIFAR-10, applying adversarial attacks, and evaluating robustness across different padding modes and kernel sizes
- Design tradeoffs: Zero padding is computationally efficient but may reduce robustness; other padding modes may improve robustness but increase computational overhead
- Failure signatures: Poor robustness to adversarial attacks, especially at image boundaries; overfitting to specific attack types
- First 3 experiments:
  1. Train ResNet-20 with zero padding and kernel size 3 on CIFAR-10; evaluate robustness against APGD-CE attacks
  2. Repeat experiment with reflect padding; compare robustness and clean accuracy
  3. Increase kernel size to 5; assess changes in robustness and clean accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different padding modes affect the spatial distribution of adversarial perturbations beyond the image boundaries?
- Basis in paper: [explicit] The authors observe that perturbations are primarily located at image boundaries and analyze their distribution under different padding modes, noting anomalies at padded areas
- Why unresolved: The paper focuses on boundary effects but doesn't fully explore how padding modes influence perturbation distributions throughout the entire feature-map, especially in deeper layers
- What evidence would resolve it: Comprehensive analysis of perturbation distributions across all spatial locations and feature-map layers for different padding modes, using various attack types and budgets

### Open Question 2
- Question: Do the observed differences in robustness between padding modes persist on datasets with non-centered objects?
- Basis in paper: [inferred] The authors acknowledge their experiments are limited to CIFAR-10, where objects are perfectly centered, and suggest results might differ on less curated datasets
- Why unresolved: The paper doesn't test on datasets with varied object placements or aspect ratios, leaving uncertainty about the generalizability of padding mode effects
- What evidence would resolve it: Replicating the experiments on datasets like COCO or OpenImages, which contain non-centered and varied object placements, would clarify if padding mode effects are dataset-dependent

### Open Question 3
- Question: How does padding mode interact with network architecture to influence adversarial robustness?
- Basis in paper: [inferred] The authors use ResNet-20 and note their results might not scale to entirely different architectures, suggesting potential interactions between padding and architectural design choices
- Why unresolved: The study is limited to one architecture type, and different architectures (e.g., transformers, attention-based models) might exhibit different sensitivities to padding modes
- What evidence would resolve it: Testing various architectures (CNNs, transformers, hybrid models) with different padding modes and analyzing robustness patterns would reveal architectural dependencies

## Limitations
- Empirical study limited to ResNet-20 on CIFAR-10, may not generalize to deeper architectures or larger-scale datasets
- LayerCAM-based perturbation analysis relies on qualitative interpretation rather than quantitative metrics for boundary effect assessment
- Outpainting approach uses specific MAT implementation without exploring alternative padding-free methods

## Confidence

- **High confidence**: Claims about zero padding not being optimal for adversarial training robustness (supported by extensive experimental evidence across multiple attack types and budgets)
- **Medium confidence**: Claims about circular padding distributing perturbations more evenly (based on qualitative LayerCAM analysis and perturbation distribution observations)
- **Medium confidence**: Claims about increasing kernel size improving robustness (supported by experimental trends but not conclusively proven as a universal recommendation)

## Next Checks

1. Test the padding mode recommendations on larger architectures (e.g., ResNet-50) and datasets (e.g., ImageNet) to verify scalability of findings

2. Conduct quantitative analysis of gradient flow at boundaries across different padding modes using gradient-based attribution methods to validate the discontinuity hypothesis

3. Implement alternative padding-free approaches (such as learnable boundary conditions or adaptive resizing) to compare with the outpainting method and establish whether padding removal itself or just the specific implementation is problematic