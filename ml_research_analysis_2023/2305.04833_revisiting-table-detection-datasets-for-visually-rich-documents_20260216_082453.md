---
ver: rpa2
title: Revisiting Table Detection Datasets for Visually Rich Documents
arxiv_id: '2305.04833'
source_url: https://arxiv.org/abs/2305.04833
tags:
- table
- dataset
- datasets
- detection
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Open-Tables and ICT-TD, two high-quality datasets
  for table detection in visually rich documents. Open-Tables is created by revisiting
  and cleaning popular open datasets (ICDAR2013, 2017, 2019, Marmot, TNCR) to address
  issues of noise, inconsistent annotations, and limited data sources.
---

# Revisiting Table Detection Datasets for Visually Rich Documents

## Quick Facts
- arXiv ID: 2305.04833
- Source URL: https://arxiv.org/abs/2305.04833
- Reference count: 40
- Primary result: Introduced Open-Tables and ICT-TD datasets with improved annotation quality and cross-domain evaluation, achieving 0.6%-2.6% higher weighted average F1-score than noisy versions.

## Executive Summary
This paper addresses the critical issues of noise and inconsistent annotations in existing table detection datasets for visually rich documents. The authors propose Open-Tables, created by cleaning and merging popular datasets (ICDAR2013, 2017, 2019, Marmot, TNCR), and ICT-TD, a new manually annotated dataset from the ICT domain. They establish strong baseline results using state-of-the-art object detection models and demonstrate that their cleaned datasets provide more reliable evaluation metrics, with cross-domain experiments showing significant performance improvements over noisy versions.

## Method Summary
The authors created Open-Tables by revisiting and cleaning popular open datasets, aligning labeling definitions, removing noisy samples, and merging them into a larger dataset. They also introduced ICT-TD, a new manually annotated dataset using PDF datasheets from the ICT domain. Four state-of-the-art object detection models (TableDet, DiffusionDet, Deformable-DETR, SparseR-CNN) with ResNet50 backbone were trained and evaluated on these datasets. The evaluation used precision, recall, and F1-score metrics at IoU thresholds ranging from 80% to 95%, with cross-domain experiments to test generalization.

## Key Results
- Open-Tables and ICT-TD datasets provide more reliable evaluation due to high quality and consistent annotations
- Models trained on cleaned Open-Tables achieved 0.6%-2.6% higher weighted average F1-score in cross-domain settings compared to noisy versions
- Baseline models achieved strong performance on both datasets, establishing benchmarks for future research

## Why This Works (Mechanism)

### Mechanism 1
High-quality annotation alignment and noise cleaning directly improve cross-domain generalization. By merging datasets with aligned annotations and removing noisy samples, the resulting Open-Tables dataset reduces false positives and negatives during evaluation, making model performance metrics more reliable. This cleaner signal allows models to learn more robust table detection patterns that transfer better to unseen domains. Core assumption: Noise in training and test sets degrades both learning and evaluation, and consistent annotation definitions across datasets prevent confusion during model training. Evidence: The authors demonstrate that benchmark models trained with cleaned Open-Tables achieve 0.6%-2.6% higher weighted average F1-score than those trained with noisy versions. Break condition: If the noise removal process accidentally discards valid table annotations or if annotation alignment introduces bias toward certain