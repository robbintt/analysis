---
ver: rpa2
title: 'Ambient Adventures: Teaching ChatGPT on Developing Complex Stories'
arxiv_id: '2308.01734'
source_url: https://arxiv.org/abs/2308.01734
tags:
- agent
- imaginary
- story
- play
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a pipeline to create imaginary play for an
  agent using large language models (LLMs) such as ChatGPT. The agent is provided
  with a real-world layout, and the LLM generates imaginary stories that can guide
  the agent to interact with the environment.
---

# Ambient Adventures: Teaching ChatGPT on Developing Complex Stories
## Quick Facts
- arXiv ID: 2308.01734
- Source URL: https://arxiv.org/abs/2308.01734
- Reference count: 3
- One-line primary result: LLM-generated stories can guide agent imaginary play but face limitations in prompting formats and understanding interactive actions

## Executive Summary
This paper proposes a pipeline using ChatGPT to generate imaginary stories that guide an agent's interactions with a real-world environment modeled as a text adventure game. The approach involves prompting the LLM with real-world objects and training samples to produce stories that can be simplified into executable action sequences. The system was evaluated in a simulated house environment where the agent attempts to follow these generated stories to complete imaginary play scenarios.

## Method Summary
The method involves using ChatGPT to generate imaginary stories based on real-world objects and human prompts, then simplifying these stories into action sequences that can be mapped to admissible actions in a text adventure game. The agent executes these sequences in a TextWorld simulation, receiving rewards for correct state changes. Iterative prompting and feedback loops are used to improve story quality and action sequence validity when the agent fails to complete tasks.

## Key Results
- LLM-generated stories can successfully guide agents through imaginary play scenarios in controlled environments
- Limitations exist in prompting formats and the LLM's understanding of interactive actions in text games
- Iterative prompting can help address some of these limitations by refining story generation

## Why This Works (Mechanism)
### Mechanism 1
ChatGPT can generate coherent imaginary stories that map real-world objects to imaginary counterparts and provide executable action sequences for an agent. By prompting ChatGPT with a topic, real-world objects, and training samples of concise stories, the model produces narratives where each sentence introduces at most one new imaginary object. These stories are then distilled into phrases linking imaginary objects to actions, and finally mapped back to admissible real-world actions.

### Mechanism 2
Text adventure games can serve as a controllable testbed to validate whether the story-derived action sequences lead to successful imaginary play. A simulated house environment is built where objects have defined admissible actions and states. The agent executes the mapped action sequence, receiving rewards for correct state changes. Success is measured by completing the sequence and reaching a predefined win state.

### Mechanism 3
Iterative prompting and feedback loops improve story quality and action sequence validity by correcting for LLM limitations. After initial story generation, results are analyzed. If the agent fails, prompts are revised to add directional information or constraints, and ChatGPT regenerates the story. This cycle continues until the action sequence leads to success.

## Foundational Learning
- **Large Language Model Prompt Engineering**: Why needed here - Effective story generation depends on crafting prompts that constrain output format (e.g., one object per sentence) and guide logical flow. Quick check question: What prompt constraint ensures each story sentence introduces only one new imaginary object?
- **Text-Based Game State Management**: Why needed here - The agent's ability to execute actions relies on accurate modeling of object states and admissible actions within the game environment. Quick check question: How does the game distinguish between stand-alone, interactive, and win actions in terms of reward structure?
- **Iterative Feedback in Machine Learning Pipelines**: Why needed here - Story generation and action mapping require multiple refinement cycles to overcome LLM limitations and ensure task completion. Quick check question: What feedback signal is used to determine whether a new story iteration is needed?

## Architecture Onboarding
- **Component map**: Real-world layout scanner -> Object-action mapper -> ChatGPT story generator -> Story distiller -> Action mapper -> TextWorld game engine -> RL agent -> Reward evaluator -> Feedback loop to ChatGPT
- **Critical path**: Real-world layout -> Story generation -> Action mapping -> Game execution -> Success/failure evaluation
- **Design tradeoffs**: Using ChatGPT offers flexibility but introduces unpredictability; rule-based generators would be more reliable but less creative. TextWorld provides a safe simulation but may not capture all real-world nuances.
- **Failure signatures**: Agent stalls due to unmapped actions. Stories become repetitive or illogical after iterations. Feedback loop fails to converge on a working action sequence.
- **First 3 experiments**: 1) Test story generation with a fixed prompt and evaluate coherence and object-per-sentence compliance. 2) Validate action mapping accuracy by comparing mapped actions to admissible actions in the game. 3) Run end-to-end trials with simple stories to measure agent success rate and identify common failure points.

## Open Questions the Paper Calls Out
- **Open Question 1**: How does the performance of the agent vary with different prompt formats when generating imaginary play stories? The paper mentions that they kept crafting prompts to direct the model to create coherent and executable stories, but does not provide a detailed analysis of how different prompt formats affect the agent's performance.

- **Open Question 2**: How does the agent handle interactive actions that require multiple steps or objects in the text game? The paper mentions that the generated story cannot associate objects picked from a previous room with those in the current room, leading to actions not allowed within the text game.

- **Open Question 3**: How does the agent's performance change when the setting of the imaginary play is modified? The paper mentions that if the setting in imaginary play is modified, the model needs new prompts for the changes.

## Limitations
- Lack of specific details about prompt engineering formats, making replication challenging
- Reliance on human-in-the-loop adjustments in the iterative feedback mechanism without quantitative measurement of effectiveness
- TextWorld testbed may not fully capture real-world object interaction complexity

## Confidence
- **High Confidence**: The core mechanism of using ChatGPT to generate stories and translate them into action sequences is technically feasible and demonstrated in controlled experiments
- **Medium Confidence**: The effectiveness of iterative prompting for improving story quality is supported by the paper's claims but lacks rigorous quantitative validation
- **Low Confidence**: The generalizability of this approach to diverse real-world environments and more complex imaginary scenarios remains unproven due to limited experimental scope

## Next Checks
1. Conduct systematic ablation studies varying prompt constraints to identify which specific formatting rules most significantly impact story coherence and actionability
2. Implement automated evaluation metrics for story quality beyond task completion, including measuring logical consistency, creativity, and adherence to the one-object-per-sentence constraint
3. Test the pipeline across multiple diverse environments (beyond house layouts) and with different LLM models to assess robustness and generalizability of the approach