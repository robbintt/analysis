---
ver: rpa2
title: 'SlAction: Non-intrusive, Lightweight Obstructive Sleep Apnea Detection using
  Infrared Video'
arxiv_id: '2309.02713'
source_url: https://arxiv.org/abs/2309.02713
tags:
- sleep
- apnea
- video
- obstructive
- events
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SlAction, a non-intrusive system for detecting
  obstructive sleep apnea (OSA) using infrared videos captured during sleep. The key
  insight is that respiratory arousal (RA) events, which occur with apnea/hypopnea
  events, exhibit more noticeable motions compared to other sleep events and have
  strong correlations with OSA severity.
---

# SlAction: Non-intrusive, Lightweight Obstructive Sleep Apnea Detection using Infrared Video

## Quick Facts
- arXiv ID: 2309.02713
- Source URL: https://arxiv.org/abs/2309.02713
- Reference count: 40
- Primary result: 87.6% average F1 score for OSA detection using infrared video with real-time inference on edge devices

## Executive Summary
SlAction presents a novel non-intrusive system for detecting obstructive sleep apnea (OSA) using infrared videos captured during sleep. The system leverages the observation that respiratory arousal (RA) events, which occur with apnea/hypopnea events, produce more noticeable body movements than other sleep events, making them detectable through video motion analysis. By using a lightweight deep neural network (MoViNet-A0) on frame-differenced video clips, SlAction achieves real-time inference on resource-constrained devices like NVIDIA Jetson Nano while maintaining high accuracy (87.6% F1 score) across 5,098 hours of sleep video data from three hospitals.

## Method Summary
SlAction processes infrared sleep videos at 2.5 FPS, converting them into frame difference images to isolate motion. The system uses 60-second clips with 30-second sliding windows as input to a lightweight MoViNet-A0 model trained to detect RA events. The model achieves binary classification between RA and non-RA events, with RA counts aggregated over time to estimate the apnea-hypopnea index (AHI). OSA is predicted when AHI exceeds 15. The approach includes face mosaicing for privacy preservation and is designed for on-device inference on edge hardware.

## Key Results
- 87.6% average F1 score for OSA detection across three hospitals
- Real-time inference (~3 seconds per 60-second clip) on NVIDIA Jetson Nano
- Spearman correlation coefficient of 0.827 between predicted and ground-truth AHI
- Model size of 5.1 MB optimized for edge deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Respiratory arousal (RA) events are more distinguishable in video motion data than apnea events directly.
- Mechanism: RA events follow apnea/hypopnea and produce more pronounced body movements, making them easier to detect via video motion analysis.
- Core assumption: The visual motion signatures of RA events are sufficiently distinct from other sleep events to enable reliable detection.
- Evidence anchors: RA events exhibit more noticeable motions compared to other sleep events and have strong correlations with OSA severity.
- Break condition: If RA events do not consistently produce detectable motion differences, or if other arousals (e.g., spontaneous arousal) have overlapping motion signatures.

### Mechanism 2
- Claim: Using frame differences as preprocessing effectively isolates motion from static video noise.
- Mechanism: By computing pixel-wise absolute differences between consecutive frames, the system filters out background noise and focuses only on motion-related changes.
- Core assumption: Sleep video motion is sparse enough that frame differences will predominantly highlight relevant RA-related motion.
- Evidence anchors: The Input Data Preprocessor refines the original infrared video footage into an optimized input clip tailored to lightweight RA detection.
- Break condition: If noise or non-motion artifacts (e.g., bedding shifts) produce false positives in the frame difference output.

### Mechanism 3
- Claim: A lightweight DNN can perform real-time RA detection on resource-constrained devices.
- Mechanism: The MoViNet-A0 architecture with (2+1)D convolutions provides efficient video understanding while keeping model size and compute requirements low.
- Core assumption: The computational budget of devices like NVIDIA Jetson Nano is sufficient for real-time inference on the proposed input size and frame rate.
- Evidence anchors: The model achieves real-time inference (~3 seconds per 60-second clip) on resource-constrained devices like NVIDIA Jetson Nano.
- Break condition: If model size or inference time exceeds device capabilities, or if accuracy degrades under real-time constraints.

## Foundational Learning

- Concept: Understanding of OSA clinical metrics (AHI, RA, apnea/hypopnea).
  - Why needed here: The system's logic and thresholds are built around AHI and RA event correlations.
  - Quick check question: What AHI threshold is used to classify OSA in this system? (Answer: 15)

- Concept: Frame differencing for motion isolation in video.
  - Why needed here: Preprocessing step that converts nearly static sleep video into motion-focused input.
  - Quick check question: What is the primary purpose of applying frame differencing to sleep video frames? (Answer: To isolate motion by removing static background)

- Concept: Sliding window analysis for temporal pattern capture.
  - Why needed here: Ensures RA events are detected within context of preceding apnea events.
  - Quick check question: What are the clip size and step size used for sliding window analysis? (Answer: 60-second clips with 30-second steps)

## Architecture Onboarding

- Component map: Raw video -> Input Data Preprocessor -> RA Detector -> AHI Estimator -> OSA Classification
- Critical path: Frame capture and preprocessing (~1.5 ms/frame) -> RA Detector inference (~3.04 s/clip) -> Total ~3.264 s per 60-second clip
- Design tradeoffs: Lower frame rate (2.5 FPS) reduces data volume but may miss very short RA events; larger clip size (60 s) captures context but increases inference latency; model size (5.1 MB) optimized for edge deployment at potential cost of some accuracy
- Failure signatures: High false positives (non-RA motions triggering RA detection); high false negatives (RA events with insufficient motion not detected); performance drop (environmental changes affecting frame differencing)
- First 3 experiments: 1) Validate frame differencing output on sample clips with known RA vs non-RA events; 2) Benchmark RA Detector inference time and accuracy on Jetson Nano with validation dataset; 3) Test AHI estimation accuracy by comparing predicted vs ground-truth AHI on held-out patients

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the correlation between respiratory arousal (RA) events and obstructive sleep apnea (OSA) severity be established as robust and consistent across different populations?
- Basis in paper: The paper states that RA events exhibit strong linear correlations with apnea/hypopnea events and are used as a proxy for OSA detection, but the dataset used may not be fully representative of diverse populations.
- Why unresolved: The study uses a specific dataset from three hospitals, which may not capture the full variability in OSA severity across different demographics, age groups, or geographic locations.
- What evidence would resolve it: Conducting a larger, more diverse study across multiple geographic regions and demographics to validate the correlation between RA events and OSA severity.

### Open Question 2
- Question: How does the performance of SlAction compare to other non-intrusive OSA detection methods in terms of accuracy and reliability?
- Basis in paper: The paper does not provide a comparative analysis with other non-intrusive OSA detection methods, focusing instead on its own system's performance.
- Why unresolved: Without comparative studies, it is unclear how SlAction's performance measures up against existing non-intrusive methods, which could impact its adoption and perceived reliability.
- What evidence would resolve it: Comparative studies that evaluate SlAction against other non-intrusive OSA detection methods in terms of accuracy, reliability, and practical deployment.

### Open Question 3
- Question: Can SlAction's model be adapted to detect other sleep disorders or health conditions using infrared video data?
- Basis in paper: The paper focuses on OSA detection but does not explore the potential for adapting the model to detect other sleep disorders or health conditions.
- Why unresolved: The current model is specifically designed for OSA detection, and its applicability to other conditions is not explored, limiting its broader utility in sleep medicine.
- What evidence would resolve it: Research and development efforts to adapt the model for detecting other sleep disorders or health conditions, followed by validation studies to assess its effectiveness in these new applications.

## Limitations
- Dataset limited to three hospitals, raising concerns about generalizability to diverse populations and environments
- Reliance on RA events as proxies for apnea/hypopnea events introduces potential uncertainty in OSA severity correlation
- Evaluation focuses on average performance metrics without reporting variance or individual patient-level results

## Confidence

- **High Confidence**: The core methodology of using frame differencing for motion isolation and lightweight MoViNet-A0 for real-time inference is well-established and technically sound. The reported inference speed on Jetson Nano is verifiable through the described computational approach.
- **Medium Confidence**: The correlation between RA events and OSA severity (AHI ≥ 15) is supported by clinical literature, but the specific threshold and relationship may vary across patient populations. The 87.6% F1 score is impressive but based on a single test set from one hospital.
- **Low Confidence**: The generalizability of the system to real-world deployment scenarios remains uncertain, particularly regarding variations in sleeping environments, camera positions, and patient demographics not represented in the current dataset.

## Next Checks

1. **Cross-Hospital Validation**: Test the trained model on the held-out Hospital C dataset (which showed 65% accuracy) with detailed analysis of false positive and false negative patterns to identify environmental or demographic factors affecting performance.

2. **Clinical Correlation Study**: Conduct a prospective study comparing SlAction predictions with concurrent gold-standard PSG measurements across a broader patient population, including those with mild OSA (AHI 5-15) to validate the AHI ≥ 15 threshold.

3. **Longitudinal Performance Assessment**: Evaluate the system's performance over extended periods (multiple nights per patient) to assess consistency and identify any temporal patterns in detection accuracy that could inform adaptive thresholding or model updating strategies.