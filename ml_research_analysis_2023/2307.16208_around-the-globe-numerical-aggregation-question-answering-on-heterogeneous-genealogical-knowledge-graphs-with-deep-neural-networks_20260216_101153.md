---
ver: rpa2
title: 'Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous
  Genealogical Knowledge Graphs with Deep Neural Networks'
arxiv_id: '2307.16208'
source_url: https://arxiv.org/abs/2307.16208
tags:
- questions
- question
- table
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GLOBE, a deep neural network-based methodology
  for numerical aggregation question-answering (QA) in the genealogical domain. The
  key challenge addressed is answering natural language questions that require mathematical
  computations (e.g., count, average, portion) over genealogical data stored as GEDCOM
  knowledge graphs.
---

# Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks

## Quick Facts
- arXiv ID: 2307.16208
- Source URL: https://arxiv.org/abs/2307.16208
- Reference count: 40
- Primary result: GLOBE achieves 87% accuracy on genealogical numerical aggregation QA, outperforming state-of-the-art methods by 66 percentage points.

## Executive Summary
This paper introduces GLOBE, a deep neural network-based methodology for numerical aggregation question-answering in the genealogical domain. The key challenge addressed is answering natural language questions requiring mathematical computations (count, average, portion) over genealogical data stored as GEDCOM knowledge graphs. GLOBE uses a two-stage pipeline: automatic table generation from genealogical graphs, followed by a modified BERT-based model adapted for genealogical numerical aggregation QA. Experiments on 1.8M individuals from 3,139 genealogical trees show GLOBE significantly outperforms existing methods, achieving 87% accuracy compared to 21% for state-of-the-art models.

## Method Summary
GLOBE transforms genealogical GEDCOM files into relational database tables using a 6NF design to fit BERT's input constraints. The pipeline uses SBERT for table selection based on question similarity, then applies a modified TaPas model with a custom "portion" aggregation function. The model is trained using weak supervision (only final answers) on automatically generated question-answer pairs. The 6NF design decomposes wide genealogical tables into narrower tables, allowing more rows to fit within BERT's 512-token limit while preserving semantic relationships through dynamic table joining.

## Key Results
- GLOBE achieves 87% accuracy on genealogical numerical aggregation QA tasks
- Outperforms state-of-the-art models by 66 percentage points (21% vs 87%)
- Successfully handles new "portion" aggregation function tailored to genealogical research needs
- Effectively processes heterogeneous genealogical data across 3,139 trees with 1.8M individuals

## Why This Works (Mechanism)

### Mechanism 1: 6NF Table Decomposition
- Claim: GLOBE improves accuracy by transforming genealogical graphs into optimized table structures (6NF) that fit BERT's input constraints.
- Mechanism: The genealogical knowledge graph (GEDCOM) is converted into a relational database using six alternative designs. The 6NF design decomposes each non-primary key attribute into separate tables, reducing row width and allowing more rows to fit within BERT's 512-token limit.
- Core assumption: Genealogical data's multi-level relationships create wide tables that exceed model input limits; decomposing these into narrower tables preserves semantic relationships while fitting BERT's constraints.
- Evidence anchors: [section] "Finally, to further reduce the data size of the DNN input, a 6NF-based [15] (sixth degree of normalization - GenAgg6NF) design was proposed where each non-primary key column in the 'aggregative with events' design (Figures 9 and 10) will be moved into a separate table (Figure 11), thus creating a large number of tables with a small number (2-3) of columns."

### Mechanism 2: SBERT Table Selection
- Claim: GLOBE's table selection using SBERT dramatically reduces the search space while maintaining high precision.
- Mechanism: SBERT is fine-tuned on question-table pairs to compute textual similarity scores. Only the top K most similar tables are selected and joined dynamically.
- Core assumption: Genealogical questions have semantic similarity to specific table structures, and SBERT can learn this mapping effectively from automatically generated question-table pairs.
- Evidence anchors: [section] "To this end, the SBERT model was fine-tuned on a subset of the above created datasets of questions and their source tables [58] to calculate the textual similarity between the given question and each of the tables' content in the dataset."

### Mechanism 3: Custom Portion Function
- Claim: GLOBE extends TaPas with a custom "portion" aggregation function that addresses genealogical research needs.
- Mechanism: The portion function calculates (count of matching rows / count of population rows) by executing two forward passes: first to count the population, then to count the subset.
- Core assumption: Genealogical research frequently requires calculating proportions within populations, which existing aggregation functions cannot express directly.
- Evidence anchors: [abstract] "The approach also introduces a new 'portion' aggregation function tailored to genealogical research needs."

## Foundational Learning

- Concept: Genealogical data modeling (GEDCOM, knowledge graphs, relational database normalization)
  - Why needed here: The entire pipeline depends on correctly representing genealogical relationships in table structures that BERT can process
  - Quick check question: What are the key differences between GenAgg1t, GenAggraw, GenAggrel, GenAggagg, GenAggevent, and GenAgg6NF designs, and when would each be appropriate?

- Concept: Transformer architecture and input constraints (BERT's 512-token limit)
  - Why needed here: Understanding why table decomposition is necessary and how the model processes table-question pairs
  - Quick check question: How does BERT's position embeddings work, and why does the 512-token limit affect table-based QA differently than text-based QA?

- Concept: Weak supervision learning and loss function design
  - Why needed here: GLOBE is trained only on final answers without explicit labels for aggregation functions or target cells
  - Quick check question: How does weak supervision differ from fully supervised learning, and what challenges does it introduce for numerical aggregation QA?

## Architecture Onboarding

- Component map: GEDCOM files → CIDOC-CRM knowledge graph → Relational database (6NF) → SBERT similarity scoring → Top K table selection → Dynamic table joining → BERT encoder → Cell selection layer + Aggregation function layer → Numerical answer

- Critical path: Question → SBERT table selection → Dynamic table joining → BERT-based QA model → Answer prediction
  The bottleneck is SBERT's similarity computation across all tables; this must be optimized for real-time performance.

- Design tradeoffs:
  - 6NF decomposition vs. join complexity: More tables mean more joins but smaller inputs
  - SBERT fine-tuning vs. generalization: Domain-specific SBERT may perform better but lose general language understanding
  - Weak supervision vs. accuracy: No explicit labels for aggregation functions may limit precision

- Failure signatures:
  - Low accuracy on portion questions: Check if population descriptor extraction is working correctly
  - Many unanswerable questions: Verify table selection is finding relevant tables and dynamic joining isn't creating tables exceeding 512 tokens
  - Low soft accuracy: Check if the model is making systematic errors on specific aggregation functions

- First 3 experiments:
  1. Compare GLOBE6NF accuracy against baseline TaPas on GenAgg1t with identical question sets to verify the 6NF design's contribution
  2. Test SBERT table selection accuracy by feeding it only correct tables and measuring if QA accuracy improves (to isolate table selection from QA model performance)
  3. Evaluate portion function accuracy by creating synthetic questions with known ground truth proportions and measuring prediction error

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GLOBE change when using different numbers of relational degrees (e.g., 1st, 2nd, 3rd degree relatives) in the family tree scope?
- Basis in paper: Explicit. The paper mentions that users can reduce the family tree scope by selecting relational degrees using the Gen-BFS algorithm, but does not report results on how this affects model performance.
- Why unresolved: The paper only mentions this as a potential solution when tables exceed the 512-token limit, but does not experimentally evaluate the impact of different scope sizes on accuracy.
- What evidence would resolve it: Systematic experiments varying the relational degree limit (1st, 2nd, 3rd, etc.) and measuring the corresponding accuracy and unanswerable question rates.

### Open Question 2
- Question: Would combining GLOBE with a graph neural network (GNN) approach, as suggested in the future work section, improve performance on genealogical QA tasks?
- Basis in paper: Explicit. The paper lists this as future research direction (point 4 in the discussion section) but does not provide experimental validation.
- Why unresolved: The paper only mentions GNNs as a potential alternative to the current table-based approach without testing this hypothesis.
- What evidence would resolve it: Comparative experiments implementing a GNN-based approach for genealogical QA and measuring accuracy against the current GLOBE model.

### Open Question 3
- Question: How robust is GLOBE to grammatical errors and paraphrasing variations in real-world user questions?
- Basis in paper: Explicit. The paper reports that 46.3% of automatically paraphrased questions were grammatically correct and meaning-preserving, and shows GLOBE can handle some grammatical errors, but doesn't fully characterize its robustness.
- Why unresolved: The paper only evaluates on a validation set of "corrected" questions and doesn't systematically test performance on noisy, real-world user input with various types of errors.
- What evidence would resolve it: Experiments testing GLOBE on intentionally corrupted questions with different types of grammatical errors, paraphrasing variations, and noise levels to establish error tolerance boundaries.

## Limitations

- The question generation methodology is insufficiently specified - only partial pattern templates and conditions are provided
- The portion function's practical utility and computational overhead are mentioned but not rigorously evaluated
- SBERT hyperparameter values for table selection are not fully specified, limiting reproduction

## Confidence

**High Confidence**: The core pipeline architecture (GEDCOM → 6NF tables → SBERT selection → TaPas-based QA) is well-specified and the performance improvement over baselines is clearly demonstrated with statistical evidence.

**Medium Confidence**: The effectiveness of the 6NF design in addressing BERT's input constraints is theoretically sound but lacks direct empirical validation. The SBERT table selection mechanism is well-described but the paper doesn't provide sufficient detail for complete reproduction.

**Low Confidence**: The question generation methodology is insufficiently specified - only partial pattern templates and conditions are provided. The portion function's practical utility and computational overhead are mentioned but not rigorously evaluated.

## Next Checks

1. **6NF Design Validation**: Create a controlled experiment comparing GLOBE6NF against a baseline using GenAgg1t (single table design) on identical question sets. Measure both accuracy and the proportion of questions that exceed BERT's 512-token limit in each design to directly validate whether 6NF decomposition actually solves the input constraint problem.

2. **Table Selection Isolation Test**: Evaluate SBERT table selection accuracy by conducting an ablation study where only relevant tables are provided to the QA model. Compare accuracy when SBERT correctly selects tables versus when all tables are provided, to isolate whether accuracy gains come from better table selection or better QA model performance.

3. **Portion Function Error Analysis**: Create a synthetic validation set with known ground truth proportions (e.g., "What portion of individuals born in 1850 lived past age 50?") and measure prediction error rates specifically for portion questions. This would validate whether the two-pass approach reliably identifies population descriptors and computes accurate proportions.