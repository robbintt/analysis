---
ver: rpa2
title: 'Causal Parrots: Large Language Models May Talk Causality But Are Not Causal'
arxiv_id: '2308.13067'
source_url: https://arxiv.org/abs/2308.13067
tags:
- causal
- data
- causes
- llms
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) can sometimes correctly answer causal
  questions, but the paper argues they do so without genuine causal understanding.
  Instead, they exploit correlations between causal facts present in their training
  data, making them "causal parrots" that recite knowledge without true reasoning.
---

# Causal Parrots: Large Language Models May Talk Causality But Are Not Causal

## Quick Facts
- arXiv ID: 2308.13067
- Source URL: https://arxiv.org/abs/2308.13067
- Reference count: 40
- Key outcome: Large language models can sometimes correctly answer causal questions, but do so without genuine causal understanding, instead exploiting correlations between causal facts in training data.

## Executive Summary
Large language models (LLMs) can correctly answer some causal questions, but the paper argues they do so without genuine causal understanding. Instead, they exploit correlations between causal facts present in their training data, making them "causal parrots" that recite knowledge without true reasoning. The authors formalize this through "meta SCMs" that encode causal facts about other SCMs, and propose the "correlation of causal facts" (CCF) conjecture: LLMs answer causal queries correctly when training data contains correlated causal facts matching the query.

## Method Summary
The paper evaluates LLM performance on causal inference tasks including common sense reasoning, causal discovery with known ground truth graphs, and knowledge base fact embeddings. Experiments use GPT-3, Luminous, and OPT models, testing causal chains, intuitive physics questions, and graph structure recovery. The CCF conjecture is tested by examining whether LLM performance correlates with the presence of causal facts in training data.

## Key Results
- GPT-3 performs best on ground truth causal graphs but struggles with intuitive physics and reasoning
- Chain-of-thought prompting improves performance but may induce biases
- Models often answer correctly when causal knowledge is present in the data, supporting the CCF conjecture
- LLMs underperform frequently, suggesting they are weak "causal parrots" rather than true causal reasoners

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs answer causal queries correctly when training data contains correlated causal facts matching the query.
- Mechanism: The "Correlation of Causal Facts" (CCF) conjecture posits that when an LLM's training data includes meta-SCM representations that encode causal relationships, the model learns to map query patterns to correct causal answers through correlation rather than genuine causal reasoning.
- Core assumption: Training data contains sufficient correlated causal facts that match the query patterns the model encounters.
- Evidence anchors:
  - [abstract] "Empirical evaluations test LLM performance on... causal discovery with known ground truth graphs" showing mixed performance supporting CCF conjecture.
  - [section 4.2] "We propose a naïve causal discovery approach for LLMs" demonstrating LLMs can recover causal graphs when queried with causal facts from training data.
  - [corpus] "Causality for Large Language Models" (arXiv:2410.15319) - weak correlation evidence, only 5 related papers found.
- Break condition: If training data lacks correlated causal facts matching query patterns, LLMs will fail to provide correct causal answers despite correct reasoning being possible.

### Mechanism 2
- Claim: LLMs exploit "meta-SCM" structures in training data that encode causal facts about other SCMs.
- Mechanism: Meta-SCMs represent causal knowledge as observational data (L1 distributions) that can answer interventional (L2) queries about other SCMs. LLMs learn these representations during training.
- Core assumption: Training data contains sufficient meta-SCM representations encoding causal facts.
- Evidence anchors:
  - [section 3] "Definition 3. Let M1 and M2 be two SCMs such that the observational distribution of M2 denoted L1(M2) can answer queries w.r.t. the interventional distributions of M1 denoted L2(M1), then M2 is called meta (w.r.t. M1)."
  - [section 4.3] "We embed causal and anti-causal facts of the ConceptNet data set to gain a set of 'labeled' causal embeddings" showing knowledge base embeddings can transfer causal knowledge.
  - [corpus] "A Causal View of Entity Bias in (Large) Language Models" (arXiv:2305.14695) - weak correlation evidence, only 5 related papers found.
- Break condition: If training data lacks meta-SCM representations encoding causal facts, LLMs cannot exploit these structures to answer causal queries correctly.

### Mechanism 3
- Claim: Chain-of-thought prompting improves LLM causal reasoning by providing exemplars that bias the model toward correct answer patterns.
- Mechanism: CoT provides the model with expected answer formats and reasoning patterns, which the model then replicates rather than genuinely reasoning through the problem.
- Core assumption: Providing exemplars biases the model toward correct answer patterns rather than genuine reasoning.
- Evidence anchors:
  - [section 4.1] "Table 1 shows strong improvements for all models on both data sets with CoT querying" demonstrating CoT effectiveness.
  - [section 4.1] "Luminous and OPT tend to only answer 'Yes' or 'No' for specific CoT setups" showing CoT induces strong bias.
  - [corpus] "Unveiling and Causalizing CoT: A Causal Pespective" (arXiv:2502.18239) - weak correlation evidence, only 5 related papers found.
- Break condition: If CoT exemplars don't match the query patterns or the model doesn't replicate the provided patterns, performance improvements won't materialize.

## Foundational Learning

- Concept: Structural Causal Models (SCMs) and Pearl's Causal Hierarchy
  - Why needed here: Understanding SCMs and the causal hierarchy is fundamental to grasping why LLMs cannot be truly causal - they operate on observational data (L1) but causal inference requires interventional (L2) or counterfactual (L3) reasoning.
  - Quick check question: Can an LLM perform true causal inference without access to interventional or counterfactual data? (Answer: No, per Pearl's Causal Hierarchy Theorem)

- Concept: Meta-SCMs and correlations of causal facts
  - Why needed here: This is the core theoretical contribution explaining how LLMs can appear causal without being causal - they exploit correlations between causal facts in training data rather than genuine causal reasoning.
  - Quick check question: How do meta-SCMs differ from regular SCMs in terms of what they represent? (Answer: Meta-SCMs encode causal facts about other SCMs within their variables)

- Concept: Knowledge base embeddings and semantic similarity
  - Why needed here: Understanding how embeddings encode causal knowledge is crucial for the knowledge base fact embeddings experiment showing how causal information can be transferred from structured knowledge to LLMs.
  - Quick check question: Why might knowledge base embeddings be more effective than direct querying for causal inference tasks? (Answer: They can capture semantic similarity beyond exact word matching)

## Architecture Onboarding

- Component map: Query generation -> LLM API calls -> Response collection -> Answer parsing -> Metric computation -> Result analysis
- Critical path: Prepare query templates → send queries to LLM → collect responses → parse and classify answers → compute evaluation metrics (SID, SHD, F1, ADS) → analyze results for evidence supporting CCF conjecture
- Design tradeoffs: Direct querying vs. knowledge base embeddings - direct querying is simpler but limited by exact wording, while embeddings can capture semantic similarity but require more complex processing
- Failure signatures: Poor performance on intuitive physics suggests lack of genuine causal understanding; inconsistent results across query wordings suggest sensitivity to surface form rather than causal structure; failure to generalize beyond training data patterns indicates correlation exploitation rather than causal reasoning
- First 3 experiments:
  1. Test causal chains with varying lengths (n=2 to 10) using both symbolic and natural word variables to establish baseline LLM performance on simple causal reasoning
  2. Test intuitive physics questions with idealized textual descriptions to evaluate "common sense" causal reasoning capabilities
  3. Test causal discovery on ground truth graphs using multiple query wordings to assess sensitivity to question formulation and measure decisiveness changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do larger language models with even more parameters and training data perform on causal inference tasks compared to GPT-3, OPT, and Luminous?
- Basis in paper: [explicit] The paper evaluates GPT-3, OPT, and Luminous on causal inference tasks and suggests that future work could investigate larger models.
- Why unresolved: The paper only evaluates three models, and it is unclear if larger models would show significantly different performance.
- What evidence would resolve it: Empirical evaluation of larger language models on the same causal inference tasks used in the paper.

### Open Question 2
- Question: Can language models be fine-tuned to improve their performance on causal inference tasks?
- Basis in paper: [explicit] The paper mentions that fine-tuning is an appropriate method for optimizing general-purpose language models for downstream tasks and poses the question of whether fine-tuning can be used to improve causal inference performance.
- Why unresolved: The paper does not conduct experiments on fine-tuning language models for causal inference.
- What evidence would resolve it: Empirical evaluation of fine-tuned language models on causal inference tasks, comparing their performance to non-fine-tuned models.

### Open Question 3
- Question: How do language models handle causal inference tasks involving non-factual information or imaginary concepts?
- Basis in paper: [explicit] The paper evaluates language models on causal chains with imaginary concepts and real-world concepts with non-factual information.
- Why unresolved: The paper does not provide a detailed analysis of how language models handle these types of tasks.
- What evidence would resolve it: Empirical evaluation of language models on a wider range of causal inference tasks involving non-factual information and imaginary concepts, analyzing their performance and reasoning.

## Limitations

- The paper's core claim that LLMs are "causal parrots" rests on the CCF conjecture, but evidence remains circumstantial and doesn't definitively prove the mechanism
- The paper doesn't adequately address whether observed limitations might stem from insufficient training data rather than fundamental architectural constraints
- The meta-SCM framework lacks concrete evidence that LLMs actually exploit these structures during inference

## Confidence

- **High confidence**: The empirical observation that LLMs show inconsistent performance across causal reasoning tasks and benefit from chain-of-thought prompting is well-supported by experimental results
- **Medium confidence**: The CCF conjecture provides a plausible explanation for LLM behavior, but the paper doesn't provide direct evidence that LLMs are specifically exploiting correlated causal facts
- **Low confidence**: The claim that LLMs cannot achieve genuine causal reasoning due to architectural limitations is speculative and not directly tested

## Next Checks

1. Conduct ablation studies systematically removing causal facts from training data to test whether performance degrades proportionally, providing stronger evidence for the CCF conjecture
2. Test whether fine-tuning LLMs on explicit causal reasoning examples improves performance on intuitive physics tasks, distinguishing between data limitations and architectural constraints
3. Design experiments to probe whether LLMs can generalize causal reasoning to novel variable relationships not present in training data, testing the correlation exploitation hypothesis more directly