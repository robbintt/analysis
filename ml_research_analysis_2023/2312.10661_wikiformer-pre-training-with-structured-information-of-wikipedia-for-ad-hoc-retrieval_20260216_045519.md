---
ver: rpa2
title: 'Wikiformer: Pre-training with Structured Information of Wikipedia for Ad-hoc
  Retrieval'
arxiv_id: '2312.10661'
source_url: https://arxiv.org/abs/2312.10661
tags:
- pre-training
- wikipedia
- wikiformer
- retrieval
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel pre-training framework called Wikiformer
  that leverages the structured information in Wikipedia to enhance ad-hoc retrieval.
  The key idea is to design four pre-training tasks tailored for IR based on the titles,
  abstracts, hierarchical heading structure, relationships between articles, and writing
  organizations of Wikipedia.
---

# Wikiformer: Pre-training with Structured Information of Wikipedia for Ad-hoc Retrieval

## Quick Facts
- arXiv ID: 2312.10661
- Source URL: https://arxiv.org/abs/2312.10661
- Authors: [Not specified in source]
- Reference count: 9
- Primary result: Achieves state-of-the-art performance on multiple IR benchmarks including MS MARCO, TREC DL 2019, TREC Covid, LeCaRD, and CAIL-LCR datasets

## Executive Summary
This paper presents Wikiformer, a novel pre-training framework that leverages Wikipedia's structured information to enhance ad-hoc retrieval performance. The approach designs four pre-training tasks specifically tailored for information retrieval based on Wikipedia's titles, abstracts, hierarchical heading structure, relationships between articles, and writing organizations. Experimental results demonstrate that Wikiformer significantly outperforms existing strong retrieval baselines in both zero-shot and fine-tuning settings across multiple benchmark datasets. The ablation study confirms that each pre-training task contributes to improving the model's retrieval ability, validating the effectiveness of utilizing Wikipedia's structured knowledge for pre-training language models.

## Method Summary
Wikiformer is a pre-training framework that leverages structured information from English Wikipedia (version 20220101) to improve ad-hoc retrieval performance. The method involves building a Wiki Structure Tree for each article and implementing four pre-training tasks: Simulated Re-ranking (SRR), Representative Words Identification (RWI), Abstract Texts Identification (ATI), and Long Texts Matching (LTM). These tasks utilize Wikipedia's hierarchical structure, including titles, abstracts, subtitles, and inter-article relationships. The model is initialized from BERT-base and pre-trained on these tasks using contrastive learning approaches. After pre-training, the model is fine-tuned on downstream datasets including MS MARCO Document Re-ranking, TREC DL 2019, TREC Covid, LeCaRD, and CAIL-LCR, with evaluation metrics including Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (nDCG).

## Key Results
- Achieves state-of-the-art performance on MS MARCO, TREC DL 2019, TREC Covid, LeCaRD, and CAIL-LCR datasets
- Demonstrates superior performance compared to existing strong retrieval baselines in both zero-shot and fine-tuning settings
- Ablation study verifies the effectiveness of each pre-training task in improving retrieval ability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Leveraging Wikipedia's hierarchical heading structure allows the model to learn multi-level semantic relationships between queries and documents.
- Mechanism: The Simulated Re-ranking (SRR) task models each Wikipedia article as a tree structure where subtitles represent representative content summaries. By traversing from root to child nodes to form queries, and using contrastive sampling, the model learns to identify relevance at different granularities.
- Core assumption: Hierarchical headings accurately represent semantic relationships between document sections.
- Evidence anchors: [abstract] "titles, abstracts, hierarchical heading (multi-level title) structure, relationship between articles, and the writing organizations of Wikipedia"; [section] "The SRR task is inspired by an important IR problem: document re-ranking."; [corpus] Weak evidence - no direct corpus analysis of heading quality provided.
- Break condition: If Wikipedia headings are poorly organized or inconsistent, the hierarchical structure would not provide reliable semantic signals.

### Mechanism 2
- Claim: Abstracts serve as high-quality positive documents for title-based queries due to their summarizing nature.
- Mechanism: The Abstract Texts Identification (ATI) task uses article titles as queries and abstracts as positive documents, contrasting them with other sections. This teaches the model that abstracts better capture user information needs when queries match article titles.
- Core assumption: Abstracts accurately summarize the entire article content.
- Evidence anchors: [abstract] "the abstract section of Wikipedia is the summarization of an article. When the user's query is the title of an article, the abstract section is more likely to match the user's information needs compared to other sections within the same article."; [section] "The abstract (the first section) of Wikipedia is regarded as the summarization of the whole article."; [corpus] Weak evidence - no empirical validation of abstract quality provided.
- Break condition: If abstracts are poorly written or incomplete, they would not provide reliable positive examples for training.

### Mechanism 3
- Claim: Representative Words Identification helps the model learn to recognize and prioritize keywords in documents.
- Mechanism: The RWI task treats subtitles as representative words and uses contrastive sampling to teach the model that positive queries (constructed from subtitles) better match their corresponding document content than negative queries.
- Core assumption: Subtitles accurately represent the main content of their corresponding sections.
- Evidence anchors: [abstract] "the subtitle is always the representative words or summarization of the corresponding section"; [section] "RWI task is inspired by an IR axiom which assumes that the user's query is the representative words extracted from the relevant documents."; [corpus] Weak evidence - no analysis of subtitle representativeness provided.
- Break condition: If subtitles are vague or misleading, the model would learn incorrect keyword prioritization.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: Enables the model to distinguish between relevant and irrelevant document-query pairs through relative comparisons rather than absolute labels.
  - Quick check question: How does contrastive learning help the model understand relevance relationships better than traditional supervised learning?

- Concept: Hierarchical document representation
  - Why needed here: Wikipedia's multi-level structure provides natural semantic hierarchies that can be leveraged for learning relevance at different granularities.
  - Quick check question: Why is hierarchical structure particularly useful for learning document relevance compared to flat document representations?

- Concept: Self-supervised learning with pseudo labels
  - Why needed here: Wikipedia's structured information can be automatically converted into training examples without human annotation, enabling large-scale pre-training.
  - Quick check question: What advantages does self-supervised pre-training have over supervised fine-tuning for information retrieval tasks?

## Architecture Onboarding

- Component map: Input → Transformer encoding → [CLS] representation → MLP → Relevance score → Loss computation
- Critical path: Input → Transformer encoding → [CLS] representation → MLP → Relevance score → Loss computation
- Design tradeoffs:
  - Using Wikipedia structure provides abundant training data but may introduce domain-specific biases
  - Four separate tasks capture different aspects of relevance but increase training complexity
  - Initializing from BERT-base reduces training cost but may limit architectural innovations
- Failure signatures:
  - Poor performance on zero-shot tasks suggests pre-training tasks aren't learning useful IR knowledge
  - Overfitting to Wikipedia style indicates need for more diverse pre-training data
  - Inconsistent results across domains suggest tasks aren't learning generalizable relevance patterns
- First 3 experiments:
  1. Ablation study: Train model with only SRR task to measure its contribution
  2. Zero-shot evaluation: Test model on unseen IR benchmark without fine-tuning
  3. Long document matching: Evaluate LTM task performance on document-to-document retrieval tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Wikiformer perform on datasets with non-English languages, and what are the specific challenges in adapting it to other languages?
- Basis in paper: [inferred] The paper mentions that Wikiformer is pre-trained on English Wikipedia and evaluates it on English datasets. It does not discuss performance on non-English datasets or the challenges of adapting to other languages.
- Why unresolved: The paper does not provide any experiments or discussion on the performance of Wikiformer on non-English datasets, leaving the question of its cross-lingual effectiveness unanswered.
- What evidence would resolve it: Experiments on non-English datasets, such as Wikipedia articles in other languages, would provide evidence of Wikiformer's performance in cross-lingual scenarios. Additionally, a discussion on the challenges and potential solutions for adapting Wikiformer to other languages would be valuable.

### Open Question 2
- Question: What is the impact of the size of the pre-training corpus on the performance of Wikiformer, and how does it compare to other pre-trained models in terms of data efficiency?
- Basis in paper: [explicit] The paper mentions that Wikiformer is pre-trained on the English Wikipedia corpus, but it does not discuss the impact of the size of the pre-training corpus on its performance. It also does not compare Wikiformer's data efficiency to other pre-trained models.
- Why unresolved: The paper does not provide any experiments or analysis on the relationship between the size of the pre-training corpus and Wikiformer's performance, nor does it compare its data efficiency to other models.
- What evidence would resolve it: Experiments varying the size of the pre-training corpus and comparing Wikiformer's performance to other pre-trained models would provide insights into its data efficiency and the impact of corpus size on its effectiveness.

### Open Question 3
- Question: How does Wikiformer handle real-time user feedback and adapt its retrieval performance accordingly?
- Basis in paper: [inferred] The paper focuses on the pre-training and fine-tuning of Wikiformer for ad-hoc retrieval tasks. It does not discuss how Wikiformer handles real-time user feedback or adapts its retrieval performance based on user interactions.
- Why unresolved: The paper does not provide any information on the model's ability to incorporate real-time user feedback or adapt its retrieval performance dynamically.
- What evidence would resolve it: Experiments evaluating Wikiformer's performance when incorporating real-time user feedback, such as click-through rates or explicit relevance judgments, would provide insights into its adaptability and responsiveness to user interactions.

## Limitations

- The effectiveness of the approach critically depends on the quality and consistency of Wikipedia's hierarchical structure, which may vary across different content types
- The pre-training leverages Wikipedia-specific structures that may not transfer well to other domains with different structural conventions
- The paper does not provide detailed analysis of the computational costs associated with the four-task pre-training approach

## Confidence

**High Confidence Claims**:
- Wikiformer achieves state-of-the-art performance on established IR benchmarks (MS MARCO, TREC DL 2019, TREC Covid, LeCaRD, CAIL-LCR)
- The four pre-training tasks (SRR, RWI, ATI, LTM) each contribute to improved retrieval performance based on ablation study results
- Pre-training with Wikipedia structure provides benefits in both zero-shot and fine-tuning settings

**Medium Confidence Claims**:
- Hierarchical heading structure effectively teaches multi-level semantic relationships
- Abstracts serve as reliable positive examples for title-based queries
- The specific combination of four pre-training tasks is optimal for IR pre-training

**Low Confidence Claims**:
- Performance guarantees on domains outside the tested benchmarks
- The exact contribution of each Wikipedia structural element to final performance
- The approach's effectiveness with different language models beyond BERT-base

## Next Checks

**Validation Check 1: Structural Quality Analysis**
Conduct a systematic analysis of Wikipedia heading quality and consistency across different article categories. Sample articles from diverse domains (science, history, culture, technical) and evaluate:
- The semantic consistency between headings and their corresponding content
- The hierarchical relationships' validity using human annotations
- The correlation between heading quality and model performance

**Validation Check 2: Cross-Domain Transferability**
Test Wikiformer's performance on non-Wikipedia structured documents such as:
- Academic papers with hierarchical sections (introduction, methods, results, discussion)
- Legal documents with structured sections (facts, arguments, conclusions)
- Technical documentation with organized subsections
Compare performance against models trained only on standard pre-training data without Wikipedia structure.

**Validation Check 3: Computational Efficiency Benchmark**
Measure and compare the computational costs of Wikiformer pre-training against standard approaches:
- Track GPU hours required for pre-training with different task combinations
- Measure inference latency on downstream tasks
- Analyze memory usage during both pre-training and inference
- Evaluate whether the performance gains justify the additional computational overhead