---
ver: rpa2
title: 'In Generative AI we Trust: Can Chatbots Effectively Verify Political Information?'
arxiv_id: '2312.13096'
source_url: https://arxiv.org/abs/2312.13096
tags:
- information
- chatbots
- chatgpt
- 'false'
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study uses AI auditing to compare two large language model
  chatbots, ChatGPT and Bing Chat, on their ability to evaluate political misinformation.
  Testing across English, Russian, and Ukrainian, it finds ChatGPT overall more accurate
  (72% correct evaluations) than Bing Chat (67%), with notable performance drops in
  lower-resource languages.
---

# In Generative AI we Trust: Can Chatbots Effectively Verify Political Information?

## Quick Facts
- arXiv ID: 2312.13096
- Source URL: https://arxiv.org/abs/2312.13096
- Reference count: 0
- Key outcome: ChatGPT outperformed Bing Chat in evaluating political misinformation across languages, with notable accuracy drops in lower-resource languages

## Executive Summary
This study uses AI auditing to compare two large language model chatbots, ChatGPT and Bing Chat, on their ability to evaluate political misinformation across English, Russian, and Ukrainian. The researchers found ChatGPT overall more accurate (72% correct evaluations) than Bing Chat (67%), with both showing substantial performance drops in lower-resource languages. The study reveals that both chatbots are more likely to avoid answering prompts in low-resource languages or on sensitive topics like the Holocaust and the Ukraine war, and that topic and source attribution have limited but statistically significant effects on evaluation accuracy.

## Method Summary
The researchers conducted AI auditing using 375 unique prompts (25 statements × 3 languages × 5 source attributions) covering five topics: COVID-19, Russian aggression against Ukraine, Holocaust, climate change, and LGBTQ+ debates. They submitted each prompt to ChatGPT and Bing Chat, collecting and manually coding 750 responses from five researchers. The study employed descriptive statistics and multinomial logistic regression to analyze chatbot performance across different accuracy categories, evaluating factors including language, chatbot model, topic, and source attribution.

## Key Results
- ChatGPT achieved 72% correct veracity evaluations compared to Bing Chat's 67%
- Both chatbots showed significant performance drops in Ukrainian (68% and 66% respectively)
- ChatGPT excelled at identifying conspiracy theories with 81-86% accuracy across all languages
- Chatbots were more likely to avoid answering sensitive topics like the Holocaust and Ukraine war

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative AI chatbots can outperform traditional ML in identifying hidden conspiratorial meanings due to their natural language comprehension capabilities.
- Mechanism: LLMs' contextual understanding allows them to recognize semantic patterns and implications beyond surface-level text, enabling detection of conspiratorial framing.
- Core assumption: Conspiracy detection requires understanding of implied meanings and context, not just keyword matching.
- Evidence anchors: [abstract] "ChatGPT demonstrated rather high performance with 81-86% correct responses for all of the languages... highlights potential advantages of LLMs over traditional machine learning (ML) techniques"
- Break condition: When conspiratorial narratives use novel linguistic patterns or rely heavily on cultural context not present in training data.

### Mechanism 2
- Claim: LLM performance varies significantly based on language resource availability, with lower-resource languages showing reduced accuracy.
- Mechanism: Training data scarcity for low-resource languages results in weaker pattern recognition and contextual understanding in those languages.
- Core assumption: Language performance correlates with the volume and quality of training data available for that language.
- Evidence anchors: [abstract] "significant disparities in how chatbots evaluate prompts in high- and low-resource languages... substantial performance drops"
- Break condition: When prompt complexity is low enough that even limited training data suffices.

### Mechanism 3
- Claim: Chatbots show varying sensitivity to source attribution, with some topics and sources triggering higher rates of non-response.
- Mechanism: Guardrails and safety filters in LLMs create conditional responses based on perceived sensitivity of topics and sources, leading to topic/source-dependent performance variation.
- Core assumption: LLMs incorporate topic and source sensitivity filters that activate differently based on perceived risk levels.
- Evidence anchors: [abstract] "chatbots are more likely to avoid answering prompts in low-resource languages or on sensitive topics like the Holocaust and the Ukraine war"
- Break condition: When prompts are rephrased to reduce perceived sensitivity.

## Foundational Learning

- Concept: AI auditing methodology
  - Why needed here: The study uses AI auditing to systematically evaluate chatbot performance across multiple dimensions and conditions
  - Quick check question: What are the key differences between AI auditing and traditional algorithm auditing?

- Concept: Multinomial logistic regression
  - Why needed here: Used to analyze factors influencing chatbot performance across different accuracy categories
  - Quick check question: How does multinomial logistic regression differ from binary logistic regression in analyzing categorical outcomes?

- Concept: Source bias modeling
  - Why needed here: The study systematically tests how attributing statements to different sources affects chatbot evaluations
  - Quick check question: What are the key considerations when designing source attribution experiments in AI auditing?

## Architecture Onboarding

- Component map: Prompt generation -> API interaction -> Response collection -> Manual coding -> Statistical analysis -> Result interpretation
- Critical path: Prompt generation → API interaction → Response collection → Manual coding → Statistical analysis → Result interpretation
- Design tradeoffs: Between prompt detail level (affecting real-world applicability) and controlled experimental conditions
- Failure signatures: Non-responses, inaccurate classifications, or inconsistent evaluations across similar prompts
- First 3 experiments:
  1. Test performance consistency by repeating identical prompts across different sessions
  2. Compare performance on controlled vs. naturalistic prompt formulations
  3. Evaluate impact of prompt formatting variations (paragraph vs. bullet points) on response quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors cause LLM performance variations across different languages beyond training data volume, and how can these be systematically addressed?
- Basis in paper: [explicit] The paper notes substantial performance drops in low-resource languages but doesn't investigate specific linguistic or technical factors
- Why unresolved: The study identifies language disparities but doesn't examine the mechanisms behind these differences
- What evidence would resolve it: Comparative analysis of linguistic features and technical architecture differences across language models

### Open Question 2
- Question: How do LLM-based chatbots perform in real-world scenarios where users don't provide detailed instructions, and what are the implications for practical misinformation detection?
- Basis in paper: [explicit] The paper acknowledges testing with highly detailed instructions that "may not be that common in a real-world environment"
- Why unresolved: The study used structured prompts with clear definitions, but real-world usage likely involves more ambiguous queries
- What evidence would resolve it: Field studies comparing chatbot performance with both structured and unstructured real-world prompts

### Open Question 3
- Question: What is the relationship between LLM detection accuracy and the potential for these systems to generate or amplify misinformation themselves?
- Basis in paper: [inferred] The paper discusses LLMs' potential for both detecting and generating false information
- Why unresolved: While the study focuses on detection capabilities, it doesn't address whether improved detection correlates with reduced generation of false content
- What evidence would resolve it: Longitudinal studies tracking both detection accuracy and generation tendencies across multiple LLM versions

## Limitations
- High non-response rates for prompts in Russian and Ukrainian, especially for Bing Chat
- Inconsistent coding of misinformation/disinformation labels due to subjective interpretation
- Potential influence of training data cutoff dates on evaluation accuracy for contemporary topics

## Confidence
- High: ChatGPT's superior performance in identifying conspiracy theories (81-86% accuracy)
- Medium: Cross-language performance claims due to small sample size per language
- Medium: Source attribution effects and sensitivity to controversial topics (statistical significance but modest effect sizes)

## Next Checks
1. Conduct inter-rater reliability analysis on the manual coding process to quantify agreement levels and identify systematic biases
2. Test chatbot performance on a parallel dataset with prompts that use varied linguistic formulations
3. Implement a temporal validation by repeating key prompts at different time intervals to evaluate consistency