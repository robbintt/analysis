---
ver: rpa2
title: Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned
  Language Models
arxiv_id: '2309.05454'
source_url: https://arxiv.org/abs/2309.05454
tags:
- level
- people
- language
- text
- grade
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how well instruction-tuned language models
  align with readability standards like Flesch Kincaid Grade Level (FKGL) and Common
  European Framework of Reference for Languages (CEFR) when generating or simplifying
  texts. Using diverse open and closed-source models, the authors prompt them to write
  stories or simplify narratives with specific readability targets.
---

# Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models

## Quick Facts
- arXiv ID: 2309.05454
- Source URL: https://arxiv.org/abs/2309.05454
- Reference count: 17
- Key outcome: Most instruction-tuned language models struggle to generate text aligned with readability standards like FKGL and CEFR, with BLOOMZ and FlanT5 showing the best performance among open-source models.

## Executive Summary
This study evaluates how well instruction-tuned language models align with readability standards like Flesch Kincaid Grade Level (FKGL) and Common European Framework of Reference for Languages (CEFR) when generating or simplifying texts. Using diverse open and closed-source models, the authors prompt them to write stories or simplify narratives with specific readability targets. Results show that most models struggle to generate outputs within the target complexity ranges, particularly for FKGL-based prompts. However, models like BLOOMZ, FlanT5, and ChatGPT show some improvement with more detailed prompts. For CEFR-based simplification, models like FlanT5 and BLOOMZ perform well, especially when prompts include explicit framework descriptions. The study highlights the need for more refined prompts and diverse training data to better align model outputs with educational readability standards.

## Method Summary
The study uses pre-trained instruction-tuned models without additional fine-tuning, generating text for story completion and narrative simplification tasks using prompts with varying levels of readability specification. The European Language Grid (ELG) corpus provides text passages at different CEFR levels. Four prompt styles with increasing readability specification are tested across six models: ChatGPT, LLaMA 2, FlanT5, BLOOMZ, Longform, and Dolly. Generated text is evaluated using FKGL formula for formula-based evaluation and a CEFR classifier for standard-based evaluation.

## Key Results
- Most models consistently generate FKGL scores outside target ranges when prompted with grade-level specifications
- BLOOMZ and FlanT5 show the best performance among open-source models for both FKGL and CEFR alignment
- ChatGPT demonstrates improvement with more detailed prompts, while other models show minimal gains from additional specification
- Explicit readability framework descriptions in prompts improve CEFR alignment for most models

## Why This Works (Mechanism)

### Mechanism 1
Models fine-tuned on diverse multitask corpora perform better on readability alignment than those with narrow instruction sets. Diverse instruction datasets provide broader context for readability standards, enabling models to internalize multiple readability frameworks rather than just memorizing single-task patterns.

### Mechanism 2
Providing explicit readability framework descriptions improves model alignment with readability targets. Detailed prompt specifications act as stronger conditioning signals that guide token prediction toward desired readability outcomes.

### Mechanism 3
Reinforcement learning from human feedback (RLHF) improves alignment with user-oriented readability preferences. RLHF fine-tuning encourages models to optimize for human judgment of readability, leading to better performance on tasks requiring adherence to human-defined readability levels.

## Foundational Learning

- **Readability metrics (FKGL, CEFR)**: Understanding how these metrics work is essential to interpret results. Quick check: What is the range of FKGL scores and what do they indicate?
- **Instruction tuning and multitask learning**: Performance differences across models are explained by the diversity and size of instruction datasets used during fine-tuning. Quick check: Why does Dolly perform worse than FlanT5 and BLOOMZ?
- **Prompt engineering and conditioning**: Prompt specificity significantly impacts output alignment. Quick check: How does adding a metric description to a prompt change model behavior?

## Architecture Onboarding

- **Component map**: ELG corpus -> prompt generator -> instruction-tuned models (BLOOMZ, FlanT5, Longform, Llama2, Dolly, ChatGPT) -> readability evaluators (FKGL formula, CEFR SVM classifier) -> result aggregation
- **Critical path**: ELG data → prompt construction → model generation → readability evaluation → result aggregation
- **Design tradeoffs**: Using standard-sized models limits performance ceiling but ensures accessibility; nucleus sampling (p=0.95) balances creativity and coherence
- **Failure signatures**: Consistently out-of-range FKGL scores; CEFR misclassification (e.g., B1 instead of A2); lack of improvement with more detailed prompts
- **First 3 experiments**:
  1. Generate story completions with no readability specification; evaluate FKGL and CEFR alignment
  2. Repeat with grade-level-only specification; compare FKGL and CEFR results
  3. Repeat with full specification (grade level, metric name, description); measure improvement in alignment

## Open Questions the Paper Calls Out

1. **How do large language models' performances vary when prompted with explicit computation information for readability metrics like FKGL versus implicit instructions?** The paper notes that models struggled to generate outputs within the target FKGL ranges, suggesting that explicit computation information might be needed.

2. **How does the scale of instruction-tuned models (e.g., 3B vs. 70B parameters) affect their ability to capture and reflect readability standards in their outputs?** The authors used standard-sized models and acknowledged they did not have the hardware for larger versions, indicating a potential area for further exploration.

3. **To what extent do instruction-tuned models trained on multilingual datasets perform differently in readability alignment tasks compared to those trained on monolingual datasets?** While BLOOMZ was included, the study did not directly compare its performance to monolingual models in a controlled manner.

## Limitations

- FKGL scores for many model outputs fell outside target ranges even when prompts specified explicit readability targets, suggesting fundamental limitations in how models internalize readability metrics
- The CEFR classifier used for evaluation may not perfectly capture the nuanced differences between language proficiency levels that human experts would recognize
- The study focuses primarily on English-language texts and may not generalize to other languages or multilingual models

## Confidence

**High Confidence**: The core finding that instruction-tuned models struggle to consistently align with FKGL and CEFR standards when generating text.

**Medium Confidence**: The conclusion that explicit prompt specifications improve readability alignment for certain models, particularly the relationship between instruction-tuning diversity and readability performance.

**Low Confidence**: The assertion that RLHF specifically drives ChatGPT's superior performance on readability tasks.

## Next Checks

1. **Cross-Lingual Validation**: Test the same prompt engineering techniques and model architectures on non-English texts to determine if readability alignment challenges are language-specific or inherent to model architectures.

2. **Human Expert Evaluation**: Conduct human expert assessments of model outputs against CEFR and FKGL targets to validate classifier accuracy and identify discrepancies between automated and human readability judgments.

3. **Training Data Analysis**: Characterize the instruction-tuning datasets used by different models to quantify the relationship between dataset diversity, size, and readability alignment performance.