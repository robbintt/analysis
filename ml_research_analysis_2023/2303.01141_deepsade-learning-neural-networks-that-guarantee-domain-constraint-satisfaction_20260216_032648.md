---
ver: rpa2
title: 'DeepSaDe: Learning Neural Networks that Guarantee Domain Constraint Satisfaction'
arxiv_id: '2303.01141'
source_url: https://arxiv.org/abs/2303.01141
tags:
- constraint
- domain
- constraints
- learning
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DeepSaDe, a method for training neural networks
  that guarantees domain constraint satisfaction for all predictions. It extends the
  SaDe framework by propagating constraints through network layers and updating the
  last layer via a mix of gradient descent and constraint satisfaction problem (CSP)
  solving.
---

# DeepSaDe: Learning Neural Networks that Guarantee Domain Constraint Satisfaction

## Quick Facts
- arXiv ID: 2303.01141
- Source URL: https://arxiv.org/abs/2303.01141
- Authors: 
- Reference count: 14
- Primary result: DeepSaDe achieves 100% constraint accuracy and zero adversity index on five use cases by extending SaDe to neural networks with constraint propagation and MaxSMT-based last layer updates.

## Executive Summary
DeepSaDe introduces a method for training neural networks that guarantee domain constraint satisfaction for all predictions. The approach extends the SaDe framework by propagating constraints through network layers and updating the last layer via a mix of gradient descent and constraint satisfaction problem (CSP) solving. Using SMT solvers, DeepSaDe enforces constraints expressed as universally quantified formulas over bounded input domains. Experiments on five use cases demonstrate that DeepSaDe achieves 100% constraint accuracy and zero adversity index, outperforming regularization-based baselines. However, DeepSaDe's predictive performance is sometimes slightly worse and training time is significantly higher (10-500x) due to the CSP solving overhead.

## Method Summary
DeepSaDe extends SaDe to neural networks by propagating constraints through layers using monotonic activation functions (ReLU, Sigmoid, Tanh) to calculate output bounds given input bounds and layer parameters. The method leverages the fact that the last layer is typically linear, allowing it to be updated independently via a MaxSMT framework where soft constraints encode training data quality and hard constraints enforce domain satisfaction. The rest of the network is trained using standard gradient descent. The training procedure involves a line search along the negative gradient direction followed by MaxSMT formulation with local search spaces and restart procedures when needed. Skip connections map relevant features to the last layer for constraint formulation, and Z3 solver handles the SMT encoding of universally quantified constraints over bounded domains.

## Key Results
- DeepSaDe achieves 100% constraint accuracy and zero adversity index across all five use cases
- Compared to regularization baselines (SL, SBR, REG), DeepSaDe guarantees constraint satisfaction while baselines have counterexamples
- Training time is 10-500x slower than baselines due to MaxSMT solver overhead
- Predictive performance is sometimes slightly worse than baselines but acceptable in safety-critical applications
- AdI (adversity index) measurements via Marabou confirm zero constraint violations in neighborhoods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraint propagation through network layers enables universal quantification over the original input domain to be translated into universal quantification over the transformed latent space.
- Mechanism: The paper proposes a method to calculate bounds for each layer's outputs given the bounds for the inputs and the layer parameters. This is done by leveraging the monotonic properties of common activation functions like ReLU, Sigmoid, and Tanh. For each neuron, the output is bounded by evaluating the minimum and maximum possible values given the input bounds and the weights. This bound calculation is recursively applied through the network layers to obtain bounds for the latent input space of the last layer. The domain constraint is then translated to be over this latent space with the input property expressed in terms of the relevant features that are mapped to the last layer via skip connections.
- Core assumption: The activation functions used are monotonically non-decreasing, which allows for the bound calculation method to work.
- Evidence anchors:
  - [abstract] "constraint propagation over the network layers"
  - [section 5.1] "We propose a way to calculate, for each layer of the network, the bounds for the outputs of the layer given the bounds for the inputs to the layer and the parameters associated with the layer."
- Break condition: If non-monotonic activation functions are used, or if the network architecture deviates significantly from fully connected layers, the bound calculation method may not be applicable.

### Mechanism 2
- Claim: Updating the last layer via a MaxSMT framework while training the rest of the network with gradient descent allows for constraint satisfaction without sacrificing the expressive power of neural networks.
- Mechanism: The paper leverages the fact that the last layer of a neural network is typically a linear layer that maps the transformed domain to the output. By formulating a MaxSMT problem for the last layer, where the soft constraints are decision constraints that encode the quality of fit on the training data and the hard constraints are the domain constraints translated to the latent space, the last layer can be updated to satisfy the constraints. The rest of the network is updated using standard gradient descent to optimize the predictive loss. This combination allows for constraint satisfaction while maintaining the expressive power of the neural network.
- Core assumption: The last layer is a linear layer that can be updated independently of the rest of the network.
- Evidence anchors:
  - [abstract] "weight updates based on a mix of gradient descent and CSP solving"
  - [section 5] "The core insight behind DeepSaDe is the fact that neural networks perform domain transformations with a series of non-linear layers up to the last one. The last layer of the network is usually a linear layer that maps the transformed domain to the output."
- Break condition: If the last layer is not linear, or if the network architecture is not amenable to separating the last layer update from the rest of the network, this mechanism may not work.

### Mechanism 3
- Claim: The line search followed by MaxSMT formulation provides an efficient way to find a solution that satisfies the constraints without getting stuck in local minima.
- Mechanism: At each iteration, the algorithm first performs a line search along the negative gradient direction to find a solution that satisfies the constraints. If no such solution is found, a MaxSMT problem is formulated with a local search space around the previous solution. This local search space is defined by a box constraint that limits the update size in each dimension. If the MaxSMT problem cannot be solved within this local search space, a restart procedure is initiated where the signs of the gradients are randomly flipped to explore a different direction.
- Core assumption: The solution space is connected and the constraints are satisfiable within the defined search spaces.
- Evidence anchors:
  - [section 5.2] "For updating the last layer, first a line search is used along the vector that minimizes the prediction loss and a fixed number of candidate solutions, from furthest to closest, are checked if they satisfy K1 and first one that does is picked."
  - [section 5.2] "Sometimes our approach cannot find a solution for the last layer because neither the line search led to a solution nor the MaxSMT problem could be solved within the local search space. In such an event, we initiate a restart procedure where the signs of the gradients are randomly flipped."
- Break condition: If the constraints are not satisfiable, or if the solution space is disconnected, the line search and MaxSMT formulation may fail to find a solution.

## Foundational Learning

- Concept: MaxSAT (Maximum Satisfiability)
  - Why needed here: The paper uses a MaxSAT framework to update the last layer of the neural network while satisfying the domain constraints. Understanding MaxSAT is crucial to grasp how the last layer is updated.
  - Quick check question: In a MaxSAT problem, what is the goal when given a set of hard constraints H and soft constraints S?

- Concept: SMT (Satisfiability Modulo Theories)
  - Why needed here: The paper uses SMT solvers to enforce constraints expressed as universally quantified formulas over bounded input domains. Understanding SMT is essential to comprehend how the constraints are translated and enforced.
  - Quick check question: How does SMT extend SAT, and what are some theories that can be used in SMT formulas?

- Concept: Universal Quantification
  - Why needed here: The domain constraints in the paper are expressed using universal quantifiers, which means they must hold for all possible values of the input variables within the bounded domain. Understanding universal quantification is key to interpreting the constraint formulations.
  - Quick check question: What is the difference between existential and universal quantification, and how are they used in the constraint formulations in the paper?

## Architecture Onboarding

- Component map: Input layer -> Hidden layers (with monotonic activations) -> Last layer (linear) -> Skip connections -> MaxSMT solver -> Gradient descent updates

- Critical path:
  1. Forward pass through the network to compute the output
  2. Backward pass to compute the gradients
  3. Line search to find a solution that satisfies the constraints
  4. If line search fails, formulate a MaxSMT problem with a local search space
  5. Solve the MaxSMT problem to update the last layer
  6. Update the rest of the network using gradient descent
  7. Repeat until convergence or maximum number of epochs reached

- Design tradeoffs:
  - Constraint satisfaction vs. predictive performance: The paper prioritizes constraint satisfaction, which may lead to slightly worse predictive performance compared to methods that do not guarantee constraints.
  - Training time: The use of MaxSMT solvers makes the training significantly slower compared to standard gradient descent methods.
  - Expressiveness of constraints: The constraints must be expressible in the SMT solver's supported theories, which may limit the types of constraints that can be enforced.

- Failure signatures:
  - High training time: If the MaxSMT solver takes too long to find a solution, the training time will be significantly higher.
  - Low constraint accuracy: If the constraints are not satisfiable or if the MaxSMT formulation is incorrect, the constraint accuracy may be low.
  - Poor predictive performance: If the constraint satisfaction requirements are too strict, it may limit the model's ability to fit the training data.

- Three first experiments:
  1. Test DeepSaDe on UC1 (household expenses) to verify constraint satisfaction for spending limits across different categories.
  2. Apply DeepSaDe to UC2 (loan approval) to ensure constraints on loan eligibility criteria are satisfied for all predictions.
  3. Evaluate DeepSaDe on UC3 (music genre classification) to verify that genre classification constraints (e.g., tempo and key signature relationships) are enforced.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions in the provided text.

## Limitations
- The method requires significantly more training time (10-500x slower) due to MaxSMT solver overhead, making it computationally expensive for large-scale applications.
- Constraints must be expressible in the SMT solver's supported theories, limiting the types of constraints that can be enforced, particularly non-algebraic functions like logarithms and trigonometric functions.
- The approach assumes linear last layers and monotonic activation functions, which may not generalize to all neural network architectures and could affect the validity of constraint propagation.

## Confidence
- **High confidence**: Constraint propagation mechanism through monotonic activations, MaxSMT framework for last layer updates
- **Medium confidence**: Universal quantification over bounded domains via SMT encoding, training procedure with line search and restarts
- **Low confidence**: Performance claims relative to baselines, computational overhead analysis, generalizability beyond tested use cases

## Next Checks
1. **Solver Robustness Test**: Run constraint satisfaction verification with timeout limits and measure failure rates to quantify the actual reliability of the 100% constraint accuracy claim
2. **Layer Architecture Impact**: Test DeepSaDe with non-linear last layers (e.g., softmax with temperature scaling) to verify the linear layer decoupling assumption
3. **Constraint Expressiveness Limits**: Systematically vary constraint complexity and measure the point where Z3 solver fails to find solutions within reasonable time, establishing practical bounds on constraint expressibility