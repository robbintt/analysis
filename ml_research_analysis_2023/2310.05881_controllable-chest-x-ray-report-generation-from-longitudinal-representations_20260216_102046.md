---
ver: rpa2
title: Controllable Chest X-Ray Report Generation from Longitudinal Representations
arxiv_id: '2310.05881'
source_url: https://arxiv.org/abs/2310.05881
tags:
- report
- anatomical
- lung
- right
- regions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to automatically generate chest X-ray
  reports by leveraging both current and prior scans, and a novel training strategy
  to improve controllability. The approach aligns anatomical representations from
  current and prior scans into a joint representation, then uses a language model
  to generate reports.
---

# Controllable Chest X-Ray Report Generation from Longitudinal Representations

## Quick Facts
- arXiv ID: 2310.05881
- Source URL: https://arxiv.org/abs/2310.05881
- Reference count: 32
- One-line primary result: State-of-the-art BLEU-4 of 0.246 and F1-CE of 0.553 for full CXR reports with improved controllability for partial reports

## Executive Summary
This paper presents a method for automatically generating chest X-ray reports by leveraging both current and prior scans through a novel training strategy. The approach extracts anatomical tokens from both scans, aligns them into a joint representation, and uses a language model to generate reports. A key innovation is the sentence-anatomy dropout training strategy, which enables controllable generation by training the model to predict only sentences corresponding to subsets of input anatomical regions. Experiments on MIMIC-CXR demonstrate state-of-the-art performance for both full and partial report generation.

## Method Summary
The method extracts anatomical tokens from current and prior CXRs using a Faster R-CNN trained to detect 36 anatomical regions and 71 findings. These tokens are concatenated and projected via an MLP into a joint longitudinal representation. A multimodal Transformer encoder-decoder takes this representation along with indication field text to generate reports. During training, sentence-anatomy dropout is applied: subsets of anatomical tokens are dropped and only corresponding sentences are included in the target, forcing the model to learn explicit mappings between anatomical regions and report content. The model is evaluated on both full reports and partial reports generated from subsets of anatomical regions.

## Key Results
- State-of-the-art BLEU-4 of 0.246 and F1-CE of 0.553 for full report generation
- Improved controllability demonstrated through partial report generation from anatomical subsets
- Superior performance compared to previous methods including M3, CoRE, and M2Gen
- Better alignment between generated and ground truth report length distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint anatomical representations from current and prior scans enable better temporal reasoning than single-scan inputs.
- Mechanism: Anatomical tokens from both scans are concatenated and projected via an MLP into a unified representation that preserves spatial correspondence across time.
- Core assumption: Anatomical tokens from prior and current CXRs can be aligned reliably because they are defined by consistent anatomical regions.
- Evidence anchors:
  - [abstract]: "we input the prior scan as an additional input, proposing a method to align, concatenate and fuse the current and prior visual information into a joint longitudinal representation"
  - [section 3.2]: Describes extracting anatomical tokens for both current and prior CXRs, concatenating them, and projecting into a joint representation via an MLP.
  - [corpus]: Weak evidence; corpus lacks strong references to longitudinal representation learning in CXR, but related works focus on single-scan approaches.
- Break condition: If anatomical alignment fails (e.g., due to severe patient positioning changes), the concatenated representation may not meaningfully represent temporal progression.

### Mechanism 2
- Claim: Sentence-anatomy dropout enforces explicit mapping between anatomical regions and report sentences, improving controllability.
- Mechanism: During training, subsets of anatomical tokens are dropped and only sentences corresponding to those regions are included in the target; the model learns to generate only the relevant report parts.
- Core assumption: Each report sentence can be traced back to one or more anatomical regions, allowing valid subset creation.
- Evidence anchors:
  - [abstract]: "sentence-anatomy dropout – a training strategy for controllability in which the report generator model is trained to predict only sentences from the original report which correspond to the subset of anatomical regions given as input"
  - [section 3.4]: Details the algorithm for finding valid sentence-anatomy subsets and using them for training.
  - [corpus]: Weak; while some works mention controllability, explicit sentence-anatomy dropout is not present in the corpus.
- Break condition: If sentence-anatomy pairs cannot be reliably extracted (e.g., due to inconsistent report structure), the dropout strategy becomes ineffective.

### Mechanism 3
- Claim: Anatomical tokens capture fine-grained, spatially localized features better than global image features for radiology reporting.
- Mechanism: Faster R-CNN is trained to detect anatomical regions and findings, producing feature vectors for each region that are then used as input to the language model.
- Core assumption: Region-level features encode clinically relevant information that correlates with report content.
- Evidence anchors:
  - [section 3.1]: Explains extracting bounding box feature vectors from Faster R-CNN for anatomical regions.
  - [section 2.2]: Notes that this approach is similar to Karwande et al. (2022) but uses anatomical representations instead of image-level features.
  - [corpus]: Weak; related works mention anatomical tokens but not as extensively validated in the corpus.
- Break condition: If Faster R-CNN fails to detect regions accurately, the token quality degrades and report generation suffers.

## Foundational Learning

- Concept: Multimodal transformer architectures
  - Why needed here: The model must integrate visual tokens (anatomical features) and textual tokens (indication field) to generate coherent reports.
  - Quick check question: Can the model generate a report if one modality is missing? (Expected: It should handle missing modalities gracefully, e.g., by zeroing out visual tokens for initial scans.)

- Concept: Object detection with Faster R-CNN
  - Why needed here: Anatomical tokens are extracted from the RoI pooling layer of a Faster R-CNN trained to detect 36 anatomical regions and 71 findings.
  - Quick check question: What happens if Faster R-CNN fails to detect an anatomical region? (Expected: A zero vector is used as a placeholder.)

- Concept: Clinical efficiency metrics (CheXbert-based F1)
  - Why needed here: Standard NLG metrics don't fully capture clinical correctness; CheXbert-derived CE metrics assess whether findings are accurately reported.
  - Quick check question: Why might CE metrics be imperfect? (Expected: CheXbert has an F1 of 0.798, so it's not perfectly accurate, leading to imperfect CE scores.)

## Architecture Onboarding

- Component map: Visual Anatomical Token Extraction (Faster R-CNN) -> Longitudinal Projection Module (MLP) -> Language Model (Transformer encoder-decoder) -> Indication Field Processing (text tokenization/encoding)
- Critical path: Anatomical tokens → Longitudinal projection → LM input → Report generation
- Design tradeoffs:
  - Using anatomical tokens vs. global features: More interpretable but depends on detection accuracy.
  - Sentence-anatomy dropout: Improves controllability but requires reliable sentence-anatomy mapping.
  - Joint representation size: 2048 dimensions balances expressiveness and computational cost.
- Failure signatures:
  - Poor anatomical detection → noisy or zeroed tokens → irrelevant report content.
  - Incorrect sentence-anatomy mapping → invalid training subsets → unstable training.
  - LM overfitting → high training metrics but poor generalization.
- First 3 experiments:
  1. Train with only current scan anatomical tokens (no priors) to confirm baseline performance.
  2. Add prior scan tokens but disable sentence-anatomy dropout to isolate the effect of longitudinal data.
  3. Enable sentence-anatomy dropout with full longitudinal data to evaluate controllability gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform when applied to other types of medical scans beyond chest X-rays, such as CT or MRI?
- Basis in paper: [explicit] The paper mentions that the method focuses only on CXR and adapting it to other types of medical scans might be challenging.
- Why unresolved: The paper does not provide any experiments or results for other types of medical scans.
- What evidence would resolve it: Conducting experiments and evaluating the performance of the proposed method on other types of medical scans, such as CT or MRI, would provide evidence of its generalizability and effectiveness beyond chest X-rays.

### Open Question 2
- Question: How does the quality of the automatically extracted anatomical bounding box annotations in the Chest ImaGenome dataset affect the performance of the proposed method?
- Basis in paper: [explicit] The paper mentions that the Chest ImaGenome dataset includes automatically extracted anatomical bounding box annotations, and the same authors pointed out some known limitations of their NLP and region extraction pipelines.
- Why unresolved: The paper does not discuss the impact of the quality of the annotations on the performance of the proposed method.
- What evidence would resolve it: Analyzing the performance of the proposed method using manually annotated anatomical bounding boxes instead of automatically extracted ones would provide evidence of the impact of annotation quality on the method's performance.

### Open Question 3
- Question: How does the proposed method handle reports that are as simple as "No change is seen" or other follow-up reports that may not contain detailed clinical findings?
- Basis in paper: [explicit] The paper mentions that some follow-up reports may be as simple as "No change is seen" and that clinical findings may not be properly extracted from such reports.
- Why unresolved: The paper does not discuss how the proposed method handles such cases or how it performs on reports with minimal clinical findings.
- What evidence would resolve it: Evaluating the performance of the proposed method on reports with minimal clinical findings, such as "No change is seen," would provide evidence of its ability to handle such cases and generate accurate reports.

## Limitations

- Anatomical token extraction quality depends heavily on Faster R-CNN detection accuracy, which varies with image quality and patient positioning.
- Sentence-anatomy dropout requires reliable mapping between report sentences and anatomical regions, which may not generalize across different report structures.
- The method has only been validated on chest X-rays and may not generalize to other medical imaging modalities.

## Confidence

- Longitudinal representation learning improves temporal reasoning: Medium
- Sentence-anatomy dropout enables controllability: Medium
- Anatomical tokens capture clinically relevant features: High (methodology), Low (deployment reliability)
- State-of-the-art performance on MIMIC-CXR: Medium (benchmark comparison only)

## Next Checks

1. **Ablation study on prior scan contribution**: Train and evaluate the model with only current scan anatomical tokens (no priors) to quantify the performance gain from longitudinal information. Compare full report generation metrics (BLEU-4, F1-CE) between single-scan and dual-scan variants.

2. **Controllability validation on partial reports**: Generate reports using only subsets of anatomical regions (e.g., lungs only, heart only) and measure precision/recall of findings specific to those regions. This would provide quantitative evidence for the controllability claim beyond full report generation.

3. **Anatomical detection accuracy assessment**: Evaluate Faster R-CNN's detection performance on MIMIC-CXR by measuring mean Average Precision (mAP) for anatomical regions and findings. Report failure cases where detection confidence falls below a threshold and analyze their impact on generated report quality.