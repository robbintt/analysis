---
ver: rpa2
title: No offence, Bert -- I insult only humans! Multiple addressees sentence-level
  attack on toxicity detection neural network
arxiv_id: '2310.13099'
source_url: https://arxiv.org/abs/2310.13099
tags:
- attack
- words
- toxicity
- detection
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel sentence-level black-box attack on
  toxicity detection models that involves appending or prepending positive words or
  sentences to a toxic message. The attack exploits the fact that toxicity detection
  models are trained on sentence-level labels and do not consider the context or intention
  of messages.
---

# No offence, Bert -- I insult only humans! Multiple addressees sentence-level attack on toxicity detection neural network

## Quick Facts
- arXiv ID: 2310.13099
- Source URL: https://arxiv.org/abs/2310.13099
- Authors: 
- Reference count: 40
- Key outcome: Novel sentence-level black-box attack on toxicity detection models using positive word/sentence appending, effective across 7 languages and various transformer-based detectors

## Executive Summary
This paper presents a novel sentence-level black-box attack on toxicity detection models that involves appending or prepending positive words or sentences to a toxic message. The attack exploits the fact that toxicity detection models are trained on sentence-level labels and do not consider the context or intention of messages. By adding positive words from SentiWordNet with a positivity score ≥ 0.85, the authors were able to lower the toxicity score of the message below the common threshold of 0.5, effectively masking the toxicity. The attack was tested on seven languages from three different language families and demonstrated to be working on various transformer-based toxicity detectors, including Google Perspective API and OpenAI Moderation API. A simple defence mechanism using adversarial training was also proposed and shown to be effective in distinguishing attacked and non-attacked texts.

## Method Summary
The attack involves appending or prepending positive words or sentences to toxic messages to lower their toxicity scores below detection thresholds. Positive words are selected from SentiWordNet with positivity scores ≥ 0.85, while sentences are taken from the Stanford Sentiment Treebank with scores ≥ 0.9 and length ≥ 100 symbols. The attack was tested on seven languages and multiple transformer-based toxicity detectors. A defense mechanism using adversarial training on DistilBERT was proposed, trained on both toxic and non-toxic messages with some toxic messages attacked, to distinguish attacked from non-attacked texts.

## Key Results
- Attack successfully reduced toxicity scores below 0.5 threshold by appending positive words or sentences
- Cross-lingual attacks were effective, exploiting multilingual model weaknesses
- Sentence-based attacks were more effective than word-based attacks
- Adversarial training defense achieved high F1 scores in distinguishing attacked vs non-attacked texts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding positive words to toxic text lowers the toxicity score below detection threshold.
- Mechanism: The toxicity detection model treats the appended positive words as part of the sentence, lowering the overall toxicity prediction.
- Core assumption: Toxicity detection models do not consider message context or intent, only sentence-level features.
- Evidence anchors:
  - [abstract] "By adding several positive words or sentences to the end of a hateful message, we are able to change the prediction of a neural network"
  - [section 2.1] "The words and sentences are selected based on their positivity scores rather than being randomly selected from non-hate ones"
  - [corpus] Weak - related papers focus on interpretability and detection improvements, not specific attacks using positive word addition
- Break condition: If model uses context-aware processing or separates intent from content, the attack fails.

### Mechanism 2
- Claim: Cross-lingual attacks work because multilingual models are less robust to language mixing.
- Mechanism: Adding positive words in a different language from the toxic message exploits the model's difficulty handling multilingual input.
- Core assumption: Multilingual models trained on multiple languages have blind spots when languages are mixed within a single input.
- Evidence anchors:
  - [section 2.2] "Adding positive words in different languages will even more separate messages to the human and to the toxicity detection system"
  - [section 2.1] "we tested the language models for seven languages aside from English"
  - [corpus] Missing - no corpus evidence for multilingual attack effectiveness
- Break condition: If model has strong cross-lingual consistency or language-specific toxicity scoring.

### Mechanism 3
- Claim: Sentence-based attacks are harder to detect than word-based attacks.
- Mechanism: Grammatically coherent positive sentences are less likely to trigger language model anomaly detection than random word lists.
- Core assumption: Language models can more easily flag incoherent word sequences than coherent sentences as adversarial.
- Evidence anchors:
  - [section 2.3] "we experimented with another version of the attack: concatenating sentences from the Stanford Sentiment Treebank with a positivity score ≥ 0.9"
  - [section 4.1] "With a sentence-based attack, adding even one sentence drastically lowered the prediction scores of the models"
  - [corpus] Weak - corpus mentions grey-box attacks but not sentence-level coherence attacks
- Break condition: If model evaluates semantic coherence or uses sentence-level context beyond simple concatenation.

## Foundational Learning

- Concept: Sentence-level toxicity scoring
  - Why needed here: Attack relies on models scoring entire sentence, not detecting intent
  - Quick check question: Does the model treat "I hate you [positive words]" as one sentence for scoring?

- Concept: Multilingual model architecture
  - Why needed here: Attack exploits weaknesses in multilingual transformer models
  - Quick check question: Can the model detect language boundaries within a single input?

- Concept: Adversarial training principles
  - Why needed here: Defense mechanism uses adversarial training to improve robustness
  - Quick check question: Does training on attacked examples improve detection of both word and sentence attacks?

## Architecture Onboarding

- Component map:
  - Input preprocessor -> Transformer encoder -> Pooling layer -> Classification head
  - Separate pipeline for word-level vs sentence-level attack detection
  - Language detection module (for cross-lingual defense)

- Critical path:
  - Input text -> Tokenization -> Context encoding -> Attention mechanism -> Classification
  - Attack insertion point: After tokenization but before classification

- Design tradeoffs:
  - Word-based attacks: Simpler to generate but more detectable
  - Sentence-based attacks: More sophisticated but require quality positive sentence corpus
  - Cross-lingual: More effective but requires multilingual model support

- Failure signatures:
  - Model output remains above threshold despite attack
  - Language detection fails in cross-lingual scenarios
  - Adversarial training overfits to specific attack patterns

- First 3 experiments:
  1. Test toxicity score reduction with 1-5 positive words appended to "I hate you"
  2. Measure effectiveness of cross-lingual attack (English hate + Russian positive words)
  3. Evaluate adversarial training by testing on both word and sentence attacks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the sentence-based attack on toxicity detection models when the positive sentences are semantically coherent with the toxic message rather than just grammatically correct?
- Basis in paper: [explicit] The paper mentions that the sentence-based attack uses sentences from the Stanford Sentiment Treebank with high positivity scores, but it does not explore the impact of semantic coherence between the toxic message and the positive sentences.
- Why unresolved: The paper only tests the sentence-based attack with sentences that are grammatically correct but may not be semantically related to the toxic message. This leaves open the question of whether the attack would be more or less effective if the positive sentences were semantically coherent with the toxic message.
- What evidence would resolve it: Experiments comparing the effectiveness of the sentence-based attack using semantically coherent positive sentences versus grammatically correct but semantically unrelated positive sentences would resolve this question.

### Open Question 2
- Question: How does the proposed attack perform on toxicity detection models that consider context or intention of messages, rather than just sentence-level labels?
- Basis in paper: [explicit] The paper states that the attack exploits the fact that toxicity detection models are trained on sentence-level labels and do not consider the context or intention of the message. However, it does not test the attack on models that do consider context or intention.
- Why unresolved: The paper only tests the attack on models that are known to rely on sentence-level labels, leaving open the question of how effective the attack would be on models that consider context or intention.
- What evidence would resolve it: Experiments testing the attack on models that consider context or intention of messages would resolve this question.

### Open Question 3
- Question: How does the proposed attack perform on toxicity detection models that are specifically designed to handle multilingual texts?
- Basis in paper: [explicit] The paper tests the attack on multilingual models and cross-lingual settings, but it does not explore how the attack performs on models that are specifically designed to handle multilingual texts.
- Why unresolved: The paper only tests the attack on multilingual models that may not be specifically designed for multilingual text handling, leaving open the question of how effective the attack would be on models that are specifically designed for this purpose.
- What evidence would resolve it: Experiments testing the attack on models that are specifically designed to handle multilingual texts would resolve this question.

### Open Question 4
- Question: How effective are the proposed defence mechanisms against more sophisticated attack variants, such as those that involve semantically coherent positive sentences or context-aware perturbations?
- Basis in paper: [explicit] The paper proposes a simple defence mechanism using adversarial training, but it only tests this defence against the word- and sentence-based attacks described in the paper. It does not explore how effective the defence would be against more sophisticated attack variants.
- Why unresolved: The paper only tests the defence against the specific attack variants described, leaving open the question of how effective the defence would be against more sophisticated attack variants.
- What evidence would resolve it: Experiments testing the proposed defence mechanisms against more sophisticated attack variants would resolve this question.

## Limitations
- The attack relies heavily on the assumption that toxicity detection models treat entire sentences as monolithic units without contextual understanding, which may not hold for more advanced models with contextual awareness.
- Cross-lingual attack effectiveness lacks empirical validation, as the paper doesn't provide specific corpus evidence for mixed-language scenarios.
- The proposed defense mechanism using adversarial training shows promising results but may lead to overfitting on specific attack patterns, potentially reducing generalization to unseen attack variants.

## Confidence
- High confidence: The core mechanism of using positive words to lower toxicity scores works reliably across multiple languages and transformer models, as demonstrated by consistent score reduction below the 0.5 threshold.
- Medium confidence: The sentence-level attack approach shows effectiveness, but the claim that it's "harder to detect" than word-based attacks needs more rigorous validation against language model anomaly detection systems.
- Low confidence: Cross-lingual attack claims lack sufficient empirical support, and the defense mechanism's robustness against evolving attack strategies remains uncertain.

## Next Checks
1. Test attack effectiveness on models with explicit context-awareness features (e.g., models that separate message intent from content) to validate the core assumption about sentence-level processing.
2. Conduct a systematic evaluation of cross-lingual attack effectiveness using controlled language-mixing scenarios and measure model performance degradation.
3. Evaluate the adversarial training defense against novel attack patterns not seen during training to assess generalization and prevent overfitting.