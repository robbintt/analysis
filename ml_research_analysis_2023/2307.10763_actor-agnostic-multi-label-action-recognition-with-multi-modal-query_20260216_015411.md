---
ver: rpa2
title: Actor-agnostic Multi-label Action Recognition with Multi-modal Query
arxiv_id: '2307.10763'
source_url: https://arxiv.org/abs/2307.10763
tags:
- video
- action
- msqnet
- multi-modal
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MSQNet, an actor-agnostic multi-label action
  recognition model that leverages both visual and textual modalities. By treating
  action recognition as a multi-modal target detection task within a transformer decoder
  framework, MSQNet eliminates the need for actor-specific pose estimation.
---

# Actor-agnostic Multi-label Action Recognition with Multi-modal Query

## Quick Facts
- **arXiv ID**: 2307.10763
- **Source URL**: https://arxiv.org/abs/2307.10763
- **Authors**: 
- **Reference count**: 40
- **Primary result**: MSQNet achieves up to 50% improvement over actor-specific alternatives in multi-label action recognition

## Executive Summary
This paper introduces MSQNet, an actor-agnostic multi-label action recognition model that leverages both visual and textual modalities. By treating action recognition as a multi-modal target detection task within a transformer decoder framework, MSQNet eliminates the need for actor-specific pose estimation. It employs a pretrained vision-language model to create multi-modal semantic queries, enabling a richer representation of action classes and improved generalization across different actors (humans and animals). Extensive experiments on five benchmarks show that MSQNet outperforms prior actor-specific alternatives by up to 50% in both single-label and multi-label settings, including zero-shot learning scenarios.

## Method Summary
MSQNet reformulates multi-label action classification as a multi-modal target detection task. The model uses a transformer decoder to pool action-specific features from spatio-temporal video representations. Multi-modal queries are created by fusing learnable label embeddings with video-specific embeddings from a pretrained vision-language model (CLIP). The decoder performs self-attention and cross-attention to combine video encodings for multi-label classification. The model is trained using cross-entropy loss for single-label tasks and binary cross-entropy loss for multi-label tasks.

## Key Results
- MSQNet achieves up to 50% improvement over actor-specific alternatives
- Outperforms prior methods on five benchmarks including Thumos14, Hockey, Charades, Animal Kingdom, and HMDB51
- Demonstrates strong zero-shot learning capabilities through semantic understanding of action classes
- Shows consistent performance across both human and animal action recognition tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal semantic query learning enables richer action class representations by fusing visual and textual modalities.
- Mechanism: The MSQNet model combines pretrained vision-language model embeddings (CLIP) with learnable label embeddings, allowing the transformer decoder to attend to both visual features and semantic text descriptions of actions. This dual-channel input provides more discriminative features for action classification.
- Core assumption: Visual and textual modalities contain complementary information that improves action recognition when properly fused.
- Evidence anchors:
  - [abstract] "leveraging visual and textual modalities to represent the action classes better"
  - [section 3.2] "we construct multi-modal query for our Transformer decoder network. The multi-modal query is formed by fusing the learnable label embedding and video-specific embedding."
  - [corpus] Weak evidence - no direct corpus papers mention this specific fusion approach
- Break condition: If either modality contains misleading or contradictory information that confuses the model rather than complementing each other.

### Mechanism 2
- Claim: Reformulating multi-label action classification as a multi-modal target detection task eliminates the need for actor-specific pose estimation.
- Mechanism: By treating action recognition as a specialized form of object detection using DETR framework, MSQNet learns to identify action "objects" in video sequences without requiring explicit actor pose information. The transformer decoder learns spatial-temporal attention patterns directly from video features.
- Core assumption: Action recognition can be framed as a detection problem where actions are the "objects" to detect, and this framing is more general than actor-specific approaches.
- Evidence anchors:
  - [abstract] "casts the multi-label action classification problem into a multi-modal target detection task"
  - [section 3.3] "We use the standard Transformer architecture... to pool action-specific features from the spatio-temporal video representation"
  - [corpus] Weak evidence - no corpus papers directly support this reformulation approach
- Break condition: If the detection formulation fails to capture temporal dynamics that are crucial for action recognition but not present in static object detection.

### Mechanism 3
- Claim: Pretrained vision-language models provide strong zero-shot capabilities through semantic understanding of action classes.
- Mechanism: By initializing label embeddings with CLIP text embeddings and incorporating video embeddings, the model can recognize actions it hasn't explicitly trained on by matching semantic similarity between text descriptions and visual patterns.
- Core assumption: CLIP's pretraining on massive image-text pairs captures general semantic relationships that transfer to action recognition tasks.
- Evidence anchors:
  - [abstract] "incorporating general textual embedding enables the model to exhibit zero-shot capabilities"
  - [section 3.2] "we initialize Ql with the text embeddings of the corresponding classes... using a pretrained text encoder... CLIP model"
  - [corpus] Weak evidence - no corpus papers mention zero-shot capabilities for this specific approach
- Break condition: If the semantic space learned by CLIP doesn't align well with action semantics, leading to poor zero-shot performance.

## Foundational Learning

- Concept: Multi-modal learning fundamentals
  - Why needed here: MSQNet explicitly combines visual and textual information streams, requiring understanding of how different modalities complement each other
  - Quick check question: What are the advantages and challenges of combining visual and textual information in neural networks?

- Concept: Transformer architectures and attention mechanisms
  - Why needed here: The model uses a transformer decoder with self-attention and cross-attention to fuse video features with semantic queries
  - Quick check question: How do self-attention and cross-attention differ in transformer architectures, and when would you use each?

- Concept: Object detection frameworks (DETR)
  - Why needed here: MSQNet reformulates action recognition as a detection problem using DETR-like architecture
  - Quick check question: What are the key differences between traditional object detection and DETR, and how does this affect training and inference?

## Architecture Onboarding

- Component map: Video frames → Video Encoder → Multi-modal Query Encoder → Multi-modal Decoder → Projection → Output probabilities

- Critical path: Video frames → Video Encoder → Multi-modal Query Encoder → Multi-modal Decoder → Projection → Output probabilities

- Design tradeoffs:
  - Using CLIP for text embeddings provides strong semantic understanding but adds dependency on external model
  - DETR-based formulation eliminates actor-specific design but may be less efficient than direct classification
  - Multi-modal fusion improves accuracy but increases model complexity and training time

- Failure signatures:
  - Poor attention visualization in decoder layers (attention focusing on irrelevant regions)
  - Low confidence scores across all classes (model uncertainty)
  - Large gap between training and validation performance (overfitting)
  - Degraded performance on animal actions vs human actions (actor-specific bias)

- First 3 experiments:
  1. Ablation study: Remove text embeddings to verify multi-modal contribution (expect 10-15% performance drop)
  2. Attention visualization: Examine decoder attention maps on sample videos to verify meaningful focus regions
  3. Zero-shot evaluation: Test on unseen action classes using text embeddings only to validate semantic understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the model perform if it were extended to include audio modality alongside visual and textual modalities?
- Basis in paper: [explicit] The paper discusses leveraging visual and textual modalities, but suggests future work could explore integrating additional modalities like audio.
- Why unresolved: The current model is designed for visual and textual modalities only, with no experimentation on audio integration.
- What evidence would resolve it: Comparative performance results on the same benchmarks when incorporating audio features alongside visual and textual data.

### Open Question 2
- Question: What is the optimal number of frames for different action recognition tasks, and how does this vary between human and animal actions?
- Basis in paper: [explicit] The paper experimented with 8, 10, and 16 frames, finding 16 frames optimal, but suggests this might vary by task.
- Why unresolved: The study only tested a limited range of frame counts and didn't explore variations across different types of actions.
- What evidence would resolve it: Systematic experiments varying frame counts across multiple datasets with different action types, measuring performance trade-offs.

### Open Question 3
- Question: How does the model's performance scale with increasing numbers of action classes in the zero-shot learning setting?
- Basis in paper: [explicit] The paper demonstrates zero-shot capabilities but doesn't explore how performance degrades as the number of target classes increases.
- Why unresolved: The zero-shot experiments used fixed splits without varying the number of target classes systematically.
- What evidence would resolve it: Performance metrics across multiple zero-shot splits with varying ratios of seen to unseen classes, showing the scaling relationship.

## Limitations

- Architectural details for the video encoder (layer count, attention heads, embedding dimensions) are not fully specified
- Performance contribution of each component (multi-modal queries, DETR formulation, CLIP embeddings) is not clearly isolated through ablation studies
- Limited evaluation on cross-dataset transfer scenarios to verify generalizability beyond the five benchmarks tested

## Confidence

- **High Confidence**: The core conceptual contribution of actor-agnostic action recognition using multi-modal semantic queries is well-supported and logically sound.
- **Medium Confidence**: The performance claims on benchmark datasets are credible but would benefit from more detailed ablation studies and component-wise analysis.
- **Low Confidence**: The zero-shot learning capabilities are mentioned but not extensively validated with systematic experiments on truly unseen action classes.

## Next Checks

1. **Ablation Study**: Remove text embeddings from MSQNet and retrain to quantify the specific contribution of multi-modal queries to overall performance.

2. **Cross-Actor Generalization**: Test MSQNet on a mixed human-animal dataset not seen during training to verify true actor-agnostic capabilities.

3. **Component Analysis**: Implement and compare simplified variants: (a) MSQNet without CLIP initialization, (b) MSQNet with actor-specific pose estimation, (c) MSQNet without DETR formulation to isolate architectural contributions.