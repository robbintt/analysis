---
ver: rpa2
title: 'TIES-Merging: Resolving Interference When Merging Models'
arxiv_id: '2306.01708'
source_url: https://arxiv.org/abs/2306.01708
tags:
- merging
- task
- sign
- parameters
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses interference in model merging when combining
  task-specific models into a single multitask model. The authors identify two major
  sources of interference: redundant parameter values and sign disagreements across
  models.'
---

# TIES-Merging: Resolving Interference When Merging Models

## Quick Facts
- arXiv ID: 2306.01708
- Source URL: https://arxiv.org/abs/2306.01708
- Reference count: 40
- Key outcome: TIES-Merging addresses interference in model merging by trimming redundant parameters and resolving sign conflicts, improving multitask model performance across diverse settings.

## Executive Summary
This paper addresses interference in model merging when combining task-specific models into a single multitask model. The authors identify two major sources of interference: redundant parameter values and sign disagreements across models. They propose TIES-Merging, which addresses these issues through three steps: (1) trimming redundant parameters by resetting low-magnitude changes to zero, (2) resolving sign conflicts by electing a final sign for each parameter, and (3) performing a disjoint merge using only parameters aligned with the elected sign. The method is evaluated across diverse settings including different modalities (language and vision), model sizes, domains, and fine-tuning approaches.

## Method Summary
TIES-Merging operates on task vectors derived from fine-tuned models, computing the difference between fine-tuned and pre-trained parameters. The method proceeds in three steps: First, it trims redundant parameters by resetting low-magnitude changes to zero, keeping only the top-k% magnitudes. Second, it resolves sign conflicts by electing a final sign for each parameter based on the direction with the largest total movement across models. Third, it performs a disjoint merge by averaging only parameters whose signs match the elected sign. The final merged model is created by scaling the merged task vectors and adding them to the initialization. Hyperparameters include k (trimming threshold, typically 20%) and λ (scaling factor, typically 1).

## Key Results
- TIES-Merging outperforms existing methods like Task Arithmetic, Fisher Merging, and RegMean, showing average improvements of 2.3% and 1.7% absolute for in-domain tasks in NLP and vision settings respectively.
- For out-of-domain generalization, TIES-Merging shows improvements of 1.0% and 4.4% absolute for T5-base and T5-large models.
- The method scales better as the number of tasks increases and can achieve near-multitask performance when given access to an oracle sign vector.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interference arises from redundant parameter changes during fine-tuning.
- Mechanism: Many parameters change only slightly during fine-tuning, adding noise rather than useful signal. By resetting these small-magnitude task vector values to zero (trimming), TIES-Merging removes low-signal contributions that obscure the true influence of task-specific parameters.
- Core assumption: Parameters with small-magnitude changes during fine-tuning are not critical for task performance.
- Evidence anchors:
  - [section] "Fig. 3 shows the average performance across varying values of k, demonstrating that keeping only the top-20% of values delivers comparable results to retaining all parameters."
  - [section] "This shows that many parameter changes introduced during fine-tuning are redundant."
- Break condition: If small-magnitude changes turn out to be critical for certain tasks or modalities, trimming could degrade performance.

### Mechanism 2
- Claim: Sign conflicts between models cause parameter values to cancel out when averaged.
- Mechanism: When the same parameter moves in opposite directions across different task models, simple averaging reduces its magnitude. TIES-Merging resolves this by electing a final sign for each parameter based on the direction with the largest total movement across models, then merging only parameters aligned with that sign.
- Core assumption: The sign of a parameter change corresponds to the direction of influence on task performance.
- Evidence anchors:
  - [section] "A given parameter might have a positive value for some models and a negative value for others. Consequently, employing simple averaging might compromise the performance on both tasks."
  - [section] "We then assign γp m as the sign with greater total movement."
- Break condition: If the largest movement doesn't correspond to the correct direction for some tasks, this could harm performance.

### Mechanism 3
- Claim: Disjoint merging preserves parameter magnitudes by avoiding cancellation.
- Mechanism: Instead of averaging all parameter values, TIES-Merging only averages values from models whose sign matches the elected sign. This prevents cancellation and preserves the influential magnitude of parameters.
- Core assumption: Parameters with the same sign across models are contributing positively to the merged model's performance.
- Evidence anchors:
  - [section] "we compute a disjoint mean by only keeping the parameter values from the models whose signs are the same as the aggregated elected sign and calculate their mean."
  - [section] "Fig. 6b, where we demonstrate that the ELECT step preserves the relative parameter magnitudes to avoid sign interference."
- Break condition: If important information is encoded in the magnitude differences between models with opposite signs, this approach could lose that information.

## Foundational Learning

- Concept: Task vectors and parameter-efficient fine-tuning (PEFT)
  - Why needed here: TIES-Merging operates on task vectors derived from fine-tuned models, and understanding PEFT is crucial for grasping how parameter changes are captured and merged.
  - Quick check question: What is the difference between a task vector and a fine-tuned model's parameters?

- Concept: Model merging and interference
  - Why needed here: The paper's contribution hinges on understanding how interference occurs in model merging and how to mitigate it.
  - Quick check question: What are the two main sources of interference identified in the paper?

- Concept: Sign resolution and disjoint averaging
  - Why needed here: These are the core innovations of TIES-Merging, distinguishing it from simple averaging approaches.
  - Quick check question: How does TIES-Merging's "elect sign" step differ from simply averaging all parameter values?

## Architecture Onboarding

- Component map: Fine-tuned models -> Task vector computation -> Trimming -> Sign election -> Disjoint merging -> Merged model
- Critical path:
  1. Compute task vectors from fine-tuned models
  2. Trim task vectors (reset low-magnitude changes to zero)
  3. Elect final sign vector by resolving conflicts
  4. Perform disjoint merge using only aligned-sign parameters
  5. Scale and add to initialization to get final merged model

- Design tradeoffs:
  - Choosing k: Higher k retains more parameters but may include more interference; lower k is more aggressive but might remove useful information
  - Sign resolution: Electing the sign with largest total movement might not always be optimal for all tasks
  - Disjoint merging: Preserves magnitude but could lose information encoded in differences between opposite-sign values

- Failure signatures:
  - Performance degrades when k is too low (removes critical parameters)
  - Performance degrades when λ is incorrectly scaled
  - Method fails to improve over baselines when sign conflicts are minimal or redundant parameters are rare

- First 3 experiments:
  1. Implement basic task vector computation and simple averaging merge for comparison
  2. Add trimming step with different k values to find optimal threshold
  3. Implement sign election and disjoint merging, comparing performance against baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the trimming threshold and scaling parameter interact across different model architectures and fine-tuning settings?
- Basis in paper: [explicit] The paper mentions using k=20% and λ=1 for TIES-Merging but also states these were chosen based on PEFT experiments and could vary by setting.
- Why unresolved: The paper only provides specific hyperparameter values for certain experimental conditions and doesn't explore how optimal values might change across different architectures, model sizes, or fine-tuning approaches.
- What evidence would resolve it: A systematic hyperparameter sweep across different model architectures (T5, ViT, etc.), model sizes (base vs large), and fine-tuning methods (full vs PEFT) showing how performance varies with different k and λ values.

### Open Question 2
- Question: Can the oracle sign vector be reliably estimated without expensive multitask training, and what is the minimum amount of data needed?
- Basis in paper: [explicit] Section 7.4 shows that using the multitask sign vector improves performance by 5.6%, and Section A.1 demonstrates that few-shot multitask training (32 samples) can estimate this vector with 3.8% improvement.
- Why unresolved: The paper only tests one approach (few-shot multitask training) with a fixed small sample size, and doesn't explore whether other methods or different sample sizes might be more effective or efficient.
- What evidence would resolve it: Comparative studies of different methods for estimating the sign vector (e.g., meta-learning, zero-shot prompting) with varying amounts of data, showing trade-offs between data efficiency and sign vector accuracy.

### Open Question 3
- Question: How does TIES-Merging perform when merging models trained on heterogeneous task distributions or incompatible architectures?
- Basis in paper: [inferred] The paper demonstrates effectiveness across different modalities and model sizes but doesn't test scenarios where tasks are fundamentally different in nature or where architectures have incompatible parameter spaces.
- Why unresolved: All experiments involve models trained on related tasks within the same modality, and the merging process assumes compatible parameter spaces between models.
- What evidence would resolve it: Experiments merging models across fundamentally different task types (e.g., language and vision tasks), or models with incompatible architectures (e.g., transformers and CNNs), showing whether the method can be adapted or extended to handle such cases.

## Limitations
- Core Assumption Risks: The paper's approach relies on the assumption that small-magnitude parameter changes during fine-tuning are redundant and can be safely removed.
- Sign Resolution Ambiguity: The method elects signs based on total movement magnitude, but this heuristic may not always capture the optimal configuration for all tasks.
- Trim Threshold Sensitivity: The choice of k=20% as the optimal trimming threshold was determined empirically, but this may not be universally optimal across different model architectures, fine-tuning approaches, or task distributions.

## Confidence
- High Confidence: The identification of redundant parameters and sign interference as sources of merging problems is well-supported by empirical observations.
- Medium Confidence: The specific claim that TIES-Merging outperforms existing methods by 2.3% and 1.7% absolute on average for in-domain tasks is well-supported by the experiments.
- Low Confidence: The generalization claims for out-of-domain tasks (1.0% and 4.4% absolute improvements) are based on limited experimental setups.

## Next Checks
1. **Trim Threshold Sensitivity Analysis:** Systematically evaluate TIES-Merging performance across a wider range of k values (5%, 10%, 30%, 40%) on additional task combinations to determine if the 20% threshold is truly optimal or task-specific.

2. **Alternative Sign Resolution Methods:** Implement and compare alternative sign election strategies (e.g., voting-based approaches, task-weighted movements) to assess whether the current largest-movement heuristic is optimal or if other methods could yield better performance.

3. **Cross-Architecture Generalization:** Test TIES-Merging on architectures beyond T5 and ViT (such as BERT, ResNet, or transformer-based vision models) with different fine-tuning methods (adapter-based, LoRA, etc.) to validate the method's generalizability beyond the reported experimental settings.