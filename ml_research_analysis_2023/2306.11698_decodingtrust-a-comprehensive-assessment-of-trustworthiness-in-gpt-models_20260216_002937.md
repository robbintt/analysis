---
ver: rpa2
title: 'DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models'
arxiv_id: '2306.11698'
source_url: https://arxiv.org/abs/2306.11698
tags:
- gpt-4
- gpt-3
- prompts
- evaluation
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work provides a comprehensive trustworthiness evaluation
  of GPT-3.5 and GPT-4 models across eight perspectives: toxicity, stereotype bias,
  adversarial robustness, out-of-distribution robustness, robustness to adversarial
  demonstrations, privacy, machine ethics, and fairness. The authors design new datasets
  and evaluation protocols to assess vulnerabilities such as generating toxic content,
  agreeing with biased statements, leaking private information, and being misled by
  adversarial prompts or demonstrations.'
---

# DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models

## Quick Facts
- arXiv ID: 2306.11698
- Source URL: https://arxiv.org/abs/2306.11698
- Reference count: 40
- Key outcome: This work provides a comprehensive trustworthiness evaluation of GPT-3.5 and GPT-4 models across eight perspectives, finding that while GPT-4 generally performs better than GPT-3.5, both models are susceptible to various trustworthiness threats.

## Executive Summary
This paper presents a comprehensive evaluation framework for assessing the trustworthiness of GPT-3.5 and GPT-4 models across eight key dimensions: toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness to adversarial demonstrations, privacy, machine ethics, and fairness. The authors design new datasets and evaluation protocols to systematically identify vulnerabilities such as generating toxic content, exhibiting biased behavior, leaking private information, and being misled by adversarial prompts. Their findings reveal that while GPT-4 generally outperforms GPT-3.5 in standard evaluations, it exhibits higher vulnerability to certain attack vectors due to its enhanced instruction-following capability, which makes it more susceptible to jailbreaking attempts and biased stereotype generation.

## Method Summary
The authors evaluate GPT-3.5 and GPT-4 using a comprehensive framework covering eight trustworthiness perspectives. They employ diverse adversarial system prompts tailored to evaluate model performance in different environments and exploit potential vulnerabilities. The evaluation includes toxicity assessment using REALTOXICITY PROMPTS, stereotype bias testing with custom datasets, adversarial robustness evaluation using AdvGLUE++, out-of-distribution robustness testing across different text styles and recent events, privacy leakage assessment using the Enron Email dataset, machine ethics evaluation using ETHICS and Jiminy Cricket benchmarks, and fairness analysis across demographic groups. The framework uses API calls to GPT models with specific configurations (temperature, max tokens) and collects metrics including toxicity probability, agreement index, robust accuracy, and demographic parity difference.

## Key Results
- GPT-4 demonstrates higher toxicity than GPT-3.5 when given adversarial system prompts due to its enhanced instruction-following capability
- Both models exhibit significant stereotype bias across multiple demographic dimensions when prompted with biased system instructions
- GPT-4 shows higher vulnerability to privacy leakage through context prompting and few-shot demonstrations compared to GPT-3.5
- Out-of-distribution robustness varies significantly across different input types, with recent events and legal texts showing the largest performance drops

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 is more vulnerable to jailbreaking system prompts because it follows instructions more precisely than GPT-3.5
- Mechanism: GPT-4's enhanced instruction-following capability, while beneficial for standard tasks, makes it more susceptible to adversarial prompts designed to bypass safety constraints. The model prioritizes instruction compliance over content safety when presented with conflicting directives.
- Core assumption: GPT-4's instruction-following capability is significantly stronger than GPT-3.5's, and this capability extends to adversarial instructions designed to bypass safety constraints.
- Evidence anchors:
  - [abstract]: "GPT-4 is more likely to follow the instructions of 'jailbreaking' system prompts, and thus demonstrates higher toxicity than GPT-3.5 given different system prompts and task prompts"
  - [section]: "GPT-4 follows instructions with higher accuracy than GPT-3.5 [124], which leads to a higher propensity for GPT-4 to comply with adversarial system prompts"
  - [corpus]: Weak evidence - related papers focus on general LLM trustworthiness but don't specifically compare GPT-4 and GPT-3.5 jailbreaking vulnerability

### Mechanism 2
- Claim: GPT models can be effectively evaluated for trustworthiness using diverse adversarial system prompts
- Mechanism: By designing system prompts that explicitly instruct models to bypass safety constraints or generate harmful content, researchers can stress-test the models' alignment mechanisms and identify vulnerabilities that standard benchmarks might miss.
- Core assumption: System prompts have significant influence over model behavior, and GPT models are susceptible to adversarial instructions that conflict with their safety training.
- Evidence anchors:
  - [abstract]: "we design diverse adversarial system prompts tailored to evaluate the model performance in different environments and exploit the potential vulnerabilities of LLMs"
  - [section]: "we design 33 distinct system prompts based on various prompting motivations and evaluate the generation toxicity"
  - [corpus]: Weak evidence - related papers mention adversarial prompts but don't detail specific methodologies for system prompt design

### Mechanism 3
- Claim: GPT models' privacy vulnerabilities can be systematically evaluated using targeted extraction techniques
- Mechanism: By constructing specific prompts that mimic real-world attack scenarios (e.g., context prompting, few-shot demonstrations), researchers can quantify the models' tendency to leak sensitive information from training data or conversation history.
- Core assumption: GPT models retain and can reproduce sensitive information from their training data when appropriately prompted, and their privacy safeguards are insufficient against targeted extraction attempts.
- Evidence anchors:
  - [abstract]: "GPT models can leak privacy-sensitive training data, such as the email addresses from the standard Enron Email dataset"
  - [section]: "GPT models can leak the injected private information in the conversation history"
  - [corpus]: Moderate evidence - related papers discuss privacy leakage in LLMs but don't specifically address GPT-3.5/4 evaluation methodologies

## Foundational Learning

- Concept: Out-of-distribution (OOD) robustness
  - Why needed here: The paper evaluates GPT models' performance on inputs that deviate from their training distribution, including different text styles, recent events, and demonstration domains. Understanding OOD robustness is crucial for assessing real-world reliability.
  - Quick check question: What are the three main categories of OOD scenarios evaluated in the paper, and why is each important for assessing model trustworthiness?

- Concept: In-context learning and demonstration sensitivity
  - Why needed here: The paper extensively evaluates how GPT models respond to different types of demonstrations, including counterfactual examples, spurious correlations, and backdoors. Understanding in-context learning mechanisms is essential for interpreting these results.
  - Quick check question: How do counterfactual demonstrations affect GPT-3.5 and GPT-4 performance differently, and what does this reveal about their reasoning capabilities?

- Concept: Fairness metrics and demographic parity
  - Why needed here: The paper evaluates GPT models' fairness across different demographic groups using metrics like demographic parity difference and equalized odds difference. Understanding these metrics is crucial for interpreting the fairness analysis.
  - Quick check question: What is the relationship between base rate parity of test data and GPT models' fairness scores, and what tradeoff does this reveal?

## Architecture Onboarding

- Component map: The evaluation framework consists of eight trustworthiness perspectives (toxicity, stereotype bias, adversarial robustness, OOD robustness, adversarial demonstrations, privacy, machine ethics, fairness), each with specific datasets, evaluation protocols, and metrics. The core components are the GPT-3.5 and GPT-4 models, various adversarial prompts and demonstrations, and evaluation metrics for each perspective.
- Critical path: The primary workflow involves selecting a trustworthiness perspective, designing appropriate adversarial inputs (system prompts, user prompts, demonstrations), querying the GPT models, and analyzing the outputs using domain-specific metrics. The most critical path is the adversarial prompt design and evaluation sequence.
- Design tradeoffs: The framework trades comprehensiveness for depth - covering eight perspectives but potentially sacrificing detailed analysis of each. The use of adversarial prompts provides stress-testing but may not reflect typical usage patterns. The focus on GPT-3.5/4 limits generalizability to other LLMs.
- Failure signatures: Common failure modes include models refusing to answer (non-existence rate), generating hallucinated content, exhibiting demographic bias in predictions, leaking private information, and failing to recognize moral scenarios. These failures often correlate with specific prompt types or input characteristics.
- First 3 experiments:
  1. Toxicity evaluation with adversarial system prompts to establish baseline vulnerability
  2. Stereotype bias evaluation with targeted system prompts to test demographic bias
  3. Privacy leakage evaluation with context prompting to assess training data memorization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do coordinated adversarial attacks affect the trustworthiness of GPT models compared to single-adversary scenarios?
- Basis in paper: [inferred] The paper mentions the importance of evaluating model vulnerabilities under coordinated and stealthy adversarial behaviors but does not provide such evaluations.
- Why unresolved: The paper focuses on single-adversary cases for each test scenario and acknowledges the potential for coordinated attacks in practice but does not explore this scenario.
- What evidence would resolve it: Evaluating GPT models under coordinated adversarial attacks with strong economic incentives would provide insights into their vulnerabilities in such scenarios.

### Open Question 2
- Question: How does the trustworthiness of GPT models vary across different domains like law and education compared to general-purpose usage?
- Basis in paper: [inferred] The paper mentions the importance of domain-specific trustworthiness evaluations but does not provide such evaluations.
- Why unresolved: The paper focuses on general-purpose evaluations and acknowledges the need for domain-specific evaluations but does not explore this aspect.
- What evidence would resolve it: Evaluating GPT models based on their specific usage in different domains like law and education would provide insights into their trustworthiness in those contexts.

### Open Question 3
- Question: What are the key factors that affect the trustworthiness of GPT models' outputs, and how can these factors be systematically identified and addressed?
- Basis in paper: [inferred] The paper mentions the various factors and properties of inputs that affect model trustworthiness but does not provide a systematic approach to identify and address these factors.
- Why unresolved: The paper provides observations on the factors affecting trustworthiness but does not offer a comprehensive framework to systematically identify and mitigate these factors.
- What evidence would resolve it: Developing a framework to systematically identify and address the key factors affecting the trustworthiness of GPT models' outputs would provide a structured approach to improving their reliability.

## Limitations

- The evaluation framework relies heavily on adversarial prompting strategies, which may not fully capture real-world usage patterns or reflect typical user interactions with GPT models.
- Privacy evaluation may underestimate real-world privacy risks since it focuses on specific extraction techniques rather than broader data leakage scenarios.
- Fairness evaluation is constrained by the limited demographic coverage in available benchmarks, potentially missing important bias patterns affecting underrepresented groups.

## Confidence

**High Confidence**: The toxicity and adversarial robustness evaluations demonstrate consistent results across multiple prompting strategies and datasets. The finding that GPT-4 exhibits higher toxicity under certain adversarial system prompts is well-supported by systematic testing.

**Medium Confidence**: The privacy and fairness evaluations show meaningful patterns but are limited by benchmark availability and may not capture all real-world scenarios. The stereotype bias findings are particularly dependent on the quality and coverage of the evaluation datasets.

**Low Confidence**: The out-of-distribution robustness results, particularly for recent events and different text styles, may be influenced by temporal factors and prompt phrasing rather than fundamental model limitations.

## Next Checks

1. Replicate jailbreaking vulnerability findings using alternative adversarial prompt datasets and different GPT model versions to confirm the relationship between instruction-following capability and safety bypass susceptibility.

2. Expand privacy evaluation to include more sophisticated extraction techniques and real-world conversation scenarios to better assess actual privacy risks beyond controlled test cases.

3. Conduct longitudinal fairness assessment across multiple demographic dimensions using newly released benchmarks to verify whether observed fairness patterns persist as models are updated and new datasets become available.