---
ver: rpa2
title: Guaranteed Approximation Bounds for Mixed-Precision Neural Operators
arxiv_id: '2307.15034'
source_url: https://arxiv.org/abs/2307.15034
tags:
- precision
- training
- full
- neural
- half
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a mixed-precision training approach for Fourier
  Neural Operators (FNO) to address memory and speed bottlenecks in high-resolution
  PDE problems. The method uses half-precision computations for tensor contractions,
  optimized by finding the optimal contraction order.
---

# Guaranteed Approximation Bounds for Mixed-Precision Neural Operators

## Quick Facts
- arXiv ID: 2307.15034
- Source URL: https://arxiv.org/abs/2307.15034
- Reference count: 40
- This paper introduces mixed-precision training for Fourier Neural Operators to improve memory and runtime efficiency while maintaining accuracy guarantees.

## Executive Summary
This paper addresses memory and speed bottlenecks in Fourier Neural Operators (FNO) for high-resolution PDE problems by introducing a mixed-precision training approach. The method uses half-precision computations for tensor contractions, optimized by finding the optimal contraction order. Numerical stability is ensured through tanh pre-activation before the Fourier transform and frequency mode truncation. Experiments on Navier-Stokes and Darcy flow datasets show up to 34% improvement in runtime and memory usage with little to no reduction in accuracy.

## Method Summary
The method introduces mixed-precision training for FNO by casting input tensors to half precision and applying tanh pre-activation before FFT operations to ensure numerical stability. High-frequency modes are truncated to reduce half-precision error, as these modes suffer disproportionately from low-precision representation. A precision schedule is employed where training begins in half precision, transitions to mixed precision, and finishes in full precision. Tensor contractions are optimized using einsum path optimization to maximize computational efficiency.

## Key Results
- Up to 34% improvement in runtime and memory usage compared to full-precision FNO
- Little to no reduction in accuracy (H1 and L2 losses) on Navier-Stokes and Darcy flow datasets
- Precision schedule achieves 10% better error than full precision training on Navier-Stokes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-activation with tanh before the Fourier transform mitigates numerical instability in half-precision FNO.
- Mechanism: The tanh function forces all values into the range [-1, 1], preventing overflow/underflow in the FFT while preserving trends near the mean.
- Core assumption: Half-precision arithmetic has insufficient dynamic range to represent extreme values that occur in FNO without clipping.
- Evidence anchors: [abstract] "numerical stability is ensured through tanh pre-activation before the Fourier transform"; [section] "We observe that only adding a pre-activation before the first FFT still results in overflow in the next FFT. Therefore, we add a pre-activation function before every forward FFT in the architecture."
- Break condition: If the input distribution has extremely large outliers, even tanh clipping may lose too much information, leading to poor approximation.

### Mechanism 2
- Claim: Truncating high-frequency modes reduces half-precision error because higher frequencies suffer disproportionately from low-precision representation.
- Mechanism: Half-precision error increases with frequency magnitude; by removing high-frequency modes, the model avoids the most error-prone components.
- Core assumption: In real-world signals, energy is concentrated in low frequencies, so removing high frequencies has minimal impact on accuracy.
- Evidence anchors: [section] "We also show that the range of half precision is too small to learn high frequency modes, and therefore, reducing the learnable frequency modes also helps performance."; [section] "We find that the percentage error exponentially increases. Since in real-world data, the energy is concentrated in the lowest frequency modes, and the higher frequency modes are truncated, this gives further justification for our half precision method."
- Break condition: If the target function has significant high-frequency content, truncating these modes will degrade accuracy regardless of precision.

### Mechanism 3
- Claim: Mixed-precision training with a precision schedule improves accuracy by allowing the model to stabilize in full precision after initial learning in half precision.
- Mechanism: Early training in half precision accelerates convergence and reduces memory, while switching to full precision later captures fine details.
- Core assumption: The bulk of learning can occur in half precision, and full precision is only needed for fine-tuning near convergence.
- Evidence anchors: [abstract] "we also propose a precision schedule training routine, which converts the FNO block from half, to mixed, to full precision throughout training."; [section] "the precision schedule converges to an error that is 10% better than full precision, due to its better anytime performance."
- Break condition: If the learning dynamics require high precision from the start, switching precision may disrupt training.

## Foundational Learning

- Concept: Fourier transform and its discretization
  - Why needed here: FNO relies on FFT for spectral operations; understanding discretization error is crucial for mixed-precision analysis.
  - Quick check question: What is the primary source of error when applying FFT to discrete data in FNO?
- Concept: Mixed-precision training in deep learning
  - Why needed here: The paper adapts mixed-precision techniques from standard neural networks to FNO, requiring knowledge of precision-related stability issues.
  - Quick check question: Why do standard mixed-precision techniques fail for FNO's complex-valued operations?
- Concept: Numerical stability in floating-point arithmetic
  - Why needed here: Half-precision has limited dynamic range, leading to overflow/underflow; understanding this helps explain the need for tanh pre-activation.
  - Quick check question: What is the dynamic range of half-precision compared to full-precision, and why does this matter for FFT?

## Architecture Onboarding

- Component map: Input casting to half precision → tanh pre-activation → FFT (cuFFT) → tensor contraction (half precision) → iFFT → output
- Critical path: Forward pass: input → tanh → FFT → tensor multiplication → iFFT → output; Backward pass: gradients flow through the same path with mixed precision handling
- Design tradeoffs:
  - Memory vs. accuracy: Half precision reduces memory but increases numerical error
  - Speed vs. stability: AMP provides speed but lacks complex support; custom half-precision FFT adds stability
  - Frequency truncation vs. expressiveness: Fewer modes reduce error but limit representational capacity
- Failure signatures:
  - NaNs in early epochs → numerical overflow in FFT
  - Degraded accuracy with many frequency modes → half-precision error accumulation
  - No speedup despite mixed precision → operations not actually running in half precision
- First 3 experiments:
  1. Profile memory and runtime of full-precision vs. mixed-precision FNO on a small dataset
  2. Test tanh pre-activation vs. other pre-activations (hard-clip, 2σ-clip) for numerical stability
  3. Evaluate accuracy vs. number of frequency modes in half precision to find optimal truncation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the precision schedule training routine (AMP+HALF+TANH → FFT in full precision → full precision) consistently outperform full precision training across different datasets and neural operator architectures?
- Basis in paper: [explicit] The authors report that the precision schedule achieved 10% better accuracy than full precision on Navier-Stokes and better performance with the same training time on Darcy flow.
- Why unresolved: The evaluation is limited to two specific datasets (Navier-Stokes and Darcy flow) and one neural operator architecture (FNO). The generalizability to other PDEs, datasets, and neural operator variants remains untested.
- What evidence would resolve it: Comprehensive experiments on diverse datasets (e.g., different types of PDEs, varying resolutions, different physical domains) and other neural operator architectures (e.g., DeepONet, U-FNO) would determine if the precision schedule consistently improves accuracy and efficiency.

### Open Question 2
- Question: What is the optimal frequency mode truncation strategy for mixed-precision FNO to balance accuracy and computational efficiency?
- Basis in paper: [explicit] The authors demonstrate that using too few frequency modes hurts accuracy, while too many increase runtime. They also show that half-precision has higher error for higher frequencies.
- Why unresolved: The paper only tests a limited set of frequency modes (16, 32, 64, 128) and does not explore adaptive truncation strategies based on the specific characteristics of the input data or the learned features.
- What evidence would resolve it: Developing and evaluating adaptive frequency mode truncation methods that dynamically adjust the number of modes based on the input signal characteristics, training progress, or learned features would optimize the trade-off between accuracy and efficiency.

### Open Question 3
- Question: How does the choice of pre-activation function (e.g., tanh, hard-clip, 2σ-clip) affect the stability and performance of mixed-precision FNO beyond the tanh function evaluated in the paper?
- Basis in paper: [explicit] The authors compare tanh, hard-clip, and 2σ-clip pre-activation functions and find that tanh is the most promising in terms of speed and accuracy.
- Why unresolved: The evaluation is limited to three pre-activation functions, and the potential benefits of other functions (e.g., sigmoid, ReLU, parametric activation functions) are not explored.
- What evidence would resolve it: Systematically evaluating a wider range of pre-activation functions on diverse datasets and neural operator architectures would identify the most effective function for stabilizing mixed-precision training and potentially improving performance.

## Limitations

- Theoretical guarantees for approximation bounds in mixed-precision setting are not rigorously proven
- Claim that tanh pre-activation "always" ensures stability is based on empirical observation without formal proof
- Precision scheduling benefits (10% better error) are reported for one dataset without comprehensive ablation studies

## Confidence

- High confidence: Memory and runtime improvements from half-precision tensor contractions (empirically verified)
- Medium confidence: Numerical stability benefits of tanh pre-activation (supported by experiments but mechanism could be more thoroughly analyzed)
- Low confidence: Approximation bounds guarantees (theoretical claims lack formal proof)

## Next Checks

1. **Ablation study on pre-activation functions**: Systematically compare tanh vs. hard-clip vs. 2σ-clip vs. no pre-activation across different FNO architectures and datasets to verify that tanh is optimal for numerical stability
2. **Theoretical analysis of frequency mode truncation**: Derive formal error bounds showing how half-precision error scales with frequency mode magnitude, and validate that truncation provides worst-case guarantees
3. **Cross-architecture generalization**: Test the mixed-precision approach on alternative operator learning architectures (e.g., DeepONet, U-FNO) to assess whether the approximation bounds hold beyond the specific FNO implementation studied