---
ver: rpa2
title: 'Let''s reward step by step: Step-Level reward model as the Navigators for
  Reasoning'
arxiv_id: '2310.10080'
source_url: https://arxiv.org/abs/2310.10080
tags:
- reasoning
- reward
- step
- language
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores using a Process-Supervised Reward Model (PRM)
  during the inference phase of Large Language Models (LLMs) to improve multi-step
  reasoning tasks like mathematical problem-solving and code generation. The authors
  propose a heuristic greedy search algorithm (HGS-PRM) that leverages step-level
  feedback from PRM to optimize reasoning pathways.
---

# Let's reward step by step: Step-Level reward model as the Navigators for Reasoning

## Quick Facts
- arXiv ID: 2310.10080
- Source URL: https://arxiv.org/abs/2310.10080
- Reference count: 40
- Primary result: HGS-PRM with WizardMath-13B achieved 65.4% accuracy on GSM8K, outperforming Chain of Thought (63.2%)

## Executive Summary
This paper explores using a Process-Supervised Reward Model (PRM) during inference to improve multi-step reasoning in Large Language Models (LLMs). The authors propose a heuristic greedy search algorithm (HGS-PRM) that leverages step-level feedback from PRM to optimize reasoning pathways. Results show significant improvements on mathematical reasoning benchmarks (GSM8K, MATH) and code generation tasks (HumanEval), with HGS-PRM outperforming standard Chain of Thought prompting. The paper also introduces a novel method for automatically generating step-level reward datasets for code using AST and mutation testing.

## Method Summary
The paper proposes HGS-PRM, which uses a PRM to evaluate each intermediate reasoning step during inference. The PRM provides granular reward signals (positive, neutral, negative) that guide a greedy search algorithm to accept, backtrack, or continue reasoning paths. The method was tested on mathematical reasoning tasks (GSM8K, MATH) and code generation (HumanEval) using various LLM models including LLaMA, Code-LLaMA, and WizardMath. For code tasks, the authors developed an automatic PRM training method using AST-based mutation testing to generate step-level rewards.

## Key Results
- On GSM8K benchmark, HGS-PRM with WizardMath-13B achieved 65.4% accuracy, outperforming CoT (63.2%)
- On MATH benchmark, HGS-PRM achieved 13.7% accuracy vs. CoT's 10.4%
- For HumanEval code generation, HGS-PRM with Code-LLaMA-Python-13B improved pass@1 from 41.5% to 44.5%

## Why This Works (Mechanism)

### Mechanism 1
Step-level feedback from PRM guides the LLM to correct reasoning errors before they cascade. PRM evaluates each intermediate reasoning step during inference, providing granular reward signals that allow the greedy search algorithm to accept, backtrack, or continue accordingly. Core assumption: PRM's step-level accuracy is sufficiently high to reliably distinguish correct from incorrect reasoning steps. Break condition: If PRM's precision drops below ~80%, incorrect steps may be accepted, propagating errors.

### Mechanism 2
Heuristic greedy search reduces the search space complexity compared to exhaustive methods. Instead of exploring all possible reasoning paths, the algorithm greedily accepts positive steps and backtracks on negative ones, limiting exploration to promising paths within bandwidth B. Core assumption: The PRM's positive/negative labeling is sufficiently discriminative to guide greedy decisions. Break condition: If PRM's neutral label discrimination is poor, the algorithm may waste bandwidth on suboptimal paths.

### Mechanism 3
Aligning PRM and LLM capabilities ensures optimal feedback quality. Math-specific models (WizardMath) benefit more from PRM than general models, suggesting that PRM and LLM should be matched in capability. Core assumption: PRM trained on math problems can accurately evaluate steps from math-specialized LLMs. Break condition: If LLM significantly outperforms PRM, feedback quality degrades.

## Foundational Learning

- **Concept: Chain of Thought (CoT) prompting**
  - Why needed here: Provides baseline for comparing HGS-PRM's effectiveness
  - Quick check question: How does CoT differ from step-level PRM feedback in handling errors?

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: PRM is traditionally used in RLHF; understanding this context explains why applying PRM during inference is novel
  - Quick check question: What's the key difference between using PRM during training vs. inference?

- **Concept: Abstract Syntax Trees (AST) and Mutation Testing**
  - Why needed here: These techniques enable automatic generation of step-level reward data for code
  - Quick check question: How does AST-based mutation create negative and neutral training examples?

## Architecture Onboarding

- **Component map**: LLM (reasoning generator) → PRM (step evaluator) → Greedy Search (path controller) → Output
- **Critical path**: LLM generates next step → PRM scores step → Greedy search decides accept/backtrack → Repeat until eos
- **Design tradeoffs**: Greedy search vs. BFS/MCTS (simplicity vs. exploration), PRM accuracy vs. computational cost
- **Failure signatures**: Low accuracy improvement despite PRM use, excessive backtracking, PRM bias toward certain error types
- **First 3 experiments**:
  1. Compare HGS-PRM vs. CoT on GSM8K with same LLM to verify improvement claim
  2. Test HGS-PRM with varying bandwidth B to find optimal exploration-exploitation balance
  3. Evaluate PRM's step-level accuracy on held-out data to quantify feedback reliability

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of HGS-PRM scale with increasing model size and complexity of reasoning tasks? The paper only tested limited model sizes and task complexities, making scalability unclear. Experiments with larger models and more challenging benchmarks would provide insights.

### Open Question 2
What is the optimal balance between the computational cost of HGS-PRM and its performance gains compared to simpler methods like CoT? The paper doesn't provide a detailed analysis of this trade-off, focusing instead on demonstrating effectiveness.

### Open Question 3
How robust is the PRM-based approach to different types of reasoning errors, such as semantic misunderstandings or context misinterpretations? The paper presents anecdotal examples but doesn't systematically analyze error types or their susceptibility to PRM intervention.

### Open Question 4
Can the PRM approach be extended to multimodal reasoning tasks that involve both text and other modalities (e.g., images, code)? The current PRM framework is designed for text-based feedback, and it's unclear how it would handle non-textual reasoning scenarios.

## Limitations

- PRM generalization and quality limitations - the paper doesn't thoroughly validate PRM's step-level accuracy across different reasoning domains
- Search algorithm limitations - greedy search may get trapped in local optima, with insufficient exploration of alternative strategies
- Dataset and model dependencies - results primarily for specific model sizes (7B, 13B) and datasets, with unclear effectiveness for larger models or different domains

## Confidence

- **High Confidence (7/10)**: Basic premise that step-level feedback improves reasoning is well-supported by empirical results
- **Medium Confidence (5/10)**: Claim that PRM is more effective than CoT prompting is supported but with modest improvement margins
- **Low Confidence (3/10)**: Automatic PRM generation for code using AST mutation is innovative but insufficiently validated

## Next Checks

1. **PRM Accuracy Validation**: Conduct thorough validation of PRM's step-level accuracy on held-out data, measuring precision, recall, and F1-score for positive, negative, and neutral classifications.

2. **Search Algorithm Ablation**: Compare HGS-PRM against alternative search strategies (beam search, Monte Carlo Tree Search) on the same tasks and models to determine optimal search approach.

3. **Capability Alignment Study**: Systematically test HGS-PRM across different LLM-PRM capability pairings to evaluate whether specialized models consistently outperform general models when paired with aligned PRMs.