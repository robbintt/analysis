---
ver: rpa2
title: 'MMM and MMMSynth: Clustering of heterogeneous tabular data, and synthetic
  data generation'
arxiv_id: '2310.19454'
source_url: https://arxiv.org/abs/2310.19454
tags:
- data
- synthetic
- datasets
- clustering
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a novel clustering algorithm, MMM, for heterogeneous
  tabular data, and use it to generate synthetic tabular data. We show that MMM performs
  well in clustering tasks on both synthetic and real data, outperforming other standard
  methods for purely categorical and mixed data.
---

# MMM and MMMSynth: Clustering of heterogeneous tabular data, and synthetic data generation

## Quick Facts
- arXiv ID: 2310.19454
- Source URL: https://arxiv.org/abs/2310.19454
- Reference count: 19
- Primary result: MMM clustering algorithm outperforms standard methods on heterogeneous tabular data; MMMSynth generates synthetic data that approaches real data performance

## Executive Summary
This paper presents MMM, a novel clustering algorithm for heterogeneous tabular data containing both categorical and numeric columns, and MMMSynth, a synthetic data generation method built on MMM. MMM uses an EM-based approach with Dirichlet and normal-gamma priors, integrating over unknown distributions rather than estimating parameters directly. The algorithm demonstrates superior clustering performance on synthetic benchmarks and real-world datasets compared to standard methods like k-means and GMM. MMMSynth leverages MMM's clustering to generate realistic synthetic data by sampling from per-cluster distributions and fitting cluster-specific linear models for output variables.

## Method Summary
MMM performs clustering on heterogeneous tabular data by assuming columns are conditionally independent given cluster assignments. For each cluster, it computes likelihoods by integrating over parameter spaces using Dirichlet priors for categorical columns and normal-gamma priors for numeric columns. The number of clusters is determined by maximizing marginal likelihood via thermodynamic integration or harmonic mean approximation. MMMSynth builds on MMM by first clustering the data, then generating synthetic rows by sampling from fitted per-cluster distributions for input columns and using cluster-specific linear models for output variables.

## Key Results
- MMM outperforms k-means, GMM, and k-prototypes on synthetic heterogeneous datasets with ARI scores above 0.95
- On real UCI datasets, MMM achieves higher ARI than comparison methods across multiple datasets
- MMMSynth synthetic data achieves AUC scores approaching those of models trained on real data (within 5-10% on most datasets)

## Why This Works (Mechanism)

### Mechanism 1
MMM outperforms existing clustering methods by integrating over unknown categorical and normal distributions rather than estimating their parameters. For each cluster, MMM computes likelihood by integrating over parameter space (Dirichlet for categorical, normal-gamma for continuous), acting as Bayesian marginalization that avoids overfitting to small clusters. Core assumption: columns within each cluster are conditionally independent given cluster assignment. Evidence: outperforms standard algorithms on synthetic heterogeneous data. Break condition: if columns have strong pairwise dependencies within clusters, independence assumption causes poor clustering.

### Mechanism 2
MMM recovers hidden structure by evaluating both data likelihood and marginal likelihood to determine correct number of clusters. Uses marginal likelihood (ML) to choose K, penalizing overfitting via thermodynamic integration or harmonic mean approximation. Core assumption: marginal likelihood criterion accurately balances fit and complexity for heterogeneous data. Evidence: Bayesian information criterion performed inferior on synthetic benchmarks. Break condition: if marginal likelihood approximation is inaccurate, MMM may choose wrong number of clusters.

### Mechanism 3
MMMSynth generates realistic synthetic data by preserving per-cluster column distributions and learning cluster-specific linear models for output variable. First clusters real data using MMM, then generates synthetic rows by sampling from fitted categorical/normal distributions per column. Fits noisy linear model per cluster for output variable. Core assumption: within each cluster, relationship between inputs and output is approximately linear with additive noise. Evidence: outperforms state-of-the-art synthetic data generation methods. Break condition: if true relationship is highly nonlinear within clusters, synthetic data quality suffers.

## Foundational Learning

- Concept: Dirichlet prior for categorical data
  - Why needed: MMM uses Dirichlet prior to model unknown categorical distribution for each categorical column in each cluster, allowing integration over parameter space
  - Quick check: In binary variable with Dirichlet prior (Beta prior), if we observe 3 ones and 1 zero, what is posterior probability of seeing one next?

- Concept: Normal-gamma prior for continuous data
  - Why needed: MMM uses normal-gamma prior to model unknown normal distribution for each continuous column in each cluster, enabling integration over mean and precision
  - Quick check: Given normal-gamma prior with hyperparameters μ₀, β₀, a₀, b₀, and n observations with sample mean x̄ and sum of squared deviations S, what are posterior hyperparameters?

- Concept: Marginal likelihood and Bayesian Occam's razor
  - Why needed: MMM uses marginal likelihood to choose number of clusters, balancing model fit and complexity to avoid overfitting
  - Quick check: Why does marginal likelihood penalize overfitting, and how is this related to Bayesian Occam's razor?

## Architecture Onboarding

- Component map: Input data -> MMM clustering (EM with Dirichlet/normal-gamma priors) -> Marginal likelihood estimation (TI/HMβ) -> Cluster assignments -> MMMSynth (per-cluster distribution fitting + linear model fitting) -> Synthetic data generation -> Output

- Critical path: 1) Preprocess input data (impute missing values, determine column types) 2) Run MMM clustering on input columns (excluding output) 3) For each cluster, fit distributions to each column and generate synthetic rows 4) Fit linear model to output variable per cluster and generate synthetic outputs 5) Pool synthetic clusters to form final dataset

- Design tradeoffs: MMM assumes column independence within clusters for computational tractability but may miss correlations; MMM uses integration over distributions rather than parameter estimation for robustness but slower; MMMSynth uses simple linear models for output generation which is fast but may miss nonlinear relationships

- Failure signatures: MMM produces too many small clusters (marginal likelihood estimation may be inaccurate or overfitting); MMM produces clusters that don't align with true labels (independence assumption may be violated); MMMSynth synthetic data has unrealistic correlations (linear model assumption may be violated)

- First 3 experiments: 1) Run MMM on small synthetic dataset with known clusters and compare predicted vs true labels 2) Run MMM on real dataset, visualize clusters, and assess if they capture meaningful structure 3) Generate synthetic data with MMMSynth, train model on it, and evaluate performance on real data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do we optimize the computational speed of MMM while maintaining accuracy?
- Basis: Authors state "Speed optimizations will be explored in future work"
- Why unresolved: Current MMM implementation may be computationally intensive for large datasets with no clear optimization path
- What evidence would resolve it: Comparative analysis of optimized MMM against current version on multiple datasets showing improved speed without accuracy loss

### Open Question 2
- Question: Can MMM be extended to account for correlations between columns within clusters?
- Basis: Authors mention "Currently the columns are assumed to be independent, but it will be a straightforward exercise to use a multivariate Gaussian to describe the numeric columns"
- Why unresolved: Extension is considered straightforward but actual implementation and impact on performance are unexplored
- What evidence would resolve it: Results from MMM implementation with multivariate Gaussian distributions for numeric columns showing improved clustering accuracy

### Open Question 3
- Question: How does MMM perform on datasets with high proportions of missing data?
- Basis: Authors mention data should be interpolated/imputed but don't explore MMM's performance with missing data
- Why unresolved: Impact of different imputation methods on MMM's clustering performance is unknown
- What evidence would resolve it: Performance comparison of MMM on datasets with varying levels of missing data and different imputation strategies

## Limitations

- MMM relies on strong independence assumptions within clusters that may not hold in real-world data, potentially limiting clustering accuracy
- Marginal likelihood estimation via thermodynamic integration or harmonic mean approximations could be computationally expensive or inaccurate for high-dimensional data
- Linear model assumption for synthetic output generation may fail for datasets with complex nonlinear relationships

## Confidence

- High confidence: MMM's integration over distributions provides robustness against overfitting in small clusters
- Medium confidence: MMM outperforms existing methods on benchmark datasets, though comparisons are limited to specific algorithms
- Low confidence: MMMSynth's synthetic data quality approaches real data performance across all tested scenarios

## Next Checks

1. Test MMM's robustness to violated independence assumptions by generating synthetic data with correlated columns within clusters and measuring clustering performance degradation
2. Benchmark MMMSynth against state-of-the-art GAN-based synthetic data generators on datasets with known nonlinear relationships
3. Evaluate computational scaling of MMM and MMMSynth on high-dimensional datasets (100+ columns) to assess practical feasibility