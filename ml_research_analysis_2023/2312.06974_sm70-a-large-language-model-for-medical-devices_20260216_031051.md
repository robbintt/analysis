---
ver: rpa2
title: 'SM70: A Large Language Model for Medical Devices'
arxiv_id: '2312.06974'
source_url: https://arxiv.org/abs/2312.06974
tags:
- medical
- sm70
- language
- dataset
- usmle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SM70, a 70 billion-parameter Large Language
  Model designed for medical devices, offering accurate and safe responses to medical-domain
  questions. Built on the Llama2 70B model, SM70 was fine-tuned using QLoRA with approximately
  800K data entries from the MedAlpaca dataset.
---

# SM70: A Large Language Model for Medical Devices

## Quick Facts
- arXiv ID: 2312.06974
- Source URL: https://arxiv.org/abs/2312.06974
- Authors: 
- Reference count: 3
- Key outcome: SM70, a 70B LLM for medical devices, outperforms Llama2 70B, Clinical Camel 70, and GPT-3.5 on MEDQA-USMLE, PUBMEDQA, and USMLE benchmarks while lagging GPT-4.

## Executive Summary
SM70 is a 70 billion-parameter Large Language Model designed for medical devices, offering accurate and safe responses to medical-domain questions. Built on the Llama2 70B model, SM70 was fine-tuned using QLoRA with approximately 800K data entries from the MedAlpaca dataset. Evaluations across three benchmark datasets—MEDQA-USMLE, PUBMEDQA, and USMLE—show that SM70 outperforms several established models, including Llama2 70B, Clinical Camel 70, and GPT-3.5, in handling fact-based medical queries and complex clinical decision-making scenarios. While it lags behind GPT-4, SM70 demonstrates strong potential as a clinical decision support and medical information retrieval tool. Further refinement is suggested for tasks requiring extensive medical knowledge and intricate reasoning.

## Method Summary
SM70 is built on the Llama2 70B base model and fine-tuned using QLoRA with approximately 800K data entries from the MedAlpaca dataset. The fine-tuning employed 4-bit precision with hyperparameters including sequence length 1024, LoRA 'r' 64, LoRA 'Alpha' 16, LoRA 'Dropout' 0.1, mini-batch size 32, 5 epochs, and learning rate 2e-4. The dataset was preprocessed into a unified prompt template format: "Question: Context Instruction/Question. Answer: Output". The model was evaluated on three benchmark datasets (MEDQA-USMLE, PUBMEDQA, USMLE) and compared against baseline models including Llama2 70B, Clinical Camel 70, GPT-3.5, GPT-4, and Med-Palm.

## Key Results
- SM70 outperforms Llama2 70B, Clinical Camel 70, and GPT-3.5 on MEDQA-USMLE, PUBMEDQA, and USMLE benchmarks
- SM70 shows strong performance on fact-based medical queries and complex clinical decision-making scenarios
- SM70 lags behind GPT-4 but demonstrates potential for clinical decision support and medical information retrieval

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning Llama2 70B with 800K medical QA entries from MedAlpaca improves domain-specific performance without losing general reasoning. Low-rank adaptation (QLoRA) injects compact trainable matrices into the frozen base model, allowing it to specialize on medical terminology and reasoning patterns from the QA dataset while preserving general capabilities.

### Mechanism 2
SM70 achieves strong USMLE and PUBMEDQA scores by leveraging the structure of multi-source medical QA datasets. The unified prompt template ("Question: Context Instruction/Question. Answer: Output") aligns heterogeneous datasets into a consistent format, enabling the model to learn the mapping from context to answer across different medical subdomains.

### Mechanism 3
SM70's strong comparative performance vs. GPT-3.5 and CC70 is due to scaling and task-specific fine-tuning rather than architecture alone. The 70B parameter base model provides sufficient representational capacity; domain fine-tuning on high-quality medical QA data further optimizes it for medical domain tasks, outperforming smaller or less specialized models.

## Foundational Learning

- **LoRA/QLoRA parameter-efficient fine-tuning**
  - Why needed: Enables effective domain specialization on a 70B model without the compute cost of full fine-tuning
  - Quick check: How does LoRA modify weight updates during training compared to full fine-tuning?

- **Medical QA dataset construction and prompt engineering**
  - Why needed: Proper formatting of heterogeneous datasets ensures consistent training signals and avoids domain shift
  - Quick check: What is the benefit of combining context and question into a single prompt string?

- **Evaluation on standardized medical benchmarks**
  - Why needed: These benchmarks measure both factual recall and clinical reasoning, providing a holistic view of model competence
  - Quick check: Why is it important to evaluate on both fact-based and reasoning-heavy datasets?

## Architecture Onboarding

- **Component map:**
  - Llama2 70B base model (frozen) -> QLoRA adapters (LoRA layers) -> MedAlpaca-derived QA corpus (800K entries) -> Unified prompt template pipeline -> Standard medical benchmarks (MEDQA-USMLE, PUBMEDQA, USMLE)

- **Critical path:**
  1. Load Llama2 70B and attach QLoRA adapters
  2. Load and preprocess MedAlpaca data into unified QA format
  3. Fine-tune on combined dataset with QLoRA for several epochs
  4. Evaluate on each benchmark dataset
  5. Compare results against baseline models

- **Design tradeoffs:**
  - 4-bit quantization reduces memory but may impact precision on nuanced reasoning
  - Unified prompt may simplify training but risks losing dataset-specific nuances
  - Relying on publicly available data may introduce biases or gaps in rare conditions

- **Failure signatures:**
  - Performance collapse on MEDQA-USMLE despite good PUBMEDQA scores → overfitting to fact recall
  - High variance across epochs → learning instability or poor data quality
  - Inability to generalize beyond MedAlpaca style questions → dataset bias

- **First 3 experiments:**
  1. Evaluate base Llama2 70B on MEDQA-USMLE before fine-tuning to establish baseline
  2. Fine-tune with LoRA on a subset of MedAlpaca (e.g., 100K entries) and test for overfitting
  3. Run ablation: fine-tune with standard prompt vs. unified prompt to measure prompt engineering impact

## Open Questions the Paper Calls Out

### Open Question 1
How does SM70's performance vary when evaluated on more diverse medical datasets beyond MEDQA-USMLE, PUBMEDQA, and USMLE? The paper evaluates SM70 on three specific benchmark datasets and suggests potential areas for further refinement, particularly in aspects requiring deep medical knowledge and sophisticated reasoning. Testing SM70 on a wider range of medical datasets, including those covering rare diseases, pediatric cases, or other specialized medical fields, would provide a more comprehensive understanding of its capabilities and limitations.

### Open Question 2
What are the specific privacy and security measures implemented in SM70 to ensure compliance with medical data regulations? The paper emphasizes the importance of privacy measures in medical applications and mentions the need for transparency and clear data management practices. A detailed description of the privacy and security protocols, including data encryption, access controls, and compliance with regulations such as HIPAA, would address this question.

### Open Question 3
How does SM70 perform in real-world clinical settings compared to its performance on benchmark datasets? The paper suggests SM70's potential for clinical decision support and medical information retrieval, but does not provide evidence of its performance in actual clinical environments. Conducting clinical trials or pilot studies in hospitals or clinics, where SM70 is used by medical professionals in their daily workflows, would provide valuable insights into its real-world performance and usability.

## Limitations

- Lack of detailed information about the Medical Meadow dataset composition and specific preprocessing steps applied before fine-tuning
- Evaluation focuses on three benchmark datasets without addressing potential domain gaps or reporting confidence intervals for performance metrics
- Claims about SM70's superiority for "complex clinical decision-making scenarios" are not fully substantiated with detailed analysis of failure modes

## Confidence

- **High Confidence**: The technical feasibility of using QLoRA for parameter-efficient fine-tuning on a 70B model is well-established
- **Medium Confidence**: The reported performance improvements over baseline models are plausible given the fine-tuning approach
- **Low Confidence**: Claims about SM70's superiority for complex clinical decision-making scenarios are not fully substantiated

## Next Checks

1. Conduct a thorough audit of the Medical Meadow/MedAlpaca dataset to assess its coverage of medical specialties, difficulty levels, and potential biases before drawing conclusions about model performance.

2. Systematically analyze model errors on USMLE questions to distinguish between knowledge gaps, reasoning failures, and prompt formatting issues, rather than treating all errors as equivalent.

3. Evaluate SM70 on medical question-answering datasets not used in training (e.g., MedQA, PubMedQA variants) to verify that performance gains generalize beyond the specific training distribution.