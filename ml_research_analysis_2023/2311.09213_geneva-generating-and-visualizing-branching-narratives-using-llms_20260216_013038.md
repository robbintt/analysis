---
ver: rpa2
title: 'GENEVA: GENErating and Visualizing branching narratives using LLMs'
arxiv_id: '2311.09213'
source_url: https://arxiv.org/abs/2311.09213
tags:
- beat
- storylines
- storyline
- narrative
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GRIM, a graph-based narrative visualization
  tool powered by large language models to assist game designers in creating branching
  storylines. The system generates a directed acyclic graph where each node represents
  a narrative beat and edges represent player choices.
---

# GENEVA: GENErating and Visualizing branching narratives using LLMs

## Quick Facts
- arXiv ID: 2311.09213
- Source URL: https://arxiv.org/abs/2311.09213
- Reference count: 27
- One-line primary result: Introduces GRIM, a graph-based narrative visualization tool powered by LLMs for creating branching storylines in games

## Executive Summary
This paper presents GRIM (Graph-based Narrative Generation), a tool that leverages large language models to generate and visualize branching narratives for game design. The system uses GPT-4 in a two-step process to first create detailed storylines in text format from high-level descriptions and constraints, then convert these into graph structures for visualization. By treating narrative beats as building blocks and organizing them into directed acyclic graphs, GRIM enables game designers to create complex branching storylines that can be iteratively edited and grounded in specific settings.

## Method Summary
GRIM uses a two-step LLM-based approach to generate branching narratives. First, GPT-4 generates storylines in text format based on high-level narrative descriptions and constraints (number of starts, endings, storylines, and setting details). Second, the system converts these storylines into graph structures where nodes represent narrative beats and edges represent player choices. The tool allows iterative editing by regenerating sub-graphs when designers modify nodes or edges, maintaining the original narrative constraints while incorporating changes.

## Key Results
- Successfully generates coherent branching narratives grounded in specific settings (Minecraft, 21st century, Ancient Rome, Quantum Realm)
- Maintains narrative coherence across multiple storylines while adhering to structural constraints
- Enables iterative editing of narrative graphs with automatic regeneration of storylines
- Demonstrates effectiveness using four well-known stories (Dracula, Frankenstein, Jack and the Beanstalk, Little Red Riding Hood)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system generates coherent branching narratives by first creating detailed storylines in text format and then converting them into graph structures
- Mechanism: The two-step process separates narrative generation (creative text generation) from graph encoding (structured data formatting), allowing each to be optimized independently
- Core assumption: LLMs can both generate creative narrative content and follow structured output formats when properly prompted
- Evidence anchors:
  - [abstract] "a two-step process. First we prompt GPT-4 to create the narrative graph, with branching storylines in text format. Then we use GPT-4 to encode this information into a format that can be consumed by the visualization code."
  - [section 2.2.1] "We define a storyline as a sequence of narrative beats. Narrative or story beats are often defined as significant moments in a story that evoke particular emotional reaction from the audience. We treat beats as the building blocks of a storyline."
  - [corpus] Weak - No direct citations about LLM narrative generation capabilities, but related work on LLM-generated structured data exists
- Break condition: If the LLM fails to maintain narrative coherence across multiple storylines or cannot follow the structured output format requirements

### Mechanism 2
- Claim: The system can ground stories in specific settings while maintaining narrative coherence
- Mechanism: The prompt engineering includes explicit instructions to incorporate setting-specific elements (cultural elements, physical properties, technology, etc.) while following narrative conventions
- Core assumption: LLMs have sufficient knowledge about diverse settings in their training data to generate contextually appropriate content
- Evidence anchors:
  - [section 2.2.1] "Ground storyline in unique characteristics of the input setting, including cultural elements, physical properties, technology, etc."
  - [section 3.2] "We find that GRIM is able to ground the stories in the given setting well. For example, when asked to ground Little Red Riding Hood in the game of Minecraft, it generates storylines that include healing potions, mob-infested cave, a redstone contraption and other elements that are specific to the game of Minecraft"
  - [corpus] Moderate - Related work exists on LLM grounding but not specifically for narrative generation in game contexts
- Break condition: If the LLM lacks sufficient knowledge about the specified setting or cannot integrate setting elements naturally into the narrative

### Mechanism 3
- Claim: The system enables iterative editing by regenerating sub-graphs when designers modify nodes or edges
- Mechanism: When designers make edits, the system prompts the LLM to update storylines by adding new storylines or deleting existing ones while maintaining the original narrative constraints
- Core assumption: LLMs can understand and apply complex narrative constraints when regenerating content
- Evidence anchors:
  - [section 2.3] "The designer can add or delete nodes or edges to the graph. Using GPT-4, we are able to automatically update the graph by creating entirely new sub-graphs to make the game designer's storyline updates fit within the original narrative and constraints."
  - [section 2.3] "We prompt GPT-4 with the original storylines (as generated by prompt in Section 2.2.1) and a set of guidelines sketched as below: • Update the list of storylines by adding new storylines or deleting existing storylines. • The updated storylines should include the newly added beats Nadded. • They should not include the deleted beats Ndeleted."
  - [corpus] Weak - No direct citations about iterative narrative editing systems, but related work on LLM-based editing exists
- Break condition: If the LLM cannot maintain narrative coherence when regenerating sub-graphs or fails to apply the specified constraints

## Foundational Learning

- Concept: Narrative beats as building blocks of storylines
  - Why needed here: The system treats beats as atomic narrative units that can be sequenced and re-sequenced to create branching storylines
  - Quick check question: What defines a narrative beat, and why is it important to have detailed descriptions for each beat in the generation process?

- Concept: Directed acyclic graphs (DAGs) for narrative representation
  - Why needed here: The system uses DAGs where nodes represent narrative beats and edges represent player choices, requiring understanding of graph theory concepts
  - Quick check question: Why is it important that the narrative graph is directed and acyclic, and what would happen if cycles were allowed?

- Concept: Prompt engineering for structured output generation
  - Why needed here: The system relies on carefully crafted prompts to guide the LLM in generating both narrative content and structured graph data
  - Quick check question: How do the specific constraints in the prompt (e.g., number of starts, endings, storylines) influence the generated output?

## Architecture Onboarding

- Component map: User Interface -> LLM Interface -> Prompt Manager -> Graph Renderer -> Constraint Validator
- Critical path: User provides high-level description → System generates storylines via LLM → System encodes storylines as graph → Graph is visualized → User edits graph → System regenerates storylines → Updated graph is visualized
- Design tradeoffs:
  - Using GPT-4 provides high-quality generation but increases cost and latency
  - Two-step generation process adds complexity but allows better control over output structure
  - Strict prompt constraints ensure quality but may limit creative variations
- Failure signatures:
  - Inconsistent beat descriptions across storylines
  - Graph nodes that don't correspond to actual beats
  - Edges that create cycles in the graph
  - Generated content that doesn't adhere to specified constraints
- First 3 experiments:
  1. Generate a simple narrative graph for a well-known story with minimal constraints to verify basic functionality
  2. Test grounding capabilities by generating the same story in multiple different settings
  3. Verify iterative editing by making small changes to an existing graph and checking that the system regenerates appropriate storylines

## Open Questions the Paper Calls Out

- Open Question 1: How does the grounding quality in Quantum Realm compare to more well-documented settings like Minecraft and 21st century?
  - Basis in paper: [explicit] The paper states "grounding in Minecraft or 21st century appears to be much better than grounding in quantum realm" and suggests this is because "there is more information about Minecraft and the 21st century in the language model data than there is about quantum realm."
  - Why unresolved: The paper only provides qualitative observations about grounding quality differences without systematic quantitative evaluation or specific metrics to measure grounding effectiveness across settings.
  - What evidence would resolve it: A quantitative evaluation comparing narrative elements, terminology usage, and contextual accuracy across different settings using standardized metrics would clarify the grounding quality differences.

- Open Question 2: What are the limitations of the iterative editing approach when making large-scale modifications to the narrative graph?
  - Basis in paper: [inferred] The paper describes the iterative editing process but doesn't discuss how the system performs when multiple nodes and edges are added/removed simultaneously, or when edits significantly alter the original narrative structure.
  - Why unresolved: The authors only demonstrate simple single-node additions and don't explore edge cases or performance degradation with complex modifications.
  - What evidence would resolve it: Testing the system's ability to maintain narrative coherence and adhere to constraints when making substantial edits, including measuring coherence scores and constraint satisfaction rates for complex modifications.

- Open Question 3: How does the system ensure narrative coherence when generating multiple branching storylines with shared beats?
  - Basis in paper: [explicit] The paper mentions constraints like "no more than three same consecutive beats between any two storylines" and "2 or 3 beats that are common between all storylines" but doesn't explain how these constraints affect narrative flow and player experience.
  - Why unresolved: The authors don't provide analysis of how shared beats impact storytelling quality or player engagement, nor do they explore optimal strategies for beat sharing between storylines.
  - What evidence would resolve it: User studies measuring player engagement and narrative satisfaction across different beat-sharing configurations, combined with automated coherence metrics, would clarify the impact of these constraints.

## Limitations

- The system's reliance on GPT-4 means results may vary significantly with different LLM architectures or smaller models
- Evaluation focuses primarily on qualitative assessments rather than quantitative metrics, limiting objective comparison
- Testing was limited to four well-known stories and four settings, which may not represent the full diversity of narrative design challenges

## Confidence

- High confidence: The core mechanism of using a two-step process (text generation followed by graph encoding) is technically sound and well-documented
- Medium confidence: The system's ability to ground stories in specific settings, as results show variability across different contexts
- Medium confidence: The effectiveness of iterative editing, as the paper demonstrates the capability but provides limited evaluation of edit quality and usability

## Next Checks

1. Test the system with a broader range of story types (including original stories not based on well-known tales) to assess generalizability beyond classic narratives
2. Conduct user studies with game designers to evaluate the tool's practical utility and compare the quality of AI-generated branching narratives against designer-created content
3. Implement and test the system with smaller, more cost-effective language models to determine if the approach requires GPT-4 specifically or if it generalizes to other LLM architectures