---
ver: rpa2
title: Does Conceptual Representation Require Embodiment? Insights From Large Language
  Models
arxiv_id: '2305.19103'
source_url: https://arxiv.org/abs/2305.19103
tags:
- words
- human
- language
- dimensions
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) like GPT-3.5 and GPT-4 are trained
  on text (and images for GPT-4) but not on sensory or motor experiences. This study
  compares their ratings of 4,442 English words on 8 psycholinguistic dimensions (emotion,
  salience, mental visualization, and sensory/motor domains) to human ratings.
---

# Does Conceptual Representation Require Embodiment? Insights From Large Language Models

## Quick Facts
- arXiv ID: 2305.19103
- Source URL: https://arxiv.org/abs/2305.19103
- Reference count: 40
- Large language models trained on text (and images for GPT-4) approximate human conceptual representations for abstract dimensions but struggle with sensory and motor domains

## Executive Summary
This study investigates whether large language models (LLMs) can develop human-like conceptual representations without embodied experience. GPT-3.5 and GPT-4 were compared against human ratings across 8 psycholinguistic dimensions using 4,442 English words. GPT-3.5 showed strong alignment with humans on abstract dimensions like emotion and salience, while GPT-4's multimodal training improved performance on visually-associated dimensions. Both models struggled with motor aspects, suggesting that some conceptual representations can be learned from language alone while others benefit from or require multimodal grounding.

## Method Summary
The study compared LLM-generated ratings to human psycholinguistic norms across 4,442 English words rated on 8 dimensions. Researchers used identical prompts to human instructions and collected ratings from GPT-3.5 (text-only) and GPT-4 (text+image) via API. Spearman rank correlations were calculated between model-generated and human-generated ratings at both aggregated and individual levels. The analysis included dimensions like emotion, salience, mental visualization, and sensory/motor domains using established Glasgow and Lancaster norms as benchmarks.

## Key Results
- GPT-3.5 achieved strong correlations with humans on abstract dimensions (valence r=0.9) but weaker correlations on sensory/motor domains
- GPT-4 outperformed GPT-3.5 across most dimensions, with largest gains on visually-associated dimensions (rs=0.74)
- Both models struggled with motor aspects involving foot/leg, mouth/throat, and torso, with maximum correlations of rs=0.46 for mouth/throat and minimum rs=0.39 for hand/arm

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can approximate abstract conceptual representations through linguistic co-occurrence patterns alone.
- Mechanism: Large-scale text corpora encode distributional semantics that capture relational structures among words, enabling models to infer abstract dimensions like valence or arousal without direct sensorimotor grounding.
- Core assumption: Abstract conceptual features are sufficiently encoded in language context for models to recover them statistically.
- Evidence anchors: Both models strongly align with human representations in non-sensorimotor domains but lag in sensory and motor areas; GPT-3.5 trained on language-only input correlates highly with human output, with the valence dimension achieving a correlation coefficient as high as 0.9.
- Break condition: If distributional statistics fail to capture critical semantic nuances, abstract concept inference will degrade.

### Mechanism 2
- Claim: Visual modality in GPT-4 improves its performance on dimensions linked to visual experience.
- Mechanism: Multimodal training enables richer semantic grounding for concepts that benefit from visual association, leading to higher correlations with human ratings on imageability and concreteness.
- Core assumption: Visual features in training data transfer to improved representations in visually-associated psycholinguistic dimensions.
- Evidence anchors: GPT-4's gains are associated with its additional visual learning, which also appears to benefit related dimensions like haptics and imageability; Dimensions most strongly associated with visual input showed greater improvement in GPT-4 compared to GPT-3.5 (rs = 0.74).
- Break condition: If visual features are not integrated effectively, expected improvements will not materialize.

### Mechanism 3
- Claim: LLMs capture some sensorimotor knowledge but cannot fully replicate human sensorimotor grounding.
- Mechanism: Statistical learning from text and images provides partial sensorimotor representations, but lacks the embodied experience necessary for full capture of motor dimensions.
- Core assumption: Text and image data contain sufficient cues to approximate but not fully replicate sensorimotor experience.
- Evidence anchors: GPT-4 struggles to fully capture motor aspects of conceptual knowledge such as actions with foot/leg, mouth/throat, and torso; Correlations remained smaller in most of the motor dimensions (maximum rs = 0.46 for mouth/throat and minimum rs = 0.39 for hand/arm).
- Break condition: If embodied experience is essential for certain concepts, LLM performance will plateau below human levels.

## Foundational Learning

- Concept: Spearman rank correlation as a measure of ordinal agreement
  - Why needed here: The study uses Spearman correlation to compare aggregated ratings, requiring understanding of rank-based similarity measures.
  - Quick check question: If two sets of rankings are identical except one is reversed, what would the Spearman correlation be?
- Concept: Psycholinguistic norms as standardized psychological measures
  - Why needed here: The study relies on established norms (Glasgow and Lancaster) as human benchmarks for comparison.
  - Quick check question: What are the key dimensions measured by the Glasgow Norms that were used in this study?
- Concept: Multimodal learning and its impact on model performance
  - Why needed here: The study compares GPT-3.5 (text-only) with GPT-4 (text+image) to assess visual modality benefits.
  - Quick check question: What specific psycholinguistic dimension showed the largest improvement when comparing GPT-4 to GPT-3.5?

## Architecture Onboarding

- Component map: Data collection from LLMs (via API) -> preprocessing -> correlation analysis -> visualization -> interpretation
- Critical path: Data collection → preprocessing → correlation analysis → visualization → interpretation
- Design tradeoffs: Using aggregated vs. individual-level correlations, choosing Spearman vs. other metrics, handling missing data
- Failure signatures: Low correlations indicating model-human divergence, API errors during data collection, convergence issues in Bayesian analysis
- First 3 experiments:
  1. Run the same rating tasks on a smaller dataset to verify correlation patterns before scaling up.
  2. Test a simplified model with only text vs. one with added image input to isolate visual modality effects.
  3. Conduct ablation studies by removing specific psycholinguistic dimensions to identify most critical predictors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs need sensory grounding to achieve human-like conceptual representations, or can language alone suffice?
- Basis in paper: The paper discusses this debate and states "Recent advances in large language models (LLMs) may offer new and unique perspectives" on whether embodied experience is necessary for conceptual representation.
- Why unresolved: The paper finds that GPT-4, which includes visual training, performs better than GPT-3.5 on perceptual dimensions, but both struggle with motor aspects. It's unclear if this is due to multimodal training or simply larger model size.
- What evidence would resolve it: Controlled experiments comparing LLMs with and without multimodal grounding on the same conceptual tasks, holding model size constant.

### Open Question 2
- Question: How does the addition of visual training in GPT-4 improve its alignment with human ratings on psycholinguistic dimensions?
- Basis in paper: The paper finds that "dimensions that were the most strongly associated with the visual dimension exhibited greater improvement in GPT-4 than GPT-3.5."
- Why unresolved: While the paper identifies a correlation, it cannot determine if visual training causes the improvement or if it's due to other differences between GPT-3.5 and GPT-4.
- What evidence would resolve it: Experiments isolating the effect of visual training by comparing LLMs with identical architectures but different training modalities on the same psycholinguistic tasks.

### Open Question 3
- Question: To what extent does the lack of sensory-motor grounding in LLMs impact their ability to perform high-level reasoning tasks?
- Basis in paper: The paper notes that while LLMs struggle with sensorimotor representations, they still demonstrate "an impressive ability to generate coherent and contextually appropriate responses" in complex scenarios.
- Why unresolved: The paper acknowledges that psycholinguistic norms capture important aspects of cognition but don't directly measure high-level reasoning ability, leaving the practical impact of grounding unclear.
- What evidence would resolve it: Empirical studies comparing LLM performance on reasoning tasks that specifically require sensory-motor knowledge versus those that don't, and analyzing the differences.

## Limitations

- The study relies on aggregated human ratings, potentially masking individual variation in conceptual representation
- GPT-4's performance improvements may be partially attributable to architectural differences beyond visual training
- Limited external validation of mechanisms, with no direct citations supporting key claims about multimodal transfer or sensorimotor limitations

## Confidence

- **High Confidence:** GPT-3.5's strong alignment with humans on abstract dimensions like emotion and salience (correlation up to 0.9)
- **Medium Confidence:** GPT-4's multimodal advantages on visually-associated dimensions (rs = 0.74), though architectural confounds remain
- **Medium Confidence:** LLMs' partial capture of sensorimotor knowledge but inability to fully replicate human grounding in motor dimensions

## Next Checks

1. Conduct individual-level analysis with larger sample sizes to verify robustness of aggregated correlation patterns
2. Design ablation studies comparing models with controlled architectural differences to isolate visual modality effects
3. Implement controlled experiments with synthetic data to test whether distributional semantics alone can capture specific abstract concepts