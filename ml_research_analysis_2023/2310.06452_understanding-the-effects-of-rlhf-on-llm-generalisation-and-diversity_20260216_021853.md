---
ver: rpa2
title: Understanding the Effects of RLHF on LLM Generalisation and Diversity
arxiv_id: '2310.06452'
source_url: https://arxiv.org/abs/2310.06452
tags:
- rlhf
- diversity
- generalisation
- output
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines the effects of RLHF fine-tuning on LLM generalization
  and output diversity. Key findings include: RLHF improves in-distribution and out-of-distribution
  performance compared to supervised fine-tuning (SFT).'
---

# Understanding the Effects of RLHF on LLM Generalisation and Diversity

## Quick Facts
- arXiv ID: 2310.06452
- Source URL: https://arxiv.org/abs/2310.06452
- Reference count: 40
- This paper examines the effects of RLHF fine-tuning on LLM generalization and output diversity.

## Executive Summary
This paper systematically investigates how Reinforcement Learning from Human Feedback (RLHF) affects both the generalization capabilities and output diversity of large language models compared to Supervised Fine-Tuning (SFT). The authors find that while RLHF significantly improves out-of-distribution generalization, it substantially reduces output diversity across multiple metrics. The study reveals an inherent tradeoff between these two objectives that persists even when adjusting RLHF hyperparameters. The findings suggest that current fine-tuning techniques cannot simultaneously optimize for both generalization and diversity, calling for new approaches that can achieve both goals.

## Method Summary
The study compares three fine-tuning methods: Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Behavior Cloning from human-written responses (BoN). The RLHF pipeline follows a three-stage approach: SFT on demonstration data, reward model training on preference pairs, and RLHF using Proximal Policy Optimization (PPO) with KL regularization. The authors evaluate models on summarization (TL;DR, CNN/DM) and instruction following tasks (AlpacaFarm Self-Instruct, AlpacaEval, Sequential Instructions) using GPT-4 as a simulated human evaluator. Diversity is measured using distinct n-grams (EAD), sentence-BERT cosine similarity, and NLI diversity metrics in both per-input and across-input settings.

## Key Results
- RLHF improves both in-distribution and out-of-distribution performance compared to SFT, with larger gains for bigger distribution shifts.
- RLHF substantially reduces output diversity compared to SFT across multiple metrics including distinct n-grams, sentence-BERT similarity, and NLI diversity.
- Even for different inputs, RLHF models produce less diverse text on some metrics, indicating mode collapse where models produce similar styles regardless of input.
- The tradeoff between generalization and diversity persists even when adjusting the KL penalty coefficient in RLHF.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RLHF improves out-of-distribution generalization by learning reward models that capture human preferences which are more transferable than raw demonstrations.
- **Mechanism:** During RLHF, the reward model is trained on human preference data between outputs for the same input. This creates a learned reward function that generalizes beyond the training distribution, allowing the policy to optimize for human-aligned outcomes even on unseen inputs.
- **Core assumption:** Human preferences for output quality are consistent across different input distributions.
- **Evidence anchors:** [abstract] "We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger." [section 6.1] "RLHF generalises much better. This suggests that RLHF may generalise better relative to SFT for larger distribution shifts"

### Mechanism 2
- **Claim:** RLHF reduces output diversity through KL penalty that constrains policy updates toward the SFT policy.
- **Mechanism:** The KL penalty term in RLHF reward function (βKL · DKL(πRL||πSFT)) forces the RL policy to stay close to the SFT policy, preventing exploration of diverse output distributions that might deviate significantly from the SFT baseline.
- **Core assumption:** The KL penalty coefficient effectively controls the tradeoff between performance and diversity.
- **Evidence anchors:** [abstract] "RLHF significantly reduces output diversity compared to SFT across a variety of measures" [section 6.3] "increasing the KL penalty coefficient leads to a drop in performance as expected, but also to a drop in per-input diversity"

### Mechanism 3
- **Claim:** Mode collapse occurs in RLHF when the reward model becomes overoptimized on training preferences, leading to stereotyped outputs.
- **Mechanism:** During RLHF training, the policy learns to maximize the reward model's output. If the reward model overfits to patterns in training data, the policy converges to producing outputs that satisfy those specific patterns regardless of input variation.
- **Core assumption:** Reward models can overfit to training preference data in ways that reduce output diversity.
- **Evidence anchors:** [abstract] "Even when sampling outputs for different inputs, RLHF produces less diverse text on some metrics, implying that such models tend to produce text of a specific style regardless of the input." [section 6.2] "This is the first rigorous empirical demonstration of across-input mode collapse emerging from RLHF training specifically."

## Foundational Learning

- **Concept: Distribution shift and generalization**
  - Why needed here: The paper directly compares in-distribution vs out-of-distribution performance to understand how different fine-tuning methods handle distribution shifts.
  - Quick check question: If a model performs well on training data but poorly on new data from a different distribution, what does this indicate about its generalization capabilities?

- **Concept: Diversity metrics and evaluation**
  - Why needed here: The paper uses multiple diversity metrics (distinct n-grams, sentence-BERT similarity, NLI diversity) to quantify output diversity across different dimensions.
  - Quick check question: Why might a model that produces diverse vocabulary (high EAD) still lack semantic diversity (low sentence-BERT diversity)?

- **Concept: KL divergence and policy regularization**
  - Why needed here: The KL penalty in RLHF directly affects the tradeoff between maintaining the SFT policy behavior and optimizing for reward, which impacts both performance and diversity.
  - Quick check question: What happens to the RLHF policy behavior as the KL penalty coefficient approaches zero?

## Architecture Onboarding

- **Component map:** Pretrained LLM → SFT → RM training → RLHF (PPO) OR BoN sampling → GPT-4 evaluation + diversity metrics
- **Critical path:** 1. Pretrained model initialization 2. SFT training on demonstration data 3. RM training on preference pairs 4. RLHF training with KL-regularized reward 5. Evaluation on ID and OOD test sets 6. Diversity assessment across multiple metrics
- **Design tradeoffs:** SFT: Simpler, faster, more diverse outputs but worse OOD generalization; RLHF: Better OOD performance but reduced diversity and higher complexity; BoN: Good performance but computationally expensive at inference; KL penalty: Controls diversity-performance tradeoff but doesn't eliminate the tradeoff
- **Failure signatures:** RLHF: Low diversity metrics, mode collapse (similar outputs across different inputs); SFT: Poor OOD performance, failure to capture complex preference patterns; RM: Overfitting to training preferences, poor generalization to new input types
- **First 3 experiments:** 1. Compare SFT vs RLHF on ID and OOD performance using GPT-4 evaluation 2. Measure diversity reduction in RLHF using distinct n-grams and sentence-BERT metrics 3. Sweep KL penalty coefficient to test diversity-performance tradeoff claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the fundamental cause of the tradeoff between generalization and diversity in RLHF fine-tuning?
- Basis in paper: [explicit] The paper identifies an inherent tradeoff between generalization and diversity when applying current fine-tuning techniques, but does not explain why this tradeoff exists.
- Why unresolved: The paper observes the tradeoff empirically but does not provide a theoretical explanation for why RLHF reduces diversity while improving generalization.
- What evidence would resolve it: Experiments comparing RLHF to other fine-tuning methods that control for specific factors like KL penalty or reward model usage could isolate the cause of the tradeoff. Analyzing the learned representations of RLHF models could also reveal why they collapse to certain modes.

### Open Question 2
- Question: Can novel fine-tuning methods improve both generalization and diversity without sacrificing one for the other?
- Basis in paper: [explicit] The paper concludes that more research is needed to develop methods that can improve both generalization and diversity without sacrificing one for the other.
- Why unresolved: The paper only investigates three fine-tuning methods (RLHF, SFT, BoN) and finds they all have tradeoffs. It does not explore whether alternative approaches could avoid this limitation.
- What evidence would resolve it: Developing and testing new fine-tuning algorithms that explicitly encourage diversity (e.g. via diversity-promoting objectives) while maintaining strong generalization performance would demonstrate whether the tradeoff is fundamental or a limitation of current methods.

### Open Question 3
- Question: Does the generalization-diversity tradeoff persist across different model scales and tasks?
- Basis in paper: [inferred] The paper observes the tradeoff in two tasks (summarization and instruction following) across different model sizes, but does not systematically vary these factors to determine the generality of the tradeoff.
- Why unresolved: The experiments are limited to specific model scales and tasks, so it's unclear whether the tradeoff is universal or task-dependent.
- What evidence would resolve it: Testing the tradeoff across a wider range of model sizes, tasks, and domains would reveal whether it's a general property of RLHF fine-tuning or specific to certain settings. Analyzing the scaling behavior of generalization and diversity metrics with model size could also provide insights.

## Limitations
- Limited task scope: The study focuses primarily on summarization and instruction following tasks, which may not generalize to other LLM applications like code generation or reasoning tasks.
- Single KL penalty setting: Only one KL penalty coefficient (βKL = 0.05) was tested, limiting understanding of the full tradeoff space.
- GPT-4 evaluation dependency: Using GPT-4 as a simulated human evaluator introduces potential bias and may not perfectly capture human preferences.
- Model size constraint: Experiments were conducted on 7B parameter models, and results may differ for larger or smaller models.

## Confidence
- **High confidence:** RLHF improves out-of-distribution generalization compared to SFT
- **Medium confidence:** RLHF reduces output diversity
- **Medium confidence:** Mode collapse occurs in RLHF
- **Low confidence:** The KL penalty coefficient doesn't fully control the diversity-performance tradeoff

## Next Checks
1. **KL coefficient sweep validation:** Replicate the diversity-performance tradeoff experiments across a wider range of KL penalty coefficients (e.g., βKL ∈ {0.01, 0.05, 0.1, 0.2}) to better understand the tradeoff boundary and identify potential sweet spots.

2. **Cross-task replication:** Test the core findings (generalization improvement, diversity reduction, mode collapse) on additional tasks like code generation, mathematical reasoning, or creative writing to assess generalizability of the observed tradeoffs.

3. **Alternative regularization methods:** Compare RLHF with other regularization approaches (e.g., entropy regularization, context distillation) to determine if the diversity reduction is specific to KL penalty or a broader RLHF phenomenon.