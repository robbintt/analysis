---
ver: rpa2
title: Evaluating the Performance of Large Language Models for Spanish Language in
  Undergraduate Admissions Exams
arxiv_id: '2312.16845'
source_url: https://arxiv.org/abs/2312.16845
tags:
- bard
- score
- gpt-3
- questions
- exam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of large language models (LLMs),
  specifically GPT-3.5 and BARD, in undergraduate admissions exams for the National
  Polytechnic Institute in Mexico. The exams cover Engineering/Mathematical and Physical
  Sciences, Biological and Medical Sciences, and Social and Administrative Sciences.
---

# Evaluating the Performance of Large Language Models for Spanish Language in Undergraduate Admissions Exams

## Quick Facts
- arXiv ID: 2312.16845
- Source URL: https://arxiv.org/abs/2312.16845
- Reference count: 26
- Key outcome: GPT-3.5 and BARD exceeded minimum acceptance scores on Mexican undergraduate admissions exams by up to 75%, with GPT-3.5 marginally outperforming BARD overall.

## Executive Summary
This study evaluates the performance of GPT-3.5 and BARD on undergraduate admissions exams for Mexico's National Polytechnic Institute. Both models demonstrated proficiency in Engineering/Mathematical and Physical Sciences, Biological and Medical Sciences, and Social and Administrative Sciences, with scores exceeding minimum acceptance thresholds for respective programs. GPT-3.5 showed better performance in Mathematics and Physics, while BARD excelled in History and factual questions. The findings suggest LLMs can support learning processes but also raise concerns about exam integrity and the need for format modifications to ensure fair assessments.

## Method Summary
The study manually input 140 multiple-choice questions from IPN admission exams into GPT-3.5 and BARD web interfaces, using prompts to ensure answer selection. Responses were compared against official answer sheets to calculate accuracy percentages across Engineering/Mathematical and Physical Sciences, Biological and Medical Sciences, and Social and Administrative Sciences domains.

## Key Results
- Both models exceeded minimum acceptance scores for respective academic programs by up to 75%
- GPT-3.5 outperformed BARD in Mathematics and Physics across all exams
- BARD performed better in History and questions requiring factual information
- Overall, GPT-3.5 marginally surpassed BARD with scores of 60.94% and 60.42%, respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3.5 outperforms BARD in mathematics and physics because both models are trained on broad academic datasets, but GPT-3.5's fine-tuning and prompt-response handling give it a slight edge in quantitative reasoning tasks.
- Mechanism: GPT-3.5's underlying architecture and fine-tuning process may have prioritized numerical and symbolic reasoning, leading to better performance on structured mathematical problems compared to BARD. Additionally, GPT-3.5's interface allowed for more effective iterative clarification on math word problems.
- Core assumption: The training data and fine-tuning processes of GPT-3.5 and BARD differ in ways that affect their quantitative reasoning performance, and the user interface differences enable more effective problem-solving.
- Evidence anchors:
  - [abstract] "GPT-3.5 outperformed BARD in Mathematics and Physics, while BARD performed better in History and questions related to factual information."
  - [section] "GPT-3.5 consistently outperforms BARD in Mathematics across all exams."
  - [corpus] Weak evidence for mechanism; only comparative performance noted, not underlying reasons.
- Break condition: If both models were trained on identical datasets with identical fine-tuning, or if interface differences were eliminated, this performance gap would likely narrow or disappear.

### Mechanism 2
- Claim: Both models perform well in language comprehension because they are trained on vast multilingual corpora and fine-tuned for Spanish, allowing them to handle reading and writing comprehension tasks effectively.
- Mechanism: The pre-training on diverse datasets across multiple languages, including Spanish, enables both models to understand and generate text in Spanish. The fine-tuning process further adapts them to the specific linguistic patterns and comprehension tasks required in the exams.
- Core assumption: The models' training data includes substantial Spanish-language content, and the fine-tuning process effectively adapts them to Spanish language comprehension tasks.
- Evidence anchors:
  - [abstract] "Both models demonstrated proficiency, exceeding the minimum acceptance scores for respective academic programs to up to 75% for some academic programs."
  - [section] "The models excel in solving specific and well-known academic problems where an apparent problem is presented, and a formula can be applied to find a solution..."
  - [corpus] No direct evidence about training data composition or fine-tuning specifics.
- Break condition: If the training data lacked sufficient Spanish content or if the fine-tuning process was ineffective for Spanish language tasks, performance on language comprehension would degrade.

### Mechanism 3
- Claim: BARD performs better on history and factual questions because its training or fine-tuning may have emphasized retrieval of factual information and historical knowledge.
- Mechanism: BARD's architecture or training process may have incorporated more structured factual knowledge bases or retrieval mechanisms, giving it an advantage on questions requiring specific historical or factual recall.
- Core assumption: The training or fine-tuning process for BARD included more emphasis on factual knowledge retrieval or structured historical data.
- Evidence anchors:
  - [abstract] "BARD performed better in History and questions related to factual information."
  - [section] "BARD exhibited proficiency in tasks requiring factual information, such as history-related or conceptual problems."
  - [corpus] No evidence about BARD's training emphasis on factual knowledge.
- Break condition: If both models were trained identically on factual knowledge, or if the difference was due to chance variation in the specific questions selected, this performance difference would not be reliable.

## Foundational Learning

- Concept: Large Language Models (LLMs) and their capabilities
  - Why needed here: Understanding how LLMs work is essential to interpreting their performance on exams and identifying their strengths and weaknesses.
  - Quick check question: What are the key differences between GPT-3.5 and BARD in terms of architecture and training that might explain their performance differences?

- Concept: Spanish language proficiency and its role in LLM performance
  - Why needed here: The study evaluates models in Spanish, so understanding how language affects LLM performance is crucial for interpreting results.
  - Quick check question: How might the availability and quality of Spanish training data affect the performance of GPT-3.5 and BARD on Spanish-language exams?

- Concept: Educational assessment and admission exam structure
  - Why needed here: Understanding the structure and content of the admission exams is necessary to interpret the models' performance and identify areas where they excel or struggle.
  - Quick check question: What are the key components of the IPN admission exams, and how do they test different skills and knowledge areas?

## Architecture Onboarding

- Component map: Question input -> Model response generation -> Answer comparison -> Performance calculation -> Result interpretation
- Critical path: Question input → Model response generation → Answer comparison → Performance calculation → Result interpretation
- Design tradeoffs: The manual question input process is time-consuming but allows for precise control over question formatting and model interaction. Automated input might be faster but could introduce formatting errors or unintended model behavior.
- Failure signatures: Incorrect answers due to model misunderstanding, failure to respond due to question complexity or format, inconsistent performance across different question types.
- First 3 experiments:
  1. Test model performance on a small subset of questions from each topic area to identify strengths and weaknesses.
  2. Vary the prompt format (e.g., "Seleccionar de las siguientes opciones..." vs. no prompt) to determine optimal prompting strategy.
  3. Compare model performance on text-only questions vs. questions with visual components (even when described in text) to assess impact of visual information absence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the format and content of undergraduate admissions exams be modified to ensure fair and reliable assessments in the era of widely available LLMs?
- Basis in paper: [explicit] The paper discusses concerns about potential cheating and unfair advantages if students rely on LLMs for answers, suggesting the need to modify exam formats.
- Why unresolved: The paper mentions this as a potential issue but does not propose specific solutions or modifications to address it.
- What evidence would resolve it: Studies comparing different exam formats (e.g., open-book, oral exams, project-based assessments) and their effectiveness in preventing LLM-assisted cheating while maintaining fairness and reliability.

### Open Question 2
- Question: To what extent do LLMs like GPT-3.5 and BARD impact the development of critical thinking skills in students if they become overly reliant on these models for information and problem-solving?
- Basis in paper: [explicit] The paper mentions educators' concerns that students might stop developing critical thinking skills if they become accustomed to relying solely on LLMs for answers without reasoning.
- Why unresolved: The paper highlights the concern but does not provide empirical evidence on the actual impact of LLM usage on students' critical thinking development.
- What evidence would resolve it: Longitudinal studies comparing critical thinking skills of students who frequently use LLMs with those who do not, while controlling for other factors that may influence critical thinking development.

### Open Question 3
- Question: What are the specific challenges faced by LLMs in solving math word problems presented in a textual format, and how can these challenges be addressed to improve their performance?
- Basis in paper: [explicit] The paper mentions that GPT-3.5 and BARD encounter challenges when faced with math word problems that require students to interpret and solve the problem from a textual description.
- Why unresolved: The paper identifies the issue but does not delve into the specific reasons behind the models' struggles or propose solutions to improve their performance in this area.
- What evidence would resolve it: Analysis of the types of errors made by LLMs when solving math word problems, identification of the underlying causes (e.g., language understanding, reasoning, calculation), and exploration of techniques to enhance the models' capabilities in this domain.

## Limitations

- Manual question input process introduces time constraints and potential inconsistencies in question presentation
- Evaluation limited to multiple-choice questions, missing open-ended problem-solving capabilities
- Only examines two commercial LLMs without comparing to other available models or exploring different prompting strategies systematically

## Confidence

- High Confidence: The core finding that both GPT-3.5 and BARD can exceed minimum passing scores on Spanish-language undergraduate admission exams is well-supported by the experimental methodology and results.
- Medium Confidence: The comparative performance between GPT-3.5 and BARD on different subject areas is reliable for this specific exam context, but may not generalize to other exam formats or content domains.
- Low Confidence: The proposed mechanisms explaining why GPT-3.5 outperforms BARD in mathematics and why BARD excels in factual questions lack direct evidence from training data or model architecture details.

## Next Checks

1. **Training Data Analysis:** Obtain information about the Spanish-language content and subject-specific coverage in the training corpora of both GPT-3.5 and BARD to verify whether observed performance differences align with training emphasis areas.
2. **Cross-Format Evaluation:** Test both models on the same exam content presented in different formats (multiple-choice vs. open-ended) to determine whether performance differences persist across question types.
3. **Visual Information Processing:** Evaluate model performance on textually-described visual problems versus actual visual problems to quantify the impact of the models' inability to process images on exam performance.