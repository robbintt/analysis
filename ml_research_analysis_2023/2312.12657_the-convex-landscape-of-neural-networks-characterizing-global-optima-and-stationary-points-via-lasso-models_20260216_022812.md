---
ver: rpa2
title: 'The Convex Landscape of Neural Networks: Characterizing Global Optima and
  Stationary Points via Lasso Models'
arxiv_id: '2312.12657'
source_url: https://arxiv.org/abs/2312.12657
tags:
- convex
- problem
- program
- neural
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a novel convex optimization framework to train
  two-layer neural networks with piecewise linear activations, including ReLU and
  leaky ReLU. They prove that such networks can be reformulated as convex sparse recovery
  problems (variants of group Lasso), enabling global optimization via standard convex
  solvers without heuristics or hyperparameter tuning.
---

# The Convex Landscape of Neural Networks: Characterizing Global Optima and Stationary Points via Lasso Models

## Quick Facts
- **arXiv ID**: 2312.12657
- **Source URL**: https://arxiv.org/abs/2312.12657
- **Reference count**: 40
- **Primary result**: Two-layer neural networks with piecewise linear activations can be reformulated as convex group Lasso problems, enabling global optimization without heuristics.

## Executive Summary
This paper presents a novel convex optimization framework for training two-layer neural networks with piecewise linear activations (ReLU, leaky ReLU, absolute value). By reformulating the non-convex neural network training problem as a convex sparse recovery problem (group Lasso variant), the authors enable global optimization using standard convex solvers without the need for heuristics or hyperparameter tuning. The key insight is that ReLU activations can be represented as linear inequalities over hyperplane arrangements, transforming the problem into a tractable convex program.

The framework provides polynomial-time solvability when the data matrix has bounded rank (as in convolutional networks) and includes a low-rank approximation scheme with guaranteed bounds for high-dimensional fully connected networks. Remarkably, the authors prove that all stationary points of the original non-convex objective correspond to global optima of a subsampled convex program, providing theoretical justification for the approach. Numerical experiments demonstrate that this convex method often outperforms traditional non-convex training approaches while being robust to optimizer hyperparameters.

## Method Summary
The method reformulates two-layer neural network training with piecewise linear activations as a convex sparse recovery problem. The ReLU activation function is expressed as a set of linear inequalities over hyperplane arrangements, which transforms the non-convex problem into a group Lasso-style convex program. For data matrices with bounded rank (common in convolutional networks), the number of hyperplane arrangements is polynomial in the number of samples and neurons, enabling polynomial-time solvability. For high-dimensional data, a low-rank approximation scheme is used with approximation guarantees. The convex program is solved using standard convex optimization tools (CVX, CVXPY) or custom stochastic gradient descent implementations.

## Key Results
- Two-layer neural networks with piecewise linear activations can be exactly reformulated as convex group Lasso problems
- The convex program is polynomial-time solvable when the data matrix rank is bounded
- All stationary points of the non-convex objective correspond to global optima of a subsampled convex program
- Numerical experiments show the convex approach often outperforms traditional non-convex training methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The convex program exactly solves the non-convex two-layer neural network training problem when the data matrix has bounded rank.
- Mechanism: By reformulating the ReLU activation as linear inequalities over hyperplane arrangements, the problem becomes a group Lasso-style convex program. The number of arrangements is polynomial in samples and neurons when rank(X) is bounded.
- Core assumption: The data matrix rank r is bounded (constant), which holds for convolutional networks.
- Evidence anchors:
  - [abstract]: "Remarkably, the worst-case complexity to solve the convex program is polynomial in the number of samples and number of neurons when the rank of the data matrix is bounded, which is the case in convolutional networks."
  - [section 2]: Theorem 2.1 proves the exact convex formulation and polynomial-time solvability.
- Break condition: If rank(X) grows with dimension d (e.g., fully connected networks), complexity becomes exponential in d.

### Mechanism 2
- Claim: All stationary points of the non-convex objective correspond to global optima of a subsampled convex program.
- Mechanism: Clarke stationary points satisfy specific conditions that map directly to KKT conditions of the convex subsampled problem with m arrangements.
- Core assumption: The number of sampled arrangements equals the number of hidden neurons m.
- Evidence anchors:
  - [abstract]: "We also show that all the stationary of the nonconvex training objective can be characterized as the global optimum of a subsampled convex program."
  - [section 2]: Theorem 2.2 proves this correspondence formally.
- Break condition: If the subsampling misses critical arrangement patterns, the correspondence breaks.

### Mechanism 3
- Claim: Low-rank approximation of the data matrix enables polynomial-time approximate solutions with guaranteed bounds.
- Mechanism: Using rank-k approximation of X reduces the number of hyperplane arrangements from exponential to polynomial while maintaining approximation guarantees.
- Core assumption: Singular values decay sufficiently fast so that σk+1/β is small.
- Evidence anchors:
  - [abstract]: "To extend our method to training data of arbitrary rank, we develop a novel polynomial-time approximation scheme based on zonotope subsampling that comes with a guaranteed approximation ratio."
  - [section 3]: Theorem 3.1 provides the approximation guarantee.
- Break condition: If singular values decay slowly or data is full rank, approximation error may be large.

## Foundational Learning

- Concept: Semi-infinite programming duality
  - Why needed here: The dual of the non-convex problem has infinitely many constraints (one per activation pattern), requiring semi-infinite programming theory to establish strong duality.
  - Quick check question: What is the relationship between the primal non-convex problem and its semi-infinite dual?

- Concept: Hyperplane arrangements and zonotopes
  - Why needed here: Understanding how ReLU activation patterns correspond to regions in the arrangement of hyperplanes through the origin, and how these relate to zonotope vertices.
  - Quick check question: How does the number of hyperplane arrangements grow with the rank of the data matrix?

- Concept: Group Lasso regularization and sparsity
  - Why needed here: The convex program uses group ℓp-norm regularization to promote sparsity in the mixture of linear models.
  - Quick check question: Why does the group Lasso structure naturally emerge from the convex reformulation?

## Architecture Onboarding

- Component map:
  - Data matrix X (n×d) -> Hyperplane arrangements {Di}P i=1 -> Convex program variables {wi}P i=1 -> Convex solver (CVX, CVXPY, or custom SGD) -> Solution mapping

- Critical path:
  1. Compute rank-k approximation of X if needed
  2. Sample hyperplane arrangements (exact or random)
  3. Construct convex program with appropriate constraints
  4. Solve using convex solver
  5. Map solution back to neural network parameters

- Design tradeoffs:
  - Exact vs approximate: Exact enumeration is exponential in rank but guarantees optimality; sampling is polynomial but approximate.
  - Rank-k choice: Higher k gives better approximation but more arrangements; lower k is faster but less accurate.
  - Sampling method: Gaussian sampling is simple but may miss important patterns; convolutional-based sampling is more efficient for image data.

- Failure signatures:
  - Poor test performance despite good training: Likely insufficient rank-k approximation or bad sampling.
  - Solver fails to converge: Constraints may be too loose or ill-conditioned; try regularization or different solver.
  - High number of active neurons: Regularization parameter β may be too small; increase β.

- First 3 experiments:
  1. Synthetic low-rank data (e.g., rank 2, n=10, d=5) with exact enumeration to verify optimality.
  2. MNIST with rank-50 approximation and random arrangement sampling to test scalability.
  3. CIFAR-10 with convolutional-based arrangement sampling to demonstrate practical efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the convex optimization framework for two-layer ReLU networks be extended to deeper architectures while maintaining polynomial-time solvability?
- Basis in paper: The paper focuses on two-layer networks and conjectures that more efficient solvers might be developed for larger scale experiments. The authors note that follow-up papers have extended their approach to deep linear networks, generative networks, and transformer networks via convex duality.
- Why unresolved: Extending the framework to deeper networks likely introduces additional non-convexities and computational complexities that may not be tractable with current convex optimization techniques.
- What evidence would resolve it: A theoretical proof demonstrating that the convex reformulation remains polynomial-time solvable for a specific class of deep networks, or a counterexample showing that the problem becomes NP-hard for certain deep architectures.

### Open Question 2
- Question: What is the exact relationship between the implicit regularization of neural networks trained via gradient descent and the explicit regularization induced by the convex program's nuclear norm or group ℓp norm?
- Basis in paper: The paper discusses how the convex program induces structural regularization (nuclear norm, group ℓp norm) and contrasts this with kernel methods that approximate neural network training as minimum ℓ2-norm interpolation. It also mentions that nuclear norm plays a role in the implicit regularization of linear networks trained via gradient descent.
- Why unresolved: While the paper establishes the existence of these regularization terms, the precise connection between the implicit and explicit regularization mechanisms, and their respective impacts on generalization, remains unclear.
- What evidence would resolve it: Empirical studies comparing the generalization performance of neural networks trained via gradient descent with their convex program counterparts under different regularization schemes, or a theoretical analysis characterizing the equivalence or divergence of these regularization effects.

### Open Question 3
- Question: How does the choice of piecewise linear activation function (ReLU, leaky ReLU, absolute value) affect the complexity of the hyperplane arrangements and the subsequent solvability of the convex program?
- Basis in paper: The paper explicitly considers ReLU, leaky ReLU, and absolute value activations, defining them as piecewise linear functions with different slopes for positive and negative inputs. The number of hyperplane arrangements, which determines the complexity of the convex program, is influenced by the activation function.
- Why unresolved: The paper does not provide a detailed analysis of how the specific choice of activation function impacts the number of hyperplane arrangements or the computational complexity of solving the convex program. This could have implications for the practical applicability of the framework to different types of networks.
- What evidence would resolve it: A theoretical analysis deriving bounds on the number of hyperplane arrangements for each activation function, or empirical studies comparing the performance of the convex program with different activations on various datasets.

## Limitations
- The polynomial-time complexity claims rely critically on bounded rank assumptions that may not hold for general high-dimensional data distributions.
- The approximation guarantees for the low-rank scheme depend on unknown decay rates of singular values for real-world datasets.
- The correspondence between non-convex stationary points and convex global optima assumes the subsampled arrangements capture all relevant activation patterns.

## Confidence

- **High confidence**: The exact convex reformulation for low-rank data matrices and the polynomial-time solvability proof (Theorem 2.1).
- **Medium confidence**: The approximation scheme for high-dimensional data (Theorem 3.1) and the stationary point correspondence result (Theorem 2.2), pending empirical validation.
- **Low confidence**: Claims about superiority over non-convex training methods across all datasets, as this depends heavily on problem-specific factors.

## Next Checks

1. **Rank analysis**: Measure the rank distribution of real datasets (MNIST, CIFAR-10) and compute singular value decay rates to validate the bounded-rank assumption and low-rank approximation feasibility.

2. **Arrangement sampling**: Implement systematic experiments varying the number of sampled arrangements to determine the minimum sample size needed for stable performance across different datasets.

3. **Comparison benchmark**: Conduct controlled experiments comparing the convex approach against state-of-the-art non-convex optimizers (Adam, SGD with momentum) on multiple datasets, reporting both training and test performance with statistical significance testing.