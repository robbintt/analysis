---
ver: rpa2
title: 'InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through
  Psychological Interviews'
arxiv_id: '2310.17976'
source_url: https://arxiv.org/abs/2310.17976
tags:
- personality
- chatbots
- role-playing
- questions
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for evaluating the personality
  fidelity of role-playing agents (RPAs) using psychological interviews. The authors
  propose InCharacter, which generates open-ended interview questions based on the
  Big Five and MBTI personality frameworks.
---

# InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews

## Quick Facts
- arXiv ID: 2310.17976
- Source URL: https://arxiv.org/abs/2310.17976
- Reference count: 14
- One-line primary result: InCharacter framework achieves up to 80.7% accuracy in measuring RPA personality traits

## Executive Summary
This paper introduces InCharacter, a novel framework for evaluating personality fidelity in role-playing agents (RPAs) through psychological interviews. The approach transforms traditional Likert-scale personality assessments into open-ended conversational interviews, enabling richer responses that better reflect character personalities. Using LLMs to generate interview questions and score responses based on Big Five and MBTI frameworks, the method achieves up to 80.7% alignment with human-perceived personalities. Experiments with 32 distinct characters demonstrate that state-of-the-art RPAs can effectively portray personality traits consistent with their source material.

## Method Summary
The InCharacter framework transforms standard psychological questionnaires into open-ended interview prompts tailored for role-playing agents. For each character, interview questions are generated based on Big Five Inventory and MBTI frameworks. RPAs are then interviewed in separate sessions, and their responses are analyzed by LLM evaluators acting as "seasoned psychologists" to produce structured personality scores. The system compares these scores against ground truth MBTI labels and human-perceived personalities to assess fidelity. The framework addresses limitations of traditional closed-ended assessments by eliciting more personality-rich responses through conversational prompts.

## Key Results
- InCharacter achieves up to 80.7% accuracy in measuring personality traits of role-playing agents
- RPAs show personalities highly aligned with human-perceived personalities, with 82.8% alignment rate
- The interview-style approach outperforms traditional closed-ended assessments in capturing nuanced personality traits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open-ended interview-style questions elicit more personality-rich responses than closed-ended Likert scales.
- Mechanism: Conversational prompts encourage RPAs to produce detailed, contextual answers reflecting personality traits and speech habits.
- Core assumption: RPAs have sufficient conversational depth to elaborate on character traits when prompted conversationally.
- Evidence anchors: [abstract] Open-ended answers are "more consistent with their personas"; [section 3.1] Likert-scale testing offers limited openness compared to open-ended inquiries.
- Break condition: Agents may refuse to elaborate or default to generic responses if too limited.

### Mechanism 2
- Claim: LLM-based evaluators can accurately score personality traits from open-ended responses.
- Mechanism: LLMs analyze textual responses and output structured personality scores, bypassing need for human evaluators.
- Core assumption: LLMs have sufficient contextual understanding to interpret personality-related language cues.
- Evidence anchors: [abstract] RPAs portray personality traits with 82.8% alignment; [section 4.3] LLMs convert textual remarks into scores and types.
- Break condition: Scoring accuracy may degrade if LLM lacks nuance or responses are ambiguous.

### Mechanism 3
- Claim: RPAs inherently align with human-perceived character personalities when built with character-specific system prompts and memory.
- Mechanism: Character-specific context grounds agents in rich behavioral cues for consistent personality expression.
- Core assumption: System prompt and memory provide sufficient behavioral cues for consistent trait expression.
- Evidence anchors: [abstract] RPAs exhibit personalities aligned with human-perceived personalities at 80.7% accuracy; [section 4.1] RPAs portray characters based on character-specific system prompts and memory.
- Break condition: Inconsistent or contradictory system prompts may produce off-character responses.

## Foundational Learning

- Concept: Big Five Inventory (BFI)
  - Why needed here: Provides validated framework for assessing personality across five dimensions
  - Quick check question: What are the five personality dimensions measured by the BFI?

- Concept: Myers-Briggs Type Indicator (MBTI)
  - Why needed here: Offers popular typology framework for character profiling and alignment with fan-labeled data
  - Quick check question: What are the four MBTI dimensions used to classify personality types?

- Concept: Prompt engineering for LLMs
  - Why needed here: Transforms psychological questions into natural interview-style prompts that RPAs can respond to authentically
  - Quick check question: How would you rephrase a Likert-scale question into an open-ended conversational prompt?

## Architecture Onboarding

- Component map:
  Questionnaire Design -> Prompt Transformation -> Role-Playing Chatbot Interviews -> LLM-based Scoring -> Personality Assessment
  Supporting: Ground Truth MBTI labels, BFI psychologist baselines

- Critical path:
  1. Transform standard psychological questions into open-ended interview-style prompts
  2. Conduct interviews with role-playing chatbots in separate sessions
  3. Use LLM evaluators to score responses per dimension
  4. Compare LLM scores to ground truth or human baselines

- Design tradeoffs:
  - Open-ended vs. closed-ended: Richer responses vs. structured scoring
  - LLM evaluator vs. human evaluator: Scalable automation vs. nuanced accuracy
  - Single vs. batched Q&A evaluation: More detailed per-question context vs. consistency across batches

- Failure signatures:
  - Chatbot refusal or non-compliance with interview questions
  - LLM evaluator producing inconsistent scores across batches
  - High variance in scores when evaluating Q&A pairs individually

- First 3 experiments:
  1. Run a pilot interview with one character using 5 open-ended questions and manually check response richness
  2. Compare LLM-based scoring vs. 16Personalities API scoring on a subset of responses
  3. Measure inter-batch consistency of LLM scores for one personality dimension

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed interview-style approach compare to traditional Likert-scale testing in terms of measuring personality traits of role-playing chatbots?
- Basis in paper: [explicit] The paper discusses the drawbacks of Likert-scale testing and proposes an interview-style approach.
- Why unresolved: While limitations of Likert-scale testing are mentioned, no direct comparison of the two methods' effectiveness is provided.
- What evidence would resolve it: A comparative study using both methods on the same set of RPAs with result analysis.

### Open Question 2
- Question: How does the choice of LLM (e.g., ChatGPT vs GPT-4) affect the accuracy of personality assessments in the proposed framework?
- Basis in paper: [explicit] The paper mentions experimenting with different LLMs like ChatGPT and GPT-4 as evaluators.
- Why unresolved: No detailed analysis of how LLM choice impacts personality assessment accuracy is provided.
- What evidence would resolve it: Systematic comparison of assessment results using different LLMs on the same RPAs.

### Open Question 3
- Question: How do the personality traits exhibited by role-playing chatbots align with the characters' personalities in their original fictional works?
- Basis in paper: [inferred] The paper discusses effectiveness in portraying personality traits consistent with human perceptions.
- Why unresolved: No direct comparison between RPA personality traits and original characters' personalities is provided.
- What evidence would resolve it: Study comparing RPA personality traits with original characters' personalities using expert evaluations or fan perceptions.

## Limitations
- LLM-based scoring introduces potential bias from the underlying model's own personality traits and interpretive patterns
- Framework assumes sufficient conversational depth in RPAs to elaborate on personality traits, which may not hold for all agent architectures
- Open-ended responses may introduce scoring variability that is difficult to quantify

## Confidence

- **High Confidence**: Framework's ability to outperform traditional closed-ended assessments (mechanism 1), and observed alignment between RPAs and human-perceived personalities (mechanism 3)
- **Medium Confidence**: LLM's capability to accurately score personality traits from open-ended responses (mechanism 2), due to potential variability in model performance
- **Medium Confidence**: Effectiveness of prompt engineering in eliciting personality-rich responses, depends heavily on specific RPAs being evaluated

## Next Checks

1. **Cross-LLM Validation**: Test scoring consistency across multiple LLM models to quantify and mitigate evaluator bias. Compare variance in personality scores across different LLM evaluators for the same responses.

2. **Refusal Handling Analysis**: Systematically test how framework handles RPAs that refuse to answer questions or provide minimal responses. Evaluate whether refusal rates correlate with specific personality types or question categories.

3. **Human-LLM Agreement Study**: Conduct controlled study comparing LLM-based scoring against human psychologist evaluations on same RPA responses. Measure inter-rater reliability and identify dimensions where agreement is lowest.