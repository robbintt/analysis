---
ver: rpa2
title: 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy'
arxiv_id: '2306.05268'
source_url: https://arxiv.org/abs/2306.05268
tags:
- information
- learning
- unique
- shared
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FACTOR CL, a new method for multimodal contrastive
  learning that addresses the limitations of existing approaches in handling low shared
  or high unique task-relevant information. The method factorizes task-relevant information
  into shared and unique representations, captures relevant information via maximizing
  mutual information lower bounds, and removes irrelevant information via minimizing
  upper bounds.
---

# Factorized Contrastive Learning: Going Beyond Multi-view Redundancy

## Quick Facts
- arXiv ID: 2306.05268
- Source URL: https://arxiv.org/abs/2306.05268
- Reference count: 40
- Factorized contrastive learning achieves state-of-the-art results on six multimodal benchmarks

## Executive Summary
This paper introduces FACTOR CL, a novel method for multimodal contrastive learning that addresses the limitations of existing approaches in handling low shared or high unique task-relevant information. The method factorizes task-relevant information into shared and unique representations, captures relevant information via maximizing mutual information lower bounds, and removes irrelevant information via minimizing upper bounds. Experiments on synthetic and real-world datasets demonstrate that FACTOR CL outperforms standard contrastive learning and other baselines, effectively capturing both shared and unique information.

## Method Summary
FACTOR CL addresses the limitations of standard contrastive learning by factorizing task-relevant information into shared and unique representations, and by using mutual information bounds to capture relevant information while removing irrelevant information. The method employs separate encoders for shared and unique representations, and optimizes MI lower bounds for capturing task-relevant information and MI upper bounds for removing task-irrelevant information. Multimodal augmentations are used to approximate task relevance without labels. The training procedure involves sampling-based optimization of the factorized representations and the MI bounds.

## Key Results
- FACTOR CL achieves state-of-the-art results on six multimodal benchmarks, including MIMIC-III, MOSEI/MOSI, UR-FUNNY, MUSTARD, and IRFL
- On synthetic data, FACTOR CL outperforms standard contrastive learning and other baselines when the shared-irrelevant information ratio is high
- On real-world datasets, FACTOR CL shows significant improvements over standard contrastive learning and other baselines, especially on tasks with high unique information

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The proposed upper bound INCE-CLUB enables removal of task-irrelevant shared information without separate optimization.
- **Mechanism**: INCE-CLUB provides a plug-in upper bound that can be estimated "for free" by reusing the optimal critic from INCE. This upper bound captures I(X1; X2|Y), the shared information that remains between modalities after observing the task label Y. By minimizing this upper bound during training, the model actively removes information that is shared across modalities but irrelevant to the task.
- **Core assumption**: The optimal critic from INCE can be reused to compute the upper bound, and the bound is tight enough to effectively remove task-irrelevant shared information.
- **Evidence anchors**:
  - [abstract]: "capturing task-relevant information via maximizing MI lower bounds and removing task-irrelevant information via minimizing MI upper bounds"
  - [section]: "INCE-CLUB (X1; X2) gives a desired upper bound of I(X1; X2) 'for free' while avoiding separately optimizing lower bound and upper bounds"
  - [corpus]: No direct evidence; this is a novel contribution of the paper.
- **Break condition**: If the INCE-CLUB upper bound is too loose or the optimal critic is not well-approximated, task-irrelevant shared information may not be effectively removed, leading to suboptimal representations.

### Mechanism 2
- **Claim**: Factorizing representations into shared (ZS) and unique (ZU) components enables capturing task-relevant information from both modalities.
- **Mechanism**: The method explicitly optimizes separate representations for shared information (ZS1, ZS2) and unique information (ZU1, ZU2) using different MI bounds. This allows the model to capture I(X1; X2; Y) (shared) and I(X1; Y|X2), I(X2; Y|X1) (unique) simultaneously, rather than only maximizing I(X1; X2) as in standard CL.
- **Core assumption**: Task-relevant information can be decomposed into shared and unique components, and these components can be learned separately.
- **Evidence anchors**:
  - [abstract]: "factorizing task-relevant information into shared and unique representations"
  - [section]: "ZS1 = arg max I(Z1; X2; Y), ZU1 = arg max I(Z1; Y|X2)" (Eq. 6-7)
  - [corpus]: Weak evidence; related work discusses factorized representations but not in the context of contrastive learning with MI bounds.
- **Break condition**: If task-relevant information cannot be cleanly decomposed into shared and unique components, the factorization may lead to redundant or incomplete representations.

### Mechanism 3
- **Claim**: Multimodal augmentations enable self-supervised approximation of task relevance without explicit labels.
- **Mechanism**: By defining optimal unimodal augmentations (I(Xi; X'i) = I(Xi; Y)) and multimodal augmentations (I(X1, X2; X'1, X'2) = I(X1, X2; Y)), the method replaces label-dependent MI terms with augmentation-based ones. This allows learning of task-relevant information in a self-supervised setting.
- **Core assumption**: Augmentations can preserve task-relevant information while changing task-irrelevant information, enabling them to approximate the unobserved task.
- **Evidence anchors**:
  - [abstract]: "multimodal data augmentations to approximate task relevance without labels"
  - [section]: "Definition 2. (Optimal unimodal augmentation) X'1 is an optimal unimodal augmentation for X1 when I(X; X') = I(X; Y)"
  - [corpus]: Weak evidence; optimal augmentations are proposed but not extensively validated across diverse datasets.
- **Break condition**: If augmentations fail to preserve task-relevant information or introduce task-irrelevant information, the approximation will be poor, leading to suboptimal learning.

## Foundational Learning

- **Concept**: Mutual Information (MI) and its bounds
  - **Why needed here**: The method relies on MI to quantify shared and unique information, and uses lower and upper bounds to optimize representations. Understanding MI is crucial for grasping how the method captures task-relevant information.
  - **Quick check question**: What is the difference between I(X1; X2) and I(X1; X2; Y), and how do they relate to shared and task-relevant shared information?

- **Concept**: Contrastive Learning and InfoNCE
  - **Why needed here**: The method builds upon contrastive learning, using InfoNCE objectives to estimate MI bounds. Understanding contrastive learning is essential for implementing and extending the method.
  - **Quick check question**: How does InfoNCE provide a lower bound on mutual information, and why is this useful for representation learning?

- **Concept**: Information Decomposition (Shared, Unique, and Task-Relevant Information)
  - **Why needed here**: The method decomposes task-relevant information into shared and unique components. Understanding this decomposition is key to grasping the motivation and design of the method.
  - **Quick check question**: Given two modalities X1 and X2 and a task Y, how can you decompose I(X1, X2; Y) into shared and unique task-relevant information?

## Architecture Onboarding

- **Component map**: 
  - Encoders -> MLP Heads -> Critics -> MI Bounds Optimization
  - Encoders: Two encoders (e1, e2) for modalities X1 and X2, outputting representations z1 and z2
  - MLP Heads: Additional MLP heads on top of encoders for each MI bound (shared and unique)
  - Critics: Separate critics (fÎ¸) for each MI bound, taking concatenated encoder head outputs as input
  - Augmentations: Functions for generating unimodal and multimodal augmentations (X'1, X'2)

- **Critical path**: (1) Encode input modalities X1 and X2 using encoders. (2) Apply MLP heads to encoder outputs. (3) Concatenate head outputs for each MI bound. (4) Compute MI bounds using critics. (5) Optimize representations by maximizing lower bounds and minimizing upper bounds.

- **Design tradeoffs**: (1) Factorizing representations increases model complexity but enables capturing both shared and unique information. (2) Using upper bounds to remove task-irrelevant information adds computational cost but improves representation quality. (3) Multimodal augmentations enable self-supervised learning but may not perfectly approximate task relevance.

- **Failure signatures**: (1) Poor performance on tasks with high unique information (e.g., sarcasm detection) indicates failure to capture unique information. (2) Inability to outperform standard CL on tasks with high shared information suggests ineffective removal of task-irrelevant information. (3) High variance in results across datasets indicates sensitivity to augmentations or model architecture.

- **First 3 experiments**:
  1. **Synthetic Data**: Generate synthetic data with controllable ratios of shared and unique task-relevant information. Train FACTOR CL and baselines (SimCLR, Cross+Self) and compare performance. This verifies the method's ability to capture both shared and unique information.
  2. **MIMIC-III**: Use MIMIC-III dataset with tabular and time-series modalities for disease prediction. Train FACTOR CL and baselines, and compare performance. This tests the method on a real-world dataset with high unique information (health indicators, sensor readings).
  3. **IRFL**: Use IRFL dataset with images and figurative captions. Perform continued pre-training on CLIP using FACTOR CL objectives and compare to standard fine-tuning and continued pre-training. This evaluates the method's ability to improve pre-trained multimodal models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FACTOR CL scale with increasing dimensionality of the input data? The paper demonstrates good performance on datasets with 50-dimensional latent variables, but how would it perform on higher-dimensional data like high-resolution images or complex sensor data?
- Basis in paper: [inferred] The paper mentions using 50-dimensional latent variables in the synthetic dataset generation, but does not explore higher-dimensional settings.
- Why unresolved: The paper does not provide experiments or analysis on higher-dimensional data, leaving the scalability of FACTOR CL in such settings unknown.
- What evidence would resolve it: Experiments on datasets with higher-dimensional inputs (e.g., high-resolution images, complex sensor data) comparing FACTOR CL's performance to other baselines would provide evidence on its scalability.

### Open Question 2
- Question: How sensitive is FACTOR CL to the choice of multimodal augmentations? The paper suggests that multimodal augmentations are crucial for approximating task relevance, but does not extensively explore the impact of different augmentation strategies.
- Basis in paper: [explicit] The paper introduces multimodal augmentations as a key component of FACTOR CL and discusses their role in approximating task relevance without labels.
- Why unresolved: The paper does not provide a systematic comparison of different augmentation strategies or analyze the sensitivity of FACTOR CL to the choice of augmentations.
- What evidence would resolve it: Experiments comparing FACTOR CL's performance using different augmentation strategies (e.g., varying the type, strength, or combination of augmentations) would provide evidence on its sensitivity to the choice of augmentations.

### Open Question 3
- Question: Can FACTOR CL be effectively extended to handle more than two modalities? The paper focuses on the case of two modalities, but many real-world applications involve multiple modalities.
- Basis in paper: [inferred] The paper's theoretical framework and experiments are limited to the case of two modalities, leaving the extension to more than two modalities unexplored.
- Why unresolved: The paper does not provide any analysis or experiments on the extension of FACTOR CL to handle more than two modalities.
- What evidence would resolve it: Experiments on datasets with more than two modalities comparing FACTOR CL's performance to other baselines would provide evidence on its effectiveness in handling multiple modalities.

## Limitations

- The INCE-CLUB upper bound's tightness is asserted but not proven, which may limit the effectiveness of removing task-irrelevant shared information
- The optimal augmentation definitions are theoretically sound but their practical effectiveness depends heavily on augmentation quality and may not generalize well
- The factorization assumption that task-relevant information cleanly separates into shared/unique components may not hold across all datasets, potentially leading to redundant or incomplete representations

## Confidence

- **High confidence**: The synthetic data experiments showing improved performance when both shared and unique information are present (Figure 2a, 2b)
- **Medium confidence**: The real-world dataset results showing state-of-the-art performance across six benchmarks
- **Low confidence**: The theoretical claims about INCE-CLUB being "free" and the optimal augmentation definitions, as these lack extensive empirical validation

## Next Checks

1. **Ablation study on upper bound contribution**: Remove the INCE-CLUB upper bound component and retrain on synthetic data with high shared-irrelevant information. Compare performance degradation to quantify the upper bound's actual contribution versus noise.
2. **Augmentation sensitivity analysis**: Systematically vary augmentation strength parameters across all datasets and measure performance correlation. Identify if there are regimes where optimal augmentations fail to preserve task-relevant information.
3. **Factorization necessity test**: Compare FACTOR CL against a variant that only uses shared representations (removing unique component optimization) on datasets with varying unique information ratios. This would quantify whether factorization is essential or if shared-only learning suffices.