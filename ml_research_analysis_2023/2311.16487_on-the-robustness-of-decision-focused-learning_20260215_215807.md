---
ver: rpa2
title: On the Robustness of Decision-Focused Learning
arxiv_id: '2311.16487'
source_url: https://arxiv.org/abs/2311.16487
tags:
- problem
- figure
- attack
- optimization
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the robustness of 10 decision-focused learning
  (DFL) methods against adversarial attacks. The authors propose two attack types
  (prediction-focused and decision-focused) and apply them to three optimization problems
  (knapsack, portfolio, and shortest path).
---

# On the Robustness of Decision-Focused Learning

## Quick Facts
- arXiv ID: 2311.16487
- Source URL: https://arxiv.org/abs/2311.16487
- Reference count: 6
- Key outcome: Robustness of DFL methods highly correlated with ability to find optimal solutions matching ground truth labels

## Executive Summary
This paper evaluates the robustness of 10 decision-focused learning (DFL) methods against adversarial attacks across three optimization problems. The authors propose two attack types (prediction-focused and decision-focused) and find that model robustness is strongly correlated with the ability to find optimal solutions that closely match ground truth labels. Models that deviate from ground truth while finding optimal solutions are more vulnerable to attacks. The study identifies specific DFL methods that perform best under adversarial conditions for each problem type.

## Method Summary
The paper evaluates 10 DFL methods (QPTL, DBB, FY, IMLE, IntOpt, SPO, MAP, Pairwise, Pairwise Diff, Listwise) plus a Two-Stage baseline across three optimization problems: knapsack, portfolio, and shortest path. For each problem, the authors implement two adversarial attack types using FGSM - prediction-focused attacks that maximize prediction error and decision-focused attacks that maximize decision error. Models are trained on real-world and synthetic datasets, then evaluated under various perturbation magnitudes (0.01, 0.1, 0.15) using metrics including Relative Regret Error (RRE), Mean Accuracy Error (MAE), Fooling Error (FE), and Fooling Relative Regret Error (FRRE).

## Key Results
- Model robustness is highly correlated with ability to find optimal solutions matching ground truth labels
- QPTL, TS, DBB, and IMLE show best robustness for knapsack problem
- FY and Listwise demonstrate exceptional robustness for portfolio optimization
- SPO, IMLE, and DBB perform best under attacks for shortest path problem

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DFL models are more robust when they can find optimal solutions that closely match ground truth labels.
- Mechanism: The paper hypothesizes that robustness is correlated with the model's ability to produce predictions that lead to optimal decisions without deviating significantly from ground truth labels. Models that can do this are less susceptible to adversarial attacks.
- Core assumption: There is a direct relationship between prediction accuracy (matching ground truth) and decision quality robustness against adversarial perturbations.
- Evidence anchors:
  - [abstract] "Our study proposes the hypothesis that the robustness of a model is highly correlated with its ability to find predictions that lead to optimal decisions without deviating from the ground truth label."
  - [section] "We offer empirical insights to demonstrate that robustness is highly correlated with the models ability to find the optimal solutions with respect to the ground truth labels"
  - [corpus] Weak - the corpus contains related DFL papers but no direct evidence supporting this specific robustness hypothesis
- Break condition: If the relationship between prediction accuracy and decision robustness is non-monotonic or if optimal solutions can be achieved through multiple prediction paths with varying robustness.

### Mechanism 2
- Claim: Models with high initial relative regret error are less robust to prediction-focused attacks but more robust to decision-focused attacks.
- Mechanism: Models that haven't learned to make good predictions relative to decisions are more vulnerable to attacks that maximize prediction error, while models that have found optimal solutions are more vulnerable to attacks that maximize decision error.
- Core assumption: The type of attack interacts differently with models based on their optimization convergence state.
- Evidence anchors:
  - [section] "The models with the highest initial RRE are the least robust under the PF attack while the models with the lowest initial RRE are the most robust under the PF attack. This behaviour is reversed when the models are exposed to the DF attack."
  - [corpus] Weak - no corpus evidence directly supporting this attack-specific robustness pattern
- Break condition: If the relationship between RRE and attack robustness varies across problem types or if other factors dominate attack vulnerability.

### Mechanism 3
- Claim: Portfolio optimization models demonstrate exceptional robustness because they consistently identify optimal solutions matching ground truth labels.
- Mechanism: When models consistently achieve low mean accuracy error (MAE) and maintain optimal solutions, they demonstrate high robustness against both attack types.
- Core assumption: Portfolio problems have characteristics that make optimal solution identification more reliable than other problem types.
- Evidence anchors:
  - [section] "All models maintain high levels of robustness against both types of attacks. This resilience, we believe, stems from the models successfully identifying the optimal solution based on the ground truth label c."
  - [corpus] Weak - no corpus evidence explaining why portfolio problems specifically enable this robustness pattern
- Break condition: If portfolio problem characteristics change or if other problem types can achieve similar robustness through different mechanisms.

## Foundational Learning

- Concept: Predict-then-optimize framework
  - Why needed here: Understanding how DFL models integrate prediction and optimization tasks is fundamental to grasping why certain models are more robust than others
  - Quick check question: How does a standard two-stage approach differ from decision-focused learning in terms of training objectives?

- Concept: Adversarial attack types (prediction-focused vs decision-focused)
  - Why needed here: The paper's main contribution is analyzing how different DFL models respond to two distinct attack types, which requires understanding the attack mechanisms
  - Quick check question: What is the key difference between how prediction-focused and decision-focused FGSM attacks are formulated?

- Concept: Relative regret error and its relationship to model performance
  - Why needed here: RRE is a key metric used to evaluate both model performance and robustness, and understanding its behavior is crucial for interpreting results
  - Quick check question: How does relative regret error change when a model's predictions deviate from optimal decisions?

## Architecture Onboarding

- Component map:
  Input features -> Neural network models -> Optimization solver -> Evaluation metrics -> Model updates

- Critical path:
  1. Data preprocessing and feature extraction
  2. Model prediction of optimization parameters
  3. Optimization problem solving with predicted parameters
  4. Decision quality evaluation and gradient computation
  5. Model parameter updates through backpropagation

- Design tradeoffs:
  - Problem selection: Balanced between computational tractability and real-world relevance
  - Attack magnitude: Tradeoff between meaningful perturbation and maintaining realistic scenarios
  - Model complexity: Simple linear models for knapsack/portfolio vs. CNN for image-based shortest path
  - Solver choice: Commercial solvers (Gurobi) vs. differentiable optimization layers

- Failure signatures:
  - High RRE with low MAE indicates models finding optimal solutions through non-ground-truth paths
  - Inconsistent robustness across attack types suggests optimization convergence issues
  - Computational bottlenecks with larger problem instances indicate solver integration problems

- First 3 experiments:
  1. Replicate the knapsack problem with capacity 120 to verify the observed robustness patterns between QPTL, TS, DBB, and IMLE
  2. Test portfolio optimization with degree 16 noise to confirm the exceptional robustness across all models
  3. Implement the prediction-focused FGSM attack on the Warcraft shortest path problem to validate the robustness ranking of SPO, IMLE, and DBB

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of the decision space (e.g., non-uniqueness of optimal solutions) contribute most significantly to model robustness against adversarial attacks?
- Basis in paper: [explicit] The paper observes that models with high initial relative regret error are less robust to prediction-focused attacks, while those with low initial error are less robust to decision-focused attacks. It also notes that models finding optimal solutions deviating from ground truth are more vulnerable.
- Why unresolved: The paper identifies correlations but doesn't isolate specific properties of the decision space that drive robustness. The relationship between solution uniqueness, optimality gap, and attack susceptibility needs more detailed analysis.
- What evidence would resolve it: Systematic experiments varying problem instances with different numbers of optimal solutions, analyzing how attack effectiveness changes with solution multiplicity and optimality gap size.

### Open Question 2
- Question: How do different types of adversarial attacks (prediction-focused vs decision-focused) interact with various DFL methodologies to affect robustness differently?
- Basis in paper: [explicit] The paper shows that models exhibit different robustness behaviors under PF and DF attacks, with some models being robust to one attack type but vulnerable to the other. It notes that models finding optimal solutions may be more susceptible to DF attacks.
- Why unresolved: While the paper demonstrates differential effects, it doesn't fully explain why certain methods are more vulnerable to specific attack types or what underlying characteristics make them susceptible.
- What evidence would resolve it: Detailed analysis of gradient flow through each DFL method under both attack types, identifying which components of each methodology are exploited by different attacks.

### Open Question 3
- Question: What are the fundamental limitations of current DFL approaches in achieving both high decision quality and robustness to adversarial attacks?
- Basis in paper: [inferred] The paper finds that models achieving good decision quality while deviating from ground truth labels are more vulnerable to attacks, suggesting a fundamental trade-off between optimality and robustness.
- Why unresolved: The paper identifies this trade-off but doesn't explore whether it's an inherent limitation of DFL or a result of current implementation approaches. The root cause of this vulnerability needs investigation.
- What evidence would resolve it: Development of new DFL methodologies that can simultaneously achieve high decision quality and robustness, or theoretical proofs demonstrating why such approaches cannot exist.

## Limitations
- Reliance on synthetic and small-scale real-world datasets limits generalizability to other CO problems
- Attack methodologies use relatively small perturbation magnitudes that may not capture worst-case scenarios
- Paper doesn't explore computational overhead introduced by adversarial training or defensive mechanisms

## Confidence
**High confidence**: The empirical observation that models finding optimal solutions close to ground truth labels demonstrate higher robustness against adversarial attacks.

**Medium confidence**: The hypothesis that portfolio optimization models show exceptional robustness due to their consistent identification of optimal solutions.

**Low confidence**: The generalizability of attack vulnerability patterns to problems outside the three studied domains.

## Next Checks
1. Scale-up validation: Replicate the knapsack experiment with problem sizes 10Ã— larger to verify if robustness patterns persist under computational scaling constraints
2. Attack magnitude exploration: Systematically vary perturbation magnitudes beyond 0.15 (up to 0.5 or 1.0) to determine if current results represent a regime where models are still learning
3. Cross-problem generalization: Apply the most robust models to a fourth, structurally different CO problem such as facility location or vehicle routing to test if observed robustness patterns transfer across problem domains