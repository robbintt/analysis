---
ver: rpa2
title: 'Natural Example-Based Explainability: a Survey'
arxiv_id: '2309.03234'
source_url: https://arxiv.org/abs/2309.03234
tags:
- methods
- explanations
- examples
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive overview of natural example-based
  explainable AI (XAI), focusing on methods that use training data examples directly
  as explanations without generative processes. The paper categorizes these methods
  into several groups: similar examples (retrieving closest training instances), counterfactuals
  and semi-factuals (finding examples with different or same predictions), influential
  instances (identifying training samples with the highest impact on model behavior),
  prototypes (representative samples of classes or the dataset), and concepts (abstractions
  of common elements between samples).'
---

# Natural Example-Based Explainability: a Survey

## Quick Facts
- arXiv ID: 2309.03234
- Source URL: https://arxiv.org/abs/2309.03234
- Reference count: 40
- Primary result: Comprehensive survey of natural example-based explainable AI methods that use training data examples directly as explanations without generative processes

## Executive Summary
This survey provides a comprehensive overview of natural example-based explainable AI (XAI), focusing on methods that use training data examples directly as explanations without generative processes. The paper categorizes these methods into several groups: similar examples (retrieving closest training instances), counterfactuals and semi-factuals (finding examples with different or same predictions), influential instances (identifying training samples with the highest impact on model behavior), prototypes (representative samples of classes or the dataset), and concepts (abstractions of common elements between samples). Each category is analyzed in terms of its semantic definition, cognitive impact, and added value, drawing on psychological and social science literature. The survey emphasizes the importance of plausibility and interpretability in XAI, advocating for natural examples over generative methods due to their inherent realism and alignment with human reasoning processes. The paper aims to encourage further research and development in natural example-based XAI by providing a clear framework and highlighting key methods and their applications.

## Method Summary
The survey systematically identifies and categorizes natural example-based XAI methods from existing literature, organizing them into five main groups: similar examples, counterfactuals/semi-factuals, influential instances, prototypes, and concepts. For each category, the paper analyzes the semantic definition, cognitive impact, and added value based on psychological and social science literature. The survey compares methods within each category and across categories to highlight common points and divergences. The reproduction plan involves identifying relevant literature, categorizing methods, analyzing their properties, and comparing their effectiveness. Key challenges include finding comprehensive literature coverage, objectively comparing methods with different objectives, and validating claims through user studies.

## Key Results
- Natural example-based XAI methods can be effectively categorized into five groups: similar examples, counterfactuals/semi-factuals, influential instances, prototypes, and concepts
- Natural examples are argued to be inherently more plausible than generated examples, making them more trustworthy for users
- Different example-based formats provide complementary insights into model behavior, potentially offering a more complete understanding when used together
- The choice of similarity measure has major implications for which examples are retrieved and how well they explain model behavior
- Most existing natural example-based methods focus on local explanations, with fewer approaches for global explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Example-based XAI aligns with natural human reasoning and learning patterns
- Mechanism: Humans naturally learn by forming mental representations from examples. Example-based explanations leverage this cognitive process, making them intuitive and easier to understand compared to abstract attribution maps or saliency heatmaps.
- Core assumption: The psychological literature on human reasoning and explanation formation is accurate and generalizable to understanding AI models.
- Evidence anchors: Psychological plausibility of case-based reasoning in medicine and other domains, but lack of direct empirical studies in the context of example-based explanations for XAI.
- Break condition: If psychological studies show humans do not learn or reason primarily through examples, or if user studies demonstrate example-based explanations are not more intuitive than other formats.

### Mechanism 2
- Claim: Natural examples are inherently more plausible and trustworthy than generated examples
- Mechanism: Natural examples are directly drawn from the training data, guaranteeing they belong to the data manifold and are realistic. Generated examples, even from sophisticated models like diffusion models, may fail to ensure plausibility and can create distrust.
- Core assumption: Plausibility is a necessary condition for gaining user trust in AI explanations.
- Evidence anchors: The exclusion of generative methods is justified by plausibility requirements for user trust, but lacks direct comparative user studies between natural and generated examples.
- Break condition: If user studies show generated examples are perceived as equally or more plausible than natural examples, or if methods are developed to guarantee generated examples' plausibility.

### Mechanism 3
- Claim: Different example-based formats (similar examples, counterfactuals, influential instances, prototypes, concepts) provide complementary insights into model behavior
- Mechanism: Each format highlights a different aspect of the model's decision-making process: local similarity, decision boundaries, training data influence, data distribution, and abstract concepts. Using them together gives a more complete understanding than any single format.
- Core assumption: The model's behavior can be meaningfully decomposed into these different aspects, and each format captures a distinct and valuable part of that decomposition.
- Evidence anchors: Logical categorization of methods and their different focuses, but lacks empirical studies demonstrating the complementary value of using multiple formats together.
- Break condition: If empirical studies show that different formats provide redundant or conflicting information, or if users cannot integrate insights from multiple formats.

## Foundational Learning

- Concept: Psychological basis of example-based reasoning
  - Why needed here: The paper's core argument relies on the idea that example-based explanations are intuitive because they align with how humans naturally learn and reason. Understanding this psychological foundation is crucial for evaluating the validity of this claim.
  - Quick check question: What are the key psychological mechanisms by which humans use examples to learn and reason, and how do these map to the different example-based XAI formats?

- Concept: Definition and properties of different example-based XAI formats
  - Why needed here: The paper categorizes example-based methods into several groups, each with its own semantic definition and cognitive impact. A clear understanding of these definitions and properties is necessary to evaluate the claims about their complementary value and practical applications.
  - Quick check question: How do the definitions of similar examples, counterfactuals, influential instances, prototypes, and concepts differ, and what specific insights does each format provide about the model?

- Concept: Trade-offs between natural and generated examples
  - Why needed here: The paper advocates for natural examples over generated ones due to plausibility concerns. Understanding the technical and practical trade-offs between these approaches is essential for evaluating this recommendation and for making informed decisions in real-world applications.
  - Quick check question: What are the specific advantages and disadvantages of using natural examples versus generated examples in terms of plausibility, computational cost, and the types of insights they can provide?

## Architecture Onboarding

- Component map: Data Ingestion -> Similarity Computation -> Format-Specific Processing -> Visualization/Explanation Generation -> Evaluation
- Critical path:
  1. Load trained model and training data
  2. Select example-based format(s) to use
  3. Compute similarity or influence scores
  4. Retrieve relevant training examples
  5. Generate explanations/visualizations
  6. Evaluate explanation quality

- Design tradeoffs:
  - Natural vs. Generated Examples: Natural examples guarantee plausibility but may be less optimal for highlighting specific model behaviors. Generated examples can be tailored but risk being unrealistic.
  - Local vs. Global Explanations: Local methods (e.g., similar examples, counterfactuals) explain individual predictions, while global methods (e.g., prototypes, influential instances) provide insights into overall model behavior.
  - Computational Cost: Some methods (e.g., influence functions, counterfactual search) are computationally expensive, especially for large datasets or complex models.

- Failure signatures:
  - Explanations are not intuitive or actionable for users
  - Retrieved examples are not plausible or relevant to the model's decision
  - Computational costs are prohibitive for real-time or large-scale applications
  - Different example-based formats provide conflicting or redundant information

- First 3 experiments:
  1. Implement and evaluate a simple similar examples method (e.g., k-NN in latent space) on a small image classification dataset, comparing user understanding with a baseline attribution method.
  2. Extend the similar examples method to include counterfactual and semi-factual explanations, evaluating their ability to provide insights into decision boundaries.
  3. Implement an influential instances method using influence functions, comparing the retrieved examples with similar examples and evaluating their ability to explain training data impact on model behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do natural example-based methods perform in user studies compared to generative methods, particularly for high-dimensional data like images?
- Basis in paper: [explicit] The paper argues that natural examples are inherently plausible and do not require a model to explain another model, but acknowledges the lack of user studies comparing natural and generative methods.
- Why unresolved: There is a lack of empirical evidence from user studies directly comparing the effectiveness and trustworthiness of natural example-based methods versus generative methods.
- What evidence would resolve it: Controlled user studies comparing user comprehension, trust, and satisfaction between natural example-based explanations and generative counterfactual explanations across different data types and model complexities.

### Open Question 2
- Question: What is the optimal way to define and compute similarity between examples that takes into account both the model's internal representation and the data distribution?
- Basis in paper: [explicit] The paper discusses various similarity measures (Euclidean distance, latent space distance, attribution-based) but notes that the choice of similarity space has major implications and the relevance of different approaches to reflect model behavior is questionable.
- Why unresolved: There is no consensus on which similarity measure best captures the relationship between examples as perceived by the model while remaining interpretable to humans.
- What evidence would resolve it: Comparative studies evaluating different similarity measures' ability to retrieve meaningful examples across diverse datasets and model architectures, using both quantitative metrics and qualitative human evaluation.

### Open Question 3
- Question: How can influential instance methods be made more interpretable and trustworthy for end-users, given their current limitations in explaining model behavior?
- Basis in paper: [explicit] The paper notes that influential instances are more challenging to interpret than similar examples and their effectiveness for trustworthiness is unclear, requiring further research particularly user studies.
- Why unresolved: Influential instance methods provide mathematically rigorous measures of impact but may not translate well into intuitive explanations for non-expert users.
- What evidence would resolve it: User studies evaluating different visualization and presentation techniques for influential instances, along with analysis of how well users understand and trust explanations based on influential instances compared to other explanation formats.

## Limitations
- The psychological mechanisms underlying example-based reasoning are discussed conceptually but lack direct empirical validation in the XAI context
- The survey's definition of "natural examples" as inherently more plausible than generated ones is reasonable but not empirically proven
- The categorization of methods, while comprehensive, may have some overlap between categories and does not address computational complexity considerations for large-scale deployments

## Confidence
- High confidence: The categorization framework for natural example-based XAI methods is well-defined and useful
- Medium confidence: The psychological plausibility of example-based explanations and the superiority of natural over generated examples
- Medium confidence: The complementary value of different example-based formats

## Next Checks
1. Conduct user studies comparing understanding and trust between natural and generated example-based explanations across different domains
2. Perform empirical analysis of the complementary value of combining multiple example-based formats versus using single formats
3. Investigate the computational trade-offs of different natural example-based methods on large-scale datasets and complex models