---
ver: rpa2
title: Aligning Large Language Models through Synthetic Feedback
arxiv_id: '2305.13735'
source_url: https://arxiv.org/abs/2305.13735
tags:
- human
- synthetic
- dataset
- response
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel alignment learning framework using
  synthetic feedback instead of human demonstrations or pre-aligned LLMs. The method
  involves reward modeling with synthetic comparisons from vanilla LLMs, followed
  by generating high-quality demonstrations through reward-model-guided self-play
  and further optimizing with reinforcement learning from synthetic feedback.
---

# Aligning Large Language Models through Synthetic Feedback

## Quick Facts
- arXiv ID: 2305.13735
- Source URL: https://arxiv.org/abs/2305.13735
- Authors: 
- Reference count: 40
- Key outcome: ALMoST outperforms recent open-sourced models in alignment benchmarks, showing 75% winning rate on average in A/B tests using GPT-4

## Executive Summary
This paper introduces ALMoST, a novel alignment framework that uses synthetic feedback instead of human demonstrations or pre-aligned LLMs. The method generates synthetic comparisons by prompting vanilla LLMs of different sizes and qualities, then uses these comparisons to train reward models. Through Reward-Model-guided Self-Play (RMSP) and reinforcement learning from synthetic feedback, the resulting model achieves state-of-the-art alignment performance on open-source benchmarks while avoiding the high costs of human-annotated data.

## Method Summary
The framework generates synthetic comparison data by contrasting responses from vanilla LLMs of different sizes and prompt qualities, assuming larger/well-prompted models produce better outputs. This data passes through a heuristic filter that removes short responses and those containing rejection phrases. The filtered comparisons train a reward model, which then guides RMSP to generate high-quality demonstrations. An SFT model is trained on these demonstrations, followed by PPO optimization using synthetic reward signals, resulting in an aligned model without human-annotated data.

## Key Results
- ALMoST outperforms recent open-sourced models trained on InstructGPT outputs or human-annotated instructions
- Achieves 75% winning rate on average in A/B tests using GPT-4 as judge
- Shows state-of-the-art alignment performance on HHH, TruthfulQA, and Vicuna benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger, optimally prompted LLMs produce better responses than smaller, poorly prompted ones, enabling synthetic preference ranking.
- Mechanism: The framework generates comparisons by contrasting outputs from models of different sizes and prompt qualities, assuming the larger/well-prompted model produces superior responses.
- Core assumption: Response quality scales predictably with model size and prompt optimization.
- Evidence anchors:
  - [abstract] "we assume that the response from a larger LLM with more and better demonstrations might be better overall"
  - [section 2.1] "Askell et al. (2021) demonstrate that the larger model performs somewhat better than the smaller model, and the model with longer prompts is better than the model with shorter prompts"
  - [corpus] Weak evidence - no direct citations supporting this scaling assumption in isolation
- Break condition: If model size no longer correlates with response quality, or if prompt optimization yields diminishing returns

### Mechanism 2
- Claim: Heuristic filtering based on response length and keyword patterns improves reward model training stability.
- Mechanism: Post-validation removes noisy comparisons where the chosen response is short or contains rejection phrases like "I don't know", preventing length bias in the reward model.
- Core assumption: Short responses are more likely to be stochastic generation failures rather than genuine quality differences.
- Evidence anchors:
  - [section 2.1] "we empirically find that the better response usually has a longer length than the worse one"
  - [section 2.1] "training RM only on comparisons with longer chosen responses would make the resulting model biased by length"
  - [corpus] Moderate evidence - length-based filtering is common in preference learning literature
- Break condition: If length no longer correlates with quality, or if heuristic filtering removes too many valid comparisons

### Mechanism 3
- Claim: Reward-Model-guided Self-Play (RMSP) with rejection sampling produces higher-quality synthetic demonstrations than vanilla self-play.
- Mechanism: Assistant responses are sampled multiple times and the highest-scoring response according to the synthetic reward model is selected, ensuring alignment quality in simulated conversations.
- Core assumption: The synthetic reward model can reliably distinguish between aligned and misaligned responses during generation.
- Evidence anchors:
  - [section 2.2] "we leverage the synthetic RM from the previous stage to ensure the quality of the model-to-model conversations with rejection sampling"
  - [section 4.2] "we find that the SFT model trained with RMSP outperforms the model with Self-Play in various benchmarks"
  - [corpus] Strong evidence - rejection sampling is standard in RLHF literature
- Break condition: If reward model predictions become uncorrelated with actual alignment quality during generation

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding the three-stage RLHF pipeline (SFT → RM → RLHF) is crucial for grasping how this work modifies each stage with synthetic feedback
  - Quick check question: What are the three stages of traditional RLHF and how does each stage map to this synthetic framework?

- Concept: Preference modeling and reward modeling
  - Why needed here: The synthetic comparison generation and reward model training rely on preference ranking objectives similar to traditional RLHF
  - Quick check question: How does the reward modeling objective differ when using synthetic comparisons versus human comparisons?

- Concept: In-context learning and prompt engineering
  - Why needed here: The framework heavily relies on prompting different sized models with various demonstration counts to generate synthetic data
  - Quick check question: How does the number of in-context demonstrations affect response quality in LLMs of different sizes?

## Architecture Onboarding

- Component map:
  Query generator → Response generator (multiple LLMs) → Synthetic comparator → Heuristic filter → Reward model trainer → RMSP demonstrator → SFT trainer → PPO optimizer → Aligned model
  Key components: LLaMA variants of different sizes, HHH/ Faithful prompts, heuristic filter, reward model, PPO implementation

- Critical path:
  1. Generate initial queries with 10-shot in-context learning
  2. Produce responses from 5 different prompted models
  3. Apply heuristic filtering to synthetic comparisons
  4. Train reward model on filtered comparisons
  5. Generate demonstrations using RMSP with rejection sampling
  6. Train SFT model on demonstrations
  7. Apply PPO optimization using synthetic reward signals

- Design tradeoffs:
  - Model size vs. computational cost in synthetic data generation
  - Filter strictness vs. dataset size for reward model training
  - Number of rejection samples vs. demonstration quality
  - Synthetic data diversity vs. alignment quality

- Failure signatures:
  - Reward model collapse: All responses receive similar scores
  - Length bias: Reward model overweights response length
  - Generation failure: Assistant produces off-topic or repetitive responses
  - Alignment degradation: PPO optimization reduces general capabilities

- First 3 experiments:
  1. Validate scaling assumption: Compare response quality from LLaMA-7B vs LLaMA-30B with identical prompts
  2. Test heuristic filter effectiveness: Train reward model with and without length-based filtering on synthetic data
  3. Benchmark RMSP vs vanilla self-play: Generate demonstrations both ways and compare SFT model performance on alignment tasks

## Open Questions the Paper Calls Out

Open Question 1
- Question: How does the quality of synthetic comparisons generated by ALMoST compare to human-generated comparisons in terms of training a high-quality reward model?
- Basis in paper: [explicit] The paper discusses the use of synthetic comparisons generated from vanilla LLMs, but does not provide a direct comparison to human-generated comparisons in terms of reward model quality.
- Why unresolved: While the paper demonstrates the effectiveness of synthetic comparisons, it does not provide a quantitative comparison to human-generated comparisons in terms of reward model performance.
- What evidence would resolve it: A direct comparison of reward model performance when trained on synthetic vs. human-generated comparisons, using metrics such as accuracy on held-out human preference data.

Open Question 2
- Question: What is the impact of model size on the alignment tax observed in ALMoST models?
- Basis in paper: [inferred] The paper mentions alignment tax, where aligned models may underperform unaligned models on certain tasks, and suggests that larger models might experience less of this effect.
- Why unresolved: The paper only evaluates a 7B parameter model, so the relationship between model size and alignment tax is not directly investigated.
- What evidence would resolve it: A systematic study comparing alignment tax across different model sizes (e.g., 7B, 13B, 30B) on standard benchmarks like MMLU and LAMBADA.

Open Question 3
- Question: How does the performance of ALMoST compare to models aligned using RLHF with actual human feedback?
- Basis in paper: [explicit] The paper compares ALMoST to models trained on outputs of InstructGPT or human-annotated demonstrations, but does not directly compare to models trained with RLHF using human feedback.
- Why unresolved: While the paper shows ALMoST outperforms other open-source models, it does not provide a direct comparison to state-of-the-art models trained with human feedback.
- What evidence would resolve it: A direct comparison of ALMoST to models like ChatGPT or Anthropic's models on standard alignment benchmarks and human evaluations.

Open Question 4
- Question: What is the long-term stability and safety of ALMoST models when deployed in real-world scenarios?
- Basis in paper: [inferred] The paper focuses on static benchmarks and short conversations, but does not investigate the behavior of ALMoST models in long-term or complex real-world interactions.
- Why unresolved: The paper's evaluations are limited to controlled benchmarks and short simulated conversations, not real-world deployment scenarios.
- What evidence would resolve it: A longitudinal study of ALMoST models deployed in real-world applications, monitoring for issues like concept drift, safety violations, or degradation of performance over time.

Open Question 5
- Question: How does the choice of heuristic filter in the synthetic comparison generation process affect the final model performance?
- Basis in paper: [explicit] The paper introduces a heuristic filter based on response length to improve the quality of synthetic comparisons, but does not extensively explore the impact of different filtering strategies.
- Why unresolved: While the paper shows the benefits of the length-based heuristic filter, it does not compare this approach to other potential filtering strategies or explore the sensitivity of model performance to the filter's parameters.
- What evidence would resolve it: An ablation study comparing the performance of ALMoST models when trained with different filtering strategies for synthetic comparisons, including variations in the length-based filter's parameters.

## Limitations

- The scaling assumption underlying synthetic preference ranking relies on qualitative observations rather than direct experimental validation
- Key implementation details like exact prompt formulations and filter parameters are not specified, hindering faithful reproduction
- The 75% winning rate against proprietary models depends on GPT-4 as an external judge, introducing potential evaluation biases

## Confidence

- Synthetic preference ranking mechanism: Medium
- Heuristic filtering effectiveness: Medium
- RMSP demonstration quality: High (supported by RLHF literature)
- Overall alignment benchmark performance: Medium (dependent on GPT-4 judge reliability)

## Next Checks

1. Validate scaling assumption by conducting controlled experiments comparing response quality across LLaMA variants with identical prompts, measuring the correlation between model size and response quality across multiple dimensions
2. Test heuristic filter sensitivity by training reward models with varying filter strictness levels and measuring the impact on reward model stability and alignment quality
3. Benchmark evaluation independence by comparing GPT-4 judge results with human annotations on a subset of A/B tests to quantify potential evaluation bias