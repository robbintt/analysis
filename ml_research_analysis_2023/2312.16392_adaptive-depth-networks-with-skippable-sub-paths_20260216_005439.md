---
ver: rpa2
title: Adaptive Depth Networks with Skippable Sub-Paths
arxiv_id: '2312.16392'
source_url: https://arxiv.org/abs/2312.16392
tags:
- networks
- adaptive
- training
- residual
- depth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an architectural pattern and training method
  for adaptive depth networks, where every residual stage is divided into two sub-paths
  with different properties. The first sub-path is mandatory for hierarchical feature
  learning, while the second sub-path is optimized to incur minimal performance degradation
  even if skipped.
---

# Adaptive Depth Networks with Skippable Sub-Paths

## Quick Facts
- arXiv ID: 2312.16392
- Source URL: https://arxiv.org/abs/2312.16392
- Reference count: 40
- Primary result: Achieves adaptive depth networks with significantly shorter training time than previous methods while maintaining performance across ImageNet classification, object detection, and instance segmentation tasks

## Executive Summary
This paper proposes an architectural pattern and training method for adaptive depth networks where each residual stage is divided into mandatory and skippable sub-paths. The key innovation is skip-aware self-distillation, which trains only the smallest base-net and largest super-net while implicitly optimizing all intermediate sub-networks. This approach eliminates the need for iterative self-distillation of individual sub-networks, resulting in significantly reduced training time while maintaining performance. The method is generally applicable to residual networks including CNNs and vision transformers.

## Method Summary
The method divides each residual stage into two sub-paths: a mandatory sub-path for hierarchical feature learning and a skippable sub-path optimized to incur minimal performance degradation when skipped. Skip-aware self-distillation trains only the base-net (all skippable paths disabled) and super-net (all paths enabled) using KL divergence loss to preserve intermediate feature distributions. Switchable batch normalization (or layer normalization for transformers) handles potential internal covariate shifts. The approach requires no explicit self-distillation or additional fine-tuning of individual sub-networks except the smallest base-net, resulting in significantly shorter training time than previous adaptive networks.

## Key Results
- Achieves 76.5% top-1 accuracy on ImageNet with ResNet50-based adaptive network, comparable to individually trained networks
- Reduces training time significantly compared to iterative self-distillation methods (exact time savings shown in Table 3)
- Maintains effectiveness on object detection and instance segmentation tasks using MS COCO dataset
- Demonstrates generalization across CNN and transformer architectures

## Why This Works (Mechanism)

### Mechanism 1
The second sub-path in each residual stage learns to refine features rather than learn new hierarchical representations, enabling it to be skipped with minimal performance loss. Through skip-aware self-distillation, the second sub-path is explicitly trained to preserve the feature distribution of the first sub-path by minimizing KL divergence between intermediate features. This forces the second sub-path to focus on feature refinement rather than creating new feature levels.

### Mechanism 2
Skip-aware self-distillation requires training only the base-net and super-net, not all possible sub-networks, significantly reducing training time. The algorithm jointly trains two parameter-sharing sub-networks while implicitly optimizing all intermediate sub-networks through the architectural pattern and loss function design. This creates a cascading optimization effect that optimizes all possible sub-networks without explicitly training each one.

### Mechanism 3
Switchable batch normalization (skip-aware BNs) handles internal covariate shifts that occur when different sub-networks are selected at inference time. Each mandatory sub-path has two sets of batch normalization parameters - one used when the skippable sub-path is active, and another when it's skipped. This prevents covariate shift between different sub-network configurations.

## Foundational Learning

- Concept: Residual networks and their properties
  - Why needed here: The proposed method builds upon residual network architecture to create adaptive depth networks
  - Quick check: Understanding how residual connections enable identity mappings and gradient flow

- Concept: Knowledge distillation and self-distillation
  - Why needed here: Skip-aware self-distillation is the core training mechanism that preserves feature distributions
  - Quick check: Understanding how KL divergence loss preserves feature distribution between teacher and student networks

- Concept: Internal covariate shift and batch normalization
  - Why needed here: Switchable BNs address internal covariate shifts that occur when selecting different sub-networks
  - Quick check: Understanding how batch normalization statistics change with different network configurations

- Concept: Vision transformers and layer normalization
  - Why needed here: The method extends to transformers requiring layer normalization instead of batch normalization
  - Quick check: Understanding the difference between layer normalization and batch normalization

## Architecture Onboarding

Component map: Input -> Mandatory sub-path 1 -> Skippable sub-path 1 -> Mandatory sub-path 2 -> Skippable sub-path 2 -> ... -> Output

Critical path: The mandatory sub-paths form the critical path for hierarchical feature learning, while skippable sub-paths provide optional refinement.

Design tradeoffs: The key tradeoff is between computational efficiency (skipping skippable paths) and model accuracy (keeping all paths active). The design aims to minimize accuracy loss when skipping paths.

Failure signatures: Performance degradation when skipping paths indicates failure of skip-aware self-distillation or switchable normalization. Training instability suggests improper implementation of the architectural pattern.

First experiments:
1. Implement and verify the basic ResNet50 architecture with divided residual stages
2. Test skip-aware self-distillation training with KL divergence loss
3. Validate switchable batch normalization implementation

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed skip-aware self-distillation strategy compare to traditional self-distillation methods in terms of training efficiency and final model performance across different types of residual networks (CNNs vs. Transformers)?

### Open Question 2
Can the proposed adaptive depth network approach be extended to other types of neural network architectures beyond residual networks, such as recurrent neural networks or graph neural networks?

### Open Question 3
What is the impact of the skip-aware batch normalization (BN) and layer normalization (LN) on the performance of the adaptive depth networks, and can other normalization techniques be used instead?

## Limitations

- The paper does not provide a direct comparison of the proposed method against traditional self-distillation methods in terms of training time and final model performance
- The effectiveness of switchable normalization is assumed rather than empirically validated through ablation studies
- The approach is limited to residual network architectures and has not been demonstrated on non-residual networks like RNNs or GNNs

## Confidence

- High confidence: The architectural pattern itself (mandatory + skippable sub-paths) and the basic concept of preserving feature distributions through distillation
- Medium confidence: The time savings claim, as it's demonstrated but not directly compared to the theoretical alternative of training all sub-networks
- Medium confidence: The skip-aware self-distillation mechanism's ability to optimize all sub-networks through joint training of only two parameter-sharing networks

## Next Checks

1. Ablation study on switchable normalization: Remove skip-aware BNs/layer-norms and measure performance degradation across different sub-network configurations to validate this component's necessity
2. Direct time comparison: Implement and train a version that iteratively trains all possible sub-networks, then compare total training time to the proposed skip-aware approach
3. Feature analysis: Visualize and quantify the feature distribution changes when skipping sub-paths to verify that the second sub-path truly learns only refinement rather than new feature levels