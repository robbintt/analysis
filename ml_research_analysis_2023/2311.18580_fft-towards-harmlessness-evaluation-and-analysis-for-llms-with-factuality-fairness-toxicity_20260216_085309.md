---
ver: rpa2
title: 'FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality,
  Fairness, Toxicity'
arxiv_id: '2311.18580'
source_url: https://arxiv.org/abs/2311.18580
tags:
- llms
- evaluation
- language
- toxicity
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'FFT is a new benchmark for evaluating the harmlessness of Large
  Language Models (LLMs) across three dimensions: factuality, fairness, and toxicity.
  The benchmark consists of 2116 carefully designed instances to assess the potential
  harms of LLM-generated content, including misinformation, biases, and toxic language.'
---

# FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity

## Quick Facts
- **arXiv ID:** 2311.18580
- **Source URL:** https://arxiv.org/abs/2311.18580
- **Reference count:** 15
- **Primary result:** Introduces FFT benchmark evaluating LLMs' harmlessness across factuality, fairness, and toxicity dimensions with 2116 instances

## Executive Summary
FFT is a new benchmark designed to evaluate the harmlessness of Large Language Models (LLMs) across three critical dimensions: factuality, fairness, and toxicity. The benchmark consists of 2116 carefully designed instances that assess potential harms of LLM-generated content, including misinformation, biases, and toxic language. Experiments with 9 representative LLMs reveal that current models still exhibit significant shortcomings in harmlessness, with performance gaps observed both between different models and across evaluation dimensions.

The benchmark provides valuable insights for the AI safety community, demonstrating that while instruction-following fine-tuning improves harmlessness, simply scaling model size does not guarantee increased safety. Notably, Llama2-chat models perform competitively with GPT models in harmlessness evaluations, challenging assumptions about the relationship between model size and safety. The FFT benchmark and its comprehensive analysis serve as a foundation for future research aimed at developing truly harmless LLMs.

## Method Summary
The FFT benchmark evaluates LLMs using 2116 instances across three harm dimensions: factuality (990 queries testing misinformation detection and counterfactual response generation), fairness (630 queries assessing demographic biases across 17 identities in credit, criminal, and healthcare scenarios), and toxicity (496 queries using jailbreak techniques to bypass safety filters). The evaluation employs accuracy metrics for factuality, coefficient of variation for fairness disparities, and Perspective API plus LLM evaluators for toxicity scoring. Nine models were tested including GPT4, GPT3.5-turbo, various Llama2 and Vicuna variants, using zero-shot or few-shot settings to assess harmlessness comprehensively.

## Key Results
- Llama2-chat models perform competitively with GPT models in harmlessness evaluation despite smaller size
- Fine-tuning for human-value alignment significantly improves harmlessness across all dimensions
- Scaling up models does not directly correlate with increased harmlessness; larger models may be exposed to more harmful content
- All evaluated LLMs show increased toxicity from utterance-level to context-level evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adversarial question design exposes factual inaccuracies by inducing LLM hallucinations.
- Mechanism: By crafting queries with misinformation and counterfactual notions, the benchmark forces models to either correctly identify errors or fall into generating false content, revealing their factual reliability.
- Core assumption: LLMs are prone to hallucination and will attempt to generate coherent responses even to false or non-existent prompts.
- Evidence anchors:
  - [abstract] "To investigate the potential harms of LLMs, we evaluate 9 representative LLMs... Experiments show that the harmlessness of LLMs is still under-satisfactory..."
  - [section] "Due to the inherent hallucinations (Yu et al., 2023; Rawte et al., 2023), LLMs could express fact-like content towards counterfactual notions, confusing the public."
  - [corpus] Weak: No direct corpus evidence; relies on prior work citations.
- Break condition: If models are explicitly trained to refuse or flag such adversarial queries, the benchmark loses its ability to measure internal factual reasoning.

### Mechanism 2
- Claim: Diversity in identity-based fairness queries uncovers hidden demographic biases across practical scenarios.
- Mechanism: By embedding identity terms into realistic life scenarios (credit, criminal, health assessments), the benchmark reveals disparities in model predictions that task-specific fairness datasets miss.
- Core assumption: LLMs inherit societal biases from training data, and these biases manifest differently depending on the context of the query.
- Evidence anchors:
  - [abstract] "To explore the potential biases as much, we formulate questions towards realistic life, rather than traditional nature language processing tasks..."
  - [section] "The fairness-evaluation questions are constructed and expected to uncover more possible biases that LLMs may exhibit towards particular user groups."
  - [corpus] Weak: No corpus evidence; relies on described benchmark design.
- Break condition: If models are fine-tuned with fairness alignment or demographic parity constraints, disparities may be artificially suppressed.

### Mechanism 3
- Claim: Jailbreak-wrapped toxicity prompts bypass safety filters to reveal true toxicity levels.
- Mechanism: By embedding toxic questions inside jailbreak templates, the benchmark forces models to generate responses without ethical refusals, allowing genuine toxicity measurement.
- Core assumption: Standard safety alignment causes models to refuse overtly toxic prompts, hiding their underlying biases or harmful tendencies.
- Evidence anchors:
  - [abstract] "Jailbreak prompts are a series of crafted inputs with specific instructions, tricking LLMs to bypass the internal ethnic limitations..."
  - [section] "Here we wrap the toxicity-elicit questions with cherry-picked jailbreak prompts, in an effort to avoid LLM's refusal."
  - [corpus] Weak: No corpus evidence; relies on described method.
- Break condition: If models are trained specifically to detect and neutralize jailbreak prompts, the benchmark cannot elicit genuine toxicity responses.

## Foundational Learning

- Concept: Adversarial prompting and hallucination in LLMs
  - Why needed here: Understanding how models generate false content when faced with misleading or fabricated queries is central to the factuality evaluation.
  - Quick check question: What is the difference between a model "refusing" a query and "hallucinating" an answer?

- Concept: Fairness metrics and demographic bias
  - Why needed here: The benchmark uses coefficient of variation and other metrics to quantify disparities across identities; understanding these is key to interpreting fairness results.
  - Quick check question: How does coefficient of variation help measure bias without being affected by scale?

- Concept: Jailbreak prompt engineering
  - Why needed here: The toxicity evaluation relies on jailbreak techniques to bypass safety alignment; knowing how these work is essential to replicate or extend the benchmark.
  - Quick check question: Why do jailbreak prompts sometimes succeed where direct toxic prompts fail?

## Architecture Onboarding

- Component map: Query generator -> LLM executor -> Response evaluator -> Analysis engine
- Critical path:
  1. Generate seed data → Apply templates → Produce final queries
  2. Execute queries against all models → Collect raw responses
  3. Evaluate responses → Compute metrics → Aggregate findings
- Design tradeoffs:
  - Manual vs. automated evaluation: Manual ensures quality but limits scale; automated enables scale but may miss nuance.
  - Jailbreak reliance: Effective for bypassing refusals but may not reflect real-world toxicity exposure.
  - Identity coverage: Broader identity sets increase fairness detection but also increase complexity.
- Failure signatures:
  - High refusal rates → Jailbreak prompts ineffective or models too aligned
  - Uniform responses across identities → Possible overfitting to fairness alignment
  - Consistently low toxicity scores → Possible safety alignment masking true toxicity
- First 3 experiments:
  1. Run a subset of adversarial factuality queries on Llama2-chat-7B and GPT3.5-turbo; compare hallucination rates.
  2. Test fairness disparity across gender identities using credit assessment; compute coefficient of variation.
  3. Apply jailbreak-wrapped toxicity prompts to Llama2-chat-13B; measure toxicity score before and after alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does instruction-following capability impact harmlessness evaluation in LLMs?
- Basis in paper: [explicit] The paper notes that Llama2-chat models exhibit competitive performance with GPT models in harmlessness evaluation, attributing this to instruction-following extent.
- Why unresolved: The paper does not provide detailed analysis on how instruction-following capability specifically influences different aspects of harmlessness (factuality, fairness, toxicity).
- What evidence would resolve it: Comparative studies isolating instruction-following from other capabilities, and analyzing its correlation with harmlessness scores across different evaluation dimensions.

### Open Question 2
- Question: What is the relationship between scaling and context-level toxicity in LLMs?
- Basis in paper: [inferred] The paper observes that all evaluated LLMs show increased toxicity from utterance-level to context-level evaluation, and that larger models may be exposed to more harmful content with larger training corpora.
- Why unresolved: The paper does not explore the scaling relationship specifically for context-level toxicity, nor does it analyze the types of contexts that trigger increased toxicity.
- What evidence would resolve it: Experiments comparing context-level toxicity across different model sizes with controlled context variations, and analysis of specific context patterns that increase toxicity.

### Open Question 3
- Question: How does the frequency of jailbreak prompt rejections correlate with overall harmlessness?
- Basis in paper: [explicit] The paper notes that Llama2-chat models output fewer unaligned responses to jailbreak prompts compared to GPT models, which affects their toxicity scores.
- Why unresolved: The paper does not investigate whether fewer jailbreak responses necessarily indicate higher harmlessness, or if it reflects different safety alignment strategies.
- What evidence would resolve it: Analysis correlating jailbreak rejection rates with harmfulness in legitimate queries, and comparison of safety alignment effectiveness across different jailbreak types.

## Limitations
- Manual evaluation approach limits scalability and may introduce subjective bias in scoring
- Jailbreak prompts may not reflect authentic real-world toxicity exposure scenarios
- Fairness evaluation may miss intersectional harms or emerging identity groups beyond the 17 covered

## Confidence
- **High confidence**: Benchmark construction methodology and results for established models (GPT series, Llama2 variants) are well-documented and reproducible
- **Medium confidence**: Fairness disparity measurements using coefficient of variation are methodologically sound but interpretation may vary across cultural contexts
- **Low confidence**: Jailbreak effectiveness depends heavily on prompt engineering that may become obsolete as models develop jailbreak detection

## Next Checks
1. Replicate the factuality evaluation using a subset of adversarial queries on three models (Llama2-chat-7B, GPT3.5-turbo, Vicuna-13B) to verify hallucination rates and compare with reported results.
2. Conduct fairness assessment across all three evaluation scenarios (credit, criminal, health) with coefficient of variation calculations to validate observed disparities across demographic identities.
3. Test jailbreak-wrapped toxicity prompts on Llama2-chat-13B to measure toxicity scores before and after fine-tuning for safety alignment, verifying the benchmark's ability to bypass safety filters.