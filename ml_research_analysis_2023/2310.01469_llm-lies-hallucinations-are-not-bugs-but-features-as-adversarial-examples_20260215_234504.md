---
ver: rpa2
title: 'LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples'
arxiv_id: '2310.01469'
source_url: https://arxiv.org/abs/2310.01469
tags:
- llms
- hallucination
- attack
- which
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores hallucinations in Large Language Models (LLMs)
  by reframing them as adversarial examples. The authors propose a gradient-based
  token replacement method to automatically trigger hallucinations in two ways: by
  perturbing semantically coherent prompts and by using meaningless Out-of-Distribution
  (OoD) prompts.'
---

# LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples

## Quick Facts
- **arXiv ID**: 2310.01469
- **Source URL**: https://arxiv.org/abs/2310.01469
- **Reference count**: 7
- **Primary result**: Successfully triggered hallucinations in Vicuna-7B and LLaMA2-7B-chat with up to 92.31% success rate using gradient-based token replacement.

## Executive Summary
This paper reframes LLM hallucinations as adversarial examples rather than mere bugs, demonstrating that transformer models can be manipulated to produce specific pre-defined tokens through input perturbations. The authors propose a gradient-based token replacement method that achieves high success rates (up to 92.31% for weak semantic attacks on Vicuna-7B) in eliciting hallucinations. The study shows that both semantically coherent prompts and completely random Out-of-Distribution prompts can trigger hallucinations, suggesting this behavior is a fundamental feature of LLMs rather than just a training data artifact. A simple entropy-based defense strategy is proposed to mitigate such attacks.

## Method Summary
The authors use a gradient-based token replacement strategy to automatically trigger hallucinations in LLMs. They create a hallucination dataset with common-sense questions and manually fabricated fake facts, then initialize adversarial prompts (either semantic or random tokens) and iteratively replace tokens using first-order approximations of log-likelihood changes. The method includes two attack modes: weak semantic attacks that preserve human-understandable meaning, and Out-of-Distribution attacks using random tokens. Success is measured by whether the model outputs the pre-defined hallucinated responses. An entropy-based defense mechanism is proposed, using the entropy of the first token prediction to refuse responding to potentially adversarial prompts.

## Key Results
- Weak semantic attacks achieved 92.31% success rate on Vicuna-7B and 46.15% on LLaMA2-7B-chat
- OoD attacks with 30-token random prompts achieved 65.38% success rate on Vicuna-7B
- Entropy threshold of 1.6 refused 46.1% of OoD prompts and 61.5% of weak semantic prompts while allowing all raw prompts
- Longer adversarial prompts generally yielded higher attack success rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can be manipulated to produce specific pre-defined tokens by perturbing its input sequence.
- Mechanism: The gradient-based token replacing strategy computes first-order approximations of the change in log-likelihood produced by swapping tokens, selecting top-k replacements that maximize the likelihood of the desired output.
- Core assumption: LLMs share similar adversarial vulnerability features with conventional neural networks, making them susceptible to input perturbations.
- Break condition: If the gradient signal becomes too noisy or the top-k selection no longer correlates with successful hallucination generation.

### Mechanism 2
- Claim: Non-sense Out-of-Distribution prompts can elicit LLMs to respond with hallucinations.
- Mechanism: By initializing prompts with random tokens and iteratively replacing them to maximize likelihood of pre-defined hallucinated responses, the model generates coherent-sounding but factually incorrect answers.
- Core assumption: LLMs have learned patterns that allow them to generate plausible responses even to meaningless inputs, revealing a fundamental vulnerability.
- Break condition: If the model's response entropy consistently remains high or fails to converge to meaningful-looking text.

### Mechanism 3
- Claim: Hallucinations are a fundamental feature of LLMs rather than just training data artifacts.
- Mechanism: The success of attacks using both semantically coherent perturbations and completely random prompts demonstrates that hallucination behavior extends beyond memorization issues.
- Core assumption: The vulnerability to adversarial prompts is an inherent property of transformer-based architectures, not just a consequence of training data quality.
- Break condition: If ablation studies show hallucination rates drop significantly when training on cleaner data or with different architectures.

## Foundational Learning

- **Concept: Gradient-based adversarial attacks**
  - Why needed here: The paper uses gradient ascent to find token replacements that maximize the likelihood of desired hallucinated outputs
  - Quick check question: How does computing the gradient of log-likelihood with respect to input tokens enable finding adversarial examples?

- **Concept: Out-of-Distribution (OoD) detection and response**
  - Why needed here: The paper demonstrates that OoD prompts can trigger hallucinations, showing models respond to inputs far from training distribution
  - Quick check question: What characteristics distinguish OoD prompts that successfully trigger hallucinations from those that don't?

- **Concept: Entropy-based confidence measures**
  - Why needed here: The proposed defense uses entropy thresholds on token predictions to identify potentially adversarial prompts
  - Quick check question: How does entropy of the first token prediction differ between clean and attacked prompts?

## Architecture Onboarding

- **Component map**: Hallucination dataset generator -> Gradient-based token replacer -> Two attack modes (weak semantic and OoD) -> Entropy-based defense mechanism
- **Critical path**: Generate hallucinated QA pairs → Initialize adversarial prompt (semantic or random) → Iteratively replace tokens using gradient guidance → Evaluate success rate → Apply entropy threshold defense
- **Design tradeoffs**: Weak semantic attacks preserve human-understandable meaning but may be easier to detect; OoD attacks are harder to detect but require more optimization iterations. The entropy defense is simple but may have false positives.
- **Failure signatures**: Attack failure occurs when log-likelihood optimization plateaus without generating desired hallucinations; defense failure occurs when entropy thresholds are set too low (missing attacks) or too high (blocking legitimate queries).
- **First 3 experiments**:
  1. Run weak semantic attack on Vicuna-7B with a simple QA pair to verify the gradient-based token replacement works
  2. Test OoD attack with 20-token random initialization to observe success rate trends with prompt length
  3. Implement entropy threshold defense and measure recall/precision tradeoff curves at different threshold values

## Open Questions the Paper Calls Out
- How does the success rate of OoD attacks scale with the length of the adversarial prompt beyond the tested range (up to 30 tokens)?
- Can the entropy-based defense threshold be optimized to balance refusal of adversarial prompts while minimizing false positives on benign prompts?
- Are there more effective defense mechanisms against hallucination attacks than the simple entropy threshold proposed, given the high computational cost of adversarial training for LLMs?

## Limitations
- Limited empirical scope testing only two models (Vicuna-7B and LLaMA2-7B-chat) without generalization to larger models or different architectures
- Evaluation constraints using narrow token-level success metric rather than semantic plausibility or potential harm assessment
- Gradient approximation concerns using first-order approximations without thorough exploration of hyperparameter sensitivity
- Defense simplicity lacking comparison to more sophisticated detection methods

## Confidence
- **High confidence**: The empirical demonstration that gradient-based token replacement can successfully trigger hallucinations in both semantic and OoD attack modes
- **Medium confidence**: The claim that hallucinations represent a fundamental architectural feature rather than training artifacts
- **Low confidence**: The assertion that this work provides "theoretical evidence" for the adversarial nature of hallucinations

## Next Checks
- Test whether the same gradient-based attack methodology works on larger, more capable models (GPT-3.5/4, Claude) and whether success rates scale predictably with model size
- Implement a more comprehensive evaluation framework that measures not just token-level success but semantic plausibility, factual consistency, and potential harm of hallucinated outputs
- Conduct ablation experiments varying training data quality, model architecture (different attention mechanisms, layer counts), and optimization hyperparameters to determine which factors most strongly influence hallucination vulnerability