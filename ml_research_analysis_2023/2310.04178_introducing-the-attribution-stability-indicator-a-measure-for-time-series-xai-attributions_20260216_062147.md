---
ver: rpa2
title: 'Introducing the Attribution Stability Indicator: a Measure for Time Series
  XAI Attributions'
arxiv_id: '2310.04178'
source_url: https://arxiv.org/abs/2310.04178
tags:
- time
- series
- attributions
- attribution
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The Attribution Stability Indicator (ASI) is a novel measure for
  evaluating the robustness and trustworthiness of attribution techniques in time
  series XAI. It extends perturbation analysis by incorporating correlations between
  original and perturbed time series, as well as attributions, to capture five key
  requirements: class flip, prediction probability changes, attribution distances,
  time series perturbation distances, and user-based weighting.'
---

# Introducing the Attribution Stability Indicator: a Measure for Time Series XAI Attributions

## Quick Facts
- arXiv ID: 2310.04178
- Source URL: https://arxiv.org/abs/2310.04178
- Reference count: 35
- Primary result: ASI provides more diverse evaluation scores than simple flip counting by integrating multiple similarity measures

## Executive Summary
The Attribution Stability Indicator (ASI) is a novel measure for evaluating the robustness and trustworthiness of attribution techniques in time series XAI. It extends perturbation analysis by incorporating correlations between original and perturbed time series, as well as attributions, to capture five key requirements: class flip, prediction probability changes, attribution distances, time series perturbation distances, and user-based weighting. Experiments on three time series classification datasets demonstrate that ASI provides more diverse scores than simple flip counting, revealing spurious correlations and helping select appropriate attribution techniques and projection methods.

## Method Summary
ASI evaluates attribution techniques by applying perturbations to time series based on attributions and measuring five requirements: class flip occurrences, changes in prediction probabilities (using Jensen-Shannon distance), distances between original and perturbed attributions (using Pearson correlation), distances between original and perturbed time series, and user-defined weighting. The measure combines these requirements with weighting parameters to provide a comprehensive evaluation of attribution quality. The method was tested on three time series classification datasets (FordA, FordB, ElectricDevices) using pre-trained CNN and ResNet models with various attribution techniques.

## Key Results
- ASI provides more diverse scores than simple flip counting, revealing spurious correlations
- The measure successfully identifies working attribution techniques across different datasets
- Visualization of attribution distributions and projections aids in understanding model behavior and selecting appropriate attribution methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASI achieves more diverse evaluation scores than simple flip counting by integrating multiple similarity measures.
- Mechanism: ASI combines five requirements using the Jensen-Shannon distance for prediction probabilities and Pearson correlation for attribution and time series similarity, with user-defined weighting to capture different aspects of attribution quality.
- Core assumption: The combination of these five requirements with proper weighting provides a more comprehensive evaluation than any single metric.
- Evidence anchors:
  - [abstract] "ASI provides more diverse scores than simple flip counting, revealing spurious correlations and helping select appropriate attribution techniques"
  - [section 3.3] "We propose the Attribution Stability Indicator ( ASI) as another measure to not only use one single quality metric"
- Break condition: If the weighting parameters are not properly tuned, the measure could become dominated by one requirement, reducing its effectiveness in capturing diverse aspects of attribution quality.

### Mechanism 2
- Claim: ASI enables better understanding of model behavior through visualization of attribution distributions and projections.
- Mechanism: By providing ASI scores for whole datasets and visualizing attribution projections, users can identify working attribution techniques and problematic samples.
- Core assumption: The ASI score distribution and attribution projections provide meaningful insights into attribution quality and model behavior.
- Evidence anchors:
  - [abstract] "ASI successfully identifies working attribution techniques and aids in understanding model behavior through visualization of attribution distributions and projections"
  - [section 4] "Collecting all possible scores for ASI on both the training and test datasets can be visualized in a distribution overview to gain insights"
- Break condition: If the projection technique (UMAP) creates local minima or non-optimal solutions, the visualizations may not accurately represent attribution quality differences.

### Mechanism 3
- Claim: ASI can reveal spurious correlations in attribution techniques that flip counting might miss.
- Mechanism: By incorporating probability distribution changes and attribution distances, ASI can detect cases where attribution techniques appear to work (based on flips) but actually capture spurious correlations.
- Core assumption: Changes in prediction probabilities and attribution distances are sensitive indicators of spurious correlations that simple flip counting misses.
- Evidence anchors:
  - [abstract] "ASI provides more diverse scores than simple flip counting, revealing spurious correlations"
  - [section 4] "The number of flips can be the same or similar for different attribution techniques [24], while ASI can show a score more diverse"
- Break condition: If the perturbation function introduces artifacts that affect both the original and perturbed instances similarly, ASI might incorrectly identify spurious correlations where none exist.

## Foundational Learning

- Jensen-Shannon Distance
  - Why needed here: Used to measure similarity between prediction probability distributions before and after perturbation
  - Quick check question: What property of the Jensen-Shannon distance makes it suitable for comparing prediction probabilities in ASI?

- Pearson Correlation Coefficient
  - Why needed here: Used to measure similarity between original and perturbed time series, and between original and perturbed attributions
  - Quick check question: Why is Pearson correlation appropriate for measuring attribution similarity in time series data?

- UMAP Projection
  - Why needed here: Used to visualize attribution distributions in 2D space to identify working attribution techniques
  - Quick check question: What is a potential limitation of using UMAP for visualizing attribution distributions?

## Architecture Onboarding

- Component map:
  Time series classification model (CNN or ResNet) -> Attribution technique (Saliency, DeepLIFT, etc.) -> Perturbation function (zero substitution, OOD low substitution) -> ASI calculator (combines five requirements with weighting) -> Visualization component (UMAP projections of attributions)

- Critical path:
  1. Generate attributions for time series using selected technique
  2. Apply perturbation function based on attributions
  3. Calculate ASI score using five requirements
  4. Visualize attribution distributions and projections
  5. Analyze results to select working attribution techniques

- Design tradeoffs:
  - Weighting flexibility vs. parameter tuning complexity
  - Pearson correlation simplicity vs. potential limitations for non-linear relationships
  - UMAP projection interpretability vs. potential local minima issues

- Failure signatures:
  - ASI scores not showing expected diversity across attribution techniques
  - Projection visualizations not showing clear class separation
  - ASI scores inconsistent with manual inspection of individual samples

- First 3 experiments:
  1. Compare ASI scores for a single sample across different attribution techniques using default weights
  2. Visualize attribution projections for a small dataset using different weighting schemes
  3. Test ASI's ability to detect spurious correlations by comparing flip counts vs. ASI scores on a known problematic dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ASI measure perform when applied to out-of-distribution (OOD) time series data, and what modifications would be needed to incorporate OOD detection into the perturbation analysis?
- Basis in paper: [explicit] The paper mentions this as a future work suggestion, noting that incorporating an OOD check for perturbed time series based on attributions could introduce a plausibility check and focus on more realistic time series.
- Why unresolved: The current ASI formulation does not include any mechanism for detecting or handling OOD samples, which could affect the reliability of the measure when applied to real-world data.
- What evidence would resolve it: Empirical results comparing ASI scores on in-distribution vs. OOD data, along with modifications to the perturbation function that incorporate OOD detection, would demonstrate whether this improves the measure's robustness.

### Open Question 2
- Question: How sensitive is the ASI measure to the choice of weighting parameters, and can automated methods be developed to optimize these weights for specific datasets or tasks?
- Basis in paper: [explicit] The paper acknowledges that requirements can change from dataset to dataset and includes a weighting term to allow users to steer the measure, but notes that the selected weights may be misleading for certain datasets like ElectricDevices.
- Why unresolved: The paper only provides one example of weighting (W = (0.5, 1, 0.5, 3)) and suggests that other weights might be better for different datasets, but does not explore how to systematically determine optimal weights.
- What evidence would resolve it: Results from systematically varying the weights across multiple datasets and tasks, along with automated methods for weight optimization based on dataset characteristics, would clarify the sensitivity and potential for automation.

### Open Question 3
- Question: How does the ASI measure compare to other attribution evaluation metrics like infidelity when applied to time series data, and what are the key differences in their behavior and interpretation?
- Basis in paper: [explicit] The paper mentions comparing ASI to the infidelity metric of Yeh et al. [34] using Captum on the CNN of the FordA dataset, noting that infidelity demonstrated more diverse scores between train and test data.
- Why unresolved: The comparison is brief and only covers one dataset and model, leaving open questions about the general behavior of ASI versus infidelity across different time series datasets and models.
- What evidence would resolve it: Comprehensive experiments comparing ASI and infidelity across multiple time series datasets, models, and attribution techniques would reveal their relative strengths and weaknesses in different contexts.

## Limitations

- The optimal weighting scheme for ASI requirements remains dataset-dependent and unvalidated across diverse scenarios
- Sensitivity of ASI to perturbation function choice and parameter settings is not fully characterized
- Correlation between high ASI scores and actual attribution quality in real-world applications needs further validation

## Confidence

The confidence in ASI's effectiveness is **Medium** for the claim that it provides more diverse evaluation scores than simple flip counting, as the evidence shows ASI produces different scores but doesn't conclusively demonstrate superiority in all scenarios. The claim that ASI reveals spurious correlations is **Low** confidence due to limited empirical validation. The visualization benefits are **Medium** confidence based on the described methodology but lack quantitative validation.

## Next Checks

1. **Weight Sensitivity Analysis**: Systematically vary the weighting parameters in ASI across the five requirements and measure how this affects the ranking of attribution techniques on each dataset.

2. **Perturbation Strategy Comparison**: Compare ASI scores using different perturbation functions (zero substitution vs. OOD low substitution) to determine which strategy provides more reliable attribution evaluation.

3. **Real-world Application Test**: Apply ASI to a time series classification problem from a real domain (e.g., medical time series) and validate whether ASI-identified attribution techniques align with domain expert expectations.