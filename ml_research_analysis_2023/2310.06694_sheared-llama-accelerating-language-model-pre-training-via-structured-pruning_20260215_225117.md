---
ver: rpa2
title: 'Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning'
arxiv_id: '2310.06694'
source_url: https://arxiv.org/abs/2310.06694
tags:
- pruning
- training
- data
- arxiv
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently producing smaller
  yet competitive large language models (LLMs) by leveraging existing pre-trained,
  larger models. The authors propose a method called "LLM-shearing" that combines
  targeted structured pruning and dynamic batch loading.
---

# Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning

## Quick Facts
- arXiv ID: 2310.06694
- Source URL: https://arxiv.org/abs/2310.06694
- Authors: 
- Reference count: 37
- One-line primary result: Prunes LLaMA2-7B to create smaller competitive models (1.3B, 2.7B) using only 3% of compute budget

## Executive Summary
This paper introduces LLM-shearing, a method for efficiently producing smaller yet competitive large language models by leveraging existing pre-trained, larger models. The approach combines targeted structured pruning with dynamic batch loading to compress LLaMA2-7B into smaller models while maintaining competitive performance. The authors demonstrate that their Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes on a wide range of downstream tasks while requiring only 3% of the compute budget compared to training from scratch.

## Method Summary
The method employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner; and (2) dynamic batch loading, which adjusts domain proportions on the fly based on the model's performance to ensure balanced loss reduction across domains. The pruned models undergo continued pre-training to recover performance, with the entire process requiring only a fraction of the compute needed to train equivalent models from scratch.

## Key Results
- Sheared-LLaMA-1.3B and Sheared-LLaMA-2.7B outperform state-of-the-art open-source models of equivalent sizes
- The approach requires only 3% of compute budget compared to training equivalent models from scratch
- Sheared-LLaMA models achieve competitive performance on a wide range of downstream tasks and instruction tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Targeted structured pruning enables efficient model compression while preserving key capabilities by aligning pruned architectures to well-established pre-trained model configurations.
- Mechanism: The pruning process searches for subnetworks within the source model that match a pre-specified target architecture, preserving performance by learning pruning masks at multiple granularities (layers, heads, dimensions) via a constrained optimization formulation.
- Core assumption: Existing pre-trained model architectures represent an optimized balance between model expressivity and inference efficiency, making them suitable targets for pruning.
- Evidence anchors:
  - [abstract]: "Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner"
  - [section 2.1]: "Our method learns a set of pruning masks on model parameters at different granularities—from global ones like layers and hidden dimensions... to local ones like attention heads and intermediate dimensions."
  - [corpus]: Weak - no direct mention of structured pruning or architectural optimization in related works
- Break condition: If the target architecture significantly deviates from optimal configurations for the given task domain, performance will degrade despite preservation of overall model structure.

### Mechanism 2
- Claim: Dynamic batch loading accelerates convergence and improves data efficiency by adjusting domain proportions based on relative loss reduction rates.
- Mechanism: The algorithm dynamically updates the proportion of data from each domain in each training batch based on the difference between current and reference validation losses, ensuring balanced loss reduction across domains.
- Core assumption: Pruned models retain varying levels of knowledge across different domains, and simply using the original pre-training data distribution results in inefficient learning.
- Evidence anchors:
  - [section 2.2]: "We propose dynamic batch loading, a more efficient algorithm to simply adjust domain proportions on the fly based on the model performance... The goal is to ensure the model achieves the reference loss roughly simultaneously across all domains."
  - [section 4.1]: "Dynamic batch loading is designed to balance the rate of loss reduction across domains, so that the losses reach the reference value at approximately the same time."
  - [corpus]: Weak - no direct mention of dynamic data loading or loss-based domain weighting in related works
- Break condition: If the reference losses are poorly estimated or domains have highly variable difficulty, the dynamic adjustment may lead to unstable training or suboptimal convergence.

### Mechanism 3
- Claim: Continued pre-training after pruning is essential for recovering performance and achieving competitive results with minimal compute compared to training from scratch.
- Mechanism: The pruned model undergoes additional pre-training on the same data distribution as the original model, allowing it to refine learned representations and recover any performance loss from the pruning process.
- Core assumption: The pruned model serves as a strong initialization that can benefit from further training, rather than requiring a complete retraining from scratch.
- Evidence anchors:
  - [abstract]: "Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes... while using only a fraction of the compute budget to train those models from scratch."
  - [section 3.2]: "Sheared-LLaMA significantly outperforms existing LLMs of similar sizes, while using only a fraction of the compute budget to train those models from scratch."
  - [corpus]: Weak - no direct mention of continued pre-training or compute efficiency comparisons in related works
- Break condition: If the pruning process removes critical parameters or the continued pre-training is insufficient, the model may fail to recover competitive performance.

## Foundational Learning

- Concept: Scaling laws for language models
  - Why needed here: The paper relies on fitting scaling laws to predict reference losses for hypothetical models at different scales, which guides the dynamic batch loading process and evaluation of pruning effectiveness.
  - Quick check question: How does the scaling law equation L(N,D) = E + A*N^α + B*D^β relate model size and dataset size to expected loss?

- Concept: Structured pruning techniques
  - Why needed here: The paper employs targeted structured pruning to remove entire layers, attention heads, and dimensions while preserving model structure, which requires understanding different pruning granularities and their effects.
  - Quick check question: What is the difference between structured pruning and unstructured pruning, and why is structured pruning preferred for accelerating inference?

- Concept: Domain adaptation and data distribution
  - Why needed here: The paper observes that pruned models retain varying knowledge across different domains (e.g., GitHub vs C4), necessitating dynamic adjustment of domain proportions during training.
  - Quick check question: Why might a pruned model retain more knowledge from smaller, lower-entropy domains compared to larger, higher-entropy domains?

## Architecture Onboarding

- Component map: LLaMA2-7B model -> Targeted structured pruning (layer, head, dimension masks) -> Dynamic batch loading (domain proportion adjustment) -> Continued pre-training (50B tokens) -> Sheared-LLaMA-1.3B/2.7B

- Critical path: The critical path involves (1) pruning the source model to match the target architecture using targeted structured pruning, (2) dynamically adjusting domain proportions during training using dynamic batch loading, and (3) continuing pre-training to recover performance and achieve competitive results.

- Design tradeoffs: The approach trades off some architectural flexibility (inheriting source model characteristics like GLU variants) for efficiency gains, and accepts the computational cost of continued pre-training to recover performance after pruning.

- Failure signatures: Performance degradation may occur if the target architecture is poorly chosen, if dynamic batch loading fails to balance domain learning, or if continued pre-training is insufficient to recover pruned capabilities.

- First 3 experiments:
  1. Verify that targeted structured pruning can successfully compress LLaMA2-7B to match the target architecture while preserving performance on validation tasks.
  2. Test dynamic batch loading by comparing loss trajectories and downstream performance with and without dynamic domain weighting during continued pre-training.
  3. Compare the compute efficiency and final performance of Sheared-LLaMA against models trained from scratch on similar data budgets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Sheared-LLaMA models change when pruned from larger LLaMA models (e.g., LLaMA2-13B or LLaMA2-70B) compared to pruning from LLaMA2-7B?
- Basis in paper: [explicit] The paper states "While we only conduct experiments with up to 7B parameter models, our LLM-shearing algorithm is highly generalizable and can be extended to large language models of any size in future work."
- Why unresolved: The authors only experimented with pruning from a 7B parameter LLaMA model, leaving the performance impact of pruning from larger LLaMA models unexplored.
- What evidence would resolve it: Conducting experiments to prune smaller LLaMA models from larger LLaMA models (e.g., LLaMA2-13B or LLaMA2-70B) and comparing their performance to the current Sheared-LLaMA models.

### Open Question 2
- Question: How does the choice of reference losses (scaling reference vs. source reference) impact the downstream performance of Sheared-LLaMA models on math and coding tasks?
- Basis in paper: [explicit] The paper mentions that Sheared-LLaMA outperforms baselines on math and coding tasks but lags behind models trained on more ArXiv and GitHub data. It also discusses using scaling reference and source reference for dynamic batch loading.
- Why unresolved: The paper only briefly mentions the performance on math and coding tasks and does not explore the impact of different reference loss choices on these specific tasks.
- What evidence would resolve it: Conducting experiments to train Sheared-LLaMA models using both scaling reference and source reference for dynamic batch loading, and evaluating their performance on math and coding tasks.

### Open Question 3
- Question: How does the performance of Sheared-LLaMA models change when using different pre-training datasets (e.g., LLaMA data, OPT data, or other large-scale datasets) compared to RedPajama?
- Basis in paper: [inferred] The paper uses RedPajama for pruning and continued pre-training, but does not explore the impact of using different pre-training datasets.
- Why unresolved: The authors only experimented with RedPajama as the pre-training dataset, leaving the performance impact of using other datasets unexplored.
- What evidence would resolve it: Conducting experiments to train Sheared-LLaMA models using different pre-training datasets and comparing their performance to the current models trained on RedPajama.

## Limitations
- Evaluation focuses on a single base model (LLaMA2-7B) pruned to two target sizes, limiting generalizability
- Computational efficiency claims rely on comparisons with training from scratch, lacking direct comparisons with other pruning methods
- Dynamic batch loading effectiveness depends on accurate scaling law estimates for reference losses, which may not generalize across domains

## Confidence
- High Confidence: Claims about computational efficiency (3% of compute vs training from scratch) are well-supported by measured training steps and token counts. The structured pruning methodology and its implementation details are clearly specified and reproducible.
- Medium Confidence: Downstream task performance improvements over existing models are demonstrated, but the evaluation covers a limited set of benchmarks. The effectiveness of dynamic batch loading is supported by loss trajectory observations but lacks ablation studies isolating its contribution.
- Low Confidence: The claim that pruned models retain varying knowledge across domains (GitHub vs C4) is observed but not thoroughly analyzed. The generalizability of the pruning approach across different base model families remains untested.

## Next Checks
1. **Scaling Law Parameter Verification**: Validate the accuracy of scaling law predictions by comparing predicted reference losses against actual training curves across multiple domains, particularly for domains with different data characteristics.

2. **Architecture Sensitivity Analysis**: Test the pruning approach across multiple base models (e.g., Mistral, Llama-1 series) and target architectures to assess robustness and identify architectural constraints that may limit performance.

3. **Dynamic Batch Loading Ablation**: Conduct controlled experiments comparing continued pre-training with static vs. dynamic domain weighting to isolate the contribution of the dynamic batch loading mechanism to final performance.