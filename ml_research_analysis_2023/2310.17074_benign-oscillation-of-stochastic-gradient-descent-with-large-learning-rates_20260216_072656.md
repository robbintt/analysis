---
ver: rpa2
title: Benign Oscillation of Stochastic Gradient Descent with Large Learning Rates
arxiv_id: '2310.17074'
source_url: https://arxiv.org/abs/2310.17074
tags:
- learning
- lemma
- data
- training
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large learning rates in stochastic
  gradient descent (SGD) training of neural networks can lead to better generalization.
  The key idea is that oscillating training dynamics induced by large learning rates
  help the network learn weaker but important features in the data that are crucial
  for generalization.
---

# Benign Oscillation of Stochastic Gradient Descent with Large Learning Rates

## Quick Facts
- arXiv ID: 2310.17074
- Source URL: https://arxiv.org/abs/2310.17074
- Reference count: 40
- Primary result: Large learning rates in SGD training induce beneficial oscillations that help neural networks learn weak but important features, improving generalization

## Executive Summary
This paper investigates how large learning rates in stochastic gradient descent (SGD) training can lead to better generalization in neural networks. The key insight is that oscillating training dynamics induced by large learning rates help the network learn weaker but crucial features in the data. Through a feature-noise data generation model with weak and strong features, the authors show that SGD with large learning rates can effectively learn the weak features, while small learning rate SGD only learns the strong features. Experiments on image classification demonstrate the benefits of "benign oscillation" for generalization.

## Method Summary
The paper analyzes a two-layer convolutional neural network trained on a feature-noise data generation model. The model consists of weak features (small ℓ2-norm, appear in all data), strong features (larger ℓ2-norm, appear in a subset), and noise patches. SGD updates are performed with either large or small learning rates. The analysis tracks inner products during training to understand how different features are learned under different learning rate regimes. Experiments are conducted on CIFAR-10 using ResNet-18 with different learning rates to compare generalization performance.

## Key Results
- Large learning rate SGD induces beneficial oscillations that accumulate over time and drive learning of weak features
- Small learning rate SGD converges smoothly but fails to learn weak features, resulting in poor generalization on data without strong features
- Experiments on CIFAR-10 demonstrate improved test accuracy with large learning rates due to benign oscillation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large learning rate SGD training enables effective learning of weak features that are crucial for generalization.
- **Mechanism:** Oscillation prevents over-greedy convergence which would only leverage the most prominent components (strong features). Instead, fluctuations accumulate linearly over time and drive learning of weaker but important patterns.
- **Core assumption:** Weak features have smaller ℓ2-norm but appear in all data points, while strong features have larger ℓ2-norm but only appear in a subset.
- **Evidence anchors:**
  - [abstract] "oscillating SGD with a large learning rate can effectively learn the weak features in the presence of those strong features"
  - [section] "the oscillation prevents the over-greedy convergence and serves as the engine that drives the learning of less-prominent data patterns"
  - [corpus] Weak - No direct corpus evidence found for this specific mechanism. Evidence comes from the paper's theoretical analysis and feature-noise data generation model.
- **Break condition:** If learning rate is too small, SGD converges smoothly and only learns strong features, failing to generalize to data without strong features.

### Mechanism 2
- **Claim:** Oscillation in SGD training accumulates linearly rather than canceling out, driving weak feature learning.
- **Mechanism:** When SGD values oscillate around the target label, the fluctuations do not cancel with each other. Instead, the summation of deviations accumulates linearly over time, creating momentum for learning weak features.
- **Core assumption:** The magnitude of oscillation is bounded away from the target by a uniform constant δ > 0.
- **Evidence anchors:**
  - [section] "the fluctuations would accumulate linearly over time (Lemma D.3). This further serves as the engine driving the learning of the weak features"
  - [abstract] "the oscillation of the NN weights caused by the large learning rate SGD training turns out to be beneficial to the generalization"
  - [corpus] Weak - No direct corpus evidence found for this specific accumulation mechanism. Evidence comes from the paper's theoretical analysis.
- **Break condition:** If oscillation magnitude is too small or if convergence is too smooth, the accumulation effect disappears and weak features remain unlearned.

### Mechanism 3
- **Claim:** Small learning rate SGD training fails to learn weak features, resulting in poor generalization.
- **Mechanism:** Smooth convergence with small learning rates causes the network to quickly fit training data using strong features, making little progress on weak features. When tested on data without strong features, the network fails.
- **Core assumption:** The weak feature learning progress is proportional to the magnitude of accumulated oscillations.
- **Evidence anchors:**
  - [abstract] "NNs trained by SGD with a small learning rate can only learn the strong features but makes little progress in learning the weak features"
  - [section] "smooth and rapid convergence achieved by SGD with small learning rates would not help the NN learn the weak features"
  - [corpus] Weak - No direct corpus evidence found for this specific failure mode. Evidence comes from the paper's theoretical analysis comparing large vs small learning rates.
- **Break condition:** If weak features have comparable strength to strong features, or if all testing data contains strong features, the failure mode may not manifest.

## Foundational Learning

- **Concept:** Feature learning perspective in deep learning
  - Why needed here: The paper builds upon the feature learning framework to explain how different types of features (weak vs strong) are learned under different training regimes
  - Quick check question: Can you explain the difference between weak features and strong features in the context of this paper's data generation model?

- **Concept:** Stochastic gradient descent dynamics with large learning rates
  - Why needed here: Understanding how SGD behaves differently with large vs small learning rates is crucial for explaining the oscillation phenomenon and its effects on feature learning
  - Quick check question: What happens to SGD training when the learning rate exceeds the inverse of the objective smoothness?

- **Concept:** Edge of stability regime in neural network optimization
  - Why needed here: The paper connects the oscillation phenomenon to the "edge of stability" regime, where the sharpness of the loss Hessian hovers just above 2/η
  - Quick check question: How does the edge of stability regime relate to the generalization performance of neural networks?

## Architecture Onboarding

- **Component map:** Two-layer CNN with filters applied to three patches separately -> Data generation model with weak features, strong features, and noise patches -> SGD updates with configurable learning rates
- **Critical path:** Track inner products ⟨wj,r, ju⟩, ⟨wj,r, jv⟩, and ⟨wj,r, ξ⟩ during training to understand how different features are learned under different learning rate regimes
- **Design tradeoffs:** Large learning rates enable learning of weak features through oscillation but may lead to unstable training. Small learning rates provide stable convergence but only learn strong features, sacrificing generalization.
- **Failure signatures:** If SGD with large learning rates fails to show oscillation patterns, or if the accumulated fluctuations are too small, weak feature learning may not occur. If SGD with small learning rates shows signs of learning weak features (which contradicts the theory), the analysis assumptions may be violated.
- **First 3 experiments:**
  1. Implement the feature-noise data generation model with two types of features and test SGD with large vs small learning rates on a simple classification task
  2. Track the inner product dynamics during training to observe the accumulation of fluctuations under large learning rate regime
  3. Test the trained networks on data that lacks strong features to verify the generalization differences predicted by the theory

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the "benign oscillation" phenomenon generalize to more complex data generation models beyond the simple feature-noise model with weak and strong features?
- Basis in paper: [explicit] The paper mentions that the data model can be extended for generality, e.g., multiple features, more patches, multi-class data.
- Why unresolved: The current theory focuses on a specific data model with two types of features and Gaussian noise. It's unclear how the oscillation mechanism would work with more complex feature interactions or non-Gaussian noise distributions.
- What evidence would resolve it: Theoretical analysis of SGD dynamics with large learning rates on extended data models, supported by empirical experiments on datasets with more complex feature structures.

### Open Question 2
- Question: What is the precise relationship between the learning rate regime (small vs. large) and the implicit bias towards certain types of minima in the loss landscape?
- Basis in paper: [explicit] The paper discusses how small learning rate SGD leads to smooth convergence but poor generalization, while large learning rate SGD causes oscillation that helps learn weak features. It mentions the "edge of stability" phenomenon but doesn't fully explain the connection.
- Why unresolved: While the paper shows that oscillation helps learn weak features, it doesn't provide a complete characterization of how different learning rate regimes bias SGD towards different types of minima (e.g., flat vs. sharp) and how this affects generalization.
- What evidence would resolve it: Detailed theoretical analysis of the Hessian spectrum at convergence for different learning rate regimes, combined with experiments on how this affects generalization performance.

### Open Question 3
- Question: How does the benign oscillation phenomenon extend to other optimization algorithms beyond vanilla SGD, such as Adam or momentum-based methods?
- Basis in paper: [inferred] The paper focuses on SGD with large learning rates and its oscillation-induced generalization benefits. However, it's unclear whether similar effects occur with other optimizers that also exhibit oscillatory behavior.
- Why unresolved: The analysis relies on specific properties of SGD updates and may not directly apply to adaptive methods or momentum-based optimizers. The interaction between these algorithms' dynamics and the feature learning process is not explored.
- What evidence would resolve it: Comparative analysis of different optimizers' performance on the same feature-noise data model, with a focus on identifying conditions under which oscillation-like phenomena occur and their effects on generalization.

## Limitations

- The theoretical analysis relies on a simplified feature-noise data generation model that may not fully capture the complexity of real-world datasets.
- The conditions for benign oscillation (e.g., specific feature strength ratios, learning rate ranges) need further validation on diverse architectures and tasks beyond the two-layer CNN and ResNet-18 experiments presented.
- The paper does not fully characterize the relationship between the learning rate regime and the implicit bias towards different types of minima in the loss landscape.

## Confidence

- **High Confidence:** The observation that large learning rates lead to oscillation and that this oscillation can accumulate over time (Mechanism 2)
- **Medium Confidence:** The specific mechanism by which oscillation enables weak feature learning and the failure mode of small learning rates (Mechanisms 1 and 3)
- **Medium Confidence:** The experimental results on CIFAR-10 showing improved generalization with large learning rates, though the connection to the theoretical model requires more exploration

## Next Checks

1. Test the feature learning dynamics on more complex architectures (e.g., Vision Transformers) and diverse datasets to verify the generalizability of the benign oscillation phenomenon.

2. Conduct ablation studies to identify the critical factors for benign oscillation, such as the minimum feature strength ratio, learning rate thresholds, and network architecture properties.

3. Analyze the impact of benign oscillation on other aspects of model behavior, such as robustness to adversarial attacks, out-of-distribution generalization, and calibration, to understand the broader implications of this phenomenon.