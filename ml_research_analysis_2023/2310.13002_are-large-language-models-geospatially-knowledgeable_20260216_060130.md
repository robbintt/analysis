---
ver: rpa2
title: Are Large Language Models Geospatially Knowledgeable?
arxiv_id: '2310.13002'
source_url: https://arxiv.org/abs/2310.13002
tags:
- geospatial
- llms
- cities
- language
- city
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the geospatial knowledge, awareness, and
  reasoning capabilities of large language models (LLMs). The authors propose three
  experimental tasks: probing LLMs for geo-coordinates of cities, using geospatial
  and non-geospatial prepositions to gauge geospatial awareness, and utilizing multidimensional
  scaling to assess geospatial reasoning.'
---

# Are Large Language Models Geospatially Knowledgeable?

## Quick Facts
- arXiv ID: 2310.13002
- Source URL: https://arxiv.org/abs/2310.13002
- Reference count: 37
- Primary result: LLMs demonstrate geospatial knowledge, awareness, and reasoning capabilities, with larger models performing better on geospatial tasks.

## Executive Summary
This paper evaluates the geospatial knowledge, awareness, and reasoning capabilities of large language models (LLMs) through three experimental tasks. The authors probe LLMs for geo-coordinates of cities, test geospatial preposition sensitivity, and assess geospatial reasoning using multidimensional scaling. Results demonstrate that larger and more sophisticated LLMs encode geospatial knowledge more effectively and can be prompted to generate accurate geographic information. While LLMs show promising geospatial reasoning capabilities, further improvements are needed for practical applications requiring high accuracy.

## Method Summary
The study uses auto-regressive LLMs (OPT, LLaMA, and Alpaca) to evaluate geospatial capabilities through zero-shot and few-shot prompting on three tasks: coordinate prediction, geospatial preposition awareness, and geospatial reasoning via MDS. The experiments use the MaxMind database with 3,527 cities (population >100k) and 93 cities in contiguous US for awareness tasks. Beam search with 5 beams serves as the decoding strategy. The authors measure mean error distances in kilometers for coordinate predictions and analyze the correlation between LLM-generated distances and actual geographic distances for reasoning tasks.

## Key Results
- Larger LLMs demonstrate superior geospatial knowledge encoding, with better coordinate prediction accuracy
- Geospatial prepositions ("near," "far," "close to") significantly influence the spatial relationships in LLM-generated city lists
- LLMs can support geospatial reasoning through distance prediction with high Pearson correlation (0.92) to actual distances
- Zero-shot and few-shot prompting successfully elicit geospatial knowledge without model fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs encode geospatial knowledge through textual patterns in training data
- Mechanism: LLMs learn statistical co-occurrence patterns between city names and geographic descriptors during pretraining, storing this knowledge in model parameters
- Core assumption: Geographic information appears in structured forms in training data that LLMs can recognize
- Evidence anchors: Strong correlation between generation frequency and city co-occurrence suggests LLMs extract geospatial knowledge beyond simple pattern matching

### Mechanism 2
- Claim: Geospatial awareness emerges from LLMs' ability to associate spatial prepositions with actual distance relationships
- Mechanism: Through exposure to natural language describing spatial relationships, LLMs learn that certain prepositions correspond to specific distance ranges
- Core assumption: Training data contains sufficient examples of spatially accurate language usage
- Evidence anchors: Generation patterns show strong correlation between preposition use and actual distance relationships

### Mechanism 3
- Claim: LLMs support geospatial reasoning through distance prediction and MDS transformation
- Mechanism: LLMs learn implicit distance relationships from textual descriptions, generating values that correlate with actual distances
- Core assumption: LLMs capture relative distance information retrievable through appropriate prompting
- Evidence anchors: High Pearson correlation coefficient (0.92) between actual and predicted distances validates reasoning capability

## Foundational Learning

- Concept: Autoregressive language modeling
  - Why needed here: Essential for understanding how LLMs generate text token-by-token based on context for effective prompt design
  - Quick check question: How does beam search decoding differ from greedy decoding, and why might it be preferred for geospatial tasks?

- Concept: Prompt engineering and in-context learning
  - Why needed here: Critical for crafting prompts that elicit geospatial knowledge without model fine-tuning
  - Quick check question: What's the difference between zero-shot and few-shot prompting, and how might each affect geospatial coordinate prediction accuracy?

- Concept: Multidimensional scaling (MDS)
  - Why needed here: Mathematical technique used to convert distance matrices into spatial layouts for evaluating geospatial reasoning
  - Quick check question: How does classical MDS differ from metric MDS, and which would be more appropriate for transforming LLM-generated distance estimates?

## Architecture Onboarding

- Component map: Prompt template processor -> LLM core (transformer decoder) -> Beam search decoder -> Distance extraction -> MDS transformation pipeline -> Coordinate prediction -> Error calculation
- Critical path: Prompt → LLM generation → Post-processing → Distance extraction → MDS computation → Coordinate prediction → Error calculation
- Design tradeoffs: Larger models provide better geospatial knowledge but increase computational cost; zero-shot inference preserves model integrity but may sacrifice accuracy compared to fine-tuning
- Failure signatures: Poor coordinate prediction accuracy indicates insufficient geospatial knowledge encoding; inconsistent distance predictions suggest unreliable spatial awareness; high MDS transformation errors reveal reasoning limitations
- First 3 experiments:
  1. Implement zero-shot geo-coordinate prediction for a small set of well-known cities to establish baseline capability
  2. Test geospatial preposition awareness by generating cities for "near" vs "and" contexts and measuring distance differences
  3. Validate MDS pipeline using actual distances before incorporating LLM-predicted distances to ensure the transformation process works correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of large language models in geospatial tasks compare to specialized geospatial models?
- Basis in paper: The paper evaluates LLMs for geospatial tasks but does not compare their performance to specialized geospatial models
- Why unresolved: The study focuses on assessing LLM capabilities without benchmarking against dedicated geospatial models
- What evidence would resolve it: Conducting experiments that compare the performance of LLMs and specialized geospatial models on the same geospatial tasks

### Open Question 2
- Question: Can continuous prompts significantly improve the geospatial knowledge and awareness of large language models?
- Basis in paper: The paper suggests that continuous prompts could potentially improve accuracy and prediction rate for coordinate prediction tasks
- Why unresolved: The study does not experiment with continuous prompts and leaves the impact of this approach unexplored
- What evidence would resolve it: Implementing and testing continuous prompts in geospatial tasks to measure their effect on LLM performance

### Open Question 3
- Question: How does the training data influence the geospatial awareness of large language models?
- Basis in paper: The paper analyzes the effect of training data on geospatial awareness but findings are not conclusive
- Why unresolved: The study does not provide a definitive answer on the extent to which training data affects geospatial awareness
- What evidence would resolve it: Conducting a more comprehensive analysis of training data and its impact on geospatial awareness of LLMs

## Limitations

- Evaluation focuses exclusively on cities with populations over 100,000, creating potential sampling bias
- Reliance on zero-shot and few-shot prompting without exploring fine-tuning approaches that could yield more accurate results
- Does not address potential temporal inconsistencies in training data where geographic information might become outdated

## Confidence

- High confidence: LLMs encode geospatial knowledge retrievable through appropriate prompting strategies
- Medium confidence: LLMs demonstrate geospatial awareness through preposition sensitivity
- Medium confidence: LLMs can support geospatial reasoning tasks through distance prediction and MDS transformation

## Next Checks

1. **Temporal validation check**: Test LLM geospatial knowledge on cities that have undergone significant changes (name changes, boundary modifications, or administrative restructuring) to assess whether models retain outdated information

2. **Fine-tuning comparison check**: Implement a small-scale supervised fine-tuning experiment on geographically annotated data to quantify performance improvements over zero-shot approaches

3. **Cross-linguistic capability check**: Evaluate the same geospatial tasks using multilingual LLMs and prompts in different languages to determine whether geospatial knowledge encoding is language-dependent