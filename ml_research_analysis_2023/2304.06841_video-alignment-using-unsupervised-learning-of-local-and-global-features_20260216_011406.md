---
ver: rpa2
title: Video alignment using unsupervised learning of local and global features
arxiv_id: '2304.06841'
source_url: https://arxiv.org/abs/2304.06841
tags:
- video
- features
- action
- videos
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an unsupervised method for video alignment
  using local and global features extracted from each frame. The key idea is to model
  a video as a multidimensional time series, where each dimension represents a different
  feature of the frames over time.
---

# Video alignment using unsupervised learning of local and global features

## Quick Facts
- arXiv ID: 2304.06841
- Source URL: https://arxiv.org/abs/2304.06841
- Reference count: 40
- Key outcome: Achieves state-of-the-art video alignment results using unsupervised learning of local (pose, box) and global (VGG) features with Diagonalized Dynamic Time Warping

## Executive Summary
This paper introduces an unsupervised method for video alignment that combines local features (pose estimation and person detection) with global features (VGG network) to create a rich time series representation of video frames. The method introduces Diagonalized Dynamic Time Warping (DDTW), which penalizes alignment paths that deviate significantly from the diagonal, enforcing the assumption that similar actions progress roughly linearly in time. The approach achieves state-of-the-art results on the Penn Action dataset and a subset of UCF101, outperforming previous methods like TCC in phase classification accuracy and Enclosed Area Error.

## Method Summary
The method extracts local features using MeTRAbs for pose estimation and YOLOv5 with Deep Sort for person detection, combined with global features from a pre-trained VGG16 network with Gaussian weighting. These features are concatenated into 166-dimensional vectors per frame, normalized, and aligned using DDTW, which adds a penalty to the alignment cost when the warping path deviates beyond a margin from the diagonal. The approach requires no training and can be used for framewise labeling of action phases in datasets with only a few labeled videos.

## Key Results
- Achieves state-of-the-art results on Penn Action dataset with 79.15% correct phase rate
- Outperforms TCC and other self-supervised methods in phase classification accuracy
- Demonstrates effectiveness of combining local and global features for video alignment
- Introduces Enclosed Area Error (EAE) as a sensitive metric for alignment quality evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining local (pose and box) and global (VGG) features creates a richer time series representation that improves alignment robustness across varying execution speeds and camera angles.
- Mechanism: Local features capture detailed subject motion while global features capture scene-level context, allowing alignment to leverage both subject-specific dynamics and environmental consistency.
- Core assumption: Actions with the same semantic content share similar feature trajectories over time despite differences in appearance and execution speed.
- Evidence anchors: Abstract states the combination of three machine vision tools creates effective features; section confirms the combination provides effective representation for accurate alignment.
- Break condition: If actions involve multiple subjects or unpredictable background changes, global features may introduce noise that overwhelms local motion cues.

### Mechanism 2
- Claim: DDTW enforces alignment paths to stay close to the diagonal, improving robustness to execution speed variations while avoiding excessive warping.
- Mechanism: DDTW adds a penalty when the warping path deviates beyond a margin from the diagonal, reflecting the assumption that similar actions progress roughly linearly in time across different executions.
- Core assumption: Temporal progression of similar actions is roughly linear across different executions, so large deviations from diagonal indicate misalignment.
- Evidence anchors: Section states the main advantage is no training required and approach can be used for framewise labeling; section confirms DTW path is close to diagonal.
- Break condition: If actions contain multiple overlapping phases or non-monotonic motion, the diagonal constraint may force incorrect alignments.

### Mechanism 3
- Claim: EAE metric provides more sensitive evaluation of alignment quality than phase classification accuracy by measuring geometric deviation from ground truth alignment.
- Mechanism: EAE computes the area between predicted alignment path and ground truth piecewise-linear path, normalized by total table area, capturing both timing errors and phase boundary shifts.
- Core assumption: Ground truth alignment can be approximated as piecewise-linear between known phase boundaries, making area-based error meaningful.
- Evidence anchors: Section defines EAE as area between ground truth and alignment path divided by whole rectangle area; section presents EAE as new metric for video synchronization.
- Break condition: If ground truth alignment is highly non-linear or phase boundaries are ambiguous, EAE may not reflect perceptual alignment quality.

## Foundational Learning

- Concept: Pose estimation and keypoint extraction
  - Why needed here: Provides detailed spatial information about subject's body configuration over time, essential for capturing action dynamics.
  - Quick check question: What is the coordinate transformation applied to key points after extraction, and why is it necessary?

- Concept: Dynamic Time Warping (DTW) and its variants
  - Why needed here: Enables alignment of sequences with different lengths and temporal distortions without requiring training.
  - Quick check question: How does DDTW differ from standard DTW in terms of path constraints and penalty functions?

- Concept: Feature normalization and preprocessing
  - Why needed here: Ensures features from different modalities are on comparable scales and removes position/appearance biases.
  - Quick check question: What normalization steps are applied to time series before alignment, and what is their purpose?

## Architecture Onboarding

- Component map:
  - Input: Video frames
  - Local feature extraction: Pose estimation (MeTRAbs), Person detection (Yolo V5 + Deep Sort)
  - Global feature extraction: VGG16 with Gaussian weighting
  - Feature processing: Interpolation, normalization, concatenation
  - Alignment: DDTW algorithm
  - Evaluation: EAE metric, phase classification accuracy

- Critical path:
  1. Extract local features (pose, box) from each frame
  2. Extract global features using VGG
  3. Concatenate and normalize features into time series
  4. Apply DDTW to align two time series
  5. Evaluate alignment using EAE and phase classification

- Design tradeoffs:
  - Local vs. global feature weighting: More local features improve subject motion modeling but may lose environmental context
  - DDTW margin and penalty coefficient: Tighter margins reduce alignment flexibility but improve robustness to timing variations
  - Feature dimensionality: Higher dimensions capture more detail but increase computational cost and noise sensitivity

- Failure signatures:
  - High EAE but good phase classification: Alignment is correct in phase order but timing is off
  - Low EAE but poor phase classification: Alignment follows ground truth path but misses phase transitions
  - Unstable pose/box detection: Missing or noisy local features degrade alignment quality

- First 3 experiments:
  1. Baseline alignment using only VGG features (no local features) to quantify local feature contribution
  2. Vary DDTW margin and penalty coefficient on validation set to find optimal parameters
  3. Test alignment on synthetic videos with controlled speed variations to evaluate robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DDTW perform on videos with significant variations in speed or non-linear execution of actions compared to conventional DTW?
- Basis in paper: The paper introduces DDTW with penalty for paths deviating from diagonal based on assumption that similar actions are linearly corresponding, but doesn't explore how DDTW handles non-linear execution or significant speed variations.
- Why unresolved: Evaluation focuses on datasets where videos are trimmed and have relatively consistent speed, not testing DDTW's robustness to non-linear execution.
- What evidence would resolve it: Experiments on datasets with non-linear execution, significant speed variations, or complex action sequences would demonstrate DDTW's effectiveness or limitations in these scenarios.

### Open Question 2
- Question: What is the impact of different feature extraction algorithms on video alignment performance, and how sensitive is the method to quality of these algorithms?
- Basis in paper: The paper uses specific algorithms (MeTRAbs for pose estimation, Deep Sort with YOLOv5 for person detection) but doesn't explore impact of using different algorithms or sensitivity to their quality.
- Why unresolved: Experiments use fixed algorithms without comparing alternatives or testing robustness to algorithm quality variations.
- What evidence would resolve it: Comparative experiments using different feature extraction algorithms and testing with varying quality of these algorithms would show their impact on alignment performance.

### Open Question 3
- Question: How does the proposed method scale with number of videos and actions, and what are computational requirements for large-scale applications?
- Basis in paper: While the paper demonstrates effectiveness on specific dataset, it doesn't address scalability to larger datasets or multiple action types, nor does it discuss computational requirements.
- Why unresolved: Experiments are limited to single dataset with specific number of actions, not exploring scalability or computational efficiency.
- What evidence would resolve it: Experiments on larger datasets with more actions, along with analysis of computational requirements and runtime, would demonstrate method's scalability and efficiency.

## Limitations
- Relies heavily on accurate pose estimation and person detection, which may fail in crowded scenes or with partial occlusions
- Diagonal constraint in DDTW assumes roughly linear temporal progression, which may not hold for actions with repetitive or non-monotonic motion patterns
- Choice of 166-dimensional feature vectors appears arbitrary and may not be optimal for all action types

## Confidence
- High confidence: Core concept of combining local and global features for video alignment is well-supported by empirical results
- Medium confidence: Effectiveness of DDTW compared to standard DTW is demonstrated but could benefit from more extensive ablation studies
- Low confidence: Generalization capability to datasets beyond Penn Action and UCF101 has not been thoroughly tested

## Next Checks
1. Test the method on videos with multiple subjects performing actions to evaluate robustness to complex scenes
2. Conduct ablation studies to determine the optimal feature combination and dimensionality
3. Evaluate alignment quality using alternative metrics beyond EAE and phase classification to verify results consistency