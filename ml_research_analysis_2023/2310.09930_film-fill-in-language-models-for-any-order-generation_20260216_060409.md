---
ver: rpa2
title: 'FiLM: Fill-in Language Models for Any-Order Generation'
arxiv_id: '2310.09930'
source_url: https://arxiv.org/abs/2310.09930
tags:
- film
- language
- option
- arxiv
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FiLM is a fill-in language model designed for flexible sequence
  generation in any order. It extends masked language modeling by adopting varying
  mask probabilities from a Beta distribution, enabling bidirectional context use
  and improved generative capabilities.
---

# FiLM: Fill-in Language Models for Any-Order Generation

## Quick Facts
- arXiv ID: 2310.09930
- Source URL: https://arxiv.org/abs/2310.09930
- Reference count: 32
- Primary result: FiLM outperforms previous infilling methods on text completion and story completion tasks, achieving higher ROUGE scores and stronger human and GPT-4 evaluations.

## Executive Summary
FiLM (Fill-in Language Model) introduces a novel approach to flexible sequence generation that allows insertion of missing text at any position while maintaining fluency and coherence. By extending masked language modeling with variable mask probabilities sampled from a Beta distribution, FiLM learns to handle sequences with different numbers of masks, avoiding the oversimplification of fixed mask ratios. The model can be trained from scratch or fine-tuned from existing language models and demonstrates superior performance on text infilling tasks compared to previous methods, with perplexity approaching that of strong left-to-right language models as model size increases.

## Method Summary
FiLM extends masked language modeling by adopting varying mask probabilities from a Beta distribution (Beta(2.5, 2.5)) to enhance generative capabilities. During training, tokens are independently masked with probability p sampled from this distribution, creating sequences with varying numbers of masks. The model uses a decoder-only transformer architecture similar to GPT models with special [MASK] tokens indicating positions to be filled. At inference, FiLM can insert missing text at any position using a left-to-right decoding strategy with top-p sampling. The approach enables bidirectional context use during both training and inference, allowing the model to leverage surrounding context when generating masked tokens.

## Key Results
- FiLM outperforms previous infilling methods on text completion and story completion tasks, achieving higher ROUGE scores and stronger human and GPT-4 evaluations.
- As model size increases, FiLM's perplexity approaches that of strong left-to-right language models of similar sizes, narrowing the gap from 5.85 to 2.74 on WikiText-103 when scaling from 124M to 1558M parameters.
- Left-to-right decoding strategy performs consistently well despite FiLM's any-order training objective, providing a simple and effective generation approach.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Varying mask probabilities from a Beta distribution improves generative capacity compared to fixed mask ratios.
- Mechanism: By sampling mask probability p from Beta(2.5, 2.5) during training, FiLM learns to handle sequences with different numbers of masks, avoiding both the oversimplification of low mask ratios and the information scarcity of high mask ratios.
- Core assumption: A Beta distribution with mode around 0.5 provides an optimal balance between information availability and task difficulty during training.
- Evidence anchors:
  - [abstract] "FiLM adopts a strategy inspired by diffusion models that utilize varying noise levels"
  - [section 4.1] "Table 1 shows that a fixed noise schedule δ(0.15) leads to substantially higher perplexity... highlighting the necessity for a variable mask probability"
  - [section 3.1] "Different values of α, β allow for skewing towards different values of mask probabilities and thereby avoiding extreme values"

### Mechanism 2
- Claim: Left-to-right decoding order performs consistently well despite FiLM's any-order training objective.
- Mechanism: Even though FiLM is trained on randomly masked sequences, the sequential nature of language means that left-to-right decoding provides a natural and effective generation order that aligns with human language processing.
- Core assumption: The sequential dependency structure inherent in natural language makes left-to-right generation a robust default strategy.
- Evidence anchors:
  - [section 4.1] "While FiLM is trained to predict a random subset of words, decoding from left to right substantially outperforms decoding in a random order"
  - [section 3.2] "This mirrors a CLM, but with the additional conditioning on the subsequent context"
  - [section 4.1] "Given the simplicity and superior performance of left-to-right decoding... we select this order for FiLM in subsequent experiments"

### Mechanism 3
- Claim: Scaling FiLM to larger model sizes reduces the performance gap with traditional left-to-right language models.
- Mechanism: As model capacity increases, FiLM's ability to leverage bidirectional context during both training and inference becomes more effective, allowing it to approach the performance of autoregressive models while maintaining its flexible generation capabilities.
- Core assumption: Larger models can better capture complex dependencies in bidirectional contexts, compensating for the non-autoregressive training objective.
- Evidence anchors:
  - [abstract] "as the model size grows, FiLM's perplexity approaches that of strong left-to-right language models of similar sizes"
  - [section 4.2] "As the pretrained model scales from 124M GPT2-small to 1558M GPT2-xl, the difference in perplexity decreases from 5.85 to 2.74 on WikiText-103"
  - [section 4.2] "This narrowing gap suggests that FiLM could benefit from further scaling and holds considerable potential as an alternative LLM"

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: FiLM builds upon MLM by introducing variable masking probabilities, so understanding fixed-ratio MLM is essential for grasping FiLM's innovations.
  - Quick check question: What is the primary difference between how BERT and FiLM mask tokens during training?

- Concept: Beta distribution properties
  - Why needed here: The Beta distribution is central to FiLM's noise schedule, and understanding its shape parameters and modes is crucial for tuning the model.
  - Quick check question: How do the α and β parameters of a Beta distribution affect the mode of the distribution?

- Concept: Perplexity calculation for non-autoregressive models
  - Why needed here: FiLM requires a specialized perplexity evaluation method that accounts for its any-order generation capability.
  - Quick check question: Why does FiLM's perplexity calculation divide by n+1 instead of n?

## Architecture Onboarding

- Component map: [MASK] token insertion -> Beta distribution sampling -> Token masking -> Bidirectional context prediction -> Left-to-right decoding -> Token generation
- Critical path: Training → Noise schedule sampling → Token masking → Bidirectional context prediction → Decoding → Position selection → Token generation
- Design tradeoffs: FiLM sacrifices some generation efficiency (compared to purely autoregressive models) for flexibility in generation order and improved use of bidirectional context.
- Failure signatures: High perplexity on language modeling tasks, repetitive or incoherent text in infilling tasks, or failure to generate text from scratch indicate issues with the noise schedule or decoding strategy.
- First 3 experiments:
  1. Train FiLM with different Beta distribution parameters (e.g., Beta(1,1), Beta(2,2), Beta(3,2)) on a small dataset and compare perplexity.
  2. Compare different decoding strategies (random, left-to-right, min-entropy) on a held-out validation set.
  3. Fine-tune FiLM from an existing MLM (e.g., RoBERTa) versus training from scratch to evaluate the impact of bidirectional pretraining.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does FiLM's perplexity performance scale proportionally with model size compared to CLMs?
- Basis in paper: [explicit] The paper observes that as pretrained models scale from 124M GPT2-small to 1558M GPT2-xl, the difference in perplexity decreases from 5.85 to 2.74 on WikiText-103 and from 7.96 to 3.86 on One Billion Word.
- Why unresolved: The analysis only covers models up to 1.6B parameters. The trend of diminishing perplexity gap with scaling needs verification on much larger models (e.g., 10B+ parameters) to determine if FiLM can match CLM performance at scale.
- What evidence would resolve it: Experimental results showing perplexity comparisons between FiLM and CLM on models larger than 1.6B parameters, particularly demonstrating whether the perplexity gap continues to narrow or plateaus.

### Open Question 2
- Question: How does FiLM's performance vary when trained as a primary pretraining objective rather than fine-tuned from existing models?
- Basis in paper: [inferred] The paper notes that FiLM fine-tuned from RoBERTa (bidirectional) performs better than from GPT2 (unidirectional), and speculates that using FiLM directly as the pretraining objective might yield further improvements.
- Why unresolved: All experiments in the paper use fine-tuning from existing models. The paper does not report results from training FiLM from scratch as the primary objective, leaving open the question of whether this approach could achieve superior performance.
- What evidence would resolve it: Direct comparison of FiLM trained from scratch as the primary pretraining objective versus FiLM fine-tuned from RoBERTa and GPT2 on the same datasets, measuring both perplexity and downstream task performance.

### Open Question 3
- Question: What is the optimal decoding strategy for FiLM when generating longer sequences or in constrained generation tasks?
- Basis in paper: [explicit] The paper evaluates several decoding strategies (random, left-to-right, right-to-left, min-entropy, max-entropy) and finds left-to-right and min-entropy perform best, but notes that exploring strategies to accelerate generation is a promising direction.
- Why unresolved: The paper only evaluates decoding strategies on relatively short sequences and does not explore their effectiveness on longer generations or constrained tasks where certain tokens must appear at specific positions.
- What evidence would resolve it: Systematic evaluation of FiLM's decoding strategies on long-form generation tasks and constrained generation scenarios, measuring both generation quality (through automated metrics and human evaluation) and computational efficiency.

## Limitations

- The evaluation relies heavily on automatic metrics like ROUGE and GPT-4 judgments, which may not fully capture human preferences for fluency and coherence.
- The Beta distribution parameters (2.5, 2.5) are presented as optimal but lack systematic sensitivity analysis across different datasets or model scales.
- The comparison with other infilling methods is limited, as many contemporary approaches are not included in the evaluation.

## Confidence

**High Confidence**: The architectural description and training procedure for FiLM are clearly specified and technically sound. The experimental methodology for measuring perplexity and infilling performance is well-defined, and the results showing improved performance over fixed-ratio masking are reproducible.

**Medium Confidence**: The claims about left-to-right decoding being the optimal strategy are supported by ablation studies, but the paper doesn't explore whether task-specific decoding orders might yield better results for certain applications. The scaling analysis shows trends but doesn't establish asymptotic behavior conclusively.

**Low Confidence**: The GPT-4 evaluation methodology lacks transparency in prompt design and evaluation criteria. The human evaluation sample size (30 examples) is relatively small for drawing robust conclusions about qualitative performance differences.

## Next Checks

1. **Noise Schedule Sensitivity**: Conduct a systematic ablation study testing different Beta distribution parameters (e.g., Beta(1,1), Beta(2,2), Beta(3,2)) across multiple model scales and datasets to determine whether Beta(2.5, 2.5) is universally optimal or task-dependent.

2. **Task-Specific Decoding Strategies**: Evaluate FiLM's performance using alternative decoding orders (e.g., minimum entropy first, right-to-left, or random) on the ROCStories completion task to determine if left-to-right is truly optimal across all use cases or if task-specific strategies could yield improvements.

3. **Cross-Dataset Generalization**: Train FiLM on one dataset (e.g., WikiText-103) and evaluate its infilling performance on out-of-distribution data (e.g., One Billion Word or ROCStories) to assess whether the variable masking approach generalizes across different domains and writing styles.