---
ver: rpa2
title: 'FOCUS: Effective Embedding Initialization for Monolingual Specialization of
  Multilingual Models'
arxiv_id: '2305.14481'
source_url: https://arxiv.org/abs/2305.14481
tags:
- language
- tokens
- focus
- vocabulary
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes FOCUS, a method to initialize embeddings when
  specializing multilingual models like XLM-R to a single language by replacing or
  extending its vocabulary. FOCUS represents new tokens as weighted combinations of
  semantically similar overlapping tokens from the original and target vocabularies,
  using sparsemax to select and weight them based on cosine similarity in a static
  embedding space.
---

# FOCUS: Effective Embedding Initialization for Monolingual Specialization of Multilingual Models

## Quick Facts
- arXiv ID: 2305.14481
- Source URL: https://arxiv.org/abs/2305.14481
- Reference count: 22
- Primary result: FOCUS initialization using weighted combinations of overlapping tokens improves MLM loss and downstream task performance compared to random initialization and WECHSEL

## Executive Summary
FOCUS addresses the challenge of specializing multilingual models like XLM-R to a single language by providing an effective method for initializing embeddings when replacing or extending the vocabulary. The method represents new tokens as weighted combinations of semantically similar overlapping tokens from the pretrained vocabulary, using sparsemax to select and weight them based on cosine similarity in a static embedding space. Experiments demonstrate that FOCUS outperforms random initialization and WECHSEL on MLM loss and downstream tasks across German, Arabic, and Swahili, with particular advantages when replacing the full vocabulary.

## Method Summary
FOCUS initializes embeddings for a specialized monolingual model by first computing vocabulary overlap between the target language and the pretrained multilingual model. For overlapping tokens, it directly copies their embeddings. For new tokens, it computes cosine similarities between their static embeddings (trained on target language data) and the overlapping tokens' static embeddings, then applies sparsemax to obtain weights for a weighted combination of overlapping token embeddings. This approach requires neither bilingual dictionaries nor embedding space alignment, making it computationally efficient while leveraging semantic similarity between overlapping tokens to provide meaningful initialization for new tokens.

## Key Results
- FOCUS outperforms random initialization and WECHSEL on MLM loss during LAPT
- FOCUS shows superior downstream task performance (NLI, QA, NER) compared to baselines
- Vocabulary replacement with FOCUS achieves 40% faster training and over 55% reduction in parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FOCUS outperforms random initialization by representing new tokens as weighted combinations of semantically similar overlapping tokens from the pretrained vocabulary.
- Mechanism: FOCUS uses sparsemax over cosine similarities between static embeddings of target tokens and overlapping tokens to select and weight the most relevant pretrained tokens, then combines their pretrained embeddings to initialize the new token.
- Core assumption: Overlapping tokens share similar semantics between the target language and the pretrained vocabulary, allowing their embeddings to serve as meaningful initialization for new tokens.
- Evidence anchors:
  - [abstract] "FOCUS represents newly added tokens as weighted combinations of tokens in the overlap of the pretrained and new vocabularies."
  - [section 3] "FOCUS represents newly added tokens as combinations of semantically similar shared tokens."
  - [corpus] Found 25 related papers; average neighbor FMR=0.438 suggests moderate relevance of related work but no direct replication of FOCUS mechanism.
- Break condition: If the overlap between vocabularies is too small or contains tokens with divergent semantics across languages, the initialization quality degrades.

### Mechanism 2
- Claim: FOCUS initialization reduces training time and improves downstream task performance compared to vocabulary extension approaches.
- Mechanism: By fully replacing the large multilingual vocabulary with a smaller language-specific vocabulary, FOCUS enables faster training (40% faster in experiments) and smaller models (over 55% reduction in parameters).
- Core assumption: A smaller, language-specific vocabulary is sufficient for the target language and eliminates the need to process tokens irrelevant to that language.
- Evidence anchors:
  - [abstract] "replacing this with a language-specific 50k token vocabulary reduces the model size quite dramatically, by over 55%."
  - [section 1] "training with a language-specific 50k token vocabulary is 40% faster than extending the original 250k token vocabulary."
  - [corpus] Found 25 related papers; average neighbor FMR=0.438 suggests moderate relevance of related work but no direct replication of vocabulary replacement benefits.
- Break condition: If the language requires a larger vocabulary than anticipated, performance may suffer due to vocabulary size constraints.

### Mechanism 3
- Claim: FOCUS leverages existing overlapping tokens to avoid the need for bilingual dictionaries or embedding space alignment.
- Mechanism: FOCUS directly copies embeddings for overlapping tokens and only needs to compute similarities between static embeddings of new tokens and overlapping tokens, bypassing cross-lingual alignment steps required by methods like WECHSEL.
- Core assumption: The pretrained model's embeddings for overlapping tokens already encode target language semantics sufficiently well for direct copying.
- Evidence anchors:
  - [abstract] "FOCUS represents newly added tokens as combinations of tokens in the overlap of the pretrained and new vocabularies."
  - [section 3] "This requires neither bilingual dictionaries nor the alignment of embedding spaces from different languages."
  - [corpus] Found 25 related papers; average neighbor FMR=0.438 suggests moderate relevance of related work but no direct replication of bypassing bilingual dictionary requirement.
- Break condition: If overlapping tokens have significantly different semantics across languages, direct copying and combination will lead to poor initialization.

## Foundational Learning

- Concept: Static embeddings (fastText) trained on tokenized target language data
  - Why needed here: FOCUS uses these static embeddings to compute cosine similarities between tokens for the sparsemax weighting step.
  - Quick check question: What would happen if we used randomly initialized embeddings instead of trained static embeddings for the similarity computation?

- Concept: Sparsemax activation function
  - Why needed here: Sparsemax provides sparse probability distributions over similar tokens, allowing dynamic selection of the most relevant overlapping tokens without assuming a fixed number of neighbors.
  - Quick check question: How does sparsemax differ from softmax in handling cases where only one or two tokens are highly similar versus many moderately similar tokens?

- Concept: Vocabulary overlap analysis
  - Why needed here: Understanding the composition of overlapping tokens (symbols, names, code-switched words, language-specific words) is crucial for assessing how well FOCUS can leverage them.
  - Quick check question: Based on the German overlap analysis showing 46% German sub-words, 18% English/code-switched, and 10% names/entities, what types of tokens would be most reliable for initialization?

## Architecture Onboarding

- Component map:
  - Monolingual corpus -> Tokenizer (SentencePiece Unigram) -> Static embeddings (fastText) -> Similarity computation (cosine) -> Sparsemax weighting -> New token initialization -> XLM-R model

- Critical path:
  1. Train language-specific tokenizer on monolingual corpus
  2. Compute vocabulary overlap with pretrained model
  3. Train fastText static embeddings on tokenized target data
  4. For each new token, compute cosine similarities to overlapping tokens
  5. Apply sparsemax to get weights
  6. Initialize new token embeddings as weighted sum of overlapping token embeddings
  7. Continue with LAPT using MLM objective

- Design tradeoffs:
  - Vocabulary size: 50k vs larger (better coverage vs smaller model and faster training)
  - Character coverage: 100% vs tuned (more single characters in vocabulary vs better token quality)
  - Fuzzy matching: exact vs case-insensitive (more overlaps vs potential semantic mismatches)
  - Static embedding quality: more training epochs vs computational cost

- Failure signatures:
  - Poor downstream task performance despite good MLM loss: likely vocabulary overlap contains too many semantically divergent tokens
  - Slow convergence: initialization weights may be poorly calibrated
  - High variance across runs: sensitive to random seed due to limited effective overlap

- First 3 experiments:
  1. Run FOCUS initialization on German vocabulary, compare MLM loss curves with random initialization and WECHSEL
  2. Test vocabulary replacement vs extension for Arabic, measure training speed and downstream task performance
  3. Vary the number of overlapping tokens used in sparsemax weighting (k parameter) to find optimal balance for Swahili

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FOCUS perform when initializing embeddings for languages with even fewer resources than Swahili, where the overlap between target and source vocabularies is minimal?
- Basis in paper: [explicit] The paper discusses limitations regarding low-resource languages and the availability of monolingual data for further pretraining.
- Why unresolved: The experiments were conducted on German, Arabic, and Swahili, which have varying levels of resources. The paper acknowledges the need for further evaluation on languages with fewer resources.
- What evidence would resolve it: Experiments on languages with fewer resources than Swahili, showing performance metrics (MLM loss, downstream task results) to compare with the current results.

### Open Question 2
- Question: Would using a different auxiliary embedding space, such as the embedding layer of a pretrained Transformer model instead of static embeddings, improve FOCUS's initialization quality?
- Basis in paper: [inferred] The paper mentions concurrent work by Ostendorff and Rehm (2023) that uses the embedding layer of a pretrained Transformer model as an auxiliary embedding space, suggesting this as a potential improvement.
- Why unresolved: The paper uses static embeddings (fasttext) for FOCUS and does not compare it with using the embedding layer of a pretrained Transformer model.
- What evidence would resolve it: Comparative experiments using both static embeddings and the embedding layer of a pretrained Transformer model as auxiliary spaces, measuring initialization quality and downstream task performance.

### Open Question 3
- Question: How does FOCUS perform when applied to GPT decoder models instead of BERT-like Transformer models?
- Basis in paper: [explicit] The paper states that FOCUS is currently evaluated only for BERT-like Transformer models and mentions the need for future work to investigate its use on GPT decoder models.
- Why unresolved: The experiments were conducted exclusively on BERT-like models, and the paper explicitly notes this as a limitation.
- What evidence would resolve it: Experiments applying FOCUS to GPT decoder models, comparing performance metrics (MLM loss, downstream task results) with those obtained using BERT-like models.

## Limitations
- Limited experimental scope to only three languages (German, Arabic, Swahili) constrains generalizability
- Assumes 50k token vocabulary is sufficient for all target languages, which may not hold for morphologically rich languages
- Does not address potential catastrophic forgetting when replacing the entire vocabulary
- Relies on static embeddings that may not capture context-dependent semantics

## Confidence
- **High Confidence** in claims about training speed improvements (40% faster) and parameter reduction (55% smaller models) from vocabulary replacement
- **Medium Confidence** in the superiority of FOCUS initialization over random initialization and WECHSEL for downstream task performance
- **Low Confidence** in the broader claim that FOCUS can be applied to any language without modification

## Next Checks
1. **Cross-linguistic validation**: Test FOCUS on a diverse set of 5-10 additional languages spanning different language families (e.g., Japanese, Turkish, Finnish, Hebrew, Hindi) to assess robustness across varying morphological complexity and script systems.

2. **Multilingual capability assessment**: After vocabulary replacement with FOCUS initialization, evaluate the model's ability to retain some multilingual understanding by testing cross-lingual transfer performance on tasks like XNLI or MLQA, measuring degradation compared to the original XLM-R.

3. **Static vs contextual embedding comparison**: Replace the fastText static embeddings with contextualized embeddings from a smaller language model (e.g., mBERT or language-specific model) for the similarity computation step, and measure whether this improves initialization quality for languages with significant semantic divergence in overlapping tokens.