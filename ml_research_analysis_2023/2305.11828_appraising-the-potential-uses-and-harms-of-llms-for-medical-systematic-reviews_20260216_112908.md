---
ver: rpa2
title: Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews
arxiv_id: '2305.11828'
source_url: https://arxiv.org/abs/2305.11828
tags:
- llms
- reviews
- systematic
- medical
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This qualitative study explores expert perspectives on the potential
  uses and harms of large language models (LLMs) for medical systematic reviews. Through
  interviews with 16 international systematic review experts, the research identified
  several potential uses for LLMs, including drafting first versions, generating templates,
  creating plain language summaries, distilling information, and crosschecking human-written
  content.
---

# Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews

## Quick Facts
- arXiv ID: 2305.11828
- Source URL: https://arxiv.org/abs/2305.11828
- Reference count: 40
- Key outcome: Experts see potential for LLMs in drafting and summarizing but express major concerns about accuracy, transparency, and quality control

## Executive Summary
This qualitative study examines expert perspectives on using large language models for medical systematic reviews through interviews with 16 international experts. While experts identified several potential applications including drafting first versions, generating templates, creating plain language summaries, and crosschecking human-written content, they expressed significant concerns about confidently composed but inaccurate outputs, fabricated references, and lack of comprehensiveness. The study concludes that while LLMs may assist certain aspects of systematic review production, human oversight and verification remain critical for ensuring accuracy and reliability.

## Method Summary
The study conducted 16 semi-structured interviews with international systematic review experts including methodologists, practitioners, journal editors, clinicians, and guideline developers. Researchers generated 128 LLM outputs using titles from recently published Cochrane reviews, employing multiple models (Galactica, BioMedLM, ChatGPT) and prompting strategies. The qualitative analysis identified themes around potential uses, concerns, and downstream harms of LLMs in systematic review writing.

## Key Results
- Experts identified potential LLM uses including drafting first versions, generating templates, creating plain language summaries, and crosschecking human-written content
- Major concerns included confidently composed but inaccurate outputs, fabricated references, lack of comprehensiveness, and potential downstream harms like decreased accountability
- Human oversight and verification were deemed critical for ensuring accuracy and reliability of LLM-assisted systematic reviews

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain expert validation reduces perceived risk of LLM-generated systematic reviews
- Mechanism: Experts trust outputs more when they can verify provenance, check for fabricated references, and crosscheck conclusions against known evidence
- Core assumption: Transparency and verifiability directly influence trust in LLM outputs
- Evidence anchors:
  - [abstract] "Informed by this qualitative analysis, we identify criteria for rigorous evaluation of biomedical LLMs aligned with domain expert views."
  - [section] "Participants emphasized the importance of transparency... having references... and knowing that the outputs are genuinely derived from these would permit one to verify outputs..."
  - [corpus] Weak signal: related papers focus on transparency and trust in AI-assisted reviews, but no direct citation match
- Break condition: If verification is too time-consuming relative to manual writing, efficiency gains are lost

### Mechanism 2
- Claim: LLMs can assist systematic review production when used as drafting scaffolds rather than final products
- Mechanism: Experts view LLMs as tools for generating first drafts, templates, or plain language summaries, but not as autonomous reviewers
- Core assumption: LLMs lack the methodological rigor required for systematic reviews but can reduce initial writing burden
- Evidence anchors:
  - [abstract] "LLMs can assist in the writing process by drafting summaries, generating templates, distilling information, and crosschecking information."
  - [section] "All participants found at least some of the samples of the LLM outputs to be well-written and plausible as 'real' systematic reviews... About half of the participants expressed that LLMs would be useful for writing the first draft of a review."
  - [corpus] Related paper: "AiReview: An Open Platform for Accelerating Systematic Reviews with LLMs" - focuses on LLMs as assistive tools
- Break condition: If users treat LLM outputs as final without verification, risk of misinformation increases

### Mechanism 3
- Claim: Unknown provenance and hallucination are the primary barriers to LLM adoption in medical systematic reviews
- Mechanism: Experts distrust outputs due to inability to trace references, fabricated statistics, and lack of risk of bias assessment
- Core assumption: Medical systematic reviews require verifiable, traceable evidence to be credible
- Evidence anchors:
  - [abstract] "experts also raised concerns about confidently composed but inaccurate outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews."
  - [section] "One of the most important reasons why participants described the LLM-generated outputs to be difficult to trust was not knowing where the studies included in the summaries came from (unknown provenance)."
  - [corpus] Related paper: "Beyond the Benefits: A Systematic Review of the Harms and Consequences of Generative AI in Computing Education" - discusses similar trust issues
- Break condition: If LLMs can reliably cite verifiable sources and assess risk of bias, adoption barriers decrease

## Foundational Learning

- Concept: Evidence-based medicine and systematic review methodology
  - Why needed here: Understanding why LLMs fall short requires knowing what systematic reviews require (comprehensive search, risk of bias assessment, transparent methodology)
  - Quick check question: What are the key methodological steps in a systematic review that LLMs currently cannot replicate?

- Concept: Large language model limitations (hallucination, lack of true comprehension)
  - Why needed here: Explains why experts are skeptical of LLM outputs despite their fluency
  - Quick check question: How does the "black box" nature of LLMs contribute to the unknown provenance problem?

- Concept: Human-in-the-loop validation processes
  - Why needed here: Critical for designing systems where LLMs assist rather than replace human experts
  - Quick check question: What verification steps would make experts comfortable using LLM-generated drafts?

## Architecture Onboarding

- Component map:
  - LLM generation module → Verification interface → Reference checking system → Risk of bias assessment tool → User feedback loop
- Critical path:
  - User prompt → LLM generates draft → System highlights unverifiable claims → Expert reviews flagged items → System learns from corrections
- Design tradeoffs:
  - Speed vs. verification depth: faster drafts require more rigorous checking
  - Autonomy vs. control: more LLM autonomy increases efficiency but requires stronger safeguards
  - Transparency vs. usability: detailed provenance information may overwhelm users
- Failure signatures:
  - High rate of unverifiable claims passing through → System needs better detection
  - Users skipping verification steps → Interface needs to enforce workflow
  - Experts spending more time verifying than writing manually → Efficiency gains not realized
- First 3 experiments:
  1. Compare expert verification time for LLM drafts vs. manual writing
  2. Test different levels of LLM autonomy with varying verification requirements
  3. Evaluate user trust in drafts with different transparency levels (full references vs. summary only)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can large language models be effectively evaluated for accuracy, transparency, comprehensiveness of included studies, readability & clear structure, and providing details important to evidence summaries for medical systematic reviews?
- Basis in paper: explicit
- Why unresolved: The paper identifies these evaluation criteria as important based on expert interviews, but does not provide specific methodologies or benchmarks for how to actually implement such evaluations
- What evidence would resolve it: Development of standardized evaluation frameworks, datasets, and metrics specifically designed to assess LLM performance on systematic review generation tasks across these identified dimensions

### Open Question 2
- Question: What are the optimal prompting strategies for generating high-quality systematic review drafts from large language models?
- Basis in paper: explicit
- Why unresolved: The paper mentions that different prompts can lead to substantially different results, but does not explore or recommend specific prompting techniques optimized for systematic review generation
- What evidence would resolve it: Systematic experimentation comparing various prompt engineering approaches, including few-shot examples, chain-of-thought prompting, and task-specific instructions, to identify prompting strategies that consistently produce accurate and comprehensive review outputs

### Open Question 3
- Question: How can the trade-off between efficiency gains from using large language models and the time required for human verification be optimized in systematic review production?
- Basis in paper: explicit
- Why unresolved: While the paper discusses the importance of human verification, it does not quantify the relationship between LLM assistance and verification burden, nor propose methods to minimize this verification overhead
- What evidence would resolve it: Empirical studies measuring time savings and verification effort across different LLM-assisted workflows, coupled with development of verification tools that automate parts of the quality assurance process

## Limitations
- Qualitative study based on 16 expert interviews, limiting generalizability despite diverse recruitment
- Relies on subjective expert assessments without independent verification of LLM output accuracy claims
- Does not empirically test proposed solutions (transparency features, reference checking) for addressing expert concerns

## Confidence

- Uses and benefits of LLMs: Medium confidence
- Harms and concerns: High confidence
- Proposed evaluation criteria: Medium confidence

## Next Checks

1. Conduct a controlled study comparing accuracy and efficiency of LLM-assisted vs. manual systematic review production with objective quality metrics
2. Test expert trust levels with LLM outputs that include different levels of provenance transparency (full citations, partial references, no references)
3. Survey a larger, more diverse sample of systematic reviewers to assess generalizability of the identified uses and harms across different review contexts and experience levels