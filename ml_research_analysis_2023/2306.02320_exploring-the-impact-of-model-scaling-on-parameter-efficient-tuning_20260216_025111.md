---
ver: rpa2
title: Exploring the Impact of Model Scaling on Parameter-Efficient Tuning
arxiv_id: '2306.02320'
source_url: https://arxiv.org/abs/2306.02320
tags:
- methods
- trainable
- parameters
- performance
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how model scaling impacts the performance
  of parameter-efficient tuning (PET) methods. The authors introduce a flexible PET
  method called Arbitrary PET (APET) that allows for arbitrary module structures and
  any number of trainable parameters.
---

# Exploring the Impact of Model Scaling on Parameter-Efficient Tuning

## Quick Facts
- arXiv ID: 2306.02320
- Source URL: https://arxiv.org/abs/2306.02320
- Reference count: 40
- Key outcome: Model scaling enables parameter-efficient tuning to achieve full-parameter fine-tuning performance with fewer trainable parameters

## Executive Summary
This paper investigates how model scaling impacts the performance of parameter-efficient tuning (PET) methods. The authors introduce APET (Arbitrary PET), a flexible method allowing arbitrary module structures and parameter counts. Experiments across 11 NLP tasks and 3 PLM scales reveal that scaling mitigates performance differences between PET methods and enables tuning with fewer parameters to match full fine-tuning performance. The study finds that all tuning methods require similar numbers of trainable parameters to exceed random guessing performance on the same models, suggesting a shared low-dimensional adaptation subspace.

## Method Summary
The authors introduce APET (Arbitrary Parameter-Efficient Tuning), which generalizes existing PET methods by allowing arbitrary module structures and parameter counts. APET generates trainable weights through element-wise multiplication of base weights with binary pruning masks. The method includes two variants: APETADJACENT (parameters distributed in adjacent locations) and APETDISCRETE (parameters distributed discretely). Experiments compare APET against four PET methods (Prompt, BitFit, LoRA, Adapter) across 11 NLP tasks using T5 and BERT model families at different scales, measuring performance, parameter efficiency, and convergence speed.

## Key Results
- Model scaling reduces performance gaps between different PET methods
- Larger models enable tuning methods to achieve full-parameter fine-tuning performance with fewer trainable parameters
- All tuning methods require similar numbers of trainable parameters to exceed random guessing performance on the same models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model scaling increases parameter redundancy, allowing arbitrary selection of trainable parameters without greatly degrading performance
- Mechanism: Larger models have higher parameter redundancy which provides more degrees of freedom in selecting which parameters to train
- Core assumption: Larger models have sufficient parameter redundancy that arbitrary selection of trainable parameters still maintains performance
- Evidence anchors:
  - [abstract]: "we find that when a PLM's scale grows up to tens of billions of parameters, the performance gap among different PET methods shrinks"
  - [section 6]: "As the model scale increases, the larger model has higher parameter redundancy (Aghajanyan et al., 2021), allowing arbitrary selection of trainable parameters for tuning without greatly degrading performance"
  - [corpus]: Weak - no direct citations found in neighbor papers

### Mechanism 2
- Claim: Larger models enable parameter-efficient tuning to achieve full-parameter fine-tuning performance with fewer trainable parameters
- Mechanism: Adaptations on larger PLMs can be re-parameterized into lower dimensional subspaces, reducing the number of parameters needed for effective adaptation
- Core assumption: The dimensionality of the adaptation subspace decreases with model scale
- Evidence anchors:
  - [abstract]: "model scaling enables tuning methods to achieve performance comparable to full-parameter fine-tuning by optimizing fewer tunable parameters"
  - [section 6]: "Aghajanyan et al. (2021) further demonstrate that adaptation on a larger PLM can be re-parameterized into the lower dimensional space"
  - [corpus]: Weak - neighbor papers focus on different aspects of PET but don't directly address this scaling relationship

### Mechanism 3
- Claim: All tuning methods require similar numbers of trainable parameters to exceed random guessing performance on the same models
- Mechanism: Adaptations of tuning methods can be re-parameterized into a unified low-dimensional subspace shared across different tuning methods
- Core assumption: The low-dimensional adaptation subspace is shared across all tuning methods for the same task
- Evidence anchors:
  - [abstract]: "we also observe that all tuning methods require almost the same number of trainable parameters to drive PLMs"
  - [section 6]: "Qin et al. (2021) shows that this low dimensional subspace is shared among all NLP tasks for the same tuning methods. Yi et al. (2022) further suggests that this subspace is also shared among various tuning methods"
  - [corpus]: Weak - no direct citations found in neighbor papers

## Foundational Learning

- Concept: Parameter-efficient tuning (PET) methods
  - Why needed here: Understanding how PET methods work is fundamental to analyzing their scaling behavior
  - Quick check question: What distinguishes PET methods from full fine-tuning in terms of parameter updates?

- Concept: Model scaling effects on representation spaces
  - Why needed here: The paper's core hypothesis relies on how scaling affects the effective dimensionality of model adaptations
  - Quick check question: How does increasing model size affect the dimensionality of adaptation subspaces?

- Concept: Low-rank adaptation and subspace representations
  - Why needed here: Many PET methods use low-rank approximations, and understanding this helps explain scaling effects
  - Quick check question: What is the relationship between low-rank adaptation and the number of trainable parameters needed?

## Architecture Onboarding

- Component map: PLM (BERT/T5 series) -> PET method (Prompt, BitFit, LoRA, Adapter, APET) -> Performance
- Critical path: For understanding scaling effects, the critical path is: Model → PET method → Performance, where scaling affects the middle component's relationship with the others
- Design tradeoffs: The paper trades off between exploring many PET methods versus controlling parameter counts precisely, solved by introducing APET
- Failure signatures: If scaling doesn't reduce performance gaps, or if parameter count thresholds don't follow the observed patterns, the core hypothesis would fail
- First 3 experiments:
  1. Reproduce Figure 3 to observe performance gaps across different model scales
  2. Implement APET and verify it can match existing PET methods with same parameter counts
  3. Test the high and low parameter threshold phenomenon on a new task not in the original paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the module structure of APET methods affect convergence speed, and what specific structural characteristics lead to faster convergence?
- Basis in paper: [explicit] The paper states that "APET ADJACENT method requires fewer training steps than APET DISCRETE method to achieve convergence on T5XXL" and discusses this in the context of model scaling mitigating structural effects on performance but not on convergence speed.
- Why unresolved: While the paper observes differences in convergence speed between APET methods with different structures, it does not provide a detailed analysis of which structural characteristics specifically lead to faster convergence or why these differences persist even with model scaling.
- What evidence would resolve it: Systematic experiments varying different structural aspects of APET modules (e.g., adjacency patterns, parameter distribution, connectivity) and measuring their impact on convergence speed across different model scales would help identify the key structural factors affecting convergence.

### Open Question 2
- Question: What is the theoretical explanation for why all tuning methods require a similar number of trainable parameters to exceed random guess performance on different tasks?
- Basis in paper: [explicit] The paper observes that "all tuning methods require almost the same number of trainable parameters to exceed random guess performance on different tasks" and discusses this from an optimization perspective, mentioning that adaptations can be re-parameterized into a unified low-dimensional subspace shared among tasks and tuning methods.
- Why unresolved: The paper provides an intuitive explanation based on optimization perspectives and shared subspaces, but lacks a rigorous theoretical proof or deeper mechanistic understanding of why this phenomenon occurs across different tasks and tuning methods.
- What evidence would resolve it: A formal mathematical proof demonstrating that the minimal number of trainable parameters required to exceed random performance is determined by the intrinsic dimensionality of the task-specific subspace, independent of the tuning method used.

### Open Question 3
- Question: Does model scaling facilitate zero-shot transfer between different types of NLP tasks, or does it primarily enhance performance within task categories?
- Basis in paper: [explicit] The paper conducts experiments on task transferability in zero-shot settings and finds that APET methods can transfer well within the same task type but struggle with different types of tasks, stating that "the power of scale does not necessarily facilitate the generalization ability of AFP methods."
- Why unresolved: While the paper demonstrates that model scaling does not improve cross-task type transfer, it does not explore potential mechanisms or architectural modifications that could leverage scale to enhance generalization across task types.
- What evidence would resolve it: Experiments testing various transfer learning techniques (e.g., meta-learning, task-adaptive pretraining) with large-scale models to determine if specific approaches can exploit model scale to improve cross-task generalization.

## Limitations
- The core hypothesis about parameter redundancy enabling arbitrary selection relies heavily on theoretical assumptions without direct empirical validation
- Experiments cover only 3 model scales (small, base, large) and 3 PLM families, potentially missing effects at extreme scales
- The shared subspace assumption for mechanism 3 relies on cited works that don't appear in neighbor papers, suggesting weaker connections in broader literature

## Confidence
**High confidence**: The empirical observation that all PET methods require similar parameter counts to exceed random guessing performance (mechanism 3). This finding is directly measured and shows consistent patterns across 11 tasks and 3 model scales.

**Medium confidence**: The claim that model scaling reduces performance gaps between different PET methods (mechanism 1). While the empirical results support this, the underlying mechanism relies on theoretical assumptions about parameter redundancy that aren't fully validated within the paper.

**Low confidence**: The assertion that adaptations can be re-parameterized into lower-dimensional subspaces shared across all tuning methods (mechanism 3). This relies heavily on cited works (Qin et al., 2021; Yi et al., 2022) that don't appear in the neighbor corpus, suggesting this may be a more speculative connection.

## Next Checks
1. **Cross-scale subspace analysis**: Test whether the low-dimensional adaptation subspaces are truly shared across PET methods by performing singular value decomposition on adaptation matrices from different methods on the same task and comparing the principal components.

2. **Extreme scaling validation**: Extend experiments beyond the current three model scales to include models with hundreds of billions of parameters to verify whether the observed trends continue or if there are diminishing returns or breaking points.

3. **Task-type sensitivity analysis**: Investigate whether certain task types (e.g., long-form reasoning vs. classification) show different scaling behaviors or require different parameter counts to exceed random performance, which would challenge the universal applicability of the shared subspace hypothesis.