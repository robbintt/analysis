---
ver: rpa2
title: 'In-Context Probing: Toward Building Robust Classifiers via Probing Large Language
  Models'
arxiv_id: '2305.14171'
source_url: https://arxiv.org/abs/2305.14171
tags:
- in-context
- probing
- learning
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces in-context probing (ICP) as an alternative
  to in-context learning (ICL) for building robust classifiers using large language
  models (LLMs). ICP contextualizes input representations with instructions but uses
  a light-weight probe instead of autoregressive decoding to predict labels.
---

# In-Context Probing: Toward Building Robust Classifiers via Probing Large Language Models

## Quick Facts
- arXiv ID: 2305.14171
- Source URL: https://arxiv.org/abs/2305.14171
- Reference count: 7
- Key outcome: ICP achieves competitive performance with ICL while being more robust to instruction variations, especially for smaller models

## Executive Summary
This paper introduces in-context probing (ICP) as an alternative to in-context learning (ICL) for building robust classifiers using large language models. ICP contextualizes input representations with instructions but uses a light-weight probe instead of autoregressive decoding to predict labels. This approach is more robust to variations in instructions and performs competitively with ICL, especially for smaller models. Experiments on three classification tasks show that ICP achieves comparable or better performance with significantly less sensitivity to instruction changes.

## Method Summary
ICP works by passing instruction-text pairs through a Flan-T5 encoder, extracting last-layer representations, and applying an attention-based probe to generate label predictions. The probe consists of a fixed query vector that computes attention weights over token representations, which are then weighted and fed to a linear classifier. The method is trained on a small set of labeled examples (as few as 40 for small models) using standard cross-entropy loss, with 30% of training data held out for validation and early stopping applied.

## Key Results
- ICP achieves comparable or better F1 scores than ICL on all three tasks (MRPC, WNLI, TweetEval) across model sizes
- ICP shows significantly lower performance variance across instruction variations compared to ICL
- For smaller models (flan-small through flan-large), ICP substantially outperforms ICL, while for flan-xxl the performance gap narrows

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL performance is highly unstable to minor prompt variations
- Mechanism: The decoder-based output generation in ICL is sensitive to surface form competition and calibration issues, causing large variance in predictions even for semantically equivalent instructions
- Core assumption: The language model's autoregressive decoder is not robust to small changes in input phrasing, and this instability propagates to downstream task performance
- Evidence anchors: [abstract] "the effectiveness of in-context learning is dependent to the provided context, and the performance on a downstream task can vary a lot depending on the instruction."

### Mechanism 2
- Claim: ICP is more robust to instruction variations than ICL
- Mechanism: By using a fixed attentional probe on contextualized representations, ICP avoids the instability of autoregressive decoding and directly maps input to label via learned parameters
- Core assumption: The attention-weighted representation captures task-relevant features consistently across instruction variations, and the linear probe is stable enough to generalize
- Evidence anchors: [abstract] "we show that in-context probing is significantly more robust to changes in instructions."

### Mechanism 3
- Claim: ICP achieves competitive or better performance than ICL with fewer training examples
- Mechanism: ICP leverages labeled data to train a probe, allowing direct optimization of the classification objective, whereas ICL relies solely on in-context demonstrations without gradient updates
- Core assumption: A small number of labeled examples suffices to train a probe that generalizes well to the task, and the encoder representations are sufficiently informative
- Evidence anchors: [abstract] "we further show that ICP performs competitive or superior to finetuning and can be particularly helpful to build classifiers on top of smaller models, with less than a hundred training examples."

## Foundational Learning

- Concept: Attention-based probing
  - Why needed here: ICP relies on attention-weighted averaging of encoder representations to produce a fixed-size task vector
  - Quick check question: What happens to the probe output if the attention weights are uniform versus highly peaked?

- Concept: In-context learning limitations
  - Why needed here: Understanding ICL's instability and dependence on demonstrations is key to appreciating ICP's motivation
  - Quick check question: Why does flipping labels in ICL demonstrations rarely affect performance?

- Concept: Encoder-only vs decoder-only architectures
  - Why needed here: ICP works with encoder representations, while ICL uses decoder autoregression; knowing the architectural difference is crucial for implementation
  - Quick check question: Which part of the model generates the final output in ICL versus ICP?

## Architecture Onboarding

- Component map: Tokenized input -> Flan-T5 encoder -> Last-layer representations -> Attention probe -> Linear classifier -> Label distribution
- Critical path:
  1. Tokenize instruction+input
  2. Pass through encoder to get last-layer representations
  3. Compute attention weights using fixed query vector
  4. Weight representations and feed to linear layer
  5. Apply softmax to predict label
- Design tradeoffs:
  - Simpler probe (single attention head) vs richer probe (multiple layers/heads)
  - Fixed query vs learnable query vector
  - Number of training examples vs overfitting risk
- Failure signatures:
  - High variance in performance across instruction variations
  - Probe underfitting (near-random predictions)
  - Encoder representations not capturing task-relevant signal
- First 3 experiments:
  1. Compare ICP F1 variance across 5 instruction variants vs ICL for a single task/model
  2. Measure ICP performance as a function of training sample size (0 to 200) for each model size
  3. Evaluate ICP vs ICL on a held-out test set with different random seeds for training example selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of in-context probing compare to other parameter-efficient finetuning methods like adapters, LoRA, or prefix tuning on low-resource classification tasks?
- Basis in paper: The paper briefly mentions parameter-efficient finetuning methods but doesn't directly compare ICP to them
- Why unresolved: The paper focuses on comparing ICP only to in-context learning and traditional finetuning
- What evidence would resolve it: Direct experimental comparison of ICP against adapters, LoRA, and prefix tuning on the same tasks and model sizes

### Open Question 2
- Question: What is the theoretical explanation for why in-context probing is more robust to instruction variations than in-context learning?
- Basis in paper: The paper observes that "in-context probing is significantly less sensitive to subtle changes in instructions compared to in-context learning" but doesn't provide a theoretical explanation
- Why unresolved: The paper demonstrates the empirical finding but doesn't explore the underlying reasons why probing representations are more stable than decoding outputs
- What evidence would resolve it: Analysis of the learned probe weights, examination of how different instructions affect intermediate representations, or theoretical analysis of the decoding vs. probing mechanisms

### Open Question 3
- Question: How does the performance of in-context probing scale with increasing model size beyond flan-xxl (11B parameters)?
- Basis in paper: The paper tests models up to flan-xxl but notes that "larger models are better at learning in context"
- Why unresolved: The paper only tests up to 11B parameters, while much larger models exist that might show different scaling behaviors
- What evidence would resolve it: Experiments testing ICP on models with 100B+ parameters to determine if the relative performance gap between ICP and ICL continues to narrow or changes direction

## Limitations
- Robustness claims rely primarily on within-paper variance measurements rather than external replication
- Performance comparisons focus on ICL rather than traditional fine-tuning baselines
- Results are limited to sentence-level classification tasks, limiting generalizability

## Confidence
- High confidence: ICP architecture is well-specified and the basic implementation is reproducible
- Medium confidence: The robustness advantage over ICL is supported by internal variance measurements but lacks external validation
- Low confidence: Comparative performance claims relative to fine-tuning are not well-supported by direct experimental evidence

## Next Checks
1. Replicate the instruction variation experiment using a different set of instruction formulations and compare ICP F1 variance against both ICL and established robust prompting techniques
2. Implement traditional fine-tuning on the same Flan-T5 models using the same training data splits and compare absolute performance metrics to ICP across all model sizes and tasks
3. Evaluate ICP on at least two additional task types beyond sentence classification to assess whether the observed advantages extend beyond the studied task domains