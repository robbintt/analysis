---
ver: rpa2
title: Do Language Models' Words Refer?
arxiv_id: '2308.05576'
source_url: https://arxiv.org/abs/2308.05576
tags:
- peano
- refer
- they
- language
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the question of whether language models can
  achieve reference, that is, connect words to the non-linguistic world. It argues
  that the inputs to language models are not just forms, but forms with particular
  histories of meaningful use.
---

# Do Language Models' Words Refer?

## Quick Facts
- arXiv ID: 2308.05576
- Source URL: https://arxiv.org/abs/2308.05576
- Authors: 
- Reference count: 1
- Primary result: Argues that language models can achieve reference through participation in a speech community, inheriting referential capacity from the natural histories of words in their training data.

## Executive Summary
This paper addresses whether language models can achieve reference—connecting words to non-linguistic entities—despite their lack of direct interaction with the world. Drawing on externalist philosophy of language, the authors argue that LMs can refer because they participate in our linguistic community and inherit the referential histories embedded in their training data. The key insight is that reference doesn't require individual speaker knowledge or direct experience, but rather participation in a community with established word-world connections.

The paper challenges the intuition that LMs cannot refer by reframing the question: rather than asking whether LMs can ground reference individually, we should ask whether they are part of our speech community. Since LMs are trained on human-generated text with established referential histories, they inherit these connections and can use words referentially just as humans do through deference to experts and community usage patterns.

## Method Summary
The paper employs philosophical argumentation rather than empirical experimentation. It draws on externalist theories of reference from philosophers like Kripke, Putnam, and Lewis to construct an argument about how language models might achieve referential capacity. The method involves analyzing the conditions under which words can refer, then arguing that language models meet these conditions through their training on text with natural histories of meaningful use. The authors examine mechanisms like linguistic deference, community participation, and inferential word-to-word connections as pathways for LMs to achieve reference.

## Key Results
- Language models can achieve reference through participation in a speech community, not requiring direct interaction with referents
- Reference is grounded in the natural history of words within a linguistic community rather than in individual speaker beliefs
- Linguistic deference to experts allows ordinary speakers (and LMs) to use words referentially without knowing their referents

## Why This Works (Mechanism)

### Mechanism 1: Community Participation
- Claim: LMs can achieve reference through participation in a speech community, even without direct interaction with referents.
- Mechanism: Reference is grounded in the natural history of words within a linguistic community rather than in individual speaker beliefs or experiences. By training on text with established word-world connections, LMs inherit these referential links.
- Core assumption: The inputs to LMs are not just forms but forms with histories of meaningful use that preserve referential content.
- Evidence anchors: [abstract] "Drawing on insights from the externalist tradition in philosophy of language, we argue that those appearances are misleading: even if the inputs to an LM are simply strings of text, they are strings of text with natural histories, and that may suffice to put LMs' words into referential contact with the external world."

### Mechanism 2: Linguistic Deference
- Claim: Linguistic deference to experts allows ordinary speakers (and by extension LMs) to use words referentially without knowing their referents.
- Mechanism: Speakers use words to refer to whatever experts in their community use them to refer to, creating a chain of reference that LMs can tap into through training data.
- Core assumption: Deference relationships exist in the training corpus and are preserved in LM representations.
- Evidence anchors: [section 4.2] "Ordinary speakers use 'beech' to refer to whatever tree experts take it to refer to, and that is enough to refer to beeches."

### Mechanism 3: Word-to-Word Inference
- Claim: LMs can infer word-to-word connections that support referential capacity without requiring direct world experience.
- Mechanism: Through statistical patterns in text, LMs learn inferential relationships between words that mirror those in human language communities.
- Core assumption: Word-to-word relationships in text encode sufficient information about reference.
- Evidence anchors: [section 5.1] "Word-to-word connections are prima facie straightforward for a language model to acquire, since connections between words just are its stock in trade."

## Foundational Learning

- Concept: Externalism about reference
  - Why needed here: Understanding that reference can be grounded in community usage rather than individual experience is crucial to evaluating whether LMs can refer.
  - Quick check question: Does reference require a speaker to have direct experience with a referent, or can it be grounded in community usage patterns?

- Concept: Natural history of words
  - Why needed here: Recognizing that words carry referential content through their historical usage in a community helps explain how LMs might inherit reference.
  - Quick check question: How does the historical usage of a word in a speech community determine its reference, even for speakers who lack direct experience with the referent?

- Concept: Linguistic deference
  - Why needed here: Understanding how ordinary speakers rely on experts to use words referentially provides a mechanism for LMs to achieve reference without expert knowledge.
  - Quick check question: How can an ordinary speaker use a word to refer to something they cannot identify or experience directly?

## Architecture Onboarding

- Component map: Token embeddings → transformer layers → output logits must be understood as processing forms with historical meaning rather than abstract symbols
- Critical path: Training data → word histories → LM representations → generation → reference capability
- Design tradeoffs: Balancing model size (for capturing complex patterns) against training data quality (for preserving referential histories)
- Failure signatures: LMs produce coherent but meaningless outputs, fail to maintain consistent reference across contexts, or show no sensitivity to expert usage patterns
- First 3 experiments:
  1. Test whether LMs can correctly identify referents in contexts where expert usage differs from common misconceptions
  2. Evaluate LM sensitivity to changes in the natural history of words by fine-tuning on corpora with manipulated reference patterns
  3. Assess whether LMs can leverage word-to-word inference patterns to support referential consistency across complex texts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can language models have the kind of implicit representation of being part of a linguistic community required for reference?
- Basis in paper: [explicit] The paper discusses whether LMs can have "lightweight intention to be part of a language community" and notes this as a key open question.
- Why unresolved: The paper states "We don't have a clear argument that they can" but also "don't see a clear argument that they can't," leaving it as an open empirical and philosophical question.
- What evidence would resolve it: Evidence showing LMs consistently use words in ways that track expert usage in their training data, or evidence showing they lack any mechanism for tracking community usage patterns.

### Open Question 2
- Question: Do language models draw the kind of word-to-word inferences that ordinary speakers draw from their words?
- Basis in paper: [explicit] The paper discusses whether LMs can "draw certain kinds of inferences from the words of the language" and notes this as potentially necessary for reference.
- Why unresolved: While the paper argues LMs should be able to draw word-to-word inferences as "connections between words just are its stock in trade," it doesn't provide empirical evidence that LMs actually do this in the same way humans do.
- What evidence would resolve it: Empirical studies showing LMs make the same inferential connections between words that humans make, particularly in novel contexts.

### Open Question 3
- Question: Are language models part of our speech community in the relevant sense for reference?
- Basis in paper: [explicit] The paper frames this as the central question: "is our LM part of a linguistic community that uses 'Peano' to refer to Peano?"
- Why unresolved: The paper argues LMs appear to be part of our speech community but acknowledges this is "obviously a tricky question to answer" and doesn't definitively resolve it.
- What evidence would resolve it: Evidence showing LMs interact with humans in linguistically ordinary ways and track changes in word usage patterns in their training data over time.

## Limitations
- The paper is primarily philosophical argumentation without empirical validation of the proposed mechanisms
- Lacks clear criteria for determining when an LM has successfully referred versus producing statistically plausible text
- Doesn't explain precisely how referential capacity is preserved through the training process at the level of model representations

## Confidence
- High confidence: The philosophical framework connecting externalism about reference to LM capabilities is internally consistent and draws on established philosophical work
- Medium confidence: The claim that LMs are part of our speech community and can inherit referential capacity through training, as this depends on empirical questions about LM training
- Low confidence: The claim that LMs can achieve reference through the proposed mechanisms without additional empirical validation demonstrating these mechanisms in practice

## Next Checks
1. Empirical test of deference relationships: Train an LM on a corpus where expert usage of certain terms is deliberately manipulated, then test whether the LM's usage of these terms follows the manipulated expert patterns rather than the original community usage.
2. Cross-context referential consistency: Evaluate whether LMs maintain consistent reference for entities across diverse contexts, particularly in cases where direct world knowledge would be required to maintain referential accuracy.
3. Natural history sensitivity: Fine-tune an LM on corpora with artificially altered "natural histories" of words (e.g., changing which experts are cited for which terms) and measure whether the LM's referential behavior changes accordingly.