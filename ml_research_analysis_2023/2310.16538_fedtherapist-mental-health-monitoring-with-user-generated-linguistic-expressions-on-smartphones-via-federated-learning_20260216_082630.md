---
ver: rpa2
title: 'FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions
  on Smartphones via Federated Learning'
arxiv_id: '2310.16538'
source_url: https://arxiv.org/abs/2310.16538
tags:
- data
- text
- mental
- health
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FedTherapist, a privacy-preserving mobile mental
  health monitoring system using user-generated text via federated learning. It leverages
  continuous speech and keyboard input on smartphones while preserving privacy through
  on-device training.
---

# FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning

## Quick Facts
- arXiv ID: 2310.16538
- Source URL: https://arxiv.org/abs/2310.16538
- Reference count: 40
- Key outcome: FedTherapist achieved 0.15 AUROC improvement and 8.21% MAE reduction in predicting depression, stress, anxiety, and mood compared to non-language features.

## Executive Summary
FedTherapist is a privacy-preserving mobile mental health monitoring system that uses federated learning to analyze user-generated text from smartphones. The system employs a Context-Aware Language Learning (CALL) methodology to capture mental health signals from continuous speech and keyboard input while preserving privacy through on-device training. The system integrates temporal contexts like time, location, and activity to enhance mental health monitoring capabilities.

## Method Summary
The system implements a Fixed-BERT + MLP model with Context-Aware Language Learning methodology, trained using federated learning on smartphone data. It collects continuous speech and keyboard input along with temporal context features (time, location, motion, application usage) from participants. The federated learning approach enables training across distributed devices without centralizing raw text data, with model updates aggregated securely.

## Key Results
- 0.15 AUROC improvement in depression detection compared to non-language features
- 8.21% MAE reduction in stress, anxiety, and mood prediction
- Demonstrated feasibility of on-device NLP for mental health monitoring with 46 participants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Federated Learning enables mental health monitoring without collecting raw text data from users.
- Mechanism: Model training occurs locally on each device, with only model updates shared and aggregated securely, preventing the server from accessing actual text data.
- Core assumption: Secure aggregation prevents reconstruction of individual model updates.
- Evidence anchors:
  - "FL decentralizes model training on client devices (e.g., smartphones) using locally stored data (McMahan et al., 2016), ensuring privacy on FedTherapist by only collecting securely aggregated model updates."
  - "FL globally trains a model from the distributed user data on multiple mobile devices. We apply FL with FedTherapist to utilize the privacy-sensitive text data on smartphones for effective mental health monitoring."
- Break condition: If secure aggregation is compromised or gradients can be reverse-engineered to recover user text.

### Mechanism 2
- Claim: Context-Aware Language Learning enhances mental health signal detection.
- Mechanism: Integrating temporal contexts (time, location, motion, application usage) with language models improves the ability to capture mental health signals from noisy text data.
- Core assumption: Temporal context features provide meaningful mental health signal enhancement beyond language alone.
- Evidence anchors: Implicit in methodology description; not explicitly validated through ablation studies.
- Break condition: If temporal features introduce bias or noise that degrades model performance.

## Foundational Learning

### Federated Learning
- Why needed: Enables privacy-preserving training across distributed devices without centralizing sensitive user data.
- Quick check: Verify that model updates cannot be traced back to individual users and that secure aggregation is properly implemented.

### Context-Aware Language Learning (CALL)
- Why needed: Captures mental health signals more effectively by integrating temporal contexts with language models.
- Quick check: Evaluate whether temporal features improve performance through ablation studies comparing with language-only models.

### Fixed-BERT + MLP Architecture
- Why needed: Provides a lightweight, efficient model suitable for mobile deployment while maintaining sufficient capacity for mental health prediction.
- Quick check: Monitor model performance and resource usage to ensure it meets smartphone deployment requirements.

## Architecture Onboarding

### Component Map
User Input -> Speech/Keyboard Processing -> Temporal Context Extraction -> Fixed-BERT + MLP Model -> Federated Learning Aggregation -> Mental Health Prediction

### Critical Path
Data collection (speech/keyboard + temporal features) → Model training via federated learning → Mental health prediction outputs

### Design Tradeoffs
- Privacy vs. model accuracy: Federated learning preserves privacy but may limit model performance compared to centralized training
- Model complexity vs. mobile efficiency: Fixed-BERT + MLP balances prediction capability with smartphone resource constraints
- Context feature selection vs. signal noise: Temporal features may enhance detection but also introduce potential confounders

### Failure Signatures
- Poor convergence in federated learning indicating communication or aggregation issues
- High variance in model updates suggesting inconsistent data quality across devices
- Degradation in prediction accuracy pointing to insufficient context feature relevance

### First 3 Experiments
1. Implement secure aggregation verification to test gradient reconstruction resistance
2. Conduct demographic bias analysis across different age groups and linguistic backgrounds
3. Deploy with extended participant pool (3+ months) to assess battery impact and real-world performance

## Open Questions the Paper Calls Out
- What is the impact of using additional temporal contexts beyond time, location, motion, and application on the accuracy of FedTherapist?
- The paper only explores four types of contexts and does not investigate other potential contexts that could enhance the model's ability to sense mental health signals.

## Limitations
- Sparse federated learning implementation details with no explicit discussion of secure aggregation protocols
- 46-participant sample size may not capture population-level diversity needed for mental health applications
- No addressing of potential biases in mental health signal detection across different demographic groups or language patterns

## Confidence

**High Confidence Claims:**
- Technical feasibility of running BERT-based models on smartphones for mental health monitoring
- Superiority of combined language and temporal features over language-only features
- General privacy benefits of federated learning for sensitive text data

**Medium Confidence Claims:**
- Specific AUROC and MAE improvements reported (0.15 and 8.21%)
- Effectiveness of CALL methodology for capturing mental health signals
- Generalizability of findings to broader populations

**Low Confidence Claims:**
- Claims about long-term sustainability and real-world deployment feasibility
- Privacy guarantees without explicit secure aggregation details
- System's performance across diverse mental health conditions and populations

## Next Checks

1. **Implement secure aggregation verification**: Test whether FedTherapist's federated learning implementation can withstand gradient reconstruction attacks and verify that model updates cannot be traced back to individual users' text data.

2. **Conduct demographic bias analysis**: Evaluate FedTherapist's performance across different age groups, genders, and linguistic backgrounds to identify potential disparities in mental health signal detection accuracy.

3. **Longitudinal real-world deployment**: Deploy FedTherapist with a larger, more diverse participant pool over an extended period (3+ months) to assess battery impact, user retention, and model drift in real-world conditions.