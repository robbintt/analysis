---
ver: rpa2
title: 'MOTOR: A Time-To-Event Foundation Model For Structured Medical Records'
arxiv_id: '2301.03150'
source_url: https://arxiv.org/abs/2301.03150
tags:
- gid00001
- gid00036
- gid00032
- data
- motor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MOTOR, a self-supervised time-to-event foundation
  model trained on structured medical records. MOTOR addresses the challenge of training
  time-to-event models with limited labeled data by pretraining on large-scale electronic
  health record data (up to 55M patient records, 9B clinical events).
---

# MOTOR: A Time-To-Event Foundation Model For Structured Medical Records

## Quick Facts
- arXiv ID: 2301.03150
- Source URL: https://arxiv.org/abs/2301.03150
- Authors: 
- Reference count: 40
- Primary result: 143M parameter time-to-event foundation model trained on 55M patient records, achieving 4.6% better time-dependent C statistics than state-of-the-art.

## Executive Summary
MOTOR is a self-supervised time-to-event foundation model that addresses the challenge of limited labeled data in medical prediction tasks. By pretraining on 55 million patient records with 9 billion clinical events, MOTOR learns general representations of medical codes and their temporal patterns that transfer effectively to specific clinical prediction tasks. The model demonstrates significant improvements over state-of-the-art approaches, achieving 6.6% better C-statistic performance across multiple tasks and showing robust performance even with limited labeled data. MOTOR also shows strong cross-site portability when adapted to the MIMIC-IV dataset.

## Method Summary
MOTOR uses a 143M parameter Transformer backbone with rotary position embeddings and local attention to process sequential medical events. The model is pretrained on 8,192 tasks auto-generated from EHR codes using a piecewise exponential modeling approach with shared time-dependent risk factors. During pretraining, the model learns to predict medical codes and their temporal patterns without requiring labeled outcomes. For fine-tuning on specific clinical tasks, only the linear head parameters are updated while the Transformer backbone remains frozen, providing computational efficiency. The pretraining leverages a low-rank transformation from patient representations to hazard rates across tasks, enabling efficient parameter sharing.

## Key Results
- Improves time-dependent C statistics by 4.6% over state-of-the-art models
- Achieves up to 95% better label efficiency compared to non-pretrained approaches
- Demonstrates robustness to temporal distributional shifts with strong performance on MIMIC-IV adaptation
- Maintains high performance across tasks with varying censoring rates and sample sizes

## Why This Works (Mechanism)

### Mechanism 1
Pretraining on 8,192 tasks auto-generated from EHR codes creates general time-to-event representations that transfer effectively to novel clinical tasks. The model learns distributed representations of medical codes and their temporal patterns across diverse clinical outcomes during pretraining, capturing shared statistical structures useful for any time-to-event prediction task. The temporal distribution of medical events in EHRs contains generalizable patterns that benefit downstream time-to-event tasks, even when specific target tasks are excluded from pretraining.

### Mechanism 2
Using piecewise exponential modeling with shared time-dependent risk factors across tasks enables efficient parameter sharing during pretraining. The low-rank transformation from patient representations to hazard rates allows the same time bins and base hazard structure to be reused across all 8,192 pretraining tasks, dramatically reducing parameter count while maintaining flexibility. Clinical events can be meaningfully modeled using piecewise constant hazard rates within time bins that are shared across different medical conditions.

### Mechanism 3
Fine-tuning only the linear head parameters while freezing the Transformer backbone provides computational efficiency without sacrificing performance. The pretrained Transformer has already learned useful temporal and medical code representations; only the task-specific linear transformation from these representations to hazard rates needs adaptation, reducing fine-tuning parameters by orders of magnitude. The learned patient representations are sufficiently general that they don't require updating for new tasks, and only the mapping to specific outcomes needs adaptation.

## Foundational Learning

- **Concept: Time-to-event modeling vs classification**
  - Why needed here: MOTOR addresses the specific challenges of estimating full probability distributions over event times rather than binary outcomes at fixed horizons
  - Quick check question: What is the key advantage of time-to-event models over classification when dealing with censored observations?

- **Concept: Self-supervised learning and foundation models**
  - Why needed here: The paper relies on pretraining a general model on unlabeled EHR data before fine-tuning on specific tasks with limited labels
  - Quick check question: How does self-supervised pretraining help overcome the limited labeled data problem in time-to-event modeling?

- **Concept: Transformer architectures with causal masking**
  - Why needed here: The model must process sequential EHR data without "cheating" by looking at future information when making predictions
  - Quick check question: Why is causal masking necessary when applying Transformers to temporal medical data?

## Architecture Onboarding

- **Component map:** Medical event sequence → Code embeddings + Time embeddings → 4-layer Transformer with rotary position embeddings and local attention → Patient representations for each event → Linear transformation to time-bin hazard rates → Task-specific linear head for target task hazard rates

- **Critical path:** Medical event sequence → Transformer encoding → Patient representations → Hazard rate computation → Loss calculation (negative log-likelihood)

- **Design tradeoffs:**
  - Causal masking vs bidirectional attention: Ensures temporal validity but may limit context
  - Local attention windows vs full attention: Enables longer sequences but limits global dependencies
  - Piecewise exponential vs parametric distributions: More flexible but requires more parameters
  - Fine-tuning only linear head vs full fine-tuning: Computationally efficient but may miss representation adaptation

- **Failure signatures:**
  - Poor performance on target tasks: Likely indicates insufficient pretraining data or poor task generalization
  - Overfitting on small target datasets: May require stronger regularization or full fine-tuning
  - Computational bottlenecks: Likely in attention computation or sequence length handling

- **First 3 experiments:**
  1. Train MOTOR on pretraining tasks and evaluate C-statistic on pretraining task holdout to verify pretraining effectiveness
  2. Fine-tune on a simple target task with full training data to establish baseline performance
  3. Perform ablation study: MOTOR vs MOTOR without pretraining (MOTOR-WP) on the same target task to isolate pretraining benefit

## Open Questions the Paper Calls Out

### Open Question 1
How does the time-to-event pretraining task perform on medical tasks with more complex censoring patterns than those studied in this work? The paper notes that MOTOR is robust to temporal distributional shifts and achieves strong performance across tasks with varying censoring rates, but does not explore more complex censoring scenarios like informative censoring or competing risks.

### Open Question 2
What is the impact of using different ontologies or code systems for the pretraining tasks on MOTOR's performance? The paper uses OMOP Common Data Model and ICD codes for defining tasks, but does not explore how different coding systems or ontologies might affect pretraining and downstream task performance.

### Open Question 3
How does MOTOR's performance scale with even larger pretraining datasets beyond the 55M patient records used in this study? The paper evaluates pretraining on subsets of the data but does not explore scaling to datasets larger than the 55M patient records used.

### Open Question 4
Can MOTOR be effectively adapted to tasks with very few positive cases (e.g., rare diseases) without overfitting? The paper demonstrates MOTOR's sample efficiency on tasks with reasonable case counts but does not specifically address extreme class imbalance or rare disease prediction.

## Limitations

- Temporal distribution shift: Limited analysis of how MOTOR performs when temporal patterns shift dramatically between pretraining and target domains
- Data-specific generalization: Evaluation limited to specific clinical conditions with unknown generalizability to rare diseases
- Implementation complexity: Complex training pipeline may limit practical adoption and reproducibility

## Confidence

- **High confidence** in the core technical approach: Transformer architectures with causal masking for time-to-event prediction are well-established
- **Medium confidence** in the pretraining effectiveness: Performance improvements demonstrated but more direct ablation studies would strengthen claims
- **Medium confidence** in computational efficiency claims: 95% label efficiency depends on specific baseline models chosen

## Next Checks

1. **Temporal robustness evaluation**: Test MOTOR on target tasks where temporal patterns have shifted significantly (e.g., comparing performance on data from different decades or during disease outbreaks)

2. **Pretraining task diversity analysis**: Conduct controlled experiments varying the number and diversity of pretraining tasks to determine minimum effective pretraining requirements

3. **Cross-domain portability test**: Evaluate MOTOR's performance when transferred to completely different medical domains (e.g., from adult to pediatric populations, or from hospital to outpatient settings)