---
ver: rpa2
title: Large-scale Pretraining Improves Sample Efficiency of Active Learning based
  Molecule Virtual Screening
arxiv_id: '2309.11687'
source_url: https://arxiv.org/abs/2309.11687
tags:
- learning
- docking
- rate
- screening
- molecules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the effectiveness of pretrained transformer-based
  language models (MoLFormer) and graph neural networks (MolCLR) in Bayesian optimization
  active learning for virtual screening of large molecule libraries. Pretrained models
  are shown to identify 58.97% of the top-50000 compounds by docking score after screening
  only 0.6% of a 99.5 million compound library, representing an 8% improvement over
  the previous state-of-the-art baseline.
---

# Large-scale Pretraining Improves Sample Efficiency of Active Learning based Molecule Virtual Screening

## Quick Facts
- arXiv ID: 2309.11687
- Source URL: https://arxiv.org/abs/2309.11687
- Authors: 
- Reference count: 35
- Key outcome: Pretrained models identify 58.97% of top-50000 compounds by docking score after screening only 0.6% of a 99.5 million compound library, representing an 8% improvement over previous state-of-the-art.

## Executive Summary
This study evaluates the effectiveness of pretrained transformer-based language models (MoLFormer) and graph neural networks (MolCLR) in Bayesian optimization active learning for virtual screening of large molecule libraries. The research demonstrates that pretrained models consistently achieve higher hit recovery rates and enrichment factors compared to non-pretrained models like D-MPNN, LightGBM, and random forest. The work shows significant improvements in sample efficiency, with pretrained models identifying a large fraction of top compounds by screening only a small percentage of massive chemical libraries.

## Method Summary
The study uses the MolPAL framework with batched Bayesian optimization, employing surrogate models (MoLFormer, MolCLR, D-MPNN, LightGBM, RF), acquisition functions (Greedy, UCB), and docking score or ROCS similarity as objective functions. Models are pretrained on large unlabeled datasets (MoLFormer on 1.1 billion molecules, MolCLR on 10 million molecules from PubChem) before being applied to active learning tasks. The method iteratively trains models, predicts on unexplored compounds, and acquires new batches for docking screening based on predicted scores.

## Key Results
- Pretrained models (MoLFormer, MolCLR) identify 58.97% of top-50000 compounds by screening only 0.6% of a 99.5 million compound library
- 8% absolute improvement over previous state-of-the-art baseline
- Consistent performance advantages across datasets ranging from 50,000 to 99.5 million compounds
- Superior performance in both structure-based (docking) and ligand-based (3D similarity) virtual screening scenarios

## Why This Works (Mechanism)

### Mechanism 1
Pretrained transformer-based models (MoLFormer) achieve superior hit recovery by leveraging masked language modeling pretraining on large chemical spaces. The transformer learns atomic-level spatial relationships through self-supervised mask prediction, enabling better generalization to unseen molecular structures during active learning.

### Mechanism 2
Graph neural networks with contrastive pretraining (MolCLR) outperform non-pretrained GNNs by learning better molecular representations. Contrastive learning creates augmented views of molecular graphs, forcing the GNN to learn invariant features that capture essential molecular properties relevant to docking.

### Mechanism 3
Pretrained models maintain advantage even with smaller acquisition batch sizes in active learning. Better representations allow models to make more accurate predictions with less data, enabling effective learning from smaller batches.

## Foundational Learning

- Concept: Bayesian optimization in active learning
  - Why needed here: The paper uses batched Bayesian optimization (MolPAL) to iteratively select compounds for docking
  - Quick check question: What is the difference between greedy and UCB acquisition strategies in Bayesian optimization?

- Concept: Molecular representation learning
  - Why needed here: The study compares different molecular representation methods (SMILES-based transformers vs graph-based GNNs)
  - Quick check question: How do transformers and graph neural networks differ in how they process molecular data?

- Concept: Virtual screening metrics
  - Why needed here: Performance is evaluated using top-k retrieval rates and enrichment factors
  - Quick check question: How is enrichment factor calculated in virtual screening?

## Architecture Onboarding

- Component map: Surrogate model -> Acquisition function -> Objective function -> Active learning loop
- Critical path: Pretraining -> Active learning setup -> Initial training -> Iterative acquisition -> Performance evaluation
- Design tradeoffs: MoLFormer (higher accuracy, longer pretraining) vs LightGBM (lower accuracy, faster inference); Greedy acquisition (more stable, potentially less diverse) vs UCB (more diverse, potentially noisier); Larger batch sizes (faster convergence, potentially less efficient) vs smaller batches (slower, potentially more thorough)
- Failure signatures: Low hit recovery rates (poor surrogate model or acquisition function); High variance across runs (insufficient training data or unstable model architecture); Degradation with larger libraries (inadequate pretraining scale or representation capacity)
- First 3 experiments: 1) Compare MoLFormer and D-MPNN on Enamine 50k with greedy acquisition; 2) Test effect of UCB acquisition weight (Î²) on both models; 3) Evaluate performance on Enamine HTS with varying batch sizes

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of pretrained models vary when using different molecular featurization methods (e.g., graph-based vs. string-based representations)? The paper compares MoLFormer (string-based) and MolCLR/D-MPNN (graph-based) but does not systematically explore how performance varies with different featurization approaches.

### Open Question 2
What is the impact of pretraining dataset size and chemical diversity on model performance in virtual screening? The paper mentions specific pretraining dataset sizes but does not investigate how pretraining scale affects performance.

### Open Question 3
How do pretrained models perform on virtual screening tasks for different protein targets or chemical spaces? The paper tests models on only three datasets but does not explore performance across diverse protein targets or chemical spaces.

### Open Question 4
What is the optimal trade-off between acquisition batch size and top-k retrieval rate across different dataset sizes? The paper notes that smaller batch sizes improve top-k retrieval rates but does not systematically explore the optimal batch size for different dataset scales.

## Limitations
- Results primarily validated on three specific Enamine datasets, limiting generalizability to other chemical spaces or target proteins
- Pretraining datasets not fully characterized, making it difficult to assess whether similar performance gains would transfer to domains with different molecular distributions
- Limited exploration of how different molecular featurization methods affect performance

## Confidence

- High confidence: Pretrained models consistently outperform non-pretrained baselines on the tested datasets
- Medium confidence: The 8% absolute improvement over previous state-of-the-art is robust but may not generalize across all chemical spaces
- Low confidence: The specific mechanisms by which pretraining improves sample efficiency are inferred but not experimentally isolated

## Next Checks

1. Test pretrained models on molecular libraries from different chemical vendors to assess generalizability beyond Enamine datasets
2. Conduct ablation studies isolating pretraining effects from active learning efficiency to confirm the source of performance gains
3. Evaluate model performance on different protein targets to verify that improvements are not target-specific artifacts