---
ver: rpa2
title: 'GEANN: Scalable Graph Augmentations for Multi-Horizon Time Series Forecasting'
arxiv_id: '2307.03595'
source_url: https://arxiv.org/abs/2307.03595
tags:
- graph
- forecasting
- time
- series
- geann
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the cold-start problem in multi-horizon time
  series forecasting, where models struggle to forecast accurately due to limited
  historical data. The proposed method, GEANN, uses graph neural networks (GNNs) as
  a data augmentation mechanism to enhance encoder-decoder architectures typically
  used in sequence-to-sequence forecasting.
---

# GEANN: Scalable Graph Augmentations for Multi-Horizon Time Series Forecasting

## Quick Facts
- arXiv ID: 2307.03595
- Source URL: https://arxiv.org/abs/2307.03595
- Authors: 
- Reference count: 15
- Key outcome: GEANN improves cold-start forecasting for e-commerce demand prediction, achieving 0.997 P50 QL and 0.985 P90 QL on a large dataset vs 1.000 for baseline.

## Executive Summary
GEANN addresses the cold-start problem in multi-horizon time series forecasting by using graph neural networks (GNNs) to augment encoder representations with inter-series relationships. The method scales to large graphs with millions of nodes by inducing subgraphs via L-hop neighborhood sampling. Evaluated on retail demand forecasting datasets, GEANN consistently outperforms competitive baselines, with the most substantial gains for newly launched or recently out-of-stock products.

## Method Summary
GEANN enhances encoder-decoder architectures with a graph ensemble module (GEM) that processes static graphs through GNN layers. The model uses either data-driven kNN graphs or domain knowledge-defined browse node graphs to capture inter-series relationships. During training, subgraph sampling reduces memory requirements while preserving relational context. The GEM outputs graph-augmented features that are combined with encoder states to improve decoder performance.

## Key Results
- GEANN-bw (browse node) achieves 0.997 P50 QL and 0.985 P90 QL on large dataset vs 1.000 for baseline
- More substantial gains for cold-start products: newly launched and recently out-of-stock items
- Performance improves with graph ensemble, especially when combining browse node and kNN graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph neural networks augment encoder representations to improve cold-start forecasting by encoding inter-series relationships that Seq2Seq models miss.
- Mechanism: GEANN uses GNN layers to transform encoder hidden states Ht into graph-aware embeddings Gt using static graphs, which are then combined with Ht to enhance the decoder's input.
- Core assumption: The inter-series relationships captured by the graph are predictive of future values, especially for cold-start products.
- Evidence anchors:
  - [abstract] "These GNN-based features can capture complex inter-series relationships, and their generation process can be optimized end-to-end with the forecasting task."
  - [section 2.2] "We leverage the forecasting quality of MQ-CNN with graph-encoded information as an add-on component to improve representation learning."
- Break condition: If the graph structure does not reflect true predictive relationships, performance gains disappear.

### Mechanism 2
- Claim: Scalability is achieved by inducing subgraphs from large graphs via L-hop neighborhood sampling, reducing memory and computation while preserving signal for seed nodes.
- Mechanism: During training, for each mini-batch, GEANN pre-computes the L-hop induced subgraph for each seed node using "top-k" neighborhood sampling. Only these subgraphs are used in forward/backward passes.
- Core assumption: The L-hop neighborhood captures sufficient relational context for effective forecasting while keeping subgraph size manageable.
- Evidence anchors:
  - [section 2.2] "We instead propose a learning algorithm for GEANN... which applies 'top-k' neighborhood sampling and induces 'L-hop' subgraphs"
  - [section 3.2] "Leveraging a graph with millions of nodes is a rare challenge considered in the previous graph-based methods"
- Break condition: If L or k are too small, the subgraph may miss important relational signals; if too large, memory constraints are violated.

### Mechanism 3
- Claim: Domain knowledge-defined graphs (e.g., browse nodes) provide more stable and informative signals than data-driven graphs for large-scale forecasting.
- Mechanism: Browse node graphs encode product relationships based on co-occurrence in the same catalog nodes, which reflects substitutability and complementarity.
- Core assumption: Co-browsing behavior correlates with demand similarity, especially for cold-start products.
- Evidence anchors:
  - [section 2.2] "We use browse nodes... products that belong to the same browse node are likely to be co-browsed by customers"
  - [section 3.3] "The two graph ensemble method performs on-par with the baselines mainly due to the under-performance of the kNN graph"
- Break condition: If browse node structure does not reflect true demand relationships, gains may not materialize.

## Foundational Learning

- Concept: Encoder-decoder architecture with dilated convolutions (MQ-CNN).
  - Why needed here: GEANN builds on MQ-CNN's encoder-decoder structure, using its encoder outputs as input to the GNN module.
  - Quick check question: What is the role of the dilated convolutions in MQ-CNN's encoder, and how do they affect temporal receptive fields?

- Concept: Graph neural networks and message passing.
  - Why needed here: GNNs are used to aggregate neighborhood information from static graphs to enhance encoder representations.
  - Quick check question: How does a 2-layer GCN aggregate information from 2-hop neighbors, and what is the effect on node embeddings?

- Concept: Quantile loss for probabilistic forecasting.
  - Why needed here: The model optimizes for quantile forecasts (P50, P90) using quantile loss, which is asymmetric and robust to outliers.
  - Quick check question: How does the quantile loss function differ for P50 vs P90, and why is this important for demand forecasting?

## Architecture Onboarding

- Component map: Encoder (MQ-CNN dilated CNN) -> GEM (GNN layers on static graphs) -> Decoder (MQ-CNN decoder)
- Critical path: Forward pass: encoder -> GEM -> decoder. Backward pass: gradients flow through GEM and encoder.
- Design tradeoffs: Using static graphs avoids costly dynamic graph construction but may miss evolving relationships. GEM adds parameters but is modular and can be swapped with other GNN types.
- Failure signatures: If GEM is ineffective, P50/P90 QL will not improve over baseline. If scalability fails, training will OOM on GPU. If graph stability is low, kNN graphs will underperform browse node graphs.
- First 3 experiments:
  1. Train GEANN-bw on small dataset with batch size 256, 2-layer GCN, k=10, L=2; compare P50/P90 QL to MQ-CNN.
  2. Train GEANN-kNN on small dataset; check if stability analysis predicts underperformance.
  3. Scale GEANN-bw to large dataset (2M nodes); verify subgraph sampling keeps memory under 16GB per GPU.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GEANN vary with different GNN architectures beyond GCN, such as GAT or GraphSAGE?
- Basis in paper: [explicit] The paper mentions that while GCN layers are used in the current implementation, other types of graph learning layers such as graphSAGE or GAT could be easily extended.
- Why unresolved: The paper only evaluates the performance of GEANN using GCN layers and does not explore the impact of using different GNN architectures on the model's performance.
- What evidence would resolve it: Experimental results comparing the performance of GEANN using different GNN architectures (e.g., GCN, GAT, GraphSAGE) on the same datasets and tasks.

### Open Question 2
- Question: What is the optimal number of graphs to use in the graph ensemble module for different datasets and tasks?
- Basis in paper: [explicit] The paper uses an ensemble of up to two graphs (browse node and kNN embedding) in its experiments but does not explore the impact of using different numbers of graphs on the model's performance.
- Why unresolved: The paper does not provide a systematic study on the effect of the number of graphs in the ensemble on the model's performance across different datasets and tasks.
- What evidence would resolve it: Experimental results comparing the performance of GEANN using different numbers of graphs in the ensemble on various datasets and tasks, identifying the optimal number of graphs for each case.

### Open Question 3
- Question: How does the stability of the kNN graph vary with different similarity metrics and embedding methods?
- Basis in paper: [explicit] The paper uses Pearson correlation as the similarity metric for constructing the kNN graph and observes low stability in the graph. However, it does not explore the impact of using different similarity metrics or embedding methods on the graph's stability.
- Why unresolved: The paper only investigates the stability of the kNN graph using Pearson correlation as the similarity metric and does not explore the effect of other similarity metrics or embedding methods on the graph's stability.
- What evidence would resolve it: Experimental results comparing the stability of kNN graphs constructed using different similarity metrics (e.g., cosine similarity, Euclidean distance) and embedding methods (e.g., pretrained embeddings, learned embeddings) on the same datasets and tasks.

## Limitations

- Performance gains are primarily evaluated on retail demand forecasting, which may not generalize to other domains like energy or finance.
- The stability analysis of kNN graphs shows correlation with performance but does not establish causation or explore alternative similarity metrics.
- Scalability analysis focuses on subgraph sampling but does not systematically explore the tradeoff between L-hop size, neighborhood size, and forecasting accuracy.

## Confidence

- High: GEANN improves P50/P90 QL on cold-start products vs baselines (supported by clear numerical improvements in Table 3).
- Medium: Graph ensemble approach (browse node + kNN) consistently outperforms single-graph or no-graph baselines (supported by ablation results, but gains vary by dataset).
- Low: Stability analysis of kNN graphs directly predicts underperformance (correlation is shown, but causation is not rigorously established).

## Next Checks

1. **Cross-domain generalization**: Apply GEANN to a non-retail time series dataset (e.g., energy load or traffic) and compare P50/P90 QL gains to retail datasets.
2. **Graph quality impact**: Systematically vary L (hop size) and k (neighborhood size) in subgraph sampling and measure the tradeoff between memory usage and forecasting accuracy.
3. **Dynamic vs static graphs**: Replace static browse node graphs with dynamic graphs constructed from recent embeddings and evaluate if performance improves for rapidly evolving product relationships.