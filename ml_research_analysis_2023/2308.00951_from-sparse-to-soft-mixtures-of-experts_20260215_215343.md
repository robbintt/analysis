---
ver: rpa2
title: From Sparse to Soft Mixtures of Experts
arxiv_id: '2308.00951'
source_url: https://arxiv.org/abs/2308.00951
tags:
- experts
- soft
- tokens
- choice
- slots
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Soft MoE introduces a fully differentiable sparse Transformer that
  sidesteps the discrete routing challenges of traditional MoE methods by using soft
  assignments via weighted averages of input tokens. This approach eliminates token
  dropping, expert imbalance, and inference batch effects, while enabling scaling
  to thousands of experts.
---

# From Sparse to Soft Mixtures of Experts

## Quick Facts
- arXiv ID: 2308.00951
- Source URL: https://arxiv.org/abs/2308.00951
- Authors: 
- Reference count: 40
- Key outcome: Soft MoE achieves better image classification performance than dense models with 10.5× lower inference cost through differentiable soft routing

## Executive Summary
This paper introduces Soft MoE, a fully differentiable sparse Transformer architecture that replaces discrete routing decisions with continuous weighted averages of input tokens. Unlike traditional sparse MoE methods that drop tokens and create expert imbalance, Soft MoE passes different weighted combinations of all input tokens to each expert slot, enabling deterministic per-example behavior and scaling to thousands of experts without increased computational cost.

The method demonstrates superior performance on image classification tasks, with Soft MoE-Base/16 outperforming dense ViT-Huge/14 on upstream and few-shot metrics while achieving 10.5× lower inference cost. The architecture scales well to larger models, with Soft MoE-Large/16 surpassing ViT-Huge/14 in performance with only 2% increased inference time despite having 40× more parameters.

## Method Summary
Soft MoE replaces discrete routing with continuous softmax-weighted averaging of tokens for both dispatch and combine operations. Each input token contributes to every expert slot through weighted combinations, eliminating token dropping and expert imbalance. The method scales efficiently by processing slots rather than tokens directly, making the number of experts irrelevant to computational cost. The architecture maintains deterministic per-example behavior by avoiding batch-level routing decisions.

## Key Results
- Soft MoE-Base/16 outperforms dense ViT-Huge/14 on JFT-4B and ImageNet-10-shot with 10.5× lower inference cost
- Soft MoE-Large/16 surpasses ViT-Huge/14 in performance with only 2% increased inference time despite 40× more parameters
- Soft MoE scales to 128 experts in 16 MoE layers with over 40× more parameters than dense baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft MoE avoids token dropping and expert imbalance by using continuous weighted averages instead of discrete routing
- Mechanism: Each input token contributes to every expert slot through softmax-weighted combinations, ensuring all tokens participate in all expert computations
- Core assumption: Softmax-weighted averaging preserves gradient flow while maintaining sparse computational efficiency
- Evidence anchors:
  - [abstract] "Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert"
  - [section 2.1] "Soft MoE is basically immune to token dropping and expert unbalance since every slot is filled with a weighted average of all tokens"
  - [corpus] Weak evidence - no direct corpus citations on this specific mechanism

### Mechanism 2
- Claim: Soft MoE achieves scaling to thousands of experts without increased computational cost
- Mechanism: Total FLOPs depend on total slots, not number of experts; experts process slots rather than tokens directly
- Core assumption: Vectorized operations can efficiently handle large numbers of experts when processing slots
- Evidence anchors:
  - [abstract] "Soft MoE scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40x more parameters than ViT Huge/14, with only 2% increased inference time"
  - [section 2.2] "The total number of experts is irrelevant in this calculation: few experts with many slots per expert or many experts with few slots per expert will have matching costs if the total number of slots is identical"
  - [corpus] Weak evidence - no direct corpus citations on scaling behavior

### Mechanism 3
- Claim: Soft MoE provides deterministic per-example behavior unlike sparse MoEs with batch-level determinism
- Mechanism: Each input sequence is processed independently without token competition across sequences
- Core assumption: Removing batch-level routing decisions eliminates distributional shifts between training and inference
- Evidence anchors:
  - [abstract] "there are no batch-effects at inference, where one input can affect routing (due to limited expert capacity), and hence prediction, for other inputs"
  - [section 2.2] "Since it combines all tokens in each input sequence, we just set the group size to be a single sequence. Every expert does handle tokens from every input, maybe somewhat limiting the amount of high-level specialization"
  - [corpus] Weak evidence - no direct corpus citations on determinism properties

## Foundational Learning

- Concept: Softmax-weighted averaging for differentiable routing
  - Why needed here: Enables continuous gradient flow through routing decisions while maintaining sparse computation
  - Quick check question: What happens to the softmax output as input values become increasingly different?

- Concept: Computational complexity analysis for sparse architectures
  - Why needed here: Understanding how FLOPs scale with experts vs slots is crucial for designing efficient models
  - Quick check question: If you double the number of experts but keep slots constant, how does the FLOPs change?

- Concept: Layer normalization and its interaction with softmax
  - Why needed here: Prevents softmax collapse when applied after layer normalization in high dimensions
  - Quick check question: Why does applying softmax directly after layer norm cause problems as model dimension increases?

## Architecture Onboarding

- Component map:
  - Input tokens → Dispatch weights (softmax over XΦ) → Slot creation (weighted average) → Expert processing → Combine weights (softmax over XΦ) → Output tokens
  - Learnable parameters Φ control both dispatch and combine weights
  - Each expert processes a subset of slots independently

- Critical path:
  - Token mixing through dispatch weights
  - Expert computation on slots
  - Token reconstruction through combine weights
  - Residual connection with previous layer output

- Design tradeoffs:
  - More experts → better specialization but higher memory usage
  - More slots per expert → better performance but increased computation
  - One slot per expert → optimal balance for most cases

- Failure signatures:
  - Softmax weights collapsing to one-hot vectors (check dispatch/combine weight distributions)
  - Memory overflow with many experts (monitor device memory usage)
  - Performance degradation with large model dimensions (check if normalization is applied)

- First 3 experiments:
  1. Implement basic Soft MoE layer with 4 experts and 4 slots, verify gradients flow through routing
  2. Compare performance with identity routing vs learned routing on small dataset
  3. Test scaling behavior by increasing experts from 4 to 64 while keeping slots constant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Soft MoE perform on autoregressive tasks like language modeling, where causal masking is required?
- Basis in paper: [explicit] The paper explicitly states "Auto-regressive decoding One of the key aspects of Soft MoE consists in smartly merging all tokens in the input. This makes the use of Soft MoEs in auto-regressive decoders difficult, since causality between past and future tokens has to be preserved during training."
- Why unresolved: The paper notes this as a limitation but does not provide any experimental results or analysis of Soft MoE performance on autoregressive tasks.
- What evidence would resolve it: Experiments comparing Soft MoE against dense and sparse MoE models on language modeling benchmarks (e.g., perplexity on WikiText, LM1B) would resolve this.

### Open Question 2
- Question: What is the optimal number of experts and slots per expert when scaling to extremely large models (e.g., trillion-parameter scale)?
- Basis in paper: [inferred] The paper shows Soft MoE scales well to thousands of experts in vision tasks, but doesn't explore extreme scaling regimes. The analysis of optimal slots per expert (1-2) is based on moderate-scale experiments.
- Why unresolved: The paper only explores up to 4096 experts and doesn't systematically study the relationship between model scale, expert count, and slots per expert at trillion-parameter scales.
- What evidence would resolve it: Experiments training Soft MoE models with 10K-100K experts and varying slots per expert on large-scale datasets, measuring performance vs. compute efficiency trade-offs.

### Open Question 3
- Question: How does Soft MoE routing behave when applied to tasks with longer sequence lengths (e.g., 4K-8K tokens) compared to the 196-752 token images used in the experiments?
- Basis in paper: [inferred] The paper uses image classification with relatively short sequences (196-752 tokens). The scaling analysis assumes constant slots per expert, which would become expensive for longer sequences.
- Why unresolved: The paper doesn't explore how the number of slots per expert should scale with sequence length, or how routing behavior changes with longer sequences.
- What evidence would resolve it: Experiments varying sequence length from 256 to 8192 tokens with Soft MoE, measuring routing stability, expert utilization, and performance trade-offs with different slots per expert configurations.

### Open Question 4
- Question: What is the impact of different expert architectures (e.g., depth-wise separable convolutions vs. MLPs) on Soft MoE performance and efficiency?
- Basis in paper: [explicit] The paper states "In practice, all experts apply the same function with different parameters, usually an MLP" but doesn't explore alternative expert architectures.
- Why unresolved: The paper only evaluates Soft MoE with MLP experts and doesn't investigate whether other expert architectures (depth-wise separable convolutions, attention-based experts) would be more effective.
- What evidence would resolve it: Experiments comparing Soft MoE with different expert architectures (MLPs, depth-wise separable convolutions, attention-based experts) on the same tasks, measuring performance and efficiency trade-offs.

## Limitations

- Lack of rigorous empirical validation for claims about eliminating expert imbalance and batch effects
- Limited exploration of extreme scaling regimes (trillion-parameter models) and their optimal configurations
- No investigation of alternative expert architectures beyond standard MLPs

## Confidence

- **High confidence**: The mathematical framework for Soft MoE routing and the computational complexity analysis are well-established and verifiable through the provided equations.
- **Medium confidence**: The experimental results showing performance improvements over dense models are convincing, but the comparisons to other MoE variants could be more comprehensive.
- **Low confidence**: Claims about eliminating batch effects and achieving deterministic behavior are based on architectural arguments rather than rigorous experimental validation across diverse batch sizes and sequence lengths.

## Next Checks

1. **Routing Distribution Analysis**: Measure and visualize the entropy of dispatch and combine weights across different model dimensions and sequence lengths to verify that softmax doesn't collapse to one-hot vectors, particularly when applying the proposed L2 normalization.

2. **Expert Utilization Study**: Quantify expert activation patterns and token routing distributions to empirically verify that Soft MoE maintains balanced expert utilization compared to sparse MoE methods, measuring both average utilization and variance across experts.

3. **Memory vs FLOPs Scaling**: Systematically test the scaling behavior by measuring both computational cost (FLOPs) and memory usage as the number of experts increases from 4 to 1024, verifying that memory bandwidth doesn't become the bottleneck as claimed.