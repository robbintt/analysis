---
ver: rpa2
title: Teaching Smaller Language Models To Generalise To Unseen Compositional Questions
arxiv_id: '2308.00946'
source_url: https://arxiv.org/abs/2308.00946
tags:
- training
- datasets
- association
- retrieval
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling smaller language
  models to generalize to unseen compositional questions. The core method combines
  multi-task supervised pretraining on diverse reasoning tasks with a dense retrieval
  system that retrieves evidential paragraph fragments.
---

# Teaching Smaller Language Models To Generalise To Unseen Compositional Questions

## Quick Facts
- arXiv ID: 2308.00946
- Source URL: https://arxiv.org/abs/2308.00946
- Authors: 
- Reference count: 37
- Primary result: Retrieval-augmented training significantly improves zero-shot performance on compositional reasoning tasks for smaller language models

## Executive Summary
This paper addresses the challenge of enabling smaller language models to generalize to unseen compositional questions. The authors propose a method combining multi-task supervised pretraining on diverse reasoning tasks with a dense retrieval system that retrieves evidential paragraph fragments. By augmenting the training data with retrieval-augmented training datasets (RATD), the approach teaches models to weigh partial evidence, ignore irrelevant context, and reason to plausible answers when perfect evidence is unavailable. The method shows significant improvements in zero-shot performance on various evaluation datasets including StrategyQA, CommonsenseQA, DROP, IIRC, ARC-DA, and Musique.

## Method Summary
The method combines three key components: multi-task supervised pretraining on 93 diverse reasoning tasks, a dense retrieval system with multi-hop extensions, and retrieval-augmented training datasets (RATD). The retrieval system uses RoBERTa-base to encode questions and documents into a shared vector space, then retrieves top-k relevant paragraphs using inner product search. The RATD datasets are constructed by running the retrieval system against Wikipedia to create training contexts that expose models to heuristic reasoning strategies. The QA model is based on BART architecture and is trained with error-based sampling across tasks, with additional stages of multitask pretraining to build numerical literacy and reasoning abilities.

## Key Results
- Significant improvement in zero-shot performance on StrategyQA, CommonsenseQA, DROP, IIRC, ARC-DA, and Musique datasets
- Retrieval-augmented training datasets (RATD) successfully impart heuristic reasoning strategies to smaller models
- Multi-task supervised pretraining on diverse reasoning tasks builds foundational reasoning abilities that transfer to unseen compositional questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense retrieval with multi-hop extensions enables smaller models to access relevant evidence fragments from large corpora.
- Mechanism: The system encodes questions and documents into a shared vector space, then retrieves top-k relevant paragraphs using inner product search. Multi-hop training allows the retriever to chain relevant documents together.
- Core assumption: Sufficient relevant information exists in the corpus to answer compositional questions, even if not in a single paragraph.
- Evidence anchors:
  - [abstract] "a dense retrieval system that aims to retrieve a set of evidential paragraph fragments"
  - [section 2.1] "For each training sample, positive documents from other training examples in the batch are used as negatives"
  - [corpus] No explicit evidence; assumes Wikipedia coverage is sufficient
- Break condition: If relevant information doesn't exist in the corpus, or if questions require reasoning beyond what can be captured in 512-token contexts.

### Mechanism 2
- Claim: Retrieval-augmented training datasets (RATD) expose models to heuristic reasoning strategies when perfect evidence is unavailable.
- Mechanism: By constructing training contexts using the retrieval system against Wikipedia, the model learns to weigh partial evidence, ignore irrelevant context, and reason to plausible answers when deductive reasoning isn't possible.
- Core assumption: Models can learn heuristic reasoning strategies from noisy, partially relevant contexts that resemble real retrieval outputs.
- Evidence anchors:
  - [abstract] "retrieval-augmented training datasets which are designed to expose our models to a variety of heuristic reasoning strategies such as weighing partial evidence or ignoring an irrelevant context"
  - [section 1] "we measure the effect of adding datasets to our QA Model training regime that are designed to impart heuristic strategies for reasoning to a plausible rather than an entailed answer"
  - [corpus] No explicit evidence; relies on Wikipedia noise being sufficient for this learning
- Break condition: If training contexts are too noisy or too dissimilar from evaluation contexts, the heuristic strategies may not transfer.

### Mechanism 3
- Claim: Multi-task supervised pretraining on diverse reasoning tasks builds foundational reasoning abilities that transfer to unseen compositional questions.
- Mechanism: Training on 93 tasks covering various reasoning types (numerical, temporal, multi-hop, etc.) creates a model with broad reasoning capabilities that can be applied to new question types.
- Core assumption: Learning abstract reasoning patterns from diverse tasks enables generalization to novel compositional questions.
- Evidence anchors:
  - [abstract] "multi-task supervised pretraining on up to 93 tasks designed to instill diverse reasoning abilities"
  - [section 2.4] "Noting that some of the numerical pretraining tasks take much longer to train to a reasonable degree of proficiency than our textual question-answering tasks, we continue training our QA models from their original pretraining checkpoint with two additional stages of multitask pretraining"
  - [corpus] No explicit evidence; relies on diversity of pretraining tasks being sufficient
- Break condition: If the pretraining tasks don't cover the reasoning patterns needed for specific evaluation datasets, or if the model overfits to training patterns.

## Foundational Learning

- Concept: Dense vector retrieval and maximum inner product search (MIPS)
  - Why needed here: The retrieval system relies on encoding questions and documents into the same vector space and finding relevant documents through inner product similarity
  - Quick check question: Can you explain how FAISS performs approximate nearest neighbor search using HNSW?

- Concept: Multi-task learning with error-based sampling
  - Why needed here: The training regime uses error-based sampling to focus on tasks the model performs poorly on, while also using uniform sampling for stability
  - Quick check question: How does error-based sampling differ from standard uniform sampling in multi-task learning?

- Concept: Numerical literacy and digit tokenization
  - Why needed here: The model uses digit tokenization to improve numerical reasoning, and specific pretraining tasks are designed to build numeracy skills
  - Quick check question: What is digit tokenization and why does it help with numerical reasoning in language models?

## Architecture Onboarding

- Component map:
  - Retriever: Dense passage retriever with multi-hop extensions (RoBERTa-base)
  - Reranker: Refines retrieved documents by scoring paragraph and sentence relevance (ELECTRA-large)
  - Evidence Set Scorer: Evaluates sufficiency of entire evidence sets for answering queries (ELECTRA-large)
  - QA Model: BART-based generative model that reasons over concatenated query and context
  - Iterator: Coordinates the above components across multiple retrieval hops

- Critical path: Question → Retriever (multiple hops) → Reranker → Evidence Set Scorer → QA Model → Answer
- Design tradeoffs: Smaller models trade off some reasoning ability for efficiency, while retrieval compensates by providing external context rather than memorizing all knowledge
- Failure signatures:
  - Poor retrieval: Low recall on in-domain datasets, irrelevant contexts for evaluation
  - Weak reranking: Low sentence-level precision, evidence sets missing key information
  - Inadequate pretraining: Poor performance on numerical or unanswerable questions, failure to generalize to new question types
  - RATD issues: Memorization of training patterns instead of learning heuristics, or failure to transfer learned strategies

- First 3 experiments:
  1. Evaluate retriever performance on in-domain datasets (Hover, HotpotQA) to verify retrieval quality
  2. Test QA model on DROP with digit tokenization enabled to confirm numerical reasoning capability
  3. Run Base vs Base+RATD comparison on StrategyQA to verify heuristic reasoning improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the retrieval system change when using a larger or more diverse corpus, such as a full web corpus instead of just Wikipedia?
- Basis in paper: [explicit] The paper uses Wikipedia as the corpus for retrieval experiments but mentions that the method is most applicable to question-answering tasks where sufficient information can be retrieved from a modest sequence length.
- Why unresolved: The paper does not explore the impact of using different corpora on retrieval performance, leaving open the question of how well the system generalizes to larger or more diverse sources of information.
- What evidence would resolve it: Experimental results comparing retrieval performance on different corpora, such as a full web corpus, would provide insights into the scalability and generalizability of the retrieval system.

### Open Question 2
- Question: Can the heuristic reasoning strategies imparted by the RATD datasets be further improved or refined by incorporating additional types of reasoning or by using more sophisticated training methods?
- Basis in paper: [explicit] The paper discusses the use of RATD datasets to expose models to heuristic reasoning strategies, such as weighing partial evidence or ignoring irrelevant context, but does not explore the limits or potential improvements of these strategies.
- Why unresolved: The paper does not investigate the effectiveness of different types of heuristic reasoning strategies or the impact of more advanced training methods on the performance of the models.
- What evidence would resolve it: Comparative experiments evaluating the performance of models trained with different types of heuristic reasoning strategies or using more advanced training methods would provide insights into the potential for further improvement.

### Open Question 3
- Question: How does the performance of the models change when evaluated on datasets with different levels of compositionality or complexity, beyond the datasets used in the paper?
- Basis in paper: [explicit] The paper defines compositionality as questions requiring reasoning over multiple facts or numerical operations and evaluates models on datasets with varying levels of compositionality, but does not explore the limits of the models' ability to handle more complex or diverse types of compositionality.
- Why unresolved: The paper does not investigate the performance of the models on datasets with more complex or diverse forms of compositionality, leaving open the question of how well the models generalize to different types of compositional reasoning tasks.
- What evidence would resolve it: Experimental results comparing the performance of the models on datasets with different levels of compositionality or complexity would provide insights into the generalizability and limitations of the models' ability to handle compositional reasoning tasks.

## Limitations
- Limited exploration of retrieval performance on truly out-of-domain compositional questions that differ significantly from pretraining data
- Uncertainty about whether improvements represent genuine strategy transfer versus memorization of training patterns
- Specific architecture choices may not generalize to even smaller models or different model types

## Confidence
**High Confidence**: The retrieval system improves factual question answering performance across multiple datasets (CommonsenseQA, StrategyQA, Musique). The experimental design is sound and results are statistically significant with appropriate baselines.

**Medium Confidence**: The multi-task pretraining regime successfully builds diverse reasoning abilities that transfer to unseen compositional questions. While improvements are shown, the paper doesn't provide strong evidence that models are learning abstract reasoning patterns versus dataset-specific heuristics.

**Low Confidence**: The claim that RATD datasets specifically teach heuristic reasoning strategies is the weakest. The paper demonstrates performance improvements but doesn't provide direct evidence that models are learning to weigh partial evidence or ignore irrelevant context in a generalizable way.

## Next Checks
1. **Strategy Transfer Analysis**: Design experiments to test whether models trained with RATD can apply learned heuristics to completely novel compositional question types not seen in any pretraining or RATD datasets.

2. **Retrieval Quality Analysis**: Measure retrieval recall@5 and recall@10 on compositional questions that require multi-hop reasoning but weren't in the pretraining data.

3. **Ablation Studies on RATD Components**: Create controlled experiments removing specific heuristic training scenarios from RATD to determine which components are actually responsible for performance improvements.