---
ver: rpa2
title: 'VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation
  in Street View'
arxiv_id: '2307.06082'
source_url: https://arxiv.org/abs/2307.06082
tags:
- navigation
- right
- forward
- agent
- left
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VELMA, an embodied LLM agent for urban vision
  and language navigation in Street View. The agent uses a verbalization of the trajectory
  and visual environment observations as contextual prompt for the next action.
---

# VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View

## Quick Facts
- arXiv ID: 2307.06082
- Source URL: https://arxiv.org/abs/2307.06082
- Authors: 
- Reference count: 40
- Primary result: Achieves 25%-30% relative improvement in task completion over state-of-the-art for vision and language navigation in Street View

## Executive Summary
VELMA introduces a novel approach to vision and language navigation (VLN) in Street View by verbalizing trajectory and visual observations rather than processing raw images. The system extracts landmarks from navigation instructions and uses CLIP to determine their visibility in panorama views, creating textual descriptions that serve as prompts for LLM decision-making. This verbalization embodiment enables strong few-shot performance with only two in-context examples, achieving 10% and 23% task completion rates on Touchdown and Map2seq datasets respectively. Finetuning the LLM on verbalized trajectories further improves performance by 25%-30% relative to previous state-of-the-art methods.

## Method Summary
VELMA verbalizes environment observations for LLM agents to perform vision and language navigation in Street View. The method extracts landmarks from navigation instructions using GPT-3, then at each step uses CLIP to score landmark visibility in the current panorama view. These observations are verbalized into text format and appended to the LLM prompt to predict the next action. The system modifies the Street View environment to fix intersection alignment issues where automatic rotation created semantic inconsistencies with navigation instructions. The agent can operate through in-context learning with few examples or be finetuned on training text sequences using LoRA adaptation.

## Key Results
- VELMA achieves 25%-30% relative improvement in task completion over ORAR* baseline through finetuning
- Few-shot performance reaches 10% task completion on Touchdown and 23% on Map2seq with only two in-context examples
- Text-based environment observations show less overfitting compared to vector-level image feature fusion methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Verbalization embodiment enables LLM agents to navigate without direct image input
- Mechanism: Landmarks extracted from instructions are scored for visibility using CLIP, then verbalized for LLM reasoning
- Core assumption: Verbalized observations preserve sufficient spatial information for navigation decisions
- Evidence anchors: CLIP-based visibility scoring described in section 5.2; landmark extraction using GPT-3
- Break condition: If verbalized observations lack critical spatial information needed for navigation

### Mechanism 2
- Claim: Modified environment fixes action sequence alignment at intersections
- Mechanism: Agent no longer automatically rotates when moving forward; direction selected based on outgoing edges
- Core assumption: Original automatic rotation created semantic inconsistencies with instructions
- Evidence anchors: Section 3.1 describes unpredictable rotation behavior and modified state transition function
- Break condition: If new direction selection creates confusion at complex intersections

### Mechanism 3
- Claim: Finetuning on verbalized trajectories significantly improves performance
- Mechanism: LLaMA-7B finetuned on ~6,000 training text sequences with LoRA adaptation
- Core assumption: LLM can learn navigation policies from textual descriptions of visual observations
- Evidence anchors: Section 6.3 shows 10-16% relative improvement over ORAR* baseline
- Break condition: If finetuning leads to overfitting on training text patterns

## Foundational Learning

- Concept: CLIP-based landmark detection
  - Why needed here: To determine which landmarks mentioned in instructions are visible in current view
  - Quick check question: What threshold value is used to determine if a landmark is visible?

- Concept: In-context learning with few-shot examples
  - Why needed here: To demonstrate LLM's ability to perform navigation without finetuning
  - Quick check question: How many in-context examples are used in few-shot experiments?

- Concept: Response-based learning optimization
  - Why needed here: To train agent to optimize task completion rather than exact path following
  - Quick check question: What mixing ratio Î» balances teacher-forcing and student-forcing during training?

## Architecture Onboarding

- Component map: Landmark Extractor -> Landmark Scorer (CLIP) -> Verbalizer -> LLM Agent -> Environment
- Critical path: 1) Extract landmarks from instructions 2) Score visibility using CLIP 3) Verbalize observations 4) Append to prompt 5) Predict next action 6) Execute in environment
- Design tradeoffs: Text-only interface enables few-shot learning but may lose spatial precision; unsupervised landmark scoring avoids annotation costs but requires threshold tuning; modified environment improves semantic consistency but changes benchmark
- Failure signatures: Low task completion rates suggest LLM cannot reason from verbalized observations; high variance indicates sensitivity to initialization; poor performance on specific landmark types may indicate CLIP bias
- First 3 experiments: 1) Test few-shot performance with different LLM sizes 2) Compare finetuned vs in-context learning on development set 3) Ablate image model in landmark scorer

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the text provided. However, several important questions emerge from the methodology and results that warrant further investigation.

## Limitations

- The verbalization approach may lose critical spatial information that could be captured by direct image processing
- CLIP-based landmark detection relies on a fixed threshold that wasn't systematically validated across diverse urban environments
- Modified state transition function may introduce new challenges at complex intersections not present in original datasets

## Confidence

**High Confidence**: The 25%-30% relative improvement claim through finetuning is well-supported by quantitative comparisons against ORAR* baseline across multiple datasets and metrics.

**Medium Confidence**: The claim that text-based observations are less prone to overfitting than vector-level fusion is supported by comparative results, though deeper analysis of specific failure modes would strengthen this.

**Low Confidence**: The assumption that verbalized observations preserve sufficient spatial information for effective navigation wasn't rigorously validated through ablation studies.

## Next Checks

1. **CLIP threshold sensitivity analysis**: Systematically vary the landmark visibility threshold from 0.6 to 0.9 and measure impact on task completion rates across different urban environments to determine optimal threshold.

2. **Cross-environment generalization test**: Evaluate VELMA on navigation instructions from different cities or countries not present in training data to assess generalization beyond Touchdown and Map2seq environments.

3. **Detailed failure case analysis**: Conduct qualitative analysis of 50 failed navigation episodes to identify common failure patterns, such as specific landmark types, intersection configurations, or instruction phrasings that consistently cause agent failures.