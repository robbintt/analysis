---
ver: rpa2
title: Improving equilibrium propagation without weight symmetry through Jacobian
  homeostasis
arxiv_id: '2309.02214'
source_url: https://arxiv.org/abs/2309.02214
tags:
- neural
- networks
- learning
- jacobian
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work studies the effects of weight asymmetry on the applicability
  of equilibrium propagation (EP), an alternative learning algorithm to backpropagation
  for physical neural networks. The authors introduce generalized EP, which can be
  formulated without weight symmetry, and analytically isolate the two main sources
  of bias in gradient estimation: the finite-size nudge and the Jacobian asymmetry.'
---

# Improving equilibrium propagation without weight symmetry through Jacobian homeostasis

## Quick Facts
- arXiv ID: 2309.02214
- Source URL: https://arxiv.org/abs/2309.02214
- Reference count: 40
- Primary result: Jacobian homeostasis enables training deep networks with asymmetric weights on ImageNet 32x32, achieving near-symmetric performance

## Executive Summary
This paper addresses a fundamental limitation of Equilibrium Propagation (EP) when applied to physical neural networks: the requirement for weight symmetry. The authors introduce generalized EP that can work with asymmetric weights and analytically identify the two main sources of gradient estimation bias. They propose a novel homeostatic objective that directly penalizes Jacobian asymmetries at the network's fixed point, dramatically improving learning capability. The framework enables training complex architectures on challenging datasets like ImageNet 32x32 without requiring symmetric connections, making EP more viable for neuromorphic hardware implementations.

## Method Summary
The authors extend Equilibrium Propagation to handle asymmetric weights through three key innovations: (1) a holomorphic extension using complex-valued neurons and the Cauchy integral formula to eliminate finite-nudge bias, (2) a Jacobian homeostasis loss that penalizes functional asymmetries of the Jacobian at the fixed point, and (3) continuous-time gradient estimation that eliminates the need for separate free and nudged phases. The homeostatic objective directly targets the asymmetric part of the Jacobian, improving alignment between EP's neuronal error vectors and backpropagation gradients. This framework enables training with arbitrary weight asymmetry while maintaining effective learning dynamics.

## Key Results
- Jacobian homeostasis enables training on ImageNet 32x32 with asymmetric weights, achieving top-1 accuracy within 2% of symmetric case
- Holomorphic EP with Cauchy integral estimation eliminates finite-nudge bias for complex-differentiable networks
- The homeostatic loss significantly improves Jacobian symmetry, correlating with better gradient estimates
- Continuous-time estimation allows phase-free training while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Holomorphic Equilibrium Propagation (hEP) computes exact gradients for finite-size nudges even when the Jacobian is asymmetric.
- Mechanism: The Cauchy integral formula allows exact gradient estimation via integration over a closed path in the complex plane, bypassing the need for infinitesimal nudges.
- Core assumption: The system's dynamics are holomorphic (complex differentiable) and remain well-defined on the integration path.
- Evidence anchors:
  - [abstract] "For complex-differentiable non-symmetric networks, we show that the finite nudge does not pose a problem, as exact derivatives can still be estimated via a Cauchy integral."
  - [section 3.1] "we show that the finite nudge does not pose a problem, as exact derivatives can still be estimated via a Cauchy integral."
  - [corpus] Weak - corpus does not mention Cauchy integrals or holomorphic EP.
- Break condition: If the dynamics are not holomorphic (e.g., use ReLU or absolute value) or the integration path crosses singularities.

### Mechanism 2
- Claim: Jacobian homeostasis reduces bias from asymmetric feedback weights by directly penalizing the asymmetric part of the Jacobian.
- Mechanism: A homeostatic loss minimizes the norm of the skew-symmetric part of the Jacobian (A) at the fixed point, improving alignment between EP's neuronal error vectors and backpropagation.
- Core assumption: The Jacobian's asymmetric component (A) is the primary source of bias when weights are asymmetric.
- Evidence anchors:
  - [abstract] "To mitigate this issue, we present a new homeostatic objective that directly penalizes functional asymmetries of the Jacobian at the network's fixed point."
  - [section 3.4] "we present a new homeostatic objective that directly penalizes functional asymmetries of the Jacobian at the network's fixed point."
  - [corpus] Weak - corpus does not discuss Jacobian homeostasis.
- Break condition: If the Jacobian's asymmetric component is negligible or if the homeostatic objective introduces other optimization instabilities.

### Mechanism 3
- Claim: Continuous-time estimation of hEP gradients allows training without explicit free and nudged phases.
- Mechanism: By averaging over multiple oscillation cycles of the teaching signal, both the fixed point and the gradient can be estimated continuously without separating phases.
- Core assumption: The system's timescale is fast enough relative to the teaching signal to maintain equilibrium throughout oscillations.
- Evidence anchors:
  - [section 3.2] "Moreover, the free fixed point does not need to be evaluated to obtain dβu∗."
  - [section 3.2] "Crucially, there was no free phase in this setting."
  - [corpus] Weak - corpus does not mention continuous-time estimation.
- Break condition: If the system cannot maintain equilibrium during oscillations due to slow dynamics or noise.

## Foundational Learning

- Concept: Equilibrium Propagation (EP) as an alternative to backpropagation
  - Why needed here: EP is the core algorithm being extended; understanding its mechanism is essential to grasp the contributions.
  - Quick check question: How does EP compute gradients differently from backpropagation?

- Concept: Complex differentiability (holomorphic functions)
  - Why needed here: Holomorphic EP relies on complex differentiability to use the Cauchy integral formula for exact gradient estimation.
  - Quick check question: Why can't standard EP use the Cauchy integral formula?

- Concept: Jacobian symmetry and its role in gradient estimation
  - Why needed here: The paper isolates Jacobian asymmetry as a source of bias; understanding this relationship is crucial.
  - Quick check question: How does Jacobian asymmetry affect the alignment of EP's neuronal error vectors with backpropagation?

## Architecture Onboarding

- Component map:
  - Vector field F governing dynamics -> Cauchy integral estimator -> Jacobian homeostasis loss -> Training objective

- Critical path:
  1. Define vector field F and ensure it's holomorphic
  2. Implement hEP with Cauchy integral estimation
  3. Add Jacobian homeostasis loss to training objective
  4. (Optional) Implement continuous-time estimation

- Design tradeoffs:
  - Holomorphic EP requires complex-valued neurons or phase coding, adding implementation complexity
  - Jacobian homeostasis adds computational overhead but enables training with asymmetric weights
  - Continuous-time estimation eliminates phase separation but may require longer simulation times

- Failure signatures:
  - Poor performance despite homeostasis: likely indicates Jacobian asymmetry is not the primary bias source
  - Training instability with continuous-time estimation: likely indicates dynamics too slow relative to teaching signal
  - No improvement with holomorphic EP: likely indicates non-holomorphic components in the vector field

- First 3 experiments:
  1. Train a simple MLP on Fashion MNIST with holomorphic EP (N=6 points) and compare to classic EP (N=1)
  2. Add Jacobian homeostasis to the above experiment and measure improvement in Jacobian symmetry and performance
  3. Implement continuous-time estimation and verify it matches the ground truth dβu∗ within statistical uncertainty

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between functional symmetry (as measured by the homeostatic objective) and perfect weight symmetry in achieving optimal gradient estimates?
- Basis in paper: [explicit] The paper shows that perfect weight symmetry implies functional symmetry, but the converse is not true. The authors demonstrate that the homeostatic loss can improve training even in architectures without reciprocal connectivity.
- Why unresolved: The paper demonstrates that functional symmetry can be achieved without perfect weight symmetry, but does not quantify the precise relationship or determine if there exists an optimal degree of weight symmetry versus functional symmetry for different tasks and architectures.
- What evidence would resolve it: Systematic experiments varying the degree of weight symmetry while maintaining functional symmetry (via the homeostatic objective) across different architectures and tasks, measuring the impact on training dynamics and final performance.

### Open Question 2
- Question: How can complex-valued neurons be physically implemented in neuromorphic hardware to enable holomorphic equilibrium propagation without the finite nudge bias?
- Basis in paper: [explicit] The paper discusses that holomorphic EP requires neurons to oscillate in the complex plane, and mentions this could potentially be implemented through phase coding in addition to firing rates, but notes this requires a fast carrier wave similar to gamma rhythm in neurobiology.
- Why unresolved: The paper identifies the physical interpretation challenge of complex-valued neurons and suggests phase coding as a possibility, but does not provide a concrete implementation strategy or demonstrate feasibility in hardware.
- What evidence would resolve it: A working implementation of complex-valued neurons in neuromorphic hardware using phase coding, with experimental validation showing reduced bias in gradient estimates compared to real-valued implementations.

### Open Question 3
- Question: What is the theoretical limit of how much the homeostatic objective can improve gradient estimates in highly asymmetric networks, and are there diminishing returns?
- Basis in paper: [inferred] The paper shows that the homeostatic loss improves functional symmetry and enables training on challenging datasets, but the improvement on simpler tasks like Fashion MNIST was modest, suggesting there may be limits to its effectiveness.
- Why unresolved: The paper demonstrates the effectiveness of the homeostatic objective but does not explore its theoretical limitations or characterize whether there are diminishing returns as networks become more asymmetric or deeper.
- What evidence would resolve it: Theoretical analysis establishing bounds on the improvement achievable through the homeostatic objective, combined with empirical validation across a range of network depths and asymmetries showing where improvements plateau.

## Limitations
- The holomorphic extension requires complex-valued neurons or phase coding, which may be challenging to implement in physical hardware
- Numerical stability of the Cauchy integral estimator depends on integration path discretization and may suffer from discretization errors
- The effectiveness of Jacobian homeostasis in very deep networks with complex architectures remains unproven
- Continuous-time estimation assumes fast enough dynamics relative to the teaching signal, which may not hold for physical substrates

## Confidence

- High Confidence: The identification of Jacobian asymmetry as a source of bias in EP with asymmetric weights is well-supported by the mathematical analysis in Section 3.4.
- Medium Confidence: The Cauchy integral approach for exact gradient estimation in holomorphic EP is theoretically sound, but its practical implementation details and numerical stability remain uncertain.
- Low Confidence: The effectiveness of continuous-time estimation for eliminating phase separation in practical training scenarios, as the analysis assumes idealized conditions.

## Next Checks

1. **Numerical Stability Analysis**: Implement the Cauchy integral estimator for holomorphic EP and systematically evaluate its numerical stability across different network architectures and integration path discretizations. Compare the variance of gradient estimates with classic EP under finite nudges.

2. **Depth Scaling Experiment**: Train deeper networks (10+ layers) with Jacobian homeostasis on CIFAR-100 and measure how the bias reduction scales with depth. Monitor both performance and the actual Jacobian symmetry metrics throughout training.

3. **Physical Substrate Simulation**: Implement a simulation of a physical neural network substrate with realistic dynamics (e.g., RC circuits with parasitics) and evaluate whether continuous-time estimation can maintain equilibrium throughout oscillations without phase separation.