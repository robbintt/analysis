---
ver: rpa2
title: Power-Enhanced Residual Network for Function Approximation and Physics-Informed
  Inverse Problems
arxiv_id: '2310.15690'
source_url: https://arxiv.org/abs/2310.15690
tags:
- plain
- network
- error
- neural
- plot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a Power-Enhancing residual network designed
  to improve neural network capabilities for function approximation and physics-informed
  inverse problems. By incorporating power terms into residual elements, the architecture
  enhances the stability of weight updating, leading to better convergence and accuracy.
---

# Power-Enhanced Residual Network for Function Approximation and Physics-Informed Inverse Problems

## Quick Facts
- **arXiv ID**: 2310.15690
- **Source URL**: https://arxiv.org/abs/2310.15690
- **Reference count**: 12
- **Key outcome**: Power-Enhancing residual network improves neural network capabilities for function approximation and physics-informed inverse problems through power terms in residual elements.

## Executive Summary
This study introduces a Power-Enhancing residual network architecture that incorporates power terms into residual connections to improve stability during weight updating and facilitate better convergence and accuracy. The proposed SQR-SkipResNet architecture consistently outperforms conventional plain neural networks, particularly for non-smooth functions, demonstrating superior accuracy, convergence speed, and computational efficiency. Experimental validation on 2D and 3D interpolation tasks and inverse Burgers' equation solving confirms the architecture's adaptability across various scenarios.

## Method Summary
The method involves modifying traditional residual networks by incorporating power terms into skip connections, specifically implementing squared residuals every other layer in the SQR-SkipResNet architecture. The study uses L-BFGS-B and Adam optimizers to train four neural network architectures (Plain NN, ResNet, SkipResNet, and SQR-SkipResNet) on smooth and non-smooth test functions, as well as real-world datasets like the Mt. Eden volcano and Stanford Bunny model. Performance is evaluated using Mean Squared Error, Relative L2 Norm Error, and Maximum Absolute Error metrics.

## Key Results
- Power-enhanced SkipResNet demonstrates superior accuracy on non-smooth functions compared to plain networks
- Deeper SQR-SkipResNet architectures show improved convergence rates compared to plain networks
- The architecture achieves better computational efficiency while maintaining or improving accuracy across 2D and 3D interpolation tasks

## Why This Works (Mechanism)

### Mechanism 1
Power-enhanced residual connections improve gradient stability during training by reducing variance of gradient updates across layers. The power term amplifies contribution of earlier layer features, creating smoother gradient flow and reducing vanishing/exploding gradient effects through element-wise power operations.

### Mechanism 2
Power-enhanced SkipResNet demonstrates superior performance on non-smooth functions due to better feature reuse across layers. The power operation creates multiplicative feature interactions that allow the network to capture sharp transitions and discontinuities more effectively than simple additive residuals.

### Mechanism 3
Deeper SQR-SkipResNet architectures show improved convergence rates compared to plain networks due to enhanced feature propagation through power-enhanced skip connections. The combination creates multiple data flow paths with varying computational complexity, allowing deeper networks to selectively activate more complex paths for feature refinement.

## Foundational Learning

- **Concept**: Residual networks and skip connections
  - Why needed here: Understanding how skip connections prevent vanishing gradients is fundamental to grasping why power-enhanced variants improve upon them
  - Quick check question: What problem do skip connections solve in very deep neural networks, and how do they accomplish this?

- **Concept**: Physics-Informed Neural Networks (PINNs)
  - Why needed here: The paper applies the power-enhanced architecture to inverse Burgers' equation, requiring understanding of how PINNs incorporate physical constraints
  - Quick check question: How do PINNs differ from standard neural networks in terms of loss function formulation and training objectives?

- **Concept**: Function approximation theory and universal approximation theorem
  - Why needed here: The core contribution involves improving neural network capability for function approximation, requiring understanding of what makes certain architectures better approximators
  - Quick check question: What does the universal approximation theorem guarantee, and what are its limitations regarding depth and width requirements?

## Architecture Onboarding

- **Component map**: Input → Pre-processing layers → Power-Enhanced SkipResNet blocks (alternating residual and power operations) → Post-processing layers → Output
- **Critical path**: Data flow through residual blocks with power operations applied every other layer; gradient computation flowing backward through same path
- **Design tradeoffs**: Power-enhanced connections improve accuracy but increase computational cost; deeper networks improve feature extraction but may overfit; power values affect stability vs expressiveness
- **Failure signatures**: Training divergence with large power values; convergence plateaus at suboptimal accuracy with insufficient depth; CPU time becomes prohibitive with excessive width
- **First 3 experiments**:
  1. Compare convergence speed of SQR-SkipResNet vs plain network on Franke function (F1) with 500 training points
  2. Test accuracy on non-smooth functions (F2, F3) with varying network depths to find optimal configuration
  3. Apply to Stanford Bunny 3D interpolation to validate real-world performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal power term value for different network depths and function types?
- Basis in paper: The authors state "Our extensive numerical experiments support our approach, indicating that a power of 2 is effective for networks with fewer than 30 hidden layers. However, for deeper networks, a larger power can contribute to network stability."
- Why unresolved: The paper only tests power of 2 and suggests larger powers might be better for deeper networks, but doesn't systematically explore the relationship between power term, network depth, and function characteristics.

### Open Question 2
How does the Power-Enhancing SkipResNet architecture perform on high-dimensional problems (4D, 5D+) compared to conventional methods?
- Basis in paper: The paper only tests 2D and 3D interpolation problems, but doesn't explore higher-dimensional spaces where neural networks typically struggle.
- Why unresolved: The authors demonstrate effectiveness in 2D and 3D but don't investigate scalability to higher dimensions where the "curse of dimensionality" becomes problematic.

### Open Question 3
What is the theoretical justification for why the squared residual (p=2) improves network performance?
- Basis in paper: The paper demonstrates empirical improvements but doesn't provide theoretical analysis of why the power term enhances performance.
- Why unresolved: The authors show that p=2 works well empirically but don't explain the mathematical or theoretical reasons behind this improvement in terms of optimization landscape, gradient flow, or expressivity.

## Limitations
- Optimal power term selection remains heuristic rather than theoretically grounded
- Limited generalization evidence to other physics-informed problems beyond Burgers' equation
- Computational efficiency gains not comprehensively benchmarked against alternative approaches

## Confidence
- **High confidence**: Architectural implementation details and baseline comparisons are clearly specified and reproducible
- **Medium confidence**: Claims about superior performance on non-smooth functions are supported by test cases but limited to three specific functions
- **Low confidence**: Generalization claims to other physics-informed inverse problems lack sufficient empirical support

## Next Checks
1. **Power term sensitivity analysis**: Systematically evaluate SQR-SkipResNet performance across power values p ∈ [1.5, 3.0] to identify optimal ranges and stability thresholds
2. **Cross-domain generalization**: Apply the architecture to Navier-Stokes equation solving and structural mechanics problems to test transferability beyond Burgers' equation
3. **Noise robustness testing**: Evaluate performance degradation under increasing noise levels (SNR 40dB to 0dB) on interpolation tasks to assess practical applicability to real-world data