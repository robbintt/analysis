---
ver: rpa2
title: 'MultiQG-TI: Towards Question Generation from Multi-modal Sources'
arxiv_id: '2307.04643'
source_url: https://arxiv.org/abs/2307.04643
tags:
- question
- image
- input
- text
- multiqg-ti
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the new task of multi-modal question generation
  from image and text sources. The proposed method, MultiQG-TI, enables text-only
  question generators to process visual input by leveraging image-to-text and OCR
  models to convert images into textual descriptions, which are then fed to the question
  generator.
---

# MultiQG-TI: Towards Question Generation from Multi-modal Sources

## Quick Facts
- arXiv ID: 2307.04643
- Source URL: https://arxiv.org/abs/2307.04643
- Authors: 
- Reference count: 11
- Key outcome: MultiQG-TI significantly outperforms ChatGPT with few-shot prompting on multi-modal question generation despite having hundred-times fewer trainable parameters

## Executive Summary
This paper introduces MultiQG-TI, a novel approach for generating textual questions from multi-modal inputs of text and images. The method converts visual information into textual descriptions using an image-to-text model and OCR, then feeds these along with background text to a fine-tuned text-based question generator. Experiments on the ScienceQA dataset demonstrate that MultiQG-TI achieves superior performance compared to both text-only approaches and ChatGPT, while being more efficient to train due to its modular design.

## Method Summary
MultiQG-TI processes multi-modal inputs by first using an image-to-text model (BLIP-2) to generate textual descriptions of images, and an OCR model (PaddleOCR) to extract any text within the images. These textual representations are then formatted with the input background text and fed to a fine-tuned text-based question generator (Flan-T5 variants). The system is trained using next word prediction, with only the question generator module being fine-tuned while other components remain frozen. This modular approach leverages pre-trained models to efficiently process visual information in a text-compatible format.

## Key Results
- MultiQG-TI significantly outperforms ChatGPT with few-shot prompting on ScienceQA dataset
- The approach achieves better results than text-only question generators by incorporating both visual and textual signals
- Fine-tuning only the question generator while keeping other components fixed provides an efficient training approach
- Ablation studies confirm the necessity of both visual and textual information for quality question generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MultiQG-TI enables text-only question generators to process visual input by converting images into textual descriptions
- Mechanism: The system uses an image-to-text model and an OCR model to extract textual descriptions and text content from images, then formats these with the input text and feeds them to a text-based question generator
- Core assumption: Textual representations of visual information are sufficient for a text-based question generator to produce meaningful questions about the visual content
- Evidence anchors:
  - [abstract] "we leverage an image-to-text model and an optical character recognition model to obtain the textual description of the image and extract any texts in the image, respectively, and then feed them together with the input texts to the question generator"
  - [section] "We propose a novel method, dubbed MultiQG-TI, for generating textual questions from multi-modal inputs of texts and images. The idea is simple: we enable a text-based question generator to 'see' by feeding it visual information in the form of text"
  - [corpus] Weak evidence - no direct citations to this specific mechanism, but related work on text-only models processing visual tasks exists (Wang et al., 2022)
- Break condition: If the image-to-text or OCR models fail to capture critical visual information that's necessary for generating meaningful questions, the approach will break down

### Mechanism 2
- Claim: Fine-tuning only the question generator while keeping other components fixed is efficient and effective
- Mechanism: The modular design allows using pre-trained, frozen components (image-to-text, OCR) while only training the question generator on task-specific data
- Core assumption: The pre-trained image-to-text and OCR models are sufficiently capable at their tasks, and only the question generator needs task-specific adaptation
- Evidence anchors:
  - [abstract] "We only fine-tune the question generator while keeping the other components fixed"
  - [section] "These components are readily available and require no or minimal fine-tuning, making MultiQG-TI easy to use and efficient to train"
  - [corpus] No direct evidence in corpus - this is a design choice specific to this paper
- Break condition: If the pre-trained components are not sufficiently capable, or if the question generator needs adaptation beyond what's provided by fine-tuning on limited data

### Mechanism 3
- Claim: Including both visual and textual signals improves question generation quality compared to single-modality approaches
- Mechanism: The question generator receives richer context when both image descriptions and background text are provided, enabling it to generate questions that integrate information from multiple sources
- Core assumption: The quality of generated questions depends on the richness of input context, and multi-modal context provides more information than single-modality context
- Evidence anchors:
  - [abstract] "Additional analyses empirically confirm the necessity of both visual and textual signals for QG"
  - [section] "Table 1 also demonstrate the benefits of including both the visual and textual information when generating questions because MultiQG-TI outperforms its variants with only textual or only visual input"
  - [corpus] No direct evidence in corpus - this is a finding from the paper's experiments
- Break condition: If the image-to-text or OCR models provide redundant or conflicting information with the background text, or if the question generator cannot effectively integrate multi-modal information

## Foundational Learning

- Concept: Image-to-text generation using contrastive sampling
  - Why needed here: The image-to-text model needs to generate multiple candidate descriptions and select the best one based on the model's own perplexity
  - Quick check question: How does contrastive sampling with perplexity-based reranking help improve the quality of generated image descriptions?

- Concept: Optical character recognition (OCR) for extracting text from images
  - Why needed here: Many images contain text content that the image-to-text model may miss or poorly describe, which is crucial for question generation in educational contexts
  - Quick check question: Why is it important to use both image-to-text and OCR models rather than just one or the other?

- Concept: Fine-tuning text-based generative models with next word prediction
  - Why needed here: The question generator needs to learn to generate questions conditioned on the formatted input text containing both background information and image descriptions
  - Quick check question: What training objective is used for fine-tuning the question generator, and why is it appropriate for this task?

## Architecture Onboarding

- Component map:
  Image and background text -> Image-to-text model (BLIP-2) -> OCR model (PaddleOCR) -> Input formatting module -> Question generator (Flan-T5) -> Generated question

- Critical path:
  1. Input image and background text
  2. Image-to-text model generates description
  3. OCR model extracts text from image
  4. Input formatting module creates formatted input
  5. Question generator produces question
  6. Evaluation metrics score the generated question

- Design tradeoffs:
  - Modular vs. end-to-end: Modular approach uses pre-trained components but may lose some fine-grained visual information
  - Model size vs. efficiency: Larger models perform better but require more resources
  - Sampling strategy: Contrastive sampling with reranking improves quality but increases inference time

- Failure signatures:
  - Poor image descriptions -> questions that don't match visual content
  - Missed text in images -> incomplete context for question generation
  - Hallucinations in question generator -> factually incorrect questions
  - Format issues -> model fails to process input correctly

- First 3 experiments:
  1. Verify image-to-text and OCR models work correctly on sample images
  2. Test input formatting with various background text and image combinations
  3. Fine-tune question generator on small subset and evaluate question quality manually

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MultiQG-TI's performance compare to end-to-end multimodal question generation models?
- Basis in paper: [inferred] The paper discusses MultiQG-TI's performance against text-only models and ChatGPT, but does not compare it to end-to-end multimodal models
- Why unresolved: The paper does not explore or discuss end-to-end multimodal question generation models, which could potentially offer better performance by directly processing both image and text inputs
- What evidence would resolve it: Experiments comparing MultiQG-TI's performance against end-to-end multimodal question generation models on the same dataset and metrics

### Open Question 2
- Question: What is the impact of using more advanced image-to-text models on MultiQG-TI's performance?
- Basis in paper: [explicit] The paper discusses the impact of different image-to-text model sizes on performance, but does not explore the use of more advanced models
- Why unresolved: The paper only tests MultiQG-TI with a limited set of image-to-text models and does not investigate the potential benefits of using more advanced or specialized models
- What evidence would resolve it: Experiments using MultiQG-TI with various state-of-the-art image-to-text models and comparing their performance on question generation tasks

### Open Question 3
- Question: How does MultiQG-TI perform on datasets with more diverse and complex multimodal inputs, such as those containing audio or video?
- Basis in paper: [inferred] The paper focuses on image and text inputs, but does not explore the model's performance on more diverse multimodal datasets
- Why unresolved: The paper's experiments are limited to the ScienceQA dataset, which only contains image and text inputs, and does not investigate the model's generalization to other types of multimodal data
- What evidence would resolve it: Experiments evaluating MultiQG-TI's performance on datasets with diverse multimodal inputs, such as those containing audio, video, or other types of data, and comparing the results to its performance on image and text inputs

### Open Question 4
- Question: What are the pedagogical implications of using MultiQG-TI-generated questions in real-world educational settings?
- Basis in paper: [explicit] The paper mentions the potential for using MultiQG-TI in educational scenarios but does not provide any empirical evidence or analysis of its effectiveness
- Why unresolved: The paper does not conduct any user studies or evaluations to assess the quality, relevance, or impact of MultiQG-TI-generated questions in actual educational contexts
- What evidence would resolve it: User studies or classroom experiments evaluating the effectiveness of MultiQG-TI-generated questions in enhancing learning outcomes, student engagement, or other educational metrics compared to traditional question generation methods

## Limitations
- The approach depends on the quality of pre-trained image-to-text and OCR models, which are treated as black boxes
- Evaluation relies solely on automatic metrics without human assessment of question quality, relevance, or factual accuracy
- The ScienceQA dataset may not represent real-world multi-modal question generation scenarios
- The claim that textual representations are "sufficient" for visual question generation is asserted but not rigorously tested

## Confidence
- **High Confidence**: The modular architecture design and implementation details are well-specified and reproducible
- **Medium Confidence**: The claim that MultiQG-TI outperforms ChatGPT with few-shot prompting is supported by reported metrics
- **Low Confidence**: The assertion that "both visual and textual signals are necessary" is based on ablation studies with potentially unoptimized baselines

## Next Checks
1. Conduct human evaluation studies to validate whether automatically generated questions are actually useful, relevant, and factually correct
2. Test the system on out-of-distribution images and text to assess generalization beyond the ScienceQA dataset
3. Compare against end-to-end multi-modal question generation models that process images directly rather than through textual intermediaries