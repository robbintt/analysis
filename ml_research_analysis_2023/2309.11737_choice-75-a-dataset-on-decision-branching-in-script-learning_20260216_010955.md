---
ver: rpa2
title: 'Choice-75: A Dataset on Decision Branching in Script Learning'
arxiv_id: '2309.11737'
source_url: https://arxiv.org/abs/2309.11737
tags:
- option
- scenario
- goal
- scenarios
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Choice-75, a new dataset for studying decision
  branching in script learning. While previous work focused on linear event sequences,
  Choice-75 challenges models to predict the better of two options given a descriptive
  scenario.
---

# Choice-75: A Dataset on Decision Branching in Script Learning

## Quick Facts
- arXiv ID: 2309.11737
- Source URL: https://arxiv.org/abs/2309.11737
- Reference count: 10
- Key outcome: A dataset of 75 scripts with 600+ scenarios testing decision branching in script learning, showing LLMs struggle with multi-hop reasoning and scenarios requiring implicit commonsense knowledge

## Executive Summary
Choice-75 introduces a novel dataset for studying decision branching in script learning, where models must predict the better of two options given descriptive scenarios. Unlike previous work focusing on linear event sequences, this dataset challenges models with scenarios requiring multi-hop reasoning and integration of implicit commonsense knowledge. The dataset contains 75 scripts with over 600 scenarios categorized by difficulty (easy, medium, hard, N/A), annotated based on the number of reasoning steps required. Experiments with large language models demonstrate strong performance on easy and medium scenarios but significant room for improvement on hard cases and scenarios with no clear optimal choice.

## Method Summary
The dataset is created using a combination of manual scenario writing and machine generation with human review. Scenarios are generated using a two-step prompting approach to simulate multi-hop reasoning, where complex scenarios are created that lead to simpler base scenarios. The resulting scenarios are categorized into four difficulty levels based on reasoning complexity. The evaluation uses in-context learning with large language models (text-davinci-003, gpt-3.5-turbo) in two prompt formats (naive and story), testing performance across all difficulty levels.

## Key Results
- LLMs achieve high accuracy on easy scenarios requiring one reasoning step
- Performance degrades significantly on hard scenarios requiring multi-hop reasoning and implicit commonsense knowledge
- User profile format scenarios prove more challenging than traditional text scenarios due to noise and irrelevant information
- Models struggle with scenarios having no clear optimal choice, highlighting limitations in modeling human-like decision-making

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset structure enables multi-hop reasoning by progressively increasing scenario complexity.
- Mechanism: Scenarios are categorized into difficulty levels (easy, medium, hard, N/A) based on the number of reasoning steps required. Easy scenarios need one step, medium require implicit knowledge or multiple steps, and hard require both common sense and multiple reasoning steps.
- Core assumption: Human judgment of reasoning complexity aligns with actual model performance.
- Evidence anchors:
  - [abstract] "Experiments with large language models (LLM) show strong performance on easy and medium scenarios, but significant room for improvement on hard multi-hop reasoning cases"
  - [section 2.1] "We defined four levels: easy, medium, hard, and N/A (for those scenarios without an optimal choice)"
- Break condition: If human judgment of difficulty levels doesn't align with actual model performance, the progressive complexity structure fails to provide meaningful benchmarking.

### Mechanism 2
- Claim: Human-in-the-loop generation produces more challenging scenarios than purely automated methods.
- Mechanism: The paper uses a two-step prompting approach where first a base scenario is generated, then a more complex scenario that leads to the base scenario is created. This simulates multi-hop reasoning.
- Core assumption: Recursive prompting can effectively generate multi-step reasoning scenarios.
- Evidence anchors:
  - [section 2.3] "we do a two-step prompting to simulate multi-hop reasoning... first prompt a text-davinci-003 model to generate a scenario that leads to one choice... then do another few-shot prompting to generate a new scenario that leads to the scenario-base"
  - [abstract] "we also present preliminary results with current large language models (LLM)... there is still notable room for improvement in many hard scenarios"
- Break condition: If the generated scenarios don't actually require multi-hop reasoning or if the recursive prompting produces incoherent scenarios.

### Mechanism 3
- Claim: User profile format increases task difficulty by requiring integration of multiple information types.
- Mechanism: User profiles contain preferences, financial situations, occupations, hobbies, etc. The model must filter relevant information from noise to make optimal choices.
- Core assumption: Heterogeneous information in user profiles better simulates real-world decision making.
- Evidence anchors:
  - [section 2.3] "User profiles, compared to textual scenarios, may be closer to real-life situations where the traits of a user are mined from heterogeneous data sources... Such profiles inevitably include noise, making the task more challenging"
  - [abstract] "significant room for improvement on hard multi-hop reasoning cases and scenarios with no clear optimal choice"
- Break condition: If the additional information in user profiles doesn't actually increase difficulty or if models can easily ignore irrelevant information.

## Foundational Learning

- Concept: Multi-hop reasoning
  - Why needed here: The dataset specifically tests models' ability to make decisions requiring multiple reasoning steps, especially in hard scenarios
  - Quick check question: Can you identify a scenario that requires chaining together three different pieces of information to reach the correct answer?

- Concept: In-context learning
  - Why needed here: The paper uses in-context learning rather than fine-tuning, presenting demonstrations in prompts to guide model predictions
  - Quick check question: What are the key differences between in-context learning and fine-tuning, and when would you choose one over the other?

- Concept: Human-in-the-loop dataset creation
  - Why needed here: The paper uses human review and curation to ensure quality of machine-generated scenarios
  - Quick check question: What are the advantages and disadvantages of human-in-the-loop approaches compared to fully automated dataset creation?

## Architecture Onboarding

- Component map: Dataset creation (manual writing, machine generation) -> Human review -> Dataset assembly -> Model prompting (in-context learning) -> Performance evaluation
- Critical path: Scenario generation → Human review → Dataset assembly → Model prompting → Performance evaluation
- Design tradeoffs: Manual scenario writing ensures quality but is time-consuming; machine generation is faster but requires human curation; user profiles add realism but increase complexity
- Failure signatures: Low performance on hard scenarios, misalignment between human difficulty ratings and model performance, inability to handle user profile format
- First 3 experiments:
  1. Test baseline model performance on easy scenarios only to establish lower bound
  2. Compare naive vs. story prompt formats across all difficulty levels
  3. Evaluate model performance on user profile scenarios vs. traditional text scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on Choice-75 generalize to other domains beyond daily procedures?
- Basis in paper: [inferred] The paper notes that Choice-75 is built from proScript, which focuses on daily procedures, and acknowledges this as a limitation, suggesting that specific adaptation would be required for different domains.
- Why unresolved: The dataset's distribution is inherently limited to daily procedures, and the paper does not provide experiments or analysis on how well the models perform on tasks from other domains.
- What evidence would resolve it: Experiments testing the models on decision-making tasks from various domains, such as medical, legal, or technical fields, would provide evidence of generalization.

### Open Question 2
- Question: What is the impact of cultural bias on the models' decision-making performance in Choice-75?
- Basis in paper: [explicit] The paper mentions that the dataset might have biases from the annotator and that these biases could lead to inappropriate predictions for users from different cultural backgrounds.
- Why unresolved: The paper does not provide an analysis of how cultural differences affect the models' ability to make decisions that align with human reasoning across diverse populations.
- What evidence would resolve it: Conducting experiments with annotators and users from various cultural backgrounds and comparing the models' performance across these groups would reveal the impact of cultural bias.

### Open Question 3
- Question: How does the inclusion of user profiles as scenarios affect the models' ability to predict decisions compared to standard textual scenarios?
- Basis in paper: [explicit] The paper introduces user profiles as a type of scenario and notes that they include noise, making the task more challenging. It also provides an example where the model fails to predict the correct choice given a user profile.
- Why unresolved: While the paper introduces user profiles and mentions their difficulty, it does not provide a detailed analysis of how user profiles compare to textual scenarios in terms of model performance or the specific challenges they present.
- What evidence would resolve it: A comparative analysis of model performance on user profiles versus textual scenarios, including error analysis and identification of specific challenges posed by user profiles, would provide insights into their impact.

## Limitations
- Dataset size (75 scripts with 600+ scenarios) is relatively small compared to other NLP benchmarks
- Difficulty annotation relies on human judgment without clear operational criteria for "number of reasoning steps"
- Evaluation uses only two large language models without testing on other model families or sizes

## Confidence

- **High**: Basic dataset creation and scenario generation methodology
- **Medium**: Difficulty level categorization and its relationship to model performance
- **Medium**: Claims about multi-hop reasoning challenges and user profile complexity

## Next Checks
1. Conduct inter-annotator agreement analysis on difficulty level assignments to verify consistency in the reasoning steps annotation scheme.
2. Test additional model architectures (including smaller open-source models) on the dataset to determine if performance patterns hold across different model families.
3. Perform ablation studies comparing performance on user profile scenarios versus traditional text scenarios to isolate the effect of information heterogeneity on model performance.