---
ver: rpa2
title: How Abstract Is Linguistic Generalization in Large Language Models? Experiments
  with Argument Structure
arxiv_id: '2311.04900'
source_url: https://arxiv.org/abs/2311.04900
tags:
- sentences
- novel
- type
- passive
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the degree of abstract linguistic generalization
  in large language models (LLMs) by focusing on argument structure. Using novel tokens
  for nouns and verbs, the authors find that LLMs demonstrate robust Type 1 knowledge,
  generalizing the use of novel argument tokens across related contexts.
---

# How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure

## Quick Facts
- arXiv ID: 2311.04900
- Source URL: https://arxiv.org/abs/2311.04900
- Reference count: 18
- Key outcome: LLMs show robust Type 1 generalization but fail at Type 2 generalizations, relying on linear order heuristics rather than abstract structural mappings

## Executive Summary
This study investigates the degree of abstract linguistic generalization in large language models by examining their ability to generalize argument structure knowledge. Using novel tokens for nouns and verbs, the authors find that LLMs successfully generalize the use of novel argument tokens across related contexts (Type 1), but fail to generalize between structurally related contexts that were not seen during pre-training (Type 2). The models show a bias toward linear order rather than abstract structural relationships, suggesting limitations in their representation of linguistic knowledge.

## Method Summary
The study fine-tunes pre-trained BERT, DistilBERT, and RoBERTa models on small datasets containing novel tokens (THAX and GORX) in specific argument structure contexts. Models are then evaluated on their ability to predict these novel tokens in structurally related but unobserved contexts. The experiments systematically vary the relationship between fine-tuning and test contexts, comparing Type 1 generalizations (within the same structural alternation) against Type 2 generalizations (across different structural alternations). Embedding analysis is used to understand the mechanisms underlying successful and failed generalizations.

## Key Results
- LLMs successfully generalize novel noun tokens across related contexts within the same structural alternation (Type 1)
- LLMs fail to generalize novel verb selectional preferences across structurally related contexts like active and passive sentences (Type 2)
- Generalization failures are attributed to reliance on linear order heuristics rather than abstract structural mappings
- The semantically-organized structure of embedding space enables Type 1 generalization through clustering of novel tokens with similar existing words

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Large language models demonstrate robust Type 1 generalization by learning novel token embeddings that cluster with semantically similar existing words.
- **Mechanism**: During fine-tuning, novel tokens are placed into embedding space based on their syntactic distribution in fine-tuning contexts. This clustering enables the model to generalize the novel token's use to structurally similar but unseen contexts by drawing on the distributional properties of the words in the same embedding subspace.
- **Core assumption**: Semantic and syntactic similarity are reflected in the embedding space structure, so clustering a novel token with similar existing tokens transfers their distributional knowledge.
- **Evidence anchors**:
  - [abstract] "they generalize the use of novel argument tokens across related contexts that were seen during pre-training (e.g., the active object and passive subject of the verb spray), succeeding by making use of the semantically-organized structure of the embedding space for word embeddings."
  - [section] "We find that LLMs perform well in generalizing the distribution of a novel noun argument between related contexts that were seen during pre-training (e.g., the active object and passive subject of the verb spray), succeeding by making use of the semantically-organized structure of the embedding space for word embeddings."
  - [corpus] Weak: The corpus neighbors focus on conflict resolution, hallucinations, and linguistic annotation but do not directly support this specific mechanism.

### Mechanism 2
- **Claim**: LLMs fail at Type 2 generalization because they rely on surface-level linear order heuristics rather than abstract structural mappings.
- **Mechanism**: When generalizing novel verb selectional preferences, LLMs use the relative linear order of arguments (e.g., subject before verb before object) as a heuristic. This leads to correct predictions when the order matches the fine-tuning data but incorrect predictions when the order is reversed, such as in passives.
- **Core assumption**: The model has not learned to map thematic roles independently of surface word order, instead encoding preferences based on position relative to the verb in the input sequence.
- **Evidence anchors**:
  - [abstract] "However, they fail at generalizations between related contexts that have not been observed during pre-training, but which instantiate more abstract, but well-attested structural generalizations (e.g., between the active object and passive subject of an arbitrary verb). Instead, in this case, LLMs show a bias to generalize based on linear order."
  - [section] "Experiment 2’s results suggest the LLMs under study do not encode Type 2 generalizations that cut across active and passive sentences for a novel verb seen only in active sentences... Generalization seems to be guided by the relative linear order of the arguments and the verb."
  - [corpus] Weak: The corpus neighbors do not provide direct evidence for this linear order heuristic failure in Type 2 generalization.

### Mechanism 3
- **Claim**: LLMs overgeneralize by treating unpassivizable verbs as passivizable, indicating a failure to learn verb-specific syntactic constraints.
- **Mechanism**: When presented with a novel verb, the model may incorrectly generalize its argument distribution to passive forms, even when the verb's class (e.g., have, cost) does not allow passivization. This overgeneralization arises from the model's reliance on linear order rather than learning verb-class specific rules.
- **Core assumption**: The model does not distinguish between verbs that can and cannot passivize, instead applying a uniform generalization strategy based on linear position.
- **Evidence anchors**:
  - [section] "There is a caveat regarding the failure of overgeneralization: not all verbs that appear in SVO active sentences can passivize... To investigate this, we extracted predictions from the baseline versions of the models used in experiment 2 for the argument positions of simple passive sentences like those in (11) and their (grammatical) simple active counterparts for a variety of good subject and good object nouns."
  - [corpus] Weak: The corpus neighbors do not address this specific issue of overgeneralization to unpassivizable verbs.

## Foundational Learning

- **Concept**: Type 1 vs Type 2 generalization
  - **Why needed here**: The study distinguishes between generalization within a structural context (Type 1) and generalization across structurally defined relations (Type 2). Understanding this distinction is crucial for interpreting the experimental results and their implications for LLM capabilities.
  - **Quick check question**: Can you explain the difference between generalizing a novel noun's distribution across related verb structures (Type 1) versus generalizing a novel verb's selectional preferences across active and passive sentences (Type 2)?

- **Concept**: Embedding space structure and semantic clustering
  - **Why needed here**: The success of Type 1 generalization relies on the model's ability to place novel tokens in embedding space such that they cluster with semantically and syntactically similar existing words. This clustering enables the transfer of distributional knowledge.
  - **Quick check question**: How does the semantically-organized structure of the embedding space allow a model to generalize the use of a novel noun token across different argument structures of a known verb?

- **Concept**: Linear order vs structural position
  - **Why needed here**: The failure of Type 2 generalization is attributed to the model's reliance on linear order heuristics rather than abstract structural mappings. Understanding this distinction is key to interpreting why the model succeeds in some contexts but fails in others.
  - **Quick check question**: Why does the model's preference for linear order lead to correct predictions in active sentences but incorrect predictions in passive sentences when generalizing a novel verb's selectional preferences?

## Architecture Onboarding

- **Component map**: Pre-trained transformer-based encoder (BERT/DistilBERT/RoBERTa/MultiBERT) → frozen parameters except novel token embeddings during fine-tuning → masked language modeling objective → prediction of masked tokens based on context
- **Critical path**: For Type 1 generalization: novel token → embedding update during fine-tuning → clustering with semantically similar existing tokens → successful prediction in structurally similar but unseen contexts. For Type 2 generalization: novel verb → embedding update during fine-tuning → failure to learn abstract structural mappings → reliance on linear order heuristics → incorrect predictions in contexts with reversed argument order.
- **Design tradeoffs**: Only updating novel token embeddings in experiment 1 allows cleaner isolation of generalization mechanism but may limit learning of complex selectional preferences. Unfreezing more parameters in experiment 2 allows learning these preferences but introduces risk of catastrophic forgetting and overfitting to fine-tuning data.
- **Failure signatures**: Type 1 generalization failure manifests as inability to predict novel token in structurally similar but unseen contexts. Type 2 generalization failure manifests as reliance on linear order heuristics, leading to incorrect predictions in contexts with reversed argument order (e.g., passives) but correct predictions when order matches fine-tuning data.
- **First 3 experiments**:
  1. Replicate experiment 1 with a different alternation: Fine-tune model on novel noun in theme-object structure of dative alternation and test generalization to goal-object structure and other related contexts
  2. Test Type 1 generalization with more complex structures: Fine-tune model on novel noun in simple active sentence and test generalization to relative clauses or questions to see if model handles longer-distance dependencies
  3. Test Type 2 generalization with known passivizable verb: Fine-tune model on novel verb known to passivize in active sentences and test generalization to passive sentences to see if model learns abstract structural mappings when verb's passivizability is not in question

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do language models acquire and represent Type 2 knowledge for argument structure generalizations, and what are the underlying mechanisms that enable this representation?
- Basis in paper: [explicit] The authors discuss the limitations of current language models in representing Type 2 generalizations and suggest that their training is data-intensive due to the lack of abstract structural knowledge.
- Why unresolved: The paper does not provide a definitive answer on how language models can acquire and represent Type 2 knowledge for argument structure generalizations. It only highlights the limitations and suggests that current models rely on surface-level properties like linear order.
- What evidence would resolve it: Experimental studies that demonstrate the successful acquisition and representation of Type 2 knowledge in language models, along with detailed analyses of the underlying mechanisms and representations.

### Open Question 2
- Question: What are the specific factors that contribute to the data-intensive nature of training language models, and how can these factors be addressed to improve the efficiency of model training?
- Basis in paper: [explicit] The authors suggest that the data-intensive nature of training language models is related to their inability to represent abstract structural generalizations, which leads to a reliance on surface-level properties.
- Why unresolved: The paper does not provide a comprehensive analysis of the factors contributing to the data-intensive nature of training language models. It only hints at the relationship between the lack of abstract structural knowledge and the need for extensive training data.
- What evidence would resolve it: Studies that identify and quantify the specific factors contributing to the data-intensive nature of training language models, as well as research on methods to address these factors and improve training efficiency.

### Open Question 3
- Question: How can the findings from this study on argument structure generalizations be extended to other linguistic domains, and what are the implications for the development of more abstract and generalizable language models?
- Basis in paper: [inferred] The study focuses on argument structure generalizations, but the authors suggest that the findings may have broader implications for the development of more abstract and generalizable language models.
- Why unresolved: The paper does not explore the potential applications of the findings to other linguistic domains or discuss the implications for the development of more abstract and generalizable language models.
- What evidence would resolve it: Research that applies the findings from this study to other linguistic domains and demonstrates the development of language models with improved abstract and generalizable representations.

## Limitations

- The experimental scope is narrow, focusing only on two novel tokens and a limited set of verbs, which may not generalize to broader linguistic phenomena
- The failure analysis for Type 2 generalization relies heavily on qualitative interpretation rather than quantitative measures of structural understanding
- The study does not investigate whether different model architectures or training objectives might mitigate these limitations
- The mechanism explanations for both success and failure cases are largely inferred from embedding analysis rather than directly tested through intervention experiments

## Confidence

**High Confidence**: The finding that LLMs successfully generalize novel noun tokens across related contexts (Type 1) is well-supported by experimental results and embedding analysis. The evidence anchors clearly show this pattern across multiple models and contexts.

**Medium Confidence**: The claim that LLMs fail at Type 2 generalizations due to linear order bias is plausible but less rigorously tested. While the experimental results support this interpretation, alternative explanations (such as insufficient training data or architectural constraints) are not fully ruled out.

**Low Confidence**: The specific mechanism explanations for both Type 1 success (embedding clustering) and Type 2 failure (linear order heuristics) are largely speculative. The study provides correlational evidence but lacks direct causal tests of these proposed mechanisms.

## Next Checks

1. **Intervention Experiment**: Systematically manipulate the fine-tuning data to include explicit structural cues (e.g., adding syntactic markers or dependency relations) and test whether this improves Type 2 generalization performance, directly testing whether linear order bias is the primary limiting factor.

2. **Architecture Comparison**: Repeat the experiments across a broader range of model architectures (including encoder-decoder models and models with explicit structural biases) to determine whether the observed limitations are inherent to transformer-based LLMs or specific to the models tested.

3. **Novel Token Ablation**: Conduct controlled experiments varying the number and types of novel tokens, their initial embeddings, and fine-tuning procedures to isolate the conditions under which embedding clustering succeeds or fails for Type 1 generalization.