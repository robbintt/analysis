---
ver: rpa2
title: GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval
arxiv_id: '2310.20158'
source_url: https://arxiv.org/abs/2310.20158
tags:
- retrieval
- query
- documents
- https
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new method called RRR for zero-shot information
  retrieval that combines the strengths of two popular paradigms - GAR (Generation-Augmented
  Retrieval) and RAG (Retrieval-Augmented Generation). The key idea is to iteratively
  improve retrieval and rewrite stages in a feedback loop.
---

# GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval

## Quick Facts
- arXiv ID: 2310.20158
- Source URL: https://arxiv.org/abs/2310.20158
- Reference count: 40
- Key outcome: RRR achieves state-of-the-art results on 6 out of 8 datasets with up to 17% relative gains in nDCG@10 and Recall@100 metrics.

## Executive Summary
This paper introduces the RRR (Retrieve-Rewrite-Rerank) method, a novel approach for zero-shot information retrieval that combines Generation-Augmented Retrieval (GAR) and Retrieval-Augmented Generation (RAG) paradigms. The method iteratively improves retrieval and rewriting stages through a feedback loop, using large language models for query rewriting, relevance assessment, and re-ranking. RRR demonstrates significant performance improvements over existing state-of-the-art methods on multiple benchmark datasets.

## Method Summary
RRR is a three-stage iterative algorithm for zero-shot information retrieval. In the first stage, documents are retrieved using BM25 and filtered based on relevance scores from a GPT-3.5-Turbo model. In the second stage, the retrieved documents are used to generate query rewrites using GPT-4. This process is repeated for a few iterations to improve recall. In the final stage, a re-ranker (GPT-3.5-Turbo + GPT-4) re-orders the documents based on their relevance to the original query, improving precision. The method uses a sliding window technique to generate relevance scores and requires tuning of hyperparameters such as the number of rewrites and relevance threshold.

## Key Results
- RRR achieves state-of-the-art results on 6 out of 8 datasets in the BEIR and TREC-DL benchmarks.
- The method shows up to 17% relative gains in nDCG@10 and Recall@100 metrics compared to previous best methods.
- RRR outperforms existing methods by 12.8% in nDCG@10 on average across all 8 datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative feedback loop improves recall by progressively refining queries.
- Mechanism: The rewriter generates query rewrites based on retrieved documents, which are then used to fetch more relevant documents in the next iteration. This process continues, with each iteration potentially uncovering new relevant documents.
- Core assumption: The rewriter can generate meaningful rewrites that improve retrieval quality.
- Evidence anchors:
  - [abstract] "Our method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in the zero-shot setting."
  - [section] "The key design principle is that the rewrite-retrieval stages improve the recall of the system"
  - [corpus] Weak: The corpus does not directly support this mechanism. It lists related papers but doesn't provide evidence for the effectiveness of the feedback loop.
- Break condition: If the rewriter generates rewrites that deviate too much from the original query intent, the feedback loop could degrade performance.

### Mechanism 2
- Claim: Relevance model filters out spurious documents, improving precision.
- Mechanism: The relevance model assigns scores to retrieved documents based on their relevance to the original query. Documents with scores below a threshold are filtered out, ensuring that only highly relevant documents are used for rewriting and final output.
- Core assumption: The relevance model can accurately assess document relevance.
- Evidence anchors:
  - [abstract] "A key design principle is that the rewrite-retrieval stages improve the recall of the system and a final re-ranking stage improves the precision."
  - [section] "We use σ(q, ·) to filter poor quality retrievals in recurrence (1)."
  - [corpus] Weak: The corpus does not provide evidence for the effectiveness of the relevance model.
- Break condition: If the relevance model is inaccurate, it could filter out relevant documents or include irrelevant ones.

### Mechanism 3
- Claim: Re-ranker improves precision by re-ordering documents based on their relevance to the original query.
- Mechanism: The re-ranker takes the documents retrieved and filtered by the previous stages and re-orders them based on their relevance to the original query. This ensures that the most relevant documents appear at the top of the final ranked list.
- Core assumption: The re-ranker can accurately assess document relevance.
- Evidence anchors:
  - [abstract] "A key design principle is that the rewrite-retrieval stages improve the recall of the system and a final re-ranking stage improves the precision."
  - [section] "Stage 3. Re-rank: In the final stage, we first rank the documents S by their relevance scores w.r.t. input query q, σ(q, z), z ∈ S. This becomes the initial ordering for the LLM-based re-ranker model h"
  - [corpus] Weak: The corpus does not provide evidence for the effectiveness of the re-ranker.
- Break condition: If the re-ranker is inaccurate, it could reorder documents incorrectly, leading to a less relevant final ranked list.

## Foundational Learning

- Concept: Information Retrieval (IR) basics
  - Why needed here: Understanding IR fundamentals is crucial for grasping the problem RRR solves.
  - Quick check question: What are the three main stages in a typical IR pipeline?
- Concept: Large Language Models (LLMs) for IR
  - Why needed here: RRR leverages LLMs for rewriting, relevance assessment, and re-ranking.
  - Quick check question: How can LLMs be used to improve IR performance?
- Concept: Zero-shot learning
  - Why needed here: RRR is designed for zero-shot IR, meaning it doesn't require labeled data from the target domain.
  - Quick check question: What is the key challenge in zero-shot IR, and how does RRR address it?

## Architecture Onboarding

- Component map: Query → Rewriter (GPT-4) → Retriever (BM25) → Relevance Model (GPT-3.5-Turbo) → Filtered Documents → Re-ranker (GPT-3.5-Turbo + GPT-4) → Final Ranked List
- Critical path: Query → Rewriter → Retriever → Relevance Model → Filtered Documents → Re-ranker → Final Ranked List
- Design tradeoffs:
  - Using LLMs for rewriting, relevance, and re-ranking improves performance but increases computational cost.
  - The number of rewrites (Nrw) and relevance threshold (τ) are hyperparameters that need to be tuned.
- Failure signatures:
  - Poor performance on datasets where BM25 already excels.
  - Degraded performance if the rewriter deviates too much from the original query intent.
  - Inaccurate relevance scores from the relevance model leading to incorrect filtering.
- First 3 experiments:
  1. Evaluate RRR on a single dataset (e.g., TREC-COVID) and compare its performance to BM25 and other baselines.
  2. Vary the number of rewrites (Nrw) and observe its impact on recall.
  3. Disable the re-ranker and assess its contribution to overall performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively distill the rewrite and relevance models in RRR to reduce computational costs while maintaining performance?
- Basis in paper: [inferred] The paper mentions that the current implementation uses large LLMs for the rewriter and relevance models, and suggests that future work could explore distillation techniques to create more compact models.
- Why unresolved: The paper does not provide any experimental results or insights on distillation techniques for the rewrite and relevance stages of RRR.
- What evidence would resolve it: Experiments comparing the performance of RRR using distilled models for the rewrite and relevance stages against the original implementation using large LLMs.

### Open Question 2
- Question: Can we design a more effective re-ranking model for the zero-shot setting that goes beyond the sliding-window technique used in RRR?
- Basis in paper: [explicit] The paper states that an open question is designing compact yet effective models for re-ranking in the zero-shot setting that improve upon the retrieval/relevance models.
- Why unresolved: The paper does not explore alternative re-ranking techniques or provide any insights into how to design a more effective re-ranking model for the zero-shot setting.
- What evidence would resolve it: Experiments comparing the performance of RRR using alternative re-ranking techniques against the current implementation using the sliding-window technique.

### Open Question 3
- Question: How does the performance of RRR vary across different types of queries (e.g., factual vs. open-ended)?
- Basis in paper: [inferred] The paper presents results on various datasets with different types of queries but does not analyze the performance of RRR across different query types.
- Why unresolved: The paper does not provide any insights into how RRR performs on different types of queries or how the algorithm could be adapted to better handle specific query types.
- What evidence would resolve it: Experiments analyzing the performance of RRR on different types of queries and identifying potential adaptations to the algorithm to improve performance on specific query types.

## Limitations
- The method relies heavily on the quality of GPT models for rewriting, relevance assessment, and re-ranking, which may limit its applicability to domains where these models perform poorly.
- The iterative nature of the approach could be computationally expensive, especially for large document collections.
- The performance gains are primarily demonstrated on English-language datasets, raising questions about cross-lingual generalization.

## Confidence
- **High Confidence:** The iterative refinement mechanism and the combination of GAR and RAG paradigms are technically sound approaches that align with established IR principles.
- **Medium Confidence:** The claim of state-of-the-art performance is supported by experimental results, but the paper doesn't provide detailed ablation studies to isolate the contribution of each component.
- **Medium Confidence:** The zero-shot nature of the approach is well-demonstrated, but the method's performance compared to few-shot alternatives is not thoroughly explored.

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of the rewriter, relevance model, and re-ranker components to the overall performance.
2. Evaluate RRR's performance on non-English datasets to assess its cross-lingual generalization capabilities.
3. Compare RRR's computational efficiency against other state-of-the-art zero-shot IR methods, particularly for large-scale document collections.