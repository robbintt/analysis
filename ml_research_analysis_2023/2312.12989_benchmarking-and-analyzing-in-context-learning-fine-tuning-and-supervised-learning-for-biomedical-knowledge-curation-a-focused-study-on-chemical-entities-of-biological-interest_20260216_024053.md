---
ver: rpa2
title: 'Benchmarking and Analyzing In-context Learning, Fine-tuning and Supervised
  Learning for Biomedical Knowledge Curation: a focused study on chemical entities
  of biological interest'
arxiv_id: '2312.12989'
source_url: https://arxiv.org/abs/2312.12989
tags:
- task
- learning
- were
- triples
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study compares three NLP paradigms\u2014in-context learning\
  \ (ICL), fine-tuning (FT), and supervised learning (ML)\u2014for biomedical knowledge\
  \ curation using the ChEBI database. ICL with GPT-4 achieved best accuracy scores\
  \ of 0.916, 0.766, and 0.874 for three curation tasks."
---

# Benchmarking and Analyzing In-context Learning, Fine-tuning and Supervised Learning for Biomedical Knowledge Curation: a focused study on chemical entities of biological interest

## Quick Facts
- arXiv ID: 2312.12989
- Source URL: https://arxiv.org/abs/2312.12989
- Reference count: 40
- Fine-tuning and supervised learning outperform ICL when sufficient task-specific data is available

## Executive Summary
This study benchmarks three NLP paradigms—in-context learning (ICL), fine-tuning (FT), and supervised learning (ML)—for biomedical knowledge curation using the ChEBI database. ICL with GPT-4 achieved best accuracy scores of 0.916, 0.766, and 0.874 for three curation tasks. ML (trained on ~260,000 triples) outperformed ICL in all tasks (accuracy differences: +0.11, +0.22, +0.17). Fine-tuned PubmedBERT performed similarly to best ML models in tasks 1 & 2 (F1 differences: -0.014 and +0.002), but worse in task 3 (-0.048). Simulations revealed performance declines in both ML and FT models with smaller and higher imbalanced training data, where ICL (particularly GPT-4) excelled in tasks 1 & 3 with less than 6,000 triples. However, ICL underperformed ML/FT in task 2. The study concludes that while ICL can be a useful assistant for knowledge curation, it does not make ML and FT paradigms obsolete, especially when sufficient task-related training data is available.

## Method Summary
The study compared ICL, FT, and ML paradigms for biomedical knowledge curation using ChEBI database triples. Researchers implemented three binary classification tasks (true vs random, true vs reversed, true vs related triples) and evaluated performance across five training data scenarios with varying sizes (6K-260K triples) and imbalance ratios (1:1 to 1:10). They used GPT-4, GPT-3.5, and BioGPT for ICL; fine-tuned PubmedBERT for FT; and Random Forest/LSTM models with various embeddings for ML. Models were assessed using accuracy and F1 scores, with additional feature importance analysis and token adaptation experiments.

## Key Results
- ML trained on ~260,000 triples outperformed ICL across all three tasks (accuracy differences: +0.11, +0.22, +0.17)
- GPT-4 excelled with <6,000 training triples in tasks 1 & 3, but never surpassed ML/FT in task 2
- Fine-tuned PubmedBERT achieved similar performance to best ML models in tasks 1 & 2 (F1 differences: -0.014 and +0.002), but underperformed in task 3 (-0.048)
- W2V-Chem embeddings with Random Forest achieved surprisingly good performance despite simple architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning (ICL) excels when training data is scarce (<6,000 triples) or highly imbalanced (positive:negative ≥1:8).
- Mechanism: Pretrained foundational models (GPT-4) leverage their extensive pre-training to generalize from few examples, bypassing the need for task-specific fine-tuning.
- Core assumption: The foundational model's pre-training corpus is sufficiently diverse to cover the target domain's semantic space.
- Evidence anchors:
  - [abstract]: "When training data had 6,000 triples or fewer, GPT-4 was superior to ML/FT models in tasks 1 and 3."
  - [section]: "GPT-4 outperformed both ML-based and fine-tuned approaches in the two most extreme scenarios..."
  - [corpus]: Weak - corpus neighbors discuss ICL but not specifically this data-scarce scenario.
- Break condition: If the target domain is highly specialized and not well-represented in the foundational model's pre-training corpus.

### Mechanism 2
- Claim: Fine-tuning pretrained language models (e.g., PubmedBERT) achieves superior performance when abundant task-specific data is available (≥55,000 triples).
- Mechanism: Fine-tuning adapts the pretrained model's weights to the specific semantic patterns and task nuances present in the domain-specific training data.
- Core assumption: The pretrained model's architecture and initial weights are well-suited for the target task.
- Evidence anchors:
  - [abstract]: "Fine-tuned PubmedBERT performed similarly to best ML models in tasks 1 & 2... but worse in task 3."
  - [section]: "Fine-tuned models outperformed all ML based approaches in the first two tasks."
  - [corpus]: Weak - corpus neighbors discuss fine-tuning but not specifically this performance comparison.
- Break condition: If the task-specific data is too limited to provide meaningful signal for fine-tuning.

### Mechanism 3
- Claim: Supervised learning using domain-specific embeddings (e.g., W2V-Chem) achieves strong performance with moderate amounts of task-specific data.
- Mechanism: Pretrained embeddings capture domain semantics, and supervised learning adapts these embeddings to the specific classification task.
- Core assumption: The domain-specific embeddings adequately represent the semantic relationships relevant to the task.
- Evidence anchors:
  - [abstract]: "ML (trained on ~260,000 triples) outperformed ICL in accuracy across all tasks."
  - [section]: "W2V-Chem... achieved surprisingly good performances... This demonstrates the effectiveness of a simple model (word2vec) with a small task-related corpus."
  - [corpus]: Weak - corpus neighbors discuss embeddings but not specifically this task-specific adaptation.
- Break condition: If the domain-specific corpus used to train the embeddings is too small or not representative of the task.

## Foundational Learning

- Concept: Tokenization and its impact on embedding quality
  - Why needed here: Different tokenization strategies (e.g., PubmedBERT tokenizer vs. NLTK) affect how entities are represented as vectors, influencing model performance.
  - Quick check question: How does the choice of tokenizer affect the representation of chemical entity names with special characters or abbreviations?

- Concept: Feature importance analysis in random forest models
  - Why needed here: Understanding which features (tokens) are most important for classification helps identify and mitigate issues like over-reliance on uninformative tokens.
  - Quick check question: What happens to random forest model performance if uninformative tokens (e.g., short tokens like "1", "2") are removed?

- Concept: Handling imbalanced data in classification tasks
  - Why needed here: The study explicitly simulates scenarios with varying levels of data imbalance, requiring techniques to address potential performance degradation.
  - Quick check question: What are the common techniques for handling imbalanced data, and how might they be applied in this context?

## Architecture Onboarding

- Component map: Data preprocessing -> Tokenization -> Embedding generation -> Model training (Random Forest, LSTM, FT PubmedBERT, ICL) -> Evaluation (Accuracy, F1, Kappa) -> Analysis (Feature importance, Token adaptation, Simulations)
- Critical path: Data preprocessing → Model training → Evaluation → Analysis
- Design tradeoffs:
  - Model complexity vs. data availability: Simpler models (e.g., Random Forest) may perform well with limited data, while more complex models (e.g., fine-tuned LLMs) require more data.
  - Domain specificity vs. generalization: Domain-specific embeddings may capture nuances better, but general embeddings may generalize better to unseen data.
- Failure signatures:
  - Overfitting: High performance on training data but poor generalization to test data.
  - Underfitting: Poor performance on both training and test data.
  - Data imbalance issues: Significantly lower performance on the minority class.
- First 3 experiments:
  1. Implement and evaluate Random Forest models using different embedding strategies (random, GloVe, domain-specific) on Task 1.
  2. Fine-tune PubmedBERT on Task 2 and compare its performance to Random Forest models.
  3. Conduct ICL experiments with GPT-4 on Task 3 under varying data availability scenarios (e.g., 6,000 triples vs. 55,000 triples).

## Open Questions the Paper Calls Out

The study acknowledges that its findings are based on ChEBI triples with limited relationship complexity. Real-world knowledge graphs often involve multi-hop reasoning and more complex relationships that weren't tested. The authors also note that the performance of these paradigms on ontologies with different semantic characteristics (e.g., medical ontologies vs chemical ontologies) remains an open question, as ChEBI has specific characteristics (highly imbalanced relationships, chemical entity focus) that may not generalize to other domains.

## Limitations

- Performance comparisons may be influenced by differences in tokenization strategies (PubmedBERT vs. GPT tokenizers) that weren't explicitly controlled
- Evaluation focuses on binary classification tasks with balanced test sets, while real-world biomedical curation often involves multi-class scenarios and highly imbalanced data distributions
- Conclusions about ICL's superiority in low-data regimes depend heavily on specific GPT-4 implementation and prompt engineering, which are not fully specified

## Confidence

**High Confidence**: The finding that supervised learning with ~260,000 training triples outperforms both ICL and fine-tuning across all three tasks is robust, supported by extensive experiments across multiple data scenarios.

**Medium Confidence**: The claim that GPT-4 excels with <6,000 training triples is supported but could vary with different prompt formulations or GPT model versions. The performance differences between fine-tuned PubmedBERT and ML models (F1 differences of -0.014 to +0.002) are within the margin of experimental variation.

**Low Confidence**: The generalization of findings to other biomedical knowledge curation tasks beyond the three ChEBI-based binary classification tasks tested.

## Next Checks

1. **Reproduce the low-data regime experiments**: Replicate the ICL experiments with GPT-4 on Tasks 1 and 3 using only 6,000 training triples to verify the claimed performance advantage over fine-tuned models.

2. **Test alternative tokenization strategies**: Implement a controlled experiment where all models use the same tokenization approach to isolate the effect of tokenization on performance differences between paradigms.

3. **Evaluate on multi-class extension**: Design and test an extension of Task 2 (true vs. reversed) to a three-class problem (true, reversed, unrelated) to assess whether ICL's advantages persist in more complex classification scenarios.