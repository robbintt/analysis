---
ver: rpa2
title: Why is parameter averaging beneficial in SGD? An objective smoothing perspective
arxiv_id: '2302.09376'
source_url: https://arxiv.org/abs/2302.09376
tags:
- step
- averaged
- size
- stochastic
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Averaged stochastic gradient descent (SGD) is known to generalize
  better than vanilla SGD by preferring flatter minima, but theoretical justification
  for this benefit remains limited. This paper analyzes the implicit bias of averaged
  SGD through the lens of smoothed objectives, where smoothing is induced by stochastic
  gradient noise and controlled by step size.
---

# Why is parameter averaging beneficial in SGD? An objective smoothing perspective

## Quick Facts
- **arXiv ID**: 2302.09376
- **Source URL**: https://arxiv.org/abs/2302.09376
- **Reference count**: 40
- **Key outcome**: Averaged SGD achieves O(η) convergence error on the smoothed objective vs O(√η) for vanilla SGD, enabling better generalization to flat minima.

## Executive Summary
This paper analyzes the implicit bias of averaged stochastic gradient descent (SGD) toward flat minima through the lens of smoothed objectives induced by stochastic gradient noise. The authors prove that under certain regularity conditions, averaged SGD can optimize the smoothed objective more accurately (up to O(η) error) than vanilla SGD (up to O(√η) error). This improved optimization accuracy enables averaged SGD to converge more stably to flat minima, especially with larger step sizes that destabilize vanilla SGD. Empirical results on CIFAR image classification confirm that averaged SGD achieves higher test accuracies and better generalization, particularly on more challenging datasets where stronger implicit bias is beneficial.

## Method Summary
The method analyzes averaged SGD theoretically and empirically. Theoretically, the authors characterize the implicit bias of averaged SGD toward flat minima by analyzing the smoothed objective induced by stochastic gradient noise. They prove convergence bounds showing averaged SGD achieves O(η) error while vanilla SGD achieves O(√η) error under regularity conditions. Empirically, they implement averaged SGD with large step sizes and multi-step learning rate decay schedules on CIFAR10 and CIFAR100 using ResNet-50 and WideResNet-28-10 architectures. They compare test classification accuracy against vanilla SGD and SAM, evaluating the effect of different step size schedules and tail-averaging strategies.

## Key Results
- Averaged SGD converges to the smoothed objective minimizer with O(η) error, while vanilla SGD achieves only O(√η) error under regularity conditions.
- Larger step sizes amplify implicit bias toward flat minima, but averaging stabilizes this bias by counteracting the instability introduced by large step sizes.
- Averaged SGD with large step sizes achieves higher test accuracies and better generalization on CIFAR datasets compared to vanilla SGD, especially on more challenging datasets like CIFAR100.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Averaged SGD improves optimization accuracy on the smoothed objective compared to vanilla SGD.
- **Mechanism:** Parameter averaging reduces variance in the stochastic gradients, allowing more precise convergence toward the minimizer of the smoothed objective function.
- **Core assumption:** The smoothed objective has one-point strong convexity at its minimizer and satisfies additional regularity conditions (bounded Hessian approximation error and positive definiteness at the minimizer).
- **Evidence anchors:**
  - [abstract]: "averaged SGD can optimize the smoothed objective more accurately (up to O(η) error) than vanilla SGD (up to O(√η) error)"
  - [section]: "Theorem 2 shows the convergence of the averaged SGD... the averaged SGD can optimize the smoothed objective more precisely than SGD under the same step size when SGD also approaches the solution"
- **Break condition:** If the smoothed objective does not satisfy the required regularity conditions (A4, A5) uniformly for the chosen step size, the O(η) improvement may not hold.

### Mechanism 2
- **Claim:** Larger step sizes amplify implicit bias toward flat minima, but can destabilize vanilla SGD; averaging stabilizes this bias.
- **Mechanism:** A large step size increases the stochastic gradient noise amplitude, which smooths the effective objective and eliminates sharp minima. Averaging over iterations counteracts the instability introduced by large step sizes, allowing convergence to the smoothed minimizer.
- **Core assumption:** The noise amplitude is proportional to the step size and the original objective has diverse sharpness across minima.
- **Evidence anchors:**
  - [abstract]: "averaged SGD can efficiently optimize the smoothed objective which avoids sharp local minima"
  - [section]: "The success of using a large step size can be attributed to the strong implicit bias... SGD with a large step size cannot stay in sharp regions because of the amplified stochastic gradient noise"
- **Break condition:** If the step size is too large relative to the objective's smoothness, both SGD and averaged SGD may diverge regardless of averaging.

### Mechanism 3
- **Claim:** Parameter averaging enables better control of the bias-optimization tradeoff inherent in SGD.
- **Mechanism:** Small step sizes yield accurate optimization but weak bias toward flat minima; large step sizes yield strong bias but poor accuracy. Averaging allows use of larger step sizes (stronger bias) while maintaining optimization accuracy, effectively improving the tradeoff.
- **Core assumption:** The required conditions for Theorem 2 hold, particularly that the smoothed objective's curvature structure allows the averaging to reduce error to O(η).
- **Evidence anchors:**
  - [abstract]: "averaged SGD can get closer to a solution of a penalized objective on the sharpness than the vanilla stochastic gradient descent using the same step size"
  - [section]: "averaged SGD can improve the above tradeoff... it can optimize the smoothed objective more precisely than SGD under the same step size"
- **Break condition:** If the variance of iterates is too high or the smoothed objective's curvature is poorly conditioned, averaging may not sufficiently improve optimization accuracy.

## Foundational Learning

- **Concept: Stochastic Gradient Noise and Its Effect on Optimization**
  - Why needed here: The paper hinges on understanding how stochastic gradient noise induces implicit regularization and affects convergence.
  - Quick check question: How does the variance of stochastic gradients influence the effective smoothness of the optimization objective?

- **Concept: Implicit Regularization in Deep Learning**
  - Why needed here: The mechanism relies on SGD's tendency to prefer flat minima, which is central to the improved generalization discussed.
  - Quick check question: Why do flat minima tend to generalize better than sharp minima in neural network training?

- **Concept: Parameter Averaging Schemes (e.g., SWA)**
  - Why needed here: The paper builds on existing averaging techniques and extends their theoretical justification.
  - Quick check question: What is the primary difference between tail-averaging and standard running average in SGD?

## Architecture Onboarding

- **Component map:** Objective function f(w) -> Smoothed objective F(v) via convolution with noise -> Stochastic gradient noise model (ϵₜ₊₁(w)) with bounded variance -> Update rules for SGD and averaged SGD -> Theoretical bounds from Theorems 1 and 2 -> Experimental evaluation pipeline

- **Critical path:**
  1. Define objective f and noise model satisfying Assumptions A1-A2.
  2. Compute smoothed objective F and verify conditions A3-A5.
  3. Run SGD with step size η and track iterates {vₜ}.
  4. Run averaged SGD, collect averages, and compare convergence to F's minimizer.
  5. Validate theoretical bounds empirically and measure generalization.

- **Design tradeoffs:**
  - Larger step sizes increase implicit bias but risk instability; averaging mitigates instability at the cost of increased computation per iteration.
  - More aggressive smoothing (larger η) eliminates sharp minima but may introduce bias if the smoothed objective poorly approximates f.
  - The averaging window length trades off between variance reduction and responsiveness to the optimization landscape.

- **Failure signatures:**
  - If iterates diverge or oscillate excessively, the step size may be too large relative to L.
  - If convergence to sharp minima persists, the smoothing may be insufficient (η too small).
  - If empirical error bounds are much worse than O(√η) or O(η), assumptions A4-A5 may be violated.

- **First 3 experiments:**
  1. Implement the one-dimensional example from Section 9 with varying δ and r to observe the separation between SGD and averaged SGD.
  2. Run CIFAR10 experiments with small vs. large step sizes and compare training/test loss curves and Hessian traces.
  3. Test the effect of minibatch size on the convergence bounds by scaling σ1 and σ2 and observing the impact on error rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions can we prove a separation between SGD and averaged SGD's convergence rates to the true minimizer of the original objective f, rather than just the smoothed objective F?
- Basis in paper: [inferred] The paper proves averaged SGD converges to F's minimizer up to O(η) error while SGD converges up to O(√η) error, but doesn't directly address when this translates to better convergence to f's minimizer.
- Why unresolved: The relationship between convergence to F's minimizer and convergence to f's minimizer is not fully characterized, especially when f has sharp minima that are smoothed out in F.
- What evidence would resolve it: Examples where both methods converge to F's minimizer, but averaged SGD achieves significantly better final error on f due to its tighter O(η) bound.

### Open Question 2
- Question: Can the theory be extended to analyze the effect of minibatch size on the bias-optimization tradeoff for averaged SGD?
- Basis in paper: [explicit] The paper mentions minibatch size could be incorporated by dividing variance terms, but doesn't explore this.
- Why unresolved: The interaction between minibatch size, step size, and averaging in controlling bias and optimization accuracy is not fully understood theoretically.
- What evidence would resolve it: Bounds showing how minibatch size interacts with step size and averaging to affect convergence rates and implicit bias.

### Open Question 3
- Question: What is the relationship between the type of noise injected (e.g. Gaussian, uniform, data augmentation) and the implicit bias of averaged SGD towards flat minima?
- Basis in paper: [inferred] The paper uses uniform noise for analysis but mentions data augmentation as a source of noise in practice.
- Why unresolved: Different noise distributions may have different smoothing effects and thus different impacts on the implicit bias.
- What evidence would resolve it: Empirical and theoretical analysis comparing averaged SGD's performance and bias under different noise distributions on various datasets and architectures.

## Limitations
- Theoretical claims rely on strict regularity assumptions (A4, A5) about the smoothed objective's curvature that may not hold in deep learning landscapes.
- O(η) vs O(√η) error bounds are asymptotic and may not manifest clearly in finite-sample or large-batch regimes.
- Empirical validation is limited to CIFAR image classification with specific architectures, leaving generalization to other domains uncertain.

## Confidence

**High**: The O(η) vs O(√η) convergence error separation under stated assumptions.
**Medium**: The empirical correlation between averaging, larger step sizes, and improved generalization.
**Low**: Quantitative claims about the strength of implicit bias as a function of step size in real neural networks.

## Next Checks

1. Test averaged SGD with large step sizes on non-image datasets (e.g., language modeling on WikiText-2) to assess domain robustness.
2. Systematically vary the smoothing parameter δ in the one-dimensional toy example and measure the gap between SGD and averaged SGD convergence.
3. Analyze the Hessian spectrum at converged solutions for both SGD and averaged SGD to directly quantify flatness differences.