---
ver: rpa2
title: 'Human-in-the-loop: Towards Label Embeddings for Measuring Classification Difficulty'
arxiv_id: '2311.08874'
source_url: https://arxiv.org/abs/2311.08874
tags:
- label
- images
- classes
- classification
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a Bayesian approach to model label uncertainty
  in supervised learning when multiple annotators label each instance, potentially
  with disagreement. Instead of assuming a single ground truth label, it embeds each
  instance into a K-dimensional space (K = number of classes) using a Dirichlet-Multinomial
  framework.
---

# Human-in-the-loop: Towards Label Embeddings for Measuring Classification Difficulty

## Quick Facts
- arXiv ID: 2311.08874
- Source URL: https://arxiv.org/abs/2311.08874
- Reference count: 35
- Primary result: Bayesian Dirichlet-Multinomial framework models label uncertainty via K-dimensional embeddings, producing correlation matrices that reflect semantic class relationships

## Executive Summary
This paper proposes a Bayesian approach to handle label uncertainty in supervised learning when multiple annotators provide potentially conflicting labels for each instance. Instead of assuming a single ground truth label, the method embeds each instance into a K-dimensional space using a Dirichlet-Multinomial framework, where K equals the number of classes. The approach estimates these embeddings via a stochastic Expectation-Maximization algorithm with MCMC sampling, producing correlation matrices that reveal semantic relationships between classes. Applied to three benchmark datasets, the method demonstrates promise for improving machine learning models by incorporating label uncertainty directly into training.

## Method Summary
The method treats each instance's annotation distribution as a draw from a latent K-dimensional embedding space, using a Dirichlet-Multinomial model. Each instance is assigned a K-dimensional embedding vector Z, which parameterizes a Dirichlet distribution from which observed label distributions are drawn. The model uses MCMC sampling within an EM framework to estimate the posterior distribution of Z given observed annotations. This approach handles varying numbers of annotators per instance and produces correlation matrices that reflect semantic similarities between classes. The method extends beyond image classification to any multi-label scenario where label uncertainty exists.

## Key Results
- The Dirichlet-Multinomial model successfully captures label uncertainty by embedding instances into a continuous K-dimensional space
- Correlation matrices derived from estimated embeddings reveal semantic relationships between classes across three benchmark datasets
- The model handles varying numbers of annotators per instance, preserving information that would be lost by majority voting
- Estimated embeddings capture classification difficulty and uncertainty, showing potential for improving machine learning models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Dirichlet-Multinomial model captures label uncertainty by treating each instance's annotation distribution as a draw from a latent embedding space
- Mechanism: Each instance is assigned a K-dimensional embedding vector Z. These embeddings parameterize a Dirichlet distribution from which the observed label distributions are drawn. The model uses MCMC sampling within an EM framework to estimate the posterior distribution of Z given the observed annotations
- Core assumption: Label uncertainty is best represented by a continuous distribution over classes rather than a single discrete label
- Evidence anchors: [abstract]: "The main idea of this work is to drop the assumption of a ground truth label and instead embed the annotations into a multidimensional space"; [section 2.3]: "We obtain a Dirichlet-Multinomial model by assuming a K-dimensional ground truth Z_i for each image"

### Mechanism 2
- Claim: The correlation matrix derived from estimated embeddings reveals semantic relationships between classes
- Mechanism: After estimating embeddings for all instances, the correlation matrix of these embeddings is computed. High positive correlations indicate classes that are frequently confused, while negative correlations suggest well-distinguishable classes
- Core assumption: Semantic similarities between classes manifest as correlations in the embedding space
- Evidence anchors: [abstract]: "Besides the embeddings themselves, we can investigate the resulting correlation matrices, which reflect the semantic similarities of the original classes very well for all three exemplary datasets"; [section 3.1]: "The correlation patterns between classes are visible in the correlation matrix shown in Figure 4"

### Mechanism 3
- Claim: The model handles varying numbers of annotators per instance, preserving information that would be lost by majority voting
- Mechanism: The model directly incorporates the number of annotations per instance (J_i) into the multinomial distribution that generates the observed votes. This allows the model to give more weight to instances with more annotations
- Core assumption: The number of annotations per instance is informative about the reliability of that instance's label distribution
- Evidence anchors: [section 2.1]: "To keep notation simple we will drop index i from the number of voters per image and write J subsequently. We emphasise though, that images can be labelled by different number of voters, as our examples demonstrate"; [section 3.2]: "The proposed approach for estimating suitable label embeddings is independent of the number of annotations per image"

## Foundational Learning

- Concept: Bayesian inference and posterior estimation
  - Why needed here: The model uses Bayesian methods to estimate the posterior distribution of embeddings given observed annotations
  - Quick check question: What is the role of the prior distribution in Bayesian inference, and how does it affect the posterior estimates in this model?

- Concept: Expectation-Maximization (EM) algorithm
  - Why needed here: The model uses a stochastic version of the EM algorithm to estimate the parameters of the embedding space
  - Quick check question: What are the E-step and M-step in the EM algorithm, and how do they work together to find maximum likelihood estimates?

- Concept: Markov Chain Monte Carlo (MCMC) sampling
  - Why needed here: MCMC is used within the EM framework to sample from the posterior distribution of embeddings
  - Quick check question: What is the purpose of MCMC sampling in Bayesian inference, and how does it help estimate the posterior distribution when analytical solutions are intractable?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Model specification -> Estimation algorithm -> Analysis tools -> Application interface

- Critical path:
  1. Preprocess annotation data (handle varying J_i)
  2. Initialize model parameters (prior mean and covariance)
  3. Run stochastic EM algorithm:
     - E-step: Sample embeddings using MCMC
     - M-step: Update prior parameters based on sampled embeddings
  4. Analyze results (compute correlations, visualize embeddings)
  5. Integrate embeddings into ML pipeline

- Design tradeoffs:
  - Computational cost vs. accuracy: More MCMC samples improve posterior estimates but increase runtime
  - Dimensionality of embedding space: Higher K captures more nuance but increases parameter count
  - Prior specification: Strong priors regularize estimates but may bias results if misspecified

- Failure signatures:
  - High variance in MCMC samples indicates poor mixing or multimodal posteriors
  - Correlation matrix with all near-zero entries suggests embeddings aren't capturing class relationships
  - Negative correlations between expected similar classes indicate model misspecification

- First 3 experiments:
  1. Verify binary case: Apply model to a simple binary classification dataset and check that embeddings align with intuitive class separation
  2. Test varying annotator counts: Create synthetic data with different numbers of annotations per instance and verify the model properly weights them
  3. Evaluate correlation recovery: Generate synthetic data with known class correlations and check if the model recovers them in the estimated correlation matrix

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating label embeddings into machine learning training compare to traditional majority voting in terms of classification accuracy and uncertainty calibration?
- Basis in paper: [explicit] The paper suggests that using embeddings instead of single labels could improve model calibration and predictive uncertainties, but states this is "beyond the scope of this paper"
- Why unresolved: The paper only sketches this idea theoretically without empirical validation. No experiments are conducted comparing models trained on embeddings versus traditional labels
- What evidence would resolve it: Controlled experiments training identical models on (1) majority vote labels, (2) embedding-based labels, and (3) soft label distributions, measuring accuracy, calibration error, and uncertainty quality

### Open Question 2
- Question: Can the Dirichlet-Multinomial model reliably estimate embeddings for instances with only a single annotation?
- Basis in paper: [explicit] The paper states "The proposed strategy can also be used for instances without multiple labels. With the distributional framework, we can estimate embeddings for new observations with only one label, incorporating the information gained from other cases"
- Why unresolved: While mentioned as a potential application, the paper doesn't demonstrate or validate this capability. No experiments show performance with mixed annotation availability
- What evidence would resolve it: Experiments showing embedding quality and classification performance when some instances have single annotations versus multiple annotations

### Open Question 3
- Question: How sensitive are the correlation matrices to different prior specifications and MCMC sampling configurations?
- Basis in paper: [explicit] The paper mentions using a multivariate Gaussian prior and MCMC sampling, but doesn't explore sensitivity to these choices
- Why unresolved: The paper reports stable correlation matrices but doesn't systematically investigate how results change with different priors, MCMC chain lengths, or burn-in periods
- What evidence would resolve it: Systematic sensitivity analysis varying prior parameters, MCMC chain lengths, and burn-in periods, showing stability/robustness of correlation matrices

## Limitations
- The paper lacks critical implementation details including specific prior distributions, MCMC sampling parameters, and computational complexity analysis
- Evaluation is limited to three datasets without ablation studies on hyperparameter sensitivity or comparison with alternative uncertainty quantification methods
- The paper doesn't address potential overfitting to annotation patterns or scalability concerns for larger datasets

## Confidence
- **High Confidence**: The theoretical foundation of the Dirichlet-Multinomial model for label uncertainty is sound and well-established in Bayesian statistics
- **Medium Confidence**: The claim that correlation matrices reflect semantic similarities is supported by qualitative analysis in three datasets, but lacks quantitative validation or comparison with ground truth semantic relationships
- **Medium Confidence**: The model's ability to handle varying numbers of annotators is demonstrated empirically but not thoroughly analyzed for edge cases (very few annotations per instance)

## Next Checks
1. **MCMC Diagnostics**: Run trace plots and autocorrelation analysis on the MCMC samples to verify convergence and mixing properties. Poor convergence would invalidate the embedding estimates

2. **Synthetic Data Testing**: Generate synthetic datasets with known class correlations and varying annotation patterns to test whether the model can recover the ground truth structure in the correlation matrix

3. **Ablation Study**: Compare the proposed method against simpler baselines (majority voting, uncertainty sampling) on classification performance to quantify the benefit of incorporating label embeddings into downstream models