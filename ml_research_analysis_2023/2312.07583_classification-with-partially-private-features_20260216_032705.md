---
ver: rpa2
title: Classification with Partially Private Features
arxiv_id: '2312.07583'
source_url: https://arxiv.org/abs/2312.07583
tags:
- private
- accuracy
- features
- privacy
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses differentially private classification when
  some features are sensitive while others and the label are public. The authors propose
  a novel AdaBoost adaptation called Boosting with Random Classifiers (BRC) that trains
  private and public classifiers separately, using random linear classifiers for private
  features without noise injection on weights.
---

# Classification with Partially Private Features

## Quick Facts
- arXiv ID: 2312.07583
- Source URL: https://arxiv.org/abs/2312.07583
- Reference count: 33
- One-line primary result: AdaBoost adaptation using random classifiers for private features outperforms private logistic regression and public-only baselines on partially private classification tasks

## Executive Summary
This paper introduces Boosting with Random Classifiers (BRC), a novel differentially private classification algorithm designed for scenarios where some features are sensitive while others and the label are public. Unlike traditional approaches that inject noise into classifier weights, BRC uses random linear classifiers for private features and perturbs only the accuracy computation with Laplace noise. The surprising empirical finding is that randomly generated classifiers, when combined through AdaBoost, achieve high accuracy without using data weights for private features. The method demonstrates significant performance improvements over existing approaches across four real-world datasets, particularly for moderate privacy budgets (ε ≥ 0.02).

## Method Summary
BRC is an AdaBoost adaptation that trains public and private classifiers separately in each iteration. Public classifiers are trained on public features using standard weighted learning, while private classifiers are generated randomly without using data weights. The algorithm computes weighted accuracy for both classifiers and perturbs only the private classifier's accuracy with Laplace noise. The better classifier (based on accuracy distance from 50%) is selected and weighted, then weights are updated for the next iteration. This approach achieves ε-differential privacy while avoiding noise injection into classifier weights, reducing overall noise compared to methods that perturb both weights and accuracy.

## Key Results
- BRC outperforms differentially private logistic regression and public-only baselines on four real-world datasets
- Significant accuracy gains observed for moderate privacy budgets (ε ≥ 0.02)
- Random linear classifiers prove effective as weak learners in AdaBoost without data weights
- Method extends to standard private setting, providing comparable or higher accuracy than private logistic regression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random linear classifiers in each iteration can serve as weak learners when combined via AdaBoost, even without using data weights.
- Mechanism: The algorithm generates random linear classifiers with coefficients and intercepts drawn uniformly from [-1, 1]. AdaBoost assigns higher weights to classifiers whose accuracy is farthest from random guessing (50%), effectively amplifying any slight edge over chance.
- Core assumption: The distribution of random classifiers is sufficiently diverse that at least some will have accuracy slightly better than 50% on the given dataset.
- Evidence anchors:
  - [abstract]: "As a surprising observation, we show that boosting randomly generated classifiers suffices to achieve high accuracy."
  - [section]: "In Step 5 are chosen using the following procedure: At each iteration, randomly flip each label with probability p ≈ 0.49 and find the optimal classifier treating each point as having unit weight."
- Break condition: If the dataset is highly complex or has very low signal-to-noise ratio, random classifiers may consistently have accuracy close to 50%, making them ineffective weak learners.

### Mechanism 2
- Claim: Using public features to train classifiers without noise injection provides accurate public classifiers that contribute to overall model performance.
- Mechanism: Public features and labels are used to train non-private classifiers using standard weighted learning. These classifiers are not subject to noise injection, preserving their accuracy while still contributing to the ensemble.
- Core assumption: Public features contain sufficient information to train classifiers that achieve better than random accuracy.
- Evidence anchors:
  - [abstract]: "In this paper, we consider differentially private classification when some features are sensitive, while the rest of the features and the label are not."
  - [section]: "The public classifier (Step 4) is learnt using the public weights in a standard fashion, and these weights are also used to assess the accuracy of this classifier on the data set in Step 6."
- Break condition: If public features are highly correlated with private features but contain no independent signal, the approach may not provide significant advantage over fully private methods.

### Mechanism 3
- Claim: Adding noise only to accuracy computation (not to classifier weights) reduces overall noise while maintaining privacy.
- Mechanism: Laplace noise is added only when computing the accuracy of private classifiers. The actual classifier generation doesn't use weights, eliminating the need for weight perturbation. This reduces total noise compared to methods that perturb both weights and accuracy.
- Core assumption: The sensitivity of accuracy computation is low enough that Laplace noise provides sufficient privacy without overwhelming the signal.
- Evidence anchors:
  - [section]: "The key difference with classical AdaBoost as well as previous differentially private versions of AdaBoost [11] is the way in which the weights are used in Algorithm 2."
  - [section]: "In contrast to AdaBoost that uses weights log(1−cerrt/cerrt), we use the former since we need to perturb err pri t with Laplace noise and this might it negative."
- Break condition: If the dataset is very small, the noise added to accuracy computation may dominate the signal, making classifier selection unreliable.

## Foundational Learning

- Concept: Differential Privacy and Laplace Mechanism
  - Why needed here: The entire approach relies on providing formal privacy guarantees through differential privacy, specifically using the Laplace mechanism for accuracy computation.
  - Quick check question: What is the relationship between the privacy parameter ε, the sensitivity of a function, and the scale of Laplace noise added?

- Concept: AdaBoost Algorithm and Weak Learners
  - Why needed here: The algorithm builds upon AdaBoost's framework of combining weak learners, but with novel modifications for the partially private setting.
  - Quick check question: In standard AdaBoost, how are weak learner weights αt computed, and why does the modified version use a different formula?

- Concept: Sensitivity Analysis
  - Why needed here: The privacy analysis requires bounding the sensitivity of accuracy computation to determine appropriate noise levels.
  - Quick check question: How is the ℓ1-sensitivity of a function defined, and how is it used to determine the scale of Laplace noise in the Laplace mechanism?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Feature splitting -> Classifier generation -> Accuracy computation -> Ensemble building -> Privacy wrapper

- Critical path:
  1. Preprocess data and split into public/private features
  2. For each iteration:
     - Train public classifier on public features
     - Generate random linear classifier for private features
     - Compute and perturb accuracies
     - Select and weight the better classifier
     - Update weights for next iteration
  3. Combine classifiers into final ensemble
  4. Validate privacy guarantees

- Design tradeoffs:
  - Random vs learned private classifiers: Random generation avoids weight perturbation but may sacrifice some accuracy
  - Noise level vs privacy budget: Higher privacy requires more noise, potentially degrading performance
  - Number of iterations vs convergence: More iterations may improve accuracy but increase computational cost and potential overfitting

- Failure signatures:
  - Accuracy plateaus early: May indicate insufficient signal in public features or too much noise in private classifier selection
  - High variance across runs: Could suggest inadequate privacy budget or unstable random classifier generation
  - Performance worse than public-only baseline: May indicate poor parameter tuning or inappropriate feature split

- First 3 experiments:
  1. Run with large ε (e.g., 10) to verify the algorithm works without privacy constraints and establishes baseline performance
  2. Test with moderate ε (e.g., 0.1) and default parameters to evaluate practical privacy-accuracy tradeoff
  3. Vary the number of iterations T while keeping other parameters fixed to study convergence behavior and optimal iteration count

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does boosting random linear classifiers work so effectively in AdaBoost?
- Basis in paper: [explicit] The paper mentions that AdaBoost can generalize well even with weak classifiers, and they exploit this by using random classifiers in BRC. They state "AdaBoost can somewhat magically generalize even with weak classifiers; see [26] and citations within for a thorough discussion."
- Why unresolved: The paper doesn't provide a formal proof or explanation for why randomly generated classifiers work effectively in AdaBoost, only empirical observations and intuition.
- What evidence would resolve it: A formal theoretical analysis showing why random classifiers can be effective weak learners in AdaBoost, or empirical evidence demonstrating the conditions under which this approach succeeds or fails.

### Open Question 2
- Question: How can BRC be extended to handle non-linear classification with partially private features?
- Basis in paper: [inferred] The paper mentions that "BRC always produces linear classifiers" and suggests that "For non-linear classification with partially private features, we may need an approach not based on boosting."
- Why unresolved: The current BRC framework is specifically designed for linear classification and relies on the structure of AdaBoost. Extending it to non-linear classification would require a different algorithmic approach that maintains differential privacy while handling non-linear decision boundaries.
- What evidence would resolve it: Development of a non-linear classification algorithm that incorporates differential privacy for partially private features, along with empirical evaluation showing its effectiveness compared to existing methods.

### Open Question 3
- Question: How can the framework handle scenarios where labels are private rather than public?
- Basis in paper: [explicit] The paper notes that "Algorithm 2 implicitly assumes the label is public knowledge" and states that "Our framework could be extended when the labels are private by adding noise to that classifier."
- Why unresolved: The current algorithm trains the public classifier without noise because labels are assumed public. When labels are private, this step would need modification to maintain differential privacy, but the paper doesn't provide a concrete solution.
- What evidence would resolve it: A modified version of BRC that incorporates label privacy, along with theoretical analysis proving its differential privacy guarantees and empirical evaluation showing its performance.

## Limitations

- The approach assumes linear separability or near-separability of the data, which may not hold for complex real-world datasets
- Performance heavily depends on the quality of public features, with no analysis of feature correlation between public and private domains
- The random classifier generation may struggle with high-dimensional feature spaces or datasets with subtle patterns

## Confidence

- Privacy guarantees and theoretical analysis: High
- Empirical performance claims: Medium
- Generalization of random classifier approach: Medium

## Next Checks

1. Test BRC on synthetic datasets with varying degrees of feature correlation between public and private features to understand the impact of feature dependency on performance
2. Conduct ablation studies varying the distribution of random classifier parameters (currently uniform [-1,1]) to determine optimal sampling strategies
3. Evaluate performance on high-dimensional datasets to assess scalability and identify potential breakdown points for the random classifier approach