---
ver: rpa2
title: Tuning-Free Maximum Likelihood Training of Latent Variable Models via Coin
  Betting
arxiv_id: '2305.14916'
source_url: https://arxiv.org/abs/2305.14916
tags:
- learning
- coin
- svgd
- rate
- soul
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two new particle-based algorithms for training
  latent variable models via marginal maximum likelihood estimation. The first, SVGD
  EM, is based on discretizing a Stein variational gradient flow of the free energy
  functional, while the second, Coin EM, uses coin betting techniques from convex
  optimization to achieve tuning-free training.
---

# Tuning-Free Maximum Likelihood Training of Latent Variable Models via Coin Betting

## Quick Facts
- arXiv ID: 2305.14916
- Source URL: https://arxiv.org/abs/2305.14916
- Reference count: 40
- Key outcome: Introduces SVGD EM and Coin EM algorithms for tuning-free training of latent variable models, validated across high-dimensional settings

## Executive Summary
This paper presents two novel particle-based algorithms for marginal maximum likelihood estimation in latent variable models. SVGD EM discretizes a Stein variational gradient flow of the free energy functional, while Coin EM leverages coin betting techniques from online convex optimization to achieve fully tuning-free training. Both methods are theoretically motivated and empirically validated on synthetic and real-world datasets, demonstrating competitive performance against existing particle-based approaches while removing the need for hyperparameter tuning.

## Method Summary
The paper introduces two algorithms for training latent variable models via marginal maximum likelihood estimation. SVGD EM discretizes a Wasserstein gradient flow of the free energy functional, updating parameters with Euclidean gradients and particles with Stein variational updates. Coin EM applies coin betting to optimize both parameters and particles without learning rates by maintaining "wealth" values that adapt based on past gradients. Both methods can be run in marginal variants when the M-step is tractable. The algorithms are validated on hierarchical models, Bayesian logistic regression, and VAEs, showing competitive performance without hyperparameter tuning.

## Key Results
- Coin EM achieves tuning-free convergence on high-dimensional hierarchical models (dz=100) without any learning rate selection
- Both SVGD EM and Coin EM demonstrate competitive performance with PGD, PMGD, and SOUL across multiple datasets
- The marginal variants provide computational advantages when the M-step is tractable, with similar convergence behavior to full algorithms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Coin EM converges without learning rates by using coin betting to directly optimize over the free energy functional
- **Mechanism:** Coin betting maintains a sequence of "wealth" values that adapt automatically based on past gradients. In Coin EM, the θ updates use a betting fraction derived from the running average of past parameter gradients, while the particle updates use a kernelized betting fraction based on past Wasserstein gradients
- **Core assumption:** The sequence of gradients has bounded magnitude or can be adaptively normalized
- **Evidence anchors:** Abstract states inspiration from coin betting techniques; section 2.4 describes reduction to two coin betting games
- **Break condition:** If gradients become unbounded or the betting fraction becomes unstable, convergence can fail

### Mechanism 2
- **Claim:** SVGD EM decreases the free energy at each iteration under suitable learning rate conditions
- **Mechanism:** SVGD EM discretizes a gradient flow of the free energy functional combining Euclidean gradients for parameters and Wasserstein gradients for particle distributions. The descent lemma shows that with appropriate learning rate γ, the free energy decreases by a quantity proportional to the squared norm of parameter gradients plus kernelized Wasserstein gradients
- **Core assumption:** The learning rate γ satisfies γ < (α-1)/(αBC²) where B bounds kernel norms and C bounds Wasserstein gradient norms
- **Evidence anchors:** Abstract mentions descent lemma guarantee; Theorem 1 in section 2.3 states the descent inequality with explicit learning rate condition
- **Break condition:** If γ is too large, the algorithm becomes unstable; if too small, convergence is slow

### Mechanism 3
- **Claim:** The marginal variants of SVGD EM and Coin EM can be used when the M-step is tractable
- **Mechanism:** When θ*(μ) = arg maxθ F(θ,μ) has a closed form, the algorithms update only the particle positions using exact parameter values rather than approximate gradient steps. This removes one source of approximation error
- **Core assumption:** The M-step mapping μ ↦ θ*(μ) is computationally tractable and differentiable
- **Evidence anchors:** Section C describes marginal variants for models with tractable M-steps
- **Break condition:** If the M-step becomes intractable or the mapping is not smooth, the marginal algorithms break down

## Foundational Learning

- **Concept:** Free energy functional and its connection to marginal maximum likelihood estimation
  - Why needed here: The entire algorithm design is based on viewing MLE as minimization of F(θ,μ), so understanding this connection is fundamental to modifying or extending the methods
  - Quick check question: Can you write the free energy functional for a simple latent variable model and explain how coordinate descent on it relates to EM?

- **Concept:** Wasserstein gradient flows and Stein variational gradient descent
  - Why needed here: SVGD EM is built on the discretization of a Wasserstein gradient flow, so understanding the continuous-time dynamics and their discretization is crucial for algorithm stability and convergence analysis
  - Quick check question: How does replacing the Wasserstein gradient with its image under the integral operator lead to the kernelized updates in SVGD EM?

- **Concept:** Coin betting and parameter-free online learning
  - Why needed here: Coin EM's key innovation is using coin betting to remove learning rates, so understanding the betting fraction update rule and its convergence properties is essential for implementing and debugging the algorithm
  - Quick check question: What is the betting fraction in coin betting, and how does it adapt based on past outcomes to achieve parameter-free convergence?

## Architecture Onboarding

- **Component map:** Free energy evaluator -> Gradient estimator -> Coin betting engine (for Coin EM) / Learning rate controller (for SVGD EM) -> Parameter update -> Particle update -> Kernel machinery
- **Critical path:** For each iteration: (1) Compute parameter gradient using current particles, (2) Update θ using coin betting or fixed learning rate, (3) Compute particle gradient using updated θ, (4) Update particles using coin betting or SVGD-like rule, (5) Repeat
- **Design tradeoffs:** 
  - SVGD EM offers faster convergence when learning rate is well-tuned but requires careful tuning
  - Coin EM removes tuning burden at potential cost of slower initial convergence
  - Marginal variants trade computational simplicity for model restrictions
  - Particle count affects approximation quality vs computational cost
- **Failure signatures:**
  - Parameter divergence indicates learning rate too large (SVGD EM) or unstable betting fractions (Coin EM)
  - Slow convergence suggests learning rate too small or insufficient particles
  - Poor posterior approximation indicates inadequate kernel bandwidth or particle diversity
  - Numerical instability in kernel operations suggests scaling issues
- **First 3 experiments:**
  1. Implement Coin EM on the toy hierarchical model with dz=100, θ=1, N=10 particles, and verify it converges to the true parameter without any learning rate tuning
  2. Compare SVGD EM and Coin EM on the Bayesian logistic regression model, testing different learning rates for SVGD EM to demonstrate tuning sensitivity
  3. Test marginal variants on a model with tractable M-step, comparing convergence speed to full algorithms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we establish theoretical convergence rates and guarantees for Coin EM in both the population limit and finite particle settings?
- Basis in paper: The authors explicitly state this as an open problem regarding extending results to finite particle case and establishing convergence rates
- Why unresolved: The paper focuses on descent lemma for SVGD EM and empirical performance, but theoretical convergence guarantees for Coin EM remain open due to complexity of analyzing coin betting dynamics in particle-based variational inference
- What evidence would resolve it: Rigorous mathematical proof establishing finite-time convergence rates or asymptotic convergence guarantees for Coin EM, building on techniques for analyzing other coin betting algorithms or SVGD methods

### Open Question 2
- Question: Can the computational complexity of O(N²) per update for both algorithms be reduced to enable scaling to very large numbers of particles?
- Basis in paper: Authors note this as a limitation preventing use with very large numbers of particles
- Why unresolved: Current implementation requires pairwise interactions between all particles, leading to quadratic scaling. While standard for particle-based methods, fundamental algorithmic improvements may be possible
- What evidence would resolve it: Development of approximation scheme or algorithmic modification that reduces computational complexity while maintaining competitive performance, or proof that such improvement is impossible without sacrificing accuracy

### Open Question 3
- Question: How does the performance of Coin EM and SVGD EM compare to alternative tuning-free methods for training latent variable models, such as amortized variational inference or normalizing flows?
- Basis in paper: Experimental evaluation is limited to comparisons with particle-based methods, leaving open comparison to broader landscape of tuning-free approaches
- Why unresolved: Paper focuses on particle-based methods, not systematically comparing against other modern tuning-free approaches
- What evidence would resolve it: Comprehensive experimental comparisons on benchmark datasets against range of alternative methods including amortized VI, normalizing flows, and other learning-rate-free optimization techniques

## Limitations

- Both algorithms have O(N²) computational complexity per update, limiting scalability to large particle counts
- Theoretical convergence guarantees are established for SVGD EM but remain open for Coin EM in both population and finite particle settings
- Performance depends critically on kernel choice and bandwidth, which require careful selection despite overall tuning-free approach

## Confidence

- **High Confidence:** Fundamental connection between free energy minimization and MLE, basic SVGD update rules, and coin betting framework for online learning
- **Medium Confidence:** Specific adaptations of coin betting to Wasserstein setting, descent lemma conditions for SVGD EM, and practical performance on benchmark datasets
- **Low Confidence:** Stability of betting fractions in high-dimensional settings, generalization of marginal variants to arbitrary latent variable models, and impact of kernel choice on long-term convergence

## Next Checks

1. **Gradient Norm Analysis:** Systematically measure and visualize the gradient norms during training to verify boundedness assumptions and identify potential instability regions

2. **Kernel Sensitivity Study:** Perform ablation studies varying kernel bandwidth and type to quantify their impact on convergence speed and final performance

3. **High-Dimensional Scaling:** Test algorithms on synthetic models with increasing latent dimensionality (dz > 1000) to evaluate claimed robustness to high-dimensional settings and identify breaking points