---
ver: rpa2
title: How do languages influence each other? Studying cross-lingual data sharing
  during LM fine-tuning
arxiv_id: '2305.13286'
source_url: https://arxiv.org/abs/2305.13286
tags:
- samples
- language
- test
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies cross-lingual data sharing in multilingual language
  models using TracIn, a training data attribution method. The authors analyze how
  much models rely on data from different languages when making predictions for a
  particular test language.
---

# How do languages influence each other? Studying cross-lingual data sharing during LM fine-tuning

## Quick Facts
- **arXiv ID**: 2305.13286
- **Source URL**: https://arxiv.org/abs/2305.13286
- **Reference count**: 34
- **Key outcome**: This paper studies cross-lingual data sharing in multilingual language models using TracIn, a training data attribution method. The authors analyze how much models rely on data from different languages when making predictions for a particular test language. They find that models rely on data from multiple languages, even when the test language was seen during fine-tuning. This reliance increases as fine-tuning progresses, and languages can both reinforce and complement the knowledge acquired from the test language's data. Additionally, the authors find that cross-lingual sharing behavior differs between zero-shot testing and fine-tuning scenarios.

## Executive Summary
This paper investigates how multilingual language models (MLLMs) share and leverage information across languages during fine-tuning. Using TracIn, a training data attribution method, the authors quantify the influence of training samples from different languages on model predictions for a given test language. They find that MLLMs rely on data from multiple languages from the early stages of fine-tuning, and this reliance gradually increases as fine-tuning progresses. The study reveals that languages can both reinforce and complement each other's knowledge, depending on the task and data characteristics. The authors also demonstrate that cross-lingual sharing behavior differs between zero-shot testing and fine-tuning scenarios, providing insights into the dynamics of multilingual learning.

## Method Summary
The authors fine-tune XLM-R base models on multilingual datasets (XNLI, PAWS-X, MARC) using concatenated samples from five languages per task. They then use TracIn to compute influence scores between test samples and all training samples, identifying the top 100 most influential training samples for each test language. By analyzing the distribution of languages among these influential samples, they quantify cross-lingual influence and distinguish between reinforcing (translations) and complementary information sharing. The analysis is conducted across different fine-tuning epochs to track how cross-lingual sharing evolves during training.

## Key Results
- MLLMs rely on data from multiple languages during fine-tuning, not just the target language
- Cross-lingual influence increases as fine-tuning progresses
- Languages can both reinforce and complement each other's knowledge, depending on the task
- Cross-lingual sharing behavior differs between zero-shot testing and fine-tuning scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual models learn language-agnostic representations that are useful for zero-shot transfer.
- Mechanism: During joint training, the model encounters patterns that are common across languages, leading to shared latent spaces where language-specific details are abstracted away.
- Core assumption: Language-agnostic components exist and can be isolated from language-specific components in the representation space.
- Evidence anchors:
  - [abstract] "Multilingual joint learning is often motivated by the idea that when multilingual large language models (MLLMs) learn information for multiple languages simultaneously, they can detect and leverage common universal patterns across them."
  - [section 2.2] "Singh et al. (2019) find that mBERT representations can be partitioned by language subspaces, suggesting that little cross-lingual sharing emerges. Yet, other works show that mBERT representations can be split into a language-specific component, and a language-neutral component that facilitates cross-lingual sharing."
  - [corpus] "Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets" suggests multilingual models can leverage source language information for target languages.
- Break condition: If language-specific components dominate the representation space, the model will not benefit from shared universal patterns.

### Mechanism 2
- Claim: Cross-lingual sharing increases as fine-tuning progresses, indicating dynamic adaptation of representations.
- Mechanism: As the model fine-tunes on multiple languages, it gradually adjusts its parameters to better capture cross-lingual relationships, leading to increased reliance on data from other languages.
- Core assumption: The model's parameter space can be dynamically adjusted to capture cross-lingual relationships during fine-tuning.
- Evidence anchors:
  - [abstract] "We find that MLLMs rely on data from multiple languages from the early stages of fine-tuning and that this reliance gradually increases as fine-tuning progresses."
  - [section 8.2] "We plot for each language which percentage of samples coming from itself were included in the top 100 most influential samples across different fine-tuning epochs. From this, we see that for both tasks, the languages start relying less on their own fine-tuning data after fine-tuning epoch 2."
  - [corpus] "Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space" suggests language distance affects cross-lingual transfer performance.
- Break condition: If the model's representations become too specialized for individual languages, cross-lingual sharing will decrease.

### Mechanism 3
- Claim: Languages can both reinforce and complement each other's knowledge, depending on the task and data.
- Mechanism: When languages share similar content, they reinforce each other's knowledge. When languages provide different perspectives, they complement each other's knowledge.
- Core assumption: The model can distinguish between reinforcing and complementary information from different languages.
- Evidence anchors:
  - [abstract] "We further study how different fine-tuning languages influence model performance on a given test language and find that they can both reinforce and complement the knowledge acquired from data of the test language itself."
  - [section 8.1] "We report these percentages in Table 4, and find that for XNLI, over half of the contributions from different languages are translations of the most influential samples from the respective test language, indicating that the model largely benefits from reinforcing data from other languages. For PAWS-X, this is not the case, indicating that here the biggest benefit of cross-lingual sharing can more likely be attributed to the model learning to pick up on new, complementary information from other languages."
  - [corpus] "Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs" suggests opinions may or may not transfer between languages.
- Break condition: If the model cannot distinguish between reinforcing and complementary information, it will not benefit from cross-lingual sharing.

## Foundational Learning

- Concept: Gradient-based influence methods
  - Why needed here: To quantify the influence of training samples from different languages on the model's predictions for a given test language.
  - Quick check question: How does TracIn approximate the influence of a training sample on a test sample?

- Concept: Cross-lingual transfer
  - Why needed here: To understand how the model leverages information from one language to improve its performance on another language.
  - Quick check question: What are the potential benefits and limitations of cross-lingual transfer in multilingual models?

- Concept: Language representation subspaces
  - Why needed here: To understand how the model represents and processes information from different languages.
  - Quick check question: How do language-specific and language-neutral components in the representation space contribute to cross-lingual sharing?

## Architecture Onboarding

- Component map: XLM-R base model -> MLP classification head -> TracIn influence computation
- Critical path: 1) Fine-tune the model on multilingual data. 2) Compute influence scores using TracIn. 3) Analyze the influence scores to understand cross-lingual sharing.
- Design tradeoffs: The choice of tasks (XNLI, PAWS-X, MARC) affects the nature of cross-lingual sharing observed. The number of training samples and test samples affects the computational cost and statistical significance of the results.
- Failure signatures: If the influence scores are dominated by outliers, the analysis will be biased. If the tasks are too simple, the model may not learn meaningful cross-lingual representations.
- First 3 experiments:
  1. Fine-tune the model on XNLI data and compute influence scores for a subset of test samples.
  2. Analyze the influence scores to identify the most influential training samples and their languages.
  3. Repeat steps 1-2 for PAWS-X and MARC to compare cross-lingual sharing across tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanisms that allow some languages to act as "complementary" sources of information rather than just reinforcing what the model learns from the test language's data?
- Basis in paper: [explicit] The paper explicitly identifies that languages can both reinforce and complement the knowledge acquired from the test language's data, and distinguishes between reinforcing (translations) and complementary samples.
- Why unresolved: While the paper identifies the existence of complementary information sharing, it does not investigate the specific linguistic or structural features that make certain languages more likely to provide complementary information rather than just reinforcing existing knowledge.
- What evidence would resolve it: A detailed linguistic analysis comparing the types of knowledge shared by reinforcing vs. complementary language pairs, potentially including feature-level analyses of what grammatical, semantic, or syntactic information is being transferred.

### Open Question 2
- Question: How does the cross-lingual sharing behavior change when fine-tuning on more diverse tasks beyond classification, such as sequence labeling or generation tasks?
- Basis in paper: [inferred] The paper acknowledges that TracIn operates on the sequence-level and is most suitable for classification and regression tasks, and explicitly speculates that cross-lingual sharing might exhibit different behavior for tasks where language-specific information plays a bigger role.
- Why unresolved: The study is limited to classification tasks due to computational constraints and the nature of TracIn, leaving open whether the observed patterns of cross-lingual sharing generalize to other NLP tasks.
- What evidence would resolve it: Extending the analysis to sequence labeling or generation tasks using methods that can handle segment-level analysis, and comparing the patterns of cross-lingual data reliance across different task types.

### Open Question 3
- Question: What are the long-term effects of language data imbalance on cross-lingual sharing patterns, beyond the immediate fine-tuning period?
- Basis in paper: [explicit] The paper investigates how data imbalance during fine-tuning affects cross-lingual influence, showing that over-represented languages exert more influence, but notes this trend doesn't always steadily increase.
- Why unresolved: The study only examines immediate effects of data imbalance during the fine-tuning process, without investigating whether these patterns persist in the model's long-term behavior or transfer to other tasks.
- What evidence would resolve it: Longitudinal studies tracking cross-lingual sharing patterns after fine-tuning across multiple tasks, and experiments varying the degree and duration of data imbalance to identify threshold effects.

## Limitations
- Focus on only three datasets and five languages per task may limit generalizability
- Limited sample size (25 test samples per language) raises questions about statistical robustness
- Does not address potential confounding factors such as language similarity or dataset-specific characteristics
- TracIn with limited checkpoints may miss important dynamics between checkpoint intervals

## Confidence

**High Confidence:**
- Models rely on data from multiple languages during fine-tuning, not just the target language
- Cross-lingual influence increases as fine-tuning progresses
- The nature of cross-lingual sharing differs between tasks (reinforcement vs. complementarity)

**Medium Confidence:**
- The observed patterns will generalize to other multilingual tasks and language pairs
- TracIn accurately captures the influence of training data across languages
- The influence scores are not dominated by outliers or artifacts of the method

**Low Confidence:**
- The specific percentages of cross-lingual influence will remain stable across different model sizes or architectures
- The identified reinforcing vs. complementary relationships are task-invariant properties

## Next Checks

1. **Statistical Validation**: Replicate the analysis with larger sample sizes (e.g., 100+ test samples per language) to verify that the observed cross-lingual sharing patterns remain statistically significant and stable across different sample selections.

2. **Methodological Robustness**: Apply alternative influence estimation methods (such as Representer Point Selection or GradientÃ—Activation) to verify that the observed cross-lingual sharing patterns are not artifacts of TracIn's specific approximation.

3. **Generalization Testing**: Extend the analysis to additional language pairs with varying degrees of linguistic similarity (e.g., including languages from different families or scripts) to determine whether the observed cross-lingual sharing patterns hold across diverse linguistic contexts.