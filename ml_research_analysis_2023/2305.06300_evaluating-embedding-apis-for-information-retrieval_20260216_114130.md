---
ver: rpa2
title: Evaluating Embedding APIs for Information Retrieval
arxiv_id: '2305.06300'
source_url: https://arxiv.org/abs/2305.06300
tags:
- apis
- retrieval
- embedding
- bm25
- cohere
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates the effectiveness of semantic embedding APIs
  for information retrieval tasks, specifically domain generalization and multilingual
  retrieval. The authors assess three APIs (OpenAI, Cohere, and Aleph-Alpha) on the
  BEIR and MIRACL benchmarks.
---

# Evaluating Embedding APIs for Information Retrieval

## Quick Facts
- arXiv ID: 2305.06300
- Source URL: https://arxiv.org/abs/2305.06300
- Reference count: 11
- Key outcome: Re-ranking BM25 results using embedding APIs is a cost-effective approach for information retrieval, with OpenAI's ada2 model performing best on average.

## Executive Summary
This paper evaluates semantic embedding APIs (OpenAI, Cohere, Aleph-Alpha) for information retrieval tasks, focusing on domain generalization and multilingual retrieval. The authors assess these APIs on the BEIR and MIRACL benchmarks, comparing their performance against BM25 and open-source models. They find that re-ranking BM25 results using embedding APIs is a budget-friendly approach, with OpenAI's ada2 model performing best on average. For multilingual retrieval, hybrid models combining BM25 and APIs yield the best results, with APIs excelling on low-resource languages while open-source models perform better on high-resource languages.

## Method Summary
The study evaluates embedding APIs as re-rankers on top-100 BM25 results for the BEIR and MIRACL benchmarks. For BEIR, 18 datasets across 9 domains are used, with nDCG@10 and Recall@100 as metrics. For MIRACL, 18 languages with 725K relevance judgments are evaluated. The APIs are compared against baselines (TASB, mDPR) and hybrid models (Cohere+BM25, mDPR+BM25). The reproduction plan involves indexing corpora using BM25, retrieving top-100 passages, encoding with APIs, re-ranking based on similarity, and computing effectiveness metrics.

## Key Results
- Re-ranking BM25 results using embedding APIs is a cost-effective approach for information retrieval
- OpenAI's ada2 model performs best on average for domain generalization tasks
- Hybrid models combining BM25 and embedding APIs yield the best results for multilingual retrieval

## Why This Works (Mechanism)

### Mechanism 1
Re-ranking BM25 results using embedding APIs is more cost-effective than full ranking. Embedding APIs can encode queries and retrieved passages more efficiently than encoding entire document collections. By first using BM25 to retrieve a smaller candidate set, the API only needs to process a manageable number of passages per query, significantly reducing token consumption and cost. The core assumption is that the re-ranked results from API-enhanced BM25 are sufficiently better than pure BM25 to justify the additional API calls.

### Mechanism 2
Hybrid models combining BM25 and embedding APIs outperform standalone API usage in multilingual retrieval. BM25 excels at lexical matching in high-resource languages, while embedding APIs provide semantic understanding, especially in low-resource languages. Combining both methods leverages their respective strengths and compensates for individual weaknesses. The core assumption is that the interpolation of BM25 and API scores is additive and improves overall retrieval effectiveness across language resources.

### Mechanism 3
Embedding APIs are particularly effective on low-resource languages compared to open-source models. Commercial embedding APIs are trained on large, diverse datasets that include low-resource languages, providing better semantic representations than smaller, open-source models that may lack sufficient training data for these languages. The core assumption is that the training data for commercial APIs includes adequate representation of low-resource languages to enable effective embeddings.

## Foundational Learning

- Concept: Dense Retrieval with Bi-Encoders
  - Why needed here: Understanding how semantic embedding APIs function as bi-encoders is crucial for implementing re-ranking strategies and hybrid models.
  - Quick check question: What is the primary difference between a bi-encoder and a cross-encoder in dense retrieval?

- Concept: BM25 Retrieval
  - Why needed here: BM25 is used as a first-stage retriever in the experiments; understanding its operation is essential for implementing and tuning the re-ranking pipeline.
  - Quick check question: How does BM25 calculate the relevance score between a query and a document?

- Concept: Multilingual Information Retrieval
  - Why needed here: The paper evaluates APIs across multiple languages; knowledge of multilingual retrieval challenges and strategies is necessary to interpret the results.
  - Quick check question: What are the main challenges in building retrieval models that work effectively across multiple languages?

## Architecture Onboarding

- Component map: Data Collection → BM25 Index Construction → BM25 Retrieval (Top-100) → Embedding API Encoding (Queries + Passages) → Similarity Scoring → Re-ranking → Evaluation
- Critical path: The critical path for re-ranking is: BM25 Retrieval → Embedding API Encoding → Similarity Scoring → Re-ranking. Latency is dominated by API call time, which is around 400ms per call in bulk mode.
- Design tradeoffs: Cost vs. Effectiveness: Full ranking with APIs is more effective but significantly more expensive; re-ranking is cheaper but may miss relevant documents not in the BM25 top-100. Language Resource: Hybrid models are best for multilingual retrieval but increase complexity and cost.
- Failure signatures: Poor re-ranking effectiveness may indicate that the API is not well-suited for the domain or language, or that the BM25 top-100 is too restrictive. High costs may suggest inefficient use of API calls, such as not batching requests or using full ranking unnecessarily.
- First 3 experiments:
  1. Implement BM25 retrieval on a small dataset and evaluate nDCG@10.
  2. Add OpenAI's ada2 re-ranking on top of BM25 and compare nDCG@10.
  3. Construct a hybrid model with BM25 and ada2 for a multilingual dataset and evaluate nDCG@10.

## Open Questions the Paper Calls Out

### Open Question 1
How do embedding APIs perform on tasks requiring deep semantic understanding versus tasks relying on lexical matching? The paper finds that embedding APIs struggle on datasets collected based on lexical matching (e.g., BioASQ, Signal-1M) but perform reasonably well on most other domains. This question remains unresolved because the paper does not provide a detailed analysis of why certain tasks are more challenging for embedding APIs or what specific characteristics of these tasks contribute to their performance differences. A comprehensive analysis of task characteristics and their correlation with embedding API performance would provide insights into the strengths and limitations of these APIs.

### Open Question 2
How does the performance of embedding APIs vary across different languages, and what factors contribute to these variations? The paper shows that embedding APIs perform better on low-resource languages compared to high-resource languages in multilingual retrieval tasks. This question remains unresolved because the paper does not explore the underlying reasons for the performance differences across languages, such as the availability of training data, linguistic characteristics, or the impact of pre-training on specific languages. A detailed analysis of the linguistic features of different languages, their representation in the training data, and the impact on embedding API performance would help understand the factors influencing cross-lingual effectiveness.

### Open Question 3
How do embedding APIs handle fairness and bias in information retrieval tasks? The paper mentions that safe deployment of retrieval systems requires evaluation of fairness, but does not delve into this aspect of API evaluation. This question remains unresolved because the paper does not assess the fairness of embedding APIs in relation to protected groups or explore the potential biases that may arise from the training data or model architecture. A thorough evaluation of embedding APIs on fairness metrics would provide insights into their potential biases and the need for fairness-aware design or post-processing techniques.

## Limitations

- The evaluation framework relies on a limited set of commercial APIs and excludes several emerging embedding providers
- The study focuses primarily on re-ranking rather than full ranking strategies, potentially missing performance gains available through alternative architectures
- Cost comparisons do not account for potential variations in API performance under different load conditions or long-term availability concerns

## Confidence

- **High Confidence**: The comparative effectiveness of re-ranking BM25 results using embedding APIs is well-supported by the experimental results across multiple datasets.
- **Medium Confidence**: The claim about API superiority on low-resource languages is supported by the MIRACL results but would benefit from additional low-resource language datasets for validation.
- **Low Confidence**: The assertion that open-source models work better on high-resource languages is made but not extensively validated across different high-resource language combinations.

## Next Checks

1. Cross-Validation with Alternative Benchmarks: Test the same API models on additional multilingual IR benchmarks (e.g., CLEF datasets) to verify consistency of low-resource language performance claims.

2. Cost-Performance Trade-off Analysis: Conduct experiments varying the re-ranking cutoff (top-50, top-200) to quantify the diminishing returns and identify optimal budget allocation points.

3. Temporal Stability Assessment: Re-run key experiments after a 3-6 month interval to measure API performance drift and assess the stability of observed effectiveness patterns over time.