---
ver: rpa2
title: Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning
arxiv_id: '2310.12921'
source_url: https://arxiv.org/abs/2310.12921
tags:
- reward
- clip
- tasks
- task
- humanoid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We use vision-language models (VLMs) as zero-shot reward models
  for reinforcement learning. Our method, VLM-RMs, uses CLIP to provide rewards from
  natural language prompts describing desired tasks.
---

# Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning

## Quick Facts
- arXiv ID: 2310.12921
- Source URL: https://arxiv.org/abs/2310.12921
- Reference count: 38
- One-line primary result: Using CLIP to provide rewards from natural language prompts enables zero-shot RL for complex tasks like humanoid kneeling and splits

## Executive Summary
This paper introduces VLM-RMs, a method for using vision-language models (VLMs) as zero-shot reward models in reinforcement learning. The approach leverages CLIP's ability to encode images and text into a shared embedding space, using cosine similarity between state embeddings and task descriptions as a reward signal. By training MuJoCo humanoid agents to perform complex tasks using only single-sentence prompts with minimal engineering, the authors demonstrate that larger VLMs trained with more compute and data are significantly better at providing useful reward signals. The method is particularly effective for tasks that are difficult to specify with manual reward functions.

## Method Summary
VLM-RMs use CLIP to compute rewards by comparing the embedding of the current environment state to the embedding of a text prompt describing the desired task. The method employs goal-baseline regularization to project state embeddings onto the direction between baseline and task embeddings, removing irrelevant features. The approach is tested on MuJoCo humanoid tasks like kneeling, lotus position, and splits, with rewards computed from simple text prompts. The method is evaluated using human assessment of task success and comparison to ground truth rewards using EPIC distance.

## Key Results
- Larger VLMs trained with more compute and data provide better reward signals
- The largest CLIP model successfully learned all tested tasks using only single-sentence prompts
- Goal-baseline regularization can improve reward model quality but has limited benefit for humanoid tasks
- VLM-RMs are effective for tasks difficult to specify with manual reward functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision-language models can directly compute reward signals by comparing image embeddings to language prompts.
- Mechanism: CLIP encodes images and text into a shared embedding space. The cosine similarity between a current state's image embedding and a task description embedding provides a zero-shot reward signal.
- Core assumption: The VLM's embedding space preserves semantic similarity between visual states and textual descriptions of tasks.
- Evidence anchors:
  - [abstract] "We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs."
  - [section] "We use cos-similarity between the CLIP embedding of the current environment state and a simple language prompt as a reward function."
  - [corpus] Weak evidence - related work focuses on vision-language models in RL but lacks direct mechanistic detail on embedding similarity as reward.
- Break condition: If the VLM's embedding space doesn't preserve semantic similarity between visual states and textual task descriptions, the reward signal will be meaningless.

### Mechanism 2
- Claim: Goal-baseline regularization improves reward model quality by projecting out irrelevant features.
- Mechanism: By providing a baseline prompt describing a neutral state, the method projects state embeddings onto the direction between baseline and task embeddings, removing components orthogonal to task-relevant information.
- Core assumption: The direction from baseline to task embedding captures all relevant information about task completion.
- Evidence anchors:
  - [section] "We obtain the goal-baseline regularized CLIP reward model (RCLIP-Reg) by projecting our state embedding onto the line spanned by the baseline and task embeddings."
  - [section] "The direction from b to g captures the change from the environment's baseline to the target state."
  - [corpus] No direct evidence - related work doesn't discuss baseline regularization for VLMs.
- Break condition: If the baseline-to-task direction doesn't capture all relevant task information, important features will be incorrectly removed.

### Mechanism 3
- Claim: Larger VLMs trained with more compute and data are better reward models.
- Mechanism: As VLM scale increases, the model's ability to discriminate between subtle visual differences improves, leading to better reward signal quality.
- Core assumption: The improvement in VLM capabilities from scale directly translates to improved reward modeling performance.
- Evidence anchors:
  - [abstract] "We find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models."
  - [section] "We detect a clear positive trend between model scale, and the EPIC distance of the reward model from human labels."
  - [corpus] No direct evidence - scaling laws for VLMs as reward models hasn't been studied in related work.
- Break condition: If VLM capabilities improve in ways that don't translate to better reward discrimination, or if other factors become limiting, scaling may stop improving reward quality.

## Foundational Learning

- Concept: Reinforcement learning in POMDPs
  - Why needed here: The method trains RL agents using the VLM-derived rewards, requiring understanding of RL algorithms and POMDP formulation.
  - Quick check question: What are the key components of a POMDP and how do they relate to vision-based RL tasks?

- Concept: Vision-language models and contrastive learning
  - Why needed here: The method relies on CLIP's ability to encode images and text into a shared embedding space using contrastive learning.
  - Quick check question: How does CLIP's contrastive learning objective create meaningful embeddings for both images and text?

- Concept: Reward shaping and specification gaming
  - Why needed here: Using VLMs as reward models introduces new considerations about reward specification and potential for specification gaming.
  - Quick check question: What are the risks of misspecified rewards when using VLMs, and how might agents exploit them?

## Architecture Onboarding

- Component map: Environment → Image renderer → CLIP image encoder → CLIP text encoder → Cosine similarity → Reward signal → RL algorithm (SAC/DQN) → Policy → Actions → Environment
- Critical path: The main computational path is through image rendering, CLIP encoding, and reward computation. Each step must be efficient for real-time RL.
- Design tradeoffs: Zero-shot approach trades potential reward accuracy for sample efficiency and ease of specification. Goal-baseline regularization adds complexity but may improve reward quality.
- Failure signatures: Poor reward signal quality manifests as RL agent not learning or learning incorrect behaviors. This could stem from VLM capability limitations, poor prompt specification, or computational issues.
- First 3 experiments:
  1. Test CLIP reward computation on simple environments with known ground truth rewards (CartPole, MountainCar)
  2. Implement goal-baseline regularization and test on same environments
  3. Train RL agent using VLM reward on simple task and evaluate success rate

This architecture is designed to be a drop-in replacement for traditional reward functions in RL algorithms. The key insight is leveraging pre-trained VLMs' ability to understand natural language task descriptions and map them to visual states, eliminating the need for manual reward engineering or expensive human feedback collection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of VLM-RMs scale beyond the largest publicly available CLIP models (ViT-bigG-14)?
- Basis in paper: [explicit] The paper observes a strong scaling trend with model size and states "future VLMs are likely to be useful as reward models in an even broader range of tasks."
- Why unresolved: The paper only tests up to the largest publicly available CLIP model and notes a sharp phase transition in performance between ViT-H-14 and ViT-bigG-14.
- What evidence would resolve it: Training and evaluating VLM-RMs using even larger CLIP models or custom-trained VLMs to determine if the scaling trend continues.

### Open Question 2
- Question: What are the limitations of goal-baseline regularization in improving VLM-RM performance?
- Basis in paper: [explicit] The paper mentions goal-baseline regularization can improve reward model quality but finds it doesn't help for some humanoid tasks and might be more useful for smaller CLIP models.
- Why unresolved: The paper provides limited analysis on when and why goal-baseline regularization helps or hurts performance.
- What evidence would resolve it: Systematic experiments varying task complexity, VLM model size, and regularization strength to understand the conditions under which goal-baseline regularization is beneficial.

### Open Question 3
- Question: How robust are VLM-RMs to optimization pressure from RL agents, and can they prevent specification gaming?
- Basis in paper: [inferred] The paper discusses potential limitations of VLM-RMs related to misspecification if the text description doesn't capture human intent, and mentions the importance of ensuring robustness and safety in practical applications.
- Why unresolved: The paper doesn't test the robustness of VLM-RMs under optimization pressure or investigate potential specification gaming issues.
- What evidence would resolve it: Experiments training RL agents with VLM-RMs on increasingly complex tasks while monitoring for unintended behaviors or reward hacking.

## Limitations

- Performance depends heavily on the VLM's ability to understand natural language task descriptions, which may fail for complex or nuanced tasks.
- The method hasn't been thoroughly tested for robustness to prompt variations or sensitivity to small changes in task descriptions.
- Potential for specification gaming exists if the VLM's interpretation of task descriptions doesn't align with human intent.

## Confidence

**High Confidence:** The core mechanism of using CLIP's embedding similarity as a reward signal is well-supported by the experimental results, particularly for simple pose-based tasks. The scaling effect with larger VLMs is demonstrated convincingly.

**Medium Confidence:** The effectiveness of goal-baseline regularization and the robustness of VLM-RMs to prompt engineering are supported by experiments, but could benefit from more systematic ablation studies. The method's ability to handle more complex tasks beyond simple poses is plausible but not fully validated.

**Low Confidence:** The paper's claims about VLM-RMs being a general solution for reward specification in RL are premature. The evaluation scope is limited, and potential failure modes like specification gaming or reward misspecification are not thoroughly explored.

## Next Checks

1. **Prompt Robustness Study:** Systematically vary the wording of task prompts while keeping the semantic meaning constant to quantify the sensitivity of VLM-RMs to prompt engineering.

2. **Complex Task Evaluation:** Test VLM-RMs on more complex, temporally-extended tasks that require planning and sequential decision-making, beyond simple static poses.

3. **Reward Misspecification Analysis:** Deliberately construct prompts that could lead to specification gaming and evaluate whether agents exploit these loopholes, comparing VLM-RMs to manually-designed reward functions in terms of susceptibility to such exploits.