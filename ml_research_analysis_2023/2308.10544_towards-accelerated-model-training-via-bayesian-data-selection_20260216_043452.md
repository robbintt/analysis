---
ver: rpa2
title: Towards Accelerated Model Training via Bayesian Data Selection
arxiv_id: '2308.10544'
source_url: https://arxiv.org/abs/2308.10544
tags:
- data
- training
- learning
- selection
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently training deep learning
  models on noisy, duplicated, or biased data by developing a principled data selection
  method. The core idea is to use a lightweight Bayesian treatment of the model and
  zero-shot predictors built on large-scale pre-trained models to estimate the impact
  of each training sample on generalization.
---

# Towards Accelerated Model Training via Bayesian Data Selection

## Quick Facts
- arXiv ID: 2308.10544
- Source URL: https://arxiv.org/abs/2308.10544
- Reference count: 40
- Key outcome: Method accelerates deep learning training on noisy, duplicated, or biased data by selecting the most valuable samples using Bayesian treatment and zero-shot predictors.

## Executive Summary
This paper presents a principled data selection method for accelerating deep learning model training, particularly in scenarios with noisy, duplicated, or biased data. The approach uses a lightweight Bayesian treatment of the model combined with zero-shot predictors from large-scale pre-trained models to estimate the impact of each training sample on generalization. By selecting samples that maximize a theoretically grounded objective, the method significantly reduces training iterations while maintaining or improving final accuracy.

## Method Summary
The method estimates the marginal influence of each training sample on generalization loss using a lower bound of KL divergence between true and predictive distributions. It employs Laplace approximation with last-layer and KFAC approximations to maintain computational efficiency in large networks. Zero-shot predictors built on pre-trained models like CLIP serve as proxies for clean validation distributions, eliminating the need for expensive clean holdout sets. The selection objective balances model uncertainty with alignment to the zero-shot predictor's distribution, with a trade-off coefficient α controlling their relative importance.

## Key Results
- Achieves up to 19% higher final accuracy on WebVision dataset with fewer training iterations
- Outperforms strong baselines including uniform sampling, gradient norm, and RHO-LOSS
- Demonstrates robustness across CIFAR-10, CIFAR-100 with label noise (20%, 50%) and class imbalance (10×, 100×)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bayesian data selection reduces redundant training by estimating a sample's marginal influence on generalization loss.
- Mechanism: Uses lower bound of KL divergence between true and predictive distributions to estimate how much each sample changes generalization, then selects samples that maximize this bound.
- Core assumption: The lower bound is tight enough to preserve the ranking of samples by their true influence on generalization.
- Evidence anchors:
  - [abstract] "examining the data's impact on the model's generalization loss"
  - [section] "a theoretically grounded and universal objective for data selection"
  - [corpus] Weak - related papers focus on sample selection under label noise, not generalization bounds.
- Break condition: If the lower bound becomes too loose, the method may select samples that don't improve generalization.

### Mechanism 2
- Claim: Using zero-shot predictors as proxies for clean holdout data eliminates the need for expensive clean validation sets.
- Mechanism: Large-scale pre-trained models like CLIP provide a distribution that approximates the true data generating distribution, allowing estimation of the second term in the selection objective.
- Core assumption: Pre-trained zero-shot predictors capture general patterns sufficient to approximate the true data distribution for the target task.
- Evidence anchors:
  - [section] "off-the-shelf zero-shot predictors, built upon large-scale pre-trained models [35, 41]"
  - [section] "these models often contain generally applicable information for solving specific downstream tasks"
  - [corpus] Weak - related work focuses on sample selection, not zero-shot predictor integration.
- Break condition: If the pre-trained model's distribution diverges significantly from the target task, the proxy becomes unreliable.

### Mechanism 3
- Claim: Laplace approximation with last-layer and KFAC approximations enables efficient Bayesian treatment in large networks.
- Mechanism: The method converts point estimates to Gaussian posteriors using only last-layer weights, reducing computational burden while maintaining uncertainty estimates.
- Core assumption: Last-layer uncertainty is sufficient to capture the model's predictive uncertainty for data selection purposes.
- Evidence anchors:
  - [section] "we consider combining last-layer and KFAC approximations to reduce the burden"
  - [section] "the last-layer one enjoys a much faster evaluation procedure"
  - [corpus] Weak - related papers focus on sample selection metrics, not approximation methods.
- Break condition: If full-network uncertainty is needed for accurate selection, the approximation fails.

## Foundational Learning

- Concept: Bayesian inference and posterior estimation
  - Why needed here: The method requires estimating posterior distributions over model parameters to compute uncertainty and selection criteria.
  - Quick check question: Can you explain how Bayes' rule updates prior beliefs with observed data to form a posterior?

- Concept: Laplace approximation
  - Why needed here: Provides a computationally efficient way to approximate intractable posteriors with Gaussian distributions.
  - Quick check question: What is the relationship between the Hessian of the log-posterior and the covariance matrix in Laplace approximation?

- Concept: Kronecker-factored approximation (KFAC)
  - Why needed here: Reduces the memory and computational cost of working with large Fisher information matrices in Bayesian neural networks.
  - Quick check question: How does KFAC approximate the Fisher matrix using Kronecker products of layer-wise matrices?

## Architecture Onboarding

- Component map:
  - Data pipeline -> Zero-shot predictor (CLIP) -> Bayesian treatment (Laplace approximation) -> Selection module -> Training loop

- Critical path:
  1. Forward pass through model to get predictions and features
  2. Compute selection objective using Bayesian posterior and zero-shot predictor
  3. Select top-k samples based on objective
  4. Backward pass and parameter update on selected samples
  5. Update posterior statistics (A, G matrices) for next iteration

- Design tradeoffs:
  - Using zero-shot predictors trades accuracy for accessibility of clean validation data
  - Last-layer approximation trades full uncertainty for computational efficiency
  - Selection frequency vs. computational overhead in computing objectives

- Failure signatures:
  - Model converges slowly or gets stuck: Selection objective may be ranking samples poorly
  - Final accuracy plateaus below baseline: Zero-shot predictor may not match true data distribution
  - Training becomes unstable: Posterior approximation may be too loose or too tight

- First 3 experiments:
  1. Run on CIFAR-10 with clean labels to verify basic functionality and compare to uniform sampling
  2. Introduce 10% symmetric label noise to test noise robustness and compare to RHO-LOSS
  3. Test on CIFAR-100 to verify scalability and check if zero-shot predictor still provides benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is the proposed method to the choice of the zero-shot predictor (e.g., CLIP-RN50 vs. CLIP-ViT-B/16) in terms of both training speed and final accuracy?
- Basis in paper: [explicit] The paper shows that using different CLIP backbones (RN50 vs. ViT-B/16) yields similar speedup effects despite different zero-shot accuracies, suggesting robustness to the choice of predictor.
- Why unresolved: While the paper demonstrates that the choice of backbone doesn't significantly impact training speed, it doesn't explore other types of zero-shot predictors or the effect of using a fine-tuned vs. raw zero-shot predictor.
- What evidence would resolve it: Experiments comparing the method's performance with different zero-shot predictors (e.g., CLIP vs. other models, fine-tuned vs. raw) and their impact on training speed and final accuracy.

### Open Question 2
- Question: What is the impact of the trade-off coefficient α on the method's performance across different datasets and noise levels?
- Basis in paper: [explicit] The paper shows that varying α affects training speed and final accuracy, with too large α leading to degraded performance, emphasizing the importance of balancing the zero-shot predictor term.
- Why unresolved: The paper only explores a limited range of α values and doesn't investigate how the optimal α might vary with dataset characteristics or noise levels.
- What evidence would resolve it: A comprehensive study varying α across different datasets and noise levels to determine the optimal range and its relationship to dataset properties.

### Open Question 3
- Question: How does the proposed method perform in scenarios with extreme class imbalance, beyond the 10 and 100 imbalance ratios tested in the paper?
- Basis in paper: [explicit] The paper tests the method on CIFAR-10/100 with imbalance ratios of 10 and 100, showing improved performance compared to baselines, especially as the imbalance ratio increases.
- Why unresolved: The paper doesn't explore scenarios with even higher class imbalance ratios, which are common in real-world applications.
- What evidence would resolve it: Experiments on datasets with higher imbalance ratios (e.g., 1000 or more) to assess the method's scalability and effectiveness in extreme imbalance scenarios.

### Open Question 4
- Question: How does the proposed method's performance compare to other data selection methods when applied to non-image data, such as text or tabular data?
- Basis in paper: [explicit] The paper focuses on image classification tasks and doesn't explore the method's applicability to other data modalities.
- Why unresolved: The method's reliance on a zero-shot predictor and its specific Bayesian treatment may not generalize well to other data types.
- What evidence would resolve it: Experiments applying the method to text classification or tabular data tasks and comparing its performance to other data selection methods in those domains.

## Limitations

- The method relies on approximations (last-layer uncertainty, zero-shot predictor proxies) that may not capture full model complexity
- Performance depends on the quality and alignment of the zero-shot predictor with the target task
- Computational overhead of Bayesian treatment and selection objective may offset acceleration benefits on smaller datasets

## Confidence

**High confidence**: The method's ability to accelerate training on CIFAR datasets with label noise and class imbalance, as demonstrated by empirical results showing up to 19% higher final accuracy on WebVision with fewer training iterations.

**Medium confidence**: The theoretical foundation of the data selection objective using KL divergence bounds, as the paper provides a principled approach but relies on assumptions about the tightness of the lower bound and the quality of the zero-shot predictor proxy.

**Low confidence**: The scalability and effectiveness of the method on highly complex datasets or models beyond those tested, as the paper focuses on CIFAR and WebVision with ResNet-18 and ViT-B/16 architectures.

## Next Checks

1. **Stress test with extreme label noise**: Validate the method's robustness by testing on datasets with 50%+ label noise, comparing selection performance against baselines and measuring the degradation in selection quality.

2. **Cross-domain zero-shot predictor evaluation**: Test the method using zero-shot predictors trained on different domains (e.g., CLIP trained on art images for natural image classification) to assess the impact of domain mismatch on selection performance.

3. **Ablation study on approximation layers**: Evaluate the contribution of the last-layer vs. full-network uncertainty by comparing selection performance when using different levels of Bayesian approximation, and analyze the trade-off between accuracy and computational efficiency.