---
ver: rpa2
title: LoRA ensembles for large language model fine-tuning
arxiv_id: '2310.00035'
source_url: https://arxiv.org/abs/2310.00035
tags:
- lora
- ensembles
- ensemble
- arxiv
- regularization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LoRA ensembles, a method for fine-tuning
  large language models (LLMs) that improves both accuracy and uncertainty quantification.
  The key idea is to construct an ensemble of LLM fine-tuning adapters, where each
  adapter is trained with a different random initialization.
---

# LoRA ensembles for large language model fine-tuning

## Quick Facts
- arXiv ID: 2310.00035
- Source URL: https://arxiv.org/abs/2310.00035
- Authors: 
- Reference count: 20
- This paper introduces LoRA ensembles, a method for fine-tuning large language models (LLMs) that improves both accuracy and uncertainty quantification.

## Executive Summary
This paper addresses the challenge of fine-tuning large language models (LLMs) while maintaining good calibration and uncertainty quantification. The authors propose LoRA ensembles, which construct an ensemble of LLM fine-tuning adapters, each trained with different random initializations using Low-Rank Adapters (LoRA). This approach improves predictive accuracy and calibration on commonsense reasoning tasks while being memory-efficient compared to full model ensembles.

## Method Summary
The method uses Low-Rank Adapters (LoRA) to create an ensemble of LLM fine-tuning adapters, where each adapter is trained with a different random initialization. LoRA works by decomposing the weight update matrix into two low-rank matrices (A and B), reducing the number of trainable parameters significantly. The ensemble makes predictions by averaging the normalized probabilities from each adapter. Regularization techniques like KL regularization, early stopping, and weight decay on the B matrix are used to further improve calibration by constraining adapter updates while maintaining diversity.

## Key Results
- LoRA ensembles consistently improve predictive accuracy and uncertainty quantification across six commonsense reasoning datasets
- Regularization techniques combined with LoRA ensembles further improve both accuracy and calibration
- The method provides better out-of-distribution detection performance compared to baseline approaches
- Memory efficiency is maintained compared to full model ensembles due to LoRA's parameter-efficient nature

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LoRA ensembles improve calibration by averaging predictions across multiple independently fine-tuned LoRA adapters, reducing overconfident predictions.
- **Mechanism:** Each ensemble component uses a different random initialization for the LoRA adapter matrices, creating diverse predictions that average out overconfident errors. The low-rank structure of LoRA ensures the ensemble is memory-efficient compared to full model ensembles.
- **Core assumption:** The diversity introduced by random adapter initialization is sufficient to produce well-calibrated ensemble predictions.

### Mechanism 2
- **Claim:** Regularization techniques combined with LoRA ensembles further improve both accuracy and calibration by keeping ensemble components close to the pre-trained model while maintaining diversity.
- **Mechanism:** Techniques like KL regularization, early stopping, and weight decay on the B matrix of LoRA adapters constrain the adapter updates, preventing overfitting and overconfidence while allowing the ensemble to capture uncertainty through diversity.
- **Core assumption:** The combination of regularization and ensemble diversity leads to better generalization than either approach alone.

### Mechanism 3
- **Claim:** The source of randomness in LoRA ensembles (initialization vs. dataset shuffling) affects ensemble diversity and performance, with dataset shuffling contributing more when regularization is applied.
- **Mechanism:** Random initialization and stochastic gradient descent (SGD) noise from dataset shuffling both contribute to diversity. When regularization constrains the initialization effect, SGD noise becomes the dominant source of diversity.
- **Core assumption:** Both sources of randomness are necessary for optimal ensemble performance, but their relative contributions depend on regularization strength.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: LoRA is the core technique that makes LLM ensembles memory-efficient by learning only low-rank adapter matrices instead of full model weights.
  - Quick check question: What are the two matrices that compose a LoRA adapter, and what is their typical initialization pattern?

- **Concept: Deep Ensembles**
  - Why needed here: Understanding how averaging predictions across multiple models reduces overconfidence and improves calibration is crucial for grasping why LoRA ensembles work.
  - Quick check question: How does a deep ensemble compute its final prediction, and what is the key requirement for the individual models?

- **Concept: Expected Calibration Error (ECE)**
  - Why needed here: ECE is the primary metric for evaluating whether a model's predicted confidences match its actual accuracy, which is the main problem LoRA ensembles address.
  - Quick check question: What does a high ECE score indicate about a model's predictions, and why is this problematic for real-world deployment?

## Architecture Onboarding

- **Component map:**
  Pre-trained LLM (frozen weights) -> Multiple LoRA adapters (one per ensemble component) -> Adapter initialization (random A matrix, zero B matrix) -> Regularization strategies (KL, early stopping, weight decay) -> Ensemble prediction aggregation (averaging normalized probabilities)

- **Critical path:**
  1. Load pre-trained LLM once
  2. Initialize M LoRA adapters with different random seeds
  3. Fine-tune each adapter independently with regularization
  4. Store adapters (small memory footprint)
  5. At inference, load base model + adapter ensemble
  6. Average normalized predictions across adapters

- **Design tradeoffs:**
  - Adapter rank (r=8 used) vs. performance: Higher rank increases parameters and memory but may improve performance
  - Number of ensemble components (M=5 used) vs. marginal gains: More components improve calibration but with diminishing returns
  - Regularization strength vs. diversity: Stronger regularization improves calibration but may reduce diversity benefits

- **Failure signatures:**
  - All ensemble components produce nearly identical predictions → regularization too strong or initialization not diverse enough
  - NLL increases dramatically during training → overfitting or overconfidence despite ensemble
  - ECE remains high → ensemble diversity insufficient or normalization not properly implemented

- **First 3 experiments:**
  1. Single LoRA adapter fine-tuning (baseline) vs. LoRA ensemble with M=2 to verify improvement exists
  2. LoRA ensemble with different regularization strengths (weight decay on B matrix) to find optimal calibration
  3. LoRA ensemble with varying number of components (M=3, M=5, M=10) to study diminishing returns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity of LoRA ensembles vary with different initialization strategies for the LoRA adapter matrices?
- Basis in paper: [explicit] The paper discusses the importance of random initialization for ensemble diversity but does not systematically compare different initialization strategies for the LoRA adapter matrices (e.g., random initialization vs. zero initialization of B).
- Why unresolved: The paper mentions that the adapter initialization becomes a source of diversity but does not provide a detailed analysis of how different initialization strategies affect the ensemble's performance and diversity.
- What evidence would resolve it: A controlled experiment comparing the performance and diversity of LoRA ensembles using different initialization strategies for the LoRA adapter matrices.

### Open Question 2
- Question: What is the optimal rank (r) for LoRA adapters in ensemble fine-tuning, and how does it affect the trade-off between accuracy and computational efficiency?
- Basis in paper: [explicit] The paper uses a fixed rank (r = 8) for LoRA adapters but does not explore the impact of varying the rank on ensemble performance.
- Why unresolved: The choice of rank affects both the number of trainable parameters and the expressiveness of the adapter. A lower rank reduces computational overhead but may limit the adapter's ability to capture complex patterns.
- What evidence would resolve it: A systematic study varying the rank of LoRA adapters and evaluating the resulting ensemble's accuracy, calibration, and computational efficiency.

### Open Question 3
- Question: How does the performance of LoRA ensembles compare to other ensemble methods, such as BatchEnsemble or Monte Carlo dropout, in terms of accuracy, calibration, and computational efficiency?
- Basis in paper: [explicit] The paper briefly mentions BatchEnsemble as a potential alternative but does not provide a direct comparison with LoRA ensembles.
- Why unresolved: While the paper demonstrates the benefits of LoRA ensembles, it does not evaluate their performance against other ensemble methods that are also designed for parameter-efficient fine-tuning.
- What evidence would resolve it: A comprehensive comparison of LoRA ensembles with other ensemble methods on the same tasks and metrics, including accuracy, calibration, and computational efficiency.

## Limitations

- The analysis does not fully explore the computational overhead of managing multiple LoRA adapters during training and inference, particularly in distributed settings.
- The paper lacks detailed ablation studies on how different random initialization strategies affect ensemble diversity beyond basic seed variation.
- Claims about regularization improving accuracy are supported but show mixed results, with accuracy improvements being "more inconsistent" according to the authors.

## Confidence

- **High confidence**: The core claim that LoRA ensembles improve calibration (measured by ECE) is well-supported by consistent experimental results across six diverse datasets.
- **Medium confidence**: Claims about regularization improving accuracy and calibration are supported but show mixed results, with accuracy improvements being "more inconsistent."
- **Low confidence**: The exact contribution of different sources of randomness (initialization vs. dataset shuffling) to ensemble performance is acknowledged as difficult to decompose.

## Next Checks

1. **Diversity Analysis**: Conduct a detailed correlation analysis of predictions across ensemble components to quantify how much diversity is actually achieved and whether it correlates with calibration improvements.

2. **Scaling Study**: Evaluate LoRA ensembles on larger model sizes (7B+ parameters) and more ensemble components (M=10, M=20) to assess whether benefits scale and to identify any operational bottlenecks.

3. **Regularization Sensitivity**: Perform a systematic hyperparameter sweep of regularization techniques (KL strength, weight decay, early stopping criteria) to map the relationship between regularization strength and ensemble performance across all metrics.