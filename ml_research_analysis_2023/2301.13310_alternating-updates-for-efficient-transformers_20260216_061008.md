---
ver: rpa2
title: Alternating Updates for Efficient Transformers
arxiv_id: '2301.13310'
source_url: https://arxiv.org/abs/2301.13310
tags:
- altup
- arxiv
- computation
- layer
- alternating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Alternating Updates (AltUp), a method to increase
  the capacity of transformer models without incurring significant computational overhead.
  AltUp works by widening the token embedding dimension and using a lightweight prediction-compute-correct
  mechanism to efficiently update subblocks of the representation at each layer.
---

# Alternating Updates for Efficient Transformers

## Quick Facts
- arXiv ID: 2301.13310
- Source URL: https://arxiv.org/abs/2301.13310
- Authors: 
- Reference count: 13
- Key outcome: Introduces Alternating Updates (AltUp), a method to increase transformer model capacity without significant computational overhead by widening token embeddings and using a lightweight prediction-compute-correct mechanism.

## Executive Summary
This paper introduces Alternating Updates (AltUp), a method to increase transformer model capacity without significant computational overhead. AltUp works by widening token embeddings and using a lightweight prediction-compute-correct mechanism to efficiently update subblocks of the representation at each layer. The authors evaluate AltUp on T5X models across language tasks, demonstrating consistent performance improvements with only marginal increases in computation time. They also show that AltUp can be synergistically combined with existing techniques like Mixture-of-Experts models.

## Method Summary
AltUp increases model capacity by widening token embeddings while keeping transformer layer computation constant. The representation vector is widened to Kd dimensions, but only a subblock of width d is processed by each transformer layer. The remaining subblocks are updated through a lightweight predict-compute-correct mechanism using learned linear mappings. Different subblocks are selected for computation at each layer in a round-robin fashion, ensuring all parts of the representation are updated over successive layers. The method is evaluated on T5X Encoder-Decoder models of various sizes (small, base, large) on benchmark language tasks including GLUE, SuperGLUE, and SQuAD.

## Key Results
- AltUp consistently improves performance across all model sizes and tasks with only marginal computation time increase
- On SuperGLUE and SQuAD benchmarks, AltUp enables up to 87% speedup relative to dense baselines at the same accuracy
- AltUp can be synergistically combined with Mixture-of-Experts models for even higher capacity and performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AltUp increases model capacity by widening token embeddings while keeping transformer layer computation constant
- Mechanism: The representation vector is widened to Kd dimensions, but only a subblock of width d is processed by each transformer layer. The remaining subblocks are updated through a lightweight predict-compute-correct mechanism that uses learned linear mappings to infer and correct the uncomputed blocks
- Core assumption: Wider embeddings allow more information storage per token without requiring proportionally more computation
- Evidence anchors: [abstract] "AltUp enables the widening of the token embedding while only incurring a negligible increase in latency" [section] "AltUp increases the token representation width, but does so without incurring (significant) additional computational complexity"
- Break condition: If prediction and correction steps become computationally expensive or if linear mappings fail to accurately propagate information between subblocks

### Mechanism 2
- Claim: The alternating selection of subblocks ensures all parts of the representation are updated over successive layers
- Mechanism: Different subblocks are selected for computation at each layer in a round-robin fashion. This ensures that over a sequence of K layers, all subblocks receive direct transformer computation, while the remaining layers use prediction-correction to update other subblocks
- Core assumption: Round-robin access to subblocks provides balanced computation and prevents any single subblock from becoming stale
- Evidence anchors: [section] "For a sequence of layers, alternating through the sub-blocks, that is, if the sub-blocks are indexed with zero-based index, then sub-block i mod K is selected for the computation step for layer i"
- Break condition: If alternating pattern creates artifacts or if certain subblocks become systematically less important than others

### Mechanism 3
- Claim: The predict-compute-correct algorithm approximates higher-order function updates through first-order corrections
- Mechanism: The predictor estimates the next layer's representation, the transformer layer computes the true update for one subblock, and the corrector applies a first-order correction to align the prediction with the actual computation result
- Core assumption: First-order correction is sufficient to capture essential changes from transformer layer computation
- Evidence anchors: [section] "Alternatively, one could also view the Predict-Compute-Correct algorithm as a Taylor expansion of a target function that we are looking to estimate"
- Break condition: If approximation error accumulates over many layers or if system becomes unstable due to incorrect predictions

## Foundational Learning

- Concept: Representation dimension vs. transformer layer width
  - Why needed here: Understanding distinction between embedding width (information storage) and layer width (processing power) is crucial for grasping how AltUp achieves efficiency
  - Quick check question: What is the computational complexity difference between widening embeddings versus widening transformer layers?

- Concept: Sparse vs. dense computation
  - Why needed here: AltUp's efficiency comes from computing only a subset of representation at each layer, similar to sparse methods but simpler to implement
  - Quick check question: How does AltUp's computation pattern differ from Mixture-of-Experts routing in terms of parameter access and computational overhead?

- Concept: Linear prediction and correction mechanisms
  - Why needed here: Predict-compute-correct algorithm relies on learned linear mappings for efficient representation updates
  - Quick check question: What initialization scheme is used for predictor coefficients, and why is this choice important?

## Architecture Onboarding

- Component map: Embedding layer -> Predictor -> Transformer (single subblock) -> Corrector
- Critical path: Input → Widened embedding → Predictor → Transformer (single subblock) → Corrector → Output
- Design tradeoffs:
  - Wider embeddings increase model capacity but require more memory
  - Alternating selection ensures balanced computation but may introduce ordering dependencies
  - Simple linear predictors/correctors keep computation low but may limit expressiveness
- Failure signatures:
  - Degraded performance if predictor/corrector mappings are poorly learned
  - Instability if alternating pattern creates systematic biases
  - Memory issues if embedding width is increased too aggressively
- First 3 experiments:
  1. Implement AltUp with K=2 on a small T5 model and compare pretraining accuracy to baseline
  2. Test different subblock selection strategies (same vs. alternating) to validate empirical findings
  3. Measure computational overhead of AltUp compared to dense widening to verify efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do alternating updates perform in non-language domains like computer vision or speech processing?
- Basis in paper: [explicit] The authors state "In future work, we plan to conduct a theoretical investigation of alternating updates to develop a deeper understanding of its effectiveness across differing applications"
- Why unresolved: Paper only evaluates AltUp on language tasks (GLUE, SuperGLUE, SQuAD) using T5 transformer models
- What evidence would resolve it: Experiments applying AltUp to vision transformers (ViT) on ImageNet or speech transformers on Librispeech

### Open Question 2
- Question: What is the theoretical explanation for why alternating updates improves model performance?
- Basis in paper: [explicit] Authors mention "we plan to conduct a theoretical investigation of alternating updates to develop a deeper understanding of its effectiveness"
- Why unresolved: While paper demonstrates empirical effectiveness, it does not provide theoretical justification for why predict-compute-correct mechanism works
- What evidence would resolve it: Formal analysis showing how alternating update mechanism relates to optimization dynamics

### Open Question 3
- Question: What is the optimal number of alternating blocks (K) for different model sizes and tasks?
- Basis in paper: [explicit] Paper experiments with K=2 and K=4, noting that "for T5 base, for example, we observe a monotonically increasing trend for the pretraining accuracy as memory sizes increases" but that "on finetuning tasks, we observe some tasks (SuperGLUE) continue to benefit from more memory, while others (GLUE and SQuAD) do not"
- Why unresolved: Paper only tests two values of K and observes inconsistent behavior across tasks
- What evidence would resolve it: Systematic experiments mapping optimal K values across model sizes, tasks, and training regimes

## Limitations
- Limited empirical validation across different model scales and domains
- Unclear scaling behavior for very large models or extreme widening factors
- Limited investigation into potential interactions with other architectural modifications

## Confidence
- Mechanism claims: Medium - provides theoretical justification but lacks comprehensive empirical validation
- Efficiency claims: Medium - demonstrates computational gains but analysis is limited to specific model sizes and tasks
- Synergy claims: Low - supported by single experimental result without detailed analysis of potential interactions

## Next Checks
1. **Component Ablation**: Systematically disable predictor or corrector components in controlled experiments to quantify their individual contributions to performance gains
2. **Scaling Analysis**: Evaluate AltUp across broader range of model sizes (including extremely large models) and different widening factors to understand limits of efficiency benefits
3. **Generalization Testing**: Apply AltUp to non-language tasks (e.g., vision, speech) and different architectural variants to validate broader applicability beyond specific T5X models tested