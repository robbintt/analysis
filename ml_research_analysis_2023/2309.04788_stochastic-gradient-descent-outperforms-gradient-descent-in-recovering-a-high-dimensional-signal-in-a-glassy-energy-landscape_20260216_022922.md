---
ver: rpa2
title: Stochastic Gradient Descent outperforms Gradient Descent in recovering a high-dimensional
  signal in a glassy energy landscape
arxiv_id: '2309.04788'
source_url: https://arxiv.org/abs/2309.04788
tags:
- gradient
- signal
- high-dimensional
- which
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the effectiveness of Stochastic Gradient Descent
  (SGD) compared to Gradient Descent (GD) in optimizing high-dimensional non-convex
  loss functions, using dynamical mean field theory (DMFT). The authors consider a
  prototypical high-dimensional inference problem: reconstructing a hidden high-dimensional
  signal from non-linear measurements.'
---

# Stochastic Gradient Descent outperforms Gradient Descent in recovering a high-dimensional signal in a glassy energy landscape

## Quick Facts
- arXiv ID: 2309.04788
- Source URL: https://arxiv.org/abs/2309.04788
- Reference count: 0
- SGD with small batch sizes outperforms GD in high-dimensional non-convex optimization, achieving lower recovery thresholds.

## Executive Summary
This paper analyzes the effectiveness of Stochastic Gradient Descent (SGD) compared to Gradient Descent (GD) in optimizing high-dimensional non-convex loss functions using dynamical mean field theory (DMFT). The authors study a prototypical high-dimensional inference problem of reconstructing a hidden signal from non-linear measurements, showing that SGD with small batch sizes systematically outperforms GD in reaching the hidden signal. By deriving and integrating DMFT equations, they demonstrate that SGD achieves a lower recovery threshold, quantified by a power law fit of the relaxation time with sample complexity.

## Method Summary
The paper uses DMFT to analyze SGD and GD dynamics in the high-dimensional limit, deriving equations for order parameters like magnetization and correlation functions. The authors implement a dynamical rule for both algorithms with varying batch sizes, then integrate the DMFT equations numerically to track the evolution of order parameters. Performance is measured by mean squared displacement between reconstructed and true signals, with recovery thresholds determined through power law fitting of relaxation times. The analysis focuses on a specific inference problem with quadratic measurements of binary signals.

## Key Results
- SGD with small batch sizes systematically outperforms GD in reaching the hidden signal in high-dimensional non-convex landscapes
- The recovery threshold for SGD with small batch size is smaller than that of GD, quantified by power law fits of relaxation time with sample complexity
- DMFT framework accurately captures the long-time dynamics of SGD in the high-dimensional limit

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SGD introduces a self-annealing noise that helps escape spurious local minima faster than GD
- Mechanism: Stochastic selection of mini-batches introduces effective out-of-equilibrium noise that acts like temperature, driving the system out of aging states into a time-translation invariant regime for faster relaxation
- Core assumption: Mini-batch selection noise can be modeled as a scalar random variable with the same statistics as selection variables
- Evidence anchors:
  - Abstract states "SGD largely outperforms GD for sufficiently small batch sizes"
  - Section explains "SGD noise provides an out-of-equilibrium driven dynamics which leads the system away from aging"
  - Corpus includes paper showing "SGD-like relaxation is equivalent to Metropolis dynamics in discrete optimization"

### Mechanism 2
- Claim: Effective temperature from SGD noise enables reaching the signal at lower sample complexity than GD
- Mechanism: Self-annealing process allows SGD to reach the signal at sample complexity α*(b) < α*(1) where b=1 corresponds to GD
- Core assumption: Power law fit τ(b) ∝ |α - α*(b)|^(-z(b)) accurately describes relaxation time near recovery transition
- Evidence anchors:
  - Abstract mentions "power law fit of the relaxation time shows recovery threshold for SGD with small batch size is smaller than GD"
  - Section states "α*(b) ≃ {1.84, 2.26, 2.28} for b = 0.1, 0.2, 1" and "recovery threshold for GD is found at α*(1) = 2.28"
  - Corpus does not provide direct evidence for this mechanism

### Mechanism 3
- Claim: DMFT framework accurately captures long-time dynamics of SGD in high-dimensional limit
- Mechanism: Auxiliary fields and saddle-point analysis in DMFT describe evolution of magnetization m(t) and correlation function C(t,t') for SGD
- Core assumption: DMFT equations derived in appendix accurately represent SGD dynamics in infinite dimensional limit
- Evidence anchors:
  - Section derives DMFT equations from functional integral representation
  - Section states "DMFT equations can be integrated numerically rather efficiently" with results showing SGD's superiority
  - Corpus does not provide direct evidence for DMFT framework

## Foundational Learning

- Concept: Dynamical Mean Field Theory (DMFT)
  - Why needed here: DMFT analyzes SGD dynamics in high-dimensional limit where system size N → ∞, tracking order parameters over long times
  - Quick check question: What are the key steps in deriving DMFT equations for a given dynamical system?

- Concept: Glassy energy landscapes
  - Why needed here: Inference problem has glassy energy landscape with many spurious minima; understanding glassy systems is crucial for interpreting SGD dynamics and self-annealing noise role
  - Quick check question: How does presence of many spurious minima affect optimization dynamics in a glassy system?

- Concept: Sample complexity
  - Why needed here: Sample complexity α controls difficulty of inference problem; paper investigates how SGD and GD perform at different sample complexities
  - Quick check question: How does sample complexity relate to number of measurements and dimensionality of signal in inference problem?

## Architecture Onboarding

- Component map: Loss function H[x] -> Dynamical rule for SGD/GD -> Order parameters m(t) and C(t,t') -> DMFT equations -> Self-energies Λ_C and Λ_R
- Critical path: Integration of DMFT equations requires computing self-energies Λ_C and Λ_R, which depend on auxiliary fields C_A and R_A obtained by solving linear systems
- Design tradeoffs: Batch size b balances computational efficiency of SGD with ability to escape spurious minima; too small b leads to strong noise and slow convergence, while b=1 may get stuck in bad local minima
- Failure signatures: If magnetization m(t) does not approach 1 or correlation function C(t,t) does not approach 1, algorithm has failed to recover signal; if relaxation time τ(b) diverges or becomes too large, algorithm is ineffective
- First 3 experiments:
  1. Verify DMFT equations by comparing numerical integration results with direct SGD simulations for small N
  2. Investigate effect of different batch size schedules on recovery threshold and relaxation time
  3. Extend analysis to other inference problems with different loss functions and measurement models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the recovery threshold of SGD be computed exactly, especially close to the point where it diverges?
- Basis in paper: [explicit] Paper mentions recovery threshold is at sample complexity strictly smaller than GD but exact computation is unclear
- Why unresolved: Dynamics is still a two-step process close to recovery threshold, making exact threshold difficult to compute
- What evidence would resolve it: Theoretical framework or mathematical proof that accurately calculates recovery threshold for SGD in high-dimensional non-convex optimization

### Open Question 2
- Question: How does SGD noise contribute to out-of-equilibrium driven dynamics that leads system away from aging into TTI regime?
- Basis in paper: [explicit] Paper mentions SGD noise provides out-of-equilibrium driven dynamics leading away from aging into stationary TTI regime
- Why unresolved: Exact mechanism by which SGD noise induces this behavior is not well understood
- What evidence would resolve it: Detailed analysis of DMFT equations explaining role of SGD noise in TTI regime and its impact on recovery threshold

### Open Question 3
- Question: How does batch size affect effectiveness of SGD in optimizing high-dimensional non-convex cost functions compared to other optimization algorithms?
- Basis in paper: [explicit] Paper compares performances of SGD with varying batch sizes to GD showing small batch sizes systematically outperform GD
- Why unresolved: Exact relationship between batch size and effectiveness of SGD is not fully understood, especially in high-dimensional non-convex optimization
- What evidence would resolve it: Further experimental or theoretical studies investigating impact of different batch sizes on SGD performance in various high-dimensional non-convex optimization problems

## Limitations
- Analysis assumes infinite-dimensional limit where DMFT becomes exact, but finite-size effects could significantly alter conclusions for practical systems
- Specific inference problem studied (quadratic measurements of binary signals) may not capture full complexity of real-world non-convex optimization landscapes
- Quantitative values of recovery thresholds are model-specific and depend critically on particular inference problem formulation

## Confidence
- High Confidence: DMFT framework derivation and numerical integration are methodologically sound; power-law fitting follows established statistical mechanics approaches
- Medium Confidence: Superiority of small-batch SGD demonstrated within specific model studied, but mechanism may not generalize to all non-convex problems
- Medium Confidence: Quantitative values of recovery thresholds are model-specific and depend on particular inference problem formulation

## Next Checks
1. Perform systematic simulations at increasing but finite system sizes N to quantify deviations from DMFT predictions and establish range of validity
2. Test SGD superiority mechanism on different inference problems with varying loss function structures to assess robustness
3. Investigate whether adaptive batch size schedules that start small and increase over time can combine benefits of noise-induced escape with stability of larger batches in later optimization stages