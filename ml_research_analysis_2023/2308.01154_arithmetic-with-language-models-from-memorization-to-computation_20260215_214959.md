---
ver: rpa2
title: 'Arithmetic with Language Models: from Memorization to Computation'
arxiv_id: '2308.01154'
source_url: https://arxiv.org/abs/2308.01154
tags:
- input
- addition
- output
- multiplication
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how language models (LMs) perform arithmetic
  computations beyond memorized training data. The authors design controlled experiments
  using binary addition and multiplication tasks with a small LM (701K parameters)
  based on transformer architecture.
---

# Arithmetic with Language Models: from Memorization to Computation

## Quick Facts
- arXiv ID: 2308.01154
- Source URL: https://arxiv.org/abs/2308.01154
- Reference count: 4
- A small transformer-based language model successfully learns binary arithmetic through an encoding-regression-decoding approach rather than pure memorization

## Executive Summary
This paper investigates how language models perform arithmetic computations beyond their training data through controlled experiments with binary addition and multiplication tasks. Using a small transformer-based language model (701K parameters), the authors demonstrate that the model can learn these tasks and achieve near-perfect generalization on unseen data. The key finding is that the model employs an Encoding-Regression-Decoding (ERD) approach, where input tokens are mapped to internal value representations, arithmetic operations are performed in this continuous value space, and results are decoded back to tokens. The study reveals that position embeddings and attention layers are critical for this computation, while encoder layers are not necessary.

## Method Summary
The researchers designed controlled experiments using binary arithmetic tasks with a transformer-based language model. They created a dataset with all possible combinations of 7-bit binary numbers (2^14 total combinations), split into training and validation sets. The model was trained separately on addition and multiplication tasks using a 5-token vocabulary and fixed-length input sequences. Training employed cross-entropy loss with the Adam optimizer, targeting 95% sequence accuracy on the validation set. The authors conducted ablation studies removing positional embeddings and encoder layers, and performed correlation analysis of internal representations across transformer layers to understand the computational mechanism.

## Key Results
- The LM successfully learned both binary addition and multiplication tasks, achieving near-perfect generalization on unseen data
- Position embedding and attention layers were found to be critical for token-to-value transformation, while encoder layers were not necessary
- Correlation analysis of internal representations supported the ERD hypothesis, showing that the model performs computation in value space after encoding from tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model solves arithmetic tasks via an Encoding-Regression-Decoding (ERD) approach rather than pure memorization.
- Mechanism: Input tokens are mapped to internal value representations, regression is performed in this value space, and results are decoded back to tokens.
- Core assumption: The model can learn a transformation from token space to a continuous value space where arithmetic operations are naturally performed.
- Evidence anchors:
  - [abstract] "Our findings support the hypotheses that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate internal representation."
  - [section] "The experiments on the interpolation/extrapolation capabilities and correlation of input-output representations with internal embedding suggest that the model solve the computational task as a supervised regression problem in the value space after an initial encoding from token to values, and a final decoding from output value to tokens."
  - [corpus] Weak - no direct corpus evidence for ERD specifically, but related work on representation learning exists.

### Mechanism 2
- Claim: Position embedding and attention layers are critical for token-to-value transformation, while encoder layers are not necessary.
- Mechanism: Position embeddings provide necessary spatial information for encoding token sequences, and attention mechanisms enable the mapping between token and value representations.
- Core assumption: The decoder can learn the encoding/decoding transformations without needing a separate encoder component.
- Evidence anchors:
  - [abstract] "The study also demonstrates that position embedding and attention layers are critical for this computation, while encoder layers are not necessary."
  - [section] "positional embedding and attention layer s are mandatory for the LM in order to properly perform token to value transformation (and vice versa)."
  - [corpus] Weak - corpus does not provide specific evidence for this architectural claim.

### Mechanism 3
- Claim: The model achieves generalization through interpolation in value space rather than memorization of token sequences.
- Mechanism: By mapping to value space, the model can interpolate between training examples even when input token sequences are far apart.
- Core assumption: The value space representation allows for meaningful similarity metrics that enable interpolation.
- Evidence anchors:
  - [abstract] "these tasks cannot be solved by pure memorization or smooth interpolation and investigating how an LM learn them can improve our understanding of the underlying mechanisms."
  - [section] "the encoding performed during stage 1 makes irrelevant the selection performed according to VS_t because after encoding the corresponding data point remains scattered in the value space and the regressor can easily interpolate among them."
  - [corpus] Weak - no direct corpus evidence for this interpolation mechanism.

## Foundational Learning

- Concept: Token representation and embedding spaces
  - Why needed here: Understanding how discrete tokens map to continuous vector representations is fundamental to grasping how the model performs arithmetic.
  - Quick check question: Can you explain the difference between token-level and value-level representations in this context?

- Concept: Supervised regression in high-dimensional spaces
  - Why needed here: The core computation mechanism relies on learning a regression function in the value space.
  - Quick check question: How would you design a regression problem to learn binary addition in a continuous space?

- Concept: Position encoding and attention mechanisms
  - Why needed here: These components are critical for the model's ability to transform between token and value representations.
  - Quick check question: What role does positional information play in the encoding of arithmetic expressions?

## Architecture Onboarding

- Component map: Tokenizer → Embedding Layer → Positional Encoding → Attention Layers → Feed-Forward Networks → Output Layer
- Critical path: Input tokens → embeddings → positional encoding → attention → value space computation → decoding → output tokens
- Design tradeoffs: Small vocabulary and fixed-length representations simplify the problem but limit expressiveness; removing encoder layers reduces complexity but may limit some transformations.
- Failure signatures: Inability to generalize beyond training data, poor performance on extrapolation tasks, high correlation between token and value representations indicating lack of proper encoding.
- First 3 experiments:
  1. Train with random output to verify that memorization is not the primary mechanism.
  2. Test extrapolation by holding out contiguous regions of the value space during training.
  3. Analyze correlation between input/output distances and internal embedding distances across layers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ERD (Encoding-Regression-Decoding) approach differ when scaling from binary arithmetic to decimal or floating-point operations?
- Basis in paper: [explicit] The paper focuses on binary addition and multiplication with a small vocabulary, suggesting ERD's applicability to other mathematical tasks like square roots, but does not explore decimal or floating-point operations.
- Why unresolved: The paper does not investigate how the model handles larger vocabularies or non-binary arithmetic, which may introduce additional complexities.
- What evidence would resolve it: Experiments testing the ERD approach on decimal and floating-point arithmetic tasks, comparing performance and computational mechanisms with binary operations.

### Open Question 2
- Question: What specific role do positional embeddings play in the token-to-value encoding and value-to-token decoding stages of the ERD process?
- Basis in paper: [explicit] The ablation study shows that removing positional embeddings drastically reduces performance, indicating their importance, but does not detail their exact function in encoding and decoding.
- Why unresolved: The paper highlights the necessity of positional embeddings but does not explore their detailed impact on the transformation between token and value representations.
- What evidence would resolve it: Analysis of how positional embeddings influence the internal representations and transformations in the ERD process, possibly through visualization or correlation studies.

### Open Question 3
- Question: Can the ERD framework be adapted to tasks requiring complex reasoning, such as logical deductions or chain-of-thought reasoning?
- Basis in paper: [explicit] The paper suggests extending the work to tasks that cannot be easily mapped to regression problems, like chain of reasoning and logic deductions, but does not provide experimental results.
- Why unresolved: The paper does not explore how the ERD approach applies to non-mathematical reasoning tasks, which may require different computational strategies.
- What evidence would resolve it: Experiments demonstrating the adaptation of the ERD framework to tasks involving logical reasoning, comparing performance and computational mechanisms with arithmetic tasks.

## Limitations

- Limited model scale and task complexity: The study uses a relatively small LM with binary arithmetic tasks, raising questions about generalization to larger models and more complex operations.
- Corpus evidence gaps: Supporting corpus shows weak direct evidence for the proposed mechanisms, particularly the ERD hypothesis and interpolation claims.
- Training data constraints: The controlled dataset uses all possible combinations of 7-bit operands, which may not reflect naturally occurring training data.

## Confidence

- High confidence: Basic findings that the LM successfully learns binary arithmetic with high accuracy on unseen data
- Medium confidence: ERD mechanism supported by correlation analysis but primarily indirect evidence
- Medium confidence: Architectural claims about encoder layers being unnecessary supported by ablation studies but may be architecture-specific
- Low confidence: Interpolation mechanism claims supported by weak corpus evidence and indirect experimental results

## Next Checks

- Check 1: Test the ERD mechanism on larger models and more complex arithmetic tasks by training the same experimental setup on a larger transformer model (e.g., 10M+ parameters) and extending to decimal arithmetic or multi-step operations.
- Check 2: Conduct controlled ablation studies on position encoding variants by replacing sinusoidal position encoding with learned position embeddings or no position encoding to determine if the mechanism relies specifically on sinusoidal encoding.
- Check 3: Analyze value space representations across layers using principal component analysis to visualize and quantify how input representations transform through transformer layers and verify the existence of a continuous value space for regression.