---
ver: rpa2
title: Revealing the Proximate Long-Tail Distribution in Compositional Zero-Shot Learning
arxiv_id: '2312.15923'
source_url: https://arxiv.org/abs/2312.15923
tags:
- prior
- class
- visual
- attribute
- seen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and addresses a "proximate long-tail distribution"
  problem in Compositional Zero-Shot Learning (CZSL), where visual bias from state-object
  combinations creates attribute imbalance that hinders learning distinguishable class
  prototypes. The authors transform CZSL into a proximate class imbalance problem
  and mathematically analyze the role of class prior in this context.
---

# Revealing the Proximate Long-Tail Distribution in Compositional Zero-Shot Learning

## Quick Facts
- arXiv ID: 2312.15923
- Source URL: https://arxiv.org/abs/2312.15923
- Reference count: 40
- Primary result: Identifies proximate long-tail distribution in CZSL and achieves state-of-the-art performance without additional parameters

## Executive Summary
This paper addresses a fundamental challenge in Compositional Zero-Shot Learning (CZSL) - the visual bias created by imbalanced state-object compositions that hinders learning distinguishable class prototypes. The authors identify this as a "proximate long-tail distribution" problem and propose transforming CZSL into a class imbalance problem solvable through logit adjustment. Their approach, ProLT, incorporates visual bias as a "proximate class prior" into classifier training and inference, achieving state-of-the-art performance on three benchmark datasets without introducing additional parameters.

## Method Summary
The method involves training independent classifiers for states and objects, using their predictions to estimate attribute priors that approximate the visual bias distribution. These priors are then incorporated into the composition classifier through logit adjustment during both training and inference. The approach leverages mutual information maximization principles and ensemble-based methods to compensate for compositions that encounter relative disadvantages within the class distribution.

## Key Results
- ProLT achieves state-of-the-art performance on MIT-States, UT-Zappos, and C-GQA datasets
- Improves AUC by 0.6-2.0% over previous methods without introducing additional parameters
- Effectively addresses attribute imbalance by transforming CZSL into a proximate class imbalance problem

## Why This Works (Mechanism)

### Mechanism 1
The attribute imbalance in CZSL closely approximates a long-tail distribution, making logit adjustment effective. Visual bias from state-object combinations causes some classes to be under-represented in the classifier's effective sample space, mimicking long-tail class imbalance. Logit adjustment using attribute priors compensates for this by boosting underrepresented classes during both training and inference.

### Mechanism 2
Mutual information maximization between compositions and visual features is equivalent to optimizing with class prior adjustment. The CZSL optimization objective can be reinterpreted as maximizing mutual information I(Y;X). By incorporating class priors into the log-probability term, the model effectively adjusts its predictions to better align with the true data distribution, improving generalization.

### Mechanism 3
Ensemble of independent state/object classifiers with composition classifier provides complementary information that mitigates visual bias. Independent classifiers for states and objects capture different aspects of the visual space than the composition classifier. Their predictions serve as a proxy for attribute priors, which when combined with the composition classifier's logits, yield more balanced and accurate predictions.

## Foundational Learning

- **Concept: Class Imbalance and Long-Tail Distributions**
  - Why needed here: Understanding how skewed class distributions affect classifier performance is crucial for recognizing why attribute imbalance in CZSL resembles a long-tail problem.
  - Quick check question: What is the primary challenge posed by long-tail class distributions in classification tasks?

- **Concept: Mutual Information in Machine Learning**
  - Why needed here: The paper's theoretical framework relies on reinterpreting the optimization objective as maximizing mutual information between visual features and compositions.
  - Quick check question: How does mutual information relate to the Kullback-Leibler divergence in the context of classification?

- **Concept: Logit Adjustment for Class Imbalance**
  - Why needed here: The core technique used in ProLT is logit adjustment, which modifies the classifier's output logits based on class priors to achieve more balanced predictions.
  - Quick check question: In logit adjustment, how does incorporating the log of class prior affect the final softmax probabilities?

## Architecture Onboarding

- **Component map**: Input image -> Visual features (ResNet-18) -> Three parallel embedders (Vs, Vo, Vy) -> Three prototype learners (Ps, Po, Py) -> Three classifiers (Cs, Co, Cy) -> Attribute prior estimation -> Logit adjustment -> Final prediction

- **Critical path**: Input image → Visual features (ResNet-18) → Three parallel embedders (Vs, Vo, Vy) → Three prototype learners (Ps, Po, Py) → Three classifiers (Cs, Co, Cy) → Attribute prior estimation → Logit adjustment → Final prediction

- **Design tradeoffs**: Using independent classifiers adds computational overhead but provides complementary information; estimating attribute priors from model predictions introduces approximation error but avoids explicit bias modeling; logit adjustment modifies decision boundaries but may hurt performance on well-classified classes.

- **Failure signatures**: Significant performance gap between seen and unseen classes indicates poor domain adaptation; degradation when ablating attribute prior suggests visual bias is a significant issue; sensitivity to hyper-parameters η and λ indicates instability in prior estimation.

- **First 3 experiments**:
  1. Train Cs and Co with cross-entropy loss, evaluate their accuracy on state/object classification
  2. Train Cy with vanilla cross-entropy, compare AUC/HM with ProLT to measure impact of attribute prior
  3. Perform ablation study by setting η=0 and p=0 in ProLT, observe performance drop to baseline levels

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the attribute prior estimation using individual sample posterior probabilities (Eq. 8) compare to alternative methods like class frequency-based priors or mutual information-based priors in terms of effectiveness for mitigating visual bias?

- **Open Question 2**: How does the performance of ProLT scale with increasing dataset size or complexity, particularly for datasets with a larger number of state-object compositions?

- **Open Question 3**: How does the proposed attribute prior approach generalize to other compositional learning tasks beyond zero-shot learning, such as few-shot learning or meta-learning?

## Limitations
- The assumption that visual bias can be accurately captured through independent state/object classifiers may not hold for all dataset compositions
- Logit adjustment effectiveness depends on quality of attribute prior estimation, which remains an approximation
- Mutual information interpretation relies on accurate posterior approximations that may not generalize across all datasets

## Confidence
- Major uncertainties: Medium confidence in proximate long-tail characterization; High confidence in technical implementation; Low confidence in mutual information interpretation's practical impact
- Confidence labels: Medium (proximate long-tail characterization), High (technical implementation), Low (mutual information interpretation)

## Next Checks
1. Conduct ablation studies removing independent classifiers to quantify the exact contribution of attribute prior estimation to performance gains
2. Compare ProLT's performance against explicit long-tail classification methods on CZSL datasets to validate the proximate long-tail characterization
3. Test the robustness of attribute prior estimation by introducing synthetic visual bias to measure how well ProLT compensates for different bias distributions