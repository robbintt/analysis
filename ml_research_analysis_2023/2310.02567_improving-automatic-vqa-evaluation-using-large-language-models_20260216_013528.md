---
ver: rpa2
title: Improving Automatic VQA Evaluation Using Large Language Models
arxiv_id: '2310.02567'
source_url: https://arxiv.org/abs/2310.02567
tags:
- answer
- answers
- candidate
- reference
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LA VE, an automatic VQA evaluation metric
  that uses large language models (LLMs) to score the correctness of a candidate answer
  given a set of reference answers. The key idea is to formulate VQA evaluation as
  an answer-rating task, where the LLM is instructed to rate the candidate answer
  as correct, ambiguous/incomplete, or incorrect.
---

# Improving Automatic VQA Evaluation Using Large Language Models

## Quick Facts
- arXiv ID: 2310.02567
- Source URL: https://arxiv.org/abs/2310.02567
- Authors: 
- Reference count: 12
- Primary result: LA VE achieves significantly higher correlation with human judgments compared to existing VQA metrics across several VQA models and benchmarks

## Executive Summary
This paper introduces LA VE, an automatic VQA evaluation metric that uses large language models (LLMs) to score the correctness of candidate answers given reference answers. The key innovation is formulating VQA evaluation as an answer-rating task where the LLM rates answers on a 1-3 scale (incorrect, ambiguous/incomplete, correct) rather than using exact string matching. LA VE demonstrates significantly higher correlation with human judgments than traditional VQA Accuracy metrics across multiple datasets and model types.

## Method Summary
LA VE formulates VQA evaluation as an answer-rating task where an instruction-tuned LLM is prompted to rate a candidate answer on a 1-3 scale based on the question and multiple reference answers. The method uses in-context learning with demonstrations, filtering low-frequency reference answers to reduce noise, and optionally incorporating visual context through image descriptions. The LLM's ratings are then mapped to a [0,1] score for comparison with human judgments using Spearman's ρ and Kendall's τ correlation metrics.

## Key Results
- LA VE achieves significantly higher correlation with human judgments than VQA Accuracy across multiple VQA models and benchmarks
- Using 4-8 demonstrations improves LA VE's performance, with diminishing returns beyond 8 demonstrations
- Filtering low-frequency reference answers (below 25% of maximum frequency) consistently improves correlation, particularly for datasets with 10 reference answers per question
- Visual context provides minimal benefit while increasing computational overhead

## Why This Works (Mechanism)

### Mechanism 1
Large language models can capture human preference for answer correctness in VQA by leveraging in-context learning on diverse reference answers. LA VE formulates VQA evaluation as an answer-rating task where the LLM is instructed to score a candidate answer given the question and multiple reference answers. The LLM uses its training on human language patterns to assess semantic equivalence, over/under-specification, synonyms, and multiple valid answers that VQA Accuracy misses.

### Mechanism 2
Using a 3-point rating scale (incorrect, ambiguous/incomplete, correct) captures the inherent ambiguity in VQA questions better than binary matching. The 1-3 scale allows the LLM to assign partial credit for answers that are technically correct but under-specified or address different aspects of the question than the references. This aligns better with human judgment which often gives partial credit for ambiguous questions.

### Mechanism 3
Filtering low-frequency reference answers reduces noise and improves LLM rating accuracy. When there are 10 reference answers per question, some are outliers that don't represent the majority judgment. Filtering answers with frequency below 25% of the maximum frequency removes these outliers before presenting to the LLM.

## Foundational Learning

- Concept: In-context learning with LLMs
  - Why needed here: LA VE needs to adapt a frozen LLM to the VQA evaluation task without fine-tuning, using only prompt engineering and demonstrations
  - Quick check question: What are the three main components of an in-context learning prompt for LA VE?

- Concept: Correlation coefficient measurement
  - Why needed here: The paper evaluates LA VE by measuring its correlation with human judgments using Spearman's ρ and Kendall's τ to assess alignment
  - Quick check question: What's the difference between Spearman's rank correlation and Pearson correlation, and why is Spearman more appropriate here?

- Concept: VQA Accuracy failure modes
  - Why needed here: Understanding why VQA Accuracy fails (multiple answers, synonyms, verbosity, etc.) is crucial for designing LA VE to address these specific issues
  - Quick check question: What are the four most prevalent failure modes of VQA Accuracy identified in the analysis?

## Architecture Onboarding

- Component map: Question, Candidate answer, Reference answers -> Prompt construction -> LLM inference -> Rating extraction -> [0,1] score

- Critical path: Construct prompt → Send to LLM → Extract rating → Map to [0,1] score

- Design tradeoffs:
  - Using visual context (image descriptions) increases computational overhead but provides minimal benefit
  - More demonstrations improve performance but increase latency and cost
  - Binary questions are particularly challenging due to reference answer contradictions

- Failure signatures:
  - Low correlation with human judgment indicates poor demonstration selection or prompt issues
  - Inconsistent ratings for similar answers suggests demonstration noise
  - High computational cost suggests unnecessary visual context inclusion

- First 3 experiments:
  1. Test LA VE with different numbers of demonstrations (1, 4, 8) on validation set to find optimal tradeoff
  2. Compare correlation with and without reference answer filtering on VQAv2
  3. Test LA VE with and without visual context on VG-QA (single reference answer) to confirm no benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LA VE perform on VQA tasks with more than 10 reference answers per question?
- Basis in paper: [inferred]
- Why unresolved: The paper only tests LA VE with datasets that have up to 10 reference answers (VQAv2). It is unclear if LA VE's performance would scale with a larger number of reference answers.
- What evidence would resolve it: Testing LA VE on a VQA dataset with more than 10 reference answers per question and comparing its performance to VQA Accuracy and other baselines.

### Open Question 2
- Question: Can LA VE be adapted to evaluate VQA tasks in languages other than English?
- Basis in paper: [inferred]
- Why unresolved: The paper only evaluates LA VE on English VQA datasets. It is unclear if LA VE can be easily adapted to evaluate VQA tasks in other languages.
- What evidence would resolve it: Testing LA VE on VQA datasets in other languages and comparing its performance to VQA Accuracy and other baselines.

### Open Question 3
- Question: How does LA VE handle questions that require complex reasoning or commonsense knowledge?
- Basis in paper: [inferred]
- Why unresolved: The paper does not explicitly discuss how LA VE handles questions that require complex reasoning or commonsense knowledge. It is unclear if LA VE can effectively evaluate answers to such questions.
- What evidence would resolve it: Testing LA VE on VQA datasets with questions that require complex reasoning or commonsense knowledge and analyzing its performance on such questions.

## Limitations
- LA VE's performance on binary questions remains problematic due to contradictory reference answers
- The computational cost of LLM inference scales poorly compared to simple string matching
- Generalizability to non-English VQA tasks is uncertain due to language-specific reasoning patterns

## Confidence
- Core claim (LLM-based rating outperforms string-matching metrics): High
- Claims about handling ambiguous questions and visual context: Medium
- Generalizability to non-English tasks: Low

## Next Checks
1. Cross-lingual validation: Test LA VE on multilingual VQA datasets to assess language-specific reasoning effects
2. Binary question robustness: Design controlled experiment with consistent reference answers for binary questions to measure correlation improvement
3. Computational efficiency analysis: Benchmark LA VE's inference time and cost against VQA Accuracy across different LLM model sizes and prompt configurations