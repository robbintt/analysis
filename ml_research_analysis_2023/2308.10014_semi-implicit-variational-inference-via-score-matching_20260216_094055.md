---
ver: rpa2
title: Semi-Implicit Variational Inference via Score Matching
arxiv_id: '2308.10014'
source_url: https://arxiv.org/abs/2308.10014
tags:
- variational
- score
- inference
- sivi
- sivi-sm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SIVI-SM, a new approach to semi-implicit
  variational inference (SIVI) that leverages score matching instead of the traditional
  evidence lower bound (ELBO). The key innovation is a minimax reformulation of the
  score matching objective, which allows efficient handling of the intractable variational
  densities inherent in SIVI.
---

# Semi-Implicit Variational Inference via Score Matching

## Quick Facts
- arXiv ID: 2308.10014
- Source URL: https://arxiv.org/abs/2308.10014
- Authors: 
- Reference count: 40
- Key outcome: Introduces SIVI-SM, a new approach to semi-implicit variational inference that leverages score matching instead of ELBO, achieving better posterior approximation and predictive performance.

## Executive Summary
This paper proposes SIVI-SM, a novel method for semi-implicit variational inference that uses score matching as an alternative to the traditional ELBO objective. The key innovation is a minimax reformulation of the score matching objective, which enables efficient handling of the intractable variational densities inherent in SIVI. By leveraging the hierarchical structure of semi-implicit variational families and denoising score matching, SIVI-SM provides a more accurate approximation to the posterior distribution compared to ELBO-based SIVI methods.

## Method Summary
SIVI-SM reformulates the semi-implicit variational inference problem using a minimax objective based on score matching. The hierarchical structure of the semi-implicit variational family allows the intractable score term to be replaced with an expectation over the conditional distribution. This enables the use of denoising score matching to handle the intractable variational densities. The method alternates between updating the variational parameters using gradient descent and updating the score network using gradient ascent. The score network is trained to approximate the difference between the target and variational score functions.

## Key Results
- SIVI-SM closely matches the accuracy of MCMC and outperforms ELBO-based SIVI methods in terms of predictive performance and posterior approximation.
- Experimental results on synthetic distributions and real-world Bayesian inference tasks demonstrate the effectiveness and efficiency of SIVI-SM.
- The proposed method provides a more accurate approximation to the posterior distribution compared to ELBO-based SIVI methods, particularly for complex multimodal distributions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hierarchical structure of semi-implicit variational families allows the intractable score term $\nabla_x \log q_\phi(x)$ to be replaced with an expectation over the conditional distribution, making the score matching objective tractable.
- Mechanism: By exploiting the semi-implicit hierarchical construction $x \sim q_\phi(x|z), z \sim q_\xi(z)$, the intractable score $\nabla_x \log q_\phi(x)$ is rewritten as an expectation $\mathbb{E}_{z \sim q_\xi(z), x \sim q_\phi(x|z)}[\nabla_x \log q_\phi(x|z)]$, which can be estimated via Monte Carlo sampling.
- Core assumption: The conditional layer $q_\phi(x|z)$ is reparameterizable and its score function $\nabla_x \log q_\phi(x|z)$ is easy to evaluate.
- Evidence anchors:
  - [abstract]: "Leveraging the hierarchical structure of semi-implicit variational families, the score matching objective allows a minimax formulation where the intractable variational densities can be naturally handled with denoising score matching."
  - [section]: "Substituting Eq. 9 into Eq. 8 completes our reformulation."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.413, average citations=0.0. Top related titles: Kernel Semi-Implicit Variational Inference, Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration, A Kernel Approach for Semi-implicit Variational Inference.
- Break condition: If the conditional layer is not reparameterizable or its score function is intractable, this mechanism fails.

### Mechanism 2
- Claim: The minimax reformulation of the score matching objective enables the use of denoising score matching to handle intractable variational densities.
- Mechanism: The squared norm $\|\mathbf{S}(x) - \nabla_x \log q_\phi(x)\|^2_2$ is reformulated as a maximum over a function $f(x)$, allowing the intractable score term to be replaced with a learnable neural network $f_\psi(x)$ that is trained via denoising score matching.
- Core assumption: The optimal $f^*$ is the difference between the target and variational score functions, and can be approximated by a neural network.
- Evidence anchors:
  - [abstract]: "Leveraging the hierarchical structure of semi-implicit variational families, the score matching objective allows a minimax formulation where the intractable variational densities can be naturally handled with denoising score matching."
  - [section]: "Based on this observation, we can rewrite the optimization problem in 7 as $\min_\phi \max_f \mathbb{E}_{x \sim q_\phi(x)}[2f(x)^T [\mathbf{S}(x) - \nabla_x \log q_\phi(x)] - \|f(x)\|^2_2]$."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.413, average citations=0.0. Top related titles: Kernel Semi-Implicit Variational Inference, Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration, A Kernel Approach for Semi-implicit Variational Inference.
- Break condition: If the neural network approximation $f_\psi(x)$ cannot accurately represent the difference between the target and variational score functions, the minimax reformulation fails.

### Mechanism 3
- Claim: The proposed method closely matches the accuracy of MCMC and outperforms ELBO-based SIVI methods in terms of predictive performance and posterior approximation.
- Mechanism: By minimizing the Fisher divergence between the target and variational score functions, SIVI-SM provides a more accurate approximation to the posterior than ELBO-based methods, which rely on surrogates or expensive MCMC runs.
- Core assumption: The score matching objective is a valid and effective measure of closeness between the target and variational distributions.
- Evidence anchors:
  - [abstract]: "We show that SIVI-SM closely matches the accuracy of MCMC and outperforms ELBO-based SIVI methods in a variety of Bayesian inference tasks."
  - [section]: "We demonstrate the effectiveness and efficiency of our method on both synthetic distributions and a variety of real data Bayesian inference tasks."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.413, average citations=0.0. Top related titles: Kernel Semi-Implicit Variational Inference, Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration, A Kernel Approach for Semi-implicit Variational Inference.
- Break condition: If the score matching objective is not a valid measure of closeness or the neural network approximation is poor, the claimed accuracy may not be achieved.

## Foundational Learning

- Concept: Variational Inference (VI)
  - Why needed here: SIVI-SM is a method for semi-implicit variational inference, which extends traditional VI to handle more complex variational families.
  - Quick check question: What is the main goal of variational inference, and how does it differ from MCMC methods?

- Concept: Score Matching
  - Why needed here: SIVI-SM uses score matching as an alternative training objective to the traditional ELBO, allowing it to handle intractable variational densities.
  - Quick check question: What is the score matching objective, and how does it differ from the ELBO?

- Concept: Denoising Score Matching
  - Why needed here: SIVI-SM leverages denoising score matching to handle the intractable variational densities in a minimax formulation.
  - Quick check question: How does denoising score matching differ from standard score matching, and why is it useful for high-dimensional data?

## Architecture Onboarding

- Component map: Hierarchical variational family -> Score network -> Reparameterization trick -> Optimization
- Critical path:
  1. Sample from the hierarchical variational distribution using the reparameterization trick.
  2. Compute the score matching objective using the sampled data and the current score network.
  3. Update the variational parameters $\phi$ using the gradient of the score matching objective.
  4. Update the score network parameters $\psi$ using the gradient of the score matching objective.

- Design tradeoffs:
  - Using score matching instead of ELBO allows handling intractable variational densities but requires learning a score network.
  - The minimax reformulation enables denoising score matching but introduces an additional optimization loop.
  - The semi-implicit hierarchical construction provides a rich variational family but requires careful design of the conditional and mixing distributions.

- Failure signatures:
  - Poor posterior approximation: The score network $f_\psi(x)$ fails to accurately represent the difference between the target and variational score functions.
  - Slow convergence: The alternating optimization between $\phi$ and $\psi$ does not converge quickly or gets stuck in local optima.
  - High variance: The Monte Carlo estimates of the score matching objective have high variance, leading to noisy gradients.

- First 3 experiments:
  1. Implement the semi-implicit variational family with a simple conditional distribution (e.g., Gaussian) and mixing distribution (e.g., standard normal).
  2. Implement the score network $f_\psi(x)$ as a simple multilayer perceptron with a few hidden layers.
  3. Test the implementation on a simple 2D synthetic distribution (e.g., banana-shaped or multimodal Gaussian) and compare the posterior approximation to MCMC and ELBO-based SIVI methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the approximation error of the neural network fψ(x) impact the overall accuracy of SIVI-SM, and can this error be effectively bounded?
- Basis in paper: [explicit] The paper discusses the approximation error of fψ(x) in Proposition 1, which shows that the approximation error can be controlled by two terms: the approximation ability of the variational distribution and the approximation/optimization error of the neural network fψ(x).
- Why unresolved: The paper provides a theoretical bound but does not empirically investigate the impact of this error on the overall performance of SIVI-SM.
- What evidence would resolve it: Empirical studies comparing the performance of SIVI-SM with different neural network architectures and sizes, and their corresponding approximation errors.

### Open Question 2
- Question: Can the score matching objective in SIVI-SM be extended to other divergences beyond the Fisher divergence, and what would be the implications?
- Basis in paper: [inferred] The paper focuses on the Fisher divergence but does not explore other potential divergences that could be used with score matching.
- Why unresolved: The paper does not investigate alternative divergences or their potential benefits or drawbacks.
- What evidence would resolve it: Experiments comparing SIVI-SM with other divergences, such as the Wasserstein distance or the Jensen-Shannon divergence, and analyzing their performance.

### Open Question 3
- Question: How does the choice of the noise distribution qσ(˜x|x) in denoising score matching affect the performance of SIVI-SM, and is there an optimal choice?
- Basis in paper: [explicit] The paper mentions that a small noise is required for accurate approximation of the true data score in denoising score matching.
- Why unresolved: The paper does not explore the impact of different noise distributions or their parameters on the performance of SIVI-SM.
- What evidence would resolve it: Experiments comparing SIVI-SM with different noise distributions and their parameters, and analyzing their impact on the accuracy of the variational posterior.

## Limitations

- The paper does not provide convergence guarantees for the alternating optimization scheme between variational parameters and the score network.
- The performance claims are based primarily on synthetic experiments and relatively small-scale real data benchmarks, with limited evaluation on high-dimensional posterior inference tasks.
- The computational complexity of training the score network, particularly for high-dimensional data, is not thoroughly analyzed.

## Confidence

- **High confidence**: The mathematical formulation of the minimax reformulation and its connection to denoising score matching is well-established and theoretically sound.
- **Medium confidence**: The experimental results showing improved performance over ELBO-based SIVI methods are promising but require more extensive validation on diverse real-world datasets.
- **Low confidence**: The claim that SIVI-SM "closely matches the accuracy of MCMC" needs more rigorous validation, particularly for high-dimensional problems where MCMC diagnostics are more challenging.

## Next Checks

1. Implement ablation studies to quantify the contribution of the minimax reformulation versus the hierarchical structure to overall performance.
2. Evaluate on larger-scale Bayesian inference tasks (e.g., hierarchical models with hundreds of parameters) to test scalability.
3. Compare the computational efficiency of SIVI-SM against other semi-implicit variational methods using wall-clock time rather than iteration count.