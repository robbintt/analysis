---
ver: rpa2
title: Would I have gotten that reward? Long-term credit assignment by counterfactual
  contribution analysis
arxiv_id: '2306.16803'
source_url: https://arxiv.org/abs/2306.16803
tags:
- reward
- policy
- rewarding
- contribution
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the credit assignment problem in reinforcement\
  \ learning by proposing a new family of model-based algorithms called Counterfactual\
  \ Contribution Analysis (COCOA). The key idea is to measure the contribution of\
  \ actions toward rewarding outcomes using counterfactual reasoning\u2014asking \"\
  Would the agent still have achieved this reward if it had taken another action?\"\
  \u2014instead of measuring contributions toward states as done in previous work."
---

# Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis

## Quick Facts
- arXiv ID: 2306.16803
- Source URL: https://arxiv.org/abs/2306.16803
- Reference count: 40
- Key outcome: Proposes COCOA, a model-based credit assignment method that measures contributions relative to rewarding outcomes rather than states, achieving lower variance and improved long-term credit assignment

## Executive Summary
This paper addresses the credit assignment problem in reinforcement learning by introducing Counterfactual Contribution Analysis (COCOA), a new family of algorithms that measure the contribution of actions toward rewarding outcomes using counterfactual reasoning. Unlike previous approaches that measure contributions toward states, COCOA asks "Would the agent still have achieved this reward if it had taken another action?" This approach avoids spurious contributions that lead to high variance and enables long-term credit assignment without relying on temporal discounting. Experiments on a suite of problems demonstrate that COCOA achieves lower bias and variance compared to Hindsight Credit Assignment and common baselines, resulting in improved performance and sample efficiency.

## Method Summary
COCOA introduces contribution coefficients that measure how much an action contributed to achieving a specific rewarding outcome, calculated as the ratio of action probabilities under the hindsight distribution to the policy distribution, minus one. The method learns these coefficients by training hindsight models to classify which action led to a rewarding outcome given the state and outcome. This approach uses inverse dynamics models trained with hindsight information, making it compatible with discrete actions without requiring path-wise derivatives through differentiable world models. The contribution coefficients are then used to estimate policy gradients, enabling credit assignment that focuses on outcomes rather than intermediate states.

## Key Results
- COCOA achieves lower bias and variance compared to Hindsight Credit Assignment and REINFORCE baselines
- The method successfully handles long-term credit assignment without temporal discounting
- COCOA demonstrates improved performance and sample efficiency across a suite of test environments
- The approach can disentangle different tasks by measuring contributions toward specific rewarding outcomes

## Why This Works (Mechanism)

### Mechanism 1
Measuring contributions relative to rewarding outcomes reduces variance compared to measuring contributions relative to states. State representations must encode sufficient information for policy decisions, but this same richness creates spurious contributions because almost every action leads to a slightly different state. Rewarding outcomes encode only the relevant reward information, filtering out irrelevant state variations.

### Mechanism 2
COCOA can disentangle different tasks by measuring contributions toward specific rewarding outcomes instead of the full expected sum of rewards. Value functions estimate expected sums of future rewards, mixing contributions from different tasks. COCOA measures contributions toward individual rewarding outcomes, allowing it to learn that actions relevant for one task don't influence rewarding outcomes in other tasks.

### Mechanism 3
Learning contribution coefficients using inverse dynamics models trained with hindsight information enables model-based credit assignment compatible with discrete actions. Instead of using path-wise derivatives through differentiable world models (which require continuous actions), COCOA learns contribution coefficients by training inverse models to predict which action led to a rewarding outcome given the state and outcome.

## Foundational Learning

- **Concept**: Counterfactual reasoning and Do-interventions in causal inference
  - Why needed here: COCOA's contribution coefficients are equivalent to performing Do-interventions on causal graphs to estimate effects on future rewards
  - Quick check question: How does the contribution coefficient formula w(s,a,u') = pπ(a|s,u')/π(a|s) - 1 relate to causal Do-calculus?

- **Concept**: Bias-variance tradeoff in policy gradient estimation
  - Why needed here: Understanding why COCOA achieves lower variance than HCA and REINFORCE requires knowledge of how different credit assignment methods affect estimator variance
  - Quick check question: Why does using rewards instead of states as rewarding outcomes lead to lower variance in COCOA?

- **Concept**: Partially observable Markov decision processes (POMDPs) and belief states
  - Why needed here: COCOA must handle environments where the agent doesn't observe complete state information, requiring internal representations that balance policy capability with backward credit assignment
  - Quick check question: Why does HCA suffer from spurious contributions in POMDPs when using internal belief states?

## Architecture Onboarding

- **Component map**: Policy network -> Hindsight model -> Reward feature network -> Contribution coefficient calculator -> Gradient estimator

- **Critical path**:
  1. Collect trajectories using current policy
  2. Train reward feature network to predict rewards from state-action pairs
  3. Train hindsight model to classify actions given state and future rewarding outcomes
  4. Compute contribution coefficients from hindsight model
  5. Calculate policy gradient and update policy

- **Design tradeoffs**:
  - Using states vs rewards vs learned features as rewarding outcomes: states provide rich information for latent learning but cause spurious contributions; rewards provide low variance but may lack discriminative power; learned features balance both needs
  - Direct hindsight distribution learning vs contrastive ratio estimation: direct classification is simpler but may be harder to train; contrastive methods may be more stable but require careful implementation
  - Deterministic vs stochastic rewarding outcomes: deterministic outcomes are fully predictive but may cause excessive variance; stochastic outcomes reduce variance at the cost of bias

- **Failure signatures**:
  - High variance in gradient estimates despite using COCOA: likely spurious contributions from state-based encodings or insufficient smoothing
  - Poor performance despite low variance: hindsight model not learning accurate action classification or reward features not predictive enough
  - Slow adaptation to reward changes: using reward values directly instead of learned reward features that generalize

- **First 3 experiments**:
  1. Implement COCOA-reward on linear key-to-door environment and compare performance to REINFORCE and HCA baselines
  2. Test COCOA-feature on reward-aliasing variant where distractor and treasure rewards have same value
  3. Measure bias-variance tradeoff by comparing ground-truth contribution coefficients vs learned ones in shadow training setup

## Open Questions the Paper Calls Out

### Open Question 1
Can the contribution coefficients be learned from non-rewarding observations using generative models that combine state representations with reward contributions? The paper mentions leveraging forward dynamics models and generative models to enable learning contribution coefficients from synthetic trajectories or non-rewarding observations in Appendix G. This remains unresolved as the paper proposes this as a potential avenue for future work but does not provide concrete experiments or results.

### Open Question 2
Can the insights from COCOA be extended to temporal difference methods to reduce the variance of TD learning while maintaining long-term credit assignment? The paper mentions this as an exciting direction for future research in the discussion section, noting the connection between COCOA and TD(λ) methods. No theoretical analysis or experimental results on combining COCOA with TD methods are provided.

### Open Question 3
How does the performance of COCOA change when applied to partially observable Markov decision processes (POMDPs)? The paper discusses the challenges of spurious contributions in POMDPs in Appendix F.5, noting that HCA degrades to REINFORCE due to the internal state encoding previous actions. The paper does not provide any experiments or analysis of COCOA performance in POMDP settings.

## Limitations
- The theoretical analysis assumes access to ground truth contribution coefficients computed via dynamic programming, which may not be feasible in complex, high-dimensional environments
- The effectiveness of COCOA in sparse reward settings remains untested, as experiments focus on environments with frequent rewarding outcomes
- The computational overhead of training both reward feature networks and hindsight models could limit scalability to larger problems

## Confidence

- **High Confidence**: The core mechanism of reducing variance by measuring contributions relative to rewarding outcomes rather than states is well-supported by both theoretical analysis and empirical results. The linear key-to-door experiments clearly demonstrate this advantage.
- **Medium Confidence**: The claims about disentangling different tasks and faster adaptation to reward changes are supported by specific experiments but would benefit from more diverse test environments to establish generalizability.
- **Medium Confidence**: The bias-variance analysis using dynamic programming provides strong evidence for COCOA's effectiveness, though the artificial nature of shadow training limits ecological validity.

## Next Checks

1. Test COCOA in a sparse reward environment (e.g., Montezuma's Revenge style tasks) to evaluate performance when rewarding outcomes are rare and training hindsight models becomes challenging.

2. Implement a scalability test using a larger, continuous control environment (e.g., MuJoCo tasks) to measure computational overhead and assess whether the contribution coefficient estimation remains tractable.

3. Conduct an ablation study varying the granularity of rewarding outcomes (using raw rewards vs. binned rewards vs. learned features) to quantify the tradeoff between variance reduction and information preservation.