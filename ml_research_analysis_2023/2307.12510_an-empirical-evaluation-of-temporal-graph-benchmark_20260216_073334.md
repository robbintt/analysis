---
ver: rpa2
title: An Empirical Evaluation of Temporal Graph Benchmark
arxiv_id: '2307.12510'
source_url: https://arxiv.org/abs/2307.12510
tags:
- dynamic
- graph
- learning
- methods
- dimension
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends DyGLib to TGB and provides an empirical evaluation
  of TGB. The authors point out some existing issues in the current version of TGB,
  including mismatched data statistics, inaccurate evaluation metric computation,
  and potential numerical errors in the loss function.
---

# An Empirical Evaluation of Temporal Graph Benchmark

## Quick Facts
- arXiv ID: 2307.12510
- Source URL: https://arxiv.org/abs/2307.12510
- Reference count: 15
- One-line primary result: DyGLib extension to TGB identifies evaluation issues and provides more comprehensive baseline comparisons

## Executive Summary
This work extends the DyGLib framework to the Temporal Graph Benchmark (TGB) and provides an empirical evaluation of eleven dynamic graph learning methods. The authors identify several issues in the current TGB implementation, including mismatched data statistics, inaccurate evaluation metric computation, and potential numerical errors in the loss function. Through comprehensive experiments, they demonstrate that different methods show varying performance across datasets, and that DyGLib implementations can significantly improve upon the reported results in TGB.

## Method Summary
The authors implement eleven dynamic graph learning methods in DyGLib and evaluate them on TGB datasets using standardized hyperparameters. The evaluation framework includes early stopping, multiple runs for averaging, and proper handling of temporal graph data. They identify and correct issues in TGB's evaluation pipeline, including NDCG computation errors and log_softmax/CrossEntropyLoss incompatibilities. The experiments are conducted on NVIDIA RTX 3090 and Tesla V100 GPUs with batch size 200, learning rate 0.0001, and 100 epochs with early stopping.

## Key Results
- Different methods show varying performance across different datasets, with no single method dominating all tasks
- Some baselines show significantly improved performance when implemented in DyGLib compared to TGB results
- DyGLib's implementation correctly handles NDCG computation by dividing by time slots rather than examples, avoiding artificially low scores
- The unified framework reduces implementation variance and enables more reliable method comparisons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The DyGLib extension to TGB provides more comprehensive baseline comparisons by implementing eleven methods versus the nine reported in TGB.
- Mechanism: By standardizing implementation details and hyperparameters across multiple dynamic graph learning methods, DyGLib reduces implementation variance that could obscure true method performance differences.
- Core assumption: The methods implemented in DyGLib are faithful reproductions of the original algorithms with equivalent hyperparameters.
- Evidence anchors:
  - [abstract] "Compared with [2], we include eleven popular dynamic graph learning methods for more exhaustive comparisons."
  - [section 2.3] "From Table 2, we have two key observations. Firstly, different methods show varying performance on different datasets..."
  - [corpus] "Average neighbor FMR=0.467" - moderate relatedness, suggesting the DyGLib extension addresses a recognized gap in comprehensive evaluation.
- Break condition: If the DyGLib implementations differ significantly from the original papers' implementations or use suboptimal hyperparameters, the improved performance may not reflect true method capabilities.

### Mechanism 2
- Claim: The DyGLib framework identifies and corrects issues in TGB's evaluation pipeline, leading to more accurate performance measurements.
- Mechanism: By providing a unified implementation of evaluation metrics and data processing, DyGLib eliminates inconsistencies between different method implementations that could affect metric computation.
- Core assumption: The TGB evaluation pipeline contains errors that significantly impact reported results, and DyGLib correctly implements these components.
- Evidence anchors:
  - [section 2.2] "Inaccurate Evaluation Metric Computation. The calculation of the NDCG metric on dynamic node property prediction is problematic."
  - [section 2.2] "Potential Numerical Errors in Loss Function. In the TGB repository, the node predictor provides the outputs computed by log_softmax function..."
  - [corpus] "Average citations=0.0" - limited external validation of the correction claims, suggesting this mechanism relies heavily on the paper's internal analysis.
- Break condition: If the identified issues in TGB are minor or if DyGLib introduces new errors in its evaluation pipeline, the performance improvements may be artifacts rather than genuine corrections.

### Mechanism 3
- Claim: The unified DyGLib framework enables researchers to reproduce and build upon existing results more easily, accelerating research progress.
- Mechanism: By providing standardized implementations and addressing known issues in the benchmark, DyGLib reduces the barrier to entry for evaluating new methods against established baselines.
- Core assumption: Researchers will adopt DyGLib as their primary evaluation framework for dynamic graph learning methods.
- Evidence anchors:
  - [abstract] "This work aims to ease the researchers' efforts in evaluating various dynamic graph learning methods on TGB..."
  - [section 1] "This work extends DyGLib to TGB to provide more comprehensive and reliable experimental results for the follow-up research."
  - [corpus] "Average neighbor FMR=0.467" - moderate relatedness to existing research infrastructure, suggesting the framework addresses a real need but may face adoption challenges.
- Break condition: If the research community continues using alternative evaluation frameworks or if DyGLib's implementation complexity prevents widespread adoption, the acceleration of research progress will be limited.

## Foundational Learning

- Concept: Dynamic graph learning fundamentals
  - Why needed here: Understanding the temporal nature of graph data and how models capture evolving node/edge relationships is essential for interpreting benchmark results and method comparisons.
  - Quick check question: What distinguishes dynamic graph learning from static graph learning, and why do temporal dependencies matter for prediction tasks?

- Concept: Temporal graph benchmark evaluation metrics
  - Why needed here: The paper discusses issues with MRR and NDCG calculations in TGB, so understanding these metrics and their proper computation is crucial for assessing the validity of reported results.
  - Quick check question: How are MRR and NDCG computed for dynamic link prediction versus dynamic node property prediction tasks?

- Concept: Transformer architectures for temporal graphs
  - Why needed here: Several methods evaluated (TGA T, TGN, DyGFormer) use attention mechanisms or Transformer-based approaches, requiring understanding of how these architectures handle temporal information.
  - Quick check question: How do attention mechanisms capture temporal dependencies in dynamic graphs, and what are the key architectural differences between TGA T, TGN, and DyGFormer?

## Architecture Onboarding

- Component map:
  - Data processing pipeline: Loads TGB datasets, handles temporal splitting, manages node/edge attributes
  - Model implementations: Eleven dynamic graph learning methods with standardized interfaces
  - Evaluation framework: Computes MRR/NDCG metrics, handles early stopping, manages multiple runs
  - Hyperparameter management: Default configurations, supports dataset-specific tuning
  - Visualization and analysis: Performance comparison tools, statistical analysis of results

- Critical path:
  1. Load dataset → 2. Initialize model with hyperparameters → 3. Train with early stopping → 4. Evaluate on validation/test sets → 5. Aggregate results across runs

- Design tradeoffs:
  - Computational efficiency vs. model complexity: Some methods (DyGFormer) require more resources but may capture richer temporal patterns
  - Hyperparameter generality vs. dataset specificity: Using default hyperparameters across datasets saves time but may not optimize performance for each dataset
  - Implementation completeness vs. maintenance burden: Including eleven methods provides comprehensive comparisons but increases maintenance complexity

- Failure signatures:
  - Out-of-memory errors during training indicate model or batch size issues
  - Extremely low metric values suggest data loading or preprocessing errors
  - Inconsistent results across runs may indicate random seed or initialization problems
  - Missing results in tables often indicate computational cost issues or implementation bugs

- First 3 experiments:
  1. Run JODIE on tgbl-wiki with default hyperparameters to verify basic functionality
  2. Compare TGN performance on tgbl-review using both DyGLib and TGB implementations to validate the improved results
  3. Test the evaluation pipeline by running a simple baseline (Moving Average) on tgbn-trade to verify metric calculations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal hyperparameter settings for the evaluated dynamic graph learning methods across all TGB datasets?
- Basis in paper: [explicit] The paper states that due to computational resource limitations, the optimal hyperparameters of baselines on the small-scale tgbl-wiki dataset are used as the default setting for other datasets, and more comprehensive selections of hyperparameters are expected in the future.
- Why unresolved: The paper does not provide the optimal hyperparameter settings for each method on each dataset, and the authors acknowledge the need for more exhaustive hyperparameter selection in the future.
- What evidence would resolve it: Running extensive hyperparameter tuning experiments for each method on each TGB dataset to find the optimal settings.

### Open Question 2
- Question: How can the mismatched data statistics issue in TGB be resolved, specifically for the node and link counts in the datasets for dynamic node property prediction?
- Basis in paper: [explicit] The paper points out that the numbers of nodes and links on datasets for dynamic node property prediction reported in [2] are inconsistent with those in the released datasets, and it is desired to revise this issue in the future version of [2].
- Why unresolved: The paper does not provide a solution to this issue, and it is left as a problem to be addressed in the future.
- What evidence would resolve it: Updating the TGB repository with the correct node and link counts for the datasets, or providing a clear explanation for the discrepancies.

### Open Question 3
- Question: How can the inaccurate evaluation metric computation issue in TGB be fixed, particularly for the NDCG metric on dynamic node property prediction?
- Basis in paper: [explicit] The paper identifies that the calculation of the NDCG metric on dynamic node property prediction is problematic, as the computed results of each time slot are divided by the number of examples rather than the number of time slots, leading to extremely low performance of the baselines.
- Why unresolved: The paper does not provide a solution to this issue, and it is left as a problem to be addressed in the future.
- What evidence would resolve it: Correcting the NDCG metric calculation in the TGB repository to divide by the number of time slots instead of the number of examples.

### Open Question 4
- Question: How can the potential numerical errors in the loss function for dynamic node property prediction in TGB be addressed?
- Basis in paper: [explicit] The paper points out that the node predictor in the TGB repository provides the outputs computed by log_softmax function, but the nn.CrossEntropyLoss expects raw logits as inputs and applies log softmax internally, which could potentially lead to incorrect values.
- Why unresolved: The paper does not provide a solution to this issue, and it is left as a problem to be addressed in the future.
- What evidence would resolve it: Modifying the TGB repository to ensure that the node predictor outputs raw logits instead of log_softmax values, or updating the loss function to handle log_softmax inputs correctly.

## Limitations

- The empirical improvements may be influenced by hyperparameter tuning differences rather than fundamental implementation advantages
- Lack of ablation studies isolating the impact of individual corrections (evaluation metric fixes, loss function adjustments, implementation standardization)
- Computational cost prevented testing all combinations of methods and datasets, potentially missing performance patterns

## Confidence

This evaluation has several notable limitations. The empirical improvements reported for DyGLib implementations versus TGB baselines may be influenced by hyperparameter tuning differences rather than fundamental implementation advantages. The paper does not provide ablation studies isolating the impact of individual corrections (evaluation metric fixes, loss function adjustments, and implementation standardization). Additionally, while eleven methods are evaluated, the computational cost prevented testing all combinations of methods and datasets, potentially missing performance patterns.

Confidence in the core claims is **Medium**. The identification of specific technical issues in TGB (incorrect NDCG computation, log_softmax/CrossEntropyLoss combination) appears technically sound based on standard deep learning practices. However, the magnitude of performance improvements and their attribution to DyGLib's implementation quality versus other factors remains uncertain without controlled experiments. The observation that different methods excel on different datasets is well-supported but represents a known characteristic of dynamic graph learning rather than a novel finding.

## Next Checks

1. Conduct controlled experiments comparing DyGLib and TGB implementations of TGN on tgbl-review with identical hyperparameters to isolate implementation effects from tuning differences.

2. Verify the NDCG computation correction by calculating metrics both ways (with and without division by time slots) on a small subset of tgbn-trade data to confirm the impact of the reported bug.

3. Perform runtime profiling of all eleven methods on a representative dataset to quantify the computational overhead of the more complex implementations and assess whether performance gains justify resource requirements.