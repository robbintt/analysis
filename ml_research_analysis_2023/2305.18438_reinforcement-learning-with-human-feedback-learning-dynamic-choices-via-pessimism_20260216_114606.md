---
ver: rpa2
title: 'Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism'
arxiv_id: '2305.18438'
source_url: https://arxiv.org/abs/2305.18438
tags:
- have
- theorem
- function
- proof
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies offline reinforcement learning with human feedback
  (RLHF) under the Dynamic Discrete Choice (DDC) model, where human choices are modeled
  as optimal actions under an unknown reward function plus Gumbel noise. The main
  challenges are: learning human behavior policy from limited data, recovering the
  unobservable reward function, and handling distribution shift.'
---

# Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism

## Quick Facts
- arXiv ID: 2305.18438
- Source URL: https://arxiv.org/abs/2305.18438
- Reference count: 40
- Primary result: First theoretical guarantee for offline RLHF under the dynamic discrete choice model with single-policy coverage

## Executive Summary
This paper studies offline reinforcement learning with human feedback (RLHF) under the Dynamic Discrete Choice (DDC) model, where human choices are modeled as optimal actions under an unknown reward function plus Gumbel noise. The authors propose a three-stage Dynamic-Choice-Pessimistic-Policy-Optimization (DCPPO) method that learns optimal policies from human choices without direct reward observation. The method involves estimating human behavior policy and value functions via maximum likelihood estimation (MLE), recovering the reward function by minimizing Bellman mean squared error, and using pessimistic value iteration to find a near-optimal policy. Theoretical guarantees show that DCPPO nearly matches classical pessimistic offline RL algorithms in terms of suboptimality bounds for both linear MDPs and RKHS model classes.

## Method Summary
The DCPPO algorithm addresses offline RLHF by learning human behavior policy and value functions through MLE, recovering the reward function via Bellman MSE minimization, and applying pessimistic value iteration with uncertainty quantifiers. The three-stage approach enables learning optimal policies from human choices without direct reward observation, handling distribution shift through pessimism. The method works under single-policy coverage assumptions and provides theoretical guarantees for both linear MDPs (O(n-1/2) suboptimality) and RKHS models (O(nκ* - 1/2) suboptimality), where κ* depends on eigenvalue decay conditions.

## Key Results
- First theoretical guarantee for offline RLHF under the DDC model with single-policy coverage
- Suboptimality bounds nearly match classical pessimistic offline RL algorithms in terms of dependency on distribution shift and dimension
- Achieves O(n-1/2) suboptimality for linear MDPs and O(nκ* - 1/2) for RKHS models
- Works without direct observation of rewards, learning them from human choice data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DCPPO learns optimal policies from human choices without direct reward observation using MLE, Bellman MSE, and pessimistic value iteration
- Mechanism: First estimates human behavior policy and value functions via MLE from choice data, then recovers reward function by minimizing Bellman MSE using learned values, and finally applies pessimistic value iteration with uncertainty quantifiers to handle distribution shift
- Core assumption: Human decisions follow the DDC model with Gumbel noise and MDP has sufficient structure for function approximation
- Evidence anchors: The abstract describes the three-stage process and mentions the algorithm's efficiency for linear MDP and RKHS model classes

### Mechanism 2
- Claim: Uncertainty quantifiers in pessimistic value iteration effectively control estimation error from reward learning
- Mechanism: Constructs uncertainty quantifiers (Γh(s,a)) as β·∥φ(s,a)∥(Λh+λI)-1 for linear models or β·λ-1/2·(K(z,z)-kh(z)⊤(Kh+λI)-1kh(z))1/2 for RKHS, bounding Bellman error from reward estimation and value function approximation
- Core assumption: Uncertainty quantifier covers true Bellman error with high probability and dataset provides sufficient coverage of optimal policy
- Evidence anchors: The abstract mentions pessimism to tackle insufficient coverage, and the paper proves efficiency for both linear and RKHS model classes

### Mechanism 3
- Claim: DCPPO achieves suboptimality bounds nearly matching classical pessimistic offline RL algorithms
- Mechanism: Combines accurate reward estimation through MLE and BMSE minimization with effective pessimism via uncertainty quantifiers to achieve O(n-1/2) for linear MDPs and O(nκ* - 1/2) for RKHS models
- Core assumption: Function approximation model class is rich enough to capture true reward and value functions with satisfactory eigenvalue decay conditions for RKHS
- Evidence anchors: The abstract states suboptimality nearly matches classical pessimistic offline RL, and Theorem 4.3 shows matching dependence on data size and distribution

## Foundational Learning

- Concept: Dynamic Discrete Choice (DDC) Model
  - Why needed here: Provides theoretical framework for modeling human decision-making under bounded rationality with Gumbel noise in RLHF setting
  - Quick check question: What is the key difference between the DDC model and the standard MDP Bellman equation, and how does the discount factor γ in the DDC model relate to human myopia?

- Concept: Maximum Likelihood Estimation (MLE) for Policy and Value Function Recovery
  - Why needed here: Used in first stage of DCPPO to estimate human behavior policy and state-action value functions from choice data
  - Quick check question: How does the identifiability assumption (Q(s,a0) = 0 for some a0) ensure that MLE estimates are well-defined, and what happens if this assumption is violated?

- Concept: Pessimistic Value Iteration and Uncertainty Quantification
  - Why needed here: Key mechanism for handling distribution shift in offline RL through uncertainty quantifiers
  - Quick check question: How does construction of uncertainty quantifier Γh(s,a) in DCPPO relate to elliptical potential term in linear models and effective dimension in RKHS models?

## Architecture Onboarding

- Component map: Data Input → MLE Estimation → Reward Recovery → Pessimistic Value Iteration → Output Policy
- Critical path: Sequential execution of three stages: MLE estimation → Reward recovery → Pessimistic planning, where each stage depends on previous output
- Design tradeoffs:
  - Model Class Richness vs. Estimation Error: Richer model classes may capture complex functions better but increase estimation error and computational complexity
  - Uncertainty Quantifier Tightness vs. Pessimism: Tighter quantifiers lead to less conservative policies but may underestimate true error, while looser quantifiers are more robust but overly conservative
  - Single-Policy Coverage vs. Dataset Size: Assuming single-policy coverage makes DCPPO more practical but may require larger datasets compared to well-explored dataset assumptions
- Failure signatures:
  - High estimation error in MLE stage: Large deviations between estimated and true policies/values, indicated by high ∥π̂h - πb,h∥ or ∥Q̂h - Qπb,γh∥
  - Inaccurate reward recovery: Large errors in estimated reward function, indicated by high |rh - r̂h| for some (s,a) pairs
  - Overly conservative or aggressive policies: Suboptimal performance due to either too much pessimism (underutilization of data) or too little pessimism (overfitting to dataset)
- First 3 experiments:
  1. Synthetic MDP with known reward: Generate synthetic data from MDP with known reward, apply DCPPO, and compare learned policy's performance to optimal policy
  2. Linear MDP with varying dataset sizes: Test DCPPO on linear MDP with different dataset sizes to verify O(n-1/2) suboptimality bound and understand scaling with data
  3. RKHS MDP with different eigenvalue decay conditions: Apply DCPPO to RKHS MDP with varying eigenvalue decay conditions to understand how eigenvalue decay affects performance and verify theoretical bounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the exponential factor O(e^H |A|) in the reward estimation error be improved?
- Basis in paper: The paper notes this exponential dependency comes from estimating Qπb,γh with logistic regression and occurs in logistic bandit literature, leaving it as an open question if this factor can be improved
- Why unresolved: This appears to be a fundamental limitation inherited from the logistic regression approach used in the first stage of the algorithm
- What evidence would resolve it: Theoretical analysis showing improved bounds without the exponential factor, or a new algorithmic approach that avoids logistic regression in the policy estimation phase

### Open Question 2
- Question: How does the algorithm perform when the human behavior policy doesn't cover the optimal policy well?
- Basis in paper: The algorithm relies on single-policy coverage assumption (Assumption 4.2), but this is weaker than well-explored datasets. The paper doesn't explore what happens when this assumption is violated
- Why unresolved: The theoretical guarantees depend on this coverage assumption, and the paper doesn't provide analysis for cases where the behavior policy has poor coverage of the optimal policy
- What evidence would resolve it: Empirical studies showing performance degradation as coverage decreases, or theoretical analysis of error bounds under varying coverage conditions

### Open Question 3
- Question: What is the impact of the discount factor γ on the algorithm's performance and sample complexity?
- Basis in paper: The paper mentions γ measures the myopia of the agent and that the case γ=0 corresponds to a myopic human agent, but doesn't analyze how different values of γ affect the results
- Why unresolved: The discount factor appears in the dynamic discrete choice model formulation but isn't thoroughly analyzed in terms of its effect on estimation accuracy or suboptimality bounds
- What evidence would resolve it: Theoretical analysis showing how the bounds scale with γ, or empirical results comparing performance across different γ values

### Open Question 4
- Question: How does the algorithm scale to continuous action spaces?
- Basis in paper: The paper focuses on finite action sets |A|, and the logistic regression approach in the first stage would need significant modification for continuous actions
- Why unresolved: The current algorithmic framework is built around discrete action spaces, and extending to continuous spaces would require different techniques for policy estimation
- What evidence would resolve it: A modified algorithm for continuous actions with corresponding theoretical guarantees, or empirical results showing performance on problems with continuous action spaces

## Limitations

- The theoretical guarantees rely heavily on the DDC model assumption and single-policy coverage assumption, with unverified performance in more complex, real-world scenarios
- The paper does not address the computational complexity of the three-stage DCPPO algorithm in practice, which could be significant for large-scale applications
- Performance with neural network function approximation remains unverified, as theoretical guarantees are only provided for linear MDPs and RKHS model classes

## Confidence

- High confidence: The theoretical framework and suboptimality bounds for linear MDPs and RKHS models are well-established and rigorously proven
- Medium confidence: The practical performance of DCPPO in real-world RLHF scenarios, especially with neural network function approximation, is less certain due to reliance on strong assumptions and lack of empirical validation
- Low confidence: The computational complexity and scalability of the DCPPO algorithm for large-scale problems are not thoroughly analyzed or discussed

## Next Checks

1. Empirical validation on real-world RLHF datasets: Apply DCPPO to benchmark RLHF datasets (e.g., human preferences in recommendation systems or robotics) and compare its performance to state-of-the-art offline RLHF methods in terms of both learning efficiency and final policy quality

2. Ablation study on model class richness: Investigate the impact of using different function approximation model classes (e.g., linear, RBF, neural networks) on the performance of DCPPO in terms of suboptimality bounds and computational efficiency

3. Sensitivity analysis on uncertainty quantifier tightness: Empirically study how the tightness of the uncertainty quantifier affects the final policy's performance, exploring the tradeoff between pessimism and exploration in the presence of distribution shift