---
ver: rpa2
title: 'FAIR: A Causal Framework for Accurately Inferring Judgments Reversals'
arxiv_id: '2306.11585'
source_url: https://arxiv.org/abs/2306.11585
tags:
- causal
- legal
- judgments
- knowledge
- intelligence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FAIR, a causal framework for improving legal
  judgment prediction by mining and injecting causal relationships. The method uses
  instrumental variables to identify causal effects between case facts and judgment
  outcomes, then applies causal smoothing to integrate this knowledge into neural
  networks.
---

# FAIR: A Causal Framework for Accurately Inferring Judgments Reversals

## Quick Facts
- arXiv ID: 2306.11585
- Source URL: https://arxiv.org/abs/2306.11585
- Reference count: 25
- Key outcome: FAIR improves legal judgment prediction accuracy by up to 11.54% over baselines through causal knowledge injection

## Executive Summary
This paper introduces FAIR, a causal framework that improves legal judgment prediction by mining and injecting causal relationships between case facts and outcomes. The method uses instrumental variables extracted through law article prediction to identify causal effects, then applies causal smoothing to integrate this knowledge into neural networks. Experiments on real Chinese legal data show significant performance improvements over baselines, with accuracy gains of up to 11.54%. The framework also reveals limitations in large language models' generalization for legal tasks, demonstrating that causal knowledge injection enhances both accuracy and interpretability.

## Method Summary
FAIR constructs a causal graph using legal expert knowledge and encoder training, then estimates causal effects using instrumental variables approach with law article prediction as the instrument. The framework injects these causal relationships into neural networks through Causal Smoothing, which modifies label smoothing to reflect treatment effect magnitudes rather than uniform uncertainty. The method is validated on a legal judgment prediction task using Chinese labor relationship determination cases.

## Key Results
- FAIR achieves accuracy gains of 11.54% over BERT and 9.72% over Longformer baselines
- Instrumental variable extraction via law article prediction successfully enables causal inference
- Causal Smoothing improves model generalization by reducing overconfidence on difficult cases
- Large language models show limited performance even with causal knowledge prompting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instrumental variable extraction via law article prediction enables causal inference when confounders are unobserved
- Mechanism: The framework trains an encoder to predict relevant legal articles from case facts, then uses the encoded features as an instrumental variable to block backdoor paths while satisfying exogeneity and relevance conditions
- Core assumption: The law article prediction task successfully extracts the part of case facts relevant to treatment but not to outcome, creating a valid instrumental variable
- Break condition: If the encoder fails to isolate treatment-relevant features from outcome-relevant features, the instrumental variable becomes invalid and causal estimates become biased

### Mechanism 2
- Claim: Causal Smoothing modifies label smoothing to reflect treatment effect magnitudes rather than uniform uncertainty
- Mechanism: Instead of uniform label smoothing with hyperparameter ε, Causal Smoothing uses ε_i = ω * Σ_j ATE(t_ij, 0) where treatment effects vary by sample and label, creating sample-specific soft labels that reflect causal importance
- Core assumption: The estimated ATE values accurately represent the relative importance of different treatment factors for each sample
- Break condition: If ATE estimates are systematically biased or if the relationship between ATE magnitude and label importance is non-monotonic, the smoothing may degrade rather than improve performance

### Mechanism 3
- Claim: Causal knowledge injection improves model generalization by reducing overconfidence on difficult cases
- Mechanism: By incorporating causal relationships as prior knowledge through Causal Smoothing, the model adjusts its confidence calibration based on the causal importance of different factors, making predictions more robust to variations in factual descriptions
- Core assumption: The causal relationships capture essential judgment criteria that generalize beyond the specific training distribution
- Break condition: If the causal relationships are too specific to the training domain or if they conflict with the actual decision boundaries, injection may reduce rather than improve generalization

## Foundational Learning

- Concept: Instrumental variables and causal identification
  - Why needed here: The framework relies on instrumental variables to identify causal effects when confounders are unobserved in the legal judgment reversal problem
  - Quick check question: What are the two key conditions an instrumental variable must satisfy to enable valid causal inference?

- Concept: Label smoothing and confidence calibration
  - Why needed here: Causal Smoothing builds on label smoothing principles but modifies them to incorporate causal knowledge instead of uniform uncertainty
  - Quick check question: How does uniform label smoothing with ε differ from sample-specific smoothing based on treatment effect estimates?

- Concept: Encoder-decoder architectures and feature extraction
  - Why needed here: The framework uses an encoder trained on law article prediction to extract instrumental variable features from case facts
  - Quick check question: Why is it important that the encoder be trained on a related but distinct task (law article prediction) rather than directly on the downstream prediction task?

## Architecture Onboarding

- Component map: Encoder → Causal inference engine → Neural network with Causal Smoothing → Downstream prediction task
- Critical path: Training encoder on law article prediction → Using encoded features as instrumental variable → Estimating ATE via instrumental variables approach → Injecting ATE into neural network via Causal Smoothing → Training downstream model
- Design tradeoffs: Using instrumental variables trades statistical efficiency for causal validity when confounders are unobserved; Causal Smoothing trades some precision for better calibration and generalization
- Failure signatures: Poor encoder training leads to invalid instrumental variables; biased ATE estimates lead to incorrect smoothing; inappropriate ω hyperparameter leads to under/over-smoothing
- First 3 experiments:
  1. Train encoder on law article prediction and evaluate prediction accuracy to verify it captures relevant features
  2. Run sensitivity tests with bootstrap samples and placebo treatments to validate ATE estimates
  3. Compare Causal Smoothing with uniform label smoothing on a simplified downstream task to verify the mechanism works before full integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we better quantify and analyze the impact of judges' subjective will on judgment reversals?
- Basis in paper: The paper states that it is difficult to quantify and analyze factors such as judges' subjective will when modeling judgments reversals.
- Why unresolved: Subjective factors are inherently challenging to measure objectively and incorporate into causal models.
- What evidence would resolve it: Research developing methods to capture and quantify subjective judicial factors, potentially through interviews, surveys, or natural language processing of judicial reasoning.

### Open Question 2
- Question: What are the optimal strategies for designing prompts to maximize ChatGPT's utilization of injected causal knowledge in legal tasks?
- Basis in paper: The paper found that ChatGPT's performance improved with different levels of prompts but was still limited, suggesting room for optimization.
- Why unresolved: The relationship between prompt design and causal knowledge utilization by large language models is not well understood.
- What evidence would resolve it: Systematic studies varying prompt structure, content, and length to determine their impact on LLM performance with causal knowledge injection.

### Open Question 3
- Question: How can we extend FAIR to handle more complex legal scenarios involving multiple interdependent causal relationships?
- Basis in paper: The paper mentions that FAIR uses instrumental variables to identify causal effects, but does not explore more complex causal structures.
- Why unresolved: Legal scenarios often involve intricate causal relationships that may not be captured by simple instrumental variable approaches.
- What evidence would resolve it: Research developing and validating more sophisticated causal inference methods for complex legal scenarios, potentially incorporating techniques like Bayesian networks or structural causal models.

## Limitations
- The validity of instrumental variable approach depends heavily on assumptions about unconfoundedness and exogeneity that are difficult to verify empirically
- Experimental results may not generalize beyond the specific labor law domain or Chinese legal system
- LLM comparison is limited to prompting approaches without fine-tuning, leaving absolute performance gaps unclear

## Confidence
- Causal inference claims: Medium - depends on unverifiable assumptions about unconfoundedness and exogeneity
- Generalization claims: Medium - improvements demonstrated only on specific Chinese labor law dataset
- LLM comparison claims: Low - limited to prompting without fine-tuning, absolute performance gaps unknown

## Next Checks
1. Cross-domain validation: Test FAIR on legal judgment datasets from different jurisdictions and legal domains to assess generalizability
2. Ablation study on causal components: Systematically remove causal smoothing while keeping other architectures identical to isolate causal knowledge contribution
3. Robustness to noise and distribution shift: Evaluate performance when case facts contain irrelevant information or test distribution differs from training