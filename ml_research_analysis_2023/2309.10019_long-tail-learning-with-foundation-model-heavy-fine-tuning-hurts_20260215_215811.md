---
ver: rpa2
title: 'Long-Tail Learning with Foundation Model: Heavy Fine-Tuning Hurts'
arxiv_id: '2309.10019'
source_url: https://arxiv.org/abs/2309.10019
tags:
- uni00000003
- uni00000044
- uni00000046
- uni00000013
- uni0000004c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Heavy fine-tuning in long-tail learning can degrade tail-class
  performance due to overfitting, while lightweight fine-tuning preserves generalization.
  PEL introduces parameter-efficient fine-tuning with semantic-aware classifier initialization
  and test-time ensembling, achieving state-of-the-art accuracy on ImageNet-LT, Places-LT,
  and iNaturalist 2018 with fewer than 20 epochs and significantly fewer parameters
  than prior methods.
---

# Long-Tail Learning with Foundation Model: Heavy Fine-Tuning Hurts

## Quick Facts
- **arXiv ID**: 2309.10019
- **Source URL**: https://arxiv.org/abs/2309.10019
- **Reference count**: 39
- **Key outcome**: PEL achieves state-of-the-art accuracy on ImageNet-LT, Places-LT, and iNaturalist 2018 with fewer than 20 epochs and significantly fewer parameters than prior methods.

## Executive Summary
This paper challenges the conventional wisdom that heavy fine-tuning always improves performance in long-tail learning scenarios. Through extensive experiments, the authors demonstrate that parameter-efficient fine-tuning (PEFT) with semantic-aware classifier initialization and test-time ensembling consistently outperforms full fine-tuning on long-tailed datasets. The proposed PEL framework leverages pre-trained vision-language models while introducing minimal task-specific parameters, effectively preventing overfitting on tail classes that plague traditional fine-tuning approaches.

## Method Summary
The PEL framework introduces parameter-efficient fine-tuning with semantic-aware classifier initialization and test-time ensembling. It freezes the pre-trained CLIP ViT backbone and introduces lightweight PEFT modules (like AdaptFormer) into each Transformer block. The classifier weights are initialized using CLIP's textual features for each class, and predictions are ensembled over 5 image crops at test time. The method trains for 10-20 epochs using LA loss with batch size 128, significantly reducing training time compared to full fine-tuning approaches.

## Key Results
- PEL achieves state-of-the-art accuracy on ImageNet-LT, Places-LT, and iNaturalist 2018
- Uses fewer than 20 epochs and significantly fewer parameters than prior methods
- Demonstrates superior performance on tail classes where heavy fine-tuning typically degrades
- Shows robust performance across different long-tailed dataset configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heavy fine-tuning degrades tail-class performance due to overfitting.
- Mechanism: Fine-tuning all parameters introduces too many degrees of freedom relative to the limited tail-class samples, causing the model to overfit and lose generalization on rare classes.
- Core assumption: Pre-trained representations already contain discriminative features; adding more trainable parameters risks memorizing training patterns rather than learning robust features.
- Evidence anchors:
  - [abstract] "heavy fine-tuning may even lead to non-negligible performance deterioration on tail classes"
  - [section 2] "full fine-tuning often comes with significant computational overheads on large-scale datasets. Moreover, it tends to encounter severe overfitting on long-tailed datasets due to the limited amount of tail-class samples."
  - [corpus] Weak/no direct evidence linking overfitting to tail-class degradation in cited papers; inference based on general fine-tuning literature.
- Break condition: If the pre-trained model has poor initial tail-class representations, overfitting may be less harmful than poor initialization.

### Mechanism 2
- Claim: Parameter-efficient fine-tuning (PEFT) preserves generalization by freezing most parameters and tuning only a small subset.
- Mechanism: By keeping the pre-trained backbone fixed and introducing only a few task-specific parameters, the model maintains the learned semantic features while adapting to the new task with minimal risk of overfitting.
- Core assumption: The frozen parameters encode robust visual-linguistic knowledge from pre-training; the small PEFT module can adapt this knowledge without disrupting it.
- Evidence anchors:
  - [abstract] "introduces parameter-efficient fine-tuning with semantic-aware classifier initialization and test-time ensembling"
  - [section 3.3] "parameter-efficient fine-tuning (PEFT) which freezes the pre-trained model while introducing a few learnable parameters for the adaptation"
  - [corpus] Weak evidence; corpus papers mention fine-tuning but not the PEFT mechanism specifically.
- Break condition: If the task requires significant architectural changes, freezing most parameters may limit adaptability.

### Mechanism 3
- Claim: Semantic-aware classifier initialization accelerates convergence and improves tail-class accuracy by leveraging pre-trained class semantics.
- Mechanism: Initializing classifier weights using CLIP's textual features for each class embeds prior semantic knowledge, giving the model a better starting point and reducing the need for extensive training.
- Core assumption: Textual embeddings capture meaningful relationships between classes; using them as classifier initialization transfers this semantic structure to the visual classifier.
- Evidence anchors:
  - [abstract] "semantic-aware classifier initialization technique derived from the CLIP textual encoder"
  - [section 3.3] "we propose to leverage the embedded semantic knowledge... using textual features associated with class labels to initialize the classifier weights"
  - [corpus] No direct evidence; assumption based on CLIP's known semantic alignment capabilities.
- Break condition: If class semantics are not well-represented in the textual encoder or the task is domain-specific with different semantics.

## Foundational Learning

- Concept: Long-tail class imbalance and its impact on model generalization.
  - Why needed here: Understanding why tail classes suffer under standard fine-tuning is crucial for designing PEFT strategies.
  - Quick check question: Why does a class with 5 samples have a higher risk of overfitting than a class with 1000 samples during fine-tuning?

- Concept: Parameter-efficient fine-tuning (PEFT) methods and their trade-offs.
  - Why needed here: Choosing the right PEFT module (e.g., Adapter, LoRA, VPT) balances adaptation capacity and overfitting risk.
  - Quick check question: How does the number of tunable parameters in a PEFT module compare to the classifier parameters, and why does this matter?

- Concept: Test-time ensembling and its role in improving robustness.
  - Why needed here: Aggregating predictions from multiple image crops mitigates biases introduced by cropping and improves generalization.
  - Quick check question: What is the computational trade-off of applying test-time ensembling with 5 crops versus a single crop?

## Architecture Onboarding

- Component map:
  Pre-trained CLIP ViT-B/16 backbone (frozen) -> PEFT modules (AdaptFormer) inserted into each Transformer block -> Semantic-aware classifier (initialized with CLIP text features) -> Test-time ensembling over 5 image crops

- Critical path:
  1. Load CLIP model and freeze backbone
  2. Insert PEFT modules into Transformer blocks
  3. Initialize classifier with textual features
  4. Fine-tune PEFT parameters and classifier using LA loss
  5. Apply test-time ensembling at inference

- Design tradeoffs:
  - More PEFT parameters → higher adaptation capacity but increased overfitting risk
  - Larger bottleneck dimension r → better performance but more parameters
  - More crops in TTE → better generalization but slower inference

- Failure signatures:
  - Overfitting: Training accuracy much higher than test accuracy, especially on tail classes
  - Underfitting: Both training and test accuracy low, possibly due to too few tunable parameters
  - Poor convergence: Slow improvement or plateauing early, possibly due to bad initialization

- First 3 experiments:
  1. Compare full fine-tuning vs. PEFT on a small long-tail subset to observe overfitting
  2. Test different PEFT modules (Adapter vs. LoRA vs. VPT) with fixed classifier initialization
  3. Evaluate impact of semantic-aware initialization vs. random initialization on convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PEL's performance scale with larger pre-trained models (e.g., ViT-L/14 or even larger vision-language models) and what are the computational implications?
- Basis in paper: [inferred] The paper focuses on ViT-B/16 and mentions that PEL is a general framework compatible with various PEFT methods. It also discusses computational efficiency but doesn't explore scaling to larger models.
- Why unresolved: The paper doesn't investigate performance and efficiency trade-offs with larger backbone models, which would be crucial for real-world deployment.
- What evidence would resolve it: Experiments comparing PEL's performance and computational costs across different model sizes (e.g., ViT-B/16, ViT-L/14, CLIP ViT-H/14) on long-tailed datasets.

### Open Question 2
- Question: What is the theoretical justification for why parameter-efficient fine-tuning prevents overfitting on tail classes better than full fine-tuning?
- Basis in paper: [explicit] The paper states that PEFT "prevents the decline of generalization ability benefiting from image-text pre-training" and "mitigates overfitting" but doesn't provide theoretical analysis.
- Why unresolved: The paper presents empirical evidence of PEFT's benefits but lacks mathematical or theoretical explanation for why this occurs.
- What evidence would resolve it: Formal theoretical analysis or mathematical proof demonstrating why PEFT methods preserve generalization better than full fine-tuning in long-tailed scenarios.

### Open Question 3
- Question: How sensitive is PEL's performance to the choice of bottleneck dimension r in AdaptFormer, and is there an optimal way to determine this hyperparameter for different datasets?
- Basis in paper: [explicit] The paper discusses choosing r = 2⌊log₂(K/2L)⌋ and shows performance is "robust to the change of dimensions" but doesn't explore the sensitivity or provide a principled selection method.
- Why unresolved: While the paper provides a heuristic for choosing r and shows it works well, it doesn't analyze how performance varies with different choices or provide guidance for optimal selection.
- What evidence would resolve it: Comprehensive ablation studies showing PEL's performance across a wide range of r values for different datasets, plus analysis of correlations between optimal r and dataset characteristics.

## Limitations

- The claim that heavy fine-tuning specifically degrades tail-class performance due to overfitting lacks direct experimental evidence within this work.
- The mechanism assumes pre-trained representations are sufficient for tail classes, which may not hold for all domains.
- The semantic-aware initialization relies on CLIP's textual encoder, but the quality of these embeddings for specialized domains remains untested.

## Confidence

- **High Confidence**: PEFT methods generally reduce overfitting risk compared to full fine-tuning, and the proposed architecture is technically sound.
- **Medium Confidence**: The specific claim that heavy fine-tuning hurts tail-class performance is plausible but not directly proven in this work; evidence is inferential from related literature.
- **Medium Confidence**: Semantic-aware initialization likely provides benefits based on CLIP's known capabilities, but the extent of improvement depends on task domain.

## Next Checks

1. **Overfitting Analysis**: Run controlled experiments comparing full fine-tuning vs. PEFT on a long-tailed subset with varying sample sizes per class to directly measure overfitting impact on tail classes.
2. **Semantic Initialization Quality**: Evaluate the CLIP textual embeddings for domain-specific classes by measuring their semantic similarity to ground truth class relationships in specialized datasets.
3. **PEFT Module Sensitivity**: Systematically vary the bottleneck dimension r and number of tunable parameters to identify the optimal trade-off between adaptation capacity and overfitting risk for different long-tail distributions.