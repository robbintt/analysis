---
ver: rpa2
title: 'Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop
  Feedback'
arxiv_id: '2307.11049'
source_url: https://arxiv.org/abs/2307.11049
tags:
- learning
- human
- goal
- exploration
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Human Guided Exploration (HuGE) addresses the challenge of sparse
  reward exploration in goal-conditioned reinforcement learning by decoupling human
  feedback from policy learning. Instead of using human feedback directly as rewards,
  HuGE trains a parametric goal selector from binary state comparisons to guide exploration
  toward promising states while policy learning proceeds via self-supervised hindsight
  relabeling.
---

# Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback

## Quick Facts
- arXiv ID: 2307.11049
- Source URL: https://arxiv.org/abs/2307.11049
- Reference count: 40
- Key outcome: HuGE enables learning goal-conditioned policies from sparse human feedback by decoupling exploration guidance from policy learning

## Executive Summary
HuGE addresses the challenge of sparse reward exploration in goal-conditioned reinforcement learning by using human feedback to guide exploration rather than directly shaping rewards. The method trains a parametric goal selector from binary state comparisons while policy learning proceeds via self-supervised hindsight relabeling, enabling effective learning from noisy, asynchronous, and infrequent non-expert feedback. Experiments demonstrate HuGE outperforms methods relying on direct human reward modeling or indiscriminate exploration across navigation and manipulation tasks in both simulation and real-world robot learning.

## Method Summary
HuGE decouples human feedback from policy learning by using feedback to train a goal selector that guides exploration, while the policy learns via self-supervised hindsight relabeling from collected trajectories. Human feedback takes the form of binary state comparisons (A closer to goal than B), which train an unnormalized distance function. The goal selector then guides frontier expansion exploration toward promising states. Policy learning uses goal-conditioned supervised learning (GCSL) with hindsight relabeling to learn optimal goal-reaching behavior from the exploration data, independent of the exploration guidance.

## Key Results
- HuGE outperforms direct human reward modeling, indiscriminate exploration, and demonstration-only baselines across navigation and manipulation tasks
- Pretraining from demonstrations accelerates learning and reduces human feedback requirements
- The approach scales to real-world robot learning, achieving tasks like pick-and-place and drawing from visual inputs using crowdsourced data from 109 non-expert annotators
- HuGE is robust to label noise and incomplete feedback, converging to optimal policies despite imperfect guidance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling human feedback from policy learning prevents the propagation of noisy or incorrect human preferences into the final policy.
- Mechanism: Human feedback steers exploration while self-supervised learning from exploration data yields unbiased policies.
- Core assumption: Self-supervised policy learning algorithm is unbiased and converges to optimal policy regardless of exploration bias.
- Evidence anchors: [abstract] "bifurcating human feedback and policy learning", [section 4.1] "decoupling the process of exploration from the policy learning"

### Mechanism 2
- Claim: Learning a parametric goal selector from binary comparisons allows sample-efficient exploration guidance even with infrequent asynchronous feedback.
- Mechanism: Binary comparisons train goal selector estimating state-goal distances, used to sample exploration goals.
- Core assumption: Parametric function can learn state-goal distance function from noisy binary comparisons.
- Evidence anchors: [section 4.2] "Learning State-Goal Distances from Binary Comparisons", [section 5.4] "Learning a parametric goal selector (Ours) is more sample efficient"

### Mechanism 3
- Claim: Pretraining policy and goal selector from demonstrations accelerates learning and reduces human feedback needed.
- Mechanism: Demonstrations pretrain policy via imitation learning and goal selector via pairwise state comparisons.
- Core assumption: Demonstrations contain useful information about goal-reaching behavior.
- Evidence anchors: [section 4.3] "Boostrapping Learning from Trajectory Demonstrations", [section 5.1] "pretraining HuGE with 5 noisy trajectories gives significant decrease"

## Foundational Learning

- Concept: Goal-conditioned reinforcement learning
  - Why needed here: Method solves goal-reaching tasks requiring policies that can reach various goal states.
  - Quick check question: What is the difference between goal-conditioned RL and standard RL?

- Concept: Hindsight relabeling
  - Why needed here: Allows self-supervised learning of goal-reaching policies from collected data even when desired goals are not initially reached.
  - Quick check question: How does hindsight relabeling address the challenge of sparse rewards in goal-conditioned RL?

- Concept: Exploration strategies (frontier expansion, density-based exploration)
  - Why needed here: Understanding limitations of different exploration methods is crucial for appreciating HuGE's directed exploration approach.
  - Quick check question: What are the drawbacks of indiscriminate exploration methods like Go-Explore in goal-reaching tasks?

## Architecture Onboarding

- Component map: Human Feedback Interface -> Goal Selector (neural network) -> Exploration Module (frontier expansion) -> Data Buffer -> Policy (GCSL with hindsight relabeling) -> Environment

- Critical path:
  1. Collect human feedback (binary comparisons)
  2. Train goal selector from feedback
  3. Use goal selector to guide exploration (frontier expansion)
  4. Collect trajectories via exploration
  5. Relabel trajectories using hindsight relabeling
  6. Train policy on relabeled data
  7. Repeat steps 3-6 until policy converges

- Design tradeoffs:
  - Tradeoff between query frequency and number of labels needed: More frequent queries lead to faster convergence but require more labels
  - Tradeoff between goal selector accuracy and exploration efficiency: More accurate goal selector leads to more efficient exploration but may require more human feedback to train

- Failure signatures:
  - Policy not improving: Check if goal selector is accurately guiding exploration or if self-supervised learning is working correctly
  - Goal selector not learning: Check if human feedback is informative or if the goal selector architecture is appropriate
  - Excessive exploration: Check if goal selector is too noisy or if exploration parameters need adjustment

- First 3 experiments:
  1. Implement goal selector training from synthetic binary comparisons
  2. Implement frontier expansion using goal selector for exploration guidance
  3. Implement self-supervised policy learning with hindsight relabeling on collected data

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but raises several implicit ones regarding scalability to higher-dimensional state spaces, alternative human feedback interfaces, and comparison to model-based exploration methods.

## Limitations
- Performance dependence on quality of human feedback, though robust to noise
- Lack of comparison to more recent goal-conditioned RL methods that use demonstrations
- Absence of scalability analysis for very high-dimensional state spaces beyond tested robotic manipulation tasks

## Confidence
- Decoupling mechanism (Mechanism 1): High confidence, supported by ablation studies and real-world experiments
- Goal selector sample efficiency (Mechanism 2): Medium confidence, lacks detailed hyperparameter tuning analysis
- Pretraining benefits (Mechanism 3): High confidence, demonstrated across multiple experimental settings

## Next Checks
1. **Noise robustness stress test**: Systematically evaluate performance degradation under varying levels of human feedback noise beyond the 25% tested
2. **Generalization to novel goals**: Test whether policies learned via HuGE can generalize to goals outside the distribution of human feedback
3. **Comparison to modern GCRL methods**: Benchmark against recent goal-conditioned RL approaches that also use demonstrations to establish relative performance gains