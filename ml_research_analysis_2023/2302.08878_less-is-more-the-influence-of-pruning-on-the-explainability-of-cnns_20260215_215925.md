---
ver: rpa2
title: 'Less is More: The Influence of Pruning on the Explainability of CNNs'
arxiv_id: '2302.08878'
source_url: https://arxiv.org/abs/2302.08878
tags:
- explainability
- pruning
- more
- image
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how network pruning affects the explainability
  of convolutional neural networks (CNNs). The authors hypothesize that reducing the
  number of parameters through pruning can improve human interpretability of model
  decisions.
---

# Less is More: The Influence of Pruning on the Explainability of CNNs

## Quick Facts
- arXiv ID: 2302.08878
- Source URL: https://arxiv.org/abs/2302.08878
- Reference count: 40
- Primary result: Mild pruning (CPR 2) improves both explainability and model performance, while higher pruning rates degrade interpretability.

## Executive Summary
This study investigates how network pruning affects the explainability of convolutional neural networks (CNNs). The authors hypothesize that reducing the number of parameters through pruning can improve human interpretability of model decisions. To test this, they prune a VGG-16 CNN with different compression rates (2, 4, 8, 32) and evaluate explainability using human-grounded experiments on Amazon Mechanical Turk. Two experiments are conducted: one assessing perceived reasonableness of heat-maps (subjective) and another measuring classification accuracy from occlusion-maps (objective). Results show that mild pruning (CPR 2) improves both explainability and model performance, while higher pruning rates degrade interpretability. A "sweet spot" is identified where reduced complexity enhances transparency without sacrificing accuracy, suggesting network pruning as a viable method to improve explainable AI.

## Method Summary
The authors fine-tune a pre-trained VGG-16 model on the Imagenette dataset (subset of ImageNet). They then apply iterative magnitude-based pruning to create models with compression rates (CPR) of 2, 4, 8, and 32, fine-tuning after each pruning step. Explainability is evaluated using Grad-CAM heat-maps and occlusion-maps, with human-grounded experiments conducted on Amazon Mechanical Turk. Two experiments assess subjective reasonableness of heat-maps and objective classification accuracy from occlusion-maps. The study compares the explainability and performance of pruned models against the unpruned baseline.

## Key Results
- Mild pruning (CPR 2) improves both explainability and model performance compared to the unpruned baseline
- Higher compression rates (CPR 4, 8, 32) degrade interpretability while also reducing accuracy
- Human accuracy in occlusion-map classification is highest for the CPR 2 model (86.40% vs 85.84% for unpruned)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mild pruning (CPR 2) improves Grad-CAM heat-map interpretability by reducing parameter redundancy while preserving important feature detectors.
- Mechanism: Pruning removes redundant filters that contribute noise to saliency maps, leaving more focused and discriminative heat-maps that align better with human visual reasoning.
- Core assumption: Grad-CAM relies on the last convolutional layer's feature maps; reducing redundant filters in that layer enhances the clarity of attribution maps without losing discriminative power.
- Evidence anchors:
  - [abstract] "lower compression rates have a positive influence on explainability"
  - [section 2.6] "50% of neurons can be removed without any changes in the output"
  - [corpus] Weak: No direct pruning-explainability citation, only general compression robustness studies
- Break condition: If pruning removes too many filters critical for class discrimination, Grad-CAM will highlight irrelevant regions, degrading explainability.

### Mechanism 2
- Claim: Mild pruning improves human classification accuracy from occlusion-maps because pruned models rely on more salient object features rather than latent context.
- Mechanism: Over-parameterized models attend to spurious correlations and background context; pruning forces the model to focus on object-centric features that are easier for humans to recognize in occlusion-maps.
- Core assumption: Occlusion-maps hide most of the image, so only the most discriminative features are visible; models that focus on those features will produce occlusion-maps humans can interpret.
- Evidence anchors:
  - [section 4.3] "CPR 2 model base their decision on the object in question... itself"
  - [section 4.2] "human-rater accuracy for the CPR 2 model (86.40%) over those produced with the unpruned model (85.84%)"
  - [corpus] Weak: No occlusion-map human study, only compression robustness
- Break condition: If pruning is too aggressive, the model loses discriminative power and occlusion-maps become ambiguous or misleading.

### Mechanism 3
- Claim: Mild pruning improves perceived reasonableness in subjective heat-map tasks because it reduces visual clutter in Grad-CAM overlays.
- Mechanism: Over-parameterized networks produce heat-maps with scattered high activations; pruning reduces this noise, yielding cleaner, more interpretable overlays that align with human intuition.
- Core assumption: Humans judge "reasonableness" by how well heat-map highlights correspond to visually salient object parts; cleaner heat-maps yield higher agreement.
- Evidence anchors:
  - [section 4.2] "proportion of participants rating both algorithms equally reasonable declines when increasing the CPR"
  - [section 4.1] "CPR 1 has worse explainability than the CPR 2 model"
  - [corpus] Weak: No subjective heat-map study, only compression benchmarks
- Break condition: If pruning removes important fine-grained features, heat-maps may under-highlight relevant regions, reducing perceived reasonableness.

## Foundational Learning

- Concept: Grad-CAM saliency map generation
  - Why needed here: The study's core evaluation method is Grad-CAM heat-maps and occlusion-maps; understanding how they are computed is essential to interpret pruning effects.
  - Quick check question: How does Grad-CAM compute the importance weight αc_k for each channel?
- Concept: Network pruning compression rate (CPR)
  - Why needed here: CPR is the independent variable; knowing that CPR 2 means 50% of parameters remain is critical to interpreting results.
  - Quick check question: If a CPR of 8 is applied to VGG-16, how many parameters remain relative to the original?
- Concept: Krippendorff's alpha for inter-rater agreement
  - Why needed here: The study uses Krippendorff's alpha to measure agreement among Mechanical Turk raters; understanding its scale helps interpret low reliability scores.
  - Quick check question: What does a Krippendorff's alpha of 0.086 indicate about rater agreement?

## Architecture Onboarding

- Component map: VGG-16 backbone → Grad-CAM hooks → Heat-map generation → Human rating pipeline (MTurk)
- Critical path: Model pruning → Fine-tuning → Grad-CAM extraction → Heat-map/occlusion-map rendering → MTurk task deployment
- Design tradeoffs: Iterative pruning vs one-shot; structured vs unstructured pruning; heat-maps vs occlusion-maps; five-point vs three-point Likert scale
- Failure signatures: Low Krippendorff's alpha (≤0.1) indicates disagreement; accuracy drop >5% suggests pruning too aggressive; high "I don't know" rates indicate occlusion-maps too ambiguous
- First 3 experiments:
  1. Replicate CPR 2 vs CPR 1 heat-map reasonableness test with a small MTurk sample to validate agreement levels
  2. Generate occlusion-maps for CPR 2, 4, 8 on a subset of classes and measure human accuracy to confirm the sweet spot
  3. Compare Grad-CAM activation distributions before and after CPR 2 pruning to visualize reduced noise

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the relationship between neural network pruning and explainability generalize across different CNN architectures (e.g., ResNet, Inception, Vision Transformers)?
- Basis in paper: [explicit] The authors note that "it would be interesting and probably more challenging to extend the experimental setup to other tasks such as object detection, image segmentation, or even image captioning" and "the generalization to other CNN architectures such as ResNets, Inception or EfficientNets should be examined."
- Why unresolved: The study only tested VGG-16 architecture, and the authors acknowledge that results may not directly translate to other architectures.
- What evidence would resolve it: Replicating the experiments with different CNN architectures and comparing explainability metrics across architectures would provide conclusive evidence.

### Open Question 2
- Question: How does the optimal pruning compression rate for explainability vary across different image classification tasks and datasets?
- Basis in paper: [explicit] The authors mention that "it would be interesting and probably more challenging to extend the experimental setup to other tasks such as object detection, image segmentation, or even image captioning" and tested only image classification on Imagenette dataset.
- Why unresolved: The study focused on a single task (image classification) and dataset, leaving uncertainty about generalizability to other tasks and data.
- What evidence would resolve it: Conducting similar experiments across multiple image classification datasets and extending to other computer vision tasks would reveal task-specific optimal pruning rates.

### Open Question 3
- Question: What is the impact of neural network pruning on the internal mechanisms of Grad-CAM and how do these changes relate to human perception of explainability?
- Basis in paper: [inferred] The authors suggest in the future work section that "it might be worthy to examine the impact of NN pruning on the internal mechanisms of GradCAM (e.g. in the used activation maps) and how these changes are reﬂected in our human-grounded experiment results."
- Why unresolved: While the study showed correlations between pruning and explainability, it did not investigate the underlying mechanisms of how pruning affects the Grad-CAM algorithm itself.
- What evidence would resolve it: Analyzing the activation maps and saliency patterns at different pruning rates, combined with ablation studies of Grad-CAM components, would clarify the mechanistic relationship between pruning and explainability.

## Limitations
- Limited to VGG-16 architecture, may not generalize to other CNN architectures
- Subjective explainability metrics depend on human interpretation and Mechanical Turk participant selection
- No ablation studies on pruning methods (structured vs. unstructured) or layer-specific effects

## Confidence
- High confidence: The correlation between mild pruning and improved accuracy metrics is well-supported by empirical results.
- Medium confidence: The subjective explainability improvements, while statistically significant, depend on human interpretation that may vary across contexts.
- Low confidence: The generalizability of the "sweet spot" finding to other architectures, datasets, or pruning methodologies remains untested.

## Next Checks
1. Replicate the study using ResNet and EfficientNet architectures on larger datasets like full ImageNet to test architectural generalizability.
2. Conduct cross-cultural validation of the subjective explainability metrics with participants from different geographic regions.
3. Implement ablation studies comparing structured vs. unstructured pruning methods while controlling for compression rate.