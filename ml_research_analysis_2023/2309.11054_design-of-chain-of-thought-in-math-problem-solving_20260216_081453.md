---
ver: rpa2
title: Design of Chain-of-Thought in Math Problem Solving
arxiv_id: '2309.11054'
source_url: https://arxiv.org/abs/2309.11054
tags:
- program
- language
- cots
- arxiv
- wolfram
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper conducts a comprehensive comparison of chain-of-thought
  (CoT) designs for math problem solving, focusing on natural language CoTs and three
  types of program CoTs: self-describing, comment-describing, and non-describing.
  The study examines the impact of programming language (Python vs.'
---

# Design of Chain-of-Thought in Math Problem Solving

## Quick Facts
- arXiv ID: 2309.11054
- Source URL: https://arxiv.org/abs/2309.11054
- Reference count: 12
- Primary result: Program CoTs outperform natural language CoTs for math problem solving, with self-describing Python programs achieving the best results

## Executive Summary
This paper conducts a comprehensive comparison of chain-of-thought (CoT) designs for math problem solving, focusing on natural language CoTs and three types of program CoTs: self-describing, comment-describing, and non-describing. The study examines the impact of programming language (Python vs. Wolfram) on program CoT performance using datasets GSM8K, MATHQA, and SVAMP. Using supervised fine-tuning on a 30B parameter model, the authors compare various methods including majority voting and reward model reranking. The results show that program CoTs generally outperform natural language CoTs, with self-describing programs achieving the best performance. Python is found to be a better choice than Wolfram for program CoTs.

## Method Summary
The authors fine-tune a 30B parameter model (Galactica) using supervised learning on datasets annotated with various CoT types. They compare natural language CoTs with three program CoT variants (self-describing, comment-describing, and non-describing) in both Python and Wolfram languages. The evaluation uses multiple methods: direct execution, majority voting across sampled solutions, and reward model reranking. The study systematically analyzes execution rates, precision, and diversity metrics across different CoT types and programming languages.

## Key Results
- Program CoTs outperform natural language CoTs across all datasets
- Self-describing programs achieve the highest correct@100 scores
- Python outperforms Wolfram for program CoTs
- The best-performing combination (30B model with reward model reranking) significantly outperforms GPT-3.5-turbo on all three datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Program CoTs outperform natural language CoTs because they allow direct execution and verification of intermediate steps, reducing errors in complex reasoning.
- Mechanism: Programs can be executed by an interpreter (Wolfram Mathematica or Python) to verify intermediate results, while natural language reasoning cannot be automatically checked.
- Core assumption: The program CoTs generated by the model are syntactically correct and executable, producing reliable intermediate results.
- Evidence anchors:
  - [abstract] "One advantage of the program CoTs is that their validity can be easily verified by executing the programs."
  - [section 2.2] "The Wolfram Language, with the Wolfram Mathematica as its execution engine, is an expressive and versatile language that can effectively represent complex mathematical concepts."
- Break condition: If the generated programs contain syntax errors or logical flaws that prevent execution, the verification advantage disappears.

### Mechanism 2
- Claim: Self-describing programs achieve higher performance than comment-describing and non-describing programs because they provide more diverse reasoning paths.
- Mechanism: Using natural language variable names from the problem statement allows the model to generate more varied and context-relevant programs, increasing the chance of finding correct solutions through sampling methods.
- Core assumption: Diversity in program structure and variable naming leads to better coverage of possible solution paths during majority voting and reranking.
- Evidence anchors:
  - [abstract] "The results show that self-describing program offers greater diversity and thus can generally achieve higher performance."
  - [section 6.2] "SDP has the lowest execution rate but the highest correct@100 score and relatively high precision."
- Break condition: If the increased diversity leads to more execution errors or if the model cannot effectively sample from the diverse space.

### Mechanism 3
- Claim: Python outperforms Wolfram Language for program CoTs because Python programs are more similar to the pre-training corpus, improving generation quality.
- Mechanism: The model has been trained on more Python code examples than Wolfram code, making it better at generating syntactically correct and semantically meaningful Python programs.
- Core assumption: Pre-training data distribution strongly influences the model's ability to generate correct programs in a given language.
- Evidence anchors:
  - [abstract] "We also find that Python is a better choice of language than Wolfram for program CoTs."
  - [section 5.3] "The program CoTs in Python work better than the program CoTs in Wolfram."
- Break condition: If the problem domain favors Wolfram's mathematical functions, or if the pre-training data contains sufficient Wolfram examples.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CoT is the fundamental technique being extended from natural language to program representations, so understanding its basic mechanism is essential.
  - Quick check question: What is the main purpose of CoT prompting in mathematical problem solving?

- Concept: Supervised fine-tuning vs. in-context learning
  - Why needed here: The paper uses supervised fine-tuning on annotated datasets rather than few-shot prompting, requiring understanding of how parameter updates improve performance.
  - Quick check question: How does supervised fine-tuning differ from few-shot prompting in terms of model adaptation?

- Concept: Reward model reranking
  - Why needed here: Reranking is one of the key evaluation methods, requiring understanding of how reward models score and select among candidate solutions.
  - Quick check question: What is the role of a reward model in the reranking process for CoT solutions?

## Architecture Onboarding

- Component map: Data collection pipeline: few-shot prompting → LLM annotation → automatic verification → manual refinement → Model training: supervised fine-tuning → reward model training → sampling strategies → Evaluation methods: direct execution → majority voting → reward model reranking

- Critical path: Question → CoT generation → program execution/verification → answer extraction → evaluation (voting/reranking)

- Design tradeoffs:
  - Language choice: Python (more training data, general purpose) vs. Wolfram (better math functions, less common)
  - Program type: SDP (diverse, error-prone) vs. CDP (balanced) vs. NDP (precise, less informative)
  - Sampling strategy: number of samples (more = better coverage but higher cost)

- Failure signatures:
  - Low execution rate: indicates syntax errors or incompatible program structure
  - High null-result answers: suggests logical errors in problem-solving approach
  - Poor majority voting performance: indicates lack of diversity or precision in generated solutions

- First 3 experiments:
  1. Compare execution rates and correct@100 scores across SDP, CDP, and NDP on a small validation set
  2. Test Python vs. Wolfram performance on identical problems to quantify language effect
  3. Vary the number of samples in majority voting to find the optimal balance between performance and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between diversity and precision in chain-of-thought designs for math problem solving?
- Basis in paper: [explicit] The authors observe that self-describing programs provide more diversity while comment-describing programs offer higher precision. They conclude that having a balance of diversity and precision is crucial for higher performance in voting and reranking.
- Why unresolved: The paper demonstrates the trade-off between diversity and precision but does not provide a quantitative method for determining the optimal balance between these two factors.
- What evidence would resolve it: Experiments varying the ratio of SDP to CDP in ensembles, measuring the resulting performance to find the optimal mix for different types of math problems.

### Open Question 2
- Question: How can different chain-of-thought types be effectively combined to maximize performance in math problem solving?
- Basis in paper: [explicit] The authors show that combining NL, SDP, and CDP CoT types in equal proportions during majority voting and reranking leads to significant improvements over the best individual CoT type. They state this suggests potential for further CoT design that integrates strengths of all CoT types.
- Why unresolved: While the paper demonstrates the potential of combining CoT types, it does not explore more sophisticated methods of integration beyond equal weighting.
- What evidence would resolve it: Experiments with weighted combinations of CoT types, or with adaptive selection of CoT types based on problem characteristics, to determine optimal integration strategies.

### Open Question 3
- Question: What is the impact of CoT representation on the performance of different model scales in math problem solving?
- Basis in paper: [inferred] The authors compare 6.7B and 30B models across different CoT types and languages, showing varying performance gains. They note that larger models can significantly improve performance over smaller models on all datasets.
- Why unresolved: The paper does not systematically analyze how the effectiveness of different CoT representations scales with model size.
- What evidence would resolve it: Experiments with a wider range of model scales (e.g., 1B, 3B, 13B, 65B) using the same CoT representations to map performance curves and identify scaling patterns.

## Limitations

- The annotation process for creating CoTs across datasets is only partially specified, making exact replication challenging
- The study uses a single 30B parameter model (Galactica), limiting generalizability to other model families or scales
- The comparison focuses on three program CoT variants and two languages, potentially missing other effective representations or programming languages

## Confidence

**High Confidence:**
- Program CoTs outperform natural language CoTs across all datasets
- Self-describing programs achieve the highest correct@100 scores
- Python consistently outperforms Wolfram for program CoTs

**Medium Confidence:**
- Self-describing programs provide greater diversity leading to better performance
- The specific ranking of CoT types (SDP > CDP > NDP) holds across all conditions
- Reward model reranking provides consistent improvements across all datasets

**Low Confidence:**
- The mechanisms explaining why Python outperforms Wolfram are fully understood
- The specific 8% improvement over GPT-3.5-turbo on SVAMP is robust to different evaluation protocols
- The relative performance of CDP vs NDP is stable across different model scales

## Next Checks

1. **Execution Rate Analysis**: Verify the execution rates reported for each program CoT type (SDP, CDP, NDP) by independently executing the generated programs on a held-out validation set. This would confirm whether the claimed 70-90% execution rates are reproducible and whether they correlate with performance as stated.

2. **Language Swap Experiment**: Conduct a controlled experiment swapping Python and Wolfram implementations for identical problems to quantify the exact performance difference attributable to language choice. This would validate whether the observed Python advantage holds when controlling for other variables like problem domain or annotation quality.

3. **Sample Size Sensitivity**: Systematically vary the number of samples used in majority voting (e.g., 10, 50, 100, 200) to determine if the reported correct@100 performance is robust or whether diminishing returns suggest the true optimum lies elsewhere. This would test the claim that self-describing programs benefit more from sampling due to diversity.