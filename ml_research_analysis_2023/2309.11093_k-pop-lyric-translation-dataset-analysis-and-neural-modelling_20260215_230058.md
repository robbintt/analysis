---
ver: rpa2
title: 'K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling'
arxiv_id: '2309.11093'
source_url: https://arxiv.org/abs/2309.11093
tags:
- lyrics
- translation
- english
- dataset
- lyric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces a novel singable lyric translation dataset
  with 1,000 song pairs, of which approximately 89% are K-pop songs, aligned line-by-line
  and section-by-section. We revealed unique characteristics of K-pop lyric translation:
  high English usage (30.2% of lines fully in English, 20.7% mixed), section-wise
  semantic similarity rather than line-wise, and high phoneme repetition variability.'
---

# K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling

## Quick Facts
- arXiv ID: 2309.11093
- Source URL: https://arxiv.org/abs/2309.11093
- Reference count: 40
- Primary result: Introduced a novel singable K-pop lyric translation dataset with 1,000 aligned pairs, revealing unique characteristics like high English usage (30.2% lines fully English, 20.7% mixed) and section-wise semantic similarity.

## Executive Summary
This study presents the first comprehensive analysis of K-pop lyric translation characteristics and develops a neural model specifically for singable lyric translation. The research introduces a manually aligned dataset of 1,000 Korean-English song pairs, where 89% are K-pop songs, enabling detailed analysis of translation patterns unique to this genre. The dataset's line-by-line and section-by-section alignment allows for evaluation of both singability constraints and semantic coherence at appropriate granularities. The neural model employs a three-stage training approach, starting with general machine translation data, then non-singable machine-translated lyrics, and finally fine-tuning on the singable dataset. This approach successfully improves syllable count matching and phonetic repetition fidelity while maintaining reasonable semantic accuracy.

## Method Summary
The method involves creating a manually aligned dataset of Korean-English song lyrics with 1,000 pairs, then training a transformer-based Marian MT model through three stages: first on 500K general Korean-English pairs, then on 10K machine-translated lyrics, and finally fine-tuning on the singable dataset. The model uses optional syllable tokens (<SYL>) to condition generation on target syllable counts. Training employs Adam optimizer with 6 layers, 8 attention heads, and batch size 8, stopping when validation loss plateaus for 3 epochs. Evaluation metrics include syllable count distance (SCD), error rate, section-wise semantic similarity (Semsec), next-sentence prediction (NSP) score, and phoneme repetition metrics (pho, Phodeg, Phovar).

## Key Results
- The fine-tuned models significantly improved syllable count matching, reducing SCD from 0.45 to 0.23
- Phoneme repetition fidelity closely matched real-world K-pop translation patterns after fine-tuning
- The dataset revealed unique K-pop characteristics: 30.2% of lines fully in English, 20.7% mixed, and section-wise rather than line-wise semantic similarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Line-by-line and section-by-section alignment enables models to learn singability constraints without explicit melody input
- Mechanism: Manual alignment forces source-target pairs to have matching syllable counts and rhythmic patterns, allowing models to infer these constraints from text alone
- Core assumption: Human alignment accurately captures singable correspondence between Korean and English lyrics
- Evidence anchors: Manual alignment described in dataset creation; weak corpus support for alignment quality

### Mechanism 2
- Claim: <SYL> tokens explicitly teach the model to match syllable counts during generation
- Mechanism: <SYL> tokens encode target syllable counts, conditioning the model to produce lines of specified length
- Core assumption: Models can effectively learn to map <SYL> token values to actual syllable counts
- Evidence anchors: Comparison of models with/without syllable tokens; weak corpus support for token effectiveness

### Mechanism 3
- Claim: Fine-tuning on singable data significantly improves phonetic repetition patterns
- Mechanism: Exposure to real-world singable pairs teaches characteristic high phoneme repetition and variability patterns
- Core assumption: Singable lyrics in dataset exhibit same phonetic patterns as target K-pop translations
- Evidence anchors: Qualitative examples showing improved phonetic patterns; weak corpus support for phonetic transfer

## Foundational Learning

- Concept: Line-by-line and section-by-section alignment
  - Why needed here: Enables evaluation of singability constraints and semantic coherence at appropriate granularities
  - Quick check question: What is the difference between line-wise and section-wise semantic similarity in this context?

- Concept: Syllable counting and tokenization
  - Why needed here: Critical for ensuring generated lyrics match melodic structure and for implementing <SYL> conditioning
  - Quick check question: How does the <SYL> token value relate to actual syllable counts in English versus Korean text?

- Concept: Phoneme repetition metrics (pho, Phodeg, Phovar)
  - Why needed here: Quantifies K-pop's characteristic phonetic patterns for both evaluation and model learning
  - Quick check question: Why does a lower pho value indicate higher phoneme repetition?

## Architecture Onboarding

- Component map: Data preprocessing pipeline: General MT data → Machine-translated lyrics → Singable dataset -> Transformer model with Korean-English tokenizer -> NLL loss minimization -> Evaluation suite

- Critical path: Load aligned dataset → Apply <SYL> token insertion → Train on general MT data → Fine-tune on machine-translated lyrics → Fine-tune on singable dataset → Evaluate on held-out K-pop lyrics

- Design tradeoffs:
  - Line-wise vs section-wise generation: Line-wise offers better semantic coherence but struggles with section-level patterns
  - <SYL> token inclusion: Improves syllable matching but may constrain semantic flexibility
  - Manual alignment vs automatic: Manual ensures quality but limits scalability

- Failure signatures:
  - High SCD and error rate: Model fails to match syllable counts
  - Low NSP score: Generated lines lack semantic coherence
  - Phodeg mismatch: Model fails to capture K-pop's repetitive phonetic patterns

- First 3 experiments:
  1. Train baseline model without <SYL> tokens on general MT data only
  2. Add <SYL> tokens and retrain to measure impact on syllable matching
  3. Fine-tune on singable dataset and compare all metrics against semi-supervised model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of non-singable machine-translated lyrics affect the overall performance of the neural lyric translation model compared to using only singable human-translated lyrics?
- Basis in paper: The paper mentions using non-singable machine-translated lyrics due to data scarcity but does not provide detailed comparison of model performance with and without this data
- Why unresolved: No explicit comparison of models trained with and without non-singable machine-translated lyrics dataset
- What evidence would resolve it: Experiments training models with and without non-singable data and comparing performance on various metrics

### Open Question 2
- Question: What is the impact of using different syllable tokenization methods on singability and semantic accuracy?
- Basis in paper: Paper discusses <SYL> tokens but notes efficacy only proven in Mandarin text generation
- Why unresolved: No comprehensive comparison of different syllable tokenization methods' impact
- What evidence would resolve it: Experiments with different syllable tokenization methods comparing singability and semantic accuracy

### Open Question 3
- Question: How does the model's performance vary across different genres of K-pop songs?
- Basis in paper: Paper focuses on K-pop characteristics but doesn't explore genre-specific variations
- Why unresolved: No detailed analysis of performance across different K-pop genres
- What evidence would resolve it: Experiments evaluating model performance on different K-pop genres and analyzing genre-specific influences

## Limitations

- The dataset contains only one citation for related work, making independent validation of alignment quality and K-pop translation patterns difficult
- The trade-off between singability and semantic accuracy remains poorly quantified, relying on qualitative examples rather than systematic measurement
- The three-stage training procedure introduces uncertainty about whether improvements are additive or simply reflect the most recent fine-tuning stage

## Confidence

- High confidence: Dataset creation methodology and basic characteristics (1,000 song pairs, 89% K-pop content, line-by-line alignment)
- Medium confidence: Unique characteristics of K-pop lyric translation (high English usage, section-wise semantic similarity, phoneme repetition patterns)
- Low confidence: Effectiveness of three-stage training procedure in achieving optimal trade-off between singability and semantic accuracy

## Next Checks

1. **Independent alignment validation**: Recruit 3-5 native Korean/English bilingual speakers to independently align 100 randomly selected song pairs and measure inter-rater agreement to establish manual alignment quality.

2. **Cross-genre comparison**: Apply the same pho metrics to lyric translations from other genres (Western pop, Japanese anime songs, Chinese pop) to determine whether K-pop patterns are truly unique or genre-independent.

3. **Ablation study of training stages**: Train separate models using only one or two of the three training datasets and compare their performance on the validation set to isolate which training stages contribute most to singability versus semantic accuracy.