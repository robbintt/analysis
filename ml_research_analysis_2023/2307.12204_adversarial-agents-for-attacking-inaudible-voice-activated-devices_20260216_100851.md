---
ver: rpa2
title: Adversarial Agents For Attacking Inaudible Voice Activated Devices
arxiv_id: '2307.12204'
source_url: https://arxiv.org/abs/2307.12204
tags:
- attack
- inaudible
- devices
- network
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel application of reinforcement learning
  to simulate and analyze inaudible attacks on voice-activated devices, specifically
  using Microsoft's CyberBattleSim framework. The study models a five-node network
  scenario involving an Amazon Echo Dot, email account, iPhone, smart door, and classified
  laptop to demonstrate how inaudible commands can be used to gain unauthorized access
  to sensitive information.
---

# Adversarial Agents For Attacking Inaudible Voice Activated Devices

## Quick Facts
- arXiv ID: 2307.12204
- Source URL: https://arxiv.org/abs/2307.12204
- Reference count: 20
- One-line primary result: Reinforcement learning can simulate and optimize inaudible attacks on voice-activated devices, with Deep-Q learning with exploitation proving most effective

## Executive Summary
This paper presents a novel application of reinforcement learning to simulate and analyze inaudible attacks on voice-activated devices. Using Microsoft's CyberBattleSim framework, the study models a five-node network scenario involving an Amazon Echo Dot, email account, iPhone, smart door, and classified laptop to demonstrate how inaudible commands can be used to gain unauthorized access to sensitive information. The research identifies Deep-Q learning with exploitation as the optimal reinforcement learning algorithm for rapidly conquering all nodes in the simulated network, underscoring the critical need for new cybersecurity measures to address this emerging attack surface.

## Method Summary
The research uses CyberBattleSim to simulate inaudible attacks on voice-activated devices, modeling a five-node network with specific properties and vulnerabilities. Six reinforcement learning algorithms were implemented and compared, with Deep-Q learning with exploitation emerging as optimal. The attack methodology involves using Single Upper Sideband Amplitude Modulation (SUSBAM) to generate inaudible commands in the 16-22 kHz range that bypass human perception while still being processed by VAS microphones.

## Key Results
- Deep-Q learning with exploitation proved optimal for rapid node ownership in the simulated network
- Inaudible commands using SUSBAM modulation can successfully compromise interconnected VAS devices
- The five-node scenario demonstrated complete network takeover in minimal steps using RL-optimized attack paths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement learning agents can discover and execute inaudible voice commands to compromise interconnected VAS devices.
- Mechanism: The RL agent navigates a simulated network graph where nodes represent devices. By learning from reward signals tied to successful node ownership, the agent identifies attack paths that chain inaudible commands across devices.
- Core assumption: The simulation environment accurately reflects real-world vulnerabilities in VAS hardware and network topology.
- Evidence anchors:
  - [abstract] "Using Microsoft's CyberBattleSim framework, we evaluated six reinforcement learning algorithms and found that Deep-Q learning with exploitation proved optimal..."
  - [section] "The research models the CyberBattleSim as a compatible RL environment...to investigate lateral movements across different VAS node connections."
  - [corpus] Weak: Related papers focus on acoustic attacks but do not confirm RL-based simulations.

### Mechanism 2
- Claim: Inaudible voice commands can bypass human perception while still being processed by VAS microphones.
- Mechanism: The attack uses near-ultrasonic modulation (16-22 kHz) via Single Upper Sideband Amplitude Modulation (SUSBAM) to encode commands that the microphone demodulates into actionable audio for the VAS.
- Core assumption: The VAS microphone's analog-to-digital conversion does not filter out these near-ultrasonic frequencies.
- Evidence anchors:
  - [section] "The method transforms a spoken command to a voice-activated device using the modulated audio signal converted into a frequency range (16-22 kHz) beyond human-adult hearing."
  - [abstract] "Our findings underscore the critical need for understanding non-conventional networks and new cybersecurity measures...particularly those characterized by mobile devices, voice activation, and non-linear microphones susceptible to malicious actors operating stealth attacks in the near-ultrasound or inaudible ranges."
  - [corpus] Weak: Related papers discuss acoustic attacks but do not confirm SUSBAM modulation specifics.

### Mechanism 3
- Claim: Deep-Q learning with exploitation outperforms other RL algorithms in this attack simulation.
- Mechanism: The algorithm learns a Q-matrix approximating optimal state-action values, then exploits this knowledge to maximize node ownership rewards in minimal steps without exploring alternative paths.
- Core assumption: The state-action space is finite and the Q-matrix can be learned effectively without missing better global solutions.
- Evidence anchors:
  - [abstract] "We evaluated six reinforcement learning algorithms and found that Deep-Q learning with exploitation proved optimal, leading to rapid ownership of all nodes in fewer steps."
  - [section] "Table 2... Deep Q-Learning (DQL) uses neural network to approximate Q-function, suitable for high-dimensional state spaces... Exploiting DQL uses trained DQL to choose the action with the highest estimated Q-value."
  - [corpus] Missing: No direct comparison of RL algorithms in related work.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: RL algorithms like Q-learning rely on MDP theory to model the decision-making environment with states, actions, and rewards.
  - Quick check question: What is the difference between an MDP and a deterministic state machine?

- Concept: Microphone signal processing and non-linear demodulation
  - Why needed here: Understanding how inaudible frequencies are converted into audible commands is critical to modeling the attack surface.
  - Quick check question: How does a non-linear microphone respond to near-ultrasonic input signals?

- Concept: Cyber attack tactics and techniques (MITRE ATT&CK)
  - Why needed here: The simulation maps attack stages to known frameworks, requiring familiarity with concepts like lateral movement and indirect command execution.
  - Quick check question: What MITRE ATT&CK technique describes command injection through voice-activated systems?

## Architecture Onboarding

- Component map:
  CyberBattleSim environment -> RL agent (Deep-Q learning with exploitation) -> Simulated VAS nodes (Echo Dot, iPhone, laptop, etc.) -> Reward system -> Inaudible command generator (SUSBAM modulation)

- Critical path:
  1. Initialize CyberBattleSim with network topology
  2. Agent explores nodes via RL actions
  3. Agent discovers and owns nodes using inaudible commands
  4. Agent chains ownership across devices to access target data

- Design tradeoffs:
  - Exploration vs. exploitation: Exploitation speeds convergence but risks local optima
  - Simulation fidelity vs. computational cost: More realistic models slow training
  - Generalization vs. specificity: Broad simulations miss device-specific vulnerabilities

- Failure signatures:
  - Agent fails to progress beyond initial nodes (reward structure issue)
  - Simulation diverges from real-world attack feasibility (environment modeling issue)
  - RL agent learns ineffective policies (hyperparameter or architecture issue)

- First 3 experiments:
  1. Run baseline CyberBattleSim with random search to establish lower performance bound
  2. Implement Deep-Q learning with epsilon-greedy exploration to compare with baseline
  3. Test inaudible command feasibility in a controlled simulation of a single VAS device

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Deep-Q learning with exploitation compare to other reinforcement learning algorithms in larger, more complex network scenarios involving voice-activated devices?
- Basis in paper: [explicit] The paper identifies Deep-Q learning with exploitation as optimal for the five-node network scenario, leading to rapid ownership of all nodes in fewer steps.
- Why unresolved: The study focused on a small-scale network model, and the scalability and effectiveness of the algorithm in larger, more complex scenarios remain unexplored.
- What evidence would resolve it: Empirical results from simulations involving larger networks with more nodes and diverse configurations, comparing the performance of Deep-Q learning with exploitation against other reinforcement learning algorithms.

### Open Question 2
- Question: What are the specific vulnerabilities in voice-activated devices that allow inaudible commands to be executed successfully, and how can these be mitigated?
- Basis in paper: [explicit] The paper discusses the issue of authentication in voice-activated devices and the susceptibility to inaudible commands due to the design of microphones and digital signal processing.
- Why unresolved: While the paper highlights the problem, it does not delve into the technical details of these vulnerabilities or propose specific mitigation strategies.
- What evidence would resolve it: Technical analysis of the vulnerabilities in voice-activated devices, including the microphone design and signal processing, and the development and testing of potential mitigation strategies.

### Open Question 3
- Question: How can the CyberBattleSim framework be extended to model the initial reconnaissance stage of inaudible attacks on voice-activated devices?
- Basis in paper: [inferred] The paper focuses on the delivery, exploitation, and action stages of the attack chain, with reconnaissance modeled as a fundamental vulnerability of VAS to reveal email and discover other assets.
- Why unresolved: The paper does not provide a detailed approach for modeling the reconnaissance stage, which is crucial for understanding the full attack process.
- What evidence would resolve it: A comprehensive extension of the CyberBattleSim framework that includes detailed modeling of the reconnaissance stage, including the methods attackers use to gather information and identify vulnerabilities before launching an inaudible attack.

## Limitations
- The simulation environment's fidelity to real-world VAS behavior remains unverified, particularly regarding microphone non-linearities
- Specific SUSBAM modulation parameters and reward function design choices are underspecified
- The study focuses on a small-scale network model, limiting generalizability to larger, more complex scenarios

## Confidence
- High confidence in the methodological framework and CyberBattleSim integration
- Medium confidence in the RL algorithm comparisons and performance rankings
- Low confidence in the attack mechanism's practical feasibility without hardware validation

## Next Checks
1. Implement the SUSBAM modulation technique in a controlled acoustic environment to verify inaudible command generation and successful VAS processing
2. Compare CyberBattleSim attack paths with actual device firmware vulnerabilities using penetration testing on real VAS hardware
3. Test the Deep-Q learning with exploitation algorithm on alternative network configurations to assess robustness and generalization capabilities