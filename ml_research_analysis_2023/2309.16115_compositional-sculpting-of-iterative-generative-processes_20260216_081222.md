---
ver: rpa2
title: Compositional Sculpting of Iterative Generative Processes
arxiv_id: '2309.16115'
source_url: https://arxiv.org/abs/2309.16115
tags:
- classifier
- diffusion
- distributions
- base
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Compositional Sculpting is a general method for composing iterative
  generative models like GFlowNets and diffusion models. The core idea is to define
  a probabilistic model over base distributions and observations, and use classifier
  guidance to sample from the resulting compositions.
---

# Compositional Sculpting of Iterative Generative Processes

## Quick Facts
- arXiv ID: 2309.16115
- Source URL: https://arxiv.org/abs/2309.16115
- Reference count: 40
- Key outcome: Introduces Compositional Sculpting as a general method for composing iterative generative models like GFlowNets and diffusion models using classifier guidance

## Executive Summary
This paper introduces Compositional Sculpting, a method for composing iterative generative models through classifier-guided sampling. The approach defines a probabilistic model over base distributions and observations, then uses classifier guidance to sample from the resulting compositions. Two key binary operations - harmonic mean and contrast - are introduced, analogous to product and negation for energy-based models. The method is demonstrated to work for both GFlowNets and diffusion models, enabling flexible control over the composition by emphasizing or de-emphasizing regions where specific base models have high density.

## Method Summary
Compositional Sculpting defines a probabilistic model where each base generative model (GFlowNet or diffusion model) is treated as a component of a mixture prior. The composition is achieved by conditioning this mixture on observations about which base model generated each object. Classifier guidance is then used to sample from the resulting conditional posterior. The method introduces harmonic mean and contrast operations for composing pairs of distributions, and extends these to multiple components. Training involves first training the base models, then training a classifier to approximate the posterior distribution given intermediate states, and finally applying classifier guidance to sample from the composition.

## Key Results
- Demonstrates compositional sculpting for both GFlowNets and diffusion models using the same classifier guidance framework
- Introduces harmonic mean and contrast operations as alternatives to product and negation for energy-based models
- Shows empirical effectiveness on molecular generation and image generation tasks
- Enables flexible control over composition by adjusting classifier guidance strength and mixture weights

## Why This Works (Mechanism)

### Mechanism 1
Classifier guidance enables direct sampling from composite distributions without MCMC corrections. The method frames composition as conditioning a probabilistic model on observations about which base model generated each object. Core assumption: The base models' sampling policies can be combined via a uniform mixture prior, and classifier-guided sampling yields unbiased samples from the composition.

### Mechanism 2
Binary harmonic mean and contrast operations are alternatives to product and negation operations for EBMs. The harmonic mean operation assigns high likelihood to points with high density under both base distributions, while contrast emphasizes regions where one distribution is high but the other is low. Core assumption: The conditional distributions implied by the observation model result in valid probability distributions for these operations.

### Mechanism 3
GFlowNet and diffusion models can be composed using the same classifier guidance framework due to their shared sequential refinement structure. Both models generate objects through iterative refinement processes, allowing classifier guidance to be applied in a similar manner to achieve composition. Core assumption: The sequential nature of both GFlowNets and diffusion models allows classifier guidance to be applied in a similar manner to achieve composition.

## Foundational Learning

- Concept: Classifier guidance in diffusion models
  - Why needed here: The method extends classifier guidance to sample from compositions of base distributions
  - Quick check question: How does classifier guidance modify the sampling process in diffusion models to achieve conditional generation?

- Concept: GFlowNet forward policies and trajectory balance
  - Why needed here: Understanding how GFlowNets generate objects through sequences of actions is crucial for applying classifier guidance to their composition
  - Quick check question: What is the role of the trajectory balance condition in ensuring valid sampling from GFlowNet policies?

- Concept: Probabilistic modeling with conditioning
  - Why needed here: The method frames composition as conditioning a probabilistic model on observations about which base model generated each object
  - Quick check question: How does conditioning a mixture distribution on observations about the generating base model lead to the harmonic mean and contrast operations?

## Architecture Onboarding

- Component map: Base models -> Probabilistic model -> Classifier -> Classifier-guided sampling
- Critical path: Train base models ‚Üí Define probabilistic model ‚Üí Train classifier ‚Üí Apply classifier guidance to sample from composition
- Design tradeoffs: Mixture weights vs. classifier guidance strength (ùõº parameter), number of observations (ùëõ) vs. complexity of classifier, sequential vs. simultaneous conditioning of observations
- Failure signatures: High classifier cross-entropy loss, samples deviating from expected composition, numerical instability in harmonic mean or contrast operations
- First 3 experiments:
  1. Implement harmonic mean operation on two simple Gaussian base distributions
  2. Apply contrast operation to two GFlowNets trained on molecular generation with different reward functions
  3. Chain binary operations to compose three diffusion models for colored MNIST digit generation

## Open Questions the Paper Calls Out

### Open Question 1
What are the precise mathematical conditions under which the harmonic mean operation (p1 ‚äó p2) is well-defined for arbitrary distributions? The paper states it's undefined when distributions have disjoint supports but doesn't provide a complete characterization.

### Open Question 2
How does the performance of classifier-guided compositional sculpting scale with the number of base models (m) and observations (n)? The paper only presents results for small values of m and n without investigating scalability.

### Open Question 3
What are the theoretical guarantees for the convergence of classifier-guided sampling in compositional sculpting? The paper focuses on practical implementation but doesn't establish formal convergence guarantees.

## Limitations
- Numerical stability issues when base distributions have low or zero density in certain regions
- Potential scalability challenges with many base models and observations
- Limited theoretical analysis of convergence properties for classifier-guided sampling

## Confidence
**High Confidence**: Core formulation of composition as probabilistic modeling is mathematically sound, harmonic mean and contrast operations are correctly defined, connection between classifier guidance and sampling from posterior is valid

**Medium Confidence**: Empirical results demonstrate effectiveness on tested domains, claim that similar methods apply to both GFlowNets and diffusion models is supported by their shared structure, classifier guidance can produce valid samples from composed distributions

**Low Confidence**: Performance on more complex distributions or higher-dimensional data, scalability to compositions involving many base models, robustness when base models have significant overlap or disjoint supports

## Next Checks
1. Conduct numerical stability analysis by systematically testing harmonic mean and contrast operations on base distributions with varying degrees of overlap and density
2. Evaluate classifier generalization by testing the method's performance when classifier training data is limited, comparing composition quality with different amounts of training data
3. Apply compositional sculpting to a different type of iterative generative model (e.g., autoregressive models) to assess whether classifier guidance can still be effectively used for composition in this new domain