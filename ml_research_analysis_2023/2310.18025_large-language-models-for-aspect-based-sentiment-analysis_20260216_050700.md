---
ver: rpa2
title: Large language models for aspect-based sentiment analysis
arxiv_id: '2310.18025'
source_url: https://arxiv.org/abs/2310.18025
tags:
- gpt-3
- task
- aspect
- polarity
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fine-tuned GPT-3.5 achieves a state-of-the-art F1 score of 83.8
  on the SemEval-2014 Task 4 joint aspect term extraction and polarity classification
  task, improving upon InstructABSA by 5.7%. This comes at the price of 1000 times
  more model parameters and thus increased inference cost compared to specialized
  models.
---

# Large language models for aspect-based sentiment analysis

## Quick Facts
- arXiv ID: 2310.18025
- Source URL: https://arxiv.org/abs/2310.18025
- Authors: 
- Reference count: 11
- Fine-tuned GPT-3.5 achieves state-of-the-art F1 score of 83.8 on SemEval-2014 Task 4

## Executive Summary
This paper investigates the use of large language models (LLMs) for aspect-based sentiment analysis (ABSA), comparing zero-shot, few-shot, and fine-tuned approaches. The authors find that fine-tuning GPT-3.5 on the SemEval-2014 dataset significantly outperforms previous state-of-the-art methods, achieving an F1 score of 83.8. They also demonstrate that detailed prompts improve performance in zero-shot and few-shot settings but are unnecessary for fine-tuned models. The study highlights the potential of LLMs for ABSA tasks while also raising questions about the cost-effectiveness and generalizability of the fine-tuning approach.

## Method Summary
The authors evaluate GPT-3.5 and GPT-4 on the SemEval-2014 Task 4 dataset for joint aspect term extraction and polarity classification. They compare zero-shot, few-shot, and fine-tuned approaches, using function calling to enforce JSON output structure. The fine-tuning process involves training the model on 80% of the dataset for 3 epochs, with the remaining 20% used for validation. The authors also test the impact of detailed prompts on performance in zero-shot and few-shot settings.

## Key Results
- Fine-tuned GPT-3.5 achieves a state-of-the-art F1 score of 83.8 on the SemEval-2014 Task 4 joint aspect term extraction and polarity classification task, improving upon InstructABSA by 5.7%.
- Detailed prompts improve performance in zero-shot and few-shot settings but are not necessary for fine-tuned models.
- The fine-tuned model shows signs of potential overfitting, with training accuracy reaching 100% while validation accuracy fluctuates around 76%.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning GPT-3.5 with domain-specific data improves F1 score by 34% over zero-shot GPT-3.5
- Mechanism: Fine-tuning adapts the general-purpose transformer to the specific aspect-term extraction and polarity classification patterns in SemEval-2014
- Core assumption: The benchmark's annotation rules can be learned from training examples without overfitting to spurious correlations
- Evidence anchors:
  - [abstract] "Fine-tuned GPT-3.5 achieves a state-of-the-art F1 score of 83.8 on the joint aspect term extraction and polarity classification task of the SemEval-2014 Task 4, improving upon InstructABSA by 5.7%"
  - [section] "Validation accuracy fluctuates around 76%" and "training accuracy of 100% was reached"
- Break condition: If the fine-tuning dataset is too small or unrepresentative, the model may overfit and fail on novel review patterns

### Mechanism 2
- Claim: Detailed prompts help zero-shot and few-shot performance but are unnecessary for fine-tuned models
- Mechanism: Fine-tuned models internalize the task format and annotation rules, reducing reliance on prompt engineering for performance
- Core assumption: The fine-tuning process effectively transfers the task knowledge from training examples to inference
- Evidence anchors:
  - [abstract] "Our results also indicate that detailed prompts improve performance in zero-shot and few-shot settings but are not necessary for fine-tuned models"
  - [section] "even the No prompt model outperformed the previous state-of-the-art"
- Break condition: If fine-tuning data is noisy or inconsistent, the model may require stronger prompts to disambiguate conflicting signals

### Mechanism 3
- Claim: Using function calling with JSON schema standardizes outputs and reduces inference complexity for non-fine-tuned models
- Mechanism: Function calling enforces structured output format without manual parsing, making LLMs practical for structured ABSA tasks
- Core assumption: The JSON schema can fully describe the output requirements and the model can reliably follow it
- Evidence anchors:
  - [section] "We opted for JSON as a standard and use OpenAI’s function calling feature to enforce the format"
  - [section] "The resulting fine-tuned models do not need a JSON schema to produce structured output"
- Break condition: If the schema is underspecified or the model's output generation is unstable, parsing errors or format violations may occur

## Foundational Learning

- Concept: Aspect-based sentiment analysis (ABSA)
  - Why needed here: ABSA is the core task being evaluated; understanding it is essential for interpreting model performance
  - Quick check question: What distinguishes ABSA from general sentiment analysis in terms of input and output?
- Concept: Zero-shot, few-shot, and fine-tuning learning paradigms
  - Why needed here: The paper compares these three approaches to using LLMs for ABSA; knowing their differences is key
  - Quick check question: How does few-shot learning differ from fine-tuning in terms of parameter updates and task adaptation?
- Concept: Function calling and structured output in LLMs
  - Why needed here: The paper uses function calling to enforce JSON output; understanding this feature is important for implementation
  - Quick check question: What is the advantage of using function calling over manual output parsing in structured tasks?

## Architecture Onboarding

- Component map: Input preprocessing → LLM inference (with optional fine-tuning) → Function calling for JSON output → Postprocessing and evaluation
- Critical path: Data → Fine-tuning (if used) → Prompt engineering (if zero/few-shot) → LLM API call → Output parsing → F1 calculation
- Design tradeoffs: Fine-tuned models offer higher accuracy but increased cost and training complexity; zero-shot models are cheaper but less accurate
- Failure signatures: Low F1 score due to annotation rule mismatch; high false positive rate for non-noun aspects; polarity classification errors on neutral examples
- First 3 experiments:
  1. Test zero-shot performance with the Guidelines summary prompt and function calling to establish baseline
  2. Test few-shot performance with 6 in-context examples using the same prompt to measure prompt impact
  3. Fine-tune GPT-3.5 on SemEval-2014 and evaluate on test set to confirm performance gain over non-fine-tuned models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of fine-tuned LLMs on ABSA compare across different datasets and real-world use cases beyond SemEval-2014?
- Basis in paper: [explicit] The authors note that their study was limited to a single dataset and task, and suggest that generalizability to other datasets and real-world use cases remains a topic for further investigation.
- Why unresolved: The current study focused on a specific benchmark dataset and task, limiting the understanding of how well the findings generalize to other scenarios.
- What evidence would resolve it: Testing fine-tuned LLMs on a variety of ABSA datasets and real-world use cases, comparing their performance to specialized models, and analyzing the types of errors made in different contexts.

### Open Question 2
- Question: How does the performance of fine-tuned LLMs on ABSA tasks change when considering aspect terms that are not noun phrases, such as verbs or adjectives?
- Basis in paper: [inferred] The authors mention that the annotation rules of SemEval 2014 specify that only noun phrases can be aspects, but in real-world applications, it may be desirable to extract aspects that are not noun phrases.
- Why unresolved: The current study focused on the joint aspect term extraction and polarity classification task on the SemEval-2014 dataset, which only considers noun phrases as aspects.
- What evidence would resolve it: Creating a new annotation scheme that includes non-noun phrase aspect terms and testing the performance of fine-tuned LLMs on this new task formulation.

### Open Question 3
- Question: How does the performance of fine-tuned LLMs on ABSA tasks compare to open-source LLMs like Llama 2?
- Basis in paper: [explicit] The authors suggest that testing the performance of a fine-tuned GPT-4 and comparing it to open-source LLMs like Llama 2 would be an interesting avenue for follow-up work.
- Why unresolved: The current study only tested the performance of OpenAI's LLMs (GPT-3.5 and GPT-4) on the ABSA task.
- What evidence would resolve it: Fine-tuning open-source LLMs like Llama 2 on the ABSA task and comparing their performance to fine-tuned GPT models in terms of F1 score, computational resources, and cost-efficiency.

## Limitations

- The study only evaluates on the SemEval-2014 dataset, which may not represent the full diversity of real-world ABSA applications across different domains and languages.
- The fine-tuning process shows signs of potential overfitting, with training accuracy reaching 100% while validation accuracy fluctuates around 76%.
- The study does not explore the impact of different fine-tuning hyperparameters, such as learning rate and batch size, which could significantly affect performance outcomes.

## Confidence

**High Confidence Claims:**
- Fine-tuned GPT-3.5 achieves state-of-the-art performance on SemEval-2014 Task 4
- Detailed prompts improve zero-shot and few-shot performance
- Function calling effectively enforces structured JSON output

**Medium Confidence Claims:**
- Fine-tuning provides better performance than prompt engineering alone
- The 5.7% improvement over InstructABSA is practically significant
- The model generalizes well to unseen examples within the SemEval-2014 domain

**Low Confidence Claims:**
- The performance gains will transfer to other ABSA datasets and domains
- The fine-tuning approach is cost-effective for production deployment
- The model's behavior will remain stable with different GPT-3.5 variants

## Next Checks

1. **Cross-Dataset Generalization Test**: Evaluate the fine-tuned model on multiple ABSA datasets beyond SemEval-2014 (e.g., Twitter, restaurant reviews from other sources) to assess domain transferability and identify potential overfitting to the original dataset's annotation patterns.

2. **Cost-Benefit Analysis**: Compare the F1 score improvements against the actual inference costs and latency for both fine-tuned and specialized models, including a break-even analysis to determine at what scale the fine-tuned approach becomes economically viable.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary key fine-tuning hyperparameters (learning rate, batch size, number of epochs) and measure their impact on both validation performance and generalization to identify optimal settings and potential overfitting indicators.