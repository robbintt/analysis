---
ver: rpa2
title: Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization
arxiv_id: '2305.13091'
source_url: https://arxiv.org/abs/2305.13091
tags:
- chatgpt
- evaluation
- summarization
- summary
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the use of ChatGPT (specifically gpt-3.5-turbo)
  as a zero-shot evaluator for abstractive summarization systems. The authors compare
  different evaluation methods including reason-then-score, multiple-choice question
  (MCQ), and head-to-head comparison approaches across four dimensions: coherence,
  consistency, fluency, and relevance.'
---

# Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization

## Quick Facts
- arXiv ID: 2305.13091
- Source URL: https://arxiv.org/abs/2305.13091
- Reference count: 6
- Key outcome: ChatGPT outperforms automatic metrics but struggles with high-quality summaries and shows inconsistent performance across dimensions

## Executive Summary
This paper investigates whether ChatGPT can serve as a human-level evaluator for abstractive summarization systems. The authors compare three evaluation methods - reason-then-score, multiple-choice questions, and head-to-head comparisons - across four dimensions (coherence, consistency, fluency, and relevance). While ChatGPT outperforms traditional automatic metrics like BERTScore and MoverScore, it shows significant limitations including inconsistent performance across different summarization systems, poor handling of high-quality summaries, and dimension-dependent effectiveness. The authors conclude that despite promising results, ChatGPT is not yet ready to replace human evaluators for abstractive summarization tasks.

## Method Summary
The study uses the SummEval benchmark dataset containing expert human annotations for 12 abstractive summarization systems on CNN/Daily Mail articles. Three zero-shot evaluation methods are implemented with ChatGPT (gpt-3.5-turbo, temperature=0): Reason-Then-Score (RTS) where ChatGPT generates reasons and scores, Multiple-Choice Question (MCQ) where ChatGPT selects from predefined reasons, and Head-to-Head (H2H) comparisons. The methods are evaluated across four dimensions using correlation metrics (Spearman, Pearson, Kendall's Tau) with human scores, and meta-correlation is introduced to measure evaluation stability across different system quality levels.

## Key Results
- ChatGPT outperforms traditional automatic metrics but shows significant limitations as a human replacement
- MCQ method demonstrates better stability and correlation with human evaluations than RTS method
- ChatGPT's evaluation capability deteriorates as summary quality increases, showing negative meta-correlation
- Performance is dimension-dependent, with strength in consistency evaluation but weakness in relevance evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MCQ evaluation method improves ChatGPT's stability and correlation with human evaluations compared to RTS method.
- Mechanism: By constraining ChatGPT to select from predefined reasons rather than generating them freely, the MCQ method prevents unreliable free-form reasoning that could mislead scoring.
- Core assumption: ChatGPT's free-form reasoning generation can be unreliable and may introduce errors that negatively impact evaluation consistency.
- Evidence anchors: The paper finds that while ChatGPT and GPT-4 outperform automatic metrics, they are not ready as human replacements due to significant limitations including inconsistent ratings across dimensions.

### Mechanism 2
- Claim: ChatGPT's evaluation capability deteriorates as the quality of summarization systems improves, as measured by meta-correlation.
- Mechanism: Meta-correlation captures how evaluation performance varies across systems of different quality levels, with significant negative values indicating systematic performance degradation with higher-quality summaries.
- Core assumption: There is a systematic relationship between system quality and evaluation difficulty for LLMs.
- Evidence anchors: The paper identifies that ChatGPT struggles to compare candidates with close performance and becomes more unreliable with higher-quality summaries by obtaining lower correlation with humans.

### Mechanism 3
- Claim: ChatGPT's effectiveness as an evaluator is dimension-dependent, with particular strength in consistency evaluation but weakness in relevance evaluation.
- Mechanism: Different evaluation dimensions may rely on different cognitive abilities, and ChatGPT's architecture may be better suited for some dimensions than others.
- Core assumption: The dimensions of coherence, consistency, fluency, and relevance require different types of reasoning and understanding.
- Evidence anchors: While the H2H method achieves impressive performance on the consistency dimension, it is much less effective on other dimensions.

## Foundational Learning

- Concept: Correlation metrics (Spearman, Pearson, Kendall's Tau)
  - Why needed here: The paper uses multiple correlation metrics to compare ChatGPT's evaluations with human evaluations across different summarization systems and dimensions.
  - Quick check question: What is the key difference between Spearman correlation and Pearson correlation, and when would you use each in evaluation tasks?

- Concept: Meta-correlation as a stability metric
  - Why needed here: The paper introduces meta-correlation to measure how ChatGPT's evaluation performance varies across systems of different quality levels, which is crucial for understanding its reliability as an evaluator.
  - Quick check question: How does meta-correlation differ from standard correlation, and what does a significant negative meta-correlation indicate about an evaluator's performance?

- Concept: Likert scale evaluation
  - Why needed here: The paper uses Likert scale scoring (1-5) for multiple evaluation dimensions, which is a common approach in human evaluation of summarization systems.
  - Quick check question: What are the advantages and disadvantages of using a 5-point Likert scale for evaluating abstractive summaries compared to binary or continuous scales?

## Architecture Onboarding

- Component map: Input article → Summary generation → ChatGPT evaluation (via chosen method) → Correlation with human scores → Stability analysis
- Critical path: Input article → Summary generation → ChatGPT evaluation (via chosen method) → Correlation with human scores → Stability analysis
- Design tradeoffs: The paper chooses zero-shot evaluation with minimal prompt tuning versus potentially more effective but labor-intensive prompt engineering or fine-tuning approaches.
- Failure signatures: Inconsistent evaluation scores across similar systems, low correlation with human evaluations, significant negative meta-correlation indicating performance degradation with system quality.
- First 3 experiments:
  1. Implement the MCQ evaluation method and compare its correlation with human scores against the RTS method across all four dimensions.
  2. Calculate meta-correlation for both MCQ and RTS methods to assess evaluation stability across different summarization systems.
  3. Test ChatGPT's H2H comparison performance on the consistency dimension versus other dimensions to validate dimension-dependent effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve ChatGPT's evaluation capability for high-quality summaries, particularly addressing the observed negative meta-correlation between evaluation performance and summary quality?
- Basis in paper: The paper identifies that ChatGPT's evaluation capability deteriorates with increasing summary quality, showing negative meta-correlation between evaluation performance and summary quality.
- Why unresolved: The paper identifies this as a key limitation but doesn't provide solutions or explain the underlying causes of this deterioration.
- What evidence would resolve it: Empirical studies testing different prompting strategies, evaluation methods, or model modifications specifically designed to maintain evaluation performance across varying summary quality levels.

### Open Question 2
- What is the minimal number of summaries required for reliable head-to-head comparisons using ChatGPT, and how does this vary across different evaluation dimensions?
- Basis in paper: The paper suggests that while evaluating many summaries is laborious for humans, it may not be the case for LLMs, and mentions this as a potential research direction.
- Why unresolved: The current experiments only use 100 compared summaries, which may be insufficient for reliable comparisons. The paper doesn't investigate the relationship between sample size and evaluation reliability.
- What evidence would resolve it: Systematic experiments varying the number of compared summaries and measuring the stability and reliability of evaluation results across different dimensions.

### Open Question 3
- How can we effectively combine ChatGPT evaluations with human evaluations to create a hybrid evaluation system that maintains reproducibility while reducing human effort?
- Basis in paper: The paper suggests this as a future research direction, noting that combining LLMs with human evaluation could save time and cost while maintaining reproducibility.
- Why unresolved: The paper doesn't explore what specific methods or strategies could be used to integrate LLM and human evaluations effectively.
- What evidence would resolve it: Development and testing of concrete hybrid evaluation frameworks that demonstrate improved efficiency and reproducibility compared to pure human evaluation.

## Limitations

- The study focuses exclusively on gpt-3.5-turbo rather than more advanced models like GPT-4, which may have different evaluation capabilities
- The paper doesn't explore how different prompt engineering strategies or temperature settings might affect evaluation performance
- The study uses a single benchmark dataset (SummEval) and 12 summarization systems, which may not capture the full diversity of summarization tasks

## Confidence

**Confidence Level: Low-Medium** The paper demonstrates significant limitations in ChatGPT's ability to serve as a human-level evaluator, but several factors constrain generalizability of findings.

**Confidence Level: Medium** The dimension-dependent evaluation performance is well-documented, but underlying reasons for why ChatGPT excels at consistency while struggling with relevance remain unclear.

**Confidence Level: Medium-High** The meta-correlation analysis revealing ChatGPT's performance deterioration with higher-quality systems is a significant finding, but the linear relationship assumption may oversimplify the actual relationship.

## Next Checks

1. **Model Comparison Validation**: Replicate the evaluation study using GPT-4 instead of gpt-3.5-turbo to determine whether observed limitations are model-specific or represent fundamental challenges for LLM evaluators across different model versions.

2. **Prompt Engineering Impact**: Systematically test different prompt formulations, including more detailed instructions, examples, and role assignments, to quantify how much prompt engineering can improve evaluation stability and correlation with human judgments across all four dimensions.

3. **Cross-Dataset Generalization**: Apply the evaluation methods to additional summarization datasets (e.g., XSum, Newsroom, or domain-specific corpora) and systems not included in the original study to assess whether observed limitations generalize across different summarization tasks and domains.