---
ver: rpa2
title: The cross-sectional stock return predictions via quantum neural network and
  tensor network
arxiv_id: '2304.12501'
source_url: https://arxiv.org/abs/2304.12501
tags:
- quantum
- learning
- network
- neural
- stock
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies quantum and quantum-inspired machine learning
  (ML) algorithms to cross-sectional stock return prediction. The authors compare
  quantum neural networks and tensor networks against classical models (linear regression
  and neural networks) using a Japanese stock market dataset.
---

# The cross-sectional stock return predictions via quantum neural network and tensor network

## Quick Facts
- arXiv ID: 2304.12501
- Source URL: https://arxiv.org/abs/2304.12501
- Reference count: 32
- Key outcome: Tensor network achieves IR=0.69, outperforming classical models in cross-sectional stock return prediction

## Executive Summary
This study applies quantum and quantum-inspired machine learning algorithms to cross-sectional stock return prediction. The authors compare quantum neural networks and tensor networks against classical models (linear regression and neural networks) using a Japanese stock market dataset. They conduct portfolio backtesting from 2008-2021 and evaluate performance via information ratio. The tensor network model achieves the best risk-adjusted excess return (IR = 0.69), outperforming all classical benchmarks. The quantum neural network performs comparably to classical neural networks in overall period but shows superior performance in recent years (IR = 0.45 vs 0.16), suggesting better adaptation to changing market conditions and reduced overfitting. The results indicate quantum and quantum-inspired methods can effectively capture non-linear relationships in financial data.

## Method Summary
The study uses Japanese stocks from the TOPIX500 index (2008-2021) with 10 input features (value, quality, momentum, size, market beta factors). Monthly rebalancing is employed with cross-sectional ranking of features and returns. Four models are tested: linear regression (baseline), neural networks (NN1 with 3 layers, 92 parameters; NN2 with 4 layers, 93 parameters), quantum circuit learning with 10 qubits/depth 3 (90 parameters), and tensor network with bond dimension 2 (76 parameters). Training uses 3-year rolling windows with 1-year test periods, optimizing with stochastic gradient descent or DMRG for tensor networks. Equal-weighted long-only portfolios are constructed on top quintile predicted stocks, with performance evaluated via information ratio, tracking error, and excess return.

## Key Results
- Tensor network achieves information ratio of 0.69, outperforming all classical benchmarks
- Quantum neural network performs comparably to classical neural networks overall but shows superior performance in recent years (IR=0.45 vs 0.16)
- Both quantum-inspired models maintain performance in recent market conditions while classical neural networks deteriorate
- Quantum circuit learning performs similarly to linear regression when bond dimension is set to m=1, indicating capture of non-linear relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tensor networks capture cross-sectional relationships better than classical neural networks for equity return prediction.
- Mechanism: The tensor network's ability to efficiently represent high-order interactions among input features allows it to model complex dependencies in financial data that linear and standard neural network models miss.
- Core assumption: The relationships among the 10 financial features are non-linear and involve higher-order interactions that are compressible through tensor factorization.
- Evidence anchors: Abstract shows tensor network outperforms classical models; section states TN can have effective architectures for financial data; corpus references demonstrate tensor networks' effectiveness on complex datasets.

### Mechanism 2
- Claim: Quantum neural networks adapt better to changing market conditions than classical neural networks.
- Mechanism: QNN's parameter-shift gradient optimization and quantum circuit architecture provide better regularization, reducing overfitting to historical data and improving generalization to new market regimes.
- Core assumption: Recent market environment presents different statistical relationships than training period, and QNN architecture provides implicit regularization against overfitting.
- Evidence anchors: Abstract notes both quantum models perform well in latest market environment; section shows QCL beats classical models in recent periods; section states neural networks overfit to previous market environment.

### Mechanism 3
- Claim: Cross-sectional ranking preprocessing is essential for model performance in equity return prediction.
- Mechanism: Converting raw features to cross-sectional ranks normalizes scale differences across features and removes absolute level effects, allowing models to focus on relative positioning within the universe.
- Core assumption: Predictive signal for stock returns comes from relative feature values within cross-section at each time point, not from absolute values or multi-period trends.
- Evidence anchors: Section describes cross-sectional ranking of all features and returns; section notes QCL architecture requires one qubit per feature; no direct corpus evidence - appears to be domain-specific preprocessing choice.

## Foundational Learning

- Concept: Tensor network factorization and contraction
  - Why needed here: Understanding how MPS represents high-dimensional data through bond dimensions is crucial for interpreting model capacity and designing architectures
  - Quick check question: If a tensor has dimensions (10,10,10) and uses MPS with bond dimension m=2, how many parameters does the MPS representation have compared to the full tensor?

- Concept: Quantum circuit learning and parameter-shift rule
  - Why needed here: The optimization method differs fundamentally from classical backpropagation and affects convergence and generalization
  - Quick check question: How does the parameter-shift rule compute gradients differently from classical automatic differentiation, and why is this important for NISQ devices?

- Concept: Cross-sectional vs time-series prediction approaches
  - Why needed here: The paper's methodology relies on ranking-based cross-sectional analysis; understanding the distinction from time-series approaches is critical for proper application
  - Quick check question: What are the key differences in data requirements and assumptions between cross-sectional and time-series equity return prediction?

## Architecture Onboarding

- Component map: Feature extraction -> cross-sectional ranking -> model input -> training (3-year window) -> prediction (1-year test) -> portfolio construction -> performance evaluation -> repeat with shifted window

- Critical path: Data preprocessing → model training (3-year window) → prediction (1-year test) → portfolio construction → performance evaluation → repeat with shifted window

- Design tradeoffs:
  - Feature count limited to 10 due to QCL qubit requirements vs. classical models that can handle more
  - Fixed 3-year training window vs. potentially adaptive windows for regime changes
  - Equal-weighted portfolios vs. optimal weighting schemes
  - Monthly rebalancing vs. more/less frequent adjustments

- Failure signatures:
  - QCL performance similar to linear model indicates failure to capture non-linearities
  - TN underperformance suggests bond dimension too small for problem complexity
  - NN models failing in recent periods indicates overfitting to historical regimes
  - Information ratio approaching zero indicates no alpha generation

- First 3 experiments:
  1. Run QCL with bond dimension m=1 (effectively linear model) to establish baseline non-linearity capture
  2. Test TN with increased bond dimension (m=4, m=8) to find capacity threshold
  3. Implement rolling 1-month model retraining to compare against annual retraining for overfitting analysis

## Open Questions the Paper Calls Out

- Does the superior performance of tensor networks and quantum neural networks generalize to other asset classes beyond stocks, such as bonds or currencies?
- What is the fundamental mechanism by which quantum and tensor network models mitigate overfitting compared to classical neural networks?
- How does the performance of quantum neural networks scale with increasing number of input features and model parameters?
- Can quantum recurrent neural networks outperform classical RNN variants for time-series return prediction in financial markets?

## Limitations
- Small feature set (10 features) constrained by quantum circuit requirements may not capture full complexity of cross-sectional return prediction
- Equal-weighted portfolio construction may not represent optimal investment strategies
- Bond dimension m=2 for tensor networks represents significant hyperparameter decision not thoroughly explored for sensitivity

## Confidence
- High confidence: Tensor network superiority over classical benchmarks (IR=0.69 vs ~0.3-0.4 for classical models)
- Medium confidence: QNN performance improvement in recent periods (IR=0.45 vs 0.16 for classical NN) requires cautious interpretation
- Medium confidence: Claim about quantum-inspired models better capturing non-linear relationships is supported by empirical results but lacks rigorous ablation studies

## Next Checks
1. Bond dimension sensitivity analysis: Systematically test tensor network performance across m=1,2,3,4,8 to establish whether m=2 represents optimal or suboptimal capacity
2. Classical model re-benchmarking: Implement and test classical neural networks with depth and width matching quantum circuit parameter count (90 parameters) to ensure fair comparison
3. Out-of-sample regime testing: Conduct additional backtesting on Japanese market data from 2022-2024 to validate whether QNN's recent period advantage persists in new market conditions