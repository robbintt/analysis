---
ver: rpa2
title: Streaming Object Detection on Fisheye Cameras for Automatic Parking
arxiv_id: '2305.14713'
source_url: https://arxiv.org/abs/2305.14713
tags:
- bounding
- bangle
- detection
- angle
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a real-time object detection framework for
  fisheye cameras in automatic parking systems. The key innovation is a dual-flow
  perception module that fuses motion information from consecutive frames to predict
  future object positions, addressing the time-lag problem in streaming perception.
---

# Streaming Object Detection on Fisheye Cameras for Automatic Parking

## Quick Facts
- arXiv ID: 2305.14713
- Source URL: https://arxiv.org/abs/2305.14713
- Reference count: 40
- Key outcome: Real-time object detection framework for fisheye cameras using dual-flow perception and periodic angle loss, improving streaming perception accuracy.

## Executive Summary
This paper presents a real-time object detection framework designed for fisheye cameras in automatic parking systems. The key innovation is a dual-flow perception (DFP) module that fuses motion information from consecutive frames to predict future object positions, addressing the time-lag problem in streaming perception. The framework uses rotated bounding boxes to better represent objects in fisheye images and introduces a new periodic angle loss function to handle the angular periodicity of rotated boxes. Experimental results demonstrate significant improvements in both standard detection performance and streaming perception accuracy.

## Method Summary
The proposed method uses a one-stage detector based on YOLOX-s architecture with CSPDarknet backbone and FPN. The core innovations include a dual-flow perception module that fuses current and previous frame features to predict future object positions, and a periodic angle loss function for rotated bounding box regression. The model is trained on the WoodScape dataset for offline detection and evaluated on the Argoverse-HD dataset for online streaming detection. Training involves 10,000 iterations with SGD optimizer and batch size of 24.

## Key Results
- Periodic angle loss function improves detection performance on WoodScape dataset (AP0.5 increases from 0.295 to 0.379)
- DFP module improves streaming perception accuracy on Argoverse-HD dataset (sAP0.5 increases from 0.227 to 0.240)
- Rotated bounding boxes better capture object geometry in fisheye images compared to standard axis-aligned boxes

## Why This Works (Mechanism)

### Mechanism 1
The dual-flow perception (DFP) module improves streaming accuracy by fusing motion information from consecutive frames. It uses two feature maps from consecutive frames, applies shared 1x1 convolution to each, concatenates them, and uses residuals to combine dynamic motion features with static current frame features. This allows the model to learn motion trends and predict future object positions. The mechanism assumes motion information from consecutive frames can be extracted and used for prediction. If motion between frames is too chaotic or unpredictable, the DFP module cannot learn useful motion patterns.

### Mechanism 2
The periodic angle loss function improves rotated bounding box regression by handling angular periodicity. It uses modular arithmetic to ensure angle predictions wrap correctly at π boundaries, preventing the model from treating 0 and 2π as far apart. The piecewise form provides steeper gradients near problematic regions. The mechanism assumes angle regression should treat angles as periodic values rather than linear values to avoid discontinuities at the 0/2π boundary. If objects rarely have significant rotation or angle range is limited, benefits may be minimal.

### Mechanism 3
Rotated bounding boxes are more suitable than standard boxes for fisheye images due to radial distortion. Fisheye cameras introduce radial distortion that causes standard axis-aligned bounding boxes to contain more background information and provide less accurate localization. Rotated bounding boxes can align with actual object orientation, reducing background noise and improving detection accuracy. The mechanism assumes objects in fisheye images have orientations that benefit from rotated rather than axis-aligned boxes. If field of view is limited or objects are predominantly upright, benefits diminish.

## Foundational Learning

- Concept: Understanding radial distortion in fisheye cameras
  - Why needed here: The entire motivation for using rotated bounding boxes and specialized loss functions stems from the radial distortion properties of fisheye cameras
  - Quick check question: What happens to straight lines in the real world when captured by a fisheye camera?

- Concept: Streaming perception metrics (sAP)
  - Why needed here: The paper evaluates performance using sAP, which integrates delay and accuracy, unlike traditional AP metrics
  - Quick check question: How does sAP differ from standard AP in terms of what it measures?

- Concept: Periodic functions and modular arithmetic
  - Why needed here: The periodic angle loss function relies on understanding how to handle periodic values (angles) mathematically
  - Quick check question: Why is it problematic to use standard L1/L2 loss for angle regression?

## Architecture Onboarding

- Component map: Image -> Backbone -> FPN -> DFP -> Detection Head -> Loss Calculation
- Critical path: Image → Backbone → FPN → DFP → Detection Head → Loss Calculation
- Design tradeoffs: Rotated bounding boxes increase annotation complexity but improve localization accuracy; DFP module adds computational overhead but improves streaming performance; Periodic angle loss is slightly more complex but handles angular periodicity better than standard losses
- Failure signatures: Poor AP/sAP scores despite good training loss may indicate issues with DFP module or periodic loss implementation; High false positive rate with rotated boxes may indicate angle regression problems; Performance degradation on standard images may indicate overfitting to fisheye characteristics
- First 3 experiments: 1) Test baseline performance with standard vs rotated bounding boxes on fisheye dataset; 2) Compare periodic angle loss vs L1 loss on rotated bounding box angle regression; 3) Evaluate DFP module impact by comparing with and without motion information fusion on streaming data

## Open Questions the Paper Calls Out

### Open Question 1
How does the dual-flow perception module perform in scenarios with highly dynamic objects versus mostly static scenes? The paper states the dual-flow module is designed to fuse motion information and predict future object positions, but does not provide comparative performance metrics for different motion dynamics. Comparative sAP performance metrics for datasets with varying levels of object motion dynamics would resolve this question.

### Open Question 2
What is the computational overhead of the dual-flow perception module compared to standard object detection models without streaming perception? The paper emphasizes real-time detection but does not provide explicit timing comparisons between models with and without the dual-flow module. Frame processing time comparisons between the proposed model and baseline models on the same hardware would resolve this question.

### Open Question 3
How does the proposed periodic angle loss function compare to other angle regression methods specifically designed for fisheye images? The paper proposes a new periodic angle loss function and claims it is better than traditional L1/L2 loss, but does not compare it to other fisheye-specific angle regression methods. Comparative experiments using other angle regression methods on the same fisheye datasets would resolve this question.

## Limitations

- The periodic angle loss function's piecewise formulation may introduce implementation complexity that affects reproducibility
- The DFP module's performance heavily depends on motion consistency between consecutive frames, which may vary significantly in real-world parking scenarios
- The use of rotated bounding boxes requires precise angle annotations, and any annotation errors could disproportionately affect model performance due to the angular periodicity handling

## Confidence

- High Confidence: The core premise that fisheye distortion necessitates rotated bounding boxes is well-established in the field
- Medium Confidence: The dual-flow perception module's effectiveness is supported by experimental improvements in sAP metrics, but the mechanism's robustness across different motion patterns remains uncertain
- Medium Confidence: The periodic angle loss function shows significant AP improvements in experiments, but the piecewise formulation may introduce training instabilities not fully explored

## Next Checks

1. Implement the periodic angle loss function with the exact piecewise formulation described and verify it correctly handles angle wrapping at π boundaries through unit tests
2. Test DFP module sensitivity by varying the time interval between consecutive frames to determine the optimal frame rate for motion prediction
3. Evaluate performance on standard camera data to confirm the model doesn't overfit to fisheye-specific characteristics and can generalize across different camera types