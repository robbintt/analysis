---
ver: rpa2
title: Exploring Dataset-Scale Indicators of Data Quality
arxiv_id: '2311.04016'
source_url: https://arxiv.org/abs/2311.04016
tags:
- classes
- dataset
- data
- scaling
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work studies two dataset-level indicators of data quality:
  label set size and class balance. The authors find that horizontally scaling (increasing
  number of classes) improves both accuracy and robustness more than vertical scaling
  (increasing samples per class), even when the new classes are out-of-distribution.'
---

# Exploring Dataset-Scale Indicators of Data Quality

## Quick Facts
- arXiv ID: 2311.04016
- Source URL: https://arxiv.org/abs/2311.04016
- Authors: 
- Reference count: 10
- Key outcome: Horizontal scaling (more classes) improves accuracy and robustness more than vertical scaling (more samples per class), even with out-of-distribution classes. Class imbalance primarily harms performance due to underrepresented classes.

## Executive Summary
This paper introduces two key dataset-level indicators of data quality—label set size and class balance—and demonstrates their impact on model performance. Through systematic experiments, the authors show that horizontally scaling datasets (increasing the number of classes) yields greater accuracy and robustness improvements than vertically scaling (increasing samples per class), even when new classes are out-of-distribution. They also find that class imbalance harms performance primarily due to underrepresented classes rather than overrepresented ones. These insights are supported by two proposed indicators: the percentage of samples in the most common classes and the percentage of classes with very few samples.

## Method Summary
The study uses a composite dataset called JANuS, created by combining ImageNet-100 with additional classes from OpenImages and synthetic FractalDB data. Models are trained using a modified ResNet-50 architecture with SimCLR augmentations, batch size 256, and 256 epochs. The authors compare horizontal scaling (increasing label set size while holding total samples constant) against vertical scaling (increasing samples per class while reducing label set size). Performance is evaluated on ImageNet-100 validation accuracy and average robustness across four distribution shifts. Class imbalance is analyzed by creating datasets with varying degrees of left-skewedness and long-tailedness.

## Key Results
- Horizontal scaling consistently outperforms vertical scaling in both accuracy and robustness, even with out-of-distribution classes
- Underrepresented classes (not overrepresented ones) are the primary driver of performance degradation in imbalanced datasets
- Proposed indicators (% samples in common classes, % classes with few samples) effectively predict model performance and can guide dataset design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Horizontal scaling improves accuracy and robustness more than vertical scaling
- Mechanism: Adding more classes expands the feature space the model learns to represent, leading to better generalization
- Core assumption: The model can integrate representations from out-of-distribution classes without catastrophic interference
- Evidence anchors: Experimental results showing H-scaling outperforms V-scaling on both accuracy and robustness metrics
- Break condition: If OOD classes are too dissimilar from target classes, performance may degrade

### Mechanism 2
- Claim: Underrepresented classes primarily drive performance decline in imbalanced datasets
- Mechanism: Rare classes lack sufficient gradient updates for the model to learn discriminative features
- Core assumption: Model performance is limited by lack of representation in rare classes rather than saturation from common classes
- Evidence anchors: Analysis showing accuracy loss correlates with very small class sizes rather than left-skewedness alone
- Break condition: If rare classes are easily separable or extremely few in number, impact may be negligible

### Mechanism 3
- Claim: Proposed indicators predict model performance and guide dataset design
- Mechanism: These indicators provide quantitative measures of dataset quality that correlate with downstream accuracy and robustness
- Core assumption: Strong relationship exists between indicators and performance across different datasets and architectures
- Evidence anchors: Validation of indicators against model performance on controlled experiments
- Break condition: Indicators may not generalize well to datasets with significantly different characteristics

## Foundational Learning

- Concept: Dataset-level vs. sample-level quality indicators
  - Why needed here: Paper focuses on dataset-scale properties rather than individual sample quality
  - Quick check question: What is the difference between dataset-level and sample-level quality indicators, and why is this distinction important for foundation models?

- Concept: Horizontal vs. vertical scaling in dataset design
  - Why needed here: Paper introduces and contrasts these scaling types to determine which is more effective
  - Quick check question: How do horizontal and vertical scaling differ, and what are the implications of each for model accuracy and robustness?

- Concept: Class imbalance and its impact on model performance
  - Why needed here: Paper investigates effects of class imbalance and proposes indicators to predict its impact
  - Quick check question: Why does class imbalance affect model performance, and how can indicators help mitigate its negative effects?

## Architecture Onboarding

- Component map:
  - Dataset preparation: JANuS (composite dataset), OpenImages, ImageNet, FractalDB
  - Model architecture: Modified ResNet-50 (baseline), other timm library models
  - Training: Mixed precision, batch size 256, 256 epochs, SimCLR augmentations
  - Evaluation: Accuracy on IN100 validation set, average robustness on distribution shifts

- Critical path:
  1. Prepare datasets with varying label set sizes and class balances
  2. Train models with specified architectures and training parameters
  3. Evaluate model performance on accuracy and robustness metrics
  4. Analyze results to derive key indicators and insights

- Design tradeoffs:
  - Horizontal vs. vertical scaling: Horizontal scaling provides better gains but may require more diverse data sources
  - Class balance: Rebalancing underrepresented classes improves performance but may require additional data collection
  - Training from scratch vs. fine-tuning: Fine-tuning on larger label sets can outperform naive horizontal scaling but requires careful stage management

- Failure signatures:
  - Poor accuracy on target classes despite large label set size: Indicates ineffective integration of OOD classes
  - Degradation in robustness metrics: Suggests overfitting or insufficient representation of rare classes
  - Inconsistent performance across distribution shifts: May indicate dataset-specific biases or inadequate augmentation

- First 3 experiments:
  1. Compare horizontal scaling vs. vertical scaling on a small dataset with controlled label set size
  2. Investigate impact of class imbalance by creating datasets with varying degrees of left-skewedness and long-tailedness
  3. Test proposed indicators on diverse datasets to validate their predictive power

## Open Questions the Paper Calls Out
- Optimal ratio of horizontal to vertical scaling for maximizing performance remains unquantified
- Benefits of horizontal scaling may diminish or reverse when scaling to extremely large label sets (beyond 100,000 classes)
- Generalizability of proposed indicators across different computer vision tasks beyond image classification is unknown
- The mechanism by which horizontal scaling with OOD classes improves robustness and accuracy is not explained
- Interaction between horizontal scaling benefits and class rebalancing when applied simultaneously is unexplored

## Limitations
- Findings may not generalize to datasets where OOD classes are highly dissimilar from target classes
- Analysis assumes fixed total sample count, which may not reflect real-world data collection constraints
- Proposed indicators may not capture all aspects of data quality relevant to different model architectures or tasks

## Confidence
- High confidence: Underrepresented classes primarily drive performance decline in imbalanced datasets
- Medium confidence: Superiority of horizontal scaling depends on overlap between OOD and ID classes
- Low confidence: Generalizability of proposed indicators across diverse vision tasks and architectures

## Next Checks
1. Test indicator performance on a dataset with extreme OOD classes (e.g., adding completely unrelated categories) to establish bounds on horizontal scaling benefits
2. Validate the %<k Classes indicator on non-vision datasets (text, tabular) to assess cross-domain applicability
3. Conduct ablation studies varying the ratio of OOD to ID samples in horizontal scaling to determine optimal mixing ratios for different target domain characteristics