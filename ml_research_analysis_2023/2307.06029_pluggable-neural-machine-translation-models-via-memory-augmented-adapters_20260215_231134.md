---
ver: rpa2
title: Pluggable Neural Machine Translation Models via Memory-augmented Adapters
arxiv_id: '2307.06029'
source_url: https://arxiv.org/abs/2307.06029
tags:
- memory
- adapter
- translation
- style
- proceedings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a pluggable NMT framework using memory-augmented
  adapters to satisfy user requirements in style and domain customization without
  retraining the model. The key innovation is constructing multi-granular continuous
  memory based on user-provided samples and introducing a new adapter architecture
  that combines original model representations with retrieved memory results.
---

# Pluggable Neural Machine Translation Models via Memory-augmented Adapters

## Quick Facts
- **arXiv ID**: 2307.06029
- **Source URL**: https://arxiv.org/abs/2307.06029
- **Reference count**: 40
- **Key outcome**: Memory-augmented adapters enable pluggable NMT customization for style and domain adaptation without retraining, achieving better BLEU scores and style similarity than several baselines.

## Executive Summary
This paper introduces a pluggable neural machine translation framework using memory-augmented adapters to customize pre-trained models for user-specific style and domain requirements without retraining. The key innovation is constructing multi-granular continuous memory from user-provided text samples and introducing a new adapter architecture that combines original model representations with retrieved memory results. A memory dropout training strategy prevents spurious dependencies between the NMT model and memory. Experiments show the method outperforms several baselines on style and domain customization tasks while being effective even with limited user data.

## Method Summary
The method constructs multi-granular continuous memory by extracting phrases at different syntactic granularities from user-provided samples using parse trees, then storing corresponding representations from both source and target sides at each decoder layer. A memory-augmented adapter architecture uses attention over memory items with queries from the NMT model, then applies gated fusion to combine the anchor from the original model with the retrieved result. During training, memory dropout randomly drops subsets of memory items, with losses computed for both full memory usage and dropped memory usage to encourage consistency. The approach works by plugging these adapters into a frozen pre-trained NMT model, allowing customization without model retraining.

## Key Results
- The method achieves higher BLEU scores than several baselines on style and domain customization tasks
- Memory-augmented adapters show better style similarity metrics while maintaining translation quality
- The approach scales effectively to larger models (up to 596M parameters) and works with limited user data (as low as 250 sentences)

## Why This Works (Mechanism)

### Mechanism 1
Multi-granular continuous memory improves retrieval quality by balancing contextualized information and retrieval difficulty. The system extracts phrases at multiple syntactic granularities from user-provided samples using parse trees, then stores corresponding representations from both source and target sides at each decoder layer. Different decoder layers require different amounts of contextual information, and phrases of appropriate length can provide this information without making retrieval too difficult.

### Mechanism 2
Memory-augmented adapter architecture allows effective interpolation between original model representations and retrieved memory results. The adapter uses attention over memory items with queries from the NMT model, then applies gated fusion to combine the anchor from the original model with the retrieved result. The original model's representations contain complementary information to the retrieved memory results, and a learnable gate can effectively combine them.

### Mechanism 3
Memory dropout training strategy reduces spurious dependencies between the NMT model and memory. During training, random subsets of memory items are dropped, and the loss includes both full memory usage and dropped memory usage with an agreement loss to encourage consistency. Forcing the model to work with incomplete memory forces it to learn more robust representations that don't over-rely on specific memory items.

## Foundational Learning

- **Concept**: Transformer architecture and attention mechanisms
  - Why needed here: The method builds on Transformer layers and modifies both self-attention and cross-attention modules
  - Quick check question: Can you explain how multi-head attention works in a Transformer decoder layer and how it differs from encoder layers?

- **Concept**: Neural machine translation fundamentals
  - Why needed here: The approach customizes existing NMT models for style and domain adaptation
  - Quick check question: What are the key differences between style customization and domain adaptation in NMT, and how does this method address both?

- **Concept**: Memory-augmented models and retrieval-augmented generation
  - Why needed here: The method combines parametric adapters with non-parametric memory retrieval
  - Quick check question: How does kNN-MT's approach to retrieval differ from this method's memory-augmented adapter, and what are the advantages of each?

## Architecture Onboarding

- **Component map**: Frozen NMT model -> Multi-granular continuous memory -> Memory-augmented adapters -> Training loop with memory dropout -> Optional kNN decoding integration

- **Critical path**: 1) Parse user-provided text to extract multi-granular phrases, 2) Translate phrases using frozen NMT model to create parallel phrase pairs, 3) Store encoder outputs and decoder self-attention outputs at each layer, 4) During training, apply memory dropout and compute losses, 5) During inference, retrieve from memory and apply gated fusion

- **Design tradeoffs**: Memory granularity vs. retrieval accuracy (shorter phrases are easier to match but contain less context), memory size vs. inference speed (larger memories provide more coverage but slow down retrieval), dropout rate vs. model stability (higher dropout prevents overfitting but may destabilize training)

- **Failure signatures**: Poor BLEU scores with no improvement over baseline (memory retrieval may be failing), high perplexity but good style similarity (model may be generating fluent but incorrect translations), training instability or divergence (memory dropout rate may be too aggressive)

- **First 3 experiments**: 1) Validate memory construction: extract phrases from sample data and verify the multi-granular structure, 2) Test basic adapter functionality: integrate memory-augmented adapter into one decoder layer and measure retrieval accuracy, 3) Evaluate ablation on memory dropout: compare training with and without memory dropout to verify its effect on performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed memory-augmented adapter perform on even larger language models (e.g., models with over 10 billion parameters) compared to smaller adapters or fine-tuning? The authors mention this deserves further exploration and only test on one larger model scale (596M parameters).

### Open Question 2
What is the optimal granularity distribution strategy across decoder layers for different types of text (e.g., highly repetitive vs. diverse vocabulary) or different language pairs? The study only evaluates one task with one distribution strategy.

### Open Question 3
How does the memory-augmented adapter's performance degrade when the user-provided data contains noisy or adversarial examples designed to manipulate the style/domain? The authors evaluate with limited data but don't test robustness against noisy or adversarial inputs.

## Limitations
- Limited experimental scope across languages and domains beyond WMT De-En style customization
- Memory dropout mechanism validation lacks comprehensive ablation studies
- Evaluation focuses on automated metrics without extensive human evaluation

## Confidence

**High confidence**: The core architectural innovation of combining frozen NMT models with memory-augmented adapters through gated fusion is well-specified and technically sound. The experimental results showing improvements over baseline methods are convincing.

**Medium confidence**: The memory-augmented adapter architecture and training procedure are clearly described, but some implementation details are underspecified. The multi-granular memory approach is intuitively appealing but lacks comprehensive ablation studies.

**Low confidence**: The claims about memory dropout preventing spurious dependencies and improving generalization are weakly supported by the experimental evidence. The method's scalability to very large models is not thoroughly validated.

## Next Checks

1. **Memory granularity ablation study**: Systematically vary the phrase length ranges at each decoder layer and measure the impact on retrieval accuracy and translation quality to determine if the current heuristic choices are optimal.

2. **Memory dropout sensitivity analysis**: Conduct experiments with different memory dropout rates (0.05, 0.1, 0.2) and analyze the training stability and final performance to validate the necessity and optimal setting of the memory dropout mechanism.

3. **Cross-domain generalization test**: Evaluate the method on a diverse set of style and domain adaptation tasks beyond the current WMT De-En style customization, including low-resource language pairs and specialized domains like legal or medical translation, to assess the method's broader applicability.