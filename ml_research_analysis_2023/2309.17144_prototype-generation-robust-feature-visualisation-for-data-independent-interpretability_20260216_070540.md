---
ver: rpa2
title: 'Prototype Generation: Robust Feature Visualisation for Data Independent Interpretability'
arxiv_id: '2309.17144'
source_url: https://arxiv.org/abs/2309.17144
tags:
- similarity
- prototypes
- images
- prototype
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Prototype Generation, a feature visualization
  method designed to generate class-representative inputs while maintaining natural
  activation patterns in neural networks. The approach addresses criticisms that feature
  visualizations produce unnatural internal activations by optimizing inputs to maximize
  class logits while constraining activations to remain close to natural image distributions.
---

# Prototype Generation: Robust Feature Visualisation for Data Independent Interpretability

## Quick Facts
- arXiv ID: 2309.17144
- Source URL: https://arxiv.org/abs/2309.17144
- Reference count: 28
- Primary result: Prototypes maintain higher path similarity to natural activations than natural images themselves in 56.7-65.6% of network layers

## Executive Summary
Prototype Generation is a feature visualization method that generates class-representative inputs while maintaining natural activation patterns in neural networks. Unlike traditional feature visualization that produces unnatural adversarial inputs by optimizing internal activations, this approach maximizes class logits while constraining activations to stay close to natural image distributions. The method successfully generates interpretable prototypes that reveal model biases and maintain higher or equal path similarity to natural activations compared to natural images across most network layers.

## Method Summary
The method optimizes randomly initialized images to maximize class logits while constraining activations through random affine transformations and high-frequency penalties. It uses three loss components: logit maximization (Lc), high-frequency penalty (Lhf), and probability variance loss (Lpv). Random affine transformations (scale: 0.7-1.3, rotation: 180Â°, translation: 0.5-0.5) are applied during optimization to prevent out-of-distribution adversarial inputs. The optimization runs for 512 steps using Adam optimizer with learning rate 0.05, generating prototypes that maintain natural activation patterns while maximizing class-representative features.

## Key Results
- Prototypes maintain higher path similarity (Spearman correlation) to natural activations than natural images in 56.7-65.6% of network layers
- L1 distance analysis shows prototype activations are closer to mean natural activations than individual natural images in 67.2-82.1% of layers
- Prototypes successfully reveal model biases, such as skin-tone preferences in academic gown classification
- The method works across different model architectures (Resnet-18 and InceptionV1) on 11 ImageNet classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prototype Generation maintains natural activation patterns by constraining optimization to stay close to natural image distributions.
- Mechanism: The method uses random affine transformations and high-frequency penalties during optimization to prevent generation of unnatural adversarial inputs while maximizing class logits.
- Core assumption: Natural images occupy a distinct region in activation space that can be approximated by constraining optimization within certain bounds.
- Evidence anchors:
  - [abstract] "optimizing inputs to maximize class logits while constraining activations to remain close to natural image distributions"
  - [section] "we apply random affine transformations to constrain the optimisation process and discourage the generation of out-of-distribution (OOD) adversarial inputs"
  - [corpus] Weak evidence - corpus papers discuss feature visualization but don't specifically address activation distribution constraints
- Break condition: If the natural image distribution assumption fails (e.g., model activations are highly irregular or adversarial examples exist everywhere), the method would fail to generate natural prototypes.

### Mechanism 2
- Claim: Prototypes capture class-representative features by maximizing output logits rather than internal activations.
- Mechanism: By optimizing for class logits instead of internal neurons, the method ensures prototypes represent what the model needs to see for classification rather than what individual neurons respond to.
- Core assumption: Class logits encode more holistic class information than individual neuron activations, making them better targets for generating class-representative prototypes.
- Evidence anchors:
  - [abstract] "optimizing inputs to maximize class logits"
  - [section] "Our approach differs from the existing feature visualisation methodology in a number of ways... we do so by maximising the output logit of the class we wish to generate"
  - [corpus] Weak evidence - corpus papers discuss feature visualization but don't specifically address logit-based optimization
- Break condition: If logits don't capture class-representative information (e.g., in models with complex multi-task heads or adversarial training), prototypes may not represent meaningful class features.

### Mechanism 3
- Claim: Prototype quality is validated through path similarity metrics comparing activation trajectories to natural images.
- Mechanism: The method uses Spearman correlation and L1 distance to measure similarity between prototype activation paths and average natural image activation paths across network layers.
- Core assumption: Natural images have consistent activation patterns that can be captured by averaging across many examples, and prototypes should match these patterns.
- Evidence anchors:
  - [abstract] "quantitatively measuring similarity between the internal activations of our generated prototypes and natural images"
  - [section] "We quantify closeness between activations by calculating L1 distance and spearman similarity as defined in Section 2"
  - [corpus] Moderate evidence - Geirhos et al. paper provides the path similarity framework used here
- Break condition: If natural images don't have consistent activation patterns (e.g., highly diverse classes or noisy data), the path similarity metric becomes unreliable for validation.

## Foundational Learning

- Concept: Feature visualization fundamentals
  - Why needed here: Understanding how feature visualization works provides context for why Prototype Generation differs from existing methods
  - Quick check question: What is the main difference between optimizing for internal activations versus output logits in feature visualization?

- Concept: Neural network activation spaces
  - Why needed here: The method relies on understanding how inputs map to activation patterns across network layers
  - Quick check question: Why might averaging activations across many natural images provide a good reference for "natural" activation patterns?

- Concept: Optimization with constraints
  - Why needed here: The method uses multiple loss terms and regularizations to guide prototype generation
  - Quick check question: How do high-frequency penalties and random transformations help prevent generation of unnatural inputs?

## Architecture Onboarding

- Component map:
  Randomly initialized image -> Preprocessing pipeline -> Adam optimization loop (512 steps) -> Loss computation (Lc, Lhf, Lpv) -> Random affine transformations -> Generated prototype

- Critical path:
  1. Initialize random image
  2. Apply preprocessing for target model
  3. Optimize with combined loss function
  4. Apply random affine transformations each step
  5. Generate final prototype after 512 steps

- Design tradeoffs:
  - Using logits vs internal activations trades specificity for class-representativeness
  - Random transformations vs fixed preprocessing trades stability for naturalness
  - Number of optimization steps (512) vs quality vs computation time

- Failure signatures:
  - High-frequency artifacts in generated images
  - Unbalanced class logits in prototypes
  - Low path similarity despite high logit values
  - Poor performance on out-of-distribution test data

- First 3 experiments:
  1. Generate prototypes for a simple binary classification model to verify basic functionality
  2. Compare path similarity metrics for prototypes vs natural images on a small dataset
  3. Test prototype generation with different regularization strengths to find optimal parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical guarantee that prototype generation will always produce class-representative features that maintain natural activation paths?
- Basis in paper: [explicit] The paper states "We cannot, as of now, provide a formal proof that feature visualisation of this nature will consistently offer useful insights across all use cases and models."
- Why unresolved: While the method shows empirical success on specific networks (ResNet-18 and InceptionV1) and ImageNet classes, there is no theoretical framework establishing when and why the method will work for arbitrary neural networks and classes.
- What evidence would resolve it: A formal mathematical proof showing conditions under which prototype generation maintains natural activation paths, or a comprehensive empirical study across diverse model architectures and datasets demonstrating consistent success.

### Open Question 2
- Question: How do different choices of regularisation parameters affect the reliability and interpretability of generated prototypes?
- Basis in paper: [explicit] The paper discusses testing various regularisation parameters (scale, rotation, translation) and finding optimal values based on path similarity, but doesn't provide a systematic framework for selecting these parameters.
- Why unresolved: The paper shows that regularisation parameters can be tuned for specific cases, but doesn't establish whether these optimal parameters generalize across different models, datasets, or classes, or provide guidance on parameter selection when prior knowledge is limited.
- What evidence would resolve it: A comprehensive study examining how different regularisation parameter choices affect prototype quality across diverse model architectures, datasets, and classes, potentially establishing guidelines for parameter selection.

### Open Question 3
- Question: Can prototype generation be effectively extended to non-image data modalities like tabular data or text?
- Basis in paper: [explicit] The paper states "We will also apply prototype generation to other modalities such as tabular data and language, to see if insights similar to Section 4 can be gleaned from prototypes in these other modalities as well."
- Why unresolved: The method is demonstrated only on image data (ImageNet classes), and there is no theoretical or empirical work establishing how the method might need to be adapted for different data types with different structures and feature spaces.
- What evidence would resolve it: Successful implementation and validation of prototype generation on at least one non-image modality, demonstrating that the method can produce interpretable, class-representative prototypes and reveal model biases similar to those found in image data.

## Limitations
- The method's reliance on path similarity metrics assumes consistent activation patterns across natural images, which may not hold for highly diverse classes
- The small sample size of 11 ImageNet classes limits generalizability to other datasets and classes
- Probability variance loss implementation details are underspecified, making exact reproduction challenging

## Confidence
- High confidence: The method's basic framework and loss function structure are clearly defined
- Medium confidence: Quantitative results showing improved path similarity over natural images
- Low confidence: The claim that prototypes can replace test datasets for model debugging without validation on held-out data

## Next Checks
1. Test prototype generation on a held-out ImageNet subset to verify the method's utility for model debugging without test data access
2. Implement ablation studies removing individual regularization terms (affine transformations, high-frequency penalties) to quantify their contribution to naturalness
3. Compare prototype generation performance across different model architectures (beyond Resnet-18 and InceptionV1) to assess generalizability