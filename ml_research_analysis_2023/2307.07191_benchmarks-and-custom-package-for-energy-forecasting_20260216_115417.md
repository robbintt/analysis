---
ver: rpa2
title: Benchmarks and Custom Package for Energy Forecasting
arxiv_id: '2307.07191'
source_url: https://arxiv.org/abs/2307.07191
tags:
- forecasting
- load
- data
- function
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a comprehensive load forecasting benchmark
  and package for energy forecasting, addressing the need for specialized tools in
  this domain. The package splits the forecasting process into five modules: data
  preprocessing, feature engineering, forecasting methods, postprocessing, and evaluation
  metrics, covering both probabilistic and point forecasting.'
---

# Benchmarks and Custom Package for Energy Forecasting

## Quick Facts
- arXiv ID: 2307.07191
- Source URL: https://arxiv.org/abs/2307.07191
- Reference count: 40
- This paper introduces a comprehensive load forecasting benchmark and package for energy forecasting, addressing the need for specialized tools in this domain.

## Executive Summary
This paper introduces a comprehensive load forecasting benchmark and package for energy forecasting, addressing the need for specialized tools in this domain. The package splits the forecasting process into five modules: data preprocessing, feature engineering, forecasting methods, postprocessing, and evaluation metrics, covering both probabilistic and point forecasting. It includes domain-specific feature engineering that captures the relationship between temperature and calendar variables, and allows users to customize loss functions to minimize costs associated with power grid dispatch. Extensive experiments on multiple datasets at different levels demonstrate the effectiveness of the proposed methods, with the temperature-based feature engineering significantly improving forecasting accuracy for aggregated-level loads.

## Method Summary
The package provides a modular framework for energy forecasting, dividing the process into five components: data preprocessing, feature engineering, forecasting methods, postprocessing, and evaluation. Key innovations include temperature-based feature engineering that captures calendar-temperature interactions through one-hot encoded calendar variables multiplied by temperature powers, and a custom loss function framework using piecewise linearization to model cost relationships. The package supports both probabilistic and point forecasting methods, with extensive evaluation metrics and visualization tools. The framework is implemented in Python with comprehensive documentation and example notebooks.

## Key Results
- Temperature-based feature engineering significantly improves forecasting accuracy for aggregated-level loads
- Custom loss functions allow optimization for power grid dispatch costs rather than pure accuracy
- The modular framework enables flexible model construction for various forecasting scenarios
- Extensive experiments demonstrate effectiveness across 11 datasets at different granularity levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific feature engineering improves load forecasting accuracy by explicitly modeling the coupling between temperature and calendar variables.
- Mechanism: The package constructs features as products of temperature variables (first, second, and third powers) with one-hot encoded calendar variables, capturing the ternary relationship between temperature, calendar effects, and load.
- Core assumption: The relationship between temperature and load is not constant but depends on calendar variables (e.g., weekday vs weekend, season).
- Evidence anchors: [abstract] "domain-specific feature engineering that captures the relationship between temperature and calendar variables", [section] "we apply one-hot encoding to calendar variables and then model this coupling relationship by taking their products with temperature to the first, second, and third powers as features"
- Break condition: If calendar effects are negligible or if the load series is dominated by other factors (e.g., sudden behavioral changes from external shocks like COVID-19).

### Mechanism 2
- Claim: Custom loss functions that model the relationship between forecasting error and real-world costs improve decision-relevant forecasting performance.
- Mechanism: The package provides a method to construct asymmetric, differentiable loss functions by fitting a piecewise linear function (or smoothed version) to data pairs of forecasting error and associated costs, then integrating this into gradient descent training.
- Core assumption: The cost of forecasting errors is not symmetric (e.g., over-forecasting vs under-forecasting have different economic impacts in power grid dispatch).
- Evidence anchors: [abstract] "allows users to customize loss functions to minimize costs associated with power grid dispatch", [section] "we also provide a method to customize the loss function based on the forecasting error, integrating it into our forecasting framework"
- Break condition: If the true cost function is highly non-linear or discontinuous, or if the available data to fit the loss function is insufficient or noisy.

### Mechanism 3
- Claim: Splitting the forecasting pipeline into modular components (preprocessing, feature engineering, forecasting methods, postprocessing, evaluation) enables flexible, tailored model construction.
- Mechanism: Users can combine any components from the five-module framework to build models suited to specific forecasting scenarios, rather than being locked into a monolithic approach.
- Core assumption: Different forecasting problems benefit from different combinations of preprocessing, feature engineering, and forecasting methods; no single approach is optimal for all cases.
- Evidence anchors: [abstract] "splits the forecasting process into five modules: data preprocessing, feature engineering, forecasting methods, postprocessing, and evaluation metrics", [section] "we divide the overall forecasting process into several parts to address potential issues in load forecasting the power"
- Break condition: If the modular design introduces excessive overhead or complexity for simple forecasting tasks where a standard pipeline suffices.

## Foundational Learning

- Concept: Load forecasting vs. traditional time series forecasting
  - Why needed here: Load forecasting is heavily influenced by external factors (temperature, calendar) and aims to minimize dispatch costs rather than just accuracy, requiring specialized methods.
  - Quick check question: What are the two main differences between load forecasting and traditional time series forecasting according to the paper?

- Concept: Probabilistic vs. point forecasting
  - Why needed here: Probabilistic forecasting provides prediction intervals, giving more information for decision-making in power grid dispatch, which is crucial for cost optimization.
  - Quick check question: Why is probabilistic forecasting considered more reliable for power grid applications?

- Concept: Feature engineering for time series with external variables
  - Why needed here: Capturing the interaction between time series data (load) and external variables (temperature, calendar) is key to improving forecasting accuracy in this domain.
  - Quick check question: How does the package's feature engineering strategy capture the relationship between temperature and calendar variables?

## Architecture Onboarding

- Component map: Raw data -> Preprocessing -> Feature Engineering -> Forecasting -> Postprocessing -> Evaluation
- Critical path: Raw data → Preprocessing → Feature Engineering → Forecasting → Postprocessing → Evaluation
- Design tradeoffs:
  - Flexibility vs. complexity: Modular design allows customization but may increase overhead.
  - Interpretability vs. accuracy: Simple models with feature engineering may outperform complex black-box models.
  - Cost vs. accuracy: Custom loss functions optimize for dispatch costs rather than pure accuracy.
- Failure signatures:
  - Poor accuracy despite complex models: Likely due to lack of appropriate feature engineering or custom loss function.
  - Unstable training: Possible issues with custom loss function fitting or data preprocessing.
  - Suboptimal cost performance: Custom loss function may not be well-aligned with actual dispatch costs.
- First 3 experiments:
  1. Load a simple dataset, apply basic preprocessing, and run a default forecasting model (e.g., FFNN) to verify the pipeline works.
  2. Add the temperature-calendar feature engineering to the same model and compare performance to assess the impact of feature engineering.
  3. Implement a simple custom loss function based on a synthetic cost function and train a model to see if it improves cost-related metrics compared to MSE.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of load forecasting models vary across different levels of granularity (e.g., building-level vs. aggregated-level) when using the proposed temperature-based feature engineering?
- Basis in paper: [explicit] The paper mentions that temperature-based feature engineering significantly improves forecasting accuracy for aggregated-level loads but may decrease performance for building-level loads due to significant deviations not caused by temperature.
- Why unresolved: The paper provides limited experimental results comparing the performance across different levels of granularity, focusing more on aggregated-level datasets.
- What evidence would resolve it: Comprehensive experiments comparing the performance of load forecasting models with and without temperature-based feature engineering across multiple datasets at different levels (building-level and aggregated-level).

### Open Question 2
- Question: What is the optimal number of intervals for the piecewise linearization function when customizing the loss function based on forecasting error and real requirements?
- Basis in paper: [explicit] The paper mentions that the number of segments K can be determined by setting the upper bound of the fitting error, but it does not provide specific guidance on how to choose the optimal number of intervals.
- Why unresolved: The optimal number of intervals may vary depending on the dataset and the specific forecasting scenario, and the paper does not provide a clear methodology for determining this.
- What evidence would resolve it: Experiments comparing the performance of load forecasting models with different numbers of intervals in the piecewise linearization function, and analysis of the impact on forecasting accuracy and computational efficiency.

### Open Question 3
- Question: How does the performance of deep learning models for load forecasting compare to non-deep learning models when using the proposed temperature-based feature engineering and custom loss function?
- Basis in paper: [explicit] The paper mentions that non-deep learning methods perform better than deep learning methods without the temperature transformation strategy, but deep learning methods have great improvements with temperature transformation.
- Why unresolved: The paper provides limited experimental results comparing the performance of deep learning and non-deep learning models, focusing more on the impact of temperature-based feature engineering on deep learning models.
- What evidence would resolve it: Comprehensive experiments comparing the performance of deep learning and non-deep learning models with and without temperature-based feature engineering and custom loss function across multiple datasets and forecasting scenarios.

## Limitations

- Domain Shift Sensitivity: Temperature-based feature engineering may underperform on datasets with significant external shocks where load patterns deviate from historical meteorological relationships.
- Custom Loss Function Complexity: Piecewise linearization approach requires domain knowledge and may be challenging to implement correctly without detailed documentation.
- Computational Overhead: Modular framework may introduce computational overhead compared to monolithic implementations.

## Confidence

- High Confidence: The modular framework design and its implementation are well-documented and reproducible. The temperature-calendar coupling feature engineering mechanism is clearly specified with theoretical justification.
- Medium Confidence: The effectiveness of the custom loss function framework is supported by methodology description, but practical implementation details and sensitivity analysis are limited.
- Low Confidence: Claims about computational efficiency and scalability to very large datasets are not substantiated with empirical evidence.

## Next Checks

1. **Domain Shift Validation**: Test the package's performance on datasets with known structural breaks (e.g., COVID-19 affected data, extreme weather events) to quantify the degradation of temperature-based feature engineering under abnormal conditions.

2. **Custom Loss Function Robustness**: Conduct sensitivity analysis on the piecewise linearization fitting process by varying breakpoint positions and smoothing parameters, measuring the impact on both forecasting accuracy and cost optimization performance.

3. **Computational Benchmarking**: Measure training and inference times for various model configurations within the framework, comparing against monolithic implementations of equivalent models to quantify the modular design's computational overhead.