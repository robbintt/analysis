---
ver: rpa2
title: Coupling Large Language Models with Logic Programming for Robust and General
  Reasoning from Text
arxiv_id: '2307.07696'
source_url: https://arxiv.org/abs/2307.07696
tags:
- holds
- agent
- semantic
- action
- parse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel neuro-symbolic approach that leverages
  the semantic parsing strengths of large language models (LLMs) to convert natural
  language into logical facts for use in answer set programs. The combined LLM+ASP
  framework achieves state-of-the-art performance across multiple NLP benchmarks (bAbI,
  StepGame, CLUTRR, gSCAN) without task-specific retraining.
---

# Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text

## Quick Facts
- arXiv ID: 2307.07696
- Source URL: https://arxiv.org/abs/2307.07696
- Authors: 
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on multiple NLP benchmarks by combining LLMs with ASP for reasoning without task-specific retraining

## Executive Summary
This paper introduces a novel neuro-symbolic approach that combines large language models (LLMs) with Answer Set Programming (ASP) to achieve robust and general reasoning from natural language text. The method leverages LLMs as few-shot semantic parsers to convert natural language into logical facts, which are then processed by ASP for multi-step logical reasoning. The approach achieves state-of-the-art performance across multiple NLP benchmarks including bAbI, StepGame, CLUTRR, and gSCAN without requiring task-specific retraining. The modular design enables interpretable error analysis and dataset validation, achieving high accuracy (often >99%) while identifying dataset errors that other models may overlook.

## Method Summary
The method combines LLMs with Answer Set Programming (ASP) to perform natural language reasoning. LLMs are used as few-shot semantic parsers to convert natural language sentences into logical atomic facts, while ASP provides the reasoning engine for multi-step logical inference. The approach uses pre-trained LLMs without task-specific retraining, with prompts carefully engineered to guide the parsing process. ASP knowledge modules encode domain-specific rules and constraints, and the system can identify the source of errors through its modular design. The framework is evaluated on multiple NLP benchmarks, demonstrating state-of-the-art performance while requiring minimal domain-specific knowledge encoding.

## Key Results
- Achieves state-of-the-art accuracy across multiple NLP benchmarks (bAbI, StepGame, CLUTRR, gSCAN)
- Resolves multi-step reasoning tasks that LLMs alone struggle with, achieving >99% accuracy on many tasks
- Identifies and corrects dataset errors that other models may overlook, detecting 107 wrong data instances in StepGame
- Demonstrates high accuracy without task-specific retraining, requiring only domain-specific ASP knowledge modules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can act as effective few-shot semantic parsers for diverse natural language inputs.
- Mechanism: Pre-trained LLMs have learned rich distributed representations that allow them to generalize from a small number of examples to parse linguistically variable sentences into logical atomic facts.
- Core assumption: The LLM's training data exposed it to sufficient linguistic diversity that it can map novel phrasings to canonical logical forms.
- Evidence anchors:
  - [abstract] "we observe that a large language model can serve as a highly effective few-shot semantic parser"
  - [section 3.1] "GPT-3 can turn various sentences...into the same atomic fact"
  - [corpus] "LLMs have revolutionized many areas...by achieving state-of-the-art performance on extensive downstream tasks"
- Break condition: The LLM fails to parse sentences that contain domain-specific knowledge not present in its training data, requiring explicit instruction in the prompt.

### Mechanism 2
- Claim: ASP provides robust symbolic reasoning that can handle multi-step logical inference.
- Mechanism: ASP's declarative nature and support for complex constraints allows it to perform systematic multi-step reasoning over the parsed facts, overcoming the shallow reasoning limitations of LLMs.
- Core assumption: The problem can be encoded as a set of logical rules and constraints that ASP can efficiently solve.
- Evidence anchors:
  - [abstract] "combining large language models and answer set programs leads to an attractive dual-process, neuro-symbolic reasoning"
  - [section 3.2] "The location module contains rules for spatial reasoning in a 2D grid space"
  - [section 4.5] "Our experiments confirm that LLMs like GPT-3 are still not good at multi-step reasoning"
- Break condition: The problem requires probabilistic or weighted reasoning not easily expressible in standard ASP.

### Mechanism 3
- Claim: The modular design enables interpretable error analysis and dataset validation.
- Mechanism: By separating semantic parsing (LLM) from reasoning (ASP), errors can be traced to their source - parsing errors, logical constraints, or dataset labels - enabling correction and improvement.
- Core assumption: The error sources are sufficiently distinct that they can be reliably identified and corrected.
- Evidence anchors:
  - [abstract] "the high accuracy and transparency allow us to easily identify the source of errors"
  - [section 4.5] "There are three sources of errors: semantic parsing in LLMs, symbolic constraints, and the dataset itself"
  - [section 4.3] "we detected 107 wrong data instances in the first 1000 data in StepGame"
- Break condition: Errors occur in combinations that make it difficult to isolate the root cause.

## Foundational Learning

- Concept: Semantic parsing
  - Why needed here: Converting natural language to logical forms is the critical bridge between human-readable input and machine-reasonable logic.
  - Quick check question: Given the sentence "Mary went to the kitchen", what would be the expected logical fact?

- Concept: Answer Set Programming
  - Why needed here: ASP provides the reasoning engine that can handle complex multi-step inference over the parsed facts.
  - Quick check question: What is the key difference between ASP and traditional logic programming in terms of handling negation?

- Concept: Prompt engineering
  - Why needed here: Effective few-shot learning with LLMs requires carefully crafted prompts that guide the model to produce desired outputs.
  - Quick check question: Why might enumerating all possible cases in a prompt be more effective than providing examples only?

## Architecture Onboarding

- Component map:
  Input: Natural language context and query -> LLM semantic parser: Converts sentences to atomic facts -> ASP knowledge modules: Domain-specific logical rules -> ASP solver: Performs reasoning to generate answers -> Output: Predicted answer

- Critical path:
  1. Parse each sentence independently using LLM with appropriate prompt
  2. Combine parsed facts with ASP knowledge modules
  3. Run ASP solver to generate answer
  4. Map answer back to natural language if needed

- Design tradeoffs:
  - Accuracy vs. cost: Larger LLMs provide better parsing but at higher API cost
  - Independence vs. context: Parsing sentences independently is more robust but may miss dependencies
  - Prompt specificity vs. generality: Detailed prompts improve accuracy but require more domain knowledge

- Failure signatures:
  - Incorrect parsing: LLM produces wrong atomic facts
  - Reasoning failure: ASP cannot find a solution given the facts
  - Dataset errors: Ground truth labels are incorrect

- First 3 experiments:
  1. Run bAbI Task 1 with simple prompts to verify basic parsing and reasoning
  2. Test StepGame with increasing chain lengths to evaluate multi-step reasoning
  3. Apply to CLUTRR dataset to validate family relationship reasoning and error detection capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the LLM+ASP approach scale to much larger knowledge bases with thousands of entities and complex relationships?
- Basis in paper: [inferred] The paper notes that "The total amount of knowledge that needs to be encoded for all of the above datasets is not too large" and that knowledge modules can be reused across tasks, suggesting this hasn't been tested at scale.
- Why unresolved: The experiments were conducted on relatively small datasets (bAbI, StepGame, CLUTRR, gSCAN) with limited entities and relationships. No experiments were conducted with large-scale knowledge bases.
- What evidence would resolve it: Experiments demonstrating performance on benchmarks with 1000+ entities and complex multi-hop reasoning, showing whether accuracy degrades and how much additional knowledge encoding is required.

### Open Question 2
- Question: Can the framework handle uncertainty and conflicting information in real-world text data?
- Basis in paper: [inferred] The paper mentions that "One may think that the logic rules are too rigid" and notes that "there are many weighted or probabilistic rules that can be defeated" but states these weren't needed for the benchmark problems tested.
- Why unresolved: All tested datasets appear to have clean, unambiguous data. The paper doesn't address how the system handles conflicting facts, uncertain information, or probabilistic reasoning.
- What evidence would resolve it: Experiments on datasets with contradictory information, noisy data, or probabilistic facts, demonstrating how the system resolves conflicts or represents uncertainty in its reasoning.

### Open Question 3
- Question: What is the computational overhead of using GPT-3 for semantic parsing compared to specialized semantic parsers?
- Basis in paper: [inferred] The paper mentions API costs for GPT-3 queries (e.g., "$41 for bAbI with 20k examples") but doesn't compare this to training or inference costs of specialized models.
- Why unresolved: While the paper demonstrates effectiveness, it doesn't provide a cost-benefit analysis comparing few-shot prompting with GPT-3 against training specialized semantic parsers for each task.
- What evidence would resolve it: A detailed comparison of computational costs (API fees, training time, inference speed) between the LLM+ASP approach and specialized semantic parsing models across multiple tasks.

## Limitations
- Effectiveness depends heavily on LLM quality for semantic parsing, which may degrade with specialized or ambiguous language
- Requires manual design of ASP knowledge modules for each domain, limiting scalability to diverse problem spaces
- Performance shows variability with smaller, more cost-effective LLMs, suggesting sensitivity to model capacity

## Confidence

**High Confidence:**
- The LLM can effectively parse natural language into logical facts using few-shot prompting (supported by consistent high accuracy across multiple benchmarks)
- ASP provides robust multi-step reasoning capabilities (demonstrated by performance on tasks requiring logical inference)
- The modular design enables interpretable error analysis (verified through systematic error categorization in experiments)

**Medium Confidence:**
- The approach achieves state-of-the-art performance without task-specific retraining (based on benchmark comparisons, but dependent on specific model versions and prompt engineering)
- The system can identify dataset errors (demonstrated on specific datasets but not comprehensively validated across all domains)

## Next Checks

1. **Cross-LLM Consistency Test**: Evaluate the system using multiple GPT-3 variants (including smaller, more cost-effective models) across all benchmarks to assess the sensitivity to model capacity and identify the minimum viable model for acceptable performance.

2. **Domain Generalization Experiment**: Apply the pre-trained system to a novel domain (e.g., medical reasoning or legal text analysis) without retraining to test the true generalization capabilities beyond the reported benchmarks.

3. **Error Source Isolation Validation**: Systematically generate synthetic examples where specific components are intentionally flawed (parsing errors, incomplete ASP rules, or incorrect labels) to validate the error analysis framework's ability to correctly identify the root cause in controlled scenarios.