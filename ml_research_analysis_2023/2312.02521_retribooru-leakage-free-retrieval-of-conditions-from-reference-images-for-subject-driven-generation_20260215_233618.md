---
ver: rpa2
title: 'RetriBooru: Leakage-Free Retrieval of Conditions from Reference Images for
  Subject-Driven Generation'
arxiv_id: '2312.02521'
source_url: https://arxiv.org/abs/2312.02521
tags:
- images
- prompt
- more
- image
- reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces RetriBooru, a new anime dataset with detailed
  annotations for identity and clothing. It proposes two tasks: same-character generation
  and concept composition, and a new baseline method (RetriNet) that retrieves conditions
  from multiple reference images.'
---

# RetriBooru: Leakage-Free Retrieval of Conditions from Reference Images for Subject-Driven Generation

## Quick Facts
- arXiv ID: 2312.02521
- Source URL: https://arxiv.org/abs/2312.02521
- Reference count: 32
- Key outcome: Introduces RetriBooru dataset and RetriNet method for subject-driven generation with improved identity preservation and diversity

## Executive Summary
This paper introduces RetriBooru, a new anime dataset with detailed annotations for identity and clothing, and proposes a novel approach for subject-driven image generation. The method retrieves conditions from multiple reference images to generate personalized content while maintaining prompt control. A new diversity metric called Similarity Weighted Diversity (SWD) is introduced to better evaluate generation flexibility. Experiments demonstrate improved performance over existing methods in terms of identity preservation, prompt consistency, and diversity metrics.

## Method Summary
The proposed RetriNet method uses a retrieval-augmented generation approach with frozen encoders and trainable retrieval and SD decoder blocks. The architecture employs cross-attention layers to selectively extract identity information from multiple reference images while ignoring irrelevant details like backgrounds. The training process uses a masked loss function that focuses learning on relevant regions, and employs prompt dropout to balance between reference image information and text prompts.

## Key Results
- Outperforms existing methods (IP-Adapter, FastComposer) in identity preservation and prompt consistency
- Achieves higher diversity scores measured by the novel SWD metric
- Demonstrates effective same-character generation and concept composition tasks on the RetriBooru dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frozen SD encoder/decoder blocks prevent overfitting to reference image styles and backgrounds
- Mechanism: By preserving pretrained knowledge through frozen components, the model extracts relevant semantic information without copying background details
- Core assumption: Pretrained SD models have learned useful general features that should be preserved
- Evidence anchors: Abstract mentions avoiding augmentation from target images, section describes frozen initialization
- Break condition: If pretrained features don't generalize well to target domain

### Mechanism 2
- Claim: Cross-attention structure enables selective extraction of identity information from multiple reference images
- Mechanism: Cross-attention layers learn to attend to relevant parts (faces) while ignoring irrelevant information (backgrounds)
- Core assumption: Cross-attention can distinguish between relevant and irrelevant visual information
- Evidence anchors: Abstract mentions cross-attention enabling retrieval through control blocks
- Break condition: If cross-attention learns to attend to wrong regions or copies too much information

### Mechanism 3
- Claim: VQA-based SWD metric better captures generation flexibility than traditional similarity metrics
- Mechanism: Uses VQA models to generate text descriptions and measures cosine distance between embeddings
- Core assumption: VQA models can generate consistent, meaningful descriptions of generated images
- Evidence anchors: Abstract introduces SWD metric, section describes VQA-based evaluation
- Break condition: If VQA models generate inconsistent or irrelevant descriptions

## Foundational Learning

- Concept: Diffusion models and denoising process
  - Why needed here: Builds on Stable Diffusion architecture and modifies training process
  - Quick check question: What are key differences between denoising diffusion probabilistic models and latent diffusion models?

- Concept: Cross-attention mechanisms
  - Why needed here: Relies on cross-attention layers to retrieve information from reference images
  - Quick check question: How does cross-attention differ from self-attention in terms of what it attends to?

- Concept: RAG (Retrieval-Augmented Generation) paradigm
  - Why needed here: Frames subject-driven generation as a RAG problem
  - Quick check question: What are key components of a typical RAG system and how do they map to image generation?

## Architecture Onboarding

- Component map: Reference images → VAE encoder → Retrieval encoder → Cross-attention → U-Net decoder → Generated image
- Critical path: Information flows from reference images through retrieval components to generation output
- Design tradeoffs:
  - Freezing vs training SD components: Frozen preserves generalization but limits adaptation
  - Number of reference images: More references enable complex compositions but increase computational cost
  - Prompt dropout probability: Higher dropout forces reliance on references but may hurt prompt control
- Failure signatures:
  - Mode collapse: Generated images look too similar to each other
  - Background leakage: Generated images copy background styles from references
  - Identity loss: Generated images don't preserve subject identity well
- First 3 experiments:
  1. Train with reconstruction task on RetriBooru dataset, evaluate CLIP-I/CLIP-T scores
  2. Compare with IP-Adapter and FastComposer on human face datasets, qualitative evaluation
  3. Ablation study: Vary prompt dropout probability, evaluate impact on CLIP and diversity metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve clothing identity clustering to capture finer details beyond colors?
- Basis in paper: [explicit] Current clustering overlooks textures and design patterns, too strict and rejects usable data
- Why unresolved: Color-based clustering is too simplistic; more powerful VQA models needed for detailed features
- What evidence would resolve it: Advanced VQA model that accurately clusters clothing identities based on detailed features

### Open Question 2
- Question: What is the optimal probability of dropping text prompts during training for concept composition tasks?
- Basis in paper: [explicit] Concept composition tasks are sensitive to different prompt droprates
- Why unresolved: Changing droprate leads to noisy observations, finding optimal balance is challenging
- What evidence would resolve it: Systematic experiments varying prompt droprate with comprehensive evaluation

### Open Question 3
- Question: How can we design a more effective metric for evaluating generation flexibility and diversity?
- Basis in paper: [explicit] SWD metric needs refinement, should be re-weighted by existing similarity scores
- Why unresolved: While SWD is a step forward, better metrics are needed for comprehensive evaluation
- What evidence would resolve it: Improved diversity metric considering both similarity and diversity aspects

## Limitations

- Experimental validation limited to anime-style images and simple compositions
- Effectiveness for complex scenarios (multiple subjects, diverse backgrounds, non-anime domains) untested
- Claims about preventing overfitting and leakage need more rigorous quantitative validation

## Confidence

- High confidence: Dataset creation methodology and basic experimental setup are well-documented and reproducible
- Medium confidence: SWD metric and implementation are novel but validation across different domains is limited
- Low confidence: Claims about preventing overfitting and leakage through frozen encoders need more rigorous quantitative validation

## Next Checks

1. Test frozen encoder approach on non-anime dataset (e.g., human faces or real-world objects) to verify generalization claims
2. Conduct ablation studies comparing different levels of encoder freezing to quantify trade-off between generalization and adaptation
3. Evaluate SWD metric against human judgments of diversity to validate its correlation with perceptual diversity