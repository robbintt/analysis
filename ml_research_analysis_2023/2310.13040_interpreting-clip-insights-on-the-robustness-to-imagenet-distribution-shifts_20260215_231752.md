---
ver: rpa2
title: 'Interpreting CLIP: Insights on the Robustness to ImageNet Distribution Shifts'
arxiv_id: '2310.13040'
source_url: https://arxiv.org/abs/2310.13040
tags:
- concepts
- features
- imagenet
- robust
- outlier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates what distinguishes robust models from
  non-robust ones, focusing on CLIP models. Through a detailed analysis of 16 robust
  zero-shot CLIP vision encoders with various backbones and pretraining sets, and
  comparing them to less robust models, the authors find two signatures of robustness
  in the representation spaces of these models: Robust models exhibit outlier features,
  characterized by activations several orders of magnitude above average, which induce
  privileged directions in the model''s representation space.'
---

# Interpreting CLIP: Insights on the Robustness to ImageNet Distribution Shifts

## Quick Facts
- **arXiv ID**: 2310.13040
- **Source URL**: https://arxiv.org/abs/2310.13040
- **Reference count**: 40
- **Primary result**: Identifies two signatures of robustness in CLIP models: outlier features inducing privileged directions, and encoding more unique (polysemantic) concepts

## Executive Summary
This paper investigates what distinguishes robust multimodal models from non-robust ones by analyzing CLIP vision encoders. Through a comprehensive study of 16 robust zero-shot CLIP models compared to less robust variants, the authors identify two key signatures of robustness in representation spaces. They find that robust models exhibit outlier features with activations orders of magnitude above average, which create privileged directions explaining most predictive power. Additionally, robust models encode substantially more unique concepts, leading to polysemantic features. The authors demonstrate these signatures can be detected without access to shifted datasets, potentially serving as practical tools for assessing model robustness during deployment.

## Method Summary
The study analyzes representation spaces of CLIP models using quantitative interpretability tools. The method involves loading pretrained CLIP models with various backbones (ResNet and ViT) and pretraining sets, computing activation vectors on ImageNet test sets, and applying kurtosis analysis to detect outlier features. SVD decomposition of classification head weight matrices identifies privileged directions, while concept probing on the Broden dataset quantifies unique concepts encoded in representation space. The analysis compares zero-shot models against finetuned and supervised counterparts, examining correlations between these signatures and Effective Robustness (ER) metrics.

## Key Results
- Robust models exhibit outlier features with activations several orders of magnitude above average
- Robust models encode 400-700 unique concepts versus 300-600 for non-robust models
- Kurtosis values >> 3 in robust models serve as proxy for ImageNet shift robustness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Outlier features induce privileged directions that explain most of model performance and robustness
- **Mechanism**: Activation vectors in robust models contain components several orders of magnitude above average. These outlier features propagate through the linear classification head via SVD decomposition, creating privileged directions with high importance scores (Equation 4). These privileged directions capture most predictive power, allowing pruning of up to 80% of less important directions without performance loss.
- **Core assumption**: The SVD decomposition of the classification head weight matrix accurately captures the importance of representation space directions
- **Evidence anchors**: [abstract]: "Robust models exhibit outlier features characterized by their activations, with some being several orders of magnitude above average"; [section 3]: "The strong activation of outlier features in robust models does not necessarily explain the performances... we now introduce the notion of privileged directions"
- **Break condition**: If the linear head weight matrix doesn't effectively utilize outlier features, or if the SVD-based importance metric fails to capture true predictive importance

### Mechanism 2
- **Claim**: Robust models encode substantially more unique concepts than non-robust models
- **Mechanism**: Robust zero-shot CLIP models encode a higher number of unique concepts in their representation space (Table 3). This leads to polysemantic features where directions encode multiple unrelated concepts. The Broden dataset concept probing shows zero-shot models encode 400-700 unique concepts versus 300-600 for finetuned models.
- **Core assumption**: The Broden dataset concept probing accurately measures unique concepts encoded in representation space
- **Evidence anchors**: [abstract]: "Robust models encode substantially more concepts in their representation space"; [section 4]: "zero-shot models encode substantially more concepts than their finetuned and supervised counterparts"
- **Break condition**: If concept probing misses important concepts not in Broden dataset, or if the concept encoding metric doesn't correlate with true model capability

### Mechanism 3
- **Claim**: Activation kurtosis serves as a proxy for ImageNet shift robustness
- **Mechanism**: Kurtosis measures presence of outlier features (Equation 2). Robust zero-shot models show kurtosis >> 3, while finetuned and supervised models show kurtosis ≈ 3. This tracks ER metric across models, with kurtosis decreasing as ER decreases (Table 2). The trend holds when interpolating between zero-shot and finetuned models (Appendix G).
- **Core assumption**: Kurtosis is a reliable indicator of outlier feature presence and correlates with robustness
- **Evidence anchors**: [abstract]: "Through an SVD analysis, we show that these outlier features are propagated to the logits of downstream classifiers"; [section 3]: "All models with high robustness have outlier features, as indicated by their kurtosis ≫ 3"
- **Break condition**: If kurtosis fails to capture outlier features in certain architectures, or if outlier features don't correlate with robustness in new model types

## Foundational Learning

- **Concept**: SVD decomposition and its interpretation
  - *Why needed here*: Essential for identifying privileged directions and understanding how outlier features propagate to logits
  - *Quick check question*: If W = UΣV^T, what do the columns of V represent and why are they important for this analysis?

- **Concept**: Kurtosis and its interpretation
  - *Why needed here*: Critical for detecting outlier features which are a signature of robustness
  - *Quick check question*: What does kurtosis >> 3 indicate about the distribution of activation vectors?

- **Concept**: Concept probing methodology
  - *Why needed here*: Used to quantify unique concepts encoded in representation space and measure polysemanticity
  - *Quick check question*: How does the AP threshold of 0.9 determine whether a concept is "encoded" in a direction?

## Architecture Onboarding

- **Component map**: Model pool includes ResNet50/101 and ViT-B-16/32 architectures with various pretraining sets (OpenAI, LAION-400M/2B, YFCC15M, CC12M, DataComp). Analysis focuses on last layer activation vectors and linear classification head weight matrix W.
- **Critical path**: Load pretrained models → compute activation vectors on ImageNet test set → calculate kurtosis and perform SVD on W → perform concept probing on Broden dataset → analyze results for robustness signatures
- **Design tradeoffs**: Tradeoff between robustness (high kurtosis, many concepts) and interpretability (polysemanticity makes features harder to interpret). Also tradeoff between ER and in-distribution accuracy when finetuning.
- **Failure signatures**: Low kurtosis (≈ 3) indicates lack of outlier features and likely low robustness. Small number of unique concepts indicates potential robustness issues. Absence of privileged directions in SVD analysis suggests non-robust model.
- **First 3 experiments**:
  1. Load a zero-shot CLIP model and compute activation kurtosis on ImageNet test set - expect >> 3
  2. Perform SVD on classification head weight matrix and compute importance scores for all directions - expect one strongly privileged direction
  3. Run concept probing on last layer representation space - expect 400-700 unique concepts for zero-shot model

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do outlier features and privileged directions exist in other robust model architectures beyond CLIP, such as purely vision-based models or language models?
- **Basis in paper**: [explicit] The paper discusses outlier features in CLIP models and briefly mentions NoCLIP (a pure vision model) as having privileged directions. However, it does not extensively investigate other model architectures.
- **Why unresolved**: The paper's scope is limited to CLIP models and NoCLIP, leaving open the question of whether this phenomenon is specific to CLIP or more general across robust architectures.
- **What evidence would resolve it**: Systematic investigation of outlier features and privileged directions across diverse robust model families, including purely vision-based models, language models, and other multimodal architectures.

### Open Question 2
- **Question**: What is the precise mechanism by which outlier features contribute to robustness against distribution shifts?
- **Basis in paper**: [explicit] The paper identifies outlier features as a signature of robustness and shows they induce privileged directions, but does not explain the underlying mechanism of how they confer robustness.
- **Why unresolved**: While the paper establishes correlation between outlier features and robustness, it does not provide a causal explanation or mechanistic understanding of how outlier features specifically enable models to generalize better to distribution shifts.
- **What evidence would resolve it**: Experiments that ablate outlier features while controlling for other factors, or theoretical analysis linking outlier feature properties to specific robustness mechanisms (e.g., invariance to certain types of perturbations).

### Open Question 3
- **Question**: Can we develop more interpretable robust models by explicitly controlling the number of unique concepts encoded, potentially at the cost of some robustness?
- **Basis in paper**: [inferred] The paper finds that robust models encode more unique concepts but are highly polysemantic, making them less interpretable. This suggests a potential trade-off between robustness and interpretability.
- **Why unresolved**: The paper identifies the trade-off but does not explore whether it's possible to design models that maintain robustness while reducing polysemanticity and improving interpretability.
- **What evidence would resolve it**: Development and evaluation of model architectures or training procedures that explicitly limit concept superposition while maintaining high robustness, along with interpretability metrics demonstrating improved feature clarity.

## Limitations
- The kurtosis analysis may not generalize to architectures beyond CLIP-style vision encoders
- The concept probing methodology using Broden dataset may miss domain-specific concepts not present in its limited scope
- The connection between these representation-space signatures and actual out-of-distribution performance remains correlational rather than causal

## Confidence
- **High confidence**: The observation that zero-shot CLIP models exhibit kurtosis values significantly greater than 3, and that finetuned models show kurtosis closer to 3
- **Medium confidence**: The relationship between outlier features and privileged directions, and the claim about robust models encoding substantially more unique concepts
- **Medium confidence**: The use of kurtosis as a proxy for ImageNet shift robustness

## Next Checks
1. **Architecture Generalization Test**: Apply the kurtosis and SVD analysis to non-CLIP models (e.g., standard ResNet trained with different objectives) to verify whether the outlier feature/privileged direction signature generalizes beyond CLIP architectures.

2. **Concept Probing Expansion**: Repeat the concept probing analysis using a more comprehensive concept dataset (beyond Broden) and test whether the same trend of zero-shot models encoding more unique concepts holds across different concept categories and domains.

3. **Intervention Experiment**: Design an experiment where you artificially increase kurtosis in a finetuned model (through weight manipulation or targeted training) and measure whether this intervention actually improves robustness on distribution-shifted datasets, establishing causal rather than merely correlational evidence.