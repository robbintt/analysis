---
ver: rpa2
title: From Adaptive Query Release to Machine Unlearning
arxiv_id: '2307.11228'
source_url: https://arxiv.org/abs/2307.11228
tags:
- unlearning
- algorithm
- query
- learning
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies efficient machine unlearning for adaptive query
  release, where learning algorithms perform adaptive queries from structured classes
  like linear and prefix-sum queries. The authors propose using binary tree mechanisms
  to design TV-stable learning algorithms and construct maximal couplings for efficient
  unlearning.
---

# From Adaptive Query Release to Machine Unlearning

## Quick Facts
- **arXiv ID**: 2307.11228
- **Source URL**: https://arxiv.org/abs/2307.11228
- **Reference count**: 40
- **Primary result**: Proposes efficient machine unlearning for adaptive query release using binary tree mechanisms with O(ρ) relative unlearning complexity for prefix-sum queries

## Executive Summary
This paper bridges the gap between adaptive query release and machine unlearning by developing efficient unlearning algorithms for learning tasks that perform adaptive queries. The authors focus on prefix-sum queries and propose using binary tree mechanisms to maintain TV-stable learning procedures that enable efficient unlearning. They show that when a data point is deleted, the algorithm can reuse most of the computation from the original training through maximal coupling, achieving unlearning complexity that is only a factor of ρ (the TV stability parameter) more expensive than retraining. The framework is applied to stochastic convex optimization and generalized linear models, yielding improved excess population risk bounds with dimension-independent rates for GLMs.

## Method Summary
The method centers on using binary tree mechanisms for prefix-sum queries, where noise is added in a correlated structure rather than independently. The TreeLearn algorithm builds a binary tree where each leaf corresponds to a data point, and internal nodes store noisy prefix-sum queries. For unlearning, TreeUnlearn uses reflection coupling to maximally couple the output distributions under original and updated datasets, allowing most of the computation to be reused. The key insight is that prefix-sum queries have bounded sensitivity, enabling efficient unlearning with O(ρ) relative complexity. For SCO applications, they combine this with variance-reduced Frank-Wolfe (smooth) or Dual Averaging (non-smooth) methods. For GLMs, they use Johnson-Lindenstrauss dimension reduction to achieve dimension-independent rates.

## Key Results
- Achieves O(ρ) relative unlearning complexity for prefix-sum queries, where unlearning is only a factor of ρ more expensive than retraining
- For SCO with smooth losses: O(1/√n + √d/(nρ)) excess population risk with O(ρn) expected gradient complexity
- For SCO with non-smooth losses: O(1/√n + (d/(nρ))^{1/4}) excess population risk
- For GLMs: dimension-independent rates achieved through Johnson-Lindenstrauss dimension reduction
- Extends to dynamic streams with insertions and deletions, achieving weak unlearning with better update times

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The binary tree mechanism enables efficient unlearning for prefix-sum queries by maintaining correlated noise structure.
- **Mechanism**: Instead of adding fresh independent noise to each prefix-sum query, the binary tree mechanism adds correlated noise where the correlation structure is described by a binary tree. When unlearning, the algorithm can adjust noise values along specific paths in the tree rather than recomputing everything.
- **Core assumption**: The query class being learned (prefix-sum) has bounded sensitivity, and the tree structure maintains sufficient correlation between queries.
- **Evidence anchors**:
  - [abstract] "We give a binary-tree based ρ-TV stable learning procedure and a corresponding unlearning procedure with a eO(ρ) relative unlearning complexity."
  - [section 4.1] "The key idea in the binary tree mechanism is that instead of adding fresh independent noise to each prefix-sum query, it is better to add correlated noise, where the correlation structure is described by a binary tree."
- **Break condition**: If the prefix-sum queries don't have bounded sensitivity, or if the tree structure becomes too unbalanced, the noise correlation breaks down and unlearning efficiency degrades.

### Mechanism 2
- **Claim**: Reflection coupling maximally couples the learning algorithm's output distributions under original and updated datasets.
- **Mechanism**: When a data point is deleted, the algorithm computes what the query responses would have been without that point, then uses reflection coupling to generate samples from the new distribution using the old noise values.
- **Core assumption**: The learning algorithm is ρ-TV stable, meaning the total variation distance between outputs on original and updated datasets is bounded by ρ.
- **Evidence anchors**:
  - [abstract] "We use the technique of reflection coupling described below."
  - [section 2] "Reflection Coupling is a classical technique in probability to maximally couple symmetric probability distributions."
- **Break condition**: If the TV stability parameter ρ becomes too large (approaching 1), the probability of needing to retrain approaches 1, making unlearning as expensive as retraining.

### Mechanism 3
- **Claim**: The JL (Johnson-Lindenstrauss) method enables dimension-independent rates for generalized linear models by reducing dimensionality while preserving distances.
- **Mechanism**: The method embeds high-dimensional data into a lower-dimensional space using a random projection matrix, then runs a base algorithm on the reduced data. This preserves the essential structure needed for accurate learning while reducing computational complexity.
- **Core assumption**: The random projection matrix satisfies the (β, γ)-JL property, meaning it approximately preserves inner products between vectors.
- **Evidence anchors**:
  - [abstract] "Furthermore, in the special case of Generalized Linear Models (GLMs)... we get dimension-independent rates"
  - [section 5.3] "The method, described in Algorithm 4, simply embeds the dataset into a low dimensional space, via a JL matrix Φ, and then runs a base algorithm on the low dimensional dataset."
- **Break condition**: If the embedding dimension k is too small relative to the problem parameters, the JL property fails to hold, leading to loss of accuracy.

## Foundational Learning

- **Concept: Total Variation (TV) stability**
  - Why needed here: TV stability provides the theoretical foundation for constructing efficient unlearning algorithms through maximal coupling. It bounds how much the output distribution changes when a single data point is removed.
  - Quick check question: If an algorithm has TV stability parameter ρ = 0.1, what is the maximum probability that the unlearning algorithm will need to retrain from scratch? (Answer: ≤ 0.1)

- **Concept: Maximal coupling**
  - Why needed here: Maximal coupling allows the unlearning algorithm to reuse most of the computation from the original training by finding the closest possible match between the original and updated output distributions.
  - Quick check question: If two distributions have TV distance 0.2, what is the minimum probability that a maximal coupling will produce different samples from the two distributions? (Answer: ≥ 0.2)

- **Concept: Prefix-sum queries**
  - Why needed here: Prefix-sum queries are the specific query class that enables efficient unlearning for many optimization algorithms, as they can be answered using the binary tree mechanism with correlated noise.
  - Quick check question: Why is a prefix-sum query of the form q({wi}i, S) = Σi∈S pi({wj}j≤i, zi) more amenable to efficient unlearning than an arbitrary linear query? (Answer: Because prefix-sum queries have bounded sensitivity and can be answered using correlated noise in a binary tree structure)

## Architecture Onboarding

- **Component map**: TreeLearn -> Binary tree construction -> Query storage -> Noise addition; TreeUnlearn -> Deleted point identification -> Tree traversal -> Reflection coupling -> Retraining decision

- **Critical path**: For unlearning: identify the deleted point's leaf node → compute replacement query values → traverse up the tree adjusting nodes → perform rejection sampling/reflection coupling → potentially trigger retraining

- **Design tradeoffs**: The system trades off between TV stability parameter ρ (smaller is better for unlearning efficiency) and accuracy. Smaller ρ requires more noise, which can hurt statistical performance. The tree depth also affects both memory usage and unlearning speed.

- **Failure signatures**: 
  - If unlearning becomes as slow as retraining, the TV stability parameter is too large or the query class doesn't have bounded sensitivity.
  - If accuracy degrades significantly after unlearning, the noise variance σ may be too small to maintain TV stability.
  - If memory usage explodes, the binary tree may be growing too large without proper pruning.

- **First 3 experiments**:
  1. Implement the binary tree mechanism for simple prefix-sum queries (like computing cumulative sums) and verify that noise is properly correlated across queries.
  2. Test the unlearning algorithm on a small dataset with known prefix-sum queries, measuring the actual unlearning complexity vs. retraining complexity.
  3. Evaluate the trade-off between TV stability parameter ρ and accuracy by running the algorithm with different noise levels and measuring both metrics.

## Open Questions the Paper Calls Out

1. **Extending to more query classes**: Can the prefix-sum query framework be extended to handle more general query classes beyond linear and prefix-sum queries? The paper mentions this as a future direction but doesn't provide specific extensions or methods.

2. **Scaling with number of unlearning requests**: How does the unlearning efficiency scale with the number of unlearning requests in a dynamic stream? The paper provides results for one unlearning request and gives bounds for V unlearning requests, but doesn't analyze the scaling behavior as the number of requests grows large.

3. **Non-i.i.d. data distributions**: Can the unlearning framework be extended to non-i.i.d. data distributions or adaptive adversaries? The paper explicitly assumes i.i.d. data and oblivious unlearning requests, which are strong assumptions not always met in practice.

## Limitations

- **Empirical validation gap**: The paper lacks experimental results demonstrating the practical performance of the proposed algorithms, making it difficult to assess real-world applicability of theoretical guarantees.

- **Complexity constants**: The bounds hide significant constants, particularly in the noise variance settings (σ² = 64B²log²(n)/ρ²), which may impact practical performance.

- **Assumption sensitivity**: The results heavily depend on bounded sensitivity assumptions for the query classes, which may not hold for all natural learning problems.

## Confidence

- **Medium Confidence**: The O(ρ) relative unlearning complexity for prefix-sum queries is well-supported by theoretical analysis and binary tree mechanism construction.
- **Low Confidence**: The dimension-independent rates for generalized linear models rely on JL transform properties but lack empirical validation.
- **Medium Confidence**: The extension to dynamic streams with insertions and deletions shows theoretical promise but "weak unlearning" guarantees are less rigorous than exact unlearning results.

## Next Checks

1. Implement and benchmark the binary tree mechanism on synthetic prefix-sum query tasks to verify the O(ρ) unlearning complexity empirically across different values of ρ and dataset sizes.

2. Conduct a simulation study comparing the dimension-independent GLM rates against standard algorithms across varying dimensions to validate the practical benefits of the JL-based approach.

3. Test the dynamic stream unlearning algorithm on real-world streaming datasets with both insertions and deletions to evaluate the practical impact of the "weak unlearning" guarantees on update times and accuracy.