---
ver: rpa2
title: Globally Normalising the Transducer for Streaming Speech Recognition
arxiv_id: '2307.10975'
source_url: https://arxiv.org/abs/2307.10975
tags:
- normalised
- training
- speech
- transducer
- normalisation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the label bias problem in streaming Transducer
  models, which restricts their ability to revise predictions and results in suboptimal
  word error rates and increased latency. The core method idea is to replace local
  normalization with global normalization during training by approximating the loss
  function using an N-best list of hypotheses, rather than approximating the model
  itself as previous work did.
---

# Globally Normalising the Transducer for Streaming Speech Recognition

## Quick Facts
- arXiv ID: 2307.10975
- Source URL: https://arxiv.org/abs/2307.10975
- Reference count: 22
- Key outcome: Globally normalized streaming Transducer models reduce WER by 9-11% relative compared to locally normalized baseline on LibriSpeech

## Executive Summary
This paper addresses the label bias problem in streaming Transducer models by replacing local normalization with global normalization during training. The core innovation is approximating the global normalization loss function using an N-best list of hypotheses rather than approximating the model itself as previous work did. Experiments on LibriSpeech show significant WER improvements (4.7 → 4.2 test-clean, 12.5 → 11.1 test-other) and reduced token emission latency, closing nearly half the gap between streaming and lookahead modes.

## Method Summary
The paper proposes globally normalizing the Transducer during training by approximating the loss function using an N-best list of hypotheses instead of the model itself. This is achieved through three key techniques: interpolating between local and global normalization during training using a log-linear combination, adding regularization to encourage roughly normalized output weights, and using custom batch beam search to generate the N-best list. The method maintains streaming capability while addressing the label bias problem that restricts prediction revision in locally normalized models.

## Key Results
- Streaming WER reduction: 4.7 → 4.2 (test-clean) and 12.5 → 11.1 (test-other), representing 9-11% relative improvement
- Reduced average token emission latency compared to baseline
- Closed approximately 44% of the gap between streaming (4.7 WER) and lookahead (3.7 WER) modes
- Training time increased from 2.5 to 20 hours per epoch due to N-best computation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local normalization restricts the model's ability to revise predictions because each emission must be normalized immediately.
- Mechanism: In streaming mode, the model computes P(y_v | x_1:t(v), y_1:v-1) for each step v. With local normalization, this probability must sum to 1 over all possible y_v, preventing the model from adjusting probabilities of previously emitted tokens based on future context.
- Core assumption: The factorization in equation (2) is valid for streaming recognition, but the local normalization constraint prevents proper global probability computation.
- Evidence anchors:
  - [abstract] "the Transducer has a mathematical flaw which, simply put, restricts the model's ability to change its mind"
  - [section 2.1] "To produce a normalised distribution over the first word, the complete observation must have been seen"

### Mechanism 2
- Claim: Global normalization allows the model to compute proper probabilities over complete label sequences by considering all possible alignments.
- Mechanism: Instead of normalizing at each step, the model computes unnormalized scores f(y_v | ·) and introduces a global normalization term Z(x) that sums over all possible label sequences, allowing proper probability computation across the entire sequence.
- Core assumption: The global normalization term Z(x) can be approximated effectively using an N-best list of hypotheses.
- Evidence anchors:
  - [section 2.2] "the solution in this work is straightforward: instead of normalising locally, this paper proposes to normalise globally"
  - [section 3] "the sum over all hypotheses in the normalisation term is approximated, by considering only a smaller set of hypotheses"

### Mechanism 3
- Claim: Interpolating between local and global normalization during training helps the model transition smoothly and enables effective beam search.
- Mechanism: The model starts with local normalization (α = 1) and gradually interpolates to global normalization (α = 0) using log-linear combination, while regularizing output weights to encourage approximate normalization.
- Core assumption: The interpolation weight schedule can be tuned to balance training stability with the benefits of global normalization.
- Evidence anchors:
  - [section 3.1] "this work will propose slowly interpolating from a locally normalised to a globally normalised model while training"
  - [section 4.2] "As the interpolation weight is reduced, over the course of a few epochs, the model loss goes down"

## Foundational Learning

- Concept: Forward-backward algorithm for sequence models
  - Why needed here: Used to compute gradients for both the reference path and N-best hypotheses during training
  - Quick check question: What is the difference between forward algorithm and forward-backward algorithm in this context?

- Concept: Beam search for hypothesis generation
  - Why needed here: Used to generate the N-best list of hypotheses for approximating the global normalization term
  - Quick check question: Why can't standard beam search work directly with globally normalized models?

- Concept: Label bias problem in sequence models
  - Why needed here: The fundamental issue that local normalization creates in streaming models
  - Quick check question: How does label bias manifest in the simple "mail order" vs "nail polish" example?

## Architecture Onboarding

- Component map: Audio → Conformer → Joiner → Unnormalized scores → N-best beam search → Forward-backward on N+1 hypotheses → Loss computation
- Critical path: Audio → Conformer → Joiner → Unnormalized scores → N-best beam search → Forward-backward on N+1 hypotheses → Loss computation
- Design tradeoffs:
  - N-best list size vs training speed (10-best gives 20h/epoch vs 2h/epoch for baseline)
  - Interpolation schedule vs model performance
  - Regularization strength vs approximate normalization quality
- Failure signatures:
  - Training loss plateaus early (beam search finding poor competitors)
  - WER improvement stalls after initial gains
  - Memory errors during forward-backward computation
- First 3 experiments:
  1. Verify interpolation schedule: Train with α = 1.0 → 0.8 → 0.6 and monitor N-best quality
  2. Test N-best sensitivity: Compare 5-best vs 10-best vs 20-best training performance
  3. Validate regularization: Check output weight normalization statistics during training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of the interpolation weight α for balancing training stability and performance in globally normalized Transducer models?
- Basis in paper: [explicit] The paper mentions that α = 0.3 performs best for "test-clean" but does not explore other potential optimal values.
- Why unresolved: The paper does not conduct a thorough hyperparameter sweep for α, instead using initial experiments to select a value.
- What evidence would resolve it: A comprehensive grid search or Bayesian optimization study varying α from 0 to 1 with fine-grained steps would reveal the optimal value.

### Open Question 2
- Question: How does the size of the N-best list impact the trade-off between computational cost and model performance?
- Basis in paper: [explicit] The paper uses N = 10 for the N-best list but mentions that increasing N further might decrease the word error rate.
- Why unresolved: The paper only tests N = 2, 5, and 10, and does not explore larger values of N.
- What evidence would resolve it: Training models with varying N (e.g., 15, 20, 50) and measuring both WER and training time would quantify the trade-off.

### Open Question 3
- Question: Can the training efficiency of globally normalized Transducer models be improved without sacrificing performance?
- Basis in paper: [explicit] The paper notes that training is significantly slower (20 hours vs 2.5 hours per epoch) due to the need for N-best list generation and multiple forward-backward passes.
- Why unresolved: The paper does not explore alternative methods to reduce computational cost, such as parallelization or approximation techniques.
- What evidence would resolve it: Implementing and testing strategies like distributed training, model pruning, or approximate algorithms for N-best list generation would demonstrate potential improvements.

## Limitations

- The method requires 10× longer training time (20 hours/epoch vs 2 hours/epoch) due to N-best computation and multiple forward-backward passes
- The approach depends on careful tuning of the interpolation schedule and regularization parameters
- The N-best list approximation quality is not theoretically bounded and may break down for complex recognition tasks

## Confidence

**High Confidence (9/10)**: Experimental results showing WER reduction from 4.7 → 4.2 (test-clean) and 12.5 → 11.1 (test-other) for streaming models, representing 9-11% relative improvement.

**Medium Confidence (7/10)**: Mechanism explanation for why local normalization causes label bias in streaming Transducers, based on sound theoretical arguments but lacking quantitative validation.

**Low Confidence (4/10)**: Claim that the method "closes almost half the gap between streaming and lookahead modes" without accounting for other architectural differences between streaming and lookahead implementations.

## Next Checks

1. **N-best sensitivity analysis**: Systematically vary the N-best list size (5, 10, 15, 20, 30) and measure both the approximation quality (through entropy of the normalization term) and the downstream WER/latency performance.

2. **Beam search competitor quality evaluation**: Implement metrics to measure the quality and diversity of competitor hypotheses found by the N-best beam search during training, tracking statistics such as average path length and edit distance to reference.

3. **Interpolation schedule robustness test**: Conduct an ablation study varying the interpolation schedule parameters (starting α, ending α, number of epochs, schedule shape) to identify the most critical hyperparameters and their sensitivity.