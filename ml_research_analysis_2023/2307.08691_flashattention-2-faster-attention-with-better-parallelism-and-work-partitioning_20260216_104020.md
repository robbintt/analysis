---
ver: rpa2
title: 'FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning'
arxiv_id: '2307.08691'
source_url: https://arxiv.org/abs/2307.08691
tags:
- attention
- flashattention
- pass
- memory
- speed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FlashAttention-2 addresses the inefficiency of FlashAttention\
  \ by improving work partitioning between thread blocks and warps on GPUs, resulting\
  \ in a 2\xD7 speedup and reaching 50-73% of the theoretical maximum FLOPs/s on A100\
  \ GPUs. The key improvements include: 1) reducing non-matmul FLOPs, 2) parallelizing\
  \ attention computation across thread blocks, and 3) distributing work between warps\
  \ to minimize shared memory communication."
---

# FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning

## Quick Facts
- arXiv ID: 2307.08691
- Source URL: https://arxiv.org/abs/2307.08691
- Reference count: 20
- Key outcome: 2× speedup over FlashAttention, reaching 50-73% of theoretical maximum FLOPs/s on A100 GPUs

## Executive Summary
FlashAttention-2 addresses the inefficiency of FlashAttention by improving work partitioning between thread blocks and warps on GPUs. The key improvements include reducing non-matmul FLOPs, parallelizing attention computation across thread blocks, and distributing work between warps to minimize shared memory communication. These optimizations yield up to 225 TFLOPs/s per A100 GPU when used to train GPT-style models, achieving 72% model FLOPs utilization.

## Method Summary
FlashAttention-2 is an optimized implementation of the attention mechanism for GPUs that improves upon the original FlashAttention by parallelizing attention computation across thread blocks and distributing work between warps within each thread block. The algorithm reduces non-matmul FLOPs through tweaks to the online softmax algorithm and uses tiling to minimize memory reads/writes. The implementation achieves 50-73% of theoretical maximum FLOPs/s on A100 GPUs and up to 10× faster than standard attention implementations.

## Key Results
- 2× speedup over FlashAttention
- 50-73% of theoretical maximum FLOPs/s on A100 GPUs
- Up to 10× faster than standard attention implementations

## Why This Works (Mechanism)

### Mechanism 1
Parallelizing attention computation across thread blocks and warps reduces shared memory communication and increases GPU occupancy. The algorithm splits work across thread blocks (parallel over sequence length, batch, and heads) and within each thread block splits work across warps (parallel over Q blocks while keeping K and V accessible to all warps). This avoids the inefficient "split-K" scheme where all warps need to write intermediate results to shared memory, synchronize, and then add them up.

### Mechanism 2
Tweaking the online softmax algorithm to reduce non-matmul FLOPs increases overall throughput by allowing more time spent on faster matmul operations. The algorithm maintains an unscaled version of the output and only scales at the very end, and only stores the logsumexp instead of both the max and sum of exponentials. This reduces the number of non-matmul FLOPs performed.

### Mechanism 3
Using online softmax with tiling reduces memory reads/writes compared to standard attention implementations. Instead of materializing the large S and P matrices to HBM, the algorithm loads blocks of inputs from HBM to SRAM, computes attention with respect to that block, and then updates the output without writing the intermediate matrices to HBM. The online softmax trick allows rescaling the output of each block to finally get the right result.

## Foundational Learning

- **GPU memory hierarchy and execution model**: Understanding how to optimize for the GPU memory hierarchy and execution model is crucial for achieving high performance in FlashAttention-2. The algorithm relies on careful partitioning of work between thread blocks and warps, and minimizing shared memory communication. Quick check: What are the key components of the GPU memory hierarchy and how do they impact performance?

- **Attention mechanism and backpropagation**: Understanding the attention mechanism and its backpropagation is necessary for understanding how FlashAttention-2 modifies the algorithm to reduce non-matmul FLOPs while maintaining correctness. The algorithm relies on the online softmax trick and careful reordering of operations. Quick check: How does the attention mechanism work and what are the key equations for its backpropagation?

- **Parallel programming and synchronization**: Understanding parallel programming and synchronization is crucial for understanding how FlashAttention-2 parallelizes the attention computation across thread blocks and warps. The algorithm relies on careful partitioning of work and minimizing synchronization between warps. Quick check: What are the key challenges in parallel programming and how can they be addressed in the context of GPU computing?

## Architecture Onboarding

- **Component map**: Thread blocks -> Warps -> Shared memory -> HBM
- **Critical path**: Load blocks of Q, K, V from HBM to SRAM -> Compute attention with respect to each block using online softmax -> Update the output without writing intermediate matrices to HBM -> Write the final output to HBM
- **Design tradeoffs**: Increasing parallelism vs. overhead, block size vs. register usage, memory usage vs. recomputation
- **Failure signatures**: Low occupancy, register spilling, memory bandwidth bottleneck
- **First 3 experiments**: 
  1. Measure performance on a range of sequence lengths and head dimensions to identify optimal block size and parallelism configuration.
  2. Compare performance with and without the causal mask to quantify the impact of the mask on performance.
  3. Measure performance on different GPU architectures (e.g., A100 vs. H100) to identify any architecture-specific optimizations needed.

## Open Questions the Paper Calls Out

### Open Question 1
How much further can FlashAttention-2 be optimized for H100 GPUs using new hardware features like TMA and 4th-gen Tensor Cores? The authors mention that they expect another 1.5-2x speedup on H100 GPUs by using new instructions, but leave this to future work.

### Open Question 2
Can auto-tuning be used to automatically select optimal block sizes for FlashAttention-2 instead of manual tuning? The authors mention that manual tuning of block sizes is currently required but could benefit from auto-tuning to avoid manual labor.

### Open Question 3
How does FlashAttention-2 compare to other long-sequence attention methods like local, dilated, or block-sparse attention in terms of speed and memory usage? The authors suggest combining FlashAttention-2 with high-level algorithmic changes for even longer sequences, but don't provide direct comparisons.

## Limitations
- Performance claims are based on A100 GPUs specifically, with uncertain generalization to other architectures
- End-to-end training performance claims lack detailed training configuration specifications
- Work partitioning mechanism between warps lacks specific implementation details

## Confidence

- **High Confidence**: The 2× speedup over FlashAttention and general parallelization mechanism are well-supported by theoretical analysis and empirical measurements
- **Medium Confidence**: The claim of reaching 50-73% of theoretical maximum FLOPs/s is credible based on microbenchmarks, but end-to-end training performance claims are harder to verify
- **Medium Confidence**: The mechanism of reducing non-matmul FLOPs through tweaked online softmax is sound, but exact performance gains are difficult to isolate

## Next Checks

1. Reproduce microbenchmark results by implementing the described work partitioning scheme between thread blocks and warps on A100 GPUs, measuring FLOPs utilization across different sequence lengths and head dimensions to verify the claimed 50-73% utilization range.

2. Implement and test the backward pass with the described parallel work partitioning to verify that gradients are computed correctly and efficiently, particularly focusing on how K and V blocks are managed across warps.

3. Conduct ablation studies to isolate the contribution of each optimization (parallel thread blocks, parallel warps, reduced non-matmul FLOPs, online softmax) by implementing variants with individual components disabled, measuring their individual impact on performance.