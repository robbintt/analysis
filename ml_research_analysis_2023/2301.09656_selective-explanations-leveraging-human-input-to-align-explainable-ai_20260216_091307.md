---
ver: rpa2
title: 'Selective Explanations: Leveraging Human Input to Align Explainable AI'
arxiv_id: '2301.09656'
source_url: https://arxiv.org/abs/2301.09656
tags:
- explanations
- input
- selective
- human
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces selective explanations that present a subset
  of AI model reasons aligned with user preferences to address the gap between AI
  explanations and human communication. The framework leverages human input on a small
  sample to infer beliefs about recipient preferences, then generates explanations
  by prioritizing relevant features.
---

# Selective Explanations: Leveraging Human Input to Align Explainable AI

## Quick Facts
- arXiv ID: 2301.09656
- Source URL: https://arxiv.org/abs/2301.09656
- Reference count: 40
- Key outcome: Selective explanations improved decision accuracy (81.8% vs 74.6%), reduced over-reliance on incorrect AI (31.5% vs 44.4%), and enhanced perceived AI understanding

## Executive Summary
This paper introduces selective explanations that present a subset of AI model reasons aligned with user preferences to address the gap between AI explanations and human communication. The framework leverages human input on a small sample to infer beliefs about recipient preferences, then generates explanations by prioritizing relevant features. Through two experiments with N=118 and N=161 participants, selective explanations reduced over-reliance on AI when wrong (31.5% vs 44.4%), improved decision accuracy (81.8% vs 74.6%), and consistently enhanced perceived understanding of the model. The approach also improved subjective perceptions of AI usefulness and task efficiency, though requiring user input increased overall workload. Results demonstrate that selective explanations better align with human explanation consumption patterns and can lead to more appropriate reliance on AI systems.

## Method Summary
The framework uses a two-step approach: first collecting human input on feature relevance from a small sample of instances, then training a word-level logistic regression model to predict feature relevance for new instances. Explanations are generated by graying out irrelevant features identified through this preference-inference process. The system was tested using the IMDB movie review dataset with BERT fine-tuned for sentiment classification, LIME for feature importance scores, and both self-input and annotator-based input methods. Participants evaluated AI predictions with different explanation types, measuring decision accuracy, reliance on AI, and subjective perceptions of usefulness and understanding.

## Key Results
- Decision accuracy improved from 74.6% to 81.8% with selective explanations
- Over-reliance on incorrect AI predictions decreased from 44.4% to 31.5%
- Participants consistently reported better perceived understanding of the model with selective explanations
- User input requirement increased overall workload despite improving perceived usefulness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective explanations improve decision accuracy by aligning AI explanations with human understanding patterns.
- Mechanism: The framework filters and prioritizes AI-generated explanations based on inferred human preferences, reducing cognitive load and improving comprehension.
- Core assumption: Humans have identifiable preferences for which features they consider relevant to sentiment judgment tasks.
- Evidence anchors:
  - [abstract] "selective explanations better align with human explanation consumption patterns and can lead to more appropriate reliance on AI systems"
  - [section] "selective explanations produced explanations that were better aligned with the groundtruth, improved decision outcomes, and decreased over-reliance when the AI predictions were wrong"
- Break condition: If human preferences cannot be reliably inferred from small sample input, or if preferences vary too widely between individuals for generalization to work

### Mechanism 2
- Claim: Providing user input to AI explanations improves perceived usefulness and understanding of AI.
- Mechanism: Allowing users to provide input creates a sense of agency and control over AI outputs, increasing engagement and positive perceptions.
- Core assumption: Users value having control over AI explanations and feel more positively about systems they can influence.
- Evidence anchors:
  - [abstract] "the opportunity to provide one's own input to augment AI explanations...improved the perceived usefulness of AI"
  - [section] "participants who were in the two conditions with selective explanations reported higher perceived usefulness of the AI tool"
- Break condition: If providing input creates too much cognitive burden or if users feel their input doesn't meaningfully impact the explanations

### Mechanism 3
- Claim: Annotator-based selective explanations reduce over-reliance on incorrect AI predictions.
- Mechanism: Input from similar users helps identify which features the model "wrongly picked" in incorrect predictions, allowing those to be de-emphasized in explanations.
- Core assumption: There are shared preferences across similar users about which features are relevant for sentiment judgment tasks.
- Evidence anchors:
  - [abstract] "Our experiments demonstrate the promise of selective explanations in reducing over-reliance on AI"
  - [section] "selective explanations based on annotator input substantially increases the percentage of highlighted words that support the correct label in incorrectly predicted instances"
- Break condition: If annotator preferences don't generalize to individual users, or if the input sample is too small to capture relevant feature preferences

## Foundational Learning

- Concept: Feature importance in text classification models
  - Why needed here: The framework relies on understanding which words contribute to model predictions and how to modify their presentation
  - Quick check question: How does LIME calculate feature importance scores for text classification?

- Concept: Human explanation selection principles
  - Why needed here: The framework is inspired by how humans selectively present explanations based on relevance, abnormality, and changeability
  - Quick check question: What are the three main selectivity goals identified in human explanation research?

- Concept: Cognitive load theory
  - Why needed here: Understanding why selective explanations might reduce cognitive burden compared to full explanations
  - Quick check question: How does reducing irrelevant information in explanations affect cognitive processing requirements?

## Architecture Onboarding

- Component map: Input collection → Preference inference model → Selective explanation generation → Visual presentation
- Critical path: Input collection → Preference inference → Explanation generation → User decision
- Design tradeoffs: Personalization (better fit but more work) vs. standardization (less work but potentially less effective)
- Failure signatures: Decreased accuracy despite selective explanations, increased cognitive load, user disengagement
- First 3 experiments:
  1. Compare accuracy between original explanations and selective explanations with self-input
  2. Test different input elicitation methods (open-ended vs. critique-based) for effectiveness
  3. Evaluate annotator-based selective explanations vs. self-input paradigms for efficiency and effectiveness

## Open Questions the Paper Calls Out

1. How can selective explanations be optimized for different selectivity goals (relevance, abnormality, changeability) across various XAI use cases?
2. What are the most effective methods for eliciting human input to generate selective explanations, and how do they vary based on the type of data (text, image, tabular)?
3. How can the computational algorithms for incorporating human input into predicting which features align with recipient preferences be improved for unseen instances?

## Limitations

- The framework's reliance on human input introduces potential biases and scalability challenges
- The IMDB dataset represents only one domain of text classification, limiting generalizability
- The two-step approach may not generalize well to more complex models or multi-modal data
- The critique-based input method may not capture all relevant feature preferences

## Confidence

- High confidence: The framework's ability to reduce over-reliance on incorrect AI predictions (supported by experimental results showing 31.5% vs 44.4% over-reliance rates)
- Medium confidence: The improvement in decision accuracy through preference alignment (81.8% vs 74.6% accuracy, though domain-specific)
- Medium confidence: The enhancement of perceived AI usefulness and understanding through user input (subjective measures, though statistically significant)

## Next Checks

1. Test the framework across multiple domains (e.g., medical diagnosis, financial forecasting) to assess generalizability beyond sentiment classification
2. Evaluate the framework with more complex AI models (e.g., multi-modal systems, deep neural networks) to test scalability
3. Conduct longitudinal studies to measure whether selective explanations improve long-term AI adoption and appropriate reliance patterns