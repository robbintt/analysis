---
ver: rpa2
title: On the Importance of Backbone to the Adversarial Robustness of Object Detectors
arxiv_id: '2305.17438'
source_url: https://arxiv.org/abs/2305.17438
tags:
- adversarial
- object
- detectors
- robustness
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper reveals that previous works overestimated adversarial
  robustness of object detectors, leading to a false sense of security. It demonstrates
  that adversarially pre-trained backbone networks are essential for enhancing adversarial
  robustness in object detectors.
---

# On the Importance of Backbone to the Adversarial Robustness of Object Detectors

## Quick Facts
- arXiv ID: 2305.17438
- Source URL: https://arxiv.org/abs/2305.17438
- Reference count: 40
- One-line primary result: Adversarially pre-trained backbones are essential for enhancing adversarial robustness in object detectors, achieving state-of-the-art results without structural modifications

## Executive Summary
This paper challenges the conventional understanding of adversarial robustness in object detectors by revealing that previous works significantly overestimated detector robustness. The authors demonstrate that the key to achieving strong adversarial robustness lies in using adversarially pre-trained backbone networks, rather than focusing on detection-specific modules. By proposing a simple yet effective recipe for fast adversarial fine-tuning on object detectors, they achieve significantly better adversarial robustness than previous methods while maintaining computational efficiency.

## Method Summary
The paper proposes a two-stage training approach: first, using adversarially pre-trained backbone networks (ResNet-50 or ConvNeXt-T) from upstream classification tasks, then performing adversarial fine-tuning on object detectors with reduced learning rates for the backbone. The method employs FreeAT for efficient adversarial training, which recycles gradient perturbations across iterations. The detection-specific modules use AdamW optimizer with normal learning rates, while the backbone learning rate is decayed by a factor of 0.1 during fine-tuning.

## Key Results
- Adversarially pre-trained backbones are essential for enhancing adversarial robustness in object detectors
- The proposed simple recipe achieves state-of-the-art adversarial robustness without structural modifications
- Backbone networks play a more important role than detection-specific modules in adversarial robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarially pre-trained backbones transfer robustness to downstream object detectors
- Mechanism: The backbone learns robust feature representations during upstream adversarial training that remain useful when fine-tuned for detection tasks
- Core assumption: Features learned to resist adversarial perturbations in classification generalize to detection tasks
- Evidence anchors:
  - [abstract] "we found that adversarially pre-trained backbone networks were essential for enhancing the adversarial robustness of object detectors"
  - [section] "upstream adversarial pre-training is vital to the adversarial robustness of object detectors"
  - [corpus] No direct evidence found in neighbors; corpus relevance is weak
- Break condition: If adversarial perturbations in detection differ fundamentally from classification, or if fine-tuning destroys robust features

### Mechanism 2
- Claim: Reducing backbone learning rate preserves adversarial robustness during detection fine-tuning
- Mechanism: Slower learning on backbone prevents overwriting of adversarially robust features while detection-specific modules adapt normally
- Core assumption: Detection-specific modules can learn without degrading backbone robustness if learning rates are properly balanced
- Evidence anchors:
  - [section] "decay the learning rate of the backbone by a factor of 0.1 when performing AT on object detectors"
  - [section] "We use the AdamW optimizer" for detection-specific modules
  - [corpus] No direct evidence found in neighbors; corpus relevance is weak
- Break condition: If backbone still degrades significantly with reduced learning rate, or if detection performance suffers too much

### Mechanism 3
- Claim: Detection-specific modules matter less than backbone networks for adversarial robustness
- Mechanism: When backbone is adversarially pre-trained, different detection architectures achieve similar robustness levels
- Core assumption: Robust feature extraction is the primary determinant of detector robustness
- Evidence anchors:
  - [section] "backbone networks play a more important role than detection-specific modules"
  - [section] "Despite the heterogeneous detection-specific modules, the detectors with upstream adversarially pre-trained backbones achieved similar detection accuracy"
  - [corpus] No direct evidence found in neighbors; corpus relevance is weak
- Break condition: If certain detection architectures prove inherently more robust regardless of backbone

## Foundational Learning

- Concept: Adversarial training fundamentals
  - Why needed here: Understanding how adversarial examples are generated and used in training
  - Quick check question: What is the difference between standard training and adversarial training in terms of loss function?

- Concept: Transfer learning and pre-training paradigms
  - Why needed here: The paper relies on transferring robustness from classification to detection
  - Quick check question: Why might pre-training on classification tasks benefit downstream detection tasks?

- Concept: Object detection architecture components
  - Why needed here: Understanding backbone vs. detection-specific modules is crucial
  - Quick check question: What are the typical components of a modern object detector beyond the backbone?

## Architecture Onboarding

- Component map: Backbone (feature extractor) → Detection-specific modules (neck + heads) → Output
- Critical path: Upstream adversarial pre-training → Detection fine-tuning with reduced backbone LR → Evaluation
- Design tradeoffs: Speed vs. robustness (full PGD-AT vs. FreeAT), backbone capacity vs. computational cost
- Failure signatures: Backbone losing robustness during fine-tuning, detection-specific modules overfitting to benign examples
- First 3 experiments:
  1. Replicate backbone-only adversarial training on ImageNet and evaluate on detection dataset
  2. Compare full learning rate vs. reduced learning rate fine-tuning on backbone
  3. Test transferability of adversarial examples across different detection architectures with same backbone

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the content, several important questions emerge:

### Open Question 1
- Question: Does adversarially pre-trained backbone transferability extend to other dense prediction tasks beyond object detection?
- Basis in paper: [explicit] The authors suggest their conclusions about backbone importance might apply to other dense prediction tasks like segmentation and instance segmentation.
- Why unresolved: The paper focuses exclusively on object detection; no experiments were conducted on other dense prediction tasks.
- What evidence would resolve it: Experiments applying the same adversarially pre-trained backbone approach to tasks like semantic segmentation, instance segmentation, or depth estimation, comparing performance gains to object detection results.

### Open Question 2
- Question: How does the effectiveness of upstream adversarial pre-training compare to alternative methods of improving adversarial robustness, such as data augmentation with synthetic adversarial examples?
- Basis in paper: [inferred] The paper emphasizes upstream adversarial pre-training but doesn't compare it to other data augmentation strategies.
- Why unresolved: The authors focus on their specific training recipe without benchmarking against other data augmentation approaches.
- What evidence would resolve it: Controlled experiments comparing upstream adversarial pre-training against techniques like adversarial data augmentation, Mixup, CutMix, or other data synthesis methods for improving adversarial robustness.

### Open Question 3
- Question: What is the relationship between the scale of upstream adversarial pre-training data and the resulting adversarial robustness in downstream object detection?
- Basis in paper: [explicit] The authors note that improving adversarial robustness requires much more data than improving benign accuracy, but don't explore data scale effects.
- Why unresolved: The paper uses a fixed upstream dataset (ImageNet) without varying dataset size.
- What evidence would resolve it: Experiments training adversarially pre-trained backbones on different-sized subsets of upstream data and measuring resulting robustness gains in downstream detection tasks.

## Limitations
- Limited to specific backbone architectures (ResNet-50 and ConvNeXt-T) and detection frameworks
- Robustness evaluated only against white-box PGD attacks, not other attack types
- Computational efficiency gains need further validation across different hardware configurations

## Confidence

- **High Confidence:** The core finding that adversarially pre-trained backbones significantly enhance detector robustness is well-supported by comparative experiments and ablation studies. The effectiveness of reducing backbone learning rates during fine-tuning is also strongly evidenced.
- **Medium Confidence:** The claim that detection-specific modules matter less than backbones for adversarial robustness is supported but could benefit from testing more diverse detector architectures.
- **Low Confidence:** The paper's assertion about the general applicability of their "simple recipe" to other backbone networks and detection tasks requires further validation beyond the tested configurations.

## Next Checks

1. Test the transferability of adversarial robustness across different backbone architectures (e.g., Swin Transformer, EfficientNet) to validate the generalizability of the findings.
2. Evaluate the proposed method's performance against black-box and physical-world adversarial attacks to assess real-world applicability.
3. Conduct a scalability study to measure the computational efficiency and robustness gains when applying the method to larger datasets and more complex detection tasks.