---
ver: rpa2
title: 'LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically
  Encrypted Inference'
arxiv_id: '2309.14331'
source_url: https://arxiv.org/abs/2309.14331
tags:
- non-linear
- lingcn
- accuracy
- polynomial
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LinGCN addresses privacy-preserving inference for graph convolutional
  networks (GCNs) using homomorphic encryption (HE), which suffers from high computational
  overhead due to multiplication depth in HE operations. The proposed framework introduces
  a differentiable structural linearization algorithm with node-level non-linear location
  selection, a compact node-wise polynomial replacement policy with second-order trainable
  activation functions, and an enhanced HE solution enabling finer-grained operator
  fusion.
---

# LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference

## Quick Facts
- **arXiv ID:** 2309.14331
- **Source URL:** https://arxiv.org/abs/2309.14331
- **Reference count:** 40
- **Key outcome:** LinGCN achieves 14.2× latency speedup compared to CryptoGCN while maintaining ~75% accuracy on NTU-XVIEW skeleton joint dataset

## Executive Summary
LinGCN addresses the computational bottleneck of homomorphic encryption (HE) inference for graph convolutional networks (GCNs) by introducing a structural linearization framework that reduces multiplicative depth. The framework combines node-level non-linear location selection with polynomial activation functions and enhanced HE operator fusion. Experiments demonstrate significant latency improvements while maintaining competitive accuracy, with the method showing particular effectiveness for skeleton-based action recognition tasks using STGCN architectures.

## Method Summary
LinGCN employs a three-part optimization strategy: differentiable structural linearization that selectively prunes non-linear operations at the node level to minimize multiplication depth, node-wise polynomial replacement with second-order trainable activation functions that approximate ReLU behavior while being HE-friendly, and enhanced HE operator fusion that combines polynomial coefficients with convolution weights to further reduce computational overhead. The framework is trained using a two-level distillation approach from an all-ReLU teacher model, with the final inference implemented using CKKS homomorphic encryption with AMA packing format.

## Key Results
- 14.2× latency speedup compared to CryptoGCN baseline while maintaining ~75% accuracy on NTU-XVIEW dataset
- For STGCN-6-256 model, achieves 85.78% accuracy with 6371s latency, improving accuracy by 10.47% over CryptoGCN
- Significant reduction in multiplication depth through node-wise structural linearization, with successful synchronization across all nodes in a layer
- Effective operator fusion eliminates separate multiplication operations for polynomial coefficients, reducing overall computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structural linearization with node-wise non-linear location selection reduces multiplicative depth in CKKS-based HE inference.
- Mechanism: The differentiable structural linearization algorithm uses a parameterized discrete indicator function to selectively prune non-linear operators at the node level. This creates a fine-grained structural reduction that synchronizes multiplication depths across all nodes in a layer.
- Core assumption: Node-wise pruning is more effective than layer-wise pruning for maintaining accuracy while reducing depth.
- Evidence anchors:
  - [abstract]: "This strategy promotes fine-grained node-level non-linear location selection, resulting in a model with minimized multiplication depth."
  - [section 3.2]: "Our scheme provides nodes with the flexibility to perform non-linear operations at preferred positions. This fine-grained, structural non-linear reduction may improve the non-linear reduced structure..."
  - [corpus]: Weak evidence - no corpus papers directly discuss node-wise linearization for HE.

### Mechanism 2
- Claim: Node-wise polynomial replacement with second-order trainable activation functions maintains accuracy while reducing non-linear operations.
- Mechanism: Replaces ReLU with trainable second-order polynomials (y = c·w₂x² + w₁x + b) at each node, trained via distillation from an all-ReLU teacher model to preserve performance.
- Core assumption: Polynomial functions can approximate ReLU behavior well enough to maintain accuracy while being more HE-friendly.
- Evidence anchors:
  - [abstract]: "A compact node-wise polynomial replacement policy with a second-order trainable activation function..."
  - [section 3.3]: "We suggest using a node-wise trainable polynomial function as the non-linear function... include a small constant c to adjust the gradient scale of the w2 parameter."
  - [corpus]: Weak evidence - corpus papers don't discuss polynomial replacement for HE specifically.

### Mechanism 3
- Claim: Operator fusion in HE encoding reduces multiplication depth by combining polynomial coefficients with convolution weights.
- Mechanism: Plaintext weights from polynomial functions are fused into preceding convolution layers, eliminating separate multiplication operations in the HE computation graph.
- Core assumption: Fusion doesn't significantly impact numerical stability or precision in the HE domain.
- Evidence anchors:
  - [section 4.1]: "we can fuse the plaintext weights c · w2 of node-wise polynomial function into GCNConv layer and save one multiplication depth budget."
  - [appendix A.4]: "we can fuse the plaintext weights c · w2 from the second polynomial function into the convolution operator and save the multiplication depth budget."
  - [corpus]: Weak evidence - no corpus papers discuss this specific fusion technique for HE.

## Foundational Learning

- Concept: Homomorphic Encryption (HE) and CKKS scheme
  - Why needed here: LinGCN is specifically designed to optimize HE-based GCN inference by reducing multiplication depth, which is the primary bottleneck in CKKS.
  - Quick check question: What happens to the ciphertext level after each multiplication in CKKS, and why does this create latency issues?

- Concept: Graph Convolutional Networks (GCNs) and STGCN architecture
  - Why needed here: LinGCN targets STGCN models, which combine spatial graph convolutions with temporal convolutions for skeleton-based action recognition.
  - Quick check question: How does the GCNConv operation differ from standard 2D convolution, and why does this matter for HE optimization?

- Concept: Model distillation and knowledge transfer
  - Why needed here: The polynomial replacement is trained via two-level distillation from an all-ReLU teacher to maintain accuracy.
  - Quick check question: What is the purpose of using KL-divergence loss in addition to cross-entropy when training the polynomial student model?

## Architecture Onboarding

- Component map:
  - Teacher model (all-ReLU baseline) -> Student model with structural linearization (indicator parameter h) -> Polynomial replacement layer (second-order trainable function) -> HE inference engine with AMA packing and operator fusion

- Critical path:
  1. Train teacher ReLU model
  2. Initialize student model and auxiliary parameter hw
  3. Apply structural linearization (Algorithm 1)
  4. Replace ReLU with polynomial function
  5. Train with distillation loss
  6. Export for HE inference

- Design tradeoffs:
  - Node-wise vs. layer-wise pruning: Node-wise offers finer granularity but requires more complex optimization
  - Polynomial order: Second-order balances approximation quality with HE efficiency
  - Distillation strength: Higher η improves accuracy but may reduce the benefits of linearization

- Failure signatures:
  - Accuracy collapse after linearization: Indicates structural pruning removed too many critical non-linearities
  - Training instability: Suggests polynomial replacement or distillation parameters need adjustment
  - HE inference errors: Points to issues with operator fusion or parameter scaling

- First 3 experiments:
  1. Verify structural linearization preserves teacher accuracy on a small STGCN-3-128 model with full non-linear layers
  2. Test polynomial replacement with fixed h on the same model to validate the activation function
  3. Run HE inference with AMA packing on the reduced model to measure latency improvement over CryptoGCN baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of polynomial degree N affect the security level and latency trade-off in LinGCN, and what is the optimal N for different model depths?
- Basis in paper: [explicit] The paper discusses the impact of polynomial degree N on latency and mentions that a larger N leads to higher latency for homomorphic encryption operations.
- Why unresolved: The paper does not provide a detailed analysis of how N affects the security level or a method to determine the optimal N for different model depths.
- What evidence would resolve it: A comprehensive study comparing different N values across various model depths, measuring both security level and latency, would provide insights into the optimal N for each scenario.

### Open Question 2
- Question: Can the structural linearization algorithm be extended to other types of neural networks beyond GCNs, and what modifications would be necessary?
- Basis in paper: [inferred] The paper focuses on LinGCN for GCNs, but the structural linearization algorithm could potentially be adapted for other neural network architectures.
- Why unresolved: The paper does not explore the application of the structural linearization algorithm to other neural network types or discuss necessary modifications.
- What evidence would resolve it: Experiments applying the structural linearization algorithm to different neural network architectures, such as CNNs or Transformers, and analyzing the results would provide insights into its broader applicability.

### Open Question 3
- Question: How does the performance of LinGCN compare to other privacy-preserving machine learning techniques, such as differential privacy or secure multi-party computation, in terms of accuracy and latency?
- Basis in paper: [inferred] The paper focuses on LinGCN's performance compared to CryptoGCN but does not discuss its performance relative to other privacy-preserving techniques.
- Why unresolved: The paper does not provide a comparison of LinGCN with other privacy-preserving machine learning methods.
- What evidence would resolve it: A comparative study evaluating LinGCN against other privacy-preserving techniques, such as differential privacy or secure multi-party computation, in terms of accuracy and latency would provide a comprehensive understanding of its performance.

## Limitations
- The node-wise structural linearization requires careful hyperparameter tuning (η, φ, c) and may not generalize well across different graph topologies or dataset characteristics
- Operator fusion implementation details are underspecified, particularly the interaction between plaintext polynomial weights and ciphertext convolution operations
- The 14.2× speedup comparison to CryptoGCN is based on a specific NTU-XVIEW configuration and may not scale proportionally to larger or different datasets

## Confidence

- **High Confidence:** The core HE optimization mechanism (reducing multiplication depth through structural linearization) is well-supported by the mathematical framework and experimental results
- **Medium Confidence:** The polynomial replacement approach maintains accuracy within acceptable bounds, though the approximation quality may vary with different activation patterns
- **Low Confidence:** The exact implementation details for operator fusion and the AMA packing scheme are not fully specified, creating potential reproducibility challenges

## Next Checks

1. **Structural Linearization Validation:** Verify that the indicator parameter h converges to a sparse configuration that maintains teacher model accuracy while reducing multiplication depth by at least 50% on a baseline STGCN-3-128 model
2. **Polynomial Approximation Quality:** Measure the KL-divergence between ReLU and polynomial activation outputs across the full input range to ensure the second-order approximation remains within 5% error tolerance
3. **HE Operator Fusion Integrity:** Implement and validate the coefficient modulus packing scheme with c=0.01 scaling, ensuring that fused weights do not exceed the maximum plaintext modulus (Qmax) while maintaining the reduced multiplication depth budget