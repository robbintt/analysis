---
ver: rpa2
title: An engine to simulate insurance fraud network data
arxiv_id: '2308.11659'
source_url: https://arxiv.org/abs/2308.11659
tags:
- claims
- fraud
- data
- claim
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a simulation engine to generate synthetic
  insurance fraud network data, addressing the scarcity of publicly available data
  for fraud detection research. The engine mimics real-life insurance fraud data sets
  and allows users to control various data-generating mechanisms, including policyholder
  characteristics, contract-specific features, claim frequency and severity, and social
  network structure.
---

# An engine to simulate insurance fraud network data

## Quick Facts
- arXiv ID: 2308.11659
- Source URL: https://arxiv.org/abs/2308.11659
- Reference count: 24
- One-line primary result: Introduces a simulation engine to generate synthetic insurance fraud network data for testing fraud detection methods.

## Executive Summary
This paper presents a simulation engine for generating synthetic insurance fraud network data, addressing the scarcity of publicly available data for fraud detection research. The engine allows users to control various data-generating mechanisms, including policyholder characteristics, contract features, claim frequency and severity, and social network structure. By iteratively generating claim labels and incorporating social network features, the engine accurately replicates user-specified effect sizes and class imbalances. The synthetic data enables researchers and practitioners to examine methodological challenges and test fraud detection models in diverse settings, highlighting the added value of social network analytics in identifying fraudulent claims.

## Method Summary
The simulation engine consists of seven steps: generating policyholder and contract features, simulating claim frequency and severity using GLMs, building a bipartite social network linking claims to parties, generating fraud labels through an iterative process incorporating social network features, and simulating expert judgment with business rules and probabilistic outcomes. The engine uses copulas to introduce realistic dependencies between features and optimizes the intercept term to achieve the target class imbalance. Two synthetic datasets are generated: one with and one without social network effects, demonstrating the engine's ability to replicate specified effect sizes and induce homophily patterns when social features are included.

## Key Results
- The simulation engine accurately replicates user-specified class imbalances and effect sizes for policyholder and contract features.
- Inclusion of social network features induces homophily patterns, with fraudulent claims more densely connected in the network.
- Fraud detection models developed on synthetic data with social network features achieve higher AUC and Top Decile Lift compared to models without network features.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The simulation engine accurately replicates user-specified effect sizes for features used in fraud generation, including social network features.
- Mechanism: The engine uses a logistic regression model where coefficients (βf) control feature influence. It iteratively generates claim labels, incorporating social network features, and adjusts βf to achieve the target class imbalance.
- Core assumption: The iterative generation process converges to stable feature distributions and effect sizes that reflect the specified model.
- Evidence anchors:
  - [abstract]: "The simulation engine enables researchers and practitioners to examine several methodological challenges as well as to test their (development strategy of) insurance fraud detection models in a range of different settings."
  - [section 3.3]: "The user can specify which features are included in f xijk and determine their effect size in βf."
  - [section 4.1]: "Our results highlight the toolbox’s capability to simulate synthetic data according to the user-defined parameters."
- Break condition: If the iterative label generation diverges or fails to converge, the specified effect sizes may not be accurately reproduced.

### Mechanism 2
- Claim: The inclusion of social network features in the fraud generation model creates homophily patterns in the synthetic data.
- Mechanism: Social network features (like n2.ratioFraud) are engineered from the bipartite claim-party network structure. When included in the fraud model with non-zero βf, these features induce clustering of fraudulent claims among parties and claims that are densely connected.
- Core assumption: Homophily emerges because the fraud model assigns higher fraud probability to claims with many fraudulent neighbors.
- Evidence anchors:
  - [abstract]: "The simulation engine enables researchers and practitioners to examine several methodological challenges...as well as to test their (development strategy of) insurance fraud detection models in a range of different settings."
  - [section 2.2]: "One particularly promising approach to capture the characteristics of fraudsters is through social network analytics...this approach potentially uncovers organized schemes of collaborating fraudsters."
  - [section 4.1]: "In DN etwork, the fraudulent claims are more densely connected to each other compared to what we expect by chance (D > 1)."
- Break condition: If social network features are omitted or their βf values are set to zero, homophily patterns will not emerge.

### Mechanism 3
- Claim: The simulation engine replicates the real-world fraud investigation process, including expert judgment and missing labels.
- Mechanism: After generating ground-truth labels (Yijk), the engine applies business rules to flag suspicious claims, then simulates expert judgment with probabilistic outcomes (e.g., 99% accuracy for non-fraudulent, 99% for fraudulent). Unflagged claims are labeled as uninvestigated.
- Core assumption: The simulated expert judgment mirrors real-world detection accuracy and bias, preserving the class imbalance and missing data patterns.
- Evidence anchors:
  - [section 3.4]: "For a claim that is flagged by the business rules in step one, we first look at its ground truth label Yijk. If Yijk = non-fraudulent, we take a random draw from Y expert ijk ∼ Bern(0.01)."
  - [section 2.1]: "The expert-based approach is not entirely infallible...Claims that are judged to be non-fraudulent by the investigation may in reality be fraudulent and vice versa."
  - [section 4.1]: "Only a small fraction of claims are subjected to investigation (approximately 9%), and among those investigated, only a minority are found to be fraudulent."
- Break condition: If the business rules or expert judgment probabilities are mis-specified, the distribution of investigated vs. uninvestigated claims will deviate from realistic patterns.

## Foundational Learning

- Concept: Generalized Linear Models (GLMs) for claim frequency and severity.
  - Why needed here: The engine uses Poisson and Gamma GLMs to simulate claim counts and costs as functions of policyholder and contract characteristics.
  - Quick check question: What is the role of the exposure term (wij) in the Poisson GLM for claim frequency?
- Concept: Social network analysis and bipartite graphs.
  - Why needed here: The engine constructs a bipartite network linking claims to parties (policyholders, garages, etc.) and engineers features like neighborhood size and fraud ratios.
  - Quick check question: How does the BiRank algorithm rank claims for fraud exposure in a bipartite network?
- Concept: Copulas for modeling dependence between variables.
  - Why needed here: The engine uses copulas (e.g., Frank, Ali-Mikhail-Haq) to introduce realistic dependencies between features like car age and value.
  - Quick check question: What is the effect of a negative Frank copula parameter on the joint distribution of two variables?

## Architecture Onboarding

- Component map: Data generators (policyholder, contract, claim, social network, fraud label, expert judgment) -> Parameter store -> Dependency engine (copulas) -> Label generator (iterative) -> Output assembler
- Critical path:
  1. Generate policyholder and contract features.
  2. Simulate claim frequency and severity.
  3. Build social network structure.
  4. Generate fraud labels (iterative with social features).
  5. Apply expert judgment simulation.
  6. Assemble final dataset.
- Design tradeoffs:
  - Flexibility vs. complexity: Many user parameters enable diverse scenarios but increase configuration burden.
  - Realism vs. tractability: Iterative label generation with social features adds realism but requires careful tuning to avoid divergence.
  - Speed vs. fidelity: Generating large networks and iterating labels can be computationally expensive; caching intermediate results can help.
- Failure signatures:
  - Divergent or unstable feature distributions after label generation.
  - Class imbalance far from target after convergence.
  - Social network features not inducing expected homophily.
- First 3 experiments:
  1. Generate a small dataset (nph=1000) with only policyholder features, no social network, and verify class imbalance matches target.
  2. Generate a dataset with social network features included, set moderate βf for n2.ratioFraud, and check for emergence of homophily (D > 1, H < 1).
  3. Generate two datasets (with and without social network features) and fit logistic regression models; compare AUC and TDL to confirm added value of network features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the simulation engine handle the dynamic nature of fraud when generating synthetic data?
- Basis in paper: [explicit] The paper discusses the dynamic nature of fraud as a challenge in fraud detection research, stating that fraudsters adapt their tactics in response to detection systems, causing the typical fraudster profile to evolve over time.
- Why unresolved: The paper does not provide details on how the simulation engine specifically accounts for this dynamic nature when generating synthetic data.
- What evidence would resolve it: Information on whether the simulation engine incorporates mechanisms to simulate evolving fraud patterns or if it assumes static fraud characteristics throughout the data generation process.

### Open Question 2
- Question: What is the impact of the iterative claim label generation process on the accuracy of the specified effect sizes, particularly for social network features?
- Basis in paper: [explicit] The paper mentions that the iterative process of generating claim labels may lead to deviations in the estimated effect size of the n2.ratioFraud feature from its originally specified value.
- Why unresolved: The paper does not provide a detailed analysis of how this iterative process affects the accuracy of effect sizes for other features, especially social network features.
- What evidence would resolve it: A study comparing the specified effect sizes with the estimated effect sizes across multiple iterations and synthetic data sets, particularly focusing on social network features.

### Open Question 3
- Question: How does the simulation engine's performance compare to other synthetic data generation methods for insurance fraud detection?
- Basis in paper: [inferred] The paper introduces a novel simulation engine for generating synthetic insurance fraud network data but does not compare its performance to existing methods.
- Why unresolved: The paper focuses on describing the simulation engine's capabilities and does not provide a comparative analysis with other synthetic data generation approaches.
- What evidence would resolve it: A benchmark study comparing the simulation engine's performance in terms of data realism, computational efficiency, and ability to capture complex fraud patterns against other synthetic data generation methods.

## Limitations

- The convergence properties of the iterative label generation process, particularly when incorporating social network features, are not fully characterized.
- The realism of the simulated expert judgment process, including the 99% accuracy rates and business rule flagging, is based on assumptions that may not generalize to all real-world fraud investigation settings.
- The scalability of the engine to very large datasets (e.g., millions of claims) is not evaluated, raising questions about computational feasibility in production environments.

## Confidence

- High confidence: The engine accurately reproduces specified class imbalances and effect sizes for policyholder and contract features in the absence of social network effects.
- Medium confidence: The inclusion of social network features induces homophily patterns as expected, but the magnitude and stability of these effects across different network structures is uncertain.
- Low confidence: The engine's ability to scale to large, complex fraud scenarios involving multiple parties and claim types is not validated.

## Next Checks

1. Conduct convergence analysis by running the label generation algorithm with varying numbers of iterations and social network densities; assess the stability of feature distributions and effect sizes.
2. Evaluate the impact of different business rule thresholds and expert judgment accuracies on the distribution of investigated vs. uninvestigated claims; compare results to real-world fraud investigation data if available.
3. Stress-test the engine by generating datasets with increasing numbers of policyholders and claims; measure runtime and memory usage to identify scalability bottlenecks.