---
ver: rpa2
title: Be Careful When Evaluating Explanations Regarding Ground Truth
arxiv_id: '2311.04813'
source_url: https://arxiv.org/abs/2311.04813
tags:
- explanation
- explanations
- truth
- methods
- pipelines
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper highlights a critical issue in evaluating explanation
  methods for deep learning models by demonstrating that such evaluations often primarily
  assess the quality of the model-explanation pipeline rather than the explanation
  method itself. The authors propose a framework for jointly evaluating the robustness
  of AI systems that combine deep learning models with explanation methods, focusing
  on alignment with human perception through segmentation masks.
---

# Be Careful When Evaluating Explanations Regarding Ground Truth

## Quick Facts
- arXiv ID: 2311.04813
- Source URL: https://arxiv.org/abs/2311.04813
- Reference count: 17
- One-line primary result: Evaluating explanations against human perception masks primarily tests model quality rather than explanation method quality

## Executive Summary
This paper reveals a critical flaw in how explanation methods for deep learning models are evaluated: when explanations are assessed against human-defined segmentation masks, the evaluation actually measures the quality of the entire model-explanation pipeline rather than the explanation method itself. The authors propose a novel framework that jointly evaluates the robustness of AI systems combining deep learning models with explanation methods. Through controlled fine-tuning experiments, they demonstrate that model-explanation pipelines can be systematically aligned or misaligned with ground truth while maintaining predictive performance, revealing significant vulnerabilities in current evaluation practices.

## Method Summary
The framework evaluates robustness of model-explanation pipelines using medical imaging datasets (CheXpert for training, CheXlocalize for evaluation). Three model architectures (DenseNet-201, ViT-base, Swin-ViT-base) are trained with various initialization strategies and paired with four explanation methods (Vanilla Gradient, Integrated Gradients, SmoothGrad, Layer-wise Relevance Propagation). A fine-tuning procedure with differentiable alignment loss is used to create aligned and misaligned versions of the pipelines. Robustness is quantified as the difference in alignment accuracy between these versions, measured through mass and rank accuracy metrics. The framework also examines the impact of pre-training on robustness across different class labels and model architectures.

## Key Results
- Pre-training models on RadImageNet improves robustness of model-explanation pipelines compared to ImageNet-21k pre-training
- Vision transformers show greater robustness to adversarial alignment attacks than DenseNet models
- Different class labels exhibit varying levels of robustness in model-explanation pipelines
- The proposed fine-tuning procedure successfully creates pipelines with controlled alignment to ground truth

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Evaluating explanations of image classifiers regarding ground truth primarily evaluates model quality rather than explanation method quality
- **Mechanism**: Explanations are inherently tied to a model's feature attribution and decision-making process. When explanations are generated for a model, their alignment with human-defined masks depends on the model's internal feature representations and learned correlations, not solely on the explanation algorithm's properties
- **Core assumption**: Explanation method output is a function of both the algorithm and model behavior; therefore, evaluations involving human perception masks must consider the combined pipeline
- **Evidence anchors**: Abstract states that such evaluations primarily evaluate model quality rather than explanation methods; paper emphasizes that localization performance of deep learning models dominates evaluation outcomes
- **Break condition**: If explanation methods were entirely model-agnostic and could be evaluated in isolation, pipeline effects would not dominate evaluation outcomes

### Mechanism 2
- **Claim**: Fine-tuning models to align or misalign explanations with ground truth creates a spectrum of pipeline robustness
- **Mechanism**: Introducing regularization that encourages explanations to match or deviate from ground truth masks allows controlled manipulation of model feature attribution while maintaining classification performance
- **Core assumption**: Fine-tuning can manipulate model feature attribution without changing model performance, enabling controlled robustness testing
- **Evidence anchors**: Paper introduces fine-tuning procedure to (mis)align model-explanation pipelines with ground truth and quantifies discrepancy between alignment scenarios
- **Break condition**: If fine-tuning significantly alters model performance or cannot be done without changing architecture, robustness measures become unreliable

### Mechanism 3
- **Claim**: Pre-training on datasets similar to target domain improves model-explanation pipeline robustness
- **Mechanism**: Pre-training on related data provides better initial feature representations that are more likely to align with human perception of relevant features, resulting in more consistent explanations with ground truth masks
- **Core assumption**: Pre-training on similar data provides better inductive bias for feature learning that translates to more robust explanations
- **Evidence anchors**: Coefficient related to pre-training is negative, showing RadImageNet pre-training improves robustness; sign consistency between rank and mass accuracy observed
- **Break condition**: If pre-training on similar dataset does not improve feature alignment or target domain is too different, robustness improvement may not be observed

## Foundational Learning

- **Concept**: Differentiable alignment loss
  - Why needed here: Framework relies on differentiable loss function to fine-tune models so explanations align or misalign with human perception masks; understanding this loss is essential for implementing robustness evaluation
  - Quick check question: What is the role of alignment loss in fine-tuning process, and how does it differ from standard classification loss?

- **Concept**: Post-hoc local interpretation methods
  - Why needed here: Framework evaluates various explanation methods (Vanilla Gradient, Integrated Gradients, SmoothGrad, Layer-wise Relevance Propagation); knowing how these methods work is crucial for interpreting results and understanding limitations
  - Quick check question: How do post-hoc explanation methods differ from built-in model interpretability techniques, and why are they used in this framework?

- **Concept**: Robustness quantification in model-explanation pipelines
  - Why needed here: Framework introduces method to quantify robustness by measuring difference in alignment accuracy between aligned and misaligned models; understanding this quantification is key to evaluating framework's effectiveness
  - Quick check question: How is robustness defined in this framework, and what does it measure in terms of model-explanation pipeline performance?

## Architecture Onboarding

- **Component map**: Datasets (CheXpert, CheXlocalize) -> Models (DenseNet-201, ViT-base, Swin-ViT-base) -> Explanation methods (VG, IG, SG, LRP) -> Fine-tuning procedures (alignment/misalignment) -> Evaluation metrics (mass/rank accuracy, robustness)

- **Critical path**:
  1. Load and preprocess datasets (CheXpert for training, CheXlocalize for evaluation)
  2. Train models on CheXpert with different initialization strategies
  3. Apply explanation methods to trained models
  4. Fine-tune model-explanation pipelines for alignment and misalignment using CheXlocalize test set
  5. Evaluate alignment and robustness using CheXlocalize validation set

- **Design tradeoffs**:
  - Fine-tuning vs. retraining: Fine-tuning allows controlled manipulation without altering architecture but may not capture all model behavior aspects
  - Single-label vs. multi-label alignment: Framework focuses on single-label alignment for simplicity but extending to multi-label could provide more comprehensive evaluation
  - Dataset choice: Medical imaging data ensures real-world relevance but may limit generalizability to other domains

- **Failure signatures**:
  - Poor convergence in fine-tuning: If alignment/misalignment fine-tuning doesn't converge, robustness measures may be unreliable
  - Inconsistent alignment metrics: Low correlation between mass and rank accuracy may indicate issues with evaluation metrics or data
  - Overfitting during fine-tuning: If model overfits to CheXlocalize test set, robustness measures may not generalize

- **First 3 experiments**:
  1. Baseline alignment evaluation: Evaluate alignment of pre-trained DenseNet with ground truth masks using single explanation method (Vanilla Gradient) to establish baseline
  2. Fine-tuning for alignment: Fine-tune DenseNet to align explanations with ground truth masks and measure change in alignment accuracy
  3. Fine-tuning for misalignment: Fine-tune DenseNet to misalign explanations with ground truth masks and compare alignment accuracy with aligned model to quantify robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively measure quality of explanation methods independently from model's localization performance?
- Basis in paper: [explicit] Paper highlights that evaluating explanations regarding ground truth often primarily evaluates model quality rather than explanation method quality
- Why unresolved: Current evaluation metrics conflate model and explanation method performance, making it difficult to isolate contribution of each component
- What evidence would resolve it: Development of new evaluation metrics that separately assess model prediction quality and explanation method's ability to highlight relevant features regardless of model performance

### Open Question 2
- Question: What are implications of using different types of model pre-training (ImageNet-21k vs. RadImageNet) on robustness of model-explanation pipelines in medical imaging tasks?
- Basis in paper: [explicit] Paper shows pre-training on RadImageNet improves robustness compared to ImageNet-21k
- Why unresolved: Study provides evidence of pre-training impact on robustness but doesn't explore underlying reasons or broader implications for different pre-training data and tasks
- What evidence would resolve it: Further experiments comparing effects of various pre-training datasets and tasks on robustness, along with analysis of learned features and their alignment with human perception

### Open Question 3
- Question: How can we design explanation methods that are inherently robust to adversarial attacks and model manipulation without compromising ability to provide meaningful insights?
- Basis in paper: [inferred] Paper demonstrates vulnerability of model-explanation pipelines to adversarial attacks that manipulate explanations without affecting predictive performance
- Why unresolved: Current post-hoc explanation methods are susceptible to manipulation; need explanation methods more resilient to attacks while providing valuable insights
- What evidence would resolve it: Development and evaluation of new explanation methods incorporating robustness mechanisms (adversarial training, regularization) and comparison of performance against existing methods in terms of robustness and interpretability

## Limitations

- Evaluation framework relies heavily on quality of ground truth segmentation masks, which may be subjective or inconsistent across annotators
- Fine-tuning procedure assumes explanations can be meaningfully manipulated without degrading model performance, which may not hold for all model architectures or explanation methods
- Results based on medical imaging data may not generalize to other domains where feature relevance and human perception differ significantly

## Confidence

| Claim | Confidence |
|---|---|
| Evaluating explanations against ground truth primarily tests model quality | High |
| Pre-training on similar datasets improves robustness | Medium |
| Fine-tuning can create controlled alignment scenarios | Medium |

## Next Checks

1. Apply the framework to non-medical domains (natural images or tabular data) to test generalizability of robustness findings across different data types
2. Compare robustness measures when using alternative ground truth annotations (bounding boxes vs. segmentation masks) to assess sensitivity to annotation quality
3. Evaluate whether similar robustness patterns emerge when using explanation methods that incorporate higher-level semantic features or causal relationships rather than purely gradient-based approaches