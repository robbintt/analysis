---
ver: rpa2
title: Task Grouping for Automated Multi-Task Machine Learning via Task Affinity Prediction
arxiv_id: '2310.16241'
source_url: https://arxiv.org/abs/2310.16241
tags:
- tasks
- pairwise
- gain
- task
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automatically grouping tasks
  for multi-task learning (MTL) to maximize performance gains over single-task learning
  (STL). The authors propose a novel approach that predicts task affinity using various
  features (dataset characteristics, STL performance, pairwise task relationships)
  and employs a randomized search algorithm with a predictor-driven quick-reject mechanism
  to efficiently find optimal task groupings.
---

# Task Grouping for Automated Multi-Task Machine Learning via Task Affinity Prediction

## Quick Facts
- **arXiv ID**: 2310.16241
- **Source URL**: https://arxiv.org/abs/2310.16241
- **Reference count**: 40
- **Primary result**: Proposed task affinity prediction approach achieves better MTL performance than baselines while requiring fewer MTL trainings during search

## Executive Summary
This paper addresses the challenge of automatically grouping tasks for multi-task learning (MTL) to maximize performance gains over single-task learning (STL). The authors propose a novel approach that predicts task affinity using various features (dataset characteristics, STL performance, pairwise task relationships) and employs a randomized search algorithm with a predictor-driven quick-reject mechanism to efficiently find optimal task groupings. Experiments on four benchmark datasets demonstrate that the approach significantly outperforms baseline methods including exhaustive/random search, hierarchical clustering, and k-means clustering. The method achieves better MTL performance while requiring fewer MTL trainings during the search process.

## Method Summary
The method involves training single-task learning (STL) models for each task, computing pairwise task affinities using relative MTL gains, extracting task features (dataset characteristics, STL performance metrics), training a predictor to estimate task affinities, and using a randomized search algorithm with quick-reject to find optimal task groupings. The predictor is periodically updated during search using newly evaluated groups. The approach uses feed-forward neural networks with task-specific and shared layers for both STL and MTL models, and evaluates performance using MSE for regression and log-loss for classification tasks.

## Key Results
- The proposed task affinity prediction approach significantly outperforms baseline methods including exhaustive search, hierarchical clustering, and k-means clustering
- The method achieves better MTL performance while requiring fewer MTL trainings during the search process
- Task features like STL curve gradients, dataset size, and variance in target attributes are identified as strong predictors of MTL gain

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pairwise task affinity can be predicted using features from single-task learning performance and dataset characteristics
- **Mechanism**: Task relatedness in MTL is quantified through relative MTL gain, which measures improvement when two tasks are learned together versus separately. A predictor is trained using 70 features capturing task-specific characteristics and pairwise relationships
- **Core assumption**: Task affinity can be effectively predicted using computationally tractable features without training expensive MTL models for every pair
- **Evidence anchors**: Abstract states inherent task features and STL characteristics help predict whether tasks should be learned together using MTL; section details using relative MTL gain and feature sets
- **Break condition**: If chosen features fail to capture true task relationships or correlation is spurious rather than causal

### Mechanism 2
- **Claim**: Groupwise task affinity can be predicted using aggregated pairwise affinities and task-specific parameters
- **Mechanism**: For groups of tasks, the approach uses average pairwise MTL gain across all task pairs in a group, along with average dot products of task-specific parameter vectors from MTL models
- **Core assumption**: Group affinity can be approximated by aggregating pairwise affinities rather than requiring expensive group-level training
- **Evidence anchors**: Section discusses using mean, variance, and standard deviation of pairwise relative MTL gains as groupwise features; another feature uses average dot products of task-specific parameter vectors
- **Break condition**: If task interactions are non-linear or higher-order effects dominate, simple averaging may miss critical negative transfer effects

### Mechanism 3
- **Claim**: A predictor-driven randomized search with quick-reject can efficiently find optimal task groupings without exhaustive search
- **Mechanism**: The algorithm uses an affinity predictor to estimate MTL gain of proposed task groupings. If predicted gain is low, grouping is rejected without training full MTL model. Predictor is periodically retrained using newly evaluated groups
- **Core assumption**: Early predictions can reliably filter out poor groupings, and predictor improves sufficiently with retraining to guide search effectively
- **Evidence anchors**: Abstract mentions randomized search algorithm employing predictor to minimize MTL trainings; section explains predictor guiding randomized search toward better partitions
- **Break condition**: If predictor makes too many false positives or false negatives, search efficiency gains disappear

## Foundational Learning

- **Concept**: Multi-Task Learning (MTL) fundamentals
  - Why needed here: Entire paper builds on understanding when and how MTL outperforms Single-Task Learning (STL)
  - Quick check question: What is negative transfer in MTL and how does it differ from transfer learning?

- **Concept**: Feature engineering for task relationships
  - Why needed here: Predictor's effectiveness depends on choosing features that capture task relatedness
  - Quick check question: Why might dataset size and target variance be predictive of MTL gain between tasks?

- **Concept**: Randomized search algorithms and hyperparameter optimization
  - Why needed here: Task grouping approach uses randomized local search with predictor-guided acceptance
  - Quick check question: How does simulated annealing's acceptance probability relate to the paper's approach?

## Architecture Onboarding

- **Component map**: Data preprocessing → Feature extraction (single-task + pairwise) → Predictor training → Randomized search with quick-reject → Periodic predictor retraining → Output optimal grouping
- **Critical path**: Feature computation → Predictor evaluation → Grouping decision (accept/reject) → MTL training (only if accepted) → Performance evaluation → Predictor update (periodic)
- **Design tradeoffs**: Predictor accuracy vs. computational cost of training; search exploration (random mutations) vs. exploitation (following predictor guidance); feature richness vs. predictor complexity and overfitting risk
- **Failure signatures**: Predictor consistently rejects good groupings → feature set insufficient; search converges to poor solution → predictor needs more training data or better architecture; MTL training dominates runtime → quick-reject threshold too permissive
- **First 3 experiments**: 1) Feature correlation study: Compute Pearson correlation between all proposed features and relative MTL gain on small subset of task pairs; 2) Predictor ablation: Train predictors with different feature subsets and measure R² on held-out data; 3) Quick-reject validation: Run search with different acceptance thresholds and measure tradeoff between MTL trainings and solution quality

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does performance of task affinity prediction vary when using more complex neural network architectures beyond feed-forward networks studied?
- **Basis in paper**: [inferred] Paper states "specific MTL architectures are not a vital component" and uses feed-forward networks, but does not explore other architectures
- **Why unresolved**: Study focuses on feed-forward networks and explicitly notes MTL architectures are not primary contribution, leaving performance of other architectures unexplored
- **What evidence would resolve it**: Experimental results comparing task affinity prediction performance using different neural network architectures (e.g., recurrent, convolutional, transformer-based) on same benchmark datasets

### Open Question 2
- **Question**: What is impact of task affinity prediction accuracy on computational efficiency of task grouping algorithm in very large-scale settings with thousands of tasks?
- **Basis in paper**: [explicit] Paper demonstrates computational advantages on datasets with up to 139 tasks but does not explore scaling to thousands of tasks
- **Why unresolved**: Experiments focus on benchmarks with 29-139 tasks, and computational complexity and predictor performance at much larger scales remains untested
- **What evidence would resolve it**: Results showing predictor accuracy and computational time for task grouping algorithms when applied to datasets with 1000+ tasks, comparing different predictor architectures and search strategies

### Open Question 3
- **Question**: How do identified task features (STL curve gradients, dataset size, variance) perform as predictors across different types of machine learning problems beyond four benchmark datasets studied?
- **Basis in paper**: [explicit] Paper identifies these features as strong predictors but only validates on four specific benchmark datasets
- **Why unresolved**: Study's generalizability to other problem domains (e.g., image classification, reinforcement learning, time series forecasting) is not established
- **What evidence would resolve it**: Experiments demonstrating predictive power of these features across diverse problem domains and data types, including transfer learning scenarios

## Limitations
- Predictor performance may degrade on datasets with non-linear task relationships or when negative transfer effects are dominant
- Randomized search may converge to local optima rather than global optimal groupings, particularly in high-dimensional task spaces
- Study focuses on feed-forward neural networks with specific architectural choices, limiting generalizability to other model architectures

## Confidence
- High confidence in core mechanism of using feature-based prediction for task affinity (supported by strong experimental results and ablation studies)
- Medium confidence in generalizability of feature importance (feature rankings may vary across different domains and model architectures)
- Medium confidence in search efficiency claims (comparisons with exhaustive search are strong, but real-world applicability to larger task sets needs validation)

## Next Checks
1. Test predictor robustness across different neural network architectures (CNNs, transformers) to verify feature-based affinity prediction is architecture-agnostic
2. Evaluate performance on datasets with known negative transfer cases to assess predictor's ability to avoid harmful task groupings
3. Scale experiments to larger task sets (100+ tasks) to verify search efficiency benefits hold at realistic problem sizes