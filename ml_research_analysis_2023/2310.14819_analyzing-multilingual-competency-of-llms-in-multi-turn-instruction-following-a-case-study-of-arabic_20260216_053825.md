---
ver: rpa2
title: 'Analyzing Multilingual Competency of LLMs in Multi-Turn Instruction Following:
  A Case Study of Arabic'
arxiv_id: '2310.14819'
source_url: https://arxiv.org/abs/2310.14819
tags:
- arabic
- english
- llms
- questions
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a detailed examination of the proficiency of
  open Large Language Models (LLMs) in responding to multi-turn instructions in Arabic.
  Using a customized Arabic translation of the MT-Bench benchmark suite, the authors
  employed GPT-4 as a uniform evaluator for both English and Arabic queries to assess
  and compare the performance of the LLMs on various open-ended tasks.
---

# Analyzing Multilingual Competency of LLMs in Multi-Turn Instruction Following: A Case Study of Arabic

## Quick Facts
- arXiv ID: 2310.14819
- Source URL: https://arxiv.org/abs/2310.14819
- Reference count: 11
- Primary result: Fine-tuned base models using multilingual and multi-turn datasets can perform competitively to models trained from scratch on multilingual data

## Executive Summary
This paper presents a detailed examination of the proficiency of open Large Language Models (LLMs) in responding to multi-turn instructions in Arabic. Using a customized Arabic translation of the MT-Bench benchmark suite, the authors employed GPT-4 as a uniform evaluator for both English and Arabic queries to assess and compare the performance of the LLMs on various open-ended tasks. The findings reveal variations in model responses on different task categories when instructed in English or Arabic. The study also found that fine-tuned base models using multilingual and multi-turn datasets could be competitive to models trained from scratch on multilingual data. Finally, the authors hypothesize that an ensemble of small, open LLMs could perform competitively to proprietary LLMs on the benchmark. The paper offers insights and recommendations for pushing forward the competency of Arabic LLMs.

## Method Summary
The study employs a customized Arabic translation of the MT-Bench benchmark suite to evaluate open LLMs' performance on multi-turn instruction following tasks. GPT-4 serves as a uniform judge for both English and Arabic prompts, using identical evaluation criteria and scoring on a scale of 1-10 across categories like helpfulness, relevance, accuracy, and creativity. The evaluation pipeline involves generating responses from multiple open LLMs (limited to 33B parameters or less), evaluating these responses using GPT-4, and comparing performance across languages and models. The authors also explore the potential of ensemble methods by combining responses from multiple smaller models.

## Key Results
- Fine-tuned base models using multilingual and multi-turn datasets perform competitively to models trained from scratch on multilingual data
- An optimal ensemble of small, open LLMs achieves a 32% performance improvement over the best individual open LLM
- LLMs fine-tuned specifically for Arabic or multilingual capabilities (e.g., Jais, Phoenix) outperform generic models on Arabic instruction following tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 as a judge provides consistent evaluation across Arabic and English prompts.
- Mechanism: Using the same judging prompt and evaluation criteria for both languages ensures comparable scoring standards.
- Core assumption: GPT-4's proficiency in Arabic is sufficient to provide reliable judgments.
- Evidence anchors:
  - [abstract] "We employ GPT-4 as a uniform evaluator for both English and Arabic queries to assess and compare the performance of the LLMs"
  - [section] "MT-Bench prompts a judge LLM with an instruction to rate the responses on a scale of 1-10... clearly defining the evaluation task and criteria."
- Break condition: If GPT-4's Arabic proficiency is significantly lower than English, judgments may be biased or inconsistent.

### Mechanism 2
- Claim: Fine-tuning on multilingual and multi-turn datasets improves Arabic instruction following performance.
- Mechanism: Models trained on diverse language data and conversational contexts better generalize to Arabic multi-turn instructions.
- Core assumption: The fine-tuning datasets contain sufficient Arabic and multi-turn instruction examples.
- Evidence anchors:
  - [abstract] "We find that fine-tuned base models using multilingual and multi-turn datasets could be competitive to models trained from scratch on multilingual data."
  - [section] "LLMs fine-tuned specifically for Arabic or for multilingual capabilities (e.g. Jais, Phoenix) are better than generic models"
- Break condition: If the fine-tuning datasets lack sufficient Arabic examples or conversational data, improvements may not materialize.

### Mechanism 3
- Claim: Ensembling small, specialized LLMs can achieve performance competitive with larger proprietary models.
- Mechanism: Combining models with complementary strengths captures a broader range of capabilities than any single model.
- Core assumption: Different models excel at different task categories, allowing effective ensemble construction.
- Evidence anchors:
  - [abstract] "Finally, we hypothesize that an ensemble of small, open LLMs could perform competitively to proprietary LLMs on the benchmark."
  - [section] "Based on our ARABIC MT-BENCH, the optimal ensemble model achieves an MT score of 6.70. This represents a 32% increase in performance compared to the best individual open LLM"
- Break condition: If the models' strengths do not complement each other or if selecting the best response from the ensemble is not feasible, performance gains may be limited.

## Foundational Learning

- Concept: Understanding LLM evaluation benchmarks and their limitations
  - Why needed here: To properly interpret the results and understand what the ARABIC MT-BENCH measures
  - Quick check question: What are the key differences between conventional NLP benchmarks and instruction-following benchmarks like MT-BENCH?

- Concept: Multilingual model training and fine-tuning techniques
  - Why needed here: To understand why certain models perform better on Arabic tasks and how to improve them
  - Quick check question: How does fine-tuning on multilingual datasets affect a model's performance on specific languages?

- Concept: Ensemble methods for LLM responses
  - Why needed here: To implement the proposed approach of combining multiple models for better performance
  - Quick check question: What are the main challenges in selecting the best response from an ensemble of LLMs?

## Architecture Onboarding

- Component map:
  Benchmark dataset (ARABIC MT-BENCH) -> Judge LLM (GPT-4) -> Multiple open LLMs -> Evaluation pipeline -> Analysis results

- Critical path:
  1. Load benchmark questions and reference answers
  2. Generate responses from each LLM
  3. Evaluate responses using GPT-4 judge
  4. Analyze results and compare performance across models and languages

- Design tradeoffs:
  - Using GPT-4 as judge: Pros - consistent evaluation; Cons - potential bias and resource cost
  - Fine-tuning vs. training from scratch: Pros - faster and cheaper; Cons - may not achieve the same performance as models trained on more diverse data
  - Small models vs. large models: Pros - more efficient and potentially easier to fine-tune; Cons - may lack the capacity for complex reasoning

- Failure signatures:
  - Inconsistent scoring across languages may indicate bias in the judge LLM
  - Poor performance on Arabic despite multilingual training may suggest insufficient Arabic data in the training set
  - Large performance gaps between first and second turns may indicate difficulty in maintaining context

- First 3 experiments:
  1. Compare ARABIC MT-BENCH scores for a multilingual model vs. an English-only model to validate the benchmark's ability to detect multilingual proficiency
  2. Fine-tune a base LLM on additional Arabic data and evaluate its performance on the benchmark to test the impact of language-specific training
  3. Create an ensemble of two or three open LLMs and compare its performance to individual models to assess the potential benefits of ensembling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the extent of catastrophic forgetting in multilingual LLMs during continual fine-tuning, particularly for Arabic instruction following?
- Basis in paper: Inferred from the discussion on Jais-13B-chat's large drop in performance in the second-turn questions on the English benchmark and the hypothesis that longer fine-tuning periods might increase the risk of catastrophic forgetting.
- Why unresolved: The paper does not provide direct evidence or experiments to quantify catastrophic forgetting in multilingual LLMs. Further investigation is needed to understand the extent and mechanisms of this phenomenon.
- What evidence would resolve it: Experiments measuring performance degradation across languages during continual fine-tuning of multilingual LLMs, with a focus on Arabic instruction following tasks.

### Open Question 2
- Question: How does the quality of manual curation of translated benchmark questions affect the performance scores of LLMs in multilingual instruction following tasks?
- Basis in paper: Explicit mention of the thorough manual curation of the Arabic translation of the MT-Bench benchmark questions and its importance in ensuring the quality of the question set.
- Why unresolved: The paper does not provide quantitative analysis of the impact of manual curation on LLM performance scores. It is unclear how much the quality of translation and curation affects the reliability and validity of the benchmark.
- What evidence would resolve it: Comparative experiments evaluating LLM performance on automatically translated vs. manually curated benchmark questions, measuring the consistency and reliability of scores.

### Open Question 3
- Question: What is the optimal strategy for selecting responses from an ensemble of small, open LLMs to achieve performance comparable to proprietary LLMs on multilingual instruction following tasks?
- Basis in paper: Inferred from the discussion on the hypothetical optimal ensemble model and the challenge of defining a criterion to select the best response among ensemble LLMs.
- Why unresolved: The paper does not provide a concrete method or empirical results for selecting responses from an ensemble of LLMs. The effectiveness of voting mechanisms or other selection strategies is not explored.
- What evidence would resolve it: Experiments comparing different ensemble selection strategies, such as majority voting, confidence-based selection, or LLM-judge voting, and their impact on benchmark performance.

## Limitations
- The use of GPT-4 as a uniform judge across languages may introduce bias due to varying proficiency levels
- Auto-translation process, despite manual curation, may introduce linguistic artifacts affecting model performance
- The study focuses on Arabic and may not generalize to other low-resource languages

## Confidence
- High confidence: Fine-tuned models on multilingual and multi-turn datasets perform competitively to models trained from scratch
- Medium confidence: The ensemble hypothesis showing a 32% performance improvement
- Low confidence: The uniformity of GPT-4 as a judge across languages

## Next Checks
1. Run a small-scale human evaluation study comparing GPT-4's Arabic judgments against human ratings for a subset of responses to quantify any systematic bias
2. Evaluate the same models on an additional Arabic instruction-following benchmark to assess whether the ARABIC MT-BENCH results generalize across different evaluation frameworks
3. Conduct a linguistic review of the auto-translated benchmark to identify and quantify any systematic translation artifacts that might disadvantage certain models or task types