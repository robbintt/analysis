---
ver: rpa2
title: A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating
  Disorder Content on Social Media
arxiv_id: '2307.06775'
source_url: https://arxiv.org/abs/2307.06775
tags:
- pro-ed
- content
- were
- eating
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a multimodal deep learning model to identify
  pro-eating disorder (Pro-ED) content on social media using both text and image inputs.
  The approach combined RoBERTa for natural language processing and MaxViT for image
  classification through late fusion, achieving an accuracy of 95.9% and F1-score
  of 0.959 on a balanced Twitter dataset.
---

# A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media

## Quick Facts
- arXiv ID: 2307.06775
- Source URL: https://arxiv.org/abs/2307.06775
- Reference count: 9
- Primary result: Multimodal model combining RoBERTa and MaxViT achieves 95.9% accuracy and 0.959 F1-score on Pro-ED content detection

## Executive Summary
This study develops a multimodal deep learning model to identify pro-eating disorder (Pro-ED) content across social media platforms. The approach combines RoBERTa for natural language processing and MaxViT for image classification through late fusion, achieving state-of-the-art performance on Twitter data. The model successfully generalizes to Tumblr and Reddit, producing classifications consistent with prior research. Time-series analysis reveals a significant decline in Pro-ED content since 2014, with a notable rebound beginning around 2018, likely influenced by the Covid-19 pandemic.

## Method Summary
The study employs late fusion of pre-trained RoBERTa and MaxViT models to create a multimodal classifier for Pro-ED content detection. The approach uses random undersampling to balance the dataset across Pro-ED, Not-Pro-ED, and Neutral classes, with 2,230 instances per class. Text preprocessing includes cleaning and tokenization, while images undergo standard normalization. The model is trained on 60% of the balanced dataset, validated on 20%, and tested on 20%. Performance is evaluated using accuracy, F1-score, precision, and recall metrics.

## Key Results
- Achieved 95.9% accuracy and 0.959 F1-score on balanced Twitter dataset
- Model generalizes to Tumblr and Reddit with classifications consistent with prior research
- Time-series analysis shows Pro-ED content declined since 2014, rebounded from 2018, likely due to Covid-19 pandemic

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Late fusion of RoBERTa and MaxViT achieves high accuracy by leveraging complementary strengths of NLP and CV models without architectural compromises
- Mechanism: Late fusion preserves pre-trained weights of both models and combines their outputs after individual classification, allowing each to specialize in its modality (text or image) while benefiting from the other's insight
- Core assumption: The modalities provide independent and complementary information for detecting Pro-ED content, and late fusion is sufficient to integrate these signals effectively
- Evidence anchors: [abstract] "multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively"; [section] "late fusion because it permits the use of pre-trained DL models without necessitating any changes in their model architecture or weights"
- Break condition: If modalities are not independent or complementary (e.g., if text and images always convey redundant information), late fusion would not improve accuracy over unimodal models

### Mechanism 2
- Claim: Dataset balancing via undersampling ensures that the minority class (Pro-ED) is adequately represented, preventing model bias toward majority classes
- Mechanism: Random undersampling reduces the size of majority classes (Neutral and Not-Pro-ED) to match the minority class (Pro-ED), creating a balanced dataset that allows the model to learn all classes equally
- Core assumption: Model performance improves when training data is balanced, and undersampling is an effective method for achieving this balance without significant information loss
- Evidence anchors: [section] "Previous research has shown that DL models are able to generalize better when the dataset upon which they are trained and evaluated has an equal number of instances of all classes... Accordingly, the Not-Pro-ED and the Neutral classes were randomly sampled until they, too, had only 2,230 Tweets each"
- Break condition: If majority classes contain unique information not present in the minority class, undersampling could lead to information loss and reduced model performance

### Mechanism 3
- Claim: Fine-tuning pre-trained transformer models (RoBERTa and MaxViT) on the Pro-ED dataset significantly improves their performance compared to training from scratch or using non-fine-tuned models
- Mechanism: Pre-trained transformers have already learned general language/image patterns from large corpora (e.g., Twitter for RoBERTa, ImageNet for MaxViT), and fine-tuning adapts these patterns to the specific task of Pro-ED detection
- Core assumption: The pre-trained models have learned relevant features that can be adapted to the Pro-ED detection task, and the dataset is large enough to allow effective fine-tuning
- Evidence anchors: [abstract] "The most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively"
- Break condition: If pre-trained models have not learned relevant features for the Pro-ED task, or if the dataset is too small for effective fine-tuning, performance may not improve

## Foundational Learning

- Concept: Multimodal learning
  - Why needed here: The study aims to detect Pro-ED content that often combines textual and visual elements, requiring models that can process both modalities
  - Quick check question: What are the three main families of multimodal fusion approaches, and which one was chosen for this study?

- Concept: Transformer architectures
  - Why needed here: Transformers are the state-of-the-art models for both NLP (RoBERTa) and computer vision (MaxViT), providing the necessary capabilities for processing text and images
  - Quick check question: What is the key mechanism in transformers that allows them to understand relationships between different parts of the input?

- Concept: Dataset balancing techniques
  - Why needed here: The original dataset had a significant class imbalance, with the Pro-ED class being the minority, which could lead to biased models
  - Quick check question: What undersampling technique was used to balance the dataset, and why is balancing important for model performance?

## Architecture Onboarding

- Component map:
  Input (Twitter posts with text and image) -> Text processing (RoBERTa) + Image processing (MaxViT) -> Late fusion of individual model outputs -> Classification (Pro-ED, Neutral, Not-Pro-ED)

- Critical path:
  1. Preprocess text and images (tokenization, resizing, normalization)
  2. Feed text into RoBERTa and images into MaxViT
  3. Obtain individual classifications from RoBERTa and MaxViT
  4. Combine classifications (e.g., weighted averaging, voting)
  5. Output final classification

- Design tradeoffs:
  - Late fusion vs. early/intermediate fusion: Late fusion preserves pre-trained weights but may miss early interactions between modalities
  - Model complexity: Using state-of-the-art models (RoBERTa, MaxViT) provides high performance but increases computational requirements
  - Dataset size: Fine-tuning requires a sufficient amount of data; undersampling may lead to information loss

- Failure signatures:
  - Poor performance on any class: Indicates insufficient representation of that class in the training data or model bias
  - Low accuracy on multimodal posts: Suggests that the fusion strategy is not effectively integrating the modalities
  - Overfitting: High performance on training data but poor generalization to unseen data

- First 3 experiments:
  1. Train and evaluate unimodal RoBERTa and MaxViT models on the balanced dataset to establish baseline performance
  2. Implement and evaluate the late fusion of RoBERTa and MaxViT on the same dataset
  3. Test the trained fusion model on a held-out test set and on data from other social media platforms (Tumblr, Reddit) to assess generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors caused the decrease in Pro-ED content on Twitter starting in 2014, and why did this decrease stop/reverse around 2018?
- Basis in paper: [explicit] The authors state "The cause of this decrease are yet unknown" and discuss possible factors like increased moderation and platform migration
- Why unresolved: The paper identifies potential explanations but does not definitively establish causation through controlled experiments or longitudinal user behavior analysis
- What evidence would resolve it: Controlled experiments comparing content moderation policies across platforms, user migration tracking studies, and sentiment analysis of community discussions before and after 2014/2018

### Open Question 2
- Question: How well can the MaxViT + RoBERTa model generalize to other forms of Pro-ED content beyond Twitter, Tumblr, and Reddit, such as videos or live streams?
- Basis in paper: [inferred] The authors note their model only analyzed text+image posts and suggest future work should examine video analysis
- Why unresolved: The study was limited to multimodal posts (text + images) and did not test performance on other media types or emerging platforms
- What evidence would resolve it: Testing the model on labeled datasets containing Pro-ED videos, audio clips, and other media types, and comparing performance metrics across modalities

### Open Question 3
- Question: What is the long-term trajectory of Pro-ED content prevalence on social media platforms, and what interventions are most effective at reducing harmful content?
- Basis in paper: [explicit] The authors note that Pro-ED content rebounded after 2018 and may continue growing, but the long-term trends and intervention effectiveness remain unclear
- Why unresolved: The study only analyzed data up to April 2023 and did not evaluate intervention strategies or predict future trends beyond linear regression projections
- What evidence would resolve it: Multi-year longitudinal studies tracking Pro-ED content across multiple platforms, A/B testing of different moderation approaches, and predictive modeling of future content trends

## Limitations

- Limited quantitative validation on non-Twitter platforms (Tumblr, Reddit) reduces confidence in cross-platform generalizability
- Random undersampling for class balancing may have discarded potentially informative instances from majority classes
- Temporal analysis lacks control for overall platform usage trends, making causal attribution uncertain

## Confidence

- High Confidence: Multimodal architecture combining RoBERTa and MaxViT with late fusion can achieve high accuracy on balanced Twitter datasets for Pro-ED detection
- Medium Confidence: The model generalizes to other social media platforms (Tumblr, Reddit) in a manner consistent with prior research
- Medium Confidence: Pro-ED content on Twitter has shown a significant decline since 2014 with a rebound starting around 2018, likely influenced by the COVID-19 pandemic

## Next Checks

1. **Quantitative Cross-Platform Validation**: Evaluate the trained model on Tumblr and Reddit datasets using the same performance metrics (accuracy, F1-score, precision, recall) applied to the Twitter dataset to provide comparable quantitative results

2. **Alternative Balancing Strategies**: Replicate the study using different class balancing techniques (SMOTE, class weighting, or ensemble methods) to assess whether the high performance is robust to different approaches for handling class imbalance

3. **Temporal Trend Control Analysis**: Control for overall Twitter usage trends during the same period to determine whether the observed Pro-ED content changes exceed general platform growth/decline patterns, providing stronger evidence for the effectiveness of platform moderation policies