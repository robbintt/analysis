---
ver: rpa2
title: Zero-shot Audio Topic Reranking using Large Language Models
arxiv_id: '2309.07606'
source_url: https://arxiv.org/abs/2309.07606
tags:
- retrieval
- reranking
- topic
- performance
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a zero-shot reranking approach to enhance audio
  topic retrieval performance in multimodal video search by examples (MVSE). The method
  employs large language models (LLMs) to rerank the top N retrieved results from
  an initial retrieval stage based on embeddings.
---

# Zero-shot Audio Topic Reranking using Large Language Models

## Quick Facts
- arXiv ID: 2309.07606
- Source URL: https://arxiv.org/abs/2309.07606
- Authors: 
- Reference count: 0
- This paper proposes a zero-shot reranking approach to enhance audio topic retrieval performance in multimodal video search by examples (MVSE).

## Executive Summary
This paper introduces a zero-shot reranking approach to improve audio topic retrieval in multimodal video search by examples. The method leverages large language models (LLMs) to rerank top-N retrieved results from an initial embedding-based retrieval stage. Three text sources are explored: ASR transcriptions, automatic summaries, and human-written synopses. Experiments on the BBC Rewind corpus demonstrate that zero-shot pairwise reranking with Flan-T5-3B significantly improves retrieval ranking, achieving an nDCG@3 score of 0.52 compared to 0.47 for the baseline.

## Method Summary
The method employs a two-stage pipeline: initial retrieval using topic embeddings extracted from text representations, followed by zero-shot reranking using LLMs. For reranking, pairwise comparison is used where the LLM evaluates which of two passages is more topically relevant, with the win-ratio determining final ranking order. The approach explores three input sources - ASR transcriptions, automatic summaries generated from ASR using Llama2-chat, and human-written synopses. Flan-T5-3B is used for pairwise reranking while GPT-4 is evaluated for listwise reranking.

## Key Results
- Zero-shot pairwise reranking with Flan-T5-3B improved nDCG@3 from 0.47 to 0.52
- Listwise reranking with GPT-4 showed improvement while GPT-3.5 performed similarly to baseline
- AutoSum of ASR transcriptions helped narrow the performance gap between ASR and synopsis-based retrieval
- Pairwise reranking was more cost-effective than listwise while achieving comparable performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot pairwise reranking improves topic retrieval by leveraging LLM-based comparison of topic relevance between passages
- Mechanism: The system compares pairs of retrieved passages using prompt-based classification where the LLM evaluates which passage is more topically relevant, with win-ratios determining final ranking
- Core assumption: The LLM can accurately judge topic relevance between passages without task-specific training data
- Evidence anchors:
  - [abstract]: "zero-shot pairwise reranking method shows comparable performance to the listwise counterpart while being more cost-effective"
  - [section]: "In Figure 3, we illustrate the prompt-based classification method used in our PRL experiments... This avoids an expensive beam search while also leading to a very simple decoding setup"
- Break condition: If the LLM's topic classification capability is limited or if the prompt fails to properly guide the comparison

### Mechanism 2
- Claim: Summarization of ASR transcriptions improves retrieval performance by reducing noise and focusing on key topic information
- Mechanism: Automatic summarization (AutoSum) is applied to lengthy, noisy ASR transcriptions to create more concise representations that better capture the core topic content
- Core assumption: AutoSum-generated summaries contain sufficient topic-relevant information while being less noisy than full ASR transcriptions
- Evidence anchors:
  - [section]: "ASR transcriptions can often be lengthy and noisy, especially when the longest audio in the dataset is over 30 minutes. Alternatively, we generate a more concise automatic summary using Llama2-chat"
- Break condition: If AutoSum fails to preserve critical topic information or introduces summarization errors that mislead the retrieval system

### Mechanism 3
- Claim: Listwise reranking with larger LLMs (GPT-4) outperforms smaller models and baseline retrieval due to superior contextual understanding
- Mechanism: Listwise reranking processes all top-N candidates simultaneously, allowing the LLM to consider global context and relationships between passages when generating the reordered list
- Core assumption: Larger LLMs have better contextual understanding and can effectively compare multiple passages simultaneously for topic relevance
- Evidence anchors:
  - [section]: "For the LRL method, when the GPT-3.5 model is adopted the performance is similar to the baseline system while GPT-4 yields better performance"
- Break condition: If the LLM cannot effectively process all candidates within its context window or if global context doesn't improve over pairwise comparisons

## Foundational Learning

- Concept: Information retrieval metrics (nDCG, Precision@k)
  - Why needed here: To evaluate and compare the effectiveness of different retrieval and reranking approaches
  - Quick check question: What's the difference between precision@3 and nDCG@3, and when would each be more appropriate?

- Concept: Embedding-based retrieval systems
  - Why needed here: The baseline system uses topic embeddings extracted from text representations for initial retrieval before reranking
  - Quick check question: How do embedding similarity measures like cosine similarity enable rapid retrieval in large archives?

- Concept: Prompt engineering for LLMs
  - Why needed here: Both listwise and pairwise reranking rely on carefully crafted prompts to guide the LLM's behavior for topic relevance assessment
  - Quick check question: What are the key differences between prompt-based classification and free generation approaches in LLM applications?

## Architecture Onboarding

- Component map: Audio input → ASR transcription → (Optional AutoSum) → Topic embedding extraction → Initial retrieval → LLM reranking → Final ranked output
- Critical path:
  1. ASR transcription generation (Whisper model)
  2. Topic embedding extraction (fine-tuned RoBERTa on TDT dataset)
  3. Initial retrieval using cosine similarity between embeddings
  4. LLM-based reranking of top-N results
  5. Final output generation
- Design tradeoffs:
  - ASR vs. synopsis input: ASR is more universally available but noisier; synopses are cleaner but require human annotation
  - Listwise vs. pairwise reranking: Listwise considers global context but has token limitations; pairwise is more scalable but may miss global relationships
  - Model size vs. cost: Larger LLMs (GPT-4) perform better but are more expensive than smaller models (Flan-T5-3B)
- Failure signatures:
  - Low precision at top ranks despite high nDCG: Suggests the reranking is improving lower-ranked items but not the top results
  - Degradation when switching from ASR to AutoSum: Indicates the summarization is losing critical topic information
  - Similar performance between baseline and reranked results: Suggests the reranking prompts are not effectively guiding the LLM
- First 3 experiments:
  1. Compare ASR vs. AutoSum vs. synopsis input for initial retrieval performance on a small subset of the data
  2. Test pairwise reranking with different N values (5, 10, 20) to find the optimal candidate list size
  3. Compare listwise reranking with GPT-4 vs. pairwise reranking with Flan-T5-3B on the same query set to measure cost-performance tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different LLM reranking strategies (listwise vs pairwise) on retrieval performance across various video archive sizes?
- Basis in paper: [explicit] The paper compares listwise and pairwise reranking methods and notes that pairwise performs similarly while being more cost-effective
- Why unresolved: The paper only evaluates on a single dataset (BBC Rewind corpus) and doesn't explore performance across different archive sizes
- What evidence would resolve it: Experiments showing retrieval performance using both reranking strategies on archives of varying sizes, with quantitative comparison of computational cost vs performance gains

### Open Question 2
- Question: How do different ASR model qualities affect the effectiveness of LLM reranking for audio topic retrieval?
- Basis in paper: [inferred] The paper uses Whisper's small.en model and shows that even with imperfect ASR transcriptions, reranking can improve performance, suggesting potential dependencies on ASR quality
- Why unresolved: The paper only uses one ASR model configuration and doesn't explore how varying ASR quality impacts reranking effectiveness
- What evidence would resolve it: Systematic experiments comparing reranking performance using ASR outputs from models of varying quality levels, with correlation analysis between ASR WER and reranking gains

### Open Question 3
- Question: What is the optimal number of candidates to rerank for maximizing retrieval performance while minimizing computational cost?
- Basis in paper: [explicit] The paper includes ablation studies on different sizes of candidate lists (N=5, 10, 20) and finds N=10 optimal
- Why unresolved: The paper only explores three values of N and doesn't provide guidance on how to determine optimal N for different use cases or datasets
- What evidence would resolve it: A comprehensive study examining reranking performance across a wider range of N values, with analysis of the trade-off between performance gains and computational costs across different datasets and use cases

## Limitations
- Weak empirical evidence for some claimed mechanisms, particularly the effectiveness of automatic summarization
- Inconsistent performance between different LLM sizes in listwise reranking without clear explanation
- Limited exploration of how archive size affects reranking strategy effectiveness

## Confidence
- High confidence: The overall pipeline architecture and zero-shot reranking approach is technically sound and demonstrably effective
- Medium confidence: The superiority of pairwise over listwise reranking is supported but the cost-performance tradeoff analysis is limited
- Low confidence: The specific mechanisms by which AutoSum improves performance beyond ASR transcriptions are not empirically validated

## Next Checks
1. Conduct ablation studies comparing ASR transcriptions directly against AutoSum summaries to quantify information preservation and noise reduction effects
2. Test pairwise reranking with different LLM model sizes (e.g., Flan-T5-base, Flan-T5-large) to establish the minimum effective model size and better understand the cost-performance tradeoff
3. Evaluate reranking performance across different query difficulty levels (easy vs. hard queries) to determine if improvements are uniform or concentrated on specific query types