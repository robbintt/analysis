---
ver: rpa2
title: On the Cultural Gap in Text-to-Image Generation
arxiv_id: '2307.02971'
source_url: https://arxiv.org/abs/2307.02971
tags:
- image
- cultural
- images
- caption
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the cultural gap in text-to-image generation,
  where models trained on predominantly Western data struggle to generate culturally
  diverse images. To address this, the authors introduce the Challenging Cross-Cultural
  (C3) benchmark, a dataset of 9,889 challenging prompts designed to evaluate cross-cultural
  generation capabilities.
---

# On the Cultural Gap in Text-to-Image Generation

## Quick Facts
- arXiv ID: 2307.02971
- Source URL: https://arxiv.org/abs/2307.02971
- Reference count: 27
- This paper addresses the cultural gap in text-to-image generation, where models trained on predominantly Western data struggle to generate culturally diverse images.

## Executive Summary
This paper addresses the cultural gap in text-to-image generation, where models trained on predominantly Western data struggle to generate culturally diverse images. To address this, the authors introduce the Challenging Cross-Cultural (C3) benchmark, a dataset of 9,889 challenging prompts designed to evaluate cross-cultural generation capabilities. They also propose a novel multi-modal metric that considers both textual and visual elements, including object-text alignment, to filter fine-tuning data in the target culture. Experimental results show that their metric outperforms existing methods in selecting high-quality translated captions for fine-tuning, leading to improved cross-cultural image generation. The C3 benchmark and associated resources are made publicly available to facilitate future research in this area.

## Method Summary
The authors introduce the Challenging Cross-Cultural (C3) benchmark with 9,889 prompts designed to evaluate cross-cultural text-to-image generation. They propose a multi-modal filtering metric combining text-text alignment (LaBSE), image-text alignment (CLIP), and object-text alignment (GRiT) to select high-quality translated captions for fine-tuning. The method fine-tunes Stable Diffusion v1-4 on filtered laion2b-zh data, then evaluates on C3 using human evaluation across six criteria: Object Presence, Object Localization, Cultural Appropriateness, Visual Aesthetics, Semantic Consistency, and Cohesion.

## Key Results
- The proposed multi-modal metric outperforms existing single-modal methods in selecting high-quality translated captions for fine-tuning
- Fine-tuned models achieve significantly better performance than the vanilla model trained only on English-centric data
- The C3 benchmark effectively reveals cross-cultural generation weaknesses, with only 57% of generated images rated above average compared to 78% on COCO

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal alignment combining text-text, image-text, and object-text scores improves caption quality assessment over single-modal methods.
- Mechanism: The filtering metric combines three alignment scores (LaBSE for text-text, CLIP for image-text, and GRiT for object-text) to capture complementary aspects of translation quality. The combined score correlates better with human judgment than any single alignment method alone.
- Core assumption: Each alignment dimension captures unique and complementary information about translation quality that cannot be fully represented by the others.
- Evidence anchors:
  - [abstract] "Experimental results show that our multi-modal metric provides stronger data selection performance on the C3 benchmark than existing metrics"
  - [section] "Our metric outperforms both LaBSE and CLIP in terms of correlation with human evaluation scores across all criteria"
  - [corpus] Weak evidence - related papers focus on cultural bias but don't directly test multi-modal alignment approaches
- Break condition: If any alignment component becomes unreliable (e.g., GRiT fails to detect cultural objects), the combined metric's effectiveness degrades.

### Mechanism 2
- Claim: Fine-tuning diffusion models on culturally-specific data with high-quality translated captions improves cross-cultural image generation.
- Mechanism: By filtering low-quality translated captions using the multi-modal metric, the fine-tuning process receives better aligned image-text pairs, leading to improved generation of culturally-specific elements.
- Core assumption: The quality of translated captions directly impacts the model's ability to learn culturally-specific visual concepts during fine-tuning.
- Evidence anchors:
  - [abstract] "Experimental results show that our multi-modal metric provides stronger data selection performance on the C3 benchmark than existing metrics"
  - [section] "Fine-tuned models achieve significantly better performance than the vanilla model that is trained only on the English-centric data"
  - [corpus] Weak evidence - related work shows cultural gaps but doesn't demonstrate this specific fine-tuning approach
- Break condition: If the translated captions contain systematic cultural biases or mistranslations that persist through filtering, the model will still generate culturally inappropriate images.

### Mechanism 3
- Claim: The C3 benchmark effectively reveals cross-cultural generation weaknesses by containing prompts with cultural elements rarely seen in training data.
- Mechanism: The benchmark's prompts are designed to trigger failures in generating culturally-specific objects, which are then revealed through human evaluation across multiple criteria including cultural appropriateness, object presence, and localization.
- Core assumption: Prompts containing culturally-specific elements that are rare in training data will expose generation failures that common benchmarks miss.
- Evidence anchors:
  - [abstract] "C3 benchmark with comprehensive evaluation criteria... assess how well-suited a model is to a target culture"
  - [section] "Figure 3 lists the comparison results. Clearly, 78% of the generated images on COCO are rated above average... while the ratio on C3 is 57%"
  - [corpus] Moderate evidence - related papers also identify cultural gaps but don't provide this specific benchmarking approach
- Break condition: If models learn to generate plausible but culturally incorrect images, the benchmark may not capture all types of cultural failures.

## Foundational Learning

- Concept: Text-to-image generation using diffusion models
  - Why needed here: Understanding the base model architecture (Stable Diffusion) is crucial for implementing the fine-tuning approach
  - Quick check question: What are the key components of a diffusion model and how does the text encoder influence image generation?

- Concept: Cross-modal alignment and embedding spaces
  - Why needed here: The multi-modal metric relies on computing alignments between different modalities using pre-trained models (LaBSE, CLIP, GRiT)
  - Quick check question: How do pre-trained models like CLIP and LaBSE create embedding spaces that enable cross-modal similarity calculations?

- Concept: Cultural bias in AI models and its manifestations
  - Why needed here: Understanding how cultural gaps manifest in generated images is essential for designing appropriate evaluation criteria
  - Quick check question: What are the common ways cultural bias appears in text-to-image generation outputs?

## Architecture Onboarding

- Component map:
  - Original non-English captions → Translation → Multi-modal filtering → Filtered dataset
  - Stable Diffusion base → Fine-tuning on filtered data → Cross-cultural generation
  - C3 benchmark prompts → Image generation → Human evaluation across 6 criteria

- Critical path: Prompt → Translation → Multi-modal filtering → Fine-tuning → Generation → Evaluation

- Design tradeoffs:
  - Using more complex multi-modal filtering increases computational cost but improves data quality
  - Including object detection adds explicit object-text alignment but depends on object detector quality
  - Human evaluation provides comprehensive assessment but is time-consuming and subjective

- Failure signatures:
  - Low correlation between multi-modal metric and human judgment indicates filtering is not capturing translation quality
  - High scores on C3 but poor real-world cultural appropriateness indicates benchmark may not capture all cultural nuances
  - Fine-tuning with filtered data but no improvement suggests translation quality isn't the limiting factor

- First 3 experiments:
  1. Compare LaBSE, CLIP, and combined metric correlation with human judgment on a small translated caption sample
  2. Generate images using vanilla Stable Diffusion on C3 benchmark and evaluate failure types
  3. Fine-tune on a small filtered dataset and compare generation quality against random sampling baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the proposed multi-modal metric compare to human evaluation in selecting high-quality translated captions for fine-tuning across different cultural contexts beyond Chinese culture?
- Basis in paper: [explicit] The paper states that the proposed metric outperforms existing methods in selecting high-quality translated captions for fine-tuning, leading to improved cross-cultural image generation. However, the experiments were conducted specifically on Chinese cultural data.
- Why unresolved: The paper only provides experimental results on Chinese cultural data. It is unclear how the proposed metric would perform on data from other cultural contexts.
- What evidence would resolve it: Conducting experiments using the proposed metric on translated captions from other cultural contexts and comparing the results with human evaluation and existing methods would provide evidence of its effectiveness across different cultures.

### Open Question 2
- Question: How does the proposed multi-modal metric perform in filtering low-quality translated captions when the original captions are in languages other than English?
- Basis in paper: [inferred] The paper proposes a multi-modal metric that considers both textual and visual elements to filter low-quality translated captions. However, the experiments were conducted with English as the target language for translation.
- Why unresolved: The paper does not provide evidence of the metric's performance when the original captions are in languages other than English. It is unclear if the metric's effectiveness is language-dependent.
- What evidence would resolve it: Conducting experiments using the proposed metric to filter low-quality translated captions when the original captions are in languages other than English and comparing the results with existing methods would provide evidence of its language-agnostic performance.

### Open Question 3
- Question: How does the performance of the proposed multi-modal metric in filtering low-quality translated captions change as the size of the fine-tuning dataset increases?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the proposed multi-modal metric in filtering low-quality translated captions for a dataset of 300K instances. However, it is unclear how the metric's performance scales with larger datasets.
- Why unresolved: The paper does not provide evidence of the metric's performance on larger datasets. It is unclear if the metric's effectiveness remains consistent as the dataset size increases.
- What evidence would resolve it: Conducting experiments using the proposed metric to filter low-quality translated captions on datasets of varying sizes and comparing the results with existing methods would provide evidence of its scalability.

## Limitations

- The human evaluation is conducted by a small team of three annotators (all fluent in both English and Chinese), which may introduce systematic biases despite best efforts at inter-annotator agreement
- The C3 benchmark, while comprehensive with 9,889 prompts, only evaluates Chinese cultural elements and may not generalize to other cultures
- The study focuses on a single diffusion model (Stable Diffusion v1-4) without exploring whether results transfer to other architectures

## Confidence

- Multi-modal filtering metric effectiveness: High confidence (supported by strong correlation with human judgment)
- C3 benchmark ability to reveal cultural gaps: Medium confidence (demonstrates gaps but may not capture all cultural nuances)
- Fine-tuning approach improving cross-cultural generation: Medium confidence (shown on C3 but not in broader deployment)

## Next Checks

1. Expand human evaluation to include annotators from diverse cultural backgrounds and linguistic expertise to assess cultural appropriateness more comprehensively
2. Test the fine-tuned models on real-world Chinese social media images to validate generalization beyond the benchmark
3. Apply the same methodology to another non-Western culture (e.g., Arabic or Hindi) to verify the approach's cultural transferability