---
ver: rpa2
title: On minimizing the training set fill distance in machine learning regression
arxiv_id: '2307.10988'
source_url: https://arxiv.org/abs/2307.10988
tags:
- training
- regression
- distance
- fill
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work focuses on minimizing the training set fill distance
  for regression tasks in machine learning. We derive an upper bound for the maximum
  expected prediction error, which is linearly dependent on the training set fill
  distance, conditional to the knowledge of data features.
---

# On minimizing the training set fill distance in machine learning regression

## Quick Facts
- arXiv ID: 2307.10988
- Source URL: https://arxiv.org/abs/2307.10988
- Reference count: 40
- Key outcome: FPS sampling significantly reduces maximum prediction error for regression tasks by minimizing training set fill distance.

## Executive Summary
This work proposes minimizing the training set fill distance as a principled approach to improve regression model robustness. The authors derive an upper bound showing that the maximum expected prediction error is linearly dependent on the fill distance, conditional on data features. They validate this theoretically and empirically using Kernel Ridge Regression (KRR) and Feedforward Neural Networks (FNN) on QM7 and QM9 molecular datasets, demonstrating that Farthest Point Sampling (FPS) significantly outperforms random sampling and other baselines in reducing maximum absolute error.

## Method Summary
The method centers on FPS, which selects training points to minimize the fill distance (maximum distance from any data point to its nearest training point). FPS is implemented greedily with O(|D||LFPS|) complexity and provides a 2-approximation to the NP-hard optimal solution. The approach is tested on QM7 (7,165 molecules, Coulomb matrix features) and QM9 (130,202 molecules, Mordred features) datasets using KRR with Gaussian kernel and FNN with ReLU activations. Training sets of varying sizes are selected using FPS and baseline methods (random, facility location, k-medoids++), and models are evaluated on MAXAE and MAE metrics.

## Key Results
- FPS reduces MAXAE from ~40 eV to ~20 eV for both QM7 and QM9 datasets compared to random sampling
- FPS improves model stability for Gaussian kernel regression by avoiding overfitting to small fill distances
- The maximum prediction error bound is linearly dependent on the fill distance, as proven theoretically

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing training set fill distance directly reduces maximum expected prediction error for Lipschitz continuous regression models
- Mechanism: The theoretical bound shows maximum expected prediction error is linearly dependent on fill distance; FPS minimizes this distance
- Core assumption: Data satisfies Lipschitz continuity assumptions for both error function and model predictions
- Evidence anchors: Theorem 4.4 provides formal bound; abstract states bound is linearly dependent on fill distance

### Mechanism 2
- Claim: FPS preferentially samples isolated or tail points in data distribution, improving model robustness
- Mechanism: FPS selects points farthest apart, naturally including sparse, isolated data points that are harder to predict
- Core assumption: Data distribution has isolated points whose nearest neighbors are far away
- Evidence anchors: Section shows FPS samples isolated molecules even for low training budgets; Figure 3 shows FPS selects points across full density spectrum

### Mechanism 3
- Claim: FPS provides computationally feasible 2-approximation to optimal fill distance minimization
- Mechanism: FPS iteratively selects points to maximize minimum distance to already selected points
- Core assumption: k-center problem is NP-hard, making 2-approximation acceptable and computationally efficient
- Evidence anchors: Section states FPS provides 2-approximation; implementation complexity is O(|D||LFPS|)

## Foundational Learning

- Concept: Lipschitz continuity of regression models and error functions
  - Why needed here: Theoretical bound relies on Lipschitz assumptions to connect fill distance to prediction error
  - Quick check question: How would you verify that both a regression model f and error function l are Lipschitz continuous in their inputs?

- Concept: Fill distance and k-center clustering
  - Why needed here: Fill distance quantifies how well training set covers feature space; minimizing it is equivalent to solving k-center problem
  - Quick check question: What is fill distance of a set of points, and how does it relate to maximum distance from any point to its nearest training point?

- Concept: Coreset selection strategies (random, facility location, k-medoids++)
  - Why needed here: These are baseline sampling methods compared against FPS
  - Quick check question: How do random sampling, facility location, and k-medoids++ differ in their selection criteria and theoretical motivations?

## Architecture Onboarding

- Component map: Data preprocessing → Feature extraction → Sampling strategy (FPS, baselines) → Model training (KRR, FNN) → Evaluation (MAXAE, MAE)
- Critical path: 1) Load and preprocess dataset, 2) Apply sampling strategy to select training set, 3) Train regression model, 4) Evaluate model on held-out data, 5) Repeat for multiple runs and training sizes
- Design tradeoffs: FPS vs. random (better for robustness/MAXAE but potentially worse for average error/MAE), KRR vs. FNN (KRR more data-efficient but FNN scales better), sampling budget (small budgets favor FPS for robustness)
- Failure signatures: MAXAE not decreasing with FPS (Lipschitz assumptions violated or dataset too uniform), FPS runtime too long (dataset too large for complexity), MAE worse with FPS (FPS may miss central dense regions)
- First 3 experiments: 1) Run FPS and random sampling on small synthetic dataset with known isolated points, 2) Compare FPS, facility location, and k-medoids++ on QM7 with KRR for sizes 100, 200, 400, 3) Increase training size until FPS and baselines converge in MAXAE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does minimizing training set fill distance consistently improve model performance across different regression tasks and datasets?
- Basis in paper: Paper demonstrates improvements in MAXAE but not consistently in MAE
- Why unresolved: Paper doesn't provide comprehensive analysis across different metrics or generalization to various tasks
- What evidence would resolve it: Empirical studies evaluating impact on various regression tasks and datasets with multiple performance metrics

### Open Question 2
- Question: What is the optimal trade-off between training set fill distance minimization and model complexity?
- Basis in paper: Paper focuses on fill distance minimization without exploring relationship to model complexity
- Why unresolved: Paper doesn't investigate how model complexity affects optimal trade-off with fill distance minimization
- What evidence would resolve it: Empirical studies comparing generalization performance of models with varying complexity and fill distances

### Open Question 3
- Question: How does choice of distance metric in feature space affect effectiveness of fill distance minimization?
- Basis in paper: Paper assumes L2-distance but mentions results can be generalized to other metrics
- Why unresolved: Paper doesn't analyze how different distance metrics impact fill distance minimization effectiveness
- What evidence would resolve it: Empirical studies comparing performance using different distance metrics

## Limitations
- Theoretical bound relies critically on Lipschitz continuity assumptions that may not hold for all datasets or model architectures
- Computational complexity of FPS (O(|D||LFPS|)) may become prohibitive for extremely large datasets
- Exact hyperparameter settings for KRR and FNN models are not specified, making exact reproduction difficult

## Confidence

- **High Confidence**: Empirical demonstration that FPS reduces MAXAE compared to baselines on QM7 and QM9 datasets
- **Medium Confidence**: Theoretical derivation of bound relating fill distance to maximum expected prediction error
- **Medium Confidence**: Claim that FPS provides 2-approximation to optimal fill distance minimization

## Next Checks

1. Test Lipschitz continuity assumption on synthetic datasets with varying smoothness properties to identify when theoretical bound breaks down
2. Conduct ablation studies varying KRR and FNN hyperparameters to determine their impact on relative performance of FPS versus baselines
3. Evaluate FPS performance on progressively larger molecular datasets to identify point where computational complexity becomes prohibitive