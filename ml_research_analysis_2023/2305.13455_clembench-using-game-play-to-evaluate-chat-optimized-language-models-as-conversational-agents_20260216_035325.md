---
ver: rpa2
title: 'Clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational
  Agents'
arxiv_id: '2305.13455'
source_url: https://arxiv.org/abs/2305.13455
tags:
- game
- guess
- word
- grid
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces clembench, a framework for evaluating chat-optimized\
  \ language models (cLLMs) as conversational agents through game-based interactions.\
  \ The authors implement five dialogue games\u2014taboo, wordle variants, drawing,\
  \ reference, and scorekeeping\u2014to systematically test cLLMs' ability to follow\
  \ rules and perform game objectives."
---

# Clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents

## Quick Facts
- arXiv ID: 2305.13455
- Source URL: https://arxiv.org/abs/2305.13455
- Reference count: 40
- Primary result: Introduces a framework for evaluating chat-optimized LLMs via game-based dialogue interactions, showing newer models consistently outperform older ones.

## Executive Summary
This paper introduces clembench, a framework for evaluating chat-optimized language models (cLLMs) as conversational agents through game-based interactions. The authors implement five dialogue games—taboo, wordle variants, drawing, reference, and scorekeeping—to systematically test cLLMs' ability to follow rules and perform game objectives. Games are played in self-play mode with models like GPT-4, Claude, and Luminous under a Game Master's supervision. Results show that newer models consistently outperform older ones in both rule-following (completion rate) and game performance, with GPT-4 achieving the highest scores (e.g., 60.59 average quality score, 92 F1 in drawing). However, even the best models show substantial room for improvement, with human-level performance assumed to be near the ceiling. The framework is publicly available and designed for extensibility.

## Method Summary
Clembench evaluates chat-optimized LLMs by embedding them in structured, turn-based dialogue games. A Game Master orchestrates self-play, providing prompt templates and validating each move against formal constraints. The framework supports five games—Taboo, Wordle variants, Drawing, Reference, and Scorekeeping—each probing different aspects of situated language understanding. Performance is measured via completion rate (rule-following) and quality scores (game performance). The framework is open-sourced and designed for modular extension with new games.

## Key Results
- Newer models (GPT-4, Claude) outperform older ones in both rule-following and game performance across all five games.
- GPT-4 achieves the highest scores, with 60.59 average quality score and 92 F1 in the Drawing game.
- All models show substantial room for improvement; human-level performance is assumed to be near the ceiling.
- Completion rates and quality scores reliably track the LLM development cycle, reflecting instruction tuning and general language understanding.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clembench evaluates cLLMs' rule-following and game-playing abilities by embedding them in structured dialogue games.
- Mechanism: A Game Master orchestrates self-play by providing prompt templates and validating each move against formal constraints, enabling controlled testing of both instruction compliance and task performance.
- Core assumption: Models can be treated as agents that follow linguistic instructions and maintain turn-based dialogue state.
- Evidence anchors:
  - [abstract] "Can LLMs be evaluated meaningfully by exposing them to constrained game-like settings that are built to challenge specific capabilities?"
  - [section 3.2] "Not all kinds of Dialogue Games in the sense of Schlangen (2023a) can be realised with LLMs as players. For now, the games need to be text-based... and they need to be turn-based..."
  - [corpus] Weak – no direct citation, but the corpus lists similar frameworks (e.g., "clembench-2024", "Orak"), supporting the general idea of game-based LLM evaluation.
- Break condition: If a model fails to follow the prescribed response format or exceeds a set number of reprompts, the game is aborted, halting evaluation for that instance.

### Mechanism 2
- Claim: Different game variants probe distinct aspects of situated language understanding (e.g., multimodal grounding, analogical reasoning, scorekeeping).
- Mechanism: Each game targets a specific subcomponent from Schlangen's situated language understanding model (Figure 2 in the paper), such as discourse model (Taboo), world model (Wordle with clues), or agent model (Private/Shared).
- Core assumption: Games can be mapped to representational/processing demands in situated language understanding.
- Evidence anchors:
  - [abstract] "Our general framework for implementing and evaluating games with LLMs is available at..."
  - [section 4] "All games described here challenge the rule-following capabilities of the players. In all games, the game objectives and the rules, including formal constraints on the game moves, are described verbally to the player."
  - [corpus] Weak – the corpus includes related work on game-based evaluation but does not directly validate the theoretical mapping of games to situated understanding subcomponents.
- Break condition: If a model cannot maintain the required internal state across turns (e.g., track shared vs. private information), performance on that game variant will plateau regardless of model size.

### Mechanism 3
- Claim: Performance differences across games track the LLM development cycle, with newer models outperforming older ones.
- Mechanism: Self-play across five games reveals that newer models (GPT-4, Claude) achieve higher completion rates and quality scores, suggesting better instruction tuning and general language understanding.
- Core assumption: Model improvements in instruction-following generalize across diverse game mechanics.
- Evidence anchors:
  - [abstract] "Both this capability and the quality of the game play, measured by how well the objectives of the different games are met, follows the development cycle, with newer models performing better."
  - [section 5] "The GPT family tends to perform better than the other models we tested, with an increase in quality from 3 to 3.5 to 4."
  - [corpus] Weak – corpus does not directly compare models across games, but does mention benchmarks that track model improvements over time.
- Break condition: If a model achieves near-ceiling performance on all games, the benchmark loses discriminative power and cannot track further improvements.

## Foundational Learning

- Concept: Turn-based self-play with a Game Master.
  - Why needed here: Enables systematic, reproducible testing of cLLMs in dialogue games without human intervention.
  - Quick check question: How does the Game Master enforce game rules, and what happens when a model fails to comply?

- Concept: Prompt templates and response parsing.
  - Why needed here: Ensures consistent input/output formatting so the Game Master can validate moves and maintain game state.
  - Quick check question: What are the consequences of a model producing an invalid response format, and how many reprompts are allowed?

- Concept: Metrics for both rule-following (completion rate) and performance (quality scores).
  - Why needed here: Distinguishes between models that can play by the rules and those that play well, enabling fine-grained evaluation.
  - Quick check question: How are completion rate and quality score computed, and why is it important to track both?

## Architecture Onboarding

- Component map: Clembench consists of a Python framework that handles prompt templates, Game Master logic, model API routing, dataset management, and evaluation. Games are modular plug-ins implementing specific dialogue mechanics.
- Critical path: 1) Load game instance → 2) Generate prompt for Player A → 3) Parse and validate response → 4) Forward to Player B (or Game Master logic) → 5) Repeat until win/lose/abort → 6) Compute metrics.
- Design tradeoffs: Self-play avoids human cost but may not capture human-like strategic behavior; strict rule enforcement ensures reproducibility but may prematurely abort games where models could recover with guidance.
- Failure signatures: High abort rates indicate poor instruction following; low quality scores despite high completion suggest superficial compliance without strategic depth; performance gaps between models reveal differences in instruction tuning and general language understanding.
- First 3 experiments:
  1. Run a simple game (e.g., Taboo) with a small set of instances using GPT-3.5 to confirm basic functionality and prompt parsing.
  2. Test a multimodal game (e.g., Drawing) to verify character-grid input/output handling and incremental state updates.
  3. Compare two models (e.g., GPT-3.5 vs. GPT-4) on the same dataset to observe differences in completion rate and quality, validating the benchmark's discriminative power.

## Open Questions the Paper Calls Out

- Question: How do chat-optimized language models perform in multi-turn dialogue games that require maintaining and updating a shared knowledge base?
  - Basis in paper: [inferred] The paper mentions that current models show substantial room for improvement in scorekeeping games and that human-level performance would be near the ceiling.
  - Why unresolved: The paper only tested models in self-play mode with two players, not in actual human-machine interactions where knowledge bases need to be maintained across multiple turns.
  - What evidence would resolve it: Experiments comparing model performance in self-play vs human-machine interactions in multi-turn dialogue games with complex knowledge bases.

- Question: Can chat-optimized language models generalize their game-playing abilities to novel game types not seen during training?
  - Basis in paper: [inferred] The paper discusses the need for models to follow rules and perform game objectives but does not test their ability to learn new games.
  - Why unresolved: The paper only tested models on a fixed set of implemented games, not their ability to learn and play entirely new games.
  - What evidence would resolve it: Experiments testing models' ability to learn and play novel games after being given only a brief description of the rules.

- Question: How do different model architectures (e.g., transformer vs. other) impact performance in dialogue games that require multimodal grounding?
  - Basis in paper: [explicit] The paper mentions that the drawing and reference games require multimodal grounding but only tests transformer-based models.
  - Why unresolved: The paper does not compare different model architectures, only different versions of transformer-based models.
  - What evidence would resolve it: Experiments comparing performance of different model architectures on the same set of dialogue games.

## Limitations

- Reproducibility is limited by incomplete specification of prompt templates and exact game configurations.
- Self-play methodology may not capture human-like conversational dynamics or strategic behavior.
- Theoretical mapping of games to situated language understanding subcomponents lacks direct empirical validation.

## Confidence

- **High confidence**: The general framework design, self-play methodology, and the observed trend that newer models outperform older ones are well-supported by the reported results and framework architecture.
- **Medium confidence**: The claim that Clembench effectively evaluates cLLMs as conversational agents is supported, but the lack of full specification for prompts and datasets introduces some uncertainty about exact reproducibility.
- **Medium confidence**: The mapping of games to specific aspects of situated language understanding is theoretically grounded but lacks direct empirical validation within the paper.

## Next Checks

1. **Reproduce baseline results**: Run the Clembench framework with the provided codebase using a standard model (e.g., GPT-3.5) on a small subset of game instances to verify basic functionality and metric computation matches the reported patterns.

2. **Validate prompt sensitivity**: Systematically vary prompt templates and model parameters (e.g., temperature, max tokens) for a single game (e.g., Taboo) to assess how sensitive completion rates and quality scores are to these factors.

3. **Compare self-play vs. human-play**: For a subset of game instances, conduct a small-scale human vs. model comparison to evaluate whether self-play results are representative of human-level performance and to identify any systematic biases introduced by the self-play setup.