---
ver: rpa2
title: Label Propagation Techniques for Artifact Detection in Imbalanced Classes using
  Photoplethysmogram Signals
arxiv_id: '2308.08480'
source_url: https://arxiv.org/abs/2308.08480
tags:
- data
- algorithm
- artifacts
- pulse
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the application of label propagation techniques
  to detect artifacts in photoplethysmogram (PPG) signals, particularly in scenarios
  with imbalanced class distributions where clean PPG samples are significantly outnumbered
  by artifact-contaminated samples. The research compared the performance of supervised
  classifiers (including conventional classifiers and neural networks) with the semi-supervised
  Label Propagation (LP) algorithm for artifact classification in PPG signals.
---

# Label Propagation Techniques for Artifact Detection in Imbalanced Classes using Photoplethysmogram Signals

## Quick Facts
- arXiv ID: 2308.08480
- Source URL: https://arxiv.org/abs/2308.08480
- Reference count: 34
- Key outcome: Label Propagation achieves 91% precision, 90% recall, and 90% F1 score for artifact detection in imbalanced PPG datasets

## Executive Summary
This study investigates the application of label propagation techniques for detecting artifacts in photoplethysmogram (PPG) signals, particularly in scenarios with imbalanced class distributions where clean PPG samples are significantly outnumbered by artifact-contaminated samples. The research compares supervised classifiers with the semi-supervised Label Propagation (LP) algorithm for artifact classification in PPG signals from pediatric intensive care settings. The LP algorithm demonstrates superior performance in detecting artifacts when clean samples are rare, achieving strong precision and recall metrics even with limited expert annotations.

## Method Summary
The study preprocesses PPG signals through bandpass filtering (0.5-5 Hz), pulse segmentation, resampling to 256 samples per pulse, and normalization. Expert labeling is performed on 5% of the dataset, which is then used to train and evaluate the Label Propagation algorithm with a KNN kernel. SMOTE oversampling is applied to balance the imbalanced classes. The LP algorithm is compared against traditional classifiers (KNN, SVM, Decision Tree) and neural networks (MLP, Transformers, FCN) using precision, recall, F1-score, and AUROC as evaluation metrics.

## Key Results
- Label Propagation achieves 91% precision, 90% recall, and 90% F1 score for the "artifacts" class
- KNN supervised model shows 89% precision, 95% recall, and 92% F1 score
- LP algorithm outperforms traditional classifiers in detecting artifacts in imbalanced pediatric ICU data
- AUROC of 0.97 achieved by the Label Propagation approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Label Propagation works effectively for artifact detection in imbalanced PPG datasets because it propagates labels through similarity-based neighborhoods, allowing rare artifact examples to influence nearby clean samples.
- Mechanism: The algorithm creates a graph where pulses are nodes weighted by similarity. Clean pulses surrounded by artifact pulses inherit artifact labels through iterative label propagation, even when artifacts are scarce.
- Core assumption: Nearby pulses in feature space share similar artifact/no-artifact status, and the similarity graph accurately captures these relationships.
- Evidence anchors: [abstract] "the LP algorithm achieves a precision of 91%, a recall of 90%, and an F1 score of 90% for the 'artifacts' class"; [section II-C4] "The Label Propagation algorithm is a machine learning technique used for semi-supervised learning tasks. It involves propagating labels or class information from labeled data points to unlabeled data points in a graph or network"

### Mechanism 2
- Claim: Oversampling minority classes (SMOTE) improves classification performance by creating synthetic artifact examples that help classifiers learn artifact patterns better.
- Mechanism: SMOTE generates synthetic artifact examples by interpolating between existing artifact samples and their nearest neighbors, creating a more balanced training distribution.
- Core assumption: The synthetic examples generated by SMOTE preserve the essential characteristics of artifact pulses while expanding the feature space coverage.
- Evidence anchors: [section II-C3] "SMOTE is the best oversampling method. SMOTE selects a minority class instance and identifies its k-nearest neighbors in the feature space. It then creates new synthetic examples along the line segments connecting the selected instance and its neighbors"; [section IV] "SMOTE is the best oversampling method"

### Mechanism 3
- Claim: Traditional classifiers like KNN outperform complex neural networks on small, imbalanced medical datasets because they make fewer assumptions and don't overfit.
- Mechanism: KNN relies on local neighborhood information and distance metrics, which works well when the dataset is limited and features are well-defined, avoiding the overfitting risk of deeper architectures.
- Core assumption: The feature space captures sufficient information for distance-based classification, and the dataset size doesn't support complex model architectures.
- Evidence anchors: [abstract] "the KNN supervised model demonstrated good results with a precision of 89%, a recall of 95% and an F1 score of 92%"; [section IV] "KNN is the classifier that gives the best result" and outperforms neural networks like MLP and Transformers

## Foundational Learning

- Concept: Semi-supervised learning
  - Why needed here: Only 5-10% of the dataset is expert-annotated, requiring algorithms that can propagate labels from limited labeled data to the majority unlabeled data
  - Quick check question: What distinguishes semi-supervised learning from purely supervised or unsupervised approaches in the context of medical signal annotation?

- Concept: Imbalanced class handling
  - Why needed here: The dataset contains ~80% clean pulses and only ~20% artifact-contaminated pulses, which would bias classifiers toward the majority class without rebalancing
  - Quick check question: How do different resampling strategies (oversampling vs undersampling) affect classifier performance on imbalanced medical datasets?

- Concept: Signal preprocessing and feature extraction
  - Why needed here: Raw PPG signals contain noise and artifacts that must be filtered and segmented into individual pulses before classification can be effective
  - Quick check question: Why is temporal feature extraction (256 samples per pulse) preferred over frequency-domain features for this specific artifact detection task?

## Architecture Onboarding

- Component map: Raw PPG signal -> Preprocessing (filtering, segmentation, resampling, normalization) -> Feature extraction -> Sampling method -> Label Propagation (semi-supervised) OR Traditional classifiers (KNN, SVM, etc.) OR Neural networks (MLP, Transformers, FCN) -> Evaluation (precision, recall, F1, AUROC)
- Critical path: Raw PPG signal -> Preprocessing -> Feature extraction -> Classifier training/testing -> Performance evaluation
- Design tradeoffs: Simplicity and interpretability (traditional classifiers) vs. potential performance gains (neural networks) vs. label efficiency (semi-supervised approaches)
- Failure signatures: Low precision/recall on artifact class indicates poor feature representation or sampling issues; high false positives suggest the model confuses artifacts with normal signal variations
- First 3 experiments:
  1. Baseline: Test KNN with default parameters on preprocessed data without sampling to establish performance baseline
  2. Sampling comparison: Compare KNN performance with different sampling methods (SMOTE, ADASYN, ROS, RUS) to find optimal class balance
  3. Semi-supervised validation: Test Label Propagation with different annotation proportions (2.5%, 5%, 7.5%, 10%) to find optimal trade-off between annotation effort and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Label Propagation algorithm maintain consistent performance across different patient age groups in the pediatric intensive care unit?
- Basis in paper: [inferred] The paper focuses on pediatric data but does not analyze performance across age subgroups.
- Why unresolved: The study uses a single pediatric dataset without age-stratified evaluation.
- What evidence would resolve it: Age-stratified performance metrics for the Label Propagation algorithm across different pediatric age groups.

### Open Question 2
- Question: How does the Label Propagation algorithm perform when applied to PPG signals from non-pediatric intensive care units or general hospital settings?
- Basis in paper: [inferred] The algorithm was only validated on pediatric ICU data, limiting generalizability.
- Why unresolved: The study is restricted to a single pediatric ICU database without cross-institutional validation.
- What evidence would resolve it: Performance comparison of the algorithm on PPG data from adult ICUs and non-critical care settings.

### Open Question 3
- Question: What is the impact of different motion artifact types (e.g., periodic vs. aperiodic) on the algorithm's classification accuracy?
- Basis in paper: [explicit] The paper mentions motion artifacts cause irregular high-amplitude fluctuations but doesn't analyze artifact type-specific performance.
- Why unresolved: The study treats all artifacts as a single class without characterizing different motion artifact patterns.
- What evidence would resolve it: Detailed analysis of classification performance for different motion artifact patterns and their characteristics.

## Limitations
- Evaluation based on single pediatric ICU dataset limits external validity
- Performance may not generalize to different signal characteristics or noise patterns
- Limited comparison with advanced neural architectures optimized for time-series data

## Confidence
- High: Performance metrics of LP algorithm (91% precision, 90% recall, 90% F1) on the tested dataset
- Medium: Generalization of results to other clinical settings and patient populations
- Medium: Superiority of semi-supervised approach over supervised methods in all imbalanced scenarios

## Next Checks
1. Test the LP algorithm on external PPG datasets from different clinical environments to verify cross-domain performance
2. Compare performance with more advanced neural architectures specifically designed for time-series (e.g., 1D CNNs, LSTMs) using the same dataset
3. Conduct ablation studies to determine the contribution of each preprocessing step (filtering, segmentation, normalization) to final performance