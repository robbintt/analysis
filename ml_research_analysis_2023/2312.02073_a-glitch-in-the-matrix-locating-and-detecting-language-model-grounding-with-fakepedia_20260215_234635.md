---
ver: rpa2
title: A Glitch in the Matrix? Locating and Detecting Language Model Grounding with
  Fakepedia
arxiv_id: '2312.02073'
source_url: https://arxiv.org/abs/2312.02073
tags:
- factual
- grounding
- knowledge
- computational
- grounded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the mechanisms underlying large language model
  (LLM) grounding, particularly when contextual information conflicts with stored
  parametric knowledge. The authors introduce Fakepedia, a novel dataset of counterfactual
  texts designed to isolate grounding abilities.
---

# A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia

## Quick Facts
- arXiv ID: 2312.02073
- Source URL: https://arxiv.org/abs/2312.02073
- Reference count: 12
- Key outcome: Introduces Fakepedia dataset and MGCT method to detect grounding vs factual recall in LLMs with 92.8% accuracy

## Executive Summary
This work studies the mechanisms underlying large language model (LLM) grounding, particularly when contextual information conflicts with stored parametric knowledge. The authors introduce Fakepedia, a novel dataset of counterfactual texts designed to isolate grounding abilities. They benchmark various LLMs on Fakepedia and find that GPT-4-turbo prefers parametric knowledge, while Mistral-7B most robustly chooses grounded answers. Using their Masked Grouped Causal Tracing (MGCT) method, they analyze LLM components when answering Fakepedia queries. They identify distinct computational patterns between grounded and ungrounded responses, and demonstrate that inspection of the computational graph alone can predict LLM grounding with 92.8% accuracy.

## Method Summary
The authors generate the Fakepedia dataset by creating counterfactual paragraphs that logically imply false facts, forcing models to choose between contextual grounding and parametric knowledge. They benchmark LLMs on this dataset using various prompt schemes to measure grounding accuracy. The MGCT method extends causal mediation analysis to LLMs by corrupting model states with special tokens and measuring how different components (attention heads, MLPs) affect predictions when restored. They train an XGBoost classifier on MGCT features to automatically predict whether responses are grounded or rely on factual recall.

## Key Results
- GPT-4-turbo strongly prefers parametric knowledge over contextual grounding, while Mistral-7B shows the most robust grounding behavior
- MGCT analysis reveals that MLP effects on the last subject token are highly predictive of ungrounded (factual recall) responses
- Grounding shows distributed computational patterns across multiple components, while factual recall concentrates in specific MLPs and attention heads
- XGBoost classifier trained on MGCT features achieves 92.8% accuracy in predicting whether LLM responses are grounded

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4-turbo's strong preference for parametric knowledge over contextual grounding is detectable through masked grouped causal tracing (MGCT) by observing high MLP effects on the last subject token.
- Mechanism: When GPT-4-turbo encounters conflicting information, it retrieves stored factual knowledge from MLP parameters rather than integrating context. MGCT reveals this by showing that MLP activations on the last subject token have high indirect effects on ungrounded predictions.
- Core assumption: MLPs function as distributed key-value memory stores for factual knowledge, and their activation patterns can distinguish grounded from ungrounded responses.
- Evidence anchors:
  - [abstract] "they identify distinct computational patterns between grounded and ungrounded responses, and demonstrate that inspection of the computational graph alone can predict LLM grounding with 92.8% accuracy"
  - [section 6.2] "it is clear that the MLPs' activations, particularly on the last subject token, have an high effect when producing ungrounded answers"
  - [corpus] Weak evidence - corpus contains no directly relevant papers about MLP grounding mechanisms
- Break condition: If MLPs do not serve as factual knowledge repositories, or if grounding relies on attention mechanisms instead, this detection method would fail.

### Mechanism 2
- Claim: Grounding in LLMs is a distributed process without clear localization, unlike factual recall which concentrates in specific MLPs and attention heads.
- Mechanism: When models ground answers in context, the computational effects are spread across multiple token positions and layers, whereas factual recall shows concentrated effects in specific MLPs and attention heads at the last subject token.
- Core assumption: The distributed nature of grounding can be detected through MGCT by observing that no single component has high predictive power for grounded responses.
- Evidence anchors:
  - [abstract] "grounding is a distributed process without clear localization"
  - [section 6.2] "contrary to factual recall, grounding is a distributed process without a clear localization"
  - [corpus] Weak evidence - corpus contains no directly relevant papers about distributed grounding processes
- Break condition: If grounding actually localizes to specific components under certain conditions, this mechanism would break.

### Mechanism 3
- Claim: The Fakepedia dataset successfully isolates grounding behavior by creating counterfactual scenarios where the grounded answer is always non-factual, forcing models to rely on context rather than parametric knowledge.
- Mechanism: By constructing paragraphs that logically imply counterfactual triplets, Fakepedia creates situations where the only correct answer (given the context) contradicts the model's stored knowledge, allowing researchers to distinguish between grounded and ungrounded responses.
- Core assumption: The dataset construction method (using GPT-3.5-turbo to generate counterfactual paragraphs with subsequent filtering) produces high-quality instances where context clearly implies the target counterfactual.
- Evidence anchors:
  - [section 4.2] "We perform a manual annotation on 100 randomly sampled paragraphs assessing whether the text produced by gpt-3.5-turbo correctly implies the counterfactual triplet while not implying the factual triplet from which the counterfactual triplet is derived. We found an accuracy of 96%"
  - [section 5] "Fakepedia-MH, which necessitates reasoning about information within the context, poses a greater challenge for models overall"
  - [corpus] Weak evidence - corpus contains no directly relevant papers about counterfactual grounding datasets
- Break condition: If the generated paragraphs fail to clearly imply the counterfactual triplets, or if models can guess correctly without true grounding, the isolation would fail.

## Foundational Learning

- Concept: Causal mediation analysis
  - Why needed here: To distinguish between statistical correlations and true causal mechanisms underlying grounded vs. ungrounded responses
  - Quick check question: What is the difference between measuring P(o|intervention) versus P(o|observation) in the context of identifying grounding mechanisms?

- Concept: Counterfactual reasoning in NLP
  - Why needed here: The entire experimental setup relies on creating counterfactual scenarios to isolate grounding behavior from factual recall
  - Quick check question: How does a counterfactual paragraph differ from a contradictory paragraph in terms of the reasoning required to answer correctly?

- Concept: Transformer architecture components (attention heads, MLPs, residual connections)
  - Why needed here: MGCT analysis requires understanding how different components contribute to predictions and how interventions affect each component type differently
  - Quick check question: Why does MGCT restore full columns (all layers for a specific token) rather than individual states when measuring effects?

## Architecture Onboarding

- Component map: Fakepedia generation pipeline -> MGCT implementation -> XGBoost classifier
- Critical path: 1) Generate counterfactual paragraphs using GPT-3.5-turbo with filtering, 2) Run clean, corrupted, and restored passes through target LLM using MGCT, 3) Aggregate MGCT effects by token position and component type, 4) Train XGBoost classifier on MGCT features to predict grounded vs. ungrounded responses
- Design tradeoffs: The choice of EOS token for corruption versus embedding noise trades robustness for interpretability. Using full column restoration trades granularity for efficiency. The Fakepedia generation trades manual curation for scalability.
- Failure signatures: If MLPs don't show strong effects for ungrounded responses, if attention heads dominate the effects instead, or if grounding shows clear localization like factual recall, the core hypothesis fails. If XGBoost accuracy drops below 80%, the computational patterns may not be predictive.
- First 3 experiments:
  1. Run MGCT on GPT2-XL with Fakepedia-base and verify that MLP effects on last subject token are significantly higher for ungrounded responses (t-test p < 0.01)
  2. Train XGBoost classifier on GPT2-XL MGCT features and verify accuracy exceeds 90% on held-out test set
  3. Apply trained classifier to LLaMA-7B MGCT outputs and verify it predicts grounded/ungrounded with similar accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific computational patterns distinguish grounding from factual recall in LLMs, beyond the general observation that grounding is more distributed?
- Basis in paper: [explicit] The paper identifies that grounding is a distributed process without clear localization, unlike factual recall which relies on specific MLPs and attention heads. It also notes that MLP effects on the last subject token are predictive of ungrounded answers.
- Why unresolved: While the paper shows that grounding is more distributed, it does not provide a detailed account of the specific computational patterns or information flow mechanisms involved in grounding compared to factual recall.
- What evidence would resolve it: Detailed causal mediation analysis comparing the flow of information through different components during grounding versus factual recall tasks, identifying specific patterns of activation or interaction between components.

### Open Question 2
- Question: How does the frequency of entities in the training data influence the model's decision between using contextual information or factual recall?
- Basis in paper: [inferred] The paper mentions that entity frequency influences the model's choice between contextual information and factual recall, referencing related work by Razeghi et al. (2022) and others, but does not explore this mechanism in detail.
- Why unresolved: The paper does not investigate how entity frequency is encoded in the model or how it affects the computational process of deciding between grounding and factual recall.
- What evidence would resolve it: Experiments manipulating entity frequency in training data and analyzing changes in grounding vs. recall behavior, along with examination of how frequency information is represented in model activations.

### Open Question 3
- Question: What determines whether an LLM engages in factual recall or grounding when both processes are possible?
- Basis in paper: [explicit] The paper discusses the tension between grounding and factual recall, noting that larger models tend to prefer parametric knowledge, but does not identify the specific factors that determine which process is engaged.
- Why unresolved: The paper does not provide a clear model or set of factors that predict when a model will choose grounding over factual recall or vice versa.
- What evidence would resolve it: A predictive model or set of rules that can accurately determine, based on model architecture, training data, prompt characteristics, and entity properties, whether the model will engage in grounding or factual recall in a given scenario.

## Limitations
- Dataset construction validity: While 96% manual annotation accuracy is high, the study relies heavily on GPT-3.5-turbo's ability to create truly counterfactual scenarios that force grounding behavior
- Generalization of computational patterns: MGCT accuracy of 92.8% is measured within the same model family, with cross-model generalization showing lower accuracy
- Causal inference validity: The grouping strategy may obscure important granular patterns, and the tradeoff between coarse-grained and fine-grained analysis needs more validation

## Confidence
- High Confidence: The Fakepedia dataset construction methodology and its effectiveness in creating counterfactual scenarios
- Medium Confidence: The claim that GPT-4-turbo prefers parametric knowledge while Mistral-7B robustly chooses grounded answers
- Low Confidence: The universal claim about grounding being a "distributed process without clear localization" versus factual recall showing "concentrated effects"

## Next Checks
1. Apply MGCT analysis to at least three additional model families (different transformer architectures, different training paradigms) to verify whether computational patterns generalize
2. Test MGCT with different corruption strategies - varying granularity from token-level to layer-level - to determine if identified patterns are artifacts of the specific restoration approach
3. Perform controlled study where Fakepedia dataset is systematically degraded to measure sensitivity of MGCT signatures to dataset quality