---
ver: rpa2
title: Towards Last-layer Retraining for Group Robustness with Fewer Annotations
arxiv_id: '2309.08534'
source_url: https://arxiv.org/abs/2309.08534
tags:
- dataset
- group
- retraining
- worst-group
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of last-layer retraining
  for group robustness when group annotations are limited. The authors show that class-balanced
  last-layer retraining can substantially improve worst-group accuracy even with minimal
  worst-group data in the reweighting set.
---

# Towards Last-layer Retraining for Group Robustness with Fewer Annotations

## Quick Facts
- arXiv ID: 2309.08534
- Source URL: https://arxiv.org/abs/2309.08534
- Reference count: 40
- This paper investigates the effectiveness of last-layer retraining for group robustness when group annotations are limited.

## Executive Summary
This paper explores last-layer retraining as a technique to improve group robustness in neural networks when group annotations are scarce. The authors demonstrate that class-balanced last-layer retraining can significantly improve worst-group accuracy even with minimal worst-group data in the reweighting set. They introduce Selective Last-Layer Finetuning (SELF), a method that uses model disagreements or misclassifications to construct more group-balanced reweighting sets. SELF achieves near state-of-the-art results while requiring less than 3% of held-out class annotations. The findings suggest that last-layer retraining can provide substantial improvements in group robustness without requiring additional data or annotations beyond standard empirical risk minimization.

## Method Summary
The paper investigates last-layer retraining for group robustness, focusing on scenarios with limited group annotations. The approach involves training an initial ERM model on the full dataset, then holding out a portion of data for class-balanced last-layer retraining. The authors propose SELF (Selective Last-Layer Finetuning), which selects points based on model disagreements or misclassifications to construct a more group-balanced reweighting set. SELF can use early-stop disagreement, dropout disagreement, or misclassification-based selection methods. The last layer is then finetuned on the selected, class-balanced subset. The method is evaluated on four benchmark datasets (Waterbirds, CelebA, CivilComments, MultiNLI) using ResNet-50 for vision tasks and BERT for language tasks.

## Key Results
- Class-balanced last-layer retraining substantially improves worst-group accuracy even when the reweighting dataset has only a small proportion of worst-group data.
- SELF achieves near state-of-the-art results with less than 3% of held-out class annotations.
- Model disagreement effectively upsamples worst-group data, as shown through both empirical and theoretical analysis.
- Splitting the dataset and performing last-layer retraining improves worst-group accuracy on Waterbirds, CelebA, and CivilComments compared to using the entire dataset for ERM.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Class-balanced last-layer retraining improves worst-group accuracy even with minimal worst-group data in the reweighting set.
- Mechanism: By training the last layer on a class-balanced subset of the held-out data, the model learns to upweight core features that correlate with the true label across all groups, while downweighting spurious features that only correlate with the label in majority groups.
- Core assumption: The learned features from ERM training contain core features that are predictive of the true label for all groups, even if the model overweights spurious features in the last layer.
- Evidence anchors:
  - [abstract] "last-layer retraining can greatly improve worst-group accuracy even when the reweighting dataset has only a small proportion of worst-group data"
  - [section 4.1] "last-layer retraining can substantially improve worst-group accuracy even when the reweighting dataset has only a small proportion of worst group data"
  - [corpus] "Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious Correlations?" (weak evidence - only mentions title similarity)
- Break condition: If the ERM model fails to learn any core features that correlate with the true label across all groups, class-balanced last-layer retraining will not improve worst-group accuracy.

### Mechanism 2
- Claim: Selective last-layer finetuning (SELF) using model disagreements or misclassifications upsamples worst-group data in the reweighting set.
- Mechanism: By selecting points that are disagreed upon by the ERM model and a regularized model (early-stopped or dropout), SELF constructs a reweighting set that contains a higher proportion of worst-group data, which improves worst-group accuracy when the last layer is finetuned on this set.
- Core assumption: Model disagreements or misclassifications are more likely to occur on worst-group data, allowing SELF to effectively upsample these points.
- Evidence anchors:
  - [abstract] "Our empirical and theoretical results present the first evidence that model disagreement upsamples worst-group data"
  - [section 5.2] "Figure 3 should be compared with Table 4. For example, misclassificationSELF samples too much worst-group data on CivilComments while early-stop disagreement samples a near-balanced amount"
  - [corpus] "Bias Amplification Enhances Minority Group Performance" (weak evidence - only mentions title similarity)
- Break condition: If model disagreements or misclassifications do not preferentially occur on worst-group data, SELF will not effectively upsample these points and improve worst-group accuracy.

### Mechanism 3
- Claim: Holding out a subset of the training data for class-balanced last-layer retraining can improve worst-group accuracy compared to using the entire dataset for ERM.
- Mechanism: By reducing the amount of data used for ERM training and using a held-out subset for class-balanced last-layer retraining, the model is less likely to overfit to spurious correlations in the full training set, leading to improved worst-group accuracy.
- Core assumption: The performance of ERM on the first split is limited by dataset bias rather than sample variance, and holding out 5% of data does not significantly degrade ERM performance.
- Evidence anchors:
  - [section 4.2] "we believe this method is especially relevant to practitioners, and it can be easily implemented with little change to data processing or model training workflows"
  - [section 4.2] "Figure 2 indicates that splitting the dataset and performing last-layer retraining substantially improves worst-group accuracy on Waterbirds, CelebA, and CivilComments"
  - [corpus] No relevant evidence found
- Break condition: If ERM performance is not stable when holding out 5% of data, or if the held-out subset is not representative of the full training set, holding out data for last-layer retraining may not improve worst-group accuracy.

## Foundational Learning

- Concept: Spurious correlations and minority groups
  - Why needed here: The paper focuses on improving group robustness by addressing the reliance on spurious correlations that create underrepresented minority groups in the training data.
  - Quick check question: What is a spurious correlation, and how does it lead to poor generalization on minority groups?

- Concept: Empirical risk minimization (ERM) and its limitations
  - Why needed here: ERM is the standard training procedure for neural networks, but it tends to overfit to spurious correlations and generalize poorly on minority groups, which is the problem the paper aims to address.
  - Quick check question: How does ERM training lead to over-reliance on spurious correlations, and what are the consequences for group robustness?

- Concept: Last-layer retraining and feature reweighting
  - Why needed here: The paper proposes last-layer retraining as a solution to improve group robustness, based on the hypothesis that ERM models learn core features that correlate with the true label across all groups, but overweight spurious features in the last layer.
  - Quick check question: What is the deep feature reweighting (DFR) technique, and how does last-layer retraining improve group robustness?

## Architecture Onboarding

- Component map:
  - ERM model (trained on the full training set or a subset)
  - Held-out dataset (used for class-balanced last-layer retraining or SELF)
  - Last layer retraining (trained on a class-balanced subset of the held-out data)
  - SELF (selective last-layer finetuning using model disagreements or misclassifications)
  - Validation set (used for model selection and hyperparameter tuning)

- Critical path:
  1. Train an ERM model on the full training set or a subset
  2. Split the validation set in half: use one half for held-out data and the other half for model selection
  3. For class-balanced last-layer retraining:
     a. Select a class-balanced subset of the held-out data
     b. Retrain the last layer of the ERM model on this subset
  4. For SELF:
     a. Use the ERM model and a regularized model (early-stopped or dropout) to select points with high disagreement or misclassification
     b. Request class annotations for the selected points
     c. Finetune the last layer of the ERM model on the class-balanced set of selected points
  5. Evaluate the model on the test set and report worst-group accuracy

- Design tradeoffs:
  - Using the full training set for ERM vs. holding out a subset for last-layer retraining
  - Class-balanced last-layer retraining vs. SELF (which requires fewer group annotations but may have lower performance)
  - Early-stop disagreement vs. dropout disagreement in SELF (early-stop disagreement performs better but requires access to the training data)

- Failure signatures:
  - Low worst-group accuracy despite using class-balanced last-layer retraining or SELF
  - High variance in worst-group accuracy across multiple runs
  - Overfitting to the held-out data during last-layer retraining or SELF finetuning

- First 3 experiments:
  1. Compare class-unbalanced ERM to class-balanced ERM on the full training set to verify the importance of class balancing during ERM training.
  2. Compare class-balanced last-layer retraining to DFR on the held-out data to verify the effectiveness of class balancing in the reweighting set.
  3. Compare different SELF variants (misclassification, early-stop misclassification, dropout disagreement, early-stop disagreement) to identify the best approach for upsampling worst-group data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy in general for obtaining the regularized model in disagreement SELF (dropout, early-stopping, or a different technique) and why?
- Basis in paper: Explicit - The paper discusses different techniques for obtaining the regularized model in disagreement SELF, including dropout and early-stopping, but does not definitively conclude which is optimal.
- Why unresolved: The paper shows that early-stop disagreement SELF performs the best overall, but it does not provide a theoretical explanation for why this method is superior to others.
- What evidence would resolve it: A theoretical analysis comparing the effectiveness of different regularization techniques in upsampling worst-group data in the context of disagreement SELF.

### Open Question 2
- Question: To what extent does the precise data selected matter for reweighting, e.g., which of disagreement SELF and misclassification SELF would perform better when hyperparameters are set to equalize group balance?
- Basis in paper: Inferred - The paper suggests that the specific data selected for reweighting might contribute to SELF results beyond group balance, as disagreement SELF often outperforms misclassification SELF despite having access to less information.
- Why unresolved: The paper does not provide a detailed comparison of the performance of disagreement SELF and misclassification SELF when hyperparameters are adjusted to equalize group balance.
- What evidence would resolve it: Empirical results comparing the performance of disagreement SELF and misclassification SELF with equalized group balance across various datasets and settings.

### Open Question 3
- Question: Why does last-layer retraining on a held-out split of the training dataset improve group robustness?
- Basis in paper: Explicit - The paper observes that splitting the dataset and performing last-layer retraining improves worst-group accuracy on some datasets, but does not provide a clear explanation for this phenomenon.
- Why unresolved: The paper notes that it is counterintuitive that reducing the quantity of data used for ERM and performing last-layer retraining would increase worst-group accuracy, and leaves this as an open question for future research.
- What evidence would resolve it: A theoretical analysis or empirical study that elucidates the mechanism by which last-layer retraining on a held-out split improves group robustness, potentially involving the interplay between ERM and the characteristics of the reweighting dataset.

## Limitations

- The effectiveness of model disagreement-based sampling (SELF) is heavily dependent on the quality of the initial ERM model and the specific regularization techniques used.
- The computational overhead of evaluating model disagreements for every data point may be prohibitive for large-scale applications.
- The theoretical analysis relies on simplified linear model assumptions that may not fully capture the complexity of deep neural networks in practice.

## Confidence

- **High Confidence**: The core claim that class-balanced last-layer retraining improves worst-group accuracy with minimal worst-group data is well-supported by extensive empirical results across four benchmark datasets. The ablation studies and comparison with DFR provide robust evidence.

- **Medium Confidence**: The effectiveness of SELF methods for upsampling worst-group data through model disagreements shows promising results but requires further theoretical justification. The claim that early-stop disagreement outperforms other disagreement measures needs more rigorous validation across diverse datasets.

- **Low Confidence**: The theoretical guarantees regarding the number of class annotations needed for different disagreement measures are based on linear model analysis that may not generalize to complex neural networks. The claim about "free lunch" for group robustness may be overstated given the need for held-out data and potential computational costs.

## Next Checks

1. Conduct controlled experiments to isolate the impact of model disagreement versus other factors (e.g., regularization strength, early stopping criteria) on worst-group data sampling effectiveness.

2. Evaluate the robustness of SELF methods when the initial ERM model has poor performance or when the spurious correlations are particularly strong, to test the limits of the approach.

3. Extend the theoretical analysis to include non-linear models and investigate the conditions under which model disagreement-based sampling consistently upsamples worst-group data across different architectures and datasets.