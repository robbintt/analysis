---
ver: rpa2
title: The Obscure Limitation of Modular Multilingual Language Models
arxiv_id: '2311.12375'
source_url: https://arxiv.org/abs/2311.12375
tags:
- language
- mlms
- modular
- languages
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper exposes the limitation of modular multilingual language
  models (MLMs) in multilingual inference scenarios with unknown languages. Existing
  evaluations of modular MLMs exclude the involvement of language identification (LID)
  modules, which obscures the performance of real-case multilingual scenarios.
---

# The Obscure Limitation of Modular Multilingual Language Models

## Quick Facts
- arXiv ID: 2311.12375
- Source URL: https://arxiv.org/abs/2311.12375
- Reference count: 13
- Key outcome: Incorporating LID models into modular MLMs causes 7-8% accuracy drop compared to direct fine-tuning

## Executive Summary
This paper exposes a critical limitation in modular multilingual language models (MLMs) when deployed in real-world scenarios with unknown languages. While existing evaluations of modular MLMs typically exclude language identification (LID) modules, this work demonstrates that incorporating off-the-shelf LID models significantly degrades performance. The authors evaluate 5 LID models on the MASSIVE dataset and find that the performance gap stems from distribution shift, label mismatch, and linguistic challenges like code-mixing, suggesting that modular MLMs require careful consideration of the complete inference pipeline including language identification.

## Method Summary
The authors evaluate 5 off-the-shelf LID models (FastText, CLD3, CLD2, langid.py, LangDetect) on the MASSIVE dataset (52 typologically-diverse languages) and select FastText and CLD3 for further evaluation. They compare MAD-X with mBERT backbone using LID-based adapter selection against direct fine-tuned MLMs and MAD-X without LID, measuring accuracy across 24 languages from three resource groups (HRL, MRL, LRL).

## Key Results
- Incorporating LID models causes 7-8% accuracy decay in modular MLM performance compared to direct fine-tuning
- LID accuracy varies significantly across high, medium, and low resource languages
- Distribution shift, label mismatch, and linguistic phenomena (code-mixing, creole languages) are identified as potential causes of performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Off-the-shelf LID models degrade modular MLM performance because they were not trained on the same data distribution as the modular MLM.
- Mechanism: The LID models are trained on general web data or different datasets, while modular MLMs are fine-tuned on specific task data (MASSIVE). This domain shift causes misclassifications, leading to wrong language adapter selection.
- Core assumption: The performance drop is primarily due to distribution mismatch between LID training and MLM inference data.
- Evidence anchors:
  - [abstract] "distribution shift of LIDs caused by domain and time differences"
  - [section] "Several potential limitations that might occur include: 1) distribution shift of LIDs caused by domain and time differences"
  - [corpus] Weak or missing; no direct evidence from corpus neighbors.
- Break condition: If LID models are fine-tuned on the same task data as the MLM, or if the MLM's training data is a subset of the LID's training data.

### Mechanism 2
- Claim: Label mismatch between LID and language adapters causes performance decay.
- Mechanism: LID models output ISO language codes, while language adapters might be trained on language metadata that doesn't align exactly (e.g., dialect vs. language, regional variants).
- Core assumption: The language identification output space does not perfectly match the adapter selection space.
- Evidence anchors:
  - [abstract] "label mismatch between LID and the language adapter"
  - [section] "2) label mismatch between LID and the language adapter"
  - [corpus] Weak or missing; no direct evidence from corpus neighbors.
- Break condition: If LID output space is aligned with adapter space, or if adapters are trained to handle label mismatches.

### Mechanism 3
- Claim: Linguistic phenomena like code-mixing and creole languages degrade LID accuracy, which cascades to MLM performance.
- Mechanism: LID models trained on monolingual data struggle with multilingual inputs, causing misclassifications that propagate to the wrong adapter.
- Core assumption: The evaluation dataset contains code-mixed or creole examples that LID models are not robust to.
- Evidence anchors:
  - [abstract] "other linguistic problems that affect LIDs such as code-mixing and creole language"
  - [section] "3) other linguistic problems that affect LIDs such as code-mixing and creole language"
  - [corpus] Weak or missing; no direct evidence from corpus neighbors.
- Break condition: If the dataset is strictly monolingual, or if LID models are specifically trained to handle code-mixing.

## Foundational Learning

- Concept: Modular language model architecture
  - Why needed here: Understanding how language adapters work and why they require language identification at inference.
  - Quick check question: In a modular MLM, how is the correct language adapter selected during inference?

- Concept: Language identification (LID) task and evaluation
  - Why needed here: To understand how LID models are evaluated and what metrics are used (accuracy in this paper).
  - Quick check question: What is the primary evaluation metric used for LID models in this paper?

- Concept: Distribution shift and domain adaptation
  - Why needed here: To understand why LID models trained on one data distribution perform poorly on another.
  - Quick check question: What is the term for the phenomenon where a model trained on one data distribution performs poorly on a different distribution?

## Architecture Onboarding

- Component map: Input text → LID model → Language code → Adapter selection → MLM with adapter → Task output
- Critical path: Input text → LID model → Language code → Adapter selection → MLM with adapter → Task output
- Design tradeoffs: Modularity vs. performance; flexibility to add new languages vs. accuracy loss from pipelining.
- Failure signatures: Accuracy drop of ~7-8% when adding LID; performance worse than direct fine-tuning; LID accuracy varies by resource group.
- First 3 experiments:
  1. Measure LID accuracy on the MASSIVE dataset per language group (HRL, MRL, LRL).
  2. Evaluate modular MLM performance with and without LID on MASSIVE.
  3. Test LID robustness on code-mixed or multilingual inputs if available in the dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of modular MLMs change when using language adapters trained on domain-specific data (e.g., legal, medical) compared to general-purpose adapters?
- Basis in paper: [inferred] The paper discusses potential limitations like distribution shift caused by domain and time differences, suggesting that domain-specific data could impact performance.
- Why unresolved: The paper focuses on general multilingual evaluation without exploring domain-specific scenarios.
- What evidence would resolve it: Comparative experiments evaluating modular MLMs with domain-specific vs. general-purpose language adapters across various domains.

### Open Question 2
- Question: Can the performance gap between modular MLMs with LID and direct fine-tuning be reduced by using a more advanced LID model specifically designed for the target language families?
- Basis in paper: [explicit] The paper evaluates off-the-shelf LID models and finds they significantly decrease modular MLM performance, raising the question of how to close this gap.
- Why unresolved: The paper uses existing LID models without exploring custom-designed or optimized models for the specific language families in the dataset.
- What evidence would resolve it: Experiments comparing modular MLM performance using various LID models, including custom-designed models for specific language families.

### Open Question 3
- Question: How does code-mixing and creole language usage in real-world scenarios affect the performance of modular MLMs with LID compared to direct fine-tuning?
- Basis in paper: [explicit] The paper mentions linguistic problems like code-mixing and creole language as potential limitations of using off-the-shelf LID with modular MLMs.
- Why unresolved: The evaluation uses monolingual data, not reflecting real-world multilingual scenarios with code-mixing.
- What evidence would resolve it: Experiments evaluating modular MLMs with LID on code-mixed datasets and comparing performance to direct fine-tuning approaches.

### Open Question 4
- Question: What is the impact of label mismatch between LID outputs and language adapter labels on the overall performance of modular MLMs?
- Basis in paper: [explicit] The paper discusses label mismatch between LID and language adapters as a potential limitation.
- Why unresolved: The paper does not quantify or analyze the specific impact of label mismatches on performance.
- What evidence would resolve it: Detailed analysis measuring the frequency and impact of label mismatches on modular MLM accuracy, potentially with methods to mitigate this issue.

### Open Question 5
- Question: How does the performance of modular MLMs with LID change when using dynamic adapter selection instead of fixed LID-based selection?
- Basis in paper: [inferred] The current approach uses LID for static adapter selection, but the paper suggests exploring solutions to performance gaps.
- Why unresolved: The paper only considers static adapter selection based on LID outputs.
- What evidence would resolve it: Comparative experiments evaluating modular MLMs with static vs. dynamic adapter selection strategies, measuring accuracy and efficiency trade-offs.

## Limitations
- The paper identifies mechanisms for performance degradation but lacks empirical evidence to isolate and quantify their individual contributions
- Only two LID models were selected from the initial five evaluated without clear justification
- Results are limited to a single modular MLM architecture (MAD-X with mBERT) and task (MASSIVE dataset)

## Confidence
- **High**: The empirical observation that adding LID degrades modular MLM performance by 7-8% accuracy
- **Medium**: The identification of potential mechanisms (distribution shift, label mismatch, linguistic problems) that could explain the performance gap
- **Low**: The causal attribution of performance degradation to specific mechanisms without empirical isolation or quantification

## Next Checks
1. **Ablation study to isolate mechanisms**: Conduct controlled experiments where LID models are fine-tuned on the same data distribution as the modular MLM, and separately test with perfectly aligned label spaces. This would quantify the individual contributions of distribution shift and label mismatch to the performance gap.

2. **Broader architecture and task evaluation**: Replicate the experiments with different modular MLM architectures (e.g., AdapterHub, ParallelAdapter) and on multiple multilingual tasks/datasets to assess the generalizability of the observed limitations across the modular MLM landscape.

3. **Error analysis of misclassifications**: Perform detailed analysis of where and why LID models fail on the MASSIVE dataset, specifically examining code-mixed examples, dialectal variations, and cases of label mismatch. This would provide empirical evidence for the linguistic phenomena hypothesis rather than relying on theoretical discussion.