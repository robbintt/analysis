---
ver: rpa2
title: Curriculum Reinforcement Learning via Morphology-Environment Co-Evolution
arxiv_id: '2309.12529'
source_url: https://arxiv.org/abs/2309.12529
tags:
- morphology
- environment
- environments
- agent
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a curriculum reinforcement learning method
  via morphology-environment co-evolution (MECE). The core idea is to train three
  RL policies in alternation: (1) a control policy to complete tasks, (2) a morphology
  policy to evolve the agent''s morphology to be adaptive to diverse environments,
  and (3) an environment policy to modify the training environments to boost the morphology
  evolution.'
---

# Curriculum Reinforcement Learning via Morphology-Environment Co-Evolution

## Quick Facts
- arXiv ID: 2309.12529
- Source URL: https://arxiv.org/abs/2309.12529
- Reference count: 27
- Key outcome: MECE significantly outperforms state-of-the-art morphology optimization methods in learning efficiency and final performance on multiple tasks.

## Executive Summary
This paper proposes a curriculum reinforcement learning method via morphology-environment co-evolution (MECE) that trains three RL policies in alternation: a control policy for task completion, a morphology policy for evolving agent structure, and an environment policy for modifying training environments. The co-evolutionary process creates a curriculum where each policy's learning progress provides feedback to train the others, leading to improved generalization and learning efficiency. The method demonstrates significant performance gains over state-of-the-art approaches across 2D locomotion, 3D locomotion, and gap-crossing tasks.

## Method Summary
MECE trains three alternating RL policies using PPO: (1) a control policy implemented as a GNN that learns to complete tasks on various morphologies, (2) a morphology policy that evolves the agent's physical structure based on the control policy's learning progress, and (3) an environment policy that modifies training environments to create increasingly challenging tasks. The morphology and environment policies are triggered dynamically based on reward criteria that measure the control policy's learning progress. When learning plateaus, the corresponding policy (morphology or environment) is updated to provide new challenges and accelerate learning.

## Key Results
- MECE outperforms state-of-the-art morphology optimization methods (NGE, Transform2Act, enhanced POET) on 2D locomotion, 3D locomotion, and gap-crossing tasks
- The co-evolution between morphology and environment is identified as the key factor for success through ablation studies
- The dynamic update window based on reward criteria ensures efficient co-evolution by triggering policy updates when learning plateaus
- The GNN-based control policy successfully generalizes across different morphologies, enabling effective learning without retraining

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The morphology and environment policies create a curriculum that accelerates control policy learning.
- **Mechanism:** By training two RL policies to modify morphology and environment respectively, MECE generates a dynamic curriculum. The morphology policy optimizes the agent's physical structure for better generalization across diverse environments, while the environment policy creates increasingly challenging tasks to stimulate further morphological evolution. This co-evolutionary process provides dense, informative feedback to the control policy, enabling faster adaptation and learning.
- **Core assumption:** The morphology and environment policies can be effectively trained using rewards based on the control policy's learning progress, and this co-evolutionary process will lead to improved generalization and learning efficiency.
- **Evidence anchors:**
  - [abstract] "The morphology and environment policies create a curriculum to train the control policy continuously, and the environment policy creates a curriculum to optimize the morphology policy."
  - [section] "MECE enables three policies to help in each other's training through the co-evolving mechanism, where each policy is trained on a curriculum of easy-to-hard tasks utilizing dense feedback from the training experience of other policies."
- **Break condition:** If the morphology or environment policies fail to provide meaningful improvements to the control policy's learning progress, or if the co-evolutionary process leads to overfitting to specific morphologies or environments rather than generalization.

### Mechanism 2
- **Claim:** The dynamic update window based on reward criteria ensures efficient co-evolution.
- **Mechanism:** MECE employs two independent criteria for the morphology reward (rm) and environment reward (re) to determine when to update the morphology or environment. This dynamic update window adapts to the learning progress of the control policy, morphology policy, and environment policy, ensuring that each policy is updated when it can provide the most benefit to the others.
- **Core assumption:** The reward criteria rm and re accurately reflect the learning progress of the control policy and can be used to effectively schedule updates to the morphology and environment.
- **Evidence anchors:**
  - [abstract] "In MECE, we train the control policy and automatically determine when to apply the other two policies to modify the morphology or change the environment: they are triggered by the slow progress on the current morphology and environment, respectively."
  - [section] "A nominal value of rm corresponds to the adaptability of the current morphology to different environments and indicates that training with the current morphology cannot significantly increase π's performance. Similarly, a minor re indicates that modifying the agent's morphology will have a negligible effect on its performance in the current environment, and the environment should be modified to be more challenging to boost the morphology evolution."
- **Break condition:** If the reward criteria are not well-calibrated or if the dynamic update window leads to oscillations or instability in the co-evolutionary process.

### Mechanism 3
- **Claim:** The use of a GNN for the control policy enables generalization across different morphologies.
- **Mechanism:** The control policy is implemented as a GNN that takes the agent's skeleton structure as input and outputs actions. This allows the policy to capture the structural constraints and correlations between joints, enabling it to adapt to different morphologies without requiring retraining from scratch.
- **Core assumption:** The GNN architecture is sufficiently expressive to capture the relevant features of different morphologies and generalize to new ones.
- **Evidence anchors:**
  - [abstract] "We use a graph neural network (GNN) for the control policy because (1) it can be generalized to different morphologies and learn fundamental control skills; (2) it captures the skeleton structure and encodes physical constraints."
  - [section] "We represent the agent's skeleton by a graph G = (V, A, E) where each node u ∈ V represents a joint and each edge e ∈ E represents a bone connecting two joints. The GNN takes zu as inputs and propagate messages across nodes on the graph to produce a hidden state for each node."
- **Break condition:** If the GNN architecture is not sufficiently expressive or if the training data is not diverse enough to enable generalization across morphologies.

## Foundational Learning

- **Concept:** Markov Decision Processes (MDPs)
  - **Why needed here:** The paper formulates the control policy, morphology policy, and environment policy as MDPs, which provide a framework for sequential decision-making under uncertainty. Understanding MDPs is crucial for understanding how the policies are trained and how they interact with each other.
  - **Quick check question:** What are the key components of an MDP, and how do they relate to the problem of training an RL agent?

- **Concept:** Graph Neural Networks (GNNs)
  - **Why needed here:** The control policy is implemented as a GNN, which allows it to capture the structural constraints and correlations between joints in the agent's skeleton. Understanding GNNs is important for understanding how the control policy generalizes across different morphologies.
  - **Quick check question:** How do GNNs differ from traditional neural networks, and what advantages do they offer for learning on graph-structured data?

- **Concept:** Proximal Policy Optimization (PPO)
  - **Why needed here:** The paper uses PPO to train the three RL policies. Understanding PPO is important for understanding the training algorithm and how it ensures stable and efficient learning.
  - **Quick check question:** What are the key features of PPO that distinguish it from other policy gradient methods, and how do these features contribute to its stability and efficiency?

## Architecture Onboarding

- **Component map:** Control Policy (π) <- Morphology Policy (πm) <- Environment Policy (πe) <- Scheduler
- **Critical path:** The critical path is the sequence of steps that must be executed to train the three policies and generate the final morphology and control policy. It involves: (1) Initializing the three policies and the initial agent morphology and environment. (2) Training the control policy using PPO on the current morphology and environment. (3) Evaluating the control policy's learning progress using a short evaluation window. (4) Updating the morphology or environment based on the reward criteria rm and re. (5) Repeating steps 2-4 until the maximum number of iterations is reached.
- **Design tradeoffs:**
  - The choice of GNN architecture for the control policy involves a tradeoff between expressiveness and computational efficiency.
  - The choice of reward criteria rm and re involves a tradeoff between sensitivity to learning progress and robustness to noise.
  - The choice of scheduler parameters involves a tradeoff between exploration and exploitation of the morphology and environment spaces.
- **Failure signatures:**
  - If the control policy fails to learn, it may indicate issues with the GNN architecture, the training algorithm, or the exploration strategy.
  - If the morphology or environment policies fail to provide meaningful improvements, it may indicate issues with the reward criteria, the scheduler, or the diversity of the training data.
  - If the co-evolutionary process leads to overfitting, it may indicate issues with the diversity of the morphologies or environments, or the generalization ability of the policies.
- **First 3 experiments:**
  1. Train the control policy on a fixed morphology and environment to establish a baseline for comparison.
  2. Train the morphology and environment policies using the reward criteria rm and re, and evaluate their ability to improve the control policy's learning progress.
  3. Train the three policies jointly using MECE, and evaluate the final morphology and control policy's performance on unseen environments.

## Open Questions the Paper Calls Out
- How does the morphology-environment co-evolution (MECE) approach perform when applied to more complex and diverse environments?
- How does the choice of reward functions for the morphology and environment policies impact the overall performance of MECE?
- How does MECE compare to other curriculum learning approaches in terms of learning efficiency and final performance?

## Limitations
- The paper does not explore the scalability of MECE to more complex and diverse environments
- The sensitivity of MECE's performance to different reward function designs is not extensively investigated
- A comprehensive comparison with other curriculum learning approaches is not provided

## Confidence
- Performance claims: Medium (empirical results are compelling but some implementation details are missing)
- Scalability claims: Low (limited exploration of complex environments)
- Generalization claims: Medium (demonstrated on limited morphologies but untested in more complex spaces)

## Next Checks
1. Conduct a thorough hyperparameter sensitivity analysis for the morphology and environment policies to determine the robustness of the co-evolutionary process.
2. Evaluate the method's performance on more complex tasks with higher-dimensional morphology and environment spaces to assess scalability.
3. Implement a variant of the method using different GNN architectures or policy gradient algorithms to validate the generality of the approach.