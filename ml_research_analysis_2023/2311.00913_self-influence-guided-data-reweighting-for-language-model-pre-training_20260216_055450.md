---
ver: rpa2
title: Self-Influence Guided Data Reweighting for Language Model Pre-training
arxiv_id: '2311.00913'
source_url: https://arxiv.org/abs/2311.00913
tags:
- data
- training
- presence
- pre-training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PRESENCE addresses the problem of data reweighting for large-scale
  language model pre-training, where all data samples are traditionally treated equally
  despite varying quality and relevance. The method leverages self-influence (SI)
  scores, which measure a sample's impact on model training and prediction, to guide
  sample reweighting.
---

# Self-Influence Guided Data Reweighting for Language Model Pre-training

## Quick Facts
- arXiv ID: 2311.00913
- Source URL: https://arxiv.org/abs/2311.00913
- Reference count: 21
- Primary result: PRESENCE improves cross-lingual transfer and model robustness, achieving F1 gains of 1.77 on XQuAD and 0.35 on XNLI compared to baseline pre-training

## Executive Summary
PRESENCE addresses the challenge of data reweighting in large-scale language model pre-training by leveraging self-influence (SI) scores to weigh training samples. The method introduces a two-stage strategy: direct weighting that emphasizes high SI samples early in training to promote novelty and learning, followed by inverse weighting that de-emphasizes high SI samples later to limit noise and ensure stability. Evaluated across multiple model sizes and tasks, PRESENCE consistently improves performance over baseline pre-training with randomly sampled data, demonstrating effectiveness in enhancing cross-lingual transfer and model robustness.

## Method Summary
PRESENCE is an online and adaptive data reweighting method that uses self-influence scores to weigh samples in a training batch. The approach calculates SI scores at the microbatch level using the first encoder and decoder layers for computational efficiency. During initial training, direct weighting (τ > 0) emphasizes high SI samples to drive learning using influential samples. In later stages, inverse weighting (τ < 0) de-emphasizes high SI samples to limit noise and ensure stability. The method is evaluated on mT5-base and mT5-large models pre-trained on mC4 data, with downstream fine-tuning on multilingual tasks including XQuAD, MLQA, TyDi QA, XNLI, and WikiAnn NER.

## Key Results
- mT5-base+PRESENCE achieves F1 gains of 1.77 on XQuAD compared to baseline
- mT5-base+PRESENCE achieves accuracy improvement of 0.35 on XNLI compared to baseline
- PRESENCE demonstrates consistent improvements across multiple model sizes, datasets, and tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-influence scores correlate with sample quality in pre-training data
- Mechanism: Self-influence (SI) scores measure how much a training sample affects its own prediction during training. Higher SI scores indicate samples that are more influential in reducing model loss, which correlates with noisy or outlier samples.
- Core assumption: SI scores computed on pre-training data reflect the same properties as in supervised settings - that is, high SI indicates noisy or domain-mismatched samples
- Evidence anchors: The paper verifies the ability of SI scores to predict sample quality by investigating the relationship between SI scores and noisy or domain-mismatched samples, observing substantially high average self-influence scores for noisy samples across all languages as well as for domain mismatched text in English.

### Mechanism 2
- Claim: Two-stage reweighting based on training progress improves learning dynamics
- Mechanism: During initial training (direct weighting), high SI samples are emphasized to drive novelty and learning. In later stages (inverse weighting), high SI samples are de-emphasized to limit noise and ensure stability.
- Core assumption: Training loss decreases exponentially early in pre-training, making early samples less informative, while later samples may be noisy
- Evidence anchors: The paper formulates a two-stage reweighting strategy based on observations of training dynamics, using direct weighting in the first stage to emphasize high SI samples and inverse weighting in the second stage to de-emphasize them.

### Mechanism 3
- Claim: Microbatch-level reweighting provides computational efficiency and stability for large-scale pre-training
- Mechanism: Instead of computing SI scores for individual samples, PRESENCE computes them at the microbatch level and applies reweighting to gradient aggregation, making it scalable to large batch sizes.
- Core assumption: Microbatch gradients can effectively represent sample-level information for reweighting purposes
- Evidence anchors: The paper uses SI scores in an online joint setting by reweighting samples at the minibatch level for computational efficiency, adapting to large-scale pre-training scenarios.

## Foundational Learning

- Concept: Influence functions and TracIn
  - Why needed here: PRESENCE relies on self-influence scores calculated using TracIn, a scalable influence function approximation
  - Quick check question: How does TracIn differ from traditional influence functions in terms of computational complexity and scalability?

- Concept: Learning rate scheduling and warmup
  - Why needed here: The two-stage reweighting in PRESENCE has parallels with learning rate schedulers that warmup and then decay
  - Quick check question: What is the relationship between the two-stage reweighting strategy and learning rate warmup/decay schedules?

- Concept: Gradient accumulation and microbatching
  - Why needed here: PRESENCE adapts to large-scale pre-training by operating at the microbatch level for computational efficiency
  - Quick check question: How does microbatch gradient computation differ from standard batch gradient computation in terms of memory and computational requirements?

## Architecture Onboarding

- Component map:
  Input pipeline -> Model architecture -> SI computation module -> Reweighting controller -> Training loop

- Critical path:
  1. Load microbatch from dataset
  2. Compute gradients for each sample in microbatch
  3. Calculate self-influence scores from gradient norms
  4. Apply softmax with temperature τ to get sample weights
  5. Aggregate weighted gradients
  6. Update model parameters

- Design tradeoffs:
  - Using first layers for SI computation vs full model: Computational efficiency vs potentially less accurate influence scores
  - Two discrete temperature values vs continuous scheduling: Simplicity vs potentially more optimal weighting
  - Microbatch-level vs sample-level SI: Scalability vs granularity of reweighting

- Failure signatures:
  - Degraded performance with high magnitude temperatures (|τ| too large): Indicates gradient weight variance is too high
  - Slower convergence with inverse weighting only: Suggests model needs initial warming up
  - No improvement over baseline: May indicate SI scores aren't capturing relevant sample characteristics

- First 3 experiments:
  1. Implement SI computation on first encoder and decoder layers and verify correlation with noisy samples
  2. Test single-stage reweighting with τ=1 (direct) and τ=-1 (inverse) to observe individual effects
  3. Implement two-stage reweighting with I=100,000 steps and compare performance against baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PRESENCE vary with different temperature scaling strategies beyond the current two-stage approach?
- Basis in paper: The paper discusses the impact of temperature τ on single-stage reweighting and mentions the potential for automated reweighting strategies using temperature scaling schedules.
- Why unresolved: The paper only explores two discrete values of τ (τ1 = 1 and τ2 = -1) and mentions the need for further investigation into more granular and automated temperature scaling.
- What evidence would resolve it: Experiments comparing the performance of PRESENCE with various temperature scaling schedules (e.g., continuous scheduling, learning rate-inspired schedules) and different temperature ranges.

### Open Question 2
- Question: How does the choice of layers used for calculating self-influence scores impact the effectiveness of PRESENCE?
- Basis in paper: The paper mentions that self-influence scores are currently calculated using only the first layer of the encoder and decoder for computational efficiency, but suggests that using more layers might lead to more representative weighting information.
- Why unresolved: The paper does not explore the impact of using different layer combinations or all layers for calculating self-influence scores.
- What evidence would resolve it: Experiments comparing the performance of PRESENCE with self-influence scores calculated using different layer combinations or all layers, and analyzing the computational trade-offs.

### Open Question 3
- Question: How does the relationship between self-influence scores and sample quality vary across different languages and data sources in the pre-training corpora?
- Basis in paper: The paper mentions the need to explore relationships between samples in the pre-training corpora and influence functions across languages, data sources, and domains as future work.
- Why unresolved: The paper primarily focuses on the English language and mC4 dataset, and does not investigate the generalizability of self-influence scores across different languages and data sources.
- What evidence would resolve it: Experiments evaluating the performance of PRESENCE on diverse language corpora and data sources, and analyzing the correlation between self-influence scores and sample quality across different languages and domains.

## Limitations
- Computational overhead of calculating self-influence scores at microbatch level adds additional gradient computations and memory requirements
- Temperature values (τ=1 and τ=-1) and switch point (I=100,000 steps) appear arbitrary without systematic ablations
- Evaluation focuses primarily on mT5 variants, leaving questions about generalization to other architectures

## Confidence

- **High Confidence**: The core mechanism of using self-influence scores to identify noisy or domain-mismatched samples is well-supported by empirical correlation analysis. The two-stage reweighting strategy has strong theoretical grounding in curriculum learning principles.
- **Medium Confidence**: The effectiveness of PRESENCE on downstream tasks is demonstrated across multiple benchmarks, but the magnitude of improvements may not be consistent across all possible model sizes and datasets.
- **Low Confidence**: The computational efficiency claims are difficult to verify without implementation details of the microbatch SI calculation, and the paper doesn't provide wall-clock time comparisons with baseline training.

## Next Checks

1. **Implement ablation study on temperature values and switch point**: Systematically vary τ1, τ2, and the switch step I to identify optimal values for different model sizes and datasets, rather than relying on the fixed values used in the paper.

2. **Measure computational overhead and training efficiency**: Implement timing measurements to quantify the additional computational cost of SI score calculation and compare wall-clock training time against baseline models across different batch sizes and microbatch configurations.

3. **Test generalization to different architectures**: Apply PRESENCE to BERT-style masked language models and GPT-style causal language models to verify that the benefits extend beyond mT5 encoder-decoder models, testing on GLUE, SuperGLUE, and other standard benchmarks.