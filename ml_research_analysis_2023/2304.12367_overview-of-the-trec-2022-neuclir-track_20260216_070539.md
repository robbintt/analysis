---
ver: rpa2
title: Overview of the TREC 2022 NeuCLIR Track
arxiv_id: '2304.12367'
source_url: https://arxiv.org/abs/2304.12367
tags:
- native
- documents
- english
- topics
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TREC 2022 NeuCLIR studied neural approaches to cross-language information
  retrieval across Chinese, Persian, and Russian newswire documents using English
  queries. The track included ad hoc retrieval, reranking, and monolingual tasks,
  with 172 runs from 12 teams.
---

# Overview of the TREC 2022 NeuCLIR Track

## Quick Facts
- arXiv ID: 2304.12367
- Source URL: https://arxiv.org/abs/2304.12367
- Reference count: 40
- Key outcome: Neural hybrid systems combining dense and sparse retrieval achieved the best cross-language retrieval performance in TREC 2022 NeuCLIR.

## Executive Summary
TREC 2022 NeuCLIR studied neural approaches to cross-language information retrieval across Chinese, Persian, and Russian newswire documents using English queries. The track included ad hoc retrieval, reranking, and monolingual tasks, with 172 runs from 12 teams. Neural hybrid systems combining dense and sparse retrieval achieved the best performance, outperforming both purely neural and non-neural baselines. Reranking runs, starting from BM25 initial rankings, further improved retrieval effectiveness. The evaluation showed that systems using machine translation of queries or documents generally performed well, with some end-to-end CLIR models achieving strong results without translation.

## Method Summary
TREC 2022 NeuCLIR evaluated cross-language information retrieval for Chinese, Persian, and Russian newswire documents using English queries. Participants used document collections limited to 5 million documents per language, with 114 English topics and machine-translated documents and queries provided as resources. Neural hybrid systems combining dense and sparse retrieval achieved the best results, with reranking runs building on BM25 initial rankings. Retrieval effectiveness was measured using nDCG@20, MAP, RBP, and recall at 1000.

## Key Results
- Neural hybrid systems combining dense and sparse retrieval achieved the best performance, outperforming both purely neural and non-neural baselines.
- Reranking runs, starting from BM25 initial rankings, further improved retrieval effectiveness.
- Systems using machine translation of queries or documents generally performed well, with some end-to-end CLIR models achieving strong results without translation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural hybrid systems combining dense and sparse retrieval achieve the best cross-language retrieval performance.
- Mechanism: Dense retrieval models capture semantic similarity in embedding space, while sparse retrieval models preserve exact term matching and lexical signals. Combining both captures complementary information across language boundaries.
- Core assumption: Dense and sparse models retrieve different but relevant document sets that can be effectively merged or re-ranked.
- Evidence anchors:
  - [abstract] states that "Neural hybrid systems combining dense and sparse retrieval achieved the best performance, outperforming both purely neural and non-neural baselines."
  - [section 8.1] shows hybrid approaches on average achieve nDCG@20 of 0.419 (Chinese), 0.439 (Persian), and 0.516 (Russian), significantly higher than dense-only (0.199-0.224) or sparse-only (0.283-0.294) runs.
- Break condition: If dense and sparse models retrieve largely overlapping document sets, the hybrid combination provides minimal benefit.

### Mechanism 2
- Claim: Reranking on top of BM25 initial rankings improves retrieval effectiveness by applying more complex scoring models.
- Mechanism: BM25 provides a strong first-stage recall, and reranking models can apply deeper semantic understanding and cross-language matching that would be too expensive to run on full collections.
- Evidence anchors:
  - [section 2.2] describes reranking as suitable for "second-stage scoring models, rather than on models which search an entire collection."
  - [section 6] mentions machine translation of documents and queries as resources, enabling models to operate in a single language during reranking.
  - [table 6] shows reranking runs achieve competitive nDCG@20 scores (0.299-0.391) despite starting from a limited set of 1,000 documents.
- Break condition: If the initial BM25 ranking is too poor (missing relevant documents entirely), reranking cannot recover lost recall.

### Mechanism 3
- Claim: Query translation enables strong cross-language retrieval performance comparable to end-to-end CLIR models.
- Mechanism: Translating queries into the document language allows standard monolingual retrieval techniques to be applied, leveraging well-developed monolingual IR algorithms and resources.
- Evidence anchors:
  - [section 6] provides machine-translated queries and documents as resources for participants.
  - [table 6] shows systems using query translation (marked "Query Trans") achieve competitive performance, with some outperforming monolingual runs.
  - [section 8.2] notes that "Systems using machine translation of queries or documents generally performed well."
- Break condition: If query translation quality is poor or introduces ambiguity, the retrieval performance degrades significantly.

## Foundational Learning

- Concept: Cross-language information retrieval (CLIR) fundamentals
  - Why needed here: Understanding how to bridge language barriers in information retrieval is essential for participating in NeuCLIR and designing effective systems.
  - Quick check question: What are the main approaches to CLIR, and how do they differ in terms of query translation, document translation, and end-to-end models?

- Concept: Dense vs. sparse retrieval models
  - Why needed here: The track results show that hybrid systems combining dense (semantic) and sparse (lexical) retrieval perform best, so understanding their differences and complementarity is crucial.
  - Quick check question: How do dense retrieval models (e.g., DPR, ANCE) differ from sparse models (e.g., BM25, SPLADE) in terms of representation and retrieval approach?

- Concept: Reranking and its role in IR pipelines
  - Why needed here: Reranking is a key task in NeuCLIR, and understanding how it fits into the overall retrieval pipeline is necessary for designing effective systems.
  - Quick check question: What is the purpose of reranking in an IR pipeline, and what are the typical characteristics of reranking models compared to first-stage retrieval models?

## Architecture Onboarding

- Component map: Query processing (translation if needed) -> First-stage retrieval (dense + sparse) -> Merge/rank initial results -> (optional) Reranking -> Output top 1,000 documents
- Critical path: For a hybrid system: translate query (if using query translation) -> first-stage retrieval (dense + sparse) -> merge/rank initial results -> (optional) reranking -> output top 1,000 documents
- Design tradeoffs: Translation quality vs. computational cost, recall vs. precision in first-stage retrieval, complexity of reranking models vs. their impact on effectiveness, and use of provided resources vs. custom implementations
- Failure signatures: Poor performance due to translation errors, insufficient recall in first-stage retrieval, over-reliance on one retrieval modality, or failure to leverage provided resources effectively
- First 3 experiments:
  1. Compare dense-only, sparse-only, and hybrid retrieval on a small subset of topics to validate the complementary nature of the approaches.
  2. Test the impact of using machine-translated queries vs. human-translated queries on retrieval performance.
  3. Evaluate the effectiveness of reranking on top of BM25 initial rankings using a simple cross-lingual reranker.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the best neural CLIR approaches compare to the strongest statistical approaches to CLIR in terms of both effectiveness and resource requirements?
- Basis in paper: [explicit] The paper lists this as one of the questions NeuCLIR aims to answer, and provides some comparative results showing neural hybrid systems outperform statistical baselines.
- Why unresolved: While initial results show neural approaches outperforming statistical ones, the paper doesn't provide a comprehensive comparison across all metrics (including resource usage) or a definitive answer on which approach is superior overall.
- What evidence would resolve it: A systematic evaluation comparing the best neural systems against the best statistical approaches across multiple languages, using consistent evaluation metrics and measuring both effectiveness and computational/resource requirements.

### Open Question 2
- Question: What is the optimal pooling depth for CLIR collections to ensure high reusability while maintaining efficiency?
- Basis in paper: [inferred] The paper discusses pooling methodology and reusability experiments, showing that deeper pools find more relevant documents but at diminishing returns. It mentions identifying topics with too many unjudged relevant documents.
- Why unresolved: The paper doesn't establish a definitive optimal pooling depth. It shows that some topics may have many unjudged relevant documents even at depth 50, but doesn't provide clear guidance on balancing thoroughness with efficiency.
- What evidence would resolve it: Empirical studies measuring the trade-off between pool depth, retrieval effectiveness, and reusability across different languages and collections, potentially using techniques like the Leave-Out-Unique experiments described.

### Open Question 3
- Question: How does assessor disagreement in relevance judgments affect system rankings and what strategies can mitigate this?
- Basis in paper: [explicit] The paper reports variable agreement levels among assessors (Cohen's kappa ranging from 0.081 to 0.777) and explores whether system preference order changes with different assessors.
- Why unresolved: While the paper shows strong rank correlation despite disagreement, it doesn't explore the impact of disagreement on absolute effectiveness scores or provide strategies to improve agreement or account for it in evaluation.
- What evidence would resolve it: Studies comparing system rankings and effectiveness scores using different assessors' judgments, experiments with different relevance scales or adjudication processes, and analysis of which types of topics or documents show the most disagreement.

## Limitations

- The paper does not fully disclose the exact neural model architectures and hyperparameters used by top-performing teams, making exact replication challenging.
- Details on the machine translation models and training data used for document and query translation are not provided, which may affect reproducibility.
- The evaluation relies on relevance judgments pooled from submitted runs, which may introduce bias toward the retrieval methods used by participants.

## Confidence

- High confidence: Neural hybrid systems combining dense and sparse retrieval achieved the best performance.
- Medium confidence: Reranking on top of BM25 initial rankings improves retrieval effectiveness.
- Medium confidence: Query translation enables strong cross-language retrieval performance comparable to end-to-end CLIR models.

## Next Checks

1. Implement a neural hybrid system combining dense and sparse retrieval and evaluate its performance on the NeuCLIR-1 document collections.
2. Test the impact of using machine-translated queries vs. human-translated queries on retrieval performance to validate the effectiveness of query translation.
3. Evaluate the effectiveness of reranking on top of BM25 initial rankings using a simple cross-lingual reranker and compare with first-stage retrieval performance.