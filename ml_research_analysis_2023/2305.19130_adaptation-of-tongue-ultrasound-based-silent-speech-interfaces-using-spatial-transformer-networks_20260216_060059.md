---
ver: rpa2
title: Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial
  Transformer Networks
arxiv_id: '2305.19130'
source_url: https://arxiv.org/abs/2305.19130
tags:
- network
- speaker
- speech
- adaptation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of adapting silent speech interfaces
  (SSI) to new speakers or recording sessions, as current models are highly speaker-specific
  and perform poorly when the recording equipment is remounted. The proposed solution
  involves integrating a spatial transformer network (STN) module into the deep learning
  framework for ultrasound tongue imaging-based SSI.
---

# Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks

## Quick Facts
- arXiv ID: 2305.19130
- Source URL: https://arxiv.org/abs/2305.19130
- Reference count: 0
- One-line primary result: Adapting only the STN module reduces MSE by 75-76% for cross-speaker and 88-92% for cross-session adaptation.

## Executive Summary
This paper addresses the challenge of adapting ultrasound tongue imaging-based silent speech interfaces (SSI) to new speakers or recording sessions, where current models are highly speaker-specific and perform poorly when recording equipment is remounted. The authors propose integrating a spatial transformer network (STN) module into the deep learning framework to learn affine transformations that compensate for equipment misalignment and speaker differences. By adapting only the STN module while keeping the main regression network frozen, the approach achieves substantial MSE reductions with minimal computational overhead, as the STN represents only about 10% of the network's parameters.

## Method Summary
The method involves integrating an STN module into a deep learning framework for ultrasound tongue imaging-based SSI. The STN contains a localization network that estimates six affine transformation parameters conditioned on each input ultrasound image. During adaptation, the STN parameters are fine-tuned on new speaker or session data while the main regression network remains frozen. This selective adaptation leverages the STN's ability to normalize input data without requiring full network retraining, achieving significant MSE reductions for cross-speaker and cross-session adaptation.

## Key Results
- Adapting only the STN module reduces MSE by 75-76% for cross-speaker adaptation and 88-92% for cross-session adaptation
- The STN module represents only about 10% of the network's parameters
- Within-speaker variance of STN parameters is very low (order of 0.01) compared to between-speaker variance (3-5 times larger)

## Why This Works (Mechanism)

### Mechanism 1
The STN learns an affine transformation to compensate for misalignment between recording sessions and anatomical differences between speakers. The localization network estimates six affine transformation parameters that encode translation, scaling, rotation, shearing, and cropping operations. These transformations are applied before the main regression network, effectively normalizing the input data to a canonical pose. The core assumption is that variability between sessions and speakers can be captured by a linear (affine) transformation, making full network retraining unnecessary.

### Mechanism 2
Adapting only the STN module achieves substantial MSE reductions because it leverages the general features learned by the regression network while normalizing input data through affine transformation. During adaptation, the STN parameters are fine-tuned on new data while the regression network remains frozen, which is computationally efficient and reduces the risk of overfitting with limited adaptation data. The core assumption is that the regression network learned on the base speaker/session has general features that can be reused across speakers/sessions if the input data is properly normalized.

### Mechanism 3
The low within-speaker variance of STN parameters (order of 0.01) compared to between-speaker variance (3-5 times larger) indicates that a single affine transformation per speaker/session could be sufficient. The STN learns speaker/session-specific transformations that are consistent across samples from the same speaker/session, suggesting that the learned transformations capture systematic rather than idiosyncratic variations. This consistency supports the idea that a global transformation per speaker/session could work effectively.

## Foundational Learning

- Concept: Spatial Transformer Networks (STNs)
  - Why needed here: STNs provide a differentiable way to learn spatial transformations of input data, which is crucial for handling misalignment issues in ultrasound tongue imaging across different recording sessions and speakers.
  - Quick check question: What are the three main components of an STN and what role does each play in the transformation process?

- Concept: Ultrasound Tongue Imaging (UTI) and Silent Speech Interfaces
  - Why needed here: Understanding the specific challenges of UTI-based SSI, including sensitivity to probe mounting and speaker anatomy, is essential for appreciating why adaptation is necessary and how the STN addresses these issues.
  - Quick check question: Why does dismounting and remounting the ultrasound probe cause performance degradation in SSI systems?

- Concept: Deep Learning Adaptation Techniques
  - Why needed here: The paper employs selective adaptation (freezing most of the network while adapting only the STN), which is a specific adaptation strategy that balances performance improvement with computational efficiency and data efficiency.
  - Quick check question: What are the advantages and disadvantages of freezing the main regression network while only adapting the STN during speaker/session adaptation?

## Architecture Onboarding

- Component map: Input ultrasound images (64×128) → STN module (localization network outputs 6 affine parameters) → Grid generator and sampler apply affine transformation → Main regression network (2D-CNN or 3D-CNN) → 80-dimensional mel-spectrogram output → Optional WaveGlow vocoder

- Critical path: Input → STN transformation → Main network processing → Mel-spectrogram output

- Design tradeoffs:
  - 2D vs 3D convolution: 2D is simpler and easier to visualize but loses temporal information; 3D captures temporal dynamics but is more computationally expensive
  - STN placement: Applied at input level for global normalization vs. later in network for localized normalization
  - Adaptation scope: Full network adaptation vs. selective STN adaptation vs. STN + output layer adaptation

- Failure signatures:
  - Poor adaptation performance: STN parameters not converging or not capturing relevant variations
  - Overfitting: Adaptation performance good on adaptation data but poor on test data
  - Underfitting: Minimal improvement from adaptation despite significant domain shift
  - Degraded base performance: Adaptation negatively affects performance on base speaker/session

- First 3 experiments:
  1. Baseline evaluation: Train network on single speaker/session with and without STN to verify STN has no effect on base performance
  2. Cross-speaker adaptation: Take model trained on speaker 048 and adapt only the STN to speakers 049, 102, 103; measure MSE improvement
  3. Cross-session adaptation: Take model trained on main session of speaker 048 and adapt only the STN to additional sessions (s048-2 through s048-5); measure MSE improvement and compare to cross-speaker results

## Open Questions the Paper Calls Out

- Question: How does the effectiveness of spatial transformer networks (STN) compare to other domain adaptation techniques for cross-speaker and cross-session adaptation in ultrasound tongue imaging-based silent speech interfaces?
- Question: What is the impact of using a 3D localization network instead of a 2D localization network in the spatial transformer network (STN) module for cross-speaker and cross-session adaptation?
- Question: How does the amount of adaptation material affect the performance of the spatial transformer network (STN) module for cross-speaker and cross-session adaptation?

## Limitations

- The effectiveness depends critically on the assumption that inter-speaker and inter-session variations can be adequately captured by affine transformations
- Limited number of speakers (4) and sessions (1-5 per speaker) may not generalize to broader populations
- The paper doesn't address whether learned transformations are anatomically plausible or if they might introduce artifacts

## Confidence

- High Confidence: Technical implementation of STNs and their integration into the SSI framework is well-established and correctly described
- Medium Confidence: Quantitative results showing MSE reduction are reported accurately, but unexpected ablation study results reduce confidence in interpretation
- Low Confidence: Generalizability to different ultrasound systems, anatomical variations beyond tested speakers, and real-world deployment scenarios

## Next Checks

1. Independently replicate the full ablation study (no adaptation, STN-only, STN+output layer, full adaptation) with larger speaker pools and different ultrasound systems to verify the counterintuitive result that full network adaptation performs worse than selective STN adaptation

2. Visualize and analyze the learned affine transformations across speakers to determine if they correspond to plausible articulatory differences or if they might be capturing equipment artifacts or other non-linguistic factors

3. Train and test the adaptation framework on ultrasound data from a different ultrasound system or clinical protocol to assess whether the affine transformation approach generalizes beyond the specific recording setup used in the original study