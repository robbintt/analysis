---
ver: rpa2
title: 'Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness
  of Large Language Models'
arxiv_id: '2307.08487'
source_url: https://arxiv.org/abs/2307.08487
tags:
- instruction
- instructions
- explicit
- normal
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a benchmark called Latent Jailbreak to evaluate
  the safety and robustness of large language models (LLMs). The authors construct
  a dataset of prompts that embed malicious instructions within regular tasks like
  translation.
---

# Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models

## Quick Facts
- arXiv ID: 2307.08487
- Source URL: https://arxiv.org/abs/2307.08487
- Authors: [Not specified in input]
- Reference count: 6
- Primary result: Current LLMs struggle with safety and robustness when faced with latent jailbreak prompts containing sensitive topics, with varying success rates depending on instruction verbs and prompt structure.

## Executive Summary
This paper introduces Latent Jailbreak, a benchmark designed to evaluate the safety and robustness of large language models (LLMs) by embedding malicious instructions within regular tasks. The authors construct a dataset of prompts that combine benign tasks with hidden harmful instructions, then use a hierarchical annotation framework to assess model responses. The results reveal that LLMs exhibit varying jailbreak rates depending on instruction verbs, prompt structure (prefix vs suffix), and cue words. The study highlights the need for balanced approaches to ensure both safety and robustness in LLMs.

## Method Summary
The authors construct a dataset of 416 latent jailbreak prompts using 13 templates, 16 target group words, and 2 instruction positions (prefix/suffix). They generate 30 outputs per prompt using nucleus sampling, then label a small subset using a hierarchical annotation framework for safety and robustness. The remaining outputs are automatically labeled using k-NN on sentence-BERT embeddings fine-tuned on the labeled subset. Success rates for jailbreaking and robustness are calculated, and a trustworthiness metric is defined as P = (1 - PJ) * PR where PJ is jailbreak success rate and PR is robustness rate.

## Key Results
- Current LLMs exhibit different jailbreak rates for different instruction verbs in explicit instructions
- LLMs perform better in text safety when explicit instructions are placed as prefixes rather than suffixes
- Cue words, particularly "sentence," significantly influence the model's ability to distinguish between explicit and implicit instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model's robustness varies based on the position of explicit instructions (prefix vs suffix) within the prompt.
- Mechanism: When explicit instructions are placed as a suffix, models are more likely to follow implicit malicious instructions, likely due to training data bias where instructions are predominantly prefixes.
- Core assumption: The model's training data predominantly contains instruction prefixes, causing it to prioritize the first instruction it encounters.
- Evidence anchors:
  - [abstract] "Our results show that current LLMs not only have a preference for certain instruction verbs, but also exhibit different jailbreak rates for different instruction verbs in explicit instructions."
  - [section 7] "we observe that in terms of text safety, these three models perform better when the instruction is explicitly placed as a prefix rather than a suffix in Figure 5."
- Break condition: If the model is trained on a balanced dataset with both prefix and suffix instructions, the position effect should diminish.

### Mechanism 2
- Claim: Certain instruction verbs trigger higher jailbreak rates than others.
- Mechanism: Models have learned associations between specific verbs and task completion, making them more susceptible to jailbreak when those verbs are used in explicit instructions.
- Core assumption: The model has learned verb-specific task completion patterns during training that can be exploited.
- Evidence anchors:
  - [abstract] "Our results show that current LLMs not only have a preference for certain instruction verbs, but also exhibit different jailbreak rates for different instruction verbs in explicit instructions."
  - [section 7] "Our results show that current LLMs not only have a preference for certain instruction verbs, but also exhibit different jailbreak rates for different instruction verbs in explicit normal instructions."
- Break condition: If the model is retrained with balanced verb usage or with explicit jailbreak resistance training, the verb-specific jailbreak rates should equalize.

### Mechanism 3
- Claim: Cue words significantly influence the model's ability to distinguish between explicit and implicit instructions.
- Mechanism: Cue words act as delimiters that help the model identify the start and end of the instruction to be executed, affecting its ability to filter out implicit malicious instructions.
- Core assumption: The model uses cue words as structural markers to parse and prioritize instructions.
- Evidence anchors:
  - [section 7] "From the results of prompt types P2 to P5, we analyze and find that large language models have varying degrees of sensitivity to different cue words, especially BELLE-7B-2M, which is particularly sensitive to the cue word 'sentence'."
  - [section 7] "Overall, the cue word 'sentence' has a significant benefit for all evaluated large language models, resulting in lower jailbreak success rates and higher robustness."
- Break condition: If the model is trained without cue words or with alternative delimiters, the cue word effect should disappear.

## Foundational Learning

- Concept: Instruction-following data format (Pn ⊕ C ⊕ Pm)
  - Why needed here: This format is the basis for constructing latent jailbreak prompts and understanding how models process instructions.
  - Quick check question: How would changing the order of Pn, C, and Pm affect the model's response?

- Concept: Hierarchical annotation framework
  - Why needed here: This framework is used to evaluate both text safety and output robustness, which are the key metrics being measured.
  - Quick check question: What are the two levels of the hierarchical annotation framework, and what does each evaluate?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: This is mentioned as a method used to align models with human values, but it can also cause over-refusal.
  - Quick check question: How might RLHF training contribute to the model's sensitivity to certain topics or instructions?

## Architecture Onboarding

- Component map: Prompt construction module -> Model evaluation pipeline -> Annotation framework -> Analysis module
- Critical path: Prompt construction → Model evaluation → Annotation → Analysis
- Design tradeoffs:
  - Safety vs. robustness: Models that are more safety-focused may refuse benign instructions (over-refusal), while models that prioritize instruction-following may generate unsafe content.
  - Automation vs. accuracy: The automatic labeling method trades some accuracy for scalability, but is validated against human-labeled data.
- Failure signatures:
  - High jailbreak rates for certain instruction verbs
  - Positional bias in instruction processing
  - Sensitivity to cue words
- First 3 experiments:
  1. Test model robustness with balanced instruction positions (equal prefix and suffix prompts)
  2. Evaluate jailbreak rates for different instruction verbs across models
  3. Measure the impact of various cue words on safety and robustness metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different instruction verbs affect the probability of generating unsafe content in large language models?
- Basis in paper: [explicit] The paper mentions that current LLMs not only have a preference for certain instruction verbs, but also exhibit different jailbreak rates for different instruction verbs in explicit normal instructions.
- Why unresolved: The paper does not provide a detailed analysis of the specific impact of different instruction verbs on the probability of generating unsafe content.
- What evidence would resolve it: A comprehensive study analyzing the impact of various instruction verbs on the probability of generating unsafe content in large language models.

### Open Question 2
- Question: How does the position of explicit normal instructions affect the safety and robustness of large language models?
- Basis in paper: [explicit] The paper discusses the effect of the position of explicit normal instructions on the safety and robustness of LLMs, stating that the models perform better when the instruction is explicitly placed as a prefix rather than a suffix.
- Why unresolved: The paper does not provide a detailed analysis of the specific impact of different instruction positions on the safety and robustness of LLMs.
- What evidence would resolve it: A detailed study comparing the safety and robustness of LLMs when explicit normal instructions are placed in different positions.

### Open Question 3
- Question: How do different cue words affect the sensitivity of large language models to malicious instructions?
- Basis in paper: [explicit] The paper mentions that large language models have varying degrees of sensitivity to different cue words, especially BELLE-7B-2M, which is particularly sensitive to the cue word "sentence".
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of different cue words on the sensitivity of LLMs to malicious instructions.
- What evidence would resolve it: A detailed study analyzing the impact of various cue words on the sensitivity of large language models to malicious instructions.

## Limitations
- Limited diversity of prompt templates and target groups may not represent full range of potential jailbreak scenarios
- Automatic labeling approach may introduce systematic biases when scaling to full dataset
- Focus on text-based safety may miss multimodal safety concerns

## Confidence
- Claim: LLMs have varying sensitivity to different instruction verbs → Medium
- Claim: Position of explicit instructions affects jailbreak rates → Medium
- Claim: Cue words significantly influence model's ability to distinguish between explicit and implicit instructions → High

## Next Checks
1. Cross-validate with alternative prompt construction methods using different templates and target groups
2. Conduct human-in-the-loop evaluation of model responses to validate automatic labeling accuracy
3. Test transferability of findings to other LLM architectures and training approaches