---
ver: rpa2
title: 'Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language
  Models'
arxiv_id: '2305.14623'
source_url: https://arxiv.org/abs/2305.14623
tags:
- claim
- evidence
- claims
- verification
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a zero-shot fact-checking framework using large
  language models (LLMs). The method, called SELF-CHECKER, breaks down complex claims
  into simpler sub-claims, retrieves relevant documents using generated search queries,
  selects evidence sentences from those documents, and predicts a final verdict on
  the claim's veracity.
---

# Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models

## Quick Facts
- arXiv ID: 2305.14623
- Source URL: https://arxiv.org/abs/2305.14623
- Authors: 
- Reference count: 7
- One-line primary result: SELF-CHECKER achieves competitive fact-checking performance using zero-shot LLMs with modular architecture

## Executive Summary
This paper introduces SELF-CHECKER, a zero-shot fact-checking framework that leverages large language models (LLMs) through a modular, plug-and-play architecture. The system decomposes complex claims into simpler sub-claims, generates search queries, retrieves relevant documents, selects evidence sentences, and predicts veracity verdicts. By using in-context learning with GPT-3, SELF-CHECKER demonstrates that LLMs can perform fact-checking tasks without fine-tuning, though performance still lags behind state-of-the-art fine-tuned models, particularly in verdict prediction.

## Method Summary
SELF-CHECKER implements fact-checking as a pipeline of four LLM modules: claim processor (decomposes complex claims into atomic propositions), query generator (creates search queries from claims), evidence seeker (retrieves and selects relevant sentences from documents), and verdict counselor (aggregates sub-claim predictions to determine final veracity). Each module is prompted with task instructions and in-context examples, using GPT-3 (text-davinci-003) via OpenAI API. The framework is evaluated on Fever and WiCE datasets using accuracy, F1 score, and Fever score metrics, with temperature settings ranging from 0.2 to 0.8 across modules and majority voting for final verdicts.

## Key Results
- SELF-CHECKER achieves competitive accuracy on Fever dataset compared to zero-shot baselines
- Evidence retrieval F1 scores demonstrate effective document processing despite modular design
- Verdict prediction remains the primary bottleneck, underperforming fine-tuned state-of-the-art models
- Modular architecture enables flexible deployment in low-resource environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can perform fact-checking tasks in a zero-shot setting by following structured prompts
- Mechanism: The framework breaks down complex fact-checking into modular subtasks (claim decomposition, query generation, evidence retrieval, verdict prediction) and uses in-context learning to guide LLM behavior through example demonstrations
- Core assumption: LLMs can understand task instructions and learn from few examples to perform specialized fact-checking subtasks
- Evidence anchors:
  - [abstract] "This framework provides a fast and efficient way to construct fact-checking systems in low-resource environments"
  - [section] "Each module is implemented by prompting a LLM. The detailed example prompts are provided in Appendix A"
  - [corpus] Weak - corpus contains related work but no direct validation of zero-shot capability claims
- Break condition: If LLM fails to follow task instructions or produces inconsistent outputs across modules

### Mechanism 2
- Claim: Decomposing complex claims into simpler sub-claims improves fact-checking accuracy
- Mechanism: The claim processor module breaks down multi-fact claims into atomic propositions, making evidence retrieval and verification more tractable for subsequent modules
- Core assumption: Simple claims are easier to verify than complex claims because they involve fewer pieces of information to cross-reference
- Evidence anchors:
  - [abstract] "The complex claims are decomposed into several simple sub-claims"
  - [section] "If a specific claim to verify is already provided, the claim processor can also break down a long and complex claim into a set of simple claims"
  - [corpus] Weak - related papers discuss claim decomposition but no direct evidence provided for accuracy improvement
- Break condition: If decomposition creates ambiguity or loses semantic relationships between facts

### Mechanism 3
- Claim: Aggregating sub-claim verdicts produces accurate final verdicts for complex claims
- Mechanism: The verdict counselor module collects individual sub-claim predictions and applies logical aggregation rules to determine the overall claim veracity
- Core assumption: Individual sub-claim predictions can be combined using logical rules to accurately represent the complex claim's truth value
- Evidence anchors:
  - [abstract] "The verdict counselor determines the veracity of a claim by leveraging the pieces of evidence gathered for it"
  - [section] "These individual labels are then aggregated to obtain the final result"
  - [corpus] Weak - no specific evidence for aggregation effectiveness provided in corpus
- Break condition: If aggregation rules fail to handle conflicting sub-claim predictions appropriately

## Foundational Learning

- Concept: In-context learning
  - Why needed here: LLMs must learn to perform fact-checking tasks without fine-tuning by understanding prompts and examples
  - Quick check question: Can the LLM generate appropriate search queries from a claim when provided with task instructions and examples?

- Concept: Modular decomposition
  - Why needed here: Complex fact-checking is broken into manageable subtasks that can be handled by specialized LLM modules
  - Quick check question: Does the claim processor correctly identify all verifiable claims within a complex input?

- Concept: Evidence-based reasoning
  - Why needed here: Verdict prediction requires analyzing retrieved evidence to determine claim veracity
  - Quick check question: Can the evidence seeker accurately identify relevant sentences from retrieved documents?

## Architecture Onboarding

- Component map: Policy agent → Claim processor → Query generator → Evidence seeker → Verdict counselor → Output
- Critical path: Claim → Sub-claims → Queries → Documents → Evidence sentences → Verdict prediction
- Design tradeoffs: Zero-shot flexibility vs. fine-tuned model accuracy; modular independence vs. inter-module coordination
- Failure signatures: Inconsistent sub-claim decomposition, irrelevant document retrieval, incorrect evidence selection, aggregation errors
- First 3 experiments:
  1. Test claim processor on complex claims to verify decomposition quality
  2. Evaluate query generator with known claims to assess retrieval relevance
  3. Run end-to-end verification on simple claims to establish baseline accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the efficiency of the SELF-CHECKER's verdict counselor module be improved to match or surpass state-of-the-art models?
- Basis in paper: [explicit] The paper mentions that the verdict counselor is the primary bottleneck in the overall performance of the SELF-CHECKER, and there is significant room for improvement in verdict prediction despite the superior capabilities of LLMs in various NLP tasks.
- Why unresolved: The paper does not provide specific methods or techniques to improve the verdict counselor's efficiency.
- What evidence would resolve it: Experimental results demonstrating improved accuracy and performance of the verdict counselor module after implementing proposed improvements.

### Open Question 2
- Question: Can incorporating additional working memory in the SELF-CHECKER expedite the verification process by leveraging past information?
- Basis in paper: [inferred] The paper suggests that future work can focus on enhancing the efficiency of the SELF-CHECKER by incorporating additional working memory to expedite the verification process.
- Why unresolved: The paper does not provide specific details on how to implement additional working memory or its potential impact on the system's performance.
- What evidence would resolve it: Experimental results showing improved efficiency and performance of the SELF-CHECKER after incorporating additional working memory.

### Open Question 3
- Question: How can more efficient strategies for utilizing LLMs in each subtask of fact-checking be explored to optimize the performance of the SELF-CHECKER?
- Basis in paper: [explicit] The paper mentions that exploring more efficient strategies for utilizing LLMs in each subtask of fact-checking holds promise for optimizing performance.
- Why unresolved: The paper does not provide specific strategies or techniques for utilizing LLMs more efficiently in each subtask.
- What evidence would resolve it: Experimental results demonstrating improved performance of the SELF-CHECKER after implementing proposed strategies for utilizing LLMs more efficiently in each subtask.

## Limitations
- Zero-shot performance claims lack comparison against proper baselines using identical evaluation settings
- Limited analysis of failure modes beyond accuracy metrics (e.g., hallucination patterns, robustness to adversarial claims)
- No ablation study on the modular architecture to isolate contribution of each component
- Unclear whether in-context examples were optimized or randomly selected

## Confidence
- **High**: The modular decomposition approach is technically sound and the implementation details are clearly specified
- **Medium**: The experimental results on Fever and WiCE datasets are reproducible with the provided specifications
- **Low**: Claims about efficiency gains relative to fine-tuned models lack proper benchmarking data

## Next Checks
1. Conduct ablation experiments removing individual modules to quantify their contribution to overall accuracy
2. Test the framework on adversarial examples designed to trigger LLM hallucinations or reasoning errors
3. Compare zero-shot performance against few-shot variants using varying numbers of in-context examples to find the efficiency-accuracy tradeoff point