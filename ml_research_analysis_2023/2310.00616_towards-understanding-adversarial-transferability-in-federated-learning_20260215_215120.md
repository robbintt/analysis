---
ver: rpa2
title: Towards Understanding Adversarial Transferability in Federated Learning
arxiv_id: '2310.00616'
source_url: https://arxiv.org/abs/2310.00616
tags:
- s114
- s101
- s116
- s115
- s110
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the security risk of malicious clients
  in federated learning (FL) who initially act as benign participants but later use
  their training data to launch transferable adversarial attacks. The study evaluates
  how robust FL systems are to such covert attacks compared to centralized learning.
---

# Towards Understanding Adversarial Transferability in Federated Learning

## Quick Facts
- arXiv ID: 2310.00616
- Source URL: https://arxiv.org/abs/2310.00616
- Authors: 
- Reference count: 40
- Key outcome: FL systems show higher robustness to transferable adversarial attacks than centralized learning, particularly when clean accuracy is comparable

## Executive Summary
This paper investigates security risks in federated learning where malicious clients initially act as benign participants but later use their training data to launch transferable adversarial attacks. The study evaluates how robust FL systems are to such covert attacks compared to centralized learning. Results show that with only 3% of client data, attackers can achieve over 80% attack success rate. FL systems demonstrate higher robustness than centralized counterparts, particularly when accuracy on clean data is comparable.

## Method Summary
The study uses CIFAR-10 dataset split into 100 iid partitions with ResNet50 and CNN architectures. Centralized baseline models are trained with SGD with momentum for 200 epochs, while FL baseline uses FedAvg with 400 rounds and 5 local epochs for ResNet50. Transfer attacks are simulated by training substitute models on limited client data subsets and generating adversarial examples via PGD. The paper evaluates transfer accuracy and success rate across various FL configurations including data heterogeneity (dirichlet α, imbalance), decentralization (number of clients), and averaging (clients per round). Correlation analysis examines how these factors affect robustness.

## Key Results
- With only 3% of client data, attackers achieve over 80% attack success rate
- FL systems demonstrate higher robustness than centralized counterparts when clean accuracy is comparable
- Both empirical experiments and theoretical analysis support the findings that decentralized training and averaging dilute malicious alterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Federated learning models are more robust to transfer attacks than centralized models when clean accuracy is comparable.
- Mechanism: Decentralized training on distributed data and averaging of model updates dilute malicious alterations, reducing transferability of adversarial examples.
- Core assumption: The heterogeneity and dispersion of data across clients create decision boundaries that differ significantly from centralized training.
- Evidence anchors:
  - [abstract] "The increased robustness is attributed to decentralized training on distributed data and averaging of model updates, which dilute malicious alterations."
  - [section] "We investigate two intrinsic properties of FL, namely the property of distributed training and the averaging operation, and their correlation with federated robustness."
  - [corpus] Weak evidence; no direct corpus papers discussing transferability in this specific federated setting.
- Break condition: If the data distribution across clients becomes too similar or centralized, the robustness advantage diminishes.

### Mechanism 2
- Claim: Increasing the number of clients in federated training reduces the transferability of adversarial examples.
- Mechanism: More clients lead to greater heterogeneity and decentralization, which increases the discrepancy between the source and target model decision boundaries.
- Core assumption: The source model (trained with limited data) cannot accurately approximate the target model's decision boundary when the target is trained on more diverse data.
- Evidence anchors:
  - [section] "We can observe that T.Rate drops with an increasing number of users, which demonstrate that more decentralized data leads to lower transferability."
  - [section] "The heterogeneous distribution leads to a loss landscape significantly different from the ones of iid and centralized model and also results in a loss landscape with high variance [7]."
  - [corpus] Weak evidence; no direct corpus papers discussing this specific relationship.
- Break condition: If the source model is also trained in a federated manner with similar data heterogeneity, the advantage may be reduced.

### Mechanism 3
- Claim: Averaging over more clients in each round improves federated model robustness against transfer attacks.
- Mechanism: Averaging reduces the variance in the aggregated model, making it less susceptible to adversarial perturbations from substitute models.
- Core assumption: The variance reduction from averaging outweighs the potential benefit of having more data from fewer clients.
- Evidence anchors:
  - [section] "Both CNN and ResNet50 exhibit a decreasing trend as the number of clients used to average increases."
  - [section] "This evidence demonstrates that averaging in FedAvg improves the robustness of the federated model against transfer attacks."
  - [corpus] Weak evidence; no direct corpus papers discussing this specific relationship.
- Break condition: If the averaging is done over too few clients or the clients are highly non-IID, the benefit may be reduced.

## Foundational Learning

- Concept: Adversarial transferability
  - Why needed here: Understanding how adversarial examples generated from one model can fool another is crucial for evaluating the security of federated learning systems.
  - Quick check question: What factors influence the transferability of adversarial examples between models?
- Concept: Federated learning architecture
  - Why needed here: Knowledge of how federated learning works, including data partitioning, local training, and aggregation, is essential for understanding the proposed attack and defense mechanisms.
  - Quick check question: How does the distribution of data across clients affect the training process in federated learning?
- Concept: Statistical hypothesis testing
  - Why needed here: The paper uses hypothesis testing to validate the correlation between various factors and adversarial transferability, which is important for drawing reliable conclusions.
  - Quick check question: What is the significance of a p-value in hypothesis testing, and how is it used to determine statistical significance?

## Architecture Onboarding

- Component map: Data partitioning -> Local training -> Aggregation -> Attack simulation
- Critical path: Partition data → Train local models → Aggregate updates → Evaluate robustness → Simulate attack
- Design tradeoffs:
  - Data heterogeneity vs. model convergence: More heterogeneous data can improve robustness but may slow down convergence.
  - Number of clients vs. communication overhead: More clients improve robustness but increase communication costs.
  - Aggregation method vs. robustness: Different aggregation methods (e.g., FedAvg, Krum) may have different impacts on robustness.
- Failure signatures:
  - High transferability rates: Indicates that the federated model is vulnerable to transfer attacks.
  - Low accuracy on clean data: May suggest that the model is overfitting to the training data.
  - Inconsistent results across experiments: Could indicate issues with the experimental setup or data partitioning.
- First 3 experiments:
  1. Evaluate transferability between centralized and federated models with comparable clean accuracy.
  2. Test transferability with substitute models trained on varying numbers of client data.
  3. Assess the impact of data heterogeneity and decentralization on transferability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the transfer rate change when malicious clients use data augmentation techniques beyond AutoAugment, such as CutMix or MixUp?
- Basis in paper: [explicit] The paper shows that AutoAugment and ImageNet pretraining improve transfer rates from 3-5% to 36-48% with 1-2% of client data, and mentions the potential for further improvement with more advanced attacks like Auto-Attack and Skip Attack.
- Why unresolved: The study only evaluates AutoAugment and pretraining, leaving the impact of other augmentation techniques unexplored.
- What evidence would resolve it: Experiments comparing transfer rates using different augmentation methods (e.g., CutMix, MixUp) on the same dataset and attack scenarios.

### Open Question 2
- Question: Does the heterogeneity of data (e.g., non-IID distributions) have a more significant impact on transfer robustness when using deeper architectures like ResNet50 compared to shallower ones like CNN?
- Basis in paper: [inferred] The paper shows that heterogeneity reduces transfer rates and mentions that ResNet50 has higher transfer rates than CNN in some scenarios, suggesting architecture may influence the effect of heterogeneity.
- Why unresolved: The paper doesn't directly compare the impact of heterogeneity across different architectures.
- What evidence would resolve it: Experiments measuring transfer rates across various architectures (e.g., ResNet50, CNN, VGG) under different heterogeneity levels (e.g., Dirichlet alpha, class imbalance).

### Open Question 3
- Question: How does the choice of aggregation method (e.g., Krum, Trimmed Mean) affect the transfer robustness compared to FedAvg in scenarios with varying numbers of malicious clients?
- Basis in paper: [explicit] The paper shows that increasing the number of clients to average per round improves transfer robustness for FedAvg and mentions that Krum, Geometric Mean, and Trimmed Mean also exhibit decreasing transfer rates with more averaged clients.
- Why unresolved: The paper doesn't compare the effectiveness of different aggregation methods in depth or under varying malicious client ratios.
- What evidence would resolve it: Experiments comparing transfer rates across aggregation methods (e.g., FedAvg, Krum, Trimmed Mean) with different numbers of malicious clients and data heterogeneity levels.

## Limitations
- Conclusions based on CIFAR-10 dataset with fixed architectures (ResNet50, CNN), limiting generalizability
- Assumes all malicious clients behave identically, not accounting for adaptive or coordinated attacks
- Correlation analysis supports robustness factors but lacks controlled ablation studies isolating each mechanism

## Confidence
- High confidence: Transfer attack success rates with limited client data (80%+ at 3%) are well-supported by empirical results across multiple experiments.
- Medium confidence: Claims about averaging reducing variance and improving robustness are supported by correlation analysis but lack theoretical proof of mechanism.
- Medium confidence: The heterogeneity and decentralization explanations for robustness are plausible given experimental trends but don't rule out other contributing factors.

## Next Checks
1. Test transferability on non-image datasets (e.g., medical or text data) to verify robustness claims generalize beyond CIFAR-10.
2. Implement adaptive attack strategies where attackers observe model updates and adjust their approach, testing whether baseline robustness holds.
3. Conduct ablation studies isolating each mechanism (data heterogeneity, averaging, decentralization) by systematically varying one factor while holding others constant.