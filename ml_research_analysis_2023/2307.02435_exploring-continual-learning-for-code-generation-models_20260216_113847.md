---
ver: rpa2
title: Exploring Continual Learning for Code Generation Models
arxiv_id: '2307.02435'
source_url: https://arxiv.org/abs/2307.02435
tags:
- code
- learning
- tasks
- task
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new continual learning benchmark for code
  generation models that covers code generation, translation, summarization, and refinement
  tasks with different input and output programming languages. The authors evaluate
  popular CL methods from NLP and Vision domains on this benchmark and find that prompt
  pooling suffers from catastrophic forgetting due to unstable training of the prompt
  selection mechanism caused by stark distribution shifts in coding tasks.
---

# Exploring Continual Learning for Code Generation Models

## Quick Facts
- arXiv ID: 2307.02435
- Source URL: https://arxiv.org/abs/2307.02435
- Reference count: 17
- Key outcome: Introduces CODE TASK-CL benchmark and PP-TF method that improves Prompt Pooling by 21.54% on code generation continual learning tasks

## Executive Summary
This paper addresses the challenge of continual learning (CL) for code generation models, where standard methods suffer from catastrophic forgetting due to large distribution shifts between programming tasks. The authors introduce CODE TASK-CL, a new benchmark covering code generation, translation, summarization, and refinement tasks with different input and output programming languages. They identify that prompt pooling methods suffer from unstable training of the prompt selection mechanism and propose Prompt Pooling with Teacher Forcing (PP-TF) to stabilize training. Their evaluation shows PP-TF outperforms standard prompt pooling by 21.54% and establishes a training pipeline applicable to CL on code models.

## Method Summary
The paper introduces Prompt Pooling with Teacher Forcing (PP-TF) to address catastrophic forgetting in prompt-based continual learning for code generation. PP-TF stabilizes training by removing the unstable E-Step (prompt selection based on cosine similarity) and instead assigning each (key, prompt) pair to fixed tasks. The method maintains a pool of 500 learnable prompts with associated keys, where the query function maps inputs to query vectors. During training, PP-TF selects top-k prompts assigned to the current task, preventing keys from being pulled toward queries from different tasks. The authors also establish a training pipeline and evaluate multiple CL methods including experience replay with full model finetuning on their newly introduced CODE TASK-CL benchmark.

## Key Results
- PP-TF improves Prompt Pooling performance by 21.54% on the CODE TASK-CL benchmark
- Experience Replay with full model finetuning (CodeT5 + ER) outperforms prompting-based methods when a replay buffer is available
- CODE TASK-CL benchmark provides a comprehensive evaluation framework for CL methods on code generation tasks across four distinct programming tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt Pooling suffers from catastrophic forgetting due to unstable training of the prompt selection mechanism when tasks have stark distribution shifts
- Mechanism: During sequential learning, the E-Step (selecting top-k keys based on cosine similarity) causes prompt keys to move toward the current task's queries, pulling them away from previous tasks' queries. This leads to unstable key-query alignment across tasks.
- Core assumption: The distribution shift between tasks is large enough that the same prompt keys cannot effectively represent queries from multiple tasks simultaneously.
- Evidence anchors:
  - [abstract] "effective methods like Prompt Pooling (PP) suffer from catastrophic forgetting due to the unstable training of the prompt selection mechanism caused by stark distribution shifts in coding tasks"
  - [section 4] "In the CL context, tasks are sequentially trained which makes training unstable. Hence, we propose Prompt Pooling with Teacher Forcing(PP-TF) that removes the E-Step by assigning each {(ki, Pi)} pair to fixed tasks"
  - [corpus] Weak - the related papers focus on general continual learning but don't specifically address the E-Step instability issue
- Break condition: If tasks have similar distributions or the query function produces similar representations across tasks, the E-Step may not cause catastrophic forgetting

### Mechanism 2
- Claim: PP-TF stabilizes training by enforcing constraints on prompt selection through task assignments
- Mechanism: By assigning specific (key, prompt) pairs to fixed tasks during training, PP-TF prevents the dynamic reallocation of prompts that causes instability. The M-Step optimization keeps keys aligned with their assigned task's queries.
- Core assumption: Task assignments can be determined a priori and that the number of prompts is sufficient to allocate meaningful subsets to each task.
- Evidence anchors:
  - [abstract] "Prompt Pooling with Teacher Forcing(PP-TF), that stabilizes training by enforcing constraints on the prompt selection mechanism"
  - [section 4] "we propose Prompt Pooling with Teacher Forcing(PP-TF) that removes the E-Step by assigning each {(ki, Pi)} pair to fixed tasks and only performs the M-Step of optimizing the keys"
  - [section 5.2.2] "When learning the CodeTrans, some keys used for the previous task are pulled toward CodeTrans's queries" - this is what PP-TF prevents
- Break condition: If the number of tasks exceeds the number of available prompts, or if task assignments are unknown or cannot be determined, the constraint-based approach may fail

### Mechanism 3
- Claim: Experience Replay (ER) with full model finetuning (CodeT5 + ER) outperforms prompting-based methods when a replay buffer is available
- Mechanism: ER provides direct gradient signals from previous tasks that prevent forgetting by keeping parameters from drifting too far from previously learned task representations.
- Core assumption: A sufficient replay buffer exists and can be maintained, and the model has enough capacity to learn all tasks without interference.
- Evidence anchors:
  - [abstract] "we find when a replay buffer is available, the simple experience-replay (De Lange et al., 2019) method outperforms other CL methods"
  - [section 5.1] "Experience Replay (ER) (Riemer et al., 2019) involves maintaining a memory buffer B of examples from the previous task"
  - [section 5.2.1] "CodeT5 + ER which finetunes the full CodeT5 model with ER performs the best with an average test BLEU score of 49.21%"
- Break condition: If the replay buffer is too small or if privacy constraints prevent storing previous examples, ER may not be feasible

## Foundational Learning

- Concept: Catastrophic Forgetting
  - Why needed here: The paper's central problem is that standard continual learning methods suffer from catastrophic forgetting when applied to code generation tasks with large distribution shifts
  - Quick check question: What happens to a neural network's performance on task A when it is sequentially trained on task B without any forgetting prevention mechanism?

- Concept: Prompt Tuning and Prompt Pooling
  - Why needed here: The paper compares various prompting-based continual learning methods and proposes PP-TF as a solution to the instability problem in Prompt Pooling
  - Quick check question: How does Prompt Pooling differ from Task-Specific Prompt Tuning in terms of parameter efficiency and knowledge sharing across tasks?

- Concept: BLEU Score and Continual Learning Metrics
  - Why needed here: The paper evaluates methods using BLEU score and introduces <BLEU> and <Forget> metrics specific to continual learning evaluation
  - Quick check question: How is the <Forget> metric calculated and what does it measure in the context of continual learning?

## Architecture Onboarding

- Component map:
  - Pre-trained CodeT5 model (frozen during prompting-based methods)
  - Prompt pool: 500 learnable prompts with associated keys
  - Query function: Fixed CodeT5 encoder that maps inputs to query vectors
  - Replay buffer: Optional memory of previous task examples (5000 samples in main experiments)
  - Task assignment mechanism: For PP-TF, maps prompts to specific tasks

- Critical path:
  1. Encode input using fixed query function to obtain query vector
  2. For PP: Select top-k prompts using cosine similarity between query and all keys
  3. For PP-TF: Select top-k prompts assigned to the current task
  4. Prepend selected prompts to input and feed to CodeT5
  5. Compute loss and update only selected prompts and keys
  6. For methods with ER: Sample from replay buffer and include in training

- Design tradeoffs:
  - Prompt Pooling vs Task-Specific Prompts: PP is more parameter-efficient but suffers from instability; TSPT is stable but scales linearly with task count
  - Full finetuning vs Prompt tuning: Full finetuning (CodeT5 + ER) performs best but requires replay buffer and more computation; prompting is more efficient but may underperform
  - Buffer size: Larger buffers improve ER performance but increase memory requirements and may have privacy implications

- Failure signatures:
  - Catastrophic forgetting: Test BLEU drops significantly on earlier tasks after learning new tasks
  - Unstable training: Fluctuating validation BLEU scores during training, especially when switching between tasks
  - Poor knowledge sharing: Task-Specific Prompts perform well but don't leverage shared knowledge across tasks

- First 3 experiments:
  1. Implement PP-TF and verify it outperforms standard PP on the CODE TASK-CL benchmark (expect ~21% improvement)
  2. Test PP-TF with different buffer sizes to find the minimum buffer size needed for competitive performance with CodeT5 + ER
  3. Evaluate PP-TF on a simplified version of CODE TASK-CL with fewer tasks or less distribution shift to verify the mechanism is working as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Prompt Pooling with Teacher Forcing (PP-TF) scale with an increasing number of tasks in the CODE TASK-CL benchmark?
- Basis in paper: [inferred] The paper mentions that PP-TF is recommended for cases with no buffer and a large number of tasks, but does not provide experimental results for scaling to many tasks.
- Why unresolved: The paper only evaluates the methods on a fixed set of four tasks, so there is no empirical evidence for how the methods perform as the number of tasks increases.
- What evidence would resolve it: Experiments evaluating PP-TF and other methods on CODE TASK-CL benchmark with a larger number of tasks (e.g., 10, 20, or 50 tasks) would provide insight into how well the methods scale.

### Open Question 2
- Question: How does the performance of PP-TF compare to other state-of-the-art continual learning methods specifically designed for code generation tasks?
- Basis in paper: [explicit] The paper compares PP-TF to several popular CL methods from NLP and Vision domains, but does not explore methods specifically designed for code generation tasks.
- Why unresolved: The paper focuses on adapting existing CL methods to code generation tasks rather than exploring methods specifically designed for this domain.
- What evidence would resolve it: Implementing and evaluating state-of-the-art CL methods specifically designed for code generation tasks (e.g., methods based on code-specific architectures or pre-trained models) on the CODE TASK-CL benchmark would provide a more comprehensive comparison.

### Open Question 3
- Question: How does the performance of PP-TF change when using different pre-trained code generation models (e.g., CodeT5-large, CodeParrot, or PolyCoder)?
- Basis in paper: [explicit] The paper uses CodeT5-small as the pre-trained model for all experiments, but does not explore the impact of using different pre-trained models.
- Why unresolved: The paper does not provide evidence for how the choice of pre-trained model affects the performance of PP-TF and other CL methods.
- What evidence would resolve it: Replicating the experiments with different pre-trained code generation models and comparing the results would provide insight into the robustness and generalizability of PP-TF and other methods.

## Limitations

- The specific task combinations and distribution shifts in CODE TASK-CL may not fully represent real-world continual learning scenarios
- Hyperparameter sensitivity, particularly for PP-TF's task assignment strategy and experience replay buffer size, is not extensively explored
- The initialization strategy for prompt keys is not specified in detail, which could affect the stability of the E-Step in Prompt Pooling

## Confidence

**High Confidence Claims**:
- PP-TF improves upon standard Prompt Pooling by 21.54% on the CODE TASK-CL benchmark
- Experience Replay with full model finetuning outperforms prompting-based methods when a replay buffer is available
- The CODE TASK-CL benchmark is a valuable resource for evaluating continual learning methods on code generation tasks

**Medium Confidence Claims**:
- The instability in Prompt Pooling is primarily caused by the E-Step's dynamic key-query alignment across tasks with large distribution shifts
- Task assignments in PP-TF can be determined a priori and effectively prevent catastrophic forgetting
- The proposed training pipeline is generally applicable to continual learning on code models

**Low Confidence Claims**:
- PP-TF will consistently outperform other methods across all possible task sequences and distribution shifts in code generation
- The specific improvements observed are solely due to the teacher forcing mechanism and not influenced by other factors like prompt initialization or training schedule
- The benchmark's task design perfectly captures the challenges of real-world code generation continual learning

## Next Checks

1. **Ablation Study on Task Assignment**: Systematically vary the task assignment strategy in PP-TF (random assignment, balanced assignment, assignment based on task similarity) to verify that the specific assignment method is responsible for the performance gains and not just the constraint enforcement itself.

2. **Cross-Benchmark Evaluation**: Evaluate PP-TF and other methods on additional code generation datasets or task combinations not included in CODE TASK-CL to test the generalizability of the findings beyond the specific benchmark design.

3. **Buffer Size Sensitivity Analysis**: Conduct experiments with varying replay buffer sizes (e.g., 1000, 5000, 10000 samples) for both CodeT5 + ER and PP-TF (if modified to use ER) to determine the minimum buffer size required for competitive performance and to understand the tradeoff between memory requirements and performance.