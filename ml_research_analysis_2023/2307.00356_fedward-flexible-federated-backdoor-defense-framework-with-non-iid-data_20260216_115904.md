---
ver: rpa2
title: 'Fedward: Flexible Federated Backdoor Defense Framework with Non-IID Data'
arxiv_id: '2307.00356'
source_url: https://arxiv.org/abs/2307.00356
tags:
- fedward
- backdoor
- clients
- federated
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fedward, a Flexible Federated Backdoor Defense
  Framework to address backdoor attacks in federated learning (FL). The framework
  uses Amplified Magnitude Sparsification (AmGrad) and Adaptive OPTICS Clustering
  (AutoOPTICS) to effectively identify and eliminate malicious models while maintaining
  benign performance.
---

# Fedward: Flexible Federated Backdoor Defense Framework with Non-IID Data

## Quick Facts
- arXiv ID: 2307.00356
- Source URL: https://arxiv.org/abs/2307.00356
- Reference count: 19
- Key outcome: Fedward reduces backdoor attack success rates by 33% to 75% compared to existing clustering defenses across three benchmark datasets

## Executive Summary
This paper introduces Fedward, a federated learning defense framework designed to combat backdoor attacks in Non-IID data scenarios. The framework combines three key components: Amplified Magnitude Sparsification (AmGrad) to enhance malicious model detection, Adaptive OPTICS Clustering (AutoOPTICS) to clearly separate malicious and benign models, and Adaptive Clipping to maintain performance in heterogeneous environments. Evaluated on MNIST, FMNIST, and CIFAR10 datasets, Fedward demonstrates significant improvements in defending against distributed backdoor attacks while maintaining benign model accuracy.

## Method Summary
Fedward is a federated backdoor defense framework that uses amplified magnitude sparsification (AmGrad) to extract and amplify major model updates, adaptive OPTICS clustering (AutoOPTICS) to separate malicious from benign models using clear distance criteria, and adaptive clipping based on benign group size to maintain performance in Non-IID scenarios. The method is evaluated against five baseline defenses using three benchmark datasets under varying levels of poisoning data rate (PDR) and Non-IID data rate (NIR).

## Key Results
- Fedward reduces average backdoor attack success rates by 33% to 75% compared to existing clustering defense methods
- The framework achieves backdoor attack success rates of 96.98%, 90.74%, and 89.8% on MNIST, FMNIST, and CIFAR10 respectively under Non-IID data scenarios
- Maintains benign model accuracy while effectively eliminating backdoor attacks across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AmGrad amplifies malicious model updates by isolating and scaling the largest gradients in each layer
- Mechanism: The method extracts the sign vector and maximum absolute gradient from each layer, then multiplies them to amplify the largest magnitude components
- Core assumption: Malicious models have larger gradient magnitudes than benign models in at least some layers
- Evidence anchors:
  - [abstract] "we propose amplified magnitude sparsification (AmGrad) to extract the major local model update and then amplify the major update from the maximum absolute gradient in each layer of the model"
  - [section] "our Fedward presents amplified magnitude sparsification (AmGrad)...to sustain sign vector {-1, 1 } and the maximum each layer of model update"
  - [corpus] No direct evidence in corpus neighbors about gradient magnitude amplification
- Break condition: If benign and malicious models have similar gradient magnitude distributions across all layers

### Mechanism 2
- Claim: AutoOPTICS clustering effectively separates malicious from benign models using clear distance criteria
- Mechanism: Uses OPTICS clustering with distance-based separation instead of density-based clustering like HDBSCAN
- Core assumption: Malicious models form a distinct cluster with different feature distributions than benign models
- Evidence anchors:
  - [abstract] "we adopt the adaptive OPTICS clustering ( AutoOPTICS) approach, which has clear distance criteria for dividing malicious models and benign models"
  - [section] "our Fedward adopts adaptive OPTICS clustering (line 9 in Algorithm 1), which applies to varying degrees of deviation of X"
  - [corpus] Corpus contains related work on clustering defenses but no specific evidence about OPTICS effectiveness
- Break condition: If malicious models are distributed throughout the benign cluster rather than forming a distinct group

### Mechanism 3
- Claim: Adaptive clipping maintains performance in Non-IID scenarios by using benign group size as boundary constraint
- Mechanism: Sets clipping threshold based on the number of samples in the benign group rather than fixed values
- Core assumption: The number of benign clients provides a reliable constraint for setting appropriate clipping bounds
- Evidence anchors:
  - [abstract] "Fedward uses the adaptive clipping method by regarding the number of samples in the benign group as constraints on the boundary"
  - [section] "the adaptive clipping method takes the number of samples in the benign group as constraints on the boundary for applying to the Non-IID scenario"
  - [corpus] No direct corpus evidence about adaptive clipping based on benign group size
- Break condition: If the benign group size is too small or too variable to provide stable clipping bounds

## Foundational Learning

- Concept: Federated Learning fundamentals
  - Why needed here: Understanding how federated learning works is essential to grasp why backdoor attacks are possible and how defenses work
  - Quick check question: What are the key differences between centralized and federated learning that affect security considerations?

- Concept: Clustering algorithms (DBSCAN vs OPTICS vs HDBSCAN)
  - Why needed here: Different clustering approaches have different properties that affect their ability to detect malicious clients in Non-IID settings
  - Quick check question: How does OPTICS' use of distance criteria differ from HDBSCAN's density-based approach in handling heterogeneous data distributions?

- Concept: Gradient sparsification techniques
  - Why needed here: Understanding how gradient sparsification works helps explain why amplifying the largest gradients can help detect malicious updates
  - Quick check question: What is the relationship between gradient magnitude and the potential for malicious behavior in federated learning?

## Architecture Onboarding

- Component map: Local training → AmGrad computation → model update submission → distance computation → AutoOPTICS clustering → adaptive clipping → global model aggregation

- Critical path:
  1. Local models are trained and uploaded with AmGrad-augmented gradients
  2. Server computes pairwise distances between all uploaded models
  3. AutoOPTICS identifies the benign cluster based on distance criteria
  4. Adaptive clipping applies based on the benign group size
  5. Clipped models are aggregated to form the global model

- Design tradeoffs:
  - AmGrad vs no sparsification: Better malicious detection but potential information loss
  - OPTICS vs HDBSCAN: Clearer distance criteria but potentially higher computational cost
  - Adaptive clipping vs fixed clipping: Better Non-IID performance but requires benign group detection first

- Failure signatures:
  - High AER despite defense activation indicates clustering failure
  - Drastic accuracy drop indicates over-aggressive clipping
  - ASR remains high indicates AmGrad not amplifying malicious updates sufficiently

- First 3 experiments:
  1. Test AmGrad alone on IID data to verify gradient amplification works
  2. Test AutoOPTICS clustering on synthetic data with known malicious clusters
  3. Test adaptive clipping with controlled benign group sizes to find optimal clipping thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Fedward perform under varying degrees of data heterogeneity beyond the tested Non-IID scenarios?
- Basis in paper: [explicit] The paper states Fedward can maintain performance for the Non-IID scenario and evaluates it on benchmark datasets, but does not explore the full spectrum of data heterogeneity.
- Why unresolved: The experimental setup only tests specific Non-IID rates (NIR = 25%, 50%, 75%) and does not explore extreme or varying degrees of heterogeneity.
- What evidence would resolve it: Testing Fedward on datasets with different levels of heterogeneity, including extreme cases, and comparing its performance to other methods.

### Open Question 2
- Question: Can Fedward's components (AmGrad, AutoOPTICS, Adaptive Clipping) be adapted for use in other federated learning defense frameworks?
- Basis in paper: [explicit] The paper introduces these components as part of Fedward, but does not discuss their potential application in other frameworks.
- Why unresolved: The paper focuses on the effectiveness of these components within Fedward, without exploring their adaptability to other defense mechanisms.
- What evidence would resolve it: Implementing these components in other federated learning defense frameworks and evaluating their effectiveness.

### Open Question 3
- Question: What is the impact of Fedward on the convergence speed and overall efficiency of the federated learning process?
- Basis in paper: [inferred] The paper emphasizes Fedward's ability to eliminate backdoor attacks while maintaining performance, but does not discuss its impact on convergence speed or efficiency.
- Why unresolved: The paper does not provide any metrics or analysis on how Fedward affects the speed or efficiency of the federated learning process.
- What evidence would resolve it: Measuring the convergence speed and computational efficiency of Fedward compared to other defense methods in federated learning scenarios.

## Limitations

- Limited evaluation to three benchmark datasets with relatively small model sizes
- Lack of ablation studies to isolate the contribution of each component
- Absence of testing against sophisticated adaptive attackers that could circumvent the defense

## Confidence

- AmGrad mechanism: Low - lacks empirical validation showing consistent gradient magnitude differences
- AutoOPTICS clustering: Medium - claims clear distance criteria but lacks comparative analysis with other approaches
- Adaptive clipping: Low - conceptually sound but lacks rigorous analysis of stability under varying Non-IID conditions

## Next Checks

1. Conduct ablation studies to measure the individual contribution of AmGrad, AutoOPTICS, and adaptive clipping to overall defense performance
2. Test the framework against adaptive backdoor attacks that specifically target the defense mechanisms
3. Evaluate performance on larger, more complex datasets (e.g., ImageNet subset) with deeper model architectures