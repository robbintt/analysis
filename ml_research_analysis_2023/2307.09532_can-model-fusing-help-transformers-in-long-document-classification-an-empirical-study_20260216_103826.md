---
ver: rpa2
title: Can Model Fusing Help Transformers in Long Document Classification? An Empirical
  Study
arxiv_id: '2307.09532'
source_url: https://arxiv.org/abs/2307.09532
tags:
- classification
- document
- long
- fusing
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates model fusing as a way to adapt BERT-like
  transformers for long document classification. The approach divides long documents
  into parts, trains separate BERT models on each part, and then fuses their weights
  via averaging before fine-tuning the fused model.
---

# Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study

## Quick Facts
- arXiv ID: 2307.09532
- Source URL: https://arxiv.org/abs/2307.09532
- Reference count: 20
- Primary result: Model fusing underperforms BERT and Longformer baselines on long document classification

## Executive Summary
This paper investigates model fusing as an approach to adapt BERT-like transformers for long document classification. The method divides long documents into parts, trains separate BERT models on each part, fuses their weights via averaging, and fine-tunes the fused model. Experiments on four document classification datasets show that this approach underperforms compared to standard BERT and Longformer baselines. The authors suggest that the poor performance stems from incorrect assumptions about equal part contributions and information loss during document splitting.

## Method Summary
The approach divides long documents into three equal parts, trains separate BERT models on each part using standard fine-tuning configurations, fuses the models by averaging their weights, and then fine-tunes the fused model on combined part data. The method assumes that averaging weights preserves information about different document parts and that fine-tuning can correct any misalignment between hidden and output layers.

## Key Results
- Model fusing achieves F1 scores around 0.45-0.55 on document classification tasks
- Longformer achieves F1 scores up to 0.83, significantly outperforming model fusing
- The approach consistently underperforms both standard BERT and Longformer baselines across all four tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model fusing can theoretically combine complementary patterns learned by different sub-models.
- Mechanism: When multiple BERT models are trained on different document parts, they learn part-specific representations. Averaging their hidden layer weights creates a fused model that supposedly integrates these representations.
- Core assumption: All document parts contribute equally to the final classification, and averaging weights preserves meaningful information.
- Evidence anchors:
  - [abstract] "Model Fusion refers to the idea of combining several fine-tuned models... The motivation behind using Model Fusion is that multiple models can identify different patterns using different parts of their network, and it is possible to merge multiple models into one model, which will be capable of having all information compressed into a single model."
  - [section] "By averaging the weights, we assume that the characteristics of each part of the document are being merged into one fused model."
  - [corpus] Weak evidence - no related papers directly validate averaging weights for long document classification.
- Break condition: If parts contribute unequally to classification, or if averaging destroys important activation patterns, the fused model will underperform.

### Mechanism 2
- Claim: Dividing documents into equal parts allows each sub-model to focus on local information without sequence length constraints.
- Mechanism: Each part is trained independently on a BERT model, bypassing the 512-token limit. The fused model theoretically gains capacity to handle longer sequences by combining these local models.
- Core assumption: Local information learned from each part can be effectively combined to understand the full document.
- Evidence anchors:
  - [section] "The main idea is to understand the data in a part-localised manner to tackle the length issue."
  - [abstract] "To implement this idea, we divide long documents into multiple parts and use these parts to train part-wise models."
  - [corpus] Moderate evidence - similar approaches exist for long text processing but not specifically using model fusing.
- Break condition: If document structure means information flow breaks at part boundaries, or if local context is insufficient for global understanding.

### Mechanism 3
- Claim: Fine-tuning the fused model on combined part data can correct misalignment between hidden and output layers.
- Mechanism: After averaging weights, the fused model undergoes further fine-tuning using all parts of documents together, theoretically aligning the merged hidden representations with the classification task.
- Core assumption: The fine-tuning step can adequately correct any degradation from the averaging process.
- Evidence anchors:
  - [section] "This step is important as once we merge the models together, the weights of hidden layers are not finely coupled with the output layers. In order to correct this, further fine-tuning step is important and performed using all parts of the document."
  - [abstract] "Once we complete this, the fused model is ready to predict on the test data."
  - [corpus] No direct evidence in corpus for this specific fine-tuning approach after model fusing.
- Break condition: If averaging causes irreversible damage to the model's decision-making process, fine-tuning cannot adequately recover performance.

## Foundational Learning

- Concept: Understanding BERT's architecture and limitations
  - Why needed here: The entire approach builds on BERT, and the 512-token limit is the core problem being addressed.
  - Quick check question: What happens to tokens beyond position 512 in standard BERT during training and inference?

- Concept: Weight averaging in neural networks
  - Why needed here: The fusing mechanism relies on averaging weights across multiple models, which has mathematical implications for the resulting model's behavior.
  - Quick check question: If you average weights of 5 and 0.1, what is the result and how does it compare to the original values?

- Concept: Document structure and information distribution
  - Why needed here: The approach assumes equal contribution from all document parts, which may not reflect how information is actually distributed in documents.
  - Quick check question: In a typical legal document, which sections (beginning, middle, end) usually contain the most classification-relevant information?

## Architecture Onboarding

- Component map: Document → Part 1 (BERT model 1) → Part 2 (BERT model 2) → Part 3 (BERT model 3) → Weight averaging → Fused model → Fine-tuning → Prediction pipeline
- Critical path: Data preparation → Sub-model training → Model fusing → Fine-tuning → Prediction
- Design tradeoffs: Simplicity of average fusing vs potential information loss; equal part division vs document structure; computational cost of multiple models vs single long-sequence model
- Failure signatures: Low F1 scores compared to baselines; similar performance trends across datasets but consistently worse; performance degradation when averaging weights with large differences
- First 3 experiments:
  1. Test simple averaging vs weighted averaging of sub-model weights on a small dataset
  2. Compare performance when dividing documents into 2, 3, and 4 parts
  3. Evaluate the impact of fine-tuning duration on fused model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would model fusing perform if parts were weighted differently based on their contribution to the final classification?
- Basis in paper: [explicit] The authors suggest that the equal contribution assumption may be flawed and propose weighted bias as a potential improvement.
- Why unresolved: The paper only tested equal weighting and did not explore weighted fusion strategies.
- What evidence would resolve it: Comparative experiments showing F1 scores for weighted vs. equal weighting across the same datasets.

### Open Question 2
- Question: Would preserving information flow between document parts (e.g., through recurrence or cross-attention) improve fusing performance?
- Basis in paper: [inferred] The authors note that dividing documents may cause "information flow breaks" that models could suffer from.
- Why unresolved: The paper used independent part training without mechanisms to maintain inter-part context.
- What evidence would resolve it: Experiments comparing standard splitting with approaches that maintain inter-part dependencies.

### Open Question 3
- Question: Could alternative fusion methods (e.g., attention-based or gate-based) outperform simple averaging for long document classification?
- Basis in paper: [explicit] The authors acknowledge that averaging weights might not be ideal and suggest this as a limitation.
- Why unresolved: The paper only tested average fusing and did not explore more sophisticated fusion techniques.
- What evidence would resolve it: Comparative experiments testing different fusion methods while keeping other variables constant.

## Limitations

- The core assumption that averaging weights preserves meaningful information about long documents appears to be flawed, as evidenced by the poor performance compared to baselines
- The equal contribution assumption for all document parts may not reflect actual information distribution in documents
- The study lacks ablation experiments to isolate which component of the approach causes the performance degradation

## Confidence

- Low Confidence: The core claim that model fusing can effectively handle long document classification
- Medium Confidence: The observation that Longformer outperforms both BERT and the proposed model fusing approach on long documents
- Medium Confidence: The hypothesis that information loss during document splitting contributes to poor performance

## Next Checks

1. Conduct an ablation study comparing model fusing performance using different averaging strategies (simple average vs weighted average) and with overlapping document parts to isolate whether information loss or averaging methodology is the primary cause of underperformance.

2. Perform error analysis on a subset of documents to determine whether mistakes correlate with specific parts of documents, helping to validate or refute the equal contribution assumption.

3. Implement a sliding window approach where overlapping segments are processed by a single model and combined, providing a stronger baseline for how to handle long documents compared to pure averaging.