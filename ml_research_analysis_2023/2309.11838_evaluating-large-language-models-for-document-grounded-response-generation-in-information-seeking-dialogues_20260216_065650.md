---
ver: rpa2
title: Evaluating Large Language Models for Document-grounded Response Generation
  in Information-Seeking Dialogues
arxiv_id: '2309.11838'
source_url: https://arxiv.org/abs/2309.11838
tags:
- information
- response
- dialogue
- agent
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) like ChatGPT
  for document-grounded response generation in information-seeking dialogues. The
  authors use the MultiDoc2Dial corpus, which contains task-oriented dialogues grounded
  in multiple documents across four social service domains.
---

# Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues

## Quick Facts
- **arXiv ID:** 2309.11838
- **Source URL:** https://arxiv.org/abs/2309.11838
- **Reference count:** 7
- **Key outcome:** GPT-based methods outperform shared task winners and human responses in appropriateness for document-grounded dialogues, despite including information not in the grounding segments

## Executive Summary
This paper evaluates large language models (LLMs) for document-grounded response generation in information-seeking dialogues using the MultiDoc2Dial corpus. The authors compare two GPT-3.5-turbo methods: one using only dialogue context (ChatCompletion) and another using document retrieval with LlamaIndex (GPTLama). Human evaluation reveals that both GPT variants significantly outperform the shared task winning system and human responses on appropriateness, despite generating responses that often include information not present in the relevant document segments. The study demonstrates strong potential for LLMs in this domain while highlighting challenges in assessing accuracy and the prevalence of hallucinations.

## Method Summary
The study evaluates two prompting methods using GPT-3.5-turbo on the MultiDoc2Dial corpus containing task-oriented dialogues across four social service domains. The ChatCompletion method sends dialogue context and system prompts to the LLM without document retrieval, while the GPTLama method uses LlamaIndex to extract relevant information from indexed documents before combining it with user input for response generation. Responses are evaluated using automatic metrics (RougeL, METEOR) and human evaluation where annotators rate appropriateness on a 5-point Likert scale and identify whether responses contain information from grounding segments or external sources.

## Key Results
- GPTChat achieved an average appropriateness score of 3.96, significantly higher than GPTLama (3.68) and human responses (3.72)
- 73% of GPTChat responses and 60% of GPTLama responses contained information not present in the grounding segments
- Automatic metrics like RougeL and METEOR showed poor correlation with human judgment of appropriateness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The GPTChat method can generate highly appropriate responses in document-grounded dialogue tasks even without explicit retrieval from documents.
- Mechanism: By providing dialogue context and a system prompt specifying the domain and role, the GPT model leverages its pretraining knowledge to understand the user's query and generate contextually relevant responses.
- Core assumption: The topics covered in the four domains of the MultiDoc2Dial corpus were part of the GPT model's training data, enabling it to understand and respond appropriately.
- Evidence anchors:
  - [abstract]: "While both ChatGPT variants are more likely to include information not present in the relevant segments, possibly including a presence of hallucinations, they are rated higher than both the shared task winning system and human responses."
  - [section 3.2.1]: "The GPTChat system, therefore, provides a reference for how well the GPT-model performs on the given task without having additional knowledge extracted from the associated documents, but instead relies on the capabilities of the GPT-model to understand the input and retrieve information from the data it was trained on."
- Break condition: If the topics covered in the dialogue domains were not part of the GPT model's training data, or if the dialogue context is insufficient for the model to understand the user's query.

### Mechanism 2
- Claim: The GPTLama method improves response accuracy by grounding it in the associated documents.
- Mechanism: LlamaIndex extracts relevant information from the indexed documents using semantic search, and this information is then combined with the user input to generate a response via the LLM.
- Core assumption: The information retrieved from the documents by LlamaIndex is relevant and sufficient to improve the accuracy of the response generated by the LLM.
- Evidence anchors:
  - [section 3.2.2]: "By using LlamaIndex tools we aimed at improved accuracy of response generation by grounding it in the associated documents. Since it first retrieves knowledge from documents and then sends it together with the user input to the LLM for response generation its closer to the retrieval-augmented response generation method in the two baseline models."
- Break condition: If the information retrieved by LlamaIndex is not relevant or insufficient, or if the combination of retrieved information and user input is not effective for generating an accurate response.

### Mechanism 3
- Claim: Human evaluation is necessary to assess the appropriateness of responses in document-grounded dialogue tasks, as automatic evaluation metrics are not sufficient.
- Mechanism: Human annotators rate the appropriateness of the responses on a 5-point Likert scale and assess whether the responses contain information from the grounding segments or not.
- Core assumption: Human judgment is a more reliable indicator of response appropriateness than automatic metrics in this task.
- Evidence anchors:
  - [abstract]: "Observing that document-grounded response generation via LLMs cannot be adequately assessed by automatic evaluation metrics as they are significantly more verbose, we perform a human evaluation where annotators rate the output of the shared task winning system, the two Chat-GPT variants outputs, and human responses."
  - [section 5.2]: "Objective evaluation results showed that typical word-overlap based metrics are not suitable to fully assess the performance of these methods and human evaluation indicated that ChatGPT-based models have strong potential in this domain, even exceeding appropriateness-scores for the human-authored reference responses."
- Break condition: If human annotators are not consistent in their judgments or if the evaluation criteria are not clear enough.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG is a method that combines information retrieval with text generation, which is relevant to the GPTLama method that retrieves information from documents before generating a response.
  - Quick check question: What are the main components of a RAG architecture and how do they interact?

- Concept: Prompt engineering
  - Why needed here: Both GPTChat and GPTLama methods rely on carefully crafted prompts to guide the LLM in generating appropriate responses. Understanding how to design effective prompts is crucial.
  - Quick check question: What are the key elements of a prompt for a document-grounded dialogue task and how do they influence the generated response?

- Concept: Evaluation metrics for dialogue systems
  - Why needed here: The paper discusses both automatic metrics (e.g., RougeL, METEOR) and human evaluation for assessing the performance of the response generation methods. Understanding the strengths and limitations of different metrics is important.
  - Quick check question: What are the main challenges in evaluating document-grounded dialogue systems and how do automatic metrics and human evaluation address these challenges?

## Architecture Onboarding

- Component map:
  MultiDoc2Dial corpus -> Preprocessing -> GPT-3.5-turbo (ChatCompletion or GPTLama) -> Generated responses -> Evaluation (automatic metrics + human evaluation)

- Critical path:
  1. Load the MultiDoc2Dial corpus and preprocess the dialogues and documents
  2. For GPTChat: Send the dialogue context and system prompt to the GPT model to generate a response
  3. For GPTLama: Use LlamaIndex to extract relevant information from the documents, combine it with the user input, and send it to the GPT model to generate a response
  4. Evaluate the generated responses using automatic metrics and human evaluation

- Design tradeoffs:
  - GPTChat vs. GPTLama: GPTChat is simpler and faster but relies solely on the GPT model's pretraining knowledge, while GPTLama is more complex but can ground the response in external documents
  - Automatic vs. human evaluation: Automatic metrics are faster and more scalable but may not capture the nuances of response appropriateness, while human evaluation is more reliable but more time-consuming and subjective

- Failure signatures:
  - GPTChat: Responses may be appropriate but contain hallucinations or information not present in the documents
  - GPTLama: Responses may be grounded in the documents but less fluent or coherent if the retrieved information is not well-integrated
  - Automatic evaluation: Metrics may not correlate well with human judgment, especially for longer and more diverse responses

- First 3 experiments:
  1. Run GPTChat on a small subset of the MultiDoc2Dial test set and manually inspect the generated responses for appropriateness and hallucinations
  2. Run GPTLama on the same subset and compare the generated responses with GPTChat in terms of grounding and fluency
  3. Evaluate both methods using automatic metrics (e.g., RougeL, METEOR) and compare the results with human evaluation on a small sample of responses

## Open Questions the Paper Calls Out

The paper highlights three main open questions: (1) How to better assess factual accuracy and veracity of generated responses given the prevalence of hallucinations, (2) Whether the strong performance of LLMs is due to pre-training exposure to similar topics rather than genuine comprehension, and (3) How to scale human evaluation methods to larger datasets while maintaining consistency and reliability.

## Limitations
- Narrow domain coverage with only four social service domains may limit generalizability
- Strong performance may result from GPT model's pre-training exposure rather than true understanding
- Small human evaluation sample (25 dialogues) may not capture full variability of responses

## Confidence

Medium confidence in the core findings:

| Claim | Confidence Level |
|-------|------------------|
| GPTChat outperforms other methods in appropriateness | High |
| Both GPT variants generate information outside grounding segments | High |
| Automatic metrics poorly correlate with human judgment | High |
| Pre-training exposure contributes to strong performance | Medium |

## Next Checks

1. Test the methods on out-of-domain dialogues to assess generalizability and determine if performance relies on pre-training exposure
2. Conduct a larger-scale human evaluation with multiple annotators per response to establish inter-rater reliability and assess consistency
3. Implement a follow-up verification task where humans check the factual accuracy of generated responses against source documents to quantify the practical impact of hallucinations