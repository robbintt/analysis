---
ver: rpa2
title: 'gym-saturation: Gymnasium environments for saturation provers (System description)'
arxiv_id: '2309.09022'
source_url: https://arxiv.org/abs/2309.09022
tags:
- https
- gym-saturation
- clause
- provers
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new version of the gym-saturation package,
  a collection of OpenAI Gym environments for guiding saturation-style provers using
  reinforcement learning. The key improvements include compatibility with two popular
  provers (Vampire and iProver), decoupling of proof state representation from reinforcement
  learning, and examples of using external embedding models.
---

# gym-saturation: Gymnasium environments for saturation provers (System description)

## Quick Facts
- arXiv ID: 2309.09022
- Source URL: https://arxiv.org/abs/2309.09022
- Authors: 
- Reference count: 40
- This paper introduces an updated version of gym-saturation with improved prover compatibility, decoupled proof state representation, and demonstration of external embedding models for guiding saturation-style provers using reinforcement learning.

## Executive Summary
This paper presents a new version of gym-saturation, a collection of OpenAI Gym environments for guiding saturation-style theorem provers using reinforcement learning. The key improvements include compatibility with two popular provers (Vampire and iProver), decoupling of proof state representation from reinforcement learning, and examples of using external embedding models. The authors demonstrate the ease of experimentation by applying two reinforcement learning algorithms (Thompson sampling and Proximal policy optimization) to guide the provers. The updated package follows the Gymnasium API instead of the outdated OpenAI Gym, enabling broader ecosystem integration.

## Method Summary
The method involves creating Gymnasium environments that interface with standard versions of Vampire and iProver provers. The environments use observation wrappers to transform raw clause data into tensor representations, allowing different embedding strategies without modifying the underlying prover. The action space consists of clause selection from available clauses, with a simple reward function providing 1.0 for successful refutation and 0.0 otherwise. The authors demonstrate usage with Thompson sampling for multi-armed bandit setup and Proximal Policy Optimization (PPO) with observation wrappers for clause embeddings, using Ray RLlib for implementation.

## Key Results
- Compatible with standard Vampire and iProver binaries without requiring modifications
- Decouples proof state representation from RL through observation wrapper architecture
- Demonstrates integration with external embedding models like ast2vec for clause representation
- Provides examples using Thompson sampling and PPO algorithms for guiding provers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: gym-saturation decouples proof state representation from RL per se
- Mechanism: By providing observation wrappers that transform raw clause data into tensor representations, the environment allows different embedding strategies without modifying the underlying prover
- Core assumption: Different embedding strategies can be swapped without affecting prover performance or RL training dynamics
- Evidence anchors:
  - [abstract]: "we also have decoupled the proof state representation from reinforcement learning per se and provided examples of using a known ast2vec Python code embedding model"
  - [section 3.2]: "Observation is a Python dictionary with several keys: real_obs is a tuple of all clauses... can be transformed to tensor representation by so-called observation wrappers"
- Break condition: If observation wrappers introduce significant latency or the embedding quality becomes the bottleneck for RL performance

### Mechanism 2
- Claim: Using unpatched, stable provers increases research accessibility and reproducibility
- Mechanism: gym-saturation works with standard Vampire and iProver binaries instead of experimental or modified versions, reducing setup complexity
- Core assumption: Standard provers provide sufficient interface flexibility for RL guidance through existing features like manual clause selection mode
- Evidence anchors:
  - [abstract]: "We contribute usage examples with two different provers: Vampire and iProver"
  - [section 3]: "Contrary to that, gym-saturation works with unmodiﬁed stable versions of Vampire and iProver"
- Break condition: If standard provers lack necessary interface features for RL guidance or their performance degrades significantly compared to specialized versions

### Mechanism 3
- Claim: gymnasium API compatibility enables broader ecosystem integration
- Mechanism: gym-saturation follows Gymnasium [35] API instead of outdated OpenAI Gym, allowing use with modern RL frameworks
- Core assumption: Gymnasium API is sufficiently stable and feature-complete for theorem proving applications
- Evidence anchors:
  - [abstract]: "following the updated Gymnasium [35] API instead of the outdated OpenAI Gym"
  - [section 3.1]: "gym-saturation is compatible with Gymnasium [35], a maintained fork of now-outdated OpenAI Gym standard of RL-environments"
- Break condition: If Gymnasium API changes break compatibility or lacks essential features for theorem proving environments

## Foundational Learning

- OpenAI Gym/Gymnasium environment design patterns
  - Why needed here: Understanding the standard RL environment interface is crucial for extending or modifying gym-saturation
  - Quick check question: What are the required methods and attributes for a Gymnasium environment?

- Saturation-style theorem proving basics
  - Why needed here: Understanding how Vampire and iProver work internally helps in designing effective observation and action spaces
  - Quick check question: What is the given clause algorithm and how does it differ from other proving strategies?

- Reinforcement learning algorithm fundamentals
  - Why needed here: Knowing how different RL algorithms (Thompson sampling, PPO) work helps in selecting appropriate methods for theorem proving
  - Quick check question: What are the key differences between policy-based and value-based RL algorithms?

## Architecture Onboarding

- Component map:
  - Environment classes (Vampire-v0, iProver-v0) -> Observation wrappers -> External embedding services (ast2vec server) -> RL algorithms (Ray RLlib)

- Critical path:
  - Environment reset → clause parsing → observation transformation → agent action → prover step → reward calculation → repeat until termination

- Design tradeoffs:
  - Using standard provers vs. modified versions (accessibility vs. performance)
  - Real-time clause embedding vs. precomputed embeddings (flexibility vs. latency)
  - Simple reward structure vs. complex shaping (simplicity vs. learning efficiency)

- Failure signatures:
  - High latency in observation transformation indicates embedding service issues
  - Agent consistently failing to find proofs suggests reward shaping problems
  - Environment crashes during step execution indicate prover communication issues

- First 3 experiments:
  1. Run Vampire-v0 with random action selection to verify basic environment functionality
  2. Implement simple feature-based observation wrapper and test with basic RL algorithm
  3. Connect to external ast2vec service and verify clause embedding transformation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a universal first-order logic embedding model that works across different theorem proving systems (saturation-style provers, tableaux-based provers, SMT-solvers, and semantic reasoners)?
- Basis in paper: [explicit] The authors envision an upcoming project to create a universal first-order logic embedding model usable by various theorem proving systems.
- Why unresolved: This requires developing a representation that captures the nuances of different logical formalisms and proving strategies, which is a complex machine learning challenge.
- What evidence would resolve it: Successful implementation and validation of such a universal embedding model across multiple theorem proving systems, demonstrating improved performance compared to system-specific embeddings.

### Open Question 2
- Question: What is the optimal latency for a representation service in automated theorem proving, and how can we achieve it?
- Basis in paper: [explicit] The authors discuss latency considerations, noting that a representation service needs to be much faster (around 60µs) than their current implementation (2ms average) to keep up with provers like Vampire.
- Why unresolved: Achieving sub-millisecond latency requires advanced optimizations and potentially new hardware or architectural approaches.
- What evidence would resolve it: Development of a representation service that consistently achieves the required latency while maintaining or improving embedding quality, demonstrated through integration with high-performance provers.

### Open Question 3
- Question: How can we generate and prioritize training problems to better transfer search patterns learned on simpler theorems to harder ones?
- Basis in paper: [explicit] The authors hope researchers can focus on advanced questions like generating and prioritizing training problems for better transfer learning.
- Why unresolved: This requires understanding the relationship between problem difficulty and transferable features, as well as developing effective curriculum learning strategies.
- What evidence would resolve it: Empirical results showing improved performance on complex theorems when using a curriculum of increasingly difficult problems, with clear evidence of knowledge transfer.

## Limitations

- The paper provides conceptual architecture without quantitative empirical results comparing the new version against baselines
- Effectiveness depends heavily on quality of external embedding services which are not standardized across theorem proving domains
- Standard provers may still require specialized modifications for optimal RL performance despite claims of sufficient interface flexibility

## Confidence

- High confidence: API compatibility with Gymnasium, basic environment functionality with standard provers
- Medium confidence: Decoupling of proof state representation from RL, use of external embedding models
- Low confidence: Actual performance improvements in theorem proving tasks, scalability with complex problems

## Next Checks

1. Benchmark gym-saturation's performance against previous versions using standardized TPTP problem suites to quantify improvements in proof success rates and training efficiency
2. Systematically test the latency impact of different observation wrapper implementations and external embedding services on RL training convergence
3. Evaluate the robustness of action masking mechanisms under various prover behaviors and clause generation rates to ensure stable RL training