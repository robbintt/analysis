---
ver: rpa2
title: Compositional preference models for aligning LMs
arxiv_id: '2310.13011'
source_url: https://arxiv.org/abs/2310.13011
tags:
- preference
- feature
- score
- features
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes compositional preference models (CPMs), which
  decompose a global preference score into interpretable features and use a prompted
  language model (LM) to score each feature. CPMs then aggregate the feature scores
  using a logistic regression classifier to obtain the final preference score.
---

# Compositional preference models for aligning LMs

## Quick Facts
- arXiv ID: 2310.13011
- Source URL: https://arxiv.org/abs/2310.13011
- Reference count: 40
- This paper proposes compositional preference models (CPMs) that decompose global preference scores into interpretable features and use prompted language models to score each feature, then aggregate scores using logistic regression.

## Executive Summary
This paper addresses key challenges in preference modeling for language model alignment: overfitting, overoptimization, and lack of interpretability. The proposed compositional preference model (CPM) decomposes a global preference score into interpretable features, scores each feature using a prompted language model, and aggregates these scores with logistic regression. The approach aims to provide more robust and interpretable preference models that are less prone to reward hacking while maintaining effective alignment with human preferences.

## Method Summary
The compositional preference model extracts 13 interpretable features (helpfulness, factuality, readability, etc.) from response pairs using prompted language models. These feature scores are standardized and combined through logistic regression to predict preference judgments. The model is trained on pairwise comparison datasets and evaluated using best-of-n sampling experiments to assess robustness, generalization, and resistance to overoptimization. The approach contrasts with standard preference models that directly predict preference from response pairs without feature decomposition.

## Key Results
- CPMs show less divergence between independently trained models, indicating better generalization and robustness
- CPMs demonstrate less divergence from gold preference models under overoptimization, showing better resistance to reward hacking
- Best-of-n samples from CPMs are preferred over samples from conventional preference models in human-like evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing global preference scores into interpretable features reduces overfitting by limiting the parameter space to meaningful human-interpretable dimensions
- Mechanism: The compositional preference model (CPM) transforms a single scalar preference prediction task into a linear combination of feature scores extracted by a language model. This constrains the model to learn only weights for pre-selected features rather than arbitrary correlations in the preference data
- Core assumption: The selected features (helpfulness, factuality, readability, etc.) are genuinely predictive of human preference judgments and capture the underlying dimensions of preference
- Evidence anchors:
  - [abstract]: "CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgement"
  - [section 3.1]: "We use 13 features: helpfulness, specificity, intent, factuality, easy-to-understand, relevance, readability, enough-detail, biased, fail-to-consider-individual-preferences, repetitive, fail-to-consider-context and too-long"
  - [corpus]: Weak evidence - corpus shows related work on preference modeling but doesn't directly validate the feature selection assumption
- Break condition: If the selected features poorly correlate with actual human preferences or miss important dimensions, the decomposition becomes counterproductive

### Mechanism 2
- Claim: Using a prompted language model as a feature extractor provides scalable and robust feature scoring that transfers knowledge from pre-training
- Mechanism: The CPM uses a language model (GPT-3.5 or Flan-T5) to extract scalar scores for each feature by prompting with feature-specific templates. This leverages the LM's pre-existing understanding of concepts like helpfulness or factuality rather than training from scratch
- Core assumption: The language model has learned representations during pre-training that capture human notions of the selected features
- Evidence anchors:
  - [abstract]: "CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that we believe to underlie the human preference judgement"
  - [section 3.1]: "we use a prompted LM to assign to a pair (x, y) a scalar score for each individual feature c = 1, ..., C"
  - [corpus]: Weak evidence - corpus mentions language models for preference elicitation but doesn't specifically address prompted feature extraction
- Break condition: If the language model's pre-training data poorly represents the features or if the prompting strategy fails to elicit accurate scores

### Mechanism 3
- Claim: The linear combination of features via logistic regression provides interpretable decision boundaries while maintaining classification performance
- Mechanism: The feature scores are standardized and combined using logistic regression to predict preference outcomes. This creates a simple, interpretable model where feature coefficients indicate importance for preference prediction
- Core assumption: The relationship between feature differences and preference outcomes is approximately linear
- Evidence anchors:
  - [section 3.2]: "we employ logistic regression to classify the preferred response in a pairwise comparison dataset"
  - [section 4.6]: "CPMs, as linear models, have a high degree of interpretability Hastie et al. (2009)"
  - [corpus]: Weak evidence - corpus shows related work on preference modeling but doesn't specifically validate the linear combination approach
- Break condition: If the relationship between features and preferences is highly non-linear, the linear model will underperform

## Foundational Learning

- Concept: Logistic regression as a binary classifier
  - Why needed here: CPMs use logistic regression to combine feature scores into preference predictions, so understanding this algorithm is essential for implementation and debugging
  - Quick check question: How does logistic regression handle the fact that the preference scores are standardized feature differences rather than raw values?

- Concept: Prompt engineering for language models
  - Why needed here: The CPM relies on extracting feature scores via prompting, so understanding how to design effective prompts for different features is crucial
  - Quick check question: What prompt templates would you design to elicit scores for "helpfulness" versus "factuality" from a language model?

- Concept: Best-of-n sampling and overoptimization
  - Why needed here: The paper uses best-of-n sampling to evaluate robustness and overoptimization, so understanding this evaluation methodology is important for reproducing results
  - Quick check question: How does increasing n in best-of-n sampling affect the likelihood of reward hacking in standard preference models versus CPMs?

## Architecture Onboarding

- Component map: Feature extraction (LM prompts) -> Feature standardization -> Logistic regression -> Preference prediction
- Critical path: 1) Prompt language model to extract feature scores for training data 2) Standardize feature scores and compute pairwise differences 3) Train logistic regression on feature differences to predict preferences 4) During inference, extract features for new responses and combine via trained classifier
- Design tradeoffs:
  - Feature selection vs. model complexity: More features increase expressiveness but risk overfitting and reduce interpretability
  - Language model choice vs. computational cost: Larger LMs may extract better features but increase inference latency
  - Linear combination vs. non-linear models: Linear models are interpretable but may miss complex relationships
- Failure signatures:
  - Poor performance on test data: Likely indicates feature selection issues or poor prompt design
  - High variance between independently trained models: Suggests overfitting to training data
  - Large divergence from reference PMs: Indicates potential overoptimization or feature extraction problems
- First 3 experiments:
  1. Compare CPM performance with different feature subsets to identify most important features
  2. Test different language models (GPT-3.5 vs. Flan-T5) for feature extraction quality
  3. Vary the number of responses in best-of-n sampling to measure overoptimization resistance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of features in CPMs affect the performance of the preference model in terms of generalization and robustness to overoptimization?
- Basis in paper: [explicit] The paper discusses the use of 13 features for evaluating responses, but does not explore the impact of different feature choices on model performance
- Why unresolved: The paper uses a fixed set of features and does not investigate how different feature selections might influence the model's ability to generalize or resist overoptimization
- What evidence would resolve it: Experiments comparing the performance of CPMs with different sets of features, evaluating their generalization and robustness to overoptimization, would provide insights into the impact of feature selection

### Open Question 2
- Question: Can the feature extraction process in CPMs be further improved by using more advanced language models or by incorporating additional techniques such as chain-of-thought prompting?
- Basis in paper: [inferred] The paper uses GPT-3.5 and Flan-T5-XL for feature extraction, but does not explore the potential benefits of using more advanced models or techniques
- Why unresolved: The paper does not investigate whether using more capable language models or incorporating techniques like chain-of-thought prompting could enhance the feature extraction process and improve the overall performance of CPMs
- What evidence would resolve it: Experiments comparing the performance of CPMs using different language models or incorporating additional techniques like chain-of-thought prompting would provide insights into the potential improvements in feature extraction

### Open Question 3
- Question: How does the size of the dataset used to train CPMs affect their ability to generalize and resist overoptimization?
- Basis in paper: [explicit] The paper conducts experiments with different dataset sizes, but does not explore the relationship between dataset size and model performance in terms of generalization and overoptimization
- Why unresolved: The paper does not investigate how the size of the training dataset impacts the model's ability to generalize to unseen data or resist overoptimization
- What evidence would resolve it: Experiments varying the size of the training dataset and evaluating the resulting model's generalization and robustness to overoptimization would provide insights into the relationship between dataset size and model performance

## Limitations

- Feature selection lacks rigorous validation - the 13 features may not fully capture human preference dimensions
- Limited exploration of feature extraction consistency across different language models
- Linear combination assumption may be too restrictive for complex preference relationships

## Confidence

- High confidence in experimental methodology and implementation details
- Medium confidence in claimed robustness benefits due to synthetic overoptimization data
- Low confidence in feature selection mechanism due to lack of validation

## Next Checks

1. Conduct ablation studies removing individual features to quantify their contribution to preference prediction and identify potential redundant features
2. Test feature score consistency across multiple language models and prompt variations to establish the reliability of the feature extraction mechanism
3. Compare CPM performance against non-linear preference models (e.g., small neural networks) while maintaining interpretability to determine if the linear assumption is optimal