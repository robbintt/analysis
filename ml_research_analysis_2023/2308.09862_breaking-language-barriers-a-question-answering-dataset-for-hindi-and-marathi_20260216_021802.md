---
ver: rpa2
title: 'Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi'
arxiv_id: '2308.09862'
source_url: https://arxiv.org/abs/2308.09862
tags:
- dataset
- hindi
- language
- answer
- marathi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scarcity of high-quality Question-Answering
  datasets for low-resource languages, specifically Hindi and Marathi. To overcome
  this challenge, the authors propose a novel approach to translate the SQuAD 2.0
  dataset into these languages, creating the largest QA dataset for Hindi and Marathi,
  each containing 28,000 samples.
---

# Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi

## Quick Facts
- **arXiv ID**: 2308.09862
- **Source URL**: https://arxiv.org/abs/2308.09862
- **Reference count**: 8
- **Primary result**: Created the largest QA dataset for Hindi and Marathi (28,000 samples each) by translating SQuAD 2.0

## Executive Summary
This paper addresses the scarcity of high-quality Question-Answering datasets for low-resource languages, specifically Hindi and Marathi. The authors propose a novel approach to translate the SQuAD 2.0 dataset into these languages, creating the largest QA dataset for Hindi and Marathi, each containing 28,000 samples. The core method involves using machine translation and a sliding window technique to accurately determine the answer start index within the context, ensuring precise alignment of the translated answer. The authors evaluate various Transformer-based models, including multilingual and language-specific architectures, on the newly created datasets.

## Method Summary
The authors use IndicTrans by AI4Bharat to translate the SQuAD 2.0 dataset into Hindi and Marathi. To address the challenge of accurately determining the answer index within the translated context, they employ a sliding window technique that iterates through the context to find the subset with the highest similarity to the translated answer text. The resulting datasets are then used to fine-tune various Transformer-based models, including mBERT, XLM-RoBERTa, DistilBERT, Hindi BERT, and MahaBERT, using PyTorch with a batch size of 2, AdamW optimizer, and a learning rate of 1e-5.

## Key Results
- Created 28,000-sample QA datasets for both Hindi and Marathi
- Language-specific models (Hindi BERT, MahaBERT) outperformed multilingual models
- Hindi BERT achieved 47.84% exact match, MahaBERT achieved 42.97% exact match
- Sliding window technique successfully aligned translated answers with contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating SQuAD 2.0 to Hindi and Marathi using IndicTrans followed by a similarity-based sliding window approach yields a high-quality dataset.
- Mechanism: The approach first machine translates the SQuAD 2.0 dataset, then uses a sliding window technique to identify the correct answer start index by comparing contextual similarity between the translated answer and segments of the translated context.
- Core assumption: The translated answer will appear as a contiguous substring in the translated context, and similarity scores will reliably locate it.
- Evidence anchors:
  - [abstract] "Our approach tries to resolve the difficulty in accurately determining the index of the answer within the context."
  - [section] "To identify the indices of the answer within the context, we employed a sliding window technique...By iterating through the context with this window size, we aimed to locate the subset of the context that exhibited the highest similarity to the answer text."
  - [corpus] Weak evidence - related papers discuss similar challenges but do not validate this specific technique.
- Break condition: If the translated answer is split across multiple sentences or paraphrased significantly, the sliding window may fail to locate the correct index.

### Mechanism 2
- Claim: Language-specific models (Hindi BERT and MahaBERT) outperform general multilingual models on low-resource QA tasks.
- Mechanism: These models are fine-tuned on monolingual corpora specific to the target language, allowing them to capture language-specific nuances and patterns that multilingual models cannot.
- Core assumption: Language-specific pretraining on large monolingual corpora transfers better to downstream QA tasks than multilingual pretraining.
- Evidence anchors:
  - [abstract] "The results demonstrate that language-specific models, such as Hindi BERT and MahaBERT, outperform general multilingual models, achieving exact match scores of 47.84% and 42.97% for Hindi and Marathi, respectively."
  - [section] "The reason for Indic models outperforming general models is that these language-specific models focus solely on one language...This dedicated training approach helps them understand the unique aspects of that language."
  - [corpus] Moderate evidence - several related papers also show language-specific models performing better for Indian languages.
- Break condition: If the monolingual corpus is too small or of poor quality, the advantage over multilingual models may diminish.

### Mechanism 3
- Claim: Using MahaNLP library for NER tag prediction helps understand dataset composition and validate answer quality.
- Mechanism: By running NER on the answer texts, the authors can quantify what types of entities appear in answers (persons, organizations, locations, etc.) and ensure the dataset covers diverse entity types.
- Core assumption: NER tag distribution in answers reflects the dataset's coverage and quality.
- Evidence anchors:
  - [section] "Moreover, an analysis of NER tags reveals additional insights into the dataset...We use the MahaNLP library, trained on the L3Cube MahaNER dataset, to predict the NER tags."
  - [abstract] "We evaluate the dataset on various architectures and release the best-performing models for both Hindi and Marathi, which will facilitate further research in these languages."
  - [corpus] Weak evidence - no related papers explicitly mention NER analysis for QA dataset validation.
- Break condition: If the NER model is not well-trained on the target language, the tag predictions may be inaccurate and misleading.

## Foundational Learning

- Concept: Sliding window technique for string matching
  - Why needed here: To locate the translated answer within the translated context when direct index mapping fails due to translation differences.
  - Quick check question: How does the sliding window approach handle cases where the translated answer appears multiple times in the context?

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding why multilingual models work for low-resource languages but are outperformed by language-specific models.
  - Quick check question: What are the key differences between multilingual pretraining and monolingual pretraining that affect downstream QA performance?

- Concept: Evaluation metrics for QA systems
  - Why needed here: To properly assess model performance using Exact Match, Rouge-N, Rouge-L, and BLEU scores.
  - Quick check question: Why did the authors choose n=2 for both Rouge and BLEU scores, and how would the results differ with n=1 or n=3?

## Architecture Onboarding

- Component map: SQuAD 2.0 -> IndicTrans translation -> sliding window index adjustment -> dataset storage -> Pretrained LM -> fine-tuning -> evaluation
- Critical path: 1. Translate SQuAD 2.0 using IndicTrans 2. Apply sliding window technique to find answer indices 3. Fine-tune selected LMs on the processed dataset 4. Evaluate and select best-performing models
- Design tradeoffs:
  - Translation quality vs. speed: Using machine translation is faster than manual translation but may introduce errors that affect answer indexing
  - Sliding window window size: Larger windows increase accuracy but computational cost
  - Model selection: Using multiple models provides comparison but increases training time
- Failure signatures:
  - Low Exact Match scores with high Rouge scores: Indicates models generate similar content but not exact answers
  - High BLEU scores with low Rouge scores: Suggests n-gram overlap but poor overall sentence quality
  - Consistently failed sliding window matches: Indicates significant translation divergence between answer and context
- First 3 experiments:
  1. Train mBERT on a small subset of the translated dataset to establish a baseline
  2. Apply the sliding window technique to a manually translated sample to verify correctness
  3. Compare training curves of mBERT vs. HindiBERT on the same data to observe learning dynamics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do the models generalize to real-world scenarios in Marathi and Hindi?
- Basis in paper: [inferred] The authors note that their dataset is created from SQuAD 2.0 and that their method can be applied to create datasets for other Indic languages. They also note that the availability of large-scale datasets is important for training deep learning models, and that the scarcity of datasets for Indian languages hinders progress.
- Why unresolved: The authors only evaluate their models on the translated SQuAD 2.0 dataset, which may not fully represent real-world question-answering scenarios in Marathi and Hindi. Additionally, the authors do not discuss the potential limitations of using machine translation to create the dataset, which could introduce biases or errors that affect model performance.
- What evidence would resolve it: Testing the models on a diverse set of real-world question-answering tasks in Marathi and Hindi, such as those involving different domains, question types, and answer formats. Evaluating the models' performance on human-generated questions and answers, rather than just machine-translated ones. Investigating the impact of using different machine translation models or techniques on the quality and diversity of the dataset.

### Open Question 2
- Question: How does the proposed sliding window technique for answer extraction compare to other methods?
- Basis in paper: [explicit] The authors describe their sliding window technique for determining the answer start index within the context, which involves using a window of length equal to the size of the answer text and iterating through the context to locate the subset with the highest similarity to the answer text. They also mention that they use the MahaNLP library to calculate similarity.
- Why unresolved: The authors do not compare their sliding window technique to other methods for answer extraction, such as using pre-trained models or rule-based approaches. It is unclear whether their technique is the most effective or efficient for this task.
- What evidence would resolve it: Comparing the performance of the sliding window technique to other methods for answer extraction on the same dataset, using metrics such as accuracy, precision, and recall. Investigating the computational complexity and resource requirements of each method. Analyzing the impact of different similarity measures or thresholds on the performance of the sliding window technique.

### Open Question 3
- Question: How does the performance of the models on the Marathi and Hindi datasets compare to their performance on the original SQuAD 2.0 dataset?
- Basis in paper: [explicit] The authors evaluate their models on the translated Marathi and Hindi datasets and report their performance using metrics such as exact match, Rouge-2, Rouge-L, and BLEU scores. However, they do not compare these results to the performance of the models on the original SQuAD 2.0 dataset.
- Why unresolved: It is unclear how well the models perform on the translated datasets compared to the original dataset, which could provide insights into the quality and difficulty of the translated data.
- What evidence would resolve it: Evaluating the models on the original SQuAD 2.0 dataset using the same metrics as for the translated datasets. Comparing the performance of the models on the two datasets to identify any differences or trends. Investigating the impact of factors such as language, domain, and question type on the models' performance.

## Limitations
- Translation quality and its impact on downstream performance is not quantitatively measured
- Sliding window technique lacks detailed validation and may fail with paraphrased or split answers
- Computational resources required for translating the full SQuAD 2.0 dataset are not discussed

## Confidence
- High Confidence: Language-specific models (Hindi BERT, MahaBERT) outperform multilingual models (supported by specific EM scores)
- Medium Confidence: Sliding window technique effectiveness (supported by use but lacks detailed validation)
- Low Confidence: NER analysis providing dataset quality insights (minimally discussed and weakly supported)

## Next Checks
1. Manually evaluate a sample of translated QA pairs to quantify translation accuracy and identify patterns of translation errors
2. Create controlled experiments where translated answers are deliberately modified to test sliding window robustness
3. Document computational resources required for translating the full SQuAD 2.0 dataset and estimate scaling implications