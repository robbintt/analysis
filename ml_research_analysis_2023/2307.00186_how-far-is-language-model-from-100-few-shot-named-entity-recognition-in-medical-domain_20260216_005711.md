---
ver: rpa2
title: How far is Language Model from 100% Few-shot Named Entity Recognition in Medical
  Domain
arxiv_id: '2307.00186'
source_url: https://arxiv.org/abs/2307.00186
tags:
- entity
- medical
- few-shot
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an empirical comparison of Small Language Models
  (SLMs) and Large Language Models (LLMs) for few-shot medical Named Entity Recognition
  (NER), a task requiring near-perfect accuracy due to critical implications for human
  life. The authors evaluate 16 NER models (12 SLMs and 4 LLMs) on two standard medical
  NER datasets, BC5CDR and NCBI.
---

# How far is Language Model from 100% Few-shot Named Entity Recognition in Medical Domain

## Quick Facts
- arXiv ID: 2307.00186
- Source URL: https://arxiv.org/abs/2307.00186
- Authors: 
- Reference count: 11
- Primary result: LLMs outperform SLMs in few-shot medical NER when provided with high-quality examples and appropriate logical frameworks, but still face challenges with long, special, or out-of-vocabulary entities

## Executive Summary
This paper presents an empirical comparison of Small Language Models (SLMs) and Large Language Models (LLMs) for few-shot medical Named Entity Recognition (NER), a task requiring near-perfect accuracy due to critical implications for human life. The authors evaluate 16 NER models (12 SLMs and 4 LLMs) on two standard medical NER datasets, BC5CDR and NCBI. They find that LLMs outperform SLMs when provided with high-quality examples and appropriate logical frameworks, but still encounter challenges such as misidentification, wrong template prediction, and difficulty extracting long, special, or out-of-vocabulary entities. To address these issues, the authors propose a novel approach called RT (Retrieving and Thinking), which retrieves relevant examples using KNN and guides the LLM to recognize entities step-by-step. Experimental results show that RT significantly outperforms strong open baselines on both datasets.

## Method Summary
The paper evaluates 16 NER models (12 SLMs and 4 LLMs) on BC5CDR and NCBI medical NER datasets using few-shot learning with 1-shot and 5-shot settings. The RT framework implements KNN-based retrieval to find semantically similar training examples, followed by a step-by-step reasoning process using chain-of-thought prompting. The evaluation uses micro F1 score as the primary metric, comparing LLM performance with and without the RT framework against traditional SLM approaches.

## Key Results
- LLMs significantly outperform SLMs in few-shot medical NER when provided with high-quality examples and appropriate logical frameworks
- RT framework with KNN retrieval and chain-of-thought prompting achieves substantial improvements over strong open baselines on both BC5CDR and NCBI datasets
- Despite improvements, LLMs still struggle with long entities, special characters, and out-of-vocabulary terms, preventing near-perfect accuracy
- The quantity of samples has greater impact on SLMs compared to LLMs, with GPT-NER showing comparable performance across 1-shot and 5-shot settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieving relevant examples via KNN improves LLM performance on few-shot medical NER by providing contextually similar training instances
- Mechanism: The KNN-based retrieval finds sentences containing similar entity types and contexts, which are then used as demonstrations in the LLM's in-context learning
- Core assumption: Similar entity types and contexts in the training data will provide better demonstrations than random examples
- Evidence anchors: [abstract] "RT (Retrieving and Thinking), which retrieves relevant examples using KNN" [section] "In the initial step, we employ a random selection of LLMs to determine the labels that correspond to the test sentence... we retrieve the most pertinent examples for the given test sentence"

### Mechanism 2
- Claim: Chain-of-thought prompting improves entity recognition accuracy by breaking the task into simpler sub-decisions
- Mechanism: The model first identifies whether a token is an entity, then determines its label type through a step-by-step reasoning process
- Core assumption: Decomposing complex NER decisions into binary classification steps (is this an entity? what type?) reduces cognitive load on the model
- Evidence anchors: [abstract] "employing a step-by-step reasoning process" [section] "In the implementation of this step, we incorporate thinking examples within the instruction... These thinking examples aid the Language Labeling Model (LLM) in gradually recognizing medical entities"

### Mechanism 3
- Claim: LLMs require high-quality instructions and appropriate logical frameworks to outperform SLMs in few-shot medical NER
- Mechanism: The performance gap between LLMs and SLMs emerges when LLMs receive well-structured prompts with relevant examples and clear reasoning steps
- Core assumption: LLMs' superior performance is conditional on proper prompt engineering rather than inherent capability
- Evidence anchors: [abstract] "LLMs outperform SLMs when provided with high-quality examples and appropriate logical frameworks" [section] "LLMs demonstrate superior performance over SLMs when the LLMs are provided with high-quality instructions"

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The paper relies on LLMs learning from demonstrations within prompts rather than fine-tuning
  - Quick check question: How does in-context learning differ from traditional fine-tuning approaches for NER?

- Concept: Few-shot learning paradigm
  - Why needed here: The evaluation framework uses K-shot learning where only K examples per class are available
  - Quick check question: What are the key differences between 1-shot, 5-shot, and zero-shot learning in NER tasks?

- Concept: Named Entity Recognition tagging schemes
  - Why needed here: The paper uses "IO" tagging scheme where "I" indicates tokens inside entities and "O" indicates outside tokens
  - Quick check question: How does the "IO" scheme differ from "BIO" or "BILOU" schemes in handling entity boundaries?

## Architecture Onboarding

- Component map: Input sentence -> KNN retriever -> Label classifier -> Example selector -> LLM with chain-of-thought prompting -> Output formatter
- Critical path: Input sentence -> KNN retrieval -> Label identification -> Example selection -> Step-by-step reasoning -> Entity extraction
- Design tradeoffs: KNN retrieval adds computational overhead but improves example quality; chain-of-thought increases inference steps but improves accuracy
- Failure signatures: Poor entity boundary detection (misidentification), incorrect label assignment (class collision), inability to handle special characters (unable to generate symbols)
- First 3 experiments:
  1. Compare KNN-retrieved examples vs random examples on a small test set to validate retrieval effectiveness
  2. Test chain-of-thought vs direct prompting on simple entity recognition to validate the reasoning approach
  3. Evaluate RT performance with different K values in KNN to find optimal balance between retrieval quality and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can language models be improved to achieve near-perfect accuracy in medical Named Entity Recognition (NER)?
- Basis in paper: [explicit] The paper highlights that LLMs still encounter challenges in medical NER tasks, such as misidentification, wrong template prediction, and difficulty extracting long, special, or out-of-vocabulary entities. Despite LLMs outperforming SLMs, they still fall short of achieving 100% accuracy.
- Why unresolved: The paper introduces a novel approach called RT (Retrieving and Thinking) to improve LLM performance, but it does not achieve 100% accuracy. The reasons for the remaining inaccuracies, such as the inability to extract long, special entities, and out-of-vocabulary entities, are not fully addressed.
- What evidence would resolve it: Evidence that demonstrates the effectiveness of new methods or techniques in addressing the identified challenges and achieving near-perfect accuracy in medical NER tasks would help resolve this question.

### Open Question 2
- Question: How does the quality of examples impact the performance of language models in medical NER tasks?
- Basis in paper: [explicit] The paper mentions that the effectiveness of LLMs is greatly influenced by the careful selection of appropriate examples and the application of sound entity recognition logic. However, it also notes that the effectiveness of using retrieved relevant samples instead of random samples varies depending on the specific datasets employed in the evaluation.
- Why unresolved: The paper does not provide a clear answer on the optimal quality and selection criteria for examples that consistently improve LLM performance in medical NER tasks. The impact of example quality may vary across different datasets and contexts.
- What evidence would resolve it: Empirical studies comparing the performance of language models using different qualities and selection criteria for examples in medical NER tasks would provide insights into the optimal approach for improving model performance.

### Open Question 3
- Question: How does the amount of training data affect the performance of Small Language Models (SLMs) compared to Large Language Models (LLMs) in medical NER tasks?
- Basis in paper: [explicit] The paper states that the quantity of samples has a greater impact on SLMs compared to LLMs. It provides an example where GPT-NER demonstrates comparable performance on both 1-shot and 5-shot datasets, while SLMs like MetaMER and ProtoBERT show significant performance differences between the two.
- Why unresolved: The paper does not provide a detailed analysis of the specific thresholds or data requirements that determine the performance differences between SLMs and LLMs in medical NER tasks. It also does not explore the potential for improving SLM performance through data augmentation or other techniques.
- What evidence would resolve it: Empirical studies comparing the performance of SLMs and LLMs using varying amounts of training data in medical NER tasks would help identify the data requirements and potential for improving SLM performance.

## Limitations

- The evaluation only considers two medical NER datasets (BC5CDR and NCBI), which may not represent the full complexity of medical text
- The comparison between SLMs and LLMs is potentially confounded by different training approaches (fine-tuning vs in-context learning)
- The RT framework's performance improvements are demonstrated only on the specific datasets used, without testing generalization to other medical domains or languages

## Confidence

- **High Confidence**: LLMs outperform SLMs in few-shot medical NER when provided with high-quality examples and appropriate logical frameworks
- **Medium Confidence**: RT framework significantly outperforms strong open baselines on evaluated datasets
- **Low Confidence**: Language models are approaching "100% few-shot NER" in the medical domain

## Next Checks

1. Evaluate RT on additional medical NER datasets beyond BC5CDR and NCBI, including clinical notes, pathology reports, and radiology findings to assess real-world applicability and robustness across medical subdomains

2. Measure RT's computational overhead and latency in production settings, including KNN retrieval time, memory requirements, and inference costs, to determine practical deployment feasibility

3. Systematically categorize RT's failure modes (misidentification, wrong template prediction, special entity handling) and compare them against human expert performance on the same tasks to establish a realistic baseline for "near-perfect" accuracy