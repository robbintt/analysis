---
ver: rpa2
title: 'Detecting agreement in multi-party dialogue: evaluating speaker diarisation
  versus a procedural baseline to enhance user engagement'
arxiv_id: '2311.03021'
source_url: https://arxiv.org/abs/2311.03021
tags:
- system
- agreement
- answer
- diarisation
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compared two approaches for detecting agreement in
  multi-party spoken interactions: a speaker diarisation method and a simple frequency-and-proximity-based
  baseline. The baseline system was more accurate at detecting agreement (0.44 average
  accuracy) than the diarised system (0.28), and also led to more engaging interactions
  in a cooperative quiz game.'
---

# Detecting agreement in multi-party dialogue: evaluating speaker diarisation versus a procedural baseline to enhance user engagement

## Quick Facts
- **arXiv ID**: 2311.03021
- **Source URL**: https://arxiv.org/abs/2311.03021
- **Reference count**: 32
- **Primary result**: A frequency-and-proximity baseline for detecting agreement outperformed a diarisation-based approach, with 0.44 vs 0.28 average accuracy in a cooperative quiz game.

## Executive Summary
This study compared two approaches for detecting agreement in multi-party spoken interactions: a speaker diarisation method and a simple frequency-and-proximity-based baseline. The baseline system was more accurate at detecting agreement (0.44 average accuracy) than the diarised system (0.28), and also led to more engaging interactions in a cooperative quiz game. The results suggest that, for certain applications like simple quiz games, non-diarised systems can be more effective than current diarisation methods, which still struggle with noisy environments and similar voices.

## Method Summary
The study compared two dialogue management systems for detecting agreement in multi-party spoken interactions during a cooperative flag-identification quiz game. The baseline system used a frequency-and-proximity approach, counting consecutive "give answer" intents and inferring agreement when the last two answers matched and a threshold was met. The diarised system attempted to attribute answers to specific speakers using Google Cloud STT diarisation before comparing for agreement. Both systems used Rasa NLU for intent and entity recognition, and OpenAI GPT for generating game utterances and clues. The study involved 8 participants (4 pairs) playing 3-flag games, with agreement detection accuracy and engagement assessed through observation and semi-structured interviews.

## Key Results
- The baseline system achieved 0.44 average agreement detection accuracy, compared to 0.28 for the diarised system.
- The procedural baseline led to more engaging interactions than the diarised system in the cooperative quiz game.
- Speaker diarisation struggled with noisy environments and similar voices, resulting in failed agreement detection.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The procedural baseline achieves higher agreement detection accuracy because it relies on simple frequency-and-proximity rules rather than complex diarization.
- Mechanism: The baseline counts consecutive "give answer" intents and compares the last two answers. If they match and the count meets a threshold, agreement is inferred.
- Core assumption: Participants repeat answers or explicitly agree within a small number of turns; false positives from repeated guesses are minimized by the threshold.
- Evidence anchors:
  - [abstract] "The baseline system was more accurate at detecting agreement (0.44 average accuracy) than the diarised system (0.28)"
  - [section] "agreement = Y iff. (Nanswers ≥ NA) and (answert−1 = answert)" with NA = 3 tuned via experimentation
  - [corpus] Weak support: neighbors focus on diarization and multi-party dialogue but do not directly discuss frequency-based agreement detection
- Break condition: If participants use elimination or non-repetitive reasoning, the threshold may miss agreement or delay detection.

### Mechanism 2
- Claim: The diarized system underperforms because Google Cloud STT diarization fails to reliably separate speakers in noisy, similar-voice environments.
- Mechanism: The DM compares current answers from each identified speaker; if both match, agreement is inferred. Poor diarization leads to misattribution and missed agreement.
- Core assumption: Speaker diarization is accurate enough to attribute utterances correctly in real-time.
- Evidence anchors:
  - [abstract] "diarised system (0.28)" vs baseline (0.44) accuracy
  - [section] "The diarised system could not reliably distinguish participants' voices, resulting in failed agreement detection"
  - [corpus] Some support: "Speaker diarization(SD) is a classic task in speech processing and is crucial in multi-party scenarios" but current methods still struggle
- Break condition: In clear acoustic conditions with dissimilar voices, diarization could become accurate enough to surpass the baseline.

### Mechanism 3
- Claim: Engagement is higher with the procedural system because it provides quicker, more reliable responses, reducing user frustration from repeated attempts.
- Mechanism: The system detects agreement after three consecutive matching answers, then immediately asks for confirmation, keeping interaction flow natural.
- Core assumption: Users value responsiveness over perfect understanding; repeated confirmations are less frustrating than silence or misattribution.
- Evidence anchors:
  - [abstract] "our procedural system was more engaging to players"
  - [section] "players became frustrated with the game as they did not know what they could say to get a response" when systems were unresponsive
  - [corpus] Limited: neighbors discuss engagement and responsiveness but not specifically tied to agreement detection method
- Break condition: If the baseline threshold is set too high, it may miss agreement and frustrate users by requiring excessive repetition.

## Foundational Learning

- Concept: Speaker diarization basics (segmentation, clustering, voice ID)
  - Why needed here: To understand why the diarized system fails and how the baseline avoids this complexity
  - Quick check question: What are the two main stages of speaker diarization, and how do they differ from simple frequency counting?

- Concept: Intent and entity recognition in conversational AI
  - Why needed here: Both systems rely on NLU to detect "Give answer", "Agree", and "Disagree" intents and country entities
  - Quick check question: How does entity recognition failure (e.g., confusing "Cyprus" with "Cypress") affect implicit agreement detection?

- Concept: Dialogue state tracking and confirmation logic
  - Why needed here: The DM uses agreement detection to trigger confirmation questions, which must be timed correctly to maintain engagement
  - Quick check question: Why does the baseline use a threshold of 3 turns, and what happens if agreement occurs earlier?

## Architecture Onboarding

- Component map:
  - Frontend: Laptop screen + TTS voice output
  - STT: Google Cloud Speech-to-Text (diarized vs non-diarized modes)
  - NLU: Rasa Open Source (intent/entity extraction)
  - DM: Rule-based Dialogue Manager (baseline vs diarized logic)
  - NLG: OpenAI GPT API (generated host utterances and clues)
  - Game logic: Flag quiz with four-choice answers, agreement detection, clue/skip commands

- Critical path:
  1. TTS asks flag question
  2. STT captures audio → NLU → DM
  3. DM updates agreement state (baseline: count + compare; diarized: speaker-specific compare)
  4. If agreement detected → DM triggers confirmation
  5. DM responds based on user input

- Design tradeoffs:
  - Baseline: Simple, reliable, but can miss quick agreements or require extra turns
  - Diarized: Theoretically cleaner logic but fragile under real-world noise and voice similarity
  - Fixed threshold vs adaptive: Baseline uses fixed NA=3; adaptive could improve but adds complexity

- Failure signatures:
  - Baseline: Missed agreement if first two answers match; false positives if same answer repeated for emphasis
  - Diarized: Agreement never detected if diarization misattributes speakers; long delays if system waits for correct attribution
  - NLU: Low entity recognition leads to no implicit agreement detection

- First 3 experiments:
  1. Run the baseline with NA=2, NA=3, NA=4 and measure agreement detection rate vs user frustration (interview/feedback)
  2. Force diarization to use only one speaker label and compare agreement detection to baseline (isolates diarization error)
  3. Simulate high-noise audio to baseline and diarized system to quantify robustness difference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of the baseline system change with more than two participants?
- Basis in paper: [explicit] The paper notes that the threshold-based approach might need adaptation for more players due to increased negation statements.
- Why unresolved: The study only tested with pairs of participants, so the impact of additional players on the system's accuracy is unknown.
- What evidence would resolve it: Conducting experiments with groups of three or more participants and comparing the agreement detection accuracy to the two-player scenario.

### Open Question 2
- Question: What is the optimal threshold value (NA) for agreement detection in different types of quiz games or conversational contexts?
- Basis in paper: [explicit] The paper mentions that the threshold of 3 was found to provide suitable performance for the flag identification game, but acknowledges it might differ with additional players or different game types.
- Why unresolved: The study only evaluated one specific game type, so the generalizability of the threshold value to other contexts is unclear.
- What evidence would resolve it: Testing the baseline system with various quiz games or conversational scenarios and determining the threshold value that maximizes agreement detection accuracy in each case.

### Open Question 3
- Question: How do different noise levels and audio qualities affect the performance of diarization systems in multi-party interactions?
- Basis in paper: [inferred] The paper mentions that diarization is challenging in noisy environments and with similar voices, but does not provide specific data on how different noise levels impact performance.
- Why unresolved: The study did not systematically vary noise levels or audio quality to assess their impact on diarization accuracy.
- What evidence would resolve it: Conducting experiments with the diarized system in controlled environments with varying levels of background noise and audio quality, and measuring the resulting agreement detection accuracy.

## Limitations
- The baseline accuracy of 0.44 is modest and may not generalize beyond simple, repetitive quiz contexts where participants naturally echo answers.
- The study's small sample (8 participants, 4 pairs) and limited game rounds constrain statistical power and external validity.
- Unknown NLU training data and GPT prompt details may affect reproducibility of engagement and agreement detection results.

## Confidence
- **High confidence**: The procedural baseline outperforms diarization in this specific task and context; diarization struggles with noisy, similar-voice environments
- **Medium confidence**: The engagement benefit of the procedural system is plausible but relies on limited qualitative evidence
- **Low confidence**: The baseline's 0.44 accuracy is presented as a clear win without deeper analysis of error types or alternative thresholds

## Next Checks
1. Test the baseline system with varying thresholds (NA=2, 3, 4) and measure both agreement detection accuracy and participant frustration via post-game interviews
2. Run the diarized system with artificially perfect speaker attribution to isolate whether poor diarization or flawed logic causes low accuracy
3. Conduct a larger, more diverse user study (n≥20 pairs) across multiple game types to assess external validity and robustness of both systems