---
ver: rpa2
title: 'KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology
  Report Summarization'
arxiv_id: '2307.07409'
source_url: https://arxiv.org/abs/2307.07409
tags:
- report
- task
- radiology
- pages
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present CheXOFA, a pre-trained vision-language model
  for chest X-ray report summarization. The model is first pre-trained on general-domain
  multimodal datasets using a unified sequence-to-sequence schema, then transferred
  to the medical domain.
---

# KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization

## Quick Facts
- arXiv ID: 2307.07409
- Source URL: https://arxiv.org/abs/2307.07409
- Authors: 
- Reference count: 15
- Primary result: Achieved first place on RadSum23 hidden test set with 18.62 BLEU and 43.20 F1-RadGraph scores

## Executive Summary
This paper presents CheXOFA, a pre-trained vision-language model for chest X-ray report summarization. The model leverages transfer learning from general-domain multimodal tasks to the medical domain, fine-tuning on MIMIC-CXR for the specific task of converting findings sections into impression sections. The system employs ensemble methods and factual calibration to improve factual correctness, achieving state-of-the-art results that surpass the second-best model by 2.3 points in BLEU and 2.9 points in F1-CheXbert scores.

## Method Summary
CheXOFA is a vision-language model initialized from the OFA backbone and pre-trained on general multimodal tasks in sequence-to-sequence format. The model is then fine-tuned on MIMIC-CXR for report summarization, where it jointly encodes chest X-ray images with findings sections to generate impression sections. The system employs ensemble methods across multiple model variants, selecting predictions based on similarity scoring, followed by factual calibration that checks medical observations against disease labels from a classification model to ensure consistency.

## Key Results
- Achieved first place on RadSum23 hidden test set leaderboard
- Obtained 18.62 BLEU and 43.20 F1-RadGraph scores
- Improved over second-best model by 2.3 points in BLEU and 2.9 points in F1-CheXbert scores

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Pre-training Transfer
Pre-training on diverse multimodal tasks improves downstream radiology report generation by providing rich representation spaces that transfer well to the medical domain. The CheXOFA model benefits from skills learned during pre-training on tasks like image classification, language modeling, and image captioning. This broad pre-training provides a foundation that can be effectively adapted to specialized medical report generation. The core assumption is that general multimodal reasoning skills transfer to medical contexts, though this may break if pre-training tasks are too dissimilar from medical report generation.

### Mechanism 2: Ensemble with Factual Calibration
Ensemble methods with factual calibration improve factual correctness by combining diverse model predictions and ensuring consistency with medical labels. Multiple model predictions are combined using similarity scoring, with the best prediction selected based on mutual similarity. Factual calibration then checks extracted medical observations against disease labels from a classification model to ensure consistency. The core assumption is that diverse predictions contain complementary information, though this benefit diminishes if ensemble models produce highly similar predictions.

### Mechanism 3: Multimodal Input Integration
Multimodal input (image + text) improves report summarization quality by providing complementary context that enhances text-only summarization. The model jointly encodes visual information from chest X-ray images with findings section text, leveraging both modalities to generate more accurate impression sections. The core assumption is that visual information provides discriminative context for text generation, though this may add noise if visual features don't provide relevant information for certain findings.

## Foundational Learning

- **Sequence-to-sequence learning with transformer architectures**: Needed because the model must generate text summaries from multimodal inputs (images and reports). Quick check: What is the difference between encoder-only, decoder-only, and encoder-decoder transformer architectures?

- **Vision-language pre-training and transfer learning**: Needed because the model leverages pre-training on general tasks before fine-tuning on medical data. Quick check: How does pre-training on general multimodal tasks help with specialized medical report generation?

- **Medical domain knowledge and terminology**: Needed because the model must accurately represent medical findings and maintain factual correctness. Quick check: What are common chest X-ray findings that the model needs to recognize?

## Architecture Onboarding

- **Component map**: Visual encoder (ResNet-based) → Patch processing → Visual features → Text encoder (BPE tokenization) → Linguistic features → Transformer encoder-decoder → Multimodal fusion → Sequence generation → Ensemble module → Similarity scoring → Prediction selection → Factual calibration module → CheXbert labeler → Consistency checking → Output

- **Critical path**: Image/text input → Multimodal encoding → Transformer generation → Ensemble selection → Factual calibration → Output

- **Design tradeoffs**: Multimodal vs text-only (multimodal provides better accuracy but increases computational cost), ensemble size vs performance (larger ensembles improve accuracy but increase inference time), beam search width vs quality (wider beams find better outputs but slow inference)

- **Failure signatures**: Low BLEU/ROUGE scores (model struggles with language generation quality), low F1-CheXbert/F1-RadGraph scores (factual errors or hallucinations in medical content), high variance between ensemble members (model uncertainty or poor convergence)

- **First 3 experiments**: 1) Train text-only CheXOFA on MIMIC-CXR and compare to multimodal version, 2) Test ensemble performance with 2, 4, and 8 model variants, 3) Evaluate factual calibration effectiveness by measuring F1-CheXbert improvements

## Open Questions the Paper Calls Out

### Open Question 1
How effective is the factual calibration technique in improving the factual correctness of generated summaries? The paper only provides a single performance improvement number (0.3 F1-CheXbert and 0.8 F1-RadGraph) without detailed analysis of its effectiveness in different scenarios or ablation studies.

### Open Question 2
How does the ensemble method contribute to the overall performance improvement? The paper mentions performance gains from ensemble methods but lacks detailed analysis of the ensemble method's contribution or ablation studies on different evaluation metrics.

### Open Question 3
How does the pre-training task contribute to the downstream task performance? While the paper mentions that pre-training with report generation improves report summarization, it doesn't provide detailed analysis of the pre-training task's contribution or ablation studies on different evaluation metrics.

## Limitations
- Model architecture details beyond standard Transformer seq2seq are not fully specified
- Exact hyperparameters for training (learning rate, batch size, beam width) are unspecified
- Limited analysis of domain shift between MIMIC-CXR training data and CheXpert test data

## Confidence

- **High confidence**: Multimodal pre-training improving medical report generation (supported by established transfer learning literature)
- **Medium confidence**: Ensemble and factual calibration effectiveness (sparse implementation details provided)
- **Medium confidence**: Reported results (lack of open-source code and detailed ablation studies)

## Next Checks

1. Replicate core finding by training text-only CheXOFA on MIMIC-CXR and comparing performance to multimodal version to quantify visual modality contribution

2. Conduct controlled experiments varying ensemble size (2, 4, 8 models) while holding other factors constant to determine optimal configuration

3. Implement and test factual calibration procedure independently by measuring F1-CheXbert improvements with and without calibration across multiple runs