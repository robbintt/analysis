---
ver: rpa2
title: Automating reward function configuration for drug design
arxiv_id: '2312.09865'
source_url: https://arxiv.org/abs/2312.09865
tags:
- reward
- function
- functions
- molecules
- drug
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing reward functions
  for generative molecular design (GMD) in drug discovery, which is traditionally
  a manual and error-prone process. The authors propose an automated approach that
  constructs rankings over experimental data using Pareto dominance relationships
  and trains a neural network to approximate the reward function such that these rankings
  are preserved.
---

# Automating reward function configuration for drug design

## Quick Facts
- arXiv ID: 2312.09865
- Source URL: https://arxiv.org/abs/2312.09865
- Reference count: 17
- This paper proposes an automated approach to configure reward functions for generative molecular design in drug discovery, validated on both synthetic and real drug discovery project data.

## Executive Summary
This paper addresses the challenge of designing reward functions for generative molecular design in drug discovery, which is traditionally a manual and error-prone process. The authors propose an automated approach that constructs rankings over experimental data using Pareto dominance relationships and trains a neural network to approximate the reward function such that these rankings are preserved. They validate their method through two case studies: one simulating DMTA cycles using synthetic goals from the GuacaMol benchmark, and another using historical data from four real drug discovery projects.

## Method Summary
The method constructs Pareto dominance rankings over molecules based on their assay results, creating preference pairs where one molecule dominates another. A neural network is then trained to approximate a reward function that preserves these rankings, using cross-entropy loss on the preference pairs. The learned reward function is used to guide a generative molecular design tool (Graph GA) in proposing new molecules. The approach requires only the selection of relevant evaluation functions rather than manual specification of weights and scaling parameters, making it less biased than traditional methods.

## Key Results
- The learned reward functions guided a GMD tool to generate molecules with scores comparable to those achieved using the target reward function within 10 DMTA iterations
- In real drug discovery projects, the method yielded reward functions that outperformed human-defined functions, achieving up to 0.4 improvement in Spearman's correlation against ground truth evaluation functions
- The automated approach eliminated the need for manual weight tuning and scaling parameter specification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pareto dominance ranking reduces bias compared to manual weight tuning
- Mechanism: Instead of manually specifying scalarization weights, the method uses pairwise Pareto dominance relations to encode preferences. A molecule dominates another if it is at least as good on all assays and strictly better on at least one.
- Core assumption: The true project goal can be encoded as a point T in assay space and dominance over T preserves ordering relevant to that goal.

### Mechanism 2
- Claim: Learning a reward network from preference pairs approximates the true utility function
- Mechanism: A neural network is trained to predict which of two molecules is preferred, using the softmax probability and cross-entropy loss. The network learns weights and scaling for each assay component to align predicted rankings with Pareto-based rankings.
- Core assumption: The preference structure over molecules induced by Pareto dominance reflects the underlying reward function we want to learn.

### Mechanism 3
- Claim: Iterative DMTA simulation improves reward function adaptation over time
- Mechanism: In simulated cycles, the reward function is updated using all available data up to that cycle, then used to guide a GMD tool to propose new molecules. This loop allows the reward to adapt as more diverse data becomes available.
- Core assumption: Early suboptimal molecules still contain information about the project's goal space and can be used to bootstrap learning.

## Foundational Learning

- Concept: Pareto dominance and multi-objective optimization
  - Why needed here: The method relies on constructing partial rankings over molecules using dominance instead of scalarization. Understanding dominance is essential to grasp how preferences are encoded.
  - Quick check question: Given two molecules with assay values (0.8, 0.6) and (0.7, 0.7), does either dominate the other relative to target (0.75, 0.65)?

- Concept: Inverse Reinforcement Learning (IRL)
  - Why needed here: The method frames reward learning as IRLâ€”inferring a reward function from observed preferences (dominance pairs) rather than from optimal trajectories.
  - Quick check question: How does IRL differ from imitation learning when applied to molecular design?

- Concept: Reinforcement Learning-guided generative design
  - Why needed here: The learned reward is used to guide a GMD algorithm (Graph GA) to explore chemical space. Understanding how reward shapes exploration is key to interpreting results.
  - Quick check question: What role does the reward function play in shaping the exploration vs exploitation balance in Graph GA?

## Architecture Onboarding

- Component map: Experimental data -> Pareto dominance ranking -> Preference pairs -> Neural network reward function -> Graph GA GMD tool -> Generated molecules
- Critical path: 1. Load assay data for molecules 2. Build dominance pairs 3. Initialize reward network 4. Train via cross-entropy 5. Use reward to guide GMD 6. Evaluate and iterate